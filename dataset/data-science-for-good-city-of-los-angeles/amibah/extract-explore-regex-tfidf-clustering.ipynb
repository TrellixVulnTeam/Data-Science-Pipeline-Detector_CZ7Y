{"cells":[{"metadata":{"trusted":true},"cell_type":"markdown","source":"# DATASCIENCE4GOOD : City of LA"},{"metadata":{},"cell_type":"markdown","source":"## Help the City of Los Angeles to structure and analyze its job descriptions"},{"metadata":{},"cell_type":"markdown","source":"### **Please do not hesitate to comment, critic and vote so it will help to improve the approach by doing better or differently the things :)** <br>"},{"metadata":{},"cell_type":"markdown","source":"## Table of contents\n1. [Introduction](#intro)<br>\n    1.1. [Libraries](#lib)<br>\n    1.2. [Purpose & strategy](#purp)<br>\n    1.3. [Folder content](#Folcont)<br>\n    \n2. [Structure of the bulletins](#struct)<br>\n    2.1. [Example](#expl)<br>\n    2.2. [Load the structure](#load)<br>\n    2.3. [Standardization of the structure](#stand)<br>\n    \n3. [Extraction of data](#extract)<br>\n    3.1. [Administration data : <br>CLASS CODE, OPEN DATE, REVISED DATE, EXAM OPEN TO](#admin)<br>\n    3.2. [Salary data : <br> *QUANTITATIVE* : DEPARTEMENT (WATER POWER, HARBOR, AIRPORT) & GENERAL SALARY, RANGE & FLAT SALARY,<br> *QUALITATIVE* : LOWER & HIGHER PAY REASON](#sala)<br>\n    3.3. [Minimum requirement qualification : ](#minireq)<br>"},{"metadata":{},"cell_type":"markdown","source":"## 1. Introduction\n\n<a id=\"intro\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### 1.1. Libraries\n\n<a id=\"lib\"></a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Librairies for data treatment\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport collections as col\nfrom collections import Counter\n\n#Libraries for text treatment\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nstopWords = set(stopwords.words('english'))\nfrom textblob import TextBlob\nfrom difflib import SequenceMatcher\n\n#Library for file loading\nimport os\nimport csv\n\n#Library for ML\nfrom sklearn import feature_extraction\n\npd.options.display.max_colwidth = 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2. Purpose & strategy\n<a id=\"purp\"></a>"},{"metadata":{},"cell_type":"markdown","source":"The puporse of this challenge is to struture, extract and analyse the data of the job descriptions contained in the bulletins.\nOur strategy to achieve thoses challenges both on the form and the content:\n* Standardize the structure of the bulletins - Form\n* Extract the data from the bulletins - Content\n* Analyse how the form and the content influence the applicants"},{"metadata":{},"cell_type":"markdown","source":"### 1.3. Folder content\n<a id=\"Folcont\"></a>"},{"metadata":{},"cell_type":"markdown","source":"The main folder CityofLA include two subfodlers:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for subfold in os.listdir(\"../input/cityofla/CityofLA/\"):\n    print(subfold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The subfolder Aditionnal data contain th following files or folders :"},{"metadata":{"trusted":true},"cell_type":"code","source":"for subfold in os.listdir(\"../input/cityofla/CityofLA/Additional data\"):\n    print(subfold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The three csv files job_titles, kaggle_data_dictionnary and sample_job_class_export_template.\nLet's explore them to get information about the documents."},{"metadata":{"trusted":true},"cell_type":"code","source":"for subfold in os.listdir(\"../input/cityofla/CityofLA/Additional data\") :\n    if \".csv\" in subfold :\n        file_csv=pd.read_csv(\"../input/cityofla/CityofLA/Additional data/\"+subfold)\n        print(subfold + ', ' + str(file_csv.shape[0]) + ' rows, ' + str(file_csv.shape[1]) + ' columns')\n        display(file_csv.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> \nWe will focus on the 668 job positions that we should retrieve in the folder Job Bulletins"},{"metadata":{"trusted":true},"cell_type":"code","source":"file_txt=os.listdir(\"../input/cityofla/CityofLA/Job Bulletins/\")\nprint('Number of elements in the folder Job Bulletins : ' + str(len(file_txt)))\nexpl=5\nprint('Here the {0} first txt job bulletins : '.format(str(expl)))\nfor name_txt in file_txt[:expl]:\n    print(name_txt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice here that there is a difference between the number of the job positions specified by the csv file job_titles and the number of job descriptions as txt file.\nLet'sb check the gap between the two list."},{"metadata":{},"cell_type":"markdown","source":"We can expect that several job descriptions rely to a same job position."},{"metadata":{"trusted":true},"cell_type":"code","source":"frequencyBulletin=col.Counter([re.split(\"\\s+[0-9]|\\.+\",x)[0] for x in sorted(file_txt)])\nfrequencyBulletin=pd.DataFrame.from_dict(frequencyBulletin,orient='index',columns=['Count_txt'])\nfrequencyBulletin.index.names = ['Name_txt']\nfrequencyBulletin.reset_index(inplace=True)\nfrequencyBulletin.loc[frequencyBulletin.Count_txt>1]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can expect that some job bulettins are not reprensented in the list of job title positions contained in the csv."},{"metadata":{"trusted":true},"cell_type":"code","source":"jobTitle=pd.read_csv(\"../input/cityofla/CityofLA/Additional data/job_titles.csv\", names=['Name_csv'])\njobTitle.Name_csv=jobTitle.Name_csv.replace(to_replace=\"'|&\", value='_', regex=True)\ntitleJobBulletin=frequencyBulletin.merge(jobTitle, how='outer', left_on='Name_txt', right_on='Name_csv')\ntitleJobBulletin.loc[(titleJobBulletin.Name_txt.isnull())|(titleJobBulletin.Name_csv.isnull())]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobTitle=pd.read_csv(\"../input/cityofla/CityofLA/Additional data/job_titles.csv\", names=['Name_csv'])\njobTitle.Name_csv=jobTitle.Name_csv.replace(to_replace=\"'|&\", value='_', regex=True)\ntitleJobBulletin=frequencyBulletin.merge(jobTitle, how='outer', left_on='Name_txt', right_on='Name_csv')\nprint(\"\"\"Thus we know that \nfor {0} job titles we have a bulletin;\nfor {1} job bulletins we don't have any reference in the job title csv file.\"\"\".format(1+len(titleJobBulletin.loc[(~titleJobBulletin.Name_txt.isnull())&(~titleJobBulletin.Name_csv.isnull()),'Name_txt']),\n                                                                                     len(titleJobBulletin.loc[(~titleJobBulletin.Name_txt.isnull())&(titleJobBulletin.Name_csv.isnull()),'Name_csv'])-1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Structure of the bulletin\n\n<a id=\"struct\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### 2.1. Example\n\n<a id=\"expl\"></a>"},{"metadata":{},"cell_type":"markdown","source":"We take as example a bulletin to have a first understanding of the structure."},{"metadata":{"trusted":true},"cell_type":"code","source":"job=\"SENIOR REAL ESTATE OFFICER 1961 0413018 (2).txt\"\nprint(job)\nfile=open(r\"../input/cityofla/CityofLA/Job Bulletins/\"+job,\"r\")\ntxt_file=file.read()\ntxt_file[:500]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2. Load the structure\n\n<a id=\"load\"></a>"},{"metadata":{},"cell_type":"markdown","source":"We analyse the bulletin txt file and different elements structure the document :\n* Elements in CAPITAL letters\n* \\n for new line symbol\n* \\t for tab symbol\n* Succesions of dots\n* List of points 1., 2., 3.,... ect"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fromTxt2Dataframe (string,job):\n    \"\"\"\n    Input (string) : content of the bulletin txt file\n    Output (dataframe) : \n                        header the title part (capital letter) ; \n                        row the  content between two title parts (list with each item is a line)\n    This function transform the txt file into a semi structure dataframe as first step    \n    \"\"\"\n\n    # we seperate each line\n\n    superList=[]\n    cellList=[]\n    key=''\n    note_flag=0\n    \n    #string=re.sub(re.split(\"\\s+[0-9]|\\.+\",job)[0].replace(' ','[\\n\\s]*'), \"\", string)\n    \n    string=re.sub(\"\\t\", \" \", string)\n    string=re.sub(\" +\", \" \", string)\n    string=re.sub(\"\\s\\.+\", \"\", string)\n    string=re.sub(\"[\\s ]*\\n[\\s ]*\", \"\\n\", string)\n    string=re.sub(\"^\\s+\", \"\", string)\n    string=[x for x in re.split(\"[\\n\\t]\",string) if not (x==''or x==' ')]\n    \n    stringLen=len(string)-1\n    \n    superList.append(('JOB_NAME_TXT',re.sub(\" +\", \" \",re.split(\"\\s+[0-9]|\\.+\",job)[0]).upper()))  \n    \n    if string[0]=='CAMPUS INTERVIEWS ONLY':\n        start=2\n        superList.append((string[0],string[0]))\n        title=string[1]\n        item=string[start]\n        while item.isupper() and re.search(\"[0-9]\", item)==None:\n            title=title + \" \" + item\n            start += 1\n            item=string[start]\n        superList.append(('JOB_TITLE_TXT',title.upper()))\n\n    else :\n        start=1\n        title=string[0]\n        item=string[start]\n        while item.isupper() and re.search(\"[0-9]\", item)==None:\n            title=title + \" \" + item\n            start += 1\n            item=string[start]\n        superList.append(('JOB_TITLE_TXT',title.upper()))\n    \n    key='ADMINISTRATION'\n    \n    for index, item in enumerate(string[start:]):\n        # we keep capital letters as title part\n        if item.isupper() and re.search(\"[0-9]\", item)==None:\n            if cellList:\n                if re.search(r\"^NOTES*\",item):\n                    if note_flag==0:\n                        superList.append((key,cellList))\n                        cellList=[]\n                        key=key + \" \" + item\n                        note_flag=1\n                else :\n                    superList.append((key,cellList))\n                    cellList=[]\n                    key=item\n                    note_flag=0\n            else:\n                key=key+' '+item\n            \n            #print(note_flag, key)\n        # otherwise it's the content of the part\n        else:\n            cellList.append(item)\n            if index==stringLen:\n                superList.append((key,cellList))\n    \n    df_position=pd.DataFrame([dict(superList)],columns=dict(superList).keys())\n    return df_position","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fromTxt2Dataframe(txt_file,job)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3. Standardization of the structure\n\n<a id=\"stand\"></a>"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We need to standardize the headers of the dataframe we built :\n* Remove stop words and ponctuation if any\n* Correct wording mistake\n* Singularize words\n* Combine NOTE(S) headers "},{"metadata":{"trusted":true},"cell_type":"code","source":"def standardize2Aggregate(listOfFile,dictOfStandard=dict(), step='standardize'):\n    \n    list_metaData=[]\n    df1_metaData=pd.DataFrame()\n    \n    for job in listOfFile:\n        file=open(r\"../input/cityofla/CityofLA/Job Bulletins/\"+job,\"r\")\n        try:\n            txt_file=file.read()\n            df=fromTxt2Dataframe(txt_file,job)\n\n            # we map the headers of the document with the standardize header before to aggregate\n            if step=='aggregate':\n                #print(df.columns)\n                df.columns=[ x if ((x==\"JOB_NAME_TXT\") | (x==\"JOB_TITLE_TXT\")) else dictOfStandard[x]\n                        #\" \".join(x.translate(str.maketrans(string.punctuation,' '*len(string.punctuation))).split())\n                        for x in list(df.columns)]\n                df1_metaData = df1_metaData.append(df, sort=False)\n\n            list_metaData.append([x  for x in list(df.columns) if((x!=\"JOB_NAME_TXT\") & (x!=\"JOB_TITLE_TXT\"))])\n\n        except:print(job)\n\n        file.close()\n        \n    if step=='aggregate':\n        return list_metaData,df1_metaData\n    else:\n        return list_metaData","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"listOfHeaders=standardize2Aggregate(file_txt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will ignore the bulletin related to the Police Job Commander position that failed to be read\n\nWe collected the headers of the bulletins that are going to be standardized by lemmatization\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatizeSentence(sentence):\n    sent = sentence\n    tag_dict = {\"J\": 'a', \n                \"N\": 'n', \n                \"V\": 'v', \n                \"R\": 'r'}\n    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n    return lemmatized_list\n\ndef reduceSentence(sentence):\n    sentence=TextBlob(sentence.translate(str.maketrans(string.punctuation,' '*len(string.punctuation))).lower())\n    sentence=sentence.correct()\n    sentence=lemmatizeSentence(sentence)\n    sentenceback=[re.sub(\"MORTIFICATION\",\"CERTIFICATION\",x.upper()) for x in list(sentence) if x not in stopWords]\n    return sentenceback\n\ndef add_or_append(dictionary, key, value):\n    dictionary[key]=value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mappingHeaders=dict()\nfor value in set(x for l in listOfHeaders for x in l):\n    add_or_append(mappingHeaders,value, \"_\".join(reduceSentence(value)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"listOfHeaders, dfStandarized =standardize2Aggregate(file_txt,mappingHeaders,'aggregate')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We would like to check the matching between the name of the file and the title position they content.\nSo we use the similarity string between JOB_TITLE_TXT and JOB_NALE_TXT"},{"metadata":{"trusted":true},"cell_type":"code","source":"def ratio_sim(row) :\n    return SequenceMatcher(None,row['JOB_NAME_TXT'],row['JOB_TITLE_TXT']).ratio()\ndf=dfStandarized[['JOB_NAME_TXT','JOB_TITLE_TXT']]\ndf['similarity']=df.apply(ratio_sim,axis=1)\ndf.sort_values('similarity').head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We note that 3 job bulletins title does not match the file name :\n* ANIMAL CARE TECHNICIAN SUPERVISOR 4313 122118.txt\n* SENIOR EXAMINER OF QUESTIONED DOCUMENTS 3231 072216 REVISED 072716.txt\n* WASTEWATER COLLECTION SUPERVISOR 4113 121616.txt\n\nThe bulletin VOCATIONAL WORKER DEP OF PUBLIC WORKS does not have a common framework shared with the other files. We remove it from our analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"display(dfStandarized.loc[dfStandarized.JOB_NAME_TXT!='VOCATIONAL WORKER DEPARTMENT OF PUBLIC WORKS'].head(5))\ndfStandarized=dfStandarized.loc[dfStandarized.JOB_NAME_TXT!='VOCATIONAL WORKER DEPARTMENT OF PUBLIC WORKS']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The last action to perform is to make \"hand\" correction by analysing the similarity between some headers to merge columns to enhance the completion of information."},{"metadata":{"trusted":true},"cell_type":"code","source":"a=dfStandarized.count().sort_values(ascending=False).reset_index()\na.columns=['HEADERS','Count']\ndisplay(a.head(10))\nlist_ratio=[]\na=a.sort_values('Count',ascending=False)\nlist_name=a['HEADERS']\nlist_valeur=a['Count']\nfor x in range(len(list_name)):\n    for y in range(x,len(list_name)):\n        if not (bool(re.search(\"_NOTE$\",list_name[x])) ^ bool(re.search(\"_NOTE$\",list_name[y]))):\n            correl=dfStandarized.loc[(~dfStandarized[list_name[x]].isnull()) & (~dfStandarized[list_name[y]].isnull()),[list_name[x],list_name[y]]].shape[0]\n            list_ratio.append([list_name[x],list_name[y],round(SequenceMatcher(None,list_name[x], list_name[y]).ratio(),4)*100,\n                                  list_valeur[x],list_valeur[y],correl])\ndf_ratio=pd.DataFrame(list_ratio, columns=['Col1','Col2','Similarity','Num_Col1','Num_Col2','Overlap'])\ndf_ratio.loc[(df_ratio['Col1']!=df_ratio['Col2'])\n             &(df_ratio['Similarity']>0)\n             &(df_ratio['Overlap']==0)\n            ].sort_values(['Similarity'], ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"listToCorrect=[\"ANNUALSALARY_NOTE\",\"ANNUALSALARY\",\n               \"EXAMINATION_GIVE_INTERDEPARTMENTAL_PROMOTION_BASIS_NAVY\",\n               \"PASS_SCORE_QUALIFY_TEST_NOTE\",\"REQUIREMENT_MIMINUMUM_QUALIFICATION\",\n               \"PASS_SCORE_QUALIFY_TEST\",\"EXAM_GIVE_INTERDEPARTMENTAL_PROMOTION_BASIS\",\n               \"EXAMINATION_GIVE_OPEN_COMPETITIVE_INTERDEPARTMENTAL_PROMOTION_BASIS\",\"SELECTION_PROCEDURE\",\n               \"EQUAL_OPPORTUNITY_EMPLOYER\",\"EQUAL_EMPLOYMENT_OPPORTUNITY_EMPLOYER_EQUAL_EMPLOYMENT_OPPORTUNITY_EMPLOYER\",\n               \"REQUIREMENT\",\"REQUIREMENT_MINIMUM_REQUIREMENT\",\n               \"MINIMUM_REQUIREMENT\",\n               \"REQUIREMENT_NOTE\",\"APPLICATION_DEADLINE_NOTE_EXPERT_REVIEW_COMMITTEE\"]\nlistByCorrect=[\"ANNUAL_SALARY_NOTE\",\"ANNUAL_SALARY\",\n               \"EXAMINATION_GIVE_INTERDEPARTMENTAL_PROMOTION_BASIS\",\n               \"PASS_SCORE_QUALIFYING_TEST_NOTE\",\"REQUIREMENT_MINIMUM_QUALIFICATION\",\n               \"PASS_SCORE_QUALIFYING_TEST\",\"EXAMINATION_GIVE_INTERDEPARTMENTAL_PROMOTION_BASIS\",\n               \"EXAMINATION_GIVE_INTERDEPARTMENTAL_PROMOTION_OPEN_COMPETITIVE_BASIS\",\"SELECTION_PROCESS\",\n               \"EQUAL_EMPLOYMENT_OPPORTUNITY_EMPLOYER\",\"EQUAL_EMPLOYMENT_OPPORTUNITY_EMPLOYER\",\n               \"REQUIREMENT_MINIMUM_QUALIFICATION\",\"REQUIREMENT_MINIMUM_QUALIFICATION\",\n               \"REQUIREMENT_MINIMUM_QUALIFICATION\",\n               \"REQUIREMENT_MINIMUM_QUALIFICATION_NOTE\",\"EXPERT_REVIEW_COMMITTEE\"]\nfor index, item in enumerate(listToCorrect):\n    dfStandarized.loc[~dfStandarized[listToCorrect[index]].isnull(),listByCorrect[index]]=dfStandarized.loc[~dfStandarized[listToCorrect[index]].isnull(),listToCorrect[index]]\n    dfStandarized.drop([listToCorrect[index]], axis=1,inplace=True)\ndfStandarized.reset_index(drop=True,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=dfStandarized.count().sort_values(ascending=False)\na.columns=['HEADERS','Count']\na.head(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Extraction and analyse of data\n\n<a id=\"extract\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### 3.1. Administration data\n<a id=\"admin\"></a>\n* CLASS CODE\n* OPEN DATE\n* REVISED DATE\n* EXAM OPEN TO"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfStandarized['CLASS_CODE']=dfStandarized['ADMINISTRATION'].apply(lambda x: ' '.join(x)).str.lower()\\\n.apply(lambda x : re.search(\"class code: (\\S+)*\",x).group(1) if re.search(\"class code: (\\S+)*\",x) else np.NaN )\n\ndfStandarized.loc[dfStandarized.CLASS_CODE.isnull(),'CLASS_CODE']=dfStandarized.loc[dfStandarized.CLASS_CODE.isnull(),'JOB_TITLE_TXT'].str.lower()\\\n.apply(lambda x : re.search(\"class code: (\\S+)*\",x).group(1) if re.search(\"class code: (\\S+)*\",x) else np.NaN )\ndfStandarized['JOB_TITLE_TXT']=dfStandarized['JOB_TITLE_TXT'].apply(lambda x : re.sub(\" CLASS CODE: (\\S+)*\",\"\",x))\n\ndfStandarized['OPEN_DATE']=dfStandarized['ADMINISTRATION'].apply(lambda x: ' '.join(x)).str.lower()\\\n.apply(lambda x : re.search(\"open date: (\\S+)*\",x).group(1) if re.search(\"open date: (\\S+)*\",x) else None )\n\ndfStandarized['REVISED_DATE']=dfStandarized['ADMINISTRATION'].apply(lambda x: ' '.join(x)).str.lower()\\\n.apply(lambda x : re.search(\"revised: (\\S+)*\",x).group(1) if re.search(\"revised: (\\S+)*\",x) else None )\n\ndfStandarized['EXAM_OPEN_TO']=dfStandarized['ADMINISTRATION'].apply(lambda x: ' '.join(x)).str.lower()\\\n.apply(lambda x : re.search(\"\\(exam (.+)*\\)\",x).group(1) if re.search(\"\\(exam ([\\w\\s]+)*\",x) else None )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can have a look now to the administration data structured in our dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfStandarized[['JOB_TITLE_TXT','ADMINISTRATION','CLASS_CODE','OPEN_DATE','REVISED_DATE','EXAM_OPEN_TO']].head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to have to standardized the verbatim related to the EXAM_OPEN_TO in order to get only the two related designation :\n* *open to all, including current city employees* (as well as *open to all, including city employees* and *open to all including current city employees*\n* *open to current city employees* (as similar with *open to all current city employees*)"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfStandarized.EXAM_OPEN_TO.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 3.2. Salary data\n<a id=\"sala\"></a>\n* ANNUAL SALARY\n* NOTE"},{"metadata":{},"cell_type":"markdown","source":"#### Quantitative data"},{"metadata":{},"cell_type":"markdown","source":"Let's have a quick look to how the ANNUAL_SALARY data is presented :"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfStandarized.head(10)['ANNUAL_SALARY'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can notice than some salary are expressed in term on range **salary1 to salary2** and other in term of flat rate **salary (flat-rated)**<br>\nThus we will focus first to retreive those two kind of information."},{"metadata":{"trusted":true},"cell_type":"code","source":"regex_range_salary=\"\\$*\\s*(\\d+\\,*\\s*\\d+)+\\** to \\$*\\s*(\\d+\\,*\\s*\\d+)+\"\nregex_flat=\"(flat\\srate)+\"\nregex_flat_salary=\"\\$*\\s*(\\d+\\,*\\d+)+\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfStandarized['ANNUAL_SALARY']=dfStandarized['ANNUAL_SALARY']\\\n.apply(lambda x: list(filter(lambda x: x!='',[' '.join(x[i:i+2]) if bool(re.search(\" (to)$\",x[i])) \n                                              else x[i]  if (i==0)|\n                                              ((bool(re.search(\" (to)$\",x[i-1]))==False) & (i>0)) else '' for i in range(len(x)) ])))\n\ndfStandarized['ANNUAL_SALARY_REDUCED_BIS']=dfStandarized['ANNUAL_SALARY']\\\n.apply(lambda x: [re.sub(regex_range_salary,\"xx\",y.lower()) if re.search(regex_range_salary,y.lower()) else y.lower() for y in x])\\\n.apply(lambda x: [re.sub(regex_flat_salary,\"ff\",y) if re.search(regex_flat,' '.join(reduceSentence(y)).lower()) else y for y in x])\\\n.apply(lambda x :[re.sub('(LOS ANGELES)|(SALARY)|(RANGE)|(DEPARTMENT)|(WORLD)|(POSITION)|\\d', r'',' '.join(reduceSentence(y))) for y in x] )\\\n.apply(lambda x: [re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1',' '.join(reduceSentence(y))) for y in x])\\\n.apply(lambda x: [re.sub(r'\\b([\\w\\s]+)( \\1\\b)+', r'\\1',' '.join(reduceSentence(y))) for y in x])\\\n.apply(lambda x: [re.sub(r'\\b([\\w\\s]+)( \\1\\b)+', r'\\1',' '.join(reduceSentence(y))) for y in x])\\\n.apply(lambda x :[re.sub('\\s+', r' ',y) for y in x] )\\\n.apply(lambda x :[re.sub('^\\s+|\\s+$', r'',y) for y in x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus we get a list of the most common typology of salary as in the bulletin:\n* Some just contain salaries as range (XX), other as flat rated (FF FLAT RATE)\n* Other are related to specific Departements : Water and Power, Airport, Harbor"},{"metadata":{"trusted":true},"cell_type":"code","source":"a=dfStandarized['ANNUAL_SALARY_REDUCED_BIS']\nx=Counter([y for x in a for y in x])\nsorted(x.items(), key=lambda pair: pair[1], reverse=True)[:10]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here below some example of verbatim reduction from ANNUAL_SALARY as in the bulletin to ANNUAL_SALARY_REDUCED_BIS which get the pattern of the salary"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfStandarized.head(10)[['ANNUAL_SALARY','ANNUAL_SALARY_REDUCED_BIS']].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_salary=dict()\ndict_salary['AIRPORT FLAT RATE FF']='AIRPORT FF FLAT RATE'\ndict_salary['WATER POWER FLAT RATE FF']='WATER POWER FF FLAT RATE'\ndict_salary['XX WATER POWER XX']='WATER POWER XX'\ndict_salary['XX FLAT RATE']='FF FLAT RATE'\ndfStandarized['ANNUAL_SALARY_REDUCED_BIS']=dfStandarized['ANNUAL_SALARY_REDUCED_BIS'].apply(lambda x: [dict_salary[y] if y in dict_salary.keys() else y for y in x])\\\n.apply(lambda x :[re.sub('(WATER POWER FF FLAT RATE XX)|(WATER POWER XX FF FLAT RATE)', r'WATER POWER XX WATER POWER FF FLAT RATE',y) for y in x] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_get_salary={\n    \n    \"WATER_POWER_SALARY_RANGE\" : [\"(WATER POWER XX)\" , regex_range_salary],\n    \"WATER_POWER_SALARY_FLAT\" : [\"(WATER POWER FF FLAT RATE)\", regex_flat_salary],\n    \n    \"AIRPORT_SALARY_RANGE\" : [\"(AIRPORT XX)\" , regex_range_salary],\n    \"AIRPORT_SALARY_FLAT\" : [\"(AIRPORT FF FLAT RATE)\" , regex_flat_salary],\n    \n    \"HARBOR_SALARY_RANGE\" : [\"(HARBOR XX)\" , regex_range_salary],\n    \"HARBOR_SALARY_FLAT\" : [\"(HARBOR FF FLAT RATE)\", regex_flat_salary],\n    \n    \"GENERAL_SALARY_RANGE\" : [\"(XX)\",regex_range_salary],\n    \"GENERAL_SALARY_FLAT\" : [\"(FF FLAT RATE)\", regex_flat_salary]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def f(x,y):\n    a=[re.findall(y[1],x['ANNUAL_SALARY_MODIFIED'][i].lower())\n       for i,z in enumerate(x['ANNUAL_SALARY_REDUCED_BIS'])\n       if (re.search(y[0],z)) and (re.search(y[1],x['ANNUAL_SALARY_MODIFIED'][i].lower()))]\n    return [item for sublist in a for item in sublist] if a!=[] else None\n\ndef g(x,y):\n    b=[re.sub(y[1],'Substitute',x['ANNUAL_SALARY_MODIFIED'][i].lower()) \n       if re.search(y[0],z) else x['ANNUAL_SALARY_MODIFIED'][i].lower()\n       for i,z in enumerate(x['ANNUAL_SALARY_REDUCED_BIS'])]\n    return b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfStandarized['ANNUAL_SALARY_MODIFIED']=dfStandarized['ANNUAL_SALARY']\n\nfor new_column in dict_get_salary.keys():\n    print(new_column)\n    dfStandarized[new_column]=dfStandarized[['ANNUAL_SALARY_MODIFIED','ANNUAL_SALARY_REDUCED_BIS']]\\\n    .apply(lambda x : f(x,dict_get_salary[new_column]), axis=1)\n    dfStandarized['ANNUAL_SALARY_MODIFIED']=dfStandarized[['ANNUAL_SALARY_MODIFIED','ANNUAL_SALARY_REDUCED_BIS']]\\\n    .apply(lambda x : g(x,dict_get_salary[new_column]), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We detect that for one offer, none salary is mentionned : "},{"metadata":{"trusted":true},"cell_type":"code","source":"dfStandarized[dfStandarized[['WATER_POWER_SALARY_RANGE','WATER_POWER_SALARY_FLAT',\n                            'AIRPORT_SALARY_RANGE','AIRPORT_SALARY_FLAT',\n                            'HARBOR_SALARY_RANGE','HARBOR_SALARY_FLAT',\n                            'GENERAL_SALARY_RANGE','GENERAL_SALARY_FLAT']].isna().all(1)][['JOB_NAME_TXT','ANNUAL_SALARY_MODIFIED']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can have a look now to the salary data structured in our dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfStandarized.head(5)[['ANNUAL_SALARY',\n                       'GENERAL_SALARY_RANGE','WATER_POWER_SALARY_RANGE', 'AIRPORT_SALARY_RANGE', 'HARBOR_SALARY_RANGE',\n                       'GENERAL_SALARY_FLAT','WATER_POWER_SALARY_FLAT', 'AIRPORT_SALARY_FLAT', 'HARBOR_SALARY_FLAT']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How about to start a small visualization that those data ? <br>\nWe will try to first if we can make any conclusion about how are the range salary for specific departments compare with salary without specification fo departement"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfSalaryComparaison=dfStandarized[dfStandarized[['WATER_POWER_SALARY_RANGE',\n                                                                     'AIRPORT_SALARY_RANGE',\n                                                                     'HARBOR_SALARY_RANGE',\n                                                                     'GENERAL_SALARY_RANGE']].isnull().sum(axis=1)<3]\ndfSalary=pd.DataFrame()\nfor x in ['WATER_POWER_SALARY_RANGE', 'AIRPORT_SALARY_RANGE', 'HARBOR_SALARY_RANGE','GENERAL_SALARY_RANGE']:\n    a=pd.DataFrame(dfSalaryComparaison.loc[~dfSalaryComparaison[x].isnull(),x].tolist())\n    a['JOB_NAME']=dfSalaryComparaison.loc[~dfSalaryComparaison[x].isnull(),'JOB_NAME_TXT'].values\n    a=a.set_index('JOB_NAME').stack().reset_index(name='new')[['JOB_NAME','new']]\n    b=pd.DataFrame(a['new'].values.tolist(),columns=['LOWER_RANGE','UPPER_RANGE']).applymap(lambda x : re.sub('[^0-9]','',x))\n    b['JOB_NAME']=  a['JOB_NAME']\n    b['DEPARTEMENT_POSITION']=x\n    dfSalary=dfSalary.append(b, ignore_index=True, sort=True)\n\ndfSalary[['LOWER_RANGE','UPPER_RANGE']]=dfSalary[['LOWER_RANGE','UPPER_RANGE']].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.scatterplot(x=\"LOWER_RANGE\", y=\"UPPER_RANGE\",\n                hue=\"DEPARTEMENT_POSITION\",style=\"DEPARTEMENT_POSITION\",\n                data=dfSalary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Maybe some initial conclusions :\n* as general statement, the lowest salary for water and power departement are higher than the lowest salary for unspecified department\n* netherless, the range ratio is higher for unspecified departement than for specific departement"},{"metadata":{},"cell_type":"markdown","source":"Let's compare now the ranged salaries with the flated ones"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfSalaryComparaison=dfStandarized\n\ndfSalary=pd.DataFrame()\nfor x in ['WATER_POWER_SALARY_RANGE', 'AIRPORT_SALARY_RANGE', 'HARBOR_SALARY_RANGE','GENERAL_SALARY_RANGE']:\n    a=pd.DataFrame(dfSalaryComparaison.loc[~dfSalaryComparaison[x].isnull(),x].tolist())\n    a['JOB_NAME']=dfSalaryComparaison.loc[~dfSalaryComparaison[x].isnull(),'JOB_NAME_TXT'].values\n    a=a.set_index('JOB_NAME').stack().reset_index(name='new')[['JOB_NAME','new']]\n    b=pd.DataFrame(a['new'].values.tolist(),columns=['LOWER_RANGE','UPPER_RANGE']).applymap(lambda x : re.sub('[^0-9]','',x))\n    b['JOB_NAME']=  a['JOB_NAME']\n    b['DEPARTEMENT_POSITION']='SALARY_RANGE'#x\n    dfSalary=dfSalary.append(b, ignore_index=True, sort=True)\n\nfor x in ['WATER_POWER_SALARY_FLAT', 'AIRPORT_SALARY_FLAT', 'HARBOR_SALARY_FLAT','GENERAL_SALARY_FLAT']:\n    a=pd.DataFrame(dfSalaryComparaison.loc[~dfSalaryComparaison[x].isnull(),x].tolist())\n    a['JOB_NAME']=dfSalaryComparaison.loc[~dfSalaryComparaison[x].isnull(),'JOB_NAME_TXT'].values\n    a=a.set_index('JOB_NAME').stack().reset_index(name='LOWER_RANGE')[['JOB_NAME','LOWER_RANGE']]\n    b=pd.DataFrame(a['LOWER_RANGE']).applymap(lambda x : re.sub('[^0-9]','',x))\n    b['JOB_NAME']=  a['JOB_NAME']\n    b['DEPARTEMENT_POSITION']='SALARY_FLAT'#x\n    b['UPPER_RANGE']=b['LOWER_RANGE']\n    dfSalary=dfSalary.append(b, ignore_index=True, sort=True)\n\ndfSalary[['LOWER_RANGE','UPPER_RANGE']]=dfSalary[['LOWER_RANGE','UPPER_RANGE']].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\nsns.scatterplot(x=\"LOWER_RANGE\", y=\"UPPER_RANGE\",\n                hue=\"DEPARTEMENT_POSITION\",style=\"DEPARTEMENT_POSITION\",\n                data=dfSalary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Heree also maybe some initial conclusions :\n* as general statement, the flated salaries are lower than the ranged salaries"},{"metadata":{},"cell_type":"markdown","source":"#### Qualitative"},{"metadata":{},"cell_type":"markdown","source":"In addition to quantitative data related to salary, there are qualitative data contained for the mostly contained in the NOTES part attached to the ANNUAL SALARY part."},{"metadata":{},"cell_type":"markdown","source":"Let's have a quick look to how the ANNUAL_SALARY_NOTE data is presented :"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfStandarized.head(5)[['JOB_NAME_TXT','ANNUAL_SALARY_NOTE']].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As previously done with the qualitative data, we will reduce the dimentionnality of the sentence in order to regularize the text and find the most common elements"},{"metadata":{"trusted":true},"cell_type":"code","source":"regex_url='https?:\\/\\/.*[\\r\\n]*'\n\ndfStandarized['ANNUAL_SALARY_NOTE_REDUCED_BIS']=dfStandarized['ANNUAL_SALARY_NOTE']\\\n.apply(lambda x : x if type(x) == list else [])\\\n.apply(lambda x: [re.sub(regex_url,\"URLPDF\",y.lower()) if re.search(regex_url,y.lower()) else y.lower() for y in x])\\\n.apply(lambda x: [re.sub('^\\d. ',\"\",y.lower()) if re.search('^\\d. ',y.lower()) else y.lower() for y in x])\\\n.apply(lambda x :[re.sub('(LOS ANGELES)|(SALARY)|(RANGE)|(DEPARTMENT)|(WORLD)|(POSITION)|\\d', r'',' '.join(reduceSentence(y))) for y in x] )\\\n.apply(lambda x: [re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1',' '.join(reduceSentence(y))) for y in x])\\\n.apply(lambda x: [re.sub(r'\\b([\\w\\s]+)( \\1\\b)+', r'\\1',' '.join(reduceSentence(y))) for y in x])\\\n.apply(lambda x: [re.sub(r'\\b([\\w\\s]+)( \\1\\b)+', r'\\1',' '.join(reduceSentence(y))) for y in x])\\\n.apply(lambda x :[re.sub('\\s+', r' ',y) for y in x] )\\\n.apply(lambda x :[re.sub('^\\s+|\\s+$', r'',y) for y in x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus we get a list of the most common typology of informations related to the note salary as in the bulletin:\n* Grade position,\n* Night and shift work bonus,\n* Part time position,\n* Start level salary,\n* Multiple range grade,\n* Salary to confirm,\n* Possible salary change,\n* Confirm hiring salary"},{"metadata":{"trusted":true},"cell_type":"code","source":"a=dfStandarized['ANNUAL_SALARY_NOTE_REDUCED_BIS']\nx=Counter([y for x in a for y in x])\nsorted(x.items(), key=lambda pair: pair[1], reverse=True)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_reduced=['ANNUAL_SALARY_REDUCED_BIS','ANNUAL_SALARY_NOTE_REDUCED_BIS']\n\ndfStandarized['LOWER_PAY_SALARY_CRITERIA']=dfStandarized[columns_reduced].apply(lambda x: 'LOWER PAY GRADE POSITION' if sum([bool(re.search(\"LOW\",y))for y in x[columns_reduced[1]]+x[columns_reduced[0]]])>0 \n                                                                                else None, axis=1)\ndfStandarized['HIGHER_PAY_SALARY_CRITERIA']=dfStandarized[columns_reduced].apply(lambda x: 'NIGHT WORK' if sum([bool(re.search(\"NIGHT\",y)) for y in x[columns_reduced[1]]+x[columns_reduced[0]]])>0\n                                                                                 else('SHIFT WORK'if sum([bool(re.search(\"ASSIGN\",y)) for y in x[columns_reduced[1]]+x[columns_reduced[0]]])>0\n                                                                                      else None), axis=1)\ndfStandarized['PART_TIME_SALARY_CRITERIA']=dfStandarized[columns_reduced].apply(lambda x: 'PART TIME' if sum([bool(re.search(\"PART TIME\",y)) for y in x[columns_reduced[1]]+x[columns_reduced[0]]])>0\n                                                                                else None, axis=1)\ndfStandarized['BEGIN_RANGE_SALARY_CRITERIA']=dfStandarized[columns_reduced].apply(lambda x: 'BEGIN SALARY RANGE' if sum([bool(re.search(\"(START|BEGIN) PAY\",y)) for y in x[columns_reduced[1]]+x[columns_reduced[0]]])>0\n                                                                                  else None, axis=1)\ndfStandarized['MULTIPLE_PAY_SALARY_CRITERIA']=dfStandarized[columns_reduced].apply(lambda x: 'COVER MULTIPLE PAY GRADE' if sum([bool(re.search(\"MULTIPLE\",y)) for y in x[columns_reduced[1]]+x[columns_reduced[0]]])>0\n                                                                                   else None, axis=1)\ndfStandarized['CONFIRM_PAY_SALARY_CRITERIA']=dfStandarized[columns_reduced].apply(lambda x: 'CONFIRM SALARY BEFORE' if sum([bool(re.search(\"ACCEPT\",y)) for y in x[columns_reduced[1]]+x[columns_reduced[0]]])>0\n                                                                                  else None, axis=1)\ndfStandarized['CHANGE_PAY_SALARY_CRITERIA']=dfStandarized[columns_reduced].apply(lambda x: 'CURRENT SALARY SUBJECT TO CHANGE' if sum([bool(re.search(\"CHANGE\",y)) for y in x[columns_reduced[1]]+x[columns_reduced[0]]])>0\n                                                                                 else None, axis=1)\ndfStandarized['RECIPROCITY_SALARY_INFORMATION']=dfStandarized[columns_reduced].apply(lambda x: 'CITY LA AND LADWP' if sum([bool(re.search(\"URLPDF\",y)) for y in x[columns_reduced[1]]+x[columns_reduced[0]]])>0\n                                                                                 else None, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfStandarized.head(5)[['JOB_NAME_TXT',\n                       'LOWER_PAY_SALARY_CRITERIA','HIGHER_PAY_SALARY_CRITERIA', 'PART_TIME_SALARY_CRITERIA', 'BEGIN_RANGE_SALARY_CRITERIA',\n                       'MULTIPLE_PAY_SALARY_CRITERIA','CONFIRM_PAY_SALARY_CRITERIA', 'CHANGE_PAY_SALARY_CRITERIA', 'RECIPROCITY_SALARY_INFORMATION']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For some positions no additional information is provided about the salay :"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfStandarized[dfStandarized[['LOWER_PAY_SALARY_CRITERIA','HIGHER_PAY_SALARY_CRITERIA', 'PART_TIME_SALARY_CRITERIA', 'BEGIN_RANGE_SALARY_CRITERIA',\n                       'MULTIPLE_PAY_SALARY_CRITERIA','CONFIRM_PAY_SALARY_CRITERIA', 'CHANGE_PAY_SALARY_CRITERIA', 'RECIPROCITY_SALARY_INFORMATION']].isna().all(1)][['JOB_NAME_TXT','ANNUAL_SALARY','ANNUAL_SALARY_NOTE']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 3.3. Requirement minimum qualification data\n<a id=\"minireq\"></a>\n* REQUIREMENT MINIMUM QUALIFICATION\n* NOTE"},{"metadata":{},"cell_type":"markdown","source":"We can notice here that the text of this part is much more unstructured and diversed that the annual salsry data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfStandarized.head(5)[['JOB_NAME_TXT','REQUIREMENT_MINIMUM_QUALIFICATION']].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So our strategy here wil be a little bit more advanced through the following steps:\n1. Apply first a stemmatization and correction of the texts\n2. Tokenize then the texts\n3. Vectorize the corpus using TFIDF method\n4. Clusterize using hierarchical aggregation algorithm\n5. Visualize the main words behind those clusters"},{"metadata":{},"cell_type":"markdown","source":"**Stemmatization and correction**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfStandarized['REQUIREMENT_MINIMUM_QUALIFICATION_REDUCED_BIS']=dfStandarized['REQUIREMENT_MINIMUM_QUALIFICATION']\\\n.apply(lambda x: [re.sub('^\\d\\. ',\"\",y) if re.search('^\\d\\. ',y) else y for y in x])\\\n.apply(lambda x: [re.sub('^[a-z]{1}\\. ',\"\",y) if re.search('^[a-z]{1}\\. ',y) else y for y in x])\\\n.apply(lambda x :' '.join(x))\\\n.apply(lambda x :' '.join(reduceSentence(x)).lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfStandarized.loc[np.random.randint(1, 682, size=(1, 3))[0],\n                  ['REQUIREMENT_MINIMUM_QUALIFICATION_REDUCED_BIS']].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tokenization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize_only(text):\n    tokens=[word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    filtered_tokens=[]\n    \n    for token in tokens:\n        if re.search('[a-zA-Z]',token):\n            filtered_tokens.append(token)\n    return filtered_tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"totalvocab_tokenized=[]\n\nfor i in dfStandarized['REQUIREMENT_MINIMUM_QUALIFICATION_REDUCED_BIS'].values:\n    allwords_tokenized=tokenize_only(i)\n    totalvocab_tokenized.extend(allwords_tokenized)    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_frame=pd.DataFrame({'words': totalvocab_tokenized}, index=totalvocab_tokenized)\nprint ('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Vectorization TFIDF**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vectorizer = TfidfVectorizer(min_df=0.2,max_df=0.8,\n                                   use_idf=True, tokenizer= tokenize_only, ngram_range=(1,1))\n\ntfidf_matrix = tfidf_vectorizer.fit_transform(dfStandarized['REQUIREMENT_MINIMUM_QUALIFICATION_REDUCED_BIS'].values)\n\nterms=tfidf_vectorizer.get_feature_names()\n\nprint(tfidf_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Clustering hierarchical**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.cluster.hierarchy import ward, dendrogram\ndist = 1 - cosine_similarity(tfidf_matrix)\nlinkage_matrix = ward(dist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(15, 20))\nax= dendrogram(linkage_matrix, orientation=\"right\",labels=dfStandarized['JOB_NAME_TXT'].values )\n\nplt.tick_params(axis='x', which='both', bottom='off', top='off', labelbottom='off')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By a visual analysis we can quickly decide to cluster the corpus in 18 groups (this is a first approach)"},{"metadata":{"trusted":true},"cell_type":"code","source":"k=18","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(15, 20))\nax= dendrogram(\n    linkage_matrix,orientation=\"right\",\n    truncate_mode='lastp',  # show only the last p merged clusters\n    p=k,  # show only the last p merged clusters\n    show_contracted=True  # to get a distribution impression in truncated branches\n)\nplt.tick_params(axis='x', which='both', bottom='off', top='off', labelbottom='off')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster.hierarchy import fcluster\ndfStandarized['REQUIREMENT_MINIMUM_QUALIFICATION_CLUSTER_HIER']=fcluster(linkage_matrix, k, criterion='maxclust')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom wordcloud import WordCloud\n\ntfidf_vectorizer_bis = TfidfVectorizer(min_df=0.2,max_df=0.8,\n                                       use_idf=True, ngram_range=(1,1))\n\nfunct = lambda x: ' '.join([vocab_frame.loc[y].values.tolist()[0][0] for y in x.split(' ')])\n\ndictResultCluster=dict()\n\nfor i in range(k):\n    df=dfStandarized.loc[dfStandarized.REQUIREMENT_MINIMUM_QUALIFICATION_CLUSTER_HIER==i+1,\n                          'REQUIREMENT_MINIMUM_QUALIFICATION_REDUCED_BIS']\n    tfidf_matrix_bis = tfidf_vectorizer_bis.fit_transform(df.values)\n    terms_bis=tfidf_vectorizer_bis.get_feature_names()\n\n    km=KMeans(n_clusters=1)\n    km.fit(tfidf_matrix_bis)\n    order_centroids = km.cluster_centers_.argsort()[:,::-1]\n    \n    print('Cluster '+str(i+1) + ' : size '+ str(df.shape[0]))\n    list_cluster=[]\n    for ind in order_centroids[0,:]:\n        try : \n            list_cluster.append(funct(terms_bis[ind])) \n        except : \n            pass\n    \n    comment_words=' '.join([ x \n                            for y in df\n                            for x in tokenize_only(y) if x in list_cluster\n                            ])\n    wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white',\n                min_font_size = 10).generate(comment_words) \n    \n    plt.figure(figsize = (4, 4), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n  \n    plt.show()\n\n    dictResultCluster[i+1]   =[df.shape[0],' ,'.join(list_cluster)] \n\n    #df=dfStandarized.loc[dfStandarized.REQUIREMENT_MINIMUM_QUALIFICATION_CLUSTER_HIER==i+1,\n    #                        ['REQUIREMENT_MINIMUM_QUALIFICATION']]\n    #print(df.values[np.random.randint(1, df.shape[0], size=(1, 3))[0]])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(dictResultCluster, index=['Cluster size','Words embeded'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Coming Soon --  Under construction\n![](https://scholarblogs.emory.edu/ranews/files/2017/02/under-construction-image-850x478.jpeg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}