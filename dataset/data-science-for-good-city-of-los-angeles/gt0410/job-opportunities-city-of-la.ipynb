{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Importing the required packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport nltk\nimport re\n\nimport glob\nimport io\n# Any results you write to the current directory are saved as output.\n\nimport seaborn as sns\nfrom decimal import Decimal\nimport locale\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Looking into CSV files\nAdditional data folder contains 3 CSV files:\n    1. job_titles.CSV : This file contains Job class titles of the available jobs.\n    2. sample job class export template.CSV : This file contains sample submission of csv.\n    3. kaggle_data_dictionary.CSV: This file contains the name and details of the columns to include in the exported csv file."},{"metadata":{"trusted":true},"cell_type":"code","source":"job_titles = pd.read_csv(\"../input/cityofla/CityofLA/Additional data/job_titles.csv\", header = None)\njob_titles.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_job_export = pd.read_csv('../input/cityofla/CityofLA/Additional data/sample job class export template.csv')\nsample_job_export","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kaggle_data_dictionary = pd.read_csv('../input/cityofla/CityofLA/Additional data/kaggle_data_dictionary.csv')\nkaggle_data_dictionary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Extraction"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"In this section sevaral functions are defined to get various featueres. Major features are extracted from each file and appended to the data frame, while some additional features are extracted after the data frame is made with few features."},{"metadata":{"trusted":true},"cell_type":"code","source":"code = r'Class\\s{1,2}Code:\\s*(\\d*)'\nopen_d = r'Open Date:\\s*(\\d\\d-\\d\\d-\\d\\d)'\nsal = r'(\\$(\\d+,\\d+))((\\s(to|and|-)\\s)(\\$\\d+,\\d+))?' # Taken from kaggle kernel\nsal_dwp = r'Power\\sis\\s((\\$(\\d+,\\d+))((\\s(to|and|-)\\s)(\\$\\d+,\\d+))?)'\ndut = r'DUTIES\\W+(.*\\n)' # Duties\nreq = r'REQUIREMENT(S)?(/MINIMUM\\sQUALIFICATION)?\\W+(.*\\n)' #Requirements\nend_d = r'(MONDAY|TUESDAY|WEDNESDAY|THURSDAY|FRIDAY|SATURDAY|SUNDAY)\\W+(JANUARY|FEBRUARY|MARCH|APRIL|MAY|JUNE|JULY|AUGUST|SEPTEMBER|OCTOBER|NOVEMBER|DECEMBER)(.*\\d)'\nexp_len = r'(one|two|three|four|five|six|seven|eight|nine)\\s(?=(year|years|month|months)\\sof\\s(?=(full-time|part-time)))' \nsch_typ = r'(college|university|high school|apprentice)' # School Type\nedu_y = r'(\\b\\w+(-year))(?=\\D+(college|university))' #Education Year\nedu_major = r'(college or university|college|university|apprentice)\\D+(in|as a)\\s(\\D+);' # Education Major\ncourse_l = r'(\\d+\\ssemester.+quarter(\\sunits)?)'\n#open_d = r'Open Date: (.*)\\n' ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above section containd some of the regex used to extract features. These Regular Expressions are used to extract various details needed to be included in the columns. \nThe regex for salary (sal) is taken from kernel https://www.kaggle.com/shahules/discovering-opportunities-at-la"},{"metadata":{"trusted":true},"cell_type":"code","source":"def doextraction(glob_text):\n    col = ['FILE_NAME', 'JOB_CLASS_TITLE', 'JOB_CLASS_NO', 'REQUIREMENT_SET_ID', 'REQUIREMENT_SUBSET_ID', 'JOB_DUTIES',\\\n           'EDUCATION_YEARS', 'SCHOOL_TYPE', 'EDUCATION_MAJOR', 'EXPERIENCE_LENGTH', 'FULL_TIME_PART_TIME', \\\n           'EXP_JOB_CLASS_TITLE', 'EXP_JOB_CLASS_ALT_RESP', 'EXP_JOB_CLASS_FUNCTION', 'COURSE_COUNT', 'COURSE_LENGTH',\\\n           'COURSE_SUBJECT', 'MISC_COURSE_DETAILS', 'DRIVERS_LICENSE_REQ', 'DRIV_LIC_TYPE', 'ADDTL_LIC', 'EXAM_TYPE', \\\n           'SALARY_START', 'SALARY_END', 'ENTRY_SALARY_DWP', 'REQUIREMENTS', 'APPLICATION_DEADLINE', 'OPEN_DATE']\n    \n    df = pd.DataFrame(columns = col) # Initializing a data frame with all the column names mentioned above.\n    \"\"\"Get all the files from the given glob and pass them to the extractor.\"\"\"\n    for thefile in glob.glob(glob_text)[:200]:\n        with io.open(thefile, 'r', errors = 'replace') as fyl:\n            text = fyl.read()\n            df = get_features(text, thefile, df)\n            \n    return df ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the doextraction function a dataFrame is initialized with all the column names. This function takes in path for the text files, read each file and send it to the get_features function. The dataframe with details from the text is updated after reading each file. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"with open('../input/cityofla/CityofLA/Job Bulletins/ADVANCE PRACTICE PROVIDER CORRECTIONAL CARE 2325 020808 REV 111214.txt', encoding = 'utf-8') as f:\n    print(f.read())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above output we can see the general pattern of the text files."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def get_headings(text): # Returns all the headings in the text file as a list \n    headings_list = []\n    for line in text.split('\\n'):\n        if line.isupper():\n            headings_list.append(line.strip())\n    \n    return headings_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the pattern of the text files one can observe that the headings in the file are UpperCase letters. So, Using above function I am extracting headings. Also, it is observed that the first heading is the job title."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_job_class(text): \n    # This function returns the title and the class code of the job. \n    job_title = text.strip().splitlines()[0].strip()\n    class_code = re.findall(code, text)[0]\n    \n    return job_title, class_code ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_open_date(text):\n    try:\n        open_date = re.findall(open_d, text)[0]\n    except:\n        open_date = np.NaN\n        \n    return open_date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def get_salary(text):\n    salary_range = re.search(sal, text)\n    \n    try:\n        salary_start = salary_range.group(1)\n    except:\n        salary_start = np.NaN\n        \n    try:\n        salary_end = salary_range.group(6)\n    except:\n        salary_end = np.NaN\n        \n    salary_dwp = re.search(sal_dwp, text).group(1) if re.search(sal_dwp, text) is not None else np.NaN\n    \n    return salary_start, salary_end, salary_dwp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above function text is taken as input and finds the lower and upper limit of the salary mentioned. Also, the first DWP-specific salary range is found.\nIf the salary is flat-rated then the one amount is returned."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_requirement(headings, text):\n    # All the text mentioned in the requirement section is returned.\n    x = headings.index([elm for elm in headings if elm.startswith('REQUIREMENT')][0])\n    m = re.search(headings[x], text).end()\n    n = re.search(headings[x+1], text).start()\n    requirement = text[m:n].strip()\n    \n    return requirement","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Requirements mentioned in the text are extracted by finding the character position after the Requirement heading and the position previous to next heading. After finding the characters range, requirements are extracted from string."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def get_duties(headings, text):\n    try:\n        x = headings.index([elm for elm in headings if elm.startswith('DUT')][0])\n        m = re.search(headings[x], text).end()\n        n = re.search(headings[x+1], text).start()\n        duties = text[m:n].strip()\n    except:\n        duties = np.NaN\n        \n    return duties","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Duties mentioned in the Bulletin are extracted finding the range of the characters similar to the requirements"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_req_id(requirement):\n    # This function returns requirement_set_id and requirement_subset_id.\n    req_id = re.search('(\\d)\\.', requirement.strip()[:2]).group(1) if re.search('(\\d\\.)', requirement.strip()[:2]) is \\\n            not None else np.NaN\n    req_sub_id = re.search('\\n(a)\\.', requirement).group(1) if re.search('\\n(a)\\.', requirement) is not None \\\n            else np.NaN\n    \n    return req_id, req_sub_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_exp_job(requirement):\n    exp_id = re.findall(r'(?<=Los Angeles as a )(\\w+\\s\\w+)', requirement)\n    if len(exp_id) == 1:\n        k1 = exp_id[0]\n        k1b = np.NaN\n    elif len(exp_id) > 1:\n        k1 = exp_id[0]\n        k1b = exp_id[1]\n    else:\n        k1 = k1b = np.NaN\n    \n    k2 = re.search('(?<=experience)(.+)(;|\\.|or)', requirement).group(1).strip() if re.search('(?<=experience)(.+)(;|\\.|or)', requirement)\\\n            is not None else np.NaN\n    \n    return k1, k1b, k2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this function k1 is the job title of the job one must hold to satisfy the requirement. In this case I am taking only first two letters of the job title.\nk1b is the alternate class of k1 and k2 is the field in which experience is required to satisfy this job requirements."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def get_exam_type(headings):\n    heads_text = ' '.join(headings)\n\n    if re.search('\\sINTERDEPART\\w+ PROMOT', heads_text) and re.search('\\sOPEN COMPETIT', heads_text):\n        e_type ='OPEN_INT_PROM'\n    elif re.search('\\sINTERDEPART\\w+ PROMOT', heads_text):\n        e_type = 'INT_DEPT_PROM'\n    elif re.search('\\sDEPARTMENT\\w+ PROMOT', heads_text):\n        e_type = 'T_PROM'\n    else:\n        e_type = 'OPEN'\n        \n    return e_type","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The information for the exam type is extracted in the headings list as this whole line is UpperCase letters. In this the exam type is categorized based on the description given in kaggle_data_dictionary.CSV file for exam type. Firstly, we are checking for the interdepartmental promotion and open competition followed by Interdedartmental promotion, departmental promotion and open type exam. These are labeled as described inthe description of CSV file."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_features(text, filename, df):\n    filename = filename.replace('../input/cityofla/CityofLA/Job Bulletins/', '')\n    headings = get_headings(text)\n    \n    job_title, class_code = get_job_class(text)\n    \n    open_date = get_open_date(text)\n    \n    salary_start, salary_end, salary_dwp = get_salary(text)\n    \n    requirement = get_requirement(headings, text)\n    \n    req_id, req_sub_id = get_req_id(requirement)\n    \n    duties = get_duties(headings, text)\n    \n    # e1 is the conjunction used in the requirements\n    e1 = requirement.splitlines()[0][-3:].strip() if len(requirement.splitlines()) >1 else np.NaN\n    if e1 != 'and' and e1 != 'or':\n        e1 = np.NaN\n        \n    k1, k1b, k2 =  get_exp_job(requirement)   \n    \n    # p1 and p2 are 'DRIVERS_LICENSE_REQ' and 'DRIV_LIC_TYPE'\n    if re.search('(positions may require a valid California driver\\'s license)', text, re.I) is not None:\n        p1 = 'P'\n        \n    elif re.search('(driver\\'s license is required)', text, re.I) is not None:\n        p1 = 'R' \n        \n    else:\n        p1 = np.NaN\n    \n    p2 = re.search(\"((?<=Class)\\s\\w\\s)(?=\\D+driver's)\", text, re.I).group(0).strip() if \\\n            re.search(\"((?<=Class)\\s\\w\\s)(?=\\D+driver's)\", text, re.I) is not None else np.NaN\n    \n    try:\n        x = re.findall(end_d, text)[0]\n        deadline = ''.join(x).strip()\n    except:\n        deadline = np.NaN\n     \n    exam_type = get_exam_type(headings)\n    \n    df = df.append({'FILE_NAME': filename, 'JOB_CLASS_TITLE': job_title, 'JOB_CLASS_NO': class_code,\\\n                   'REQUIREMENT_SET_ID': req_id, 'REQUIREMENT_SUBSET_ID': req_sub_id, 'JOB_DUTIES': duties,\\\n                   'EXP_JOB_CLASS_TITLE': k1, 'EXP_JOB_CLASS_ALT_RESP': k1b, 'EXP_JOB_CLASS_FUNCTION': k2, \\\n                    'DRIVERS_LICENSE_REQ': p1, 'DRIV_LIC_TYPE': p2, 'EXAM_TYPE': exam_type, 'SALARY_START': salary_start, 'SALARY_END': salary_end, \\\n                    'ENTRY_SALARY_DWP': salary_dwp, 'REQUIREMENTS': requirement, 'APPLICATION_DEADLINE': deadline, \\\n                    'OPEN_DATE': open_date}, ignore_index = True)\n    \n    # Append the features extracted from each text file.\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"get_features function takes in each text file at a time and extract features from that and appends the extracted features to a data frame.\nAlso, in the above function whether driver's license required or not and the class of divers license required are extracted from text."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df = doextraction('../input/cityofla/CityofLA/Job Bulletins/*.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_extra_features(data_df):\n    data_df['EXPERIENCE_LENGTH'] = data_df.REQUIREMENTS.apply(lambda x: re.search(exp_len,x, re.IGNORECASE).group(1)+ ' ' + re.search(exp_len,x, re.IGNORECASE).group(2) \\\n                                                              if re.search(exp_len,x, re.IGNORECASE) is not None else np.NaN)\n    data_df['FULL_TIME_PART_TIME'] = data_df.REQUIREMENTS.apply(lambda x: re.search(exp_len,x,re.I).group(3) if re.search(exp_len,x,re.I) is not None else np.NaN)\n    data_df['SCHOOL_TYPE'] = data_df.REQUIREMENTS.apply(lambda x: re.search(sch_typ, x, re.I).group(1)\\\n                                                        if re.search(sch_typ, x, re.I) is not None else np.NaN)\n\n    data_df['EDUCATION_YEARS'] = data_df.REQUIREMENTS.apply(lambda x: re.search(edu_y, x, re.I).group(1)\\\n                                                           if re.search(edu_y, x, re.I) is not None else np.NaN)\n\n\n    data_df['EDUCATION_MAJOR'] = data_df.REQUIREMENTS.apply(lambda x: re.search(edu_major, x).group(3)\\\n                                                           if re.search(edu_major, x) is not None else np.NaN)\n    \n    \n    data_df.COURSE_LENGTH = data_df.REQUIREMENTS.apply(lambda x: re.search(course_l, x, re.I).group(0) if re.search(course_l, x, re.I)\\\n                                                  is not None else np.NaN)\n    return data_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above function get_extra_features I am adding the features that can be extracted from requirements."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df = get_extra_features(data_df)\ndata_df.head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us check the percentage of missing values in each column. Five columns are completely misssing as they are not given any values."},{"metadata":{"trusted":true},"cell_type":"code","source":"percent_missing = data_df.isna().mean().round(4) * 100\nmissing_value_df = pd.DataFrame({'percent_missing': percent_missing})\nmissing_value_df.sort_values('percent_missing', ascending=False, inplace=True)\nmissing_value_df.head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# More to be added soon"},{"metadata":{},"cell_type":"markdown","source":"If you like my kernel or think it's helpful, please upvote. Thank You."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}