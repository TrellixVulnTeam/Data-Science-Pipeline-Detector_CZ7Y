{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Problem Statement**\n\nThe content, tone, and format of job bulletins can influence the quality of the applicant pool. Overly-specific job requirements may discourage diversity. The Los Angeles Mayor’s Office wants to reimagine the city’s job bulletins by using text analysis to identify needed improvements.\n\n1)As a first step to help the mayor we covert all the job bulletins into a CSV file using regex library. We name this file as abc.csv.\n\n2)We then read our abc.csv file using pandas and stored the same in varaible calling it \"data\".\n\n3)We then based on the study of the job bulletins we decided that column with name \"selection_process_notes\" as the best feature to arrive at a probabale conclusion as what the mayor should do.\n\n4)Develop a word cloud to see what words are with highest frequency. We see examination, validation and fair as the top key words. However from this wordcloud we are unable to draw any insights. We also try looking at the top ten most frequent words via a bar chart but that does not help either.\n\n5)We then move on to library \"Sentiment Analyser\" to derive the polraity of the words. In layman terms we use this library to advise us what words have negative setiments and or psotive sentiments. Not impressed with the results of sentiment analyser we then used library \"Afinn\" to arrive at sentiments. We grouped the words in selection process notes under neutral, positive and negative. Based on this we visulaized our results. \n\n6)Upon visulatization of results received usig Afinn , we find that the count of words with Neutral sentiment to be the highest at 200 compared to apprx 5 for both positive and negative sentiments.\n\n7)Insights from the above- from the below chart we can draw some insights. We can cleary see the maximum count is neutral - implying the applicants neither feel postive or negative. We need to convert these neutral sentiments to postives in order to gain traction and attract diversity among applicants\n\n8)We did further and decide on doing Topic modeling. We select two features - Job tile and Selection Process Notes to dissect and see what valuable insights could be drawn\n\n9)We use LDA model built into the gesim library to do topic modelling.\n\n10)Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is. In my experience, topic coherence score, in particular, has been more helpful. We arrive at a score of 0.50 which is average score implying that the contents of selection process notes is neutral(confusing) as a result less number of candidates are applying for the job. We recommend that the selection process notes should clealry states it objectives,make it sound positive.\n\n11)We then did word cloud of topics in the selection process notes- again \n\n12)We did clustering of topics and found the cluters were not overalping. We could conclude from this visualization that selections process notes per job title is clealry written however the sentiemnt of the same is neutral making less number of candidates to consides these job as their top pick.\n\n13)We also did Pricnle component analyis, so conclusion\n\nEach bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic.\n\nOur topic model is not good as we have non-overlapping bubbles clustered in one quadrant.\n\nA model with too many topics, will typically have many overlaps, implying the the slection process notes is confusing and not clear. Thus the selection process notes should clearly states its objective and should provide detail as when promotions are available in each class\n\nAlright, if you move the cursor over one of the bubbles, the words and bars on the right-hand side will update. These words are the salient keywords that form the selected topic.\n\nWe have successfully built a good looking topic model.\n\nGiven our prior knowledge of the number of natural topics in the document, finding the best model was fairly straightforward.\n\nConclusion \n(1) identify language that can negatively bias the pool of applicants;\n(2) improve the diversity and quality of the applicant pool; and/or\n(3) make it easier to determine which promotions are available to employees in each job class."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport matplotlib.pyplot as plt\nimport collections \nfrom collections import Counter\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dir_job_bulletins= '../input/cityofla/CityofLA/Job Bulletins'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_job_titles ='../input/cityofla/CityofLA/Additional data/job_titles.csv'\njob_titles = pd.read_csv(file_job_titles, header=None, names=['job_title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_list = []\nfor filename in os.listdir(dir_job_bulletins):\n    with open(os.path.join(dir_job_bulletins, filename), 'r', errors='ignore') as f:\n        data_list.append([filename, ''.join(f.readlines())])\njobs = pd.DataFrame(data_list, columns=['file', 'job_description'])\n\n# Drop row with id 263\njobs.drop([263], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_jobs_data(jobs, extracted_data):\n    jobs['temp'] = extracted_data\n    for index, row in jobs.iterrows():\n        extracted_data = row['temp']\n        if isinstance(extracted_data, pd.DataFrame):\n            for c in extracted_data.columns:\n                jobs.loc[index, c] = extracted_data[c][0]\n    jobs = jobs.drop('temp', axis=1) \n    return jobs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_text_by_regex_index(text, regex_dictionary):\n    regex_dictionary = pd.DataFrame(regex_dictionary, columns=['name', 'regexpr'])\n\n    result = regex_dictionary.copy()\n    result['text'] = ''\n    for index,row in regex_dictionary.iterrows():\n        find_text = re.search(row['regexpr'], text)\n        find_text = find_text.span(0)[0] if find_text else -1\n        result.loc[index, 'start'] = find_text\n    result = result[result['start'] >= 0]\n    result['end'] = result['start'].apply(lambda x: np.min(result[result['start'] > x]['start'])).fillna(len(text))\n\n    for index,row in result.iterrows():\n        extracted_text = text[int(row['start']):int(row['end'])]\n        find_reg = re.findall(row['regexpr']+'(.*)', extracted_text, re.DOTALL|re.IGNORECASE)\n        extracted_text = find_reg[0] if find_reg else ''\n        extracted_text = extracted_text.strip()\n        result.loc[index, 'text'] = extracted_text\n    return result.set_index('name')[['text']].T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regex_dictionary = [('metadata', r''), \n                      ('salary', r'(?:ANNUAL SALARY|ANNUALSALARY)'),\n                      ('duties', r'(?:DUTIES)'),\n                      ('requirements', r'(?:REQUIREMENTS/MINIMUM QUALIFICATIONS|REQUIREMENT/MINIMUM QUALIFICATION|REQUIREMENT|REQUIREMENTS|REQUIREMENT/MIMINUMUM QUALIFICATION|REQUIREMENT/MIMINUMUM QUALIFICATIONS|REQUIREMENT$/MIMINUMUM QUALIFICATION$|REQUIREMENTS)'),\n                      ('where_to_apply', r'(?:WHERE TO APPLY|HOW TO APPLY)'),\n                      ('application_deadline', r'(?:APPLICATION DEADLINE|APPLICATION PROCESS)'),\n                      ('selection_process', r'(?:SELECTION PROCESS|SELELCTION PROCESS)'),\n                      ]\nextracted_data = jobs['job_description'].dropna().apply(lambda x: extract_text_by_regex_index(x, regex_dictionary))\njobs = merge_jobs_data(jobs,extracted_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_metadata(text):\n    # ToDo: Extract additional information add the end of the text '(Exam open...)'\n    job_title = text.split('\\n')[0].strip()\n    regex_class_code = r'(?:Class Code:|Class  Code:)\\s*(\\d\\d\\d\\d)'\n    class_code = re.findall(regex_class_code, text, re.DOTALL|re.IGNORECASE)\n    class_code = class_code[0].strip() if class_code else np.NaN\n\n    regex_open_date = r'(?:Open Date:|Open date:)\\s*(\\d\\d-\\d\\d-\\d\\d)'\n    open_date = re.findall(regex_open_date, text, re.DOTALL|re.IGNORECASE)\n    open_date = open_date[0].strip() if open_date else np.NaN\n\n    regex_revised = r'(?:Revised:|Revised|REVISED:)\\s*(\\d\\d-\\d\\d-\\d\\d)'\n    revised = re.findall(regex_revised, text, re.DOTALL|re.IGNORECASE)\n    revised = revised[0].strip() if revised else np.NaN\n\n    result = pd.DataFrame({'job_title':job_title,\n                           'class_code':class_code,\n                           'open_date':open_date,\n                           'revised':revised}\n                          , index=[0])\n    result['open_date'] = pd.to_datetime(result['open_date'], infer_datetime_format=True)\n    result['revised'] = pd.to_datetime(result['revised'], infer_datetime_format=True)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs = merge_jobs_data(jobs, jobs['metadata'].dropna().apply(extract_metadata))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_salary(text):\n    regex_salary_from = r'\\$((?:\\d{1,3})(?:\\,\\d{3})*(?:\\.\\d{2})*).*'\n    salary_from = re.findall(regex_salary_from, text, re.DOTALL|re.IGNORECASE)\n    salary_from = float(salary_from[0].replace(',', '')) if salary_from else np.NaN\n    \n    regex_salary_to = r'(?:and|to) \\$((?:\\d{1,3})(?:\\,\\d{3})*(?:\\.\\d{2})*).*'\n    salary_to = re.findall(regex_salary_to, text, re.DOTALL|re.IGNORECASE)\n    salary_to = float(salary_to[0].replace(',', '')) if salary_to else np.NaN    \n    \n    regex_salary_flatrated = r'(flat-rated|Flat-Rated)'\n    salary_flatrated = re.findall(regex_salary_flatrated, text, re.DOTALL|re.IGNORECASE)\n    salary_flatrated = True if salary_flatrated else np.NaN    \n    \n    regex_salary_additional = r'(?:\\n)(.*)(?:NOTES)'\n    salary_additional = re.findall(regex_salary_additional, text, re.DOTALL|re.IGNORECASE)\n    salary_additional = salary_additional[0].strip() if salary_additional else np.NaN   \n    \n    regex_salary_notes = r'(?:NOTES:)(.*)'\n    salary_notes = re.findall(regex_salary_notes, text, re.DOTALL|re.IGNORECASE)\n    salary_notes = salary_notes[0].strip() if salary_notes else np.NaN    \n\n    result = pd.DataFrame({'salary_from':salary_from,\n                           'salary_to':salary_to,\n                           'salary_flatrated':salary_flatrated,\n                           'salary_additional':salary_additional,\n                           'salary_notes':salary_notes}\n                          , index=[0])\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs = merge_jobs_data(jobs, jobs['salary'].dropna().apply(extract_salary))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regex_dictionary = [('duties_text', r''), \n                      ('duties_notes', r'(?:NOTE:|NOTES:)'),\n                      ]\nextracted_data = jobs['duties'].dropna().apply(lambda x: extract_text_by_regex_index(x, regex_dictionary))\njobs = merge_jobs_data(jobs, extracted_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regex_dictionary = [('where_to_apply_text', r''), \n                         ('where_to_apply_notes', r'(?:NOTE:)'),\n                      ]\nextracted_data = jobs['where_to_apply'].dropna().apply(lambda x: extract_text_by_regex_index(x, regex_dictionary))\njobs = merge_jobs_data(jobs, extracted_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regex_dictionary = [('application_deadline_text', r''), \n                         ('application_deadline_notes', r'(?:NOTE:)'),\n                         ('application_deadline_review', r'(?:QUALIFICATIONS REVIEW|EXPERT REVIEW COMMITTEE)'),\n                      ]\nextracted_data = jobs['application_deadline'].dropna().apply(lambda x: extract_text_by_regex_index(x, regex_dictionary))\njobs = merge_jobs_data(jobs, extracted_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regex_dictionary = [('selection_process_text', r''), \n                         ('selection_process_notes', r'(?:NOTES:)'),\n                         ('selection_process_notice', r'(?:NOTICE:|Notice:)'),\n                      ]\nextracted_data = jobs['selection_process'].dropna().apply(lambda x: extract_text_by_regex_index(x, regex_dictionary))\njobs = merge_jobs_data(jobs, extracted_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['job_description', 'metadata', 'salary', 'duties', 'where_to_apply', 'application_deadline', 'selection_process']\njobs = jobs.drop(cols, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs.to_csv('abc.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a wordcloud\ndata=pd.read_csv('abc.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data.shape\n#data.dtypes\n#data.isnull().sum()\n\ndata = data.dropna(subset=['selection_process_notes'])\nfrom wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='ivory',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n).generate(str(data))\n\n    fig = plt.figure(1, figsize=(15, 15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n\nshow_wordcloud(data['selection_process_notes'])\n\ndef cleaning(s):\n    s = str(s)\n    s = s.lower()\n    s = re.sub('\\s\\W',' ',s)\n    s = re.sub('\\W,\\s',' ',s)\n    s = re.sub(r'[^\\w]', ' ', s)\n    s = re.sub(\"\\d+\", \"\", s)\n    s = re.sub('\\s+',' ',s)\n    s = re.sub('[!@#$_]', '', s)\n    s = s.replace(\"co\",\"\")\n    s = s.replace(\"https\",\"\")\n    s = s.replace(\",\",\"\")\n    s = s.replace(\"[\\w*\",\" \")\n    s = s.replace(\"dtype\",\"\")\n    return s\ndata['selection_process_notes'] = [cleaning(s) for s in data['selection_process_notes']]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets visualize the top word selection_process_notes in the form of a bar chart:\ntext = data.selection_process_notes[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import collections\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \n  \nstopwords = set(stopwords.words('english')) \nstopwords.update([\"the\", \"The\", \".\", \",\",\"may\"])\nwordtokens = word_tokenize(text) \nfiltered_words = [word for word in wordtokens if word not in stopwords]\ncounted_words = collections.Counter(filtered_words)\n\nwords = []\ncounts = []\nfor letter, count in counted_words.most_common(10):\n    words.append(letter)\n    counts.append(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\n\ncolors = cm.rainbow(np.linspace(0, 1, 10))\nrcParams['figure.figsize'] = 20, 10\n\nplt.title('Top words in the Selection Process Notes vs their count')\nplt.xlabel('Count')\nplt.ylabel('Words')\nplt.barh(words, counts, color=colors)\n\n# we see similar words as in the word cloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets start with the sentiment analysis. \n# we use the NLTK library to arrive at polarity scores and then use Afinn as well. conclude it with visualizations of the sentiments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using selection process notes from the abc.csv file that we created earlier. As this would be enabale us with insights \n#as what needs to be done in order to attract diversity\nfinal_X =data['selection_process_notes']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = set(stopwords.words('english')) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we use lemmatization in order to retain the meaning of the words  \nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import wordnet \nfinal_X = data['selection_process_notes']\nimport re\ntemp =[]\nsnow = WordNetLemmatizer()   # downlaod nltk wordnet before runnng this line of code\nfor sentence in final_X:\n    sentence = sentence.lower()                 # Converting to lowercase\n    cleanr = re.compile('<.*?>')\n    sentence = re.sub(cleanr, ' ', sentence)        #Removing HTML tags\n    sentence = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    sentence = re.sub(r'[.|,|)|(|\\|/]',r' ',sentence)        #Removing Punctuations\n    sentence = re.sub(r'[%]',r' ',sentence)\n    sentence = re.sub(r'\\d+',r' ',sentence)        #Removing Punctuations\n    \n    words = [snow.lemmatize(word) for word in sentence.split() if word not in stopwords.words('english')]   # Lemmatizing and removing stopwords\n    temp.append(words)\n    final_X = temp ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"listwords=[]\nfor sentence in final_X:\n    for key in sentence:\n        listwords.append(key) \n# Creating a dataframe object from listoftuples\ndfObj = pd.DataFrame(listwords,columns=['Word'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#printing polarity scores of words in selection process notes\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk import sentiment\nsid = SentimentIntensityAnalyzer()\n\nfor sentence in final_X:\n    for key in sentence:\n        scores = sid.polarity_scores(key)\n        \n        print('{1} and {0}'.format(key, scores))\n        \n        \n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"!pip install afinn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize afinn sentiment analyzer\nfrom afinn import Afinn\naf = Afinn()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsentiment_scores=[af.score(final_X) for final_X in sentence]\n\nsentiment_category = ['positive' if score > 0 \n                          else 'negative' if score < 0 \n                              else 'neutral' \n                                  for score in sentiment_scores]\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf = pd.DataFrame([list(dfObj['Word']), sentiment_scores, sentiment_category]).T\ndf.columns=['Word', 'sentiment_scores','sentiment_category']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# viuslaize the sentiment scores against the sentiment cateogry to \nimport seaborn as sns\nf, (ax1)= plt.subplots(1, 1, figsize=(10, 4))\nsp = sns.stripplot(x='sentiment_category', y=\"sentiment_scores\", hue='sentiment_category',\n                  data= df, ax=ax1)\n\n\nt = f.suptitle('Visualizing  Sentiment', fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" \nimport seaborn as sns\nf, (ax1,ax2) = plt.subplots(1, 2, figsize=(10,4))\nsp = sns.countplot(x='sentiment_category', hue='sentiment_category',\n                  data= df, ax=ax1)\nsp1 = sns.countplot(x='sentiment_scores', hue='sentiment_scores',\n                  data= df, ax=ax2)\n\nt = f.suptitle('Visualizing  Sentiment', fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=data[['job_title','selection_process_notes']]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pprint import pprint\n# Convert to list\ndata = df.selection_process_notes.values.tolist()\n\n# Remove new line characters\ndata = [re.sub('\\s+', ' ', sent) for sent in data]\n\n# Remove distracting single quotes\ndata = [re.sub(r'[.|,|)|(|\\|/]', \"\", sent) for sent in data]\n\ndata = [re.sub(r'[?|!|\\'|\"|#]', \"\", sent) for sent in data]\n\ndata = [re.sub(r'\\d+', \"\", sent) for sent in data]\n\ndata = [re.sub(r'[%]', \"\", sent) for sent in data]\n# Remove Emails\ndata = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n\npprint(data[:1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ndata_words = list(sent_to_words(data))\n\nprint(data_words[:1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n\n# Faster way to get a sentence clubbed as a trigram/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# See trigram example\nprint(trigram_mod[bigram_mod[data_words[0]]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# spacy for lemmatization\nimport spacy\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define functions for stopwords, bigrams, trigrams and lemmatization\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https://spacy.io/api/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import en_core_web_sm\nspacy.load('en')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove Stop Words\ndata_words_nostops = remove_stopwords(data_words)\n\n# Form Bigrams\ndata_words_bigrams = make_bigrams(data_words_nostops)\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n\n#python -m spacy download en works only if you have administration rights, the installation folder should have admin rights\n\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[:1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View\nprint(corpus[:1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Human readable format of corpus (term-frequency)\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id2word[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n# Enable logging for gensim - optional\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus: Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in data_lemmatized]\n\n# Build LDA model\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=10, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=10,\n                                           passes=10,\n                                           alpha='symmetric',\n                                           iterations=100,\n                                           per_word_topics=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Or, you can see a human-readable form of the corpus itself."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row_list in enumerate(ldamodel[corpus]):\n        row = row_list[0] if ldamodel.per_word_topics else row_list            \n        # print(row)\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_lemmatized)\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\ndf_dominant_topic.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display setting to show more characters in column\npd.options.display.max_colwidth = 100\n\nsent_topics_sorteddf_mallet = pd.DataFrame()\nsent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n\nfor i, grp in sent_topics_outdf_grpd:\n    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n                                            axis=0)\n\n# Reset Index    \nsent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n\n# Format\nsent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n\n# Show\nsent_topics_sorteddf_mallet.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc_lens = [len(d) for d in df_dominant_topic.Text]\n\n# Plot\nplt.figure(figsize=(4,2), dpi=160)\nplt.hist(doc_lens, bins = 1000, color='navy')\nplt.text(750, 100, \"Mean   : \" + str(round(np.mean(doc_lens))))\nplt.text(750,  95, \"Median : \" + str(round(np.median(doc_lens))))\nplt.text(750,  90, \"Stdev   : \" + str(round(np.std(doc_lens))))\nplt.text(750,  85, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01))))\nplt.text(750,  80, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99))))\nplt.gca().set(xlim=(0, 1000), ylabel='Number of Documents', xlabel='Document Word Count')\nplt.tick_params(size=16)\nplt.xticks(np.linspace(0,1000,9))\nplt.title('Distribution of Document Word Counts', fontdict=dict(size=10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How to interpret this?\n\nTopic 0 is a represented as 0.035*\"candidate\" + 0.024*\"city\" + 0.020*\"disability\" + 0.019*\"acmmodation\" + 0.018*\"list\" + 0.018*\"minimum\" + \n  0.017*\"may\" + 0.017*\"examination\" + 0.016*\"employment\" + 0.016*\"promotional\"\n  \nIt means the top 10 keywords that contribute to this topic are: ‘candidate’, ‘city’, ‘disability’.. and so on and the weight of ‘candidate’ on topic 0 is 0.035.\n\nThe weights reflect how important a keyword is to that topic.\n\nLooking at these keywords, we may summarise it  as selection process criteria along with perks of the job."},{"metadata":{},"cell_type":"markdown","source":"** Compute Model Perplexity and Coherence Score**\n\nModel perplexity and topic coherence provide a convenient measure to judge how good a given topic model is. In my experience, topic coherence score, in particular, has been more helpful."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport seaborn as sns\nimport matplotlib.colors as mcolors\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n\nfig, axes = plt.subplots(2,2,figsize=(16,14), dpi=160, sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):    \n    df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.Dominant_Topic == i, :]\n    doc_lens = [len(d) for d in df_dominant_topic_sub.Text]\n    ax.hist(doc_lens, bins = 1000, color=cols[i])\n    ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n    sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx())\n    ax.set(xlim=(0, 1000), xlabel='Document Word Count')\n    ax.set_ylabel('Number of Documents', color=cols[i])\n    ax.set_title('Topic: '+str(i), fontdict=dict(size=16, color=cols[i]))\n\nfig.tight_layout()\nfig.subplots_adjust(top=0.90)\nplt.xticks(np.linspace(0,1000,9))\nfig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=22)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. Wordcloud of Top N words in each topic\nfrom matplotlib import pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.colors as mcolors\n\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n\ncloud = WordCloud(stopwords=stop_words,\n                  background_color='white',\n                  width=2500,\n                  height=1800,\n                  max_words=10,\n                  colormap='tab10',\n                  color_func=lambda *args, **kwargs: cols[i],\n                  prefer_horizontal=1.0)\n\ntopics = lda_model.show_topics(formatted=False)\n\nfig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    fig.add_subplot(ax)\n    topic_words = dict(topics[i][1])\n    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n    plt.gca().imshow(cloud)\n    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n    plt.gca().axis('off')\n\n\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.axis('off')\nplt.margins(x=0, y=0)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\ntopics = lda_model.show_topics(formatted=False)\ndata_flat = [w for w_list in data_lemmatized for w in w_list]\ncounter = Counter(data_flat)\n\nout = []\nfor i, topic in topics:\n    for word, weight in topic:\n        out.append([word, i , weight, counter[word]])\n\ndf = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n\n# Plot Word Count and Weights of Topic Keywords\nfig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\nfor i, ax in enumerate(axes.flatten()):\n    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n    ax_twin = ax.twinx()\n    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n    ax.set_ylabel('Word Count', color=cols[i])\n    ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n    ax.tick_params(axis='y', left=False)\n    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n\nfig.tight_layout(w_pad=2)    \nfig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sentence Coloring of N Sentences\nfrom matplotlib.patches import Rectangle\n\ndef sentences_chart(lda_model=lda_model, corpus=corpus, start = 0, end = 13):\n    corp = corpus[start:end]\n    mycolors = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n\n    fig, axes = plt.subplots(end-start, 1, figsize=(20, (end-start)*0.95), dpi=160)       \n    axes[0].axis('off')\n    for i, ax in enumerate(axes):\n        if i > 0:\n            corp_cur = corp[i-1] \n            topic_percs, wordid_topics, wordid_phivalues = lda_model[corp_cur]\n            word_dominanttopic = [(lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics]    \n            ax.text(0.01, 0.5, \"Doc \" + str(i-1) + \": \", verticalalignment='center',\n                    fontsize=16, color='black', transform=ax.transAxes, fontweight=700)\n\n            # Draw Rectange\n            topic_percs_sorted = sorted(topic_percs, key=lambda x: (x[1]), reverse=True)\n            ax.add_patch(Rectangle((0.0, 0.05), 0.99, 0.90, fill=None, alpha=1, \n                                   color=mycolors[topic_percs_sorted[0][0]], linewidth=2))\n\n            word_pos = 0.06\n            for j, (word, topics) in enumerate(word_dominanttopic):\n                if j < 14:\n                    ax.text(word_pos, 0.5, word,\n                            horizontalalignment='left',\n                            verticalalignment='center',\n                            fontsize=16, color=mycolors[topics],\n                            transform=ax.transAxes, fontweight=700)\n                    word_pos += .009 * len(word)  # to move the word for the next iter\n                    ax.axis('off')\n            ax.text(word_pos, 0.5, '. . .',\n                    horizontalalignment='left',\n                    verticalalignment='center',\n                    fontsize=16, color='black',\n                    transform=ax.transAxes)       \n\n    plt.subplots_adjust(wspace=0, hspace=0)\n    plt.suptitle('Sentence Topic Coloring for Documents: ' + str(start) + ' to ' + str(end-2), fontsize=22, y=0.95, fontweight=700)\n    plt.tight_layout()\n    plt.show()\n\nsentences_chart()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sentence Coloring of N Sentences\ndef topics_per_document(model, corpus, start=0, end=1):\n    corpus_sel = corpus[start:end]\n    dominant_topics = []\n    topic_percentages = []\n    for i, corp in enumerate(corpus_sel):\n        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n        dominant_topics.append((i, dominant_topic))\n        topic_percentages.append(topic_percs)\n    return(dominant_topics, topic_percentages)\n\ndominant_topics, topic_percentages = topics_per_document(model=lda_model, corpus=corpus, end=-1)            \n\n# Distribution of Dominant Topics in Each Document\ndf = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\ndominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\ndf_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n\n# Total Topic Distribution by actual weight\ntopic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\ndf_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n\n# Top 3 Keywords for each Topic\ntopic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n\ndf_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\ndf_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\ndf_top3words.reset_index(level=0,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.ticker import FuncFormatter\n\n# Plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), dpi=120, sharey=True)\n\n# Topic Distribution by Dominant Topics\nax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_doc, width=.5, color='firebrick')\nax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\ntick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\nax1.xaxis.set_major_formatter(tick_formatter)\nax1.set_title('Number of Documents by Dominant Topic', fontdict=dict(size=10))\nax1.set_ylabel('Number of Documents')\nax1.set_ylim(0, 1000)\n\n# Topic Distribution by Topic Weights\nax2.bar(x='index', height='count', data=df_topic_weightage_by_doc, width=.5, color='steelblue')\nax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\nax2.xaxis.set_major_formatter(tick_formatter)\nax2.set_title('Number of Documents by Topic Weightage', fontdict=dict(size=10))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import bokeh","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get topic weights and dominant topics ------------\n\nfrom sklearn.manifold import TSNE\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\n\n# Get topic weights\ntopic_weights = []\nfor i, row_list in enumerate(lda_model[corpus]):\n    topic_weights.append([w for i, w in row_list[0]])\n\n# Array of topic weights    \narr = pd.DataFrame(topic_weights).fillna(0).values\n\n# Keep the well separated points (optional)\narr = arr[np.amax(arr, axis=1) > 0.35]\n\n# Dominant topic number in each doc\ntopic_num = np.argmax(arr, axis=1)\n\n# tSNE Dimension Reduction\ntsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\ntsne_lda = tsne_model.fit_transform(arr)\n\n# Plot the Topic Clusters using Bokeh\noutput_notebook()\nn_topics = 4\nmycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\nplot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n              plot_width=900, plot_height=700)\nplot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\nshow(plot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualize the topics-keywords**\n\nNow that the LDA model is built, the next step is to examine the produced topics and the associated keywords. There is no better tool than pyLDAvis package’s interactive chart and is designed to work well with jupyter notebooks."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the topics\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim  # don't skip this\nimport matplotlib.pyplot as plt\n# Visualize the topics\n# Visualize the topics\n\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\nvis","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}