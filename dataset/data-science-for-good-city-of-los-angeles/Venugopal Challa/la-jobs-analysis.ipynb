{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Data Science for Good: City of Los Angeles**"},{"metadata":{},"cell_type":"markdown","source":"**Main Theme:**\n\nWe have to help the city of  Los Angeles to structure and analyze its job descriptions so that in future they can fill the retired job positions with the better qualified people and also help the pool of job seekers to better understand the summary of requirements for particular job.\n\nAlso give some numbers like in 50,000 workers 1/3 of them will be retired by July 2020. I hope this is for city corpation of Los Angeles. Lets compare it with the problem statement. \n\n\n**Problem statement:**\nThe content, tone, and format of job bulletins can influence the quality of the applicant pool. Overly-specific job requirements may discourage diversity. The Los Angeles Mayor’s Office wants to reimagine the city’s job bulletins by using text analysis to identify needed improvements.\n\n![City Mayor](https://www.lacity.org/sites/g/files/wph1196/f/styles/isotope_tiles_full_tile__680x410_/public/FYF-Job-Fair.jpg?itok=xbHrcvIH)\n\n\n**Goals:**\nTo convert a folder full of plain-text job postings into a single structured CSV file and then to use this data to:\n        <p style=\"text-indent:40px;\">(1) identify language that can negatively bias the pool of applicants.</p> \n        <p style=\"text-indent:40px;\">(2) improve the diversity and quality of the applicant pool; and/or </p>\n        <p style=\"text-indent:40px;\">(3) make it easier to determine which promotions are available to employees in each job class.\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### importing required modules\n\nimport matplotlib.pyplot as plt\nimport re\nimport spacy\nfrom spacy import displacy\nimport nltk\nfrom fuzzywuzzy import fuzz, process\nimport warnings\nimport json\nimport gensim\nimport seaborn as sns\nimport wordcloud\nfrom PIL import Image\nimport requests\nimport textblob\nfrom gensim.parsing.preprocessing import remove_stopwords\nwarnings.filterwarnings(action=\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"job_bulletins=os.listdir(\"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Job Bulletins/\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"path_job_bulletins = \"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Job Bulletins/\"\npath_additional_data = \"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Additional data/\"\n### regex list:\n### 1. multiple spaces\nmul_sp_pat = re.compile(r\"\\s+\")\n### 2. removing \\n\nnxt_ln_pat = re.compile(r\"\\n\")\nbulletins_list1 = list()\nfor job in job_bulletins:\n    with open(\"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Job Bulletins/\"+(job), 'r', encoding='ISO-8859-1') as file: \n        bulletine = file.readlines()\n    mod_bulletine = list()\n    k = str()\n    for line in bulletine:\n        k = (mul_sp_pat.sub(repl=\" \", string=line)).strip()\n        k = nxt_ln_pat.sub(repl=\"\", string=k)\n        if k!=\"\":\n            mod_bulletine.append(k)\n    bulletins_list1.append(mod_bulletine)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## picking the headings in the each job bulletin based on the condition that heading is in uppercase.\nheading_list=list()\nfor each_bulletin in bulletins_list1:\n    ### passing the very first line in each job bulletin since it is the position name which is in uppercase. so not to confuse it with heading names\n    iteration = iter(each_bulletin)\n    next(iteration)\n    for line in iteration:\n        if line.isupper():\n            heading_list.append(line)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(heading_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### selecting the reasonable headings to form the list\nkey_words_list = ['ANNUAL SALARY','APPLICATION DEADLINE','APPLICATION PROCESS','CERTIFICATION','CONDITIONS OF EMPLOYMENT', 'DUTIES','NOTE','NOTES',\n                  'NOTICE', 'WHERE TO APPLY', 'SCORE BANDING','SELECTIVE CERTIFICATION', 'SELECTION PROCEDURE','SELELCTION PROCESS', 'SKILLS, KNOWLEDGES, ABILITIES, AND PERSONAL QUALIFICATIONS', \n                 'SPECIAL INFORMATION', 'POSITIONS AVAILABLE', 'PROCESS NOTE','QUALIFICATIONS REVIEW','REQUIRED MATERIALS','REQUIREMENTS', 'REQUIREMENTS/ MINIMUM QUALIFICATIONS','ADDITIONAL JOB INFORMATION',\n                  'ALLOWABLE CALCULATORS', 'EXPERT REVIEW COMMITTEE',  ]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = bulletins_list1[15]\nk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfinal_data_json = list()\nfor job_title,jobs in zip(job_bulletins,bulletins_list1):\n    job_document = dict()\n    job_document.update({\"job_position\":re.sub(pattern=\"\\s+\\d+.*\",repl=\"\", string=job_title.replace(\".txt\",\"\"))})\n    flag = False\n    for lines in jobs:\n        ### condition for adding the job class code\n        if True in [lines.lower().startswith(word) for word in ['classcode', 'class code', 'class code:', 'classcode:']]:\n            job_document.update({\"Class Code\" : re.findall(pattern=r\"(\\d+)\", string=lines)[0]})\n        fuzzy_process_tuple = process.extractOne(lines, key_words_list)\n        if fuzzy_process_tuple[1]>=90:\n            if flag:\n                job_document.update({dict_key: data_list})\n            flag = True\n            data_list = list()\n            dict_key = fuzzy_process_tuple[0]\n        else:\n            if flag:\n                data_list.append(lines)\n    left_out_columns = set(key_words_list)-set(job_document.keys())\n    for cols in left_out_columns:\n        job_document.update({cols:np.nan})\n    final_data_json.append(job_document)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### creating the dataframe based on the created json object\njobs_dataframe=pd.DataFrame(final_data_json)\njobs_dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Need to do some twicks here i hope the REQUIRMENTS and REQUIREMENTS/ MINIMUM QUALIFICATIONS are same as they tell what kind of qualification are required for the job position. So combine both as single column\njobs_dataframe['REQUIREMENTS/ MINIMUM QUALIFICATIONS'][jobs_dataframe['REQUIREMENTS/ MINIMUM QUALIFICATIONS'].isna()]=jobs_dataframe['REQUIREMENTS'][jobs_dataframe['REQUIREMENTS/ MINIMUM QUALIFICATIONS'].isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### we can see all the REQUIREMENTS/ MINIMUM QUALIFICATIONS has been filled up so we will remove the REQUIREMENTS column.\njobs_dataframe[jobs_dataframe['REQUIREMENTS/ MINIMUM QUALIFICATIONS'].isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs_dataframe.drop(columns=['REQUIREMENTS'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now lets clean each columns since it has lot of data in it because we appended the lines below each heading to a list**"},{"metadata":{},"cell_type":"markdown","source":"**Salary range column cleaning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#### regex patterns for finding salary figures and salary range figures with (to or -)\nsal_fig_rng_pat = re.compile(r\"(?:(\\$\\s*\\d+(?:\\,\\d+)?\\s+(?:to|-)\\s+\\$\\d+(?:\\,\\d+)?)|(\\$\\s*\\d+(?:\\,\\d+)?))\") ### compiling it once so it can fasten the pattern finding\n\n\n### according the given criteria in kaggle_data_dictionary.csv in general salary range only the first range is to be picked and the other ranges should be ignored.\nonly_first_sal_rng_pat = re.compile(r\"(?:(^\\$\\s*\\d+(?:\\,\\d+)?\\s+(?:to|-)\\s+\\$\\d+(?:\\,\\d+)?)|(^\\$\\s*\\d+(?:\\,\\d+)?))\")\n\n\nannual_sal = list()\nfor sal_list,job_title in zip(jobs_dataframe['ANNUAL SALARY'],jobs_dataframe['job_position']):\n    sample_dict = dict()\n    if str(sal_list)!=\"nan\":\n        if len(sal_list)>1:\n            flag_temp = False\n            for line_no in range(len(sal_list)):\n                if fuzz.partial_token_set_ratio(sal_list[line_no],'department')>=80 and line_no>0:\n                    temp = sal_fig_rng_pat.findall(string=sal_list[line_no])\n                    if temp:\n                        sample_dict.update({\"Department_salary\":\"\".join(temp[0])})\n                    else:\n                        sample_dict.update({\"Department_salary\":\"Not specified\"})\n                else:\n                    temp = sal_fig_rng_pat.findall(string=sal_list[line_no])\n                    if temp and not flag_temp:\n                        flag_temp = True\n                        sample_dict.update({\"General_salary\":\"\".join(temp[0])})\n                    elif temp and flag_temp:\n                        sample_dict.update({\"Other_salary_range\":\"\".join(temp[0])})\n                    else:\n                         sample_dict.update({\"General_salary\":\"Not specified\"})\n        else:\n            ## divide the line into two parts like before the word department and after the word department\n            ## before department word is general salary range and after department word is department salary range\n            dept_flg = False\n            aftr_dept_line = str()\n            gen_sal_line = str()\n            for word in sal_list[0].split():\n                if fuzz.partial_token_set_ratio(word,\"department\")>=75:\n                    dept_flg = True\n                if dept_flg:\n                    aftr_dept_line = aftr_dept_line+\" \"+word\n                else:\n                    gen_sal_line = gen_sal_line+\" \"+word\n            temp = re.findall(pattern=sal_fig_rng_pat,string=aftr_dept_line.strip())\n            if temp:\n                sample_dict.update({\"Department_salary\":\"\".join(temp[0])})\n            else:\n                sample_dict.update({\"Department_salary\":\"Not specified\"})\n            temp = re.findall(pattern=only_first_sal_rng_pat, string=gen_sal_line.strip())\n            if temp:\n                sample_dict.update({\"General_salary\":\"\".join(temp[0])})\n            else:\n                sample_dict.update({\"General_salary\":\"Not specified\"})             \n    else:\n        sample_dict = dict()\n        sample_dict.update({\"General_salary\":\"Not specified\"})\n        sample_dict.update({\"Department_salary\":\"Not specified\"})\n        \n    annual_sal.append(sample_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Annual Salary\n## Salary range has the General and Department wise salaries so im splitting here into to columns.\n## create a 2 columned dataframe with column names as General_salary_range and department_salary_range\n\nannual_salary_data = pd.DataFrame(columns=['job_position','Department_salary','General_salary','Other_salary_range'])\nannual_salary_data['job_position']=jobs_dataframe['job_position']\nannual_salary_data[['Department_salary','General_salary','Other_salary_range']]=pd.read_json(json.dumps(annual_sal))\n\n### replace the NaNs with \"Not Specified\"\nannual_salary_data.fillna(value=\"Not specified\", inplace=True)\nprint(annual_salary_data.isna().sum())\nannual_salary_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Clean REQUIREMENTS/ MINIMUM QUALIFICATIONS **"},{"metadata":{"trusted":true},"cell_type":"code","source":"requirements_df = jobs_dataframe[['job_position','REQUIREMENTS/ MINIMUM QUALIFICATIONS']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in requirements_df.loc[:,'REQUIREMENTS/ MINIMUM QUALIFICATIONS']:\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### creating the requirments, requirment subset, lemma, pos, tag, is_stop_words\nnlp=spacy.load(name=\"en_core_web_sm\")\nspacy_df = pd.DataFrame(columns=['job_title','requirement_no','requirement_subset','tokens_text','tokens_lemma','tokens_pos','tokens_tag','tokens_is_stop'])\nputn_pat = re.compile(r\"[.,\\/#!$%\\^&\\*;?<>:{}=\\_`~\\[\\]\\\"\\']\")\nbullet_pat = re.compile(r\"^\\d.|^\\w\\.\")\nfor job_title,each_req in zip(requirements_df['job_position'],requirements_df['REQUIREMENTS/ MINIMUM QUALIFICATIONS']):\n    requirment_subset=\"nan\"\n    requirement_no = \"nan\"\n    for line in each_req:\n        dummy_list = list()\n        ### finding the bullet numbers and alphabets for each line\n        req_number = bullet_pat.findall(string=line)\n        if req_number:\n            line = bullet_pat.sub(repl=\"\",string=line)\n            if req_number[0][0].replace(\".\",\"\").isalpha():\n                requirment_subset = req_number[0][0].replace(\".\",\"\")\n            else:\n                requirement_no = req_number[0][0].replace(\".\",\"\")\n                requirment_subset = \"nan\"\n        ### Puntuation removal\n        line = putn_pat.sub(repl=\" \",string=line)\n        doc = nlp(line)\n        for tokens in doc:\n            dummy_list.append([job_title,requirement_no,requirment_subset,tokens.text,tokens.lemma_,tokens.pos_,tokens.tag_,tokens.is_stop])\n        spacy_df = pd.concat([spacy_df,pd.DataFrame(dummy_list,columns=['job_title','requirement_no','requirement_subset','tokens_text','tokens_lemma','tokens_pos','tokens_tag','tokens_is_stop'])], axis=0)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_understanding  = pd.DataFrame(spacy_df[['tokens_pos','tokens_tag']].groupby(['tokens_pos','tokens_tag'])['tokens_tag'].count())\ntoken_understanding.rename({'tokens_tag':'count_of_tags'}, axis='columns', inplace=True)\ntoken_understanding.reset_index(inplace=True)\n### expanding the tags and pos so that we can get the better understanding of it\ntoken_understanding.tokens_pos = token_understanding.tokens_pos.map(lambda word: word+\" - \"+spacy.explain(word))\ntoken_understanding.tokens_tag = token_understanding.tokens_tag.map(lambda word: \"\" if not spacy.explain(word) else word+\" - \"+spacy.explain(word))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spacy_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### removing the Punctuations spaces determiner adposition coordinating conjunction\nnew_spacy_df = spacy_df[~spacy_df.tokens_pos.isin(['PUNCT','SPACE','DET','ADP','CCONJ'])]\nnew_spacy_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Duties feild analysis"},{"metadata":{},"cell_type":"markdown","source":"Lets see the duties field already while converting it to structured format we grouped all the mentioned duties of each job into lists types. Now lets analyse it to find is there any gender bias words or pronouns in it."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in jobs_dataframe.DUTIES:\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Flattening the list and removing the **NaN** in the duties column for building the word cloud excluding the stop words"},{"metadata":{"trusted":true},"cell_type":"code","source":"## flattening the list and removing the nan in the Duties column for building the Word cloud.\nflattened_duties_list =sum(list(jobs_dataframe[~jobs_dataframe.DUTIES.isna()]['DUTIES']),[])\ntext = \" \".join(flattened_duties_list)\nword_cloud = wordcloud.WordCloud(background_color=\"white\").generate(text)\nplt.figure(figsize=(10,10))\nplt.imshow(word_cloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see few words like **equal employment**, **applies sound techniques**, **building and maintaining** are repeating most of the times so we can think that most of the jobs are related to maintaining some work process and also we can see that equal employment oppurtunity also has a quite high frequency."},{"metadata":{},"cell_type":"markdown","source":"Above we did the overall duties analysis, now lets clean the each row for further deep analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"## remove the punctuations\nputn_pat = re.compile(r\"[.,\\/#!$%\\^&\\*;?<>:{}=\\_`~\\[\\]\\\"\\-\\']\")\ncleaned_duties = list(map(lambda line: putn_pat.sub(repl=\" \",string=line),flattened_duties_list))\n\n### using simple module textblob for pos tagging before i used spacy but of no use and i got some misleading results\ntb_cleaned_duties = list(map(lambda line: textblob.TextBlob(line), cleaned_duties))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After removing the junk like punctuations we tag the parts of speech to the each word in the each sentence. For your reference you can see the POS tags with there full forms [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). <br> \nLet us check the **Pronouns** to find is there any bias like mention of **he or she** words. <br>\nActually according to the POS tagging the Pronouns are of two types **Personal Pronoun** and **Possesive Pronouns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"### list the two types of Pronouns and the four types of nouns \npronouns = list()\nnouns = list()\nfor indx,duties in enumerate(tb_cleaned_duties):\n    for i in duties.tags:\n        if i[1]=='PRP' or i[1]=='PRP$' : ### Personel Pronoun and Possesive Pronoun\n            x = list(i)\n            x.append(indx)\n            pronouns.append(x)\n        if i[1] in ['NN','NNS','NNP','NNPS']: ### Noun singular, Noun plural, Popular noun singular, Popular noun plural\n            x = list(i)\n            x.append(indx)\n            nouns.append(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the below bar chart we can see the frequency of some of the pronoun words like **their, its, it, they, s and i**. Lets check why they used **\"I\"** since it is a first person pronoun (We use **I** and **me** to refer to the speaker or writer. I is the subject form and me is the object form)"},{"metadata":{"trusted":true},"cell_type":"code","source":"pronouns = pd.DataFrame(pronouns)\nplt.figure(figsize=(10,5))\npronouns[0].map(lambda line: line.lower()).value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let us see the context where they used the **\"I\"** in the duties."},{"metadata":{"trusted":true},"cell_type":"code","source":"pronouns[pronouns[0].str.lower()=='i']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the **I** is present in **264** job type duties. Lets see it.[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_duties[264]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that the person who posted the job is explaining his work itself like **\" a Vocational worker I (instead of Custodian) will perform ....\"**. When we see the complete job posting below."},{"metadata":{"trusted":true},"cell_type":"code","source":"with open (path_job_bulletins+\"Vocational Worker  DEPARTMENT OF PUBLIC WORKS.txt\", 'r', encoding='ISO-8859-1') as file:\n    posting = file.readlines()\nfor i in posting:\n    if i not in ['\\n']:\n        print(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the **4th** line it is like **VOCATIONAL WORKER Is (in-lieu of Custodian)**. So what i think is it might be some spelling mistake or it might be roman letter saying vocational 1 or 2 or 3."},{"metadata":{},"cell_type":"markdown","source":"Now drilling down in to Nouns that we collected. <br><br>\n**Nouns ending in -er, -or, -ar**<br>\n<br>\nThis ending is the most common. It's usually added to verbs to make nouns with the meaning ‘a person or thing that does something’, for example: builder, farmer, sprinkler, or beeper. <br>\nLets check it whether we have such nouns that define the person."},{"metadata":{"trusted":true},"cell_type":"code","source":"## converting the list of list to dataframe for easy analysis\nnouns = pd.DataFrame(nouns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_nouns = list()\nfor i in nouns[0].str.lower():\n    if any([i.endswith(j) for j in ['or','er','ar']]):\n        filter_nouns.append(i)\n\nplt.figure(figsize=(25,8))\npd.Series(filter_nouns).value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above chart we can see that few of the words which are not verbs but have the **er, ar, or** in there suffix. So we have to remove it and finally get only the verbs having those suffix and acting as a person or a thing in this context of duties"},{"metadata":{},"cell_type":"markdown","source":"According to my research i found a inspiring paper written by Danielle Gaucher, Justin Friesen, and Aaron C. Kay back in 2011, called [Evidence That Gendered Wording in Job Advertisements Exists and Sustains Gender Inequality](http://gender-decoder.katmatfield.com/static/documents/Gaucher-Friesen-Kay-JPSP-Gendered-Wording-in-Job-ads.pdf). which has few set of words which particularly defines the **MASCULINE** and few for **FEMININE**.\nI don't totally agree with all the entries in these wordlists. Lets create a data dictionary for but masculine and feminine words and see whats happening in the duties of each job posting.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"### all the words are converted to its root level so we can find the different variations of the same words\nmasculine_words = list(('active',\n'adventurous',\n'aggress',\n'ambitio',\n'analy',\n'assert',\n'athlet',\n'autonom',\n'battle',\n'boast',\n'challeng',\n'champion',\n'compet',\n'confident',\n'courag',\n'decid',\n'decisio,n'\n'decisive',\n'defend',\n'determin,'\n'domina',\n'dominant',\n'driven',\n'fearless',\n'fight',\n'force',\n'greedy',\n'head-strong',\n'headstrong',\n'hierarch',\n'hostil',\n'impulsive',\n'independen',\n'individual',\n'intellect',\n'lead',\n'logic',\n'objective',\n'opinion',\n'outspoken',\n'persist',\n'principle',\n'reckless',\n'self-confiden',\n'self-relian',\n'self-sufficien',\n'selfconfiden',\n'selfrelian',\n'selfsufficien',\n'stubborn',\n'superior',\n'unreasonab'))\n\nfeminine_words = list(('agree',\n'affectionate',\n'child',\n'cheer',\n'collab',\n'commit',\n'communal',\n'compassion',\n'connect',\n'considerate',\n'cooperat',\n'co-operat',\n'depend',\n'emotiona',\n'empath',\n'feel',\n'flatterable',\n'gentle',\n'honest',\n'interpersonal',\n'interdependen',\n'interpersona',\n'inter-personal',\n'inter-dependen',\n'inter-persona',\n'kind',\n'kinship',\n'loyal',\n'modesty',\n'nag',\n'nurtur',\n'pleasant',\n'polite',\n'quiet',\n'respon',\n'sensitiv',\n'submissive',\n'support',\n'sympath',\n'tender',\n'together',\n'trust',\n'understand',\n'warm',\n'whin',\n'enthusias',\n'inclusive',\n'yield',\n'share',\n'sharin',\n))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sorry if the above words hurt any of your feelings. Im just trying to find the word from the research papers mentioning. Finding the words now."},{"metadata":{"trusted":true},"cell_type":"code","source":"### skipping the jobs with no duties\nspacy.l\nduties = jobs_dataframe[~jobs_dataframe.DUTIES.isna()][['job_position','DUTIES']]\nfor i in range(duties.shape[0]):\n    doc = nlp(' '.join(duties.iloc[i,1]))\n    for d in doc:\n        print(d.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## we can see no gender biased are being used in the Duties like he,she etc.,\nnouns_words = pd.Series([i[0].lower() for i in nouns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnouns_count = nouns_words.value_counts().reset_index()\nnouns_count.rename(columns={'index':'words', 0:'count'}, inplace=True)\n### lets see the noun words used only once in duties in all the job requests\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,10))\nnouns_count[nouns_count['count']>5].plot(kind='bar', x='words',y='count')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}