{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 1. Problem statement\n\nThe City of Los Angeles faces a big hiring challenge: 1/3 of its 50,000 workers are eligible to retire by July of 2020. The city has partnered with Kaggle to create a competition to improve the job bulletins that will fill all those open positions.\n\nThe content, tone, and format of job bulletins can influence the quality of the applicant pool. Overly-specific job requirements may discourage diversity. The Los Angeles Mayor’s Office wants to reimagine the city’s job bulletins by using text analysis to identify needed improvements.\n\nThe goal is to convert a folder full of plain-text job postings into a single structured CSV file and then to use this data to:\n* (1) Identify language that can negatively bias the pool of applicants ;\n* (2) Improve the diversity and quality of the applicant pool ; and/or\n* (3) Make it easier to determine which promotions are available to employees in each job class."},{"metadata":{},"cell_type":"markdown","source":"## 2. Approach\n\nIn this kernel we focus on parts (2) and (3) of the problem statement. We present a keyword-based approach, which consists of two parts:\n* **Part 1: A keyword-based recommender system**\n\nWe design a recommender system which helps to find related vacancies, which will facilitate the search for promotions upwards in the hierarchical path, as well as opportunities for horizontal movement/job rotation ;\n* **Part 2: Automated keyword assignment**\n\nAs part 1 illustrates the need for well chosen keywords, we develop a model to try to automate keyword assignment to the vacancies as a first step to improve the visibility and searchability of the vacancies."},{"metadata":{},"cell_type":"markdown","source":"### References\n\n[1] Es Shahul (2019) Discovering opportunities at LA (https://www.kaggle.com/shahules/discovering-opportunities-at-la)\n\n* As we decided to focus on the solution itself, we made use of the code created by Shahul Es to read and preprocess the job vacancies into a single dataframe. Shahul Es has done a tremendous job in processing these files, so we are very grateful to be able to use this code.\n\n[2] Jobscan (2018) Top 500 Resume Keywords: Examples for Your Job Search (https://www.jobscan.co/blog/top-resume-keywords-boost-resume/)\n\n* We used a list of the 500 most frequently occurring job related keywords as a starting point to create the recommender system and train the keyword assignment model.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## 3. Part 1: A keyword-based recommender system"},{"metadata":{},"cell_type":"markdown","source":"Importing packages and data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport os\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import chi2, SelectKBest\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\npy.init_notebook_mode(connected=True)\nimport copy\nfrom scipy import sparse\nfrom itertools import combinations\nfrom warnings import warn\n\ndatadir=r\"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA\"\nkeywords_file = r\"../input/top-500-resume-keywords/resume_keywords_clean.txt\"\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read and preprocess the vacancies (this code section is written by [1])"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import textstat\nfiles = [dir for dir in os.walk(datadir)]\nbulletins = os.listdir(datadir + \"/Job Bulletins/\")\nadditional = os.listdir(datadir + \"/Additional data/\")\nbulletins = os.listdir(datadir + \"/Job Bulletins/\")\nadditional = os.listdir(datadir + \"/Additional data/\")\n\ncsvfiles = []\nfor file in additional:\n    if file.endswith('.csv'):\n        csvfiles.append(datadir + \"/Additional data/\" + file)\ncsvfiles = []\nfor file in additional:\n    if file.endswith('.csv'):\n        csvfiles.append(datadir + \"/Additional data/\" + file)\n\njob_title = pd.read_csv(csvfiles[0])\nsample_job = pd.read_csv(csvfiles[1])\nkaggle_data = pd.read_csv(csvfiles[2])\njob_title = pd.read_csv(csvfiles[0])\nsample_job = pd.read_csv(csvfiles[1])\nkaggle_data = pd.read_csv(csvfiles[2])\njob_title.head()\nprint(\"The are %d rows and %d cols in job_title file\" % (job_title.shape))\nprint(\"The are %d rows and %d cols in sample_job file\" % (sample_job.shape))\nprint(\"The are %d rows and %d cols in kaggle_data file\" % (kaggle_data.shape))\nprint(\"There are %d text files in bulletin directory\" % len(bulletins))\n\ndef get_headings(bulletin):\n    with open(datadir + \"/Job Bulletins/\" + bulletins[bulletin]) as f:  ##reading text files\n        data = f.read().replace('\\t', '').split('\\n')\n        data = [head for head in data if head.isupper()]\n        return data\n\ndef clean_text(bulletin):\n    with open(datadir + \"/Job Bulletins/\" + bulletins[bulletin]) as f:\n        data = f.read().replace('\\t', '').replace('\\n', '')\n        return data\n\nget_headings(1)\nget_headings(2)\n\ndef to_dataframe(num, df):\n    opendate = re.compile(r'(Open [D,d]ate:)(\\s+)(\\d\\d-\\d\\d-\\d\\d)')  # match open date\n    salary = re.compile(r'\\$(\\d+,\\d+)((\\s(to|and)\\s)(\\$\\d+,\\d+))?')  # match salary\n    requirements = re.compile(r'(REQUIREMENTS?/\\s?MINIMUM QUALIFICATIONS?)(.*)(PROCESS NOTE)')  # match requirements\n\n    for no in range(0, num):\n        with open(datadir + \"/Job Bulletins/\" + bulletins[no],\n                  encoding=\"ISO-8859-1\") as f:  # reading files\n            try:\n                file = f.read().replace('\\t', '')\n                data = file.replace('\\n', '')\n                headings = [heading for heading in file.split('\\n') if heading.isupper()]  ##getting heading from job bulletin\n\n                sal = re.search(salary, data)\n                date = datetime.strptime(re.search(opendate, data).group(3), '%m-%d-%y')\n                try:\n                    req = re.search(requirements, data).group(2)\n                except Exception as e:\n                    req = re.search('(.*)NOTES?', re.findall(r'(REQUIREMENTS?)(.*)(NOTES?)',\n                                                             data)[0][1][:1200]).group(1)\n\n                duties = re.search(r'(DUTIES)(.*)(REQ[A-Z])', data).group(2)\n                try:\n                    enddate = re.search(\n                        r'(JANUARY|FEBRUARY|MARCH|APRIL|MAY|JUNE|JULY|AUGUST|SEPTEMBER|OCTOBER|NOVEMBER|DECEMBER)\\s(\\d{1,2},\\s\\d{4})'\n                        , data).group()\n                except Exception as e:\n                    enddate = np.nan\n\n                selection = [z[0] for z in re.findall('([A-Z][a-z]+)((\\s\\.\\s)+)', data)]  ##match selection criteria\n\n                df = df.append({'File Name': bulletins[no], 'Position': headings[0].lower(), 'salary_start': sal.group(1),\n                                'salary_end': sal.group(5), \"opendate\": date, \"requirements\": req, 'duties': duties,\n                                'deadline': enddate, 'selection': selection}, ignore_index=True)\n\n                reg = re.compile(\n                    r'(One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|one|two|three|four)\\s(years?)\\s(of\\sfull(-|\\s)time)')\n                df['EXPERIENCE_LENGTH'] = df['requirements'].apply(\n                    lambda x: re.search(reg, x).group(1) if re.search(reg, x) is not None else np.nan)\n                df['FULL_TIME_PART_TIME'] = df['EXPERIENCE_LENGTH'].apply(lambda x: 'FULL_TIME' if x is not np.nan else np.nan)\n\n                reg = re.compile(r'(One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|one|two|three|four)(\\s|-)(years?)\\s(college)')\n                df['EDUCATION_YEARS'] = df['requirements'].apply(\n                    lambda x: re.search(reg, x).group(1) if re.search(reg, x) is not None else np.nan)\n                df['SCHOOL_TYPE'] = df['EDUCATION_YEARS'].apply(lambda x: 'College or University' if x is not np.nan else np.nan)\n\n            except Exception as e:\n                ''\n                #print('Failed to read file ' + bulletins[no])\n\n    return df\n\ndf = pd.DataFrame(\n    columns=['File Name', 'Position', 'salary_start', 'salary_end', 'opendate', 'requirements', 'duties', 'deadline'])\ndf = to_dataframe(len(bulletins), df)\ndf.replace(to_replace=[None], value=\"N/A\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We start our analysis by processing the job duties using the TfidfVectorizer method, with the external keyword list of 500 items ([2]) as input dictionary. Basically this algorithm creates a big matrix which lists for every keyword how well it fits with the individual vacancies."},{"metadata":{"trusted":true},"cell_type":"code","source":"field=\"duties\"\ndf_field=df[field].to_frame()\n\nresume_keywords = list(pd.read_csv(keywords_file, header=None)[0].values)\nvocab_keywords = dict(zip(resume_keywords, np.arange(len(resume_keywords))))\n\n#tf-idf: count word frequencies\ntfidf = TfidfVectorizer(vocabulary=vocab_keywords,ngram_range=[1,4])\n\n# Apply fit_transform to document: csr_mat\ncsr_mat = tfidf.fit_transform(df_field[field])\nwords = tfidf.get_feature_names() #These are the words which the TfidfVectorizer detected, in our case this is simply the vocabulary we provided.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's find the top 5 related jobs for an example vacancy. As an example we list below:\n* Job title\n* Job duties\n* Salary range\n\nThe salary range is a major criterion to find opportunities for promotions. However, we decided not to implement salary as a fixed criterion into the algorithm, since employees may also be interested in horizontal promotion/job rotation rather than moving upwards in the existing hierarchical path. Simply listing the salary range also provides a first indication."},{"metadata":{"trusted":true},"cell_type":"code","source":"job_nr=1 #An example job description","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NMF (Non-negative Matrix Factorization) can be used to find the related vacancies. However matrix multiplication also does the job and runs a bit faster."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Alternative but faster approach similar to NMF\nQQ=csr_mat.dot(csr_mat.transpose()).todense()\nrecommendations_job_nr=np.flip(np.argsort(np.asarray(QQ)[job_nr,:]))\n\nprint(\"Original job description: \" + df.loc[job_nr,\"Position\"] + \" ($\" + df.loc[job_nr,\"salary_start\"] + \" - \" + df.loc[job_nr,\"salary_end\"] + \")\")\nprint(df.loc[job_nr,field])\nprint(\"\")\n\nfor iRecommendation in range(5):\n    print(\"Recommendation #\" + str(iRecommendation+1) + \": \" + df.loc[recommendations_job_nr[iRecommendation+1],\"Position\"] + \" ($\" + df.loc[recommendations_job_nr[iRecommendation+1],\"salary_start\"] + \" - \" + df.loc[recommendations_job_nr[iRecommendation+1],\"salary_end\"] + \")\")  #\"+1\" removes the original job description itself\n    print(df.loc[recommendations_job_nr[iRecommendation+1],field])\n    print(\"\")\n\n\ndf.assign(Recommendation_1=\"\").assign(Recommendation_2=\"\").assign(Recommendation_3=\"\").assign(Recommendation_4=\"\").assign(Recommendation_5=\"\")\nfor job_nr in range(len(df)):\n    recommendations_job_nr=np.flip(np.argsort(np.asarray(QQ)[job_nr,:]))\n    for iRecommendation in range(5):\n        df.loc[job_nr,\"Recommendation_\" + str(iRecommendation+1)]=df.loc[recommendations_job_nr[iRecommendation+1],\"Position\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After inspecting the related vacancies, it is clear that these are more or less related. However, the results are obviously also not ideal:\n* Several vacancies have little or no similar vacancies, making it impossible to find the \"perfect\" promotion opportunity ;\n* The keywords file contains the 500 most often used keywords in the Jobscan.co database. The list can be expanded with more keywords specifically related to the City of Los Angeles. This one-time task can significantly improve the algorithm further.\n\nHowever, we do not necessarily need \"perfect\" matches. Contrariwise, the algorithm often provides related vacancies but in a different field, which are especially the type of vacancies of which the employee is not aware of, although some of them may be suitable positions as well!\n"},{"metadata":{},"cell_type":"markdown","source":"But how useful is the list of keywords? We can have a look at the number of occurrences of each keyword in distinct vacancies."},{"metadata":{"trusted":true},"cell_type":"code","source":"occurrences_per_keyword = np.squeeze(np.asarray(sum(csr_mat.todense()!=0)))\ndata = [go.Histogram(x=occurrences_per_keyword, xbins=dict(start=0,end=140,size= 1),opacity=0.75)]\nlayout = go.Layout(title='Number of occurrences per keyword',\n    xaxis=dict(title='Number of occurrences in different vacancies'),yaxis=dict(title='Number of keywords'),bargap=0.05)\npy.iplot(go.Figure(data=data, layout=layout))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A lot of keywords do not occur in the vacancies at all, but quite a few occur in more than 20 distinct vacancies, with one keyword reaching even 132!\n\nLet's plot the occurrence relative to the position of the keyword in the Jobscan.co ranking to investigate which keywords score high."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = go.Scatter(x = df.index.values,y = occurrences_per_keyword, mode = 'markers', text=resume_keywords, opacity=0.75)\nlayout = go.Layout(title='Jobscan.co keyword ranking vs. City of LA occurrences',\n    xaxis=dict(title='Jobscan.co keyword ranking'),yaxis=dict(title='Number of occurrences in City of LA vacancies'))\npy.iplot(go.Figure(data=[data], layout=layout))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"It is clear that keywords from the top 100 in the ranking generally also occur more frequently in the vacancies of the City of Los Angeles compared to keywords with a lower ranking. It looks like the ranking list is indeed a decent representation of the most frequently occurring keywords in vacancies in general."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Similarly we can plot a histogram of the number of keywords per vacancy, to detect vacancies which are vague."},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords_per_vacancy = np.squeeze(np.asarray(sum(csr_mat.todense().transpose()!=0)))\ndata = [go.Histogram(x=keywords_per_vacancy, xbins=dict(start=0,end=99,size= 1),opacity=0.75)]\nlayout = go.Layout(title='Number of keywords per vacancy',\n    xaxis=dict(title='Number of keywords'),yaxis=dict(title='Number of vacancies'),bargap=0.05)\npy.iplot(go.Figure(data=data, layout=layout))\nkeywords_per_vacancy==0\ndf.loc[keywords_per_vacancy==0,'Position'].head(5)\ndf['Matching_keywords']=keywords_per_vacancy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that more than 100 vacancies have no matching keywords at all. Does this mean that the job duties description is too vague?\nTo investigate this, we have listed 5 vacancies which have no matching keywords. After verifying the descriptions, they do seem to be quite accurate. This implicates that the keyword list should be extended with keywords occurring in these vacancies. The created list of vacancies with no matching keywords is therefore a good starting point to extend the keywords list further manually."},{"metadata":{},"cell_type":"markdown","source":"How well did the clustering algorithm of the vacancies itself perform? Let's investigate using a hierarchical cluster plot. \nThe plot aims to group vacancies with similar keywords together by rearranging the sequence of the vacancies and the keywords."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cm=pd.DataFrame(csr_mat.todense())\ndf_cm.index.name = 'Vacancies'\ncm=sns.clustermap(df_cm,figsize=(15, 15))\ncm.fig.suptitle('Clustered vacancies based on keywords') \ncm.ax_heatmap.set_xlabel('Keywords')\ncm.ax_heatmap.set_ylabel('Vacancies')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot above looks quite messy due to the large amount of keywords and vacancies. However it shows some clear insights:\n* The black border on the right indicates that a lot of keywords do not occur in the vacancies at all, as we saw before ;\n* The black border at the bottom indicates that several vacancies have no matches at all with any of the keywords, so there is room for improvement adding more job-specific keywords ;\n* The colored vertical stripes indicate similar vacancies based on similar keywords. Quite a few clear clusters are visible!\n\nWe finish this part of the analysis with a list of the first 20 related vacancies. It's not perfect, but seems to do it's job quite well."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.loc[cm.dendrogram_row.reordered_ind,'Position'].head(20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Part 2: Automated keyword assignment"},{"metadata":{},"cell_type":"markdown","source":"First we define a few manual functions below."},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_words(text, word_list):\n    text = text.lower()\n    for word in word_list:\n        pattern = r\"\\b\" + word.lower() + r\"\\b\"\n        text = re.sub(pattern, \"\", text)\n    return text\n\n# from DataCamp course \"Machine learning with the experts\"\ndef combine_text_columns(data_frame, to_keep):\n    \"\"\" converts all text in each row of data_frame to single vector\"\"\"\n    text_data = data_frame[to_keep]\n\n    # Replace nans with blanks\n    text_data.fillna(\"\", inplace=True)\n\n    # Join all text items in a row that have a space in between\n    return text_data.apply(lambda x: \" \".join(x), axis=1)\n\n# from github pjbull/SparseInteractions.py\nclass SparseInteractions(BaseEstimator, TransformerMixin):\n    def __init__(self, degree=2, feature_name_separator=\"_\"):\n        self.degree = degree\n        self.feature_name_separator = feature_name_separator\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if not sparse.isspmatrix_csc(X):\n            X = sparse.csc_matrix(X)\n\n        if hasattr(X, \"columns\"):\n            self.orig_col_names = X.columns\n        else:\n            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])\n\n        spi = self._create_sparse_interactions(X)\n        return spi\n\n    def get_feature_names(self):\n        return self.feature_names\n\n    def _create_sparse_interactions(self, X):\n        out_mat = []\n        self.feature_names = self.orig_col_names.tolist()\n\n        for sub_degree in range(2, self.degree + 1):\n            for col_ixs in combinations(range(X.shape[1]), sub_degree):\n                # add name for new column\n                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])\n                self.feature_names.append(name)\n\n                # get column multiplications value\n                out = X[:, col_ixs[0]]\n                for j in col_ixs[1:]:\n                    out = out.multiply(X[:, j])\n\n                out_mat.append(out)\n\n        return sparse.hstack([X] + out_mat)\n\n# from github drivendataorg/box-plots-sklearn\ndef multilabel_sample(y, size=1000, min_count=5, seed=None):\n    \"\"\" Takes a matrix of binary labels `y` and returns\n        the indices for a sample of size `size` if\n        `size` > 1 or `size` * len(y) if size =< 1.\n        The sample is guaranteed to have > `min_count` of\n        each label.\n    \"\"\"\n    try:\n        if (np.unique(y).astype(int) != np.array([0, 1])).any():\n            raise ValueError()\n    except (TypeError, ValueError):\n        raise ValueError('multilabel_sample only works with binary indicator matrices')\n\n    if (y.sum(axis=0) < min_count).any():\n        raise ValueError('Some classes do not have enough examples. Change min_count if necessary.')\n\n    if size <= 1:\n        size = np.floor(y.shape[0] * size)\n\n    if y.shape[1] * min_count > size:\n        msg = \"Size less than number of columns * min_count, returning {} items instead of {}.\"\n        warn(msg.format(y.shape[1] * min_count, size))\n        size = y.shape[1] * min_count\n\n    rng = np.random.RandomState(seed if seed is not None else np.random.randint(1))\n\n    if isinstance(y, pd.DataFrame):\n        choices = y.index\n        y = y.values\n    else:\n        choices = np.arange(y.shape[0])\n\n    sample_idxs = np.array([], dtype=choices.dtype)\n\n    # first, guarantee > min_count of each label\n    for j in range(y.shape[1]):\n        label_choices = choices[y[:, j] == 1]\n        label_idxs_sampled = rng.choice(label_choices, size=min_count, replace=False)\n        sample_idxs = np.concatenate([label_idxs_sampled, sample_idxs])\n\n    sample_idxs = np.unique(sample_idxs)\n\n    # now that we have at least min_count of each, we can just random sample\n    sample_count = int(size - sample_idxs.shape[0])\n\n    # get sample_count indices from remaining choices\n    remaining_choices = np.setdiff1d(choices, sample_idxs)\n    remaining_sampled = rng.choice(remaining_choices,\n                                   size=sample_count,\n                                   replace=False)\n\n    return np.concatenate([sample_idxs, remaining_sampled])\n\ndef multilabel_train_test_split(X, Y, size, min_count=5, seed=None):\n    \"\"\" Takes a features matrix `X` and a label matrix `Y` and\n        returns (X_train, X_test, Y_train, Y_test) where all\n        classes in Y are represented at least `min_count` times.\n    \"\"\"\n    index = Y.index if isinstance(Y, pd.DataFrame) else np.arange(Y.shape[0])\n\n    test_set_idxs = multilabel_sample(Y, size=size, min_count=min_count, seed=seed)\n    train_set_idxs = np.setdiff1d(index, test_set_idxs)\n\n    test_set_mask = index.isin(test_set_idxs)\n    train_set_mask = ~test_set_mask\n\n    return (X[train_set_mask], X[test_set_mask], Y[train_set_mask], Y[test_set_mask])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data preprocessing: Combining text columns, here finally only one text column (duties) was selected. Outcome data (keywords to predict) for the model is created by tokenizing the text data, using the resume keywords as vocabulary."},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n# data\ntext_columns = [\"duties\"]\ntext_data = combine_text_columns(df, to_keep=text_columns)\ntext_data.head()\n\n# Tfidf for y labels - with smaller ngram_range to reduce computation time\nkeywords = resume_keywords\nvec_tf = TfidfVectorizer(vocabulary=keywords, ngram_range=[1,3])\nfreq = vec_tf.fit_transform(text_data)\nfreq_df = pd.DataFrame(freq.toarray(), columns=keywords)\n\n# List most frequent keywords\ntop_keywords = freq_df.sum().sort_values(ascending=False)[0:50]\nnot_occurring = freq_df.sum()[freq_df.sum() == 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A random forest model is trained using the tokenized text as input. Only keywords occurring at least 20 times in the text data were chosen as labels to predict, due to the limited number of vacancies and hence training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# model\n# binary version of most frequent outcome (y) values = keywords\nuse_keywords = list(top_keywords[0:24].index)\nfreq_df_bin = freq_df.loc[:, freq_df.columns.isin(use_keywords)]\nfreq_df_bin[freq_df_bin>0]=1\nfreq_df_bin.shape\nX_train, X_test, y_train, y_test = multilabel_train_test_split(text_data, freq_df_bin, size=0.25, min_count=3, seed=467)\npl = Pipeline([\n        ('vectorizer', TfidfVectorizer(ngram_range=[1,3])),  # no vocab here, want to use all the text\n        ('feature_sel', SelectKBest(chi2, 100)),\n        ('int', SparseInteractions(degree=2)),\n        ('clf', OneVsRestClassifier(RandomForestClassifier(n_estimators=100)))\n    ])\npl.fit(X_train, y_train)\nprint('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model evaluation:\n\nThe following metrics were evaluated:\n\n* proportion of samples with at least 90% correctly predicted labels\n* true positive rate (heatmap value 2, corresponding to dark blue)\n* true negative rate (heatmap value 0, corresponding to grey)\n* false positive rate (heatmap value -1, corresponding to red)\n* false negative rate (heatmap value 1, corresponding to light blue)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicted probabilities for all labels\ny_pred = pl.predict_proba(X_test)\ny_pred_df = pd.DataFrame(y_pred, columns=freq_df_bin.columns, index=y_test.index)\ny_pred_df.head()\ny_pred_df.sum()\n\n# binary version\ncutoff = 0.3\ny_pred_bin = copy.deepcopy(y_pred_df)\ny_pred_bin[y_pred_bin>=cutoff]=1\ny_pred_bin[y_pred_bin<cutoff]=0\n# compare y_test and y_pred_bin\nsign = np.sign(y_test - y_pred_bin.values)\nsign[sign==0] = 1\ny_diff = (y_test + y_pred_bin.values)*sign\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,16))\nsns.heatmap(y_diff, cmap=sns.diverging_palette(20,240, n=75), center=0)\nprint(\"True positive: label predicted and present in test set labels\")\nprint(\"True negative: label not predicted and not present in test set labels\")\nprint(\"False positive: label predicted but not present in test set\")\nprint(\"False negative: label not predicted but present in test set labels\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# quantify amount of true pos/neg (red), false pos (black), false neg (white)\nflat = np.array(y_diff).flatten()\ntrue_pos = np.mean(flat==2)\ntrue_neg = np.mean(flat==0)\nfalse_pos = np.mean(flat==-1)\nfalse_neg = np.mean(flat==1)\nsamples_90_correct = np.mean(y_diff.apply(lambda row: np.mean(row==0), axis=1) >= 0.9)\n\nprint(\"Fraction 90% correct: \" + str(samples_90_correct))\nprint(\"True positives: \" + str(true_pos))\nprint(\"True negatives: \" + str(true_neg))\nprint(\"False positives: \" + str(false_pos))\nprint(\"False negatives: \" + str(false_neg))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Conclusions"},{"metadata":{},"cell_type":"markdown","source":"In order to tackle item (2) and (3) of the problem statement we designed a recommender system and a keyword-assignment model. \n\n#### Part 1: A keyword-based recommender system\n* Although **the external list of keywords** we used does not fully correspond to all provided vacancies of the City of Los Angeles, our analysis has shown that it is indeed a **good starting point**, where high ranked keywords generally occur more frequently in the vacancies ;\n* For each vacancy we provided a list of similar vacancies based on the discription of the duties. **The model effectively recognises similar vacancies**, although the fit is obviously not perfect due to the limited amount of available vacancies (some are simply not related to any of the other vacancies) and due to the fact that the external list of frequently occurring keywords is not fully representative for the available job vacancies of the City of Los Angeles ;\n* Finally, details of the clustering analysis show **clear clusters of similar vacancies** based on keywords.\n\n#### Part 2: Automated keyword assignment\n* Predicted keywords can **facilitate job searching for candidates**\n* Frequently occurring keywords can be **reasonably predicted** according to the model evaluation metrics\n* Availability of the complete list of vacancies will result in a **larger training set and better model**\n"},{"metadata":{},"cell_type":"markdown","source":"### 6. Recommendations\n\n* During our analysis we observed that the overall language of the vacancies is quite good. We suggest to focus time and means on improving the readability and searchability by adding keywords to each vacancy.\n* The external list of keywords we used should be updated with additional keywords which are related to the job vacancies of the City of Los Angeles. As a good starting point we provided in Part 1 a list with the number of keywords currently matching each vacancy: vacancies with no or a low number of matching keywords can be dealt with first. The model we developed in Part 2 can be used to initialize the first few keywords to reduce the manual work.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('Keyword_approach_output.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}