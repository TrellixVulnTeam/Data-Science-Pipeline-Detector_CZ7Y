{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2022-03-13T07:28:51.109008Z","iopub.execute_input":"2022-03-13T07:28:51.109577Z","iopub.status.idle":"2022-03-13T07:29:03.330881Z","shell.execute_reply.started":"2022-03-13T07:28:51.109472Z","shell.execute_reply":"2022-03-13T07:29:03.330171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"!pip install pycocotools","metadata":{"execution":{"iopub.status.busy":"2022-03-13T07:29:03.334037Z","iopub.execute_input":"2022-03-13T07:29:03.334234Z","iopub.status.idle":"2022-03-13T07:29:36.485026Z","shell.execute_reply.started":"2022-03-13T07:29:03.334211Z","shell.execute_reply":"2022-03-13T07:29:36.484188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport cv2\nimport pydicom\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nfrom pycocotools.coco import COCO\nimport glob\n\nimport time\nimport tqdm\nimport random\nimport torchvision.transforms as T\nimport math\nimport random\n\nimport torch\nimport torchvision\nfrom torchvision.io import read_image\nfrom torchvision.utils import draw_bounding_boxes\nimport torch.utils.data\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2022-03-13T07:29:36.487668Z","iopub.execute_input":"2022-03-13T07:29:36.48795Z","iopub.status.idle":"2022-03-13T07:29:38.386905Z","shell.execute_reply.started":"2022-03-13T07:29:36.487913Z","shell.execute_reply":"2022-03-13T07:29:38.386164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/vinbigdata-chest-xray-abnormalities-detection/train.csv\")\ndf_train","metadata":{"execution":{"iopub.status.busy":"2022-03-13T07:29:38.389009Z","iopub.execute_input":"2022-03-13T07:29:38.389267Z","iopub.status.idle":"2022-03-13T07:29:38.54589Z","shell.execute_reply.started":"2022-03-13T07:29:38.389226Z","shell.execute_reply":"2022-03-13T07:29:38.545203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.loc[df_train['image_id'] == '08d0eab34ea5bab14e7e56dabafcdb7f']","metadata":{"execution":{"iopub.status.busy":"2022-03-13T07:29:38.547113Z","iopub.execute_input":"2022-03-13T07:29:38.548515Z","iopub.status.idle":"2022-03-13T07:29:38.57492Z","shell.execute_reply.started":"2022-03-13T07:29:38.548476Z","shell.execute_reply":"2022-03-13T07:29:38.574217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport json\n\nsave_json_path = 'traincoco.json'\n\nimages = []\ncategories = []\nannotations = []\n\n# category = {}\n# category['supercategory'] = 'None'\n# category['id'] = 0\n# category['name'] = 'None'\n# categories.append(category)\n\n# df_train.fillna(0, inplace=True)\ndf_train = df_train[df_train['class_id'] != 14].reset_index(drop=True)\n\ndf_train['file_id'] = df_train['image_id'].astype('category').cat.codes\ndf_train['category_id'] = pd.Categorical(df_train['class_name'], ordered= True).codes + 1\ndf_train['annid'] = df_train.index\ndf_train\n","metadata":{"execution":{"iopub.status.busy":"2022-03-13T07:29:38.576111Z","iopub.execute_input":"2022-03-13T07:29:38.576347Z","iopub.status.idle":"2022-03-13T07:29:38.620911Z","shell.execute_reply.started":"2022-03-13T07:29:38.576318Z","shell.execute_reply":"2022-03-13T07:29:38.620114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IMG_SIZE = 512\n# df_train['x_min'] = (df_train['x_min']/df_train['width'])*IMG_SIZE\n# df_train['y_min'] = (df_train['y_min']/df_train['height'])*IMG_SIZE\n# df_train['x_max'] = (df_train['x_max']/df_train['width'])*IMG_SIZE\n# df_train['y_max'] = (df_train['y_max']/df_train['height'])*IMG_SIZE","metadata":{"execution":{"iopub.status.busy":"2022-03-13T07:29:38.622344Z","iopub.execute_input":"2022-03-13T07:29:38.622612Z","iopub.status.idle":"2022-03-13T07:29:38.626317Z","shell.execute_reply.started":"2022-03-13T07:29:38.622578Z","shell.execute_reply":"2022-03-13T07:29:38.625331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_train.class_name.unique())","metadata":{"execution":{"iopub.status.busy":"2022-03-13T07:29:38.627713Z","iopub.execute_input":"2022-03-13T07:29:38.628049Z","iopub.status.idle":"2022-03-13T07:29:38.641729Z","shell.execute_reply.started":"2022-03-13T07:29:38.628012Z","shell.execute_reply":"2022-03-13T07:29:38.640883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Export train.csv to coco json","metadata":{}},{"cell_type":"code","source":"def image(row):\n    image = {}\n    image[\"height\"] = abs(row.y_max - row.y_min)\n    image[\"weight\"] = abs(row.x_max - row.x_min)\n    image[\"id\"] = row.file_id\n    image[\"file_name\"] = row.image_id + '.dicom'\n    return image\n\ndef category(row):\n    category = {}\n    category['supercategory'] = 'None'\n    category['id'] = row.category_id\n    category['name'] = row.class_name\n    return category\n\ndef annotation(row):\n    annotation = {}\n    area = (row.x_max - row.x_min) * (row.y_max - row.y_min) \n\n    annotation['segmentation'] = []\n    annotation['iscrowd'] = 0\n    annotation['area'] = area\n    annotation['image_id'] = row.file_id\n    \n    annotation['bbox'] = [row.x_min, row.y_min, row.x_max - row.x_min, row.y_max - row.y_min]\n   \n    annotation['category_id'] = row.category_id\n    annotation['id'] = row.annid\n    return annotation\n\nfor row in df_train.itertuples():\n    annotations.append(annotation(row))\n    \nimagedf = df_train.drop_duplicates(subset=['file_id']).sort_values(by='file_id')\nfor row in imagedf.itertuples():\n    images.append(image(row))\n    \ncatdf = df_train.drop_duplicates(subset=['category_id']).sort_values(by='category_id')\nfor row in catdf.itertuples():\n    categories.append(category(row))\n    \ndata_coco = {}\ndata_coco['images'] = images\ndata_coco['categories'] = categories\ndata_coco['annotations'] = annotations\njson.dump(data_coco, open(save_json_path, 'w'), indent=4)\n\nprint(\"Create COCO json finished!\")","metadata":{"execution":{"iopub.status.busy":"2022-03-13T07:29:38.64306Z","iopub.execute_input":"2022-03-13T07:29:38.644798Z","iopub.status.idle":"2022-03-13T07:29:40.55977Z","shell.execute_reply.started":"2022-03-13T07:29:38.644764Z","shell.execute_reply":"2022-03-13T07:29:40.558305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create DataLoader custom","metadata":{}},{"cell_type":"code","source":"# class VBDDataset(Dataset):\n#     def __init__(self, df, image_dir, mode='train', transforms=None):\n#         super().__init__()\n        \n#         self.image_ids = df['image_id'].unique()\n#         self.df = df\n#         self.image_dir = image_dir\n#         self.mode = mode\n#         self.transforms = transforms\n    \n#     def __getitem__(self, idx):\n#         image_dir = \"../input/vinbigdata-chest-xray-abnormalities-detection/\" + self.mode\n#         image_id = self.image_ids[idx]\n#         records = self.df[self.df['image_id'] == image_id]\n\n#         image_file = pydicom.dcmread(os.path.join(image_dir, f'{image_id}.dicom'))\n#         image = np.array(image_file.pixel_array, dtype=np.float32)[np.newaxis]  # Add channel dimension\n#         image = torch.from_numpy(image)\n# #         image = cv2.imread(f'{self.image_dir}/{image_id}.png', cv2.IMREAD_COLOR)\n# #         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n# #         image /= 255.0\n\n#         boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n        \n#         area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n#         area = torch.as_tensor(area, dtype=torch.float32)\n#         # all the labels are shifted by 1 to accomodate background\n#         labels = torch.squeeze(torch.as_tensor((records.class_id.values+1,), dtype=torch.int64))\n        \n#         # suppose all instances are not crowd\n#         iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n        \n#         target = {}\n#         target['boxes'] = boxes\n#         target['labels'] = labels\n#         # target['masks'] = None\n#         target['image_id'] = torch.tensor([idx])\n#         target['area'] = area\n#         target['iscrowd'] = iscrowd\n#         if self.transforms:\n#             sample = {\n#                 'image': image,\n#                 'bboxes': target['boxes'],\n#                 'labels': labels\n#             }\n#             sample = self.transforms(**sample)\n#             image = sample['image']\n            \n#             target['boxes'] = torch.as_tensor(sample['bboxes'])\n\n#         return image, target, image_id\n\n#     def __len__(self):\n#         return self.image_ids.shape[0]\n        ","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-03-13T07:29:40.562664Z","iopub.execute_input":"2022-03-13T07:29:40.562858Z","iopub.status.idle":"2022-03-13T07:29:40.569057Z","shell.execute_reply.started":"2022-03-13T07:29:40.562834Z","shell.execute_reply":"2022-03-13T07:29:40.568263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dicom2array(path, voi_lut=True, fix_monochrome=True):\n    \n    dicom = pydicom.dcmread(path)\n    \n    # VOI LUT (if available by DICOM device) is used to\n    # transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n        \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    \n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2022-03-13T07:29:40.570518Z","iopub.execute_input":"2022-03-13T07:29:40.57078Z","iopub.status.idle":"2022-03-13T07:29:40.584613Z","shell.execute_reply.started":"2022-03-13T07:29:40.570744Z","shell.execute_reply":"2022-03-13T07:29:40.583819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VBDDataset(Dataset):\n    def __init__(self, root, annotation, transforms=None):\n        self.root = root\n        self.transforms = transforms\n        self.coco = COCO(annotation)\n        self.ids = list(sorted(self.coco.imgs.keys()))\n        \n    def __getitem__(self, index):\n        coco = self.coco\n        img_id = self.ids[index]\n        # List: get annotation id from coco\n        ann_ids = coco.getAnnIds(imgIds=img_id)\n        # Dictionary: target coco_annotation file for an image\n        coco_annotation = coco.loadAnns(ann_ids)\n        # path for input image\n        path = coco.loadImgs(img_id)[0]['file_name']\n        # open the input image\n#         img_arr = dicom2array(os.path.join(self.root, path))\n#         img = Image.fromarray(img_arr)\n        image_file = pydicom.dcmread(os.path.join(self.root, path))\n        img = np.array(image_file.pixel_array, dtype=np.float32)[np.newaxis]  # Add channel dimension\n        img = torch.from_numpy(img)\n#         img = Image.open(os.path.join(self.root, path))\n        \n        # number of objects in the image\n        num_objs = len(coco_annotation)\n        \n        # Bounding boxes for objects\n        # In coco format, bbox = [xmin, ymin, width, height]\n        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n        boxes = []\n        for i in range(num_objs):\n            xmin = coco_annotation[i]['bbox'][0]\n            ymin = coco_annotation[i]['bbox'][1]\n            xmax = xmin + coco_annotation[i]['bbox'][2]\n            ymax = ymin + coco_annotation[i]['bbox'][3]\n            boxes.append([xmin, ymin, xmax, ymax])\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        \n        # Labels\n        labels = torch.squeeze(torch.as_tensor((num_objs,), dtype=torch.int64))\n        \n        # Tensorise img_id\n        img_id = torch.tensor([img_id])\n        \n        # Size of bbox (Rectangular)\n        areas = []\n        for i in range(num_objs):\n            areas.append(coco_annotation[i]['area'])\n        areas = torch.as_tensor(areas, dtype=torch.float32)\n        \n        # Iscrowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n        \n        # Annotation is in dictionary format\n        my_annotation = {}\n        my_annotation['boxes'] = boxes\n        my_annotation['labels'] = labels\n        my_annotation['image_id'] = img_id\n        my_annotation['area'] = areas\n        my_annotation['iscrowd'] = iscrowd\n        \n#         if self.transforms:\n#             sample = {\n#                 'image': img,\n#                 'bboxes': my_annotation['boxes'],\n#                 'labels': labels\n#             }\n#             sample = self.transforms(**sample)\n#             img = sample['image']\n            \n#             my_annotation['boxes'] = torch.as_tensor(sample['bboxes'])\n        \n#         if self.transforms:\n#             img = self.transforms(img)\n        \n        return img, my_annotation\n    \n    def __len__(self):\n        return len(self.ids)\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-03-13T07:52:26.178138Z","iopub.execute_input":"2022-03-13T07:52:26.178446Z","iopub.status.idle":"2022-03-13T07:52:26.194713Z","shell.execute_reply.started":"2022-03-13T07:52:26.178413Z","shell.execute_reply":"2022-03-13T07:52:26.194022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_DIR = \"../input/vinbigdata-chest-xray-abnormalities-detection/train\"\nTEST_DIR = \"../input/vinbigdata-chest-xray-abnormalities-detection/test\"\n\nTRAIN_COCO = \"./traincoco.json\"","metadata":{"execution":{"iopub.status.busy":"2022-03-13T07:29:40.605175Z","iopub.execute_input":"2022-03-13T07:29:40.606096Z","iopub.status.idle":"2022-03-13T07:29:40.615156Z","shell.execute_reply.started":"2022-03-13T07:29:40.606057Z","shell.execute_reply":"2022-03-13T07:29:40.614313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2","metadata":{"execution":{"iopub.status.busy":"2022-03-13T07:29:40.616581Z","iopub.execute_input":"2022-03-13T07:29:40.616996Z","iopub.status.idle":"2022-03-13T07:29:42.218457Z","shell.execute_reply.started":"2022-03-13T07:29:40.61696Z","shell.execute_reply":"2022-03-13T07:29:42.217721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_transform():\n    return A.Compose([\n#         A.Flip(0.5),\n#         A.ShiftScaleRotate(rotate_limit=10, p=0.5),\n#         A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=1.0),\n#         A.Resize(height=512, width=512, always_apply=True),\n#         ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\n# def get_transform():\n#     custom_transforms = []\n#     custom_transforms.append(torchvision.transforms.ToPILImage())\n#     custom_transforms.append(torchvision.transforms.ToTensor())\n#     return torchvision.transforms.Compose(custom_transforms)\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_dataset = VBDDataset(root=TRAIN_DIR, annotation=TRAIN_COCO)\n\nbatch_size = 16\n\ndata_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=2,\n    collate_fn=collate_fn\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T07:52:33.187912Z","iopub.execute_input":"2022-03-13T07:52:33.188195Z","iopub.status.idle":"2022-03-13T07:52:33.675792Z","shell.execute_reply.started":"2022-03-13T07:52:33.188164Z","shell.execute_reply":"2022-03-13T07:52:33.675039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"from pydicom.pixel_data_handlers.util import apply_voi_lut\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-03-13T07:29:42.566716Z","iopub.execute_input":"2022-03-13T07:29:42.567121Z","iopub.status.idle":"2022-03-13T07:29:42.57152Z","shell.execute_reply.started":"2022-03-13T07:29:42.567081Z","shell.execute_reply":"2022-03-13T07:29:42.570668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_bb_info(df, img_id):\n    bounding_boxes_info = df.loc[df[\"image_id\"]==img_id, ['x_min', 'y_min', 'x_max', 'y_max', \"class_id\"]]\n\n    bboxes = []\n    for _, row in bounding_boxes_info.astype(np.int16).iterrows():\n        bboxes.append(list(row))\n    \n    return bboxes\n\nlabel2color = { 0:(\"Aortic enlargement\",\"#2a52be\"),\n                1:(\"Atelectasis\",\"#ffa812\"),\n                2:(\"Calcification\",\"#ff8243\"),\n                3:(\"Cardiomegaly\",\"#4682b4\"),\n                4:(\"Consolidation\",\"#ddadaf\"),\n                5:(\"ILD\",\"#a3c1ad\"),\n                6:(\"Infiltration\",\"#008000\"),\n                7:(\"Lung Opacity\",\"#004953\"),\n                8:(\"Nodule/Mass\",\"#e3a857\"),\n                9:(\"Other lesion\",\"#dda0dd\"),\n               10:(\"Pleural effusion\",\"#e6e8fa\"),\n               11:(\"Pleural thickening\",\"#800020\"),\n               12:(\"Pneumothorax\",\"#918151\"),\n               13:(\"Pulmonary fibrosis\",\"#e75480\")}\n\ndef bounding_box_plotter(img_as_arr, img_id, bounding_boxes_info):\n    fig = plt.figure(figsize=(7,7))\n    ax = fig.add_axes([0,0,1,1])\n    \n    # plot the image\n    plt.imshow(img_as_arr, cmap=\"gray\")\n    plt.title(img_id)\n\n    # add the bounding boxes\n    for row in bounding_boxes_info:\n        # each row contains 'x_min', 'y_min', 'x_max', 'y_max', \"class_id\"\n        xmin = row[0]\n        xmax = row[2]\n        ymin = row[1]\n        ymax = row[3]\n\n        width = xmax - xmin\n        height = ymax - ymin\n        \n        edgecolor = label2color[row[4]][1]\n        ax.annotate(label2color[row[4]][0], xy=(xmax - 40, ymin + 20))\n\n        # add bounding boxes to the image\n        rect = patches.Rectangle((xmin, ymin), width, height, edgecolor=edgecolor, facecolor='none')\n\n        ax.add_patch(rect)\n\n    plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-13T07:29:42.57292Z","iopub.execute_input":"2022-03-13T07:29:42.573327Z","iopub.status.idle":"2022-03-13T07:29:42.589113Z","shell.execute_reply.started":"2022-03-13T07:29:42.573291Z","shell.execute_reply":"2022-03-13T07:29:42.588194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images = glob.glob(TRAIN_DIR+\"/**.dicom\")\ntest_images = glob.glob(TEST_DIR+\"/**.dicom\")\n\nimg_ids = df_train['image_id'].unique()\nshortlisted_img_ids = img_ids[:10]\nog_imgs = [dicom2array(f'{TRAIN_DIR}/{path}.dicom') for path in shortlisted_img_ids]\n\n\nfor img_as_arr, img_id in zip(og_imgs,shortlisted_img_ids):    \n    bounding_boxes_info = get_bb_info(df_train, img_id)\n    bounding_box_plotter(img_as_arr, img_id, bounding_boxes_info)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T07:29:42.5906Z","iopub.execute_input":"2022-03-13T07:29:42.590876Z","iopub.status.idle":"2022-03-13T07:30:05.672141Z","shell.execute_reply.started":"2022-03-13T07:29:42.59084Z","shell.execute_reply":"2022-03-13T07:30:05.671239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nSEED = 1234\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-03-13T07:30:05.673531Z","iopub.execute_input":"2022-03-13T07:30:05.674029Z","iopub.status.idle":"2022-03-13T07:30:05.728406Z","shell.execute_reply.started":"2022-03-13T07:30:05.673985Z","shell.execute_reply":"2022-03-13T07:30:05.727295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # DataLoader is iterable over Dataset\n# for imgs, annotations in data_loader:\n#     imgs = list(img.to(device) for img in imgs)\n#     annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n#     print(annotations)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-13T07:46:36.483734Z","iopub.execute_input":"2022-03-13T07:46:36.484008Z","iopub.status.idle":"2022-03-13T07:50:44.26721Z","shell.execute_reply.started":"2022-03-13T07:46:36.48398Z","shell.execute_reply":"2022-03-13T07:50:44.264135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\ndef get_model_instance_segmentation(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=False)\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes + 1)\n    return model\n\nprint('Done')","metadata":{"execution":{"iopub.status.busy":"2022-03-13T07:54:40.213098Z","iopub.execute_input":"2022-03-13T07:54:40.213436Z","iopub.status.idle":"2022-03-13T07:54:40.221034Z","shell.execute_reply.started":"2022-03-13T07:54:40.213396Z","shell.execute_reply":"2022-03-13T07:54:40.220039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def train(train_data_loader, model, optimizer, idx):\n#     epoch_loss = []\n#     start_time = time.time()\n    \n#     model.train()\n    \n#     for imgs, annotations in data_loader:\n#         start_loop = time.time()\n#         imgs = list(img.to(device) for img in imgs)\n#         annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n        \n#         # Reseting Gradients\n#         optimizer.zero_grad()\n        \n#         # Calculating Loss\n#         loss_dict = model(imgs, annotations)\n#         losses = sum(loss for loss in loss_dict.values())\n#         epoch_loss.append(loss)\n        \n#         # Backward\n#         losses.backward()\n#         optimizer.step()\n#         end_loop = time.time()\n#         total_time_loop = end_loop - start_loop\n        \n#         print(f'Iteration: {idx}/{len_dataloader}, Loss: {losses}, Time: {total_time_loop}')\n#     # Overall Epoch Results\n#     end_time = time.time()\n#     total_time = end_time - start_time\n    \n#     # Loss\n#     epoch_loss = np.mean(epoch_loss)\n    \n#     return epoch_loss, total_time\n\ndef train(model, dataloader, device, epochs, optimizer):    \n    best_loss = 1e4\n    itr = 1\n    all_losses = []\n    model.train()\n    train_loss = 0\n\n    for epoch in range(epochs):\n    \n        for images, targets in dataloader:\n\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            \n            optimizer.zero_grad()\n            loss_dict = model(images, targets)\n\n            losses = sum(loss for loss in loss_dict.values())\n            loss_value = losses.item()\n\n            all_losses.append(loss_value)\n            \n            losses.backward()\n            optimizer.step()\n\n            if itr % 50 == 0:\n                print(f\"Iteration #{itr} loss: {loss_value}\")\n\n            itr += 1\n\n\n        print(f\"Epoch #{epoch} loss: {all_loss[itr - 1]}\\n\")\n        \n    return all_losses\n\nprint('Done')","metadata":{"execution":{"iopub.status.busy":"2022-03-13T08:05:46.239756Z","iopub.execute_input":"2022-03-13T08:05:46.240462Z","iopub.status.idle":"2022-03-13T08:05:46.251168Z","shell.execute_reply.started":"2022-03-13T08:05:46.240421Z","shell.execute_reply":"2022-03-13T08:05:46.250434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = len(df_train['class_name'].unique())\nnum_epochs = 10\n\nmodel = get_model_instance_segmentation(num_classes)\n\n# move model to the right device\nmodel.to(device)\n    \n# parameters\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n\n# len_dataloader = len(data_loader)\nidx = 0\n\n# for epoch in range(num_epochs):\n#     train(train_data_loader = data_loader, model = model, optimizer = optimizer, idx = idx)\n#     idx += 1\n\nlen_dataloader = len(data_loader)\n\ntrain(model, data_loader, device, num_epochs, optimizer)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T08:05:51.207631Z","iopub.execute_input":"2022-03-13T08:05:51.208342Z","iopub.status.idle":"2022-03-13T08:06:24.410349Z","shell.execute_reply.started":"2022-03-13T08:05:51.208305Z","shell.execute_reply":"2022-03-13T08:06:24.407422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}