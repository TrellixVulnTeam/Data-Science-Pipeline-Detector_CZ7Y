{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Version\n* `v13`: Fold4\n* `v12`: Fold3\n* `v10`: Fold2\n* `v09`: Fold1\n* `v03`: Fold0"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2021-01-01T09:44:43.843489Z","iopub.status.busy":"2021-01-01T09:44:43.842712Z","iopub.status.idle":"2021-01-01T09:44:53.448523Z","shell.execute_reply":"2021-01-01T09:44:53.447971Z"},"papermill":{"duration":9.633907,"end_time":"2021-01-01T09:44:53.448657","exception":false,"start_time":"2021-01-01T09:44:43.81475","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"!pip install --upgrade seaborn","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-01T09:44:53.50848Z","iopub.status.busy":"2021-01-01T09:44:53.50769Z","iopub.status.idle":"2021-01-01T09:44:54.403472Z","shell.execute_reply":"2021-01-01T09:44:54.402433Z"},"papermill":{"duration":0.926929,"end_time":"2021-01-01T09:44:54.403588","exception":false,"start_time":"2021-01-01T09:44:53.476659","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd\nfrom glob import glob\nimport shutil, os\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm.notebook import tqdm\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dim = 1024 #512, 256, 'original'\nfold = 4","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-01T09:44:54.468231Z","iopub.status.busy":"2021-01-01T09:44:54.467706Z","iopub.status.idle":"2021-01-01T09:44:54.690894Z","shell.execute_reply":"2021-01-01T09:44:54.69183Z"},"papermill":{"duration":0.262045,"end_time":"2021-01-01T09:44:54.691965","exception":false,"start_time":"2021-01-01T09:44:54.42992","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(f'../input/vinbigdata-{dim}-image-dataset/vinbigdata/train.csv')\n# train_df = pd.read_csv(f'../input/vinbigdata-512-image-dataset/vinbigdata/train.csv')\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-01T09:44:54.7506Z","iopub.status.busy":"2021-01-01T09:44:54.749926Z","iopub.status.idle":"2021-01-01T09:44:54.80575Z","shell.execute_reply":"2021-01-01T09:44:54.804838Z"},"papermill":{"duration":0.086788,"end_time":"2021-01-01T09:44:54.805857","exception":false,"start_time":"2021-01-01T09:44:54.719069","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_df['image_path'] = f'/kaggle/input/vinbigdata-{dim}-image-dataset/vinbigdata/train/'+train_df.image_id+('.png' if dim!='original' else '.jpg')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.iloc[2423][\"image_path\"]","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.027478,"end_time":"2021-01-01T09:44:54.861374","exception":false,"start_time":"2021-01-01T09:44:54.833896","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Only 14 Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"without_mass=train_df[train_df.class_id!=8]\n\nwithout_mass_files   = []\nwithout_mass_files += list(without_mass.image_path.unique())\n\nmass_files=train_df[train_df.class_id==8]['image_path'].unique()\nmass_df=train_df[train_df.class_id==8]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"without_mass_files[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n \nprint(random.randint(0,9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(random.uniform(1,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n \nprint(random.randint(0,9))\n\nrandom.uniform(1.1,5.4)\n\nrandom.random()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.makedirs('/kaggle/working/vinbigdata/labels/train', exist_ok = True)\nos.makedirs('/kaggle/working/vinbigdata/labels/val', exist_ok = True)\nos.makedirs('/kaggle/working/vinbigdata/images/train', exist_ok = True)\nos.makedirs('/kaggle/working/vinbigdata/images/val', exist_ok = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalized_text_create(desktop_path,name, boxes,box_labels):\n    #x_min.y_min,x_max,y_max--->x,y,w,h,normalized\n        \n    full_path = os.path.join(desktop_path, name+'.txt')  # 也可以创建一个.doc的word文档\n    print('full_path',full_path)\n    file = open(full_path, 'w')\n    \n    if(len(boxes)==0):\n        file.close()\n    else:\n        for i in range(len(boxes)):\n    #         file.write(str(box_labels[i])+ ' '+ str(x)+ ' '+ str(y)+ ' '+ str(w)+ ' '+ str(h)+ ' '+ '\\n') \n            file.write(str(box_labels[i])+ ' '+ str(boxes[i][0])+ ' '+ str(boxes[i][1])+ ' '+ str(boxes[i][2])+ ' '+ str(boxes[i][3])+ ' '+ '\\n')\n            print('录入'+str(box_labels[i])+ ' '+ str(boxes[i][0])+ ' '+ str(boxes[i][1])+ ' '+ str(boxes[i][2])+ ' '+ str(boxes[i][3])+ ' ')\n        file.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport random\nimport matplotlib.pyplot as plt\nimport math\n# from PIL import Image\n\nimport PIL.Image as img\ncount=2000\nfor file in tqdm(without_mass_files):\n    count-=1\n    if count==0:\n        break\n#     IMG = r'F:\\PY\\github\\copy-paste-aug-main\\u=3785402047,1898752523&fm=193&f=GIF.jpg'  # 图片地址\n    IMG = file\n    im = img.open(IMG)  # 用PIL打开一个图片\n    \n    plt.imshow(im)\n    plt.axis('on') # 关掉坐标轴为 off\n    plt.title('原图'+file) # 图像题目\n    plt.show()\n    \n    # 随机补丁的个数\n    num_patch=random.randint(1,30)\n    \n    patch_group=[]\n    \n    for i in range(num_patch):\n        col_patch=random.randint(0,mass_files.shape[0])\n        \n        x_min=mass_df.iloc[col_patch][\"x_min\"]*(dim/(1.0*mass_df.iloc[col_patch][\"width\"]))\n        x_max=mass_df.iloc[col_patch][\"x_max\"]*(dim/(1.0*mass_df.iloc[col_patch][\"width\"]))\n        y_min=mass_df.iloc[col_patch][\"y_min\"]*(dim/(1.0*mass_df.iloc[col_patch][\"height\"]))\n        y_max=mass_df.iloc[col_patch][\"y_max\"]*(dim/(1.0*mass_df.iloc[col_patch][\"height\"]))\n        box = (x_min, y_min, x_max, y_max)\n        print('box',box)\n\n        #box代表需要剪切图片的位置格式为:xmin ymin xmax ymax\n\n        mass_img=img.open(mass_df.iloc[col_patch][\"image_path\"])\n        print('mass_img.size',mass_img.size)\n        patch=mass_img.crop(box)\n        print('cut_patch.size',patch.size)\n        oper_rotate=random.uniform(1,6)\n        \n        if oper_rotate<2:\n            patch=patch.transpose(img.FLIP_LEFT_RIGHT)\n        elif oper_rotate<3:\n            patch=patch.transpose(img.FLIP_TOP_BOTTOM)\n        elif oper_rotate<4:\n            patch=patch.transpose(img.ROTATE_90)\n        elif oper_rotate<5:\n            patch=patch.transpose(img.ROTATE_180)\n        elif oper_rotate<6:\n            patch=patch.transpose(img.ROTATE_270)\n        \n        \n        oper_extend=random.uniform(0.8,1.2)\n#         print('patch.size',patch.size)\n#         print('int(patch.size[0]*oper_extend)',int(patch.size[0]*oper_extend))\n#         print('int(patch.size[1]*oper_extend)',int(patch.size[1]*oper_extend))\n        patch=patch.resize((int(patch.size[0]*oper_extend), int(patch.size[1]*oper_extend)))\n        \n        print('调整前补丁大小是:'+str(patch.size))\n#         patch=patch.resize((int(patch.size[0]*(dim/(1.0*mass_df.iloc[col_patch][\"width\"]))), int(patch.size[1]*(dim/(1.0*mass_df.iloc[col_patch][\"height\"]))) ) )\n#         patch=patch.resize((int(patch.size[0]*(dim/(1.0*im.size[0]))), int(patch.size[1]*(dim/im.size[1])) ) )\n\n        patch_group.append(patch)\n        print(\"\")\n        \n    print('patch块数：'+str(len(patch_group)))\n    \n    filename = file.split('/')[-1].split('.')[0]\n    boxes=[]\n    box_labels=[]\n    for i in range(len(patch_group)):\n        \n        \n#         print('patch_group[i]',patch_group[i].size[0])\n#         print('im.size[0]-patch_group[i].size[0]',im.size[0]-patch_group[i].size[0])\n#         print('patch_group[i]',patch_group[i].size[1])\n#         print('im.size[0]-patch_group[i].size[0]',im.size[1]-patch_group[i].size[1])\n        width_min=min(patch_group[i].size[0],im.size[0]-patch_group[i].size[0])\n        width_max=max(patch_group[i].size[0],im.size[0]-patch_group[i].size[0])\n        width_range=random.randint(width_min,width_max)\n        \n        height_min=min(patch_group[i].size[1],im.size[1]-patch_group[i].size[1])\n        height_max=max(patch_group[i].size[1],im.size[1]-patch_group[i].size[1])\n        height_range=random.randint(height_min,height_max)\n        \n#         print('width_range',width_range)\n#         print('height_range',height_range)\n        im_add=im.copy()\n        im.paste(patch_group[i], (width_range,height_range)) \n        \n        box_labels.append(0)\n        \n        boxes.append([width_range/(1.0*dim),height_range/(1.0*dim),(patch_group[i].size[0])/(1.0*dim),(patch_group[i].size[1])/(1.0*dim)])\n        normalized_text_create('/kaggle/working/vinbigdata/labels/train','patched'+filename,boxes,box_labels)\n        im=img.blend(im, im_add, 0.2)#im*(1-a)+im_add*a\n    \n    \n    \n    print(\"图结束\")\n    print('补丁图'+file)\n    plt.title('补丁图'+file) # 图像题目\n    plt.axis('on') # 关掉坐标轴为 off\n    plt.imshow(im)\n    plt.show()\n    \n    im.save('/kaggle/working/vinbigdata/images/train/patched'+filename+'.png')\n    \n    \n    print(\"\")\n    print(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import os\n# import time\n# import random\n# import pydicom \n# import cv2\n# import numpy as np\n# import pandas as pd\n# from glob import glob\n# import matplotlib.pyplot as plt\n# from random import randint\n# from pydicom.pixel_data_handlers.util import apply_voi_lut","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_dir = \"../input/vinbigdata-chest-xray-abnormalities-detection/train\"\n# test_dir = \"../input/vinbigdata-chest-xray-abnormalities-detection/test\"\n\n# train_files = os.listdir(train_dir)\n# test_files = os.listdir(test_dir)\n\n# train_df = pd.read_csv(\"../input/vinbigdata-chest-xray-abnormalities-detection/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def get_bbox_area(row):\n#     return (row['x_max']-row['x_min'])*(row['y_max']-row['y_min'])\n\n\n# new_df = train_df[train_df['class_name'] != 'No finding']\n# new_df['bbox_area'] = new_df.apply(get_bbox_area, axis=1)\n# new_df.head()\n# new_df=new_df[train_df['class_name'] == 'Nodule/Mass']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def dicom_to_np(path, voi_lut = True, fix_monochrome = True):\n#     dicom = pydicom.read_file(path)\n    \n#     # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n#     if voi_lut:\n#         data = apply_voi_lut(dicom.pixel_array, dicom)\n#     else:\n#         data = dicom.pixel_array\n               \n#     # depending on this value, X-ray may look inverted - fix that:\n#     if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n#         data = np.amax(data) - data\n        \n#     data = data - np.min(data)\n#     data = data / np.max(data)\n#     data = (data * 255).astype(np.uint8)\n        \n#     return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imgs = []\n# ids = []\n# img_ids = new_df['image_id'].values\n# class_ids = new_df['class_id'].unique()\n\n# label_to_color = {class_id:[randint(0,255) for i in range(3)] for class_id in class_ids}\n# thickness = 3\n# scale = 2\n\n\n# img_id = '2ad18a594cbaf3c6d6145a7775829554'\n# img_path = f'{train_dir}/{img_id}.dicom'\n# img = dicom_to_np(path=img_path)\n# img = cv2.resize(img, None, fx=1/scale, fy=1/scale)\n# img = np.stack([img, img, img], axis=-1)\n\n# boxes = new_df.loc[new_df['image_id'] == img_id, ['x_min', 'y_min', 'x_max', 'y_max']].values/scale\n# labels = new_df.loc[new_df['image_id'] == img_id, ['class_id']].values.squeeze()\n\n# for label_id, box in zip(labels, boxes):\n#     color = label_to_color[label_id]\n#     img = cv2.rectangle(\n#         img,\n#         (int(box[0]), int(box[1])),\n#         (int(box[2]), int(box[3])),\n#         color, thickness\n# )\n# img = cv2.resize(img, (500,500))\n# imgs.append(img)\n# ids.append(label_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show_bbox(imgs, ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def show_bbox(img_ids, img_classes):\n#     plt.figure(figsize=(16, 12))\n    \n#     for i, (img, img_class) in enumerate(zip(img_ids, img_classes)):\n#         plt.subplot(2, 3, i + 1)\n#         img = cv2.resize(img, (500,500))\n#         plt.imshow(img, cmap='gray')\n#         plt.title(f\"Class: {img_class}\", fontsize=15)\n#         plt.axis(\"off\")\n    \n#     plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-01T09:44:54.922292Z","iopub.status.busy":"2021-01-01T09:44:54.921364Z","iopub.status.idle":"2021-01-01T09:44:54.943995Z","shell.execute_reply":"2021-01-01T09:44:54.943575Z"},"papermill":{"duration":0.05543,"end_time":"2021-01-01T09:44:54.944088","exception":false,"start_time":"2021-01-01T09:44:54.888658","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df[train_df.class_id==8].reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.052492,"end_time":"2021-01-01T09:47:56.110766","exception":false,"start_time":"2021-01-01T09:47:56.058274","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ensemble_boxes","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-01T09:47:56.234311Z","iopub.status.busy":"2021-01-01T09:47:56.233425Z","iopub.status.idle":"2021-01-01T09:47:56.297206Z","shell.execute_reply":"2021-01-01T09:47:56.297652Z"},"papermill":{"duration":0.134603,"end_time":"2021-01-01T09:47:56.297774","exception":false,"start_time":"2021-01-01T09:47:56.163171","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"gkf  = GroupKFold(n_splits = 5)\ntrain_df['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(train_df, groups = train_df.image_id.tolist())):\n    train_df.loc[val_idx, 'fold'] = fold\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-01T09:47:56.420454Z","iopub.status.busy":"2021-01-01T09:47:56.419355Z","iopub.status.idle":"2021-01-01T09:47:56.443157Z","shell.execute_reply":"2021-01-01T09:47:56.443661Z"},"papermill":{"duration":0.086817,"end_time":"2021-01-01T09:47:56.443789","exception":false,"start_time":"2021-01-01T09:47:56.356972","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_files = []\nval_files   = []\nval_files += list(train_df[train_df.fold==fold].image_path.unique())\ntrain_files += list(train_df[train_df.fold!=fold].image_path.unique())\nlen(train_files), len(val_files)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# os.makedirs('/kaggle/working/vinbigdata/labels/train', exist_ok = True)\n# os.makedirs('/kaggle/working/vinbigdata/labels/val', exist_ok = True)\n# os.makedirs('/kaggle/working/vinbigdata/images/train', exist_ok = True)\n# os.makedirs('/kaggle/working/vinbigdata/images/val', exist_ok = True)\n# label_dir = '/kaggle/input/vinbigdata-yolo-labels-dataset/labels'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os.path as osp\nfrom path import Path\nfrom collections import Counter\nimport cv2\nfrom ensemble_boxes import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imagepaths = train_df['image_path'].unique()\ntrain_annotations=train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_annotations['area']=((train_annotations['x_max']-train_annotations['x_min'])/train_annotations['width'])*((train_annotations['y_max']-train_annotations['y_min'])/train_annotations['height'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# img_array  = cv2.imread('/kaggle/input/vinbigdata-512-image-dataset/vinbigdata/train/d3637a1935a905b3c326af31389cb846.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def Create_nms_box_txt(desktop_path,name):\n#     iou_thr = 0.5\n#     skip_box_thr = 0.0001\n#     viz_images = []\n# #     image_basename = Path(path).stem\n#     image_basename = name\n# #     print(f\"(\\'{image_basename}\\', \\'{path}\\')\")\n#     img_annotations = train_annotations[train_annotations.image_id==image_basename][train_annotations.class_id==8]\n\n#     boxes_viz = img_annotations[['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().tolist()\n#     labels_viz = img_annotations['class_id'].to_numpy().tolist()\n\n#     print(\"Bboxes before nms:\\n\", boxes_viz)\n#     print(\"Labels before nms:\\n\", labels_viz)\n\n#     boxes_list = []\n#     scores_list = []\n#     labels_list = []\n#     weights = []\n\n#     boxes_single = []\n#     labels_single = []\n\n#     cls_ids = img_annotations['class_id'].unique().tolist()\n#     count_dict = Counter(img_annotations['class_id'].tolist())\n#     print(count_dict)\n\n#     for cid in cls_ids:       \n#         ## Performing Fusing operation only for multiple bboxes with the same label\n#         if count_dict[cid]==1:\n#             labels_single.append(cid)\n#             boxes_single.append(img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().squeeze().tolist())\n\n#         else:\n#             cls_list =img_annotations[img_annotations.class_id==cid]['class_id'].tolist()\n#             labels_list.append(cls_list)\n#             bbox = img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy()\n\n#             ## Normalizing Bbox by Image Width and Height\n#             bbox = bbox/(img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"], img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"])\n\n\n#             bbox = np.clip(bbox, 0, 1)\n#             boxes_list.append(bbox.tolist())\n\n#             scores_list.append(np.ones(len(cls_list)).tolist())\n\n#             weights.append(1)\n\n            \n#     # Perform NMS\n#     if len(boxes_list)==0:\n#         boxes=boxes_single\n#         box_labels=labels_single\n#         print(\"Bboxes after nms:\\n\", boxes)\n#         print(\"Labels after nms:\\n\", box_labels)\n        \n#         count_dict = Counter(box_labels)\n#         print(count_dict)\n\n#         text_create(desktop_path,image_basename,boxes,box_labels,img_annotations.iloc[0][\"width\"],img_annotations.iloc[0][\"height\"])\n        \n#     else:\n#         boxes, scores, box_labels = nms(boxes_list, scores_list, labels_list, weights=weights,\n#                                     iou_thr=iou_thr)\n\n\n#         #img_array.shape[1]是宽度\n#         boxes = boxes*(img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"], img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"])\n#         boxes = boxes.round(1).tolist()\n#         box_labels = box_labels.astype(int).tolist()\n\n#         boxes.extend(boxes_single)\n#         box_labels.extend(labels_single)\n\n#         print(\"Bboxes after nms:\\n\", boxes)\n#         print(\"Labels after nms:\\n\", box_labels)\n\n#         count_dict = Counter(box_labels)\n#         print(count_dict)\n\n#         text_create(desktop_path,image_basename,boxes,box_labels,img_annotations.iloc[0][\"width\"],img_annotations.iloc[0][\"height\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# iou_thr = 0.5\n# skip_box_thr = 0.0001\n# viz_images = []\n# for i, path in tqdm(enumerate(imagepaths[5:6])):\n#     image_basename = Path(path).stem\n#     print(f\"(\\'{image_basename}\\', \\'{path}\\')\")\n#     img_annotations = train_annotations[train_annotations.image_id==image_basename]\n\n#     boxes_viz = img_annotations[['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().tolist()\n#     labels_viz = img_annotations['class_id'].to_numpy().tolist()\n\n#     print(\"Bboxes before nms:\\n\", boxes_viz)\n#     print(\"Labels before nms:\\n\", labels_viz)\n\n#     boxes_list = []\n#     scores_list = []\n#     labels_list = []\n#     weights = []\n\n#     boxes_single = []\n#     labels_single = []\n\n#     cls_ids = img_annotations['class_id'].unique().tolist()\n#     count_dict = Counter(img_annotations['class_id'].tolist())\n#     print(count_dict)\n\n#     for cid in cls_ids:       \n#         ## Performing Fusing operation only for multiple bboxes with the same label\n#         if count_dict[cid]==1:\n#             labels_single.append(cid)\n#             boxes_single.append(img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().squeeze().tolist())\n\n#         else:\n#             cls_list =img_annotations[img_annotations.class_id==cid]['class_id'].tolist()\n#             labels_list.append(cls_list)\n#             bbox = img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy()\n\n#             ## Normalizing Bbox by Image Width and Height\n#             bbox = bbox/(img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"], img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"])\n\n\n#             bbox = np.clip(bbox, 0, 1)\n#             boxes_list.append(bbox.tolist())\n\n#             scores_list.append(np.ones(len(cls_list)).tolist())\n\n#             weights.append(1)\n\n\n#     # Perform NMS\n#     boxes, scores, box_labels = nms(boxes_list, scores_list, labels_list, weights=weights,\n#                                     iou_thr=iou_thr)\n    \n#     print(\"Bboxes without multipy:\\n\", boxes)\n\n#     #img_array.shape[1]是宽度\n#     boxes = boxes*(img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"], img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"])\n#     boxes = boxes.round(1).tolist()\n#     box_labels = box_labels.astype(int).tolist()\n\n#     boxes.extend(boxes_single)\n#     box_labels.extend(labels_single)\n\n#     print(\"Bboxes after nms:\\n\", boxes)\n#     print(\"Labels after nms:\\n\", box_labels)\n\n#     count_dict = Counter(box_labels)\n#     print(count_dict)\n\n#     text_create('./',image_basename,boxes,box_labels,img_annotations.iloc[0][\"width\"],img_annotations.iloc[0][\"height\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_annotations=train_annotations[train_annotations.area<0.026]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_annotations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Create_softnms_box_txt(desktop_path,name):\n    iou_thr = 0.5\n    skip_box_thr = 0.0001\n    viz_images = []\n    sigma = 0.1\n#     image_basename = Path(path).stem\n    image_basename = name\n#     print(f\"(\\'{image_basename}\\', \\'{path}\\')\")\n    img_annotations = train_annotations[train_annotations.image_id==image_basename][train_annotations.class_id==8]\n\n    boxes_viz = img_annotations[['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().tolist()\n    labels_viz_raw = img_annotations['class_id'].to_numpy().tolist()\n    labels_viz = [0 if wd == 8 else wd for wd in labels_viz_raw]\n    \n    print('boxes_viz',boxes_viz)\n    \n    if(len(boxes_viz)==0):\n        text_create(desktop_path,image_basename,[],[],0,0)\n        return None\n\n    print('没有return')\n        \n    print(\"Bboxes before nms:\\n\", boxes_viz)\n    print(\"Labels before nms:\\n\", labels_viz)\n\n    boxes_list = []\n    scores_list = []\n    labels_list = []\n    weights = []\n\n    boxes_single = []\n    labels_single = []\n\n    cls_ids = img_annotations['class_id'].unique().tolist()\n    count_dict = Counter(img_annotations['class_id'].tolist())\n    print(count_dict)\n\n    for cid in cls_ids:       \n        ## Performing Fusing operation only for multiple bboxes with the same label\n        if count_dict[cid]==1:\n            labels_single.append(cid)\n            boxes_single.append(img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().squeeze().tolist())\n\n        else:\n            cls_list =img_annotations[img_annotations.class_id==cid]['class_id'].tolist()\n            labels_list.append(cls_list)\n            bbox = img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy()\n\n            ## Normalizing Bbox by Image Width and Height\n            bbox = bbox/(img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"], img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"])\n\n\n            bbox = np.clip(bbox, 0, 1)\n            boxes_list.append(bbox.tolist())\n\n            scores_list.append(np.ones(len(cls_list)).tolist())\n\n            weights.append(1)\n\n            \n\n    # Perform NMS\n    if len(boxes_list)==0:\n        boxes=boxes_single\n        box_labels=labels_single\n        print(\"Bboxes after nms:\\n\", boxes)\n        print(\"Labels after nms:\\n\", box_labels)\n        \n        count_dict = Counter(box_labels)\n        print(count_dict)\n\n        text_create(desktop_path,image_basename,boxes,box_labels,img_annotations.iloc[0][\"width\"],img_annotations.iloc[0][\"height\"])\n        \n    else:\n        boxes, scores, box_labels = soft_nms(boxes_list, scores_list, labels_list, weights=weights,\n                                    iou_thr=iou_thr)\n\n\n        #img_array.shape[1]是宽度\n        boxes = boxes*(img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"], img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"])\n        boxes = boxes.round(1).tolist()\n        box_labels = box_labels.astype(int).tolist()\n\n        boxes.extend(boxes_single)\n        box_labels.extend(labels_single)\n\n        print(\"Bboxes after nms:\\n\", boxes)\n        print(\"Labels after nms:\\n\", box_labels)\n\n        count_dict = Counter(box_labels)\n        print(count_dict)\n\n        text_create(desktop_path,image_basename,boxes,box_labels,img_annotations.iloc[0][\"width\"],img_annotations.iloc[0][\"height\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Create_non_maximum_weighted_box_txt(desktop_path,name):\n    iou_thr = 0.5\n    skip_box_thr = 0.0001\n    viz_images = []\n#     image_basename = Path(path).stem\n    image_basename = name\n#     print(f\"(\\'{image_basename}\\', \\'{path}\\')\")\n    img_annotations = train_annotations[train_annotations.image_id==image_basename][train_annotations.class_id==8]\n\n    boxes_viz = img_annotations[['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().tolist()\n    labels_viz = img_annotations['class_id'].to_numpy().tolist()\n\n    print(\"Bboxes before nms:\\n\", boxes_viz)\n    print(\"Labels before nms:\\n\", labels_viz)\n\n    boxes_list = []\n    scores_list = []\n    labels_list = []\n    weights = []\n\n    boxes_single = []\n    labels_single = []\n\n    cls_ids = img_annotations['class_id'].unique().tolist()\n    count_dict = Counter(img_annotations['class_id'].tolist())\n    print(count_dict)\n\n    for cid in cls_ids:       \n        ## Performing Fusing operation only for multiple bboxes with the same label\n        if count_dict[cid]==1:\n            labels_single.append(cid)\n            boxes_single.append(img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().squeeze().tolist())\n\n        else:\n            cls_list =img_annotations[img_annotations.class_id==cid]['class_id'].tolist()\n            labels_list.append(cls_list)\n            bbox = img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy()\n\n            ## Normalizing Bbox by Image Width and Height\n            bbox = bbox/(img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"], img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"])\n\n\n            bbox = np.clip(bbox, 0, 1)\n            boxes_list.append(bbox.tolist())\n\n            scores_list.append(np.ones(len(cls_list)).tolist())\n\n            weights.append(1)\n\n            \n    # Perform NMS\n    if len(boxes_list)==0:\n        boxes=boxes_single\n        box_labels=labels_single\n        print(\"Bboxes after nms:\\n\", boxes)\n        print(\"Labels after nms:\\n\", box_labels)\n        \n        count_dict = Counter(box_labels)\n        print(count_dict)\n\n        text_create(desktop_path,image_basename,boxes,box_labels,img_annotations.iloc[0][\"width\"],img_annotations.iloc[0][\"height\"])\n        \n    else:\n        boxes, scores, box_labels = non_maximum_weighted(boxes_list, scores_list, labels_list, weights=weights,\n                                    iou_thr=iou_thr)\n\n\n        #img_array.shape[1]是宽度\n        boxes = boxes*(img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"], img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"])\n        boxes = boxes.round(1).tolist()\n        box_labels = box_labels.astype(int).tolist()\n\n        boxes.extend(boxes_single)\n        box_labels.extend(labels_single)\n\n        print(\"Bboxes after nms:\\n\", boxes)\n        print(\"Labels after nms:\\n\", box_labels)\n\n        count_dict = Counter(box_labels)\n        print(count_dict)\n\n        text_create(desktop_path,image_basename,boxes,box_labels,img_annotations.iloc[0][\"width\"],img_annotations.iloc[0][\"height\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# os.makedirs('/kaggle/working/vinbigdata/labels/train', exist_ok = True)\n# os.makedirs('/kaggle/working/vinbigdata/labels/val', exist_ok = True)\n# os.makedirs('/kaggle/working/vinbigdata/images/train', exist_ok = True)\n# os.makedirs('/kaggle/working/vinbigdata/images/val', exist_ok = True)\n# label_dir = '/kaggle/input/vinbigdata-yolo-labels-dataset/labels'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # shutil.copy(file, '/kaggle/working/vinbigdata/images/train')\n\n# img_annotations = train_df[train_df.image_id=='47ed17dcb2cbeec15182ed335a8b5a9e'][train_df.class_id==8]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# img_annotations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Create_weighted_boxes_fusion_box_txt(desktop_path,name):\n    iou_thr = 0.5\n    skip_box_thr = 0.0001\n    viz_images = []\n#     image_basename = Path(path).stem\n    image_basename = name\n#     print(f\"(\\'{image_basename}\\', \\'{path}\\')\")\n    img_annotations = train_annotations[train_annotations.image_id==image_basename][train_df.class_id==8]\n\n    boxes_viz = img_annotations[['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().tolist()\n    labels_viz_raw = img_annotations['class_id'].to_numpy().tolist()\n    labels_viz = [0 if wd == 8 else wd for wd in labels_viz_raw]\n\n    print(\"Bboxes before nms:\\n\", boxes_viz)\n    print(\"Labels before nms:\\n\", labels_viz)\n\n    boxes_list = []\n    scores_list = []\n    labels_list = []\n    weights = []\n\n    boxes_single = []\n    labels_single = []\n\n    cls_ids = img_annotations['class_id'].unique().tolist()\n    count_dict = Counter(img_annotations['class_id'].tolist())\n    print(count_dict)\n\n    for cid in cls_ids:       \n        ## Performing Fusing operation only for multiple bboxes with the same label\n        if count_dict[cid]==1:\n            labels_single.append(cid)\n            boxes_single.append(img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().squeeze().tolist())\n\n        else:\n            cls_list =img_annotations[img_annotations.class_id==cid]['class_id'].tolist()\n            labels_list.append(cls_list)\n            bbox = img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy()\n\n            ## Normalizing Bbox by Image Width and Height\n            bbox = bbox/(img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"], img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"])\n\n\n            bbox = np.clip(bbox, 0, 1)\n            boxes_list.append(bbox.tolist())\n\n            scores_list.append(np.ones(len(cls_list)).tolist())\n\n            weights.append(1)\n\n            \n    # Perform NMS\n    if len(boxes_list)==0:\n        boxes=boxes_single\n        box_labels=labels_single\n        print(\"Bboxes after nms:\\n\", boxes)\n        print(\"Labels after nms:\\n\", box_labels)\n        \n        count_dict = Counter(box_labels)\n        print(count_dict)\n\n        text_create(desktop_path,image_basename,boxes,box_labels,img_annotations.iloc[0][\"width\"],img_annotations.iloc[0][\"height\"])\n        \n    else:\n        boxes, scores, box_labels = weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=weights,\n                                    iou_thr=iou_thr)\n\n\n        #img_array.shape[1]是宽度\n        boxes = boxes*(img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"], img_annotations.iloc[0][\"width\"], img_annotations.iloc[0][\"height\"])\n        boxes = boxes.round(1).tolist()\n        box_labels = box_labels.astype(int).tolist()\n\n        boxes.extend(boxes_single)\n        box_labels.extend(labels_single)\n\n        print(\"Bboxes after nms:\\n\", boxes)\n        print(\"Labels after nms:\\n\", box_labels)\n\n        count_dict = Counter(box_labels)\n        print(count_dict)\n\n        text_create(desktop_path,image_basename,boxes,box_labels,img_annotations.iloc[0][\"width\"],img_annotations.iloc[0][\"height\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_create(desktop_path,name, boxes,box_labels,w,h):\n    \n        \n    full_path = os.path.join(desktop_path, name+'.txt')  # 也可以创建一个.doc的word文档\n    print('full_path',full_path)\n    file = open(full_path, 'w')\n    \n    if(len(boxes)==0):\n        file.close()\n    else:\n        dw = 1. / (w)\n        dh = 1. / (h)\n\n        for i in range(len(boxes)):\n\n\n            x = (boxes[i][0] + boxes[i][2]) / 2.0\n            y = (boxes[i][1] + boxes[i][3]) / 2.0\n            w = boxes[i][2] - boxes[i][0]\n            h = boxes[i][3] - boxes[i][1]\n\n\n            x = x * dw\n            w = w * dw\n            y = y * dh\n            h = h * dh\n\n\n    #         file.write(str(box_labels[i])+ ' '+ str(x)+ ' '+ str(y)+ ' '+ str(w)+ ' '+ str(h)+ ' '+ '\\n') \n            file.write(str(0)+ ' '+ str(x)+ ' '+ str(y)+ ' '+ str(w)+ ' '+ str(h)+ ' '+ '\\n')\n            print('录入'+str(0)+ ' '+ str(x)+ ' '+ str(y)+ ' '+ str(w)+ ' '+ str(h)+ ' ')\n        file.close()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.083752,"end_time":"2021-01-01T09:47:56.584924","exception":false,"start_time":"2021-01-01T09:47:56.501172","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Copying Files"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_files","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.makedirs('/kaggle/working/vinbigdata/labels/train', exist_ok = True)\nos.makedirs('/kaggle/working/vinbigdata/labels/val', exist_ok = True)\nos.makedirs('/kaggle/working/vinbigdata/images/train', exist_ok = True)\nos.makedirs('/kaggle/working/vinbigdata/images/val', exist_ok = True)\nlabel_dir = '/kaggle/input/vinbigdata-yolo-labels-dataset/labels'\nfor file in tqdm(train_files):\n    shutil.copy(file, '/kaggle/working/vinbigdata/images/train')\n    filename = file.split('/')[-1].split('.')[0]\n    \n    # nms stuff\n    print(filename)\n    Create_softnms_box_txt('/kaggle/working/vinbigdata/labels/train',filename)\n    \n#     shutil.copy(os.path.join(label_dir, filename+'.txt'), '/kaggle/working/vinbigdata/labels/train')\n    \nfor file in tqdm(val_files):\n    shutil.copy(file, '/kaggle/working/vinbigdata/images/val')\n    filename = file.split('/')[-1].split('.')[0]\n    \n    # nms stuff\n    Create_softnms_box_txt('/kaggle/working/vinbigdata/labels/val',filename)\n#     shutil.copy(os.path.join(label_dir, filename+'.txt'), '/kaggle/working/vinbigdata/labels/val')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\npath = os.getcwd()#获取当前路径\n# print(path)\n\nall_files = [f for f in os.listdir('/kaggle/working/vinbigdata/labels/train' )]#输出根path下的所有文件名到一个列表中\n# all_files = [f for f in os.listdir(path )]#输出根path下的所有文件名到一个列表中\n#对各个文件进行处理\nprint(all_files)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.068822,"end_time":"2021-01-01T09:50:01.458337","exception":false,"start_time":"2021-01-01T09:50:01.389515","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Get Class Name"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-01T09:50:01.595454Z","iopub.status.busy":"2021-01-01T09:50:01.594289Z","iopub.status.idle":"2021-01-01T09:50:01.60074Z","shell.execute_reply":"2021-01-01T09:50:01.601395Z"},"papermill":{"duration":0.082234,"end_time":"2021-01-01T09:50:01.601574","exception":false,"start_time":"2021-01-01T09:50:01.51934","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class_ids, class_names = list(zip(*set(zip(train_df.class_id, train_df.class_name))))\nclasses = list(np.array(class_names)[np.argsort(class_ids)])\nclasses = list(map(lambda x: str(x), classes))\nclasses","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.056257,"end_time":"2021-01-01T09:50:01.716608","exception":false,"start_time":"2021-01-01T09:50:01.660351","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# [YOLOv5](https://github.com/ultralytics/yolov5)\n![](https://user-images.githubusercontent.com/26833433/98699617-a1595a00-2377-11eb-8145-fc674eb9b1a7.jpg)\n![](https://user-images.githubusercontent.com/26833433/90187293-6773ba00-dd6e-11ea-8f90-cd94afc0427f.png)"},{"metadata":{"papermill":{"duration":0.055699,"end_time":"2021-01-01T09:50:01.82747","exception":false,"start_time":"2021-01-01T09:50:01.771771","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# YOLOv5 Stuff"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-01T09:50:01.950234Z","iopub.status.busy":"2021-01-01T09:50:01.949285Z","iopub.status.idle":"2021-01-01T09:50:01.995866Z","shell.execute_reply":"2021-01-01T09:50:01.996316Z"},"papermill":{"duration":0.113001,"end_time":"2021-01-01T09:50:01.996448","exception":false,"start_time":"2021-01-01T09:50:01.883447","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from os import listdir\nfrom os.path import isfile, join\nimport yaml\n\ncwd = '/kaggle/working/'\n\nwith open(join( cwd , 'train.txt'), 'w') as f:\n    for path in glob('/kaggle/working/vinbigdata/images/train/*'):\n        f.write(path+'\\n')\n            \nwith open(join( cwd , 'val.txt'), 'w') as f:\n    for path in glob('/kaggle/working/vinbigdata/images/val/*'):\n        f.write(path+'\\n')\n\ndata = dict(\n    train =  join( cwd , 'train.txt') ,\n    val   =  join( cwd , 'val.txt' ),\n    nc    = 1,\n    names = classes\n    )\n\nwith open(join( cwd , 'vinbigdata.yaml'), 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\nf = open(join( cwd , 'vinbigdata.yaml'), 'r')\nprint('\\nyaml:')\nprint(f.read())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2021-01-01T09:50:02.170487Z","iopub.status.busy":"2021-01-01T09:50:02.169672Z","iopub.status.idle":"2021-01-01T09:50:08.782533Z","shell.execute_reply":"2021-01-01T09:50:08.783883Z"},"papermill":{"duration":6.702428,"end_time":"2021-01-01T09:50:08.784153","exception":false,"start_time":"2021-01-01T09:50:02.081725","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/ultralytics/yolov5\n# !git clone https://github.com/ultralytics/yolov5  # clone repo\n# %cd yolov5\nshutil.copytree('/kaggle/input/yolov5-official-v31-dataset/yolov5', '/kaggle/working/yolov5')\nos.chdir('/kaggle/working/yolov5')\n%pip install -qr requirements.txt # install dependencies\n\nimport torch\nfrom IPython.display import Image, clear_output  # to display images\n\nclear_output()\nprint('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-01T09:50:08.996106Z","iopub.status.busy":"2021-01-01T09:50:08.995246Z","iopub.status.idle":"2021-01-01T09:50:19.303278Z","shell.execute_reply":"2021-01-01T09:50:19.302769Z"},"papermill":{"duration":10.410768,"end_time":"2021-01-01T09:50:19.303402","exception":false,"start_time":"2021-01-01T09:50:08.892634","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images/\nImage(filename='runs/detect/exp/zidane.jpg', width=600)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.064911,"end_time":"2021-01-01T09:50:19.435746","exception":false,"start_time":"2021-01-01T09:50:19.370835","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Pretrained Checkpoints:\n\n| Model | AP<sup>val</sup> | AP<sup>test</sup> | AP<sub>50</sub> | Speed<sub>GPU</sub> | FPS<sub>GPU</sub> || params | FLOPS |\n|---------- |------ |------ |------ | -------- | ------| ------ |------  |  :------: |\n| [YOLOv5s](https://github.com/ultralytics/yolov5/releases/tag/v3.0)    | 37.0     | 37.0     | 56.2     | **2.4ms** | **416** || 7.5M   | 13.2B\n| [YOLOv5m](https://github.com/ultralytics/yolov5/releases/tag/v3.0)    | 44.3     | 44.3     | 63.2     | 3.4ms     | 294     || 21.8M  | 39.4B\n| [YOLOv5l](https://github.com/ultralytics/yolov5/releases/tag/v3.0)    | 47.7     | 47.7     | 66.5     | 4.4ms     | 227     || 47.8M  | 88.1B\n| [YOLOv5x](https://github.com/ultralytics/yolov5/releases/tag/v3.0)    | **49.2** | **49.2** | **67.7** | 6.9ms     | 145     || 89.0M  | 166.4B\n| | | | | | || |\n| [YOLOv5x](https://github.com/ultralytics/yolov5/releases/tag/v3.0) + TTA|**50.8**| **50.8** | **68.9** | 25.5ms    | 39      || 89.0M  | 354.3B\n| | | | | | || |\n| [YOLOv3-SPP](https://github.com/ultralytics/yolov5/releases/tag/v3.0) | 45.6     | 45.5     | 65.2     | 4.5ms     | 222     || 63.0M  | 118.0B"},{"metadata":{"papermill":{"duration":0.064016,"end_time":"2021-01-01T09:50:19.564859","exception":false,"start_time":"2021-01-01T09:50:19.500843","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Selecting Models\nIn this notebok I'm using `v5s`. To select your prefered model just replace `--cfg models/yolov5s.yaml --weights yolov5s.pt` with the following command:\n* `v5s` : `--cfg models/yolov5s.yaml --weights yolov5s.pt`\n* `v5m` : `--cfg models/yolov5m.yaml --weights yolov5m.pt`\n* `v5l` : `--cfg models/yolov5l.yaml --weights yolov5l.pt`\n* `v5x` : `--cfg models/yolov5x.yaml --weights yolov5x.pt`"},{"metadata":{"papermill":{"duration":0.064553,"end_time":"2021-01-01T09:50:19.6938","exception":false,"start_time":"2021-01-01T09:50:19.629247","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Train"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-01T09:50:19.916161Z","iopub.status.busy":"2021-01-01T09:50:19.915216Z","iopub.status.idle":"2021-01-01T15:22:16.288743Z","shell.execute_reply":"2021-01-01T15:22:16.289579Z"},"papermill":{"duration":19916.498298,"end_time":"2021-01-01T15:22:16.289734","exception":false,"start_time":"2021-01-01T09:50:19.791436","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# # !WANDB_MODE=\"dryrun\" python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --nosave --cache \n!WANDB_MODE=\"dryrun\" python train.py --img 640 --batch 16 --epochs 50 --data /kaggle/working/vinbigdata.yaml --weights yolov5s.pt --cache","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":4.919442,"end_time":"2021-01-01T15:22:26.398681","exception":false,"start_time":"2021-01-01T15:22:21.479239","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Class Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n!pip install matplotlib==3.1.3","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-01T15:22:36.714816Z","iopub.status.busy":"2021-01-01T15:22:36.713892Z","iopub.status.idle":"2021-01-01T15:22:37.752475Z","shell.execute_reply":"2021-01-01T15:22:37.752939Z"},"papermill":{"duration":6.511035,"end_time":"2021-01-01T15:22:37.753063","exception":false,"start_time":"2021-01-01T15:22:31.242028","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,20))\nplt.axis('off')\nplt.imshow(plt.imread('runs/train/exp/labels_correlogram.jpg'));","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-01T15:22:47.848164Z","iopub.status.busy":"2021-01-01T15:22:47.847303Z","iopub.status.idle":"2021-01-01T15:22:48.613974Z","shell.execute_reply":"2021-01-01T15:22:48.614481Z"},"papermill":{"duration":5.977042,"end_time":"2021-01-01T15:22:48.614609","exception":false,"start_time":"2021-01-01T15:22:42.637567","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,20))\nplt.axis('off')\nplt.imshow(plt.imread('runs/train/exp/labels.jpg'));","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":5.378338,"end_time":"2021-01-01T15:22:59.482837","exception":false,"start_time":"2021-01-01T15:22:54.104499","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Batch Image"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-01T15:23:10.192425Z","iopub.status.busy":"2021-01-01T15:23:10.19175Z","iopub.status.idle":"2021-01-01T15:23:11.776947Z","shell.execute_reply":"2021-01-01T15:23:11.777415Z"},"papermill":{"duration":7.317416,"end_time":"2021-01-01T15:23:11.777544","exception":false,"start_time":"2021-01-01T15:23:04.460128","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs/train/exp/train_batch0.jpg'))\n\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs/train/exp/train_batch1.jpg'))\n\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs/train/exp/train_batch2.jpg'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GT Vs Pred"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-01T15:23:22.104906Z","iopub.status.busy":"2021-01-01T15:23:22.10403Z","iopub.status.idle":"2021-01-01T15:23:23.51416Z","shell.execute_reply":"2021-01-01T15:23:23.514596Z"},"papermill":{"duration":6.453975,"end_time":"2021-01-01T15:23:23.514717","exception":false,"start_time":"2021-01-01T15:23:17.060742","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(3, 2, figsize = (2*5,3*5), constrained_layout = True)\nfor row in range(3):\n    ax[row][0].imshow(plt.imread(f'runs/train/exp/test_batch{row}_labels.jpg'))\n    ax[row][0].set_xticks([])\n    ax[row][0].set_yticks([])\n    ax[row][0].set_title(f'runs/train/exp/test_batch{row}_labels.jpg', fontsize = 12)\n    \n    ax[row][1].imshow(plt.imread(f'runs/train/exp/test_batch{row}_pred.jpg'))\n    ax[row][1].set_xticks([])\n    ax[row][1].set_yticks([])\n    ax[row][1].set_title(f'runs/train/exp/test_batch{row}_pred.jpg', fontsize = 12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# (Loss, Map) Vs Epoch"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,15))\nplt.axis('off')\nplt.imshow(plt.imread('runs/train/exp/results.png'));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Confusion Matrix"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(30,15))\nplt.axis('off')\nplt.imshow(plt.imread('runs/train/exp1/confusion_matrix.png'));","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":4.941983,"end_time":"2021-01-01T15:23:33.765831","exception":false,"start_time":"2021-01-01T15:23:28.823848","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Inference"},{"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2021-01-01T15:23:44.50211Z","iopub.status.busy":"2021-01-01T15:23:44.501304Z","iopub.status.idle":"2021-01-01T15:23:49.800287Z","shell.execute_reply":"2021-01-01T15:23:49.799352Z"},"papermill":{"duration":10.763143,"end_time":"2021-01-01T15:23:49.800461","exception":false,"start_time":"2021-01-01T15:23:39.037318","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"!python detect.py --weights 'runs/train/exp/weights/best.pt'\\\n--img 640\\\n--conf 0.15\\\n--iou 0.5\\\n--source /kaggle/working/vinbigdata/images/val\\\n--exist-ok","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":5.225725,"end_time":"2021-01-01T15:24:00.706026","exception":false,"start_time":"2021-01-01T15:23:55.480301","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Inference Plot"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-01T15:24:10.963448Z","iopub.status.busy":"2021-01-01T15:24:10.962552Z","iopub.status.idle":"2021-01-01T15:24:11.210595Z","shell.execute_reply":"2021-01-01T15:24:11.211731Z"},"papermill":{"duration":5.31015,"end_time":"2021-01-01T15:24:11.211904","exception":false,"start_time":"2021-01-01T15:24:05.901754","status":"completed"},"tags":[],"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport numpy as np\nimport random\nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm\n\nfiles = glob('runs/detect/exp/*')\nfor _ in range(3):\n    row = 4\n    col = 4\n    grid_files = random.sample(files, row*col)\n    images     = []\n    for image_path in tqdm(grid_files):\n        img          = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n        images.append(img)\n\n    fig = plt.figure(figsize=(col*5, row*5))\n    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n                     nrows_ncols=(col, row),  # creates 2x2 grid of axes\n                     axes_pad=0.05,  # pad between axes in inch.\n                     )\n\n    for ax, im in zip(grid, images):\n        # Iterating over the grid returns the Axes.\n        ax.imshow(im)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-01T15:24:21.60059Z","iopub.status.busy":"2021-01-01T15:24:21.599598Z","iopub.status.idle":"2021-01-01T15:24:22.413063Z","shell.execute_reply":"2021-01-01T15:24:22.411761Z"},"papermill":{"duration":5.709202,"end_time":"2021-01-01T15:24:22.413173","exception":false,"start_time":"2021-01-01T15:24:16.703971","status":"completed"},"tags":[],"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# shutil.rmtree('/kaggle/working/vinbigdata')\n# shutil.rmtree('runs/detect')\n# for file in (glob('runs/train/exp/**/*.png', recursive = True)+glob('runs/train/exp/**/*.jpg', recursive = True)):\n#     os.remove(file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}