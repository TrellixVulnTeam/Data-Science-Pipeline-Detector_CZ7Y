{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Credits\n* The code for converting from dicom to jpg is courtsey of [raddar](https://www.kaggle.com/raddar/vinbigdata-competition-jpg-data-3x-downsampled)\n* The code for visualization, different radiologists bbox consolidation, and coco conversion is courtsey of [sreevishnudamodaran](https://www.kaggle.com/sreevishnudamodaran/vinbigdata-fusing-bboxes-coco-dataset) (note: you can choose a different technoque to combine overlapping bboxes from different radiologists, I just chose weighted boxes fusion because it gave better results based on his visualizations) "},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ensemble-boxes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import rcParams\nsns.set(rc={\"font.size\":9,\"axes.titlesize\":15,\"axes.labelsize\":9,\n            \"axes.titlepad\":11, \"axes.labelpad\":9, \"legend.fontsize\":7,\n            \"legend.title_fontsize\":7, 'axes.grid' : False})\nimport cv2\nimport json\nimport pandas as pd\nimport glob\nimport os.path as osp\nfrom path import Path\nimport datetime\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport random\nimport shutil\nfrom sklearn.model_selection import train_test_split\n\nfrom ensemble_boxes import *\nimport warnings\nfrom collections import Counter\nfrom tqdm import tqdm\n\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dicom to JPG\nThe downscale factor is a hyperparameter (note: this cell is too slow so try to run the cell only once then upload the data to google drive or keep on kaggle working directory)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_xray(path, voi_lut = True, fix_monochrome = True, downscale_factor = 3):\n    dicom = pydicom.read_file(path)\n\n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n\n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255.0).astype(np.uint8)\n    new_shape = tuple([int(x / downscale_factor) for x in data.shape])\n    data = cv2.resize(data, (new_shape[1], new_shape[0]))\n\n    return data\n\nftrain = os.listdir('../input/vinbigdata-chest-xray-abnormalities-detection/train')\nftest = os.listdir('../input/vinbigdata-chest-xray-abnormalities-detection/test')\n\n! mkdir ./train\n! mkdir ./test\n\nfor i in tqdm(range(len(ftrain))):\n    img = read_xray('../input/vinbigdata-chest-xray-abnormalities-detection/train/'+ftrain[i])\n    cv2.imwrite('./train/'+ftrain[i].replace('.dicom','.jpg'), img)\n\nfor i in tqdm(range(len(ftest))):\n    img = read_xray('../input/vinbigdata-chest-xray-abnormalities-detection/test/'+ftest[i])\n    cv2.imwrite('./test/'+ftest[i].replace('.dicom','.jpg'), img)\n    \n#! cp ../input/vinbigdata-chest-xray-abnormalities-detection/train.csv ./","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read the Dataset into a Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_annotations = pd.read_csv(\"./train.csv\")\ntrain_annotations.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_annotations = train_annotations[train_annotations.class_id!=14]\ntrain_annotations['image_path'] = train_annotations['image_id'].map(lambda x:os.path.join('./train', str(x)+'.jpg'))\ntrain_annotations.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imagepaths = train_annotations['image_path'].unique()\nprint(\"Number of Images with abnormalities:\",len(imagepaths))\nanno_count = train_annotations.shape[0]\nprint(\"Number of Annotations with abnormalities:\", anno_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_img(img, size=(18, 18), is_rgb=True, title=\"\", cmap='gray'):\n    plt.figure(figsize=size)\n    plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()\n\ndef plot_imgs(imgs, cols=2, size=10, is_rgb=True, title=\"\", cmap='gray', img_size=None):\n    rows = len(imgs)//cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i, img in enumerate(imgs):\n        if img_size is not None:\n            img = cv2.resize(img, img_size)\n        fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    \ndef draw_bbox(image, box, label, color):   \n    alpha = 0.1\n    alpha_box = 0.4\n    overlay_bbox = image.copy()\n    overlay_text = image.copy()\n    output = image.copy()\n\n    text_width, text_height = cv2.getTextSize(label.upper(), cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)[0]\n    cv2.rectangle(overlay_bbox, (box[0], box[1]), (box[2], box[3]),\n                color, -1)\n    cv2.addWeighted(overlay_bbox, alpha, output, 1 - alpha, 0, output)\n    cv2.rectangle(overlay_text, (box[0], box[1]-7-text_height), (box[0]+text_width+2, box[1]),\n                (0, 0, 0), -1)\n    cv2.addWeighted(overlay_text, alpha_box, output, 1 - alpha_box, 0, output)\n    cv2.rectangle(output, (box[0], box[1]), (box[2], box[3]),\n                    color, thickness)\n    cv2.putText(output, label.upper(), (box[0], box[1]-5),\n            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n    return output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Class Definitions"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels =  [\n            \"__ignore__\",\n            \"Aortic_enlargement\",\n            \"Atelectasis\",\n            \"Calcification\",\n            \"Cardiomegaly\",\n            \"Consolidation\",\n            \"ILD\",\n            \"Infiltration\",\n            \"Lung_Opacity\",\n            \"Nodule/Mass\",\n            \"Other_lesion\",\n            \"Pleural_effusion\",\n            \"Pleural_thickening\",\n            \"Pneumothorax\",\n            \"Pulmonary_fibrosis\"\n            ]\nviz_labels = labels[1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize Original Boxes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# map label_id to specify color\n#label2color = [[random.randint(0,255) for i in range(3)] for class_id in viz_labels]\nlabel2color = [[59, 238, 119], [222, 21, 229], [94, 49, 164], [206, 221, 133], [117, 75, 3],\n                 [210, 224, 119], [211, 176, 166], [63, 7, 197], [102, 65, 77], [194, 134, 175],\n                 [209, 219, 50], [255, 44, 47], [89, 125, 149], [110, 27, 100]]\n\nthickness = 3\nimgs = []\n\nfor img_id, path in zip(train_annotations['image_id'][:6], train_annotations['image_path'][:6]):\n\n    boxes = train_annotations.loc[train_annotations['image_id'] == img_id,\n                                  ['x_min', 'y_min', 'x_max', 'y_max']].values\n    img_labels = train_annotations.loc[train_annotations['image_id'] == img_id, ['class_id']].values.squeeze()\n    \n    img = cv2.imread(path)\n    \n    for label_id, box in zip(img_labels, boxes):\n        color = label2color[label_id]\n        img = draw_bbox(img, list(np.int_(box)), viz_labels[label_id], color)\n    imgs.append(img)\n\nplot_imgs(imgs, size=9, cmap=None)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create COCO Dataset and Consolidate the Similar Annotations for Different Radiologists w/ Weighted Boxes Fusion (WBF)\n\nNote: here we consolidate the different annotations by different radiologists for the same images, another option is to not consolidate but to replicate the entries once for each different annotation"},{"metadata":{},"cell_type":"markdown","source":"## Visualize WBF"},{"metadata":{"trusted":true},"cell_type":"code","source":"iou_thr = 0.5\nskip_box_thr = 0.0001\nviz_images = []\nsigma = 0.1\n\nfor i, path in tqdm(enumerate(imagepaths[5:8])):\n    img_array  = cv2.imread(path)\n    image_basename = Path(path).stem\n    print(f\"(\\'{image_basename}\\', \\'{path}\\')\")\n    img_annotations = train_annotations[train_annotations.image_id==image_basename]\n\n    boxes_viz = img_annotations[['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().tolist()\n    labels_viz = img_annotations['class_id'].to_numpy().tolist()\n    \n    print(\"Bboxes before WBF:\\n\", boxes_viz)\n    print(\"Labels before WBF:\\n\", labels_viz)\n    \n    ## Visualize Original Bboxes\n    img_before = img_array.copy()\n    for box, label in zip(boxes_viz, labels_viz):\n        x_min, y_min, x_max, y_max = (box[0], box[1], box[2], box[3])\n        color = label2color[int(label)]\n        img_before = draw_bbox(img_before, list(np.int_(box)), viz_labels[label], color)\n    viz_images.append(img_before)\n    \n    boxes_list = []\n    scores_list = []\n    labels_list = []\n    weights = []\n    \n    boxes_single = []\n    labels_single = []\n    \n    cls_ids = img_annotations['class_id'].unique().tolist()\n    count_dict = Counter(img_annotations['class_id'].tolist())\n    print(count_dict)\n\n    for cid in cls_ids:       \n        ## Performing Fusing operation only for multiple bboxes with the same label\n        if count_dict[cid]==1:\n            labels_single.append(cid)\n            boxes_single.append(img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().squeeze().tolist())\n\n        else:\n            cls_list =img_annotations[img_annotations.class_id==cid]['class_id'].tolist()\n            labels_list.append(cls_list)\n            bbox = img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy()\n            ## Normalizing Bbox by Image Width and Height\n            bbox = bbox/(img_array.shape[0], img_array.shape[1], img_array.shape[0], img_array.shape[1])\n            bbox = np.clip(bbox, 0, 1)\n            boxes_list.append(bbox.tolist())\n            scores_list.append(np.ones(len(cls_list)).tolist())\n\n            weights.append(1)\n            \n\n    # Perform WBF\n    boxes, scores, box_labels= weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=weights,\n                                                     iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    \n    \n    boxes = boxes*(img_array.shape[0], img_array.shape[1], img_array.shape[0], img_array.shape[1])\n    boxes = boxes.round(1).tolist()\n    box_labels = box_labels.astype(int).tolist()\n\n    boxes.extend(boxes_single)\n    box_labels.extend(labels_single)\n    \n    print(\"Bboxes after WBF:\\n\", boxes)\n    print(\"Labels after WBF:\\n\", box_labels)\n    \n    ## Visualize Bboxes after operation\n    img_after = img_array.copy()\n    for box, label in zip(boxes, box_labels):\n        color = label2color[int(label)]\n        img_after = draw_bbox(img_after, list(np.int_(box)), viz_labels[label], color)\n    viz_images.append(img_after)\n    print()\n        \nplot_imgs(viz_images, cmap=None)\nplt.figtext(0.3, 0.9,\"Original Bboxes\", va=\"top\", ha=\"center\", size=25)\nplt.figtext(0.73, 0.9,\"WBF\", va=\"top\", ha=\"center\", size=25)\nplt.savefig('wbf.png', bbox_inches='tight')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building COCO DATASET"},{"metadata":{},"cell_type":"markdown","source":"### Train and Val Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(42)\n## 42 -  The Answer to the Ultimate Question of Life\nrandom.shuffle(imagepaths)\ntrain_len = round(0.75*len(imagepaths))\ntrain_paths = imagepaths[:train_len]\nval_paths = imagepaths[train_len:]\n\nprint(\"Split Counts\\nTrain Images:\\t\\t{0}\\nVal Images:\\t\\t{1}\"\n      .format(len(train_paths), len(val_paths)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining Structure"},{"metadata":{"trusted":true},"cell_type":"code","source":"now = datetime.datetime.now()\n\ndata = dict(\n    info=dict(\n        description=None,\n        url=None,\n        version=None,\n        year=now.year,\n        contributor=None,\n        date_created=now.strftime('%Y-%m-%d %H:%M:%S.%f'),\n    ),\n    licenses=[dict(\n        url=None,\n        id=0,\n        name=None,\n    )],\n    images=[\n        # license, url, file_name, height, width, date_captured, id\n    ],\n    type='instances',\n    annotations=[\n        # segmentation, area, iscrowd, image_id, bbox, category_id, id\n    ],\n    categories=[\n        # supercategory, id, name\n    ],\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_name_to_id = {}\nfor i, each_label in enumerate(labels):\n    class_id = i - 1  # starts with -1\n    class_name = each_label\n    if class_id == -1:\n        assert class_name == '__ignore__'\n        continue\n    class_name_to_id[class_name] = class_id\n    data['categories'].append(dict(\n        supercategory=None,\n        id=class_id,\n        name=class_name,\n    ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Creating Output Directories"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_output_dir = \"./vinbigdata_coco_chest_xray/train_images\"\nval_output_dir = \"./vinbigdata_coco_chest_xray/val_images\"\n\nif not osp.exists(train_output_dir):\n    os.makedirs(train_output_dir)\n    print('Coco Train Image Directory:', train_output_dir)\n    \nif not osp.exists(val_output_dir):\n    os.makedirs(val_output_dir)\n    print('Coco Val Image Directory:', val_output_dir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Doing the COCO Conversion"},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings(\"ignore\", category=UserWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Setting the output annotations json file path\ntrain_out_file = './vinbigdata_coco_chest_xray/train_annotations.json'\n\ndata_train = data.copy()\ndata_train['images'] = []\ndata_train['annotations'] = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iou_thr = 0.5\nskip_box_thr = 0.0001\nviz_images = []\n\nfor i, path in tqdm(enumerate(train_paths)):\n    img_array  = cv2.imread(path)\n    image_basename = Path(path).stem\n#     print(f\"(\\'{image_basename}\\', \\'{path}\\')\")\n    \n    ## Copy Image \n    shutil.copy2(path, train_output_dir)\n    \n    ## Add Images to annotation\n    data_train['images'].append(dict(\n        license=0,\n        url=None,\n        file_name=os.path.join('train_images', image_basename+'.jpg'),\n        height=img_array.shape[0],\n        width=img_array.shape[1],\n        date_captured=None,\n        id=i\n    ))\n    \n    img_annotations = train_annotations[train_annotations.image_id==image_basename]\n    boxes_viz = img_annotations[['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().tolist()\n    labels_viz = img_annotations['class_id'].to_numpy().tolist()\n    \n    ## Visualize Original Bboxes every 500th\n    if (i%500==0):\n        img_before = img_array.copy()\n        for box, label in zip(boxes_viz, labels_viz):\n            x_min, y_min, x_max, y_max = (box[0], box[1], box[2], box[3])\n            color = label2color[int(label)]\n            img_before = draw_bbox(img_before, list(np.int_(box)), viz_labels[label], color)\n        viz_images.append(img_before)\n    \n    boxes_list = []\n    scores_list = []\n    labels_list = []\n    weights = []\n    \n    boxes_single = []\n    labels_single = []\n\n    cls_ids = img_annotations['class_id'].unique().tolist()\n    \n    count_dict = Counter(img_annotations['class_id'].tolist())\n\n    for cid in cls_ids:\n        ## Performing Fusing operation only for multiple bboxes with the same label\n        if count_dict[cid]==1:\n            labels_single.append(cid)\n            boxes_single.append(img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().squeeze().tolist())\n\n        else:\n            cls_list =img_annotations[img_annotations.class_id==cid]['class_id'].tolist()\n            labels_list.append(cls_list)\n            bbox = img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy()\n            \n            ## Normalizing Bbox by Image Width and Height\n            bbox = bbox/(img_array.shape[0], img_array.shape[1], img_array.shape[0], img_array.shape[1])\n            bbox = np.clip(bbox, 0, 1)\n            boxes_list.append(bbox.tolist())\n            scores_list.append(np.ones(len(cls_list)).tolist())\n            weights.append(1)\n    \n    ## Perform WBF\n    boxes, scores, box_labels = weighted_boxes_fusion(boxes_list=boxes_list, scores_list=scores_list,\n                                                  labels_list=labels_list, weights=weights,\n                                                  iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    \n    boxes = boxes*(img_array.shape[0], img_array.shape[1], img_array.shape[0], img_array.shape[1])\n    boxes = boxes.round(1).tolist()\n    box_labels = box_labels.astype(int).tolist()\n    boxes.extend(boxes_single)\n    box_labels.extend(labels_single)\n    \n    img_after = img_array.copy()\n    for box, label in zip(boxes, box_labels):\n        x_min, y_min, x_max, y_max = (box[0], box[1], box[2], box[3])\n        area = round((x_max-x_min)*(y_max-y_min),1)\n        bbox =[\n                round(x_min, 1),\n                round(y_min, 1),\n                round((x_max-x_min), 1),\n                round((y_max-y_min), 1)\n                ]\n        \n        data_train['annotations'].append(dict( id=len(data_train['annotations']), image_id=i,\n                                            category_id=int(label), area=area, bbox=bbox,\n                                            iscrowd=0))\n        \n    ## Visualize Bboxes after operation every 500th\n    if (i%500==0):\n        img_after = img_array.copy()\n        for box, label in zip(boxes, box_labels):\n            color = label2color[int(label)]\n            img_after = draw_bbox(img_after, list(np.int_(box)), viz_labels[label], color)\n        viz_images.append(img_after)\n\nplot_imgs(viz_images, cmap=None)\nplt.figtext(0.3, 0.9,\"Original Bboxes\", va=\"top\", ha=\"center\", size=25)\nplt.figtext(0.73, 0.9,\"WBF\", va=\"top\", ha=\"center\", size=25)\nplt.show()\n               \nwith open(train_out_file, 'w') as f:\n    json.dump(data_train, f, indent=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Verify Annotations"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Images in the Train Annotations File:\", len(data_train['images']))\nprint(\"Number of Bboxes in the Train Annotations File:\", len(data_train['annotations']))\n\nprint(\"Number of Images in the Val Annotations File:\", len(data_val['images']))\nprint(\"Number of Bboxes in the Val Annotations File:\", len(data_val['annotations']))\n\n# Should output\n# Number of Images in the Train Annotations File: 3296\n# Number of Bboxes in the Train Annotations File: 17815\n# Number of Images in the Val Annotations File: 1098\n# Number of Bboxes in the Val Annotations File: 5880","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!find ./vinbigdata_coco_chest_xray/val_images -type f | wc -l\n# 1098","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!find ./vinbigdata_coco_chest_xray/train_images -type f | wc -l\n# 3296","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}