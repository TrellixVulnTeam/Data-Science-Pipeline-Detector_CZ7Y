{"cells":[{"metadata":{"_uuid":"b43994ce-bbcf-4f7c-aeab-c7215fb073a0","_cell_guid":"49029d66-3a75-4bc0-85b1-c225a984d5ab","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    print(dirname)\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07af30b9-e0ed-4259-9045-9454671dff12","_cell_guid":"76dd9eaf-1459-4b04-8c3b-ffd57a0b7782","trusted":true},"cell_type":"code","source":"import os\n\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport torchvision.transforms as transforms\n\nimport torch\nfrom torch import nn\nimport glob\n\nimport os\nimport numpy as np\nimport torch\nfrom PIL import Image\nimport pandas as pd\nimport ast\nimport torch\nfrom path import Path\nfrom torchvision.transforms import transforms\nimport torchvision\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nimport cv2\n#import torch.utils.data.Dataset \n\n# to resize bbox along with images\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom torch.utils.data import Dataset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"990dca67-a7c0-40eb-8adc-f0820f208159","_cell_guid":"1a9fc405-b7e2-48e0-999f-c6bac6c5a4bf","trusted":true},"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8508abdb-1b15-4d97-8c98-5d3e7d0ab783","_cell_guid":"500c611e-ceae-42bd-bff8-15e95772384d","trusted":true},"cell_type":"markdown","source":"## preprocessing : get train img_ids,labels and bounding boxes"},{"metadata":{"_uuid":"e717f323-1860-4539-8eec-2d0f1fb3cd94","_cell_guid":"29dfa9f4-3973-452f-8b93-11a130d5a66e","trusted":true},"cell_type":"code","source":"train_csv = pd.read_csv(\"/kaggle/input/vinbigdata-chest-xray-abnormalities-detection/train.csv\")\nprint(\"Train Data Size : {}\".format(train_csv.shape[0]))\ntrain_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7ad5da7-9f3b-4f9b-bc51-d79dff161c28","_cell_guid":"c77b97a8-4bf5-4e46-a5f5-97fee2552200","trusted":true},"cell_type":"code","source":"# #for debugging\n# train_csv = train_csv.dropna()\n# train_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"657e0eb1-fc42-4d2b-83ff-58616379cbb1","_cell_guid":"e34580ce-ab77-4ae2-a3d9-a32cf1dc4c8f","trusted":true},"cell_type":"code","source":"# set min x and y to 0 for \"no finding\"    \n\nfor y in [\"x_min\",\"y_min\"]:\n    print(y)\n    train_csv[y] = train_csv[y].fillna(0.0)\n\n    \n# set max x and y to 1 for \"no finding\"    \n\nfor y in [\"x_max\",\"y_max\"]:\n    print(y)\n    train_csv[y] = train_csv[y].fillna(1.0)\n    \n    \ntrain_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3aecccb3-e157-494e-90a4-e74060410b53","_cell_guid":"a950d6fa-621d-4102-aff7-457b525910cb","trusted":true},"cell_type":"markdown","source":"rcnn takes 0 for \"no finding\"\nour data has 14 for \"no finding\"\n\nmap org 14 to 0 for rcnn\nmap org 0 - 13 class ids to 1-14 for rcnn\n\ni.e now 0 means \" no finding\"  and class ids of \"1-14\" -> class ids \"0-13\" in org df"},{"metadata":{"_uuid":"bb7cddbf-25e9-4d96-aa86-f6b471f0ef4d","_cell_guid":"ce238cca-2b5e-4934-9c8c-01c182b30c91","trusted":true},"cell_type":"code","source":"train_csv[\"class_id\"] = train_csv[\"class_id\"].apply(lambda x : x+1)\ntrain_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcb6e882-db72-45cf-b37a-aca61c417df6","_cell_guid":"70b67530-6110-4e59-9f5d-ecc1ab223041","trusted":true},"cell_type":"code","source":"#now make id of 15 i,e nofinding to id of 0\ntrain_csv[\"class_id\"] = train_csv[\"class_id\"].replace(15,0)\ntrain_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fa3da54-1de2-45aa-b4c7-578e033db057","_cell_guid":"138ab094-416e-4ad3-bbf1-96435a8b37a3","trusted":true},"cell_type":"code","source":"## see more data\n\ntrain_csv.iloc[:,0:100]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"363a3581-b502-4b10-9853-1d2e944a5835","_cell_guid":"dacbf5ff-2618-4461-8da0-151ec1199179","trusted":true},"cell_type":"code","source":"#for debugging:\n\n# train_csv[\"x_diff\"] = train_csv[\"x_max\"] - train_csv[\"x_min\"]\n# train_csv[\"y_diff\"] = train_csv[\"y_max\"] - train_csv[\"y_min\"]\n# print(\n# train_csv[\"x_diff\"].min(),\n\n# train_csv[\"y_diff\"].min(),)\n\n# #dropping where diff is less than, = 20\n# index_names = train_csv[ train_csv['x_diff'] <= 20 ].index\n# print(index_names)\n# train_csv.drop(index_names, inplace = True)\n\n# index_names = train_csv[ train_csv['y_diff'] <= 20 ].index\n# print(index_names)\n# train_csv.drop(index_names, inplace = True)\n\n# train_csv.head()\n# # train_csv.loc[1522]\n# print(\n# train_csv[\"x_diff\"].min(),\n\n# train_csv[\"y_diff\"].min(),)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a5fbe3b-a2ce-4de9-8908-af90416b441f","_cell_guid":"7412a333-8fd6-4f72-a7a0-cd0bf8216e28","trusted":true},"cell_type":"code","source":"train_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3511fec9-ef0e-4e2f-9fde-31b6d613ad00","_cell_guid":"5a4a67b0-ebae-4cff-9774-180c3082ca47","trusted":true},"cell_type":"markdown","source":"\n## dataset class"},{"metadata":{"_uuid":"380d1971-b603-49b8-8b66-d0d4f52ff776","_cell_guid":"debbe324-ad93-4d6a-88c4-8a280b1eed13","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\ndef read_xray(path, voi_lut = True, fix_monochrome = True):\n    # Original from: https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \n    # \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n        \n    return data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9d8cff8-436e-46a3-bc2d-0920cd155100","_cell_guid":"65e8d304-fb87-4d4b-b66a-886908caa473","trusted":true},"cell_type":"code","source":"def resize(array, size, keep_ratio=False, resample=Image.LANCZOS):\n    # Original from: https://www.kaggle.com/xhlulu/vinbigdata-process-and-resize-to-image\n    im = Image.fromarray(array)\n    \n    if keep_ratio:\n        im.thumbnail((size, size), resample)\n    else:\n        im = im.resize((size, size), resample)\n    \n    return im","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1475204a-cd8a-4082-b05f-24268f2ab639","_cell_guid":"e02804e1-7cf8-4b33-b4da-42e9231d62df","trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset\n\n\nclass MyDataSet(Dataset):\n        \n        def __init__(self,data_frame, transforms,image_dir):\n            \n            super().__init__()\n            self.data_frame = data_frame\n            self.transforms = transforms\n            self.image_dir = Path(image_dir)\n            self.image_ids = self.data_frame['image_id'].unique()\n            \n            \n        def __getitem__(self,idx):    # what is this idx?\n            \n            image_id = self.image_ids[idx]\n            image_name = image_id + \".dicom\"\n            \n            # get all images for this image id\n            records = self.data_frame[self.data_frame[\"image_id\"]==image_id]\n            \n            # --------------- preprocess image , method 1 ------------------- #\n#             dicom_img = pydicom.read_file(self.image_dir+\"/\"+image_name)\n#             image = dicom_img.pixel_array\n            \n            # image preprocessing : taken from -> https://www.kaggle.com/chekoduadarsh/pytorch-beginner-code-faster-rcnn\n#             if \"PhotometricInterpretation\" in dicom:\n#                 if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n#                     image = np.amax(image) - image\n\n#             intercept = dicom.RescaleIntercept if \"RescaleIntercept\" in dicom else 0.0\n#             slope = dicom.RescaleSlope if \"RescaleSlope\" in dicom else 1.0\n\n#             if slope != 1:\n#                 image = slope * image.astype(np.float64)\n#                 image = image.astype(np.int16)\n\n        \n#             image += np.int16(intercept)        \n\n#             image = np.stack([image, image, image])\n#             image = image.astype('float32')\n#             image = image - image.min()\n#             image = image / image.max()\n#             image = image * 255.0\n#             image = image.transpose(1,2,0)\n\n\n\n             # --------------- preprocess image , method 2  ------------------- #    \n         \n#         set keep_ratio=True to have original aspect ratio\n#             xray_image = read_xray(self.image_dir+\"/\"+image_name)\n#             image = resize(xray_image, size=512) \n            \n            dicom_img = pydicom.read_file(self.image_dir+\"/\"+image_name)\n            image = dicom_img.pixel_array\n\n            image = read_xray(self.image_dir+\"/\"+image_name)\n            \n#              wont reszie bbox\n            if self.transforms is not None:\n                image = self.transforms(image)\n       \n            \n#             bounding boxes\n            boxes = records[['x_min','y_min','x_max','y_max']].values\n              \n#             print(\"tyoe must be {}\".format(type(boxes)))\n            boxes = torch.tensor(boxes,dtype  = torch.float64)\n\n            labels = torch.tensor(records[\"class_id\"].values,dtype = torch.int64)\n            img_id = torch.tensor([idx])\n\n            \n            target={}\n            target[\"img_id\"] = img_id\n            target[\"boxes\"] = boxes\n            target[\"labels\"] = labels\n\n\n            \n            \n            return image, target, image_id\n        \n        def __len__(self):\n            return self.image_ids.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e182003f-bba0-404d-b63e-8a824f3857e5","_cell_guid":"629b04a2-28bf-4ec4-b428-eea47e6a1692","trusted":true},"cell_type":"code","source":"train_dir_path = \"/kaggle/input/vinbigdata-chest-xray-abnormalities-detection/train\"\n\n\n# wont reszie bbox\nmytransforms = transforms.Compose([\n    transforms.ToTensor()\n])\n\n\nt_resize = A.Compose([\n#         A.Flip(0.5),\n#         A.ShiftScaleRotate(scale_limit=0.1, rotate_limit=45, p=0.25),\n        A.LongestMaxSize(max_size=800, p=1.0),\n#         Dilation(),\n        # FasterRCNN will normalize.\n#         A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\n\nmydataset = MyDataSet(train_csv,mytransforms,train_dir_path)\n# mydataset = VinBigDataset(train_csv, train_dir_path, t_resize)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02819695-2e7e-445c-b892-a56a1dfda207","_cell_guid":"eba44e27-8c6c-4bb5-a34e-137c972d322d","trusted":true},"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\n\n# split the dataset in train and test set\nindices = torch.randperm(len(mydataset)).tolist()\ndataset_train = torch.utils.data.Subset(mydataset, indices[:-1000])\ndataset_test = torch.utils.data.Subset(mydataset, indices[-1000:])\n\nmydataloader = torch.utils.data.DataLoader(dataset_train, batch_size=4, collate_fn=collate_fn) # for train\ntest_dataloader = torch.utils.data.DataLoader(dataset_test, batch_size=1, collate_fn=collate_fn)# for test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43b8cfa9-7bda-4f35-a3ba-20f97e9b9b15","_cell_guid":"c6e300ab-88c3-4941-885f-c222f4977b57","trusted":true},"cell_type":"code","source":"img_data, target_data , imid_data = next(iter(mydataloader))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"225d19e1-ef7f-4257-9072-bd5da9d95aac","_cell_guid":"2a8ddb38-e50f-4892-a700-9fa27461a04c","trusted":true},"cell_type":"code","source":"print(len(img_data))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d35065b6-aadb-4572-974c-a5f112e309cb","_cell_guid":"5304d523-413b-49a2-89b1-210525420a82","trusted":true},"cell_type":"code","source":"#check images data loader working fine\nlimit = 0\nfor t_images, t_targets, t_ids in mydataloader:\n    print(t_ids)\n    print(type(t_targets[0][\"boxes\"]))\n    for imgs in t_images:\n        print(imgs.shape)\n    limit+=1\n    \n    if limit == 2:\n        break","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54b614f6-7db5-41da-88d5-216a8b2cb376","_cell_guid":"f2912425-ac5b-4b19-9ee7-d78f9bd26dc7","trusted":true},"cell_type":"markdown","source":"## visualize some images"},{"metadata":{"_uuid":"21bcebd6-1958-473b-a045-2aedca8d3fc6","_cell_guid":"a8a499cc-0c21-41bc-b8b9-305329b26f5e","trusted":true},"cell_type":"code","source":"# for t_images, t_targets, t_ids in mydataloader:\n\n#     for i in range(4):\n        \n#         img = t_images[i].to('cpu').permute(1,2,0).squeeze()\n#         print(img.shape)\n#         boxes = t_targets[i]['boxes'].to('cpu')\n#         print(img.shape)\n#         print(boxes)\n#         for box in boxes:\n#             print(\"adding box\")\n#             img = np.asarray(img)\n# #             img = cv2.rectangle(img,(box[0],box[1]),(box[2],box[3]),(255,255,255),2)\n# #             img = img.rectangle(box, fill =\"# ffff33\", outline =\"red\")\n\n#         plt.imshow(img)\n#         plt.show()\n        \n#     break","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01030378-9ef4-4127-bd3c-24121698b546","_cell_guid":"492fa7c7-a6f7-4b30-9a1a-41703093bebf","trusted":true},"cell_type":"code","source":"#this test code is designed for batchsize = 4, may require some changes accordinlgy\n\nfor t_images, t_targets, t_ids in mydataloader:\n\n    for i in range(3):\n        print(\"**********\")\n        to_show = t_images[i].to('cpu').permute(1,2,0).numpy() # reshaping tenosr and converting to numpy for disply by plt\n        boxes = t_targets[i]['boxes'].to('cpu')\n        fig,axs = plt.subplots(1,1)\n\n        for box in boxes:\n            print(\"draw bbox\")\n            print(box)\n#             rect = patches.Rectangle((100, 100), 100, 100, linewidth=1, edgecolor='r', facecolor='none')\n                  \n            rect = patches.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], linewidth=1, edgecolor='r', facecolor='none')\n            axs.add_patch(rect)\n\n            # Add the patch to the Axes\n            axs.imshow(to_show)\n    \n    break # run for first batch only","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfd3932e-2caa-420a-a527-d2f96d4ffc33","_cell_guid":"8ffed578-56fb-4692-9e67-32db7e043b03","trusted":true},"cell_type":"markdown","source":"## model"},{"metadata":{"_uuid":"a8b26f8c-66bc-4db9-a4ad-57e2eb27b7f3","_cell_guid":"bdb852e1-381d-41de-8cf2-9bf0a6c5dd6b","trusted":true},"cell_type":"code","source":"mymodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True)\n\n# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2df8deb2-31ff-4db0-87e5-bf055d872417","_cell_guid":"9b81ec8a-39fa-42f9-961b-151781af3884","trusted":true},"cell_type":"code","source":"num_classes = 15\n\n# get number of input features for the classifier\nin_features = mymodel.roi_heads.box_predictor.cls_score.in_features\n\nmymodel.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c190dda0-a274-462e-9a2c-f21aafba2ff2","_cell_guid":"bfb94c89-c876-477d-a284-3d5b2a638764","trusted":true},"cell_type":"markdown","source":"## training"},{"metadata":{"_uuid":"455a48a0-4337-4f43-8aec-79e94d74079b","_cell_guid":"bb51839c-5df9-4c54-a273-7f2b27a9fa1e","trusted":true},"cell_type":"code","source":"params = []\n\nfor p in mymodel.parameters():\n    if p.requires_grad:\n        params.append(p)\n        \nprint(params)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7829c4e-a3ee-4c80-add5-2068ece80905","_cell_guid":"98f94ec8-ce7a-4d23-8934-1cc8ced52f52","trusted":true},"cell_type":"code","source":"# optimizer = torch.optim.SGD(params, lr = 0.001)\n\n# epochs = 2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85af214b-2eaf-43d9-a323-e4efc599e168","_cell_guid":"d14b36aa-a982-42ee-aba3-2bae2933c585","trusted":true},"cell_type":"code","source":"optimizer = torch.optim.SGD(params, lr=0.0001,\n                            momentum=0.6, weight_decay=0.0005)\n# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)\n\nepochs = 2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5680ddd-87ab-4451-a6f3-45cb42e6e3f2","_cell_guid":"4978ac64-d4f4-4c47-8c9b-fee89e463a94","trusted":true},"cell_type":"code","source":"mymodel.train()\nfor epoch in range(epochs):\n    \n    print(\"********** EPOCH # {} *************\".format(epoch+1))\n    b_num = 0\n    for t_images, t_targets, t_ids in mydataloader:\n        print(\"----- batch # {} ------\".format(b_num+1))\n        \n        if torch.cuda.is_available():\n#             print(\"transferred\")\n            t_images = [img.to(device) for img in t_images]\n            t_targets = [{k:v.to(device) for k,v in t.items()} for t in t_targets]\n            mymodel = mymodel.to(device)\n        \n#         print(t_targets)\n        myloss_dict = mymodel(t_images, t_targets) \n        \n        print(\"loss dict is {}\".format(myloss_dict))\n        \n        all_loss = sum(loss for loss in myloss_dict.values())\n        loss_value = all_loss.item()\n        \n        print(\"loss : {}\".format(loss_value))\n        \n        \n        optimizer.zero_grad()\n        all_loss.backward()\n        optimizer.step()\n        b_num+=1\n        \n    lr_scheduler.step()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bfbe663-a627-4874-84cf-190d6728df69","_cell_guid":"013901b6-cd95-420d-bf35-2b778658b48d","trusted":true},"cell_type":"markdown","source":"## testing"},{"metadata":{"_uuid":"6d750223-3b7c-4461-8d1c-9196733d687d","_cell_guid":"500b1753-0cfe-43ab-835a-d7e1ed667e36","trusted":true},"cell_type":"code","source":"mymodel.eval()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab176b1e-75d6-4f77-8a85-5e8997a78dc7","_cell_guid":"f9ea9572-ebef-4159-adb4-5f74465b1546","trusted":true},"cell_type":"code","source":"# t=0\n# for t_images, t_targets, t_ids in test_dataloader:\n#     print(\"----- test # {} ------\".format(t+1))\n\n#     if torch.cuda.is_available():\n# #             print(\"transferred\")\n#         t_images = [img.to(device) for img in t_images]\n#         t_targets = [{k:v.to(device) for k,v in t.items()} for t in t_targets]\n#         mymodel = mymodel.to(device)\n\n\n    \n#     predicted = mymodel(t_images) \n# #     print(predicted)\n\n#     label = predicted[0][\"labels\"][0]\n#     bbox = predicted[0][\"boxes\"][0]\n#     score = predicted[0][\"scores\"][0]\n    \n#     print(\"predicted label : {}\".format(label))\n#     print(\"predicted bbox : {}\".format(bbox))\n#     print(\"prediction score : {}\".format(score))\n    \n    \n#     print(\"true label : {}\".format(t_targets[0][\"labels\"]))\n#     print(\"true bbox : {}\".format(t_targets[0][\"boxes\"]))\n    \n    \n    \n    \n#     t+=1\n#     if t==3:\n#         break\n\n# # lr_scheduler.step()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18af741d-ab15-4791-afc6-b2b1b769fbcd","_cell_guid":"b7c0f485-8818-413c-a704-f57faa3a6bc6","trusted":true},"cell_type":"markdown","source":"## visualize test results"},{"metadata":{"_uuid":"bb983caa-712a-4b88-a140-15e6e8d40a92","_cell_guid":"cdf2afb0-2d06-4f8c-b5ea-9205adca093b","trusted":true},"cell_type":"markdown","source":"## submission testing"},{"metadata":{"_uuid":"710953e2-2a3b-482d-8592-575ce02fd53e","_cell_guid":"9b23c950-7a93-499a-8d69-8e7246f2c297","trusted":true},"cell_type":"code","source":"class MyTestDataSet(Dataset):\n        \n        def __init__(self,data_frame, transforms,image_dir):\n            \n            super().__init__()\n            self.data_frame = data_frame\n            self.transforms = transforms\n            self.image_dir = Path(image_dir)\n            self.image_ids = self.data_frame['image_id'].unique()\n            \n            \n        def __getitem__(self,idx):    # what is this idx?\n            \n            image_id = self.image_ids[idx]\n            image_name = image_id + \".dicom\"\n            \n            # get all images for this image id\n            records = self.data_frame[self.data_frame[\"image_id\"]==image_id]\n            \n            image = read_xray(self.image_dir+\"/\"+image_name)\n            \n#              wont reszie bbox\n            if self.transforms is not None:\n                image = self.transforms(image)\n       \n            \n            img_id = torch.tensor([idx])\n\n            \n            target={}\n            \n            image = image.to(device)\n            \n            \n       \n            \n            return image, image_id\n        \n        def __len__(self):\n            return self.image_ids.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0847da75-08f0-402f-897e-05d2696d57b4","_cell_guid":"9c81dbeb-f6ca-40a2-babe-edfafdaf0fa3","trusted":true},"cell_type":"code","source":"sub_csv = pd.read_csv(\"../input/vinbigdata-chest-xray-abnormalities-detection/sample_submission.csv\")\nprint(\"Train Data Size : {}\".format(sub_csv.shape[0]))\nsub_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48618e06-3104-4bcc-9890-3372359879f9","_cell_guid":"7ffc99c3-6a3d-41fe-9e56-44a98291e3e7","trusted":true},"cell_type":"code","source":"# test images data loader\n\ntest_dir_path = \"../input/vinbigdata-chest-xray-abnormalities-detection/test\"\n\nsub_dataset = MyTestDataSet(sub_csv,mytransforms,test_dir_path)\nsub_dataloader = torch.utils.data.DataLoader(sub_dataset, batch_size=1, collate_fn=collate_fn) # for train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d7632bd-f921-412d-9124-900ff8425f6c","_cell_guid":"0ed7e91c-36d3-4738-8f8c-a1baba305187","trusted":true},"cell_type":"code","source":"# chech dataloader working ..\nc=0\nfor img,idx in sub_dataloader:\n    print(img[0].shape)\n    \n    c+=1\n    \n    if c==3:\n        break","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db2587d3-291d-4598-bb6f-8b48f2820b30","_cell_guid":"9f889856-c8b1-4889-a0a5-9716deb81279","trusted":true},"cell_type":"code","source":"sub_preds = {}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6623992a-c668-4200-a000-84484e4a99a9","_cell_guid":"2d3ae0fd-e06a-42ba-b41d-2d1500aefb20","trusted":true},"cell_type":"code","source":"#testing code\nt=0\nmymodel.eval()\nfor t_images, t_ids in sub_dataloader:\n    print(\"-------------------------- test # {} ----------------------\".format(t+1))\n    pred_str = \"\"\n    if torch.cuda.is_available():\n#             print(\"transferred\")\n        t_images = [img.to(device) for img in t_images]\n        t_targets = [{k:v.to(device) for k,v in t.items()} for t in t_targets]\n        mymodel = mymodel.to(device)\n\n\n    \n    predicted = mymodel(t_images) \n#     print(predicted)\n#     print(t_ids)\n\n    if len(predicted[0]['labels'])==0:\n        label = 14\n        pred_str = \"14 1 0 0 1 1\"\n        \n    else:\n        \n        label = predicted[0][\"labels\"]\n        bbox = predicted[0][\"boxes\"].to('cpu').detach().numpy().astype(np.int32)\n        score = predicted[0][\"scores\"]\n    \n#     print(\"predicted label : {}\".format(label))\n#     print(\"predicted bbox : {}\".format(bbox))\n#     print(\"prediction score : {}\".format(score))\n    \n        thr = 10 if len(label)>=10 else len(label)\n        \n        for i in range(thr):\n#             print(\"add\")\n            l = label[i]\n            bb = bbox[i]\n            sc = score[i]\n    \n            if l == 0:\n                l = 14\n                pred_str += \"14 1 0 0 1 1 \"\n\n            else:\n                l -= 1\n                pred_str += \"{} {} {} {} {} {} \".format(l, sc, bb[0],bb[1],bb[2],bb[3])\n\n    sub_preds[str(t_ids[0])] = pred_str\n    print(\"prediction : {}\".format(pred_str))\n    \n    \n    t+=1\n#     if t==3:\n#         break","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03c220b6-a1b0-4c2c-bd72-85ada6261a14","_cell_guid":"2b35cb87-164a-4d09-b216-f9986f90eff9","trusted":true},"cell_type":"code","source":"print(sub_preds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd793a51-b99b-40b1-ad49-8094e2a4091d","_cell_guid":"be3f4846-5399-4b6c-a5ff-089a5c37424a","trusted":true},"cell_type":"code","source":"# set preds in sub csv file\nmy_sub = pd.DataFrame({\"image_id\":sub_preds.keys(),\"PredictionString\":sub_preds.values()})\nmy_sub.iloc[:,:200]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed73089c-0b39-4d81-aa99-4ddbe6e5c531","_cell_guid":"a3753eb9-4abb-484e-89cf-a64ab65333ba","trusted":true},"cell_type":"code","source":"my_sub.to_csv(\"my_sub_all_scs1_no_resize.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}