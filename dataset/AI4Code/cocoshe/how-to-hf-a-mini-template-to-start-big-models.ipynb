{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# How to ðŸ¤—'Hugging Face' â€”â€” A mini and basic template to start with Pre-trained Models\n> Take regression model for example","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-14T08:15:04.1625Z","iopub.execute_input":"2022-05-14T08:15:04.162862Z"}}},{"cell_type":"markdown","source":"## 1. import packages","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Choose your favourite pretrained model in huggingface","metadata":{}},{"cell_type":"code","source":"# for example\npretrained_model_path = '../input/huggingface-bert-variants/bert-base-cased/bert-base-cased'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Dataset class\n1. inherit from torch.utils.data.Dataset\n2. implement `__init__`, `__getitem__` and `__len__` methods\n3. `__getitem__` method should do these things:\n    1. tokenize the sentence\n    2. convert tokenized sentence to tensor\n    3. return the tensor and label(if you have the label)\n","metadata":{}},{"cell_type":"code","source":"class Mydataset(Dataset):\n    def __init__(self, df, pretrained_model_path):\n        self.df = df\n        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_path)\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx, train=True):\n        sample = self.df.iloc[idx].reset_index(drop=True) # get the idx row of df\n        context = sample['here is the context']\n\n        if train: # if train, we would get the label, but if test, we would not\n            label = sample['here is the label'].astype(np.float32)\n        # call tokenizer to encode the context\n        tokens = self.tokenizer(context, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n\n        # get input_ids, attention_mask from tokens (or you can just return the tokens and split it later)\n        input_ids = tokens['input_ids'].squeeze(0)\n        attention_mask = tokens['attention_mask'].squeeze(0)\n\n        # return the input_ids, attention_mask, label\n        if train:\n            return input_ids, attention_mask, label\n        else:\n            return input_ids, attention_mask\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Model class\n1. inherit from torch.nn.Module\n2. implement `__init__`, `forward`\n3. the `forward` will be called when `model(input_ids, attention_mask)`\n","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, pretrained_model_path):\n        super(Model, self).__init__()\n        self.bert = AutoModel.from_pretrained(pretrained_model_path)\n        self.down = nn.Linear(768, 1)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        last_hidden_states = outputs['last_hidden_state']  # now we got the last hidden state like [batch_size, seq_len, hidden_size], the hidden_size is 768 in bert-base\n        down = self.down(last_hidden_states) # now we got the logits like [batch_size, seq_len, 1]\n        # now we got the logits like [batch_size, 1], but to adjust the label dimension, we need to squeeze the first dimension[batch_size], we do this\n        logits = down[:, 0, :].squeeze(-1)\n\n        return logits\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### Attention:\nWe only use the first token in `logits = down[:, 0, :]` because [CLS] stands for the whole sentence meaning.\n\nBut why? That's because it's used for NSP(Next Sentence Prediction) task **when the BERT was pretraining**,\nSo the first token \"[CLS]\" contains more **high dimensional information** than the other tokens.\nWhich means you can also use the second token like `logits = down[:, 1, :]` or any other token, even sum them up.\nBut seldom we use the other tokens, because they are not trained to be use such kind of task.(They preform better in other tasks like fill-mask)\n\n> You can click [here](https://huggingface.co/docs/transformers/main/en/model_doc/bert#transformers.BertModel.forward.returns) for more details about the model output.\n","metadata":{}},{"cell_type":"markdown","source":"## 5. Load data && Split dataset && Get the parameter done\n","metadata":{}},{"cell_type":"code","source":"# load data\ndf = pd.read_csv('to/you/data/path/train.csv')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = Model(pretrained_model_path).to(device)\ndf_dataset = Mydataset(df, pretrained_model_path)\n# train params\nEPOCHS = 1\nBS = 32\n# generate loaders\nfrom sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\ntrain_dataset = Mydataset(train_df, pretrained_model_path)\nval_dataset = Mydataset(val_df, pretrained_model_path)\ntrain_dataloader = DataLoader(train_dataset, batch_size=BS, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=BS, shuffle=True)\n\n# optimizer && loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\ncriterion = nn.MSELoss()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Pytorch origin style train loop\n> Surely you can use pytorch lightning or the Trainer API of huggingface to train your model,but I prefer to write my own train loop.\nIt's more readable and easy to understand, and more flexible.\n","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef train(epoch):\n    global step\n    model.train()\n    for epoch in range(EPOCHS):\n        total_loss = 0\n        for step, batch in enumerate(tqdm(train_dataloader)):\n            # get batch from __getitem__ of dataset class\n            # and put them to device(CPU or GPU)\n            input_ids, attention_mask, label = batch\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            label = label.to(device)\n            # forward\n            logits = model(input_ids, attention_mask)\n            # calculate loss && do backward\n            optimizer.zero_grad()\n            loss = criterion(logits, label)\n            loss.backward()\n            # update parameters\n            optimizer.step()\n            total_loss += loss.item() # accumulate loss\n\n        print(f'Epoch {epoch+1}/{EPOCHS} loss: {total_loss/step}')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Pytorch origin style evaluate\n","metadata":{}},{"cell_type":"code","source":"def evaluate():\n    model.eval()\n    total_loss = 0\n    with torch.no_grad(): # no need to calculate the gradient\n        for step, batch in enumerate(tqdm(val_dataloader)):\n            # get batch from __getitem__ of dataset class\n            # and put them to device(CPU or GPU)\n            input_ids, attention_mask, label = batch\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            label = label.to(device)\n            # forward\n            logits = model(input_ids, attention_mask)\n            # calculate loss\n            loss = criterion(logits, label)\n            # record the loss\n            total_loss += loss.item()\n    # return average loss\n    return total_loss / len(val_dataloader)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Do what you want\n","metadata":{}},{"cell_type":"code","source":"def my_process():\n    train(EPOCHS)\n    val_loss = evaluate()\n    print(f'val_loss: {val_loss}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9.Main entry","metadata":{}},{"cell_type":"code","source":"if __name__ == '__main__':\n    my_process()","metadata":{},"execution_count":null,"outputs":[]}]}