{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport random\nfrom pathlib import Path\n\nimport pandas as pd\nimport plotly_express as px\nfrom IPython.display import display\nfrom plotly.offline import init_notebook_mode\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom wordcloud import WordCloud","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-31T01:22:11.660417Z","iopub.execute_input":"2022-05-31T01:22:11.661353Z","iopub.status.idle":"2022-05-31T01:22:11.666801Z","shell.execute_reply.started":"2022-05-31T01:22:11.661303Z","shell.execute_reply":"2022-05-31T01:22:11.665929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"init_notebook_mode(connected=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:22:11.6921Z","iopub.execute_input":"2022-05-31T01:22:11.69298Z","iopub.status.idle":"2022-05-31T01:22:11.698377Z","shell.execute_reply.started":"2022-05-31T01:22:11.692922Z","shell.execute_reply":"2022-05-31T01:22:11.697746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Format conversion","metadata":{}},{"cell_type":"code","source":"def read_notebook(json_notebook, orderings):\n    \"\"\"\n    Read .json notebook in AI4Code/train folder.\n    Args:\n        json_notebook: Path to .json notebook.\n        orderings: AI4Code/train_orders.csv as `pd.DataFrame` having\n            `cell_order` as index.\n\n    Returns:\n        `notebook_id`: Notebook id in `json_notebook` ex: '0a0ae595d33e5e'.\n        `notebook`: Notebook as dict.\n        `ordering`: Value in `cell_order` in train_orders.csv.\n    \"\"\"\n    notebook_id = Path(json_notebook).stem\n    ordering = orderings.loc[notebook_id]['cell_order']\n    with open(json_notebook) as raw:\n        notebook = json.load(raw)\n    return notebook_id, notebook, ordering\n\n\ndef get_notebook_cells(ordering, notebook):\n    \"\"\"\n    Assign notebook cells to their respective order, add metadata\n        for .ipynb generation.\n    Args:\n        `ordering`: Value in `cell_order` in train_orders.csv.\n        `notebook`: Notebook as dict.\n\n    Returns:\n        cells: A list of dict. Each dict is a cell.\n    \"\"\"\n    cells = []\n    for i, cell_id in enumerate(ordering.split(), 1):\n        cell = dict()\n        cell['id'] = cell_id\n        cell['cell_type'] = notebook['cell_type'][cell_id]\n        cell['source'] = notebook['source'][cell_id]\n        cell['metadata'] = {}\n        cell['outputs'] = []\n        cell['execution_count'] = i\n        cells.append(cell)\n    return cells\n\n\ndef make_displayable(json_notebook, base_notebook, orderings):\n    \"\"\"\n    Convert raw .json notebooks to browser-friendly format.\n    Args:\n        json_notebook: Path to .json notebook.\n        base_notebook: A dictionary of cell metadata.\n        orderings: AI4Code/train_orders.csv as `pd.DataFrame` having\n            `cell_order` as index.\n\n    Returns:\n        Updated notebook as dict.\n    \"\"\"\n    _, notebook, ordering = read_notebook(json_notebook, orderings)\n    base_notebook['cells'] = get_notebook_cells(ordering, notebook)\n    return base_notebook\n\n\ndef create_df(src, orderings, ancestors, verbose=True):\n    \"\"\"\n    Convert .json notebooks and corresponding data to `pd.DataFrame`\n    Args:\n        src: Path to `AI4Code/train`\n        orderings: Path to `AI4Code/train_orders.csv`\n        ancestors: Path to `AI4Code/train_ancestors.csv`\n        verbose: If False, progress will not be displayed.\n\n    Returns:\n        Converted dataset as `pd.DataFrame` having columns:\n        [`cell_id`, `cell_type` `source`, `order` `notebook_id`\n            `ancestor_id`, `parent_id`]\n    \"\"\"\n    orderings = pd.read_csv(orderings).set_index('id')\n    ancestors = pd.read_csv(ancestors).set_index('id')\n    data = []\n    json_notebooks = [*Path(src).glob('*.json')]\n    total_notebooks = len(json_notebooks)\n    for i, json_notebook in enumerate(json_notebooks, 1):\n        notebook_id, notebook, ordering = read_notebook(\n            json_notebook.as_posix(), orderings\n        )\n        ancestor_id, parent_id = ancestors.loc[notebook_id][\n            ['ancestor_id', 'parent_id']\n        ]\n        cells = get_notebook_cells(ordering, notebook)\n        df = (\n            pd.DataFrame(cells)\n            .rename(columns={'id': 'cell_id', 'execution_count': 'order'})\n            .drop(['metadata', 'outputs'], axis=1)\n        )\n        df['notebook_id'] = notebook_id\n        df['ancestor_id'] = ancestor_id\n        df['parent_id'] = parent_id\n        data.append(df)\n        if verbose:\n            print(f'\\rParsed {i}/{total_notebooks} files', end='')\n    if verbose:\n        print()\n    return pd.concat(data)\n\n\ndef create_nb_samples(src, dest, orderings, metadata, n=50, verbose=True, **kwargs):\n    \"\"\"\n    Convert a number of .json notebooks in `AI4Code/train` to browser-friendly\n        format and save the results as .ipynb.\n    Args:\n        src: Path to `AI4Code/train`.\n        dest: Path to output directory where the notebooks will be saved.\n        orderings: Path to `AI4Code/train_orders.csv`.\n        metadata: Path to `ipynb-metadata.json`.\n        n: Total notebooks to convert.\n        verbose: If False, progress will not be displayed.\n        **kwargs: kwargs passed to `json.dumps`\n\n    Returns:\n        None\n    \"\"\"\n    Path(dest).mkdir(exist_ok=True, parents=True)\n    orderings = pd.read_csv(orderings).set_index('id')\n    json_files = random.sample([*Path(src).glob('*.json')], n)\n    total = len(json_files)\n    with open(metadata) as base:\n        base_notebook = json.load(base)\n    for i, f in enumerate(json_files, 1):\n        notebook = make_displayable(f.as_posix(), base_notebook, orderings)\n        with open((Path(dest) / f'{f.stem}.ipynb').as_posix(), 'w') as converted:\n            converted.write(json.dumps(notebook, **kwargs))\n            if verbose:\n                print(f'\\rSaved {i}/{total} notebooks', end='')\n    if verbose:\n        print()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:22:11.73272Z","iopub.execute_input":"2022-05-31T01:22:11.73335Z","iopub.status.idle":"2022-05-31T01:22:11.759727Z","shell.execute_reply.started":"2022-05-31T01:22:11.733298Z","shell.execute_reply":"2022-05-31T01:22:11.758884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conversion Json --> Parquet, IPYNB\n\nThe following section parses the .json files and converts to a pandas dataframe that has the following columns:\n\n* cell_id\n* cell_type\n* source\n* order\n* notebook_id\n* ancestor_id\n* parent_id\n\nThen results are saved to `ai4code.parquet` for further processing.","metadata":{}},{"cell_type":"code","source":"%%time\n\ndf = create_df(\n    '../input/AI4Code/train',\n    '../input/AI4Code/train_orders.csv',\n    '../input/AI4Code/train_ancestors.csv',\n)\ndf.to_parquet('ai4code.parquet', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:22:11.76162Z","iopub.execute_input":"2022-05-31T01:22:11.761973Z","iopub.status.idle":"2022-05-31T01:23:29.449441Z","shell.execute_reply.started":"2022-05-31T01:22:11.761916Z","shell.execute_reply":"2022-05-31T01:23:29.448698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:23:29.45056Z","iopub.execute_input":"2022-05-31T01:23:29.451391Z","iopub.status.idle":"2022-05-31T01:23:29.785904Z","shell.execute_reply.started":"2022-05-31T01:23:29.451332Z","shell.execute_reply":"2022-05-31T01:23:29.783997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:23:29.787338Z","iopub.status.idle":"2022-05-31T01:23:29.788416Z","shell.execute_reply.started":"2022-05-31T01:23:29.788096Z","shell.execute_reply":"2022-05-31T01:23:29.78813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Create sample notebooks that can be viewed in browser","metadata":{}},{"cell_type":"code","source":"create_nb_samples(\n    '../input/AI4Code/train',\n    'notebook-samples',\n    '../input/AI4Code/train_orders.csv',\n    '../input/ai4code-ipynb-metadata/ipynb-metadata.json',\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:23:29.790016Z","iopub.status.idle":"2022-05-31T01:23:29.790674Z","shell.execute_reply.started":"2022-05-31T01:23:29.790396Z","shell.execute_reply":"2022-05-31T01:23:29.790426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Resulting notebooks are saved to `notebook-samples.zip`","metadata":{}},{"cell_type":"markdown","source":"## Preprocessing & EDA","metadata":{}},{"cell_type":"code","source":"def plot_source_counts(df, **kwargs):\n    \"\"\"\n    Display code/markdown frequencies.\n    Args:\n        df: ai4code dataset as `pd.DataFrame` having columns:\n            [`cell_id`, `cell_type` `source`, `order` `notebook_id`\n            `ancestor_id`, `parent_id`]\n        **kwargs: kwargs passed to `plotly.express._chart_types.pie`\n\n    Returns:\n        plotly.graph_objs._figure.Figure\n    \"\"\"\n    title = 'Source frequencies'\n    counts = df['cell_type'].value_counts()\n    return px.pie(\n        counts, names=counts.index, values=counts.values, title=title, **kwargs\n    )\n\n\ndef plot_ngram(frequencies, n=None, **kwargs):\n    \"\"\"\n    Plot ngram word frequencies.\n    Args:\n        frequencies: `pd.DataFrame` having columns: `word`, `frequency`.\n        n: `n` passed to `pd.DataFrame.head`.\n        **kwargs: kwargs passed to plotly.express._chart_types.histogram.\n\n    Returns:\n        plotly.graph_objs._figure.Figure\n    \"\"\"\n    frequencies = frequencies.head(n).sort_values(by='frequency')\n    return px.histogram(frequencies, x='frequency', y='word', **kwargs)\n\n\ndef generate_ngram(df, cell_type, **kwargs):\n    \"\"\"\n    Generate ngram word frequencies.\n    Args:\n        df: ai4code dataset as `pd.DataFrame` processed by\n            `ai4code.preprocessing.cleanup_text` having columns:\n            [`cell_id`, `cell_type` `source`, `order` `notebook_id`\n            `ancestor_id`, `parent_id`]\n        cell_type: str, `code` or `markdown`.\n        **kwargs: kwargs passed to `sklearn.feature_extraction.text.CountVectorizer`\n\n    Returns:\n        frequencies: `pd.DataFrame` having columns: `word`, `frequency`.\n    \"\"\"\n    corpus = df[df['cell_type'] == cell_type]['source']\n    vec = CountVectorizer(**kwargs)\n    vec.fit(corpus)\n    word_sum = vec.transform(corpus).sum(axis=0)\n    frequencies = pd.Series(\n        {word: word_sum[0, idx] for word, idx in vec.vocabulary_.items()}\n    ).reset_index()\n    frequencies.columns = ['word', 'frequency']\n    return frequencies.sort_values(by='frequency', ascending=False)\n\n\ndef generate_wordcloud(frequencies, **kwargs):\n    \"\"\"\n    Generate word cloud given word frequencies.\n    Args:\n        frequencies: `pd.DataFrame` having columns: `word`, `frequency`.\n        **kwargs: kwargs passed to `wordcloud.wordcloud.WordCloud`.\n\n    Returns:\n        `wordcloud.wordcloud.WordCloud`\n    \"\"\"\n    wc = WordCloud(**kwargs)\n    wc.fit_words(dict(frequencies.values))\n    return wc\n\n\ndef cleanup_text(df):\n    \"\"\"\n    Cleanup `source` in place.\n    Args:\n        df: ai4code dataset as `pd.DataFrame` having columns:\n            [`cell_id`, `cell_type` `source`, `order` `notebook_id`\n            `ancestor_id`, `parent_id`]\n    Returns:\n        None\n    \"\"\"\n    replacements = {\n        r'http\\S+': '',\n        r'<.*?>': '',\n        r'\\d+': '',\n        r'[^\\x00-\\x7F]+': '',\n        r'don\\'t': 'do not',\n        r'won\\'t': 'will not',\n        r'can\\'t': 'cannot',\n        r'i\\'m': 'i am',\n        r'n\\'t': ' not',\n        r'\\'re': ' are',\n        r'\\'s': ' is',\n        r'\\'d': ' would',\n        r'\\'ll': ' will',\n        r'\\'ve': ' have',\n        r'\\'m': ' am',\n        r'\\r': '',\n        r'[.|,|)|(|\\|/]': ' ',\n        r'^\\s+': '',\n        r' +': ' ',\n    }\n    df['source'].str.lower().replace({r'\\’': \"\\'\"}, regex=True, inplace=True)\n    df['source'].replace(replacements, regex=True, inplace=True)\n\n\ndef generate_ngrams(df, ranges, labels, cell_type):\n    \"\"\"\n    Generate and save word frequencies.\n    Args:\n        df: ai4code dataset as `pd.DataFrame` having columns:\n            [`cell_id`, `cell_type` `source`, `order` `notebook_id`\n            `ancestor_id`, `parent_id`]\n        ranges: A list of pairs passed to `ai4code.preprocessing.create_ngram`\n                each. ex: [(1, 1), (2, 2)]\n        labels: A list titles having the same size as `ranges` ex: ['unigram', 'bigram']\n        cell_type: str, `code` or `markdown`.\n\n    Raises: AssertionError for length mismatch.\n\n    Returns:\n        A list of `pd.DataFrame`\n    \"\"\"\n    assert len(ranges) == len(labels), (\n        f'`ranges` and `labels` should have equal sizes, got '\n        f'{len(ranges), len(labels)}'\n    )\n    frequencies = []\n    for n, label in zip(ranges, labels):\n        freq = generate_ngram(df, cell_type, ngram_range=n, stop_words='english')\n        frequencies.append(freq)\n        freq.to_parquet(\n            f'{label}_{cell_type}_frequencies.parquet',\n            index=False,\n        )\n    return frequencies","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:23:29.792395Z","iopub.status.idle":"2022-05-31T01:23:29.793102Z","shell.execute_reply.started":"2022-05-31T01:23:29.792827Z","shell.execute_reply":"2022-05-31T01:23:29.792859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ncleanup_text(df)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:23:29.794973Z","iopub.status.idle":"2022-05-31T01:23:29.795463Z","shell.execute_reply.started":"2022-05-31T01:23:29.795199Z","shell.execute_reply":"2022-05-31T01:23:29.795227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plot_source_counts(df, template='plotly_dark')\nfig.write_html('source-frequencies.html')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:23:29.796901Z","iopub.status.idle":"2022-05-31T01:23:29.797369Z","shell.execute_reply.started":"2022-05-31T01:23:29.797118Z","shell.execute_reply":"2022-05-31T01:23:29.797145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nngram_ranges = (1, 1), (2, 2), (3, 3)\nngram_labels = 'unigram', 'bigram', 'trigram'\nfrequencies = generate_ngrams(df, ngram_ranges, ngram_labels, 'markdown')\nfor label, freq_df in zip(ngram_labels, frequencies):\n    title = f'Markdown {label} frequencies'\n    fig = plot_ngram(\n        freq_df[freq_df['frequency'] > 5],\n        50,\n        template='plotly_dark',\n        title=title,\n    )\n    fig.write_html(f\"{title.lower().replace(' ', '-')}.html\")\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:23:29.799091Z","iopub.status.idle":"2022-05-31T01:23:29.799579Z","shell.execute_reply.started":"2022-05-31T01:23:29.799317Z","shell.execute_reply":"2022-05-31T01:23:29.799344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wc = generate_wordcloud(frequencies[0], width=2500, height=1200)\nwc.to_file('word-cloud-markdown.jpg')\ndisplay(wc.to_image())","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:23:29.800959Z","iopub.status.idle":"2022-05-31T01:23:29.801428Z","shell.execute_reply.started":"2022-05-31T01:23:29.801173Z","shell.execute_reply":"2022-05-31T01:23:29.801199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nngram_ranges = (1, 1), (2, 2)\nngram_labels = 'unigram', 'bigram'\nfrequencies = generate_ngrams(df, ngram_ranges, ngram_labels, 'code')\nfor label, freq_df in zip(ngram_labels, frequencies):\n    title = f'Code {label} frequencies'\n    fig = plot_ngram(\n        freq_df[freq_df['frequency'] > 5],\n        50,\n        template='plotly_dark',\n        title=title,\n    )\n    fig.write_html(f\"{title.lower().replace(' ', '-')}.html\")\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:23:29.802931Z","iopub.status.idle":"2022-05-31T01:23:29.803395Z","shell.execute_reply.started":"2022-05-31T01:23:29.803144Z","shell.execute_reply":"2022-05-31T01:23:29.80317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wc = generate_wordcloud(frequencies[0], width=2500, height=1200)\nwc.to_file('word-cloud-code.jpg')\ndisplay(wc.to_image())","metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:23:29.8049Z","iopub.status.idle":"2022-05-31T01:23:29.805372Z","shell.execute_reply.started":"2022-05-31T01:23:29.80512Z","shell.execute_reply":"2022-05-31T01:23:29.805146Z"},"trusted":true},"execution_count":null,"outputs":[]}]}