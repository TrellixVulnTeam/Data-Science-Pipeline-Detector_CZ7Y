{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!mkdir 'raw' 'tfrec'","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:50:23.05591Z","iopub.execute_input":"2022-06-24T05:50:23.056966Z","iopub.status.idle":"2022-06-24T05:50:23.893227Z","shell.execute_reply.started":"2022-06-24T05:50:23.056928Z","shell.execute_reply":"2022-06-24T05:50:23.891443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimport json\nimport os\nfrom typing import List\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport transformers\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.utils import shuffle\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:50:39.391587Z","iopub.execute_input":"2022-06-24T05:50:39.392197Z","iopub.status.idle":"2022-06-24T05:50:42.728134Z","shell.execute_reply.started":"2022-06-24T05:50:39.392155Z","shell.execute_reply":"2022-06-24T05:50:42.727173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RANDOM_STATE = 42\nMD_MAX_LEN = 64\nTOTAL_MAX_LEN = 512\nK_FOLDS = 5\nFILES_PER_FOLD = 16\nLIMIT = 1_000 if os.environ[\"KAGGLE_KERNEL_RUN_TYPE\"] == \"Interactive\" else None\nMODEL_NAME = \"microsoft/codebert-base\"\nTOKENIZER = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\nINPUT_PATH = \"../input/AI4Code\"","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:51:16.813236Z","iopub.execute_input":"2022-06-24T05:51:16.814011Z","iopub.status.idle":"2022-06-24T05:51:23.762431Z","shell.execute_reply.started":"2022-06-24T05:51:16.813972Z","shell.execute_reply":"2022-06-24T05:51:23.761123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_notebook(path: str) -> pd.DataFrame:\n    return (\n        pd.read_json(path, dtype={\"cell_type\": \"category\", \"source\": \"str\"})\n        .assign(id=os.path.basename(path).split(\".\")[0])\n        .rename_axis(\"cell_id\")\n    )\n\n\ndef clean_code(cell: str) -> str:\n    return str(cell).replace(\"\\\\n\", \"\\n\")\n\n\ndef sample_cells(cells: List[str], n: int) -> List[str]:\n    cells = [clean_code(cell) for cell in cells]\n    if n >= len(cells):\n        return cells\n    else:\n        results = []\n        step = len(cells) / n\n        idx = 0\n        while int(np.round(idx)) < len(cells):\n            results.append(cells[int(np.round(idx))])\n            idx += step\n        if cells[-1] not in results:\n            results[-1] = cells[-1]\n        return results\n\n\ndef get_features(df: pd.DataFrame) -> dict:\n    features = {}\n    for i, sub_df in tqdm(df.groupby(\"id\"), desc=\"Features\"):\n        features[i] = {}\n        total_md = sub_df[sub_df.cell_type == \"markdown\"].shape[0]\n        code_sub_df = sub_df[sub_df.cell_type == \"code\"]\n        total_code = code_sub_df.shape[0]\n        codes = sample_cells(code_sub_df.source.values, 20)\n        features[i][\"total_code\"] = total_code\n        features[i][\"total_md\"] = total_md\n        features[i][\"codes\"] = codes\n    return features\n\n\ndef tokenize(df: pd.DataFrame, fts: dict) -> dict:\n    input_ids = np.zeros((len(df), TOTAL_MAX_LEN), dtype=np.int32)\n    attention_mask = np.zeros((len(df), TOTAL_MAX_LEN), dtype=np.int32)\n    features = np.zeros((len(df),), dtype=np.float32)\n    labels = np.zeros((len(df),), dtype=np.float32)\n\n    for i, row in tqdm(\n        df.reset_index(drop=True).iterrows(), desc=\"Tokens\", total=len(df)\n    ):\n        row_fts = fts[row.id]\n\n        inputs = TOKENIZER.encode_plus(\n            row.source,\n            None,\n            add_special_tokens=True,\n            max_length=MD_MAX_LEN,\n            padding=\"max_length\",\n            return_token_type_ids=True,\n            truncation=True,\n        )\n        code_inputs = TOKENIZER.batch_encode_plus(\n            [str(x) for x in row_fts[\"codes\"]] or [\"\"],\n            add_special_tokens=True,\n            max_length=23,\n            padding=\"max_length\",\n            truncation=True,\n        )\n\n        ids = inputs[\"input_ids\"]\n        for x in code_inputs[\"input_ids\"]:\n            ids.extend(x[:-1])\n        ids = ids[:TOTAL_MAX_LEN]\n        if len(ids) != TOTAL_MAX_LEN:\n            ids = ids + [\n                TOKENIZER.pad_token_id,\n            ] * (TOTAL_MAX_LEN - len(ids))\n\n            mask = inputs[\"attention_mask\"]\n        for x in code_inputs[\"attention_mask\"]:\n            mask.extend(x[:-1])\n        mask = mask[:TOTAL_MAX_LEN]\n        if len(mask) != TOTAL_MAX_LEN:\n            mask = mask + [\n                TOKENIZER.pad_token_id,\n            ] * (TOTAL_MAX_LEN - len(mask))\n\n        input_ids[i] = ids\n        attention_mask[i] = mask\n        features[i] = (\n            row_fts[\"total_md\"] / (row_fts[\"total_md\"] + row_fts[\"total_code\"]) or 1\n        )\n        labels[i] = row.pct_rank\n\n        return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"features\": features,\n        \"labels\": labels,\n    }\n\n\ndef get_ranks(base: pd.Series, derived: List[str]) -> List[str]:\n    return [base.index(d) for d in derived]\n\n\ndef _serialize_sample(\n    input_ids: np.array,\n    attention_mask: np.array,\n    feature: np.float64,\n    label: np.float64,\n) -> bytes:\n    feature = {\n        \"input_ids\": tf.train.Feature(int64_list=tf.train.Int64List(value=input_ids)),\n        \"attention_mask\": tf.train.Feature(\n            int64_list=tf.train.Int64List(value=attention_mask)\n        ),\n        \"feature\": tf.train.Feature(float_list=tf.train.FloatList(value=[feature])),\n        \"label\": tf.train.Feature(float_list=tf.train.FloatList(value=[label])),\n    }\n    sample = tf.train.Example(features=tf.train.Features(feature=feature))\n    return sample.SerializeToString()\n\n\ndef serialize(\n    input_ids: np.array,\n    attention_mask: np.array,\n    features: np.array,\n    labels: np.array,\n    path: str,\n) -> None:\n     with tf.io.TFRecordWriter(path) as writer:\n        for args in zip(input_ids, attention_mask, features, labels):\n            writer.write(_serialize_sample(*args))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:53:45.940707Z","iopub.execute_input":"2022-06-24T05:53:45.941273Z","iopub.status.idle":"2022-06-24T05:53:45.977837Z","shell.execute_reply.started":"2022-06-24T05:53:45.941235Z","shell.execute_reply":"2022-06-24T05:53:45.976339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths = glob.glob(os.path.join(INPUT_PATH, \"train\", \"*.json\"))\nif LIMIT is not None:\n    paths = paths[:LIMIT]\ndf = (\n    pd.concat([read_notebook(x) for x in tqdm(paths, desc=\"Concat\")])\n    .set_index(\"id\", append=True)\n    .swaplevel()\n    .sort_index(level=\"id\", sort_remaining=False)\n)\n\ndf_orders = pd.read_csv(\n    os.path.join(INPUT_PATH, \"train_orders.csv\"),\n    index_col=\"id\",\n    squeeze=True,\n).str.split()\ndf_orders_ = df_orders.to_frame().join(\n    df.reset_index(\"cell_id\").groupby(\"id\")[\"cell_id\"].apply(list),\n    how=\"right\",\n)\n\nranks = {}\nfor id_, cell_order, cell_id in df_orders_.itertuples():\n    ranks[id_] = {\"cell_id\": cell_id, \"rank\": get_ranks(cell_order, cell_id)}\ndf_ranks = (\n    pd.DataFrame.from_dict(ranks, orient=\"index\")\n    .rename_axis(\"id\")\n    .apply(pd.Series.explode)\n    .set_index(\"cell_id\", append=True)\n)\n\ndf_ancestors = pd.read_csv(\n    os.path.join(INPUT_PATH, \"train_ancestors.csv\"), index_col=\"id\"\n)\ndf = (\n    df.reset_index()\n    .merge(df_ranks, on=[\"id\", \"cell_id\"])\n    .merge(df_ancestors, on=[\"id\"])\n)\n\ndf[\"pct_rank\"] = df[\"rank\"] / df.groupby(\"id\")[\"cell_id\"].transform(\"count\")\ndf = df.sort_values(\"pct_rank\").reset_index(drop=True)\n\nfeatures = get_features(df)\n\ndf = df[df[\"cell_type\"] == \"markdown\"]\ndf = df.drop([\"rank\", \"parent_id\", \"cell_type\"], axis=1).dropna()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:54:27.368743Z","iopub.execute_input":"2022-06-24T05:54:27.369267Z","iopub.status.idle":"2022-06-24T05:54:45.397819Z","shell.execute_reply.started":"2022-06-24T05:54:27.369228Z","shell.execute_reply":"2022-06-24T05:54:45.396541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv(\"data.csv\")\nwith open(\"features.json\", \"w\") as file:\n    json.dump(features, file)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:54:50.059527Z","iopub.execute_input":"2022-06-24T05:54:50.060405Z","iopub.status.idle":"2022-06-24T05:54:50.419521Z","shell.execute_reply.started":"2022-06-24T05:54:50.060363Z","shell.execute_reply":"2022-06-24T05:54:50.418287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = shuffle(df, random_state=RANDOM_STATE)\n\nfor fold, (_, split) in enumerate(\n    GroupKFold(K_FOLDS).split(df, groups=df[\"ancestor_id\"])\n):\n    print(\"=\" * 36, f\"Fold {fold}\", \"=\" * 36)\n    fold_dir = f\"tfrec/{fold}\"\n    if not os.path.exists(fold_dir):\n        os.mkdir(fold_dir)\n\n    data = tokenize(df.iloc[split], features)\n\n    np.savez_compressed(\n        f\"raw/{fold}.npz\",\n        input_ids=data[\"input_ids\"],\n        attention_mask=data[\"attention_mask\"],\n        features=data[\"features\"],\n        labels=data[\"labels\"],\n    )\n\n    for split, index in tqdm(\n        enumerate(np.array_split(np.arange(data[\"labels\"].shape[0]), FILES_PER_FOLD)),\n        desc=f\"Saving\",\n        total=FILES_PER_FOLD,\n    ):\n        serialize(\n            input_ids=data[\"input_ids\"][index],\n            attention_mask=data[\"attention_mask\"][index],\n            features=data[\"features\"][index],\n            labels=data[\"labels\"][index],\n            path=os.path.join(fold_dir, f\"{split:02d}-{len(index):06d}.tfrec\"),\n        )","metadata":{"execution":{"iopub.status.busy":"2022-06-24T06:06:28.391679Z","iopub.execute_input":"2022-06-24T06:06:28.392182Z","iopub.status.idle":"2022-06-24T06:06:36.727936Z","shell.execute_reply.started":"2022-06-24T06:06:28.392145Z","shell.execute_reply":"2022-06-24T06:06:36.726417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom typing import List\n\nimport numpy as np\nimport tensorflow as tf\nimport transformers\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2022-06-24T06:06:22.647601Z","iopub.execute_input":"2022-06-24T06:06:22.647956Z","iopub.status.idle":"2022-06-24T06:06:22.654001Z","shell.execute_reply.started":"2022-06-24T06:06:22.647924Z","shell.execute_reply":"2022-06-24T06:06:22.653142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gsutil ls $GCS_PATH","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:58:25.869202Z","iopub.execute_input":"2022-06-24T05:58:25.869806Z","iopub.status.idle":"2022-06-24T05:58:30.566525Z","shell.execute_reply.started":"2022-06-24T05:58:25.869768Z","shell.execute_reply":"2022-06-24T05:58:30.564741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cmd","metadata":{"execution":{"iopub.status.busy":"2022-06-24T06:00:10.902042Z","iopub.execute_input":"2022-06-24T06:00:10.902536Z","iopub.status.idle":"2022-06-24T06:00:11.690202Z","shell.execute_reply.started":"2022-06-24T06:00:10.902498Z","shell.execute_reply":"2022-06-24T06:00:11.68898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RANDOM_STATE = 42\nN_SPLITS = 5\nTOTAL_MAX_LEN = 512\nBASE_MODEL = \"microsoft/codebert-base\"\nGCS_PATH = KaggleDatasets().get_gcs_path(\"AI4Code\")\nEPOCHS = 5\nLR = 3e-5\nWARMUP_RATE = 0.05\nVERBOSE = 1 if os.environ[\"KAGGLE_KERNEL_RUN_TYPE\"] == \"Interactive\" else 2\n\ntry:\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    STRATEGY = tf.distribute.experimental.TPUStrategy(TPU)\n    BATCH_SIZE = 64 * STRATEGY.num_replicas_in_sync\nexcept Exception:\n    TPU = None\n    STRATEGY = tf.distribute.get_strategy()\n    BATCH_SIZE = 4\nprint(\"TensorFlow\", tf.__version__)\n\nif TPU is not None:\n    print(\"Using TPU v3-8\")\nelse:\n    print(\"Using GPU/CPU\")\n\nprint(\"Batch size:\", BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T06:03:11.943152Z","iopub.execute_input":"2022-06-24T06:03:11.943702Z","iopub.status.idle":"2022-06-24T06:03:12.430555Z","shell.execute_reply.started":"2022-06-24T06:03:11.943663Z","shell.execute_reply":"2022-06-24T06:03:12.429083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, (train_index, val_index) in enumerate(KFold(n_splits=N_SPLITS).split(range(N_SPLITS))):\n    if TPU is not None:\n        tf.tpu.experimental.initialize_tpu_system(TPU)\n\n    train_filenames = np.ravel(\n        [\n            tf.io.gfile.glob(os.path.join(GCS_PATH, \"tfrec\", str(x), \"*.tfrec\"))\n            for x in train_index\n        ]\n    )\n    steps_per_epoch = count_samples(train_filenames) // BATCH_SIZE\n    train_dataset = get_dataset(train_filenames)\n\n    val_filenames = np.ravel(\n        [\n            tf.io.gfile.glob(os.path.join(GCS_PATH, \"tfrec\", str(x), \"*.tfrec\"))\n            for x in val_index\n        ]\n    )\n    validation_steps = count_samples(val_filenames) // BATCH_SIZE\n    val_dataset = get_dataset(val_filenames, ordered=True, repeated=False, cached=True)\n\n    with STRATEGY.scope():\n        model = get_model()\n\n        total_steps = steps_per_epoch * EPOCHS\n        warmup_steps = int(WARMUP_RATE * total_steps)\n\n        optimizer = transformers.AdamWeightDecay(\n            learning_rate=WarmupLinearDecay(\n                base_learning_rate=LR,\n                warmup_steps=warmup_steps,\n                total_steps=total_steps,\n            ),\n            weight_decay_rate=0.01,\n            exclude_from_weight_decay=[\n                \"bias\",\n                \"LayerNorm.bias\",\n                \"LayerNorm.weight\",\n            ],\n        )\n        model.compile(loss=\"mae\", optimizer=optimizer)\n\n    model.fit(\n        train_dataset,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_dataset,\n        validation_steps=validation_steps,\n        epochs=EPOCHS,\n        verbose=VERBOSE,\n    )\n\n    model.save_weights(f\"model_{i}.h5\")\n    break","metadata":{"execution":{"iopub.status.busy":"2022-06-24T06:03:15.418352Z","iopub.execute_input":"2022-06-24T06:03:15.418838Z","iopub.status.idle":"2022-06-24T06:03:16.007348Z","shell.execute_reply.started":"2022-06-24T06:03:15.418803Z","shell.execute_reply":"2022-06-24T06:03:16.005388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}