{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pairwise training\nIn this notebook, we demonstrate how to use pairwise model to predict in this competition. Please note that the inference time is much longer than pointwise method or using cosine similarity. \n\n1. I used a bert-small model pretrained with pairwise-mlm.\n2. **Training with pairwise examples with negative samples randomly sampled.**\n3. Inference and predict for all the pairs for test dataset.\n\n* [Pretrain](https://www.kaggle.com/code/yuanzhezhou/ai4code-pairwise-bertsmall-pretrain/notebook)\n* [Training](https://www.kaggle.com/yuanzhezhou/ai4code-pairwise-bertsmall-training)\n* [Inference](https://www.kaggle.com/yuanzhezhou/ai4code-pairwise-bertsmall-inference)","metadata":{}},{"cell_type":"code","source":"import json\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\nfrom tqdm import tqdm\nimport os\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\npd.options.display.width = 180\npd.options.display.max_colwidth = 120\n\nBERT_PATH = \"../input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased\"\n\ndata_dir = Path('../input/AI4Code')","metadata":{"papermill":{"duration":0.122804,"end_time":"2022-05-12T10:15:14.04297","exception":false,"start_time":"2022-05-12T10:15:13.920166","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-22T04:24:14.530604Z","iopub.execute_input":"2022-05-22T04:24:14.531111Z","iopub.status.idle":"2022-05-22T04:24:14.626974Z","shell.execute_reply.started":"2022-05-22T04:24:14.531022Z","shell.execute_reply":"2022-05-22T04:24:14.626275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_TRAIN = 200\n\n\ndef read_notebook(path):\n    return (\n        pd.read_json(\n            path,\n            dtype={'cell_type': 'category', 'source': 'str'})\n        .assign(id=path.stem)\n        .rename_axis('cell_id')\n    )\n\n\npaths_train = list((data_dir / 'train').glob('*.json'))[:NUM_TRAIN]\nnotebooks_train = [\n    read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')\n]\ndf = (\n    pd.concat(notebooks_train)\n    .set_index('id', append=True)\n    .swaplevel()\n    .sort_index(level='id', sort_remaining=False)\n)\n\ndf","metadata":{"papermill":{"duration":82.291505,"end_time":"2022-05-12T10:16:36.365197","exception":false,"start_time":"2022-05-12T10:15:14.073692","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-22T04:24:14.628871Z","iopub.execute_input":"2022-05-22T04:24:14.629407Z","iopub.status.idle":"2022-05-22T04:24:19.331744Z","shell.execute_reply.started":"2022-05-22T04:24:14.629369Z","shell.execute_reply":"2022-05-22T04:24:19.331069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get an example notebook\nnb_id = df.index.unique('id')[6]\nprint('Notebook:', nb_id)\n\nprint(\"The disordered notebook:\")\nnb = df.loc[nb_id, :]\ndisplay(nb)\nprint()","metadata":{"papermill":{"duration":0.270693,"end_time":"2022-05-12T10:16:36.882443","exception":false,"start_time":"2022-05-12T10:16:36.61175","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-22T04:24:19.333199Z","iopub.execute_input":"2022-05-22T04:24:19.33351Z","iopub.status.idle":"2022-05-22T04:24:19.350694Z","shell.execute_reply.started":"2022-05-22T04:24:19.333474Z","shell.execute_reply":"2022-05-22T04:24:19.350025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_orders = pd.read_csv(\n    data_dir / 'train_orders.csv',\n    index_col='id',\n    squeeze=True,\n).str.split()  # Split the string representation of cell_ids into a list\n\ndf_orders","metadata":{"papermill":{"duration":2.835076,"end_time":"2022-05-12T10:16:39.9675","exception":false,"start_time":"2022-05-12T10:16:37.132424","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-22T04:24:19.352493Z","iopub.execute_input":"2022-05-22T04:24:19.352981Z","iopub.status.idle":"2022-05-22T04:24:22.124825Z","shell.execute_reply.started":"2022-05-22T04:24:19.352943Z","shell.execute_reply":"2022-05-22T04:24:22.124083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_orders.loc[\"002ba502bdac45\"])","metadata":{"papermill":{"duration":0.257536,"end_time":"2022-05-12T10:16:40.472139","exception":false,"start_time":"2022-05-12T10:16:40.214603","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-22T04:24:22.126047Z","iopub.execute_input":"2022-05-22T04:24:22.126304Z","iopub.status.idle":"2022-05-22T04:24:22.133325Z","shell.execute_reply.started":"2022-05-22T04:24:22.126276Z","shell.execute_reply":"2022-05-22T04:24:22.13267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cell_order = df_orders.loc[nb_id]\n\nprint(\"The ordered notebook:\")\nnb.loc[cell_order, :]","metadata":{"papermill":{"duration":0.265934,"end_time":"2022-05-12T10:16:40.98571","exception":false,"start_time":"2022-05-12T10:16:40.719776","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-22T04:24:22.134734Z","iopub.execute_input":"2022-05-22T04:24:22.135236Z","iopub.status.idle":"2022-05-22T04:24:22.154335Z","shell.execute_reply.started":"2022-05-22T04:24:22.135195Z","shell.execute_reply":"2022-05-22T04:24:22.153479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_ranks(base, derived):\n    return [base.index(d) for d in derived]\n\ncell_ranks = get_ranks(cell_order, list(nb.index))\nnb.insert(0, 'rank', cell_ranks)\n\nnb","metadata":{"papermill":{"duration":0.265625,"end_time":"2022-05-12T10:16:41.501618","exception":false,"start_time":"2022-05-12T10:16:41.235993","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-22T04:24:22.155853Z","iopub.execute_input":"2022-05-22T04:24:22.156478Z","iopub.status.idle":"2022-05-22T04:24:22.171751Z","shell.execute_reply.started":"2022-05-22T04:24:22.156441Z","shell.execute_reply":"2022-05-22T04:24:22.170311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_orders_ = df_orders.to_frame().join(\n    df.reset_index('cell_id').groupby('id')['cell_id'].apply(list),\n    how='right',\n)\n\nranks = {}\nfor id_, cell_order, cell_id in df_orders_.itertuples():\n    ranks[id_] = {'cell_id': cell_id, 'rank': get_ranks(cell_order, cell_id)}\n\ndf_ranks = (\n    pd.DataFrame\n    .from_dict(ranks, orient='index')\n    .rename_axis('id')\n    .apply(pd.Series.explode)\n    .set_index('cell_id', append=True)\n)\n\ndf_ranks","metadata":{"papermill":{"duration":2.967892,"end_time":"2022-05-12T10:16:44.752979","exception":false,"start_time":"2022-05-12T10:16:41.785087","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-22T04:24:22.173267Z","iopub.execute_input":"2022-05-22T04:24:22.173563Z","iopub.status.idle":"2022-05-22T04:24:22.271797Z","shell.execute_reply.started":"2022-05-22T04:24:22.173522Z","shell.execute_reply":"2022-05-22T04:24:22.271038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ancestors = pd.read_csv(data_dir / 'train_ancestors.csv', index_col='id')\ndf_ancestors","metadata":{"papermill":{"duration":0.44203,"end_time":"2022-05-12T10:16:45.446006","exception":false,"start_time":"2022-05-12T10:16:45.003976","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-22T04:24:22.273302Z","iopub.execute_input":"2022-05-22T04:24:22.27379Z","iopub.status.idle":"2022-05-22T04:24:22.49484Z","shell.execute_reply.started":"2022-05-22T04:24:22.273749Z","shell.execute_reply":"2022-05-22T04:24:22.494188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.reset_index().merge(df_ranks, on=[\"id\", \"cell_id\"]).merge(df_ancestors, on=[\"id\"])\ndf","metadata":{"papermill":{"duration":1.007951,"end_time":"2022-05-12T10:16:46.70626","exception":false,"start_time":"2022-05-12T10:16:45.698309","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-22T04:24:22.497983Z","iopub.execute_input":"2022-05-22T04:24:22.498198Z","iopub.status.idle":"2022-05-22T04:24:22.571385Z","shell.execute_reply.started":"2022-05-22T04:24:22.498171Z","shell.execute_reply":"2022-05-22T04:24:22.570555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"pct_rank\"] = df[\"rank\"] / df.groupby(\"id\")[\"cell_id\"].transform(\"count\")\n\ndf[\"pct_rank\"].hist(bins=10)","metadata":{"papermill":{"duration":0.862186,"end_time":"2022-05-12T10:16:47.820945","exception":false,"start_time":"2022-05-12T10:16:46.958759","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-22T04:24:22.572949Z","iopub.execute_input":"2022-05-22T04:24:22.57322Z","iopub.status.idle":"2022-05-22T04:24:22.824713Z","shell.execute_reply.started":"2022-05-22T04:24:22.573184Z","shell.execute_reply":"2022-05-22T04:24:22.824047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_cellid_source = dict(zip(df['cell_id'].values, df['source'].values))","metadata":{"execution":{"iopub.status.busy":"2022-05-22T04:24:22.826212Z","iopub.execute_input":"2022-05-22T04:24:22.826477Z","iopub.status.idle":"2022-05-22T04:24:22.833003Z","shell.execute_reply.started":"2022-05-22T04:24:22.826441Z","shell.execute_reply":"2022-05-22T04:24:22.83213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\n# import fasttext\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom nltk.stem import WordNetLemmatizer\nfrom pathlib import Path\nimport nltk\nnltk.download('wordnet')\n\nstemmer = WordNetLemmatizer()\n\ndef preprocess_text(document):\n        # Remove all the special characters\n        document = re.sub(r'\\W', ' ', str(document))\n\n        # remove all single characters\n        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n\n        # Remove single characters from the start\n        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n\n        # Substituting multiple spaces with single space\n        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n\n        # Removing prefixed 'b'\n        document = re.sub(r'^b\\s+', '', document)\n\n        # Converting to Lowercase\n        document = document.lower()\n        #return document\n\n        # Lemmatization\n        tokens = document.split()\n        tokens = [stemmer.lemmatize(word) for word in tokens]\n        tokens = [word for word in tokens if len(word) > 3]\n\n        preprocessed_text = ' '.join(tokens)\n        return preprocessed_text\n\n    \ndef preprocess_df(df):\n    \"\"\"\n    This function is for processing sorce of notebook\n    returns preprocessed dataframe\n    \"\"\"\n    return [preprocess_text(message) for message in df.source]\n\ndf.source = df.source.apply(preprocess_text)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T04:24:22.834696Z","iopub.execute_input":"2022-05-22T04:24:22.83499Z","iopub.status.idle":"2022-05-22T04:24:28.532678Z","shell.execute_reply.started":"2022-05-22T04:24:22.834949Z","shell.execute_reply":"2022-05-22T04:24:28.531949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport sys, os\ntry:\n  from transformers import DistilBertModel, DistilBertTokenizer\nexcept:\n  !pip install transformers\n  from transformers import DistilBertModel, DistilBertTokenizer\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch\n\nfrom transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling\nfrom transformers import AutoModelWithLMHead, AutoTokenizer, AutoModel\n\n# if not os.path.exists('text.txt'):\n#   with open('text.txt','w') as f:\n#     for id, item in tqdm(df.groupby('id')):\n#       df_markdown =  item[item['cell_type']=='markdown']\n#       for source, rank in df_markdown[['source', 'rank']].values:\n#         cell_source = df_markdown[df_markdown['rank']==(rank+1)]\n#         if len(cell_source):\n#           setence = source + ' [SEP] ' + cell_source.source.values[0]\n#           f.write(setence+'\\n')\n      \n\n# # Train a tokenizer\n# import tokenizers\n# from transformers import BertTokenizer, LineByLineTextDataset\n\n# tokenizer = AutoTokenizer.from_pretrained('prajjwal1/bert-small')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T04:24:28.533828Z","iopub.execute_input":"2022-05-22T04:24:28.534094Z","iopub.status.idle":"2022-05-22T04:24:48.938708Z","shell.execute_reply.started":"2022-05-22T04:24:28.53406Z","shell.execute_reply":"2022-05-22T04:24:48.93797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = AutoModelWithLMHead.from_pretrained('prajjwal1/bert-small')\n\n\n# data_collator = DataCollatorForLanguageModeling(\n#     tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n# )\n\n# from transformers import Trainer, TrainingArguments\n\n# dataset= LineByLineTextDataset(\n#     tokenizer = tokenizer,\n#     file_path = './text.txt',\n#     block_size = 128  # maximum sequence length\n# )\n\n# print('No. of lines: ', len(dataset)) # No of lines in your datset\n\n# training_args = TrainingArguments(\n#     output_dir='./',\n#     overwrite_output_dir=True,\n#     num_train_epochs=10,\n#     per_device_train_batch_size=64,\n#     save_steps=10000,\n# )\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     data_collator=data_collator,\n#     train_dataset=dataset,\n# )\n# trainer.train()\n# trainer.save_model('./')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T04:24:48.940132Z","iopub.execute_input":"2022-05-22T04:24:48.940397Z","iopub.status.idle":"2022-05-22T04:25:39.017198Z","shell.execute_reply.started":"2022-05-22T04:24:48.940363Z","shell.execute_reply":"2022-05-22T04:25:39.016425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_triplet(df, mode='train'):\n  triplets = []\n  ids = df.id.unique()\n  random_drop = np.random.random(size=10000)>0.9\n  count = 0\n\n  for id, df_tmp in tqdm(df.groupby('id')):\n    df_tmp_markdown = df_tmp[df_tmp['cell_type']=='markdown']\n\n    df_tmp_code = df_tmp[df_tmp['cell_type']=='code']\n    df_tmp_code_rank = df_tmp_code['rank'].values\n    df_tmp_code_cell_id = df_tmp_code['cell_id'].values\n\n    for cell_id, rank in df_tmp_markdown[['cell_id', 'rank']].values:\n      labels = np.array([(r==(rank+1)) for r in df_tmp_code_rank]).astype('int')\n\n      for cid, label in zip(df_tmp_code_cell_id, labels):\n        count += 1\n        if label==1:\n          triplets.append( [cell_id, cid, label] )\n          # triplets.append( [cid, cell_id, label] )\n        elif mode == 'test':\n          triplets.append( [cell_id, cid, label] )\n          # triplets.append( [cid, cell_id, label] )\n        elif random_drop[count%10000]:\n          triplets.append( [cell_id, cid, label] )\n          # triplets.append( [cid, cell_id, label] )\n    \n  return triplets\n\ntriplets = generate_triplet(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bisect import bisect\n\n\ndef count_inversions(a):\n    inversions = 0\n    sorted_so_far = []\n    for i, u in enumerate(a):\n        j = bisect(sorted_so_far, u)\n        inversions += i - j\n        sorted_so_far.insert(j, u)\n    return inversions\n\n\ndef kendall_tau(ground_truth, predictions):\n    total_inversions = 0\n    total_2max = 0  # twice the maximum possible inversions across all instances\n    for gt, pred in zip(ground_truth, predictions):\n        ranks = [gt.index(x) for x in pred]  # rank predicted order in terms of ground truth\n        total_inversions += count_inversions(ranks)\n        n = len(gt)\n        total_2max += n * (n - 1)\n    return 1 - 4 * total_inversions / total_2max","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport sys, os\ntry:\n  from transformers import DistilBertModel, DistilBertTokenizer\nexcept:\n  !pip install transformers\n  !pip install sentencepiece\n  from transformers import DistilBertModel, DistilBertTokenizer\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch\nfrom transformers import AutoModelWithLMHead, AutoTokenizer, AutoModel\n\n\nimport torch\nfrom transformers import RobertaTokenizer, RobertaConfig, RobertaModel\nfrom transformers import AlbertTokenizer, AlbertModel\n\n\nMAX_LEN = 128\n    \nclass MarkdownModel(nn.Module):\n    def __init__(self):\n        super(MarkdownModel, self).__init__()\n        self.distill_bert = AutoModel.from_pretrained(\"../input/mymodelbertsmallpretrained/checkpoint-120000\")\n        self.top = nn.Linear(512, 1)\n\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, ids, mask):\n        x = self.distill_bert(ids, mask)[0]\n        x = self.dropout(x)\n        x = self.top(x[:, 0, :])\n        x = torch.sigmoid(x) \n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\n\n\nclass MarkdownDataset(Dataset):\n    \n    def __init__(self, df, max_len, mode='train'):\n        super().__init__()\n        self.df = df\n        self.max_len = max_len\n        self.tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-small\", do_lower_case=True)\n        self.mode=mode\n\n    def __getitem__(self, index):\n        row = self.df[index]\n\n        label = row[-1]\n\n        txt = dict_cellid_source[row[0]] + '[SEP]' + dict_cellid_source[row[1]]\n\n        inputs = self.tokenizer.encode_plus(\n            txt,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding=\"max_length\",\n            return_token_type_ids=True,\n            truncation=True\n        )\n        ids = torch.LongTensor(inputs['input_ids'])\n        mask = torch.LongTensor(inputs['attention_mask'])\n\n        return ids, mask, torch.FloatTensor([label])\n\n\n\n\n    def __len__(self):\n        return len(self.df)\n\n\ntrain_ds = MarkdownDataset(triplets, max_len=MAX_LEN, mode='test')\n\ntrain_ds[1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adjust_lr(optimizer, epoch):\n    if epoch < 1:\n        lr = 5e-5\n    elif epoch < 2:\n        lr = 5e-5\n    elif epoch < 5:\n        lr = 5e-5\n    else:\n        lr = 5e-5\n\n    for p in optimizer.param_groups:\n        p['lr'] = lr\n    return lr\n    \ndef get_optimizer(net):\n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=3e-4, betas=(0.9, 0.999),\n                                 eps=1e-8 ) #1e-08)\n    return optimizer\n\nBS = 128 + 128\nNW = 8\n\ntrain_loader = DataLoader(train_ds, batch_size=BS, shuffle=True, num_workers=NW,\n                          pin_memory=False, drop_last=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_data(data):\n    return tuple(d.cuda() for d in data[:-1]), data[-1].cuda()\n\ndef validate(model, val_loader, mode='train'):\n    model.eval()\n    \n    tbar = tqdm(val_loader, file=sys.stdout)\n    \n    preds = []\n    labels = []\n\n    with torch.no_grad():\n        for idx, data in enumerate(tbar):\n            inputs, target = read_data(data)\n\n            pred = model(inputs[0], inputs[1])\n\n            preds.append(pred.detach().cpu().numpy().ravel())\n            if mode=='test':\n              labels.append(target.detach().cpu().numpy().ravel())\n    if mode=='test':\n      return np.concatenate(preds)\n    else:\n      return np.concatenate(labels), np.concatenate(preds)\n\ndef train(model, train_loader, epochs, Type='markdown'):\n    np.random.seed(0)\n    \n    optimizer = get_optimizer(model)\n\n    mixed_precision = True\n    try:  \n        from apex import amp\n    except:\n        mixed_precision = False  # not installed\n\n    # model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=1)\n    \n\n    criterion = torch.nn.L1Loss()\n    criterion = torch.nn.BCELoss()\n    \n    for e in range(epochs):   \n        model.train()\n        tbar = tqdm(train_loader, file=sys.stdout)\n        \n        lr = adjust_lr(optimizer, e)\n        \n        loss_list = []\n        preds = []\n        labels = []\n\n        for idx, data in enumerate(tbar):\n            inputs, target = read_data(data)\n\n            optimizer.zero_grad()\n            pred = model(inputs[0], inputs[1])\n\n            loss = criterion(pred, target)\n\n            loss.backward()\n            optimizer.step()\n            \n            loss_list.append(loss.detach().cpu().item())\n            preds.append(pred.detach().cpu().numpy().ravel())\n            labels.append(target.detach().cpu().numpy().ravel())\n            \n            avg_loss = np.round(np.mean(loss_list), 4)\n\n            tbar.set_description(f\"Epoch {e+1} Loss: {avg_loss} lr: {lr}\")\n\n        \n        output_model_file = f\"./my_own_model_{e}.bin\"\n        model_to_save = model.module if hasattr(model, 'module') else model\n        torch.save(model_to_save.state_dict(), output_model_file)\n\n\n\n\n\n    return model\n\nmodel = MarkdownModel()\nmodel = model.cuda()\n\n\nmodel = train(model, train_loader, epochs=1, Type='markdown')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Please upvote if you find it helpful! :D","metadata":{}}]}