{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport dill\nimport gensim\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\nfrom keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D,CuDNNLSTM, TimeDistributed\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler\nfrom sklearn.metrics import mean_squared_error\nimport sklearn\nimport gensim\nimport keras\nfrom gensim.models.word2vec import Word2Vec\nfrom gensim.models import KeyedVectors\nimport gc\nimport pickle\nimport gensim.downloader as api\nimport random\nfrom sklearn.externals import joblib\nfrom collections import defaultdict\nimport dill\nimport copy\nimport json\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom nltk.tokenize import TweetTokenizer\nfrom sklearn.model_selection import cross_validate\nfrom math import sqrt\nfrom sklearn.model_selection import KFold\nimport scipy\n\n \ndef rmsle(h, y):\n    \"\"\"\n   Compute the Root Mean Squared Log Error for hypthesis h and targets y\n   \n   Args:\n       h - numpy array containing predictions with shape (n_samples, n_targets)\n       y - numpy array containing targets with shape (n_samples, n_targets)\n   \"\"\"\n    return np.sqrt(np.square(np.log1p(h) - np.log1p(y)).mean())\n\ndef rmsle_v2(h, y):\n    \"\"\"\n   Compute the Root Mean Squared Log Error for hypthesis h and targets y\n   \n   Args:\n       h - numpy array containing predictions with shape (n_samples, n_targets)\n       y - numpy array containing targets with shape (n_samples, n_targets)\n   \"\"\"\n    return np.sqrt(np.square(h - y).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n############### Classification Version #######################\n\nEMBEDDING_FILES = [\n    #'../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\n    '../input/glove840b300dtxt/glove.840B.300d.txt'\n]\n\n\nNUM_MODELS = 1\nBATCH_SIZE = 64\nLSTM_UNITS = 200\nDENSE_HIDDEN_UNITS = 2 * LSTM_UNITS\nEPOCHS = 10\nMAX_LEN = 100\nCLASSES = 50\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\ndef build_matrix(word_index, embedding, size):\n    #embedding_index = load_embeddings(path)\n\n    embedding_matrix = np.zeros((len(word_index) + 1, size))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding[word]\n        except KeyError:\n            pass\n    return embedding_matrix\n    \n\ndef build_model(embedding_matrix, one_hot_shape):\n    words = Input(shape=(MAX_LEN,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)    \n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True),merge_mode='concat')(x)\n    #x = SpatialDropout1D(rate=0.3)(x)\n    #x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True),merge_mode='ave')(x)\n    #x = SpatialDropout1D(rate=0.3)(x)\n\n    #x = GlobalAveragePooling1D()(x) # this layer average each output from the Bidirectional layer \n    \n    x = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n\n    summary = Input(shape=(50,))\n    x_aux = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(summary)\n    x_aux = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True),merge_mode='concat')(x_aux)\n    #x_aux = SpatialDropout1D(rate=0.3)(x_aux)\n    #x_aux = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True),merge_mode='ave')(x_aux)\n    #x_aux = SpatialDropout1D(rate=0.3)(x_aux)\n  \n    #x_aux = GlobalAveragePooling1D()(x_aux)\n    x_aux = concatenate([\n        GlobalMaxPooling1D()(x_aux),\n        GlobalAveragePooling1D()(x_aux),\n    ])\n    \n    \n    one_hot = Input(shape=(one_hot_shape,))\n    hidden = concatenate([x,x_aux,one_hot])\n\n    hidden = Dense(1000, activation='relu')(hidden)\n    hidden = Dropout(0.4)(hidden)\n    hidden = Dense(800, activation='relu')(hidden)\n    hidden = Dropout(0.4)(hidden)\n    hidden = Dense(500, activation='relu')(hidden)\n    hidden = Dropout(0.4)(hidden)\n    hidden = Dense(400, activation='relu')(hidden)\n    hidden = Dropout(0.4)(hidden)\n    hidden = Dense(300, activation='relu')(hidden)\n    hidden = Dropout(0.4)(hidden)\n    hidden = Dense(100, activation='relu')(hidden)\n    result = Dense(CLASSES, activation='linear')(hidden)\n    result = Dense(CLASSES, activation='softmax')(result)\n    \n    model = Model(inputs=[words,summary,one_hot], outputs=[result])\n    #adam = keras.optimizers.Adam(lr=0.0001, clipnorm=1.0, clipvalue=0.5)\n    #model.compile(loss='mse', optimizer='adam')\n    \n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n    \n    return model\n    \n\n\n#train total\ntrain = pd.read_csv('../input/see-click-predict-fix/train.csv', sep=',')\ntrain = train[(train['num_votes'] <= 50)]\ntrain['description'].fillna(' ',inplace=True)\n#train = train[train['created_time'] > '2013-01-01 00:00:00']\n#train = train.dropna(subset=['source'])\ntrain.reset_index(inplace=True)\nprint(train.info())\n\n#train reduced\n#train = train[(train['num_votes'] > 1) & (train['num_votes'] < 50)]\n\n#train baseline\ntrain_baseline = train[~train['tag_type'].isna()]\ntrain = train_baseline\ntrain.reset_index(inplace=True)\n#print(train.info())\n\ntrain.loc[:,'description'] = train['description'].apply(lambda x:  ' '.join(gensim.utils.simple_preprocess(x)))\ntrain.loc[:,'summary'] = train['summary'].apply(lambda x:  ' '.join(gensim.utils.simple_preprocess(x)))\ntrain.loc[:,'latitude'] = train['latitude'].apply(lambda x: np.round(x,3))\ntrain.loc[:,'longitude'] = train['longitude'].apply(lambda x: np.round(x,3))\n\ntrain['hour'] = [str(pd.to_datetime(x).hour) for x in train['created_time']]\ntrain['dayofweek'] = [str(pd.to_datetime(x).weekday()) for x in train['created_time']]\ntrain['year'] = [str(pd.to_datetime(x).year) for x in train['created_time']]\n\n\ntokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(train['description']) + list(train['summary']))\n\n#test = train.sample(frac=.2,random_state=999)\n#train =  train.iloc[~train.index.isin(test.index),:]\n#y_train = train['num_votes']\n\ny_train = np.zeros(shape=(len(train),CLASSES))\nfor index,sample in enumerate(train['num_votes']):    \n    y_train[index,sample] = sample\n\n\ndescription_train = train['description']\n#description_test = test['description']\ndescription_train = tokenizer.texts_to_sequences(description_train)\n#description_test = tokenizer.texts_to_sequences(description_test)\ndescription_train = sequence.pad_sequences(description_train, maxlen=MAX_LEN)\n#description_test = sequence.pad_sequences(description_test, maxlen=MAX_LEN)\n\nsummary_train = train['summary']\n#summary_test = test['summary']\nsummary_train = tokenizer.texts_to_sequences(summary_train)\n#summary_test = tokenizer.texts_to_sequences(summary_test)\nsummary_train = sequence.pad_sequences(summary_train,50)\n#summary_test = sequence.pad_sequences(summary_test,50)\n\nfrom sklearn.preprocessing import OneHotEncoder\ndummies = OneHotEncoder(handle_unknown='ignore')\n\n#source_train = dummies.fit_transform(np.array(train[['source']]).reshape(-1,1)).todense()\n#source = dummies.fit_transform(train[['source','num_votes']]) #this should be wrong but is issuing a smaller error\n#source_test = dummies.transform(np.array(test[['source']]).reshape(-1,1)).todense()\n\n# Decimal places      Object that can be unambiguously recognized at this scale\n# 0\t                  country or large region\n# 1\t            \t  large city or district\n# 2\t             \t  town or village\n# 3               \t  neighborhood, street\n# 4                   individual street, land parcel\n# 5                   individual trees, door entrance\n# 6                   individual humans\n# 7                   practical limit of commercial surveying\n# 8                   specialized surveying (e.g. tectonic plate mapping)\n\nlatitude_train = train['latitude']\nlongitude_train = train['longitude']\n#latitude_test = test['latitude']\n#longitude_test = test['longitude']\n\nonehot_train = np.stack((train['hour'],train['dayofweek'],train['year'],latitude_train,longitude_train),axis=-1)\nonehot_train = dummies.fit_transform(onehot_train)\n#onehot_train = scipy.sparse.csr_matrix(onehot_train)\n\n#onehot_test = np.stack((test['hour'],test['dayofweek'],test['year'],latitude_test,longitude_test),axis=-1)\n#onehot_test = dummies.transform(onehot_test)\n#onehot_test = scipy.sparse.csr_matrix(onehot_test)\n\n\nembedding = KeyedVectors.load_word2vec_format(\"../input/glove2word2vec/glove_w2v.txt\",binary=False)\n#embedding = KeyedVectors.load_word2vec_format(\"../input/glove840B300dtxt/glove.840B.300d.txt\",binary=False)\nEMBEDDINGS = [embedding]\nembedding_matrix = build_matrix(tokenizer.word_index, embedding,200)\ndel embedding\ngc.collect()\n# embedding_matrix = np.concatenate(\n#     [build_matrix(tokenizer.word_index, wordvect,200) for wordvect in EMBEDDINGS], axis=-1)\n\n\ndef main(x_train,y_train,one_hot_shape): \n  checkpoint_predictions = []\n  weights = []\n\n  model = build_model(embedding_matrix,one_hot_shape)\n\n  for model_idx in range(NUM_MODELS):\n        #model = build_model(embedding_matrix,one_hot_shape)\n        for global_epoch in range(EPOCHS):\n            model.fit(\n                x_train,\n                #[Y_train, y_aux_train],\n                y_train,\n                batch_size=BATCH_SIZE,\n                epochs=1,\n                verbose=2,\n                #callbacks=[\n                 #   LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n                #]\n            )\n            #checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n            #weights.append(2 ** global_epoch)\n\n  model.save('lstm_model_3.h5')  \n  return model\n\n#This creates a matrix with all the train data. But is useless\n#description_train = np.array(description_train).reshape(len(description_train),MAX_LEN)\n#summary_train = np.array(summary_train).reshape(len(summary_train),50)\n#final_train = scipy.sparse.hstack([scipy.sparse.csr_matrix(description_train),scipy.sparse.csr_matrix(summary_train),onehot_train],format='csr')\n\nkfold = KFold(n_splits=2, shuffle=True, random_state=999)\ncvscores = []\nfor train_idx,test_idx in kfold.split(onehot_train):    \n ## this is to guarantee full batches of BATCH_SIZE examples\n #length_train = len(train_idx)%BATCH_SIZE \n #length_train = len(train_idx) - length_train\n #train_idx = train_idx[0:length_train]\n #test_idx = test_idx[0:length_train]    \n\n test_idx = (train.sample(frac=.2)).index\n train_idx = train[~train.index.isin(test_idx)].index\n\n model = main([description_train[train_idx], summary_train[train_idx], onehot_train[train_idx]], y_train[train_idx],onehot_train.get_shape()[1])\n pred = model.evaluate([description_train[test_idx],summary_train[test_idx],onehot_train[test_idx]], y_train[test_idx])\n #cvscores.append(np.mean(np.square(pred - y_train[test_idx])))\n #print('MSE %.3f' % np.mean(np.square(pred - y_train[test_idx])))\n print(model.metrics_names)\n print(pred)\n cvscores.append(pred[1])\n print('Accuracy %.3f' % pred[1])\n              \nprint('Overall accuracy: %.3f' % np.mean(cvscores))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# The below is necessary for starting Numpy generated random numbers\n# in a well-defined initial state.\nnp.random.seed(999)\n# The below is necessary for starting core Python generated random numbers\n# in a well-defined state.\nimport random as rn\nrn.seed(12345)\n\nfrom tensorflow import set_random_seed\nset_random_seed(2)\n\n#train total\ntrain = pd.read_csv('../input/see-click-predict-fix/train.csv', sep=',')\ntrain = train[(train['num_votes'] < 50)]\ntrain['description'].fillna(' ',inplace=True)\ntrain.reset_index(inplace=True)\n\n#train reduced\n#train = train[(train['num_votes'] > 1) & (train['num_votes'] < 50)]\n\n#train baseline\ntrain_baseline = train[~train['tag_type'].isna()]\n#train = train_baseline\n#train.reset_index(inplace=True)\n#print(train.info())\n\n\ntrain.loc[:,'description'] = train['description'].apply(lambda x:  ' '.join(gensim.utils.simple_preprocess(x)))\ntrain.loc[:,'summary'] = train['summary'].apply(lambda x:  ' '.join(gensim.utils.simple_preprocess(x)))\n\nEMBEDDING_FILES = [\n    #'../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\n    '../input/glove840b300dtxt/glove.840B.300d.txt'\n]\n\n\nNUM_MODELS = 1\nBATCH_SIZE = 64\nLSTM_UNITS = 300\nDENSE_HIDDEN_UNITS = 2 * LSTM_UNITS\nEPOCHS = 10\nMAX_LEN = 100\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\ndef build_matrix(word_index, embedding, size):\n    #embedding_index = load_embeddings(path)\n\n    embedding_matrix = np.zeros((len(word_index) + 1, size))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding[word]\n        except KeyError:\n            pass\n    return embedding_matrix\n    \n\ndef build_model(embedding_matrix, num_aux_targets=0):\n    words = Input(shape=(MAX_LEN,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True),merge_mode='concat')(x)\n    #x = SpatialDropout1D(rate=0.3)(x)\n    #x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True),merge_mode='ave')(x)\n    #x = SpatialDropout1D(rate=0.3)(x)\n\n    #x = GlobalAveragePooling1D()(x) # this layer average each output from the Bidirectional layer \n    x = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n       \n    summary = Input(shape=(50,))\n    x_aux = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(summary)\n    x_aux = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True),merge_mode='concat')(x_aux)\n    #x_aux = SpatialDropout1D(rate=0.3)(x_aux)\n    #x_aux = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True),merge_mode='ave')(x_aux)\n    #x_aux = SpatialDropout1D(rate=0.3)(x_aux)\n    \n    x_aux = GlobalAveragePooling1D()(x_aux)\n    x_aux = concatenate([\n        GlobalMaxPooling1D()(x_aux),\n        GlobalAveragePooling1D()(x_aux),\n    ])\n  \n    hidden = concatenate([x,x_aux])   \n\n    hidden = Dense(1000,activation='relu')(hidden)\n    hidden = Dropout(0.5)(hidden)\n    hidden = Dense(900, activation='relu')(hidden)\n    hidden = Dropout(0.4)(hidden)\n    hidden = Dense(800, activation='relu')(hidden)\n    hidden = Dropout(0.4)(hidden)\n    hidden = Dense(700, activation='relu')(hidden)\n    hidden = Dropout(0.4)(hidden)\n    hidden = Dense(500, activation='relu')(hidden)\n    hidden = Dropout(0.4)(hidden)    \n    hidden = Dense(200, activation='relu')(hidden)\n    hidden = Dropout(0.4)(hidden)    \n    hidden = Dense(100, activation='relu')(hidden)\n    hidden = Dropout(0.4)(hidden)    \n \n    result = Dense(1, activation='linear')(hidden)\n    model = Model(inputs=[words,summary], outputs=[result])\n    #adam = Adam(lr=0.0001, clipnorm=1.0, clipvalue=0.5)\n    model.compile(loss='mse', optimizer='adam')\n\n    return model\n    \n\ndef preprocess(data):\n    '''\n    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n    '''\n    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text\n\n    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n    return data\n\n\ntokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(train['description']) + list(train['summary']))\n\n#test = train.sample(frac=.2,random_state=999)\n#train =  train.iloc[~train.index.isin(test.index),:]\n\n#print(test.index.isin([194239,21704,3014,24985,17063,223071,19349,26392,31870,189610,37705]))\n\ndescription_train = train['description']\n#description_test = test['description']\ndescription_train = tokenizer.texts_to_sequences(description_train)\n#description_test = tokenizer.texts_to_sequences(description_test)\ndescription_train = sequence.pad_sequences(description_train, maxlen=MAX_LEN)\n#description_test = sequence.pad_sequences(description_test, maxlen=MAX_LEN)\n\nsummary_train = train['summary']\n#summary_test = test['summary']\nsummary_train = tokenizer.texts_to_sequences(summary_train)\n#summary_test = tokenizer.texts_to_sequences(summary_test)\nsummary_train = sequence.pad_sequences(summary_train,50)\n#summary_test = sequence.pad_sequences(summary_test,50)\n\n#y_test = test['num_votes'].apply(lambda x: np.log1p(x))\n#y_train = train['num_votes'].apply(lambda x: np.log1p(x))\n\n#y_test = test['num_votes']\ny_train = train['num_votes']\n\n\n#embedding = KeyedVectors.load_word2vec_format(\"../input/glove2word2vec/glove_w2v.txt\",binary=False)\n#embedding = KeyedVectors.load_word2vec_format(\"glove300d_word2vec.txt\",binary=False)\nembedding = KeyedVectors.load_word2vec_format(\"../input/word2vec-google/GoogleNews-vectors-negative300.bin\",binary=True)\n\nEMBEDDINGS = [embedding]\nembedding_matrix = build_matrix(tokenizer.word_index, embedding,300)\ndel embedding\ngc.collect()\n# embedding_matrix = np.concatenate(\n#     [build_matrix(tokenizer.word_index, wordvect,200) for wordvect in EMBEDDINGS], axis=-1)\n\ndef main(x_train,y_train): \n  checkpoint_predictions = []\n  weights = []\n\n  model = build_model(embedding_matrix)\n\n  for model_idx in range(NUM_MODELS):\n        #model = build_model(embedding_matrix)\n        for global_epoch in range(EPOCHS):\n            model.fit(\n                x_train,\n                #[Y_train, y_aux_train],\n                y_train,\n                batch_size=BATCH_SIZE,\n                epochs=1,\n                verbose=2,\n                callbacks=[\n                    LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n                ]\n            )\n            #checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n            #weights.append(2 ** global_epoch)\n\n  return model\n\nkfold = KFold(n_splits=2, shuffle=True, random_state=999)\ncvscores = []\nfor train_idx, test_idx in kfold.split(description_train):    \n test_idx = (train.sample(frac=.2)).index\n train_idx = train[~train.index.isin(test_idx)].index\n\n model = main([description_train[train_idx],summary_train[train_idx]],y_train[train_idx])\n pred = model.predict([description_train[test_idx],summary_train[test_idx]]).flatten()\n cvscores.append(np.mean(np.square(pred - y_train[test_idx])))\n print('MSE %.3f' % np.mean(np.square(pred - y_train[test_idx])))\n              \nprint('Overall MSE: %.3f' % np.mean(cvscores))\n\n                 \n#from sklearn.metrics import r2_score\n#print(\"R-squared: \",end=\"\")\n#print(r2_score(y_test, pred))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n\n\nEMBEDDING_FILES = [\n    #'../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\n    '../input/glove840b300dtxt/glove.840B.300d.txt'\n]\n\n\nNUM_MODELS = 1\nBATCH_SIZE = 64\nLSTM_UNITS = 200\nDENSE_HIDDEN_UNITS = 2 * LSTM_UNITS\nEPOCHS = 10\nMAX_LEN = 100\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\ndef build_matrix(word_index, embedding, size):\n    #embedding_index = load_embeddings(path)\n\n    embedding_matrix = np.zeros((len(word_index) + 1, size))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding[word]\n        except KeyError:\n            pass\n    return embedding_matrix\n    \n\ndef build_model(embedding_matrix, one_hot_shape):\n    words = Input(shape=(MAX_LEN,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)    \n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True),merge_mode='concat')(x)\n    #x = SpatialDropout1D(rate=0.3)(x)\n    #x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True),merge_mode='ave')(x)\n    #x = SpatialDropout1D(rate=0.3)(x)\n\n    #x = GlobalAveragePooling1D()(x) # this layer average each output from the Bidirectional layer \n    \n    x = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n\n    summary = Input(shape=(50,))\n    x_aux = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(summary)\n    x_aux = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True),merge_mode='concat')(x_aux)\n    #x_aux = SpatialDropout1D(rate=0.3)(x_aux)\n    #x_aux = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True),merge_mode='ave')(x_aux)\n    #x_aux = SpatialDropout1D(rate=0.3)(x_aux)\n  \n    #x_aux = GlobalAveragePooling1D()(x_aux)\n    x_aux = concatenate([\n        GlobalMaxPooling1D()(x_aux),\n        GlobalAveragePooling1D()(x_aux),\n    ])\n    \n    \n    one_hot = Input(shape=(one_hot_shape,))\n    hidden = concatenate([x,x_aux,one_hot])\n\n    hidden = Dense(1000, activation='relu')(hidden)\n    hidden = Dropout(0.4)(hidden)\n    hidden = Dense(800, activation='relu')(hidden)\n    hidden = Dropout(0.4)(hidden)\n    hidden = Dense(500, activation='relu')(hidden)\n    hidden = Dropout(0.4)(hidden)\n    hidden = Dense(400, activation='relu')(hidden)\n    hidden = Dropout(0.4)(hidden)\n    hidden = Dense(300, activation='relu')(hidden)\n    hidden = Dropout(0.4)(hidden)\n    hidden = Dense(100, activation='relu')(hidden)\n    result = Dense(1, activation='linear')(hidden)\n    \n    model = Model(inputs=[words,summary,one_hot], outputs=[result])\n    #adam = keras.optimizers.Adam(lr=0.0001, clipnorm=1.0, clipvalue=0.5)\n    model.compile(loss='mse', optimizer='adam')\n\n    return model\n    \n\ndef preprocess(data):\n    '''\n    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n    '''\n    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text\n\n    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n    return data\n\n\n#train total\ntrain = pd.read_csv('../input/see-click-predict-fix/train.csv', sep=',')\ntrain = train[(train['num_votes'] < 50)]\ntrain['description'].fillna(' ',inplace=True)\n#train = train[train['created_time'] > '2013-01-01 00:00:00']\n#train = train.dropna(subset=['source'])\ntrain.reset_index(inplace=True)\nprint(train.info())\n\n#train reduced\n#train = train[(train['num_votes'] > 1) & (train['num_votes'] < 50)]\n\n#train baseline\ntrain_baseline = train[~train['tag_type'].isna()]\ntrain = train_baseline\ntrain.reset_index(inplace=True)\n#print(train.info())\n\ntrain.loc[:,'description'] = train['description'].apply(lambda x:  ' '.join(gensim.utils.simple_preprocess(x)))\ntrain.loc[:,'summary'] = train['summary'].apply(lambda x:  ' '.join(gensim.utils.simple_preprocess(x)))\ntrain.loc[:,'latitude'] = train['latitude'].apply(lambda x: np.round(x,3))\ntrain.loc[:,'longitude'] = train['longitude'].apply(lambda x: np.round(x,3))\n\ntrain['hour'] = [str(pd.to_datetime(x).hour) for x in train['created_time']]\ntrain['dayofweek'] = [str(pd.to_datetime(x).weekday()) for x in train['created_time']]\ntrain['year'] = [str(pd.to_datetime(x).year) for x in train['created_time']]\n\n\ntokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(train['description']) + list(train['summary']))\n\n#test = train.sample(frac=.2,random_state=999)\n#train =  train.iloc[~train.index.isin(test.index),:]\n\n#y_test = test['num_votes'].apply(lambda x: np.log1p(x+1))\n#y_train = train['num_votes'].apply(lambda x: np.log1p(x+1))\n#y_test = test['num_votes']\ny_train = train['num_votes']\n\n\ndescription_train = train['description']\n#description_test = test['description']\ndescription_train = tokenizer.texts_to_sequences(description_train)\n#description_test = tokenizer.texts_to_sequences(description_test)\ndescription_train = sequence.pad_sequences(description_train, maxlen=MAX_LEN)\n#description_test = sequence.pad_sequences(description_test, maxlen=MAX_LEN)\n\nsummary_train = train['summary']\n#summary_test = test['summary']\nsummary_train = tokenizer.texts_to_sequences(summary_train)\n#summary_test = tokenizer.texts_to_sequences(summary_test)\nsummary_train = sequence.pad_sequences(summary_train,50)\n#summary_test = sequence.pad_sequences(summary_test,50)\n\nfrom sklearn.preprocessing import OneHotEncoder\ndummies = OneHotEncoder(handle_unknown='ignore')\n\n#source_train = dummies.fit_transform(np.array(train[['source']]).reshape(-1,1)).todense()\n#source = dummies.fit_transform(train[['source','num_votes']]) #this should be wrong but is issuing a smaller error\n#source_test = dummies.transform(np.array(test[['source']]).reshape(-1,1)).todense()\n\n# Decimal places      Object that can be unambiguously recognized at this scale\n# 0\t                  country or large region\n# 1\t            \t  large city or district\n# 2\t             \t  town or village\n# 3               \t  neighborhood, street\n# 4                   individual street, land parcel\n# 5                   individual trees, door entrance\n# 6                   individual humans\n# 7                   practical limit of commercial surveying\n# 8                   specialized surveying (e.g. tectonic plate mapping)\n\nlatitude_train = train['latitude']\nlongitude_train = train['longitude']\n#latitude_test = test['latitude']\n#longitude_test = test['longitude']\n\nonehot_train = np.stack((train['hour'],train['dayofweek'],train['year'],latitude_train,longitude_train),axis=-1)\nonehot_train = dummies.fit_transform(onehot_train)\n#onehot_train = scipy.sparse.csr_matrix(onehot_train)\n\n#onehot_test = np.stack((test['hour'],test['dayofweek'],test['year'],latitude_test,longitude_test),axis=-1)\n#onehot_test = dummies.transform(onehot_test)\n#onehot_test = scipy.sparse.csr_matrix(onehot_test)\n\n\nembedding = KeyedVectors.load_word2vec_format(\"../input/glove2word2vec/glove_w2v.txt\",binary=False)\n#embedding = KeyedVectors.load_word2vec_format(\"../input/glove840B300dtxt/glove.840B.300d.txt\",binary=False)\nEMBEDDINGS = [embedding]\nembedding_matrix = build_matrix(tokenizer.word_index, embedding,200)\ndel embedding\ngc.collect()\n# embedding_matrix = np.concatenate(\n#     [build_matrix(tokenizer.word_index, wordvect,200) for wordvect in EMBEDDINGS], axis=-1)\n\n\ndef main(x_train,y_train,one_hot_shape): \n  checkpoint_predictions = []\n  weights = []\n\n  model = build_model(embedding_matrix,one_hot_shape)\n\n  for model_idx in range(NUM_MODELS):\n        #model = build_model(embedding_matrix,one_hot_shape)\n        for global_epoch in range(EPOCHS):\n            model.fit(\n                x_train,\n                #[Y_train, y_aux_train],\n                y_train,\n                batch_size=BATCH_SIZE,\n                epochs=1,\n                verbose=2,\n                #callbacks=[\n                 #   LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n                #]\n            )\n            #checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n            #weights.append(2 ** global_epoch)\n\n  model.save('lstm_model_3.h5')  \n  return model\n\n#This creates a matrix with all the train data. But is useless\n#description_train = np.array(description_train).reshape(len(description_train),MAX_LEN)\n#summary_train = np.array(summary_train).reshape(len(summary_train),50)\n#final_train = scipy.sparse.hstack([scipy.sparse.csr_matrix(description_train),scipy.sparse.csr_matrix(summary_train),onehot_train],format='csr')\n\nkfold = KFold(n_splits=2, shuffle=True, random_state=999)\ncvscores = []\nfor train_idx,test_idx in kfold.split(onehot_train):    \n ## this is to guarantee full batches of BATCH_SIZE examples\n #length_train = len(train_idx)%BATCH_SIZE \n #length_train = len(train_idx) - length_train\n #train_idx = train_idx[0:length_train]\n #test_idx = test_idx[0:length_train]    \n\n test_idx = (train.sample(frac=.2)).index\n train_idx = train[~train.index.isin(test_idx)].index\n\n model = main([description_train[train_idx], summary_train[train_idx], onehot_train[train_idx]], y_train[train_idx],onehot_train.get_shape()[1])\n pred = model.predict([description_train[test_idx],summary_train[test_idx],onehot_train[test_idx]]).flatten()\n cvscores.append(np.mean(np.square(pred - y_train[test_idx])))\n print('MSE %.3f' % np.mean(np.square(pred - y_train[test_idx])))\n              \nprint('Overall MSE: %.3f' % np.mean(cvscores))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indexes = list(range(0,len(description_train)))\nprint(len(indexes))\nprint((train.index))\ntest_idx = random.sample(indexes,k=int(0.2*len(indexes)))\n\nfor v in test_idx:\n    indexes.remove(v)\nprint(len(indexes))\nprint(len(test_idx))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN = pd.read_csv('../input/see-click-predict-fix/train.csv', sep=',')\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as plticker\n\nloc = plticker.MultipleLocator(base=10.0) # this locator puts ticks at regular intervals\n#Number of non null entries for each column\nTRAIN.info()\n\nTRAIN = TRAIN[(TRAIN['num_votes'] < 50)]\nTRAIN['description'].fillna(' ',inplace=True)\nTRAIN.reset_index(inplace=True)\n\nTRAIN.loc[:,'latitude'] = TRAIN['latitude'].apply(lambda x: np.round(x,3))\nTRAIN.loc[:,'longitude'] = TRAIN['longitude'].apply(lambda x: np.round(x,3))\n\nTRAIN['hour'] = [str(pd.to_datetime(x).hour) for x in TRAIN['created_time']]\nTRAIN['dayofweek'] = [str(pd.to_datetime(x).weekday()) for x in TRAIN['created_time']]\nTRAIN['year'] = [str(pd.to_datetime(x).year) for x in TRAIN['created_time']]\n\n\n# frequency of votes\ncount_votes = pd.DataFrame(TRAIN['num_votes'].value_counts(normalize=True)) #counts the number of times each type occured\nax = count_votes.plot(kind='bar',figsize=(10,5), title=\"Frequency of votes\")\nax.set_ylabel('% of votes')\n#ax.set_xlabel('Models')\n\n# frequency of votes baseline\nbaseline = TRAIN[~TRAIN['tag_type'].isna()]\ncount_votes = pd.DataFrame(baseline['num_votes'].value_counts(normalize=True)) #counts the number of times each type occured\nax = count_votes.plot(kind='bar',figsize=(10,5), title=\"Frequency of votes baseline\")\nax.set_ylabel('% of votes')\n#ax.set_xlabel('Models')\n# Total number of examples by issue type for baseline\ncount_examples = baseline[['tag_type','num_votes']].groupby(['tag_type']).count()\nax = count_examples.plot(kind='bar',figsize=(10,5), title=\"Number of examples\")\nax.set_xlabel('Issue type')\n\n\n# Frequency issue type\ncount_issues = pd.DataFrame(TRAIN['tag_type'].value_counts(normalize=True)) #counts the number of times each type occured\nax = count_issues.plot(kind='bar',figsize=(10,5), title=\"Frequency of issue type\")\nax.set_ylabel('% of issues')\n#ax.set_xlabel('Models')\n\n#Average Number of votes per issue\nvotes_per_issue_type = TRAIN.loc[:,['tag_type','num_votes']].groupby('tag_type').mean()\nvotes_per_issue_type.sort_values(by=['num_votes'],ascending=False,inplace=True)\nax = votes_per_issue_type.plot(kind='bar',figsize=(10,5), title=\"Average number of votes per issue type\")\nax.set_ylabel('Average number of votes')\n\n#Variance of num_votes\nprint(\"Mean of num votes all dataset: %f \" % np.mean(TRAIN['num_votes']))\nprint(\"Variance of num votes all dataset: %f \" % np.var(TRAIN['num_votes']))\naux = TRAIN[(TRAIN['num_votes']>1)&(TRAIN['num_votes']<50)]\nprint(\"Mean of num votes reduced dataset: %f \" % np.mean(aux['num_votes']))\nprint(\"Variance of num votes reduced dataset: %f \" % np.var(aux['num_votes']))\naux = TRAIN[~TRAIN['tag_type'].isna()]\nprint(\"Mean of num votes baseline dataset: %f \" % np.mean(aux['num_votes']))\nprint(\"Variance of num votes baseline dataset: %f \" % np.var(aux['num_votes']))\n\n#Variance per type\nvariance_per_issue_type = TRAIN.loc[:,['tag_type','num_votes']].groupby('tag_type').var()\nvariance_per_issue_type.sort_values(by=['num_votes'],ascending=False,inplace=True)\nax = variance_per_issue_type.plot(kind='bar',figsize=(10,5), title=\"Variance of votes per issue type\")\nax.set_ylabel('Variance of votes')\n\n#Find sensitive exemples to test \n#Get max and min votes for Traffic\nprint(aux.loc[aux['tag_type'] == 'traffic',['id','num_votes']].sort_values(by=['num_votes']))\n#194239  301444         17\n#21704   104882         18\n#3014    187575         19\n#24985   312502         19\n#17063    17969         25\n#223071   88840         26\n#19349   203600         31\n#26392   296264         34\n#31870   233309         35\n#189610  173886         71\n#37705   243191        134\nprint(aux.iloc[aux.index.isin([194239,21704,3014,24985,17063,223071,19349,26392,31870,189610,37705])])\n\n#Average Number of votes per issue for Okland\nprint('Okland')\nokland = TRAIN[(TRAIN['latitude'] >= 37.80) & (TRAIN['latitude'] <= 38)]\nvotes_per_issue_type = okland.loc[:,['tag_type','num_votes']].groupby('tag_type').mean()\nvotes_per_issue_type.sort_values(by=['num_votes'],ascending=False,inplace=True)\n#print(votes_per_issue_type)\nax = votes_per_issue_type.plot(kind='bar',figsize=(10,5), title=\"Average number of votes per issue type for Okland\")\nax.set_ylabel('Average number of votes')\n\n\n\n# Ratio number_of_votes:count_issue_type - number of votes per issue type normalized by the count of each issue type\n#for i,sample in count_issues.iterrows():\n#    votes_per_issue_type.loc[i,'num_votes'] /= count_issues.loc[i,'tag_type']\n#votes_per_issue_type.sort_values(by=['num_votes'],ascending=False,inplace=True)\n#ax = votes_per_issue_type.plot(kind='bar',figsize=(10,5), title=\"\")\n#ax.set_ylabel('Ratio number_votes:number_occurences')\n\n\n#Total number per issue type for null descriptions\n#index_non_null = TRAIN[TRAIN['description'].notnull()].index\n#count_issues = pd.DataFrame(TRAIN.loc[~TRAIN.index.isin(index_non_null),'tag_type'].value_counts())\n#ax = count_issues.plot(kind='bar',figsize=(10,5), title=\"Number of examples where description is null\")\n#ax.set_ylabel('Total number of issues')\n#ax.set_xlabel('Models')\n\n#Comparing number of votes by issue type among different neighborhoods \nlocations_more_than_one_issue = TRAIN.loc[:,['latitude','longitude','num_votes']].groupby(['latitude','longitude']).count()\nlocations_more_than_one_issue.sort_values(by=['num_votes'],ascending=False,inplace=True)\nfor v in locations_more_than_one_issue.iterrows():\n    #print(v[0][0])\n    if v[1][0] < 30:\n        break\n        \n    if all(TRAIN.loc[(TRAIN['latitude'] == v[0][0])&(TRAIN['longitude'] == v[0][1]),'tag_type'].isna()):\n        continue\n        \n    aux = TRAIN.loc[(TRAIN['latitude'] == v[0][0])&(TRAIN['longitude'] == v[0][1]),\n                    ['tag_type','num_votes']].groupby(['tag_type']).mean()\n    ax = aux.plot(kind='barh',figsize=(10,5), title=str(v[0][0]) + ' ' + str(v[0][1]))\n    for i, v in enumerate(aux.values):\n        #plt.text(v, i, \" \"+str(v), color='blue', va='center', fontweight='bold') \n        ax.text(v, i, str(np.round(v,2)), color='blue', fontweight='bold')\n    plt.show()\n\n    \n\n#This prints the ten max locations and ten min locations per tag_type \n#votes_per_location_issue_type = TRAIN.loc[TRAIN['location'].isin(locations_more_than_one_issue.index),['tag_type','location','num_votes']].groupby(['location','tag_type']).sum()\n#votes_per_location_issue_type.sort_index(by=['location'],inplace=True)\n#potholes = votes_per_location_issue_type[votes_per_location_issue_type.index.isin(['pothole'],level=1)]\n#potholes_max = potholes.sort_values(by=['num_votes'],ascending=False)\n#potholes_min = potholes.sort_values(by=['num_votes'],ascending=True)\n#ax = pd.concat([potholes_max.iloc[0:10,:],potholes_min.iloc[0:10,:]]).plot(kind='bar',figsize=(10,5), title=\"\")\n#tress = votes_per_location_issue_type[votes_per_location_issue_type.index.isin(['tree'],level=1)]\n#tress_max = tress.sort_values(by=['num_votes'],ascending=False)\n#tress_min = tress.sort_values(by=['num_votes'],ascending=True)\n#ax = pd.concat([tress_max.iloc[0:10,:],tress_min.iloc[0:10,:]]).plot(kind='bar',figsize=(10,5), title=\"\")\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    \n# Average number of words\nindex_non_null = TRAIN[TRAIN['description'].notnull()].index\nmax = 0\nmin = np.inf\navg = 0\nfor i,w in TRAIN.iloc[index_non_null].iterrows():\n    words = w['description'].split()\n    if len(words) > max:\n        max = len(words)\n    if len(words) < min:\n        min = len(words)\n    avg += len(words)\navg /= len(index_non_null)\n\nprint('max: %d' %max)\nprint('min: %d' %min)\nprint('avg: %f' %avg)\n\n\nindex_non_null = TRAIN[TRAIN['summary'].notnull()].index\nmax = 0\nmin = np.inf\navg = 0\nfor i,w in TRAIN.iloc[index_non_null].iterrows():\n    words = w['summary'].split()\n    if len(words) > max:\n        max = len(words)\n    if len(words) < min:\n        min = len(words)\n    avg += len(words)\navg /= len(index_non_null)\n\nprint('max: %d' %max)\nprint('min: %d' %min)\nprint('avg: %f' %avg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN = pd.read_csv('../input/see-click-predict-fix/train.csv', sep=',')\nfrom datetime import date\nfrom datetime import datetime\nfrom datetime import time\n\n#Number of non null entries for each column\nTRAIN.info()\n\nprint(TRAIN[~TRAIN['tag_type'].isna()].info())\nprint(TRAIN[TRAIN['tag_type'].notnull()].info())\n\nprint((date.today()))\nprint(date.today().strftime('%Y-%m-%d'))\n#datetime.datetime.strptime(today_date,'%m/%d/%y')\n\n#len(TRAIN[TRAIN['tag_type'].notnull()])datetime.date.today() datetime.strptime(x,'%Y-%m-%d %H:%M:%S')\nTRAIN['created_time'] = TRAIN.loc[:,'created_time'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d %H:%M:%S').strftime('%Y-%m-%d'))\nprint(TRAIN[['created_time','num_votes']].groupby(['created_time']).count())\nprint(np.mean(TRAIN[['created_time','num_votes']].groupby(['created_time']).count()['num_votes']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.gofplots import ProbPlot\n\nplt.style.use('seaborn') # pretty matplotlib plots\nplt.rc('font', size=14)\nplt.rc('figure', titlesize=18)\nplt.rc('axes', labelsize=15)\nplt.rc('axes', titlesize=18)\n\ndef graph(formula, x_range, label=None):\n    \"\"\"\n    Helper function for plotting cook's distance lines\n    \"\"\"\n    x = x_range\n    y = formula(x)\n    plt.plot(x, y, label=label, lw=1, ls='--', color='red')\n\n\ndef diagnostic_plots(X, y, model_fit=None):\n  \"\"\"\n  Function to reproduce the 4 base plots of an OLS model in R.\n\n  ---\n  Inputs:\n\n  X: A numpy array or pandas dataframe of the features to use in building the linear regression model\n\n  y: A numpy array or pandas series/dataframe of the target variable of the linear regression model\n\n  model_fit [optional]: a statsmodel.api.OLS model after regressing y on X. If not provided, will be\n                        generated from X, y\n  \"\"\"\n\n  if not model_fit:\n      model_fit = sm.OLS(y, sm.add_constant(X)).fit()\n\n  print(model_fit.summary())\n  # create dataframe from X, y for easier plot handling\n  #dataframe = pd.concat([X, y], axis=1)\n\n  # model values\n  model_fitted_y = model_fit.fittedvalues\n  # model residuals\n  model_residuals = model_fit.resid\n  # normalized residuals\n  model_norm_residuals = model_fit.get_influence().resid_studentized_internal\n  # absolute squared normalized residuals\n  model_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n  # absolute residuals\n  model_abs_resid = np.abs(model_residuals)\n  # leverage, from statsmodels internals\n  model_leverage = model_fit.get_influence().hat_matrix_diag\n  # cook's distance, from statsmodels internals\n  model_cooks = model_fit.get_influence().cooks_distance[0]\n\n  plot_lm_1 = plt.figure()\n  plot_lm_1.axes[0] = sns.residplot(model_fitted_y, y, data=None,\n                            lowess=True,\n                            scatter_kws={'alpha': 0.5},\n                            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\n  plot_lm_1.axes[0].set_title('Residuals vs Fitted')\n  plot_lm_1.axes[0].set_xlabel('Fitted values')\n  plot_lm_1.axes[0].set_ylabel('Residuals');\n\n  # annotations\n  abs_resid = model_abs_resid.sort_values(ascending=False)\n  abs_resid_top_3 = abs_resid[:3]\n  for i in abs_resid_top_3.index:\n      plot_lm_1.axes[0].annotate(i,\n                                 xy=(model_fitted_y[i],\n                                     model_residuals[i]));\n\n  QQ = ProbPlot(model_norm_residuals)\n  plot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)\n  plot_lm_2.axes[0].set_title('Normal Q-Q')\n  plot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')\n  plot_lm_2.axes[0].set_ylabel('Standardized Residuals');\n  # annotations\n  abs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0)\n  abs_norm_resid_top_3 = abs_norm_resid[:3]\n  for r, i in enumerate(abs_norm_resid_top_3):\n      plot_lm_2.axes[0].annotate(i,\n                                 xy=(np.flip(QQ.theoretical_quantiles, 0)[r],\n                                     model_norm_residuals[i]));\n\n  plot_lm_3 = plt.figure()\n  plt.scatter(model_fitted_y, model_norm_residuals_abs_sqrt, alpha=0.5);\n  sns.regplot(model_fitted_y, model_norm_residuals_abs_sqrt,\n              scatter=False,\n              ci=False,\n              lowess=True,\n              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8});\n  plot_lm_3.axes[0].set_title('Scale-Location')\n  plot_lm_3.axes[0].set_xlabel('Fitted values')\n  plot_lm_3.axes[0].set_ylabel('$\\sqrt{|Standardized Residuals|}$');\n\n  # annotations\n  abs_sq_norm_resid = np.flip(np.argsort(model_norm_residuals_abs_sqrt), 0)\n  abs_sq_norm_resid_top_3 = abs_sq_norm_resid[:3]\n  for i in abs_norm_resid_top_3:\n      try:\n       plot_lm_3.axes[0].annotate(i,\n                                 xy=(model_fitted_y[i],\n                                     model_norm_residuals_abs_sqrt[i]));\n      except:\n          pass\n\n  plot_lm_4 = plt.figure();\n  plt.scatter(model_leverage, model_norm_residuals, alpha=0.5);\n  sns.regplot(model_leverage, model_norm_residuals,\n              scatter=False,\n              ci=False,\n              lowess=True,\n              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8});\n  plot_lm_4.axes[0].set_xlim(0, max(model_leverage)+0.01)\n  plot_lm_4.axes[0].set_ylim(-3, 5)\n  plot_lm_4.axes[0].set_title('Residuals vs Leverage')\n  plot_lm_4.axes[0].set_xlabel('Leverage')\n  plot_lm_4.axes[0].set_ylabel('Standardized Residuals');\n\n  # annotations\n  leverage_top_3 = np.flip(np.argsort(model_cooks), 0)[:3]\n  for i in leverage_top_3:\n      plot_lm_4.axes[0].annotate(i,\n                                 xy=(model_leverage[i],\n                                     model_norm_residuals[i]));\n\n  p = len(model_fit.params) # number of model parameters\n  graph(lambda x: np.sqrt((0.5 * p * (1 - x)) / x),\n        np.linspace(0.001, max(model_leverage), 50),\n        'Cook\\'s distance') # 0.5 line\n  graph(lambda x: np.sqrt((1 * p * (1 - x)) / x),\n        np.linspace(0.001, max(model_leverage), 50)) # 1 line\n  plot_lm_4.legend(loc='upper right');\n\n\ntrain = pd.read_csv('../input/see-click-predict-fix/train.csv', sep=',')\ntrain = train[(train['num_votes'] < 50)]\ntrain['description'].fillna(' ',inplace=True)\n#train = train[~train['tag_type'].isna()]\ntrain.reset_index(inplace=True)\n\ntrain.loc[:,'latitude'] = train['latitude'].apply(lambda x: np.round(x,3))\ntrain.loc[:,'longitude'] = train['longitude'].apply(lambda x: np.round(x,3))\n\ntrain['hour'] = [str(pd.to_datetime(x).hour) for x in train['created_time']]\ntrain['dayofweek'] = [str(pd.to_datetime(x).weekday()) for x in train['created_time']]\ntrain['year'] = [str(pd.to_datetime(x).year) for x in train['created_time']]\n\nfrom sklearn.preprocessing import OneHotEncoder\ndummies = OneHotEncoder(categories='auto')\n\nlatitude_train = train['latitude']\nlongitude_train = train['longitude']\n\nonehot_train = np.stack((train['hour'],train['dayofweek'],train['year'],latitude_train,longitude_train),axis=-1)\nonehot_train = dummies.fit_transform(onehot_train)\n\ntext_fields = onehot_train\n\n\n############################### StatsModels ###################################\n######### Generating Diagnostic Plots for the data #################\n\n#y_train = train['num_votes'].apply(lambda x: np.log(x))\ny_train = train['num_votes']\n#\nrows = int(.5 * (text_fields.get_shape()[0]))\ntext_fields = text_fields[0:rows,:]\ntext_fields = text_fields.toarray()\ndiagnostic_plots(text_fields,y_train[0:rows])\nplt.show()\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}