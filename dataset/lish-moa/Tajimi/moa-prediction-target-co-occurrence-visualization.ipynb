{"cells":[{"metadata":{},"cell_type":"markdown","source":"# MoA Prediction: Target Co-occurrence Visualization\nThis notebook aims to gain new insights by visualizing co-occurrence between two targets."},{"metadata":{},"cell_type":"markdown","source":"## Set Up"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom pathlib import Path\nimport itertools\nimport collections\nimport pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/lish-moa/train_targets_scored.csv\",index_col=\"sig_id\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Target Occurrence Histgram\n- Most targets occured less than 100 times.\n- `nfkb_inhibitor` and `proteasome_iinhibitor`　occured much more frequently than others!"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_sum = df.sum(axis=0)\ntarget_sum.hist(bins=50)\ntarget_sum.sort_values(ascending=False)[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Target Combination\n## How we get to know the relationship between two targets?\nWe can apply Sørensen–Dice coefficient, which is well known in natural language processing. This coefficient represents the ratio of the average number of elements and the number of common elements in the two sets.\n\n$$\\mathrm{Dice\\ Coef}. = \\frac{2n(A\\cap B)}{n(A)+n(B)}$$\n\n$$0\\leq \\mathrm{Dice\\ Coef.}\\leq 1$$\n\nWhen the Dice coef. is high, the two sets indicate that the proportion of common elements is large. Therefore, the two targets are likely to co-occur."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.columns:\n    df[col] = df[col].replace(1,col)\nsentences=df.iloc[:,:].replace(0,np.nan).stack().groupby(level=0).apply(list).tolist()\n\nsentences = [sentence for sentence in sentences if len(sentence) > 1]\nsentence_combinations = [list(itertools.combinations(sentence, 2)) for sentence in sentences]\nsentence_combinations = [[tuple(sorted(words)) for words in sentence] for sentence in sentence_combinations]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_combinations = []\nfor sentence in sentence_combinations:\n    target_combinations.extend(sentence)\n\ncombi_count = collections.Counter(target_combinations)\n\nword_associates = []\nfor key, value in combi_count.items():\n    word_associates.append([key[0], key[1], value])\n\nword_associates = pd.DataFrame(word_associates, columns=['word1', 'word2', 'intersection_count'])\n\ntarget_words = []\nfor word in target_combinations:\n    target_words.extend(word)\n\nword_count = pd.DataFrame((df!=0).sum(axis=0),columns=[\"count\"])\nword_count[\"word\"] = word_count.index\n\nword_associates = pd.merge(word_associates, word_count, left_on='word1', right_on='word', how='left')\nword_associates.drop(columns=['word'], inplace=True)\nword_associates.rename(columns={'count': 'count1'}, inplace=True)\nword_associates = pd.merge(word_associates, word_count, left_on='word2', right_on='word', how='left')\nword_associates.drop(columns=['word'], inplace=True)\nword_associates.rename(columns={'count': 'count2'}, inplace=True)\n\n# calc Dice coefficient\nword_associates['union_count'] = word_associates['count1'] + word_associates['count2'] - word_associates['intersection_count']\nword_associates['dice_coefficient'] = word_associates['intersection_count'] * 2 / (word_associates['count1'] + word_associates[\"count2\"])\n\nprint('Get Dice coefficient')\nprint(word_associates.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_associates.dice_coefficient.hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### High Dice Coef.\nThese 4 combinations are likely to co-occur. We may be able to treat these targets specially. (e.g. Combine the two labels into one)"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_associates=word_associates.sort_values(by=\"dice_coefficient\",ascending=False)\nword_associates[word_associates.dice_coefficient>=0.6]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot The Frequency Between Two Targets\nSome targets frequency is very low. We need to discuss after removing these targets because they may be noizy."},{"metadata":{"trusted":true},"cell_type":"code","source":"dice_coefficients = word_associates['dice_coefficient']\ngroup_numbers = []\nfor coefficient in dice_coefficients:\n    if coefficient < 0.025:\n        group_numbers.append(0)\n    elif coefficient < 0.04:\n        group_numbers.append(1)\n    elif coefficient < 0.08:\n        group_numbers.append(2)\n    elif coefficient < 0.15:\n        group_numbers.append(3)\n    else:\n        group_numbers.append(4)\nword_associates['group_number'] = group_numbers\n\nword_associates_group_sum = word_associates.groupby('group_number').count()\nword_associates_group_sum.reset_index(inplace=True)\nprint(word_associates_group_sum.loc[:, ['group_number', 'word1']])\nprint('')\n#word_associates.loc[:, ['count1', 'count2', 'group_number']]\nword_associates.group_number.value_counts()\nsns.pairplot(hue='group_number', data=word_associates.loc[:, ['count1', 'count2', 'group_number']])\n#plt.savefig(image_dir_path.joinpath(base_file_name+'_jaccard_group_plot.png'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_network(data, edge_threshold=0., n_word_lower = 10, fig_size=(30, 20)):\n    data = data.query('count1 >= @n_word_lower & count2 >= @n_word_lower')\n    data = data.rename(columns={'word1':'node1', 'word2':'node2', 'dice_coefficient':'value'})\n    nodes = list(set(data['node1'].tolist()+data['node2'].tolist()))\n\n    G = nx.Graph()\n    G.add_nodes_from(nodes)\n\n    for i in range(len(data)):\n        row_data = data.iloc[i]\n        if row_data['value'] > edge_threshold:\n            G.add_edge(row_data['node1'], row_data['node2'], weight=row_data['value'])\n\n    isolated = [n for n in G.nodes if len([i for i in nx.all_neighbors(G, n)]) == 0]\n    for n in isolated:\n        G.remove_node(n)\n\n    plt.figure(figsize=fig_size)\n    pos = nx.spring_layout(G, k=0.6)\n\n    pr = nx.pagerank(G)\n\n    nx.draw_networkx_nodes(G, pos, node_color=list(pr.values()),\n                           cmap=plt.cm.Reds,\n                           alpha=0.8,\n                           node_size=[60000*v for v in pr.values()])\n\n    nx.draw_networkx_labels(G, pos, fontsize=12, font_weight=\"bold\")\n    \n    edge_width = [d[\"weight\"] * 30 for (u, v, d) in G.edges(data=True)]\n    nx.draw_networkx_edges(G, pos, alpha=0.5, edge_color=\"grey\", width=edge_width)\n    plt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize Co-occurrence Between Each Targets\n- The color depth of node represents centrality.\n- The thickness of the edge represents the size of the dice coef.\n\n## Result\nWe can see the strong relationships between `flt3_inhibitor`, `pdgfr_inhibitor`, and `kit_inhibitor`. There is also a strong relationship between `nfkb_inhibitor` and `proteasome_inhibitor`.\n\n### Parameters\n- `edge_threshold`: The threshold about Dice Coef.\n- `n_word_lower`: The threshold about target frequency"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_network(data=word_associates, edge_threshold=0.04, n_word_lower=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Tobe\n- Survey between target_scored and target_nonscored."},{"metadata":{},"cell_type":"markdown","source":"# Version\n- v1: Just submit"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Acknowledgement\n\nThanks for\n- https://www.dskomei.com/entry/2019/04/07/021028\n- https://mieruca-ai.com/ai/jaccard_dice_simpson/"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}