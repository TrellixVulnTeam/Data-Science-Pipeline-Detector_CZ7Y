{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Check your OOF predictions before Stacking\n\n\nAs we are getting close to the end of the competition \"stacking\" becomes a hot subject. \n\nFrom the public kernels/discusions so far I understand that stacking has not shown any significant improvement. My early attempt also wasn't succesful, so I decided to dig a bit more.. \n\nThis remind me directly a very clever kernel by @ogrellier (Olivier) in Toxic Comments Classification competition where he is investigating distribution differences between OOF, OOF folds and Test set predictions. \n\nSee here: [things-you-need-to-be-aware-of-before-stacking](https://www.kaggle.com/ogrellier/things-you-need-to-be-aware-of-before-stacking/notebook)  \n\n\nSo I decided to do the same for MoA and publish a kernel with my findings in order to stimulate a discussion and get your feedback about this topic. \nAs Olivier said in his nb:\n>  I believe we have to tackle this issue before successfully stacking models."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightblue; border:0; color:black' role=\"tab\" aria-controls=\"home\"><center>Table of Contents</center></h2>\n\n- [Reading OOF Predictions](#2)     \n- [OOF vs Test distributions](#3)\n- [F1-scores vs Probability thresholds](#4)\n- [Conclusions](#5)\n- [References](#6)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os, sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nfrom sklearn.metrics import roc_auc_score, roc_curve, log_loss, auc\nfrom sklearn.model_selection import KFold\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport bokeh\nfrom bokeh.io import output_file, show\nfrom bokeh.layouts import column, gridplot\nfrom bokeh.plotting import figure\nfrom bokeh.palettes import brewer\nfrom bokeh.models.tools import HoverTool\nbokeh.io.output_notebook()\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Function to calculate the mean log loss of the targets including clipping\n\ndef mean_log_loss(y_true, y_pred):\n    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    metrics = []\n    for target in range(206):\n        metrics.append(log_loss(y_true[:, target], y_pred[:, target], labels=[0,1]))\n    return np.mean(metrics)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"FILTER = False # True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n# test_features = pd.read_csv('../input/lish-moa/test_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n# train_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n# train_drug = pd.read_csv('../input/lish-moa/train_drug.csv')\nsubmission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\nif FILTER:\n    # Filter non_ctl samples - to allign with the training pipeline that produced OOF\n    train_features = train_features.loc[train_features.cp_type!='ctl_vehicle']\n    train_targets_scored = train_targets_scored.iloc[train_features.index]\n\n    train_features = train_features.reset_index(drop=True)\n    train_targets_scored = train_targets_scored.reset_index(drop=True)\n\ntrain_targets_scored = train_targets_scored.set_index('sig_id')    \n# make labels \nclass_true = submission.columns[1:]\nclass_preds = [c_ + \"_oof\" for c_ in class_true]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightblue; border:0; color:black' role=\"tab\" aria-controls=\"home\"><center> Read OOF predictions </center></h2>\n"},{"metadata":{},"cell_type":"markdown","source":"Note: For demo I've just used a pair (OOF/Sub) of my own experiments (private dataset). In the next version(s) I'll try some of the public kernels too. "},{"metadata":{"trusted":true},"cell_type":"code","source":"DIR_OOF = '../input/moa-subs-oof/'\n\n# Read oof data \noof = pd.read_csv(DIR_OOF +\"10-oof.csv\",  ) # index_col='sig_id'\noof.index = train_features.sig_id.values\n# oof = oof.iloc[:, 1:]\noof.columns = class_preds\n\n# Read submission/test data \nsub = pd.read_csv(DIR_OOF +\"10-submission.csv\")\n\nprint('OOF loaded - CV score (with ctl):', np.round(mean_log_loss(train_targets_scored[class_true].values, oof.values), 7)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge OOF preds with targets\noof = oof.merge(train_targets_scored, left_index=True, right_index=True)\n\n# sanity check\noof[class_true].shape, oof[class_preds].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create your CV folds as in the training pipeline where you produced the OOF/Sub files"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create your CV folds as in the training pipeline where you produced OOF/Sub files\n\nskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=34)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightblue; border:0; color:black' role=\"tab\" aria-controls=\"home\"><center> OOF vs Test distributions (per target) </center></h2>\n"},{"metadata":{},"cell_type":"markdown","source":"> The problem now is we don't know if there are even further differences between OOF probabilities and Test predictions. If this is the case this would undermine any stacking attempt.\n\nLet see if we can see anything interesting in the probability distributions themselves?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"\nfigures = []\nfor i_class, class_name in enumerate(class_true):\n    \n    s = figure(plot_width=700, plot_height=300, title=f\"Probability logits [Target: {class_name}], [ID:{i_class}]\")\n    # per fold\n    for n_fold, (_, val_idx) in enumerate(skf.split(train_features, train_targets_scored)):\n        probas = oof[class_preds[i_class]].values[val_idx]\n        p_log = np.log((probas + 1e-5) / (1 - probas + 1e-5))\n        hist, edges = np.histogram(p_log, density=True, bins=50)\n        s.line(edges[:50], hist, legend_label=\"Fold %d\" % n_fold, color=brewer[\"Set1\"][7][n_fold])\n    # all oof\n    oof_probas = oof[class_preds[i_class]].values\n    oof_logit = np.log((oof_probas + 1e-5) / (1 - oof_probas + 1e-5))\n    hist, edges = np.histogram(oof_logit, density=True, bins=50)\n    s.line(edges[:50], hist, legend_label=\"Full OOF\", color=brewer[\"Paired\"][6][1], line_width=3)\n    # test \n    sub_probas = sub[class_name].values\n    sub_logit = np.log((sub_probas + 1e-5) / (1 - sub_probas + 1e-5))\n    hist, edges = np.histogram(sub_logit, density=True, bins=50)\n    s.line(edges[:50], hist, legend_label=\"Test\", color=brewer[\"Paired\"][6][5], line_width=3)\n    # fig specs\n    s.legend.location = 'top_left'\n#     s.legend.click_policy = 'hide'\n    s.add_tools(HoverTool(tooltips=[('(x, y)', '(@x, @y)')]))\n    figures.append(s)\n\n# put the results in a column and show\nshow(column(figures))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see here we have some differences between OOF and Test distributions for some of the targets (e.g. no. 4,5,10,20,30,71,90, 94, 97 etc.). \n\nOther OOF/Test pairs have shown:\n\n- spikes at certain values (left tail `~ -10.0` as here and middle e.g. `~ -7.0`) \n\n> At lest, this should give us a way to check if what we do in OOF will translate to test probabilities... or not!\n\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightblue; border:0; color:black' role=\"tab\" aria-controls=\"home\"><center> F1-scores vs probability thresholds  </center></h2>"},{"metadata":{},"cell_type":"markdown","source":"Next, we plot all F1-scores vs. thresholds for each target - unfold to see the plots \n\nWarning: there are a lot and look bit ugly :-)"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"\nfigures = []\nfor i_class, class_name in enumerate(class_true):\n    # create a new plot for current class\n    # Compute full score :\n    full = roc_auc_score(oof[class_true[i_class]], oof[class_preds[i_class]])\n    \n    s = figure(plot_width=750, plot_height=280, title=\"F1 score vs threshold [Target: %s]: full OOF score: %.6f\" % (class_name, full))  #avg\n    \n    for n_fold, (_, val_idx) in enumerate(skf.split(train_features, train_targets_scored)):\n        # Get False positives, true positives and the list of thresholds used to compute them\n        fpr, tpr, thresholds = roc_curve(oof[class_true[i_class]].iloc[val_idx], \n                                         oof[class_preds[i_class]].iloc[val_idx])\n        # Compute recall, precision and f1_score\n        recall = tpr\n        precision = tpr / (fpr + tpr + 1e-5)\n        f1_scores = 2 * precision * recall / (precision + recall + 1e-5)\n        # Finally plot the f1_scores against thresholds\n        s.line(thresholds, f1_scores, legend_label=\"Fold %d\" % n_fold, color=brewer[\"Set1\"][7][n_fold])\n        s.legend.location = 'top_right'\n        s.legend.click_policy = 'hide'\n        s.add_tools(HoverTool(tooltips=[('(x, y)', '(@x, @y)')]))\n    figures.append(s)\n\n# put the results in a column and show\nshow(column(figures))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightblue; border:0; color:black' role=\"tab\" aria-controls=\"home\"><center> Conclusion </center></h2>\n\n\n> As a conclusion, I would say that OOF probabilities need to be aligned before any stacking. \n\n* I'd  suggest to check yours too! Fork the kernel and replace the OOF/Sub pair with yours to start the analysis.  \n\n* I'd apprecate any feedback/suggestions for improvement and of course any further insights & explanations"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightblue; border:0; color:black' role=\"tab\" aria-controls=\"home\"><center> References/Credits </center></h2>\n\n\n* [1] [things-you-need-to-be-aware-of-before-stacking](https://www.kaggle.com/ogrellier/things-you-need-to-be-aware-of-before-stacking/notebook) by Olivier (@ogrellier), Toxic Comments Classification"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}