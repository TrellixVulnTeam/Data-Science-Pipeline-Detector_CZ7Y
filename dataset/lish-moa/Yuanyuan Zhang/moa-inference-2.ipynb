{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/lish-moa'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TabNet\n!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport matplotlib.pyplot as plt\nimport copy\nimport seaborn as sns\n\nfrom sklearn.metrics import log_loss\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _Loss, _WeightedLoss\n\nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\nfrom scipy.optimize import dual_annealing, minimize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    \nseed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_drug = pd.read_csv('../input/lish-moa/train_drug.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = train_drug.merge(train_features, on='sig_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_columns = [g for g in train_features.columns if 'g-' in g]\nc_columns = [c for c in train_features.columns if 'c-' in c]\n\ndrug_id_columns = [i for i in train_features.columns if 'drug_id' in i]\n\ncp_type_columns = [i for i in train_features.columns if 'cp_type' in i]\ncp_time_columns = [i for i in train_features.columns if 'cp_time' in i]\ncp_dose_columns = [i for i in train_features.columns if 'cp_dose' in i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_hot_encode(train, test, cols):\n    full_data = pd.concat([train, test], ignore_index=False)\n    \n    dummy = pd.get_dummies(full_data[cols], columns = cols)\n    dummy_columns = list(dummy.columns)\n    \n    train = pd.concat([train, dummy.iloc[:train.shape[0]]], axis=1)\n    test = pd.concat([test, dummy.iloc[train.shape[0]:]], axis=1)\n    \n    return train, test, dummy_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features, test_features, dummy_columns = one_hot_encode(train_features, test_features, ['cp_time', 'cp_dose'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RankGauss\ntransformer = QuantileTransformer(n_quantiles=100, random_state=42, output_distribution = 'normal')\ntrain_features[g_columns + c_columns] = transformer.fit_transform(train_features[g_columns + c_columns].values)\ntest_features[g_columns + c_columns] = transformer.transform(test_features[g_columns + c_columns].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pcaprocess(train, test, cols, pct, seed = 42):\n    pca = PCA(n_components=pct, random_state=seed)\n    train_transformed = pca.fit_transform(train[cols])\n    \n    pca_columns = [f\"pca-{i}\" for i in range(train_transformed.shape[1])]\n    \n    train = pd.concat([train, pd.DataFrame(train_transformed, index = train.index, columns=pca_columns)], axis=1)\n    \n    test_transformed = pca.transform(test[cols])\n    test = pd.concat([test, pd.DataFrame(test_transformed, index = test.index, columns=pca_columns)], axis=1)\n    \n    return train, test, pca_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features, test_features, g_pca_columns = pcaprocess(train_features, test_features, g_columns, 0.95)\ntrain_features.columns = ['g-' + col if col in g_pca_columns else col for col in train_features.columns]\ntest_features.columns = ['g-' + col if col in g_pca_columns else col for col in test_features.columns]\ng_pca_columns = ['g-' + col for col in g_pca_columns]\nprint(len(g_pca_columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features, test_features, c_pca_columns = pcaprocess(train_features, test_features, c_columns, 0.95)\ntrain_features.columns = ['c-' + col if col in c_pca_columns else col for col in train_features.columns]\ntest_features.columns = ['c-' + col if col in c_pca_columns else col for col in test_features.columns]\nc_pca_columns = ['c-' + col for col in c_pca_columns]\nprint(len(c_pca_columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def varprocess(train, test, cols, threshold):\n    var_thresh = VarianceThreshold(threshold) \n    var_thresh.fit(train[cols])\n    \n    small_var_columns = [cols[i] for i in list(var_thresh.get_support(indices=True))]\n    \n    return small_var_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_columns = varprocess(train_features, test_features, g_pca_columns + c_pca_columns, 0.8)\nprint(len(pca_columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = g_columns + c_columns + pca_columns + dummy_columns \nprint(len(feature_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols = train_targets_scored.columns[1:].values.tolist()\nlen(target_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Basic NN**"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=1500\n\nNFOLDS = 7\nBATCH_SIZE = 128\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nSEED = [0, 1, 2, 3 ,4, 5, 6]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        #self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.2)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    # This is used to solve na problem, which caused by weight_norm\n    def recalibrate_layer(self, layer):\n\n        if(torch.isnan(layer.weight_v).sum() > 0):\n            print ('recalibrate layer.weight_v')\n            layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n            layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n\n        if(torch.isnan(layer.weight).sum() > 0):\n            print ('recalibrate layer.weight')\n            layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n            layer.weight += 1e-7\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        #x = self.dropout1(x)\n        self.recalibrate_layer(self.dense1)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        self.recalibrate_layer(self.dense2)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        self.recalibrate_layer(self.dense3)\n        x = self.dense3(x)\n        \n        return x\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_predict(fold, seed):\n    \n    seed_everything(seed)\n    \n    val_idx = train[train['kfold'] == fold].index\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    x_valid =  valid_df[feature_cols].values\n    valid_dataset = TestDataset(x_valid)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    x_test = test[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.load_state_dict(torch.load(f\"../input/moatest1/nn-base-2/SEED{seed}_FOLD{fold}_.pth\", \n                                     map_location=torch.device(DEVICE)))\n    model.to(DEVICE)\n    \n    oof = np.zeros((len(train), len(target_cols)))\n    oof[val_idx] = inference_fn(model, validloader, DEVICE)\n    \n    predictions = np.zeros((len(test), len(target_cols)))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_predict(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logloss(y_true, y_pred):\n    score = 0\n    for i in range(len(target_cols)):\n        score_ = log_loss(y_true[:, i], y_pred[:, i])\n        score += score_ / len(target_cols)\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\noof_base = np.zeros((len(train), len(target_cols)))\npredictions_base = np.zeros((len(test), len(target_cols)))\ncv_loss = []\n\nfor seed in SEED:\n    if 'kfold' in train.columns:\n        train = train.drop(columns='kfold', axis=1)\n    folds = pd.read_csv(f'../input/moatest1/SEED{seed}_FOLDS.csv')\n    train = folds.merge(train, on='sig_id')\n  \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof_base += oof_ / len(SEED)\n    predictions_base += predictions_ / len(SEED)\n    loss_ = logloss(train[target_cols].values, oof_)\n    print(f\"SEED: {seed}, LOSS: {loss_}\")\n    cv_loss.append(loss_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(logloss(train[target_cols].values, oof_base))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4 Heads with Resnet**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataset:\n    def __init__(self, gen_features, gen_pca_features, cell_features, cell_pca_features):\n        self.gen_features = gen_features\n        self.gen_pca_features = gen_pca_features\n        self.cell_features = cell_features\n        self.cell_pca_features = cell_pca_features\n        \n    def __len__(self):\n        return (self.gen_features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'gen_x' : torch.tensor(self.gen_features[idx, :], dtype=torch.float),\n            'gen_pca_x' : torch.tensor(self.gen_pca_features[idx, :], dtype=torch.float),\n            'cell_x' : torch.tensor(self.cell_features[idx, :], dtype=torch.float),\n            'cell_pca_x' : torch.tensor(self.cell_pca_features[idx, :], dtype=torch.float)\n        }\n        return dct\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        gen_x = data['gen_x'].to(device)\n        gen_pca_x = data['gen_pca_x'].to(device)\n        cell_x = data['cell_x'].to(device)\n        cell_pca_x = data['cell_pca_x'].to(device)\n        \n        with torch.no_grad():\n            outputs = model(gen_x, gen_pca_x, cell_x, cell_pca_x)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_gen_features, num_gen_pca_features, \n                 num_cell_features, num_cell_pca_features, num_targets):\n        super(Model, self).__init__()\n        self.gen_batch_norm1 = nn.BatchNorm1d(num_gen_features)\n        #self.gen_dropout1 = nn.Dropout(0.2)\n        self.gen_dense1 = nn.utils.weight_norm(nn.Linear(num_gen_features, 512))\n        self.gen_batch_norm2 = nn.BatchNorm1d(512)\n        self.gen_dropout2 = nn.Dropout(0.1)\n        self.gen_dense2 = nn.utils.weight_norm(nn.Linear(512, 256))\n        \n        self.gen_pca_batch_norm1 = nn.BatchNorm1d(256 + num_gen_pca_features)\n        self.gen_pca_dropout1 = nn.Dropout(0.1)\n        self.gen_pca_dense1 = nn.utils.weight_norm(nn.Linear((256 + num_gen_pca_features), 256))\n        self.gen_pca_batch_norm2 = nn.BatchNorm1d(256)\n        #self.gen_pca_dropout2 = nn.Dropout(0.1)\n        self.gen_pca_dense2 = nn.utils.weight_norm(nn.Linear(256, 256))\n        \n        self.cell_batch_norm1 = nn.BatchNorm1d(num_cell_features)\n        #self.cell_dropout1 = nn.Dropout(0.2)\n        self.cell_dense1 = nn.utils.weight_norm(nn.Linear(num_cell_features, 128))\n        self.cell_batch_norm2 = nn.BatchNorm1d(128)\n        self.cell_dropout2 = nn.Dropout(0.1)\n        self.cell_dense2 = nn.utils.weight_norm(nn.Linear(128, 64))\n        \n        self.cell_pca_batch_norm1 = nn.BatchNorm1d(64 + num_cell_pca_features)\n        self.cell_pca_dropout1 = nn.Dropout(0.1)\n        self.cell_pca_dense1 = nn.utils.weight_norm(nn.Linear((64 + num_cell_pca_features), 64))\n        self.cell_pca_batch_norm2 = nn.BatchNorm1d(64)\n        #self.cell_pca_dropout2 = nn.Dropout(0.1)\n        self.cell_pca_dense2 = nn.utils.weight_norm(nn.Linear(64, 64))\n        \n        self.batch_norm3 = nn.BatchNorm1d(256+64)\n        #self.dropout3 = nn.Dropout(0.1)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(256+64, 256))\n        self.batch_norm4 = nn.BatchNorm1d(256)\n        self.dropout4 = nn.Dropout(0.1)\n        self.dense4 = nn.utils.weight_norm(nn.Linear(256, num_targets))\n        \n    def recalibrate_layer(self, layer):\n\n        if(torch.isnan(layer.weight_v).sum() > 0):\n            print ('recalibrate layer.weight_v')\n            layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n            layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n\n        if(torch.isnan(layer.weight).sum() > 0):\n            print ('recalibrate layer.weight')\n            layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n            layer.weight += 1e-7\n        \n    \n    def forward(self, gen_x, gen_pca_x, cell_x, cell_pca_x):\n        gen_x = self.gen_batch_norm1(gen_x)\n        #gen_x = self.gen_dropout1(gen_x)\n        self.recalibrate_layer(self.gen_dense1)\n        gen_x = F.leaky_relu(self.gen_dense1(gen_x))\n        gen_x = self.gen_batch_norm2(gen_x)\n        gen_x = self.gen_dropout2(gen_x)\n        self.recalibrate_layer(self.gen_dense2)\n        gen_x = F.leaky_relu(self.gen_dense2(gen_x))\n        \n        gen_pca_x = torch.cat((gen_x,gen_pca_x),dim=1)\n        gen_pca_x = self.gen_pca_batch_norm1(gen_pca_x)\n        gen_pca_x = self.gen_pca_dropout1(gen_pca_x)\n        self.recalibrate_layer(self.gen_pca_dense1)\n        gen_pca_x = F.leaky_relu(self.gen_pca_dense1(gen_pca_x))\n        gen_pca_x = self.gen_pca_batch_norm2(gen_pca_x)\n        #gen_pca_x = self.gen_pca_dropout2(gen_pca_x)\n        self.recalibrate_layer(self.gen_pca_dense2)\n        gen_pca_x = F.leaky_relu(self.gen_pca_dense2(gen_pca_x))\n        \n        gen = (gen_x + gen_pca_x) / 2\n\n        cell_x = self.cell_batch_norm1(cell_x)\n        #cell_x = self.cell_dropout1(cell_x)\n        self.recalibrate_layer(self.cell_dense1)\n        cell_x = F.leaky_relu(self.cell_dense1(cell_x))\n        cell_x = self.cell_batch_norm2(cell_x)\n        cell_x = self.cell_dropout2(cell_x)\n        self.recalibrate_layer(self.cell_dense2)\n        cell_x = F.leaky_relu(self.cell_dense2(cell_x))\n        \n        cell_pca_x = torch.cat((cell_x,cell_pca_x),dim=1)\n        cell_pca_x = self.cell_pca_batch_norm1(cell_pca_x)\n        cell_pca_x = self.cell_pca_dropout1(cell_pca_x)\n        self.recalibrate_layer(self.cell_pca_dense1)\n        cell_pca_x = F.leaky_relu(self.cell_pca_dense1(cell_pca_x))\n        cell_pca_x = self.cell_pca_batch_norm2(cell_pca_x)\n        #cell_pca_x = self.cell_pca_dropout2(cell_pca_x)\n        self.recalibrate_layer(self.cell_pca_dense2)\n        cell_pca_x = F.leaky_relu(self.cell_pca_dense2(cell_pca_x))\n              \n        cell = (cell_x + cell_pca_x) / 2\n        \n        x = torch.cat((gen,cell),dim=1)\n        x = self.batch_norm3(x)\n        #x = self.dropout3(x)\n        self.recalibrate_layer(self.dense3)\n        x = F.leaky_relu(self.dense3(x))\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        self.recalibrate_layer(self.dense4)\n        x = self.dense4(x)\n        return x\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_pca_columns_sub = g_pca_columns[:160]\nc_pca_columns_sub = c_pca_columns[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_predict(fold, seed):\n    \n    seed_everything(seed)\n    \n    val_idx = train[train['kfold'] == fold].index\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    gen_x_valid = valid_df[g_columns + dummy_columns].values\n    gen_pca_x_valid = valid_df[g_pca_columns_sub + dummy_columns].values\n    cell_x_valid = valid_df[c_columns + dummy_columns].values\n    cell_pca_x_valid = valid_df[c_pca_columns_sub + dummy_columns].values\n    \n    valid_dataset = TestDataset(gen_x_valid, gen_pca_x_valid, cell_x_valid, cell_pca_x_valid)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    gen_x_test = test[g_columns + dummy_columns].values\n    gen_pca_x_test = test[g_pca_columns_sub + dummy_columns].values\n    cell_x_test = test[c_columns + dummy_columns].values\n    cell_pca_x_test = test[c_pca_columns_sub + dummy_columns].values\n    \n    testdataset = TestDataset(gen_x_test, gen_pca_x_test, cell_x_test, cell_pca_x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_gen_features=len(g_columns + dummy_columns),\n        num_gen_pca_features=len(g_pca_columns_sub + dummy_columns),\n        num_cell_features=len(c_columns + dummy_columns),\n        num_cell_pca_features=len(c_pca_columns_sub + dummy_columns),\n        num_targets=len(target_cols)\n    )\n    \n    model.load_state_dict(torch.load(f\"../input/moatest1/nn-mh-2/SEED{seed}_FOLD{fold}_MH.pth\",\n                                    map_location=torch.device(DEVICE)))\n    model.to(DEVICE)\n    \n    oof = np.zeros((len(train), len(target_cols)))\n    oof[val_idx] = inference_fn(model, validloader, DEVICE)\n    \n    predictions = np.zeros((len(test), len(target_cols)))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\noof_mh = np.zeros((len(train), len(target_cols)))\npredictions_mh = np.zeros((len(test), len(target_cols)))\ncv_loss_mh = []\n\nfor seed in SEED:\n    if 'kfold' in train.columns:\n        train = train.drop(columns='kfold', axis=1)\n    folds = pd.read_csv(f'../input/moatest1/SEED{seed}_FOLDS.csv')\n    train = folds.merge(train, on='sig_id')\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof_mh += oof_ / len(SEED)\n    predictions_mh += predictions_ / len(SEED)\n    loss_ = logloss(train[target_cols].values, oof_)\n    print(f\"SEED: {seed}, LOSS: {loss_}\")\n    cv_loss_mh.append(loss_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(logloss(train[target_cols].values, oof_mh))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tabnet**"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_pca_columns_tabnet = g_pca_columns[:160]\nc_pca_columns_tabnet = c_pca_columns[:10]\n\ntabnet_cols = dummy_columns + g_columns + c_columns + g_pca_columns_tabnet + c_pca_columns_tabnet\nlen(tabnet_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test[tabnet_cols].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_predict(fold, seed):\n    seed_everything(seed)\n    \n    val_idx = train[train['kfold'] == fold].index\n\n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n\n    X_train, y_train = train_df[tabnet_cols].values, train_df[target_cols].values\n    X_val, y_val = valid_df[tabnet_cols].values, valid_df[target_cols].values\n\n    tabnet_params = dict(\n        n_d = 24,\n        n_a = 48,\n        n_steps = 1,\n        n_independent = 2,\n        n_shared = 1,\n        momentum = 0.02,\n        gamma = 1.3,\n        lambda_sparse = 0,\n        optimizer_fn = optim.Adam,\n        optimizer_params = dict(lr = 2.5e-2, weight_decay = 1e-5),\n        mask_type = \"entmax\",\n        scheduler_params = dict(\n            mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9, verbose=1),\n        scheduler_fn = torch.optim.lr_scheduler.ReduceLROnPlateau,\n        seed = seed,\n        verbose = 10\n    )\n\n    ### Model ###\n    model = TabNetRegressor(**tabnet_params)\n    \n    dir_name = f\"../input/moatest1/tabnet/SEED{seed}_FOLD{fold}_tabnet\"\n    !cp -r {dir_name}/* .\n    !zip tabnet.zip model_params.json network.pt\n    model.load_model('tabnet.zip')   \n\n    ### Predict on validation ###\n    preds_val = model.predict(X_val)\n    # Apply sigmoid to the predictions\n    preds = 1 / (1 + np.exp(-preds_val))\n    #score = np.min(model.history[\"val_logits_ll\"])\n\n    ### Save OOF for CV ###\n    oof = np.zeros((len(train), len(target_cols)))\n    oof[val_idx] = preds\n\n    ### Predict on test ###\n    preds_test = model.predict(X_test)\n    predictions = np.zeros((len(test), len(target_cols)))\n    predictions = 1 / (1 + np.exp(-preds_test))\n    \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\noof_tabnet = np.zeros((len(train), len(target_cols)))\npredictions_tabnet = np.zeros((len(test), len(target_cols)))\ncv_loss_tabnet = []\n\nfor seed in SEED:\n    if 'kfold' in train.columns:\n        train = train.drop(columns='kfold', axis=1)\n    folds = pd.read_csv(f'../input/moatest1/SEED{seed}_FOLDS.csv')\n    train = folds.merge(train, on='sig_id')\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof_tabnet += oof_ / len(SEED)\n    predictions_tabnet += predictions_ / len(SEED)\n    loss_ = logloss(train[target_cols].values, oof_)\n    print(f\"SEED: {seed}, LOSS: {loss_}\")\n    cv_loss_tabnet.append(loss_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cv_loss_tabnet)\nprint(logloss(train[target_cols].values, oof_tabnet))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(logloss(train[target_cols].values, oof_base))\nprint(logloss(train[target_cols].values, oof_mh))\nprint(logloss(train[target_cols].values, oof_tabnet))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_to_min(weights):\n    oof_blend = weights[0] * oof_base_i + (1-weights[0]-weights[1]) * oof_mh_i + weights[1] * oof_tabnet_i\n    score = log_loss(train[target_cols_i].values, oof_blend)\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ws = []\nbnds = [(0, 0.5) for _ in range(2)]\ninit_guess = [0.4, 0.2]\nfor i in range(len(target_cols)):\n    #print('optimizing for label %s'%i)\n    oof_base_i = oof_base[:, i]\n    oof_mh_i = oof_mh[:, i]\n    oof_tabnet_i = oof_tabnet[:, i]\n    target_cols_i = target_cols[i]\n    opt = minimize(loss_to_min, init_guess, method = 'L-BFGS-B', bounds = bnds)\n    ws.append(opt.x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_blend = np.zeros((len(train), len(target_cols)))\npred_blend = np.zeros((len(test), len(target_cols)))\nfor i in range(len(target_cols)):\n    w_base = ws[i][0]\n    w_mh = 1- ws[i][0] - ws[i][1]\n    w_tabnet = ws[i][1]\n    oof_blend[:,i] = w_base * oof_base[:,i] + w_mh * oof_mh[:,i] + w_tabnet * oof_tabnet[:,i]\n    pred_blend[:,i] = w_base * predictions_base[:,i] + w_mh * predictions_mh[:,i] + w_tabnet * predictions_tabnet[:,i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(logloss(train[target_cols].values, oof_blend))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mytest = test.copy()\nmytest[target_cols] = pred_blend","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = test_features[['sig_id']].merge(mytest[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}