{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport random\nimport math\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.metrics import log_loss\n\nimport category_encoders as ce\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport warnings\n#warnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission=pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\ntrain_targets_scored=pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored=pd.read_csv('/kaggle/input/lish-moa/train_targets_nonscored.csv')\ntrain_features=pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ntest_features=pd.read_csv('/kaggle/input/lish-moa/test_features.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.sig_id.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored.sum()[1:].sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that data is highly imbalanced"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = train_features[:1][[col for col in train_features if 'g-' in col]]\ng","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g= g.values.reshape(-1,1)\nplt.plot(g)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of features (g-) = \",train_features.columns.str.startswith('g-').sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c = train_features[:1][[col for col in train_features if 'c-' in col]]\nc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c=c.values.reshape(-1,1)\nplt.plot(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of features (c-) = \",train_features.columns.str.startswith('c-').sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see cp_type distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.subplot(1, 2, 1)\nx_value = train_features.cp_type.value_counts()\nplt.title(\"Training data\")\nplt.bar(x_value.index,x_value.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation representation among features of 'g-' and 'c-'"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train_features.loc[:, ['g-0', 'g-1', 'g-2','g-3','g-4', 'c-95', 'c-96', 'c-97','c-98', 'c-99']].corr(), annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_nonscored.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing training features vectors/values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for g- feature columns\ng = [col for col in train_features if 'g-' in col]\ng = sns.pairplot(train_features[g[:10]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for c- feature columns\ng_vis = sns.pairplot(train_features[[col for col in train_features if 'c-' in col][:10]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model Prediction"},{"metadata":{},"cell_type":"markdown","source":"> sample_submission=pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\n> train_targets_scored=pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\n> train_targets_nonscored=pd.read_csv('/kaggle/input/lish-moa/train_targets_nonscored.csv')\n> train_features=pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\n> test_features=pd.read_csv('/kaggle/input/lish-moa/test_features.csv')"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\n \nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 123\nEPOCHS = 49\nBATCH_SIZE = 256\nFOLDS =5\nREPEATS = 5\ntarget_cols = train_targets_scored.columns[1:]\ninput_shape = len(target_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed(seed):\n    np.random.seed(seed)\n    os.environ['seed'] = str(seed)\n    tf.random.set_seed(seed)\ndef multi_log_loss(y_true, y_pred):\n    losses = []\n    for col in y_true.columns:\n        losses.append(log_loss(y_true.loc[:, col], y_pred.loc[:, col]))\n    return np.mean(losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_input(data):\n    data['cp_type'] = (data['cp_type'] == 'trt_cp').astype(int)\n    data['cp_dose'] = (data['cp_dose'] == 'D2').astype(int)\n    return data\nx_train = preprocess_input(train_features.drop(columns=\"sig_id\"))\nx_test =preprocess_input(test_features.drop(columns=\"sig_id\"))\ny_train = train_targets_scored.drop(columns=\"sig_id\")\nN_FEATURES = x_train.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dnn_model():\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(N_FEATURES),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.3),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"relu\")),  \n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(input_shape, activation=\"sigmoid\"))\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001), loss='binary_crossentropy', metrics=[\"accuracy\"])\n    model.summary()\n    return model\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train.shape)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_train(resume_models = None, repeat_number = 0, folds = 5, skip_folds = 0):\n    \n    models = []\n    preds_df = y_train.copy()\n    \n\n    kfold = KFold(folds, shuffle = True)\n    for fold, (train_ind, val_ind) in enumerate(kfold.split(x_train)):\n        print('\\n')\n        print('-'*50)\n        print(f'Training fold {fold + 1}')\n        \n        cb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'binary_crossentropy', factor = 0.4, patience = 2, verbose = 1, min_delta = 0.0001, mode = 'auto')\n        checkpoint_path = f'repeat:{repeat_number}_Fold:{fold}.hdf5'\n        cb_checkpt = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True, save_weights_only = True, mode = 'min')\n\n        model = dnn_model()\n        model.fit(x_train.values[train_ind],\n              y_train.values[train_ind],\n              validation_data=(x_train.values[val_ind], y_train.values[val_ind]),\n              callbacks = [cb_lr_schedule, cb_checkpt],\n              epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1\n             )\n        model.load_weights(checkpoint_path)\n        preds_df.loc[val_ind, :] = model.predict(x_train.values[val_ind])\n        models.append(model)\n\n    return models, preds_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\npreds_df = []\n# seed everything\nseed(SEED)\nfor i in range(REPEATS):\n    m, f = build_train(repeat_number = i, folds=FOLDS)\n    models = models + m\n    preds_df.append(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_oof_preds = y_train.copy()\nmean_oof_preds.loc[:, target_cols] = 0\nfor i, p in enumerate(preds_df):\n    print(f\"Iterate{i + 1} Log Loss: {multi_log_loss(y_train, p)}\")\n    mean_oof_preds.loc[:, target_cols] += p[target_cols]\n\nmean_oof_preds.loc[:, target_cols] /= len(preds_df)\nprint(f\"Mean Log Loss: {multi_log_loss(y_train, mean_oof_preds)}\")\nmean_oof_preds.loc[x_train['cp_type'] == 0, target_cols] = 0\nprint(f\"Mean Log Loss: {multi_log_loss(y_train, mean_oof_preds)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = sample_submission.copy()\ntest_preds[target_cols] = 0\nfor model in models:\n    test_preds.loc[:,target_cols] += model.predict(x_test)\ntest_preds.loc[:,target_cols] /= len(models)\ntest_preds.loc[x_test['cp_type'] == 0, target_cols] = 0\ntest_preds.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}