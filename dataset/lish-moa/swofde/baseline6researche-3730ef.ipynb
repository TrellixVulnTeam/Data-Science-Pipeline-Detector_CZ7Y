{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\neps, weight_tg, weight_x3, weight_x5 = 10**(-3), 0.0, 0.0, 0.0 # last couple =0.0\nweight = 1.0 - weight_tg - weight_x3 - weight_x5\n\ndef transform(x):\n    x['cp_type'] = x[x['cp_type']=='trt_cp']\n    x.pop('cp_type')\n    x['cp_dose'] = x['cp_dose'].replace({'D1':-1, 'D2':1})\n    x['cp_time'] = x['cp_time'] // 24\n    x.join(pd.get_dummies(x['cp_time']))\n    x.pop('cp_time')\n    return x\ndef cube(x):\n    x /= 10.0\n    return 10.0 * x*x*x\n\ndef tgfunc(x):\n    return  np.tan(np.pi *x/ 4.0) \ndef application_transform(x):\n    x = 2*x-1.0\n    x1 = weight_tg*tgfunc(x) + (weight_x3 + weight_x5*x**2)*x**3 + weight*x\n    x1 = x1/2 + 0.5\n    if x1 < eps:\n        x1 = eps\n    elif x1 >= 1 - eps:\n        x1 = 1 - eps\n    return x1\n\nX_all = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ny_all = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\nwhatisit = pd.read_csv('/kaggle/input/lish-moa/train_targets_nonscored.csv')\nX = transform(X_all[X_all.columns[1:]])\nY = y_all[y_all.columns[1:]]\n\n\nfor random_state in range(2020,2070,10):\n    X_train, submission, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, \n                                                        random_state=random_state)\n    pseudoinverse_X = np.linalg.pinv(X_train)\n    subm = np.dot(submission,np.dot(pseudoinverse_X,Y_train))\n\n    dim = 206*len(submission) \n    df = pd.DataFrame(subm)\n    df = df.applymap(application_transform)\n    df = np.array(df).reshape(dim)\n    \n    print(log_loss(np.array(Y_test).reshape(dim),df))\n\nprint([eps, weight, weight_tg, weight_x3, weight_x5, len(X.columns)])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# argmax = []\n# B = np.dot(pseudoinverse_X,Y_train)\n# for i in range(B.shape[0]):\n#     for j in range(B.shape[1]):\n#         if B[i,j] > 0.005:\n#             argmax.append([i,j])\n# len(argmax)            \n\n\n### Здесь происходит добавление новых признаков x_i^3\n# X3 = X.applymap(cube)\n# X3.columns = ['cube'+ch for ch in X3.columns]\n# X = X.join(X3)\n\n## Здесь происходит добавление новых признаков ((1.0 + x_i/10.0)*(1.0 + x_j/10.0) - 1.0)*10.0\n# for t in argmax:\n#     i, j = t[0], t[1]\n#     X[str(i)+'-'+str(j)] = ((1.0+X[X.columns[i]]/10.0)*(1.0+X[X.columns[j]]/10.0)-2.0)*5.0 \n\n## Здесь происходит добавление новых признаков x_i + x_j\n# for t in argmax:\n#     i, j = t[0], t[1]\n#     X[str(i)+'-'+str(j)] = (X[X.columns[i]]*X[X.columns[j]])*0.1 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.linalg.svd(np.dot(pseudoinverse_X,Y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(num_columns):\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(num_columns),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1048, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation=\"sigmoid\"))\n    ])\n    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period=10),\n                  loss='binary_crossentropy', \n                  )\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def metric(y_true, y_pred):\n    metrics = []\n    for _target in train_targets.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n    return np.mean(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_STARTS = 7\ntf.random.set_seed(42)\n\nres = train_targets.copy()\nss.loc[:, train_targets.columns] = 0\nres.loc[:, train_targets.columns] = 0\n\nfor seed in range(N_STARTS):\n    for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits=7, random_state=seed, shuffle=True).split(train_targets, train_targets)):\n        print(f'Fold {n}')\n    \n        model = create_model(len(top_feats))\n        checkpoint_path = f'repeat:{seed}_Fold:{n}.hdf5'\n        \n        model.fit(train.values[tr][:, top_feats],\n                  train_targets.values[tr],\n                  validation_data=(train.values[te][:, top_feats], train_targets.values[te]),\n                  epochs=35, batch_size=128,\n                  callbacks=[reduce_lr_loss, cb_checkpt], verbose=2\n                 )\n        \n        model.load_weights(checkpoint_path)\n        test_predict = model.predict(test.values[:, top_feats])\n        val_predict = model.predict(train.values[te][:, top_feats])\n        \n        ss.loc[:, train_targets.columns] += test_predict\n        res.loc[te, train_targets.columns] += val_predict\n        print('')\n    \nss.loc[:, train_targets.columns] /= ((n+1) * N_STARTS)\nres.loc[:, train_targets.columns] /= N_STARTS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_all.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor random_state in range(2020,2070,10):\n    checkpoint_path = f'repeat:{random_state}.hdf5'\n    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n    cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n                                 save_weights_only = True, mode = 'min')\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, \n                                                            random_state=random_state)\n    \n    model = create_model(X.shape[1])\n    model.fit(X_train, Y_train,\n              validation_data=(X_test, Y_test), \n              epochs=35, batch_size=128,\n              callbacks=[reduce_lr_loss, cb_checkpt], verbose=2)\n    model.load_weights(checkpoint_path)\n    test_predict = model.predict(X_test)\n    \n    df = pd.DataFrame(subm)\n    df = df.applymap(application_transform)\n    df = np.array(df).reshape(dim)\n    \n    print(log_loss(np.array(Y_test),test_predict))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}