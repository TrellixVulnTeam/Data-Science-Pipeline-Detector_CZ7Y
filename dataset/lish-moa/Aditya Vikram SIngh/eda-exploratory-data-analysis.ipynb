{"cells":[{"metadata":{},"cell_type":"markdown","source":"* This will explore the dataset.\n* Will build a random model and see the performance. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df1 = pd.read_csv(\"../input/lish-moa/train_features.csv\")\ndf2 = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\ndf3 = pd.read_csv(\"../input/lish-moa/train_targets_nonscored.csv\")\n\nprint(\"shape of train_features.csv: \", df1.shape)\nprint(\"shape of train_targets_scored.csv: \", df2.shape)\nprint(\"shape of train_targets_nonscored.csv: \", df3.shape)\n\nprint(\"Total number of datapoints: {:,}\".format(df1.shape[0]))\n\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are **876** columns in `train_features.csv` file and out of these there are **875** features. \n* There are **207** labels that we need to predict for each datapoint.\n* Most of features look real-valued (continuous) and few of them look are categorical type. We'll see them detail.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data-type of features ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# knowing the data types of each column of `train_features.csv`\ndata_types = df1.dtypes\nunique_dtypes = data_types.unique()\nprint(\"number of dtypes in `train_features.csv`: \", len(unique_dtypes),\n      \"\\nAnd these are: \", unique_dtypes)\n\nObj   = []\nInt   = []\nFloat = []\nfor col, data_type in zip(df1.columns, data_types):\n    if data_type == 'object':Obj.append(col)        \n    elif data_type == 'int64':Int.append(col)\n    elif data_type == 'float64':Float.append(col)\nprint(\"number of object data type: \", len(Obj))\nprint(\"number of int64 data type: \", len(Int))\nprint(\"number of float64 data type: \", len(Float))\n\nassert len(Obj)+len(Int)+len(Float) == df1.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Obj, Int","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is only one column for `int64` column and `3` for the `object` datatype.\n* `cp_dose` and `cp_type` are categorical variable.\n* `cp_time` has `int64` datatype, but this one is also categorical variable. \n* Let's see the number of unique values in these columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of unique values in `cp_type` col is: {} and these are: {}\"\n      .format(len(df1.loc[:, \"cp_type\"].unique()), df1.loc[:, \"cp_type\"].unique()))\n\nprint(\"Number of unique values in `cp_dose` col is: {} and these are: {}\"\n      .format(len(df1.loc[:, \"cp_dose\"].unique()), df1.loc[:, \"cp_dose\"].unique()))\n\nprint(\"Number of unique values in `cp_time` col is: {} and these are: {}\"\n      .format(len(df1.loc[:, \"cp_time\"].unique()), df1.loc[:, \"cp_time\"].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* One hot encoding for `cp_type` and `cp_dose`.\n* `D1` == `0`, `D2`==`1`.\n* `trt_cp`== `0`, `ctl_vehicle` ==`.\n* Label encoding for`cp_time`, `0` for 24, `1` for 48 and `2` for 72.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1,3, figsize=(18,6))\n\nfig.suptitle(\"Count Plot of Categorical variables\", fontsize = 24)\nsns.countplot(x ='cp_type', data = df1, ax = axs[0])\naxs[0].set_title(\"For: cp_type\", fontsize= 16)\nsns.countplot(x ='cp_dose', data = df1, ax = axs[1])\naxs[1].set_title(\"For: cp_dose\", fontsize = 16)\naxs[1].set(ylabel = '')\nsns.countplot(x = 'cp_time', data = df1, ax = axs[2])\naxs[2].set_title(\"For: cp_time\", fontsize = 16)\naxs[2].set(ylabel = '')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* For both variables `cp_dose` and `cp_time` number of count is almost same for all unique values present in their columns.\n* But for `cp_type` there are very data points that corresponds to `ctl_vehicle`.\n\n**NOTE:** Use stratified spliting with `cp_type` column. This will maintain the `cp_type`'s uniques values count-ratio.  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# cp_type, cp_time, cp_dose\ndef encode_cp_time(row):\n    val = None\n    if row == 24:val = 1\n    elif row == 48:val = 2\n    else:val = 3\n    return val\n        \ndf1[\"cp_type\"] = df1[\"cp_type\"].apply(lambda x: 0 if x=='trt_cp' else 1)\ndf1[\"cp_dose\"] = df1[\"cp_dose\"].apply(lambda x: 0 if x=='D1' else 1)\ndf1[\"cp_time\"] = df1[\"cp_time\"].apply(encode_cp_time)\n\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA & Correlation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Let's normalise the features, first","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PCA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df1.iloc[:, 1:]\nx = StandardScaler().fit_transform(X)\nx = pd.DataFrame(data=x, columns = X.columns)\nx.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are `875` features, first let's reduce the dimension using **PCA**.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=None, svd_solver = 'full')\npca.fit_transform(x)\nvar = pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1,9):\n    print(\"Variance explained by top {} components: {}\".format(i*100, var[:i*100].sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* I would always prefer high variance explained (between 95-99%). But for that, I have to take atleast 600 components.\n* 800 components are explainig the 99.41% (approx) variance.\n* Now, one can select 100, 200, 300, .... any number of features and see the performance of model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Correlation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Source:** https://www.geeksforgeeks.org/exploring-correlation-in-python/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corrmat = X.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1,1, figsize =(18, 8))\nsns.heatmap(corrmat, ax = ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From above plot, it can be see that the features that starts with `c-`  looks highly correlated to each other. These are also shwoing strong correaltion with other features also.\n* Let's see the heatmap of features that starts with `c-`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = [col for col in corrmat.columns if col.startswith('c-')]\nf, ax = plt.subplots(1,1, figsize =(18, 8))\nsns.heatmap(corrmat.loc[cols, cols], ax = ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The most of thefeatures that start with `c-` are highly correlated.\n\n**NOTE:** If you're using all features, use tree-based model or high dropout in first (just after input) in neural-network architecture.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Random Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* This will predict the random value between 0 and 1 (inclusive, i.e. 0<=prob<=1) for each label of each datapoints.\n* We'll calculate `the worst model` performance. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\ndef total_loss(y_true, y_pred):\n    \"\"\"\n    y_true: numpy nd-array of shape (None , 206), None means any value\n    y_pred: numpy nd-array of shape (None , 206)\n    \"\"\"\n    losses = []\n    for i in range(y_true.shape[1]):losses.append(log_loss(y_true[:,i], y_pred[:,i], eps=1e-15))\n    return np.mean(losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/lish-moa/test_features.csv\")\nprint(\"number of test datapoints: {:,}\".format(df_test.shape[0]))\n\ny_train = df2.iloc[:, 1:].values\ny_train_pred = np.random.random_sample(y_train.shape) \ny_test_pred = np.random.random_sample((df_test.shape[0], y_train.shape[1])) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_loss = total_loss(y_train, y_train_pred)\nprint(\"train loss: \", tr_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.DataFrame(data = y_test_pred, columns = df2.columns[1:])\ntemp = pd.DataFrame(data=df_test.loc[:, 'sig_id'])\ntest_df = pd.concat([temp, test_df], ignore_index=False, axis=1)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.to_csv(\"./submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting the data**\n\n```Python\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import hstack\n\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\ndf_train = df1.merge(df2, on = 'sig_id')\nprint(\"shape: \", df_train.shape)\ndf_train.head()\n\ntrain, val = train_test_split(df_train, test_size = 0.2, random_state=42,\n                             stratify = df_train[\"cp_type\"])\nX_train, Y_train = df_train.iloc[:, :-206], df_train.iloc[:, -206:]\nX_val, Y_val = val.iloc[:, :-206], val.iloc[:, -206:]\n\nprint(\"Number of datapoints in train-set: {:,}\".format(len(X_train)))\nprint(\"Number of datapoints in val-set: {:,}\".format(len(X_val)))\n```","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}