{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.covariance import MinCovDet\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df=pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\n#df2=pd.read_csv('/kaggle/input/lish-moa/train_targets_nonscored.csv')\ntarget_df=pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ntest_df =pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\ntrain_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom scipy import stats\nnums=[]   \nfor i in range(206):\n    a=target_df2.iloc[:,i]\n    num=0\n    col=[]\n    for j in train_df2:\n        \n        one_data=train_df2.loc[a[a==1].index,j]\n        zero_data=train_df2.loc[a[a==0].index,j]\n        a2=stats.mannwhitneyu(one_data,zero_data, alternative='two-sided')\n        if a2.pvalue <0.05:\n            num+=1\n            col.append(j)\n    if num ==0:\n        nums.append(i)\n    print(i,\"は有意差が生じる説明変数が\",num,\"個で\",col)\n    print(\"---next_target------\")\nprint(\"有意差が生じない目的変数は\",nums)\n\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tar=target_df.set_index('sig_id')\ntar2=tar.sum()\ntar2.plot(kind=\"hist\")\n#tar2.plot(kind=\"box\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ラベルの数が結構まちまちなので、３つぐらいにグループ分けしてモデルを作ってみる"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ダウンサンプリング #df2はtargetでなければいけいない\ndef down_sampling(df1,df2,percent=0.5,kotei=True):\n    d1 = df1.copy()\n    d2 = df2.copy()\n    num = int(len(df1)*percent)\n    if ('sig_id' in d1.columns) or ('sig_id' in d2.columns):\n        d1 = d1.set_index('sig_id')\n        d2 = d2.set_index('sig_id')\n    \n    dd1 = d1.sample(n=num)\n    dd2 = d2.loc[dd1.index]\n    return dd1,dd2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def proceccing2(train3,mcd=None,jogen=None):\n    train4 = train3.copy()\n    if mcd==None:\n        mcd =MinCovDet(support_fraction=0.7)\n        mcd.fit(train4)\n    dis = mcd.mahalanobis(train4)\n    \n    \n    se=pd.Series(dis,index=train4.index)\n    \n    if jogen==None:\n        num =int(len(se)*0.01)\n        #    print(num)\n        iqr = se.quantile(0.75)-se.quantile(0.25)\n        jogen2= se.quantile(0.75) + 1.5*iqr\n        se_idx=se[se>jogen2].index\n        \n    else:\n        se_idx=se[se>jogen].index\n        jogen2=0\n    train4[\"out_flag\"]= 0\n    train4.loc[se_idx,\"out_flag\"]=1\n    return train4,mcd,jogen2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ダミー変数化を行い。主成分分析でcomp分に列を纏める\ndef proceccing(d,comp=10,test=False,mcd=None,):\n    df = d.copy()\n    data={\"trt_cp\":0,'ctl_vehicle':1}\n    df['cp_type']=df['cp_type'].map(data)\n    data={\"D1\":0,'D2':1}\n    df['cp_dose']=df['cp_dose'].map(data)\n    #df=df.drop('sig_id',axis=1)\n    pca = PCA(n_components=comp,random_state=0,whiten=False)\n    train2 = df.set_index('sig_id')\n    pca.fit(train2)\n    train3=pd.DataFrame(pca.transform(train2))\n    train3.index= train2.index\n    \n    return train3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#引数の仕様df1とdf2はインデックスでつなげる事が出来る。\ndef under_sampling(df1,df2):#df2がtargetの1列のシリーズ\n    d1 = df1.copy()\n    d2 = df2.copy()\n    \n    #positiveの行を全て抜き出す。\n    dd2_1 = d2[d2==1]\n    num_1 = len(dd2_1)#1の行の総数\n    \n    a = d2[d2==0]\n    num_0 =len(a)#0の行の数\n    if num_1*20 < num_0:\n        num =num_1*20\n    else:\n        num = num_0\n    \n    ##negativeの行を全て抜き出す。\n    dd2_0 = d2[d2==0]\n    if num != 0:\n        #0の行をランダムに抜き出す（件数はnumの10倍)\n        dd2_0_2 = dd2_0.sample(random_state=10,n=num)\n    else:\n        #1\n        dd2_0_2 = dd2_0.sample(random_state=10,n=100)\n    \n    #d1からpositive分とネガティブ分のデータを抜き出す\n    d1_1 =d1.loc[dd2_1.index]\n    d1_0 = d1.loc[dd2_0_2.index]\n    \n    \n    d11 = pd.concat([d1_1,d1_0])#訓練データの結合（行方向）\n    d22 = pd.concat([dd2_1,dd2_0_2])#訓練データの結合（行方向）\n    return d11,d22","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation,Dropout\nfrom keras.optimizers import SGD,Adam\nfrom keras.layers.normalization import BatchNormalization\nfrom sklearn.preprocessing import StandardScaler\n\ntrain_df2 = proceccing(train_df,comp=100)#pca\ntrain_df2,mcd,jogen=proceccing2(train_df2)#外れ値フラグを立てる\ntarget_df2 = target_df.set_index('sig_id')\n    \nsc_x =StandardScaler()\n#sc_t =StandardScaler()\nsc_df2=sc_x.fit_transform(train_df2)\n#sc_target2=sc_t.fit_transform(target_df2)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#t=target_df2.iloc[:,i]\nmodel = Sequential()\nmodel.add(Dense(units=64, activation='relu', input_dim=101))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(rate=0.5))\nmodel.add(Dense(units=32, activation='relu', input_dim=64))\nmodel.add(BatchNormalization())\n    #model.add(Dropout(rate=0.5))\nmodel.add(Dense(units=target_df2.shape[1], activation=\"sigmoid\"))\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.01))\n    #model.fit(train_x, train_t, epochs=50, batch_size=10) # エポック50, ミニバッチ 10\nep=30\nbatch_size=128\nstack =model.fit(sc_df2,target_df2, epochs=ep, batch_size=batch_size,validation_split=0.2,shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"scores=[]\nfor i in range(206):\n    t=target_df2.iloc[:,i]\n    model = Sequential()\n    model.add(Dense(units=64, activation='relu', input_dim=100))\n    model.add(BatchNormalization())\n    model.add(Dropout(rate=0.5))\n    model.add(Dense(units=32, activation='relu', input_dim=64))\n    model.add(BatchNormalization())\n    #model.add(Dropout(rate=0.5))\n    model.add(Dense(units=1, activation=\"sigmoid\"))\n    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.01))\n    #model.fit(train_x, train_t, epochs=50, batch_size=10) # エポック50, ミニバッチ 10\n    ep=15\n    batch_size=256\n    stack =model.fit(sc_df2,t, epochs=ep, batch_size=batch_size,validation_split=0.2,shuffle=True,verbose=0)\n    scores.append(stack.history['val_loss'][-1])\n    print(stack.history['loss'][-1],stack.history['val_loss'][-1])\n    print(\"next\")\n    # エポック50, ミニバッチ 10\nprint(\"finish\")\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nx = list(range(ep))\nplt.plot(x[2:], stack.history['loss'][2:], label=\"loss\")\nplt.title(\"loss\")\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n\nplt.plot(x[2:], stack.history['val_loss'][2:], label=\"val_loss\")\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = test_df[['sig_id']]\nsubmit = submit.copy()\nli=submit.values.reshape(-1)\ntest_df2=proceccing(test_df,comp=100)\ntest_df2,m,j=proceccing2(test_df2,mcd,jogen)\n\ntest_df2 = sc_x.fit_transform(test_df2)\npred=model.predict(test_df2)\nsubmit2=pd.DataFrame(pred,index=li)\nsubmit2=submit2.reset_index()\nsubmit2.columns=target_df.columns\nsubmit2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#　Xgboostによる解"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ntrain_df2 = proceccing(train_df,comp=100)\ntarget_df2 = target_df.set_index('sig_id')\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nscores=[]\nmodels=[]\navgs=[]\ntarget_num = target_df2.shape[1]\nlog_loss=[]\ncol=[\"name\",\"accuracy\",\"train_sum\",\"train_num\",\"precision\",\"recall\",\"f1_score\"]\nfor i in range(target_num):\n    \n    #while True:\n    #train,target=down_sampling(train_df2,target_df2,percent=0.3)\n\n\n    y = target_df2.iloc[:,i].values\n    print(y.shape)\n    x = train_df2.values\n    \n    \n    dtrain = xgb.DMatrix(x, label=y)\n    #dtest = xgb.DMatrix(X_test, label=y_test)\n    # 学習用のパラメータ\n    xgb_params = {\n        'max_depth': 5,\n        # 二値分類問題\n        'objective': 'binary:logistic',\n        # 評価指標\n        'eval_metric': 'logloss',\n        \n    }\n    #evallist = [(dvalid, 'eval'), (dtrain, 'train')]\n    # モデルを学習する\n    bst = xgb.train(xgb_params,\n                    dtrain,\n                    num_boost_round=10  # 学習ラウンド数は適当\n                    \n                    )\n    \n    #log_lossの計算\n    #１の確率\n    proba = bst.predict(dtrain)\n    t=0\n    for j in range(len(y)):\n        tar = y[j]\n        if tar ==0:\n            p = np.log(1-proba[i])\n        else:\n            p = np.log(proba[i])\n        t += p\n    t = -t/len(y)\n    log_loss.append(t)\n            \n        \n    \n    \n    \n    #score=[target_df2.columns[i],accuracy_score(y_pred=model.predict(X_test),y_true=y_test),y_train.sum(),len(y_train),score_p,score_r,score_f]\n    #scores.append(score)\n    print(i,\"end_logloss\",t)\n#print(\"total_avg\",np.mean(avgs))\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n\nimport matplotlib.pyplot as plt\nfor i in result:\n    if i ==\"name\":\n        continue\n    result[i].plot(kind=\"hist\",title=i)\n    plt.show()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nsubmit = test_df[['sig_id']]\nsubmit = submit.copy()\ntest_df2=proceccing(test_df)\n#test_df2.head()\nfor i in range(target_df2.shape[1]):\n    try:\n        y1 = models[i].predict_proba(test_df2)\n        y1 = y1[:,1]\n        #xgboost\n        #dtest = xgb.DMatrix(test_df2)\n        #y1 = models[i].predict(dtest)\n\n        #y1 = models[i].predict(test_df2)\n       \n        #y11=list(map(lambda x:round(x[1],5),y1))\n        y111 =list(map(lambda x: 0.0001 if x==0 else x,y1))\n        submit[target_df2.columns[i]]=y111\n    except:\n        print(i)\nsubmit.shape\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit2.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}