{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.053557,"end_time":"2020-10-20T04:58:28.146355","exception":false,"start_time":"2020-10-20T04:58:28.092798","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":6.156761,"end_time":"2020-10-20T04:58:34.32772","exception":false,"start_time":"2020-10-20T04:58:28.170959","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.backend import clear_session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n'''All code come from : https://github.com/trent-b/iterative-stratification'''\n\n\"\"\"This file includes multilabel cross validators based on an implementation of\nthe Iterative Stratification algorithm described in the following paper:\nSechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of Multi-\nLabel Data. In: Gunopulos D., Hofmann T., Malerba D., Vazirgiannis M. (eds)\nMachine Learning and Knowledge Discovery in Databases. ECML PKDD 2011. Lecture\nNotes in Computer Science, vol 6913. Springer, Berlin, Heidelberg.\n\nFrom scikit-learn 0.19.0, StratifiedKFold, RepeatedStratifiedKFold, and\nStratifiedShuffleSplit were copied and modified, retaining compatibility\nwith scikit-learn.\n\nAttribution to authors of scikit-learn/model_selection/_split.py under BSD 3 clause:\n    Alexandre Gramfort <alexandre.gramfort@inria.fr>,\n    Gael Varoquaux <gael.varoquaux@normalesup.org>,\n    Olivier Grisel <olivier.grisel@ensta.org>,\n    Raghav RV <rvraghav93@gmail.com>\n\"\"\"\n\n# Author: Trent J. Bradberry <trentjason@hotmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\n\nfrom sklearn.utils import check_random_state\nfrom sklearn.utils.validation import _num_samples, check_array\nfrom sklearn.utils.multiclass import type_of_target\n\nfrom sklearn.model_selection._split import _BaseKFold, _RepeatedSplits, \\\n    BaseShuffleSplit, _validate_shuffle_split\n\n\ndef IterativeStratification(labels, r, random_state):\n    \"\"\"This function implements the Iterative Stratification algorithm described\n    in the following paper:\n    Sechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of\n    Multi-Label Data. In: Gunopulos D., Hofmann T., Malerba D., Vazirgiannis M.\n    (eds) Machine Learning and Knowledge Discovery in Databases. ECML PKDD\n    2011. Lecture Notes in Computer Science, vol 6913. Springer, Berlin,\n    Heidelberg.\n    \"\"\"\n\n    n_samples = labels.shape[0]\n    test_folds = np.zeros(n_samples, dtype=int)\n\n    # Calculate the desired number of examples at each subset\n    c_folds = r * n_samples\n\n    # Calculate the desired number of examples of each label at each subset\n    c_folds_labels = np.outer(r, labels.sum(axis=0))\n\n    labels_not_processed_mask = np.ones(n_samples, dtype=bool)\n\n    while np.any(labels_not_processed_mask):\n        # Find the label with the fewest (but at least one) remaining examples,\n        # breaking ties randomly\n        num_labels = labels[labels_not_processed_mask].sum(axis=0)\n\n        # Handle case where only all-zero labels are left by distributing\n        # across all folds as evenly as possible (not in original algorithm but\n        # mentioned in the text). (By handling this case separately, some\n        # code redundancy is introduced; however, this approach allows for\n        # decreased execution time when there are a relatively large number\n        # of all-zero labels.)\n        if num_labels.sum() == 0:\n            sample_idxs = np.where(labels_not_processed_mask)[0]\n\n            for sample_idx in sample_idxs:\n                fold_idx = np.where(c_folds == c_folds.max())[0]\n\n                if fold_idx.shape[0] > 1:\n                    fold_idx = fold_idx[random_state.choice(fold_idx.shape[0])]\n\n                test_folds[sample_idx] = fold_idx\n                c_folds[fold_idx] -= 1\n\n            break\n\n        label_idx = np.where(num_labels == num_labels[np.nonzero(num_labels)].min())[0]\n        if label_idx.shape[0] > 1:\n            label_idx = label_idx[random_state.choice(label_idx.shape[0])]\n\n        sample_idxs = np.where(np.logical_and(labels[:, label_idx].flatten(), labels_not_processed_mask))[0]\n\n        for sample_idx in sample_idxs:\n            # Find the subset(s) with the largest number of desired examples\n            # for this label, breaking ties by considering the largest number\n            # of desired examples, breaking further ties randomly\n            label_folds = c_folds_labels[:, label_idx]\n            fold_idx = np.where(label_folds == label_folds.max())[0]\n\n            if fold_idx.shape[0] > 1:\n                temp_fold_idx = np.where(c_folds[fold_idx] ==\n                                         c_folds[fold_idx].max())[0]\n                fold_idx = fold_idx[temp_fold_idx]\n\n                if temp_fold_idx.shape[0] > 1:\n                    fold_idx = fold_idx[random_state.choice(temp_fold_idx.shape[0])]\n\n            test_folds[sample_idx] = fold_idx\n            labels_not_processed_mask[sample_idx] = False\n\n            # Update desired number of examples\n            c_folds_labels[fold_idx, labels[sample_idx]] -= 1\n            c_folds[fold_idx] -= 1\n\n    return test_folds\n\n\nclass MultilabelStratifiedKFold(_BaseKFold):\n    \"\"\"Multilabel stratified K-Folds cross-validator\n    Provides train/test indices to split multilabel data into train/test sets.\n    This cross-validation object is a variation of KFold that returns\n    stratified folds for multilabel data. The folds are made by preserving\n    the percentage of samples for each label.\n    Parameters\n    ----------\n    n_splits : int, default=3\n        Number of folds. Must be at least 2.\n    shuffle : boolean, optional\n        Whether to shuffle each stratification of the data before splitting\n        into batches.\n    random_state : int, RandomState instance or None, optional, default=None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Unlike StratifiedKFold that only uses random_state\n        when ``shuffle`` == True, this multilabel implementation\n        always uses the random_state since the iterative stratification\n        algorithm breaks ties randomly.\n    Examples\n    --------\n    >>> from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n    >>> import numpy as np\n    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n    >>> mskf = MultilabelStratifiedKFold(n_splits=2, random_state=0)\n    >>> mskf.get_n_splits(X, y)\n    2\n    >>> print(mskf)  # doctest: +NORMALIZE_WHITESPACE\n    MultilabelStratifiedKFold(n_splits=2, random_state=0, shuffle=False)\n    >>> for train_index, test_index in mskf.split(X, y):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [0 3 4 6] TEST: [1 2 5 7]\n    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n    Notes\n    -----\n    Train and test sizes may be slightly different in each fold.\n    See also\n    --------\n    RepeatedMultilabelStratifiedKFold: Repeats Multilabel Stratified K-Fold\n    n times.\n    \"\"\"\n\n    def __init__(self, n_splits=3, shuffle=False, random_state=None):\n        super(MultilabelStratifiedKFold, self).__init__(n_splits, shuffle, random_state)\n\n    def _make_test_folds(self, X, y):\n        y = np.asarray(y, dtype=bool)\n        type_of_target_y = type_of_target(y)\n\n        if type_of_target_y != 'multilabel-indicator':\n            raise ValueError(\n                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(type_of_target_y))\n\n        num_samples = y.shape[0]\n\n        rng = check_random_state(self.random_state)\n        indices = np.arange(num_samples)\n\n        if self.shuffle:\n            rng.shuffle(indices)\n            y = y[indices]\n\n        r = np.asarray([1 / self.n_splits] * self.n_splits)\n\n        test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n\n        return test_folds[np.argsort(indices)]\n\n    def _iter_test_masks(self, X=None, y=None, groups=None):\n        test_folds = self._make_test_folds(X, y)\n        for i in range(self.n_splits):\n            yield test_folds == i\n\n    def split(self, X, y, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n            Note that providing ``y`` is sufficient to generate the splits and\n            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n            ``X`` instead of actual training data.\n        y : array-like, shape (n_samples, n_labels)\n            The target variable for supervised learning problems.\n            Multilabel stratification is done based on the y labels.\n        groups : object\n            Always ignored, exists for compatibility.\n        Returns\n        -------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        \"\"\"\n        y = check_array(y, ensure_2d=False, dtype=None)\n        return super(MultilabelStratifiedKFold, self).split(X, y, groups)\n\n\nclass RepeatedMultilabelStratifiedKFold(_RepeatedSplits):\n    \"\"\"Repeated Multilabel Stratified K-Fold cross validator.\n    Repeats Mulilabel Stratified K-Fold n times with different randomization\n    in each repetition.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n    n_repeats : int, default=10\n        Number of times cross-validator needs to be repeated.\n    random_state : None, int or RandomState, default=None\n        Random state to be used to generate random state for each\n        repetition as well as randomly breaking ties within the iterative\n        stratification algorithm.\n    Examples\n    --------\n    >>> from iterstrat.ml_stratifiers import RepeatedMultilabelStratifiedKFold\n    >>> import numpy as np\n    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n    >>> rmskf = RepeatedMultilabelStratifiedKFold(n_splits=2, n_repeats=2,\n    ...     random_state=0)\n    >>> for train_index, test_index in rmskf.split(X, y):\n    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...     X_train, X_test = X[train_index], X[test_index]\n    ...     y_train, y_test = y[train_index], y[test_index]\n    ...\n    TRAIN: [0 3 4 6] TEST: [1 2 5 7]\n    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n    TRAIN: [0 1 4 5] TEST: [2 3 6 7]\n    TRAIN: [2 3 6 7] TEST: [0 1 4 5]\n    See also\n    --------\n    RepeatedStratifiedKFold: Repeats (Non-multilabel) Stratified K-Fold\n    n times.\n    \"\"\"\n    def __init__(self, n_splits=5, n_repeats=10, random_state=None):\n        super(RepeatedMultilabelStratifiedKFold, self).__init__(\n            MultilabelStratifiedKFold, n_repeats, random_state,\n            n_splits=n_splits)\n\n\nclass MultilabelStratifiedShuffleSplit(BaseShuffleSplit):\n    \"\"\"Multilabel Stratified ShuffleSplit cross-validator\n    Provides train/test indices to split data into train/test sets.\n    This cross-validation object is a merge of MultilabelStratifiedKFold and\n    ShuffleSplit, which returns stratified randomized folds for multilabel\n    data. The folds are made by preserving the percentage of each label.\n    Note: like the ShuffleSplit strategy, multilabel stratified random splits\n    do not guarantee that all folds will be different, although this is\n    still very likely for sizeable datasets.\n    Parameters\n    ----------\n    n_splits : int, default 10\n        Number of re-shuffling & splitting iterations.\n    test_size : float, int, None, optional\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of the dataset to include in the test split. If int, represents the\n        absolute number of test samples. If None, the value is set to the\n        complement of the train size. By default, the value is set to 0.1.\n        The default will change in version 0.21. It will remain 0.1 only\n        if ``train_size`` is unspecified, otherwise it will complement\n        the specified ``train_size``.\n    train_size : float, int, or None, default is None\n        If float, should be between 0.0 and 1.0 and represent the\n        proportion of the dataset to include in the train split. If\n        int, represents the absolute number of train samples. If None,\n        the value is automatically set to the complement of the test size.\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Unlike StratifiedShuffleSplit that only uses\n        random_state when ``shuffle`` == True, this multilabel implementation\n        always uses the random_state since the iterative stratification\n        algorithm breaks ties randomly.\n    Examples\n    --------\n    >>> from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n    >>> import numpy as np\n    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n    >>> msss = MultilabelStratifiedShuffleSplit(n_splits=3, test_size=0.5,\n    ...    random_state=0)\n    >>> msss.get_n_splits(X, y)\n    3\n    >>> print(mss)       # doctest: +ELLIPSIS\n    MultilabelStratifiedShuffleSplit(n_splits=3, random_state=0, test_size=0.5,\n                                     train_size=None)\n    >>> for train_index, test_index in msss.split(X, y):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n    TRAIN: [2 3 6 7] TEST: [0 1 4 5]\n    TRAIN: [1 2 5 6] TEST: [0 3 4 7]\n    Notes\n    -----\n    Train and test sizes may be slightly different from desired due to the\n    preference of stratification over perfectly sized folds.\n    \"\"\"\n\n    def __init__(self, n_splits=10, test_size=\"default\", train_size=None,\n                 random_state=None):\n        super(MultilabelStratifiedShuffleSplit, self).__init__(\n            n_splits, test_size, train_size, random_state)\n\n    def _iter_indices(self, X, y, groups=None):\n        n_samples = _num_samples(X)\n        y = check_array(y, ensure_2d=False, dtype=None)\n        y = np.asarray(y, dtype=bool)\n        type_of_target_y = type_of_target(y)\n\n        if type_of_target_y != 'multilabel-indicator':\n            raise ValueError(\n                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(\n                    type_of_target_y))\n\n        n_train, n_test = _validate_shuffle_split(n_samples, self.test_size,\n                                                  self.train_size)\n\n        n_samples = y.shape[0]\n        rng = check_random_state(self.random_state)\n        y_orig = y.copy()\n\n        r = np.array([n_train, n_test]) / (n_train + n_test)\n\n        for _ in range(self.n_splits):\n            indices = np.arange(n_samples)\n            rng.shuffle(indices)\n            y = y_orig[indices]\n\n            test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n\n            test_idx = test_folds[np.argsort(indices)] == 1\n            test = np.where(test_idx)[0]\n            train = np.where(~test_idx)[0]\n\n            yield train, test\n\n    def split(self, X, y, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n            Note that providing ``y`` is sufficient to generate the splits and\n            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n            ``X`` instead of actual training data.\n        y : array-like, shape (n_samples, n_labels)\n            The target variable for supervised learning problems.\n            Multilabel stratification is done based on the y labels.\n        groups : object\n            Always ignored, exists for compatibility.\n        Returns\n        -------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        \"\"\"\n        y = check_array(y, ensure_2d=False, dtype=None)\n        return super(MultilabelStratifiedShuffleSplit, self).split(X, y, groups)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-10-20T04:58:34.383979Z","iopub.status.busy":"2020-10-20T04:58:34.382873Z","iopub.status.idle":"2020-10-20T04:58:40.03451Z","shell.execute_reply":"2020-10-20T04:58:40.033567Z"},"papermill":{"duration":5.682045,"end_time":"2020-10-20T04:58:40.034644","exception":false,"start_time":"2020-10-20T04:58:34.352599","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_x = pd.read_csv('../input/lish-moa/train_features.csv')\ntest_x = pd.read_csv('../input/lish-moa/test_features.csv')","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T04:58:40.088821Z","iopub.status.busy":"2020-10-20T04:58:40.086962Z","iopub.status.idle":"2020-10-20T04:58:40.438261Z","shell.execute_reply":"2020-10-20T04:58:40.436989Z"},"papermill":{"duration":0.380169,"end_time":"2020-10-20T04:58:40.438417","exception":false,"start_time":"2020-10-20T04:58:40.058248","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_y = pd.read_csv('../input/lish-moa/train_targets_scored.csv')","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T04:58:40.491811Z","iopub.status.busy":"2020-10-20T04:58:40.491086Z","iopub.status.idle":"2020-10-20T04:58:40.643428Z","shell.execute_reply":"2020-10-20T04:58:40.642766Z"},"papermill":{"duration":0.181569,"end_time":"2020-10-20T04:58:40.643556","exception":false,"start_time":"2020-10-20T04:58:40.461987","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"data = pd.concat([train_x, test_x], axis=0) #두 dataframe을 합치세요\ndata = data.reset_index() #인덱스를 새로 만드세요","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T04:58:40.699683Z","iopub.status.busy":"2020-10-20T04:58:40.698095Z","iopub.status.idle":"2020-10-20T04:58:40.700423Z","shell.execute_reply":"2020-10-20T04:58:40.700978Z"},"papermill":{"duration":0.033848,"end_time":"2020-10-20T04:58:40.701124","exception":false,"start_time":"2020-10-20T04:58:40.667276","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"c_cols = []\ng_cols = []\n\nfor colname in data.columns:\n    if colname.startswith('c-'): # c-로 시작하는 column들을 c_cols에 넣으세요\n        c_cols.append(colname)\n    if colname.startswith('g-'): # g-로 시작하는 column들을 g_cols에 넣으세요\n        g_cols.append(colname)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T04:58:40.754705Z","iopub.status.busy":"2020-10-20T04:58:40.753886Z","iopub.status.idle":"2020-10-20T04:58:40.758245Z","shell.execute_reply":"2020-10-20T04:58:40.757664Z"},"papermill":{"duration":0.033646,"end_time":"2020-10-20T04:58:40.758361","exception":false,"start_time":"2020-10-20T04:58:40.724715","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"somthing_rate = 1e-15\nP_MIN = somthing_rate\nP_MAX = 1 - P_MIN\n\ndef loss_fn(yt, yp):\n    yp = np.clip(yp, P_MIN, P_MAX)\n    return log_loss(yt, yp, labels=[0,1])","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T04:58:40.812818Z","iopub.status.busy":"2020-10-20T04:58:40.811439Z","iopub.status.idle":"2020-10-20T04:58:40.954744Z","shell.execute_reply":"2020-10-20T04:58:40.955281Z"},"papermill":{"duration":0.173112,"end_time":"2020-10-20T04:58:40.955453","exception":false,"start_time":"2020-10-20T04:58:40.782341","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train = data.copy()\ntrain = train.drop(['sig_id', 'index'], axis=1) #train에서 'sig_id', 'index' 두 column들을 없애세요","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이미 correlation은 PCA를 통하여 줄여주었다. https://m.blog.naver.com/PostView.nhn?blogId=pmw9440&logNo=221478954228&proxyReferer=https:%2F%2Fwww.google.com%2F\n이 링크에 따르면 correlation이 높은 계수들은 PCA나 제거하는 것이 좋다고 한다..... \n음.... 그렇다면 g-37과 g-50을 제외하고 제거하는 것이 좋지 않을까?"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T04:58:41.010571Z","iopub.status.busy":"2020-10-20T04:58:41.009723Z","iopub.status.idle":"2020-10-20T04:58:41.014235Z","shell.execute_reply":"2020-10-20T04:58:41.013662Z"},"papermill":{"duration":0.033545,"end_time":"2020-10-20T04:58:41.01435","exception":false,"start_time":"2020-10-20T04:58:40.980805","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nlabel_encoder = LabelEncoder()\nonehot_encoder = OneHotEncoder()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T04:58:41.092867Z","iopub.status.busy":"2020-10-20T04:58:41.091819Z","iopub.status.idle":"2020-10-20T04:58:41.095256Z","shell.execute_reply":"2020-10-20T04:58:41.095783Z"},"papermill":{"duration":0.058091,"end_time":"2020-10-20T04:58:41.09593","exception":false,"start_time":"2020-10-20T04:58:41.037839","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"cp_dose_num = pd.DataFrame(label_encoder.fit_transform(train['cp_dose']), columns=['cp_dose_num']) #label encoding을 하세요\ncp_type_num = pd.DataFrame(label_encoder.fit_transform(train['cp_type']), columns=['cp_type_num']) #label encoding을 하세요","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.209926,"end_time":"2020-10-20T04:58:41.330521","exception":false,"start_time":"2020-10-20T04:58:41.120595","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train = pd.concat([cp_dose_num, cp_type_num, train], axis=1) # cp_type_num, cp_dose_num, train 세 dataframe을 합치세요\ntrain = train.drop(['cp_type', 'cp_dose'], axis=1) #기존의 'cp_type', 'cp_dose' columns을 없애세요\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T04:58:41.390263Z","iopub.status.busy":"2020-10-20T04:58:41.389266Z","iopub.status.idle":"2020-10-20T04:58:41.397271Z","shell.execute_reply":"2020-10-20T04:58:41.396714Z"},"papermill":{"duration":0.039602,"end_time":"2020-10-20T04:58:41.397389","exception":false,"start_time":"2020-10-20T04:58:41.357787","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#one hot encoding을 하세요\ncp_time_onehot = pd.DataFrame(onehot_encoder.fit_transform(train['cp_time'].to_numpy().reshape(-1, 1)).toarray())\n\n#one hot columns에 'cp_time_onehot_'이라는 prefix을 앞에 붙혀주세요\ncp_time_onehot = cp_time_onehot.add_prefix('cp_time_onehot_')","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T04:58:41.460642Z","iopub.status.busy":"2020-10-20T04:58:41.459218Z","iopub.status.idle":"2020-10-20T04:58:41.780224Z","shell.execute_reply":"2020-10-20T04:58:41.780844Z"},"papermill":{"duration":0.358003,"end_time":"2020-10-20T04:58:41.781004","exception":false,"start_time":"2020-10-20T04:58:41.423001","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train = pd.concat([cp_time_onehot, train], axis=1)\ntrain = train.drop(['cp_time'], axis=1)\n#train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Gauss Rank Scaler\nhttps://github.com/aldente0630/gauss_rank_scaler/blob/master/gauss_rank_scaler.py   PCA 이전에 scaler를 써보자"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom joblib import Parallel, delayed\nfrom scipy.interpolate import interp1d\nfrom scipy.special import erf, erfinv\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils.validation import FLOAT_DTYPES, check_array, check_is_fitted\n\n\nclass GaussRankScaler(BaseEstimator, TransformerMixin):\n    \"\"\"Transform features by scaling each feature to a normal distribution.\n    Parameters\n        ----------\n        epsilon : float, optional, default 1e-4\n            A small amount added to the lower bound or subtracted\n            from the upper bound. This value prevents infinite number\n            from occurring when applying the inverse error function.\n        copy : boolean, optional, default True\n            If False, try to avoid a copy and do inplace scaling instead.\n            This is not guaranteed to always work inplace; e.g. if the data is\n            not a NumPy array, a copy may still be returned.\n        n_jobs : int or None, optional, default None\n            Number of jobs to run in parallel.\n            ``None`` means 1 and ``-1`` means using all processors.\n        interp_kind : str or int, optional, default 'linear'\n           Specifies the kind of interpolation as a string\n            ('linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',\n            'previous', 'next', where 'zero', 'slinear', 'quadratic' and 'cubic'\n            refer to a spline interpolation of zeroth, first, second or third\n            order; 'previous' and 'next' simply return the previous or next value\n            of the point) or as an integer specifying the order of the spline\n            interpolator to use.\n        interp_copy : bool, optional, default False\n            If True, the interpolation function makes internal copies of x and y.\n            If False, references to `x` and `y` are used.\n        Attributes\n        ----------\n        interp_func_ : list\n            The interpolation function for each feature in the training set.\n        \"\"\"\n\n    def __init__(self, epsilon=1e-4, copy=True, n_jobs=None, interp_kind='linear', interp_copy=False):\n        self.epsilon = epsilon\n        self.copy = copy\n        self.interp_kind = interp_kind\n        self.interp_copy = interp_copy\n        self.fill_value = 'extrapolate'\n        self.n_jobs = n_jobs\n\n    def fit(self, X, y=None):\n        \"\"\"Fit interpolation function to link rank with original data for future scaling\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The data used to fit interpolation function for later scaling along the features axis.\n        y\n            Ignored\n        \"\"\"\n        X = check_array(X, copy=self.copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n\n        self.interp_func_ = Parallel(n_jobs=self.n_jobs)(delayed(self._fit)(x) for x in X.T)\n        return self\n\n    def _fit(self, x):\n        x = self.drop_duplicates(x)\n        rank = np.argsort(np.argsort(x))\n        bound = 1.0 - self.epsilon\n        factor = np.max(rank) / 2.0 * bound\n        scaled_rank = np.clip(rank / factor - bound, -bound, bound)\n        return interp1d(\n            x, scaled_rank, kind=self.interp_kind, copy=self.interp_copy, fill_value=self.fill_value)\n\n    def transform(self, X, copy=None):\n        \"\"\"Scale the data with the Gauss Rank algorithm\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The data used to scale along the features axis.\n        copy : bool, optional (default: None)\n            Copy the input X or not.\n        \"\"\"\n        check_is_fitted(self, 'interp_func_')\n\n        copy = copy if copy is not None else self.copy\n        X = check_array(X, copy=copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n\n        X = np.array(Parallel(n_jobs=self.n_jobs)(delayed(self._transform)(i, x) for i, x in enumerate(X.T))).T\n        return X\n\n    def _transform(self, i, x):\n        return erfinv(self.interp_func_[i](x))\n\n    def inverse_transform(self, X, copy=None):\n        \"\"\"Scale back the data to the original representation\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data used to scale along the features axis.\n        copy : bool, optional (default: None)\n            Copy the input X or not.\n        \"\"\"\n        check_is_fitted(self, 'interp_func_')\n\n        copy = copy if copy is not None else self.copy\n        X = check_array(X, copy=copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n\n        X = np.array(Parallel(n_jobs=self.n_jobs)(delayed(self._inverse_transform)(i, x) for i, x in enumerate(X.T))).T\n        return X\n\n    def _inverse_transform(self, i, x):\n        inv_interp_func = interp1d(self.interp_func_[i].y, self.interp_func_[i].x, kind=self.interp_kind,\n                                   copy=self.interp_copy, fill_value=self.fill_value)\n        return inv_interp_func(erf(x))\n\n    @staticmethod\n    def drop_duplicates(x):\n        is_unique = np.zeros_like(x, dtype=bool)\n        is_unique[np.unique(x, return_index=True)[1]] = True\n        return x[is_unique]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gauss_scaler = GaussRankScaler()\ntrain_gauss = gauss_scaler.fit_transform(train.iloc[:,5:])\ntrain_gauss = pd.DataFrame(train_gauss, columns=train.columns[5:]) #(27796, 974)\ntrain = pd.concat([train.iloc[:,:5], train_gauss], axis=1) #(27796, 979)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.029512,"end_time":"2020-10-20T04:58:41.840098","exception":false,"start_time":"2020-10-20T04:58:41.810586","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# PCA\n* gene columns : 772\n* cell columns : 100"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T04:58:41.900629Z","iopub.status.busy":"2020-10-20T04:58:41.899796Z","iopub.status.idle":"2020-10-20T04:58:42.024262Z","shell.execute_reply":"2020-10-20T04:58:42.023569Z"},"papermill":{"duration":0.15577,"end_time":"2020-10-20T04:58:42.024393","exception":false,"start_time":"2020-10-20T04:58:41.868623","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T04:58:42.0851Z","iopub.status.busy":"2020-10-20T04:58:42.083704Z","iopub.status.idle":"2020-10-20T04:58:42.08591Z","shell.execute_reply":"2020-10-20T04:58:42.086481Z"},"papermill":{"duration":0.035459,"end_time":"2020-10-20T04:58:42.086624","exception":false,"start_time":"2020-10-20T04:58:42.051165","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"pca_g = PCA(n_components=100) #pca 벡터수를 100개로 해주세요\npca_c = PCA(n_components=2) #pca 벡터수를 2개로 해주세요","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T04:58:42.148268Z","iopub.status.busy":"2020-10-20T04:58:42.146959Z","iopub.status.idle":"2020-10-20T04:58:43.736963Z","shell.execute_reply":"2020-10-20T04:58:43.73766Z"},"papermill":{"duration":1.624642,"end_time":"2020-10-20T04:58:43.737854","exception":false,"start_time":"2020-10-20T04:58:42.113212","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_pca_g = pca_g.fit_transform(train[g_cols]) #pca를 g_cols에 적용시켜주세요\ntrain_pca_c = pca_c.fit_transform(train[c_cols]) #pca를 g_cols에 적용시켜주세요","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T04:58:43.820289Z","iopub.status.busy":"2020-10-20T04:58:43.819386Z","iopub.status.idle":"2020-10-20T04:58:43.902894Z","shell.execute_reply":"2020-10-20T04:58:43.904054Z"},"papermill":{"duration":0.13149,"end_time":"2020-10-20T04:58:43.904376","exception":false,"start_time":"2020-10-20T04:58:43.772886","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train = pd.concat([train, pd.DataFrame(train_pca_g).add_prefix('pca_g_'), \n                  pd.DataFrame(train_pca_c).add_prefix('pca_c_')], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"VarianceThreshold 특성제거는 별로"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans #클러스터화 하여서 그 features를 추가해보기\n\ndef fe_cluster(train, test, n_clusters_g = 22, n_clusters_c = 4, SEED = 42):\n    \n    features_g = g_cols\n    features_c = c_cols\n    \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_Kmeans = train.iloc[:len(train_x),5:] \ntest_Kmeans = train.iloc[len(train_x):,5:] \ntrain_Kmeans, test_Kmeans = fe_cluster(train_Kmeans, test_Kmeans )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([train.iloc[:, :5], pd.concat([train_Kmeans, test_Kmeans], axis = 0)], axis = 1)\ntrain #(27796,1005)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#c_cols와 g_cols의 평균을 구해 주세요\n#마지막에 추가하기로 함\nmeans = pd.concat([train[g_cols].mean(axis=1), train[c_cols].mean(axis=1)], \n                   keys=['c_mean', 'g_mean'], axis=1)\n\ntrain = pd.concat([means,train], axis=1) #(27796,1007)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_classif\nselection_train_y = train_y.drop(['sig_id'], axis=1)\nselected_features =[]\nfor label in selection_train_y.columns:\n    selector = SelectKBest(f_classif, k = 'all')\n    selector.fit(train.iloc[:len(train_x)], selection_train_y[label])\n    selected_features.append(list(selector.scores_))\n    \nselected_features = np.array(selected_features)\nselect_standard = np.median(np.mean(selected_features,axis=0)) #각 feature마다 점수의 평균을 구하고, 그 feature 마다의 점수가 차이가 크기 때문에 median 값을 넣어주었다,\nselected_features = np.mean(selected_features, axis = 0) > select_standard\ntrain = train.iloc[:, selected_features]\ntrain","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NN models"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T04:58:44.470889Z","iopub.status.busy":"2020-10-20T04:58:44.469328Z","iopub.status.idle":"2020-10-20T04:58:44.752641Z","shell.execute_reply":"2020-10-20T04:58:44.753232Z"},"papermill":{"duration":0.333669,"end_time":"2020-10-20T04:58:44.753414","exception":false,"start_time":"2020-10-20T04:58:44.419745","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#train_test_split을 해주세요\nX_train, X_val, y_train, y_val = train_test_split(train.iloc[:len(train_x)], \n                                                  train_y.drop(['sig_id'], axis=1), \n                                                  test_size=0.2, random_state=224)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T04:58:44.821916Z","iopub.status.busy":"2020-10-20T04:58:44.820857Z","iopub.status.idle":"2020-10-20T04:58:44.824548Z","shell.execute_reply":"2020-10-20T04:58:44.823943Z"},"papermill":{"duration":0.043401,"end_time":"2020-10-20T04:58:44.824664","exception":false,"start_time":"2020-10-20T04:58:44.781263","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def build_model(hidden_layers, neurons, dropout_rate, input_shape):\n    #Sequential로 설정해주세요\n    model = tf.keras.Sequential([tf.keras.layers.Input(input_shape)])\n\n    for i in range(hidden_layers):\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dropout(dropout_rate)) #dropout을 해주세요\n        model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(neurons // 2**i, activation='swish')))\n        #//은 나누기를 하고 소수점 이하를 버리는 연산자\n    #============ Final Layer =================\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation=\"sigmoid\")))\n    \n    model.compile(optimizer=tfa.optimizers.AdamW(lr = 1e-3, weight_decay = 1e-5, clipvalue = 756), \n                  loss=BinaryCrossentropy(label_smoothing=0.001)) \n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T04:58:44.884534Z","iopub.status.busy":"2020-10-20T04:58:44.883757Z","iopub.status.idle":"2020-10-20T04:58:48.167451Z","shell.execute_reply":"2020-10-20T04:58:48.166168Z"},"papermill":{"duration":3.315592,"end_time":"2020-10-20T04:58:48.167592","exception":false,"start_time":"2020-10-20T04:58:44.852","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"layer2_model = build_model(2, 412, 0.5012546298076606, X_train.shape[1]) #2 layer model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"layer3_model = build_model(3, 824, 0.5012546298076606, X_train.shape[1] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"layer4_model = build_model(4, 1600, 0.5012546298076606, X_train.shape[1] )","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":105.707009,"end_time":"2020-10-20T05:00:33.902766","exception":false,"start_time":"2020-10-20T04:58:48.195757","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.2, min_lr=1e-6, patience=4, verbose=1, mode='auto')\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", restore_best_weights=True, patience= 10, verbose = 1)\n\ncheckpoint_path = 'model.weights'\ncb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 1, save_best_only = True, \n                             save_weights_only=True, mode = 'auto')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_model(model):\n    model.fit(X_train, y_train,\n                    batch_size = 64,\n                    epochs = 100,\n                    validation_data = (X_val, y_val),\n                    callbacks = [early, reduce_lr_loss, cb_checkpt])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_model(layer2_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_model(layer3_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_model(layer4_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. tfa.Lookahead 사용 --> val_loss 0.0185 prediction 0.1934 실패 원래대로하자\n2. Kmeans 사용 --> loss 0.0167 val_loss 0.0184, prediction \n3. Gauss Scaler까지 사용 --> loss 0.016 val_loss 0.0181 최소"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = train.iloc[len(train_x):]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble\n\nblending - https://3months.tistory.com/486 --> blending은 0.20 정도로 성능이 낮아졌다"},{"metadata":{"trusted":true},"cell_type":"code","source":"###blending###\n\nval_pred1 = layer2_model.predict(X_val)\ntest_pred1 = layer2_model.predict(test)\n\nval_pred2 = layer3_model.predict(X_val)\ntest_pred2 = layer3_model.predict(test)\n\n\nval_pred3 = layer4_model.predict(X_val)\ntest_pred3 = layer4_model.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blend_val = np.concatenate((X_val.to_numpy(), val_pred1, val_pred2, val_pred3), axis = 1)\nblend_test = np.concatenate((test.to_numpy(), test_pred1, test_pred2, test_pred3), axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"final_model = build_model(3, 824, 0.5012546298076606, blend_val.shape[1] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model.fit(blend_val, y_val, epochs = 100, validation_split=0.1, callbacks = [early, reduce_lr_loss, cb_checkpt])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"cv(Kfold) 기반 stacking ensemble - https://lsjsj92.tistory.com/559?category=853217"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_test_split은 의미가 없다! \npre_stack_X_train = train.iloc[:len(train_x)]\npre_stack_y_train = train_y.drop(['sig_id'], axis=1)\npre_stack_test = train.iloc[len(train_x):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://github.com/trent-b/iterative-stratification\ndef get_stacking_data(model, X_train, y_train, X_test, n_folds=3):\n    stk = MultilabelStratifiedKFold(n_splits=n_folds)\n    train_fold_predict = np.zeros((X_train.shape[0], y_train.shape[1]))\n    test_predict = np.zeros((X_test.shape[0], y_train.shape[1], n_folds))\n    print(\"model : \", model.__class__.__name__)\n    \n    for cnt, (train_index, valid_index) in enumerate(stk.split(X_train, y_train)):\n        X_train_ = X_train.iloc[train_index]#(?,1007)\n        y_train_ = y_train.iloc[train_index]#(?,206)\n        X_validation = X_train.iloc[valid_index]#(?,1007)\n        \n        model.fit(X_train_, y_train_, batch_size = 64,\n                    epochs = 100,\n                    validation_split = 0.2,\n                    callbacks = [early, reduce_lr_loss, cb_checkpt])\n        \n        train_fold_predict[valid_index, :] = model.predict(X_validation) #(6351,206)\n        test_predict[:,:,cnt] = model.predict(X_test) #(4763,206)\n    test_predict_mean = np.mean(test_predict, axis=2)\n    \n    return train_fold_predict, test_predict_mean\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stack_pred1, stack_test1 = get_stacking_data(layer2_model, pre_stack_X_train, pre_stack_y_train, pre_stack_test)\nstack_pred2, stack_test2 = get_stacking_data(layer3_model, pre_stack_X_train, pre_stack_y_train, pre_stack_test)\nstack_pred3, stack_test3 = get_stacking_data(layer4_model, pre_stack_X_train, pre_stack_y_train, pre_stack_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stack_train = np.concatenate((stack_pred1, stack_pred2,stack_pred3), axis = 1) #(19051, 618) --> final X_train\nstack_test = np.concatenate((stack_test1, stack_test2,stack_test3), axis=1) # --> final X_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = build_model(2, 306, 0.5012546298076606, stack_train.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model.fit(stack_train, pre_stack_y_train, epochs=100, validation_split = 0.2,\n                    callbacks = [early, reduce_lr_loss, cb_checkpt])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T05:00:36.861738Z","iopub.status.busy":"2020-10-20T05:00:36.860729Z","iopub.status.idle":"2020-10-20T05:00:37.645093Z","shell.execute_reply":"2020-10-20T05:00:37.644491Z"},"papermill":{"duration":1.519595,"end_time":"2020-10-20T05:00:37.645242","exception":false,"start_time":"2020-10-20T05:00:36.125647","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"pred = final_model.predict(stack_test)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T05:00:39.117943Z","iopub.status.busy":"2020-10-20T05:00:39.116076Z","iopub.status.idle":"2020-10-20T05:00:39.121463Z","shell.execute_reply":"2020-10-20T05:00:39.120883Z"},"papermill":{"duration":0.718767,"end_time":"2020-10-20T05:00:39.121589","exception":false,"start_time":"2020-10-20T05:00:38.402822","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"pred_df = pd.DataFrame(pred, columns=train_y.columns[1:])\nsubmmission_df = pd.concat([test_x['sig_id'], pred_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T05:00:40.96218Z","iopub.status.busy":"2020-10-20T05:00:40.961084Z","iopub.status.idle":"2020-10-20T05:00:42.869943Z","shell.execute_reply":"2020-10-20T05:00:42.868775Z"},"papermill":{"duration":2.808522,"end_time":"2020-10-20T05:00:42.870076","exception":false,"start_time":"2020-10-20T05:00:40.061554","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"submmission_df.to_csv('submission.csv', index=False) #csv파일로 export하세요","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T05:00:44.573552Z","iopub.status.busy":"2020-10-20T05:00:44.572506Z","iopub.status.idle":"2020-10-20T05:00:44.797756Z","shell.execute_reply":"2020-10-20T05:00:44.798405Z"},"papermill":{"duration":1.218089,"end_time":"2020-10-20T05:00:44.79858","exception":false,"start_time":"2020-10-20T05:00:43.580491","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"pd.read_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}