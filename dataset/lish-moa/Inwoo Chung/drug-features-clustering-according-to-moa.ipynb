{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Drug features clustering according to mechanisms of action","metadata":{}},{"cell_type":"markdown","source":"On 2021.8.2, now I'm in a hospital because my father is sick due to cerebral infarction by sudden thrombus after about one month since the 2nd pfizer vaccination. Furthermore, by the COVID-19 pandemic situation, my father and I have been isolated from the out world. My labtop display was broken when I tested my MoA(mechanisms of action) prediction model and drug feature clustering according to mechanisms of action, so my old mother bought a new monitor and brought me the monitor because I couldn't go out. I'm writing this with the sub monitor. \n\nLast year, I participated in the Mechanisms of Action contest, and that time, I developed an autoencoder and additive angular margin based MoA prediction model approaching this problem as clustering, not multi-classification. Of course, I devleoped an autoencoder based model as multi-classification. I thought the clustering model will be superior to the multi-classification model because the clustering model predicted mechanisms of action more accurately in practical mechanisms of action plot graphs than the multi-classification model, but rather the multi-classification model was superior to the clustering model in the viewpoint of the metric used that time.\n\nBy the way, MoA clustering for the multi-clasification is more random than the clustering model, and in the clustering model, MoA clustering is structured and all drugs have almost all mechanisms of action. So that time, if it is valid, I thought the mRNA vaccine can have almost all known mechanisms of action. But I needed verification.\n\nVaccination is mandatory, but our family and friends can die or have serious side effects due to unstable vaccines. Maybe, I hoped fortune for vaccination to my family. But fortune is only a fortune word to me. \n\nAfter I faced my father's accident, I searched for relevant papers, and I got it.\nRefer to https://www.medrxiv.org/content/10.1101/2021.04.30.21256383v1.\nThis paper shows the vaccine induced immune thrombotic thrombocytopenia side effect for the BNT162b2, ChAdOx1 vaccines. So I understood my father accident's reason.\n\nMoreover, I'm confident that the MoA clustering model is valid.\nFor mRNA vaccines, the vaccination can be carried out to children <= 13. It can be terrible in the human history, because mRNA vaccines' stability are almost unknown.\nRefer to https://coronavirus.quora.com/?__ni__=0&__nsrc__=4&__snid3__=24261290897&__tiids__=32992518\n\nI still think that the mRNA vaccine is only a solution to cope with rapidly changing variants and mutants. But their stability is also mandatory as like that vaccination is mandatory, and this solution must be developed very fast as the vaccination effect development.\n\nVia MoA prediction, we can identify side effects of vaccines and develop more stable vaccines. It is mandatory.","metadata":{}},{"cell_type":"markdown","source":"## Data analysis","metadata":{}},{"cell_type":"code","source":"%pylab inline\nimport pandas as pd\nimport seaborn as sns\nimport os, sys","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:15.357864Z","iopub.execute_input":"2021-08-19T04:36:15.358255Z","iopub.status.idle":"2021-08-19T04:36:16.194592Z","shell.execute_reply.started":"2021-08-19T04:36:15.358224Z","shell.execute_reply":"2021-08-19T04:36:16.193779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.special import comb\nfrom itertools import combinations\nimport json\nfrom tqdm import tqdm\nimport pdb\nimport plotly.express as px","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:16.198673Z","iopub.execute_input":"2021-08-19T04:36:16.198954Z","iopub.status.idle":"2021-08-19T04:36:17.865452Z","shell.execute_reply.started":"2021-08-19T04:36:16.198927Z","shell.execute_reply":"2021-08-19T04:36:17.864564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data_path = '/kaggle/input/lish-moa'","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:17.867927Z","iopub.execute_input":"2021-08-19T04:36:17.868289Z","iopub.status.idle":"2021-08-19T04:36:17.876112Z","shell.execute_reply.started":"2021-08-19T04:36:17.868257Z","shell.execute_reply":"2021-08-19T04:36:17.875099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convert categorical values into categorical indexes","metadata":{}},{"cell_type":"code","source":"input_df = pd.read_csv(os.path.join(raw_data_path, 'train_features.csv'))\ninput_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:17.878229Z","iopub.execute_input":"2021-08-19T04:36:17.878507Z","iopub.status.idle":"2021-08-19T04:36:23.605254Z","shell.execute_reply.started":"2021-08-19T04:36:17.87848Z","shell.execute_reply":"2021-08-19T04:36:23.603431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_df.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.608567Z","iopub.execute_input":"2021-08-19T04:36:23.608866Z","iopub.status.idle":"2021-08-19T04:36:23.616316Z","shell.execute_reply.started":"2021-08-19T04:36:23.608828Z","shell.execute_reply":"2021-08-19T04:36:23.614914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(input_df.sig_id), len(input_df.sig_id.unique())","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.620342Z","iopub.execute_input":"2021-08-19T04:36:23.620821Z","iopub.status.idle":"2021-08-19T04:36:23.63273Z","shell.execute_reply.started":"2021-08-19T04:36:23.620779Z","shell.execute_reply":"2021-08-19T04:36:23.63158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_df.cp_type.unique(), input_df.cp_time.unique(), input_df.cp_dose.unique() ","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.634882Z","iopub.execute_input":"2021-08-19T04:36:23.635419Z","iopub.status.idle":"2021-08-19T04:36:23.648444Z","shell.execute_reply.started":"2021-08-19T04:36:23.635378Z","shell.execute_reply":"2021-08-19T04:36:23.647129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_df.cp_time.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.650331Z","iopub.execute_input":"2021-08-19T04:36:23.650729Z","iopub.status.idle":"2021-08-19T04:36:23.660364Z","shell.execute_reply.started":"2021-08-19T04:36:23.650689Z","shell.execute_reply":"2021-08-19T04:36:23.659065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = input_df.cp_type.astype('category')\nres","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.662166Z","iopub.execute_input":"2021-08-19T04:36:23.662752Z","iopub.status.idle":"2021-08-19T04:36:23.679291Z","shell.execute_reply.started":"2021-08-19T04:36:23.662707Z","shell.execute_reply":"2021-08-19T04:36:23.678504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(res.cat.categories)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.680611Z","iopub.execute_input":"2021-08-19T04:36:23.680908Z","iopub.status.idle":"2021-08-19T04:36:23.687283Z","shell.execute_reply.started":"2021-08-19T04:36:23.680881Z","shell.execute_reply":"2021-08-19T04:36:23.6863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = res.cat.rename_categories(range(len(res.cat.categories)))\nres","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.689553Z","iopub.execute_input":"2021-08-19T04:36:23.690051Z","iopub.status.idle":"2021-08-19T04:36:23.703518Z","shell.execute_reply.started":"2021-08-19T04:36:23.690009Z","shell.execute_reply":"2021-08-19T04:36:23.702427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res2 = res.map(lambda x: int(x))\nres2","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.705406Z","iopub.execute_input":"2021-08-19T04:36:23.705808Z","iopub.status.idle":"2021-08-19T04:36:23.717015Z","shell.execute_reply.started":"2021-08-19T04:36:23.705767Z","shell.execute_reply":"2021-08-19T04:36:23.715695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(res2.iloc[0])","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.718398Z","iopub.execute_input":"2021-08-19T04:36:23.718746Z","iopub.status.idle":"2021-08-19T04:36:23.724891Z","shell.execute_reply.started":"2021-08-19T04:36:23.71871Z","shell.execute_reply":"2021-08-19T04:36:23.723932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_df.cp_type","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.726942Z","iopub.execute_input":"2021-08-19T04:36:23.727402Z","iopub.status.idle":"2021-08-19T04:36:23.736498Z","shell.execute_reply.started":"2021-08-19T04:36:23.72736Z","shell.execute_reply":"2021-08-19T04:36:23.735416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_df.cp_type = input_df.cp_type.astype('category')\ninput_df.cp_type","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.738507Z","iopub.execute_input":"2021-08-19T04:36:23.739043Z","iopub.status.idle":"2021-08-19T04:36:23.754857Z","shell.execute_reply.started":"2021-08-19T04:36:23.739006Z","shell.execute_reply":"2021-08-19T04:36:23.753919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_df.cp_type = input_df.cp_type.cat.rename_categories(range(len(input_df.cp_type.cat.categories)))\ninput_df.cp_type","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.756624Z","iopub.execute_input":"2021-08-19T04:36:23.757037Z","iopub.status.idle":"2021-08-19T04:36:23.768016Z","shell.execute_reply.started":"2021-08-19T04:36:23.756998Z","shell.execute_reply":"2021-08-19T04:36:23.766738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cp_v_index_series = input_df.cp_type[input_df.cp_type == 0]\ncp_v_index_series ","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.770024Z","iopub.execute_input":"2021-08-19T04:36:23.770502Z","iopub.status.idle":"2021-08-19T04:36:23.782035Z","shell.execute_reply.started":"2021-08-19T04:36:23.770463Z","shell.execute_reply":"2021-08-19T04:36:23.781189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_df.cp_time = input_df.cp_time.astype('category')\ninput_df.cp_time = input_df.cp_time.cat.rename_categories(range(len(input_df.cp_time.cat.categories)))\ninput_df.cp_dose = input_df.cp_dose.astype('category')\ninput_df.cp_dose = input_df.cp_dose.cat.rename_categories(range(len(input_df.cp_dose.cat.categories)))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.784065Z","iopub.execute_input":"2021-08-19T04:36:23.784428Z","iopub.status.idle":"2021-08-19T04:36:23.798768Z","shell.execute_reply.started":"2021-08-19T04:36:23.784392Z","shell.execute_reply":"2021-08-19T04:36:23.798043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(input_df.cp_time.cat.categories), len(input_df.cp_dose.cat.categories)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.801634Z","iopub.execute_input":"2021-08-19T04:36:23.801945Z","iopub.status.idle":"2021-08-19T04:36:23.809009Z","shell.execute_reply.started":"2021-08-19T04:36:23.801913Z","shell.execute_reply":"2021-08-19T04:36:23.807743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_df[['cp_type', 'cp_time', 'cp_dose']].head(3)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.810855Z","iopub.execute_input":"2021-08-19T04:36:23.811315Z","iopub.status.idle":"2021-08-19T04:36:23.827425Z","shell.execute_reply.started":"2021-08-19T04:36:23.811273Z","shell.execute_reply":"2021-08-19T04:36:23.826512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = input_df.isna().any()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.829081Z","iopub.execute_input":"2021-08-19T04:36:23.829451Z","iopub.status.idle":"2021-08-19T04:36:23.86759Z","shell.execute_reply.started":"2021-08-19T04:36:23.829413Z","shell.execute_reply":"2021-08-19T04:36:23.866641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res.any()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.868984Z","iopub.execute_input":"2021-08-19T04:36:23.869352Z","iopub.status.idle":"2021-08-19T04:36:23.876758Z","shell.execute_reply.started":"2021-08-19T04:36:23.869314Z","shell.execute_reply":"2021-08-19T04:36:23.875764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = input_df.iloc[0:64]","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.878422Z","iopub.execute_input":"2021-08-19T04:36:23.879022Z","iopub.status.idle":"2021-08-19T04:36:23.884028Z","shell.execute_reply.started":"2021-08-19T04:36:23.878983Z","shell.execute_reply":"2021-08-19T04:36:23.883097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res2 = res['cp_time'].values\ntype(res2)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.88556Z","iopub.execute_input":"2021-08-19T04:36:23.886213Z","iopub.status.idle":"2021-08-19T04:36:23.895675Z","shell.execute_reply.started":"2021-08-19T04:36:23.886166Z","shell.execute_reply":"2021-08-19T04:36:23.895046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res2.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.896717Z","iopub.execute_input":"2021-08-19T04:36:23.897537Z","iopub.status.idle":"2021-08-19T04:36:23.904586Z","shell.execute_reply.started":"2021-08-19T04:36:23.897499Z","shell.execute_reply":"2021-08-19T04:36:23.903498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g_feature_names = ['g-' + str(v) for v in range(772)]","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.906306Z","iopub.execute_input":"2021-08-19T04:36:23.906719Z","iopub.status.idle":"2021-08-19T04:36:23.912501Z","shell.execute_reply.started":"2021-08-19T04:36:23.906684Z","shell.execute_reply":"2021-08-19T04:36:23.91145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res2 = res[g_feature_names].values\nres2.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.914082Z","iopub.execute_input":"2021-08-19T04:36:23.914664Z","iopub.status.idle":"2021-08-19T04:36:23.925865Z","shell.execute_reply.started":"2021-08-19T04:36:23.914627Z","shell.execute_reply":"2021-08-19T04:36:23.924851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res2","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.928859Z","iopub.execute_input":"2021-08-19T04:36:23.929168Z","iopub.status.idle":"2021-08-19T04:36:23.937013Z","shell.execute_reply.started":"2021-08-19T04:36:23.929138Z","shell.execute_reply":"2021-08-19T04:36:23.936026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res2 = res['sig_id'].values","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.938787Z","iopub.execute_input":"2021-08-19T04:36:23.939197Z","iopub.status.idle":"2021-08-19T04:36:23.944016Z","shell.execute_reply.started":"2021-08-19T04:36:23.939161Z","shell.execute_reply":"2021-08-19T04:36:23.943001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(res2), res2.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.945995Z","iopub.execute_input":"2021-08-19T04:36:23.94655Z","iopub.status.idle":"2021-08-19T04:36:23.955436Z","shell.execute_reply.started":"2021-08-19T04:36:23.946505Z","shell.execute_reply":"2021-08-19T04:36:23.954288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res2","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.957408Z","iopub.execute_input":"2021-08-19T04:36:23.957926Z","iopub.status.idle":"2021-08-19T04:36:23.965479Z","shell.execute_reply.started":"2021-08-19T04:36:23.957884Z","shell.execute_reply":"2021-08-19T04:36:23.9644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gene expression and cell viability","metadata":{}},{"cell_type":"code","source":"input_vals = input_df.values\ninput_vals.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:23.974308Z","iopub.execute_input":"2021-08-19T04:36:23.974571Z","iopub.status.idle":"2021-08-19T04:36:25.124863Z","shell.execute_reply.started":"2021-08-19T04:36:23.974545Z","shell.execute_reply":"2021-08-19T04:36:25.123923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_vals[0]","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:25.126685Z","iopub.execute_input":"2021-08-19T04:36:25.127055Z","iopub.status.idle":"2021-08-19T04:36:25.136718Z","shell.execute_reply.started":"2021-08-19T04:36:25.127016Z","shell.execute_reply":"2021-08-19T04:36:25.135788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_vals_p = input_vals[:, 4:]\ninput_vals_p.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:25.138581Z","iopub.execute_input":"2021-08-19T04:36:25.139029Z","iopub.status.idle":"2021-08-19T04:36:25.146092Z","shell.execute_reply.started":"2021-08-19T04:36:25.13899Z","shell.execute_reply":"2021-08-19T04:36:25.145068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_vals_p = input_vals_p.astype('float32')","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:25.147938Z","iopub.execute_input":"2021-08-19T04:36:25.148368Z","iopub.status.idle":"2021-08-19T04:36:26.302926Z","shell.execute_reply.started":"2021-08-19T04:36:25.148325Z","shell.execute_reply":"2021-08-19T04:36:26.301964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_vals_p.shape, input_vals_p.dtype","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:26.304365Z","iopub.execute_input":"2021-08-19T04:36:26.304794Z","iopub.status.idle":"2021-08-19T04:36:26.318197Z","shell.execute_reply.started":"2021-08-19T04:36:26.304743Z","shell.execute_reply":"2021-08-19T04:36:26.317096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_vals_p[0, ...]","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:26.319852Z","iopub.execute_input":"2021-08-19T04:36:26.320288Z","iopub.status.idle":"2021-08-19T04:36:26.339779Z","shell.execute_reply.started":"2021-08-19T04:36:26.320251Z","shell.execute_reply":"2021-08-19T04:36:26.338742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ge_val = input_vals_p[:, 0:772]\nct_val = input_vals_p[:, 772:]\nge_val.shape, ct_val.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:26.341214Z","iopub.execute_input":"2021-08-19T04:36:26.341572Z","iopub.status.idle":"2021-08-19T04:36:26.350659Z","shell.execute_reply.started":"2021-08-19T04:36:26.341536Z","shell.execute_reply":"2021-08-19T04:36:26.349652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ge_val_mean = ge_val.mean(axis=-1)\nge_val_std = ge_val.std(axis=-1)\nge_val_mean.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:26.352207Z","iopub.execute_input":"2021-08-19T04:36:26.352786Z","iopub.status.idle":"2021-08-19T04:36:26.411836Z","shell.execute_reply.started":"2021-08-19T04:36:26.352744Z","shell.execute_reply":"2021-08-19T04:36:26.410719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure(figsize=(20, 20))\nplot(ge_val_mean, label='mean')\nplot(ge_val_std, label='std')\nlegend()\ngrid()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:26.413496Z","iopub.execute_input":"2021-08-19T04:36:26.413866Z","iopub.status.idle":"2021-08-19T04:36:27.022896Z","shell.execute_reply.started":"2021-08-19T04:36:26.413812Z","shell.execute_reply":"2021-08-19T04:36:27.021905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ct_val_mean = ct_val.mean(axis=-1)\nct_val_std = ct_val.std(axis=-1)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:27.024029Z","iopub.execute_input":"2021-08-19T04:36:27.025148Z","iopub.status.idle":"2021-08-19T04:36:27.036538Z","shell.execute_reply.started":"2021-08-19T04:36:27.025117Z","shell.execute_reply":"2021-08-19T04:36:27.035047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure(figsize=(20, 20))\nplot(ct_val_mean, label='mean')\nplot(ct_val_std, label='std')\nlegend()\ngrid()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:27.038242Z","iopub.execute_input":"2021-08-19T04:36:27.038785Z","iopub.status.idle":"2021-08-19T04:36:27.877454Z","shell.execute_reply.started":"2021-08-19T04:36:27.038745Z","shell.execute_reply":"2021-08-19T04:36:27.876586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target feature","metadata":{}},{"cell_type":"code","source":"target_scored_df = pd.read_csv(os.path.join(raw_data_path, 'train_targets_scored.csv'))\ntarget_scored_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:27.8787Z","iopub.execute_input":"2021-08-19T04:36:27.879219Z","iopub.status.idle":"2021-08-19T04:36:28.290515Z","shell.execute_reply.started":"2021-08-19T04:36:27.879177Z","shell.execute_reply":"2021-08-19T04:36:28.289588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_scored_df.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:28.291801Z","iopub.execute_input":"2021-08-19T04:36:28.292161Z","iopub.status.idle":"2021-08-19T04:36:28.299171Z","shell.execute_reply.started":"2021-08-19T04:36:28.292122Z","shell.execute_reply":"2021-08-19T04:36:28.29816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"moa_names = list(target_scored_df.columns)[1:]\nmoa_names","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:28.301073Z","iopub.execute_input":"2021-08-19T04:36:28.301674Z","iopub.status.idle":"2021-08-19T04:36:28.313312Z","shell.execute_reply.started":"2021-08-19T04:36:28.301634Z","shell.execute_reply":"2021-08-19T04:36:28.312173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_scored_df.sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:28.315139Z","iopub.execute_input":"2021-08-19T04:36:28.315501Z","iopub.status.idle":"2021-08-19T04:36:29.170294Z","shell.execute_reply.started":"2021-08-19T04:36:28.315464Z","shell.execute_reply":"2021-08-19T04:36:29.168974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = target_scored_df[input_df.cp_type == 0]\nres","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:29.172024Z","iopub.execute_input":"2021-08-19T04:36:29.172428Z","iopub.status.idle":"2021-08-19T04:36:29.208263Z","shell.execute_reply.started":"2021-08-19T04:36:29.17237Z","shell.execute_reply":"2021-08-19T04:36:29.207089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res.sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:29.209823Z","iopub.execute_input":"2021-08-19T04:36:29.210215Z","iopub.status.idle":"2021-08-19T04:36:29.274928Z","shell.execute_reply.started":"2021-08-19T04:36:29.210167Z","shell.execute_reply":"2021-08-19T04:36:29.273757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_scored_df","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:29.276484Z","iopub.execute_input":"2021-08-19T04:36:29.276874Z","iopub.status.idle":"2021-08-19T04:36:29.310095Z","shell.execute_reply.started":"2021-08-19T04:36:29.276822Z","shell.execute_reply":"2021-08-19T04:36:29.308977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_nonscored_df = pd.read_csv(os.path.join(raw_data_path, 'train_targets_nonscored.csv'))\ntarget_nonscored_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:29.311755Z","iopub.execute_input":"2021-08-19T04:36:29.312162Z","iopub.status.idle":"2021-08-19T04:36:30.137502Z","shell.execute_reply.started":"2021-08-19T04:36:29.312119Z","shell.execute_reply":"2021-08-19T04:36:30.136567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_nonscored_df.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:30.138773Z","iopub.execute_input":"2021-08-19T04:36:30.139131Z","iopub.status.idle":"2021-08-19T04:36:30.145895Z","shell.execute_reply.started":"2021-08-19T04:36:30.139093Z","shell.execute_reply":"2021-08-19T04:36:30.144918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_df = pd.concat([target_scored_df, target_nonscored_df.iloc[:, 1:]], axis=1)\nlen(target_df.columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:30.147808Z","iopub.execute_input":"2021-08-19T04:36:30.148219Z","iopub.status.idle":"2021-08-19T04:36:30.226859Z","shell.execute_reply.started":"2021-08-19T04:36:30.148168Z","shell.execute_reply":"2021-08-19T04:36:30.225869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_df","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:30.228368Z","iopub.execute_input":"2021-08-19T04:36:30.228743Z","iopub.status.idle":"2021-08-19T04:36:30.260186Z","shell.execute_reply.started":"2021-08-19T04:36:30.228705Z","shell.execute_reply":"2021-08-19T04:36:30.259179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_df.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:30.261758Z","iopub.execute_input":"2021-08-19T04:36:30.262142Z","iopub.status.idle":"2021-08-19T04:36:30.269497Z","shell.execute_reply.started":"2021-08-19T04:36:30.262102Z","shell.execute_reply":"2021-08-19T04:36:30.268382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = target_nonscored_df.sum()\nres","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:30.271101Z","iopub.execute_input":"2021-08-19T04:36:30.271462Z","iopub.status.idle":"2021-08-19T04:36:30.447707Z","shell.execute_reply.started":"2021-08-19T04:36:30.271423Z","shell.execute_reply":"2021-08-19T04:36:30.446971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_scored_df.iloc[:, 1:2].info()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:30.449242Z","iopub.execute_input":"2021-08-19T04:36:30.449592Z","iopub.status.idle":"2021-08-19T04:36:30.461297Z","shell.execute_reply.started":"2021-08-19T04:36:30.449554Z","shell.execute_reply":"2021-08-19T04:36:30.460303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(target_scored_df.iloc[:, 1:2])","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:30.46308Z","iopub.execute_input":"2021-08-19T04:36:30.463486Z","iopub.status.idle":"2021-08-19T04:36:30.473268Z","shell.execute_reply.started":"2021-08-19T04:36:30.463447Z","shell.execute_reply":"2021-08-19T04:36:30.472123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_scored_df.iloc[:, 1:2].iloc[:, 0].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:30.474858Z","iopub.execute_input":"2021-08-19T04:36:30.475372Z","iopub.status.idle":"2021-08-19T04:36:30.487151Z","shell.execute_reply.started":"2021-08-19T04:36:30.475335Z","shell.execute_reply":"2021-08-19T04:36:30.486062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(1, len(target_scored_df.columns)):\n    print(i, target_scored_df.iloc[:, i:(i+1)].iloc[:, 0].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:30.489142Z","iopub.execute_input":"2021-08-19T04:36:30.489549Z","iopub.status.idle":"2021-08-19T04:36:30.849316Z","shell.execute_reply.started":"2021-08-19T04:36:30.489511Z","shell.execute_reply":"2021-08-19T04:36:30.84858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_scored_df.columns[1:]","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:30.851332Z","iopub.execute_input":"2021-08-19T04:36:30.851592Z","iopub.status.idle":"2021-08-19T04:36:30.857588Z","shell.execute_reply.started":"2021-08-19T04:36:30.851565Z","shell.execute_reply":"2021-08-19T04:36:30.856601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(target_scored_df.columns[1:])","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:30.858969Z","iopub.execute_input":"2021-08-19T04:36:30.859306Z","iopub.status.idle":"2021-08-19T04:36:30.870485Z","shell.execute_reply.started":"2021-08-19T04:36:30.859268Z","shell.execute_reply":"2021-08-19T04:36:30.869609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MoA prediction model","metadata":{}},{"cell_type":"code","source":"!rm -rf keras_unsupervised","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:30.872393Z","iopub.execute_input":"2021-08-19T04:36:30.8728Z","iopub.status.idle":"2021-08-19T04:36:31.532982Z","shell.execute_reply.started":"2021-08-19T04:36:30.872759Z","shell.execute_reply":"2021-08-19T04:36:31.531849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/tonandr/keras_unsupervised.git","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:31.534619Z","iopub.execute_input":"2021-08-19T04:36:31.534988Z","iopub.status.idle":"2021-08-19T04:36:34.161939Z","shell.execute_reply.started":"2021-08-19T04:36:31.534947Z","shell.execute_reply":"2021-08-19T04:36:34.161034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd keras_unsupervised","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:34.16558Z","iopub.execute_input":"2021-08-19T04:36:34.165901Z","iopub.status.idle":"2021-08-19T04:36:34.176002Z","shell.execute_reply.started":"2021-08-19T04:36:34.165868Z","shell.execute_reply":"2021-08-19T04:36:34.175104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile setup.py\n\"\"\"Keras unsupervised setup module.\n\"\"\"\n\nfrom setuptools import setup, find_packages\nfrom os import path\nfrom io import open\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the README file\nwith open(path.join(here, 'README.md'), encoding='utf-8') as f:\n    long_description = f.read()\n\nsetup(\n    name='keras-unsupervised',  # Required\n    version='1.1.3.dev1',  # Required\n    description='Keras based unsupervised learning framework.',  # Optional\n    long_description=long_description,  # Optional\n    long_description_content_type='text/markdown',  # Optional (see note above)\n    url='https://github.com/tonandr/keras_unsupervised',  # Optional\n    author='Inwoo Chung',  # Optional\n    author_email='gutomitai@gmail.com',  # Optional\n\n    classifiers=[  # Optional\n        # How mature is this project? Common values are\n        #   3 - Alpha\n        #   4 - Beta\n        #   5 - Production/Stable\n        'Development Status :: 4 - Beta',\n\n        # Indicate who your project is intended for\n        'Intended Audience :: Developers',\n        'Intended Audience :: Science/Research',\n        #'Software Development :: Libraries',\n\n        # Pick your license as you wish\n        'License :: OSI Approved :: BSD License',\n\n        # Specify the Python versions you support here. In particular, ensure\n        # that you indicate whether you support Python 2, Python 3 or both.\n        # These classifiers are *not* checked by 'pip install'. See instead\n        # 'python_requires' below.\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.7'\n        'Programming Language :: Python :: 3.8'\n    ],\n\n    # This field adds keywords for your project which will appear on the\n    # project page. What does your project relate to?\n    #\n    # Note that this is a string of words separated by whitespace, not a list.\n    keywords='keras deepleaning unsupervised semisupervised restricted-botlzmann-machine deep-belief-network autoencoder generative-adversarial-networks',  # Optional\n\n    # You can just specify package directories manually here if your project is\n    # simple. Or you can use find_packages().\n    #\n    # Alternatively, if you just want to distribute a single Python file, use\n    # the `py_modules` argument instead as follows, which will expect a file\n    # called `my_module.py` to exist:\n    #\n    #   py_modules=[\"my_module\"],\n    #\n    packages=find_packages(exclude=['analysis', 'docs', 'docs_mkdocs', 'resource']),  # Required\n\n    # This field lists other packages that your project depends on to run.\n    # Any package you put here will be installed by pip when your project is\n    # installed, so they must be valid existing projects.\n    #\n    # For an analysis of \"install_requires\" vs pip's requirements files see:\n    # https://packaging.python.org/en/latest/requirements.html\n    install_requires=['tensorflow-probability==0.11'\n                      , 'pandas'\n                      , 'scikit-image'\n                      , 'matplotlib'\n                      , 'opencv-contrib-python'],  # Optional\n\n    # Specify which Python versions you support. In contrast to the\n    # 'Programming Language' classifiers above, 'pip install' will check this\n    # and refuse to install the project if the version does not match. If you\n    # do not support Python 2, you can simplify this to '>=3.5' or similar, see\n    # https://packaging.python.org/guides/distributing-packages-using-setuptools/#python-requires\n    python_requires='<=3.8',\n\n    # List additional groups of dependencies here (e.g. development\n    # dependencies). Users will be able to install these using the \"extras\"\n    # syntax, for example:\n    #\n    #   $ pip install sampleproject[dev]\n    #\n    # Similar to `install_requires` above, these must be valid existing\n    # projects.\n    #extras_require={  # Optional\n    #    'dev': ['check-manifest'],\n    #    'test': ['coverage'],\n    #},\n\n    # If there are data files included in your packages that need to be\n    # installed, specify them here.\n    #\n    # If using Python 2.6 or earlier, then these have to be included in\n    # MANIFEST.in as well.\n    #package_data={  # Optional\n    #    'sample': ['package_data.dat'],\n    #},\n\n    # Although 'package_data' is the preferred approach, in some case you may\n    # need to place data files outside of your packages. See:\n    # http://docs.python.org/3.4/distutils/setupscript.html#installing-additional-files\n    #\n    # In this case, 'data_file' will be installed into '<sys.prefix>/my_data'\n    #data_files=[('my_data', ['data/data_file'])],  # Optional\n\n    # To provide executable scripts, use entry points in preference to the\n    # \"scripts\" keyword. Entry points provide cross-platform support and allow\n    # `pip` to create the appropriate form of executable for the target\n    # platform.\n    #\n    # For example, the following would provide a command called `sample` which\n    # executes the function `main` from this package when invoked:\n    #entry_points={  # Optional\n    #    'console_scripts': [\n    #        'sample=sample:main',\n    #    ],\n    #},\n\n    # List additional URLs that are relevant to your project as a dict.\n    #\n    # This field corresponds to the \"Project-URL\" metadata fields:\n    # https://packaging.python.org/specifications/core-metadata/#project-url-multiple-use\n    #\n    # Examples listed include a pattern for specifying where the package tracks\n    # issues, where the source is hosted, where to say thanks to the package\n    # maintainers, and where to support the project financially. The key is\n    # what's used to render the link text on PyPI.\n    project_urls={  # Optional\n        'Bug Reports': 'https://github.com/tonandr/keras_unsupervised/issues',\n        'Source': 'https://github.com/tonandr/keras_unsupervised/',\n    },\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:34.179712Z","iopub.execute_input":"2021-08-19T04:36:34.180205Z","iopub.status.idle":"2021-08-19T04:36:34.190027Z","shell.execute_reply.started":"2021-08-19T04:36:34.180167Z","shell.execute_reply":"2021-08-19T04:36:34.188928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python setup.py sdist bdist_wheel\n!pip install -e ./","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:34.191709Z","iopub.execute_input":"2021-08-19T04:36:34.19216Z","iopub.status.idle":"2021-08-19T04:36:51.279935Z","shell.execute_reply.started":"2021-08-19T04:36:34.192084Z","shell.execute_reply":"2021-08-19T04:36:51.278934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd ..","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-19T04:36:51.281503Z","iopub.execute_input":"2021-08-19T04:36:51.281891Z","iopub.status.idle":"2021-08-19T04:36:51.291304Z","shell.execute_reply.started":"2021-08-19T04:36:51.281852Z","shell.execute_reply":"2021-08-19T04:36:51.289294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:51.292672Z","iopub.execute_input":"2021-08-19T04:36:51.293003Z","iopub.status.idle":"2021-08-19T04:36:51.302401Z","shell.execute_reply.started":"2021-08-19T04:36:51.292973Z","shell.execute_reply":"2021-08-19T04:36:51.301531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sys.path.append('/kaggle/working/keras_unsupervised')","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:51.30368Z","iopub.execute_input":"2021-08-19T04:36:51.303977Z","iopub.status.idle":"2021-08-19T04:36:51.315078Z","shell.execute_reply.started":"2021-08-19T04:36:51.303951Z","shell.execute_reply":"2021-08-19T04:36:51.314284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nCreated on Oct 8, 2020\n@author: Inwoo Chung (gutomitai@gmail.com)\n'''\n\nimport os\nimport time\nimport json\nimport random\nfrom random import shuffle\nimport ctypes\n\n#ctypes.WinDLL('cudart64_110.dll') #?\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedKFold\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.losses import Loss\nfrom tensorflow.keras.metrics import Metric\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input, Conv1D, Dense, Concatenate, Dropout\nfrom tensorflow.keras.layers import LSTM, Bidirectional, BatchNormalization, LayerNormalization\nfrom tensorflow.keras.layers import Embedding, Layer\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import (TensorBoard\n    , ReduceLROnPlateau\n    , LearningRateScheduler\n    , ModelCheckpoint\n    , EarlyStopping)\nfrom tensorflow.keras.constraints import UnitNorm\nfrom tensorflow.keras.initializers import RandomUniform, TruncatedNormal\nfrom tensorflow.keras import regularizers\n\nfrom ku.composite_layer import DenseBatchNormalization\nfrom ku.backprop import (make_decoder_from_encoder\n    , make_autoencoder_from_encoder\n    , make_autoencoder_with_sym_sc)\n\n# os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1'\n\n# Constants.\nDEBUG = True\n\nMODE_TRAIN = 0\nMODE_VAL = 1\n\nCV_TYPE_TRAIN_VAL_SPLIT = 'train_val_split'\nCV_TYPE_K_FOLD = 'k_fold'\n\nDATASET_TYPE_PLAIN = 'plain'\nDATASET_TYPE_BALANCED = 'balanced'\n\nLOSS_TYPE_MULTI_LABEL = 'multi_label'\nLOSS_TYPE_ADDITIVE_ANGULAR_MARGIN = 'additive_angular_margin'\n\nepsilon = 1e-7\n\n\nclass MoALoss(Loss):\n    def __init__(self\n                 , W\n                 , m=0.5\n                 , ls=0.2\n                 , scale=64.0\n                 , loss_type=LOSS_TYPE_ADDITIVE_ANGULAR_MARGIN\n                 , name='MoA_loss'):\n        super(MoALoss, self).__init__(name=name)\n        self.W = W\n        self.m = m\n        self.ls = ls\n        self.scale = scale\n        self.loss_type = loss_type\n\n    #@tf.function\n    def call(self, y_true, y_pred):\n        y_true = tf.cast(y_true, dtype=tf.float32)\n        pos_mask = y_true\n        neg_mask = 1.0 - y_true\n\n        # Label smoothing.\n        y_true = pos_mask * y_true * (1.0 - self.ls / 2.0) + neg_mask * (y_true + self.ls / 2.0)\n\n        '''\n        pos_log_loss = pos_mask * self.W[:, :, 0] * tf.sqrt(tf.square(y_true - y_pred))\n        pos_log_loss_mean = tf.reduce_mean(pos_log_loss, axis=0) #?\n        pos_loss = 1.0 * tf.reduce_mean(pos_log_loss_mean, axis=0)\n\n        neg_log_loss = neg_mask * self.W[:, :, 1] * tf.sqrt(tf.square(y_true - y_pred))\n        neg_log_loss_mean = tf.reduce_mean(neg_log_loss, axis=0) #?\n        neg_loss = 1.0 * tf.reduce_mean(neg_log_loss_mean, axis=0)\n\n        loss = pos_loss + neg_loss\n        '''\n\n        '''\n        loss = tf.reduce_mean(tf.sqrt(tf.square(y_true - y_pred)))\n        loss = tf.losses.binary_crossentropy(y_true, y_pred)\n        log_loss_mean = tf.reduce_mean(log_loss, axis=0) #?\n        loss = tf.reduce_mean(log_loss_mean, axis=0)\n        '''\n\n        if self.loss_type == LOSS_TYPE_ADDITIVE_ANGULAR_MARGIN:\n            A = y_pred\n            e_AM_A = tf.math.exp(self.scale * tf.math.cos(tf.math.acos(A) + self.m))\n            #d = A.shape[-1] #?\n            S = tf.tile(tf.reduce_sum(tf.math.exp(A), axis=1, keepdims=True), (1, 206))\n            S_p = S - tf.math.exp(A) + e_AM_A\n            P = e_AM_A / (S_p + epsilon)\n            #P = tf.clip_by_value(P, clip_value_min=epsilon, clip_value_max=(1.0 - epsilon))\n\n            #log_loss_1 = -1.0 * self.W[:, :, 0] * y_true * tf.math.log(P)\n            log_loss_1 = -1.0 * y_true * tf.math.log(P)\n            log_loss_2 = tf.reduce_sum(log_loss_1, axis=1)\n            loss = tf.reduce_mean(log_loss_2, axis=0)\n        elif self.loss_type == LOSS_TYPE_MULTI_LABEL:\n            y_pred = tf.sigmoid(y_pred)\n            y_pred = tf.maximum(tf.minimum(y_pred, 1.0 - 1e-15), 1e-15)\n            log_loss = -1.0 * (y_true * tf.math.log(y_pred + epsilon) + (1.0 - y_true) * tf.math.log(1.0 - y_pred + epsilon))\n            log_loss_mean = tf.reduce_mean(log_loss, axis=0) #?\n            loss = tf.reduce_mean(log_loss_mean, axis=0)\n        else:\n            raise ValueError('loss type is not valid.')\n\n        #tf.print(A, e_AM_A, S, S_p, P, log_loss_1, log_loss_2, loss)\n        return loss\n\n    def get_config(self):\n        \"\"\"Get configuration.\"\"\"\n        config = {'W': self.W\n                  , 'm': self.m\n                  , 'ls': self.ls\n                  , 'scale': self.scale\n                  , 'loss_type': self.loss_type}\n        base_config = super(MoALoss, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass MoAMetric(Metric):\n    def __init__(self, sn_t=2.45, name='MoA_metric', **kwargs):\n        super(MoAMetric, self).__init__(name=name, **kwargs)\n        self.sn_t = sn_t\n        self.total_loss = self.add_weight(name='total_loss', initializer='zeros')\n        self.count = self.add_weight(name='count', initializer='zeros')\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        E = tf.reduce_mean(tf.math.exp(y_pred), axis=1, keepdims=True)\n        E_2 = tf.reduce_mean(tf.square(tf.math.exp(y_pred)), axis=1, keepdims=True)\n        S = tf.sqrt(E_2 - tf.square(E))\n\n        e_A = (tf.exp(y_pred) - E) / (S + epsilon)\n        e_A_p = tf.where(tf.math.greater(e_A, self.sn_t), self.sn_t, 0.0)\n        p_hat = e_A_p / (tf.reduce_sum(e_A_p, axis=1, keepdims=True) + epsilon)\n\n        y_pred = tf.maximum(tf.minimum(p_hat, 1.0 - 1e-15), 1e-15)\n        y_true = tf.cast(y_true, dtype=tf.float32)\n\n        log_loss = -1.0 * (y_true * tf.math.log(y_pred + epsilon) + (1.0 - y_true) * tf.math.log(1.0 - y_pred + epsilon))\n        log_loss_mean = tf.reduce_mean(log_loss, axis=0) #?\n        loss = tf.reduce_mean(log_loss_mean, axis=0)\n\n        self.total_loss.assign_add(loss)\n        self.count.assign_add(tf.constant(1.0))\n\n    def result(self):\n        return tf.math.divide_no_nan(self.total_loss, self.count)\n\n    def reset_states(self):\n        self.total_loss.assign(0.0)\n        self.count.assign(0.0)\n\n    def get_config(self):\n        \"\"\"Get configuration.\"\"\"\n        config = {'sn_t': self.sn_t}\n        base_config = super(MoAMetric, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass _MoAPredictor(Layer):\n    def __init__(self, conf, **kwargs):\n        super(_MoAPredictor, self).__init__(**kwargs)\n\n        # Initialize.\n        self.conf = conf\n        self.hps = self.conf['hps']\n        self.nn_arch = self.conf['nn_arch']\n\n        # Design layers.\n        # First layers.\n        self.embed_treatment_type_0 = Embedding(self.nn_arch['num_treatment_type']\n                                           , self.nn_arch['d_input_feature'])\n        self.dense_treatment_type_0 = Dense(self.nn_arch['d_input_feature']\n                                       , activation='relu')\n\n        self.layer_normalization_0_1 = LayerNormalization()\n        self.layer_normalization_0_2 = LayerNormalization()\n        self.layer_normalization_0_3 = LayerNormalization()\n\n        # Autoencoder for gene expression profile.\n        input_gene_exp_1 = Input(shape=(self.nn_arch['d_gene_exp'],))\n        d_geps = [int(self.nn_arch['d_gep_init'] / np.power(2, v)) for v in range(4)]\n\n        dense_1_1 = Dense(d_geps[0], activation='swish', kernel_regularizer=regularizers.l2(self.hps['weight_decay']))\n        batch_normalization_1_1 = BatchNormalization()\n        dropout_1_1 = None # Dropout(self.nn_arch['dropout_rate'])\n        dense_batch_normalization_1_1 = DenseBatchNormalization(dense_1_1\n                                                                , batch_normalization_1_1\n                                                                , dropout=dropout_1_1)\n\n        dense_1_2 = Dense(d_geps[1], activation='swish', kernel_regularizer=regularizers.l2(self.hps['weight_decay']))\n        batch_normalization_1_2 = BatchNormalization()\n        dropout_1_2 = None # Dropout(self.nn_arch['dropout_rate'])\n        dense_batch_normalization_1_2 = DenseBatchNormalization(dense_1_2\n                                                                , batch_normalization_1_2\n                                                                , dropout=dropout_1_2)\n\n        dense_1_3 = Dense(d_geps[2], activation='swish', kernel_regularizer=regularizers.l2(self.hps['weight_decay']))\n        batch_normalization_1_3 = BatchNormalization()\n        dropout_1_3 = None #Dropout(self.nn_arch['dropout_rate'])\n        dense_batch_normalization_1_3 = DenseBatchNormalization(dense_1_3\n                                                                , batch_normalization_1_3\n                                                                , dropout=dropout_1_3)\n\n        dense_1_4 = Dense(d_geps[3], activation='swish', kernel_regularizer=regularizers.l2(self.hps['weight_decay']))\n        batch_normalization_1_4 = BatchNormalization()\n        dropout_1_4 = None #Dropout(self.nn_arch['dropout_rate'])\n        dense_batch_normalization_1_4 = DenseBatchNormalization(dense_1_4\n                                                                , batch_normalization_1_4\n                                                                , dropout=dropout_1_4)\n\n        self.encoder_gene_exp_1 = keras.Sequential([input_gene_exp_1\n                                                    , dense_batch_normalization_1_1\n                                                    , dense_batch_normalization_1_2\n                                                    , dense_batch_normalization_1_3\n                                                    , dense_batch_normalization_1_4])\n        self.decoder_gene_exp_1 = make_decoder_from_encoder(self.encoder_gene_exp_1)\n        self.dropout_1 = Dropout(self.nn_arch['dropout_rate'])\n\n        # Autoencoder for cell type.\n        input_gene_exp_2 = Input(shape=(self.nn_arch['d_cell_type'],))\n        d_cvs = [int(self.nn_arch['d_cv_init'] / np.power(2, v)) for v in range(3)]\n\n        dense_2_1 = Dense(d_cvs[0], activation='swish', kernel_regularizer=regularizers.l2(self.hps['weight_decay']))\n        batch_normalization_2_1 = BatchNormalization()\n        dropout_2_1 = None # Dropout(self.nn_arch['dropout_rate'])\n        dense_batch_normalization_2_1 = DenseBatchNormalization(dense_2_1\n                                                                , batch_normalization_2_1\n                                                                , dropout=dropout_2_1)\n\n        dense_2_2 = Dense(d_cvs[1], activation='swish', kernel_regularizer=regularizers.l2(self.hps['weight_decay']))\n        batch_normalization_2_2 = BatchNormalization()\n        dropout_2_2 = None # Dropout(self.nn_arch['dropout_rate'])\n        dense_batch_normalization_2_2 = DenseBatchNormalization(dense_2_2\n                                                                , batch_normalization_2_2\n                                                                , dropout=dropout_2_2)\n\n        dense_2_3 = Dense(d_cvs[2], activation='swish', kernel_regularizer=regularizers.l2(self.hps['weight_decay']))\n        batch_normalization_2_3 = BatchNormalization()\n        dropout_2_3 = None #Dropout(self.nn_arch['dropout_rate'])\n        dense_batch_normalization_2_3 = DenseBatchNormalization(dense_2_3\n                                                                , batch_normalization_2_3\n                                                                , dropout=dropout_2_3)\n\n        self.encoder_cell_type_2 = keras.Sequential([input_gene_exp_2\n                                                    , dense_batch_normalization_2_1\n                                                    , dense_batch_normalization_2_2\n                                                    , dense_batch_normalization_2_3])\n        self.decoder_cell_type_2 = make_decoder_from_encoder(self.encoder_cell_type_2)\n        self.dropout_2 = Dropout(self.nn_arch['dropout_rate'])\n\n        # Skip-connection autoencoder layer.\n        self.sc_aes = []\n        self.dropout_3 = Dropout(self.nn_arch['dropout_rate'])\n\n        for i in range(self.nn_arch['num_sc_ae']):\n            input_sk_ae_3 = Input(shape=(self.nn_arch['d_hidden'],))\n            d_ae_init = d_geps[-1] + d_cvs[-1] + self.nn_arch['d_input_feature']\n            d_aes = [d_ae_init, int(d_ae_init * 2), int(d_ae_init * 2), d_ae_init]\n\n            dense_3_1 = Dense(d_aes[0], activation='swish', kernel_regularizer=regularizers.l2(self.hps['weight_decay']))\n            batch_normalization_3_1 = BatchNormalization()\n            dropout_3_1 = None # Dropout(self.nn_arch['dropout_rate'])\n            dense_batch_normalization_3_1 = DenseBatchNormalization(dense_3_1\n                                                                    , batch_normalization_3_1\n                                                                    , dropout=dropout_3_1)\n\n            dense_3_2 = Dense(d_aes[1], activation='swish', kernel_regularizer=regularizers.l2(self.hps['weight_decay']))\n            batch_normalization_3_2 = BatchNormalization()\n            dropout_3_2 = None # Dropout(self.nn_arch['dropout_rate'])\n            dense_batch_normalization_3_2 = DenseBatchNormalization(dense_3_2\n                                                                    , batch_normalization_3_2\n                                                                    , dropout=dropout_3_2)\n\n            dense_3_3 = Dense(d_aes[2], activation='swish', kernel_regularizer=regularizers.l2(self.hps['weight_decay']))\n            batch_normalization_3_3 = BatchNormalization()\n            dropout_3_3 = None # Dropout(self.nn_arch['dropout_rate'])\n            dense_batch_normalization_3_3 = DenseBatchNormalization(dense_3_3\n                                                                    , batch_normalization_3_3\n                                                                    , dropout=dropout_3_3)\n\n            dense_3_4 = Dense(d_aes[3], activation='swish', kernel_regularizer=regularizers.l2(self.hps['weight_decay']))\n            batch_normalization_3_4 = BatchNormalization()\n            dropout_3_4 = None # Dropout(self.nn_arch['dropout_rate'])\n            dense_batch_normalization_3_4 = DenseBatchNormalization(dense_3_4\n                                                                    , batch_normalization_3_4\n                                                                    , dropout=dropout_3_4)\n\n            sc_encoder_3 = keras.Sequential([input_sk_ae_3\n                                                        , dense_batch_normalization_3_1\n                                                        , dense_batch_normalization_3_2\n                                                        , dense_batch_normalization_3_3\n                                                        , dense_batch_normalization_3_4])\n            sc_autoencoder_3 = make_autoencoder_from_encoder(sc_encoder_3)\n            self.sc_aes.append(make_autoencoder_with_sym_sc(sc_autoencoder_3))\n\n        # Final layers.\n        d_fs = [int(self.nn_arch['d_f_init'] / np.power(2, v)) for v in range(3)]\n\n        self.dense_4_1 = Dense(d_fs[0], activation='swish', kernel_regularizer=regularizers.l2(self.hps['weight_decay']))\n        self.dense_4_2 = Dense(d_fs[1], activation='swish', kernel_regularizer=regularizers.l2(self.hps['weight_decay']))\n        self.dense_4_3 = Dense(d_fs[2], activation='swish', kernel_regularizer=regularizers.l2(self.hps['weight_decay']))\n        self.dropout_4_3 = Dropout(self.nn_arch['dropout_rate'])\n\n        if self.conf['loss_type'] == LOSS_TYPE_MULTI_LABEL:\n            self.dense_4_4 = Dense(self.nn_arch['num_moa_annotation']\n                                   , activation='linear'\n                                   , kernel_initializer=TruncatedNormal()\n                                   , kernel_constraint=None\n                                   , kernel_regularizer=regularizers.l2(self.hps['weight_decay'])\n                                   , use_bias=False) #?\n        elif self.conf['loss_type'] == LOSS_TYPE_ADDITIVE_ANGULAR_MARGIN:\n            self.dense_4_4 = Dense(self.nn_arch['num_moa_annotation']\n                                   , activation='linear'\n                                   , kernel_initializer=TruncatedNormal()\n                                   , kernel_constraint=UnitNorm()\n                                   , kernel_regularizer=regularizers.l2(self.hps['weight_decay'])\n                                   , use_bias=False) #?\n        else:\n            raise ValueError('loss type is not valid.')\n\n    def call(self, inputs):\n        t = inputs[0]\n        g = inputs[1]\n        c = inputs[2]\n\n        # First layers.\n        t = self.embed_treatment_type_0(t)\n        t = tf.reshape(t, (-1, self.nn_arch['d_input_feature']))\n        t = self.dense_treatment_type_0(t)\n\n        t = self.layer_normalization_0_1(t)\n        g = self.layer_normalization_0_2(g)\n        c = self.layer_normalization_0_3(c)\n\n        # Gene expression.\n        g_e = self.encoder_gene_exp_1(g)\n        x_g = self.decoder_gene_exp_1(g_e)\n        x_g = tf.expand_dims(x_g, axis=-1)\n        x_g = tf.squeeze(x_g, axis=-1)\n        x_g = self.dropout_1(x_g)\n\n        # Cell type.\n        c_e = self.encoder_cell_type_2(c)\n        x_c = self.decoder_cell_type_2(c_e)\n        x_c = self.dropout_2(x_c)\n\n        # Skip-connection autoencoder and final layers.\n        x = tf.concat([t, g_e, c_e], axis=-1)\n        for i in range(self.nn_arch['num_sc_ae']):\n            x = self.sc_aes[i](x)\n            x = self.dropout_3(x)\n\n        # Final layers.\n        x = self.dense_4_1(x)\n        x = self.dense_4_2(x)\n        x = self.dense_4_3(x)\n        x = self.dropout_4_3(x)\n\n        # Normalize x.\n        if self.conf['loss_type'] == LOSS_TYPE_MULTI_LABEL:\n            x1 = self.dense_4_4(x)\n        elif self.conf['loss_type'] == LOSS_TYPE_ADDITIVE_ANGULAR_MARGIN:\n            x = x / tf.sqrt(tf.reduce_sum(tf.square(x), axis=1, keepdims=True))\n            x1 = self.dense_4_4(x)\n        else:\n            raise ValueError('loss type is not valid.')\n\n        outputs = [x_g, x_c, x1]\n        return outputs\n\n    def get_config(self):\n        \"\"\"Get configuration.\"\"\"\n        config = {'conf': self.conf}\n        base_config = super(_MoAPredictor, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:51.316877Z","iopub.execute_input":"2021-08-19T04:36:51.317367Z","iopub.status.idle":"2021-08-19T04:36:57.128876Z","shell.execute_reply.started":"2021-08-19T04:36:51.317327Z","shell.execute_reply":"2021-08-19T04:36:57.127348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MoAPredictor(object):\n    \"\"\"MoA predictor.\"\"\"\n\n    # Constants.\n    MODEL_PATH = 'MoA_predictor'\n    OUTPUT_FILE_NAME = 'submission.csv'\n    EVALUATION_FILE_NAME = 'eval.csv'\n\n    def __init__(self, conf):\n        \"\"\"\n        Parameters\n        ----------\n        conf: Dictionary\n            Configuration dictionary.\n        \"\"\"\n        # Initialize.\n        self.conf = conf\n        self.raw_data_path = self.conf['raw_data_path']\n        self.hps = self.conf['hps']\n        self.nn_arch = self.conf['nn_arch']\n        self.model_loading = self.conf['model_loading']\n\n        # Create weight for classification imbalance.\n        W = self._create_W()\n\n        # with strategy.scope():\n        if self.conf['cv_type'] == CV_TYPE_TRAIN_VAL_SPLIT:\n            if self.model_loading:\n                self.model = load_model(self.MODEL_PATH + '.h5'\n                                        , custom_objects={'MoALoss': MoALoss\n                                            , 'MoAMetric': MoAMetric\n                                            , '_MoAPredictor': _MoAPredictor}\n                                        , compile=False)\n                #self.model = load_model(self.MODEL_PATH, compile=False)\n                opt = optimizers.Adam(lr=self.hps['lr']\n                                      , beta_1=self.hps['beta_1']\n                                      , beta_2=self.hps['beta_2']\n                                      , decay=self.hps['decay'])\n                self.model.compile(optimizer=opt\n                                   , loss=['mse', 'mse', MoALoss(W\n                                                                  , self.nn_arch['additive_margin']\n                                                                  , self.hps['ls']\n                                                                  , self.nn_arch['scale']\n                                                                  , loss_type=self.conf['loss_type'])]\n                              , loss_weights=self.hps['loss_weights']\n                              , metrics=[['mse'], ['mse'], [MoAMetric(self.hps['sn_t'])]]\n                              , run_eagerly=False)\n            else:\n                # Design the MoA prediction model.\n                # Input.\n                input_t = Input(shape=(self.nn_arch['d_treatment_type'],))\n                input_g = Input(shape=(self.nn_arch['d_gene_exp'],))\n                input_c = Input(shape=(self.nn_arch['d_cell_type'],))\n\n                outputs = _MoAPredictor(self.conf, name='moap')([input_t, input_g, input_c])\n\n                opt = optimizers.Adam(lr=self.hps['lr']\n                                      , beta_1=self.hps['beta_1']\n                                      , beta_2=self.hps['beta_2']\n                                      , decay=self.hps['decay'])\n\n                self.model = Model(inputs=[input_t, input_g, input_c], outputs=outputs)\n                self.model.compile(optimizer=opt\n                                   , loss=['mse', 'mse', MoALoss(W\n                                                                  , self.nn_arch['additive_margin']\n                                                                  , self.hps['ls']\n                                                                  , self.nn_arch['scale']\n                                                                  , loss_type=self.conf['loss_type'])]\n                              , loss_weights=self.hps['loss_weights']\n                              , metrics=[['mse'], ['mse'], [MoAMetric(self.hps['sn_t'])]]\n                              , run_eagerly=False)\n                self.model.summary()\n        elif self.conf['cv_type'] == CV_TYPE_K_FOLD:\n            self.k_fold_models = []\n\n            if self.model_loading:\n                opt = optimizers.Adam(lr=self.hps['lr']\n                                      , beta_1=self.hps['beta_1']\n                                      , beta_2=self.hps['beta_2']\n                                      , decay=self.hps['decay'])\n\n                # load models for K-fold.\n                for i in range(self.nn_arch['k_fold']):\n                    self.k_fold_models.append(load_model(self.MODEL_PATH + '_' + str(i) + '.h5'\n                                                        , custom_objects={'MoALoss': MoALoss\n                                                            , 'MoAMetric': MoAMetric\n                                                            , '_MoAPredictor': _MoAPredictor}\n                                                         , compile=False))\n                    self.k_fold_models[i].compile(optimizer=opt\n                                   , loss=['mse', 'mse', MoALoss(W\n                                                                  , self.nn_arch['additive_margin']\n                                                                  , self.hps['ls']\n                                                                  , self.nn_arch['scale']\n                                                                  , loss_type=self.conf['loss_type'])]\n                              , loss_weights=self.hps['loss_weights']\n                              , metrics=[['mse'], ['mse'], [MoAMetric(self.hps['sn_t'])]]\n                              , run_eagerly=False)\n            else:\n                # Create models for K-fold.\n                for i in range(self.nn_arch['k_fold']):\n                    # Design the MoA prediction model.\n                    # Input.\n                    input_t = Input(shape=(self.nn_arch['d_treatment_type'],))\n                    input_g = Input(shape=(self.nn_arch['d_gene_exp'],))\n                    input_c = Input(shape=(self.nn_arch['d_cell_type'],))\n\n                    outputs = _MoAPredictor(self.conf, name='moap')([input_t, input_g, input_c])\n\n                    opt = optimizers.Adam(lr=self.hps['lr']\n                                          , beta_1=self.hps['beta_1']\n                                          , beta_2=self.hps['beta_2']\n                                          , decay=self.hps['decay'])\n\n                    model = Model(inputs=[input_t, input_g, input_c], outputs=outputs)\n                    model.compile(optimizer=opt\n                                   , loss=['mse', 'mse', MoALoss(W\n                                                                  , self.nn_arch['additive_margin']\n                                                                  , self.hps['ls']\n                                                                  , self.nn_arch['scale']\n                                                                  , loss_type=self.conf['loss_type'])]\n                              , loss_weights=self.hps['loss_weights']\n                              , metrics=[['mse'], ['mse'], [MoAMetric(self.hps['sn_t'])]]\n                              , run_eagerly=False)\n                    model.summary()\n\n                    self.k_fold_models.append(model)\n        else:\n            raise ValueError('cv_type is not valid.')\n\n        # Create dataset.\n        self._create_dataset()\n\n    def _create_dataset(self):\n        input_df = pd.read_csv(os.path.join(self.raw_data_path, 'train_features.csv')) #.iloc[:1024]\n        input_df.cp_type = input_df.cp_type.astype('category')\n        input_df.cp_type = input_df.cp_type.cat.rename_categories(range(len(input_df.cp_type.cat.categories)))\n        input_df.cp_time = input_df.cp_time.astype('category')\n        input_df.cp_time = input_df.cp_time.cat.rename_categories(range(len(input_df.cp_time.cat.categories)))\n        input_df.cp_dose = input_df.cp_dose.astype('category')\n        input_df.cp_dose = input_df.cp_dose.cat.rename_categories(range(len(input_df.cp_dose.cat.categories)))\n\n        # Remove samples of ctl_vehicle.\n        valid_indexes = input_df.cp_type == 1\n        input_df = input_df[valid_indexes]\n        input_df = input_df.reset_index(drop=True)\n\n        target_scored_df = pd.read_csv(os.path.join(self.raw_data_path, 'train_targets_scored.csv')) #.iloc[:1024]\n        target_scored_df = target_scored_df[valid_indexes]\n        target_scored_df = target_scored_df.reset_index(drop=True)\n        del target_scored_df['sig_id']\n        target_scored_df.columns = range(len(target_scored_df.columns))\n        n_target_samples = target_scored_df.sum().values\n\n        if self.conf['data_aug']:\n            genes = [col for col in input_df.columns if col.startswith(\"g-\")]\n            cells = [col for col in input_df.columns if col.startswith(\"c-\")]\n\n            features = genes + cells\n            targets = [col for col in target_scored_df if col != 'sig_id']\n\n            aug_trains = []\n            aug_targets = []\n            for t in [0, 1, 2]:\n                for d in [0, 1]:\n                    for _ in range(3):\n                        train1 = input_df.loc[(input_df['cp_time'] == t) & (input_df['cp_dose'] == d)]\n                        target1 = target_scored_df.loc[(input_df['cp_time'] == t) & (input_df['cp_dose'] == d)]\n                        ctl1 = input_df.loc[(input_df['cp_time'] == t) & (input_df['cp_dose'] == d)].sample(\n                            train1.shape[0], replace=True)\n                        ctl2 = input_df.loc[(input_df['cp_time'] == t) & (input_df['cp_dose'] == d)].sample(\n                            train1.shape[0], replace=True)\n                        train1[genes + cells] = train1[genes + cells].values + ctl1[genes + cells].values - ctl2[\n                            genes + cells].values\n                        aug_trains.append(train1)\n                        aug_targets.append(target1)\n\n            input_df = pd.concat(aug_trains).reset_index(drop=True)\n            target_scored_df = pd.concat(aug_targets).reset_index(drop=True)\n\n        g_feature_names = ['g-' + str(v) for v in range(self.nn_arch['d_gene_exp'])]\n        c_feature_names = ['c-' + str(v) for v in range(self.nn_arch['d_cell_type'])]\n        moa_names = [v for v in range(self.nn_arch['num_moa_annotation'])]\n\n        def get_series_from_input(idxes):\n            idxes = idxes.numpy() #?\n            series = input_df.iloc[idxes]\n\n            # Treatment.\n            if isinstance(idxes, np.int32) != True:\n                cp_time = series['cp_time'].values.to_numpy()\n                cp_dose = series['cp_dose'].values.to_numpy()\n            else:\n                cp_time = np.asarray(series['cp_time'])\n                cp_dose = np.asarray(series['cp_dose'])\n\n            treatment_type = cp_time * 2 + cp_dose\n\n            # Gene expression.\n            gene_exps = series[g_feature_names].values\n\n            # Cell viability.\n            cell_vs = series[c_feature_names].values\n\n            return treatment_type, gene_exps, cell_vs\n\n\n        def make_input_target_features(idxes):\n            treatment_type, gene_exps, cell_vs = tf.py_function(get_series_from_input, inp=[idxes], Tout=[tf.int64, tf.float64, tf.float64])\n            MoA_values = tf.py_function(get_series_from_target, inp=[idxes], Tout=tf.int32)\n            return ((treatment_type, gene_exps, cell_vs), (gene_exps, cell_vs, MoA_values))\n\n\n        def make_input_features(idx):\n            treatment_type, gene_exps, cell_vs = tf.py_function(get_series_from_input, inp=[idx], Tout=[tf.int64, tf.float64, tf.float64])\n            return treatment_type, gene_exps, cell_vs\n\n\n        def make_a_target_features(idx):\n            treatment_type, gene_exps, cell_vs = tf.py_function(get_series_from_input, inp=[idx], Tout=[tf.int64, tf.float64, tf.float64])\n            return gene_exps, cell_vs\n\n\n        def get_series_from_target(idxes):\n            idxes = idxes.numpy()\n            series = target_scored_df.iloc[idxes]\n\n            # MoA annotations' values.\n            MoA_values = series[moa_names].values\n\n            return MoA_values\n\n\n        def make_target_features(idx):\n            MoA_values = tf.py_function(get_series_from_target, inp=[idx], Tout=tf.int32)\n            return MoA_values\n\n\n        def divide_inputs(input1, input2):\n            return input1[0], input1[1], input2\n\n\n        if self.conf['cv_type'] == CV_TYPE_TRAIN_VAL_SPLIT:\n            if self.conf['dataset_type'] == DATASET_TYPE_PLAIN:\n                train_val_index = np.arange(len(input_df))\n                #np.random.shuffle(train_val_index)\n                num_val = int(self.conf['val_ratio'] * len(input_df))\n                num_tr = len(input_df) - num_val\n                train_index = train_val_index[:num_tr]\n                val_index = train_val_index[num_tr:]\n                self.train_index = train_index\n                self.val_index = val_index\n\n                # Training dataset.\n                input_dataset = tf.data.Dataset.from_tensor_slices(train_index)\n                input_dataset = input_dataset.map(make_input_features)\n\n                a_target_dataset = tf.data.Dataset.from_tensor_slices(train_index)\n                a_target_dataset = a_target_dataset.map(make_a_target_features)\n\n                target_dataset = tf.data.Dataset.from_tensor_slices(train_index)\n                target_dataset = target_dataset.map(make_target_features)\n\n                f_target_dataset = tf.data.Dataset.zip((a_target_dataset, target_dataset)).map(divide_inputs)\n\n                # Inputs and targets.\n                tr_dataset = tf.data.Dataset.zip((input_dataset, f_target_dataset))\n                tr_dataset = tr_dataset.shuffle(buffer_size=self.hps['batch_size'] * 5\n                                                , reshuffle_each_iteration=True).repeat().batch(self.hps['batch_size'])\n                self.step = len(train_index) // self.hps['batch_size']\n\n                # Validation dataset.\n                input_dataset = tf.data.Dataset.from_tensor_slices(val_index)\n                input_dataset = input_dataset.map(make_input_features)\n\n                a_target_dataset = tf.data.Dataset.from_tensor_slices(val_index)\n                a_target_dataset = a_target_dataset.map(make_a_target_features)\n\n                target_dataset = tf.data.Dataset.from_tensor_slices(val_index)\n                target_dataset = target_dataset.map(make_target_features)\n\n                f_target_dataset = tf.data.Dataset.zip((a_target_dataset, target_dataset)).map(divide_inputs)\n\n                # Inputs and targets.\n                val_dataset = tf.data.Dataset.zip((input_dataset, f_target_dataset))\n                val_dataset = val_dataset.batch(self.hps['batch_size'])\n\n                self.trval_dataset = (tr_dataset, val_dataset)\n            elif self.conf['dataset_type'] == DATASET_TYPE_BALANCED:\n                MoA_p_sets = []\n                val_index = []\n                for col in target_scored_df.columns:\n                    s = target_scored_df.iloc[:, col]\n                    s = s[s == 1]\n                    s = list(s.index)\n                    #shuffle(s)\n                    n_val = int(n_target_samples[col] * self.conf['val_ratio'])\n\n                    if n_val != 0:\n                        tr_set = s[:int(-1.0 * n_val)]\n                        val_set = s[int(-1.0 * n_val):]\n                        MoA_p_sets.append(tr_set)\n                        val_index += val_set\n                    else:\n                        MoA_p_sets.append(s)\n\n                df = target_scored_df.sum(axis=1)\n                df = df[df == 0]\n                no_MoA_p_set = list(df.index)\n                #shuffle(no_MoA_p_set)\n                val_index += no_MoA_p_set[int(-1.0 * len(no_MoA_p_set) * self.conf['val_ratio']):]\n\n                MoA_p_sets.append(no_MoA_p_set[:int(-1.0 * len(no_MoA_p_set) * self.conf['val_ratio'])])\n\n                idxes = []\n                for i in range(self.hps['rep']):\n                    for col in range(len(target_scored_df.columns) + 1):\n                        if len(MoA_p_sets[col]) >= (i + 1):\n                            idx = MoA_p_sets[col][i]\n                        else:\n                            idx = np.random.choice(MoA_p_sets[col], size=1, replace=True)[0]\n                        idxes.append(idx)\n\n                train_index = idxes\n                self.train_index = train_index\n                self.val_index = val_index\n\n                # Training dataset.\n                tr_dataset = tf.data.Dataset.from_tensor_slices(train_index)\n\n                # Inputs and targets.\n                tr_dataset = tr_dataset.shuffle(buffer_size=self.hps['batch_size'] * 5\n                                                , reshuffle_each_iteration=True).repeat().batch(self.hps['batch_size']).map(make_input_target_features\n                                                , num_parallel_calls=tf.data.experimental.AUTOTUNE)\n                self.step = len(train_index) // self.hps['batch_size']\n\n                # Validation dataset.\n                val_dataset = tf.data.Dataset.from_tensor_slices(val_index)\n\n                # Inputs and targets.\n                val_dataset = val_dataset.batch(self.hps['batch_size']).map(make_input_target_features\n                                                , num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n                # Save datasets.\n                #tf.data.experimental.save(tr_dataset, './tr_dataset')\n                #tf.data.experimental.save(val_dataset, './val_dataset')\n\n                self.trval_dataset = (tr_dataset, val_dataset)\n            else:\n                raise ValueError('dataset type is not valid.')\n        elif self.conf['cv_type'] == CV_TYPE_K_FOLD:\n            stratified_kfold = StratifiedKFold(n_splits=self.nn_arch['k_fold'])\n            # group_kfold = GroupKFold(n_splits=self.nn_arch['k_fold'])\n            self.k_fold_trval_datasets = []\n\n            for train_index, val_index in stratified_kfold.split(input_df, input_df.cp_type):\n                # Training dataset.\n                input_dataset = tf.data.Dataset.from_tensor_slices(train_index)\n                input_dataset = input_dataset.map(make_input_features)\n\n                a_target_dataset = tf.data.Dataset.from_tensor_slices(train_index)\n                a_target_dataset = a_target_dataset.map(make_a_target_features)\n\n                target_dataset = tf.data.Dataset.from_tensor_slices(train_index)\n                target_dataset = target_dataset.map(make_target_features)\n\n                f_target_dataset = tf.data.Dataset.zip((a_target_dataset, target_dataset)).map(divide_inputs)\n\n                # Inputs and targets.\n                tr_dataset = tf.data.Dataset.zip((input_dataset, f_target_dataset))\n                tr_dataset = tr_dataset.shuffle(buffer_size=self.hps['batch_size'] * 5\n                                                , reshuffle_each_iteration=True).repeat().batch(self.hps['batch_size'])\n                self.step = len(train_index) // self.hps['batch_size']\n\n                # Validation dataset.\n                input_dataset = tf.data.Dataset.from_tensor_slices(val_index)\n                input_dataset = input_dataset.map(make_input_features)\n\n                a_target_dataset = tf.data.Dataset.from_tensor_slices(val_index)\n                a_target_dataset = a_target_dataset.map(make_a_target_features)\n\n                target_dataset = tf.data.Dataset.from_tensor_slices(val_index)\n                target_dataset = target_dataset.map(make_target_features)\n\n                f_target_dataset = tf.data.Dataset.zip((a_target_dataset, target_dataset)).map(divide_inputs)\n\n                # Inputs and targets.\n                val_dataset = tf.data.Dataset.zip((input_dataset, f_target_dataset))\n                val_dataset = val_dataset.batch(self.hps['batch_size'])\n\n                self.k_fold_trval_datasets.append((tr_dataset, val_dataset))\n        else:\n            raise ValueError('cv_type is not valid.')\n\n    def _create_W(self):\n        target_scored_df = pd.read_csv(os.path.join(self.raw_data_path, 'train_targets_scored.csv'))\n        del target_scored_df['sig_id']\n\n        weights = []\n        for c in target_scored_df.columns:\n            s = target_scored_df[c]\n            s = s.value_counts()\n            s = s / s.sum()\n            weights.append(s.values)\n\n        weight = np.expand_dims(np.array(weights), axis=0)\n\n        return weight\n\n    def train(self):\n        \"\"\"Train.\"\"\"\n        reduce_lr = ReduceLROnPlateau(monitor='val_loss'\n                                      , factor=self.hps['reduce_lr_factor']\n                                      , patience=3\n                                      , min_lr=1.e-8\n                                      , verbose=1)\n        tensorboard = TensorBoard(histogram_freq=1\n                                  , write_graph=True\n                                  , write_images=True\n                                  , update_freq='epoch')\n\n        earlystopping = EarlyStopping(monitor='val_loss'\n                                      , min_delta=0\n                                      , patience=5\n                                      , verbose=1\n                                      , mode='auto')\n\n        '''\n        def schedule_lr(e_i):\n            self.hps['lr'] = self.hps['reduce_lr_factor'] * self.hps['lr']\n            return self.hps['lr']\n\n        lr_scheduler = LearningRateScheduler(schedule_lr, verbose=1)\n        '''\n\n        if self.conf['cv_type'] == CV_TYPE_TRAIN_VAL_SPLIT:\n            model_check_point = ModelCheckpoint(self.MODEL_PATH + '.h5'\n                                                , monitor='val_loss'\n                                                , verbose=1\n                                                , save_best_only=True)\n\n            hist = self.model.fit(self.trval_dataset[0]\n                                                , steps_per_epoch=self.step\n                                                , epochs=self.hps['epochs']\n                                                , verbose=1\n                                                , max_queue_size=80\n                                                , workers=4\n                                                , use_multiprocessing=False\n                                                , callbacks=[model_check_point, earlystopping] #, reduce_lr] #, tensorboard]\n                                                , validation_data=self.trval_dataset[1]\n                                                , validation_freq=1\n                                                , shuffle=True)\n        elif self.conf['cv_type'] == CV_TYPE_K_FOLD:\n            for i in range(self.nn_arch['k_fold']):\n                model_check_point = ModelCheckpoint(self.MODEL_PATH + '_' + str(i) + '.h5'\n                                                    , monitor='loss'\n                                                    , verbose=1\n                                                    , save_best_only=True)\n\n                hist = self.k_fold_models[i].fit(self.k_fold_trval_datasets[i][0]\n                                                    , steps_per_epoch=self.step\n                                                    , epochs=self.hps['epochs']\n                                                    , verbose=1\n                                                    , max_queue_size=80\n                                                    , workers=4\n                                                    , use_multiprocessing=False\n                                                    , callbacks=[model_check_point, earlystopping] #reduce_lr] #, tensorboard]\n                                                    , validation_data=self.k_fold_trval_datasets[i][1]\n                                                    , validation_freq=1\n                                                    , shuffle=True)\n        else:\n            raise ValueError('cv_type is not valid.')\n\n        #print('Save the model.')\n        #self.model.save(self.MODEL_PATH, save_format='h5')\n        # self.model.save(self.MODEL_PATH, save_format='tf')\n        return hist\n\n    def evaluate(self):\n        \"\"\"Evaluate.\"\"\"\n        assert self.conf['cv_type'] == CV_TYPE_TRAIN_VAL_SPLIT\n\n        input_df = pd.read_csv(os.path.join(self.raw_data_path, 'train_features.csv')) #.iloc[:1024]\n        input_df.cp_type = input_df.cp_type.astype('category')\n        input_df.cp_type = input_df.cp_type.cat.rename_categories(range(len(input_df.cp_type.cat.categories)))\n        input_df.cp_time = input_df.cp_time.astype('category')\n        input_df.cp_time = input_df.cp_time.cat.rename_categories(range(len(input_df.cp_time.cat.categories)))\n        input_df.cp_dose = input_df.cp_dose.astype('category')\n        input_df.cp_dose = input_df.cp_dose.cat.rename_categories(range(len(input_df.cp_dose.cat.categories)))\n\n        # Remove samples of ctl_vehicle.\n        valid_indexes = input_df.cp_type == 1  # ?\n        target_scored_df = pd.read_csv(os.path.join(self.raw_data_path, 'train_targets_scored.csv')) #.iloc[:1024]\n        target_scored_df = target_scored_df.loc[self.val_index]\n        MoA_annots = target_scored_df.columns[1:]\n\n        def make_input_features(inputs):\n            # Treatment.\n            cp_time = inputs['cp_time']\n            cp_dose = inputs['cp_dose']\n\n            treatment_type = cp_time * 2 + cp_dose\n\n            # Gene expression.\n            gene_exps = [inputs['g-' + str(v)] for v in range(self.nn_arch['d_gene_exp'])]\n            gene_exps = tf.stack(gene_exps, axis=0)\n\n            # Cell viability.\n            cell_vs = [inputs['c-' + str(v)] for v in range(self.nn_arch['d_cell_type'])]\n            cell_vs = tf.stack(cell_vs, axis=0)\n\n            return (tf.expand_dims(treatment_type, axis=-1), gene_exps, cell_vs)\n\n        # Validation dataset.\n        val_dataset = tf.data.Dataset.from_tensor_slices(input_df.loc[self.val_index].to_dict('list'))\n        val_dataset = val_dataset.map(make_input_features)\n\n        val_iter = val_dataset.as_numpy_iterator()\n\n        # Predict MoAs.\n        sig_id_list = []\n        MoAs = [[] for _ in range(len(MoA_annots))]\n\n        for i, d in tqdm(enumerate(val_iter)):\n            t, g, c = d\n            id = target_scored_df['sig_id'].iloc[i]\n            t = np.expand_dims(t, axis=0)\n            g = np.expand_dims(g, axis=0)\n            c = np.expand_dims(c, axis=0)\n\n            if self.conf['cv_type'] == CV_TYPE_TRAIN_VAL_SPLIT:\n                _, _, result = self.model.layers[-1]([t, g, c])  # self.model.predict([t, g, c])\n                result = np.squeeze(result, axis=0)\n                #result = np.exp(result) / (np.sum(np.exp(result), axis=0) + epsilon)\n\n                for i, MoA in enumerate(result):\n                    MoAs[i].append(MoA)\n            elif self.conf['cv_type'] == CV_TYPE_K_FOLD:\n                # Conduct ensemble prediction.\n                result_list = []\n\n                for i in range(self.nn_arch['k_fold']):\n                    _, _, result = self.k_fold_models[i].predict([t, g, c])\n                    result = np.squeeze(result, axis=0)\n                    #result = np.exp(result) / (np.sum(np.exp(result), axis=0) + epsilon)\n                    result_list.append(result)\n\n                result_mean = np.asarray(result_list).mean(axis=0)\n\n                for i, MoA in enumerate(result_mean):\n                    MoAs[i].append(MoA)\n            else:\n                raise ValueError('cv_type is not valid.')\n\n            sig_id_list.append(id)\n\n        # Save the result.\n        result_dict = {'sig_id': sig_id_list}\n        for i, MoA_annot in enumerate(MoA_annots):\n            result_dict[MoA_annot] = MoAs[i]\n\n        submission_df = pd.DataFrame(result_dict)\n        submission_df.to_csv(self.OUTPUT_FILE_NAME, index=False)\n\n        target_scored_df.to_csv('gt.csv', index=False)\n\n    def test(self):\n        \"\"\"Test.\"\"\"\n\n        # Create the test dataset.\n        input_df = pd.read_csv(os.path.join(self.raw_data_path, 'test_features.csv'))\n        input_df.cp_type = input_df.cp_type.astype('category')\n        input_df.cp_type = input_df.cp_type.cat.rename_categories(range(len(input_df.cp_type.cat.categories)))\n        input_df.cp_time = input_df.cp_time.astype('category')\n        input_df.cp_time = input_df.cp_time.cat.rename_categories(range(len(input_df.cp_time.cat.categories)))\n        input_df.cp_dose = input_df.cp_dose.astype('category')\n        input_df.cp_dose = input_df.cp_dose.cat.rename_categories(range(len(input_df.cp_dose.cat.categories)))\n\n        # Remove samples of ctl_vehicle.\n        valid_indexes = input_df.cp_type == 1 #?\n        target_scored_df = pd.read_csv(os.path.join(self.raw_data_path, 'train_targets_scored.csv'))\n        MoA_annots = target_scored_df.columns[1:]\n\n        def make_input_features(inputs):\n            id_ = inputs['sig_id']\n            cp_type = inputs['cp_type']\n\n            # Treatment.\n            cp_time = inputs['cp_time']\n            cp_dose = inputs['cp_dose']\n\n            treatment_type = cp_time * 2 + cp_dose\n\n            # Gene expression.\n            gene_exps = [inputs['g-' + str(v)] for v in range(self.nn_arch['d_gene_exp'])]\n            gene_exps = tf.stack(gene_exps, axis=0)\n\n            # Cell viability.\n            cell_vs = [inputs['c-' + str(v)] for v in range(self.nn_arch['d_cell_type'])]\n            cell_vs = tf.stack(cell_vs, axis=0)\n\n            return (id_, cp_type, tf.expand_dims(treatment_type, axis=-1), gene_exps, cell_vs)\n\n        test_dataset = tf.data.Dataset.from_tensor_slices(input_df.to_dict('list'))\n        test_dataset = test_dataset.map(make_input_features)\n        test_iter = test_dataset.as_numpy_iterator()\n\n        # Predict MoAs.\n        sig_id_list = []\n        MoAs = [[] for _ in range(len(MoA_annots))]\n\n        def cal_prob(logit):\n            a = logit\n            a = (a + 1.0) / 2.0\n            a = tf.where(tf.math.greater(a, self.hps['sn_t']), a, 0.0)\n            a = self.hps['m1'] * a + self.hps['m2']\n            p_h = tf.sigmoid(a).numpy()\n            return p_h\n\n        def cal_prob_2(logit):\n            y_pred = logit\n            E = tf.reduce_mean(tf.math.exp(y_pred), axis=-1, keepdims=True)\n            E_2 = tf.reduce_mean(tf.square(tf.math.exp(y_pred)), axis=-1, keepdims=True)\n            S = tf.sqrt(E_2 - tf.square(E))\n\n            e_A = (tf.exp(y_pred) - E) / (S + epsilon)\n            e_A_p = tf.where(tf.math.greater(e_A, self.hps['sn_t']), self.hps['sn_t'], 0.0)\n            p_h = e_A_p / (tf.reduce_sum(e_A_p, axis=-1, keepdims=True) + epsilon)\n            return p_h.numpy()\n\n        def cal_prob_3(logit):\n            A = logit\n            A = (A + 1.0) / 2.0\n\n            E = tf.reduce_mean(A, axis=-1, keepdims=True)\n            E_2 = tf.reduce_mean(tf.square(A), axis=-1, keepdims=True)\n            S = tf.sqrt(E_2 - tf.square(E))\n\n            #S_N = tf.abs(A - E) / (S + epsilon)\n            S_N = (A - E) / (S + epsilon)\n            #S_N = tf.where(tf.math.greater(S_N, self.hps['sn_t']), S_N, 0.0)\n            A_p = self.hps['m1'] * S_N + self.hps['m2']\n            #P_h = tf.clip_by_value(A_p / 10.0, clip_value_min=0.0, clip_value_max=1.0)\n            P_h = tf.sigmoid(A_p)\n            return P_h.numpy()\n\n        def cal_prob_4(logit):\n            a = logit\n            p_h = tf.sigmoid(a).numpy()\n            return p_h\n\n        for id_, cp_type, t, g, c in tqdm(test_iter):\n            id_ = id_.decode('utf8') #?\n            t = np.expand_dims(t, axis=0)\n            g = np.expand_dims(g, axis=0)\n            c = np.expand_dims(c, axis=0)\n\n            if self.conf['cv_type'] == CV_TYPE_TRAIN_VAL_SPLIT:\n                #_, _, result = self.model.layers[-1]([t, g, c]) #self.model.predict([t, g, c])\n                _, _, result = self.model.predict([t, g, c])\n                result = np.squeeze(result, axis=0)\n\n                if cp_type == 1:\n                    if self.conf['loss_type'] == LOSS_TYPE_MULTI_LABEL:\n                        result = cal_prob_4(result)\n                    elif self.conf['loss_type'] == LOSS_TYPE_ADDITIVE_ANGULAR_MARGIN:\n                        result = cal_prob_3(result)\n                    else:\n                        raise ValueError('loss type is not valid.')\n                else:\n                    result = np.zeros((len(result)))\n\n                for i, MoA in enumerate(result):\n                    MoAs[i].append(MoA)\n            elif self.conf['cv_type'] == CV_TYPE_K_FOLD:\n                # Conduct ensemble prediction.\n                result_list = []\n\n                for i in range(self.nn_arch['k_fold']):\n                    _, _, result = self.k_fold_models[i].predict([t, g, c])\n                    result = np.squeeze(result, axis=0)\n\n                    if cp_type == 1:\n                        if self.conf['loss_type'] == LOSS_TYPE_MULTI_LABEL:\n                            result = cal_prob_4(result)\n                        elif self.conf['loss_type'] == LOSS_TYPE_ADDITIVE_ANGULAR_MARGIN:\n                            result = cal_prob_3(result)\n                        else:\n                            raise ValueError('loss type is not valid.')\n                    else:\n                        result = np.zeros((len(result)))\n\n                    result_list.append(result)\n\n                result_mean = np.asarray(result_list).mean(axis=0)\n\n                for i, MoA in enumerate(result_mean):\n                    MoAs[i].append(MoA)\n            else:\n                raise ValueError('cv_type is not valid.')\n\n            sig_id_list.append(id_)\n\n        # Save the result.\n        result_dict = {'sig_id': sig_id_list}\n        for i, MoA_annot in enumerate(MoA_annots):\n            result_dict[MoA_annot] = MoAs[i]\n\n        submission_df = pd.DataFrame(result_dict)\n        submission_df.to_csv(self.OUTPUT_FILE_NAME, index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:57.130422Z","iopub.execute_input":"2021-08-19T04:36:57.130785Z","iopub.status.idle":"2021-08-19T04:36:57.470733Z","shell.execute_reply.started":"2021-08-19T04:36:57.130744Z","shell.execute_reply":"2021-08-19T04:36:57.469737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nseed = int(time.time())\n#seed = 1606208227\nprint(f'Seed:{seed}')\nnp.random.seed(seed)\ntf.random.set_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:57.472072Z","iopub.execute_input":"2021-08-19T04:36:57.472468Z","iopub.status.idle":"2021-08-19T04:36:57.484578Z","shell.execute_reply.started":"2021-08-19T04:36:57.472428Z","shell.execute_reply":"2021-08-19T04:36:57.483561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"%%writefile MoA_pred_conf.json\n{\n\t\"mode\": \"train\",\n\t\"raw_data_path\": \"/kaggle/input/lish-moa\",\n\t\"model_loading\": false,\n\t\"multi_gpu\": false,\n\t\"num_gpus\": 4,\n\t\"cv_type\": \"train_val_split\",\n\t\"dataset_type\": \"balanced\",\n\t\"val_ratio\": 0.1,\n\t\"loss_type\": \"additive_angular_margin\",\n\t\"data_aug\": false,\n\n\t\"hps\": {\n\t\t\"lr\": 0.001,\n\t\t\"beta_1\": 0.999,\n\t\t\"beta_2\": 0.999,\n\t\t\"decay\": 0.0,\n\t\t\"epochs\": 258,\n\t\t\"batch_size\": 512,\n\t\t\"reduce_lr_factor\": 0.96,\n\t\t\"ls\": 8.281e-5,\n\t\t\"loss_weights\": [1.0, 0.1, 100.0],\n    \"rep\": 1600,\n\t\t\"sn_t\": 1.6,\n\t\t\"m1\": 0.8081512,\n\t\t\"m2\": 0.011438734,\n    \"weight_decay\": 0.000858\n\t},\n\n\t\"nn_arch\": {\n\t\t\"k_fold\": 5,\n\t\t\"d_treatment_type\": 1,\n\t\t\"num_treatment_type\": 6,\n\t\t\"d_input_feature\": 8,\n        \"d_gene_exp\": 772,\n        \"d_cell_type\": 100,\n        \"d_gep_init\": 1024,\n\t\t\"d_cv_init\": 128,\n\t\t\"num_sc_ae\": 0,\n\t\t\"d_f_init\": 512,\n        \"num_moa_annotation\": 206,\n\t\t\"d_out\" : 772,\n\t\t\"dropout_rate\": 0.2,\n\t\t\"similarity_type\": \"diff_abs\",\n\t\t\"additive_margin\": 0.02,\n\t\t\"scale\": 1.0\n\t}\n}","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:57.486475Z","iopub.execute_input":"2021-08-19T04:36:57.487056Z","iopub.status.idle":"2021-08-19T04:36:57.494497Z","shell.execute_reply.started":"2021-08-19T04:36:57.487015Z","shell.execute_reply":"2021-08-19T04:36:57.493611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"MoA_pred_conf.json\", 'r') as f:\n    conf = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:57.496324Z","iopub.execute_input":"2021-08-19T04:36:57.49693Z","iopub.status.idle":"2021-08-19T04:36:57.504643Z","shell.execute_reply.started":"2021-08-19T04:36:57.496892Z","shell.execute_reply":"2021-08-19T04:36:57.50401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train.\nmodel = MoAPredictor(conf)\n\nts = time.time()\nhist = model.train()\nte = time.time()\n\nprint('Elasped time: {0:f}s'.format(te - ts))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:36:57.506489Z","iopub.execute_input":"2021-08-19T04:36:57.506785Z","iopub.status.idle":"2021-08-19T04:41:35.068438Z","shell.execute_reply.started":"2021-08-19T04:36:57.506759Z","shell.execute_reply":"2021-08-19T04:41:35.067498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MoA clustering analysis","metadata":{}},{"cell_type":"markdown","source":"#### Center analysis","metadata":{}},{"cell_type":"code","source":"moap = model.model.get_layer('moap')","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:41:35.069861Z","iopub.execute_input":"2021-08-19T04:41:35.070224Z","iopub.status.idle":"2021-08-19T04:41:35.07633Z","shell.execute_reply.started":"2021-08-19T04:41:35.070176Z","shell.execute_reply":"2021-08-19T04:41:35.075204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"W = moap.dense_4_4.weights[0]\nW.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:41:35.078Z","iopub.execute_input":"2021-08-19T04:41:35.078376Z","iopub.status.idle":"2021-08-19T04:41:35.087097Z","shell.execute_reply.started":"2021-08-19T04:41:35.078338Z","shell.execute_reply":"2021-08-19T04:41:35.086365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"W = W.numpy()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:41:35.088612Z","iopub.execute_input":"2021-08-19T04:41:35.089381Z","iopub.status.idle":"2021-08-19T04:41:35.098921Z","shell.execute_reply.started":"2021-08-19T04:41:35.089334Z","shell.execute_reply":"2021-08-19T04:41:35.098066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"W = W.T\nW.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:41:35.102434Z","iopub.execute_input":"2021-08-19T04:41:35.102754Z","iopub.status.idle":"2021-08-19T04:41:35.109856Z","shell.execute_reply.started":"2021-08-19T04:41:35.10268Z","shell.execute_reply":"2021-08-19T04:41:35.108779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nW_e = TSNE(n_components=2).fit_transform(W)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:41:35.111721Z","iopub.execute_input":"2021-08-19T04:41:35.112276Z","iopub.status.idle":"2021-08-19T04:41:36.422642Z","shell.execute_reply.started":"2021-08-19T04:41:35.112239Z","shell.execute_reply":"2021-08-19T04:41:36.421547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"W_e.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:41:36.424134Z","iopub.execute_input":"2021-08-19T04:41:36.424482Z","iopub.status.idle":"2021-08-19T04:41:36.431019Z","shell.execute_reply.started":"2021-08-19T04:41:36.424444Z","shell.execute_reply":"2021-08-19T04:41:36.429859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = np.arange(len(W_e))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:41:36.432763Z","iopub.execute_input":"2021-08-19T04:41:36.433366Z","iopub.status.idle":"2021-08-19T04:41:36.438669Z","shell.execute_reply.started":"2021-08-19T04:41:36.433314Z","shell.execute_reply":"2021-08-19T04:41:36.43786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure(figsize=(20, 20))\nscatter(W_e[:, 0], W_e[:, 1], c=colors, cmap=cm.prism, marker='^', s=100)\n#for i, c in enumerate(colors):\n#    annotate(str(c), (tsne_embed_features[i, 0], tsne_embed_features[i, 1]))\ngrid()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:41:36.440538Z","iopub.execute_input":"2021-08-19T04:41:36.440975Z","iopub.status.idle":"2021-08-19T04:41:36.68561Z","shell.execute_reply.started":"2021-08-19T04:41:36.440942Z","shell.execute_reply":"2021-08-19T04:41:36.68489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Clustering map for training and validation data","metadata":{}},{"cell_type":"code","source":"from matplotlib.patches import Rectangle\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\ndef plot_colortable(colors, title, sort_colors=True, emptycols=0):\n\n    cell_width = 212\n    cell_height = 22\n    swatch_width = 48\n    margin = 12\n    topmargin = 40\n\n    # Sort colors by hue, saturation, value and name.\n    if sort_colors is True:\n        by_hsv = sorted((tuple(mcolors.rgb_to_hsv(mcolors.to_rgb(color))),\n                         name)\n                        for name, color in colors.items())\n        names = [name for hsv, name in by_hsv]\n    else:\n        names = list(colors)\n\n    n = len(names)\n    ncols = 4 - emptycols\n    nrows = n // ncols + int(n % ncols > 0)\n\n    width = cell_width * 4 + 2 * margin\n    height = cell_height * nrows + margin + topmargin\n    dpi = 72\n\n    fig, ax = plt.subplots(figsize=(width / dpi, height / dpi), dpi=dpi)\n    fig.subplots_adjust(margin/width, margin/height,\n                        (width-margin)/width, (height-topmargin)/height)\n    ax.set_xlim(0, cell_width * 4)\n    ax.set_ylim(cell_height * (nrows-0.5), -cell_height/2.)\n    ax.yaxis.set_visible(False)\n    ax.xaxis.set_visible(False)\n    ax.set_axis_off()\n    ax.set_title(title, fontsize=24, loc=\"left\", pad=10)\n\n    for i, name in enumerate(names):\n        row = i % nrows\n        col = i // nrows\n        y = row * cell_height\n\n        swatch_start_x = cell_width * col\n        text_pos_x = cell_width * col + swatch_width + 7\n\n        ax.text(text_pos_x, y, name, fontsize=8,\n                horizontalalignment='left',\n                verticalalignment='center')\n\n        ax.add_patch(\n            Rectangle(xy=(swatch_start_x, y-9), width=swatch_width,\n                      height=18, facecolor=colors[name], edgecolor='0.7')\n        )\n\n    return fig","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:41:36.687193Z","iopub.execute_input":"2021-08-19T04:41:36.687554Z","iopub.status.idle":"2021-08-19T04:41:36.705202Z","shell.execute_reply.started":"2021-08-19T04:41:36.687514Z","shell.execute_reply":"2021-08-19T04:41:36.703977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(mcolors.XKCD_COLORS)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:41:36.706675Z","iopub.execute_input":"2021-08-19T04:41:36.707184Z","iopub.status.idle":"2021-08-19T04:41:36.719663Z","shell.execute_reply.started":"2021-08-19T04:41:36.707146Z","shell.execute_reply":"2021-08-19T04:41:36.718433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = mcolors.XKCD_COLORS\ncolor_items = colors.items()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:41:36.721528Z","iopub.execute_input":"2021-08-19T04:41:36.721974Z","iopub.status.idle":"2021-08-19T04:41:36.726772Z","shell.execute_reply.started":"2021-08-19T04:41:36.721935Z","shell.execute_reply":"2021-08-19T04:41:36.725848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_items = list(color_items)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:41:36.728634Z","iopub.execute_input":"2021-08-19T04:41:36.72911Z","iopub.status.idle":"2021-08-19T04:41:36.734338Z","shell.execute_reply.started":"2021-08-19T04:41:36.729072Z","shell.execute_reply":"2021-08-19T04:41:36.733369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cls_colors = [color_items[i][1] for i in range(0, len(color_items), 4)]\ncls_colors = cls_colors[:207]","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:41:36.736227Z","iopub.execute_input":"2021-08-19T04:41:36.736732Z","iopub.status.idle":"2021-08-19T04:41:36.742982Z","shell.execute_reply.started":"2021-08-19T04:41:36.736686Z","shell.execute_reply":"2021-08-19T04:41:36.742052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"moa_names = ['none'] + moa_names","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:41:36.744741Z","iopub.execute_input":"2021-08-19T04:41:36.745225Z","iopub.status.idle":"2021-08-19T04:41:36.750527Z","shell.execute_reply.started":"2021-08-19T04:41:36.745186Z","shell.execute_reply":"2021-08-19T04:41:36.749578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"moa_name_colors = dict((name, color) for name, color in zip(moa_names, cls_colors))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:41:36.75261Z","iopub.execute_input":"2021-08-19T04:41:36.753033Z","iopub.status.idle":"2021-08-19T04:41:36.760528Z","shell.execute_reply.started":"2021-08-19T04:41:36.752995Z","shell.execute_reply":"2021-08-19T04:41:36.759679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Clustering for training data","metadata":{}},{"cell_type":"code","source":"input_df = pd.read_csv(os.path.join(raw_data_path, 'train_features.csv'))\ninput_df.cp_type = input_df.cp_type.astype('category')\ninput_df.cp_type = input_df.cp_type.cat.rename_categories(range(len(input_df.cp_type.cat.categories)))\ninput_df.cp_time = input_df.cp_time.astype('category')\ninput_df.cp_time = input_df.cp_time.cat.rename_categories(range(len(input_df.cp_time.cat.categories)))\ninput_df.cp_dose = input_df.cp_dose.astype('category')\ninput_df.cp_dose = input_df.cp_dose.cat.rename_categories(range(len(input_df.cp_dose.cat.categories)))\n\n# Remove samples of ctl_vehicle.\nvalid_indexes = input_df.cp_type == 1\ninput_df = input_df[valid_indexes]\ninput_df = input_df.reset_index(drop=True)\n\ntarget_scored_df = pd.read_csv(os.path.join(raw_data_path, 'train_targets_scored.csv'))\ntarget_scored_df = target_scored_df[valid_indexes]\ntarget_scored_df = target_scored_df.reset_index(drop=True)\ndel target_scored_df['sig_id']\ntarget_scored_df.columns = range(len(target_scored_df.columns))\nn_target_samples = target_scored_df.sum().values\n\nif model.conf['data_aug']:\n    genes = [col for col in input_df.columns if col.startswith(\"g-\")]\n    cells = [col for col in input_df.columns if col.startswith(\"c-\")]\n\n    features = genes + cells\n    targets = [col for col in target_scored_df if col != 'sig_id']\n\n    aug_trains = []\n    aug_targets = []\n    for t in [0, 1, 2]:\n        for d in [0, 1]:\n            for _ in range(3):\n                train1 = input_df.loc[(input_df['cp_time'] == t) & (input_df['cp_dose'] == d)]\n                target1 = target_scored_df.loc[(input_df['cp_time'] == t) & (input_df['cp_dose'] == d)]\n                ctl1 = input_df.loc[(input_df['cp_time'] == t) & (input_df['cp_dose'] == d)].sample(\n                    train1.shape[0], replace=True)\n                ctl2 = input_df.loc[(input_df['cp_time'] == t) & (input_df['cp_dose'] == d)].sample(\n                    train1.shape[0], replace=True)\n                train1[genes + cells] = train1[genes + cells].values + ctl1[genes + cells].values - ctl2[\n                    genes + cells].values\n                aug_trains.append(train1)\n                aug_targets.append(target1)\n\n    input_df = pd.concat(aug_trains).reset_index(drop=True)\n    target_scored_df = pd.concat(aug_targets).reset_index(drop=True)\n\ng_feature_names = ['g-' + str(v) for v in range(model.nn_arch['d_gene_exp'])]\nc_feature_names = ['c-' + str(v) for v in range(model.nn_arch['d_cell_type'])]\nmoa_names = [v for v in range(model.nn_arch['num_moa_annotation'])]\n\n\ndef get_series_from_input(idxes):\n    idxes = idxes.numpy()\n    df = input_df.iloc[idxes]\n    \n    # Treatment.\n    cp_time = df['cp_time']\n    cp_dose = df['cp_dose']\n\n    treatment_type = cp_time * 2 + cp_dose\n\n    # Gene expression.\n    gene_exps = df[g_feature_names].values\n\n    # Cell viability.\n    cell_vs = df[c_feature_names].values\n\n    return treatment_type, gene_exps, cell_vs\n\n\ndef make_input_features(idx):\n    treatment_type, gene_exps, cell_vs = tf.py_function(get_series_from_input, inp=[idx], Tout=[tf.int32, tf.float64, tf.float64])\n    return treatment_type, gene_exps, cell_vs\n\n\nMoA_p_sets = []\ntrain_index = []\nval_index = []\nfor col in target_scored_df.columns:\n    s = target_scored_df.iloc[:, col]\n    s = s[s == 1]\n    s = list(s.index)\n    n_val = int(n_target_samples[col] * model.conf['val_ratio'])\n\n    if n_val != 0:\n        tr_set = s[:int(-1.0 * n_val)]\n        val_set = s[int(-1.0 * n_val):]\n        MoA_p_sets.append(tr_set)\n        train_index += tr_set\n        val_index += val_set\n    else:\n        MoA_p_sets.append(s)\n        train_index += s\n\n# Training dataset.\ntr_dataset = tf.data.Dataset.from_tensor_slices(train_index)\ntr_dataset = tr_dataset.map(make_input_features) #, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\ntr_iter = tr_dataset.as_numpy_iterator()\n\n# Validation dataset.\nval_dataset = tf.data.Dataset.from_tensor_slices(val_index)\nval_dataset = val_dataset.map(make_input_features) #, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\nval_iter = val_dataset.as_numpy_iterator()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:41:36.76251Z","iopub.execute_input":"2021-08-19T04:41:36.762791Z","iopub.status.idle":"2021-08-19T04:41:41.125536Z","shell.execute_reply.started":"2021-08-19T04:41:36.762756Z","shell.execute_reply":"2021-08-19T04:41:41.124641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_iter = tr_dataset.as_numpy_iterator()\nmoap = model.model.get_layer('moap')\nembed_feature_dicts = []\ntarget_df = target_scored_df.loc[train_index]\n\nfor i, d in tqdm(enumerate(tr_iter)):\n    t, g, c = d\n    t = np.expand_dims(t, axis=0)\n    g = np.expand_dims(g, axis=0)\n    c = np.expand_dims(c, axis=0)\n\n    # First layers.\n    t = moap.embed_treatment_type_0(t)\n    t = tf.reshape(t, (-1, model.nn_arch['d_input_feature']))\n    t = moap.dense_treatment_type_0(t)\n\n    t = moap.layer_normalization_0_1(t)\n    g = moap.layer_normalization_0_2(g)\n    c = moap.layer_normalization_0_3(c)\n\n    # Gene expression.\n    g_e = moap.encoder_gene_exp_1(g)\n    x_g = moap.decoder_gene_exp_1(g_e)\n    x_g = tf.expand_dims(x_g, axis=-1)\n    x_g = tf.squeeze(x_g, axis=-1)\n\n    # Cell type.\n    c_e = moap.encoder_cell_type_2(c)\n    x_c = moap.decoder_cell_type_2(c_e)\n    x_c = moap.dropout_2(x_c)\n\n    # Skip-connection autoencoder and final layers.\n    x = tf.concat([t, g_e, c_e], axis=-1)\n    for k in range(model.nn_arch['num_sc_ae']):\n        x = moap.sc_aes[k](x)\n\n    # Final layers.\n    x = moap.dense_4_1(x)\n    x = moap.dense_4_2(x)\n    x = moap.dense_4_3(x)\n\n    # Normalize x.\n    if conf['loss_type'] == LOSS_TYPE_MULTI_LABEL:\n        x1 = moap.dense_4_4(x)\n    elif conf['loss_type'] == LOSS_TYPE_ADDITIVE_ANGULAR_MARGIN:\n        x = x / tf.sqrt(tf.reduce_sum(tf.square(x), axis=1, keepdims=True))\n        x1 = moap.dense_4_4(x)\n    else:\n        raise ValueError('loss type is not valid.')\n    \n    # Get embed_feature_dict.\n    embed_feature_dict = {}\n    embed_feature_dict['sig_id'] = -1\n    embed_feature_dict['embed_feature'] = x.numpy().ravel()\n    series = target_df.iloc[i]\n    df = series[series == 1].to_frame()\n    embed_feature_dict['MoA_classes'] = list(df.index)\n    \n    embed_feature_dicts.append(embed_feature_dict)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:41:41.127059Z","iopub.execute_input":"2021-08-19T04:41:41.127404Z","iopub.status.idle":"2021-08-19T04:46:09.915386Z","shell.execute_reply.started":"2021-08-19T04:41:41.127368Z","shell.execute_reply":"2021-08-19T04:46:09.914404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_features = np.array([v['embed_feature'] for v in embed_feature_dicts])\ntsne_embed_features = TSNE(n_components=2).fit_transform(embed_features)\nclasses = [v['MoA_classes'] for v in embed_feature_dicts]\nfor i in tqdm(range(len(classes))):\n    if len(classes[i]) == 0:\n        classes[i] = [0]\ncolors = [v[0] for v in classes]","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:46:09.916796Z","iopub.execute_input":"2021-08-19T04:46:09.917172Z","iopub.status.idle":"2021-08-19T04:48:26.144611Z","shell.execute_reply.started":"2021-08-19T04:46:09.917133Z","shell.execute_reply":"2021-08-19T04:48:26.143697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure(figsize=(20, 20))\nx = []\ny = []\nc = []\n\nfor i in range(len(tsne_embed_features)):\n    for v in classes[i]:\n        x.append(tsne_embed_features[i, 0])\n        y.append(tsne_embed_features[i, 1])\n        c.append(cls_colors[v])\n\nscatter(x, y, c=c, alpha=1.0)\ntitle('Drug features clustering for training data')\ngrid()\nmoa_name_colors_fig = plot_colortable(moa_name_colors, \"MoA Colors\")\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:48:26.14587Z","iopub.execute_input":"2021-08-19T04:48:26.146352Z","iopub.status.idle":"2021-08-19T04:48:28.24348Z","shell.execute_reply.started":"2021-08-19T04:48:26.146315Z","shell.execute_reply":"2021-08-19T04:48:28.242552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Clustering for validation data","metadata":{}},{"cell_type":"code","source":"val_iter = val_dataset.as_numpy_iterator()\nmoap = model.model.get_layer('moap')\nembed_feature_dicts = []\ntarget_df = target_scored_df.loc[val_index]\n\nfor i, d in tqdm(enumerate(val_iter)):\n    t, g, c = d\n    id_ = input_df['sig_id'].iloc[i]\n    t = np.expand_dims(t, axis=0)\n    g = np.expand_dims(g, axis=0)\n    c = np.expand_dims(c, axis=0)\n\n    # First layers.\n    t = moap.embed_treatment_type_0(t)\n    t = tf.reshape(t, (-1, model.nn_arch['d_input_feature']))\n    t = moap.dense_treatment_type_0(t)\n\n    t = moap.layer_normalization_0_1(t)\n    g = moap.layer_normalization_0_2(g)\n    c = moap.layer_normalization_0_3(c)\n\n    # Gene expression.\n    g_e = moap.encoder_gene_exp_1(g)\n    x_g = moap.decoder_gene_exp_1(g_e)\n    x_g = tf.expand_dims(x_g, axis=-1)\n    x_g = tf.squeeze(x_g, axis=-1)\n\n    # Cell type.\n    c_e = moap.encoder_cell_type_2(c)\n    x_c = moap.decoder_cell_type_2(c_e)\n    x_c = moap.dropout_2(x_c)\n\n    # Skip-connection autoencoder and final layers.\n    x = tf.concat([t, g_e, c_e], axis=-1)\n    for i in range(model.nn_arch['num_sc_ae']):\n        x = moap.sc_aes[i](x)\n\n    # Final layers.\n    x = moap.dense_4_1(x)\n    x = moap.dense_4_2(x)\n    x = moap.dense_4_3(x)\n\n    # Normalize x.\n    if conf['loss_type'] == LOSS_TYPE_MULTI_LABEL:\n        x1 = moap.dense_4_4(x)\n    elif conf['loss_type'] == LOSS_TYPE_ADDITIVE_ANGULAR_MARGIN:\n        x = x / tf.sqrt(tf.reduce_sum(tf.square(x), axis=1, keepdims=True))\n        x1 = moap.dense_4_4(x)\n    else:\n        raise ValueError('loss type is not valid.')\n    \n    # Get embed_feature_dict.\n    embed_feature_dict = {}\n    embed_feature_dict['sig_id'] = id_\n    embed_feature_dict['embed_feature'] = x.numpy().ravel()\n    series = target_df.iloc[i]\n    df = series[series == 1].to_frame()\n    embed_feature_dict['MoA_classes'] = list(df.index)\n    \n    embed_feature_dicts.append(embed_feature_dict)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:48:28.244895Z","iopub.execute_input":"2021-08-19T04:48:28.245404Z","iopub.status.idle":"2021-08-19T04:48:56.686436Z","shell.execute_reply.started":"2021-08-19T04:48:28.245363Z","shell.execute_reply":"2021-08-19T04:48:56.684994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_features = np.array([v['embed_feature'] for v in embed_feature_dicts])\ntsne_embed_features = TSNE(n_components=2).fit_transform(embed_features)\nclasses = [v['MoA_classes'] for v in embed_feature_dicts]\nfor i in tqdm(range(len(classes))):\n    if len(classes[i]) == 0:\n        classes[i] = [0]\ncolors = [v[0] for v in classes]","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:48:56.689282Z","iopub.execute_input":"2021-08-19T04:48:56.689544Z","iopub.status.idle":"2021-08-19T04:49:07.955901Z","shell.execute_reply.started":"2021-08-19T04:48:56.689518Z","shell.execute_reply":"2021-08-19T04:49:07.954917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure(figsize=(20, 20))\n\nx = []\ny = []\nc = []\n\nfor i in range(len(tsne_embed_features)):\n    for v in classes[i]:\n        x.append(tsne_embed_features[i, 0])\n        y.append(tsne_embed_features[i, 1])\n        c.append(cls_colors[v])\n      \nscatter(x, y, c=c, alpha=1.0)\ntitle('Drug features clustering for validation data')\ngrid()\n\nmoa_name_colors_fig = plot_colortable(moa_name_colors, \"MoA Colors\")","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:49:07.957481Z","iopub.execute_input":"2021-08-19T04:49:07.959421Z","iopub.status.idle":"2021-08-19T04:49:09.574584Z","shell.execute_reply.started":"2021-08-19T04:49:07.959373Z","shell.execute_reply":"2021-08-19T04:49:09.573486Z"},"trusted":true},"execution_count":null,"outputs":[]}]}