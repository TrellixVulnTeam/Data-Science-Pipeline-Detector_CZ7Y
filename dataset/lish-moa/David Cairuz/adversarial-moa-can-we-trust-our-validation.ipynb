{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Adversarial Validation on MoA Dataset\n\nIn this notebook, we'll take a look at how similar the train and test dataset are. The way we're going to do this is by using a technique called [Adversarial Validation](http://fastml.com/adversarial-validation-part-one/).\n\nWe're going to assign new labels to the data, TRAIN (1) or TEST (0), then we'll train a classifier that will try to predict if an example comes from the train or test dataset. We hope the classifier perform no better than random - this would correspond to ROC AUC of 0.5, as said by Zygmunt in the post linked above.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport lightgbm as lgb\nfrom sklearn import model_selection","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading the datasets\n\nLet's start by reading the data, and assigning the new label and concatenating it so it becomes one dataset.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ntest_features = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features['TARGET'] = 1\ntest_features['TARGET'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train_features, test_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Processing the new dataset\n\nBefore we continue, take a look at this:\n\n> cp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs\n\nSince control perturbations (8% of our dataset) have no MoAs - and we will not be predicting for them - let's remove them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cp_type'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[data['cp_type'] == 'trt_cp'].copy()\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's also remove the sig_id and cp_type features. The TARGET will be removed further up the notebook.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['sig_id', 'cp_type'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's change the data type of the cp_dose to 'category' so our model knows how to interpret it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cp_dose'] = data['cp_dose'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and test split\n\nNow let's separate this new dataset into train and test. Don't forget to shuffle!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_data = data.drop(['TARGET'], axis=1)\ny_data = data['TARGET']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = model_selection.train_test_split(X_data, y_data, train_size=0.33, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating the model\n\nLet's create our classifier and see how it performs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = lgb.Dataset(X_train, label=y_train)\ntest = lgb.Dataset(X_test, label=y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'binary',\n         'max_depth': 5,\n         'learning_rate': 0.2,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 44,\n         \"metric\": 'auc',\n         \"verbosity\": -1}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_round = 50\nclf = lgb.train(param, train, num_round, valid_sets = [train, test], verbose_eval=50, early_stopping_rounds = 50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great, we're fine! The validation's AUC was close to 0.50. Our model was not able to distinguish train from test, so we can expect a good validation in this competition. :)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}