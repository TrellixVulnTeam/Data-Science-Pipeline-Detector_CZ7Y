{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install keras_tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom keras.layers import Input, Dense, Lambda, Layer, Activation\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras import metrics, optimizers\nfrom keras.callbacks import Callback\nimport keras\n\nimport pydot\nimport graphviz\nfrom keras.utils import plot_model\nfrom keras_tqdm import TQDMNotebookCallback\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nimport tensorflow as tf\ntf.compat.v1.disable_eager_execution()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install /kaggle/input/iterativestratification/iterative-stratification-master/\n!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\nimport pickle\nimport sys\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nfrom pytorch_tabnet.metrics import Metric\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import PCA\nfrom imblearn.over_sampling import SMOTENC\n\nimport keras\nfrom keras.layers import Lambda, Input, Dense\nfrom keras.models import Model\nfrom keras.datasets import mnist\nfrom keras.losses import mse, binary_crossentropy\nfrom keras.utils import plot_model\nfrom keras import backend as K\nimport tensorflow as tf\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom torch.autograd import Variable\n\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\nfrom hyperopt.pyll import scope as ho_scope\nfrom hyperopt.pyll.stochastic import sample as ho_sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_FEATURES = pd.read_csv('../input/lish-moa/train_features.csv')\nTRAIN_TARGETS_SCORED = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\nTRAIN_TARGETS_NONSCORED = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\nTEST_FEATURES = pd.read_csv('../input/lish-moa/test_features.csv')\nSAMPLE_SUBMISSION = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_FEATURES.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_FEATURES.drop(columns=['sig_id'], inplace=True)\nTRAIN_TARGETS_SCORED.drop(columns=['sig_id'], inplace=True)\nTEST_FEATURES.drop(columns=['sig_id'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in TRAIN_FEATURES.columns if col.startswith('g-')]\nCELLS = [col for col in TRAIN_FEATURES.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_std = {}\ntraining_features = TRAIN_FEATURES.columns.tolist()\nfor column in training_features:\n    #Skip Categorical Columns\n    if column == 'sig_id' or column == 'cp_type' or column == 'cp_dose':\n        print('Skip categorical column: ', column)\n        continue\n    (mu, sigma) = TRAIN_FEATURES[column].mean(), TRAIN_FEATURES[column].std()\n    TRAIN_FEATURES[column] = (TRAIN_FEATURES[column]-mu) / sigma\n    TEST_FEATURES[column] = (TEST_FEATURES[column] - mu) / sigma\n    mean_std[column] = (mu, sigma)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_vehicle = False\n\nif remove_vehicle:\n    kept_index = TRAIN_FEATURES['cp_type']=='trt_cp'\n    TRAIN_FEATURES = TRAIN_FEATURES.loc[kept_index].reset_index(drop=True)\n    train_targets_scored = train_targets_scored.loc[kept_index].reset_index(drop=True)\n\nTRAIN_FEATURES[\"cp_type\"] = (TRAIN_FEATURES[\"cp_type\"]==\"trt_cp\") + 0\nTRAIN_FEATURES[\"cp_dose\"] = (TRAIN_FEATURES[\"cp_dose\"]==\"D1\") + 0\n\nTEST_FEATURES[\"cp_type\"] = (TEST_FEATURES[\"cp_type\"]==\"trt_cp\") + 0\nTEST_FEATURES[\"cp_dose\"] = (TEST_FEATURES[\"cp_dose\"]==\"D1\") + 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sampling(args):\n    z_mean, z_log_var = args\n    epsilon = K.random_normal(shape=tf.shape(z_mean), mean=0., stddev=epsilon_std)\n    z = z_mean + K.exp(z_log_var / 2) * epsilon\n    return z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomVariationalLayer(Layer):\n    \"\"\"\n    Define a custom layer that learns and performs the training\n    This function is borrowed from:\n    https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder.py\n    \"\"\"\n    def __init__(self, **kwargs):\n        self.is_placeholder = True\n        super(CustomVariationalLayer, self).__init__(**kwargs)\n\n    def vae_loss(self, x_input, x_decoded, z_log_var_encoded, z_mean_encoded):\n        reconstruction_loss = original_dim * metrics.binary_crossentropy(x_input, x_decoded)\n        kl_loss = - 0.5 * K.sum(1 + z_log_var_encoded - K.square(z_mean_encoded) - \n                                K.exp(z_log_var_encoded), axis=-1)\n        return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss))\n\n    def call(self, inputs):\n        x = inputs[0]\n        x_decoded = inputs[1]\n        z_log_var_encoded = inputs[2]\n        z_mean_encoded = inputs[3]\n        loss = self.vae_loss(x, x_decoded, z_log_var_encoded, z_mean_encoded)\n        self.add_loss(loss, inputs=inputs)\n        # We won't actually use the output.\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class WarmUpCallback(Callback):\n    def __init__(self, beta, kappa):\n        self.beta = beta\n        self.kappa = kappa\n    # Behavior on each epoch\n    def on_epoch_end(self, epoch, logs={}):\n        if K.get_value(self.beta) <= 1:\n            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_dense_layer(x, activation, num_neurons):\n    x = Dense(num_neurons, kernel_initializer='glorot_uniform')(x)\n    x = BatchNormalization()(x)\n    x = Activation(activation)(x)\n    return x\n\ndef VAE(input_size, num_layers_encoder_mean, neurons_encoder_mean, activation_encoder_mean, num_layers_encoder_variance, neurons_encoder_variance, activation_encoder_variance, num_layers_decoder, neurons_decoder, activation_decoder, latent_dim, x_train, y_train, x_valid, y_valid):\n    \n    input_vae = Input(shape=(input_size, ))\n    \n    for i in range(num_layers_encoder_mean - 1):\n        if i == 0:\n            x = custom_dense_layer(input_vae, activation_encoder_mean, neurons_encoder_mean)\n        else:\n            x = custom_dense_layer(x, activation_encoder_mean, neurons_encoder_mean)\n    \n    if num_layers_encoder_mean == 1:\n        x = custom_dense_layer(input_vae, activation_encoder_mean, latent_dim)\n    else:\n        x = custom_dense_layer(x, activation_encoder_mean, latent_dim)\n    \n    for i in range(num_layers_encoder_variance - 1):\n        if i == 0:\n            y = custom_dense_layer(input_vae, activation_encoder_variance, neurons_encoder_variance)\n        else:\n            y = custom_dense_layer(y, activation_encoder_variance, neurons_encoder_variance)\n    \n    if num_layers_encoder_variance == 1:\n        y = custom_dense_layer(input_vae, activation_encoder_variance, latent_dim)\n    else:\n        y = custom_dense_layer(x, activation_encoder_variance, latent_dim)\n            \n    output_encoder = Lambda(sampling, output_shape=(latent_dim, ))([x, y])\n    \n    encoder = Model(input_vae, output_encoder, name='encoder')\n    \n    for i in range(num_layers_decoder - 1):\n        if i == 0:\n            z = custom_dense_layer(output_encoder, activation_decoder, neurons_decoder)\n        else:\n            z = custom_dense_layer(z, activation_decoder, neurons_decoder)\n            \n    decoder_to_reconstruct = Dense(input_size, kernel_initializer='glorot_uniform', activation='sigmoid')\n    \n    if num_layers_decoder == 1:\n        output_decoder = decoder_to_reconstruct(output_encoder)\n    else:\n        output_decoder = decoder_to_reconstruct(z)\n    \n    adam = optimizers.Adam()\n    vae_layer = CustomVariationalLayer()([input_vae, output_decoder, y, x])\n    vae = Model(input_vae, vae_layer)\n    vae.compile(optimizer=adam, loss=None, loss_weights=[beta])\n    \n    hist = vae.fit(np.array(x_train),\n                   shuffle=True,\n                   epochs=200,\n                   verbose=0,\n                   batch_size=50,\n                   validation_data=(np.array(x_valid), None),\n                   callbacks=[WarmUpCallback(beta, kappa),\n                              TQDMNotebookCallback(leave_inner=True, leave_outer=True), \n                              tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                                min_delta=0, patience=5, verbose=0, mode='min',\n                                baseline=None, restore_best_weights=True)])\n    \n    return vae, encoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epsilon_std = 1.0\nbeta = K.variable(0)\nkappa = 1\noriginal_dim = 772","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_EPOCH=500\ndef model_params(n_d=24, n_a=24, n_steps=1, gamma=1.3, lambda_sparse=0):\n    tabnet_params = dict(n_d=n_d, n_a=n_a, n_steps=n_steps, gamma=gamma,\n                     lambda_sparse=lambda_sparse, optimizer_fn=torch.optim.Adam,\n                     optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n                     mask_type='entmax',\n                     scheduler_params=dict(mode=\"min\",\n                                           patience=5,\n                                           min_lr=1e-5,\n                                           factor=0.9,),\n                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                     verbose=10,\n                     )\n    return tabnet_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LogitsLogLoss(Metric):\n    \"\"\"\n    LogLoss with sigmoid applied\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        \"\"\"\n        Compute LogLoss of predictions.\n\n        Parameters\n        ----------\n        y_true: np.ndarray\n            Target matrix or vector\n        y_score: np.ndarray\n            Score matrix or vector\n\n        Returns\n        -------\n            float\n            LogLoss of predictions vs targets.\n        \"\"\"\n        logits = 1 / (1 + np.exp(-y_pred))\n        aux = (1-y_true)*np.log(1-logits+1e-15) + y_true*np.log(logits+1e-15)\n        return np.mean(-aux)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_train(tabnet_params, TRAIN_FEATURES, TRAIN_TARGETS_SCORED, NB_SPLITS=2, optimization=True):\n    mskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, random_state=0, shuffle=True)\n    oof_preds = []\n    oof_targets = []\n    scores = []\n    scores_auc = []\n    for fold_nb, (train_idx, val_idx) in enumerate(mskf.split(TRAIN_FEATURES, TRAIN_TARGETS_SCORED)):\n        print(\"FOLDS : \", fold_nb)\n        ## model\n        X_train, y_train = TRAIN_FEATURES.values[train_idx, :], TRAIN_TARGETS_SCORED.values[train_idx, :]\n        X_val, y_val = TRAIN_FEATURES.values[val_idx, :], TRAIN_TARGETS_SCORED.values[val_idx, :]\n        model = TabNetRegressor(**tabnet_params)\n        \n        model.fit(X_train=X_train,\n                    y_train=y_train,\n                    eval_set=[(X_val, y_val)],\n                    eval_name = [\"val\"],\n                    eval_metric = [\"logits_ll\"],\n                    max_epochs=MAX_EPOCH,\n                    patience=10, batch_size=1024, virtual_batch_size=128,\n                    num_workers=0, drop_last=False,\n                    loss_fn=torch.nn.functional.binary_cross_entropy_with_logits);\n\n        preds_val = model.predict(X_val)\n        # Apply sigmoid to the predictions\n        preds =  1 / (1 + np.exp(-preds_val))\n        score = np.min(model.history[\"val_logits_ll\"])\n        oof_preds.append(preds_val)\n        oof_targets.append(y_val)\n        scores.append(score)\n        if optimization:\n            return scores\n        return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"space = [hp.quniform('num_layers_encoder_mean', 1, 3, 1), \n    hp.quniform('neurons_encoder_mean', 256, 2048, 32), \n    hp.choice('activation_encoder_mean', ('relu', 'selu', 'sigmoid', 'tanh', 'elu')), \n    hp.quniform('num_layers_encoder_variance', 1, 3, 1), \n    hp.quniform('neurons_encoder_variance', 256, 2048, 32), \n    hp.choice('activation_encoder_variance', ('relu', 'selu', 'sigmoid', 'tanh', 'elu')), \n    hp.quniform('num_layers_decoder', 1, 3, 1),\n    hp.quniform('neurons_decoder', 256, 2048, 32), \n    hp.choice('activation_decoder', ('relu', 'selu', 'sigmoid', 'tanh', 'elu')),\n    hp.quniform('latent_dim', 400, 770, 10),\n    hp.quniform('n_d', 8, 64, 2), \n    hp.quniform('n_steps', 1, 10, 1), \n    hp.uniform('gamma', 1, 2), \n    hp.uniform('lambda_sparse', 0, 0.01), \n    hp.quniform('n_components', 40, 100, 5),\n    hp.choice('optimization', ['True']),  \n    hp.choice('TRAINING_FEATURES', [TRAIN_FEATURES]), \n    hp.choice('TRAIN_TARGETS_SCORED', [TRAIN_TARGETS_SCORED])\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def final_model_creator(args):\n    \n    num_layers_encoder_mean, neurons_encoder_mean, activation_encoder_mean, num_layers_encoder_variance, neurons_encoder_variance, activation_encoder_variance, num_layers_decoder, neurons_decoder, activation_decoder, latent_dim, n_d, n_steps, gamma, lambda_sparse, n_components, optimization, TRAIN_FEATURES, TRAIN_TARGETS_SCORED = args\n    \n    GENES = [col for col in TRAIN_FEATURES.columns if col.startswith('g-')]\n    CELL = [col for col in TRAIN_FEATURES.columns if col.startswith('c-')]\n    \n    # Build Gene Encoder\n    TRAIN_GENE = TRAIN_FEATURES[GENES]\n    X_train, X_test, y_train, y_test = train_test_split(TRAIN_GENE, TRAIN_TARGETS_SCORED, \n                                                    test_size=0.1, random_state=42)\n    gene_vae, gene_encoder = VAE(772, int(num_layers_encoder_mean), int(neurons_encoder_mean), \n                                 activation_encoder_mean, int(num_layers_encoder_variance), int(neurons_encoder_variance), \n                                 activation_encoder_variance, int(num_layers_decoder), int(neurons_decoder), \n                                 activation_decoder, int(latent_dim), X_train, y_train, X_test, y_test)\n    \n    # Do PCA on Cell Viability\n    TRAIN_CELL = TRAIN_FEATURES[CELL]\n    pca = PCA(n_components=int(n_components))\n    \n    # Compressing Table\n    TRAIN_GENE = pd.DataFrame(gene_encoder.predict(TRAIN_GENE))\n    TRAIN_CELL = pca.fit_transform(TRAIN_CELL)\n    TRAIN_CELL = pd.DataFrame(data=TRAIN_CELL)\n    \n    #Column names\n    GENES = GENES[:int(latent_dim)]\n    CELL = CELL[:int(n_components)]\n    REST = TRAIN_FEATURES.iloc[:, :3].columns\n    \n    # Compressing to one table\n    TRAIN_FEATURES = pd.concat([TRAIN_FEATURES.iloc[:, :3], TRAIN_GENE, TRAIN_CELL], axis=1)\n    TRAIN_FEATURES.columns = list(REST) + GENES + CELL\n    \n    #Applying RankGauss for normailization\n    for col in (GENES + CELL):\n\n        transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n        vec_len = len(TRAIN_FEATURES[col].values)\n        vec_len_test = len(TEST_FEATURES[col].values)\n        raw_vec = TRAIN_FEATURES[col].values.reshape(vec_len, 1)\n        transformer.fit(raw_vec)\n\n        TRAIN_FEATURES[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    \n    # Building, compiling and training the model\n    tabnet_params = model_params(n_d=int(n_d), n_a=int(n_d), n_steps=int(n_steps), gamma=gamma, lambda_sparse=lambda_sparse)\n    sc_model = model_train(tabnet_params, NB_SPLITS=10, optimization=optimization, TRAIN_FEATURES=TRAIN_FEATURES, TRAIN_TARGETS_SCORED=TRAIN_TARGETS_SCORED)\n    \n    # return score\n    if optimization:\n        return sc_model[0]\n    return gene_encoder, sc_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#trials = Trials()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%cd '../input/moa-prediction-ba'\ntrials = pickle.load(open(\"trial_150.hyperopt\", \"rb\"))\n%cd ..\n%cd ..\n%cd working","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evals = 150\nwhile evals < 170:\n    best = fmin(fn=final_model_creator, space=space, algo=tpe.suggest, max_evals=evals+10, trials=trials)\n    with open('trial_{}'.format(str(evals+10)) + \".hyperopt\", \"wb\") as f:\n        pickle.dump(trials, f)\n    evals += 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}