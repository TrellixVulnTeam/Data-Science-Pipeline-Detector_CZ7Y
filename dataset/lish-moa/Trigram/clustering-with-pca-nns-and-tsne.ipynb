{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center>Clustering with PCA, NNs and tSNE</center></h1>\n    \n<hr>"},{"metadata":{},"cell_type":"markdown","source":"In this notebook I'm going to provide a demonstration of **how to appropriately cluster** using dimensionality reduction techniques such as PCA and tSNE. For more helpful resources check out **[this wonderful kernel by Tilii](https://www.kaggle.com/tilii7/dimensionality-reduction-pca-tsne?rvi=1)** and also be sure to check the documentation for some more in-depth explanations of the dimensionality reduction techniques."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd\nimport warnings; warnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now however, you might be wondering about how I selected the `y` variable in the code that follows. It is simply **the most balanced label in the whole data (unless I missed something glaringly obvious)**. The competition data in itself works particularly well with neural networks (and we have a nice neural network surprise at the end.)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/lish-moa/train_features.csv')\ntargs = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntest = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ny = targs[targs.columns[55]].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So now we define the clustering: we fit a PCA on the training and test data and a tSNE to the training data. tSNE takes a much longer time than PCA, so expect to wait a fair bit (however contrarily in most instances tSNE is more trustworthy than a PCA when dealing with dangerous data)."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_ = PCA(n_components=2)\npca = pca_.fit_transform(train.drop([\"sig_id\", 'cp_type', 'cp_time', 'cp_dose'], axis=1))\npca_t = pca_.fit_transform(test.drop([\"sig_id\"], axis=1))\ntsne_ = TSNE(n_components=2)\ntsne = tsne_.fit_transform(train.drop([\"sig_id\", 'cp_type', 'cp_time', 'cp_dose'], axis=1))\nprint('Explained variance for PCA', pca_.explained_variance_ratio_.sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we take the plunge and plot the output of our tSNE plot, it looks like there's only one principal cluster and everything else's grouped into a lot of other, smaller clusters. You can see very few reds in the plot, exacerbating the imbalanced classes (side effect of dimensionality reduction?)."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10, 10));colors=['green', 'red']\nplt.axis('off')\nfor color, i, ax, option in zip(colors, [0, 1], [121, 122], [pca, tsne]):\n    plt.scatter(tsne[y == i, 0], tsne[y == i, 1], color=color, s=1,\n                alpha=.8, marker='.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now however the PCA transformed data is completely, and by far much more differently clustered - almost all the reds are located in one place with several greens, which means we've not fully sequestered the reds from the greens in the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(20, 10));colors=['green', 'red']\nfor color, i, ax, option in zip(colors, [0, 1], [121, 122], [pca, pca_t]):\n    axs[0].scatter(pca[y == i, 0], pca[y == i, 1], color=color, s=1,\n                alpha=.8, marker='.')\n    axs[1].scatter(pca_t[y == i, 0], pca_t[y == i, 1], color=color, s=1,\n                alpha=.8, marker='.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are now done with traditional clustering and are currently moving on **to checking the intermediate activations of a neural network.** (*NOTE: There are are only a few predictions so the output might not be as expected*). This might not necessarily produce a better result by any means over the PCA and tSNE."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create():\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(2),\n    tf.keras.layers.Dense(128),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Activation(\"relu\"),\n    tf.keras.layers.Dense(512),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(400),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(206, activation=\"sigmoid\")\n    ])\n    model.compile(optimizer=tf.optimizers.Adam(),\n                  loss='binary_crossentropy', \n                  )\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model has been taken from the popular kernel [keras Multilabel Neural Network](https://www.kaggle.com/simakov/keras-multilabel-neural-network-v1-2) and will be utilized to plot and cluster the output of this neural network.."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create()\nmodel.fit(tsne, targs.drop([\"sig_id\"], axis=1).values.astype(float), epochs=8, verbose=False)\npreds = model.predict(pca_t)\nfig = plt.figure(figsize=(9, 9));colors=['green', 'red']\nfor color, i, ax, option in zip(colors, [0, 1], [121, 122], [pca, tsne]):\n    plt.scatter(preds[y == i, 0], preds[y == i, 1], color=color, s=1,\n                alpha=.8, marker='.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's check the output clusters - there are very, very few red points which means our model still has a long, long way to go in training. The model itself is pitifully small, so I have full confidence that the *ideal* way to plot the output activations would be to use a much larger network (perhaps LSTMs/GRUs would do the trick?). \n\nAnyways, thank you for reading this kernel, and if you like it an upvote would be much appreciated. This is a demonstration of clusters in the data and where we can go from here - so please take away something from this as well and potentially improve on my work."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}