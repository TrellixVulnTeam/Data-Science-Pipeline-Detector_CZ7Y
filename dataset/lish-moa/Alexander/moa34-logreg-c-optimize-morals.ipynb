{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Morals:\n\nFor Logistic Regression:\n\nOptimal C depends on cv chosen (i.e. depends on random seed)\n\nIt might vary about 2 times e.g. from 0.0014 to  0.0024\n\nHowever if determine C by one seed, and calculate score for that C in the other seed , the difference with the optimal seed for that score would be quite low - about 2*4-th digit of the logloss\n\nCV score varies in 3-th digit e.g. from  -0.0881  to  -0.0871 depending in random seed\n\n\nHere it is demonstrated on one target: \"cyclooxygenase_inhibitor\" (435 times contains 1, so class disbalance is less than for most other targets), but seems similar for many of them. Note that the target is not good predictable - constact prediction gives logloss: 0.0912, while models around 0.087-0.088. It might be for other targets situation is slighly less problematic. \n\n\nchanging C 1 percent changes score in 7-th digit\n\n\nFurther results in: \nhttps://www.kaggle.com/alexandervc/moa30b-cyclooxygenase-inhibitor-c-interv\nIn particular seeds used in the present notebook -are delibrately taken to show the big difference.\nAnalysis on other sees can be found in cited notebook. \n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport time\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport time\ndf = pd.read_csv('/kaggle/input/lish-moa/train_features.csv',index_col = 0)  \ndf0 = df.copy()\ndf['cp_type'] = df['cp_type'].map({'trt_cp':1.0, 'ctl_vehicle':1.0}) # Forget about control group  \ndf['cp_dose'] = df['cp_dose'].map({'D1':0.0, 'D2':1.0})\ndf['cp_time'] = df['cp_time'].map({24:0.0, 48: .5 , 72:1.0})\nX = df.copy()\nX_save = X.copy()\ndf_test = pd.read_csv('/kaggle/input/lish-moa/test_features.csv',index_col = 0)\ndf0_test = df_test.copy()\ndf_test['cp_type'] = df_test['cp_type'].map({'trt_cp':1.0, 'ctl_vehicle':0.0})\ndf_test['cp_dose'] = df_test['cp_dose'].map({'D1':0.0, 'D2':1.0})\ndf_test['cp_time'] = df_test['cp_time'].map({24:0.0, 48: .5 , 72:1.0})\n\ny = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv',index_col = 0 )\ny_save = y.copy()\nprint(y.iloc[:3,:2])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = y_save['dopamine_receptor_antagonist']\ny = y_save['cyclooxygenase_inhibitor']\n\ny.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LogLoss by constand prediction:\n-log_loss(y, np.ones_like(y)*y.mean() )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Lasso\n\nverbose = 1\ndf_stat = pd.DataFrame()\nX_holdout = None\ny_holdout = None\n\nt00 = time.time()\n\nfor rs in [0,1999]:#,1,5, 100, 200, 113, 997, 57 , 1999, 10000]:\n    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state= rs )\n    \n    for C in 0.002*np.arange(0.3,3,0.1):\n        model = LogisticRegression(C=C)\n        score_cv = cross_val_score(model, X,y, cv = skf , scoring = 'neg_log_loss')\n        print(np.round(C,10),np.mean(score_cv), score_cv, time.time() - t00, 'seconds passed' )\n        df_stat.loc[C,'CV Score RS'+str(rs)] = np.mean( score_cv )\n        \nprint(time.time() - t00, 'seconds passed' )    \n\nplt.figure(figsize = (20,6) )\nfor c in df_stat.columns:\n    plt.plot(df_stat[c],'*-' ) # df_stat['CV Score']\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l = []\nfor f in df_stat.columns:\n    a = df_stat[f].argmax()\n    c = df_stat.index[a]\n    l.append(c)\nnp.min(l), np.max(l) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show difference between worst and best loglosses \nnp.round(1e3* (df_stat.describe().loc['max',:].max() - df_stat.describe().loc['max',:].min()) , 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stat.describe().loc['max',:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stat.max()-df_stat[f]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate how much score suffes if we take optimal C for one seed and use it for another.\n\n# Conclusion: suffers quite low 1.8*4-th digit \n\nl = []\nfor f in df_stat.columns:\n    a = df_stat[f].argmax()\n    c = df_stat.index[a]\n    print(f, a, c )\n\ne1 = np.round( 1e4* (df_stat['CV Score RS1999'].max() -df_stat['CV Score RS1999'].iloc[4]) , 1)\ne2 = np.round( 1e4* (df_stat['CV Score RS0'].max() -df_stat['CV Score RS0'].iloc[9]) , 1)\ne1,e2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}