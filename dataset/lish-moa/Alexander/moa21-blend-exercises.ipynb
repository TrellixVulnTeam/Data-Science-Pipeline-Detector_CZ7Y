{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What is about \n\nSome examples to wrap up your mind how blend is working - may be useful for pedagogical reasons.\n\nDiffrent fold splitings are consiered and cv-predictions are averaged (blended) - in most cases we see improvement of scores. \nPlots showing dependence on number of blended predictions are presented.\n(Typically we see some saturation of quality after 5-10-20 blends). \nAlso we consider several variants with different fold numbers. \n\nIn different versions of the notebook we take different targets from MoA competition and examine them. \n\n\nV5 hsp_inhibitor - Logreg - results slighly contradictory - for 2 - not mononotic with respect to blend number ,  \n3-folds blending seems to be better than 5 and 10 fold blending\n\n\nV4 raf_inhibitor - LogReg - blend on 3-folds is better than on 2-folds, 5-folds a little worse than 3-fold, 10-fold is best, but convergence is slower\n\n\nV3 some examples with logreg on target 'dopamine_receptor_antagonist' which is very little predictable. \nWe see that blend can work very good on such target - killing noise and going to constant\n\nV2 Blend up to 100 iterations  (model - LGB)\n\nV1 First experiments - see that up to 5 iterations blend improves  (model - LGB)\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\n\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport time\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport time\ndf = pd.read_csv('/kaggle/input/lish-moa/train_features.csv',index_col = 0)  \ndf0 = df.copy()\ndf['cp_type'] = df['cp_type'].map({'trt_cp':1.0, 'ctl_vehicle':1.0}) # Forget about control group  \ndf['cp_dose'] = df['cp_dose'].map({'D1':0.0, 'D2':1.0})\ndf['cp_time'] = df['cp_time'].map({24:0.0, 48: .5 , 72:1.0})\nX = df.copy()\nX_save = df.copy()\ndf_test = pd.read_csv('/kaggle/input/lish-moa/test_features.csv',index_col = 0)\ndf_test['cp_type'] = df_test['cp_type'].map({'trt_cp':1.0, 'ctl_vehicle':0.0})\ndf_test['cp_dose'] = df_test['cp_dose'].map({'D1':0.0, 'D2':1.0})\ndf_test['cp_time'] = df_test['cp_time'].map({24:0.0, 48: .5 , 72:1.0})\n\ny = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv',index_col = 0 )\ny_save = y.copy()\nprint(y.iloc[:3,:2])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_save.sum(axis = 0 ).sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = y_save['cyclooxygenase_inhibitor']\nprint( y.sum(), len(y), y.sum()/len(y), ) \n\ny = y_save['dopamine_receptor_antagonist']\nprint( y.sum(), len(y), y.sum()/len(y), ) \n\ny = y_save['raf_inhibitor']\nprint( y.sum(), len(y), y.sum()/len(y), ) \n\n\ny = y_save['hsp_inhibitor']\nprint( y.sum(), len(y), y.sum()/len(y), ) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression(random_state=0)#,C = 0.001)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2 Folds Full data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X = X.iloc[:,:2]\nX = X_save.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sklearn.model_selection.cross_validate(estimator, X, y=None, *, groups=None, scoring=None, cv=None, n_jobs=None, \n# verbose=0, fit_params=None, pre_dispatch='2*n_jobs', return_train_score=False, return_estimator=False, error_score=nan)\n\nmodel = LogisticRegression(random_state=0)#,C = 0.001)\n\n\ndf_stat = pd.DataFrame()\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\n\nt00 = time.time()\n\n#cv_results = cross_validate(model, X, y, cv=3, n_jobs=-1, return_estimator=True,  )\n#cv_results = cross_val_predict(model, X, y, cv=3, n_jobs=-1)# , return_estimator=True,  )\n\ny_sum = pd.Series(np.zeros_like(y), index = y.index)\n\nn_blends = 10\nfor i,rs in enumerate(range(n_blends)):\n    t0 = time.time()\n    skf = StratifiedKFold(n_splits=2, shuffle = True, random_state = rs )\n    y_pred = y.copy()\n    list_folds_scores = []\n    for train_index, test_index in skf.split(X, y):\n        model.fit(X.iloc[train_index,:], y.iloc[train_index] )\n        y_pred_on_test_fold = model.predict_proba(X.iloc[test_index,:])[:,1]\n        y_pred.iloc[test_index] = y_pred_on_test_fold\n        score_on_fold = log_loss(y.iloc[test_index], y_pred_on_test_fold, labels = [0,1] ) \n        list_folds_scores.append(score_on_fold)\n    y_sum += y_pred \n    print( np.round( np.mean(cv_score) , 5 ) , np.round(loc_time,2),'seconds passed' )\n    y_blend = y_sum / (i+1)\n    cv_score =   log_loss(y, y_blend, labels = [0,1] )  \n    df_stat.loc[rs,'Blended CV LogLoss'] = np.mean(cv_score)\n\n    cv_score =   log_loss(y, y_pred, labels = [0,1] )  \n    loc_time = time.time()-t0\n    df_stat.loc[rs,'CV LogLoss'] = np.mean(cv_score)\n    \n    df_stat.loc[rs,'LogLoss Folds Mean'] = np.mean(cv_score)\n    df_stat.loc[rs,'LogLoss Folds Mean'] = np.mean(cv_score)\n    for t in range(len(list_folds_scores)): \n        df_stat.loc[rs,'LogLoss Fold'+str(t)] = list_folds_scores[t]\n    \n    print('Blend score: ', np.round( np.mean(cv_score) , 5 ), 'Average score', np.round( df_stat['CV LogLoss'].mean() , 5 )   ) # , np.round(total_time,2),'loc_time passed' )\n\ny_blend = y_sum / n_blends\ncv_score =   log_loss(y, y_blend, labels = [0,1] )  \nprint( np.mean(cv_score) ) # , np.round(total_time,2),'loc_time passed' )\n\ntotal_time = time.time()-t00    \n\n\nprint(np.round(total_time,2),'seconds total passed')\nprint( df_stat.mean() )\n\nlist_c = ['Blended CV LogLoss', 'CV LogLoss']\nfig = plt.figure(figsize = (16,5)); sp = 0\nsp+=1;fig.add_subplot(1,2,sp)\nfor c in list_c[:1]:\n    #df_stat[c].plot()\n    plt.plot( df_stat[c], '*-' , label = c) \nplt.legend()\n\nsp+=1;fig.add_subplot(1,2,sp)\nfor c in list_c:\n    df_stat[c].plot()\nfor t in range(len(list_folds_scores)): \n    f = 'LogLoss Fold'+str(t)\n    plt.plot( df_stat[f], '*-' , label = f ) \n\nplt.legend()\nplt.show()\n\ndf_stat\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.round(total_time,2),'seconds total passed')\nprint( df_stat.mean() )\n\nlist_c = ['Blended CV LogLoss', 'CV LogLoss']\nfig = plt.figure(figsize = (16,5)); sp = 0\nsp+=1;fig.add_subplot(1,2,sp)\nfor c in list_c[:1]:\n    #df_stat[c].plot()\n    plt.plot( df_stat[c], '*-' , label = c) \nplt.legend()\n\nsp+=1;fig.add_subplot(1,2,sp)\nfor c in list_c:\n    df_stat[c].plot()\nfor t in range(len(list_folds_scores)): \n    f = 'LogLoss Fold'+str(t)\n    plt.plot( df_stat[f], '*-' , label = f ) \n\nplt.legend()\nplt.show()\n\ndf_stat\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3 Folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"#sklearn.model_selection.cross_validate(estimator, X, y=None, *, groups=None, scoring=None, cv=None, n_jobs=None, \n# verbose=0, fit_params=None, pre_dispatch='2*n_jobs', return_train_score=False, return_estimator=False, error_score=nan)\n\nn_splits = 3\n\nmodel = LogisticRegression(random_state=0)#,C = 0.001)\n\n\ndf_stat = pd.DataFrame()\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\n\nt00 = time.time()\n\n#cv_results = cross_validate(model, X, y, cv=3, n_jobs=-1, return_estimator=True,  )\n#cv_results = cross_val_predict(model, X, y, cv=3, n_jobs=-1)# , return_estimator=True,  )\n\ny_sum = pd.Series(np.zeros_like(y), index = y.index)\n\nn_blends = 10\nfor i,rs in enumerate(range(n_blends)):\n    t0 = time.time()\n    skf = StratifiedKFold(n_splits=n_splits, shuffle = True, random_state = rs )\n    y_pred = y.copy()\n    list_folds_scores = []\n    for train_index, test_index in skf.split(X, y):\n        model.fit(X.iloc[train_index,:], y.iloc[train_index] )\n        y_pred_on_test_fold = model.predict_proba(X.iloc[test_index,:])[:,1]\n        y_pred.iloc[test_index] = y_pred_on_test_fold\n        score_on_fold = log_loss(y.iloc[test_index], y_pred_on_test_fold, labels = [0,1] ) \n        list_folds_scores.append(score_on_fold)\n    y_sum += y_pred \n    print( np.round( np.mean(cv_score) , 5 ) , np.round(loc_time,2),'seconds passed' )\n    y_blend = y_sum / (i+1)\n    cv_score =   log_loss(y, y_blend, labels = [0,1] )  \n    df_stat.loc[rs,'Blended CV LogLoss'] = np.mean(cv_score)\n\n    cv_score =   log_loss(y, y_pred, labels = [0,1] )  \n    loc_time = time.time()-t0\n    df_stat.loc[rs,'CV LogLoss'] = np.mean(cv_score)\n    \n    df_stat.loc[rs,'LogLoss Folds Mean'] = np.mean(cv_score)\n    df_stat.loc[rs,'LogLoss Folds Mean'] = np.mean(cv_score)\n    for t in range(len(list_folds_scores)): \n        df_stat.loc[rs,'LogLoss Fold'+str(t)] = list_folds_scores[t]\n    \n    print('Blend score: ', np.round( np.mean(cv_score) , 5 ), 'Average score', np.round( df_stat['CV LogLoss'].mean() , 5 )   ) # , np.round(total_time,2),'loc_time passed' )\n\ny_blend = y_sum / n_blends\ncv_score =   log_loss(y, y_blend, labels = [0,1] )  \nprint( np.mean(cv_score) ) # , np.round(total_time,2),'loc_time passed' )\n\ntotal_time = time.time()-t00    \n\n\nprint(np.round(total_time,2),'seconds total passed')\nprint( df_stat.mean() )\n\nlist_c = ['Blended CV LogLoss', 'CV LogLoss']\nfig = plt.figure(figsize = (16,5)); sp = 0\nsp+=1;fig.add_subplot(1,2,sp)\nfor c in list_c[:1]:\n    #df_stat[c].plot()\n    plt.plot( df_stat[c], '*-' , label = c) \nplt.legend()\n\nsp+=1;fig.add_subplot(1,2,sp)\nfor c in list_c:\n    df_stat[c].plot()\nfor t in range(len(list_folds_scores)): \n    f = 'LogLoss Fold'+str(t)\n    plt.plot( df_stat[f], '*-' , label = f ) \n\nplt.legend()\nplt.show()\n\ndf_stat\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.round(total_time,2),'seconds total passed')\nprint( df_stat.mean() )\n\nlist_c = ['Blended CV LogLoss', 'CV LogLoss']\nfig = plt.figure(figsize = (16,5)); sp = 0\nsp+=1;fig.add_subplot(1,2,sp)\nfor c in list_c[:1]:\n    #df_stat[c].plot()\n    plt.plot( df_stat[c], '*-' , label = c) \nplt.legend()\n\nsp+=1;fig.add_subplot(1,2,sp)\nfor c in list_c:\n    df_stat[c].plot()\nfor t in range(len(list_folds_scores)): \n    f = 'LogLoss Fold'+str(t)\n    plt.plot( df_stat[f], '*-' , label = f ) \n\nplt.legend()\nplt.show()\n\ndf_stat\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5 Folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"#sklearn.model_selection.cross_validate(estimator, X, y=None, *, groups=None, scoring=None, cv=None, n_jobs=None, \n# verbose=0, fit_params=None, pre_dispatch='2*n_jobs', return_train_score=False, return_estimator=False, error_score=nan)\n\nn_splits = 5\n\nmodel = LogisticRegression(random_state=0)#,C = 0.001)\n\n\ndf_stat = pd.DataFrame()\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\n\nt00 = time.time()\n\n#cv_results = cross_validate(model, X, y, cv=3, n_jobs=-1, return_estimator=True,  )\n#cv_results = cross_val_predict(model, X, y, cv=3, n_jobs=-1)# , return_estimator=True,  )\n\ny_sum = pd.Series(np.zeros_like(y), index = y.index)\n\nn_blends = 10\nfor i,rs in enumerate(range(n_blends)):\n    t0 = time.time()\n    skf = StratifiedKFold(n_splits=n_splits, shuffle = True, random_state = rs )\n    y_pred = y.copy()\n    list_folds_scores = []\n    for train_index, test_index in skf.split(X, y):\n        model.fit(X.iloc[train_index,:], y.iloc[train_index] )\n        y_pred_on_test_fold = model.predict_proba(X.iloc[test_index,:])[:,1]\n        y_pred.iloc[test_index] = y_pred_on_test_fold\n        score_on_fold = log_loss(y.iloc[test_index], y_pred_on_test_fold, labels = [0,1] ) \n        list_folds_scores.append(score_on_fold)\n    y_sum += y_pred \n    print( np.round( np.mean(cv_score) , 5 ) , np.round(loc_time,2),'seconds passed' )\n    y_blend = y_sum / (i+1)\n    cv_score =   log_loss(y, y_blend, labels = [0,1] )  \n    df_stat.loc[rs,'Blended CV LogLoss'] = np.mean(cv_score)\n\n    cv_score =   log_loss(y, y_pred, labels = [0,1] )  \n    loc_time = time.time()-t0\n    df_stat.loc[rs,'CV LogLoss'] = np.mean(cv_score)\n    \n    df_stat.loc[rs,'LogLoss Folds Mean'] = np.mean(cv_score)\n    df_stat.loc[rs,'LogLoss Folds Mean'] = np.mean(cv_score)\n    for t in range(len(list_folds_scores)): \n        df_stat.loc[rs,'LogLoss Fold'+str(t)] = list_folds_scores[t]\n    \n    print('Blend score: ', np.round( np.mean(cv_score) , 5 ), 'Average score', np.round( df_stat['CV LogLoss'].mean() , 5 )   ) # , np.round(total_time,2),'loc_time passed' )\n\ny_blend = y_sum / n_blends\ncv_score =   log_loss(y, y_blend, labels = [0,1] )  \nprint( np.mean(cv_score) ) # , np.round(total_time,2),'loc_time passed' )\n\ntotal_time = time.time()-t00    \n\n\nprint(np.round(total_time,2),'seconds total passed')\nprint( df_stat.mean() )\n\nlist_c = ['Blended CV LogLoss', 'CV LogLoss']\nfig = plt.figure(figsize = (16,5)); sp = 0\nsp+=1;fig.add_subplot(1,2,sp)\nfor c in list_c[:1]:\n    #df_stat[c].plot()\n    plt.plot( df_stat[c], '*-' , label = c) \nplt.legend()\n\nsp+=1;fig.add_subplot(1,2,sp)\nfor c in list_c:\n    df_stat[c].plot()\nfor t in range(len(list_folds_scores)): \n    f = 'LogLoss Fold'+str(t)\n    plt.plot( df_stat[f], '*-' , label = f ) \n\nplt.legend()\nplt.show()\n\ndf_stat\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.round(total_time,2),'seconds total passed')\nprint( df_stat.mean() )\n\nlist_c = ['Blended CV LogLoss', 'CV LogLoss']\nfig = plt.figure(figsize = (16,5)); sp = 0\nsp+=1;fig.add_subplot(1,2,sp)\nfor c in list_c[:1]:\n    #df_stat[c].plot()\n    plt.plot( df_stat[c], '*-' , label = c) \nplt.legend()\n\nsp+=1;fig.add_subplot(1,2,sp)\nfor c in list_c:\n    df_stat[c].plot()\nfor t in range(len(list_folds_scores)): \n    f = 'LogLoss Fold'+str(t)\n    plt.plot( df_stat[f], '*-' , label = f ) \n\nplt.legend()\nplt.show()\n\ndf_stat\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 10 Folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"#sklearn.model_selection.cross_validate(estimator, X, y=None, *, groups=None, scoring=None, cv=None, n_jobs=None, \n# verbose=0, fit_params=None, pre_dispatch='2*n_jobs', return_train_score=False, return_estimator=False, error_score=nan)\n\nn_splits = 10\n\nmodel = LogisticRegression(random_state=0)#,C = 0.001)\n\n\ndf_stat = pd.DataFrame()\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\n\nt00 = time.time()\n\n#cv_results = cross_validate(model, X, y, cv=3, n_jobs=-1, return_estimator=True,  )\n#cv_results = cross_val_predict(model, X, y, cv=3, n_jobs=-1)# , return_estimator=True,  )\n\ny_sum = pd.Series(np.zeros_like(y), index = y.index)\n\nn_blends = 10\nfor i,rs in enumerate(range(n_blends)):\n    t0 = time.time()\n    skf = StratifiedKFold(n_splits=n_splits, shuffle = True, random_state = rs )\n    y_pred = y.copy()\n    list_folds_scores = []\n    for train_index, test_index in skf.split(X, y):\n        model.fit(X.iloc[train_index,:], y.iloc[train_index] )\n        y_pred_on_test_fold = model.predict_proba(X.iloc[test_index,:])[:,1]\n        y_pred.iloc[test_index] = y_pred_on_test_fold\n        score_on_fold = log_loss(y.iloc[test_index], y_pred_on_test_fold, labels = [0,1] ) \n        list_folds_scores.append(score_on_fold)\n    y_sum += y_pred \n    print( np.round( np.mean(cv_score) , 5 ) , np.round(loc_time,2),'seconds passed' )\n    y_blend = y_sum / (i+1)\n    cv_score =   log_loss(y, y_blend, labels = [0,1] )  \n    df_stat.loc[rs,'Blended CV LogLoss'] = np.mean(cv_score)\n\n    cv_score =   log_loss(y, y_pred, labels = [0,1] )  \n    loc_time = time.time()-t0\n    df_stat.loc[rs,'CV LogLoss'] = np.mean(cv_score)\n    \n    df_stat.loc[rs,'LogLoss Folds Mean'] = np.mean(cv_score)\n    df_stat.loc[rs,'LogLoss Folds Mean'] = np.mean(cv_score)\n    for t in range(len(list_folds_scores)): \n        df_stat.loc[rs,'LogLoss Fold'+str(t)] = list_folds_scores[t]\n    \n    print('Blend score: ', np.round( np.mean(cv_score) , 5 ), 'Average score', np.round( df_stat['CV LogLoss'].mean() , 5 )   ) # , np.round(total_time,2),'loc_time passed' )\n\ny_blend = y_sum / n_blends\ncv_score =   log_loss(y, y_blend, labels = [0,1] )  \nprint( np.mean(cv_score) ) # , np.round(total_time,2),'loc_time passed' )\n\ntotal_time = time.time()-t00    \n\n\nprint(np.round(total_time,2),'seconds total passed')\nprint( df_stat.mean() )\n\nlist_c = ['Blended CV LogLoss', 'CV LogLoss']\nfig = plt.figure(figsize = (16,5)); sp = 0\nsp+=1;fig.add_subplot(1,2,sp)\nfor c in list_c[:1]:\n    #df_stat[c].plot()\n    plt.plot( df_stat[c], '*-' , label = c) \nplt.legend()\n\nsp+=1;fig.add_subplot(1,2,sp)\nfor c in list_c:\n    df_stat[c].plot()\nfor t in range(len(list_folds_scores)): \n    f = 'LogLoss Fold'+str(t)\n    plt.plot( df_stat[f], '*-' , label = f ) \n\nplt.legend()\nplt.show()\n\ndf_stat\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.round(total_time,2),'seconds total passed')\nprint( df_stat.mean() )\n\nlist_c = ['Blended CV LogLoss', 'CV LogLoss']\nfig = plt.figure(figsize = (16,5)); sp = 0\nsp+=1;fig.add_subplot(1,2,sp)\nfor c in list_c[:1]:\n    #df_stat[c].plot()\n    plt.plot( df_stat[c], '*-' , label = c) \nplt.legend()\n\nsp+=1;fig.add_subplot(1,2,sp)\nfor c in list_c:\n    df_stat[c].plot()\nfor t in range(len(list_folds_scores)): \n    f = 'LogLoss Fold'+str(t)\n    plt.plot( df_stat[f], '*-' , label = f ) \n\nplt.legend()\nplt.show()\n\ndf_stat\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}