{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What is about ?\n\nBlend with scipy optimization added as option.\n\nV10 C [0.5, 1, 2, 4 ], n_blends = 1\n\nV9 - n_blends = 3\n\nV8 - submitted got improvement\n\nV6 test run with change blend to simple \n\nv4 - added check in blend for worse than constant 0.016306\t0.016173 0.016062\t0.016080\n\nV3 60 minutes 0.016340\t0.016179\t0.016585\t0.016076\t0.016086\n\nV2 - test run on 25 targets\n\nV1 - test \n\n-----------------------\n\nMoA48_1\n\nV8 L1 logreg C=2, blends = 1\n\nV7 L1 logreg C=0.5, blends = 1, 0.016723 0.016541\n\nV6 L1 logreg C=1, blends = 1, Loss1 0.016308 Loss2 0.016247\n\nV5 blends 1, C = [0.25, 0.5,1,2, 4, 8, 16, 32 ] Best No blend: 0.01618 Best Blend 0.01612 Blend gain best to best in 5-th digits: 6.0 Best blend 0.01612 at C-multiplier 4 Best blend at N 4 0.016197\n\nv4 20 blends, C=1, No blend: 0.01619 Blend 0.01611 Blend gain to mean in 5-th digits: 8.0\n\nV3 - same as V2 - difference due to randomness - 0.016194 0.016169 - randomness affect less than 5th digit , it is strange - an a plot from v4 we see about 3 5th digits variation.\n\nV2 - 1,1,1 Loss1 0.016192 Loss2 (correct worse than constant) 0.016162 - difference - 3 * 5th digits quite small\n\nV1 - test run 25 targets\n\nMoA46_3\n\nV4 - filter out worse constant CV 0.016175 LB 0.01951\n\nV3 - Fit on full train, C=1 n_blends =1, LB 0.01950 CV 0.017603 is approximate\n\nFrom MoA46\n\nV4 n_blends = 1, C_blends = [2] (no blend) LB 0.01959 CV 0.017740\n\nV3 n_blends = 1, C_blends = [0.5] (no blend) LB 0.01952 CV 0.017705\n\nV2 n_blends = 1, C_blends = [1] (no blend) LB 0.01948, CV 0.017627\n\nV1 - test version does not support blend over folds yet."},{"metadata":{"trusted":true},"cell_type":"code","source":"flag_train_on_whole_train_for_the_first_model_in_blend = 0 # train on whole train (without subfolds splits)\n    # if 1 that is done for the first ONLY model in loop over n_blends \n    # thus if several C in list that will be done for each C \n\nn_blends = 1\nlist4blend_change_optimal_C_by = [0.5, 1, 2, 4 ]# [0.25, 0.5,1,2, 4, 8, 16, 32 ] \n\n\n#list_models = ['l2', 'l1']\nlist_models = ['Const Predict','l1','l2']\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data with slight preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport time\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport time\ndf = pd.read_csv('/kaggle/input/lish-moa/train_features.csv',index_col = 0)  \ndf0 = df.copy()\ndf['cp_type'] = df['cp_type'].map({'trt_cp':1.0, 'ctl_vehicle':1.0}) # Forget about control group  \ndf['cp_dose'] = df['cp_dose'].map({'D1':0.0, 'D2':1.0})\ndf['cp_time'] = df['cp_time'].map({24:0.0, 48: .5 , 72:1.0})\nX = df.copy()\nX_save = X.copy()\ndf_test = pd.read_csv('/kaggle/input/lish-moa/test_features.csv',index_col = 0)\ndf0_test = df_test.copy()\ndf_test['cp_type'] = df_test['cp_type'].map({'trt_cp':1.0, 'ctl_vehicle':0.0})\ndf_test['cp_dose'] = df_test['cp_dose'].map({'D1':0.0, 'D2':1.0})\ndf_test['cp_time'] = df_test['cp_time'].map({24:0.0, 48: .5 , 72:1.0})\n\ny = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv',index_col = 0 )\ny_save = y.copy()\nprint(y.iloc[:3,:2])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_save.sum(axis = 0 ).sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Function to return list with mask for Chris folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"# LOAD LIBRARIES (from PIP or Kaggle Dataset)\n#! pip install iterative-stratification \n!pip install /kaggle/input/iterative-stratification/iterative-stratification-master/\nfrom sklearn.model_selection import KFold\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\n# LOAD FILES\nscored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ndrug = pd.read_csv('/kaggle/input/lish-moa/train_drug.csv')\ntargets = scored.columns[1:]\nscored = scored.merge(drug, on='sig_id', how='left') \n\ndef get_list_folds_masks(SEED = 42, FOLDS = 5):\n\n    # LOCATE DRUGS\n    vc = scored.drug_id.value_counts()\n    vc1 = vc.loc[vc<=18].index.sort_values()\n    vc2 = vc.loc[vc>18].index.sort_values()\n\n    # STRATIFY DRUGS 18X OR LESS\n    dct1 = {}; dct2 = {}\n    skf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, \n              random_state=SEED)\n    tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n        dd = {k:fold for k in tmp.index[idxV].values}\n        dct1.update(dd)\n\n    # STRATIFY DRUGS MORE THAN 18X\n    skf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, \n              random_state=SEED)\n    tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\n    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n        dd = {k:fold for k in tmp.sig_id[idxV].values}\n        dct2.update(dd)\n\n    # ASSIGN FOLDS\n    scored['fold'] = scored.drug_id.map(dct1)\n    scored.loc[scored.fold.isna(),'fold'] =\\\n        scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\n    scored.fold = scored.fold.astype('int8')\n\n    df_folds = scored[['sig_id','fold']]\n    df_folds = df_folds.set_index('sig_id')\n    if 0:\n        print(df_folds['fold'].value_counts())\n    list_folds_masks = []\n    for i in range(FOLDS):\n        m = df_folds['fold'] == i\n        list_folds_masks.append((~m,m))\n    if 0:\n        for mask_train,mask_test in list_folds_masks:\n            print(mask_train.sum(), mask_test.sum(),  mask_train.sum()+ mask_test.sum())\n\n    return list_folds_masks\n\nlist_folds_masks = get_list_folds_masks(SEED = 42, FOLDS = 5)\nfor mask_train,mask_test in list_folds_masks:\n    print(mask_train.sum(), mask_test.sum(),  mask_train.sum()+ mask_test.sum())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n# list_C is min from cv3 rs0,1,100 finer grid than before - used in MoA23 v6 and above\nlist_C_MoA36 = [0.03, 0.003, 0.0002, 0.001, 0.0007, 0.001, 0.001, 0.002, 0.1, 0.005, 0.001, 0.01, 0.5, 0.01, 0.0007, 0.002, 0.001, 0.001, 0.001, 0.001, 0.002, 0.002, 0.007, 0.005, 0.005, 0.01, 0.005, 0.005, 0.003, 0.002, 0.01, 0.005, 0.002, 0.03, 0.0, 0.1, 0.005, 0.05, 0.02, 0.007, 0.003, 0.007, 0.005, 0.001, 0.003, 0.005, 0.02, 0.01, 0.01, 0.003, 0.001, 0.01, 0.02, 0.02, 0.001, 0.003, 0.005, 0.001, 0.007, 0.01, 0.0003, 0.0002, 0.005, 0.007, 0.002, 0.2, 0.005, 0.001, 0.002, 0.005, 0.005, 0.001, 0.002, 0.007, 0.007, 0.007, 0.005, 0.002, 0.003, 0.002, 0.02, 0.003, 0.0, 0.002, 0.005, 0.007, 0.05, 0.005, 0.02, 0.01, 0.05, 0.003, 0.007, 0.003, 0.002, 0.02, 0.005, 0.02, 0.002, 0.0007, 0.0007, 0.01, 0.002, 0.01, 0.002, 0.001, 0.1, 0.03, 0.005, 0.05, 0.02, 0.02, 0.01, 0.001, 0.003, 0.002, 0.01, 0.001, 0.02, 0.02, 0.005, 0.01, 0.003, 0.001, 0.003, 0.03, 0.07, 0.02, 0.002, 0.003, 0.0005, 0.002, 0.02, 0.007, 0.003, 0.005, 0.005, 0.01, 0.003, 0.003, 0.003, 0.01, 0.01, 0.002, 0.002, 0.0003, 0.02, 0.005, 0.01, 0.01, 0.01, 0.003, 0.003, 0.007, 0.007, 0.001, 0.0007, 0.01, 0.003, 0.007, 0.03, 0.003, 0.001, 1.0, 0.002, 0.02, 0.005, 0.0, 0.001, 0.05, 0.007, 0.05, 0.01, 0.03, 0.003, 0.007, 0.0005, 0.002, 0.005, 0.005, 0.002, 0.005, 0.001, 0.007, 0.007, 0.0007, 0.02, 0.005, 0.03, 0.005, 0.007, 0.001, 0.003, 0.005, 0.007, 0.002, 0.001, 0.02, 0.003, 0.01, 0.003, 0.02, 0.01, 0.0005, 0.1, 0.0007]\nlist_C_MoA45_V9_l2 = [0.036, 0.005183999999999999, 5.787037037037037e-05, 0.0003348979766803842, 0.0005787037037037038, 0.0002790816472336535, 0.00048225308641975317, 0.0003348979766803842, 0.03, 0.005183999999999999, 0.0008333333333333334, 0.0036, np.nan, 0.0036, 0.0006944444444444446, 0.0025, 0.001, 0.0005787037037037038, 0.0012, 0.0006220799999999999, 0.003, 0.0014467592592592596, np.nan, 2.790816472336535e-06, 0.0025, 0.0144, 0.0005787037037037038, 0.00043199999999999993, 0.0025, 0.0017279999999999997, 0.0025, 2.790816472336535e-06, 0.00035999999999999997, np.nan, np.nan, 0.020833333333333336, 0.0020833333333333337, 0.005787037037037038, 0.0144, np.nan, 0.0017279999999999997, 0.0048225308641975315, 0.00432, 0.0012, 0.0020736, 0.0036, 0.008333333333333333, 0.005183999999999999, 0.0017361111111111112, 0.003, 0.00020833333333333335, 0.0025, 0.008333333333333333, np.nan, 0.0005787037037037038, 0.0020833333333333337, 0.00017279999999999997, 0.00048225308641975317, 0.0005183999999999999, 0.00432, 0.00043199999999999993, 8.333333333333334e-05, 0.005183999999999999, 0.003, 0.00035999999999999997, 0.0144, 0.0036, 0.0006220799999999999, 0.00025, np.nan, 0.006220799999999999, 0.001, 0.000144, 0.006944444444444446, 0.0036, np.nan, 0.00048225308641975317, 0.0008333333333333334, 0.0005183999999999999, 0.0017279999999999997, 0.017279999999999997, np.nan, np.nan, 0.0014399999999999999, 0.0025, 0.005787037037037038, 0.036, 0.0036, 0.006944444444444446, 0.005183999999999999, 0.020833333333333336, 0.0006944444444444446, 0.006944444444444446, 8.333333333333334e-05, 0.0008333333333333334, 0.0144, 0.006220799999999999, 0.00025, 0.0017279999999999997, 0.0008333333333333334, 0.0017279999999999997, 0.005787037037037038, 0.000144, 0.005183999999999999, 0.0014399999999999999, 0.00048225308641975317, 0.05787037037037038, 0.0144, 0.005183999999999999, 0.06220799999999999, 0.006220799999999999, 0.005183999999999999, 2.790816472336535e-06, 0.00025, 0.0012056327160493829, 0.0005787037037037038, 0.005183999999999999, 6.944444444444446e-05, 0.01, 0.012, np.nan, np.nan, 0.00017279999999999997, 0.001, 2.790816472336535e-06, np.nan, 0.0144, 0.00432, 0.00012, 0.00043199999999999993, 0.00012, 0.0006944444444444446, 0.020833333333333336, 0.005183999999999999, 0.0017279999999999997, 0.003, 0.0020833333333333337, np.nan, 0.0036, 2.790816472336535e-06, 0.0025, 0.00043199999999999993, 0.00043199999999999993, 0.0003, 2.7908164723365347e-05, 0.00025, 0.012, 4.8225308641975325e-06, 0.005787037037037038, 0.01, 0.0014399999999999999, 0.0025, 0.0005183999999999999, 0.0017361111111111112, 2.790816472336535e-06, 0.0002790816472336535, 0.0005787037037037038, 0.006944444444444446, 0.0005787037037037038, 0.005787037037037038, 0.025, 0.0014399999999999999, 0.00025, 1.0, 5.183999999999999e-05, np.nan, 0.00048225308641975317, 2.790816472336535e-06, 0.0008333333333333334, 0.036, 3.348979766803842e-06, 0.017361111111111112, np.nan, 0.017361111111111112, 0.0017361111111111112, 2.790816472336535e-06, 0.00017279999999999997, 0.0012, 0.00024883199999999994, 0.001, 0.0020833333333333337, 8.333333333333334e-05, 0.0005787037037037038, 0.001, 0.0025, np.nan, 2.790816472336535e-06, 0.0017279999999999997, 0.020833333333333336, 0.0020833333333333337, 0.0036, 0.00035999999999999997, 0.005787037037037038, 0.0025, 0.008333333333333333, 0.003, np.nan, 0.0017361111111111112, 0.0010749542399999996, 0.008333333333333333, 1.4400000000000001e-05, np.nan, 0.006220799999999999, 0.0006944444444444446, 0.05787037037037038, 0.00020833333333333335]\nlist_C_l2 = list_C_MoA45_V9_l2\nlist_C_MoA45_V11_l1_seed21113 = [0.432, 0.144, 0.08333333333333334, 0.06220799999999999, 0.036, 0.05183999999999999, 0.05787037037037038, 0.05787037037037038, 0.4822530864197532, 0.1, 0.043199999999999995, 0.1, np.nan, 0.05787037037037038, 0.1736111111111111, 0.25, 0.06944444444444446, 0.06220799999999999, 0.05787037037037038, 0.06944444444444446, 0.08333333333333334, 0.06944444444444446, np.nan, 0.06220799999999999, 0.17279999999999998, 0.20833333333333334, 0.144, 0.08333333333333334, 0.08333333333333334, 0.1, 0.1, 0.036, 0.1, np.nan, np.nan, 0.5183999999999999, 0.05787037037037038, 0.3, 0.144, np.nan, 0.06944444444444446, 0.08333333333333334, 0.144, 0.036, 0.05787037037037038, 0.1, 0.36, 0.12, 0.08333333333333334, 0.06944444444444446, 0.1, 0.144, 0.12, np.nan, 0.03, 0.1, 0.05787037037037038, 0.08333333333333334, 0.06944444444444446, 0.25, 0.17279999999999998, 0.036, 0.17279999999999998, 0.08333333333333334, 0.05787037037037038, 0.432, 0.1, 0.08333333333333334, 0.05183999999999999, np.nan, 0.144, 0.020833333333333336, 0.03, 0.144, 0.12, np.nan, 0.12, 0.025, 0.06220799999999999, 0.043199999999999995, 0.17279999999999998, np.nan, np.nan, 0.06220799999999999, 0.1, 0.12, 0.4822530864197532, 0.1, 0.08333333333333334, 0.06220799999999999, 0.5787037037037037, 0.17279999999999998, 0.12, 0.043199999999999995, 0.05183999999999999, 0.144, 0.1, 0.12, 0.05787037037037038, 0.036, 0.144, 0.12, 0.043199999999999995, 0.08333333333333334, 0.08333333333333334, 0.036, 0.4822530864197532, 0.3, 0.08333333333333334, 0.3, 0.12, 0.12, 0.03, 0.06944444444444446, 0.06944444444444446, 0.05787037037037038, 0.12, 0.05787037037037038, 0.12, 0.08333333333333334, np.nan, np.nan, 0.05183999999999999, 0.17279999999999998, 0.043199999999999995, np.nan, 0.12, 0.08333333333333334, 0.1, 0.12, 0.144, 0.06220799999999999, 0.3, 0.12, 0.1, 0.1, 0.04822530864197532, np.nan, 0.17279999999999998, 0.06944444444444446, 0.12, 0.1736111111111111, 0.12, 0.05787037037037038, 0.043199999999999995, 0.08333333333333334, 0.144, 0.06944444444444446, 0.1, 0.1, 0.144, 0.05183999999999999, 0.1, 0.05183999999999999, 0.05183999999999999, 0.06944444444444446, 0.043199999999999995, 0.06944444444444446, 0.1, 0.1, 0.1736111111111111, 0.1, 0.05183999999999999, 2.9859839999999993, 0.06220799999999999, np.nan, 0.043199999999999995, 0.1, 0.06944444444444446, 0.5183999999999999, 0.12, 0.144, np.nan, 0.25, 0.08333333333333334, 0.08333333333333334, 0.025, 0.043199999999999995, 0.06944444444444446, 0.12, 0.12, 0.1, 0.036, 0.144, 0.05787037037037038, np.nan, 0.06944444444444446, 0.08333333333333334, 0.36, 0.12, 0.12, 0.1, 0.20833333333333334, 0.1, 0.06944444444444446, 0.20833333333333334, np.nan, 0.06944444444444446, 0.08333333333333334, 0.12, 0.06220799999999999, np.nan, 0.06220799999999999, 0.1, 0.4822530864197532, 0.08333333333333334]\nlist_C_MoA45_V10_l1_seed100 = [0.432, 0.144, 0.08333333333333334, 0.036, 0.05183999999999999, 0.05183999999999999, 0.06944444444444446, 0.04822530864197532, 0.5787037037037037, 0.1, 0.043199999999999995, 0.1, np.nan, 0.08333333333333334, 0.20833333333333334, 0.25, 0.05787037037037038, 0.05183999999999999, 0.06944444444444446, 0.08333333333333334, 0.08333333333333334, 0.06944444444444446, np.nan, 0.06220799999999999, 0.144, 0.20833333333333334, 0.144, 0.08333333333333334, 0.08333333333333334, 0.12, 0.1, 0.036, 0.06944444444444446, np.nan, np.nan, 0.5183999999999999, 0.05787037037037038, 0.25, 0.20736, np.nan, 0.06944444444444446, 0.144, 0.144, 0.05183999999999999, 0.08333333333333334, 0.1, 0.3, 0.1, 0.08333333333333334, 0.06944444444444446, 0.1, 0.08333333333333334, 0.17279999999999998, np.nan, 0.03, 0.08333333333333334, 0.06944444444444446, 0.08333333333333334, 0.06944444444444446, 0.20833333333333334, 0.17279999999999998, 0.043199999999999995, 0.17279999999999998, 0.08333333333333334, 0.08333333333333334, 0.36, 0.1, 0.08333333333333334, 0.05183999999999999, np.nan, 0.17279999999999998, 0.03, 0.036, 0.144, 0.17279999999999998, np.nan, 0.12, 0.025, 0.06220799999999999, 0.05183999999999999, 0.17279999999999998, np.nan, np.nan, 0.07464959999999998, 0.1, 0.12, 0.4822530864197532, 0.1, 0.08333333333333334, 0.05183999999999999, 0.40187757201646096, 0.17279999999999998, 0.12, 0.043199999999999995, 0.06220799999999999, 0.144, 0.1, 0.12, 0.08333333333333334, 0.043199999999999995, 0.144, 0.1, 0.043199999999999995, 0.12, 0.08333333333333334, 0.043199999999999995, 0.40187757201646096, 0.25, 0.08333333333333334, 0.3, 0.1, 0.12, 0.03, 0.06944444444444446, 0.06944444444444446, 0.05787037037037038, 0.1, 0.05787037037037038, 0.1, 0.08333333333333334, np.nan, np.nan, 0.05183999999999999, 0.17279999999999998, 0.043199999999999995, np.nan, 0.12, 0.08333333333333334, 0.08333333333333334, 0.12, 0.144, 0.06220799999999999, 0.3, 0.1, 0.1, 0.12, 0.05787037037037038, np.nan, 0.144, 0.06944444444444446, 0.12, 0.1736111111111111, 0.12, 0.06944444444444446, 0.036, 0.1, 0.17279999999999998, 0.06944444444444446, 0.1, 0.1, 0.144, 0.06220799999999999, 0.1, 0.05183999999999999, 0.05183999999999999, 0.05787037037037038, 0.043199999999999995, 0.08333333333333334, 0.08333333333333334, 0.08333333333333334, 0.20833333333333334, 0.1, 0.05183999999999999, 1.44, 0.05183999999999999, np.nan, 0.036, 0.1, 0.06944444444444446, 0.3, 0.144, 0.144, np.nan, 0.20833333333333334, 0.08333333333333334, 0.06944444444444446, 0.03, 0.036, 0.06944444444444446, 0.08333333333333334, 0.12, 0.1, 0.03, 0.144, 0.05787037037037038, np.nan, 0.06944444444444446, 0.08333333333333334, 0.432, 0.12, 0.12, 0.1, 0.25, 0.1, 0.1, 0.25, np.nan, 0.06944444444444446, 0.144, 0.12, 0.05183999999999999, np.nan, 0.06220799999999999, 0.1, 0.3348979766803842, 0.08333333333333334]\n\nl1 = list_C_MoA45_V11_l1_seed21113\nl2 = list_C_MoA45_V10_l1_seed100\nlist_C_l1 = [ np.min( [l1[i], l2[i]] ) for i in range(len(l1) )  ]\n\npd.Series(list_C_l1).describe() #list_C","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in list_C_l1[:10]:\n    print(c, type(c))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Core simulation\n\nLogreg for each targets\n\nwith possible blend over different C and different splits on subfolds \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef modeling(X,y, model_id = 'Const Predict', verbose = 0):\n    if model_id == 'Const Predict':\n        y_pred_oof = np.ones_like(y)*np.mean(y) \n        mn = y[df0.cp_type=='trt_cp'].mean()\n        y_pred_submit = np.ones( len(df_test) ) * mn # y.mean() \n\n    if (model_id == 'l1') or  (model_id == 'l2') :\n        if (model_id == 'l1'):         C = list_C_l1[cnt_target]\n        else: C = list_C_l2[cnt_target]\n        \n        if (np.isnan(C) or (C == 0) ):\n            return modeling(X,y, model_id = 'Const Predict')\n        \n        y_pred_submit = np.zeros( len(df_test) )\n        cnt_blend_submit = 0\n        y_pred_oof_blend = np.zeros_like(y,dtype = float)\n        cnt_blend_oof = 0\n        for C_effective in C*np.array(list4blend_change_optimal_C_by): # np.linspace(0.5,1,10): # np.array([0.5,1, 2]):\n        \n            if (model_id == 'l1'):\n                model = LogisticRegression( C = C_effective , penalty='l1', solver = 'liblinear' )  \n            else: \n                model = LogisticRegression( C = C_effective )  \n            for cnt_blends in range(n_blends):\n                rs = np.random.randint(10**7)\n                #skf = StratifiedKFold(n_splits=3, shuffle=True, random_state= rs )\n                list_folds_masks = get_list_folds_masks(SEED = rs, FOLDS = 5)\n                y_pred_oof = np.zeros_like(y,dtype = float)\n                list_loss_train = []\n                list_loss_test = []\n                \n                #for train_index, test_index in skf.split(X, y):\n                for mask_train,mask_test in list_folds_masks:\n                    X_train, X_test = X[mask_train], X[mask_test]\n                    y_train, y_test = y[mask_train], y[mask_test]\n                    #X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n                    #y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n            \n                    model.fit(X_train,y_train)\n                    #y_pred_oof[ test_index ] = model.predict_proba(X_test)[:,1]\n                    y_pred_oof[ mask_test ] = model.predict_proba(X_test)[:,1]\n\n                    # we have an option to fit the model on the whole train, and predict on submit\n                    # we do it if flag_... >0 and for the first blend in loop iteration  \n                    # thus for these conditions submit is not calculated here \n                    if not(  (flag_train_on_whole_train_for_the_first_model_in_blend > 0 ) and (  cnt_blends == 0) ):\n                        y_pred_submit = (y_pred_submit*cnt_blend_submit + model.predict_proba(df_test)[:,1])/(cnt_blend_submit+1) # blend\n                        cnt_blend_submit += 1\n\n                    list_loss_train.append( log_loss(y_train,  model.predict_proba(X_train)[:,1] , labels = [0,1]))\n                    list_loss_test.append( log_loss(y_test,  model.predict_proba(X_test)[:,1], labels = [0,1] ) )\n                        \n                    if verbose >= 100:\n                        print( np.round(list_loss_test,6) )\n                        \n                if flag_train_on_whole_train_for_the_first_model_in_blend > 0:\n                    if cnt_blends == 0:\n                        model.fit(X,y)\n                        y_pred_submit = (y_pred_submit*cnt_blend_submit + model.predict_proba(df_test)[:,1])/(cnt_blend_submit+1)                        \n                        cnt_blend_submit += 1\n\n    return y_pred_oof, y_pred_submit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.optimize import minimize, fsolve\nbnds = ((1, None), (0, None))\ndef f0(x):\n    return x[0]**2+x[1]**2\nres_scipy = minimize( f0,  [10,1] , bounds=bnds,)\nres_scipy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def blending_simple( list_preds ):\n    \n    y_pred_blend = np.zeros_like(y,dtype = float)\n    y_pred_submit = np.zeros( len(df_test), dtype = float )\n    c = 0\n    s0 = log_loss(y, np.ones_like(y)*np.mean(y) )\n    for i,t in enumerate(list_preds):\n        y_pred_oof = t[0]\n        s = log_loss(y, y_pred_oof )\n        if s >= s0: continue\n        c+=1\n        #df_stat.loc[cnt4df_stat,'Loss OOF'+str(i)] = s # should do it in stat function\n        y_pred_blend +=  y_pred_oof\n        y_pred_submit += t[1]\n        \n    cnt4df_stat = df_stat.index[-1]# len(df_stat)\n    if c > 0:\n        y_pred_blend /= c\n        y_pred_submit /= c\n        df_stat.loc[cnt4df_stat,'Loss OOF Blend Simple 2'] = log_loss(y, y_pred_blend )\n        df_stat.loc[cnt4df_stat,'Blend Simple 2 Used Const'] = 0\n    else:\n        df_stat.loc[cnt4df_stat,'Loss OOF Blend Simple 2'] = s0\n        y_pred_blend = np.ones_like(y)*np.mean(y) \n        mn = y[df0.cp_type=='trt_cp'].mean()\n        y_pred_submit = np.ones( len(df_test) ) * mn # y.mean() \n        df_stat.loc[cnt4df_stat,'Blend Simple 2 Used Const'] = 1\n        \n        \n        \n    return y_pred_blend, y_pred_submit \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef blending( list_preds) :\n    \n    if 0:\n        y_pred_blend = np.ones_like(y)*np.mean(y) \n        mn = y[df0.cp_type=='trt_cp'].mean()\n        y_pred_submit = np.ones( len(df_test) ) * mn # y.mean() \n\n    \n    rs = np.random.randint(10**7)\n    #skf = StratifiedKFold(n_splits=3, shuffle=True, random_state= rs )\n    list_folds_masks = get_list_folds_masks(SEED = rs, FOLDS = 5)\n    mask_train, mask_test = list_folds_masks[0][:2]\n    \n    list_scores = [np.nan, np.nan]\n    def func2optm(x):\n        y_pred_blend = np.zeros_like(y,dtype = float)\n        for i,y_pred in enumerate(list_preds):\n            y_pred_blend += x[i]*list_preds[i][0]\n        score_train = log_loss(y[mask_train], y_pred_blend[mask_train], labels = [0,1])\n        score_test = log_loss(y[mask_test], y_pred_blend[mask_test] , labels = [0,1] )\n        list_scores[0] = score_train\n        list_scores[1] = score_test\n        return score_train\n    \n    x0 = np.ones(len(list_preds)) * (1.0/len(list_preds)) \n    bnds = []\n    for i,y_pred in enumerate(list_preds):\n        bnds.append((0,None))\n    res_scipy = minimize(fun = func2optm, x0=  x0, bounds = bnds  ) \n    \n    \n    y_pred_blend = np.zeros_like(y,dtype = float)\n    for i,y_pred in enumerate(list_preds):\n        y_pred_blend += res_scipy.x[i]*list_preds[i][0]\n    y_pred_submit = np.zeros( len(df_test) ) \n    for i,y_pred in enumerate(list_preds):\n        y_pred_submit += res_scipy.x[i]*list_preds[i][1]\n    if log_loss(y, y_pred_blend) >= log_loss(y, np.ones_like(y)*np.mean(y)  ):\n        y_pred_blend = np.ones_like(y)*np.mean(y) \n        mn = y[df0.cp_type=='trt_cp'].mean()\n        y_pred_submit = np.ones( len(df_test) ) * mn # y.mean() \n\n    cnt4df_stat = df_stat.index[-1]# len(df_stat)\n    df_stat.loc[cnt4df_stat,'Blend Weighted'] = log_loss( y, y_pred_blend )\n    for i,y_pred in enumerate(range(len(res_scipy.x))):\n        df_stat.loc[cnt4df_stat,'W'+str(i)] = res_scipy.x[i]\n        \n    return y_pred_blend, y_pred_submit \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def df_stat_update_0( ):\n    cnt4df_stat = len(df_stat)+1\n\n    df_stat.loc[cnt4df_stat,'Target'] = target_name\n    df_stat.loc[cnt4df_stat,'Target Sum'] = np.sum(y) # target_name\n    df_stat['Target Sum'] = df_stat['Target Sum'].astype(int) # np.sum(y) # target_name\n    df_stat.loc[cnt4df_stat,'Loss Const Predict'] = log_loss( y, np.ones_like(y)*np.mean(y) ) \n    df_stat.loc[cnt4df_stat,'Seconds Passed'] = np.round(time.time()-t00,1)\n\ndef    df_stat_update_1( y_pred, model_inf = 'Blend' ):\n    cnt4df_stat = df_stat.index[-1]# len(df_stat)\n    df_stat.loc[cnt4df_stat,'Loss '+model_inf] = log_loss(y, y_pred )\n\ndef df_stat_update_list_preds( list_preds ):\n    cnt4df_stat = df_stat.index[-1]# len(df_stat)\n\n    y_blend_simple = np.zeros_like(y,dtype = float)\n    for i,t in enumerate(list_preds):\n        y_pred_oof = t[0]\n        df_stat.loc[cnt4df_stat,'Loss OOF'+str(i)] = log_loss(y, y_pred_oof )\n        y_blend_simple +=  y_pred_oof\n    y_blend_simple /= len(list_preds)\n    df_stat.loc[cnt4df_stat,'Loss OOF Blend Simple'] = log_loss(y, y_blend_simple )\n    \n    y_blend_simple = np.zeros_like(y,dtype = float)\n    c = 0\n    s0 = log_loss(y, np.ones_like(y)*np.mean(y) )\n    for i,t in enumerate(list_preds):\n        y_pred_oof = t[0]\n        s = log_loss(y, y_pred_oof )\n        if s >= s0:\n            df_stat.loc[cnt4df_stat,'Loss OOF Corrected Compare to Const'+str(i)] = s0\n            continue\n        c+=1\n        df_stat.loc[cnt4df_stat,'Loss OOF Corrected Compare to Const'+str(i)] = s\n        y_blend_simple +=  y_pred_oof\n    if c > 0:\n        y_blend_simple /= c\n        df_stat.loc[cnt4df_stat,'Loss OOF Blend Simple Better Cnst'] = log_loss(y, y_blend_simple )\n        df_stat.loc[cnt4df_stat,'Blend Simple Used Const'] = 0\n    else:\n        df_stat.loc[cnt4df_stat,'Loss OOF Blend Simple Better Cnst'] = s0\n        df_stat.loc[cnt4df_stat,'Blend Simple Used Const'] = 1\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y1 = y_pred_oof.copy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if 0:\n    l = []\n    s = log_loss(y,y1)\n    print(s, y.mean())\n    y_in = y1.copy()\n\n    s1 = np.inf\n    m_best = -1\n    a_best = None\n    for a in np.linspace(-0.01,1,100):\n        print(a,s1, a_best, m_best)\n        for m in np.linspace(0.5,2,20):\n            y_in = y1.copy()*m+a\n\n            for a_min in np.linspace(0,1,200):\n                y2 = np.clip(y_in, a_min,1)\n                s = log_loss(y,y2)\n                l.append(s)\n                if s <= np.min(l):\n                    a_best = a_min\n                    s_best = s\n                    y_best = y2.copy()\n            #print(a_best,s_best, 'a_min s_best')        \n            #print()\n\n            l = []\n            y_in = y_best.copy()\n            for a_max in np.linspace(0,1,200):\n                y2 = np.clip(y_in, 0,  a_max)# ,1)\n                s = log_loss(y,y2)\n                l.append(s)\n                if s <= np.min(l):\n                    a_best = a_max\n                    s_best = s\n                    y_best = y2\n            #print(a_best,s_best) \n            if s_best < s1:\n                print(s1,'s1 before', s_best,'s_best')\n                s1 = s_best\n                #s1 =np.min([s_best, s1])\n                m_best = m\n                a_best = a\n                print(a, m_best, s_best ,'upd')\n    s1 , m_best\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Core run\nfrom sklearn.linear_model import LogisticRegression\nverbose = 0#  100\n# flag_train_on_whole_train_for_the_first_model_in_blend = 1\n\ndf_submit = pd.DataFrame(index = df_test.index)\ndf_train_oof_pred = pd.DataFrame(index = df0.index) \n\nt00 = time.time(); t0 = time.time()\ndf_stat = pd.DataFrame()\ncnt4df_stat = 0\ndf_stat2 = pd.DataFrame()\ncnt4df_stat2 = 0\nfor cnt_target, target_name in enumerate(y_save.columns):#enumerate(['dopamine_receptor_antagonist']) : # y_save.columns):\n    # if target_name != 'dopamine_receptor_antagonist': continue\n    y = y_save[target_name]\n    df_stat_update_0( )\n    #    C = 0.003\n\n    list_models =  ['Const Predict','l1','l2']\n    list_preds = []\n    for model_id in list_models:\n        #print( model_id )\n        y_pred_oof, y_pred_submit = modeling( X, y, model_id , verbose = verbose)\n        list_preds.append( (y_pred_oof, y_pred_submit) )\n    df_stat_update_list_preds( list_preds )\n    \n    #y_pred_oof, y_pred_submit = blending( list_preds) \n    y_pred_oof, y_pred_submit = blending_simple( list_preds )    \n    df_stat_update_1( y_pred_oof, 'Blend' )\n \n    df_train_oof_pred.loc[:,target_name] = y_pred_oof\n    df_submit.loc[:,target_name] = y_pred_submit        \n    \n    str_inf = ''#  get_str_info( )\n    print(cnt_target,   target_name, str_inf, np.round(time.time()-t00,1), 'seconds' )\n    \ntotal_time = time.time()-t00    \n#df_stat.to_csv(\"df_stat.csv\")\nprint(np.round(total_time,0), np.round(total_time/60,0), np.round(total_time/3600,0),'seconds,minutes, hours total passed') ; \ndf_train_oof_pred.to_csv('df_train_oof_pred.csv')\ndf_stat2.to_csv('df_stat2.csv')\ndf_stat.to_csv('df_stat.csv')\n\ndf_submit\n\n#df_stat2.describe()\ndf_stat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.round(total_time,0), np.round(total_time/60,0), np.round(total_time/3600,0),'seconds,minutes, hours total passed') ; \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stat[['Blend Simple Used Const', 'Blend Simple 2 Used Const',]].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stat.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.round(total_time,0), np.round(total_time/60,0), np.round(total_time/3600,0),'seconds,minutes, hours total passed') ; \ndf_stat.to_csv('df_stat.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in y_save.columns:\n    #df_submit[f] = train_targets.loc[ train_features.cp_type=='trt_cp', f].mean()\n    df_submit.loc[ df0_test.cp_type!='trt_cp' ,f] = 0\ndf_submit#.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submit.to_csv(\"submission.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}