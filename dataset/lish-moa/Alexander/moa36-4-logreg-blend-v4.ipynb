{"cells":[{"metadata":{},"cell_type":"markdown","source":"Construct logregs on subfolds and blend , over different random splits to subfolds \n\nLogregs with \"optimized\" C for each target\n\n36_4 - blend many C  \n\nV4 - decrease n_blends to 1 from 2 ( number of blends for each C ) \n\nV3: Extend further C interval -  Your notebook was stopped because it exceeded the max allowed execution duration. (32401 seconds )\n\nV2 : twenty C : list4blend_change_optimal_C_by = 1.15 ** np.arange(-10,10) , Additionally each C point is double blended with different random splits to folds:  got CV 0.01543 - it is around top CV results obtained in that way. That is strange.\n\n\nv1 - ten C -> C * np.linspace(0.5,1,10)\n\n36_3 - blend three  C\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set param:\n# Set what C will be blended (relative to previosly chosen optimal C for each target)\n\nn_blends = 1\n\n\nimport numpy as np\n#np.linspace(0.5,1,10), 1.2**np.arange(-10,10) # [-2,-1,0,1,2,3,4]\n\nlist4blend_change_optimal_C_by = 1.3**np.arange(-12,20)\n\nnp.round(list4blend_change_optimal_C_by,2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# V6 - list_C is min from cv3 rs0,1,100 finer grid than before\n\nlist_C = [0.03, 0.003, 0.0002, 0.001, 0.0007, 0.001, 0.001, 0.002, 0.1, 0.005, 0.001, 0.01, 0.5, 0.01, 0.0007, 0.002, 0.001, 0.001, 0.001, 0.001, 0.002, 0.002, 0.007, 0.005, 0.005, 0.01, 0.005, 0.005, 0.003, 0.002, 0.01, 0.005, 0.002, 0.03, 0.0, 0.1, 0.005, 0.05, 0.02, 0.007, 0.003, 0.007, 0.005, 0.001, 0.003, 0.005, 0.02, 0.01, 0.01, 0.003, 0.001, 0.01, 0.02, 0.02, 0.001, 0.003, 0.005, 0.001, 0.007, 0.01, 0.0003, 0.0002, 0.005, 0.007, 0.002, 0.2, 0.005, 0.001, 0.002, 0.005, 0.005, 0.001, 0.002, 0.007, 0.007, 0.007, 0.005, 0.002, 0.003, 0.002, 0.02, 0.003, 0.0, 0.002, 0.005, 0.007, 0.05, 0.005, 0.02, 0.01, 0.05, 0.003, 0.007, 0.003, 0.002, 0.02, 0.005, 0.02, 0.002, 0.0007, 0.0007, 0.01, 0.002, 0.01, 0.002, 0.001, 0.1, 0.03, 0.005, 0.05, 0.02, 0.02, 0.01, 0.001, 0.003, 0.002, 0.01, 0.001, 0.02, 0.02, 0.005, 0.01, 0.003, 0.001, 0.003, 0.03, 0.07, 0.02, 0.002, 0.003, 0.0005, 0.002, 0.02, 0.007, 0.003, 0.005, 0.005, 0.01, 0.003, 0.003, 0.003, 0.01, 0.01, 0.002, 0.002, 0.0003, 0.02, 0.005, 0.01, 0.01, 0.01, 0.003, 0.003, 0.007, 0.007, 0.001, 0.0007, 0.01, 0.003, 0.007, 0.03, 0.003, 0.001, 1.0, 0.002, 0.02, 0.005, 0.0, 0.001, 0.05, 0.007, 0.05, 0.01, 0.03, 0.003, 0.007, 0.0005, 0.002, 0.005, 0.005, 0.002, 0.005, 0.001, 0.007, 0.007, 0.0007, 0.02, 0.005, 0.03, 0.005, 0.007, 0.001, 0.003, 0.005, 0.007, 0.002, 0.001, 0.02, 0.003, 0.01, 0.003, 0.02, 0.01, 0.0005, 0.1, 0.0007]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in list_C[:10]:\n    print(c, type(c))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport time\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport time\ndf = pd.read_csv('/kaggle/input/lish-moa/train_features.csv',index_col = 0)  \ndf0 = df.copy()\ndf['cp_type'] = df['cp_type'].map({'trt_cp':1.0, 'ctl_vehicle':1.0}) # Forget about control group  \ndf['cp_dose'] = df['cp_dose'].map({'D1':0.0, 'D2':1.0})\ndf['cp_time'] = df['cp_time'].map({24:0.0, 48: .5 , 72:1.0})\nX = df.copy()\nX_save = X.copy()\ndf_test = pd.read_csv('/kaggle/input/lish-moa/test_features.csv',index_col = 0)\ndf0_test = df_test.copy()\ndf_test['cp_type'] = df_test['cp_type'].map({'trt_cp':1.0, 'ctl_vehicle':0.0})\ndf_test['cp_dose'] = df_test['cp_dose'].map({'D1':0.0, 'D2':1.0})\ndf_test['cp_time'] = df_test['cp_time'].map({24:0.0, 48: .5 , 72:1.0})\n\ny = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv',index_col = 0 )\ny_save = y.copy()\nprint(y.iloc[:3,:2])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_save.sum(axis = 0 ).sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simulation core"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n\ndf_submit = pd.DataFrame(index = df_test.index)\ndf_train_oof_pred = pd.DataFrame(index = df0.index) \n\ndf_stat = pd.DataFrame()\nt00 = time.time()\ndf_stat = pd.DataFrame()\ncnt4df_stat = 0\nfor cnt_target, target_name in enumerate(y_save.columns):#enumerate(['dopamine_receptor_antagonist']) : # y_save.columns):\n    C = list_C[ cnt_target ]\n    y = y_save[target_name]\n    #if target_name == 'dopamine_receptor_antagonist':\n    #    C = 0.003\n    \n    if C != 0:\n        y_pred_submit = np.zeros( len(df_test) )\n        cnt_blend_submit = 0\n        y_pred_oof_blend = np.zeros_like(y,dtype = float)\n        cnt_blend_oof = 0\n        for C_effective in C*list4blend_change_optimal_C_by: # np.linspace(0.5,1,10): # np.array([0.5,1, 2]):\n            model = LogisticRegression( C = C_effective  ) #, penalty='l1', solver = 'liblinear' )  \n            for cnt in range(n_blends):\n                rs = np.random.randint(10**7)\n                skf = StratifiedKFold(n_splits=3, shuffle=True, random_state= rs )\n                y_pred_oof = np.zeros_like(y,dtype = float)\n                list_loss_train = []\n                for train_index, test_index in skf.split(X, y):\n                    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n                    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n                    model.fit(X_train,y_train)\n                    y_pred_oof[ test_index ] = model.predict_proba(X_test)[:,1]\n                    y_pred_submit = (y_pred_submit*cnt_blend_submit + model.predict_proba(df_test)[:,1])/(cnt_blend_submit+1) # blend\n                    cnt_blend_submit += 1\n\n                    list_loss_train = log_loss(y_train,  model.predict_proba(X_train)[:,1])\n\n                y_pred_oof_blend = (y_pred_oof_blend*cnt_blend_oof + y_pred_oof) / (cnt_blend_oof + 1) # blend\n                cnt_blend_oof += 1\n\n                df_stat.loc[cnt4df_stat,'Target'] = target_name\n                df_stat.loc[cnt4df_stat,'Count Blend'] = cnt_blend_oof\n                df_stat.loc[cnt4df_stat,'LogLoss OOF'] = log_loss(y, y_pred_oof )\n                df_stat.loc[cnt4df_stat,'LogLoss Blend OOF'] = log_loss(y, y_pred_oof_blend )\n                df_stat.loc[cnt4df_stat,'LogLoss Train'] = np.mean( list_loss_train )\n                df_stat.loc[cnt4df_stat,'Seconds Passed'] = np.round( time.time() - t00 )\n                df_stat.loc[cnt4df_stat,'Random Seed'] = rs\n                df_stat.loc[cnt4df_stat,'C'] = C\n                cnt4df_stat += 1\n        df_train_oof_pred.loc[:,target_name] = y_pred_oof_blend\n    else:\n        mn = y[df0.cp_type=='trt_cp'].mean()\n        print(mn, y.mean(), target_name, y.sum() )\n        y_pred_submit = np.ones_like(df_test.iloc[:,0])*mn # y.mean() \n        df_train_oof_pred.loc[:,target_name] = np.ones_like(df0.iloc[:,0])*mn\n    df_submit.loc[:,target_name] = y_pred_submit        \n    \n    print(cnt_target,   target_name, 'Blend', np.round(df_stat.loc[cnt4df_stat-1,'LogLoss Blend OOF'], 5) , 'No blend', np.round(df_stat.loc[cnt4df_stat-1,'LogLoss OOF'] ,5) )\n    \ntotal_time = time.time()-t00    \n#df_stat.to_csv(\"df_stat.csv\")\nprint(np.round(total_time,0), np.round(total_time/60,0),np.round(total_time/3600,1),'seconds, minutes, hours total passed')  \ndf_train_oof_pred.to_csv('df_train_oof_pred.csv')\ndf_submit\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.round(total_time,0), np.round(total_time/60,0),np.round(total_time/3600,1),'seconds, minutes, hours total passed')  \ndf_stat.to_csv('df_stat.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lc = list(  filter(lambda x: 'LogLoss' in x , df_stat.columns) )\n\nname_oof = lc[0]\ndf_stat2 = pd.DataFrame()\nfor i,c in enumerate(y_save.columns):\n    if c not in list(  df_stat['Target'] ) : continue\n    \n    m = df_stat['Target'] == c\n    df_stat2.loc[c,'Internal Numero'] = i \n    df_stat2.loc[c,'Target Sum'] = y_save[c].sum()\n    df_stat2.loc[c,'Blend gain * 1e5'] = np.round( - 1e5*( df_stat[m]['LogLoss Blend OOF'].iat[-1] - df_stat[m]['LogLoss OOF'].mean() ) , 1)\n    \n    df_stat2.loc[c,'Logloss predict by mean'] = log_loss(y_save[c], np.ones_like(y_save[c])*y_save[c].mean() ) \n    df_stat2.loc[c,'Logloss OOF'] = df_stat[m]['LogLoss OOF'].mean()\n    df_stat2.loc[c,'Logloss Blend OOF'] = df_stat[m]['LogLoss Blend OOF'].iat[-1]\n    df_stat2.loc[c, 'LogLoss Train'] = df_stat[m][ 'LogLoss Train'].mean()\n    \n    df_stat2.loc[c,'Logloss Std'] = df_stat[m]['LogLoss OOF'].std()\n    df_stat2.loc[c,'Logloss Train Std'] = df_stat[m]['LogLoss Train'].std()\n    #df_stat[m]\ndf_stat2['Internal Numero'] = df_stat2['Internal Numero'].astype(int)    \n\ndf_stat2.to_csv('df_stat2_by_targets_aggregated.csv')\n\ndf_stat2.sort_values('Blend gain * 1e5', ascending = False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df_stat2['Blend gain * 1e5'] > 0 ).sum() , 'out of 206',","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df_stat2['Blend gain * 1e5'] > 10 ).sum() , 'out of 206',","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df_stat2['Blend gain * 1e5'] > 50 ).sum() , 'out of 206',","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Intersting Plot - dependence of score on number of blends, compare with scores without blends"},{"metadata":{"trusted":true},"cell_type":"code","source":"lc = list(  filter(lambda x: 'LogLoss' in x , df_stat.columns) )\nd = df_stat.groupby('Count Blend')[lc].mean()\nfig = plt.figure(figsize = (15,6))\nfig.add_subplot(1,2,1)\nfor c in d.columns:\n    if 'Train' not in c:\n        plt.plot(d[c], '*-', label = c)\nplt.legend()\nplt.grid()\nfig.add_subplot(1,2,2)\nfor c in d.columns:\n    if 'Train' in c:\n        plt.plot(d[c], '*-', label = c)\nplt.legend()\nplt.grid()\nplt.show()\n#print( d[lc].mean() )\n#print(d.tail(1) )\nprint('No blend:', np.round( d['LogLoss OOF'].mean() , 5 )  , 'Blend', np.round( d.tail(1)['LogLoss Blend OOF'].iat[0], 5) )\nprint('Blend gain *1e5: ', np.round( (d['LogLoss OOF'].mean() -  d['LogLoss Blend OOF'].iat[-1])*1e5 , 0 ) )\nprint('Best blend result ',d['LogLoss Blend OOF'].min()  )\nprint('Best Blend gain *1e5: ', np.round( (d['LogLoss OOF'].mean() -  d['LogLoss Blend OOF'].min() )*1e5 , 0 ) )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stat.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in y_save.columns:\n    #df_submit[f] = train_targets.loc[ train_features.cp_type=='trt_cp', f].mean()\n    df_submit.loc[ df0_test.cp_type!='trt_cp' ,f] = 0\ndf_submit#.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submit.to_csv(\"submission.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}