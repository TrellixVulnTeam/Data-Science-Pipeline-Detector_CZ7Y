{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nimport tensorflow as tf\nfrom tensorflow.keras import layers,regularizers,Sequential,backend,callbacks,optimizers,metrics,Model,losses\nimport sys\nimport json\nimport gc\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom scipy.optimize import dual_annealing, minimize\n\nfrom tensorflow import keras\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TabNet\n!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet\n# Iterative Stratification\n!pip install /kaggle/input/iterative-stratification/iterative-stratification-master/\n\n### General ###\nimport os\nimport copy\nimport tqdm\nimport pickle\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n\n### Data Wrangling ###\nfrom scipy import stats\n\n### Data Visualization ###\nimport seaborn as sns\nplt.style.use(\"fivethirtyeight\")\n\n### Machine Learning ###\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold\n\n### Deep Learning ###\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.nn import functional as F\nfrom torch.nn.modules.loss import _WeightedLoss\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n# Tabnet \nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n### Make prettier the prints ###\nfrom colorama import Fore\nc_ = Fore.CYAN\nm_ = Fore.MAGENTA\nr_ = Fore.RED\nb_ = Fore.BLUE\ny_ = Fore.YELLOW\ng_ = Fore.GREEN\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 42\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nset_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Preprocessing Functions for each notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing for TabNet\ndef preprocessor_TabNet():\n    data_path = \"../input/lish-moa/\"\n    no_ctl = True\n    scale = \"rankgauss\"\n    variance_threshould = 0.7\n    decompo = \"PCA\"\n    ncompo_genes = 80\n    ncompo_cells = 10\n    encoding = \"dummy\"\n    \n    train = pd.read_csv(data_path + \"train_features.csv\")\n    targets = pd.read_csv(data_path + \"train_targets_scored.csv\")\n    test = pd.read_csv(data_path + \"test_features.csv\")\n    train_drug = pd.read_csv(data_path + \"train_drug.csv\")\n    submission = pd.read_csv(data_path + \"sample_submission.csv\")\n\n    if no_ctl:\n        # cp_type == ctl_vehicle\n        print(b_, \"not_ctl\")\n        train = train[train[\"cp_type\"] != \"ctl_vehicle\"]\n        test = test[test[\"cp_type\"] != \"ctl_vehicle\"]\n        targets = targets.iloc[train.index]\n        train.reset_index(drop = True, inplace = True)\n        test.reset_index(drop = True, inplace = True)\n        targets.reset_index(drop = True, inplace = True)\n\n    cols_numeric = [feat for feat in list(train.columns) if feat not in [\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"]]\n    mask = (train[cols_numeric].var() >= variance_threshould).values\n    tmp = train[cols_numeric].loc[:, mask]\n    train = pd.concat([train[[\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"]], tmp], axis = 1)\n    cols_numeric = [feat for feat in list(train.columns) if feat not in [\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"]]\n    test = pd.concat([test[[\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"]], test.loc[:,cols_numeric]], axis = 1)\n\n    GENES = [col for col in train.columns if col.startswith(\"g-\")]\n    CELLS = [col for col in train.columns if col.startswith(\"c-\")]\n\n    if scale == \"rankgauss\":\n        ### Rank Gauss ###\n        print(b_, \"Rank Gauss\")\n        for col in (GENES + CELLS):\n            transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")   # from optimal commit 9\n            vec_len = len(train[col].values)\n            vec_len_test = len(test[col].values)\n            raw_vec = train[col].values.reshape(vec_len, 1)\n            transformer.fit(raw_vec)\n        \n            train[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n            test[col] = transformer.transform(test[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n    else:\n        pass\n\n    if decompo == \"PCA\":\n        print(b_, \"PCA\")\n        GENES = [col for col in train.columns if col.startswith(\"g-\")]\n        CELLS = [col for col in train.columns if col.startswith(\"c-\")]\n    \n        pca_genes = PCA(n_components = ncompo_genes, random_state = seed)\n        pca_genes_train = pca_genes.fit_transform(train[GENES])\n    \n    \n    \n        pca_cells = PCA(n_components = ncompo_cells, random_state = seed)\n        pca_cells_train = pca_cells.fit_transform(train[CELLS])\n    \n        pca_genes_train = pd.DataFrame(pca_genes_train, columns = [f\"pca_g-{i}\" for i in range(ncompo_genes)])\n        pca_cells_train = pd.DataFrame(pca_cells_train, columns = [f\"pca_c-{i}\" for i in range(ncompo_cells)])\n        train = pd.concat([train, pca_genes_train, pca_cells_train], axis = 1)\n    \n        pca_genes_test = pca_genes.transform(test[GENES])\n        pca_cells_test = pca_cells.transform(test[CELLS])\n    \n        pca_genes_test = pd.DataFrame(pca_genes_test, columns = [f\"pca_g-{i}\" for i in range(ncompo_genes)])\n        pca_cells_test = pd.DataFrame(pca_cells_test, columns = [f\"pca_c-{i}\" for i in range(ncompo_cells)])\n        test = pd.concat([test, pca_genes_test, pca_cells_test], axis = 1)\n    \n    else:\n        pass\n\n    if encoding == \"dummy\":\n        print(b_, \"One-Hot\")\n        train = pd.get_dummies(train, columns = [\"cp_time\", \"cp_dose\"])\n        test = pd.get_dummies(test, columns = [\"cp_time\", \"cp_dose\"])\n    else:\n        pass\n# GENES = [col for col in train.columns if col.startswith(\"g-\")]\n# CELLS = [col for col in train.columns if col.startswith(\"c-\")]\n\n    for stats in tqdm.tqdm([\"sum\", \"mean\", \"std\", \"kurt\", \"skew\"]):\n        train[\"g_\" + stats] = getattr(train[GENES], stats)(axis = 1)\n        train[\"c_\" + stats] = getattr(train[CELLS], stats)(axis = 1)    \n        train[\"gc_\" + stats] = getattr(train[GENES + CELLS], stats)(axis = 1)\n    \n        test[\"g_\" + stats] = getattr(test[GENES], stats)(axis = 1)\n        test[\"c_\" + stats] = getattr(test[CELLS], stats)(axis = 1)    \n        test[\"gc_\" + stats] = getattr(test[GENES + CELLS], stats)(axis = 1)\n    \n    gsquarecols=['g-574','g-211','g-216','g-0','g-255','g-577',\n             'g-153','g-389','g-60','g-370','g-248','g-167',\n             'g-203','g-177','g-301','g-332','g-517','g-6',\n             'g-744','g-224','g-162','g-3','g-736','g-486',\n             'g-283','g-22','g-359','g-361','g-440','g-335',\n             'g-106','g-307','g-745','g-146','g-416','g-298',\n             'g-666','g-91','g-17','g-549','g-145','g-157','g-768','g-568','g-396']\n\n    for df in [train, test]:\n        df['c52_c42'] = df['c-52'] * df['c-42']\n        df['c13_c73'] = df['c-13'] * df['c-73']\n        df['c26_c13'] = df['c-23'] * df['c-13']\n        df['c33_c6'] = df['c-33'] * df['c-6']\n        df['c11_c55'] = df['c-11'] * df['c-55']\n        df['c38_c63'] = df['c-38'] * df['c-63']\n        df['c38_c94'] = df['c-38'] * df['c-94']\n        df['c13_c94'] = df['c-13'] * df['c-94']\n        df['c4_c52'] = df['c-4'] * df['c-52']\n        df['c4_c42'] = df['c-4'] * df['c-42']\n        df['c13_c38'] = df['c-13'] * df['c-38']\n        df['c55_c2'] = df['c-55'] * df['c-2']\n        df['c55_c4'] = df['c-55'] * df['c-4']\n        df['c4_c13'] = df['c-4'] * df['c-13']\n        df['c82_c42'] = df['c-82'] * df['c-42']\n        df['c66_c42'] = df['c-66'] * df['c-42']\n        df['c6_c38'] = df['c-6'] * df['c-38']\n        df['c2_c13'] = df['c-2'] * df['c-13']\n        df['c62_c42'] = df['c-62'] * df['c-42']\n        df['c90_c55'] = df['c-90'] * df['c-55']\n            \n        for feature in gsquarecols:\n            if feature in GENES:\n                df[f'{feature}_squared'] = df[feature] ** 2  \n        for feature in CELLS:\n            df[f'{feature}_squared'] = df[feature] ** 2  \n\n    train = train.merge(targets, on='sig_id')\n    train = train.merge(train_drug, on='sig_id')\n    target_cols = [x for x in targets.columns if x != 'sig_id']\n    \n    train_df = train\n    train_df.reset_index(drop=True, inplace=True)\n    test.reset_index(drop = True, inplace = True)\n    features_to_drop = [\"sig_id\", \"cp_type\"]\n\n    test.drop(features_to_drop, axis = 1, inplace = True)\n    feature_cols = [c for c in train_df.columns if c not in targets.columns]\n    feature_cols = [c for c in feature_cols if c not in [ 'sig_id', 'drug_id', 'cp_type']]\n\n    return train, targets, test, submission, target_cols, feature_cols ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, targets_TabNet, test_TabNet, submission_TabNet, target_cols, feature_cols = preprocessor_TabNet()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pd.set_option('display.max_columns', None)\n# print(train_df.shape)\n# display(train_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pd.set_option('display.max_columns', None)\n# display(targets_TabNet.head(3))\n# display(test_TabNet.head(3))\n# print(len(target_cols))\n# print(len(feature_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\ndef make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n    vc = train.drug_id.value_counts()\n    vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n    vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n\n    for seed_id in SEEDS:\n        kfold_col = 'kfold_{}'.format(seed_id)\n        \n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}\n        dct2 = {}\n\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        train[kfold_col] = train.drug_id.map(dct1)\n        train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n        train[kfold_col] = train[kfold_col].astype('int8')\n        \n    return train\n\nSEEDS = [x for x in range(42,49)]\nNFOLDS = 10\nDRUG_THRESH = 18\n\ntrain_df = make_cv_folds(train_df, SEEDS, NFOLDS, DRUG_THRESH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display(train_df.head(3))\n# print(train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TabNet Inference\ntrain_TabNet, targets_TabNet, test_TabNet, submission_TabNet\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TabNet inference\nimport json\nimport shutil\nimport zipfile\nimport io\n\nX_test = test_TabNet.values\n\n\nMAX_EPOCH = 200\ntabnet_params = dict(\n    n_d = 32,\n    n_a = 32,\n    n_steps = 1,\n    gamma = 1.3,\n    lambda_sparse = 0,\n    optimizer_fn = optim.Adam,\n    optimizer_params = dict(lr = 2e-2, weight_decay = 1e-5),\n    mask_type = \"entmax\",\n    scheduler_params = dict(\n        mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n    scheduler_fn = ReduceLROnPlateau,\n    seed = seed,\n    verbose = 10\n)\n\nclass LogitsLogLoss(Metric):\n    \"\"\"\n    LogLoss with sigmoid applied\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        \"\"\"\n        Compute LogLoss of predictions.\n\n        Parameters\n        ----------\n        y_true: np.ndarray\n            Target matrix or vector\n        y_score: np.ndarray\n            Score matrix or vector\n\n        Returns\n        -------\n            float\n            LogLoss of predictions vs targets.\n        \"\"\"\n        logits = 1 / (1 + np.exp(-y_pred))\n        aux = (1 - y_true) * np.log(1 - logits + 1e-15) + y_true * np.log(logits + 1e-15)\n        return np.mean(-aux)\n    \ntest_cv_preds = []\nfor s in SEEDS:\n    tabnet_params['seed'] = s\n#     mskf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, random_state = s, shuffle = True)\n    for fold_id in range(NFOLDS):\n        print(b_,\"FOLDS: \", r_, fold_id + 1, y_, 'seed:', tabnet_params['seed'])\n        print(g_, '*' * 60, c_)\n\n        saved_path_name = '../input/tabnetfe7seeds424810folds/TabNet_seed_'\\\n        +str(s)+'_fold_'+str(fold_id+1)\n\n        saved_file = '/kaggle/working/model'\n\n        shutil.make_archive(saved_file,'zip',saved_path_name)\n        print(f'Successfully saved model at {saved_file}.zip')\n\n        device = ('cuda' if torch.cuda.is_available() else 'cpu')\n        with zipfile.ZipFile('model.zip') as z:\n            with z.open('model_params.json') as f:\n                loaded_params = json.load(f)\n            with z.open('network.pt') as f:\n                try:\n                    saved_state_dict = torch.load(f, map_location=device)\n                except io.UnsupportedOperation:\n                    saved_state_dict = torch.load(io.bytesIO(f.read()),map_location = device)\n        loaded_model = TabNetRegressor(**loaded_params)\n        loaded_model._set_network()\n        loaded_model.network.load_state_dict(saved_state_dict)\n        loaded_model.network.eval()\n        \n\n        ### Predict on validation ###\n#         preds_val = model.predict(X_val)\n#         preds_val = loaded_model.predict(X_val)\n        # Apply sigmoid to the predictions\n#         preds = 1 / (1 + np.exp(-preds_val))\n#         score = np.min(model.history[\"val_logits_ll\"])\n#         if s == 42:\n#             print(type(preds_val), preds_val.shape, type(preds), preds.shape)\n        \n    \n        ### Save OOF for CV ###\n#         oof_preds.append(preds_val)\n#         oof_targets.append(y_val)\n#         if s == 42:\n#             print(f'{len(oof_preds)}*{len(oof_preds[0])}, {len(oof_targets)}*{len(oof_targets[0])}')\n#         scores.append(score)\n    \n        ### Predict on test ###\n        preds_test = loaded_model.predict(X_test)\n        test_cv_preds.append(1 / (1 + np.exp(-preds_test)))\n# print(len(oof_preds))\n# oof_preds_all = np.concatenate(oof_preds)\n# oof_targets_all = np.concatenate(oof_targets)\ntest_preds_all = np.stack(test_cv_preds)\n\nsaved_path_name = '../input/tabnetfe7seeds424810folds/'\noof_TabNet_all = np.load(saved_path_name + 'oof_TabNet_all.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = \"../input/lish-moa/\"\nall_feat = [col for col in submission_TabNet.columns if col not in [\"sig_id\"]]\n\n# To obtain the same lengh of test_preds_all and submission\ntest_TabNet = pd.read_csv(data_path + \"test_features.csv\")\nsig_id = test_TabNet[test_TabNet[\"cp_type\"] != \"ctl_vehicle\"].sig_id.reset_index(drop = True)\ntmp = pd.DataFrame(test_preds_all.mean(axis = 0), columns = all_feat)\ntmp[\"sig_id\"] = sig_id\n\nsubmission_TabNet = pd.merge(test_TabNet[[\"sig_id\"]], tmp, on = \"sig_id\", how = \"left\")\nsubmission_TabNet.fillna(0, inplace = True)\n\n#submission[all_feat] = tmp.mean(axis = 0)\n\n# Set control to 0\n#submission.loc[test[\"cp_type\"] == 0, submission.columns[1:]] = 0\nsubmission_TabNet.to_csv(\"submission_TabNet.csv\", index = None)\nsubmission_TabNet.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"{b_}submission_TabNet.shape: {r_}{submission_TabNet.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ndel targets_TabNet, test_TabNet, tmp, train_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nseed_everything(seed)\n\ndef preprocessor_nn_transfer():\n    variance_threshould = 0.8\n    ncompo_genes = 600\n    ncompo_cells = 50\n    \n    data_dir = '../input/lish-moa/'\n    train_features = pd.read_csv(data_dir + 'train_features.csv')\n    train_targets_scored = pd.read_csv(data_dir + 'train_targets_scored.csv')\n    train_targets_nonscored = pd.read_csv(data_dir + 'train_targets_nonscored.csv')\n    train_drug = pd.read_csv(data_dir + 'train_drug.csv')\n    test_features = pd.read_csv(data_dir + 'test_features.csv')\n    sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n    \n    train_features = train_features[train_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n    test_features = test_features[test_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n    # drop cp_type\n    train_features = train_features.drop('cp_type', axis=1)\n    test_features = test_features.drop('cp_type', axis=1)\n\n    \n    GENES = [col for col in train_features.columns if col.startswith('g-')]\n    CELLS = [col for col in train_features.columns if col.startswith('c-')]\n    \n    # Rank Gauss\n    for col in (GENES + CELLS):\n        transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n        vec_len = len(train_features[col].values)\n        vec_len_test = len(test_features[col].values)\n        raw_vec = train_features[col].values.reshape(vec_len, 1)\n        transformer.fit(raw_vec)\n\n        train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n        test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n    \n#     print('Rank Gauss')\n#     print('train_features: {}'.format(train_features.shape))\n#     print('test_features: {}'.format(test_features.shape))\n    \n    # PCA\n    # GENES\n    pca_genes = PCA(n_components = ncompo_genes, random_state = seed)\n    pca_genes_train = pca_genes.fit_transform(train_features[GENES])\n    # CELLS\n    pca_cells = PCA(n_components = ncompo_cells, random_state = seed)\n    pca_cells_train = pca_cells.fit_transform(train_features[CELLS])\n    #train\n    pca_genes_train = pd.DataFrame(pca_genes_train, columns = [f\"pca_g-{i}\" for i in range(ncompo_genes)])\n    pca_cells_train = pd.DataFrame(pca_cells_train, columns = [f\"pca_c-{i}\" for i in range(ncompo_cells)])\n    train_features = pd.concat([train_features, pca_genes_train, pca_cells_train], axis = 1)\n    #test\n    pca_genes_test = pca_genes.transform(test_features[GENES])\n    pca_cells_test = pca_cells.transform(test_features[CELLS])\n    \n    pca_genes_test = pd.DataFrame(pca_genes_test, columns = [f\"pca_g-{i}\" for i in range(ncompo_genes)])\n    pca_cells_test = pd.DataFrame(pca_cells_test, columns = [f\"pca_c-{i}\" for i in range(ncompo_cells)])\n    test_features = pd.concat([test_features, pca_genes_test, pca_cells_test], axis = 1)\n\n#     print('\\n\\nPCA')\n#     print('train_features: {}'.format(train_features.shape))\n#     print('test_features: {}'.format(test_features.shape))\n    \n    # feature selection\n    cols_numeric = [feat for feat in list(train_features.columns) if feat not in [\"sig_id\", \"cp_time\", \"cp_dose\"]]\n    mask = (train_features[cols_numeric].var() >= variance_threshould).values\n    tmp = train_features[cols_numeric].loc[:, mask]\n    train_features = pd.concat([train_features[[\"sig_id\", \"cp_time\", \"cp_dose\"]], tmp], axis = 1)\n    cols_numeric = [feat for feat in list(train_features.columns) if feat not in [\"sig_id\", \"cp_time\", \"cp_dose\"]]\n    test_features = pd.concat([test_features[[\"sig_id\", \"cp_time\", \"cp_dose\"]], test_features.loc[:,cols_numeric]], axis = 1)\n    \n    # one hot\n    train_features = pd.get_dummies(train_features, columns = ['cp_time', 'cp_dose'])\n    test_features = pd.get_dummies(test_features, columns = ['cp_time', 'cp_dose'])\n\n#     print('\\n\\nFeature selection')\n#     print('train_features: {}'.format(train_features.shape))\n#     print('test_features: {}'.format(test_features.shape))\n    \n    # Join\n    train = train_features.merge(train_targets_scored, on='sig_id')\n    train = train.merge(train_targets_nonscored, on='sig_id')\n    train = train.merge(train_drug, on='sig_id')\n    test = test_features\n\n#     print('\\n\\nJoin')\n#     print('train: {}'.format(train.shape))\n#     print('test: {}'.format(test.shape))\n    \n    \n    target_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\n    aux_target_cols = [x for x in train_targets_nonscored.columns if x != 'sig_id']\n    all_target_cols = target_cols + aux_target_cols\n\n    num_targets = len(target_cols)\n    num_aux_targets = len(aux_target_cols)\n    num_all_targets = len(all_target_cols)\n\n#     print('\\n\\nnum_targets: {}'.format(num_targets))\n#     print('num_aux_targets: {}'.format(num_aux_targets))\n#     print('num_all_targets: {}'.format(num_all_targets))\n#     print('\\n\\n')\n#     print(train.shape)\n#     print(test.shape)\n#     print(sample_submission.shape)\n#     display(train.head())\n#     print(train.columns.to_list())\n\n    return train, test, num_targets, num_aux_targets, num_all_targets, target_cols, aux_target_cols, all_target_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test, num_targets, num_aux_targets, num_all_targets, target_cols, aux_target_cols, all_target_cols = preprocessor_nn_transfer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)\n        }\n        \n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n\n        return dct\n    \n\ndef train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    return final_loss\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n\n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    return preds\n\n\nimport torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            \n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n    \nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(Model, self).__init__()\n        self.hidden_size = [1500, 1250, 1000, 750]\n        self.dropout_value = [0.5, 0.35, 0.3, 0.25]\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n        \n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_value[0])\n        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_value[1])\n        self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n\n        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n        self.dropout4 = nn.Dropout(self.dropout_value[2])\n        self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n\n        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n        self.dropout5 = nn.Dropout(self.dropout_value[3])\n        self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.leaky_relu(self.dense3(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = F.leaky_relu(self.dense4(x))\n\n        x = self.batch_norm5(x)\n        x = self.dropout5(x)\n        x = self.dense5(x)\n        return x\n    \nclass FineTuneScheduler:\n    def __init__(self, epochs):\n        self.epochs = epochs\n        self.epochs_per_step = 0\n        self.frozen_layers = []\n\n    def copy_without_top(self, model, num_features, num_targets, num_targets_new):\n        self.frozen_layers = []\n\n        model_new = Model(num_features, num_targets)\n        model_new.load_state_dict(model.state_dict())\n\n        # Freeze all weights\n        for name, param in model_new.named_parameters():\n            layer_index = name.split('.')[0][-1]\n\n            if layer_index == 5:\n                continue\n\n            param.requires_grad = False\n\n            # Save frozen layer names\n            if layer_index not in self.frozen_layers:\n                self.frozen_layers.append(layer_index)\n\n        self.epochs_per_step = self.epochs // len(self.frozen_layers)\n\n        # Replace the top layers with another ones\n        model_new.batch_norm5 = nn.BatchNorm1d(model_new.hidden_size[3])\n        model_new.dropout5 = nn.Dropout(model_new.dropout_value[3])\n        model_new.dense5 = nn.utils.weight_norm(nn.Linear(model_new.hidden_size[-1], num_targets_new))\n        model_new.to(DEVICE)\n        return model_new\n\n    def step(self, epoch, model):\n        if len(self.frozen_layers) == 0:\n            return\n\n        if epoch % self.epochs_per_step == 0:\n            last_frozen_index = self.frozen_layers[-1]\n            \n            # Unfreeze parameters of the last frozen layer\n            for name, param in model.named_parameters():\n                layer_index = name.split('.')[0][-1]\n\n                if layer_index == last_frozen_index:\n                    param.requires_grad = True\n\n            del self.frozen_layers[-1]  # Remove the last layer as unfrozen","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\n# HyperParameters\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 24\nBATCH_SIZE = 128\n\nWEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\nMAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\nDIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\nPCT_START = 0.1\n\nSEEDS = 7\nNFOLDS = 7\nDRUG_THRESH = 18\n\nfeature_cols = [c for c in train.columns if c not in all_target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id', 'drug_id']]\nnum_features = len(feature_cols)\n# Show model architecture\nmodel = Model(num_features, num_all_targets)\n# model\n\n\ndef make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n    vc = train.drug_id.value_counts()\n    vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n    vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n\n    for seed_id in range(SEEDS):\n        kfold_col = 'kfold_{}'.format(seed_id)\n        \n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}\n        dct2 = {}\n\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        train[kfold_col] = train.drug_id.map(dct1)\n        train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n        train[kfold_col] = train[kfold_col].astype('int8')\n        \n    return train\n\n\n\ntrain = make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH)\ndisplay(train.head())\n\ndef run_training(fold_id, seed_id):\n    seed_everything(seed_id)\n    \n    train_ = train\n    test_ = test\n    \n    kfold_col = f'kfold_{seed_id}'\n    trn_idx = train_[train_[kfold_col] != fold_id].index\n    val_idx = train_[train_[kfold_col] == fold_id].index\n    \n    train_df = train_[train_[kfold_col] != fold_id].reset_index(drop=True)\n    valid_df = train_[train_[kfold_col] == fold_id].reset_index(drop=True)\n    \n    # Load the fine-tuned model with the best loss\n    model = Model(num_features, num_targets)\n    model.load_state_dict(torch.load(f\"../input/nn-transfer-learning-oof/SCORED_ONLY_SEED_{seed_id}_FOLD{fold_id}_.pth\"))\n    model.to(DEVICE)\n\n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    predictions = np.zeros((len(test_), num_targets))\n    predictions = inference_fn(model, testloader, DEVICE)\n#     return oof, predictions\n    print(f\"Pretrained_Model_Loaded_SCORED_ONLYSEED: {seed_id}, FOLD: {fold_id} \")\n    return 0.02, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed_id):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold_id in range(NFOLDS):\n        oof_, pred_ = run_training(fold_id, seed_id)\n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions\n\nfrom time import time\n\n# Averaging on multiple SEEDS\nSEED = [0, 1, 2, 3, 4, 5, 6]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\ntime_begin = time()\n\nfor seed_id in SEED:\n    oof_, predictions_ = run_k_fold(NFOLDS, seed_id)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n\ntime_diff = time() - time_begin\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions\n\nfrom datetime import timedelta\nstr(timedelta(seconds=time_diff))\n\ndata_dir = '../input/lish-moa/'\ntrain_targets_scored = pd.read_csv(data_dir + 'train_targets_scored.csv')\ntrain_targets_scored.head()\n\nvalid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\n\nfor i in range(len(target_cols)):\n    score += log_loss(y_true[:, i], y_pred[:, i])\n\nprint(\"CV log_loss: \", score / y_pred.shape[1])\n\nsample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\nsub_nn_transfer = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub_nn_transfer.to_csv('submission_nn_transfer.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"saved_path_name = '../input/nn-transfer-learning-oof/'\noof_nn_transfer = np.load(saved_path_name + 'oof_nn_transfer_all.npy')\noof_nn_transfer.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del sample_submission, test, train, train_targets_scored, valid_results \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sub_nn_transfer.shape)\ndisplay(sub_nn_transfer.head())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def readin_data_resnet():\n    # Import train data, drop sig_id, cp_type\n\n    train_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\n\n    train_drug = pd.read_csv('../input/lish-moa/train_drug.csv')\n    non_ctl_idx = train_features.loc[train_features['cp_type']!='ctl_vehicle'].index.to_list()\n    train_features = train_features.drop(['sig_id','cp_type','cp_dose','cp_time'],axis=1)\n\n    train_targets_scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\n\n    targets_drug = train_targets_scored.merge(train_drug, on='sig_id')\n    train_targets_scored = train_targets_scored.drop('sig_id',axis=1)\n    #targets_drug = targets_drug.drop('sig_id',axis=1)\n    labels_train = train_targets_scored.values\n\n    # Drop training data with ctl vehicle\n\n    train_features = train_features.iloc[non_ctl_idx]\n    labels_train = labels_train[non_ctl_idx]\n    targets_drug = targets_drug.iloc[non_ctl_idx]\n    # Import test data\n\n    test_features = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\n    test_features = test_features.drop(['sig_id','cp_dose','cp_time'],axis=1)\n\n    # Import predictors from public kernel\n\n    json_file_path = '../input/ttestpcarfelogisticregression/main_predictors.json'\n\n    with open(json_file_path, 'r') as j:\n        predictors = json.loads(j.read())\n        predictors = predictors['start_predictors']\n    cs = train_features.columns.str.contains('c-')\n    gs = train_features.columns.str.contains('g-')\n    return train_features,labels_train,test_features,targets_drug,predictors,cs,gs\n\n#target_cols = train_targets_scored.drop('sig_id', axis=1).columns.values.tolist()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create g-mean, c-mean, genes_pca (2 components), cells_pca (all components)\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))\n\ndef resnet_preprocessor(train,test,cs,gs):\n    \n    \n    # PCA\n    \n    n_gs = 2 # No of PCA comps to include\n    n_cs = 100 # No of PCA comps to include\n    \n    pca_cs = PCA(n_components = n_cs)\n    pca_gs = PCA(n_components = n_gs)\n\n    train_pca_gs = pca_gs.fit_transform(train[:,gs])\n    train_pca_cs = pca_cs.fit_transform(train[:,cs])\n    test_pca_gs = pca_gs.transform(test[:,gs])\n    test_pca_cs = pca_cs.transform(test[:,cs])\n    \n    # c-mean, g-mean\n    \n    train_c_mean = train[:,cs].mean(axis=1)\n    test_c_mean = test[:,cs].mean(axis=1)\n    train_g_mean = train[:,gs].mean(axis=1)\n    test_g_mean = test[:,gs].mean(axis=1)\n    \n    # Append Features\n    \n    train = np.concatenate((train,train_pca_gs,train_pca_cs,train_c_mean[:,np.newaxis]\n                            ,train_g_mean[:,np.newaxis]),axis=1)\n    test = np.concatenate((test,test_pca_gs,test_pca_cs,test_c_mean[:,np.newaxis],\n                           test_g_mean[:,np.newaxis]),axis=1)\n    \n    # Scaler for numerical values\n\n    # Scale train data\n    scaler = preprocessing.QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\") \n\n    train = scaler.fit_transform(train)\n\n    # Scale Test data\n    test = scaler.transform(test)\n    \n    return train, test\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def resnet_model(n_features, n_features_2, n_labels, label_smoothing = 0.0005):    \n    input_1 = layers.Input(shape = (n_features,), name = 'Input1')\n    input_2 = layers.Input(shape = (n_features_2,), name = 'Input2')\n\n    head_1 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.25),\n        layers.Dense(512, activation=\"elu\"), \n        layers.BatchNormalization(),\n        layers.Dense(256, activation = \"elu\")\n        ],name='Head1') \n\n    input_3 = head_1(input_1)\n    input_3_concat = layers.Concatenate()([input_2, input_3])\n\n    head_2 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(512, \"relu\"),\n        layers.BatchNormalization(),\n        layers.Dense(512, \"elu\"),\n        layers.BatchNormalization(),\n        layers.Dense(256, \"relu\"),\n        layers.BatchNormalization(),\n        layers.Dense(256, \"elu\")\n        ],name='Head2')\n\n    input_4 = head_2(input_3_concat)\n    input_4_avg = layers.Average()([input_3, input_4]) \n\n    head_3 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dense(256, kernel_initializer='lecun_normal', activation='selu'),\n        layers.BatchNormalization(),\n        layers.Dense(n_labels, kernel_initializer='lecun_normal', activation='selu'),\n        layers.BatchNormalization(),\n        layers.Dense(n_labels, activation=\"sigmoid\")\n        ],name='Head3')\n\n    output = head_3(input_4_avg)\n\n\n    model = Model(inputs = [input_1, input_2], outputs = output)\n    model.compile(optimizer='adam', loss=losses.BinaryCrossentropy(label_smoothing=label_smoothing), metrics=logloss)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def resnet_make_cv_folds(targets_drug,target_cols, SEEDS, NFOLDS, DRUG_THRESH):\n    vc = targets_drug.drug_id.value_counts()\n    vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n    vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n   # target_cols = targets_drug.drop(['sig_id','drug_id'], axis=1).columns.values.tolist()\n\n    for seed_id in SEEDS:\n        kfold_col = 'kfold_{}'.format(seed_id)\n        \n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}\n        dct2 = {}\n\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = targets_drug.groupby('drug_id')[target_cols].mean().loc[vc1]\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = targets_drug.loc[targets_drug.drug_id.isin(vc2)].reset_index(drop=True)\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        targets_drug[kfold_col] = targets_drug.drug_id.map(dct1)\n        targets_drug.loc[targets_drug[kfold_col].isna(), kfold_col] = targets_drug.loc[targets_drug[kfold_col].isna(), 'sig_id'].map(dct2)\n        targets_drug[kfold_col] = targets_drug[kfold_col].astype('int8')\n        \n    return targets_drug\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate Seeds\n\ndef predict_resnet(train_features,test_features,labels_train, targets_drug1,SEEDS, NFOLDS,cs,gs):\n    n_labels = labels_train.shape[1]\n    n_train = train_features.shape[0]\n    n_test = test_features.shape[0]\n    p_min = 0.0005\n    p_max = 0.9995\n   \n    n_seeds = len(SEEDS)\n\n    n_folds = NFOLDS\n    y_pred = np.zeros((n_test,n_labels))\n    #oof = tf.constant(0.0)\n    oof = np.zeros((n_train,n_labels))\n    #oof = torch.zeros(n_train, n_labels)\n    #hists = []\n   # for fold in kfolds:\n    #X_train, X_test = preprocessor(train_features[train].values,\n    #                                       train_features[test].values)\n    train_all,data_test = resnet_preprocessor(train_features.values,\n                                       test_features.drop('cp_type',axis=1).values,cs,gs)\n        \n    for seed in SEEDS:\n        kfold_col = 'kfold_{}'.format(seed)\n        #kf = KFold(n_splits=n_folds,shuffle=True,random_state=seed)\n        kf = targets_drug1[kfold_col]\n        for fold in range(NFOLDS):\n            #train = targets_drug1[targets_drug1[kfold_col] != fold].index\n            #test = targets_drug1[targets_drug1[kfold_col] == fold].index\n            train = targets_drug1[kfold_col] != fold\n            test = targets_drug1[kfold_col] == fold\n            #print(train)\n           # print(train_features.shape)\n       # for train, test in kf.split(train_features):\n            #X_train, X_test = preprocessor(train_features[train].values,\n            #                              train_features[test].values)\n            X_train, X_test = train_all[train],train_all[test]\n            #_,data_test = preprocessor(train_features[train].values,\n             #                          test_features.drop('cp_type',axis=1).values)\n            X_train_2 = train_features[train][predictors].values\n            X_test_2 = train_features[test][predictors].values\n            data_test_2 = test_features[predictors].values\n            y_train = labels_train[train]\n            y_test = labels_train[test]\n            n_features = X_train.shape[1]\n            n_features_2 = X_train_2.shape[1]\n            \n           # model = resnet_model(n_features, n_features_2, n_labels)\n            #reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.1, patience=2, mode='min', min_lr=1E-5)\n            #early_stopping = callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=10, mode='min',restore_best_weights=True)\n\n            #hist = model.fit([X_train,X_train_2],y_train, batch_size=128, epochs=192,verbose=0,validation_data = ([X_test,X_test_2],y_test),\n             #                callbacks=[reduce_lr, early_stopping])\n           # hists.append(hist)\n\n            # Save Model\n            #model.save('/kaggle/input/multi-input-resnet-model/TwoHeads_seed_'+str(seed)+'_fold_'+str(fold))\n            model = keras.models.load_model('/kaggle/input/resnet-001843/TwoHeads_seed_'+str(seed)+'_fold_'+str(fold), custom_objects={'logloss':logloss})\n            # OOF Score\n            y_val = model.predict([X_test,X_test_2])\n            #print(test)\n            #oof[test] += tf.constant(y_val,dtype=tf.float32)/n_seeds\n            #oof[test] += torch.clamp(torch.sigmoid(tf.constant(y_val,dtype=tf.float32)), p_min, p_max) / nseed\n            oof[test] += y_val/ n_seeds\n\n\n\n            #oof += logloss(tf.constant(y_test,dtype=tf.float32),tf.constant(y_val,dtype=tf.float32))/(n_folds*n_seeds)\n\n            # Run prediction\n            y_pred += model.predict([data_test,data_test_2])/(n_folds*n_seeds)\n    #cv_score = F.binary_cross_entropy(oof, labels_train)\n    #print('{} folds cv_score: {:.5f}'.format(nfold, cv_score))\n    return oof,y_pred\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#oof, test_pred = predict_nn_model(train,test,targets)\ntrain_features,labels_train,test_features,targets_drug,predictors,cs,gs = readin_data_resnet()\np_min = 0.0005\np_max = 0.9995\nDRUG_THRESH = 18\nn_seeds = 7\nnp.random.seed(42)\nSEEDS = np.random.randint(0,100,size=n_seeds)\n#SEEDS = [0,42]\nNFOLDS = 10\ntarget_cols = targets_drug.drop(['sig_id','drug_id'], axis=1).columns.values.tolist()\ntargets_drug_copy = targets_drug.copy()\ntargets_drug1 = resnet_make_cv_folds(targets_drug_copy,target_cols, SEEDS, NFOLDS, DRUG_THRESH)\n\noof_rs, test_pred_rs = predict_resnet(train_features,test_features,labels_train, targets_drug1,SEEDS, NFOLDS,cs,gs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_rs = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\nsub_rs.iloc[:,1:] = np.clip(test_pred_rs,p_min,p_max)\n\n# Set ctl_vehicle to 0\nsub_rs.iloc[test_features['cp_type'] == 'ctl_vehicle',1:] = 0\n\n# Save Submission\nsub_rs.to_csv('submission_rs.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%who DataFrame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del targets_drug, targets_drug1, targets_drug_copy, train_features\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## multi-heads\n\ntrain_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ntrain_drug = pd.read_csv('../input/lish-moa/train_drug.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \n\ndef remove_ctl(no_ctl,train,test,targets):\n    if no_ctl:\n        # 删掉 cp_type==ctl_vehicle 的样本\n        print('not_ctl')\n        train = train[train['cp_type']!='ctl_vehicle']\n        test = test[test['cp_type']!='ctl_vehicle']\n        targets = targets.iloc[train.index]\n        train.reset_index(drop=True, inplace=True)\n        test.reset_index(drop=True, inplace=True)\n        targets.reset_index(drop=True, inplace=True)\n\n    return train,test,targets\n\ndef remove_small_variance(variance_threshould,train,test):\n    cols_numeric = [feat for feat in list(train.columns) if feat not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]\n    mask = (train[cols_numeric].var() >= variance_threshould).values\n    tmp_train = train[cols_numeric].loc[:, mask]\n    tmp_test = test[cols_numeric].loc[:, mask]\n    train = pd.concat([train[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp_train], axis=1)\n    test = pd.concat([test[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp_test], axis=1)\n    \n    return train,test\n\ndef pca_process(ncompo_genes,ncompo_cells,train,test):\n\n    GENES = [col for col in train.columns if col.startswith('g-')]\n    CELLS = [col for col in train.columns if col.startswith('c-')]\n\n    pca_genes = PCA(n_components=ncompo_genes, random_state=base_seed).fit(train[GENES])\n    pca_cells = PCA(n_components=ncompo_cells, random_state=base_seed).fit(train[CELLS])\n    train_pca_gene = pca_genes.transform(train[GENES])\n    test_pca_gene = pca_genes.transform(test[GENES])\n    train_pca_cell = pca_cells.transform(train[CELLS])\n    test_pca_cell = pca_cells.transform(test[CELLS])\n    train_pca_gene = pd.DataFrame(train_pca_gene, columns=[f'pca_g-{i}' for i in range(ncompo_genes)])\n    test_pca_gene = pd.DataFrame(test_pca_gene, columns=[f'pca_g-{i}' for i in range(ncompo_genes)])\n    train_pca_cell = pd.DataFrame(train_pca_cell, columns=[f'pca_c-{i}' for i in range(ncompo_cells)])\n    test_pca_cell = pd.DataFrame(test_pca_cell, columns=[f'pca_c-{i}' for i in range(ncompo_cells)])\n\n    #pca_genes = pd.DataFrame(pca_genes, columns=[f'pca_g-{i}' for i in range(ncompo_genes)])\n    #pca_cells = pd.DataFrame(pca_cells, columns=[f'pca_c-{i}' for i in range(ncompo_cells)])\n    #data_all = pd.concat([data_all, pca_genes, pca_cells], axis=1)\n    train_df = pd.concat([train, train_pca_gene, train_pca_cell], axis=1)\n    test_df = pd.concat([test, test_pca_gene, test_pca_cell], axis=1)\n    return train_df,test_df\n\n\ndef scale_data(scale, train_df,test_df):\n    if scale == 'minmax':\n        # 归一化\n        print('minmax')\n        data_all[cols_numeric] = data_all[cols_numeric].apply(scale_minmax, axis=0)\n\n    elif scale == 'rankgauss':\n        # RankGauss\n        print('rankgauss')\n        cols_numeric = [feat for feat in list(train_df.columns) if feat not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose'] and \"pca_\" not in feat]\n        #scaler = GaussRankScaler()\n        scaler = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\") \n        train_df[cols_numeric] = scaler.fit_transform(train_df[cols_numeric])\n        test_df[cols_numeric] = scaler.transform(test_df[cols_numeric])\n\n    else:\n        pass\n    return train_df,test_df\n\ndef add_features(train,test):\n    \n    GENES = [col for col in train.columns if col.startswith('g-')]\n    CELLS = [col for col in train.columns if col.startswith('c-')]\n    for stats in tqdm.tqdm(['sum', 'mean', 'std', 'kurt', 'skew']):\n        train['g_'+stats] = getattr(train[GENES], stats)(axis=1)\n        train['c_'+stats] = getattr(train[CELLS], stats)(axis=1)\n        train['gc_'+stats] = getattr(train[GENES+CELLS], stats)(axis=1)\n        test['g_'+stats] = getattr(test[GENES], stats)(axis=1)\n        test['c_'+stats] = getattr(test[CELLS], stats)(axis=1)\n        test['gc_'+stats] = getattr(test[GENES+CELLS], stats)(axis=1)\n    return train,test\n\ndef process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data\n\ndef data_processing(train,test, targets):\n    train,test,targets = remove_ctl(no_ctl,train,test,targets)\n    print(\"remove control: \", train.shape,test.shape)\n    train,test = remove_small_variance(variance_threshould,train,test)\n    print(\"remove small_var: \", train.shape,test.shape)\n    train_df,test_df = pca_process(ncompo_genes,ncompo_cells,train,test)\n    print(\"do  PCA:  \", train_df.shape,test_df.shape)\n    train_df,test_df = scale_data(scale, train_df,test_df)\n    print(\"rankGauss: \", train_df.shape,test_df.shape)\n    train_df,test_df = add_features(train_df,test_df)\n    print(\"add stats feature:\", train_df.shape,test_df.shape)\n    \n    return train_df,test_df,targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n    vc = train.drug_id.value_counts()\n    vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n    vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n\n    for seed_id in range(SEEDS):\n        kfold_col = 'kfold_{}'.format(seed_id)\n        \n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}\n        dct2 = {}\n\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        train[kfold_col] = train.drug_id.map(dct1)\n        train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n        train[kfold_col] = train[kfold_col].astype('int8')\n        \n    return train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_ctl = True\nscale = 'rankgauss'\nvariance_threshould = 0.8\ndecompo = 'PCA'\nncompo_genes = 50\nncompo_cells = 8\nencoding = 'dummy'\nbase_seed = 42\n\n# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nprint(DEVICE)\nEPOCHS = 20\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = True\n\nhidden_size=1024\n\n# for KFold parameters\nSEEDS = 7\nSEED = [0, 1, 2, 3, 4, 5, 6]\nDRUG_THRESH = 18","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything(base_seed)\ntrain, test, targets = data_processing(train_features,test_features, train_targets_scored) \n\ntarget_cols = train_targets_scored.drop('sig_id', axis=1).columns.values.tolist()\nprint(len(target_cols))\n\ntrain_comb = train.merge(targets, on='sig_id')\ntrain_comb.drop('cp_type', axis=1, inplace=True)\n\nprint(\"combine train and targets to prepare for kfolds \", train_comb.shape)\n\nfeature_cols = [c for c in process_data(train_comb).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id', 'drug_id']]\nprint(len(feature_cols))\n\ngen_cols = [c for c in feature_cols if 'g-' == c[:2]]\ncell_cols = [c for c in feature_cols if 'c-' == c[:2]]\npca_gen_cols = [c for c in feature_cols if 'pca_g' == c[:5]]\npca_cell_cols = [c for c in feature_cols if 'pca_c' == c[:5]]\nohe_cols = [c for c in feature_cols if c not in gen_cols+cell_cols+pca_gen_cols+pca_cell_cols]\nprint(len(gen_cols), len(cell_cols), len(pca_gen_cols), len(pca_cell_cols),len(ohe_cols))\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\n\nfolds = train_comb.copy()\nfolds = folds.merge(train_drug, on='sig_id')\nfolds = make_cv_folds(folds, SEEDS, NFOLDS, DRUG_THRESH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_gen_features, num_cell_features,num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.gen_batch_norm1 = nn.BatchNorm1d(num_gen_features)\n        self.gen_dropout1 = nn.Dropout(0.2)\n        self.gen_dense1 = nn.utils.weight_norm(nn.Linear(num_gen_features, hidden_size))\n        self.gen_batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.gen_dropout2 = nn.Dropout(0.3)\n        self.gen_dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.cell_batch_norm1 = nn.BatchNorm1d(num_cell_features)\n        self.cell_dropout1 = nn.Dropout(0.2)\n        self.cell_dense1 = nn.utils.weight_norm(nn.Linear(num_cell_features, int(hidden_size/2)))\n        self.cell_batch_norm2 = nn.BatchNorm1d(int(hidden_size/2))\n        self.cell_dropout2 = nn.Dropout(0.3)\n        self.cell_dense2 = nn.utils.weight_norm(nn.Linear(int(hidden_size/2), int(hidden_size/2)))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size + int(hidden_size/2) )\n        self.dropout3 = nn.Dropout(0.25)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size + int(hidden_size/2), num_targets))\n        \n        \n    \n    def forward(self, gen_x, cell_x):\n#         print(cell_x.shape)\n        gen_x = self.gen_batch_norm1(gen_x)\n        gen_x = self.gen_dropout1(gen_x)\n        gen_x = F.relu(self.gen_dense1(gen_x))\n        gen_x = self.gen_batch_norm2(gen_x)\n        gen_x = self.gen_dropout2(gen_x)\n        gen_x = F.relu(self.gen_dense2(gen_x))\n\n        cell_x = self.cell_batch_norm1(cell_x)\n        cell_x = self.cell_dropout1(cell_x)\n        cell_x = F.relu(self.cell_dense1(cell_x))\n        cell_x = self.cell_batch_norm2(cell_x)\n        cell_x = self.cell_dropout2(cell_x)\n        cell_x = F.relu(self.cell_dense2(cell_x))\n                \n        x = torch.cat((gen_x,cell_x),dim=1)\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, gen_features, cell_features, targets):\n        self.gen_features = gen_features\n        self.cell_features = cell_features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.gen_features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'gen_x' : torch.tensor(self.gen_features[idx, :], dtype=torch.float),\n            'cell_x' : torch.tensor(self.cell_features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, gen_features, cell_features):\n        self.gen_features = gen_features\n        self.cell_features = cell_features\n        \n    def __len__(self):\n        return (self.gen_features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'gen_x' : torch.tensor(self.gen_features[idx, :], dtype=torch.float),\n            'cell_x' : torch.tensor(self.cell_features[idx, :], dtype=torch.float)  \n        }\n        return dct\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        gen_x, cell_x, targets = data['gen_x'].to(device), data['cell_x'].to(device), data['y'].to(device)\n        outputs = model(gen_x, cell_x)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    \n    return final_loss\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        gen_x, cell_x, targets = data['gen_x'].to(device), data['cell_x'].to(device),  data['y'].to(device)\n        outputs = model(gen_x, cell_x)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        gen_x, cell_x = data['gen_x'].to(device), data['cell_x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(gen_x, cell_x)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            \n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_oof_predictions(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n#     kfold_col = f'kfold_{seed}'\n#     trn_idx = train[train[kfold_col] != fold].index\n#     val_idx = train[train[kfold_col] == fold].index\n\n#     train_df = train[train[kfold_col] != fold].reset_index(drop=True)\n#     valid_df = train[train[kfold_col] == fold].reset_index(drop=True)\n\n#     gen_x_train, cell_x_train, y_train  = train_df[gen_cols + pca_gen_cols + ohe_cols].values, train_df[cell_cols + pca_cell_cols + ohe_cols].values, train_df[target_cols].values\n#     gen_x_valid, cell_x_valid, y_valid =  valid_df[gen_cols + pca_gen_cols + ohe_cols].values, valid_df[cell_cols + pca_cell_cols + ohe_cols].values, valid_df[target_cols].values\n#     #     print(\"len target_cols\", len(target_cols), train_df[target_cols].shape, train_df[target_cols].reset_index(drop=True).shape)\n#     #     print(\"shapes\", x_train.shape, y_train.shape, x_valid.shape, y_valid.shape)\n\n#     train_dataset = MoADataset(gen_x_train, cell_x_train, y_train)\n#     valid_dataset = MoADataset(gen_x_valid, cell_x_valid, y_valid)\n#     trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n#     validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n\n    # predict on oof\n#     print('predict on oof...', end='')\n#     oof = np.zeros((len(train), targets.iloc[:, 1:].shape[1]))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    model = Model(\n        num_gen_features=len(gen_cols + pca_gen_cols + ohe_cols),\n        num_cell_features=len(cell_cols + pca_cell_cols + ohe_cols),\n        num_targets=len(target_cols),\n        hidden_size=hidden_size)\n    \n    model.to(DEVICE)\n    model.load_state_dict(torch.load(f\"../input/jp-multi-heads/FOLD{fold}_SEED{seed}.pth\"))\n#     valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n#     oof[val_idx] = valid_preds\n#     print('  done.')\n    \n    \n    #--------------------- PREDICTION---------------------\n    \n    gen_x_test, cell_x_test = test_[gen_cols + pca_gen_cols + ohe_cols].values, test_[cell_cols + pca_cell_cols + ohe_cols].values\n    testdataset = TestDataset(gen_x_test, cell_x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    print('predict on test...', end='')\n    predictions = np.zeros((len(test_), targets.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    print('  done.\\n')\n    \n    \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):\n#     oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        pred_ = get_oof_predictions(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n#         oof += oof_\n        \n    return predictions\n\n\n# Averaging on multiple SEEDS\n\n# oof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    predictions_ = run_k_fold(NFOLDS, seed)\n#     oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%who DataFrame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del folds, sample_submission, targets, test, train, train_comb, train_drug, train_features, train_targets_nonscored, train_targets_scored\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof = pd.DataFrame(oof, columns=target_cols)\n\n# train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n# sig_id = train_features[train_features['cp_type']!='ctl_vehicle'].sig_id.reset_index(drop=True)\n# oof['sig_id'] = sig_id\n\n# oof = pd.merge(train_features[['sig_id']], oof, on='sig_id', how='left')\n# oof.fillna(0, inplace=True)\n# oof.drop('sig_id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compute multi-heads CV\n\n# y_true = train_targets_scored[target_cols].values\n# y_pred = np.clip(oof.values, 0.0005, 0.9995)\n\n# score = 0\n# for i in range(len(target_cols)):\n#     score_ = log_loss(y_true[:, i], y_pred[:, i])\n#     score += score_ / targets.shape[1]\n    \n# print(\"multi-heads CV log_loss: \", score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_clip = np.clip(predictions, 0.0005, 0.9995)\npredictions_clip = pd.DataFrame(predictions_clip, columns=target_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_multi_heads_all = np.load( '../input/jp-multi-heads/multi_heads_oof.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submit multi-heads\ntest = pd.read_csv('../input/lish-moa/test_features.csv')\nsig_id = test[test['cp_type']!='ctl_vehicle'].sig_id.reset_index(drop=True)\npredictions_clip['sig_id'] = sig_id\n\nsub_mh = pd.merge(test[['sig_id']], predictions_clip, on='sig_id', how='left')\nsub_mh.fillna(0, inplace=True)\nsub_mh.to_csv('submission_mh.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nimport pandas as pd\nfrom time import time\n# from autograd import grad\n# import autograd.numpy as np\nimport numpy as np\nfrom numba import njit\nfrom scipy.optimize import minimize, fsolve\n\n# CPMP's logloss from https://www.kaggle.com/c/lish-moa/discussion/183010\ndef log_loss_numpy(y_pred):\n    y_true_ravel = np.asarray(y_true).ravel()\n    y_pred = np.asarray(y_pred).ravel()\n    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    loss = np.where(y_true_ravel == 1, - np.log(y_pred), - np.log(1 - y_pred))\n    return loss.mean()\n\ndef func_numpy_metric(weights):\n    oof_blend = np.tensordot(weights, oof, axes = ((0), (0)))\n    return log_loss_numpy(oof_blend)\n\ndef grad_func(weights):\n    oof_clip = np.clip(oof, 1e-15, 1 - 1e-15)\n    gradients = np.zeros(oof.shape[0])\n    for i in range(oof.shape[0]):\n        a, b, c = y_true, oof_clip[i], np.zeros((oof.shape[1], oof.shape[2]))\n        for j in range(oof.shape[0]):\n            if j != i:\n                c += weights[j] * oof_clip[j]\n        gradients[i] = -np.mean((-a*b+(b**2)*weights[i]+b*c)/((b**2)*(weights[i]**2)+2*b*c*weights[i]-b*weights[i]+(c**2)-c))\n    return gradients\n\n@njit\ndef grad_func_jit(weights):\n    oof_clip = np.minimum(1 - 1e-15, np.maximum(oof, 1e-15))\n    gradients = np.zeros(oof.shape[0])\n    for i in range(oof.shape[0]):\n        a, b, c = y_true, oof_clip[i], np.zeros((oof.shape[1], oof.shape[2]))\n        for j in range(oof.shape[0]):\n            if j != i:\n                c += weights[j] * oof_clip[j]\n        gradients[i] = -np.mean((-a*b+(b**2)*weights[i]+b*c)/((b**2)*(weights[i]**2)+2*b*c*weights[i]-b*weights[i]+(c**2)-c))\n    return gradients","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = pd.read_csv('../input/lish-moa/train_targets_scored.csv', index_col = 'sig_id').values\n\noof_dict = {'Model 1': '../input/nn-transfer-learning-oof/oof_nn_transfer_all.npy',\n            'Model 2': '../input/tabnetfe7seeds424810folds/oof_TabNet_all.npy',\n            'Resnet': '../input/prediction/resnet_oof.npy',\n            'Multi_heads': '../input/jp-multi-heads/multi_heads_oof.npy'\n           }\n\noof = np.zeros((len(oof_dict), y_true.shape[0], y_true.shape[1]))\nfor i in range(oof.shape[0]):\n    #print(list(oof_dict.values())[i])\n    oof[i] = np.load(list(oof_dict.values())[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nlog_loss_scores = {}\nfor n, key in enumerate(oof_dict.keys()):\n    score_oof = log_loss_numpy(oof[n])\n    log_loss_scores[key] = score_oof\n    print(f'{key} CV:\\t', score_oof)\nprint('-' * 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_weights = np.array([1 / oof.shape[0]] * oof.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %timeit -r 10 grad_func(test_weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %timeit -r 10 grad_func_jit(test_weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tol = 1e-10\ninit_guess = [1 / oof.shape[0]] * oof.shape[0]\nbnds = [(0, 1) for _ in range(oof.shape[0])]\ncons = {'type': 'eq', \n        'fun': lambda x: np.sum(x) - 1, \n        'jac': lambda x: [1] * len(x)}\n\nprint('Inital Blend OOF:', func_numpy_metric(init_guess))\nstart_time = time()\nres_scipy = minimize(fun = func_numpy_metric, \n                     x0 = init_guess, \n                     method = 'SLSQP', \n                     jac = grad_func_jit, # grad_func \n                     bounds = bnds, \n                     constraints = cons, \n                     tol = tol)\nprint(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] Optimised Blend OOF:', res_scipy.fun)\nprint('Optimised Weights:', res_scipy.x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Check the sum of all weights:', np.sum(res_scipy.x))\nif np.sum(res_scipy.x) - 1 <= tol:\n    print('Great! The sum of all weights equals to 1!')\nelse:\n    print('Manual adjustion is needed to modify the weights.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\nsub.iloc[:,1:] = np.clip(res_scipy.x[0]*sub_nn_transfer.iloc[:, 1:].values + \\\n                  res_scipy.x[1]*submission_TabNet.iloc[:,1:].values + \\\n                  res_scipy.x[2]*sub_rs.iloc[:,1:].values + \\\n                  res_scipy.x[3]*sub_mh.iloc[:,1:].values, 0.00005,0.99995)\n# sub.iloc[:,1:] = (0.4*sub_nn_transfer.iloc[:, 1:].values + \\\n#                   0.2*submission_TabNet.iloc[:,1:].values + \\\n#                   0.4*sub_rs.iloc[:,1:].values)\n\n# Set ctl_vehicle to 0\nsub.iloc[test_features['cp_type'] == 'ctl_vehicle',1:] = 0\n\n# Save Submission\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm submission_nn_transfer.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm submission_rs.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm submission_TabNet.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm submission_mh.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}