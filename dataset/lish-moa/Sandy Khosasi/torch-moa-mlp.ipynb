{"cells":[{"metadata":{},"cell_type":"markdown","source":"## About\n\nFew parts of this book is based on,\n\n* https://www.kaggle.com/namanj27/new-baseline-pytorch-moa\n* https://www.kaggle.com/felipebihaiek/prediction-with-swap-auto-encoder-features-0-01865\n* https://www.kaggle.com/frtgnn/introduction-to-pytorch-a-very-gentle-start"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install -r /kaggle/input/python-library/wheelhouse/requirements.txt --no-index --find-links /kaggle/input/python-library/wheelhouse","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch_optimizer\nimport lambda_networks\nimport torchsummary\nimport iterstrat\n\nprint(np.__version__)\nprint(pd.__version__)\nprint(torch.__version__)\nprint(torch_optimizer.__version__)\n# print(lambda_networks.__version__)\n# print(torchsummary.__version__)\nprint(iterstrat.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\nSEEDS = [4, SEED, 6, 89]\n\ndef set_seed(seed):\n    os.environ['PYTHONHASHSEED']=str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\nset_seed(SEED)\n\nN_EPOCHS = 250\nN_FOLDS = 10\nBATCH_SIZE = 128\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ny = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\nX_test = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission = pd.DataFrame(columns=y.columns)\ndf_submission['sig_id'] = X_test['sig_id']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocess"},{"metadata":{},"cell_type":"markdown","source":"## Preprocess - One Hot Encode"},{"metadata":{"trusted":true},"cell_type":"code","source":"del X['sig_id']\ndel y['sig_id']\ndel X_test['sig_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feature = ['cp_type', 'cp_dose']\nfor cat in cat_feature:\n    val_list = X[cat].unique()\n    for val in val_list:\n        X[f'{cat}_{val}'] = X[cat].apply(lambda i:i == val)\n        X_test[f'{cat}_{val}'] = X_test[cat].apply(lambda i:i == val)\n        \n    del X[cat]\n    del X_test[cat]\n    \nprint(X.columns[1:773])\nprint(X.columns[773:873])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocess - Convert to Tensor"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n1. Convert DataFrame data type to float32\n2. Convert DataFrame to Numpy Array\n3. Convert Numpy Array to Torch Tensor\n'''\n\nX = torch.tensor(X.astype(np.float32).to_numpy())\ny = torch.tensor(y.astype(np.float32).to_numpy())\nX_test = torch.tensor(X_test.astype(np.float32).to_numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoADataset(torch.utils.data.Dataset):\n    def __init__(self, X, y=None):\n        if y is not None:\n            assert X.shape[0] == y.shape[0]\n\n        self.X = X\n        self.y = y\n        \n    def __len__(self):\n        return self.X.shape[0]\n    \n    def __getitem__(self, idx):\n        if self.y is not None:\n            return self.X[idx], self.y[idx]\n        else:\n            return self.X[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds = MoADataset(X, y)\ntest_ds = MoADataset(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Autoencoder"},{"metadata":{},"cell_type":"markdown","source":"## Autoencoder - Train function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_ae(mode, whole_dl):\n    # Loss\n    train_loss = 0\n    lowest_loss = np.Inf\n    \n    last_epoch_with_lowest_loss = 0\n\n    # Net\n    if mode == 'AE_Gene':\n        print('Train autoencoder gene!')\n        net = AE_Gene_Net().float().to(device)\n        criterion = nn.MSELoss()\n        optimizer = torch_optimizer.RAdam(net.parameters(), lr=0.002, weight_decay=0.0000125)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer=optimizer, factor=0.5, patience=8, min_lr = 0.00005, verbose=True\n        )\n    else:\n        print('Train autoencoder cell!')\n        net = AE_Cell_Net().float().to(device)\n        criterion = nn.MSELoss()\n        optimizer = torch_optimizer.RAdam(net.parameters(), lr=0.002, weight_decay=0.00001)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer=optimizer, factor=0.25, patience=11, min_lr = 0.000025, verbose=True\n        )\n\n    # Train\n    net.train()\n    for epoch in range(N_EPOCHS):\n        for X_local in whole_dl:\n            X_local = X_local.to(device)\n\n            optimizer.zero_grad()\n            output = net(X_local)\n\n            loss = criterion(output, X_local)          \n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n\n        # post\n        train_loss /= len(whole_dl)\n        scheduler.step(train_loss)\n\n        print(f'Epoch: {epoch+1:02d}/{N_EPOCHS:02d} | Train loss: {train_loss:.08f}{\" (Saving model state!)\" if train_loss < lowest_loss else \"\"}')\n        if train_loss < lowest_loss:\n            torch.save(net.state_dict(), f'./model_{mode}.state_dict')\n            lowest_loss = train_loss\n            last_epoch_with_lowest_loss = epoch\n        if last_epoch_with_lowest_loss + 50 < epoch:\n            print('Early stopping!')\n            break\n\n        train_loss = 0\n    \n    if mode == 'AE_Gene':\n        net = AE_Gene_Net().float().to(device)\n    else:\n        net = AE_Cell_Net().float().to(device)\n    net.load_state_dict(torch.load(f'./model_{mode}.state_dict'))\n        \n    return net, lowest_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Autoencoder - Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch import nn\nfrom torch.nn import functional as F\nfrom torchsummary import summary\n\nclass AE_Gene_Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # start\n        self.encoder = nn.Sequential(\n            nn.BatchNorm1d(772),\n            nn.utils.weight_norm(nn.Linear(772, 541)),\n            nn.ReLU6(),\n\n            nn.BatchNorm1d(541),\n            nn.utils.weight_norm(nn.Linear(541, 309)),\n            nn.ReLU6(),\n        )\n        self.decoder = nn.Sequential(\n            nn.BatchNorm1d(309),\n            nn.utils.weight_norm(nn.Linear(309, 541)),\n            nn.ReLU6(),\n\n            nn.BatchNorm1d(541),\n            nn.utils.weight_norm(nn.Linear(541, 772)),\n        )\n\n    def forward(self, x, mode='reconstruct'):\n        x = self.encoder(x)\n        if mode == 'reconstruct':\n            x = self.decoder(x)\n        \n        return x\n    \nae_gene_net = AE_Gene_Net().float().to(device)\nprint(ae_gene_net)\nsummary(ae_gene_net, (772, ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AE_Cell_Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # start\n        self.encoder = nn.Sequential(\n            nn.BatchNorm1d(100),\n            nn.utils.weight_norm(nn.Linear(100, 70)),\n            nn.ReLU6(),\n\n            nn.BatchNorm1d(70),\n            nn.utils.weight_norm(nn.Linear(70, 40)),\n            nn.ReLU6(),\n        )\n        self.decoder = nn.Sequential(\n            nn.BatchNorm1d(40),\n            nn.utils.weight_norm(nn.Linear(40, 70)),\n            nn.ReLU6(),\n\n            nn.BatchNorm1d(70),\n            nn.utils.weight_norm(nn.Linear(70, 100)),\n        )\n\n    def forward(self, x, mode='reconstruct'):\n        x = self.encoder(x)\n        if mode == 'reconstruct':\n            x = self.decoder(x)\n        \n        return x\n    \nae_cell_net = AE_Cell_Net().float().to(device)\nprint(ae_cell_net)\nsummary(ae_cell_net, (100, ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Autoencoder - Train AE Gene"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"X_whole = torch.cat((X[:, 1:773], X_test[:, 1:773]), 0)\n\nwhole_ds = MoADataset(X_whole)\nwhole_dl = torch.utils.data.DataLoader(whole_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n\nae_gene_net, ae_gene_min_loss = train_ae('AE_Gene', whole_dl)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Autoencoder - Train AE Cell"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"X_whole = torch.cat((X[:, 773:873], X_test[:, 773:873]), 0)\n\nwhole_ds = MoADataset(X_whole)\nwhole_dl = torch.utils.data.DataLoader(whole_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n\nae_cell_net, ae_cell_min_loss = train_ae('AE_Cell', whole_dl)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Autoencoder - Transform Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_AE_Gene = torch.zeros([X.shape[0], 309])\nX_AE_Cell = torch.zeros([X.shape[0], 40])\n\nX_test_AE_Gene = torch.zeros([X_test.shape[0], 309])\nX_test_AE_Cell = torch.zeros([X_test.shape[0], 40])\n\nwith torch.no_grad():\n    for i in range(0, X.shape[0], BATCH_SIZE):\n        start = i\n        if start+BATCH_SIZE >= X.shape[0]:\n            end = X.shape[0]\n        else:\n            end = start+BATCH_SIZE\n\n        X_AE_Gene[start:end] = ae_gene_net(X[start:end, 1:773].to(device), mode='').cpu()\n        X_AE_Cell[start:end] = ae_cell_net(X[start:end, 773:873].to(device), mode='').cpu()\n    for i in range(0, X_test.shape[0], BATCH_SIZE):\n        start = i\n        if start+BATCH_SIZE >= X_test.shape[0]:\n            end = X_test.shape[0]\n        else:\n            end = start+BATCH_SIZE\n\n        X_test_AE_Gene[start:end] = ae_gene_net(X_test[start:end, 1:773].to(device), mode='').cpu()\n        X_test_AE_Cell[start:end] = ae_cell_net(X_test[start:end, 773:873].to(device), mode='').cpu()\n\nX = torch.cat((X, X_AE_Gene, X_AE_Cell), 1)\nX_test = torch.cat((X_test, X_test_AE_Gene, X_test_AE_Cell), 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Autoencoder - Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n\nX_length, X_test_length = X.shape[0], X_test.shape[0]\nX_temp = torch.cat((X, X_test), 0)\nprint('Shape before:', X_temp.shape)\n\nselector = VarianceThreshold(0.5)\nX_temp = selector.fit_transform(X_temp)\nX_temp = torch.tensor(X_temp)\nprint('Shape after:', X_temp.shape)\n\nX = X_temp[0:X_length]\nX_test = X_temp[X_length:X_length+X_test_length]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MLP"},{"metadata":{},"cell_type":"markdown","source":"## MLP - Train function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_mlp(train_dl, val_dl, seed, fold):\n    # Loss\n    train_loss = 0\n    val_loss = 0\n    lowest_val_loss = np.Inf\n    \n    last_epoch_with_lowest_val_loss = 0\n\n    # Net\n    net = MLPNet().float().to(device)\n    criterion = nn.BCELoss()\n#     optimizer = torch_optimizer.RAdam(net.parameters(), lr=0.001, weight_decay=0.00002)\n#     scheduler = torch.optim.lr_scheduler.OneCycleLR(\n#         optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n#         max_lr=0.01, epochs=N_EPOCHS, steps_per_epoch=len(train_dl)\n#     )\n    optimizer = torch_optimizer.AdaBelief(net.parameters(), lr=0.1, weight_decay=0.00002, rectify=False, weight_decouple=False)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n        max_lr=1.0, epochs=N_EPOCHS, steps_per_epoch=len(train_dl)\n    )\n\n    # Train/Eval\n    for epoch in range(N_EPOCHS):\n        # Train\n        net.train()\n        for X_local, y_local in train_dl:\n            X_local = X_local.to(device)\n            y_local = y_local.to(device)\n\n            optimizer.zero_grad()\n            output = net(X_local)\n\n            loss = criterion(output, y_local)                \n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            train_loss += loss.item()\n\n        # Val\n        net.eval()\n        for X_local, y_local in val_dl:\n            X_local = X_local.to(device)\n            y_local = y_local.to(device)\n\n            output = net(X_local)\n            loss = criterion(output, y_local)   \n\n            val_loss += loss.item()\n\n        # post\n        train_loss /= len(train_dl)\n        val_loss /= len(val_dl)\n#         scheduler.step(val_loss)\n\n        print(f'Epoch: {epoch+1:02d}/{N_EPOCHS:02d} | Train loss: {train_loss:.08f} - Val loss: {val_loss:.08f}{\" (Saving model state!)\" if val_loss < lowest_val_loss else \"\"}')\n        if val_loss < lowest_val_loss:\n            torch.save(net.state_dict(), f'./model_mlp_{seed:02d}_{fold:02d}.state_dict')\n            lowest_val_loss = val_loss\n            last_epoch_with_lowest_val_loss = epoch\n        if last_epoch_with_lowest_val_loss + 30 < epoch:\n            print('Early stopping!')\n            # stop training\n            break\n\n        train_loss = 0\n        val_loss = 0\n\n    net = MLPNet().float().to(device)\n    net.load_state_dict(torch.load(f'./model_mlp_{seed:02d}_{fold:02d}.state_dict'))\n        \n    return net, lowest_val_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MLP - Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MLPNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # start\n        self.bn1 = nn.BatchNorm1d(X.shape[1])\n        self.drop1 = nn.Dropout(0.2)\n        self.fc1 = nn.utils.weight_norm(nn.Linear(X.shape[1], 1280))\n\n        self.bn2 = nn.BatchNorm1d(1280)\n        self.drop2 = nn.Dropout(0.5)\n        self.fc2 = nn.utils.weight_norm(nn.Linear(1280, 1280))\n\n\n        self.bn3 = nn.BatchNorm1d(1280)\n        self.drop3 = nn.Dropout(0.5)\n        self.fc3 = nn.utils.weight_norm(nn.Linear(1280, y.shape[1]))\n\n        # generic\n        self.relu = nn.ReLU6()\n        self.sigmoid = nn.Sigmoid()\n        \n\n    def forward(self, x):\n        x = self.bn1(x)\n        x = self.drop1(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n\n        x = self.bn2(x)\n        x = self.drop2(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n\n        x = self.bn3(x)\n        x = self.drop3(x)\n        x = self.fc3(x)\n        x = self.sigmoid(x)\n        \n        return x\n    \nmlp_net = MLPNet().float().to(device)\nprint(mlp_net)\nsummary(mlp_net, (X.shape[1], ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MLP - Train"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nmlp_nets = []\nmlp_min_losses = []\n\nfor seed in SEEDS:\n    set_seed(seed)\n    skf = MultilabelStratifiedKFold(n_splits=N_FOLDS, random_state=seed, shuffle=True)\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n        print(f'Seed: {seed}, fold: {fold+1}/{N_FOLDS}')\n\n        X_train, y_train = X[train_idx], y[train_idx]\n        train_ds = MoADataset(X_train, y_train)\n        train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n\n        X_val, y_val = X[val_idx], y[val_idx]\n        val_ds = MoADataset(X_val, y_val)\n        val_dl = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\n        mlp_net, mlp_min_loss = train_mlp(train_dl, val_dl, seed, fold)\n        mlp_nets.append(mlp_net)\n        mlp_min_losses.append(mlp_min_loss)\n                \n        print('='*82)\nprint('Training Ended! ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Post-training"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'AE Gene loss: {ae_gene_min_loss}')\nprint(f'AE Cell loss: {ae_cell_min_loss}')\n\nprint('MLP CV min loss:')\n[print(l) for l in mlp_min_losses]\n\nfor i in range(len(SEEDS)):\n    print(f'MLP average CV min loss (SEED {SEEDS[i]}): {sum(mlp_min_losses[i*N_FOLDS:i*N_FOLDS+N_FOLDS])/N_FOLDS}')\nprint(f'MLP average CV min loss: {sum(mlp_min_losses)/len(mlp_min_losses)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    for i in range(0, X_test.shape[0], BATCH_SIZE):\n        start = i\n        if start+BATCH_SIZE >= X_test.shape[0]:\n            end = X_test.shape[0]\n        else:\n            end = start+BATCH_SIZE\n\n        test_results = torch.zeros([end-start, y.shape[1]])\n        for mlp_net in mlp_nets:\n            test_results += mlp_net(X_test[start:end].to(device)).cpu()\n        test_results /= int(N_FOLDS * len(SEEDS))\n\n        df_submission.iloc[start:end, 1:] = test_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}