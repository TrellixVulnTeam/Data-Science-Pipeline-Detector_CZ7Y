{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n#!pip install iterative-stratification\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport os\n\n#!pip install iterative-stratification\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\npath1='../input/lish-moa'\nos.chdir(path1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('/kaggle')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('/kaggle/input/pca-features/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_pca=pd.read_csv('/kaggle/input/pca-features/pca_features.csv')\ntargets=pd.read_csv('/kaggle/input/targets/target_kfold.csv')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat=pd.read_csv('train_features.csv')\ntrain_feat=pd.read_csv('train_features.csv')\ntest_prediction_feat=pd.read_csv('test_features.csv')\ntargets=pd.read_csv('train_targets_scored.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Data preprocess*"},{"metadata":{"trusted":true},"cell_type":"code","source":"gene_train = [col for col in train_feat.columns if 'g-' in col]\ncell_train = [col for col in train_feat.columns if 'c-' in col]\ntrain_g=train_feat[gene_train]\ntrain_c=train_feat[cell_train]\ntrain_category=train_feat[['cp_type', 'cp_time', 'cp_dose']]\ntx=train_category['cp_type'].replace({'ctl_vehicle':0, 'trt_cp':1})\ntx2=train_category['cp_dose'].replace({'D1':1, 'D2':2})\ntx3=train_category['cp_time'].replace({24:1, 48:2, 72:3})\ncp_time=pd.get_dummies(train_feat['cp_time'])\ntrain_category2=pd.concat([tx,tx2, tx3], axis=1)\ntrain_category22=pd.concat([tx,tx2, cp_time], axis=1)\ntest_g=test_prediction_feat[gene_train]\ntest_c=test_prediction_feat[cell_train]\n\ntest_cat=test_prediction_feat[['cp_type', 'cp_time', 'cp_dose']]\ntx=test_cat['cp_type'].replace({'ctl_vehicle':0, 'trt_cp':1})\ntx2=test_cat['cp_dose'].replace({'D1':1, 'D2':2})\ntx3=test_cat['cp_time'].replace({24:1, 48:2, 72:3})\ncp_time2=pd.get_dummies(test_cat['cp_time'])\ntest_category2=pd.concat([tx,tx2, tx3], axis=1)\ntest_category3 = test_category2[test_category2['cp_type'].notna()]\nCP_T=test_category3['cp_type'].astype('int64')\nCP_D=test_category3['cp_dose'].astype('int64')\nc2=pd.concat([CP_T, CP_D, cp_time2], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tabnet=pd.concat([c2, test_g, test_c], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tabnet=pd.concat([train_category22,train_g, train_c], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tabnet.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reduce features with PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n#Keeping explaining 95% of variance\npca=PCA(0.95)\npca1=PCA(381)\npca2=PCA(44)\n\ndata_c=pd.concat([train_c, test_c], axis=0)\ndata_g=pd.concat([train_g, test_g], axis=0)\ndata_genes_pca=pca1.fit_transform(data_g)\ndata_cells_pca=pca2.fit_transform(data_c)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_genes_pca.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cells_pca.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_x=train_feat[['sig_id']]\npc_c=['PC-cell-'+str(n) for n in range(data_cells_pca.shape[1])]\npc_g=['PC-gene-'+str(n) for n in range(data_genes_pca.shape[1])]\n\ndata_cells_pca=pd.DataFrame(data_cells_pca, columns=pc_c)\ndata_genes_pca=pd.DataFrame(data_genes_pca, columns=pc_g)\n\nd_c_train=data_cells_pca.loc[:int(train_feat.shape[0])]\nd_g_train=data_genes_pca.loc[:int(train_feat.shape[0])]\n\nd_c_test=data_cells_pca.loc[int(train_feat.shape[0]):]\nd_g_test=data_genes_pca.loc[int(train_feat.shape[0]):]\n\nd_g_test.reset_index(drop=True, inplace=True)\nd_c_test.reset_index(drop=True, inplace=True)\n\ndata_train_tabnet=pd.concat([id_x,train_category2, train_g, train_c], axis=1)\ndata_train_pca=pd.concat([id_x,train_category2, d_g_train, d_c_train], axis=1)\ndata_test=test_prediction_feat.drop(['sig_id', 'cp_type', 'cp_time', 'cp_dose'], axis=1)\ndata_test=pd.concat([id_x,test_category2, data_test], axis=1)\ndata_test_pca=pd.concat([id_x,test_category2,d_g_test, d_c_test], axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NN - Pytroch model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LinearRegression\n\n\nclass MoaDataset:\n    def __init__(self, dataset, features):\n        self.dataset=dataset\n        self.features=features\n        \n\n    def __len__(self):\n        return self.dataset.shape[0]\n\n    def __getitem__(self, item):\n        return{\n            'x':torch.tensor(self.dataset[item, :], dtype=torch.float),\n            'y':torch.tensor(self.features[item, :], dtype=torch.float)\n        }\n\n\nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    \n\nclass Engine:\n    def __init__(self, model, optimizer, device):\n\n        self.model =model\n        self.optimizer = optimizer\n        self.device = device\n    @staticmethod\n    def loss_fn(targets, outputs):\n        return nn.BCEWithLogitsLoss()(outputs, targets)\n\n    def train(self, data_loader):\n        self.model.train()\n        final_loss=0\n        train_accuracy=0\n\n        for data in data_loader:\n            self.optimizer.zero_grad()\n            inputs= data['x'].to(self.device)\n            targets= data['y'].to(self.device)\n            outputs=self.model(inputs)\n            #Calculate loss value\n            loss=self.loss_fn(targets, outputs)\n            loss.backward()\n            self.optimizer.step()\n            final_loss += loss.item()\n            # y_pred=LR.predict(outputs.cpu().detach().numpy().round())\n            # train_accuracy +=accuracy_score(targets.cpu().detach().numpy(), y_pred)\n        return final_loss/len(data_loader)\n\n    def validate(self, data_loader):\n        self.model.eval()\n        final_loss=0\n        valid_accuracy=0\n\n        for data in data_loader:\n            inputs= data['x'].to(self.device)\n            targets= data['y'].to(self.device)\n            outputs=self.model(inputs)\n            loss=self.loss_fn(targets, outputs)\n            final_loss += loss.item()\n            # valid_accuracy +=accuracy_score(targets.numpy(), outputs.numpy())\n\n        return final_loss/len(data_loader) \n\n# class trainsform_hyperparameters():\n#     def __init__(self, start_neurons, n_layers):\n        \nclass Model(nn.Module):\n\n    def __init__(self, num_features, num_targets, dropout1,dropout2,n_neuron1,n_neuron2):\n        super().__init__()\n        #Define model architecture\n        self.model = nn.Sequential(\n            nn.BatchNorm1d(num_features),\n            nn.utils.weight_norm(nn.Linear(num_features, n_neuron1)),\n            nn.LeakyReLU(),\n\n            nn.BatchNorm1d(n_neuron1),\n            nn.Dropout(dropout1),\n            nn.utils.weight_norm(nn.Linear(n_neuron1, n_neuron2)),\n            nn.LeakyReLU(),\n\n\n            nn.BatchNorm1d(n_neuron2),\n            nn.Dropout(dropout2),\n            nn.utils.weight_norm(nn.Linear(n_neuron2, num_targets)),\n\n\n        )\n\n    def forward(self, x):\n        #Define how the data passes through the model \n        #in this case just straight through\n        x = self.model(x)\n        return x\n\n    def load(self, model_path):\n        self.load_state_dict(torch.load(model_path))\n        self.eval()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_pca=pd.read_csv('/kaggle/input/pca-features/pca_features.csv')\ntargets=pd.read_csv('/kaggle/input/targets/target_kfold.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport os\nimport time\n\n\nDEVICE = \"cuda\"\nEPOCHS = 340\n\nfolds=targets\nfeature_data=features_pca\n\n\n#Takes the fold number as input \ndef run_training(fold, lr_x,batch_size, dropout1,dropouit2, n_neur1, n_neur2):\n    df = feature_data\n\n    #folds = ann_data.train_y\n    targets= folds.drop(['sig_id', 'kfold'], axis=1).columns\n    features = df.drop('sig_id', axis=1).columns\n    df=df.merge(folds, on='sig_id', how='left')\n\n    #The following row states that the training data is \n    # all of the rows not contaning the current fold value\n    train_df=df[df.kfold != fold].reset_index(drop=True)\n\n    #Valid data rows wich are the same as fold\n    valid_df=df[df.kfold == fold].reset_index(drop=True)\n\n    x_train = train_df[features].to_numpy()\n    x_valid = valid_df[features].to_numpy()\n    y_train = train_df[targets].to_numpy()\n    y_valid = valid_df[targets].to_numpy()\n    \n    train_dataset= MoaDataset(x_train, y_train)\n    train_loader= torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0)\n\n    #Validate \n    valid_dataset= MoaDataset(x_valid, y_valid)\n    valid_loader= torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, num_workers=0)\n\n\n    model = Model(428 , 206, dropout1,dropouit2, n_neur1, n_neur2)\n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=lr_x)\n    eng= Engine(model, optimizer, DEVICE)\n\n    best_scores_kf=0\n    #add early stopping and model saving here\n    #Use underscore '_' as it is not used\n    score=0\n    score_v=0\n    best_loss=1\n    early_stopping= 20\n    epochs_no_improve=0\n    early_stop = False\n\n    num_epochs_run=0\n    for n in range(EPOCHS):\n        num_epochs_run=n\n\n        train_loss= eng.train(train_loader)\n        valid_loss= eng.validate(valid_loader)\n        score +=train_loss\n        score_v +=valid_loss\n        #Early stopping checking if model validation loss does imporve other wise stop after n steps.\n        #Bstops if no improves is seen \n        if valid_loss < best_loss: \n            epochs_no_improve=0\n            best_loss= valid_loss\n            #print( f\"Best loss: {best_loss} at epoch: {n}\")\n            torch.save(model.state_dict(),\"/kaggle/working/best_weights.pth\")\n            if n==EPOCHS:\n                print('1')\n                \n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve == early_stopping:\n                print(f'best epoch{valid_loss}')\n                best_scores_kf=best_scores_kf+best_loss\n                early_stop= True\n                break\n            else:\n                continue\n        if early_stop:\n            print('Stopped')\n            #torch.save(model.state_dict(),weights_path+'best_pytorch_weights.pth')\n            break\n    print(num_epochs_run)\n    print(f\"CV score valid: {best_loss}\")\n    return best_loss\n\ndef train_real(lr_x,batch_size, dropout1,dropout2, n_neur1, n_neur2):\n    n_folds=5\n    loss_tot=0\n    for n in range(n_folds):\n        loss_x=run_training(n,lr_x,batch_size,dropout1,dropout2, int(n_neur1), int(n_neur2))\n        loss_tot +=loss_x\n    print(f'Average CV-loss: {(loss_tot/n_folds)}')\n\n\n\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    model.load_state_dict(torch.load(\"/kaggle/working/best_weights.pth\"))\n    pred= []\n    model.to(DEVICE)\n    for data in dataloader:\n        inputs = data['x'].to(device)\n        out= model(inputs)\n        pred.append(out.sigmoid().detach().cpu().numpy()[0])\n    return pred\n\n\n\ndef predict(x):\n    DEVICE='cuda'\n    x=x.drop(['sig_id'], axis=1)\n    x=x.to_numpy()\n    x= TestDataset(x)\n    test_loader= torch.utils.data.DataLoader(x, batch_size=1, num_workers=0)\n    model = Model(428, 206, 0.503620,0.192174, 1960, 1085)\n    model.load_state_dict(torch.load(\"/kaggle/working/best_weights.pth\"))\n    pred1= inference_fn(model, test_loader, DEVICE)\n    print(pred1)\n    return pred1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_real(0.00006890, 789, 0.893620, 0.292174, 1960, 1085)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test_pc1=data_test_pca[data_test_pca['cp_dose'].notna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p1=predict(data_test_pc1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sig_id_test=test_prediction_feat[['sig_id']]\ntest_prediction_feat.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names_mech=[col for col in targets.columns if col not in ['sig_id','kfold']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d1=pd.DataFrame(p1, columns=names_mech)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.concat([sig_id_test, d1], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.remove('/kaggle/working/best_weights.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}