{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Can we use non-scored targets to create synthetic variables?\n \nI will try to use Rapids-SVC models to trained non-scored targets and the results of the predictions will be concatenate to the features, including g- c- PCA  and categorical. Then, a neural network (NN) model will be trained exclusively for the scored targets.\n\nReferenced notebook for rapids and beginning of this notebook is forked from:\nhttps://www.kaggle.com/gogo827jz/rapids-svm-on-gpu-6000-models-in-1-hour\n\nNN referenced notebooks:\nhttps://www.kaggle.com/kushal1506/moa-pytorch-0-01859-rankgauss-pca-nn\nhttps://www.kaggle.com/riadalmadani/pytorch-cv-0-0145-lb-0-01839\n\npredictors\nhttps://www.kaggle.com/demetrypascal/t-test-pca-rfe-logistic-regression\n"},{"metadata":{},"cell_type":"markdown","source":"V3- No controls were included in the complete analysis. Only a selection of the non-scored predicted features, based on log-loss lower or equal to 0.02 were concatenated.\n\nIn v5, I will concatenate all possible non-scored predicted features."},{"metadata":{},"cell_type":"markdown","source":"# RAPIDS SVC for MoA\n\nRAPIDS cuML is a great library alows training sklearn models on GPU. Available classification models include Logistic Regresssion, SVC, Random Forest and KNN, etc..\n\nKonrad has tried to train SVR models in [SVR Modeds][1]. In this notebook, I try training 3090 SVC models in 2 hours on GPU, which should take forever on CPU...\n\n[1]: https://www.kaggle.com/konradb/build-model-svm"},{"metadata":{},"cell_type":"markdown","source":"# Try to use the nonscored targets to make synthetic variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings, sys\nwarnings.filterwarnings(\"ignore\")\n\n# Thanks to Chris's RAPIDS dataset, it only takes around 1 min to install offline\n!cp ../input/rapids/rapids.0.15.0 /opt/conda/envs/rapids.tar.gz\n!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"kw1VW6DCvgSq","outputId":"030d81e0-579d-463d-b2ed-6c714151a063"},"cell_type":"code","source":"#libraries for SVC\nimport os\nimport gc\nimport pickle\nimport datetime\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom tqdm.notebook import tqdm\nfrom time import time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#libraries for neural network\n\nimport random\n\nimport matplotlib.pyplot as plt\n\nimport copy\nimport seaborn as sns\nimport collections\n\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,QuantileTransformer,PowerTransformer,RobustScaler,Normalizer\n\nsys.path.append('../input/rank-gauss')\nfrom gauss_rank_scaler import GaussRankScaler\n\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.feature_selection import VarianceThreshold\n\nfrom joblib import dump, load\n\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR","execution_count":null,"outputs":[]},{"metadata":{"id":"dSVuPpi2vgSv"},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"Make everything without controls: SVC of nonscored targets and NN integrating synthetic variables"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"UvG3N1HHvgSv"},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\n#copies from dataframes\ndf_train=train_features.copy()\ndf_train_targets=train_targets.copy()\ndf_test=test_features.copy()\ndf_train_targets_nonscored=train_targets_nonscored.copy()\nss = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\ncols = [c for c in ss.columns.values if c != 'sig_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#elimination of controls\ndf_train = df_train[df_train['cp_type']!='ctl_vehicle']\ndf_test = df_test[df_test['cp_type']!='ctl_vehicle']\n\ndf_train_targets = df_train_targets.iloc[df_train.index]\ndf_train_targets_nonscored=df_train_targets_nonscored.iloc[df_train.index]\n\ndf_train.reset_index(drop=True, inplace=True)\ndf_test.reset_index(drop=True, inplace=True)\ndf_train_targets.reset_index(drop=True, inplace=True)\ndf_train_targets_nonscored.reset_index(drop=True, inplace=True)\n\ndel df_train_targets['sig_id']\ndel df_train_targets_nonscored['sig_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"JItYfC6jvgSy"},"cell_type":"code","source":"def preprocess(df):\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    return df\n\ndef log_loss_metric(y_true, y_pred):\n    metrics = []\n    for _target in train_targets.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels = [0,1]))\n    return np.mean(metrics)\n\ndef log_loss_metric_n(y_true, y_pred):\n    metrics = []\n    for _target in df_train_targets_nonscored.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels = [0,1]))\n    return np.mean(metrics)\n\n\ntrain = preprocess(df_train)\ntest = preprocess(df_test)\n\n#del train_targets['sig_id']\n#del train_targets_nonscored['sig_id']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"id":"Cg0gF9u5vgS1","outputId":"2dcd7162-6e0e-427c-cfc0-9e36329cd8a2"},"cell_type":"code","source":"top_feats = [  0,   1,   2,   3,   5,   6,   8,   9,  10,  11,  12,  14,  15,\n        16,  18,  19,  20,  21,  23,  24,  25,  27,  28,  29,  30,  31,\n        32,  33,  34,  35,  36,  37,  39,  40,  41,  42,  44,  45,  46,\n        48,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,\n        63,  64,  65,  66,  68,  69,  70,  71,  72,  73,  74,  75,  76,\n        78,  79,  80,  81,  82,  83,  84,  86,  87,  88,  89,  90,  92,\n        93,  94,  95,  96,  97,  99, 100, 101, 103, 104, 105, 106, 107,\n       108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n       121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 132, 133, 134,\n       135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n       149, 150, 151, 152, 153, 154, 155, 157, 159, 160, 161, 163, 164,\n       165, 166, 167, 168, 169, 170, 172, 173, 175, 176, 177, 178, 180,\n       181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 195,\n       197, 198, 199, 202, 203, 205, 206, 208, 209, 210, 211, 212, 213,\n       214, 215, 218, 219, 220, 221, 222, 224, 225, 227, 228, 229, 230,\n       231, 232, 233, 234, 236, 238, 239, 240, 241, 242, 243, 244, 245,\n       246, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260,\n       261, 263, 265, 266, 268, 270, 271, 272, 273, 275, 276, 277, 279,\n       282, 283, 286, 287, 288, 289, 290, 294, 295, 296, 297, 299, 300,\n       301, 302, 303, 304, 305, 306, 308, 309, 310, 311, 312, 313, 315,\n       316, 317, 320, 321, 322, 324, 325, 326, 327, 328, 329, 330, 331,\n       332, 333, 334, 335, 338, 339, 340, 341, 343, 344, 345, 346, 347,\n       349, 350, 351, 352, 353, 355, 356, 357, 358, 359, 360, 361, 362,\n       363, 364, 365, 366, 368, 369, 370, 371, 372, 374, 375, 376, 377,\n       378, 379, 380, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n       392, 393, 394, 395, 397, 398, 399, 400, 401, 403, 405, 406, 407,\n       408, 410, 411, 412, 413, 414, 415, 417, 418, 419, 420, 421, 422,\n       423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435,\n       436, 437, 438, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450,\n       452, 453, 454, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n       466, 468, 469, 471, 472, 473, 474, 475, 476, 477, 478, 479, 482,\n       483, 485, 486, 487, 488, 489, 491, 492, 494, 495, 496, 500, 501,\n       502, 503, 505, 506, 507, 509, 510, 511, 512, 513, 514, 516, 517,\n       518, 519, 521, 523, 525, 526, 527, 528, 529, 530, 531, 532, 533,\n       534, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547,\n       549, 550, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563,\n       564, 565, 566, 567, 569, 570, 571, 572, 573, 574, 575, 577, 580,\n       581, 582, 583, 586, 587, 590, 591, 592, 593, 595, 596, 597, 598,\n       599, 600, 601, 602, 603, 605, 607, 608, 609, 611, 612, 613, 614,\n       615, 616, 617, 619, 622, 623, 625, 627, 630, 631, 632, 633, 634,\n       635, 637, 638, 639, 642, 643, 644, 645, 646, 647, 649, 650, 651,\n       652, 654, 655, 658, 659, 660, 661, 662, 663, 664, 666, 667, 668,\n       669, 670, 672, 674, 675, 676, 677, 678, 680, 681, 682, 684, 685,\n       686, 687, 688, 689, 691, 692, 694, 695, 696, 697, 699, 700, 701,\n       702, 703, 704, 705, 707, 708, 709, 711, 712, 713, 714, 715, 716,\n       717, 723, 725, 727, 728, 729, 730, 731, 732, 734, 736, 737, 738,\n       739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751,\n       752, 753, 754, 755, 756, 758, 759, 760, 761, 762, 763, 764, 765,\n       766, 767, 769, 770, 771, 772, 774, 775, 780, 781, 782, 783, 784,\n       785, 787, 788, 790, 793, 795, 797, 799, 800, 801, 805, 808, 809,\n       811, 812, 813, 816, 819, 820, 821, 822, 823, 825, 826, 827, 829,\n       831, 832, 833, 834, 835, 837, 838, 839, 840, 841, 842, 844, 845,\n       846, 847, 848, 850, 851, 852, 854, 855, 856, 858, 860, 861, 862,\n       864, 867, 868, 870, 871, 873, 874]\n\nprint(len(top_feats))","execution_count":null,"outputs":[]},{"metadata":{"id":"0eDJ68r-vgTA"},"cell_type":"markdown","source":"# 3090 CuML SVC Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX = scaler.fit_transform(train.values[:, top_feats])\nx_tt = scaler.transform(test.values[:, top_feats])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"qiCub3F5vgTA","outputId":"a409642d-80cd-4fdb-d21b-586422655f38"},"cell_type":"code","source":"from cuml.svm import SVC\n\nN_STARTS =1 #3\nN_SPLITS =5 #5\n\nres = df_train_targets_nonscored.copy()\nres.loc[:, df_train_targets_nonscored.columns] = 0\n\n#make a dataframe to collect the sample data with dimensions of sample and columns of train_targets_nonscored\nss_non=pd.DataFrame(np.zeros((df_test.shape[0],len(df_train_targets_nonscored.columns))),columns=df_train_targets_nonscored.columns)\ncollect_tar_failure=[]\ncollect_tar_log012=[]\ncollect_tar=[]\n\nfor tar in tqdm(range(df_train_targets_nonscored.shape[1])):\n    print(tar)\n    \n    start_time = time()\n    targets = df_train_targets_nonscored.values[:, tar]\n    \n    if targets.sum() >= N_SPLITS:\n        \n        for seed in range(N_STARTS):\n\n            skf = StratifiedKFold(n_splits = N_SPLITS, random_state = seed, shuffle = True)\n\n            for n, (tr, te) in enumerate(skf.split(targets, targets)):\n\n                x_tr, x_val = X[tr], X[te]\n                y_tr, y_val = targets[tr], targets[te]\n                \n                if y_tr.sum() >= 5:\n\n                    model = SVC(probability = True, cache_size = 2000)\n                    model.fit(x_tr, y_tr)\n                    ss_non.loc[:, df_train_targets_nonscored.columns[tar]] += model.predict_proba(x_tt)[:, 1] / (N_SPLITS * N_STARTS)\n                    res.loc[te, df_train_targets_nonscored.columns[tar]] += model.predict_proba(x_val)[:, 1] / N_STARTS\n\n                else:\n\n                    print(f'Target {tar}: Seed {seed}: Fold {n}: SVC probabilistic output failure.')\n                    collect_tar_failure.append(tar)\n                    model = SVC(cache_size = 2000)\n                    model.fit(x_tr, y_tr)\n                    ss_non.loc[:, df_train_targets_nonscored.columns[tar]] += model.predict(x_tt) / (N_SPLITS * N_STARTS)\n                    res.loc[te, df_train_targets_nonscored.columns[tar]] += model.predict(x_val) / N_STARTS\n    \n        score = log_loss(df_train_targets_nonscored.loc[:, df_train_targets_nonscored.columns[tar]], res.loc[:, df_train_targets_nonscored.columns[tar]])\n        if (score<=0.02) and (tar not in collect_tar_failure):\n            collect_tar_log012.append(tar)\n        if tar not in collect_tar_failure:\n            collect_tar.append(tar)\n        \n    print(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] Target {tar}:', score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"dgIrzdQZvgTC","outputId":"a2ab66fb-1783-45aa-b1ef-6a6f719dce6a"},"cell_type":"code","source":"print(f'Model OOF Metric: {log_loss_metric_n(df_train_targets_nonscored, res)}')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(collect_tar_log012))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_non.shape,res.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#synthetic variables from SVC\ndf_train_svc = pd.DataFrame(np.array(res.iloc[:,collect_tar_log012]), columns=[f'svc-{i}' for i in range(len(collect_tar_log012))])\ndf_test_svc = pd.DataFrame(np.array(ss_non.iloc[:,collect_tar_log012]), columns=[f'svc-{i}' for i in range(len(collect_tar_log012))])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prepare data for Neural Network.\nMake analysis without controls"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#svc datasets\ndf_train_svc=df_train_svc.iloc[df_train.index]\ndf_test_svc=df_test_svc.iloc[df_test.index]\ndf_train_svc.reset_index(drop=True, inplace=True)\ndf_test_svc.reset_index(drop=True, inplace=True)\n\nprint(df_train.shape,df_train_targets.shape,df_test.shape,df_train_svc.shape, df_test_svc.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_targets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all = pd.concat([df_train, df_test], ignore_index=True)\nprint(data_all.shape)\ndata_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(data):\n    #one hot encoding\n    #no controls\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    #with controls\n    #data = pd.get_dummies(data, columns=['cp_type','cp_time','cp_dose'])\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data_all = preprocess(data_all)\n#one_hot\ndata_all=process_data(data_all)\ndata_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#change ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#change numericals for predictors tstudent significant\nnumerical_features=list(train_features.columns[4:])\n\nprint(numerical_features[:10],numerical_features[860:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all_numerical=data_all.loc[:,numerical_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all_numerical.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apply variance threshold of 0.9 to the numerical data, then apply scaling a scaler and then PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"def variance_threshold(dataframe):   \n    'return dataframe containing all features with higher variance are the threshold'\n    #return numpy data\n    selector=VarianceThreshold(threshold=0.90)\n    selector.fit_transform(dataframe)\n    mask=selector.get_support(indices=False)\n    dataframe_s=dataframe.iloc[:,mask]\n    print('number of features removed',dataframe.shape[1]-dataframe_s.shape[1])\n    return dataframe_s\n\ndef scaling_data(scaler_name,dataframe):\n    'Apply a scaling method to a dataframe, which include exclusively numerical data' \n    'quantile_normal, rank_gauss, min_max, standard, gaussian_yeo, normal_l2, robust_scaler'\n    #return numpy data\n    if scaler_name=='rank_gauss':\n        #?same quantile transform normal\n        scaler =GaussRankScaler()\n        \n    elif scaler_name=='standard':\n        scaler= StandardScaler()\n        \n    elif scaler_name=='min_max':\n        scaler= MinMaxScaler()\n        \n    elif scaler_name=='gaussian_yeo':\n        scaler=PowerTransformer(method='yeo-johnson', standardize=True, copy=False)\n        \n    elif  scaler_name=='robust_scaler':\n        scaler=RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=False)\n        #scaler=RobustScaler(with_centering=True, with_scaling=True, quantile_range=(10.0, 90.0), copy=False)\n    \n    elif scaler=='quantile_normal':\n        scaler=QuantileTransformer(n_quantiles=1000, output_distribution='normal', random_state=None, copy=False)\n        \n    elif scaler=='normal_l2':\n        scaler=Normalizer(norm='l2',copy=False)\n                \n    dataframe.loc[:,:]=scaler.fit_transform( dataframe.loc[:,:])\n    \n    return dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#features\ndata_all_var=variance_threshold(data_all_numerical)\nscaler_name='rank_gauss'\n#scaler_name='standard'\ndata_all_sc=scaling_data(scaler_name,data_all_numerical)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all_sc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check if there are nan values\n#np.isnan(data_all_sc).any()\ndata_all_sc.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def PCA_descriptors(data,ncompo_genes,ncompo_cells):\n    'introduce PCA descriptors'\n    data_all=data.copy()\n    #base_seed = 2020\n\n    GENES = [col for col in data_all.columns if col.startswith('g-')]\n    CELLS = [col for col in data_all.columns if col.startswith('c-')]\n\n    pca_genes = PCA(n_components=ncompo_genes, random_state=42).fit_transform(data_all[GENES])\n    pca_cells = PCA(n_components=ncompo_cells, random_state=42 ).fit_transform(data_all[CELLS])\n    #pca_genes = PCA(n_components=ncompo_genes).fit_transform(data_all[GENES])\n    #pca_cells = PCA(n_components=ncompo_cells).fit_transform(data_all[CELLS])\n    pca_genes = pd.DataFrame(pca_genes, columns=[f'pca_g-{i}' for i in range(ncompo_genes)])\n    pca_cells = pd.DataFrame(pca_cells, columns=[f'pca_c-{i}' for i in range(ncompo_cells)])\n    data_pca = pd.concat([pca_genes, pca_cells], axis=1)\n    \n    return data_pca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2 add PCA features\n\nscaler_name='gaussian_yeo'\n\ndata_all_g=scaling_data(scaler_name,data_all_numerical)\n\nncompo_genes = 70#70\nncompo_cells = 15\n\ndata_pca=PCA_descriptors(data_all_g,ncompo_genes,ncompo_cells)\n#data_pca=data_all_pca.iloc[:,872:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pca.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#one hot categorical data\ndata_cat=data_all.iloc[:,873:]\ndata_cat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#join all data\ndata_all_new=pd.concat([data_cat,data_all_sc, data_pca],axis=1)\n\ndata_all_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preparation for the neural network: separate data in train and test\nrows_train=df_train.shape[0]\ntrain=data_all_new[:rows_train]\ntest=data_all_new[df_train.shape[0]:]\ntest.reset_index(drop=True,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#join scv data\ntrain=pd.concat([train,df_train_svc],axis=1)\n\ntest=pd.concat([test,df_test_svc],axis=1)\ntrain.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape,df_train_targets.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add training model (NN) -Riad  & Kushal-"},{"metadata":{"trusted":true},"cell_type":"code","source":"#preparation of the data. concatenate targets to the train data \n\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n#del df_train_targets['sig_id']\ntrain=pd.concat([train,df_train_targets],axis=1)\ntarget = train[df_train_targets.columns]\n\ntarget_cols = target.columns.values.tolist()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(target_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape,target.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cv folds\nfolds = train.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=7)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)\nfolds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\n\nprint(sample_submission.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n#         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    return final_loss\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.26)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.26)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n        \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        return x\n    \n\nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            # true_dist = pred.data.clone()\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to change\n\nfeature_cols = [c for c in folds.columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS =25 #25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7           \nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=1300\nprint(num_features,num_targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = folds\n    test_ = test\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n   \n    oof = np.zeros((len(train), target.shape[1]))\n    best_loss = np.inf\n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n            \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Averaging on multiple SEEDS\n\nSEED = [0,1,2] #<-- Update\n\n\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n#SEED = [0,1,2,3,4,5,6]\nfor seed in SEED:\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_targets[target_cols].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[target_cols].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#valid_results = df_train_targets.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\ndef log_loss_metric(y_true, y_pred):\n    metrics = []\n    for _target in df_train_targets.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels = [0,1]))\n    return np.mean(metrics)\n\n\ny_true = df_train_targets[target_cols].values\ny_pred = oof\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    print(i,target_cols[i],score_ )\n    score += score_ / target.shape[1]\n    \nprint(\"CV log_loss: \", score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submit\ntest_pred=test[target_cols]\n\nsig_id = test_features[test_features['cp_type']!='ctl_vehicle'].sig_id.reset_index(drop=True)\n\ntest_pred['sig_id'] = sig_id\n\nsub = pd.merge(test_features[['sig_id']], test_pred, on='sig_id', how='left')\nsub.fillna(0, inplace=True)\n\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}