{"cells":[{"metadata":{"papermill":{"duration":0.013916,"end_time":"2020-10-16T16:19:30.403429","exception":false,"start_time":"2020-10-16T16:19:30.389513","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<center><h2 style='color:red'>MoA | Keras [NewBaseLine] with Features Engineering<br>Smoothing Vs Non-Smoothing</h2></center><hr>"},{"metadata":{"papermill":{"duration":0.012568,"end_time":"2020-10-16T16:19:30.429068","exception":false,"start_time":"2020-10-16T16:19:30.4165","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Model Based on: <a href='https://www.kaggle.com/elcaiseri/moa-keras-multilabel-classifier-nn-starter'>MoA | Keras Multilabel Classifier NN | Starter </a> Kernel.\n\n\n### What is new in this Kernel?\n 1. Features Engineering, and it contains:\n- 3 SKlearn preprocessing scaler\n- Apply Rank Gauss.\n- PCA\n- SVD <== NEW\n\n 2. Feature Selection:\n- VarianceThreshold\n\n 3. Clean Data:\n- Mapping Data\n- drop train['cp_type'] column\n\n 4. Model:\n- using LeakyReLU rather than 'relu'\n- Add model smoothing\n \n* Initialize Dense Layers with \"VarianceScaling\" / \"TruncatedNormal\" ==> ' https://keras.io/api/layers/initializers/ '\n* Monitor the loss without smoothing as well and Plot the results. (From @imeintanis comment on V5)\n\n<hr><h4>Pls <span style='color:red'>UPVOTE</span>, if you find it useful. Feedbacks is also very much appreciated.<h4>"},{"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-10-16T16:19:30.46102Z","iopub.status.busy":"2020-10-16T16:19:30.460236Z","iopub.status.idle":"2020-10-16T16:19:31.342904Z","shell.execute_reply":"2020-10-16T16:19:31.341925Z"},"papermill":{"duration":0.901014,"end_time":"2020-10-16T16:19:31.343018","exception":false,"start_time":"2020-10-16T16:19:30.442004","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterativestratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-10-16T16:19:31.373319Z","iopub.status.busy":"2020-10-16T16:19:31.372483Z","iopub.status.idle":"2020-10-16T16:19:31.375425Z","shell.execute_reply":"2020-10-16T16:19:31.374952Z"},"papermill":{"duration":0.019372,"end_time":"2020-10-16T16:19:31.375525","exception":false,"start_time":"2020-10-16T16:19:31.356153","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-10-16T16:19:31.40887Z","iopub.status.busy":"2020-10-16T16:19:31.408147Z","iopub.status.idle":"2020-10-16T16:19:37.048721Z","shell.execute_reply":"2020-10-16T16:19:37.047604Z"},"papermill":{"duration":5.660537,"end_time":"2020-10-16T16:19:37.048869","exception":false,"start_time":"2020-10-16T16:19:31.388332","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow.keras.backend as K\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.losses import BinaryCrossentropy\nimport tensorflow_addons as tfa\n\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nfrom sklearn import preprocessing\n\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\nfrom tqdm.notebook import tqdm\n\nimport math","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-10-16T16:19:37.088604Z","iopub.status.busy":"2020-10-16T16:19:37.083614Z","iopub.status.idle":"2020-10-16T16:19:42.846205Z","shell.execute_reply":"2020-10-16T16:19:42.844526Z"},"papermill":{"duration":5.78303,"end_time":"2020-10-16T16:19:42.846354","exception":false,"start_time":"2020-10-16T16:19:37.063324","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\n\ndata = train_features.append(test_features)\n\nss = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random, os, torch\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nz= np.random.randint(0, 100, size=10)\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(18, 8))\nsns.distplot(train_features.iloc[:, z], bins=30, color='red', label='Test')\nsns.distplot(test_features.iloc[:, z], bins=30, color='green', label='Train')\nplt.legend()\nplt.title('Train / Test Distribution for z Features Before Featuring Eng.')\nplt.xlabel('z Features')\nplt.ylabel('Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scaling_ss(train, test):\n    features = train.columns[4:]\n    scaler = preprocessing.StandardScaler()\n    scaler.fit(pd.concat([train[features], test[features]], axis = 0))\n    train[features] = scaler.transform(train[features])\n    test[features] = scaler.transform(test[features])\n    return train, test, features\n\n#train_features, test_features, features = scaling_ss(train_features, test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scaling_mm(train, test):\n    features = train.columns[2:]\n    scaler = preprocessing.MinMaxScaler()\n    scaler.fit(pd.concat([train[features], test[features]], axis = 0))\n    train[features] = scaler.transform(train[features])\n    test[features] = scaler.transform(test[features])\n    return train, test, features\n\n#train_features, test_features, features = scaling_mm(train_features, test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scaling_rs(train, test):\n    features = train.columns[4:]\n    scaler = preprocessing.RobustScaler()\n    scaler.fit(pd.concat([train[features], test[features]], axis = 0))\n    train[features] = scaler.transform(train[features])\n    test[features] = scaler.transform(test[features])\n    return train, test, features\n\ntrain_features, test_features, features = scaling_rs(train_features, test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RankGauss\nfor col in (GENES + CELLS):\n    transformer = QuantileTransformer(n_quantiles=206,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GENES PCA\nn_comp = 600  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CELLS PCA\nn_comp = 60  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GENES SVD\nn_comp = 450  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (TruncatedSVD(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'svd_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'svd_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CELLS SVD\nn_comp = 45  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (TruncatedSVD(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'svd_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'svd_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def c_squared(train, test):\n    \n    features_c = list(train.columns[776:876])\n    for df in [train, test]:\n        for feature in features_c:\n            df[f'{feature}_squared'] = df[feature] ** 2\n    return train, test\n\ntrain_features,test_features=c_squared(train_features,test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def c_cubed(train, test):\n    \n    features_c = list(train.columns[776:876])\n    for df in [train, test]:\n        for feature in features_c:\n            df[f'{feature}_cubed'] = df[feature] ** 3\n    return train, test\n\ntrain_features,test_features=c_cubed(train_features,test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def c_sqrt(train, test):\n    \n    features_c = list(train.columns[776:876])\n    for df in [train, test]:\n        for feature in features_c:\n            df[f'{feature}_sqrt'] = df[feature] ** 0.5\n    return train, test\n\ntrain_features,test_features=c_cubed(train_features,test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scaling_rs(train, test):\n    features = train.columns[4:]\n    scaler = preprocessing.RobustScaler()\n    scaler.fit(pd.concat([train[features], test[features]], axis = 0))\n    train[features] = scaler.transform(train[features])\n    test[features] = scaler.transform(test[features])\n    return train, test, features\n\n#train_features, test_features, features = scaling_rs(train_features, test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'New Train/Test Features Dataset Contains [{train_features.shape[1]}] Features.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = 0.9\nvar_thresh = VarianceThreshold(threshold)\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Variance Threshold Select [{train_features.shape[1]}] Features From [1836]]')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_features.copy()\ntarget = train_targets.copy()\ntest = test_features.copy()\n\ntarget = target[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntarget.drop(['sig_id'], axis=1, inplace=True)\n\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntrain.drop(['sig_id', 'cp_type'], axis=1, inplace=True)\n\ntest.drop(['sig_id', 'cp_type'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test, features = scaling_mm(train, test)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-16T16:19:42.885878Z","iopub.status.busy":"2020-10-16T16:19:42.88448Z","iopub.status.idle":"2020-10-16T16:19:43.172884Z","shell.execute_reply":"2020-10-16T16:19:43.172261Z"},"papermill":{"duration":0.311691,"end_time":"2020-10-16T16:19:43.173005","exception":false,"start_time":"2020-10-16T16:19:42.861314","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    df = df.copy()\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    #df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})    \n    df = pd.get_dummies(df, columns=['cp_time','cp_dose'])\n    return df\n\ntrain = preprocess(train)\ntest = preprocess(test)\ndata = train.append(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nz= np.random.randint(0, 100, size=10)\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(18, 8))\nsns.distplot(test.iloc[:, z], bins=30, color='red', label='Test')\nsns.distplot(train.iloc[:, z], bins=30, color='green', label='Train')\nplt.legend()\nplt.title('Train / Test Distribution for z Features After Featuring Eng.')\nplt.xlabel('z Features')\nplt.ylabel('Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(train.values), np.std(train.values), np.min(train.values), np.max(train.values)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-16T16:19:43.699017Z","iopub.status.busy":"2020-10-16T16:19:43.698363Z","iopub.status.idle":"2020-10-16T16:19:43.702842Z","shell.execute_reply":"2020-10-16T16:19:43.702315Z"},"papermill":{"duration":0.025041,"end_time":"2020-10-16T16:19:43.702946","exception":false,"start_time":"2020-10-16T16:19:43.677905","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"somthing_rate = 1e-3\nP_MIN = somthing_rate\nP_MAX = 1 - P_MIN\n\ndef loss_fn(yt, yp):\n    yp = np.clip(yp, P_MIN, P_MAX)\n    return log_loss(yt, yp, labels=[0,1])\n\nNUM_FEATURES = train.shape[1]\nNUM_FEATURES","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-16T16:19:43.747906Z","iopub.status.busy":"2020-10-16T16:19:43.746226Z","iopub.status.idle":"2020-10-16T16:19:43.748896Z","shell.execute_reply":"2020-10-16T16:19:43.74936Z"},"papermill":{"duration":0.032362,"end_time":"2020-10-16T16:19:43.74948","exception":false,"start_time":"2020-10-16T16:19:43.717118","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def create_model(num_columns, hidden_layers=1500, SEED=None):\n    model = tf.keras.Sequential([tf.keras.layers.Input(num_columns)])\n    #initializer = tf.keras.initializers.VarianceScaling(scale=2., mode='fan_in', distribution='truncated_normal', seed=SEED)#math.sqrt(6. / n) \n    initializer = tf.keras.initializers.TruncatedNormal(mean=0.5, stddev=1., seed=SEED) \n\n    model.add(tf.keras.layers.BatchNormalization())\n    #model.add(tf.keras.layers.Dropout(0.4))\n    model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(hidden_layers, kernel_initializer=initializer)))\n    #model.add(tf.keras.layers.Activation('elu'))\n    model.add(tf.keras.layers.LeakyReLU())\n    \n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.2654321))\n    model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(hidden_layers, kernel_initializer=initializer)))\n    #model.add(tf.keras.layers.Activation('elu'))\n    model.add(tf.keras.layers.LeakyReLU())\n\n    #============ Final Layer =================\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.2678923456789))\n    model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, kernel_initializer=initializer)))\n    model.add(tf.keras.layers.Activation('sigmoid'))\n    \n    tfa_opt = tfa.optimizers.Lookahead(tfa.optimizers.AdamW(lr = 1e-2, weight_decay = 1e-5), sync_period=10)\n    tf_opt = tfa.optimizers.Lookahead(tf.keras.optimizers.Adam(learning_rate=1e-3, epsilon=1e-15), sync_period=10)\n    \n    model.compile(optimizer=tfa_opt, \n                  loss=BinaryCrossentropy(),\n                  metrics=BinaryCrossentropy(label_smoothing=somthing_rate)\n                  )\n    return model","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-16T16:19:43.784646Z","iopub.status.busy":"2020-10-16T16:19:43.783963Z","iopub.status.idle":"2020-10-16T16:19:43.788012Z","shell.execute_reply":"2020-10-16T16:19:43.788505Z"},"papermill":{"duration":0.024438,"end_time":"2020-10-16T16:19:43.788625","exception":false,"start_time":"2020-10-16T16:19:43.764187","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Use All feats as top feats\ntop_feats = [i for i in range(train.shape[1])]\nprint(\"Top feats length:\",len(top_feats))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-10-16T16:19:43.824388Z","iopub.status.busy":"2020-10-16T16:19:43.823545Z","iopub.status.idle":"2020-10-16T16:19:47.10706Z","shell.execute_reply":"2020-10-16T16:19:47.106425Z"},"papermill":{"duration":3.30196,"end_time":"2020-10-16T16:19:47.107219","exception":false,"start_time":"2020-10-16T16:19:43.805259","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"mod = create_model(len(top_feats))\nmod.summary()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-16T16:19:47.143894Z","iopub.status.busy":"2020-10-16T16:19:47.143266Z","iopub.status.idle":"2020-10-16T16:19:47.147397Z","shell.execute_reply":"2020-10-16T16:19:47.146444Z"},"papermill":{"duration":0.024422,"end_time":"2020-10-16T16:19:47.147494","exception":false,"start_time":"2020-10-16T16:19:47.123072","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def metric(y_true, y_pred):\n    metrics = []\n    for _target in train_targets.columns:\n        metrics.append(loss_fn(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float)))\n    return np.mean(metrics)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2020-10-16T16:19:47.200564Z","iopub.status.busy":"2020-10-16T16:19:47.19534Z","iopub.status.idle":"2020-10-16T17:07:48.108476Z","shell.execute_reply":"2020-10-16T17:07:48.109328Z"},"papermill":{"duration":2880.947148,"end_time":"2020-10-16T17:07:48.109552","exception":false,"start_time":"2020-10-16T16:19:47.162404","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"N_STARTS = 5\n\ntrain_targets = target\n\nres = train_targets.copy()\nss.loc[:, train_targets.columns] = 0\nres.loc[:, train_targets.columns] = 0\n\nhistorys = dict()\n\n#tf.random.set_seed(42)\nseed_everything(seed=42)\nfor seed in range(N_STARTS):\n    for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits=7, random_state=seed, shuffle=True).split(train_targets, train_targets)):\n        print(f\"======{train_targets.values[tr].shape}========{train_targets.values[te].shape}=====\")\n        print(f'Seed: {seed} => Fold: {n}')\n        \n        checkpoint_path = f'repeat:{seed}_Fold:{n}.hdf5'\n        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.5, min_lr=1e-20, patience=6, verbose=1, mode='min')\n        cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 1, save_best_only = True,\n                                     save_weights_only = True, mode = 'min')\n        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", restore_best_weights=True, patience= 14, verbose = 1)\n        \n        model = create_model(len(top_feats), SEED=seed)\n        \n        history = model.fit(train.values[tr][:, top_feats],\n                  train_targets.values[tr],\n                  validation_data=(train.values[te][:, top_feats], train_targets.values[te]),\n                  epochs=100, batch_size=128,\n                  callbacks=[reduce_lr_loss, cb_checkpt, early], verbose=2\n                 )\n        historys[f'history_seed_{seed+1}_fold_{n+1}'] = history\n        print(\"Model History Saved.\")\n        \n        model.load_weights(checkpoint_path)\n        test_predict = model.predict(test.values[:, top_feats])\n        val_predict = model.predict(train.values[te][:, top_feats])\n        \n        ss.loc[:, train_targets.columns] += test_predict\n        res.loc[te, train_targets.columns] += val_predict\n        \n        print(f'OOF Metric For SEED {seed} => FOLD {n} : {metric(train_targets.loc[te, train_targets.columns], pd.DataFrame(val_predict, columns=train_targets.columns))}')\n        print('+-' * 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Smoothing vs Non-Smoothing"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-16T17:07:51.519028Z","iopub.status.busy":"2020-10-16T17:07:51.518156Z","iopub.status.idle":"2020-10-16T17:07:52.12248Z","shell.execute_reply":"2020-10-16T17:07:52.122969Z"},"papermill":{"duration":2.191779,"end_time":"2020-10-16T17:07:52.123104","exception":false,"start_time":"2020-10-16T17:07:49.931325","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Show Model loss in plots\nfor k,v in historys.items():\n    loss = []\n    val_loss = []\n    loss.append(v.history['loss'][:35])\n    val_loss.append(v.history['val_loss'][:35])\n    \n# Show Model loss in plots\nfor k,v in historys.items():\n    bin_loss = []\n    bin_val_loss = []\n    bin_loss.append(v.history['binary_crossentropy'][:35])\n    bin_val_loss.append(v.history['val_binary_crossentropy'][:35])\n    \nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 3, figsize = (23, 6))\n\nax[0].plot(np.mean(bin_loss, axis=0), 'b', label='Smoothing Loss')\nax[0].plot(np.mean(bin_val_loss, axis=0), 'r--', label='Smoothing Val Loss')\nax[0].set(title=f'{somthing_rate}-Somthing Model', yscale='log', yticks=[1,1e-1,1e-2], xlabel='Epoches', ylabel='Average Logloss')\nax[0].legend()\n\nax[1].plot(np.mean(loss, axis=0), 'b', label='Non-Smoothing Loss')\nax[1].plot(np.mean(val_loss, axis=0), 'g--',label='Non-Smoothing Val Loss')\nax[1].set(title='Non-Somthing Model', yscale='log', yticks=[1,1e-1,1e-2], xlabel='Epoches', ylabel='Average Logloss')\nax[1].legend()\n\n\nax[2].plot(np.mean(bin_val_loss, axis=0), 'r+', label='Smoothing Val Loss')\nax[2].plot(np.mean(val_loss, axis=0), 'g*',label='Non-Smoothing Val Loss')\nax[2].set(title='Somthing vs Non-Somthing Model', yscale='log', xlabel='Epoches', ylabel='Average Logloss')\nax[2].legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.loc[:, train_targets.columns] /= ((n+1) * N_STARTS)\nres.loc[:, train_targets.columns] /= N_STARTS","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-16T17:07:55.295473Z","iopub.status.busy":"2020-10-16T17:07:55.294456Z","iopub.status.idle":"2020-10-16T17:07:57.281574Z","shell.execute_reply":"2020-10-16T17:07:57.282295Z"},"papermill":{"duration":3.560315,"end_time":"2020-10-16T17:07:57.28245","exception":false,"start_time":"2020-10-16T17:07:53.722135","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"print(f'OOF Metric: {metric(train_targets, res)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save('oof_keras', res)\nnp.save('pred_keras', ss)\n\nss.to_csv('submission_test.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-16T17:08:00.755317Z","iopub.status.busy":"2020-10-16T17:08:00.7544Z","iopub.status.idle":"2020-10-16T17:08:04.01642Z","shell.execute_reply":"2020-10-16T17:08:04.015168Z"},"papermill":{"duration":5.137996,"end_time":"2020-10-16T17:08:04.016548","exception":false,"start_time":"2020-10-16T17:07:58.878552","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"ss.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":1.558457,"end_time":"2020-10-16T17:08:07.159758","exception":false,"start_time":"2020-10-16T17:08:05.601301","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Kernel still under modification.. **<span style='color:red'>Feedbacks</span>** is also very much appreciated.\nPls **<span style='color:red'>UPVOTE</span>**, if you find it useful. \n"},{"metadata":{"papermill":{"duration":1.74362,"end_time":"2020-10-16T17:08:10.495084","exception":false,"start_time":"2020-10-16T17:08:08.751464","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}