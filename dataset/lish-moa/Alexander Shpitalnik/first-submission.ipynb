{"cells":[{"metadata":{"id":"k3Iz1heP_nuZ","outputId":"6e9ce0e7-d34c-4130-9f6c-c51cd16ee876","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n!mkdir /models\nimport os\nfrom sklearn.model_selection import KFold\nfrom IPython.display import clear_output","execution_count":null,"outputs":[]},{"metadata":{"id":"mBXsGQE_Dypl","trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\n\ntest_features = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\nsample_submission = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"4aPdescUDyz1","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"mdhlEMqW_nuh","trusted":true},"cell_type":"code","source":"def preprocess(df):\n    \"\"\"Returns preprocessed data frame\"\"\"\n    df = df.copy()\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})\n    del df['sig_id']\n    return df","execution_count":null,"outputs":[]},{"metadata":{"id":"cd1b-RIi_num","trusted":true},"cell_type":"code","source":"train = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']","execution_count":null,"outputs":[]},{"metadata":{"id":"4h4yIJft_nur","trusted":true},"cell_type":"code","source":"train_targets = train_targets.loc[train['cp_type']==0].reset_index(drop=True)\ntrain = train.loc[train['cp_type']==0].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"REkiW2eF_nux","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaler.fit(train)\n\n# scale train data\ntrain = scaler.transform(train)\n# scale test data\ntest = scaler.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"id":"np6151LsZySZ","trusted":true},"cell_type":"code","source":"targets = [col for col in train_targets.columns]","execution_count":null,"outputs":[]},{"metadata":{"id":"3pmt0c4iSZ-B","trusted":true},"cell_type":"code","source":"train_targets = train_targets.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"id":"qkQ-trLh_nu-","trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return self.features.shape[0]\n    \n    def __getitem__(self, idx):\n        return {\n            'input': torch.tensor(self.features[idx, :], dtype=torch.float),\n            'target': torch.tensor(self.targets[idx, :], dtype=torch.float)\n        }\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n    \n    def __len__(self):\n        return self.features.shape[0]\n    \n    def __getitem__(self, idx):\n        return {\n            'input': torch.tensor(self.features[idx, :], dtype=torch.float)\n        }","execution_count":null,"outputs":[]},{"metadata":{"id":"Z1IAxM4s_nvK","trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.2) # 0.2\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.2) # 0.2\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.2) # 0.2\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.weight = torch.tensor([0.5]).to(device)\n        \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        \n        x = F.prelu(self.dense1(x), self.weight) # relu -> prelu\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.prelu(self.dense2(x), self.weight) # relu -> prelu\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"id":"o8KV6JWO_nvY","trusted":true},"cell_type":"code","source":"def set_seed(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"id":"ErW2iPgp_nvi","trusted":true},"cell_type":"code","source":"def get_dataloaders(num_workers, batch_size, x_train, y_train, x_valid, y_valid):\n    \"\"\"Return training and valid dataloader\"\"\"\n    \n    # load the training and valid datasets\n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n\n    # prepare data loaders\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers)\n\n    # define loaders\n    loader = {\n        \"train\": train_loader,\n        \"valid\": valid_loader\n    }\n    \n    return loader","execution_count":null,"outputs":[]},{"metadata":{"id":"Wtv7zH_p_nvm","trusted":true},"cell_type":"code","source":"def get_testloaders(num_workers, batch_size, x_test):\n    \"\"\"Return test dataloader\"\"\"\n    \n    # load the test datasets\n    test_dataset = TestDataset(x_test)\n    \n    # prepare test loader\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\n    \n    # define loaders\n    loader = {\n        'test': test_loader\n    }\n    return loader","execution_count":null,"outputs":[]},{"metadata":{"id":"M_vCUhAs_nvr","trusted":true},"cell_type":"code","source":"from torch.optim.lr_scheduler import StepLR\n\ndef train_model(n_epochs, loaders, model, optimizer, criterion, device, save_path):\n    \"\"\"Returns a trained model\"\"\"        \n    scheduler = StepLR(optimizer, step_size=2, gamma=0.96)\n    \n    # initialize tracker for minimum validation loss\n    valid_loss_min = np.Inf\n    for epoch in range(1, n_epochs + 1):\n        # decay Learning Rate\n        scheduler.step()\n        # print(f'Epoch: \\t{epoch}\\tLR: {scheduler.get_lr()}')\n        \n        # initialize variables to monitor training and validation loss\n        train_loss = 0.0\n        valid_loss = 0.0\n        \n        # train the model\n        model.train()\n        \n        #for batch_idx, (data, target) in enumerate(loaders['train']):\n        for data in loaders['train']:\n            data_input, data_target = data['input'].to(device), data['target'].to(device)\n            \n            # initialize weights to zero: clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            \n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data_input)\n\n            # calcuate loss\n            loss = criterion(output, data_target)\n            \n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            \n            # perform a single optimization step\n            optimizer.step()\n            \n            # TODO: scheduler.step()\n            \n            # update running training loss\n            # print(\"train loss : \", loss.item())\n            train_loss += (loss.item() / len(loaders['train']))\n\n        # validate the model\n        model.eval()\n        \n        for data in loaders['valid']:\n            data_input, data_target = data['input'].to(device), data['target'].to(device)\n            \n            # update the average validation loss\n            output = model(data_input)\n            \n            # calculate loss\n            loss = criterion(output, data_target)\n            \n            # update running validation loss\n            # print(\"validation loss : \", loss.item())\n            valid_loss += (loss.item() / len(loaders['valid']))\n        \n        # print training/validation statistics\n        \n        \n        # save the model if validation loss has descrased\n        if valid_loss < valid_loss_min:\n            print(f'Epoch: \\t{epoch}\\tValidation loss decreased ({valid_loss_min} -> {valid_loss}). Saving the model...')\n            torch.save(model.state_dict(), save_path) ####################################\n            valid_loss_min = valid_loss\n    clear_output(wait=True)\n    # return trained model\n    return model","execution_count":null,"outputs":[]},{"metadata":{"id":"nGO5VnHt_nvv","trusted":true},"cell_type":"code","source":"def run_training(seed, kfold, batch_size, epochs, learning_rate, weight_decay):\n    set_seed(seed)\n\n    for fold, (train_idx, valid_idx) in enumerate(kfold.split(X=train, y=train_targets)):\n        x_train, x_valid = train[train_idx], train[valid_idx]\n        y_train, y_valid = train_targets[train_idx], train_targets[valid_idx]\n        \n        # get dataloaders\n        dataloaders = get_dataloaders(0, batch_size, x_train, y_train, x_valid, y_valid)\n        \n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        model = Model(875, 206, 1024).to(device)\n        \n        criterion_moa = nn.BCEWithLogitsLoss() # for multi-lable classfication\n        optimizer_moa = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n        # train the model\n        train_model(epochs, dataloaders, model, optimizer_moa, criterion_moa, device, f'/models/model_seed_{seed}_fold_{fold}.pt')","execution_count":null,"outputs":[]},{"metadata":{"id":"_OgPorPv_nvz","trusted":true},"cell_type":"code","source":"# hyper parameters\n\nFOLDS = 5\nWORKERS = 0\nBATCH_SIZE = 128\nEPOCHS = 50\nLEARNING_RATE = 0.0002\nWEIGHT_DECAY = 0.00001\nSEED = 42","execution_count":null,"outputs":[]},{"metadata":{"id":"vP1eBmk4_nv5","outputId":"6053aec2-975a-46eb-e47e-67d264026fa7","trusted":true},"cell_type":"code","source":"\nmskf = KFold(n_splits=FOLDS)\n\nfor seed in range(35, 40):\n    run_training(seed, mskf, BATCH_SIZE, EPOCHS, LEARNING_RATE, WEIGHT_DECAY)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"F0rRMpcU_nv9","trusted":true},"cell_type":"code","source":"def inference(loaders, model, device):\n    \"\"\"Return a prediction\"\"\"\n    \n    model.eval()\n    preds = []\n    \n    for data in  loaders['test']:\n        data_input = data['input'].to(device)\n        \n        # forward pass: compute predicted outputs by passing inputs to the model\n        with torch.no_grad():\n            output = model(data_input)\n        \n        pred = output.sigmoid().detach().cpu().numpy()\n        preds.append(pred)\n        \n    return np.concatenate(preds)","execution_count":null,"outputs":[]},{"metadata":{"id":"kITL03jf_nwC","trusted":true},"cell_type":"code","source":"def run_inferencing(seed):\n    set_seed(seed)\n    \n    preds = np.zeros((len(test), 206))\n    \n    #for fold, (train_idx, valid_idx) in enumerate(mskf.split(X=train, y=train_targets)):\n    for i in range(0, FOLDS):    \n        # get dataloaders\n        dataloaders = get_testloaders(0, 128, test)\n        \n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        model = Model(875, 206, 1024)\n        \n        model.load_state_dict(torch.load(f'/models/model_seed_{seed}_fold_{i}.pt'))\n        model.to(device)\n        \n        pred = inference(dataloaders, model, device)\n        \n        preds += pred\n        \n    preds = preds / FOLDS\n    return preds\n","execution_count":null,"outputs":[]},{"metadata":{"id":"olT8Vjl__nwF","trusted":true},"cell_type":"code","source":"preds = np.zeros((len(test), 206))\nfor seed in range(35, 40):\n    preds += run_inferencing(seed)\npreds = preds / 5","execution_count":null,"outputs":[]},{"metadata":{"id":"RqlTWBE-_nwK","trusted":true},"cell_type":"code","source":"sample_submission[targets] = preds\nsample_submission.loc[test_features['cp_type']=='ctl_vehicle', targets] = 0\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"gJAu4Opi_nwU","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"IfsU0orhRVWr","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}