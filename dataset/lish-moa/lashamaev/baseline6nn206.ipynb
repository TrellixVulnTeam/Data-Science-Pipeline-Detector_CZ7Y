{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import log_loss\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intercept = 1.2308681476965497\ncoef = [-2.73886094e-01,  4.44663969e-01,  2.58309348e-01,  1.39686480e+00,\n        8.85623018e-02, -6.89611023e-01,  9.43831622e-01,  6.14106873e-01,\n       -1.96819821e+00, -1.17979323e+00,  1.73010048e+00,  4.93444493e-01,\n       -1.33187370e+00, -6.43634954e-01, -5.06055757e-01, -2.21091860e+00,\n        1.82031477e-01, -2.66061106e-01,  1.61310787e+00, -6.75414170e-01,\n       -5.18521674e-01,  1.61018081e+00, -2.92999441e+00, -2.06026293e-01,\n       -7.92974054e-01,  6.86967913e-01, -2.82228497e+00,  9.93194618e-01,\n       -4.54454416e-01, -3.87405014e+00, -3.27069479e+00,  1.20769094e+00,\n        8.91929289e-01,  8.02514455e-02, -6.70496098e-01, -4.51849489e-01,\n       -5.99070941e-01,  4.95375486e-01, -2.39922254e+00,  5.35654409e-01,\n       -2.72116456e+00,  3.11487016e+00,  7.14186073e-01, -4.51849489e-01,\n        2.57022283e-01, -6.59017018e-01,  6.80157910e-01, -6.43634954e-01,\n       -3.90612159e+00,  5.33273797e-01, -3.90612159e+00, -2.90712059e+00,\n        6.14106873e-01,  2.09554087e+00, -2.79228735e+00,  6.14106873e-01,\n        9.43831622e-01, -1.45485227e+00, -1.04505884e+00, -2.93004398e-01,\n        3.23360154e+00,  6.14106873e-01, -1.97754058e+00, -9.56749035e-01,\n       -4.45015591e+00,  1.20769094e+00, -3.90612159e+00, -4.06409994e+00,\n       -1.53574354e-01, -1.18224957e+00, -2.08962628e+00,  2.23460054e+00,\n        4.32284747e-01, -7.21765744e-01,  3.85425916e-01,  9.07376713e-01,\n        6.14106873e-01,  6.63744637e-01, -2.28354890e+00, -2.73691791e-01,\n        3.07206887e-01,  5.29504158e-01, -1.33187370e+00, -6.59017018e-01,\n        9.84199617e-01, -1.62211416e+00, -5.18521674e-01,  1.11271178e+00,\n        3.89025465e-01,  9.06884193e-02, -1.20179130e+00,  5.33066910e+00,\n       -3.15708997e+00,  4.93444493e-01,  2.59423911e+00,  1.94644258e-02,\n       -3.96202834e+00, -5.24145812e-01,  1.44333982e+00, -1.97754058e+00,\n        3.89840606e+00, -2.12735927e+00,  6.14106873e-01,  3.07206887e-01,\n       -3.64521919e+00, -1.17979323e+00,  6.14106873e-01,  2.61210887e+00,\n       -6.43634954e-01, -4.00864075e-01,  1.15085975e-02, -2.72260252e+00,\n       -9.20393560e-01, -1.35940685e+00, -7.45206273e-01,  2.28764952e-01,\n       -2.35840784e+00, -2.66061106e-01,  1.61310787e+00, -9.85509689e-01,\n       -2.11967251e+00,  3.61110987e+00,  2.23460054e+00, -1.62896905e+00,\n       -1.22885035e+00,  3.61110987e+00,  1.61310787e+00,  2.58309348e-01,\n        1.83191846e+00, -1.58297153e+00,  1.44250060e+00,  6.14106873e-01,\n       -7.76211755e-01,  1.76798897e+00, -1.99295780e+00, -1.32078648e+00,\n        1.01946241e+00,  4.58198227e-01,  1.61310787e+00,  1.25035167e+00,\n        3.61110987e+00, -2.82228497e+00, -2.12567896e+00, -2.43920930e+00,\n       -4.40206431e-01, -4.54454416e-01, -5.68312317e-02, -1.22885035e+00,\n        4.16666855e-01,  3.23360154e+00,  4.81863323e-01, -9.78539586e-01,\n        4.58198227e-01, -2.49513648e+00, -1.65020112e+00, -1.43590060e-03,\n        7.42920510e-01, -1.25614598e+00,  2.61210887e+00,  3.33649893e-01,\n        1.22515166e+00, -8.25512902e-01,  3.61110987e+00, -3.97890152e+00,\n        1.45271309e+00, -9.81797155e-01, -1.01178464e+00, -9.48172201e-01,\n       -8.18434464e-01, -3.58702404e-01, -3.27069479e+00,  2.18774699e-01,\n        1.45271309e+00, -2.19029398e-01,  4.16666855e-01,  9.84199617e-01,\n        8.32917460e-01,  3.45241213e+00,  2.45341113e+00,  9.06884193e-02,\n       -8.25512902e-01, -7.24325961e-01, -2.11192325e+00, -1.00043690e+00,\n        1.83191846e+00,  8.32917460e-01, -1.71406048e+00, -6.75171222e-01,\n       -2.11558991e+00, -9.84183727e-01, -3.04075998e+00, -1.98218871e+00,\n        3.14640381e+00,  1.34680188e+00, -9.41861421e-01, -1.71559635e+00,\n       -4.00864075e-01,  4.81863323e-01,  1.61310787e+00, -4.54151439e-01,\n        1.55078265e+00, -4.26976361e-01, -2.00530613e+00, -7.77709776e-01,\n       -6.43634954e-01, -6.09975534e-01,  7.07253781e-01, -1.71406048e+00,\n       -4.00864075e-01, -7.38362082e-01, -2.26446478e+00, -1.50281796e+00,\n       -9.82983039e-01, -2.12735927e+00, -6.51200118e-01,  2.36199978e+00,\n       -7.49018100e-01, -3.97890152e+00, -1.25614598e+00, -5.24145812e-01,\n        1.61310787e+00,  1.30631433e+00, -1.55904667e+00,  7.07253781e-01,\n        6.14106873e-01, -2.95063861e+00, -2.90397577e-02, -1.99033610e+00,\n        6.14106873e-01,  4.90816857e+00, -5.73163799e-02,  6.14106873e-01,\n       -1.60979076e+00, -2.11558991e+00, -5.23654441e+00,  1.75962986e-01,\n        1.61310787e+00, -9.42243711e-01, -7.49018100e-01,  2.57022283e-01,\n       -9.97984566e-01, -2.34241643e+00,  4.32284747e-01,  1.61310787e+00,\n       -1.68109567e+00, -4.85633392e-01, -4.85633392e-01, -3.05461810e-01,\n        1.70106265e+00,  1.70514594e+00, -2.28749426e+00, -1.29048628e+00,\n       -1.28969275e+00]\n\ndef layers_num(x):\n    return int(np.round(np.dot(np.array(x),coef) + intercept))\n\ncol = pd.read_csv('../input/lish-moa/train_targets_scored.csv').columns[1:]\n\nstartword = ['-','erase','acetylcholine','receptor_agonist','atp', 'anta', 'abl', 'peptide', 'tgf', 'calci' 'cycl', 'hsp', 'im', 'proteas', 'secret', 'synth',]\n\nfor subword in startword:\n     col = [drtitle.replace(subword,'_'+subword+'_') for drtitle in col]\n\nall = [drtitle.split('_') for drtitle in col]\nall = sorted([word for item in all for word in item])\nall = [word for word in all if len(word)>0]\n\nallcounted = {word:all.count(word) for word in all}\nallcounted = {key:value for (key, value) in allcounted.items()}\nallcounted = list(allcounted.keys()) +  ['receptor_antagonist', 'tlr_antagonist', 'tlr_antagonist',\n                   'acetylcholine_receptor_antagonist','cannabinoid_receptor_antagonist',\n                   'opioid_receptor_antagonist','ppar_receptor_antagonist',\n                   'progesterone_receptor_antagonist','serotonin_receptor_antagonist']\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n# eps, weight_tg, weight_x3, weight_x5 = 10**(-3), 0.0, 0.0, 0.0 # last couple =0.0eps, weight_tg, weight_x3, weight_x5 = 10**(-5), 0.0, 0.0, 0.0 # last couple =0.0\n# weight = 1.0 - weight_tg - weight_x3 - weight_x5\neps = 10**(-7)\n\n\n\n\ndef cube(x):\n    x /= 10.0\n    return 10.0 * x*x*x\ndef tgfunc(x):\n    return  np.tan(np.pi *x/ 4.0) \ndef application_transform(x):\n    \"\"\"  x = 2*x-1.0\n    x1 = weight_tg*tgfunc(x) + (weight_x3 + weight_x5*x**2)*x**3 + weight*x\n    x1 = x1/2 + 0.5\"\"\"\n    x1 = x\n    if x1 < eps:\n        x1 = eps\n    elif x1 >= 1 - eps:\n        x1 = 1 - eps\n    return x1\n\ndef transform(x):\n    x = x[x['cp_type']=='trt_cp']\n    x.pop('cp_type')\n    x['cp_dose'] = x['cp_dose'].replace({'D1':-1, 'D2':1})\n    x['cp_time'] = x['cp_time'] // 24\n    genes = [feature for feature in x.columns if feature.startswith('g-')]\n    cells = [feature for feature in x.columns if feature.startswith('c-')]\n    x['g-mean'] = x[genes].mean(axis=1)\n    x['g-std'] = x[genes].std(axis=1)\n    x['g-kur'] = x[genes].kurtosis(axis=1)\n    x['g-skew'] = x[genes].skew(axis=1)\n    x['c-mean'] = x[cells].mean(axis=1)\n    x['c-std'] = x[cells].std(axis=1)\n    x['c-kur'] = x[cells].kurtosis(axis=1)\n    x['c-skew'] = x[cells].skew(axis=1)\n    x['mean'] = x[genes+cells].mean(axis=1)\n    x['std'] = x[genes+cells].std(axis=1)\n    x['kur'] = x[genes+cells].kurtosis(axis=1)\n    x['skew'] = x[genes+cells].skew(axis=1)\n    \n    \n    x.join(pd.get_dummies(x['cp_time']))\n    x.pop('cp_time')\n    return x\n\ndef transform2(x):\n    x.pop('cp_type')\n    x['cp_dose'] = x['cp_dose'].replace({'D1':-1, 'D2':1})\n    x['cp_time'] = x['cp_time'] // 24\n    genes = [feature for feature in x.columns if feature.startswith('g-')]\n    cells = [feature for feature in x.columns if feature.startswith('c-')]\n    x['g-mean'] = x[genes].mean(axis=1)\n    x['g-std'] = x[genes].std(axis=1)\n    x['g-kur'] = x[genes].kurtosis(axis=1)\n    x['g-skew'] = x[genes].skew(axis=1)\n    x['c-mean'] = x[cells].mean(axis=1)\n    x['c-std'] = x[cells].std(axis=1)\n    x['c-kur'] = x[cells].kurtosis(axis=1)\n    x['c-skew'] = x[cells].skew(axis=1)\n    x['mean'] = x[genes+cells].mean(axis=1)\n    x['std'] = x[genes+cells].std(axis=1)\n    x['kur'] = x[genes+cells].kurtosis(axis=1)\n    x['skew'] = x[genes+cells].skew(axis=1)\n    \n    \n    x.join(pd.get_dummies(x['cp_time']))\n    x.pop('cp_time')\n    return x\n\nX_all = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ny_all = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\nX = transform(X_all[X_all.columns[1:]]).astype(np.float16)\nY = y_all[X_all['cp_type']=='trt_cp']\nY = Y[Y.columns[1:]].astype(np.int8)\n\ndef metric(y_true, y_pred):\n    metrics = []\n    for _target in train_targets.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n    return np.mean(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def write_results(drug_no, val_loss0, time_to_train0):\n    return {\n        'drug_no': drug_no,\n        'loss0': val_loss0,\n        'train_time0' : time_to_train0\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model1(num_columns):\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(num_columns),\n    tf.keras.layers.BatchNormalization(),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2000, activation=\"sigmoid\")),        \n    tf.keras.layers.Dense(1, activation=\"sigmoid\") #)\n    ])\n    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period=10),\n                  loss='binary_crossentropy', \n                  )\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model2(num_columns):\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(num_columns),\n    tf.keras.layers.BatchNormalization(),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1500, activation=\"sigmoid\")), \n    tf.keras.layers.BatchNormalization(),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(100, activation=\"sigmoid\")),         \n    tf.keras.layers.Dense(1, activation=\"sigmoid\") \n    ])\n    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period=10),\n                  loss='binary_crossentropy', \n                  )\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model3(num_columns):\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(num_columns),\n    tf.keras.layers.BatchNormalization(),\n    tfa.layers.WeightNormalization(\n    tf.keras.layers.Dense(900, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(\n    tf.keras.layers.Dense(300, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(\n    tf.keras.layers.Dense(30, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1, activation=\"sigmoid\") )\n    ])\n    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period=10),\n                  loss='binary_crossentropy', \n                  )\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model4(num_columns):\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(num_columns),\n    tf.keras.layers.Dense(1500, activation=\"relu\"),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n    tfa.layers.WeightNormalization(\n    tf.keras.layers.Dense(400, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.6),\n    tfa.layers.WeightNormalization(\n    tf.keras.layers.Dense(20, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1, activation=\"sigmoid\") )\n    ])\n    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period=10),\n                  loss='binary_crossentropy', \n                  )\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XXXX = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\nXXXX = transform2(XXXX[XXXX.columns[1:]]).astype(np.float16)\nXXXX_length = len(XXXX)\n\nsubmission = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\n\ntrain_results = []\nimport time\n\nfor drug_i in range(206):\n    \n    select_model = layers_num([1 if word in Y.columns[drug_i] else 0 for word in allcounted])\n    if select_model>4:\n        select_model = 4\n    if select_model<0:\n        select_model = 0\n        \n    print(f'beginning training on drug no.{drug_i} ')\n    Y_new = Y[Y.columns[drug_i]]\n    \n    if Y_new.sum()>4:\n        X_train, X_test, Y_train, Y_test = train_test_split(X, Y_new, test_size=0.10, \n             stratify=np.array(Y_new), random_state=2021)\n    else: \n        X_train, X_test, Y_train, Y_test = X, X, Y_new, Y_new\n        select_model=0\n        \n    print('model',select_model)\n    if select_model==0:\n            tbegin = time.clock()    \n            pseudoinverse_X = np.linalg.pinv(X_train.astype(np.float64))\n            test_predict = np.dot(X_test,np.dot(pseudoinverse_X,Y_train))\n\n            test_predict = np.array(list(map(application_transform, test_predict)))\n            loss0 = log_loss(np.array(Y_test),test_predict)\n            t0 = time.clock() - tbegin\n    \n    ##########11111111111111111111111    \n\n    if select_model==1:    \n            checkpoint_path = f'drug_no{drug_i+1}_rs.hdf5'\n            early_stopping = tf.keras.callbacks.EarlyStopping(patience=4, min_delta=10**(-3))\n            reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=5, verbose=1, epsilon=1e-4, mode='min')\n            cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n                                          save_weights_only = True, mode = 'min')\n\n            model = create_model1(X.shape[1])\n            tbegin = time.clock() \n            model.fit(X_train, Y_train,\n                      validation_data=(X_test, Y_test), \n                      epochs=50, batch_size=64, verbose=1,\n                      callbacks=[early_stopping,reduce_lr_loss, cb_checkpt])\n            model.load_weights(checkpoint_path)\n            test_predict = np.array(list(map(application_transform, model.predict(X_test))))   \n            \n            loss0 = log_loss(np.array(Y_test),test_predict)\n            t0 = time.clock() - tbegin\n\n    #########################2222222222222222222\n \n    if select_model==2: \n            checkpoint_path = f'drug_no{drug_i+1}_rs.hdf5'\n            early_stopping = tf.keras.callbacks.EarlyStopping(patience=4, min_delta=10**(-3))\n            reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=5, verbose=1, epsilon=1e-4, mode='min')\n            cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n                                          save_weights_only = True, mode = 'min')\n\n            model = create_model2(X.shape[1])\n            tbegin = time.clock() \n            model.fit(X_train, Y_train,\n                      validation_data=(X_test, Y_test), \n                      epochs=50, batch_size=64, verbose=1,\n                      callbacks=[early_stopping,reduce_lr_loss, cb_checkpt])\n            model.load_weights(checkpoint_path)\n\n            test_predict = np.array(list(map(application_transform, model.predict(X_test))))    \n            loss0 = log_loss(np.array(Y_test),test_predict)\n            t0 = time.clock() - tbegin\n  \n\n    \n    #########################3333333333333333333333\n    if select_model==3: \n            checkpoint_path = f'drug_no{drug_i+1}_rs.hdf5'\n            early_stopping = tf.keras.callbacks.EarlyStopping(patience=4, min_delta=10**(-3))\n            reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=5, verbose=1, epsilon=1e-4, mode='min')\n            cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n                                          save_weights_only = True, mode = 'min')\n\n            model = create_model3(X.shape[1])\n            tbegin = time.clock() \n            model.fit(X_train, Y_train,\n                      validation_data=(X_test, Y_test), \n                      epochs=50, batch_size=64, verbose=1,\n                      callbacks=[early_stopping,reduce_lr_loss, cb_checkpt])\n            model.load_weights(checkpoint_path)\n\n            test_predict = np.array(list(map(application_transform, model.predict(X_test))))    \n            loss0 = log_loss(np.array(Y_test),test_predict)\n            t0 = time.clock() - tbegin\n    \n    \n    #########################44444444444444444\n    if select_model==4: \n    \n            checkpoint_path = f'drug_no{drug_i+1}_rs.hdf5'\n            early_stopping = tf.keras.callbacks.EarlyStopping(patience=4, min_delta=10**(-3))\n            reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=5, verbose=1, epsilon=1e-4, mode='min')\n            cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n                                          save_weights_only = True, mode = 'min')\n\n            model = create_model4(X.shape[1])\n            tbegin = time.clock() \n            model.fit(X_train, Y_train,\n                      validation_data=(X_test, Y_test), \n                      epochs=50, batch_size=64, verbose=1,\n                      callbacks=[early_stopping,reduce_lr_loss, cb_checkpt])\n            model.load_weights(checkpoint_path)\n\n            test_predict = np.array(list(map(application_transform, model.predict(X_test))))    \n            loss0 = log_loss(np.array(Y_test),test_predict)\n            t0 = time.clock() - tbegin    \n    \n\n    train_results.append(write_results(drug_i, loss0, t0))\n    print('==================================================')\n    print('loss on validation:',train_results[-1])\n    print('==================================================')\n    pd.DataFrame(train_results).to_csv('resPreLast.csv',index=False)\n\n    \n    y_prediction =  np.array(list(map(application_transform, model.predict(XXXX).reshape(XXXX_length))))    \n    submission[Y.columns[drug_i]] = y_prediction     \n#     submission.to_csv('subs.csv', index=False) \n    print(y_prediction) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dff = pd.DataFrame(train_results)\ndff.to_csv('train_results.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[dff['train_time0'].sum()/60/60,dff['loss0'].mean()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"YYYY = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\n\nlength = len(submission.columns)-1\nfor i in range(len(submission)): \n    if YYYY.loc[i,'cp_type']!='trt_cp':\n        submission.loc[i] = [submission.loc[i,'sig_id']]+[0] * length    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def convert(x):\n#     x = str(x)\n#     x = x.replace('[','').replace(']','')\n#     return float(x)\n    \n    \n# submission2 = pd.read_csv('submission.csv') \n# for feature in submission2.columns[1:]:\n#     submission2[feature] = submission2[feature].apply(convert)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}