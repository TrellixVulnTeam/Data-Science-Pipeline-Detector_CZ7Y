{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Loading libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport random as rn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reproducible Results\nimport os\nos.environ['PYTHONHASHSEED'] = str(1337)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reproducible Results\nnp.random.seed(1337)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reproducible Results\nrn.seed(1337)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reproducible Results\ntf.compat.v1.set_random_seed(1337)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reproducible Results\nfrom keras import backend as K\n\nsession_conf = tf.compat.v1.ConfigProto(\n    intra_op_parallelism_threads=1, \n    inter_op_parallelism_threads=1\n)\n\nsess = tf.compat.v1.Session(\n    graph=tf.compat.v1.get_default_graph(), \n    config=session_conf\n)\n\ntf.compat.v1.keras.backend.set_session(sess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# To display the whole columns\npd.set_option('display.max_columns', 999)\n\nlow_memory=False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading Data\n### Fahad"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset is now stored in a Pandas dataframe\n\n#Fahad,I will add the data to the dataframe\ntrain_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ntrain_targets =  pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ntest_features = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To use them later in sumbission dataframe\ncolumns_names = list(train_targets.columns)\nsig_id = test_features[['sig_id']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA \n### Haifaa"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most of observations have only one label or no label, \n# and the others have more than 1 label  \n\nsns.set(style=\"whitegrid\")\nsns.countplot(x=train_targets[train_targets == 1].sum(axis=1), color =\"b\")\nplt.title(\"The Number of Lables Compared to Observations\")\nplt.xlabel(\"Number of Labels\")\nplt.ylabel(\"Number of Observations\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most of the labels are triggered by only one observation, \n# some of them have more than 4 observations\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(30,8))\nsns.countplot(x=train_targets[train_targets == 1].sum(axis=0), color ='r')\nplt.title(\"The Distribution of Observations Among Lables\")\nplt.xlabel(\"The Label\")\nplt.ylabel(\"Number of Observations\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-processing\n### Haifaa"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, LabelBinarizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove sig_id\ntrain_features = train_features.drop(['sig_id'], axis=1)\ntrain_targets = train_targets.drop(['sig_id'], axis=1)\n\ntest_features = test_features.drop(['sig_id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label encoder \nlb_enc = LabelEncoder()\n\ntrain_features['cp_type'] = lb_enc.fit_transform(train_features['cp_type'])\ntrain_features['cp_dose'] = lb_enc.fit_transform(train_features['cp_dose'])\n\ntest_features['cp_type'] = lb_enc.fit_transform(test_features['cp_type'])\ntest_features['cp_dose'] = lb_enc.fit_transform(test_features['cp_dose'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert into arrays\ntrain_features = train_features.values\ntrain_targets = train_targets.values\n\ntest_features = test_features.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reshape the features \nx_train_flat = train_features.reshape(-1,875)\nx_test_flat = test_features.reshape(-1,875)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA\n### Haifaa"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using PCA to keep most 90% important features\nfrom sklearn.decomposition import PCA\npca = PCA(0.9)\n\npca.fit(x_train_flat)\n\nPCA(copy=True, iterated_power='auto', n_components=0.9, random_state=None,\n  svd_solver='auto', tol=0.0, whiten=False)\n\npca.n_components_\n\ntrain_pca = pca.transform(x_train_flat)\ntest_pca = pca.transform(x_test_flat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pca.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Network Design (DNN)\n### Haifaa"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading required libraries for DL\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyper-parameters\nbatch_size = 100\nnum_classes = 206\nepochs = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nfrom sklearn.model_selection import KFold\n\n# define 10-fold cross validation test harness\nkfold = KFold(n_splits=10, shuffle=True, random_state=1337)\n\ncvscores = []\n\nfor train, test in kfold.split(train_features, train_targets):\n    \n    x_train, x_test = train_features.iloc[list(train)], train_features.iloc[list(test)]\n    y_train, y_test = train_targets.iloc[list(train)], train_targets.iloc[list(test)]\n\n    # The model architecture\n    model = Sequential()\n    model.add(Dense(1024, activation='tanh', input_shape=(875,)))\n    model.add(Dense(512, activation='tanh'))\n    model.add(Dense(512, activation='tanh'))\n    model.add(Dense(256, activation='tanh'))\n    model.add(Dense(num_classes, activation='sigmoid'))\n    \n    # Compile model\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy', f1_m])\n    \n    # Train the model\n    history = model.fit(x_train, \n                        y_train,\n                        batch_size=batch_size,\n                        epochs=10,\n                        callbacks=callbacks,\n                        verbose=1)                         \n    \n    # Evaluate the model\n    scores = model.evaluate(x_test, y_test, verbose=0)\n    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n    cvscores.append(scores[1] * 100)\nprint(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The network\nmodel = Sequential()\nmodel.add(Dense(1024, activation='tanh', input_shape=(280,)))\nmodel.add(Dense(512, activation='tanh'))\nmodel.add(Dense(512, activation='tanh'))\nmodel.add(Dense(256, activation='tanh'))\nmodel.add(Dense(num_classes, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To monitor validation set results , we will use F1 score scince we have multi-label classification\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy', f1_m])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\ncallbacks = [EarlyStopping(monitor='loss', mode='min', patience=5, verbose=1),\n             ModelCheckpoint('MoA.hdf5', monitor='loss', mode='min', \n                             verbose=1, save_best_only=True, save_weights_only=False, period=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training\nhistory = model.fit(train_pca, \n                    train_targets,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    callbacks=callbacks,\n                    verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction\n### Haifaa"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Obtain the predictions for test set\npredictions = model.predict(test_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the predictions into dataframe\nsubmission = pd.DataFrame(data=predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add sig_id to the results\nsubmission.insert(loc=0, column='1', value=sig_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting column names for the submission dataframe\nsubmission.columns = columns_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission.iloc[:,1:] = submission.iloc[:,1:].round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save submission file into a csv\nsubmission.to_csv('/kaggle/working/submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  ## Team Members:\n  \n  1- EDA (Fahad)  >> Not implemented yet\n  \n  2- Statistical Summary + Data Visualization (Bashayer) >> Not implemented yet\n  \n  3- Feature Engineering (Faris) >> Withdrown \n  \n  4- Pre-processing (Kholoud) >> Withdrown \n\n  5- Pre-processing + Building the model (Haifaa) >> Done\n  \n  6- Deploying the model = Sample Submission"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}