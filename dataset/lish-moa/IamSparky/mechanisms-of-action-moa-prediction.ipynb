{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\ntrain_features_df = pd.read_csv('../input/lish-moa/train_features.csv')\ntest_features_df = pd.read_csv('../input/lish-moa/test_features.csv')\ntrain_features_nonscored_df = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ntrain_features_scored_df = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\nsample_submission_df = pd.read_csv('../input/lish-moa/sample_submission.csv')\ntrain_drug_df = pd.read_csv(\"../input/lish-moa/train_drug.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train_features_df --> \", train_features_df.shape[0],\" X \",train_features_df.shape[1])\nprint(\"test_features_df --> \", test_features_df.shape[0],\" X \",test_features_df.shape[1])\nprint(\"train_features_nonscored_df --> \", train_features_nonscored_df.shape[0],\" X \",train_features_nonscored_df.shape[1])\nprint(\"train_features_scored_df --> \", train_features_scored_df.shape[0],\" X \",train_features_scored_df.shape[1])\nprint(\"sample_submission_df --> \", sample_submission_df.shape[0],\" X \",sample_submission_df.shape[1])\nprint(\"train_drug_df --> \", train_drug_df.shape[0],\" X \",train_drug_df.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_scored_df.info(verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_scored_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_distribution = dict()\nfor num in train_features_scored_df.columns[1:]:\n    class_distribution[num] = {1: list(train_features_scored_df[num]).count(1) / len(train_features_scored_df) , \n                               0: list(train_features_scored_df[num]).count(0) / len(train_features_scored_df) }\nfor i in class_distribution.items():\n  print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_features_df.info(verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_df[\"cp_type\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_df[\"cp_dose\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_df[\"cp_time\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.merge(train_features_df, train_features_scored_df, on=\"sig_id\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[train_features_scored_df.columns[1:]].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(train_features_scored_df.columns[1:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(train_df.cp_type.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!git clone https://github.com/trent-b/iterative-stratification.git\n%cd iterative-stratification\n!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create folds\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\ndf = train_df\n\ndf[\"kfold\"] = -1    \ndf = df.sample(frac=1).reset_index(drop=True)\ny = df[train_features_scored_df.columns[1:]].values\nkf = MultilabelStratifiedKFold(n_splits=6)\n\nfor f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n    df.loc[v_, 'kfold'] = f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cp_type_stored_values , cp_type_label = pd.factorize(df[\"cp_type\"])\ncp_type_stored_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cp_type_label[cp_type_stored_values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cp_dose_stored_values , cp_dose_label = pd.factorize(df[\"cp_dose\"])\ncp_dose_stored_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cp_dose_label[cp_dose_stored_values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"cp_type\"] = cp_type_stored_values\ndf[\"cp_dose\"] = cp_dose_stored_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df[\"cp_dose\"] = cp_dose_label[cp_dose_stored_values]  # for reversing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names = train_features_df.columns[1:]\nfeatures = df[col_names]\n\nfrom sklearn.preprocessing import StandardScaler\nfeatures = StandardScaler().fit_transform(features.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[train_features_df.columns[1:]] = features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torchcontrib","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nimport torch_xla\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.data_parallel as dp\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.utils.utils as xu\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset\nimport torch\n\nclass MoA_dataset_class(Dataset):\n  def __init__(self, id , tabular):\n    self.id = id\n    self.tabular = tabular\n\n    self.output = tabular[train_features_scored_df.columns[1:]].values\n      \n  def __len__(self):\n    return len(self.id)\n  \n  def __getitem__(self, index):\n    id = self.id[index]\n    tabular = self.tabular.iloc[:,:]\n    \n    X = tabular[train_features_df.columns[1:]]\n    X = X.values[index]\n    \n    \n    return {\n        'tabular_data' : torch.tensor(X, dtype = torch.float) , \n        'output' : torch.tensor(self.output[index], dtype = torch.float),\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fold = 0\ndf_train = df[df.kfold != fold].reset_index(drop=True)\ndf_valid = df[df.kfold == fold].reset_index(drop=True)\n\n# prepare transforms standard to MNIST\ntrain_data = MoA_dataset_class([i for i in range(len(df_train))] , df_train)\n\nval_data = MoA_dataset_class([i for i in range(len(df_valid))] , df_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dry run \nidx = 100 # taking validation data index for 100th tabular data\n\nprint(val_data[idx][\"tabular_data\"])\nprint(val_data[idx][\"output\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_data[idx][\"tabular_data\"].size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sampler = torch.utils.data.distributed.DistributedSampler(\n          train_data,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=True)\n\nvalid_sampler = torch.utils.data.distributed.DistributedSampler(\n          val_data,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_BATCH_SIZE = 32\n\nfrom torch.utils.data import DataLoader\n\ntraining_dataloader = DataLoader(train_data,\n                        num_workers=4,\n                        batch_size=TRAIN_BATCH_SIZE,\n                        sampler=train_sampler,\n                        drop_last=True\n                       )\n\nval_dataloader = DataLoader(val_data,\n                        num_workers=4,\n                        batch_size=TRAIN_BATCH_SIZE,\n                        sampler=valid_sampler,\n                        drop_last=False\n                       )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = xm.xla_device()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# implementing RestNet-18 for regression\n\nclass block(torch.nn.Module):\n  expansion = 1\n  \n  def __init__(self , in_channels , out_channels, identity_downsample = None , stride = 1):\n    super(block , self).__init__()\n\n    self.dense_layer_1 = torch.nn.Linear(in_channels , out_channels)\n    self.batch_norm_1 = torch.nn.BatchNorm1d(out_channels)\n\n    self.dense_layer_2 = torch.nn.Linear(out_channels , out_channels)\n    self.batch_norm_2 = torch.nn.BatchNorm1d(out_channels * (self.expansion))\n\n    self.relu = torch.nn.ReLU()\n\n    self.identity_downsample = identity_downsample\n\n  def forward(self, x):\n    identity = x\n\n    x = self.dense_layer_1(x)\n    x = self.batch_norm_1(x)\n    x = self.relu(x)\n\n    x = self.dense_layer_2(x)\n    x = self.batch_norm_2(x)\n    x = self.relu(x)\n\n    if self.identity_downsample is not None:\n      identity = self.identity_downsample(identity)\n\n    x += identity\n    x = self.relu(x)\n\n    return x\n\n\nclass RestNet_18_for_Regression(torch.nn.Module):\n  def __init__(self, block, number_of_inputs, layers , num_classes):\n    super(RestNet_18_for_Regression , self).__init__()\n\n    self.in_channels = 64\n\n    self.Zeroth_dense_layer = torch.nn.Linear(number_of_inputs , self.in_channels)\n    self.batch_norm_1 = torch.nn.BatchNorm1d(64)\n    self.relu_1 = torch.nn.ReLU()\n\n    self.First_dense_layer = torch.nn.Linear(self.in_channels , 64) # 3 is for number of channels\n    self.batch_norm_2 = torch.nn.BatchNorm1d(64)\n    self.relu_2 = torch.nn.ReLU()\n\n    self.layer1 = self._layer(block , 64 , layers[0], stride = 1)\n    self.layer2 = self._layer(block , 128 , layers[1], stride = 2)\n    self.layer3 = self._layer(block , 256 , layers[2], stride = 2)\n    self.layer4 = self._layer(block , 512 , layers[3], stride = 2)\n\n    self.fc = torch.nn.Linear(512 * block.expansion , num_classes)\n    self.sigmoid = torch.nn.Sigmoid()\n\n  def _layer(self, block, out_channels , num_residual_blocks, stride):\n    identity_downsample = None\n\n    layers = []\n\n    if stride != 1 or self.in_channels != out_channels * block.expansion :\n      identity_downsample = torch.nn.Sequential(torch.nn.Linear(self.in_channels , out_channels * block.expansion),\n                                                torch.nn.BatchNorm1d(out_channels * block.expansion))\n      \n    layers.append(block(self.in_channels, out_channels, identity_downsample, stride))\n    self.in_channels = out_channels * block.expansion\n\n    for i in range(num_residual_blocks - 1):\n      layers.append(block(self.in_channels , out_channels))\n\n    return torch.nn.Sequential(*layers) # *layers this will unpack the layers in the list\n\n  def forward(self, x):\n    x = self.Zeroth_dense_layer(x)\n    x = self.batch_norm_1(x)\n    x = self.relu_1(x)\n\n    x = self.First_dense_layer(x)\n    x = self.batch_norm_2(x)\n    x = self.relu_2(x)\n\n    x = self.layer1(x)\n    x = self.layer2(x)\n    x = self.layer3(x)\n    x = self.layer4(x)\n\n    x = x.reshape(x.shape[0], -1) #dimension change from 2 to 1\n    x = self.fc(x)\n    x = self.sigmoid(x)\n\n    return x\n\nmodel = RestNet_18_for_Regression(block, number_of_inputs = 875 , layers = [2, 2, 2, 2] , num_classes = 206)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for Stochastic Weight Averaging in PyTorch\nfrom torchcontrib.optim import SWA\n\nEPOCHS = 25\nnum_train_steps = int(len(train_data) / TRAIN_BATCH_SIZE / xm.xrt_world_size() * EPOCHS)\n\n# printing the no of training steps for each epoch of our training dataloader  \nxm.master_print(f'num_train_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n\nloss_fn = torch.nn.BCEWithLogitsLoss()\n\nbase_optimizer = torch.optim.Adam(model.parameters(), lr= 1e-4 * xm.xrt_world_size())\n\noptimizer = SWA(base_optimizer, swa_start=5, swa_freq=5, swa_lr=0.05)\n\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 5, verbose = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining the training loop\n\nfrom sklearn.metrics import log_loss\n\ndef train_loop_fn(data_loader, model, optimizer, device, scheduler):\n    running_loss = 0.0\n    score = 0.0\n\n    model.train()\n    \n    for batch_index,dataset in enumerate(data_loader):\n        tabular_data = dataset[\"tabular_data\"]\n        output = dataset[\"output\"]\n        \n        tabular_data = tabular_data.to(device, dtype=torch.float)\n        targets = output.to(device, dtype=torch.float)\n        \n        optimizer.zero_grad()\n\n        outputs = model(tabular_data)\n        print(type(outputs))\n        \n        loss = loss_fn(outputs , targets)\n\n        loss.backward()\n        xm.optimizer_step(optimizer)\n\n        running_loss += loss.item()\n\n    scheduler.step()\n            \n    train_loss = running_loss / float(len(train_data))\n\n    for i in range(predicted.shape[1]):\n        _score = log_loss(Actual[:,i], predicted[:,i])\n        score += _score / target.shape[1]\n\n    xm.master_print('training Loss: {:.4f} and Log Loss : {:.4f}'.format(train_loss , score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_loop_fn(data_loader, model, device):\n    running_loss = 0.0\n    score = 0.0\n    \n    model.eval()\n    \n    for batch_index,dataset in enumerate(data_loader):\n        tabular_data = dataset[\"tabular_data\"]\n        output = dataset[\"output\"]\n        \n        tabular_data = tabular_data.to(device, dtype=torch.float)\n        targets = output.to(device, dtype=torch.float)\n\n        outputs = model(tabular_data)\n        \n        loss = loss_fn(outputs , targets)\n\n        running_loss += loss.item()\n    \n    valid_loss = running_loss / float(len(val_data))\n\n    for i in range(predicted.shape[1]):\n        _score = log_loss(Actual[:,i], predicted[:,i])\n        score += _score\n\n    xm.master_print('validation Loss: {:.4f}and Log Loss : {:.4f}'.format(valid_loss , score/206))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _run():\n  for epoch in range(EPOCHS):\n      xm.master_print(f\"Epoch --> {epoch+1} / {EPOCHS}\")\n      xm.master_print(f\"-------------------------------\")\n      para_loader = pl.ParallelLoader(training_dataloader, [device])\n      train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler)\n\n      para_loader = pl.ParallelLoader(val_dataloader, [device])\n      eval_loop_fn(para_loader.per_device_loader(device), model, device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = _run()\n    optimizer.swap_swa_sgd()\n    \n# applying multiprocessing so that images get paralley trained in different cores of kaggle-tpu\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=1, start_method='fork')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}