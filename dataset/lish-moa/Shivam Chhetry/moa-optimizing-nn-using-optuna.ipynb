{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install iterative-stratification\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-08T13:13:45.533564Z","iopub.execute_input":"2021-09-08T13:13:45.533923Z","iopub.status.idle":"2021-09-08T13:13:51.613205Z","shell.execute_reply.started":"2021-09-08T13:13:45.533886Z","shell.execute_reply":"2021-09-08T13:13:51.612247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nif __name__ == \"__main__\":\n    TARGET_PATH = \"../input/lish-moa/train_targets_scored.csv\"\n    df = pd.read_csv(TARGET_PATH)\n    df.loc[:, 'kfold'] = -1\n    targets = df.drop(\"sig_id\", axis=1).values\n\n    mskf = MultilabelStratifiedKFold(n_splits=5)\n\n    for fold, (train_idx, valid_idx) in enumerate(mskf.split(X=df, y=targets)):\n        df.loc[valid_idx, 'kfold'] = fold\n\n    df.to_csv(\"train_targets_fold.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:13:51.615093Z","iopub.execute_input":"2021-09-08T13:13:51.615527Z","iopub.status.idle":"2021-09-08T13:13:55.263261Z","shell.execute_reply.started":"2021-09-08T13:13:51.615482Z","shell.execute_reply":"2021-09-08T13:13:55.262384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass MoaDataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, item):\n        return {\n            \"x\" : torch.tensor(self.features[item, :], dtype=torch.float32),\n            \"y\": torch.tensor(self.targets[item, :], dtype=torch.float32),\n        }\n\n# Engine class is use for evaluating and training\nclass Engine:\n    def __init__(self, model, optimizer, device):\n        self.model = model\n        self.device = device\n        self.optimizer = optimizer\n\n    @staticmethod\n    def loss_fn(targets, outputs):\n        return nn.BCEWithLogitsLoss()(outputs, targets)\n\n    # Training functions\n    def train(self, data_loader):\n        self.model.train()\n        final_loss = 0\n        for data in data_loader:\n            self.optimizer.zero_grad()\n            inputs = data[\"x\"].to(self.device)\n            targets = data[\"y\"].to(self.device)\n            outputs = self.model(inputs)\n            loss = self.loss_fn(targets, outputs)\n            loss.backward()\n            self.optimizer.step()\n            final_loss += loss.item()\n        return final_loss / len(data_loader)\n\n    # Evaluating function\n    def evaluate(self, data_loader):\n        self.model.eval()\n        final_loss = 0\n        for data in data_loader:\n            inputs = data[\"x\"].to(self.device)\n            targets = data[\"y\"].to(self.device)\n            outputs = self.model(inputs)\n            loss = self.loss_fn(targets, outputs)\n            final_loss += loss.item()\n        return final_loss / len(data_loader)\n\nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size, num_layers, dropout):\n        super().__init__()\n        layers = []\n        for _ in range(num_layers):\n            if len(layers) == 0:\n                layers.append(nn.Linear(num_features, hidden_size))\n                layers.append(nn.BatchNorm1d(hidden_size))\n                layers.append(nn.Dropout(dropout))\n                layers.append(nn.Linear(hidden_size, hidden_size))\n                layers.append(nn.BatchNorm1d(hidden_size))\n                layers.append(nn.Dropout(dropout))\n                layers.append(nn.ReLU())\n            else:\n                layers.append(nn.Linear(hidden_size, hidden_size))\n                layers.append(nn.BatchNorm1d(hidden_size))\n                layers.append(nn.Dropout(dropout))\n                \n                layers.append(nn.Linear(hidden_size, hidden_size))\n                layers.append(nn.BatchNorm1d(hidden_size))\n                layers.append(nn.Dropout(dropout))\n                layers.append(nn.ReLU())\n\n        layers.append(nn.Linear(hidden_size, num_targets))\n        self.model = nn.Sequential(*layers)   # Layers in the list\n\n    def forward(self, x):\n        return self.model(x)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:00:48.309879Z","iopub.execute_input":"2021-09-08T13:00:48.310195Z","iopub.status.idle":"2021-09-08T13:00:48.327405Z","shell.execute_reply.started":"2021-09-08T13:00:48.310168Z","shell.execute_reply":"2021-09-08T13:00:48.326285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nimport optuna\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nEPOCS = 100\n\n\ndef run_training(fold, params, save_model=False):\n    df = pd.read_csv('../input/lish-moa/train_features.csv')\n    df = df.drop([\"cp_type\", \"cp_time\", \"cp_dose\"], axis=1)\n    targets_df = pd.read_csv(\"./train_targets_fold.csv\")\n\n    feature_cols = df.drop(\"sig_id\", axis=1).columns\n    target_cols = targets_df.drop([\"sig_id\", \"kfold\"], axis=1).columns\n\n    df = df.merge(targets_df, on=\"sig_id\", how=\"left\")\n\n    train_df = df[df.kfold != fold].reset_index(drop=True)\n    valid_df = df[df.kfold == fold].reset_index(drop=True)\n\n    xtrain = train_df[feature_cols].to_numpy()\n    ytrain = train_df[target_cols].to_numpy()\n\n    xvalid = valid_df[feature_cols].to_numpy()\n    yvalid = valid_df[target_cols].to_numpy()\n\n    train_dataset = MoaDataset(features=xtrain, targets=ytrain)\n    valid_dataset = MoaDataset(features=xvalid, targets=yvalid)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=1024, num_workers=8, shuffle=True\n    )\n\n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset, batch_size=1024, num_workers=8\n    )\n\n    model =Model(\n        num_features=xtrain.shape[1],\n        num_targets=ytrain.shape[1],\n        num_layers=params[\"num_layers\"],\n        hidden_size=params[\"hidden_size\"],\n        dropout=params[\"dropout\"]\n    )\n\n    model.to(DEVICE)\n    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n    eng = Engine(model, optimizer=optimizer, device=DEVICE)\n\n    best_loss = np.inf\n    early_stopping_iter = 10\n    early_stopping_counter = 0\n\n    for epoch in range(EPOCS):\n        train_loss = eng.train(train_loader)\n        valid_loss = eng.evaluate(valid_loader)\n        print(f\"FOLD : {fold}, EPOCH : {epoch}, TRAIN_LOSS : {train_loss}, VALIDATION_LOSS : {valid_loss}\")\n        if valid_loss < best_loss:\n            best_loss = valid_loss\n            if save_model:\n                torch.save(model.state_dict(), f\"Model_{fold}.bin\")\n        else:\n            early_stopping_counter += 1\n\n        if early_stopping_counter > early_stopping_iter:\n            break\n    return best_loss\n\n\n# Optuna functions\ndef objective(trial):\n    params = {\n        \"num_layers\": trial.suggest_int(\"num_layers\", 1, 7),\n        \"hidden_size\": trial.suggest_int(\"hidden_size\", 16, 2048),\n        \"dropout\": trial.suggest_uniform(\"dropout\", 0.1, 0.7),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-6, 1e-3)\n    }\n    all_loss = []\n    for f in range(5):\n        temp_loss = run_training(f, params, save_model=False)\n        all_loss.append(temp_loss)\n\n    return np.mean(all_loss)\n\n\nif __name__ == \"__main__\":\n    study =optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=20)\n\n    print(\"Best trial: \")\n    trial_ = study.best_trial\n\n    print(trial_.values)\n    print(trial_.params)\n    scores = 0\n    for j in range(5):\n        scr = run_training(j, trial_.params, save_model=True)\n        scores += scr\n\n    print(scores / 5)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:00:58.413156Z","iopub.execute_input":"2021-09-08T13:00:58.413474Z","iopub.status.idle":"2021-09-08T13:07:51.790129Z","shell.execute_reply.started":"2021-09-08T13:00:58.413443Z","shell.execute_reply":"2021-09-08T13:07:51.789166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}