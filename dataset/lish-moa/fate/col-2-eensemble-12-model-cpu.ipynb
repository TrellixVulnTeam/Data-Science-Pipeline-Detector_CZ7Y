{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/kerasapplications/keras-team-keras-applications-3b180cb -f ./ --no-index\n!pip install ../input/efficientnet/efficientnet-1.1.0/ -f ./ --no-index\n!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import efficientnet.tfkeras as efn\nimport numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.decomposition import PCA\nfrom pickle import load,dump\nimport pickle\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.cluster import KMeans\n\nimport torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nimport gc\n\nfrom sklearn.metrics import roc_auc_score\n\nimport json\n\nfrom tensorflow.keras import layers,regularizers,Sequential,backend,callbacks,optimizers,metrics,losses\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow_addons as tfa\n\nfrom tqdm.notebook import tqdm\nimport math\n\nfrom pytorch_tabnet.tab_model import TabNetRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model1\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\ndf = pd.read_csv('../input/lish-moa/sample_submission.csv')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tns=train_targets_nonscored.iloc[:,1:]\ntns_col=list(tns.loc[:,tns.sum()>3].columns)\ntns_col=[\"sig_id\"]+tns_col\n\ntrain_targets_nonscored=train_targets_nonscored[tns_col]\n# train_targets_nonscored","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features2=train_features.copy()\ntest_features2=test_features.copy()\n\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\nfor col in (GENES + CELLS):\n\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\ndef seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n\n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    \nseed_everything(42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_comp = 600  #<--Update\npca_g = PCA(n_components=n_comp, random_state=42)\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n#gpca= (pca_g.fit(data[GENES]))\ngpca = pickle.load(open(\"../input/moa-pretained-non-scored-targets-as-meta-features/gpca.pkl\",\"rb\"))\ntrain2= (gpca.transform(train_features[GENES]))\ntest2 = (gpca.transform(test_features[GENES]))\n\ntrain_gpca = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest_gpca = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train_gpca), axis=1)\ntest_features = pd.concat((test_features, test_gpca), axis=1)\n\n#CELLS\nn_comp = 50  #<--Update\n\npca_c = PCA(n_components=n_comp, random_state=42)\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n#cpca= (pca_c.fit(data[CELLS]))\ncpca = pickle.load(open(\"../input/moa-pretained-non-scored-targets-as-meta-features/cpca.pkl\",\"rb\"))\ntrain2= (cpca.transform(train_features[CELLS]))\ntest2 = (cpca.transform(test_features[CELLS]))\n\ntrain_cpca = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest_cpca = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train_cpca), axis=1)\ntest_features = pd.concat((test_features, test_cpca), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nc_n = [f for f in list(train_features.columns) if f not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]\n#mask = (train_features[c_n].var() >= 0.85).values\nmask = pickle.load(open(\"../input/moa-pretained-non-scored-targets-as-meta-features/mask.pkl\",\"rb\"))\ntmp = train_features[c_n].loc[:, mask]\ntrain_features = pd.concat([train_features[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)\ntmp = test_features[c_n].loc[:, mask]\ntest_features = pd.concat([test_features[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef fe_cluster_genes(train, test, n_clusters_g = 22, SEED = 42):\n    \n    features_g = GENES\n    #features_c = CELLS\n    \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        #kmeans_genes = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        kmeans_genes = pickle.load(open(\"../input/moa-pretained-non-scored-targets-as-meta-features/kmeans_genes.pkl\",\"rb\"))\n        #dump(kmeans_genes, open('kmeans_genes.pkl', 'wb'))\n        train[f'clusters_{kind}'] = kmeans_genes.predict(train_.values)\n        test[f'clusters_{kind}'] = kmeans_genes.predict(test_.values)\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n   # train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_features2 ,test_features2=fe_cluster_genes(train_features2,test_features2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe_cluster_cells(train, test, n_clusters_c = 4, SEED = 42):\n    \n    #features_g = GENES\n    features_c = CELLS\n    \n    def create_cluster(train, test, features, kind = 'c', n_clusters = n_clusters_c):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n#         kmeans_cells = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n#         dump(kmeans_cells, open('kmeans_cells.pkl', 'wb'))\n        kmeans_cells = pickle.load(open(\"../input/moa-pretained-non-scored-targets-as-meta-features/kmeans_cells.pkl\",\"rb\"))\n        train[f'clusters_{kind}'] = kmeans_cells.predict(train_.values)\n        test[f'clusters_{kind}'] = kmeans_cells.predict(test_.values)\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n   # train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_features2 ,test_features2=fe_cluster_cells(train_features2,test_features2)\n\ntrain_pca=pd.concat((train_gpca,train_cpca),axis=1)\ntest_pca=pd.concat((test_gpca,test_cpca),axis=1)\n\ndef fe_cluster_pca(train, test,n_clusters=5,SEED = 42):\n        data=pd.concat([train,test],axis=0)\n#         kmeans_pca = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n#         dump(kmeans_pca, open('kmeans_pca.pkl', 'wb'))\n        kmeans_pca = pickle.load(open(\"../input/moa-pretained-non-scored-targets-as-meta-features/kmeans_pca.pkl\",\"rb\"))\n        train[f'clusters_pca'] = kmeans_pca.predict(train.values)\n        test[f'clusters_pca'] = kmeans_pca.predict(test.values)\n        train = pd.get_dummies(train, columns = [f'clusters_pca'])\n        test = pd.get_dummies(test, columns = [f'clusters_pca'])\n        return train, test\ntrain_cluster_pca ,test_cluster_pca = fe_cluster_pca(train_pca,test_pca)\n\ntrain_cluster_pca = train_cluster_pca.iloc[:,650:]\ntest_cluster_pca = test_cluster_pca.iloc[:,650:]\n\ntrain_features_cluster=train_features2.iloc[:,876:]\ntest_features_cluster=test_features2.iloc[:,876:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gsquarecols=['g-574','g-211','g-216','g-0','g-255','g-577','g-153','g-389','g-60','g-370','g-248','g-167','g-203','g-177','g-301','g-332','g-517','g-6','g-744','g-224','g-162','g-3','g-736','g-486','g-283','g-22','g-359','g-361','g-440','g-335','g-106','g-307','g-745','g-146','g-416','g-298','g-666','g-91','g-17','g-549','g-145','g-157','g-768','g-568','g-396']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe_stats(train, test):\n    \n    features_g = GENES\n    features_c = CELLS\n    \n    for df in train, test:\n        df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n        df['c52_c42'] = df['c-52'] * df['c-42']\n        df['c13_c73'] = df['c-13'] * df['c-73']\n        df['c26_c13'] = df['c-23'] * df['c-13']\n        df['c33_c6'] = df['c-33'] * df['c-6']\n        df['c11_c55'] = df['c-11'] * df['c-55']\n        df['c38_c63'] = df['c-38'] * df['c-63']\n        df['c38_c94'] = df['c-38'] * df['c-94']\n        df['c13_c94'] = df['c-13'] * df['c-94']\n        df['c4_c52'] = df['c-4'] * df['c-52']\n        df['c4_c42'] = df['c-4'] * df['c-42']\n        df['c13_c38'] = df['c-13'] * df['c-38']\n        df['c55_c2'] = df['c-55'] * df['c-2']\n        df['c55_c4'] = df['c-55'] * df['c-4']\n        df['c4_c13'] = df['c-4'] * df['c-13']\n        df['c82_c42'] = df['c-82'] * df['c-42']\n        df['c66_c42'] = df['c-66'] * df['c-42']\n        df['c6_c38'] = df['c-6'] * df['c-38']\n        df['c2_c13'] = df['c-2'] * df['c-13']\n        df['c62_c42'] = df['c-62'] * df['c-42']\n        df['c90_c55'] = df['c-90'] * df['c-55']\n        \n        \n        for feature in features_c:\n             df[f'{feature}_squared'] = df[feature] ** 2     \n                \n        for feature in gsquarecols:\n            df[f'{feature}_squared'] = df[feature] ** 2        \n        \n    return train, test\n\ntrain_features2,test_features2=fe_stats(train_features2,test_features2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_stats=train_features2.iloc[:,902:]\ntest_features_stats=test_features2.iloc[:,902:]\n\ntrain_features = pd.concat((train_features, train_features_cluster,train_cluster_pca,train_features_stats), axis=1)\ntest_features = pd.concat((test_features, test_features_cluster,test_cluster_pca,test_features_stats), axis=1)\n\ntrain = train_features.merge(train_targets_nonscored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_nonscored.columns]\n\n\ntrain = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model1=train.copy()\ntest_model1=test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n\ntrain = pd.get_dummies(train, columns=['cp_time','cp_dose'])\ntest_ = pd.get_dummies(test, columns=['cp_time','cp_dose'])\n\nfeature_cols = [c for c in train.columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['sig_id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    \ndef train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.2)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.dense1(x), 1e-3)\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 26\nBATCH_SIZE = 256\nLEARNING_RATE = 6e-4\nWEIGHT_DECAY = 1e-5\nNFOLDS = 5\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = True\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=2048","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n#     mskf = MultilabelStratifiedKFold(n_splits=5,random_state=seed)\n#     for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n#          train.loc[v_idx, 'kfold'] = int(f)\n#     train['kfold'] = train['kfold'].astype(int)\n    \n#     trn_idx = train[train['kfold'] != fold].index\n#     val_idx = train[train['kfold'] == fold].index\n    \n#     train_df = train[train['kfold'] != fold].reset_index(drop=True)\n#     valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n#     x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n#     x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n#     train_dataset = MoADataset(x_train, y_train)\n#     valid_dataset = MoADataset(x_valid, y_valid)\n#     trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n#     validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n#     model = Model(\n#         num_features=num_features,\n#         num_targets=num_targets,\n#         hidden_size=hidden_size,\n#     )\n#     model.load_state_dict(torch.load(f\"../input/moa-pretained-non-scored-targets-as-meta-features/SEED{seed}_FOLD{fold}_nonscored.pth\",\"cpu\"))\n    \n#     model.to(DEVICE)\n    \n#     optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n#                                               max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n#     loss_fn = nn.BCEWithLogitsLoss()\n    \n#     loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n#     early_stopping_steps = EARLY_STOPPING_STEPS\n#     early_step = 0\n    \n#     oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n# #     best_loss = np.inf\n    \n# #     for epoch in range(EPOCHS):\n        \n# #         train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n# #         print(f\"SEED: {seed}, FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n#     valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n#     print(f\"SEED: {seed} ,FOLD: {fold}, valid_loss: {valid_loss}\")\n        \n# #         if valid_loss < best_loss:\n            \n# #             best_loss = valid_loss\n#     oof[val_idx] = valid_preds\n# #             torch.save(model.state_dict(), f\"SEED{seed}_FOLD{fold}_nonscored.pth\")\n        \n# #         elif(EARLY_STOP == True):\n            \n# #             early_step += 1\n# #             if (early_step >= early_stopping_steps):\n#                 break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    model.load_state_dict(torch.load(f\"../input/moa-pretained-non-scored-targets-as-meta-features/SEED{seed}_FOLD{fold}_nonscored.pth\",\"cpu\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n#     return oof, predictions\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):\n\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n\n        \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Averaging on multiple SEEDS\n\nSEED = [0,1,2]  #<-- Update\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    print(seed)\n    predictions_ = run_k_fold(NFOLDS, seed)\n\n    predictions += predictions_ / len(SEED)\n\n\ntest_[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train[target_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.merge(train_targets_scored, on='sig_id')\ntarget = train[train_targets_scored.columns]\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = [c for c in train.columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['sig_id','kfold']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 26\nBATCH_SIZE = 256\nLEARNING_RATE = 6e-4\nWEIGHT_DECAY = 1e-5\nNFOLDS = 5\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = True\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=2048","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n#     mskf = MultilabelStratifiedKFold(n_splits=5,random_state=seed)\n#     for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n#          train.loc[v_idx, 'kfold'] = int(f)\n#     train['kfold'] = train['kfold'].astype(int)\n    \n#     trn_idx = train[train['kfold'] != fold].index\n#     val_idx = train[train['kfold'] == fold].index\n    \n#     train_df = train[train['kfold'] != fold].reset_index(drop=True)\n#     valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n#     x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n#     x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n#     train_dataset = MoADataset(x_train, y_train)\n#     valid_dataset = MoADataset(x_valid, y_valid)\n#     trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n#     validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n#     model = Model(\n#         num_features=num_features,\n#         num_targets=num_targets,\n#         hidden_size=hidden_size,\n#     )\n#     model.load_state_dict(torch.load(f\"../input/moa-pretained-non-scored-targets-as-meta-features/SEED{seed}_FOLD{fold}_scored.pth\",\"cpu\"))\n#     model.to(DEVICE)\n\n#     optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n#                                               max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n#     loss_fn = nn.BCEWithLogitsLoss()\n    \n#     loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n#     early_stopping_steps = EARLY_STOPPING_STEPS\n#     early_step = 0\n    \n#     oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n# #     best_loss = np.inf\n    \n# #     for epoch in range(EPOCHS):\n        \n# #         train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n# #         print(f\"SEED: {seed}, FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n#     valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n#     print(f\"SEED: {seed} ,FOLD: {fold}, valid_loss: {valid_loss}\")\n        \n# #         if valid_loss < best_loss:\n            \n# #             best_loss = valid_loss\n#     oof[val_idx] = valid_preds\n#             torch.save(model.state_dict(), f\"SEED{seed}_FOLD{fold}_scored.pth\")\n        \n#         elif(EARLY_STOP == True):\n            \n#             early_step += 1\n#             if (early_step >= early_stopping_steps):\n#                 break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    \n    model.load_state_dict(torch.load(f\"../input/moa-pretained-non-scored-targets-as-meta-features/SEED{seed}_FOLD{fold}_scored.pth\",\"cpu\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n   # return oof, predictions\n    return  predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def run_k_fold(NFOLDS, seed):\n#     oof = np.zeros((len(train), len(target_cols)))\n#     predictions = np.zeros((len(test), len(target_cols)))\n    \n#     for fold in range(NFOLDS):\n#         oof_, pred_ = run_training(fold, seed)\n        \n#         predictions += pred_ / NFOLDS\n#         oof += oof_\n        \n#     return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):\n\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n\n        \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Averaging on multiple SEEDS\n\n# SEED = [0,1,2]  #<-- Update\n# oof = np.zeros((len(train), len(target_cols)))\n# predictions = np.zeros((len(test), len(target_cols)))\n\n# for seed in SEED:\n    \n#     oof_, predictions_ = run_k_fold(NFOLDS, seed)\n#     oof += oof_ / len(SEED)\n#     predictions += predictions_ / len(SEED)\n\n# train[target_cols] = oof\n# test[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Averaging on multiple SEEDS\n\nSEED = [0,1,2]  #<-- Update\n\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    print(seed)\n    \n    predictions_ = run_k_fold(NFOLDS, seed)\n\n    predictions += predictions_ / len(SEED)\n\ntest[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n# sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp1=pd.read_csv(\"../input/moa-pretained-non-scored-targets-as-meta-features/submission.csv\")\n# tmp1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#(abs(sub.iloc[:,1:].values-tmp1.iloc[:,1:].values)).max() #3.07895243e-06","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nss_model1=sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model2"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_model1.copy()\ntest = test_model1.copy()\ntarget = train[train_targets_nonscored.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\ntrain = pd.get_dummies(train, columns=['cp_time','cp_dose'])\ntest_ = pd.get_dummies(test, columns=['cp_time','cp_dose'])\n\nfeature_cols = [c for c in train.columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['sig_id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.2)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n        self.dropout4 = nn.Dropout(0.2)\n        self.dense4 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))  \n        \n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.dense1(x), 1e-3)\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.relu(self.dense3(x))\n        \n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = self.dense4(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 26\nBATCH_SIZE = 256\nLEARNING_RATE = 6e-4\nWEIGHT_DECAY = 1e-5\nNFOLDS = 5\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = True\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=2048","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n#     mskf = MultilabelStratifiedKFold(n_splits=5,random_state=seed)\n#     for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n#          train.loc[v_idx, 'kfold'] = int(f)\n#     train['kfold'] = train['kfold'].astype(int)\n    \n#     trn_idx = train[train['kfold'] != fold].index\n#     val_idx = train[train['kfold'] == fold].index\n    \n#     train_df = train[train['kfold'] != fold].reset_index(drop=True)\n#     valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n#     x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n#     x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n#     train_dataset = MoADataset(x_train, y_train)\n#     valid_dataset = MoADataset(x_valid, y_valid)\n#     trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n#     validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n#     model = Model(\n#         num_features=num_features,\n#         num_targets=num_targets,\n#         hidden_size=hidden_size,\n#     )\n#     model.load_state_dict(torch.load(f\"../input/pretained-non-scored-targets-as-meta-3rd-layers/SEED{seed}_FOLD{fold}_nonscored.pth\",\"cpu\"))\n    \n#     model.to(DEVICE)\n    \n#     optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n#                                               max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    \n#     loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n#     early_stopping_steps = EARLY_STOPPING_STEPS\n#     early_step = 0\n    \n#     oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n# #     best_loss = np.inf\n    \n# #     for epoch in range(EPOCHS):\n        \n# #         train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n# #         print(f\"SEED: {seed}, FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n#     valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n#     print(f\"SEED: {seed} ,FOLD: {fold}, valid_loss: {valid_loss}\")\n        \n#         if valid_loss < best_loss:\n            \n# #             best_loss = valid_loss\n#     oof[val_idx] = valid_preds\n#             torch.save(model.state_dict(), f\"SEED{seed}_FOLD{fold}_nonscored.pth\")\n        \n#         elif(EARLY_STOP == True):\n            \n#             early_step += 1\n#             if (early_step >= early_stopping_steps):\n#                 break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    model.load_state_dict(torch.load(f\"../input/pretained-non-scored-targets-as-meta-3rd-layers/SEED{seed}_FOLD{fold}_nonscored.pth\",\"cpu\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return  predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):\n    \n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n    \n        \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Averaging on multiple SEEDS\n\nSEED = [0,1,2]  #<-- Update\n\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    print(seed)\n    predictions_ = run_k_fold(NFOLDS, seed)\n    \n    predictions += predictions_ / len(SEED)\n\ntest_[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.merge(train_targets_scored, on='sig_id')\ntarget = train[train_targets_scored.columns]\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\nfeature_cols = [c for c in train.columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['sig_id','kfold']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 26\nBATCH_SIZE = 256\nLEARNING_RATE = 6e-4\nWEIGHT_DECAY = 1e-5\nNFOLDS = 5\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = True\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=2048","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n#     mskf = MultilabelStratifiedKFold(n_splits=5,random_state=seed)\n#     for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n#          train.loc[v_idx, 'kfold'] = int(f)\n#     train['kfold'] = train['kfold'].astype(int)\n    \n#     trn_idx = train[train['kfold'] != fold].index\n#     val_idx = train[train['kfold'] == fold].index\n    \n#     train_df = train[train['kfold'] != fold].reset_index(drop=True)\n#     valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n#     x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n#     x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n#     train_dataset = MoADataset(x_train, y_train)\n#     valid_dataset = MoADataset(x_valid, y_valid)\n#     trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n#     validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n#     model = Model(\n#         num_features=num_features,\n#         num_targets=num_targets,\n#         hidden_size=hidden_size,\n#     )\n#     model.load_state_dict(torch.load(f\"../input/pretained-non-scored-targets-as-meta-3rd-layers/SEED{seed}_FOLD{fold}_scored.pth\",\"cpu\"))\n#     model.to(DEVICE)\n\n#     optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n#                                               max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    \n#     loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n#     early_stopping_steps = EARLY_STOPPING_STEPS\n#     early_step = 0\n    \n#     oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n# #     best_loss = np.inf\n    \n# #     for epoch in range(EPOCHS):\n        \n# #         train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n# #         print(f\"SEED: {seed}, FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n#     valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n#     print(f\"SEED: {seed} ,FOLD: {fold}, valid_loss: {valid_loss}\")\n        \n#         if valid_loss < best_loss:\n            \n#             best_loss = valid_loss\n#     oof[val_idx] = valid_preds\n#             torch.save(model.state_dict(), f\"SEED{seed}_FOLD{fold}_scored.pth\")\n        \n#         elif(EARLY_STOP == True):\n            \n#             early_step += 1\n#             if (early_step >= early_stopping_steps):\n#                 break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    \n    model.load_state_dict(torch.load(f\"../input/pretained-non-scored-targets-as-meta-3rd-layers/SEED{seed}_FOLD{fold}_scored.pth\",\"cpu\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return  predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):\n\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n        \n        \n    return  predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Averaging on multiple SEEDS\n\nSEED = [0,1,2]  #<-- Update\n\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    predictions_ = run_k_fold(NFOLDS, seed)\n    \n    predictions += predictions_ / len(SEED)\n\n\ntest[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nss_model2=sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp1=pd.read_csv(\"../input/pretained-non-scored-targets-as-meta-3rd-layers/submission.csv\")\n# tmp1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# (abs(sub.iloc[:,1:].values-tmp1.iloc[:,1:].values)).max() #3.842e-06","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model3\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_model1.copy()\ntest = test_model1.copy()\ntarget = train[train_targets_nonscored.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\ntrain = pd.get_dummies(train, columns=['cp_time','cp_dose'])\ntest_ = pd.get_dummies(test, columns=['cp_time','cp_dose'])\n\nfeature_cols = [c for c in train.columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['sig_id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.2)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n        self.dropout4 = nn.Dropout(0.2)\n        self.dense4 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm5 = nn.BatchNorm1d(hidden_size)\n        self.dropout5 = nn.Dropout(0.2)\n        self.dense5 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))  \n        \n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.dense1(x), 1e-3)\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.relu(self.dense3(x))\n        \n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = F.relu(self.dense4(x))\n        \n        x = self.batch_norm5(x)\n        x = self.dropout5(x)\n        x = self.dense5(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 26\nBATCH_SIZE = 256\nLEARNING_RATE = 6e-4\nWEIGHT_DECAY = 1e-5\nNFOLDS = 5\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = True\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=2048","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n#     mskf = MultilabelStratifiedKFold(n_splits=5,random_state=seed)\n#     for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n#          train.loc[v_idx, 'kfold'] = int(f)\n#     train['kfold'] = train['kfold'].astype(int)\n    \n#     trn_idx = train[train['kfold'] != fold].index\n#     val_idx = train[train['kfold'] == fold].index\n    \n#     train_df = train[train['kfold'] != fold].reset_index(drop=True)\n#     valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n#     x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n#     x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n#     train_dataset = MoADataset(x_train, y_train)\n#     valid_dataset = MoADataset(x_valid, y_valid)\n#     trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n#     validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n# #     model = Model(\n#         num_features=num_features,\n#         num_targets=num_targets,\n#         hidden_size=hidden_size,\n#     )\n#     model.load_state_dict(torch.load(f\"../input/pretained-non-scored-targets-as-meta-4th-layers/SEED{seed}_FOLD{fold}_nonscored.pth\",\"cpu\"))\n    \n#     model.to(DEVICE)\n    \n#     optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n#                                               max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n#     loss_fn = nn.BCEWithLogitsLoss()\n    \n#     loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n#     early_stopping_steps = EARLY_STOPPING_STEPS\n#     early_step = 0\n    \n#     oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n# #     best_loss = np.inf\n    \n#     for epoch in range(EPOCHS):\n        \n#         train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n#         print(f\"SEED: {seed}, FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n#     valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n#     print(f\"SEED: {seed} ,FOLD: {fold}, valid_loss: {valid_loss}\")\n        \n# #         if valid_loss < best_loss:\n            \n# #             best_loss = valid_loss\n#     oof[val_idx] = valid_preds\n#             torch.save(model.state_dict(), f\"SEED{seed}_FOLD{fold}_nonscored.pth\")\n        \n#         elif(EARLY_STOP == True):\n            \n#             early_step += 1\n#             if (early_step >= early_stopping_steps):\n#                 break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    \n    model.load_state_dict(torch.load(f\"../input/pretained-non-scored-targets-as-meta-4th-layers/SEED{seed}_FOLD{fold}_nonscored.pth\",\"cpu\"))\n    print(f\"../input/pretained-non-scored-targets-as-meta-4th-layers/SEED{seed}_FOLD{fold}_nonscored.pth\")\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return  predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):\n  \n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n       \n        \n    return  predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Averaging on multiple SEEDS\n\nSEED = [0,1,2]  #<-- Update\n\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    predictions_ = run_k_fold(NFOLDS, seed)\n\n    predictions += predictions_ / len(SEED)\n\n\ntest_[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.merge(train_targets_scored, on='sig_id')\ntarget = train[train_targets_scored.columns]\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\nfeature_cols = [c for c in train.columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['sig_id','kfold']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 26\nBATCH_SIZE = 256\nLEARNING_RATE = 6e-4\nWEIGHT_DECAY = 1e-5\nNFOLDS = 5\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = True\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=2048","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n#     mskf = MultilabelStratifiedKFold(n_splits=5,random_state=seed)\n#     for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n#          train.loc[v_idx, 'kfold'] = int(f)\n#     train['kfold'] = train['kfold'].astype(int)\n    \n#     trn_idx = train[train['kfold'] != fold].index\n#     val_idx = train[train['kfold'] == fold].index\n    \n#     train_df = train[train['kfold'] != fold].reset_index(drop=True)\n#     valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n#     x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n#     x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n#     train_dataset = MoADataset(x_train, y_train)\n#     valid_dataset = MoADataset(x_valid, y_valid)\n#     trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n#     validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n#     model = Model(\n#         num_features=num_features,\n#         num_targets=num_targets,\n#         hidden_size=hidden_size,\n#     )\n#     model.load_state_dict(torch.load(f\"../input/pretained-non-scored-targets-as-meta-4th-layers/SEED{seed}_FOLD{fold}_scored.pth\",\"cpu\"))\n#     model.to(DEVICE)\n\n#     optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n#                                               max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n#     loss_fn = nn.BCEWithLogitsLoss()\n    \n# #     loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n# #     early_stopping_steps = EARLY_STOPPING_STEPS\n# #     early_step = 0\n    \n#     oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n# #     best_loss = np.inf\n    \n# #     for epoch in range(EPOCHS):\n        \n# #         train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n# #         print(f\"SEED: {seed}, FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n#     valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n#     print(f\"SEED: {seed} ,FOLD: {fold}, valid_loss: {valid_loss}\")\n        \n# #         if valid_loss < best_loss:\n            \n# #             best_loss = valid_loss\n#     oof[val_idx] = valid_preds\n# #             torch.save(model.state_dict(), f\"SEED{seed}_FOLD{fold}_scored.pth\")\n        \n# #         elif(EARLY_STOP == True):\n            \n# #             early_step += 1\n# #             if (early_step >= early_stopping_steps):\n# #                 break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    \n    model.load_state_dict(torch.load(f\"../input/pretained-non-scored-targets-as-meta-4th-layers/SEED{seed}_FOLD{fold}_scored.pth\",\"cpu\"))\n    print(f\"../input/pretained-non-scored-targets-as-meta-4th-layers/SEED{seed}_FOLD{fold}_scored.pth\")\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):\n\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n\n        \n    return  predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Averaging on multiple SEEDS\n\nSEED = [0,1,2]  #<-- Update\n\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    predictions_ = run_k_fold(NFOLDS, seed)\n \n    predictions += predictions_ / len(SEED)\n\ntest[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nss_model3=sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tmp1=pd.read_csv(\"../input/pretained-non-scored-targets-as-meta-4th-layers/submission.csv\")\n# tmp1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# (abs(sub.iloc[:,1:].values-tmp1.iloc[:,1:].values)).max() #4.5585e-06","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model4"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_comp_GENES = 463\nn_comp_CELLS = 60\nVarianceThreshold_for_FS = 0.9\nDropout_Model = 0.25\n#QT_n_quantile_min=50, \n#QT_n_quantile_max=1000,\nprint('n_comp_GENES', n_comp_GENES, 'n_comp_CELLS', n_comp_CELLS, 'total', n_comp_GENES + n_comp_CELLS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_QT_par_kurt(QT_n_quantile_min=10, QT_n_quantile_max=200):\n    # Calculation parameters of function: n_quantile(kurtosis) = k1*kurtosis + k0\n    # For Train & Test datasets (GENES + CELLS features): minimum kurtosis = 1.53655, maximum kurtosis = 30.4929\n    \n    a = np.array([[1.53655,1], [30.4929,1]])\n    b = np.array([QT_n_quantile_min, QT_n_quantile_max])\n    \n    return np.linalg.solve(a, b)\n\ndef n_quantile_for_kurt(kurt, calc_QT_par_kurt_transform):\n    # Calculation parameters of function: n_quantile(kurtosis) = calc_QT_par_kurt_transform[0]*kurtosis + calc_QT_par_kurt_transform[1]\n    return int(calc_QT_par_kurt_transform[0]*kurt + calc_QT_par_kurt_transform[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor col in (GENES + CELLS):\n\n    #kurt = max(kurtosis(train_features[col]), kurtosis(test_features[col]))\n    #QuantileTransformer_n_quantiles = n_quantile_for_kurt(kurt, calc_QT_par_kurt(QT_n_quantile_min, QT_n_quantile_max))\n    #transformer = QuantileTransformer(n_quantiles=QuantileTransformer_n_quantiles,random_state=0, output_distribution=\"normal\")\n    \n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")   # from optimal commit 9\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GENES\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n\n\ngpca = pickle.load(open(\"../input/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual/gpca.pkl\",\"rb\"))\ndata2 = (gpca.transform(data[GENES]))\n\n\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp_GENES)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp_GENES)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\n\n# CELLS\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n\n\ncpca = pickle.load(open(\"../input/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual/cpca.pkl\",\"rb\"))\ndata2 = (cpca.transform(data[CELLS]))\n\n\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp_CELLS)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp_CELLS)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train_features.append(test_features)\nvar_thresh = VarianceThreshold(VarianceThreshold_for_FS)\n\nmask0 = pickle.load(open(\"../input/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual/mask0.pkl\",\"rb\"))\n\ntrain_features_transformed=train_features[train_features.columns[mask0]]\n\ntest_features_transformed=test_features[train_features.columns[mask0]]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_features.merge(train_targets_scored, on='sig_id')\n# train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n# test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntrain = train\ntest = test_features\ntarget = train[train_targets_scored.columns]\n\ntrain = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)\n\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = train.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=5)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\nprint(sample_submission.shape)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct    \n    \ndef train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data\n\nfeature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 5\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=1500","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(Dropout_Model)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(Dropout_Model)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x\n    \nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n#     trn_idx = train[train['kfold'] != fold].index\n#     val_idx = train[train['kfold'] == fold].index\n    \n#     train_df = train[train['kfold'] != fold].reset_index(drop=True)\n#     valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n#     x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n#     x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n#     train_dataset = MoADataset(x_train, y_train)\n#     valid_dataset = MoADataset(x_valid, y_valid)\n#     trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n#     validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n#     model = Model(\n#         num_features=num_features,\n#         num_targets=num_targets,\n#         hidden_size=hidden_size,\n#     )\n#     model.load_state_dict(torch.load(f\"../input/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual/SEED_{seed}_FOLD_{fold}.pth\",\"cpu\"))\n#     model.to(DEVICE)\n    \n# #     optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n# #     scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n# #                                               max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n#     loss_fn = nn.BCEWithLogitsLoss()\n#     loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n# #     early_stopping_steps = EARLY_STOPPING_STEPS\n# #     early_step = 0\n   \n#     oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n# #     best_loss = np.inf\n    \n# #     for epoch in range(EPOCHS):\n        \n# #         train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n# #         print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n#     valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n#     print(f\"FOLD: {fold}, valid_loss: {valid_loss}\")\n\n# #         if valid_loss is not None and valid_loss < best_loss:\n            \n# #             best_loss = valid_loss\n#     oof[val_idx] = valid_preds\n#             torch.save(model.state_dict(), f\"SEED_{seed}_FOLD_{fold}.pth\")\n        \n#         elif(EARLY_STOP == True):\n            \n#             early_step += 1\n#             if (early_step >= early_stopping_steps):\n#                 break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    \n    model.load_state_dict(torch.load(f\"../input/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual/SEED_{seed}_FOLD_{fold}.pth\",\"cpu\"))\n    print(f\"../input/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual/SEED_{seed}_FOLD_{fold}.pth\")\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):\n\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n\n        \n    return  predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Averaging on multiple SEEDS\n\nSEED = [0, 1, 2]\n\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    predictions_ = run_k_fold(NFOLDS, seed)\n\n    predictions += predictions_ / len(SEED)\n\ntest[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1= pd.read_csv('../input/lish-moa/train_features.csv')\ntest1= pd.read_csv('../input/lish-moa/test_features.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.loc[test1['cp_type']==\"ctl_vehicle\", target_cols] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nss_model4=sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tmp1=pd.read_csv(\"../input/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# (abs(sub.iloc[:,1:].values-tmp1.iloc[:,1:].values)).max() #2.02e-06","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model5"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import train data, drop sig_id, cp_type\n\ntrain_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\nnon_ctl_idx = train_features.loc[train_features['cp_type']!='ctl_vehicle'].index.to_list()\ntrain_features = train_features.drop(['sig_id','cp_type','cp_dose','cp_time'],axis=1)\ntrain_targets_scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ntrain_targets_scored = train_targets_scored.drop('sig_id',axis=1)\nlabels_train = train_targets_scored.values\n\n# Drop training data with ctl vehicle\n\ntrain_features = train_features.iloc[non_ctl_idx]\nlabels_train = labels_train[non_ctl_idx]\n\n# Import test data\nss = pd.read_csv('../input/lish-moa/sample_submission.csv')\ntest_features = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\ntest_features = test_features.drop(['sig_id','cp_dose','cp_time'],axis=1)\n\n# Import predictors from public kernel\n\njson_file_path = '../input/t-test-pca-rfe-logistic-regression/main_predictors.json'\n\nwith open(json_file_path, 'r') as j:\n    predictors = json.loads(j.read())\n    predictors = predictors['start_predictors']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=123):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(seed=53)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cs = train_features.columns.str.contains('c-')\ngs = train_features.columns.str.contains('g-')\n\ndef preprocessor(train,test):\n    \n    # PCA\n    \n    n_gs = 2 #2 # No of PCA comps to include\n    n_cs = 50#100 # No of PCA comps to include\n    \n    pca_cs = PCA(n_components = n_cs)\n    pca_gs = PCA(n_components = n_gs)\n\n    train_pca_gs = pca_gs.fit_transform(train[:,gs])\n    train_pca_cs = pca_cs.fit_transform(train[:,cs])\n    test_pca_gs = pca_gs.transform(test[:,gs])\n    test_pca_cs = pca_cs.transform(test[:,cs])\n    \n    # c-mean, g-mean\n    \n    train_c_mean = train[:,cs].mean(axis=1)\n    test_c_mean = test[:,cs].mean(axis=1)\n    train_c_std = train[:,cs].std(axis=1)\n    test_c_std = test[:,cs].std(axis=1)\n    train_g_mean = train[:,gs].mean(axis=1)\n    test_g_mean = test[:,gs].mean(axis=1)\n    train_g_std = train[:,gs].std(axis=1)\n    test_g_std = test[:,gs].std(axis=1)\n    \n    # Append Features\n    \n    train = np.concatenate((train,train_pca_gs,train_pca_cs,train_c_mean[:,np.newaxis]\n                            ,train_g_std[:,np.newaxis]),axis=1)\n    test = np.concatenate((test,test_pca_gs,test_pca_cs,test_c_mean[:,np.newaxis],\n                           test_g_std[:,np.newaxis]),axis=1)\n    \n    # Scaler for numerical values\n\n    # Scale train data\n    scaler = preprocessing.StandardScaler()\n\n    train = scaler.fit_transform(train)\n\n    # Scale Test data\n    test = scaler.transform(test)\n    \n    return train, test\n\nn_labels = train_targets_scored.shape[1]\nn_train = train_features.shape[0]\nn_test = test_features.shape[0]\n\n\n# Prediction Clipping Thresholds\n\np_min = 0.0005\np_max = 0.9995\n\n# OOF Evaluation Metric with clipping and no label smoothing\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(n_features, n_features_2, n_labels, label_smoothing = 0.0005):    \n    input_1 = layers.Input(shape = (n_features,), name = 'Input1')\n    input_2 = layers.Input(shape = (n_features_2,), name = 'Input2')\n\n    head_1 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.4),\n        layers.Dense(512, activation=\"elu\"), \n        layers.BatchNormalization(),\n        layers.Dropout(0.1),\n        layers.Dense(256, activation = \"elu\")\n        ],name='Head1') \n\n    input_3 = head_1(input_1)\n    input_3_concat = layers.Concatenate()([input_2, input_3])\n\n    head_2 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(512, \"relu\"),\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(512, \"elu\"),\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(256, \"relu\"),\n        layers.BatchNormalization(),\n        layers.Dropout(0.1),\n        layers.Dense(256, \"elu\")\n        ],name='Head2')\n\n    input_4 = head_2(input_3_concat)\n    input_4_avg = layers.Average()([input_3, input_4]) \n\n    head_3 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dense(256, kernel_initializer='lecun_normal', activation='selu'),\n        layers.BatchNormalization(),\n        layers.Dense(n_labels, kernel_initializer='lecun_normal', activation='selu'),\n        layers.BatchNormalization(),\n        layers.Dense(n_labels, activation=\"sigmoid\")\n        ],name='Head3')\n\n    output = head_3(input_4_avg)\n\n    \n    model = tf.keras.Model(inputs = [input_1, input_2], outputs = output)\n    opt = tf.keras.optimizers.Adam(learning_rate=0.002)\n    model.compile(opt, loss=losses.BinaryCrossentropy(label_smoothing=label_smoothing), metrics=logloss)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets=train_targets_scored.iloc[non_ctl_idx].reset_index(drop=True)\ntrain_features=train_features.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_STARTS = 3\nnp.random.seed(1)\nseeds = np.random.randint(0,100,size=N_STARTS)\nres = train_targets.copy()\nss.loc[:, train_targets.columns] = 0\nres.loc[:, train_targets.columns] = 0\n\nfor seed in seeds:\n    \n    for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits=5,random_state=seed,shuffle=True).split(train_features.values, train_targets)):\n        print(f'Fold {n}')\n        \n        X_train0,data_test = preprocessor(train_features.values,\n                           test_features.drop('cp_type',axis=1).values)\n        \n        X_train, X_test = X_train0[tr],X_train0[te]\n#         _,data_test = preprocessor(train_features.iloc[tr].values,\n#                                    test_features.drop('cp_type',axis=1).values)\n        \n        X_train_2 = train_features.iloc[tr][predictors].values\n        X_test_2 = train_features.iloc[te][predictors].values\n        data_test_2 = test_features[predictors].values\n        y_train = labels_train[tr]\n        y_test = labels_train[te]\n        n_features = X_train.shape[1]\n        n_features_2 = X_train_2.shape[1]\n        \n        model = build_model(n_features, n_features_2, n_labels)\n        \n#         reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.1, patience=2, mode='min', min_lr=1E-5)\n#         early_stopping = callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=10, mode='min',restore_best_weights=True)\n\n#         sv = tf.keras.callbacks.ModelCheckpoint(\n#             'seed-%i-fold-%i-model.h5'%(seed,n), monitor='val_loss', verbose=0, save_best_only=True,\n#             save_weights_only=True, mode='min', save_freq='epoch')\n#         model.fit([X_train,X_train_2],y_train,shuffle=True,\n#                   validation_data=([X_test,X_test_2],y_test),\n#                   epochs=30, batch_size=128,\n#                   callbacks=[sv,early_stopping,reduce_lr], verbose=1\n#                  )\n        model.load_weights(f\"../input/multi-input-resnet-model/seed-{seed}-fold-{n}-model.h5\")\n        ss.loc[:, train_targets.columns] += model.predict([data_test,data_test_2])\n        #res.loc[te, train_targets.columns] += model.predict([X_test,X_test_2])\n        print(f\"../input/multi-input-resnet-model/seed-{seed}-fold-{n}-model.h5\")\n        print('')\n    \nss.loc[:, train_targets.columns] /= ((n+1) * N_STARTS)\n#res.loc[:, train_targets.columns] /= N_STARTS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored0 = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ntest1= pd.read_csv('../input/lish-moa/test_features.csv')\n\n# tmp=train_targets_scored0.iloc[non_ctl_idx].reset_index(drop=True)\n# res_sig_id=tmp.loc[:,[\"sig_id\"]]\n\n# res=pd.concat([res_sig_id,res],axis=1)\n\n# valid_results = train_targets_scored0.drop(columns=train_targets.columns).merge(res, on='sig_id', how='left').fillna(0)\n\n\n# valid_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.loc[test1['cp_type']==\"ctl_vehicle\", train_targets.columns] = 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nss_model5=ss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tmp1=pd.read_csv(\"../input/multi-input-resnet-model-inferce/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# (abs(ss.iloc[:,1:].values-tmp1.iloc[:,1:].values)).max() #3.5646e-07","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model6\n"},{"metadata":{},"cell_type":"markdown","source":"# Model7"},{"metadata":{},"cell_type":"markdown","source":"# Model8"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\ndf = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tns=train_targets_nonscored.iloc[:,1:]\n\ntns_col=list(tns.loc[:,tns.sum()>3].columns)\ntns_col=[\"sig_id\"]+tns_col\n\ntrain_targets_nonscored=train_targets_nonscored[tns_col]\n\n\ntrain_features2=train_features.copy()\ntest_features2=test_features.copy()\n\n\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in (GENES + CELLS):\n\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n\n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    \nseed_everything(42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_comp = 600  #<--Update\npca_g = PCA(n_components=n_comp, random_state=42)\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n#gpca= (pca_g.fit(train_features[GENES]))\ngpca = pickle.load(open(\"../input/pca-of-moa-pretained-non-scored-tabnet/gpca.pkl\",\"rb\"))\ntrain2= (gpca.transform(train_features[GENES]))\ntest2 = (gpca.transform(test_features[GENES]))\n\ntrain_gpca = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest_gpca = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train_gpca), axis=1)\ntest_features = pd.concat((test_features, test_gpca), axis=1)\n\n\n#CELLS\nn_comp = 50  #<--Update\n\npca_c = PCA(n_components=n_comp, random_state=42)\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n#cpca= (pca_c.fit(train_features[CELLS]))\ncpca = pickle.load(open(\"../input/pca-of-moa-pretained-non-scored-tabnet/cpca.pkl\",\"rb\"))\ntrain2= (cpca.transform(train_features[CELLS]))\ntest2 = (cpca.transform(test_features[CELLS]))\n\ntrain_cpca = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest_cpca = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train_cpca), axis=1)\ntest_features = pd.concat((test_features, test_cpca), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_n = [f for f in list(train_features.columns) if f not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]\n#mask = (train_features[c_n].var() >= 0.85).values\nmask = pickle.load(open(\"../input/pca-of-moa-pretained-non-scored-tabnet/mask.pkl\",\"rb\"))\ntmp = train_features[c_n].loc[:, mask]\ntrain_features = pd.concat([train_features[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)\ntmp = test_features[c_n].loc[:, mask]\ntest_features = pd.concat([test_features[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe_cluster_genes(train, test, n_clusters_g = 22, SEED = 42):\n    \n    features_g = GENES\n    #features_c = CELLS\n    \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n#         kmeans_genes = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n#         dump(kmeans_genes, open('kmeans_genes.pkl', 'wb'))\n        kmeans_genes = pickle.load(open(\"../input/pca-of-moa-pretained-non-scored-tabnet/kmeans_genes.pkl\",\"rb\"))\n        train[f'clusters_{kind}'] = kmeans_genes.predict(train_.values)\n        test[f'clusters_{kind}'] = kmeans_genes.predict(test_.values)\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n   # train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_features2 ,test_features2=fe_cluster_genes(train_features2,test_features2)\n\n\ndef fe_cluster_cells(train, test, n_clusters_c = 4, SEED = 42):\n    \n    #features_g = GENES\n    features_c = CELLS\n    \n    def create_cluster(train, test, features, kind = 'c', n_clusters = n_clusters_c):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n#         kmeans_cells = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n#         dump(kmeans_cells, open('kmeans_cells.pkl', 'wb'))\n        kmeans_cells = pickle.load(open(\"../input/pca-of-moa-pretained-non-scored-tabnet/kmeans_cells.pkl\",\"rb\"))\n        train[f'clusters_{kind}'] = kmeans_cells.predict(train_.values)\n        test[f'clusters_{kind}'] = kmeans_cells.predict(test_.values)\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n   # train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_features2 ,test_features2=fe_cluster_cells(train_features2,test_features2)\n\n\ntrain_pca=pd.concat((train_gpca,train_cpca),axis=1)\ntest_pca=pd.concat((test_gpca,test_cpca),axis=1)\n\n\ndef fe_cluster_pca(train, test,n_clusters=5,SEED = 42):\n        data=pd.concat([train,test],axis=0)\n#         kmeans_pca = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n#         dump(kmeans_pca, open('kmeans_pca.pkl', 'wb'))\n        kmeans_pca = pickle.load(open(\"../input/pca-of-moa-pretained-non-scored-tabnet/kmeans_pca.pkl\",\"rb\"))\n        train[f'clusters_pca'] = kmeans_pca.predict(train.values)\n        test[f'clusters_pca'] = kmeans_pca.predict(test.values)\n        train = pd.get_dummies(train, columns = [f'clusters_pca'])\n        test = pd.get_dummies(test, columns = [f'clusters_pca'])\n        return train, test\ntrain_cluster_pca ,test_cluster_pca = fe_cluster_pca(train_pca,test_pca)\n\n\ntrain_cluster_pca = train_cluster_pca.iloc[:,650:]\ntest_cluster_pca = test_cluster_pca.iloc[:,650:]\n\n\ntrain_features_cluster=train_features2.iloc[:,876:]\ntest_features_cluster=test_features2.iloc[:,876:]\n\n\ngsquarecols=['g-574','g-211','g-216','g-0','g-255','g-577','g-153','g-389','g-60','g-370','g-248','g-167','g-203','g-177','g-301','g-332','g-517','g-6','g-744','g-224','g-162','g-3','g-736','g-486','g-283','g-22','g-359','g-361','g-440','g-335','g-106','g-307','g-745','g-146','g-416','g-298','g-666','g-91','g-17','g-549','g-145','g-157','g-768','g-568','g-396']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe_stats(train, test):\n    \n    features_g = GENES\n    features_c = CELLS\n    \n    for df in train, test:\n        df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n        df['c52_c42'] = df['c-52'] * df['c-42']\n        df['c13_c73'] = df['c-13'] * df['c-73']\n        df['c26_c13'] = df['c-23'] * df['c-13']\n        df['c33_c6'] = df['c-33'] * df['c-6']\n        df['c11_c55'] = df['c-11'] * df['c-55']\n        df['c38_c63'] = df['c-38'] * df['c-63']\n        df['c38_c94'] = df['c-38'] * df['c-94']\n        df['c13_c94'] = df['c-13'] * df['c-94']\n        df['c4_c52'] = df['c-4'] * df['c-52']\n        df['c4_c42'] = df['c-4'] * df['c-42']\n        df['c13_c38'] = df['c-13'] * df['c-38']\n        df['c55_c2'] = df['c-55'] * df['c-2']\n        df['c55_c4'] = df['c-55'] * df['c-4']\n        df['c4_c13'] = df['c-4'] * df['c-13']\n        df['c82_c42'] = df['c-82'] * df['c-42']\n        df['c66_c42'] = df['c-66'] * df['c-42']\n        df['c6_c38'] = df['c-6'] * df['c-38']\n        df['c2_c13'] = df['c-2'] * df['c-13']\n        df['c62_c42'] = df['c-62'] * df['c-42']\n        df['c90_c55'] = df['c-90'] * df['c-55']\n        \n        \n        for feature in features_c:\n             df[f'{feature}_squared'] = df[feature] ** 2     \n                \n        for feature in gsquarecols:\n            df[f'{feature}_squared'] = df[feature] ** 2        \n        \n    return train, test\n\ntrain_features2,test_features2=fe_stats(train_features2,test_features2)\n\n\ntrain_features_stats=train_features2.iloc[:,902:]\ntest_features_stats=test_features2.iloc[:,902:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.concat((train_features, train_features_cluster,train_cluster_pca,train_features_stats), axis=1)\ntest_features = pd.concat((test_features, test_features_cluster,test_cluster_pca,test_features_stats), axis=1)\n\ntrain = train_features.merge(train_targets_nonscored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n#test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features\ntarget = train[train_targets_nonscored.columns]\n\n\ntrain = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)\n\n\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n\n\ntrain = pd.get_dummies(train, columns=['cp_time','cp_dose'])\ntest_ = pd.get_dummies(test, columns=['cp_time','cp_dose'])\n\n\nfeature_cols = [c for c in train.columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['sig_id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    \n    \n    \ndef train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n    \n    \nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.2)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.dense1(x), 1e-3)\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 26\nBATCH_SIZE = 256\nLEARNING_RATE = 6e-4\nWEIGHT_DECAY = 1e-5\nNFOLDS = 5\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = True\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=2048","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n#     mskf = MultilabelStratifiedKFold(n_splits=5,random_state=seed)\n#     for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n#          train.loc[v_idx, 'kfold'] = int(f)\n#     train['kfold'] = train['kfold'].astype(int)\n    \n#     trn_idx = train[train['kfold'] != fold].index\n#     val_idx = train[train['kfold'] == fold].index\n    \n#     train_df = train[train['kfold'] != fold].reset_index(drop=True)\n#     valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n#     x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n#     x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n#     train_dataset = MoADataset(x_train, y_train)\n#     valid_dataset = MoADataset(x_valid, y_valid)\n#     trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n#     validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n#     model = Model(\n#         num_features=num_features,\n#         num_targets=num_targets,\n#         hidden_size=hidden_size,\n#     )\n\n#     model.load_state_dict(torch.load(f\"../input/pca-of-moa-pretained-non-scored-tabnet/SEED{seed}_FOLD{fold}_nonscored.pth\",\"cpu\"))\n#     model.to(DEVICE)\n    \n# #     optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n# #     scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n# #                                               max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n#     loss_fn = nn.BCEWithLogitsLoss()\n    \n# #     loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n# #     early_stopping_steps = EARLY_STOPPING_STEPS\n# #     early_step = 0\n    \n#     oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n#     best_loss = np.inf\n    \n# #     for epoch in range(EPOCHS):\n        \n# #         train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n# #         print(f\"SEED: {seed}, FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n#     valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n#     print(f\"SEED: {seed} ,FOLD: {fold}, valid_loss: {valid_loss}\")\n        \n#         if valid_loss < best_loss:\n            \n#             best_loss = valid_loss\n#     oof[val_idx] = valid_preds\n#             torch.save(model.state_dict(), f\"SEED{seed}_FOLD{fold}_nonscored.pth\")\n        \n#         elif(EARLY_STOP == True):\n            \n#             early_step += 1\n#             if (early_step >= early_stopping_steps):\n#                 break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    model.load_state_dict(torch.load(f\"../input/pca-of-moa-pretained-non-scored-tabnet/SEED{seed}_FOLD{fold}_nonscored.pth\",\"cpu\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):\n\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n\n        \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = [0,1,2]  #<-- Update\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    predictions_ = run_k_fold(NFOLDS, seed)\n\n    predictions += predictions_ / len(SEED)\n\n\ntest_[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=test_\ntest.drop(columns=[\"sig_id\"], inplace=True)\ntrain_targets_scored.drop(columns=[\"sig_id\"], inplace=True)\nX_test = test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored=train_targets_scored[train_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n# train_targets_scored","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsubmission = pd.read_csv('../input/lish-moa/sample_submission.csv')\nss=submission.copy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_EPOCH=200\nN_STARTS = 3\n\nres.loc[:, train_targets_scored.columns] = 0\nss.loc[:, train_targets_scored.columns] = 0\nNB_SPLITS = 5\n\nscores_auc = []\nfor seed in range(N_STARTS):\n    \n    seed+=42\n    #mskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, random_state=(seed+10), shuffle=True)\n    for fold_nb in range(NB_SPLITS):\n        print(\"FOLDS : \", fold_nb)\n        \n#         ## model\n#         X_train, y_train = train.values[train_idx, :], train_targets_scored.values[train_idx, :]\n#         X_val, y_val = train.values[val_idx, :], train_targets_scored.values[val_idx, :]\n# #         tabnet_params = dict(n_d=32, n_a=32, n_steps=1, gamma=1.3,\n# #                      lambda_sparse=0, optimizer_fn=torch.optim.Adam,\n# #                      optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n# #                      mask_type='entmax',\n# #                      scheduler_params=dict(mode=\"min\",\n# #                                            patience=5,\n# #                                            min_lr=1e-5,\n# #                                            factor=0.9,),\n# #                      scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n# #                      verbose=10,\n# #                      seed=seed\n# #                      )\n# #         model = TabNetRegressor(**tabnet_params)\n\n# #         model.fit(X_train=X_train,\n# #                   y_train=y_train,\n# #                   eval_set=[(X_val, y_val)],\n# #                   eval_name = [\"val\"],\n# #                   eval_metric = [\"logits_ll\"],\n# #                   max_epochs=MAX_EPOCH,\n# #                   patience=20, batch_size=1024, virtual_batch_size=128,\n# #                   num_workers=1, drop_last=False,\n# #                   # use binary cross entropy as this is not a regression problem\n# #                   loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n# #         model.save_model(\"tabnet_model\"+f\"_fold_{fold_nb}_seed_{seed}\")\n        model = TabNetRegressor()\n        model.load_model(f\"../input/pca-of-moa-pretained-non-scored-tabnet/tabnet_model_fold_{fold_nb}_seed_{seed}.zip\")\n#         preds_val = model.predict(X_val)\n#         # Apply sigmoid to the predictions\n#         preds =  1 / (1 + np.exp(-preds_val))\n# #         score = np.min(model.history[\"val_logits_ll\"])\n\n#         res.loc[val_idx, train_targets_scored.columns] += preds \n        \n\n\n        # preds on test\n        preds_test = model.predict(X_test)\n        test_preds = (1 / (1 + np.exp(-preds_test)))\n        ss.loc[:, train_targets_scored.columns] += test_preds \n\n# res.loc[:, train_targets_scored.columns] /= N_STARTS\nss.loc[:, train_targets_scored.columns] /= ((fold_nb+1) * N_STARTS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored0= pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntarget_cols=train_targets_scored0.columns[1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1= pd.read_csv('../input/lish-moa/test_features.csv')\ntrain1= pd.read_csv('../input/lish-moa/train_features.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.loc[test1['cp_type']==\"ctl_vehicle\", target_cols] = 0\n# ss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tmp1=pd.read_csv(\"../input/pca-of-moa-pretained-non-scored-tabnet/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# (abs(ss.iloc[:,1:].values-tmp1.iloc[:,1:].values)).max() #5.2197e-06","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_model8=ss\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model9"},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED=42\ndef seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(SEED)\n\ntrain_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\n\nsubmission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features2=train_features.copy()\ntest_features2=test_features.copy()\n\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in (GENES + CELLS):\n\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n    \n    \nn_comp = 600  #<--Update\npca_g = PCA(n_components=n_comp, random_state=42)\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n#gpca= (pca_g.fit(data[GENES]))\ngpca = pickle.load(open(\"../input/tabnetregressor-try-na-train-infe/gpca.pkl\",\"rb\"))\ntrain2= (gpca.transform(train_features[GENES]))\ntest2 = (gpca.transform(test_features[GENES]))\n\ntrain_gpca = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest_gpca = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train_gpca), axis=1)\ntest_features = pd.concat((test_features, test_gpca), axis=1)\n\n\n#CELLS\nn_comp = 50  #<--Update\n\npca_c = PCA(n_components=n_comp, random_state=42)\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n#cpca= (pca_c.fit(data[CELLS]))\ncpca = pickle.load(open(\"../input/tabnetregressor-try-na-train-infe/cpca.pkl\",\"rb\"))\ntrain2= (cpca.transform(train_features[CELLS]))\ntest2 = (cpca.transform(test_features[CELLS]))\n\ntrain_cpca = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest_cpca = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train_cpca), axis=1)\ntest_features = pd.concat((test_features, test_cpca), axis=1)\n\n\n\nc_n = [f for f in list(train_features.columns) if f not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]\n#mask = (train_features[c_n].var() >= 0.85).values\nmask = pickle.load(open(\"../input/tabnetregressor-try-na-train-infe/mask.pkl\",\"rb\"))\ntmp = train_features[c_n].loc[:, mask]\ntrain_features = pd.concat([train_features[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)\ntmp = test_features[c_n].loc[:, mask]\ntest_features = pd.concat([test_features[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)\n\n\ndef fe_cluster_genes(train, test, n_clusters_g = 22, SEED = 42):\n    \n    features_g = GENES\n    #features_c = CELLS\n    \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        #kmeans_genes = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        kmeans_genes = pickle.load(open(\"../input/tabnetregressor-try-na-train-infe/kmeans_genes.pkl\",\"rb\"))\n        #dump(kmeans_genes, open('kmeans_genes.pkl', 'wb'))\n        train[f'clusters_{kind}'] = kmeans_genes.predict(train_.values)\n        test[f'clusters_{kind}'] = kmeans_genes.predict(test_.values)\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n   # train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_features2 ,test_features2=fe_cluster_genes(train_features2,test_features2)\n\n\ndef fe_cluster_cells(train, test, n_clusters_c = 4, SEED = 42):\n    \n    #features_g = GENES\n    features_c = CELLS\n    \n    def create_cluster(train, test, features, kind = 'c', n_clusters = n_clusters_c):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n#         kmeans_cells = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n#         dump(kmeans_cells, open('kmeans_cells.pkl', 'wb'))\n        kmeans_cells = pickle.load(open(\"../input/tabnetregressor-try-na-train-infe/kmeans_cells.pkl\",\"rb\"))\n        train[f'clusters_{kind}'] = kmeans_cells.predict(train_.values)\n        test[f'clusters_{kind}'] = kmeans_cells.predict(test_.values)\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n   # train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_features2 ,test_features2=fe_cluster_cells(train_features2,test_features2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pca=pd.concat((train_gpca,train_cpca),axis=1)\ntest_pca=pd.concat((test_gpca,test_cpca),axis=1)\n\ndef fe_cluster_pca(train, test,n_clusters=5,SEED = 42):\n        data=pd.concat([train,test],axis=0)\n#         kmeans_pca = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n#         dump(kmeans_pca, open('kmeans_pca.pkl', 'wb'))\n        kmeans_pca = pickle.load(open(\"../input/tabnetregressor-try-na-train-infe/kmeans_pca.pkl\",\"rb\"))\n        train[f'clusters_pca'] = kmeans_pca.predict(train.values)\n        test[f'clusters_pca'] = kmeans_pca.predict(test.values)\n        train = pd.get_dummies(train, columns = [f'clusters_pca'])\n        test = pd.get_dummies(test, columns = [f'clusters_pca'])\n        return train, test\ntrain_cluster_pca ,test_cluster_pca = fe_cluster_pca(train_pca,test_pca)\n\ntrain_cluster_pca = train_cluster_pca.iloc[:,650:]\ntest_cluster_pca = test_cluster_pca.iloc[:,650:]\n\ntrain_features_cluster=train_features2.iloc[:,876:]\ntest_features_cluster=test_features2.iloc[:,876:]\n\ngsquarecols=['g-574','g-211','g-216','g-0','g-255','g-577','g-153','g-389','g-60','g-370','g-248','g-167','g-203','g-177','g-301','g-332','g-517','g-6','g-744','g-224','g-162','g-3','g-736','g-486','g-283','g-22','g-359','g-361','g-440','g-335','g-106','g-307','g-745','g-146','g-416','g-298','g-666','g-91','g-17','g-549','g-145','g-157','g-768','g-568','g-396']\n\ndef fe_stats(train, test):\n    \n    features_g = GENES\n    features_c = CELLS\n    \n    for df in train, test:\n#         df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n#         df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n#         df['g_skew'] = df[features_g].skew(axis = 1)\n#         df['c_sum'] = df[features_c].sum(axis = 1)\n#         df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n#         df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n#         df['c_skew'] = df[features_c].skew(axis = 1)\n#         df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n#         df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n#         df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n        df['c52_c42'] = df['c-52'] * df['c-42']\n        df['c13_c73'] = df['c-13'] * df['c-73']\n        df['c26_c13'] = df['c-23'] * df['c-13']\n        df['c33_c6'] = df['c-33'] * df['c-6']\n        df['c11_c55'] = df['c-11'] * df['c-55']\n        df['c38_c63'] = df['c-38'] * df['c-63']\n        df['c38_c94'] = df['c-38'] * df['c-94']\n        df['c13_c94'] = df['c-13'] * df['c-94']\n        df['c4_c52'] = df['c-4'] * df['c-52']\n        df['c4_c42'] = df['c-4'] * df['c-42']\n        df['c13_c38'] = df['c-13'] * df['c-38']\n        df['c55_c2'] = df['c-55'] * df['c-2']\n        df['c55_c4'] = df['c-55'] * df['c-4']\n        df['c4_c13'] = df['c-4'] * df['c-13']\n        df['c82_c42'] = df['c-82'] * df['c-42']\n        df['c66_c42'] = df['c-66'] * df['c-42']\n        df['c6_c38'] = df['c-6'] * df['c-38']\n        df['c2_c13'] = df['c-2'] * df['c-13']\n        df['c62_c42'] = df['c-62'] * df['c-42']\n        df['c90_c55'] = df['c-90'] * df['c-55']\n        \n        \n        for feature in features_c:\n             df[f'{feature}_squared'] = df[feature] ** 2     \n                \n        for feature in gsquarecols:\n            df[f'{feature}_squared'] = df[feature] ** 2        \n        \n    return train, test\n\ntrain_features2,test_features2=fe_stats(train_features2,test_features2)\n\n\ntrain_features_stats=train_features2.iloc[:,902:]\ntest_features_stats=test_features2.iloc[:,902:]\n\n\ntrain_features = pd.concat((train_features, train_features_cluster,train_cluster_pca,train_features_stats), axis=1)\ntest_features = pd.concat((test_features, test_features_cluster,test_cluster_pca,test_features_stats), axis=1)\n\n\ntrain = train_features\ntrain = train_features[train_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n# test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest=test_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)\n\n# train = pd.get_dummies(train, columns=['cp_time','cp_dose'])\n# test = pd.get_dummies(test, columns=['cp_time','cp_dose'])\ntrain['cp_time'] = train['cp_time'].map({24: 0, 48: 0.5, 72: 1})\ntest['cp_time'] = test['cp_time'].map({24: 0, 48: 0.5, 72: 1})\ntrain[\"cp_dose\"] = (train[\"cp_dose\"]==\"D1\") + 0\ntest[\"cp_dose\"] = (test[\"cp_dose\"]==\"D1\") + 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = [c for c in train.columns ]\nfeature_cols = [c for c in feature_cols if c not in ['sig_id']]\n\n\ntrain.drop(columns=[\"sig_id\"], inplace=True)\ntest.drop(columns=[\"sig_id\"], inplace=True)\ntrain_targets_scored.drop(columns=[\"sig_id\"], inplace=True)\nX_test = test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_EPOCH=200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored=train_targets_scored[train_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\nres=train_targets_scored.copy()\nss=submission.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_STARTS = 3\n\nres.loc[:, train_targets_scored.columns] = 0\nss.loc[:, train_targets_scored.columns] = 0\nNB_SPLITS = 5\n\nscores_auc = []\nfor seed in range(N_STARTS):\n    #seed_everything(seed)\n    mskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, random_state=(seed+10), shuffle=True)\n    for fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train, train_targets_scored)):\n        print(\"FOLDS : \", fold_nb)\n        \n        ## model\n#         X_train, y_train = train.values[train_idx, :], train_targets_scored.values[train_idx, :]\n#         X_val, y_val = train.values[val_idx, :], train_targets_scored.values[val_idx, :]\n#         tabnet_params = dict(n_d=24, n_a=40, n_steps=1, gamma=1.3,\n#                      lambda_sparse=0, optimizer_fn=torch.optim.Adam,\n#                      optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n#                      mask_type='entmax',\n#                      scheduler_params=dict(mode=\"min\",\n#                                            patience=5,\n#                                            min_lr=1e-5,\n#                                            factor=0.9,),\n#                      scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n#                      verbose=10,\n#                      seed=seed\n#                      )\n#         model = TabNetRegressor(**tabnet_params)\n\n#         model.fit(X_train=X_train,\n#                   y_train=y_train,\n#                   eval_set=[(X_val, y_val)],\n#                   eval_name = [\"val\"],\n#                   eval_metric = [\"logits_ll\"],\n#                   max_epochs=MAX_EPOCH,\n#                   patience=20, batch_size=1024, virtual_batch_size=128,\n#                   num_workers=1, drop_last=False,\n#                   # use binary cross entropy as this is not a regression problem\n#                   loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n#         model.save_model(\"tabnet_model\"+f\"_fold_{fold_nb}_seed_{seed}\")\n        model = TabNetRegressor()\n        model.load_model(f\"../input/tabnetregressor-try-na-train-infe/tabnet_model_fold_{fold_nb}_seed_{seed}.zip\")\n        \n#         preds_val = model.predict(X_val)\n#         # Apply sigmoid to the predictions\n#         preds =  1 / (1 + np.exp(-preds_val))\n# #         score = np.min(model.history[\"val_logits_ll\"])\n\n#         ## save oof to compute the CV later\n#         res.loc[val_idx, train_targets_scored.columns] += preds \n        \n\n        # preds on test\n        preds_test = model.predict(X_test)\n        test_preds = (1 / (1 + np.exp(-preds_test)))\n        ss.loc[:, train_targets_scored.columns] += test_preds \n# oof_preds_all = np.concatenate(oof_preds)\n# oof_targets_all = np.concatenate(oof_targets)\n# test_preds_all = np.stack(test_cv_preds)\n# res.loc[:, train_targets_scored.columns] /= N_STARTS\nss.loc[:, train_targets_scored.columns] /= ((fold_nb+1) * N_STARTS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols=train_targets_scored.columns\ntarget_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored0 = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ntest1= pd.read_csv('../input/lish-moa/test_features.csv')\ntrain1= pd.read_csv('../input/lish-moa/train_features.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.loc[test1['cp_type']==\"ctl_vehicle\", target_cols] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tmp1=pd.read_csv(\"../input/tabnetregressor-try-na-train-infe/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tmp1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# (abs(ss.iloc[:,1:].values-tmp1.iloc[:,1:].values)).max() #6.946e-06","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_model9=ss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model10"},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED=42\ndef seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = \"../input/lish-moa/\"\ntrain = pd.read_csv(data_path+'train_features.csv')\ntrain.drop(columns=[\"sig_id\"], inplace=True)\n\ntrain_targets_scored = pd.read_csv(data_path+'train_targets_scored.csv')\ntrain_targets_scored.drop(columns=[\"sig_id\"], inplace=True)\n\ntest = pd.read_csv(data_path+'test_features.csv')\ntest.drop(columns=[\"sig_id\"], inplace=True)\n\nsubmission = pd.read_csv(data_path+'sample_submission.csv')\n\nremove_vehicle = False\n\nif remove_vehicle:\n    kept_index = train['cp_type']=='trt_cp'\n    train = train.loc[kept_index].reset_index(drop=True)\n    train_targets_scored = train_targets_scored.loc[kept_index].reset_index(drop=True)\n\ntrain[\"cp_type\"] = (train[\"cp_type\"]==\"trt_cp\") + 0\ntrain[\"cp_dose\"] = (train[\"cp_dose\"]==\"D1\") + 0\n\ntest[\"cp_type\"] = (test[\"cp_type\"]==\"trt_cp\") + 0\ntest[\"cp_dose\"] = (test[\"cp_dose\"]==\"D1\") + 0\n\n# X_test = test.values\n\nc_cols = train.columns[train.columns.str.startswith('c-')]\ng_cols = train.columns[train.columns.str.startswith('g-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe_stats(train, test):\n    \n    features_g = list(g_cols)\n    features_c = list(c_cols)\n    \n    for df in [train, test]:\n#         df['g_sum'] = df[features_g].sum(axis = 1)\n#         df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n#         df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n#         df['g_skew'] = df[features_g].skew(axis = 1)\n#         df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n#         df['c_std'] = df[features_c].std(axis = 1)\n#         df[\"gstd_div_cmean\"]=df[features_g].std(axis = 1)/df[features_c].mean(axis = 1)\n#         df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n#         df['c_skew'] = df[features_c].skew(axis = 1)\n#         df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n#         df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n#         df['gc_std'] = df[features_g + features_c].std(axis = 1)\n#         df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n#         df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n    return train, test\n\ntrain, test = fe_stats(train, test)\nX_test = test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_EPOCH=200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_STARTS = 3\n\n# scores_auc_all= []\n\n# test_cv_preds = []\n# res.loc[:, train_targets_scored.columns] = 0\nss.loc[:, train_targets_scored.columns] = 0\nNB_SPLITS = 5\n#mskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, random_state=0, shuffle=True)\n# oof_preds = []\n# oof_targets = []\n# scores = []\nscores_auc = []\nfor seed in range(N_STARTS):\n    #seed_everything(seed)\n#     mskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, random_state=(seed+20), shuffle=True)\n    for fold_nb in range(NB_SPLITS):\n        print(\"FOLDS : \", fold_nb)\n        \n        ## model\n#         X_train, y_train = train.values[train_idx, :], train_targets_scored.values[train_idx, :]\n#         X_val, y_val = train.values[val_idx, :], train_targets_scored.values[val_idx, :]\n# #         tabnet_params = dict(n_d=24, n_a=24, n_steps=1, gamma=1.3,\n#                      lambda_sparse=0, optimizer_fn=torch.optim.Adam,\n#                      optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n#                      mask_type='entmax',\n#                      scheduler_params=dict(mode=\"min\",\n#                                            patience=5,\n#                                            min_lr=1e-5,\n#                                            factor=0.9,),\n#                      scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n#                      verbose=10,\n#                      seed=seed\n#                      )\n#         model = TabNetRegressor(**tabnet_params)\n\n#         model.fit(X_train=X_train,\n#                   y_train=y_train,\n#                   eval_set=[(X_val, y_val)],\n#                   eval_name = [\"val\"],\n#                   eval_metric = [\"logits_ll\"],\n#                   max_epochs=MAX_EPOCH,\n#                   patience=20, batch_size=1024, virtual_batch_size=128,\n#                   num_workers=1, drop_last=False,\n#                   # use binary cross entropy as this is not a regression problem\n#                   loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n#         model.save_model(\"tabnet_model\"+f\"_fold_{fold_nb}_seed_{seed}\")\n        model = TabNetRegressor()\n        model.load_model(f\"../input/tabnetregressor-2-0-train-infer-oof/tabnet_model_fold_{fold_nb}_seed_{seed}.zip\")\n\n#         preds_val = model.predict(X_val)\n#         # Apply sigmoid to the predictions\n#         preds =  1 / (1 + np.exp(-preds_val))\n\n#     #     name = cfg.save_name + f\"_fold{fold_nb}\"\n#     #     model.save_model(name)\n#         ## save oof to compute the CV later\n#         res.loc[val_idx, train_targets_scored.columns] += preds \n        \n# #         oof_preds.append(preds)\n#         oof_targets.append(y_val)\n#         scores.append(score)\n\n        # preds on test\n        preds_test = model.predict(X_test)\n        test_preds = (1 / (1 + np.exp(-preds_test)))\n        ss.loc[:, train_targets_scored.columns] += test_preds \n# oof_preds_all = np.concatenate(oof_preds)\n# oof_targets_all = np.concatenate(oof_targets)\n# test_preds_all = np.stack(test_cv_preds)\n# res.loc[:, train_targets_scored.columns] /= N_STARTS\nss.loc[:, train_targets_scored.columns] /= ((fold_nb+1) * N_STARTS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols=train_targets_scored.columns\ntrain_targets_scored0 = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ntest1= pd.read_csv('../input/lish-moa/test_features.csv')\ntrain1= pd.read_csv('../input/lish-moa/train_features.csv')\nss.loc[test1['cp_type']==\"ctl_vehicle\", target_cols] = 0\n# ss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tmp1=pd.read_csv(\"../input/tabnetregressor-2-0-train-infer-oof/submission.csv\")\n# (abs(ss.iloc[:,1:].values-tmp1.iloc[:,1:].values)).max() #1.9964e-06","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_model10=ss\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model11"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ndrug = pd.read_csv('/kaggle/input/lish-moa/train_drug.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_cols = train_features.columns[train_features.columns.str.startswith('c-')]\ng_cols = train_features.columns[train_features.columns.str.startswith('g-')]\ncont_cols = g_cols.to_list() + c_cols.to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RankGauss\n\nfor col in (list(c_cols) + list(g_cols)):\n\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n    \n    \ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe_pca(train, test, n_components_g = 28, n_components_c = 5, SEED = 123):\n    \n    features_g = list(g_cols)\n    features_c = list(c_cols)\n    \n    def create_pca(train, test, features, kind = 'g', n_components = n_components_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        pca = PCA(n_components = n_components,  random_state = SEED)\n        if kind == 'g':\n            #gpca = pca.fit(data)\n            gpca = pickle.load(open(\"../input/moa-pytorch-feature-engineering-0-01846/gpca.pkl\",\"rb\"))\n            data = gpca.transform(data)\n            #pickle.dump(gpca, open('gpca.pkl', 'wb'))\n        if kind == 'c':\n            #cpca = pca.fit(data)\n            cpca = pickle.load(open(\"../input/moa-pytorch-feature-engineering-0-01846/cpca.pkl\",\"rb\"))\n            data = cpca.transform(data)\n            #pickle.dump(cpca, open('cpca.pkl', 'wb'))\n        \n        columns = [f'pca_{kind}{i + 1}' for i in range(n_components)]\n        data = pd.DataFrame(data, columns = columns)\n        train_ = data.iloc[:train.shape[0]]\n        test_ = data.iloc[train.shape[0]:].reset_index(drop = True)\n        train = pd.concat([train, train_], axis = 1)\n        test = pd.concat([test, test_], axis = 1)\n        return train, test\n    \n    train, test = create_pca(train, test, features_g, kind = 'g', n_components = n_components_g)\n    train, test = create_pca(train, test, features_c, kind = 'c', n_components = n_components_c)\n    return train, test\n\ntrain_features, test_features = fe_pca(train_features, test_features, n_components_g = 600, n_components_c = 50, SEED = 42)\n\ntrain_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to extract common stats features\ndef fe_stats(train, test):\n    \n    features_g = list(g_cols)\n    features_c = list(c_cols)\n    \n    for df in [train, test]:\n#         df['g_sum'] = df[features_g].sum(axis = 1)\n#        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n#         df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n#         df['g_skew'] = df[features_g].skew(axis = 1)\n#         df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n#         df['c_std'] = df[features_c].std(axis = 1)\n#         df[\"gstd_div_cmean\"]=df[features_g].std(axis = 1)/df[features_c].mean(axis = 1)\n#         df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n#         df['c_skew'] = df[features_c].skew(axis = 1)\n#         df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n#         df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n#         df['gc_std'] = df[features_g + features_c].std(axis = 1)\n#         df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n#         df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n\n    return train.iloc[:,-2:], test.iloc[:,-2:]\n\n\ntrain_add,test_add=fe_stats(train_features, test_features)\n\ntmp=train_add.shape[1]\ntrain_features=train_features.iloc[:,:-tmp]\ntest_features=test_features.iloc[:,:-tmp]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# var_thresh = VarianceThreshold(0.8)  #<-- Update\n\n# #data = train_features.append(test_features)\n# #data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n# var_thresh.fit(train_features.iloc[:, 4:])\n# mask0=var_thresh.get_support(indices=True)+4\n# pickle.dump(mask0, open('mask0.pkl', 'wb'))\nmask0 = pickle.load(open(\"../input/moa-pytorch-feature-engineering-0-01846/mask0.pkl\",\"rb\"))\n\ntrain_features_transformed=train_features[train_features.columns[mask0]]\n\ntest_features_transformed=test_features[train_features.columns[mask0]]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    df = df.copy()\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 2, 'D2': 1})\n#     df = df.join(pd.get_dummies(df['cp_time'], drop_first=False, prefix='cp_time'))\n#     df = df.drop('cp_time', axis=1)\n    df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 1, 48: 2, 72: 3})\n    df.loc[df.cp_type==1,'cp_dose']=0\n    #del df['sig_id']\n    return df\n\n   \ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\n\n#del train_targets_scored['sig_id']\n\n# train_targets = train_targets.loc[train['cp_type']==0].reset_index(drop=True)\n# train = train.loc[train['cp_type']==0].reset_index(drop=True)\n\ndel train['cp_type']\ndel test['cp_type']\n\ntrain = pd.concat([train, train_add], axis = 1)\ntest = pd.concat([test, test_add], axis = 1)\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.merge(train_targets_scored, on='sig_id')\n\n\ntarget=train[train_targets_scored.columns]\n\nSEED = 42\nFOLDS = 5\n\n\ntargets=target.columns[1:]\nscored = target.merge(drug, on='sig_id', how='left') \n\n# LOCATE DRUGS\nvc = scored.drug_id.value_counts()\nvc1 = vc.loc[vc<=18].index.sort_values()\nvc2 = vc.loc[vc>18].index.sort_values()\n\n# STRATIFY DRUGS 18X OR LESS\ndct1 = {}; dct2 = {}\nskf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, \n          random_state=SEED)\ntmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\nfor fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n    dd = {k:fold for k in tmp.index[idxV].values}\n    dct1.update(dd)\n\n# STRATIFY DRUGS MORE THAN 18X\nskf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, \n          random_state=SEED)\ntmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\nfor fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n    dd = {k:fold for k in tmp.sig_id[idxV].values}\n    dct2.update(dd)\n    \nfolds = train.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=5)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)\n# folds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(train.shape)\n# print(folds.shape)\n# print(test.shape)\n# print(target.shape)\n# print(sample_submission.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    \ndef train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n#         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n\n\nclass Model(nn.Module):      # <-- Update\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.4)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.4)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data\n\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n\nfeature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 8e-6\nNFOLDS = 5            #<-- Update\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=2048","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n#     trn_idx = train[train['kfold'] != fold].index\n#     val_idx = train[train['kfold'] == fold].index\n    \n#     train_df = train[train['kfold'] != fold].reset_index(drop=True)\n#     valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n#     x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n#     x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n#     train_dataset = MoADataset(x_train, y_train)\n#     valid_dataset = MoADataset(x_valid, y_valid)\n#     trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n#     validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n#     model = Model(\n#         num_features=num_features,\n#         num_targets=num_targets,\n#         hidden_size=hidden_size,\n#     )\n    \n#     model.to(DEVICE)\n    \n#     optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n#                                               max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n#     loss_fn = nn.BCEWithLogitsLoss()\n    \n#     loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n#     early_stopping_steps = EARLY_STOPPING_STEPS\n#     early_step = 0\n    \n#     oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n#     best_loss = np.inf\n    \n#     for epoch in range(EPOCHS):\n        \n#         train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n#         print(f\"SEED: {seed}, FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n#         valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n#         print(f\"SEED: {seed} ,FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n#         if valid_loss < best_loss:\n            \n#             best_loss = valid_loss\n#             oof[val_idx] = valid_preds\n#             torch.save(model.state_dict(), f\"SEED_{seed}_FOLD_{fold}.pth\")\n        \n#         elif(EARLY_STOP == True):\n            \n#             early_step += 1\n#             if (early_step >= early_stopping_steps):\n#                 break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n   \n    model.load_state_dict(torch.load(f\"../input/moa-pytorch-feature-engineering-0-01846/SEED_{seed}_FOLD_{fold}.pth\",\"cpu\"))\n    model.to(DEVICE)\n    print(f\"../input/moa-pytorch-feature-engineering-0-01846/SEED_{seed}_FOLD_{fold}.pth\")\n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):\n  \n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Averaging on multiple SEEDS\n\nSEED = [0, 1, 2]  #<-- Update\n\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    predictions_ = run_k_fold(NFOLDS, seed)\n\n    predictions += predictions_ / len(SEED)\n\ntest[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(target_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.loc[test_features['cp_type']==\"ctl_vehicle\", target_cols] = 0\n# sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tmp1=pd.read_csv(\"../input/moa-pytorch-feature-engineering-0-01846/submission.csv\")\n# (abs(sub.iloc[:,1:].values-tmp1.iloc[:,1:].values)).max() #2.0464e-06","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_model11=sub\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model12\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\n\nc_cols = train_features.columns[train_features.columns.str.startswith('c-')]\ng_cols = train_features.columns[train_features.columns.str.startswith('g-')]\ncont_cols = g_cols.to_list() + c_cols.to_list()\nss = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(seed=42)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RankGauss\n\nfor col in (list(c_cols) + list(g_cols)):\n\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n    \n    \ndef fe_pca(train, test, n_components_g = 28, n_components_c = 5, SEED = 123):\n    \n    features_g = list(g_cols)\n    features_c = list(c_cols)\n    \n    def create_pca(train, test, features, kind = 'g', n_components = n_components_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        pca = PCA(n_components = n_components,  random_state = SEED)\n        if kind == 'g':\n            #gpca = (pca.fit(data))\n            gpca = pickle.load(open(\"../input/basic-nn-dummy-try-col/gpca.pkl\",\"rb\"))\n            data = (gpca.transform(data))\n            #pickle.dump(gpca, open('gpca.pkl', 'wb'))\n        if kind == 'c':\n            #cpca = (pca.fit(data))\n            cpca = pickle.load(open(\"../input/basic-nn-dummy-try-col/cpca.pkl\",\"rb\"))\n            data = (cpca.transform(data))\n            #pickle.dump(cpca, open('cpca.pkl', 'wb'))\n        \n        columns = [f'pca_{kind}{i + 1}' for i in range(n_components)]\n        data = pd.DataFrame(data, columns = columns)\n        train_ = data.iloc[:train.shape[0]]\n        test_ = data.iloc[train.shape[0]:].reset_index(drop = True)\n        train = pd.concat([train, train_], axis = 1)\n        test = pd.concat([test, test_], axis = 1)\n        return train, test\n    \n    train, test = create_pca(train, test, features_g, kind = 'g', n_components = n_components_g)\n    train, test = create_pca(train, test, features_c, kind = 'c', n_components = n_components_c)\n    return train, test\n\ntrain_features, test_features = fe_pca(train_features, test_features, n_components_g = 600, n_components_c = 50, SEED = 42)\n\n# train_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to extract common stats features\ndef fe_stats(train, test):\n    \n    features_g = list(g_cols)\n    features_c = list(c_cols)\n    \n    for df in [train, test]:\n#         df['g_sum'] = df[features_g].sum(axis = 1)\n#        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n#         df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n#         df['g_skew'] = df[features_g].skew(axis = 1)\n#         df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n#         df['c_std'] = df[features_c].std(axis = 1)\n#         df[\"gstd_div_cmean\"]=df[features_g].std(axis = 1)/df[features_c].mean(axis = 1)\n#         df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n#         df['c_skew'] = df[features_c].skew(axis = 1)\n#         df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n#         df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n#         df['gc_std'] = df[features_g + features_c].std(axis = 1)\n#         df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n#         df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n\n    return train.iloc[:,-2:], test.iloc[:,-2:]\n\ntrain_add,test_add=fe_stats(train_features, test_features)\n\ntmp=train_add.shape[1]\ntrain_features=train_features.iloc[:,:-tmp]\ntest_features=test_features.iloc[:,:-tmp]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_thresh = VarianceThreshold(0.8)  #<-- Update\n\n#data = train_features.append(test_features)\n#data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n# var_thresh.fit(train_features.iloc[:, 4:])\n# mask0=var_thresh.get_support(indices=True)+4\n# pickle.dump(mask0, open('mask0.pkl', 'wb'))\nmask0 = pickle.load(open(\"../input/basic-nn-dummy-try-col/mask0.pkl\",\"rb\"))\n\n\ntrain_features_transformed=train_features[train_features.columns[mask0]]\n\ntest_features_transformed=test_features[train_features.columns[mask0]]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    df = df.copy()\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 2, 'D2': 1})\n#     df = df.join(pd.get_dummies(df['cp_time'], drop_first=False, prefix='cp_time'))\n#     df = df.drop('cp_time', axis=1)\n    df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 1, 48: 2, 72: 3})\n    df.loc[df.cp_type==1,'cp_dose']=0\n    del df['sig_id']\n    return df\n\n   \ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\n\ndel train_targets['sig_id']\n\ndel train['cp_type']\ndel test['cp_type']\n\ndef process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data\n\ntrain=process_data(train)\ntest=process_data(test)\n\ntrain = pd.concat([train, train_add], axis = 1)\ntest = pd.concat([test, test_add], axis = 1)\n\n# train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smooth=0.001\np_min = smooth\np_max = 1-smooth\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -K.mean(y_true*K.log(y_pred) + (1-y_true)*K.log(1-y_pred))\n\ndef create_model(num_columns):\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(shape = (num_columns, )),\n    tf.keras.layers.BatchNormalization(),\n    #tf.keras.layers.Dropout(0.1),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1024, activation=\"swish\",kernel_initializer=\"glorot_normal\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1024, activation=\"swish\",kernel_initializer=\"glorot_normal\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"swish\",kernel_initializer=\"glorot_normal\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation=\"sigmoid\",kernel_initializer=\"glorot_normal\"))\n    ])\n    #opt = tf.optimizers.Adam(learning_rate = 0.001)\n    #opt = tfa.optimizers.SWA(opt)\n    opt=tfa.optimizers.AdamW(weight_decay=1e-6,learning_rate=0.001)\n    model.compile(opt,\n                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=smooth),metrics=[tf.keras.metrics.AUC(multi_label=True,name='auc'),logloss]\n                  )\n    return model\n\ntop_feats=list(range(len(train.columns)))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bad_target=list(train_targets.columns[train_targets.sum(axis=0)<2])\ngood_target=list(train_targets.columns[train_targets.sum(axis=0)>500])\n\ntrain_targets1=train_targets.drop(bad_target,axis=1)\ntrain_targets1=train_targets1.drop(good_target,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_STARTS = 3\n\nres = train_targets.copy()\nss.loc[:, train_targets.columns] = 0\nres.loc[:, train_targets.columns] = 0\n\nfor seed in range(N_STARTS):\n    for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits=5,random_state=seed+40,shuffle=True).split(train.values[:, top_feats], train_targets1)):\n        print(f'Fold {n}')\n\n        model = create_model(len(top_feats))\n#         reduce_lr_loss = ReduceLROnPlateau(monitor='val_logloss', factor=0.2, patience=2, mode='min', min_lr=1E-5)\n#         sv = tf.keras.callbacks.ModelCheckpoint(\n#             'seed-%i-fold-%i-model.h5'%(seed,n), monitor='val_loss', verbose=0, save_best_only=True,\n#             save_weights_only=True, mode='min', save_freq='epoch')\n        \n#         model.fit(train.values[tr][:, top_feats],\n#                   train_targets.astype(float).values[tr],shuffle=True,\n#                   validation_data=(train.values[te][:, top_feats], train_targets.astype(float).values[te]),\n#                   epochs=60, batch_size=128,\n#                   callbacks=[early_stopping,reduce_lr_loss,sv], verbose=2\n#                  )\n        model.load_weights(f\"../input/basic-nn-dummy-try-col/seed-{seed}-fold-{n}-model.h5\")\n        print(f\"../input/basic-nn-dummy-try-col/seed-{seed}-fold-{n}-model.h5\")\n        ss.loc[:, train_targets.columns] += model.predict(test.values[:, top_feats])\n#         res.loc[te, train_targets.columns] += model.predict(train.values[te][:, top_feats])\n        print('')\n    \nss.loc[:, train_targets.columns] /= ((n+1) * N_STARTS)\n# res.loc[:, train_targets.columns] /= N_STARTS\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1= pd.read_csv('../input/lish-moa/train_features.csv')\n\ntest1 = pd.read_csv('../input/lish-moa/test_features.csv')\n\ntrain_targets0 = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n\nss.loc[test1['cp_type']==\"ctl_vehicle\", train_targets.columns] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tmp1=pd.read_csv(\"../input/basic-nn-dummy-try-col-inferce/submission.csv\")\n# (abs(ss.iloc[:,1:].values-tmp1.iloc[:,1:].values)).max() #2.980e-7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_model12=ss\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model13\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_nonscore = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\n\nc_cols = train_features.columns[train_features.columns.str.startswith('c-')]\ng_cols = train_features.columns[train_features.columns.str.startswith('g-')]\ncont_cols = g_cols.to_list() + c_cols.to_list()\nss = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\n    \nseed_everything(seed=42)\n\n#RankGauss\n\nfor col in (list(c_cols) + list(g_cols)):\n\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n    \ndef preprocess(df):\n    df = df.copy()\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 2, 'D2': 1})\n#     df = df.join(pd.get_dummies(df['cp_time'], drop_first=False, prefix='cp_time'))\n#     df = df.drop('cp_time', axis=1)\n    df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 1, 48: 2, 72: 3})\n    df.loc[df.cp_type==1,'cp_dose']=0\n    del df['sig_id']\n    return df\n\ndef scaling(train, test):\n    features = train.columns[2:]\n    scaler = RobustScaler()\n    scaler.fit(pd.concat([train[features], test[features]], axis = 0))\n    train[features] = scaler.transform(train[features])\n    test[features] = scaler.transform(test[features])\n    return train, test\n\n\n    \ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\n\ndel train_targets['sig_id']\ndel train_nonscore['sig_id']\n\n# train_targets = train_targets.loc[train['cp_type']==0].reset_index(drop=True)\n# train = train.loc[train['cp_type']==0].reset_index(drop=True)\n\n\ntns=train_nonscore\ntns2=tns.loc[:,tns.sum()>3]\n# tns2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets1=pd.concat([train_targets,tns2],axis=1)\n\ndel train['cp_type']\ndel test['cp_type']\n\n# Function to extract common stats features\ndef fe_stats(train, test):\n    \n    features_g = list(g_cols)\n    features_c = list(c_cols)\n    \n    for df in [train, test]:\n#         df['g_sum'] = df[features_g].sum(axis = 1)\n#        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n#         df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n#         df['g_skew'] = df[features_g].skew(axis = 1)\n#         df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n#         df['c_std'] = df[features_c].std(axis = 1)\n#         df[\"gstd_div_cmean\"]=df[features_g].std(axis = 1)/df[features_c].mean(axis = 1)\n#         df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n#         df['c_skew'] = df[features_c].skew(axis = 1)\n#         df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n#         df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n#         df['gc_std'] = df[features_g + features_c].std(axis = 1)\n#         df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n#         df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n\n    return train.iloc[:,-2:], test.iloc[:,-2:]\n\ntrain_add,test_add=fe_stats(train, test)\n\ntmp=train_add.shape[1]\ntrain=train.iloc[:,:-tmp]\ntest=test.iloc[:,:-tmp]\n\n# train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe_pca(train, test, n_components_g = 28, n_components_c = 5, SEED = 123):\n    \n    features_g = list(g_cols)\n    features_c = list(c_cols)\n    \n    def create_pca(train, test, features, kind = 'g', n_components = n_components_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        pca = PCA(n_components = n_components,  random_state = SEED)\n        if kind == 'g':\n            #gpca = pca.fit(data)\n            gpca = pickle.load(open(\"../input/basic-nn-dummy-with-non-score/gpca.pkl\",\"rb\"))\n            data = gpca.transform(data)\n            #pickle.dump(gpca, open('gpca.pkl', 'wb'))\n        if kind == 'c':\n            #cpca = pca.fit(data)\n            cpca = pickle.load(open(\"../input/basic-nn-dummy-with-non-score/cpca.pkl\",\"rb\"))\n            data = cpca.transform(data)\n            #pickle.dump(cpca, open('cpca.pkl', 'wb'))\n            \n        columns = [f'pca_{kind}{i + 1}' for i in range(n_components)]\n        data = pd.DataFrame(data, columns = columns)\n        train_ = data.iloc[:train.shape[0]]\n        test_ = data.iloc[train.shape[0]:].reset_index(drop = True)\n        train = pd.concat([train, train_], axis = 1)\n        test = pd.concat([test, test_], axis = 1)\n        return train, test\n    \n    train, test = create_pca(train, test, features_g, kind = 'g', n_components = n_components_g)\n    train, test = create_pca(train, test, features_c, kind = 'c', n_components = n_components_c)\n    return train, test\ntrain, test = fe_pca(train, test, n_components_g = 280, n_components_c = 50, SEED = 123)\n\n# train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n\n\nvar_thresh = VarianceThreshold(0.8)  #<-- Update\ndata = train.append(test)\n\n# var_thresh.fit(data.iloc[:, 2:])\n# mask0=var_thresh.get_support(indices=True)+2\n# pickle.dump(mask0, open('mask0.pkl', 'wb'))\nmask0 = pickle.load(open(\"../input/basic-nn-dummy-with-non-score/mask0.pkl\",\"rb\"))\ndata_transformed=data[data.columns[mask0]]\n\ntrain_features_transformed = data_transformed[ : train.shape[0]]\ntest_features_transformed = data_transformed[-test.shape[0] : ]\n\n\ntrain = pd.DataFrame(train[['cp_time','cp_dose']].values.reshape(-1, 2),\\\n                              columns=['cp_time','cp_dose'])\n\ntrain = pd.concat([train, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest = pd.DataFrame(test[['cp_time','cp_dose']].values.reshape(-1, 2),\\\n                             columns=['cp_time','cp_dose'])\n\ntest= pd.concat([test, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data\n\ntrain=process_data(train)\ntest=process_data(test)\n\ntrain = pd.concat([train, train_add], axis = 1)\ntest = pd.concat([test, test_add], axis = 1)\n\ndef create_model(num_columns):\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(shape = (num_columns, )),\n    tf.keras.layers.BatchNormalization(),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"swish\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"swish\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(532, activation=\"sigmoid\"))\n    ])\n    #opt = tf.optimizers.Adam(learning_rate = 0.001)\n    #opt = tfa.optimizers.SWA(opt)\n    opt=tfa.optimizers.AdamW(weight_decay=1e-5)\n    model.compile(opt,\n                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0005),metrics=[tf.keras.metrics.AUC(multi_label=True,name='auc')]\n                  )\n    return model\n\nlen(list(train.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_feats=list(range(len(train.columns)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_STARTS = 3\n\nres = train_targets.copy()\nss.loc[:, train_targets.columns] = 0\nres.loc[:, train_targets.columns] = 0\n\nfor seed in range(N_STARTS):\n    for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits=5,random_state=seed,shuffle=True).split(train.values[:, top_feats], train_targets)):\n        print(f'Fold {n}')\n\n        model = create_model(len(top_feats))\n        #reduce_lr_loss = ReduceLROnPlateau(monitor='val_auc', factor=0.2, patience=3, verbose=1, epsilon=0.0001, mode='max')\n#         sv = tf.keras.callbacks.ModelCheckpoint(\n#             'seed-%i-fold-%i-model.h5'%(seed,n), monitor='val_loss', verbose=0, save_best_only=True,\n#             save_weights_only=True, mode='min', save_freq='epoch')\n#         model.fit(train.values[tr][:, top_feats],\n#                   train_targets1.astype(float).values[tr],shuffle=True,\n#                   validation_data=(train.values[te][:, top_feats], train_targets1.astype(float).values[te]),\n#                   epochs=60, batch_size=128,\n#                   callbacks=[early_stopping,get_lr_callback(128),sv], verbose=2\n#                  )\n        model.load_weights(f\"../input/basic-nn-dummy-with-non-score/seed-{seed}-fold-{n}-model.h5\")\n        print(f\"../input/basic-nn-dummy-with-non-score/seed-{seed}-fold-{n}-model.h5\")\n        ss.loc[:, train_targets.columns] += model.predict(test.values[:, top_feats])[:,:206]\n#         res.loc[te, train_targets.columns] += model.predict(train.values[te][:, top_feats])[:,:206]\n        print('')\n    \nss.loc[:, train_targets.columns] /= ((n+1) * N_STARTS)\n# res.loc[:, train_targets.columns] /= N_STARTS\n\n# metrics = []\n# for _target in train_targets.columns:\n#     metrics.append(log_loss(train_targets.loc[:, _target], res.loc[:, _target]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1= pd.read_csv('../input/lish-moa/train_features.csv')\n\ntest1 = pd.read_csv('../input/lish-moa/test_features.csv')\n\ntrain_targets0 = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\nss.loc[test1['cp_type']==\"ctl_vehicle\", train_targets.columns] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tmp1=pd.read_csv(\"../input/basic-nn-dummy-with-non-score-inferce/submission.csv\")\n# (abs(ss.iloc[:,1:].values-tmp1.iloc[:,1:].values)).max() #2.9964e-7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_model13=ss\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model14"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = '../input/lish-moa/'\nos.listdir(data_dir)\ntrain_features = pd.read_csv(data_dir + 'train_features.csv')\ntrain_targets_scored = pd.read_csv(data_dir + 'train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(data_dir + 'train_targets_nonscored.csv')\ntrain_drug = pd.read_csv(data_dir + 'train_drug.csv')\ntest_features = pd.read_csv(data_dir + 'test_features.csv')\nsample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\nfor col in (GENES + CELLS):\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n    \nSEED_VALUE = 42\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=SEED_VALUE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_comp = 600\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n\n# gpca= (PCA(n_components=n_comp, random_state=SEED_VALUE).fit(data[GENES]))\n# pickle.dump(gpca, open('gpca.pkl', 'wb'))\ngpca=pickle.load(open(\"../input/pytorch-transfer-learning-with-k-folds-by-drug/gpca.pkl\",\"rb\"))\n\ndata2 = gpca.transform(data[GENES])\n\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\nprint('train_features: {}'.format(train_features.shape))\nprint('test_features: {}'.format(test_features.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CELLS\nn_comp = 50\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n# cpca = (PCA(n_components=n_comp, random_state=SEED_VALUE).fit(data[CELLS]))\n# pickle.dump(cpca, open('cpca.pkl', 'wb'))\ncpca=pickle.load(open(\"../input/pytorch-transfer-learning-with-k-folds-by-drug/cpca.pkl\",\"rb\"))\n\ndata2 = cpca.transform(data[CELLS])\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\nprint('train_features: {}'.format(train_features.shape))\nprint('test_features: {}'.format(test_features.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_thresh = VarianceThreshold(0.8)\ndata = train_features.append(test_features)\n# var_thresh.fit(data.iloc[:, 4:])\n# mask0=var_thresh.get_support(indices=True)+4\n# pickle.dump(mask0, open('mask0.pkl', 'wb'))\nmask0 = pickle.load(open(\"../input/pytorch-transfer-learning-with-k-folds-by-drug/mask0.pkl\",\"rb\"))\n\ndata_transformed = data[data.columns[mask0]]\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\nprint('train_features: {}'.format(train_features.shape))\nprint('test_features: {}'.format(test_features.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train.merge(train_targets_nonscored, on='sig_id')\ntrain = train.merge(train_drug, on='sig_id')\ntrain = train[train['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n\ntrain = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)\n\ntarget_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\naux_target_cols = [x for x in train_targets_nonscored.columns if x != 'sig_id']\nall_target_cols = target_cols + aux_target_cols\n\nnum_targets = len(target_cols)\nnum_aux_targets = len(aux_target_cols)\nnum_all_targets = len(all_target_cols)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)\n        }\n        \n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n\n        return dct","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    return final_loss\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n\n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    return preds\n\nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(Model, self).__init__()\n        self.hidden_size = [1500, 1250, 1000, 750]\n        self.dropout_value = [0.5, 0.35, 0.3, 0.25]\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n        \n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_value[0])\n        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_value[1])\n        self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n\n        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n        self.dropout4 = nn.Dropout(self.dropout_value[2])\n        self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n\n        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n        self.dropout5 = nn.Dropout(self.dropout_value[3])\n        self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.leaky_relu(self.dense3(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = F.leaky_relu(self.dense4(x))\n\n        x = self.batch_norm5(x)\n        x = self.dropout5(x)\n        x = self.dense5(x)\n        return x\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data\n\nfeature_cols = [c for c in process_data(train).columns if c not in all_target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id', 'drug_id']]\nnum_features = len(feature_cols)\nnum_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 24\nBATCH_SIZE = 128\n\nWEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\nMAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\nDIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\nPCT_START = 0.1\n\nmodel = Model(num_features, num_all_targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n    vc = train.drug_id.value_counts()\n    vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n    vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n\n    for seed_id in range(SEEDS):\n        kfold_col = 'kfold_{}'.format(seed_id)\n        \n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}\n        dct2 = {}\n\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        train[kfold_col] = train.drug_id.map(dct1)\n        train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n        train[kfold_col] = train[kfold_col].astype('int8')\n        \n    return train\n\nSEEDS = 3\nNFOLDS = 5\nDRUG_THRESH = 18\n\ntrain = make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH)\n# train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold_id, seed_id):\n    seed_everything(seed_id)\n    \n    train_ = process_data(train)\n    test_ = process_data(test)\n    \n    kfold_col = f'kfold_{seed_id}'\n    trn_idx = train_[train_[kfold_col] != fold_id].index\n    val_idx = train_[train_[kfold_col] == fold_id].index\n    \n    train_df = train_[train_[kfold_col] != fold_id].reset_index(drop=True)\n    valid_df = train_[train_[kfold_col] == fold_id].reset_index(drop=True)\n    \n#     def train_model(model, tag_name, target_cols_now, fine_tune_scheduler=None):\n#         x_train, y_train  = train_df[feature_cols].values, train_df[target_cols_now].values\n#         x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols_now].values\n        \n#         train_dataset = MoADataset(x_train, y_train)\n#         valid_dataset = MoADataset(x_valid, y_valid)\n\n#         trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n#         validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n#         optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY[tag_name])\n#         scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n#                                                   steps_per_epoch=len(trainloader),\n#                                                   pct_start=PCT_START,\n#                                                   div_factor=DIV_FACTOR[tag_name], \n#                                                   max_lr=MAX_LR[tag_name],\n#                                                   epochs=EPOCHS)\n        \n#         loss_fn = nn.BCEWithLogitsLoss()\n#         loss_tr = SmoothBCEwLogits(smoothing=0.001)\n\n#         oof = np.zeros((len(train), len(target_cols_now)))\n#         best_loss = np.inf\n        \n#         for epoch in range(EPOCHS):\n#             if fine_tune_scheduler is not None:\n#                 fine_tune_scheduler.step(epoch, model)\n\n#             train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n#             valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n#             print(f\"SEED: {seed_id}, FOLD: {fold_id}, {tag_name}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}\")\n\n#             if np.isnan(valid_loss):\n#                 break\n            \n#             if valid_loss < best_loss:\n#                 best_loss = valid_loss\n#                 oof[val_idx] = valid_preds\n#                 torch.save(model.state_dict(), f\"{tag_name}_SEED_{seed_id}_FOLD{fold_id}_.pth\")\n\n#         return oof\n\n#     fine_tune_scheduler = FineTuneScheduler(EPOCHS)\n\n#     pretrained_model = Model(num_features, num_all_targets)\n#     pretrained_model.to(DEVICE)\n\n#     # Train on scored + nonscored targets\n#     train_model(pretrained_model, 'ALL_TARGETS', all_target_cols)\n\n#     # Load the pretrained model with the best loss\n#     pretrained_model = Model(num_features, num_all_targets)\n#     pretrained_model.load_state_dict(torch.load(f\"ALL_TARGETS_SEED_{seed_id}_FOLD{fold_id}_.pth\"))\n#     pretrained_model.to(DEVICE)\n\n#     # Copy model without the top layer\n#     final_model = fine_tune_scheduler.copy_without_top(pretrained_model, num_features, num_all_targets, num_targets)\n\n#     # Fine-tune the model on scored targets only\n#     oof = train_model(final_model, 'SCORED_ONLY', target_cols, fine_tune_scheduler)\n\n    # Load the fine-tuned model with the best loss\n    model = Model(num_features, num_targets)\n    model.load_state_dict(torch.load(f\"../input/pytorch-transfer-learning-with-k-folds-by-drug/SCORED_ONLY_SEED_{seed_id}_FOLD{fold_id}_.pth\",\"cpu\"))\n    print(f\"../input/pytorch-transfer-learning-with-k-folds-by-drug/SCORED_ONLY_SEED_{seed_id}_FOLD{fold_id}_.pth\")\n    model.to(DEVICE)\n\n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    predictions = np.zeros((len(test_), num_targets))\n    predictions = inference_fn(model, testloader, DEVICE)\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed_id):\n\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold_id in range(NFOLDS):\n        pred_ = run_training(fold_id, seed_id)\n        predictions += pred_ / NFOLDS\n       \n        \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = [0, 1, 2]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\n\nfor seed_id in SEED:\n    predictions_ = run_k_fold(NFOLDS, seed_id)\n\n    predictions += predictions_ / len(SEED)\n\ntest[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n# sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tmp1=pd.read_csv(\"../input/pytorch-transfer-learning-with-k-folds-by-drug/submission.csv\")\n# (abs(sub.iloc[:,1:].values-tmp1.iloc[:,1:].values)).max() #9.623e-8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_model14=sub\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nss_tot=[ss_model1,ss_model2,ss_model3,ss_model4,ss_model5,ss_model8,ss_model9,ss_model10,ss_model11,ss_model12,ss_model13,ss_model14]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in range(len(ss_tot)):\n#     print(ss_tot[i].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_ensemble=ss_tot[0].copy()\nss_ensemble.iloc[:,1:]=0\nss_ensemble1=ss_ensemble.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ss_tot[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ss_ensemble","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_m = pickle.load(open(\"../input/clip-try-col-method-ensemble-12-model/col_m.pkl\",\"rb\"))\ncol_w = pickle.load(open(\"../input/clip-try-col-method-ensemble-12-model/col_w.pkl\",\"rb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor col in range(ss_ensemble.shape[1]-1):\n    md = ss_tot[col_m[col][0]].iloc[:,1+col]\n    for i,k in enumerate(col_m[col][1:]):\n        \n        md=col_w[col][i]*ss_tot[k].iloc[:,1+col] + (1-col_w[col][i])*md\n    ss_ensemble1.iloc[:,1+col]=md","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_ensemble1.iloc[:,1:].values.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1=pd.read_csv(\"../input/lish-moa/test_features.csv\")\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\ntargets_col=sample_submission.columns[1:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ss_ensemble1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_ensemble2=ss_ensemble.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(ss_tot)):\n    ss_ensemble2.iloc[:,1:]+=ss_tot[i].iloc[:,1:]/len(ss_tot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_ensemble.iloc[:,1:]=ss_ensemble1.iloc[:,1:]*0.7+ss_ensemble2.iloc[:,1:]*0.3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_ensemble.iloc[:,1:]=ss_ensemble.iloc[:,1:].clip(0.0005,0.9995)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nss_ensemble.loc[test1['cp_type']==\"ctl_vehicle\", targets_col] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_ensemble.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}