{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Nov  1 12:07:10 2020\n@author: jaket\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mechanisms of Action - A XGb and NN Comparison\n"},{"metadata":{},"cell_type":"markdown","source":"### On Kaggle Neural nets and XGB classifiers have been the most commonly used approaches to this multioutput classification problem.<br>\n### Here, I compare the log loss of both and tune/use the best for submission. EDA has already been extensively done so I will go straight to modelling. I use PCA to reduce the variables included as the factor number is significant and highly covaried.<br>\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"import math\nimport pandas as pd\nimport numpy as np\nimport os\nimport warnings \nimport calendar","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom category_encoders import CountEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import log_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.multioutput import MultiOutputClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"warnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"SEED = 42\nnp.random.seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train = pd.read_csv('../input/lish-moa/train_features.csv')\ntest = pd.read_csv('../input/lish-moa/test_features.csv')\ntargets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA on train features<br>\n\n#### First scale the numerical data, then fit a PCA to it"},{"metadata":{"trusted":false},"cell_type":"code","source":"scaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"numeric = train.drop(['sig_id', 'cp_type', 'cp_time', 'cp_dose'], axis=1)\ncats= train.filter(items=['sig_id', 'cp_type', 'cp_time', 'cp_dose'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"scaler.fit(numeric) # Fit Scaler\nnumeric_sc = scaler.transform(numeric) # Scale data","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pca = PCA(.90)\npca.fit(numeric_sc) # Do PCA\npca.n_components_  # How many dimensions?","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_pc =pca.transform(numeric_sc)\ntrain_pc = pd.DataFrame(data=train_pc)\ntrain=pd.concat([train_pc, cats], axis=1) # Add back to cats","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print ('90% of the variance is explained in ',pca.n_components_,  ' components.') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets do the same with the test data..."},{"metadata":{"trusted":false},"cell_type":"code","source":"numeric_test = test.drop(['sig_id', 'cp_type', 'cp_time', 'cp_dose'], axis=1)\ncats_test= test.filter(items=['sig_id', 'cp_type', 'cp_time', 'cp_dose'])\nscaler.fit(numeric_test)\nnumeric_sc_test = scaler.transform(numeric_test) # Scale data","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_pc =pca.transform(numeric_sc_test)\ntest_pc = pd.DataFrame(data=test_pc)\ntest= pd.concat([test_pc, cats_test], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encode categorical variables"},{"metadata":{"trusted":false},"cell_type":"code","source":"train[['cp_type', 'cp_dose']]=train[['cp_type', 'cp_dose']].astype('category')\ntest[['cp_type', 'cp_dose']]=test[['cp_type', 'cp_dose']].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dummies_train=pd.get_dummies(train[['cp_type', 'cp_dose']])\ndummies_test=pd.get_dummies(test[['cp_type', 'cp_dose']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train=train.drop(['cp_type', 'cp_dose'], axis=1) # Delete uncoded cats\ntest=test.drop(['cp_type', 'cp_dose'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train=pd.concat([train, dummies_train], axis=1) # Add encoded cats\ntest=pd.concat([test, dummies_test], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop id col and turn to np"},{"metadata":{"trusted":false},"cell_type":"code","source":"X = train.drop('sig_id', axis=1).to_numpy()\nX_test = test.drop('sig_id', axis=1).to_numpy()\ny = targets.drop('sig_id', axis=1).to_numpy() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have X, y and X_test we dont need anything else in the environment. Kaggle throws an error 137 (OOM) without deleting all objects - so I do this for space."},{"metadata":{"trusted":true},"cell_type":"code","source":"del train\ndel test\ndel numeric\ndel cats\ndel numeric_sc\ndel train_pc\ndel numeric_test\ndel cats_test\ndel numeric_sc_test\ndel test_pc\ndel dummies_train\ndel dummies_test\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Set up cross-fold validation"},{"metadata":{"trusted":false},"cell_type":"code","source":"kf = KFold(n_splits=5)\ndef model_validation_loop (X, y, clf):\n    log_loss_list = []\n    for n, (train_idx, pred_idx) in enumerate(kf.split(X, y)):\n        print('Starting fold: ', n)\n        X_train, X_test = X[train_idx], X[pred_idx]\n        y_train, y_test = y[train_idx], y[pred_idx]\n        \n        clf.fit(X_train, y_train)\n        \n        # Get Log Loss\n        \n        preds = clf.predict_proba(X_test) # list of preds per class\n        preds = np.array(preds)[:,:,1].T # take the positive class\n        \n        loss = log_loss(np.ravel(y_test), np.ravel(preds))\n        print('Log Loss for this fold:', loss)\n        log_loss_list.append(loss)\n    \n    print('Mean Log Loss:', np.mean(log_loss_list))\n    print('_'*50)\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll compare these 2 multioutput clfs"},{"metadata":{"trusted":false},"cell_type":"code","source":"classifiers = [\n    MultiOutputClassifier(XGBClassifier()),\n    MultiOutputClassifier(MLPClassifier(random_state = 42))    \n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"clf_names= ['GradientBoost', 'NeuralNet']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The XGB model has a slight performance bonus on the Neural Net. Now we can tune the XGB model. Instead of doing a parameter search I have looked on Kaggle for parametes to save many hours. Thanks to: https://www.kaggle.com/fchmiel/xgboost-baseline-multilabel-classification"},{"metadata":{"trusted":false},"cell_type":"code","source":"params = {'colsample_bytree': 0.6522,\n          'gamma': 3.6975,\n          'learning_rate': 0.0503,\n          'max_delta_step': 2.0706,\n          'max_depth': 10,\n          'min_child_weight': 31.5800,\n          'n_estimators': 166,\n          'subsample': 0.8639}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tuned_xgb=MultiOutputClassifier(XGBClassifier().set_params(**params))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run validation fn to test improvement in performance in the basic vs tuned model."},{"metadata":{"trusted":false},"cell_type":"code","source":"model_validation_loop(X, y, tuned_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Improved model observed. Lets now predict the test data"},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_sub =  pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predict"},{"metadata":{"trusted":false},"cell_type":"code","source":"final_preds=pd.DataFrame(data=tuned_xgb.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Format"},{"metadata":{"trusted":false},"cell_type":"code","source":"ids=pd.DataFrame(data=sample_sub['sig_id'])[0:100]\nfinal_preds=pd.concat([ids, final_preds], axis=1)\nfinal_preds.columns=sample_sub.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Write"},{"metadata":{"trusted":false},"cell_type":"code","source":"final_preds.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}