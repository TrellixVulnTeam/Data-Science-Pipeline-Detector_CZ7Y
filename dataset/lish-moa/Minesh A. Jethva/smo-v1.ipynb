{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_features = pd.read_csv(\"/kaggle/input/lish-moa/train_features.csv\")\ntest_features = pd.read_csv(\"/kaggle/input/lish-moa/test_features.csv\")\n\ntrain_drug = pd.read_csv(\"/kaggle/input/lish-moa/train_drug.csv\")\n\ntrain_targets_nonscored = pd.read_csv(\"/kaggle/input/lish-moa/train_targets_nonscored.csv\")\ntrain_targets_scored = pd.read_csv(\"/kaggle/input/lish-moa/train_targets_scored.csv\")\n\nsample_submission = pd.read_csv(\"/kaggle/input/lish-moa/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape, test_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## test_features"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.sig_id.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.cp_type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.cp_time.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.cp_dose.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.columns.str[:2].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## train_targets_scored"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"( train_features.sig_id.sort_values() == train_targets_scored.sig_id.sort_values() ).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ctl_sig_ids = train_features.loc[train_features.cp_type == \"ctl_vehicle\",\"sig_id\"]\ntrain_targets_scored.set_index('sig_id').loc[ctl_sig_ids].sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = train_features.sort_values('sig_id').drop(['sig_id', 'cp_type'],1)\ntest_dataset = test_features.sort_values('sig_id').drop(['sig_id', 'cp_type'],1)\n\ntrain_targets = train_targets_scored.sort_values('sig_id').drop(['sig_id'],1)\n\ntrain_dataset.cp_dose = train_dataset.cp_dose.str[1:].astype('f')\ntest_dataset.cp_dose = test_dataset.cp_dose.str[1:].astype('f')\n\n[i.shape for i in [train_dataset, train_targets]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Test Spliting"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras as keras\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(1291)\nX_train, X_test, y_train, y_test = train_test_split(train_dataset.values, train_targets.values, test_size=0.2)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n[i.shape for i in [X_train, X_test, y_train, y_test, X_val, y_val]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer\n\ntest_dataset_X = test_dataset.values.copy()\n\nfor coli in range(X_train.shape[1]):\n\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    raw_vec = np.concatenate([X_train[:,coli:(coli+1)], np.array([[-10.0],[10.0]])])\n    transformer.fit(raw_vec)\n\n    X_train[:,coli:(coli+1)] = transformer.transform(raw_vec[:-2,:])\n    X_val[:,coli:(coli+1)] = transformer.transform(X_val[:,coli:(coli+1)])\n    X_test[:,coli:(coli+1)] = transformer.transform(X_test[:,coli:(coli+1)])\n    \n    test_dataset_X[:,coli:(coli+1)] = transformer.transform(test_dataset_X[:,coli:(coli+1)])\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Defination"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n    tf.keras.backend.clear_session()\n    model = tf.keras.Sequential([\n        tf.keras.layers.Input((X_train.shape[1],1)),\n        \n        tf.keras.layers.Conv1D(128, 3, 2, activation = 'linear'),\n        tf.keras.layers.MaxPool1D(2),\n        tf.keras.layers.BatchNormalization(),\n        \n#         tf.keras.layers.Conv1D(64, 3, 1, activation = 'relu'),\n#         tf.keras.layers.MaxPool1D(2),\n#         tf.keras.layers.BatchNormalization(),\n        \n        tf.keras.layers.Flatten(),\n        \n#         tf.keras.layers.Dropout(0.7),\n#         tf.keras.layers.Dense(750, activation = 'relu'),\n        \n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(450, activation = 'sigmoid'),\n\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(y_train.shape[1], activation = 'sigmoid')\n\n    ])\n    return model\n\nmodel = get_model()\n# model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['mae', tf.keras.metrics.AUC()])\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks=[\n    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=25, mode=\"min\", verbose=1, restore_best_weights=True),\n    tf.keras.callbacks.ModelCheckpoint(filepath=\"best_model.hdf5\", verbose=1, save_best_only=True),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                            factor=0.7,\n                                            patience=3,\n                                            verbose=1,\n                                            mode='min',\n                                            min_delta=0.0001,\n                                            cooldown=0,\n                                            min_lr=0.00001)\n] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.expand_dims(X_train,2).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = get_model()\nadam = tf.keras.optimizers.Adam(lr=0.1)\nmodel.compile(loss='binary_crossentropy', optimizer=adam,metrics=['mae', tf.keras.metrics.AUC()])\nnp.random.seed(1291)\ntf.random.set_seed(1291)\nhistory = model.fit(np.expand_dims(X_train,2), y_train, epochs=1500, batch_size=120, \n                    validation_data=(np.expand_dims(X_val,2), y_val), callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.load_model('best_model.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_learning_curve(history):\n        # plt.plot(history.epoch, history.history[\"auc\"], \".:\")\n        # plt.plot(history.epoch, history.history[\"val_auc\"], \".:\")\n\n        plt.plot(history.epoch, history.history[\"loss\"], \".:\", label=\"loss\")\n        plt.plot(history.epoch, history.history[\"val_loss\"], \".:\", label=\"val_loss\")\n        plt.legend()\n        plt.yscale('log')\n\nplot_learning_curve(history)\nval_auc = history.history[\"val_auc\"][-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(np.expand_dims(X_test,2), y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# multi label ROC, rank labels to further focus on","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = model.predict(np.expand_dims(test_dataset_X,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions.round(2)[:1,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# post processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features = test_features.sort_values('sig_id')\nctl_index = np.where(test_features.cp_type!=\"trt_cp\")[0]\n\ntest_predictions_mod = test_predictions.copy()\ntest_predictions_mod[ctl_index,:] = 0\n\ntest_predictions_mod = test_predictions_mod.round(2)\ntest_predictions_mod","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sub = sample_submission.sort_values('sig_id')\n# test_sub = pd.DataFrame(test_predictions, columns=test_sub.columns[1:])\ntest_sub = pd.DataFrame(test_predictions_mod, columns=test_sub.columns[1:])\ntest_sub[\"sig_id\"] = sample_submission.sort_values('sig_id')['sig_id']\ntest_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!head submission.csv","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}