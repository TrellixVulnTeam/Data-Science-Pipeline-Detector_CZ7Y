{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"This file includes multilabel cross validators based on an implementation of\nthe Iterative Stratification algorithm described in the following paper:\nSechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of Multi-\nLabel Data. In: Gunopulos D., Hofmann T., Malerba D., Vazirgiannis M. (eds)\nMachine Learning and Knowledge Discovery in Databases. ECML PKDD 2011. Lecture\nNotes in Computer Science, vol 6913. Springer, Berlin, Heidelberg.\n\nFrom scikit-learn 0.19.0, StratifiedKFold, RepeatedStratifiedKFold, and\nStratifiedShuffleSplit were copied and modified, retaining compatibility\nwith scikit-learn.\n\nAttribution to authors of scikit-learn/model_selection/_split.py under BSD 3 clause:\n    Alexandre Gramfort <alexandre.gramfort@inria.fr>,\n    Gael Varoquaux <gael.varoquaux@normalesup.org>,\n    Olivier Grisel <olivier.grisel@ensta.org>,\n    Raghav RV <rvraghav93@gmail.com>\n\"\"\"\n\n# Author: Trent J. Bradberry <trentjason@hotmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\n\nfrom sklearn.utils import check_random_state\nfrom sklearn.utils.validation import _num_samples, check_array\nfrom sklearn.utils.multiclass import type_of_target\n\nfrom sklearn.model_selection._split import _BaseKFold, _RepeatedSplits, \\\n    BaseShuffleSplit, _validate_shuffle_split\n\n\ndef IterativeStratification(labels, r, random_state):\n    \"\"\"This function implements the Iterative Stratification algorithm described\n    in the following paper:\n    Sechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of\n    Multi-Label Data. In: Gunopulos D., Hofmann T., Malerba D., Vazirgiannis M.\n    (eds) Machine Learning and Knowledge Discovery in Databases. ECML PKDD\n    2011. Lecture Notes in Computer Science, vol 6913. Springer, Berlin,\n    Heidelberg.\n    \"\"\"\n\n    n_samples = labels.shape[0]\n    test_folds = np.zeros(n_samples, dtype=int)\n\n    # Calculate the desired number of examples at each subset\n    c_folds = r * n_samples\n\n    # Calculate the desired number of examples of each label at each subset\n    c_folds_labels = np.outer(r, labels.sum(axis=0))\n\n    labels_not_processed_mask = np.ones(n_samples, dtype=bool)\n\n    while np.any(labels_not_processed_mask):\n        # Find the label with the fewest (but at least one) remaining examples,\n        # breaking ties randomly\n        num_labels = labels[labels_not_processed_mask].sum(axis=0)\n\n        # Handle case where only all-zero labels are left by distributing\n        # across all folds as evenly as possible (not in original algorithm but\n        # mentioned in the text). (By handling this case separately, some\n        # code redundancy is introduced; however, this approach allows for\n        # decreased execution time when there are a relatively large number\n        # of all-zero labels.)\n        if num_labels.sum() == 0:\n            sample_idxs = np.where(labels_not_processed_mask)[0]\n\n            for sample_idx in sample_idxs:\n                fold_idx = np.where(c_folds == c_folds.max())[0]\n\n                if fold_idx.shape[0] > 1:\n                    fold_idx = fold_idx[random_state.choice(fold_idx.shape[0])]\n\n                test_folds[sample_idx] = fold_idx\n                c_folds[fold_idx] -= 1\n\n            break\n\n        label_idx = np.where(num_labels == num_labels[np.nonzero(num_labels)].min())[0]\n        if label_idx.shape[0] > 1:\n            label_idx = label_idx[random_state.choice(label_idx.shape[0])]\n\n        sample_idxs = np.where(np.logical_and(labels[:, label_idx].flatten(), labels_not_processed_mask))[0]\n\n        for sample_idx in sample_idxs:\n            # Find the subset(s) with the largest number of desired examples\n            # for this label, breaking ties by considering the largest number\n            # of desired examples, breaking further ties randomly\n            label_folds = c_folds_labels[:, label_idx]\n            fold_idx = np.where(label_folds == label_folds.max())[0]\n\n            if fold_idx.shape[0] > 1:\n                temp_fold_idx = np.where(c_folds[fold_idx] ==\n                                         c_folds[fold_idx].max())[0]\n                fold_idx = fold_idx[temp_fold_idx]\n\n                if temp_fold_idx.shape[0] > 1:\n                    fold_idx = fold_idx[random_state.choice(temp_fold_idx.shape[0])]\n\n            test_folds[sample_idx] = fold_idx\n            labels_not_processed_mask[sample_idx] = False\n\n            # Update desired number of examples\n            c_folds_labels[fold_idx, labels[sample_idx]] -= 1\n            c_folds[fold_idx] -= 1\n\n    return test_folds\n\n\nclass MultilabelStratifiedKFold(_BaseKFold):\n    \"\"\"Multilabel stratified K-Folds cross-validator\n    Provides train/test indices to split multilabel data into train/test sets.\n    This cross-validation object is a variation of KFold that returns\n    stratified folds for multilabel data. The folds are made by preserving\n    the percentage of samples for each label.\n    Parameters\n    ----------\n    n_splits : int, default=3\n        Number of folds. Must be at least 2.\n    shuffle : boolean, optional\n        Whether to shuffle each stratification of the data before splitting\n        into batches.\n    random_state : int, RandomState instance or None, optional, default=None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Unlike StratifiedKFold that only uses random_state\n        when ``shuffle`` == True, this multilabel implementation\n        always uses the random_state since the iterative stratification\n        algorithm breaks ties randomly.\n    Examples\n    --------\n    >>> from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n    >>> import numpy as np\n    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n    >>> mskf = MultilabelStratifiedKFold(n_splits=2, random_state=0)\n    >>> mskf.get_n_splits(X, y)\n    2\n    >>> print(mskf)  # doctest: +NORMALIZE_WHITESPACE\n    MultilabelStratifiedKFold(n_splits=2, random_state=0, shuffle=False)\n    >>> for train_index, test_index in mskf.split(X, y):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [0 3 4 6] TEST: [1 2 5 7]\n    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n    Notes\n    -----\n    Train and test sizes may be slightly different in each fold.\n    See also\n    --------\n    RepeatedMultilabelStratifiedKFold: Repeats Multilabel Stratified K-Fold\n    n times.\n    \"\"\"\n\n    def __init__(self, n_splits=3, shuffle=False, random_state=None):\n        super(MultilabelStratifiedKFold, self).__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n\n    def _make_test_folds(self, X, y):\n        y = np.asarray(y, dtype=bool)\n        type_of_target_y = type_of_target(y)\n\n        if type_of_target_y != 'multilabel-indicator':\n            raise ValueError(\n                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(type_of_target_y))\n\n        num_samples = y.shape[0]\n\n        rng = check_random_state(self.random_state)\n        indices = np.arange(num_samples)\n\n        if self.shuffle:\n            rng.shuffle(indices)\n            y = y[indices]\n\n        r = np.asarray([1 / self.n_splits] * self.n_splits)\n\n        test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n\n        return test_folds[np.argsort(indices)]\n\n    def _iter_test_masks(self, X=None, y=None, groups=None):\n        test_folds = self._make_test_folds(X, y)\n        for i in range(self.n_splits):\n            yield test_folds == i\n\n    def split(self, X, y, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n            Note that providing ``y`` is sufficient to generate the splits and\n            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n            ``X`` instead of actual training data.\n        y : array-like, shape (n_samples, n_labels)\n            The target variable for supervised learning problems.\n            Multilabel stratification is done based on the y labels.\n        groups : object\n            Always ignored, exists for compatibility.\n        Returns\n        -------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        \"\"\"\n        y = check_array(y, ensure_2d=False, dtype=None)\n        return super(MultilabelStratifiedKFold, self).split(X, y, groups)\n\n\nclass RepeatedMultilabelStratifiedKFold(_RepeatedSplits):\n    \"\"\"Repeated Multilabel Stratified K-Fold cross validator.\n    Repeats Mulilabel Stratified K-Fold n times with different randomization\n    in each repetition.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n    n_repeats : int, default=10\n        Number of times cross-validator needs to be repeated.\n    random_state : None, int or RandomState, default=None\n        Random state to be used to generate random state for each\n        repetition as well as randomly breaking ties within the iterative\n        stratification algorithm.\n    Examples\n    --------\n    >>> from iterstrat.ml_stratifiers import RepeatedMultilabelStratifiedKFold\n    >>> import numpy as np\n    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n    >>> rmskf = RepeatedMultilabelStratifiedKFold(n_splits=2, n_repeats=2,\n    ...     random_state=0)\n    >>> for train_index, test_index in rmskf.split(X, y):\n    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...     X_train, X_test = X[train_index], X[test_index]\n    ...     y_train, y_test = y[train_index], y[test_index]\n    ...\n    TRAIN: [0 3 4 6] TEST: [1 2 5 7]\n    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n    TRAIN: [0 1 4 5] TEST: [2 3 6 7]\n    TRAIN: [2 3 6 7] TEST: [0 1 4 5]\n    See also\n    --------\n    RepeatedStratifiedKFold: Repeats (Non-multilabel) Stratified K-Fold\n    n times.\n    \"\"\"\n    def __init__(self, n_splits=5, n_repeats=10, random_state=None):\n        super(RepeatedMultilabelStratifiedKFold, self).__init__(\n            MultilabelStratifiedKFold, n_repeats=n_repeats, random_state=random_state,\n            n_splits=n_splits)\n\n\nclass MultilabelStratifiedShuffleSplit(BaseShuffleSplit):\n    \"\"\"Multilabel Stratified ShuffleSplit cross-validator\n    Provides train/test indices to split data into train/test sets.\n    This cross-validation object is a merge of MultilabelStratifiedKFold and\n    ShuffleSplit, which returns stratified randomized folds for multilabel\n    data. The folds are made by preserving the percentage of each label.\n    Note: like the ShuffleSplit strategy, multilabel stratified random splits\n    do not guarantee that all folds will be different, although this is\n    still very likely for sizeable datasets.\n    Parameters\n    ----------\n    n_splits : int, default 10\n        Number of re-shuffling & splitting iterations.\n    test_size : float, int, None, optional\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of the dataset to include in the test split. If int, represents the\n        absolute number of test samples. If None, the value is set to the\n        complement of the train size. By default, the value is set to 0.1.\n        The default will change in version 0.21. It will remain 0.1 only\n        if ``train_size`` is unspecified, otherwise it will complement\n        the specified ``train_size``.\n    train_size : float, int, or None, default is None\n        If float, should be between 0.0 and 1.0 and represent the\n        proportion of the dataset to include in the train split. If\n        int, represents the absolute number of train samples. If None,\n        the value is automatically set to the complement of the test size.\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Unlike StratifiedShuffleSplit that only uses\n        random_state when ``shuffle`` == True, this multilabel implementation\n        always uses the random_state since the iterative stratification\n        algorithm breaks ties randomly.\n    Examples\n    --------\n    >>> from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n    >>> import numpy as np\n    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n    >>> msss = MultilabelStratifiedShuffleSplit(n_splits=3, test_size=0.5,\n    ...    random_state=0)\n    >>> msss.get_n_splits(X, y)\n    3\n    >>> print(mss)       # doctest: +ELLIPSIS\n    MultilabelStratifiedShuffleSplit(n_splits=3, random_state=0, test_size=0.5,\n                                     train_size=None)\n    >>> for train_index, test_index in msss.split(X, y):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n    TRAIN: [2 3 6 7] TEST: [0 1 4 5]\n    TRAIN: [1 2 5 6] TEST: [0 3 4 7]\n    Notes\n    -----\n    Train and test sizes may be slightly different from desired due to the\n    preference of stratification over perfectly sized folds.\n    \"\"\"\n\n    def __init__(self, n_splits=10, test_size=\"default\", train_size=None,\n                 random_state=None):\n        super(MultilabelStratifiedShuffleSplit, self).__init__(\n            n_splits=n_splits, test_size=test_size, train_size=train_size, random_state=random_state)\n\n    def _iter_indices(self, X, y, groups=None):\n        n_samples = _num_samples(X)\n        y = check_array(y, ensure_2d=False, dtype=None)\n        y = np.asarray(y, dtype=bool)\n        type_of_target_y = type_of_target(y)\n\n        if type_of_target_y != 'multilabel-indicator':\n            raise ValueError(\n                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(\n                    type_of_target_y))\n\n        n_train, n_test = _validate_shuffle_split(n_samples, self.test_size,\n                                                  self.train_size)\n\n        n_samples = y.shape[0]\n        rng = check_random_state(self.random_state)\n        y_orig = y.copy()\n\n        r = np.array([n_train, n_test]) / (n_train + n_test)\n\n        for _ in range(self.n_splits):\n            indices = np.arange(n_samples)\n            rng.shuffle(indices)\n            y = y_orig[indices]\n\n            test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n\n            test_idx = test_folds[np.argsort(indices)] == 1\n            test = np.where(test_idx)[0]\n            train = np.where(~test_idx)[0]\n\n            yield train, test\n\n    def split(self, X, y, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n            Note that providing ``y`` is sufficient to generate the splits and\n            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n            ``X`` instead of actual training data.\n        y : array-like, shape (n_samples, n_labels)\n            The target variable for supervised learning problems.\n            Multilabel stratification is done based on the y labels.\n        groups : object\n            Always ignored, exists for compatibility.\n        Returns\n        -------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        \"\"\"\n        y = check_array(y, ensure_2d=False, dtype=None)\n        return super(MultilabelStratifiedShuffleSplit, self).split(X, y, groups)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nTEST_FEATURES_PATH = \"/kaggle/input/lish-moa/test_features.csv\"\nTRAIN_FEATURES_PATH = \"/kaggle/input/lish-moa/train_features.csv\"\nTRAIN_TARGETS_PATH = \"/kaggle/input/lish-moa/train_targets_scored.csv\"\nTRAIN_TARGETS_NONSCORED_PATH = \"/kaggle/input/lish-moa/train_targets_nonscored.csv\"\nSAMPLE_SUB_PATH = \"/kaggle/input/lish-moa/sample_submission.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features_df = pd.read_csv(TEST_FEATURES_PATH).sort_values(by='sig_id')\ntrain_features_df = pd.read_csv(TRAIN_FEATURES_PATH).sort_values(by='sig_id')\ntrain_targets_df = pd.read_csv(TRAIN_TARGETS_PATH).sort_values(by='sig_id')\ntrain_targets_nonscored_df = pd.read_csv(TRAIN_TARGETS_NONSCORED_PATH)\nsample_sub_df = pd.read_csv(SAMPLE_SUB_PATH).sort_values(by='sig_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_df['5-alpha_reductase_inhibitor'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"value_counts_arr = np.sort([train_targets_df[col].value_counts()[1] for col in train_targets_df.columns])\n\nprint(value_counts_arr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matplotlib.rcParams['figure.figsize'] = [10, 5]\n\nplt.hist(value_counts_arr, 50, facecolor='g', alpha=0.75)\nplt.xlabel('Number of 1\\'s')\nplt.ylabel('Number of classes')\nplt.title('Value Counts of 1\\'s in classes')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_df.select_dtypes(include=[object])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_df.cp_dose.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Dealing w/ categorical features\n\n\n# Encode training categorical features\n\n# train_features_df = pd.get_dummies(train_features_df, columns=['cp_dose', 'cp_time'])\n# test_features_df = pd.get_dummies(test_features_df, columns=['cp_dose', 'cp_time'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# p = 23814\n# train_targets_df.loc[p] = train_targets_df[train_targets_df.sig_id =='id_53b38e3be'].values[0]\n# train_targets_df.loc[p+1] = train_targets_df[train_targets_df.sig_id =='id_dc2606109'].values[0]\n# train_targets_df.loc[p, 'sig_id']  = 'id_53b10e3be'\n# train_targets_df.loc[p+1, 'sig_id']  = 'id_53b50e2be'\n\n# train_features_df.loc[p] = train_features_df[train_features_df.sig_id =='id_53b38e3be'].values[0]\n# train_features_df.loc[p+1] = train_features_df[train_features_df.sig_id =='id_dc2606109'].values[0]\n# train_features_df.loc[p, 'sig_id']  = 'id_53b10e3be'\n# train_features_df.loc[p+1, 'sig_id']  = 'id_53b50e2be'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_features_df.drop(columns=['sig_id'])\nX_test = test_features_df.drop(columns=['sig_id'])\ny = train_targets_df.drop(columns=['sig_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rank gauss\nfrom sklearn.preprocessing import QuantileTransformer\n\n\ntransformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\ntransformer.fit(X[genes+cells])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.loc[:,genes+cells] = transformer.transform(X[genes+cells])\nX_test.loc[:,genes+cells] = transformer.transform(X_test[genes+cells])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pca_number(X_genes, X_test_genes, X_cells, X_test_cells, X, X_test, n_gene=200, n_cells=10):\n    pca = PCA(n_components=n_gene)\n    data = pca.fit_transform(pd.concat([X_genes,X_test_genes]))\n    \n    X_temp = pd.DataFrame(data=data, columns = ['pca_gene'+str(i) for i in range(n_gene)])\n    X_genes = X_temp[:len(X_genes)]\n    X_test_genes = X_temp[len(X_genes):].reset_index(drop=True)\n    \n    \n    pca = PCA(n_components=n_cells)\n    data = pca.fit_transform(pd.concat([X_cells,X_test_cells]))\n    \n    X_temp = pd.DataFrame(data=data, columns = ['pca_cells'+str(i) for i in range(n_cells)])\n    \n    X_cells = X_temp[:len(X_cells)]\n    X_test_cells = X_temp[len(X_cells.index):].reset_index(drop=True)\n    New_x = pd.concat([X, X_genes, X_cells], axis=1)\n    New_xtest = pd.concat([X_test, X_test_genes, X_test_cells], axis=1)\n    \n    return New_x, New_xtest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with PCA features\nfeatures = X.columns\ngenes = [col for col in features if col.startswith('g-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cells = [col for col in features if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nn_genes=35\nn_cells=5\n# def create_cluster(X, X_test, genes,cells, n_genes=40, n_cells=10):\nkmeans = KMeans(n_genes)\ndata = kmeans.fit(X[genes].append(X_test[genes]))\nX['kmeans_g'] = data.labels_[:X.shape[0]]\nX_test['kmeans_g'] = data.labels_[X.shape[0]:]\n\n\nkmeans = KMeans(n_cells)\ndata = kmeans.fit(X[cells].append(X_test[cells]))\nX['kmeans_c'] = data.labels_[:X.shape[0]]\nX_test['kmeans_c'] = data.labels_[X.shape[0]:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'kmeans_g', 'kmeans_c','g_sum', 'g_mean', 'g_std', 'g_kurt','g_skew', 'c_sum','c_mean', 'c_std', 'c_kurt', 'c_skew', 'gc_sum','gc_mean','gc_std','gc_kurt','gc_skew','g_sum_cp_dose','g_mean_cp_dose','g_std_cp_dose','c_sum_cp_dose', 'c_mean_cp_dose', 'c_std_cp_dose'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in X, X_test:\n    df['g_sum'] = df[genes].sum(axis = 1)\n    df['g_mean'] = df[genes].mean(axis = 1)\n    df['g_std'] = df[genes].std(axis = 1)\n    df['g_kurt'] = df[genes].kurtosis(axis = 1)\n    df['g_skew'] = df[genes].skew(axis = 1)\n    df['c_sum'] = df[cells].sum(axis = 1)\n    df['c_mean'] = df[cells].mean(axis = 1)\n    df['c_std'] = df[cells].std(axis = 1)\n    df['c_kurt'] = df[cells].kurtosis(axis = 1)\n    df['c_skew'] = df[cells].skew(axis = 1)\n    df['gc_sum'] = df[genes + cells].sum(axis = 1)\n    df['gc_mean'] = df[genes + cells].mean(axis = 1)\n    df['gc_std'] = df[genes + cells].std(axis = 1)\n    df['gc_kurt'] = df[genes + cells].kurtosis(axis = 1)\n    df['gc_skew'] = df[genes + cells].skew(axis = 1)\n    df['g_sum_cp_dose'] = df['cp_dose'].map(dict(zip(['D1','D2'],df[genes+['cp_dose']].groupby(['cp_dose']).sum().values.sum(axis=1))))\n    df['g_mean_cp_dose'] = df['cp_dose'].map(dict(zip(['D1','D2'],df[genes+['cp_dose']].groupby(['cp_dose']).mean().values.mean(axis=1))))\n    df['g_std_cp_dose'] = df['cp_dose'].map(dict(zip(['D1','D2'],df[genes+['cp_dose']].groupby(['cp_dose']).std().values.std(axis=1))))\n    \n    df['c_sum_cp_dose'] = df['cp_dose'].map(dict(zip(['D1','D2'],df[cells+['cp_dose']].groupby(['cp_dose']).sum().values.sum(axis=1))))\n    df['c_mean_cp_dose'] = df['cp_dose'].map(dict(zip(['D1','D2'],df[cells+['cp_dose']].groupby(['cp_dose']).mean().values.mean(axis=1))))\n    df['c_std_cp_dose'] = df['cp_dose'].map(dict(zip(['D1','D2'],df[cells+['cp_dose']].groupby(['cp_dose']).std().values.std(axis=1))))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.get_dummies(X, columns=['cp_dose', 'cp_time'])\nX_test = pd.get_dummies(X_test, columns=['cp_dose', 'cp_time'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_genes = X[genes].copy()\nX_test_genes = X_test[genes].copy()\n#cells\nX_cells = X[cells].copy()\nX_test_cells = X_test[cells].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport torch\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, X_test = pca_number(X_genes, X_test_genes, X_cells, X_test_cells, X, X_test, n_gene=650, n_cells=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"control_mask = test_features_df['cp_type']=='ctl_vehicle'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y = y[X['cp_type']!='ctl_vehicle'].reset_index(drop=True).to_numpy()\n# X = X[X['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n# X_test = X_test[X_test['cp_type']!='ctl_vehicle'].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.drop(['cp_type'], axis=1)\nX_test = X_test.drop(['cp_type'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n\nvar_thresh = VarianceThreshold(0.8)\n\ndata = X.append(X_test)\nvar_thresh.fit(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_columns=list(var_thresh.get_support())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cats = ['cp_dose_D1','cp_dose_D2','cp_time_24','cp_time_48','cp_time_72']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extras = ['kmeans_g', 'kmeans_c','g_sum', 'g_mean', 'g_std', 'g_kurt','g_skew', 'c_sum','c_mean', 'c_std', 'c_kurt', 'c_skew', 'gc_sum','gc_mean','gc_std','gc_kurt','gc_skew','g_sum_cp_dose','g_mean_cp_dose','g_std_cp_dose','c_sum_cp_dose', 'c_mean_cp_dose', 'c_std_cp_dose']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(selected_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(selected_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_columns[872:900] = [True]*28","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = X.loc[:, selected_columns]\nt1 = X_test.loc[:, selected_columns]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.loc[:, selected_columns]\nX_test = X_test.loc[:, selected_columns]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers &> /dev/null\n\n!git clone https://github.com/fastai/fastai &> /dev/null\n!pip install -e \"fastai[dev]\" &> /dev/null\n!pip install fastai2 &> /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.tabular.all import *\nfrom fastai.tabular.data import *\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nclass Model(nn.Module):      # <-- Update\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.25)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.25)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoaDataset(torch.utils.data.Dataset):\n    def __init__(self, X, y):\n        self.X = X.values\n        self.y = y.astype(float)\n\n    def __getitem__(self, idx):\n        # token_type_ids = inputs[\"token_type_ids\"]\n        return (torch.tensor(self.X[idx], dtype=torch.float), torch.tensor(self.y[idx], dtype=torch.float))\n        \n\n    def __len__(self):\n        return len(self.y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoaTestDataset(torch.utils.data.Dataset):\n    def __init__(self, X):\n        self.X = X.values\n        self.y = np.zeros(len(X))\n\n    def __getitem__(self, idx):\n        # token_type_ids = inputs[\"token_type_ids\"]\n        \n        return (torch.tensor(self.X[idx], dtype=torch.float), torch.tensor(self.y[idx], dtype=torch.float))\n    \n        \n\n    def __len__(self):\n        return len(self.X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# test_dls = DataLoaders.from_dsets(test_dls, bs=128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@delegates(torch.optim.AdamW.__init__)\ndef pytorch_adamw(param_groups, **kwargs):\n    return OptimWrapper(torch.optim.AdamW([{'params': ps, **kwargs} for ps in param_groups]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CustomAdamW =partial(pytorch_adamw, lr=1e-3, weight_decay=1e-5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = y.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = np.zeros((test_features_df.shape[0], y.shape[1]))\nlosses = []\nSEED = [0]\nfrom fastai.data.core import DataLoaders\nmodel = Model(\n        num_features=X.shape[1],\n        num_targets=y.shape[1],\n        hidden_size=1500,\n    )\nskf = MultilabelStratifiedKFold(n_splits = 5)\n\nfor i in SEED:\n    seed_everything(i)\n    for fold, (train_index, valid_index) in enumerate(skf.split(X, y)):\n        import gc\n        gc.collect()\n        torch.cuda.empty_cache()\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        valid_dataset = MoaDataset(X_valid, y_valid)\n        train_dataset = MoaDataset(X_train, y_train)\n        dls = DataLoaders.from_dsets(train_dataset, valid_dataset, bs=128)\n        \n        dls = dls.cuda()\n        model = Model(\n        num_features=X.shape[1],\n        num_targets=y.shape[1],\n        hidden_size=2000,\n        )\n        cbs=SaveModelCallback()\n        model.to('cuda')\n        loss_func = SmoothBCEwLogits(smoothing=0.001)\n#         loss_func = nn.BCEWithLogitsLoss()\n        \n        learner = Learner(dls, model, loss_func = loss_func, opt_func=CustomAdamW)\n        learner.fit_one_cycle(100, cbs=[EarlyStoppingCallback(monitor='valid_loss', patience=5),cbs],pct_start=0.1, div_factor=1000, \n                                              max_lr=1e-3)\n        log_loss = np.array(learner.recorder.values)[:,1].min()\n        learner.load('./model')\n#         log_loss = np.array(learner.recorder.values)[:,1].min() # validation loss\n        learner.predict(torch.tensor(X_test.values, dtype=torch.float))\n        preds = learner.pred\n        test_preds += preds.sigmoid().detach().cpu().numpy()\n#         model = create_model(X.shape[1])\n#         reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.15, patience=3, verbose=1, epsilon=1e-4, mode='min')\n#         early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, mode= 'min')\n#         X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n#         y_train, y_valid = y[train_index], y[valid_index]\n#         history = model.fit(X_train.values,\n#                       y_train,\n#                       validation_data=(X_valid.values, y_valid),\n#                       epochs=100, batch_size=128,\n#                       callbacks=[reduce_lr_loss, early_stop], verbose=2\n#                      )\n\n#         test_predict = model.predict(X_test.values)\n#         test_preds+=test_predict\n# #         val_preds = model.predict(X_valid.values)\n# #         loss_one = log_loss(np.ravel(y_valid), np.ravel(val_preds))\n        losses.append(log_loss)\n        \n\nprint(np.array(losses).sum()/7)\ntest_preds[control_mask] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub_df.iloc[:,1:] = test_preds/5\n\nsample_sub_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}