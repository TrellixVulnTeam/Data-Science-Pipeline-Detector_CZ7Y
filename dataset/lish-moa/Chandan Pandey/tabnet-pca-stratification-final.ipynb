{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/pytorch-tabnet/pytorch_tabnet-2.0.1-py3-none-any.whl\n!pip install ../input/iter-strart/iterative_stratification-0.1.6-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport torch\nfrom pytorch_tabnet.tab_model import TabNetClassifier,TabNetRegressor\nfrom sklearn.metrics import log_loss\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nimport random\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nfrom sklearn.metrics import log_loss ,roc_auc_score\nfrom pickle import load,dump\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n\n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    \nseed_everything(33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\nfrom pytorch_tabnet.metrics import Metric\nfrom sklearn.metrics import roc_auc_score, log_loss\n\nclass LogitsLogLoss(Metric):\n    \n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):       \n        logits = 1 / (1 + np.exp(-y_pred))\n        aux = (1-y_true)*np.log(1-logits+1e-15) + y_true*np.log(logits+1e-15)\n        return np.mean(-aux)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load files\ndef load_data(path):\n    \n    train_feature = pd.read_csv(f'{path}train_features.csv')\n    train_target = pd.read_csv(f'{path}train_targets_scored.csv')\n    test_feature = pd.read_csv(f'{path}test_features.csv')\n    sample_output = pd.read_csv(f'{path}sample_submission.csv')\n    features = train_feature.columns[1:]\n    #scored = pd.read_csv('input/lish-moa/train_targets_scored.csv')\n    drug = pd.read_csv(f'{path}train_drug.csv')\n    targets = train_target.columns[1:]\n    train_target = train_target.merge(drug, on='sig_id', how='left')\n    \n    return train_feature,train_target,test_feature,sample_output,targets,features                         ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data pre-processing\ndef LabelEncoding(train_feature,test_feature):\n    \n    le = LabelEncoder()\n    le1 = LabelEncoder()\n    le2 = LabelEncoder()\n    train_feature['cp_type'] = pd.DataFrame(data = le.fit_transform(train_feature['cp_type']))\n    train_feature['cp_time'] = pd.DataFrame(data = le1.fit_transform(train_feature['cp_time']))\n    train_feature['cp_dose'] = pd.DataFrame(data = le2.fit_transform(train_feature['cp_dose']))\n    test_feature['cp_type'] = pd.DataFrame(data = le.transform(test_feature['cp_type']))\n    test_feature['cp_time'] = pd.DataFrame(data = le1.transform(test_feature['cp_time']))\n    test_feature['cp_dose'] = pd.DataFrame(data = le2.transform(test_feature['cp_dose']))\n    \n    return train_feature,test_feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def PreProcess(train_feature,test_feature):\n    \n    GENES = [col for col in train_feature.columns if col.startswith('g-')]\n    CELLS = [col for col in train_feature.columns if col.startswith('c-')]\n\n    for col in (GENES + CELLS):\n\n        transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n        vec_len = len(train_feature[col].values)\n        vec_len_test = len(test_feature[col].values)\n        raw_vec = train_feature[col].values.reshape(vec_len, 1)\n        transformer.fit(raw_vec)\n\n        train_feature[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n        test_feature[col] = transformer.transform(test_feature[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n\n\n    n_comp = 600  #<--Update\n    pca_g = PCA(n_components=n_comp, random_state=42)\n    data = pd.concat([pd.DataFrame(train_feature[GENES]), pd.DataFrame(test_feature[GENES])])\n    gpca= (pca_g.fit(data[GENES]))\n    train2= (gpca.transform(train_feature[GENES]))\n    test2 = (gpca.transform(test_feature[GENES]))\n\n    train_gpca = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n    test_gpca = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n    # drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n    train_feature = pd.concat((train_feature, train_gpca), axis=1)\n    test_feature = pd.concat((test_feature, test_gpca), axis=1)\n\n    dump(gpca, open('gpca.pkl', 'wb'))\n\n    #CELLS\n    n_comp = 50  #<--Update\n\n    pca_c = PCA(n_components=n_comp, random_state=42)\n    data = pd.concat([pd.DataFrame(train_feature[CELLS]), pd.DataFrame(test_feature[CELLS])])\n    cpca= (pca_c.fit(data[CELLS]))\n    train2= (cpca.transform(train_feature[CELLS]))\n    test2 = (cpca.transform(test_feature[CELLS]))\n\n    train_cpca = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n    test_cpca = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n    # drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n    train_feature = pd.concat((train_feature, train_cpca), axis=1)\n    test_feature = pd.concat((test_feature, test_cpca), axis=1)\n\n    dump(cpca, open('cpca.pkl', 'wb'))\n\n\n    c_n = [f for f in list(train_feature.columns) if f not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]\n    mask = (train_feature[c_n].var() >= 0.85).values\n    tmp = train_feature[c_n].loc[:, mask]\n    train_feature = pd.concat([train_feature[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)\n    tmp = test_feature[c_n].loc[:, mask]\n    test_feature = pd.concat([test_feature[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)\n\n    return train_feature,test_feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def VarThreshold(train_feature,test_feature):\n    \n    c_n = [f for f in list(train_feature.columns) if f not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]\n    mask = (train_feature[c_n].var() >= 0.85).values\n    tmp = train_feature[c_n].loc[:, mask]\n    train_feature = pd.concat([train_feature[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)\n    tmp = test_feature[c_n].loc[:, mask]\n    test_feature = pd.concat([test_feature[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)\n    \n    return train_feature,test_feature    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Stratify_Drugs(scored, threshold, FOLDS, SEED):\n    # LOCATE DRUGS\n    vc = scored.drug_id.value_counts()\n    vc1 = vc.loc[vc<=threshold].index.sort_values()\n    vc2 = vc.loc[vc>threshold].index.sort_values()\n\n    # STRATIFY DRUGS 19X OR LESS\n    dct1 = {}; dct2 = {}\n    skf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, \n              random_state=SEED)\n    tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n        dd = {k:fold for k in tmp.index[idxV].values}\n        dct1.update(dd)\n\n    # STRATIFY DRUGS MORE THAN 19X\n    skf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, \n              random_state=SEED)\n    tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\n    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n        dd = {k:fold for k in tmp.sig_id[idxV].values}\n        dct2.update(dd)\n\n    # ASSIGN FOLDS\n    scored['fold'] = scored.drug_id.map(dct1)\n    scored.loc[scored.fold.isna(),'fold'] = scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\n    scored.fold = scored.fold.astype('int8')\n    \n    return scored","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Train_Drug(MAX_EPOCH, FOLDS, train_feature,scored,test_feature):\n   \n    unused_feat = ['sig_id']\n    features = [ col for col in train_feature.columns if col not in unused_feat] \n    train_feature = train_feature[features]\n    #train_target = train_target.drop(\"sig_id\", axis=1)\n    test_feature = test_feature[features]\n    X_test = test_feature.values   \n    \n    tabnet_params = dict(n_d=24, n_a=24, n_steps=1, gamma=1.3,\n                     lambda_sparse=0, optimizer_fn=torch.optim.Adam,\n                     optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n                     mask_type='entmax',\n                     scheduler_params=dict(mode=\"min\",\n                                           patience=5,\n                                           min_lr=1e-5,\n                                           factor=0.9,),\n                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                     verbose=10,\n                     )\n    test_preds = []\n    for i in range(FOLDS):    \n        X_train = train_feature.loc[scored.fold != i].values\n        y_train = scored.loc[scored.fold != i][targets].values\n        X_val = train_feature.loc[scored.fold == i].values\n        y_val = scored.loc[scored.fold == i][targets].values\n        model = TabNetRegressor(**tabnet_params)\n        model.fit(X_train=X_train,\n                  y_train=y_train,\n                  eval_set=[(X_val, y_val)],\n                  eval_name = [\"val\"],\n                  eval_metric = [\"logits_ll\"],\n                  max_epochs=MAX_EPOCH,\n                  patience=50, batch_size=1024, virtual_batch_size=128,\n                  num_workers=1, drop_last=False,\n                  # use binary cross entropy as this is not a regression problem\n                  loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n        y_test = model.predict(X_test)\n        test_preds.append(1 / (1 + np.exp(-y_test)))\n\n    test_preds_total = np.stack(test_preds)\n    return test_preds_total\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Train_Simple(MAX_EPOCH,FOLDS,SEED,train_feature,train_target,test_feature):\n    \n    unused_feat = ['sig_id']\n    features = [ col for col in train_feature.columns if col not in unused_feat] \n    train_feature = train_feature[features]\n    train_target = train_target.drop([\"sig_id\",'drug_id','fold'], axis=1)\n    test_feature = test_feature[features]\n    X_test = test_feature.values   \n    \n    tabnet_params = dict(n_d=24, n_a=24, n_steps=1, gamma=1.3,\n                     lambda_sparse=0, optimizer_fn=torch.optim.Adam,\n                     optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n                     mask_type='entmax',\n                     scheduler_params=dict(mode=\"min\",\n                                           patience=5,\n                                           min_lr=1e-5,\n                                           factor=0.9,),\n                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                     verbose=10,\n                     )\n    \n    test_preds = []\n    mskf = MultilabelStratifiedKFold(n_splits=FOLDS, random_state=SEED, shuffle=True)\n\n    for fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train_feature, train_target)):\n        print(\"FOLDS : \", fold_nb)\n\n        ## model\n        X_train, y_train = train_feature.values[train_idx, :], train_target.values[train_idx, :]\n        X_val, y_val = train_feature.values[val_idx, :], train_target.values[val_idx, :]\n        model = TabNetRegressor()#**tabnet_params)\n\n        model.fit(X_train=X_train,\n                  y_train=y_train,\n                  eval_set=[(X_val, y_val)],\n                  eval_name = [\"val\"],\n                  eval_metric = [\"logits_ll\"],\n                  max_epochs=MAX_EPOCH,\n                  patience=20, batch_size=1024, virtual_batch_size=128,\n                  num_workers=1, drop_last=False,\n                  # use binary cross entropy as this is not a regression problem\n                  loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n\n        y_test = model.predict(X_test)\n        test_preds.append(1 / (1 + np.exp(-y_test)))\n\n    test_preds_total = np.stack(test_preds)\n    return test_preds_total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = 19 #Drug sample count threshold\nFOLDS = 5 #\nSEED = 33\nepochs = 500\n#data loading\ntrain_feature,train_target,test_feature,sample_output,targets,features = load_data('/kaggle/input/lish-moa/')\n#data Label Encoding\ntrain_feature,test_feature = LabelEncoding(train_feature,test_feature)\n#data pre-processing\ntrain_feature,test_feature = PreProcess(train_feature,test_feature)\n#Variance Threshold\ntrain_feature,test_feature = VarThreshold(train_feature,test_feature)\n#Stratifications\nscored = Stratify_Drugs(train_target, threshold, FOLDS, SEED)\n#Training with drug stratification\npred = Train_Drug(epochs, FOLDS, train_feature,scored,test_feature)\n#Training with simple stratification\n#pred1 = Train_Simple(epochs,FOLDS,SEED,train_feature,train_target,test_feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_output1 = sample_output.copy()\n#sample_output2 = sample_output.copy()\nall_feat = [col for col in sample_output.columns if col not in [\"sig_id\"]]\nsample_output1[all_feat] = pred.mean(axis=0)\n#sample_output2[all_feat] = pred1.mean(axis=0)\n#sample_output = pd.concat([sample_output1, sample_output2]).groupby(level=0).mean()\n# set control to 0\nsample_output1.loc[test_feature['cp_type']==0, sample_output.columns[1:]] = 0\nsample_output1.to_csv('submission.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}