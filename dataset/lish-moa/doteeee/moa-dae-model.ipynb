{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport os\nimport torch\nimport random\nimport torch.nn as nn\n\nimport torch.optim as optim\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nfrom collections import OrderedDict\nfrom  torch.nn.utils import clip_grad_norm_\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything():\n    random.seed(0)\n    torch.manual_seed(0)\n    np.random.seed(0)\n    torch.cuda.manual_seed(0)\n    torch.cuda.manual_seed_all(0)\n    \nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_df=pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ntest_features_df=pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\n\ngcolumns=[colname for colname in train_features_df.columns if colname.startswith('g-')]\nccolumns=[colname for colname in train_features_df.columns if colname.startswith('c-')]\n\nfeature_columns=gcolumns+ccolumns\nprint('Number Of gene expression features:', len(gcolumns))\nprint('Number Of Cell Viable Features:', len(ccolumns))\nprint('Total Number Of features:', len(feature_columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_df=train_features_df[train_features_df.cp_type!='ctl_vehicle'].copy()\ntest_features_df =test_features_df[test_features_df.cp_type!='ctl_vehicle'].copy()\n\nfeatures_df=pd.concat([train_features_df, test_features_df])\nfeatures_df=features_df[feature_columns].copy()\nprint(features_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DAEDataset(torch.utils.data.Dataset):\n    def __init__(self, df, alpha=None, phase='train'):\n        self.phase=phase\n        self.df=df.copy().sample(frac=1.0, random_state=42)\n        self.alpha=alpha\n    def __getitem__(self, idx):\n        x=torch.tensor(self.df.iloc[idx], dtype=torch.float32)\n        x_in=x.clone()\n        return (x_in, x)\n        \n    def __len__(self):\n        return self.df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_val, _, _= train_test_split(features_df, \n                                       features_df.iloc[:,0], \n                                       test_size=0.2, \n                                       shuffle=True,\n                                       random_state=42)\n\ntrain_df=features_df.iloc[X_train.index].copy()\n\n\n\nX_train,X_test, _, _= train_test_split(train_df, \n                                       train_df.iloc[:, 0],\n                                       test_size=0.2,\n                                       shuffle=True, \n                                       random_state=42)\n\n\ntrain_df=features_df.iloc[X_train.index].copy()\nval_df  =features_df.iloc[X_val.index].copy()\ntest_df =features_df.iloc[X_test.index].copy()\n\nprint('Train :', train_df.shape)\nprint('Val Set:', val_df.shape)\nprint('Test Set:', test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder_params={\n    'epochs': 200,\n    'batch_size': 512,\n    'lr': 0.001,\n    'weight_decay': 1e-5,\n    'input_size': features_df.shape[1],\n    'output_size': features_df.shape[1],\n    'drop_out': 0.5,\n    'code_size': 1000,\n    'alpha': 0.5\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MOAAutoencoder(nn.Module):\n    def __init__(self):\n        super(MOAAutoencoder, self).__init__()\n        self.autoencoder=nn.Sequential(OrderedDict([\n            ('encoder_layer1', nn.Linear(autoencoder_params['input_size'], 2000)),\n            ('encoder_layer1_bn', nn.BatchNorm1d(2000)),\n            ('encoder_layer1_dropout', nn.Dropout(p=autoencoder_params['drop_out'])),\n            ('encoder_layer1_activation', nn.PReLU()),\n            \n            #('encoder_layer2', nn.Linear(2000,1500)),\n            #('encoder_layer2_bn', nn.BatchNorm1d(1500)),\n            #('encoder_layer2_dropout', nn.Dropout(p=autoencoder_params['drop_out'])),\n            #('encoder_layer2_activation', nn.PReLU()),            \n            \n            \n            ('code_layer', nn.Linear(2000, 1000)),\n            ('code_layer_bn', nn.BatchNorm1d(1000)),\n            ('code_layer_dropout', nn.Dropout(p=autoencoder_params['drop_out'])),\n            ('code_layer_activation', nn.PReLU()),\n            \n            #('decoder_layer1', nn.Linear(1000, 1500)),\n            #('decoder_layer1_bn', nn.BatchNorm1d(1500)),\n            #('decoder_layer1_dropout', nn.Dropout(p=autoencoder_params['drop_out'])),\n            #('decoder_layer1_activation', nn.PReLU()),\n            \n            ('decoder_layer2', nn.Linear(1000, 2000)),\n            ('decoder_layer2_bn', nn.BatchNorm1d(2000)),\n            ('decoder_layer2_dropout', nn.Dropout(p=autoencoder_params['drop_out'])),\n            ('decoder_layer2_activation', nn.PReLU()),\n            \n            ('output_layer', nn.Linear(2000, autoencoder_params['output_size']))\n        ]))\n    def __call__(self, x):\n        x=self.autoencoder(x)\n        return x\n\n\ndef train_one_epoch(train_dataloader, criterion, optimizer, scheduler, model):\n    epoch_loss=0.0\n    model.train()\n    for batch_id, (X, y) in enumerate(train_dataloader):\n        for col_id in torch.randperm(X.shape[1])[:175]:\n            X[:, col_id]=X[torch.randperm(X.shape[0]).numpy(), col_id]\n        yout=model(X)\n        \n        loss_=criterion(yout, y)\n        optimizer.zero_grad()\n        \n        loss_.backward()\n        clip_grad_norm_(model.parameters(),1000)\n        optimizer.step()\n        scheduler.step()\n        \n        epoch_loss+=loss_.item()\n    epoch_loss/=len(train_dataloader)\n    return epoch_loss\n\ndef validate(val_dataloader, criterion, model):\n    val_loss=0.0\n    model.eval()\n    for X, y in val_dataloader:\n        with torch.no_grad():\n            yout=model(X)\n            val_loss+=criterion(yout, y)\n    val_loss/=len(val_dataloader)\n    return val_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model):\n    print('Training the Model')\n    \n    best_score=np.inf\n    best_epoch=-1\n    train_losses_=[]\n    val_losses_=[]\n    \n    train_dataset=DAEDataset(train_df, alpha=autoencoder_params['alpha'], phase='train')\n    train_dataloader=torch.utils.data.DataLoader(train_dataset,batch_size=autoencoder_params['batch_size'],shuffle=True,pin_memory=True,num_workers=4)\n    \n    criterion=nn.MSELoss()\n    optimizer=optim.Adam(model.parameters(), lr=0.002, weight_decay=autoencoder_params['weight_decay'])\n    scheduler=optim.lr_scheduler.OneCycleLR(optimizer, \n                                            max_lr=0.002,\n                                            div_factor=100,\n                                            epochs=autoencoder_params['epochs'],\n                                            steps_per_epoch=len(train_dataloader)\n                                           )\n    \n    for i in range(autoencoder_params['epochs']):\n        seed_everything()\n        train_dataset=DAEDataset(train_df, alpha=autoencoder_params['alpha'], phase='train')\n        val_dataset=DAEDataset(val_df, phase='validation')\n        \n        train_dataloader=torch.utils.data.DataLoader(train_dataset,batch_size=autoencoder_params['batch_size'],shuffle=True,pin_memory=True,num_workers=4)\n        val_dataloader=torch.utils.data.DataLoader(val_dataset,batch_size=autoencoder_params['batch_size'],shuffle=False,pin_memory=True,num_workers=4)\n        \n        epoch_loss=train_one_epoch(train_dataloader, criterion, optimizer, scheduler, model)\n        val_loss=validate(val_dataloader, criterion, model)\n        print('Epoch:{} | Epoch Train Loss:{} | Validation Loss: {}'.format(i+1, epoch_loss, val_loss) )\n        \n        train_losses_.append(epoch_loss)\n        val_losses_.append(val_loss)\n        if val_loss < best_score:\n            best_score=val_loss\n            best_epoch=i+1\n            torch.save(model.state_dict(), \"epoch{}.pth\".format(i+1))\n    print('Best SCore:{} | Best Epoch:{}'.format(best_score, best_epoch))\n    \n    plt.plot(train_losses_[5:])\n    plt.plot(val_losses_[5:])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder=MOAAutoencoder()\ntrain_model(autoencoder)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder=MOAAutoencoder()\ncriterion=nn.MSELoss()\ninit_lr=0.0001\nmax_lr=0.1\ngamma=1.05\n\n#optimizer=optim.SGD(autoencoder.parameters(), lr= init_lr,weight_decay=autoencoder_params['weight_decay'])\noptimizer=optim.Adam(autoencoder.parameters(), lr=init_lr)\nscheduler=optim.lr_scheduler.StepLR(optimizer, 1, gamma=gamma)\n\nlrs=[]\nbatch_loss=[]\n\nfor i in range(5):\n    train_dataset=DAEDataset(train_df, alpha=autoencoder_params['alpha'], phase='train')\n    train_dataloader=torch.utils.data.DataLoader(train_dataset,batch_size=autoencoder_params['batch_size'],shuffle=True,pin_memory=True,num_workers=4)\n    \n    val_dataset=DAEDataset(val_df, phase='validation')\n    val_dataloader=torch.utils.data.DataLoader(val_dataset,batch_size=autoencoder_params['batch_size'],shuffle=False,pin_memory=True,num_workers=4)\n    for batch_id, (X, y) in enumerate(train_dataloader):\n        optimizer.zero_grad()\n        ypred=autoencoder(X)\n        loss_=criterion(ypred, y)\n        loss_.backward()\n        \n        optimizer.step()\n        scheduler.step()\n        \n        if init_lr >= max_lr:\n            break\n        lrs.append(init_lr)\n        batch_loss.append(loss_.item())\n        init_lr *= gamma","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(lrs))\nprint(lrs[-1], batch_loss[-1])\n\nplt.plot(lrs, batch_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df\ntest_dataset=DAEDataset(test_df, phase='test')\ntest_dataloader=torch.utils.data.DataLoader(test_dataset,batch_size=autoencoder_params['batch_size'],shuffle=False,pin_memory=True,num_workers=4)\ncriterion=nn.MSELoss()\ntest_loss=validate(test_dataloader, criterion, autoencoder)\n\nprint(test_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}