{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nseed = 72\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\n\ntrain_features.shape, train_targets_scored.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering and Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop rows with cp_type = ctl_vehicle\n\nkp_cl = train_features[train_features['cp_type'] != 'ctl_vehicle'].index\ntrain_features = train_features.loc[kp_cl]\ntrain_targets_scored = train_targets_scored.loc[kp_cl]\n\ntrain_features.drop(['sig_id', 'cp_type'], axis=1, inplace=True)\ntrain_targets_scored.drop('sig_id', axis=1, inplace=True)\ntest_features.drop(['sig_id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#three processing functions\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold\n\ncols = train_features.columns\ncl_g = cols[cols.str.contains('g-')]\ncl_c = cols[cols.str.contains('c-')]\n\ndef tra_cp_dose(val):\n    if val == 'D1':\n        return 1.\n    elif val == 'D2':\n        return 0.\n\ndef tra_cp_time(val):\n    return val / 24.\n\ndef process1():\n    train = train_features.copy()\n    test = test_features.drop('cp_type', axis=1)\n    \n    train['cp_dose'] = train['cp_dose'].apply(tra_cp_dose)\n    train['cp_time'] = train['cp_time'].apply(tra_cp_time)\n    test['cp_dose'] = test['cp_dose'].apply(tra_cp_dose)\n    test['cp_time'] = test['cp_time'].apply(tra_cp_time)\n    \n    return train, test\n\ndef process2(n_c=60, n_g=480, seed=seed, scale=True):\n    pca_c = PCA(n_components=n_c, random_state=seed)\n    pca_g = PCA(n_components=n_g, random_state=seed)\n    SS = StandardScaler()\n    \n    train_pca_g = pd.DataFrame(pca_g.fit_transform(train_features[cl_g]),\n                           columns=[f'g-pca{i}' for i in range(n_g)], \n                           index=train_features.index)\n    train_pca_c = pd.DataFrame(pca_c.fit_transform(train_features[cl_c]), \n                           columns=[f'c-pca{i}' for i in range(n_c)], \n                           index=train_features.index)\n    \n    train = pd.concat([train_pca_g, train_pca_c], axis=1)\n    train['g-mean'] = np.mean(train_features[cl_g], axis=1)\n    train['g-std'] = np.std(train_features[cl_g], axis=1)\n    train['c-mean'] = np.mean(train_features[cl_c], axis=1)\n    train['c-std'] = np.std(train_features[cl_c], axis=1)\n    \n    test_pca_g = pd.DataFrame(pca_g.transform(test_features[cl_g]),\n                           columns=[f'g-pca{i}' for i in range(n_g)], \n                           index=test_features.index)\n    test_pca_c = pd.DataFrame(pca_c.fit_transform(test_features[cl_c]), \n                           columns=[f'c-pca{i}' for i in range(n_c)], \n                           index=test_features.index)\n    \n    test = pd.concat([test_pca_g, test_pca_c], axis=1)\n    test['g-mean'] = np.mean(test_features[cl_g], axis=1)\n    test['g-std'] = np.std(test_features[cl_g], axis=1)\n    test['c-mean'] = np.mean(test_features[cl_c], axis=1)\n    test['c-std'] = np.std(test_features[cl_c], axis=1)\n    if scale:\n        train = pd.DataFrame(SS.fit_transform(train), columns=train.columns, index=train.index)\n        test = pd.DataFrame(SS.transform(test), columns=test.columns, index=test.index)\n     \n    return train, test\n\ndef process3(n_c=80, n_g=660, tr=.8):\n    trn1, tst1 = process1()\n    trn2 ,tst2 = process2(n_c, n_g, scale=False)\n    \n    train = pd.concat([trn1, trn2], axis=1)\n    test = pd.concat([tst1, tst2], axis=1)\n    \n    QS = QuantileTransformer()\n    VT = VarianceThreshold(tr)\n    \n    train = pd.DataFrame(QS.fit_transform(VT.fit_transform(train)), index=train.index)\n    test = pd.DataFrame(QS.transform(VT.transform(test)), index=test.index)\n    \n    return train, test\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shuffling the data\nper = np.random.permutation(train_features.index)\ntarget = train_targets_scored.loc[per]\n\ntrain1, test1 = process1()\ntrain1 = train1.loc[per]\n\ntrain1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2, test2 = process2()\ntrain2 = train2.loc[per]\n\ntrain2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train3, test3 = process3()\ntrain3 = train3.loc[per]\n\ntrain3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import libraries to build model\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.metrics import binary_crossentropy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define logloss, early stopping, learning rate decay, maximum epoch\np_min = 0.001\np_max = 0.999\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return binary_crossentropy(y_true, y_pred)\n\nmax_epoch = 50\nlr_decay = ReduceLROnPlateau(monitor='val_logloss', patience=1, verbose=1, min_lr=1e-8)\nearly_stop = EarlyStopping(monitor='val_logloss', patience=5, verbose=1, restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build three models\ndef build_model1(inp_shape1, inp_shape2, tar_shape):\n    inp1 = Input((inp_shape1,), name='inp1')\n    inp2 = Input((inp_shape2,), name='inp2')\n    \n    X1 = BatchNormalization()(inp1)\n    X1 = Dropout(.2)(X1)\n    X1 = Dense(1024)(X1)\n    X1 = BatchNormalization()(X1)\n    X1 = LeakyReLU()(X1)\n    X1 = Dense(512)(X1)\n    X1 = BatchNormalization()(X1)\n    X1 = Activation('relu')(X1)\n    \n    X2 = Dropout(.3)(inp2)\n    X2 = Dense(1024)(X2)\n    X2 = BatchNormalization()(X2)\n    X2 = LeakyReLU()(X2)\n    X2 = Dense(512)(X2)\n    X2 = BatchNormalization()(X2)\n    X2 = LeakyReLU()(X2)\n    \n    inp3 = Concatenate(name='concat')([X1, X2])\n    \n    X3 = Dropout(.3)(inp3)\n    X3 = Dense(2048)(X3)\n    X3 = BatchNormalization()(X3)\n    X3 = Activation('relu')(X3)\n    X3 = Dense(512)(X3)\n    X3 = BatchNormalization()(X3)\n    X3 = Activation('relu')(X3)\n    X3 = Dropout(.4)(X3)\n    X3 = Dense(inp_shape1)(X3)\n    X3 = BatchNormalization()(X3)\n    X3 = Activation('relu')(X3)\n    \n    inp4 = Add(name='add')([X3, inp1])\n    \n    out = Dropout(.3)(inp4)\n    out = Dense(1024)(out)\n    out = BatchNormalization()(out)\n    out = LeakyReLU()(out)\n    out = Dropout(.4)(out)\n    out = Dense(512)(out)\n    out = BatchNormalization()(out)\n    out = LeakyReLU()(out)\n    out = Dropout(.5)(out)\n    out = Dense(tar_shape)(out)\n    out = Activation('sigmoid')(out)\n    \n    model = Model(inputs=[inp1, inp2], outputs=out, name='model1')\n    return model\n\n\ndef build_model2(inp_shape, tar_shape):\n    inp = Input((inp_shape,), name='inp')\n    \n    out = BatchNormalization()(inp)\n    out = Dropout(.3)(out)\n    out = Dense(1024)(out)\n    out = BatchNormalization()(out)\n    out = Activation('elu')(out)\n    out = Dropout(.3)(out)\n    out = Dense(512)(out)\n    out = BatchNormalization()(out)\n    out = Activation('elu')(out)\n    out = Dropout(.3)(out)\n    out = Dense(256)(out)\n    out = BatchNormalization()(out)\n    out = Activation('elu')(out)\n    out = Dropout(.3)(out)\n    out = Dense(tar_shape)(out)\n    out = BatchNormalization()(out)\n    out = Activation('sigmoid')(out)\n    \n    model = Model(inputs=inp, outputs=out, name='model2')\n    return model\n\ndef build_model3(inp_shape, tar_shape):\n    inp = Input((inp_shape,), name='inp')\n    \n    out = BatchNormalization()(inp)\n    out = Dropout(.3)(out)\n    \n    out = Dense(2048)(out)\n    out = BatchNormalization()(out)\n    out = Activation('elu')(out)\n    \n    out = Dropout(.5)(out)\n    \n    out = Dense(256)(out)\n    out = BatchNormalization()(out)\n    out = Activation('elu')(out)\n    \n    out = Dropout(.3)(out)\n    \n    out = Dense(1024)(out)\n    out = BatchNormalization()(out)\n    out = Activation('elu')(out)\n    \n    out = Dropout(.5)(out)\n    \n    out = Dense(tar_shape)(out)\n    out = BatchNormalization()(out)\n    out = Activation('sigmoid')(out)\n    \n    model = Model(inputs=inp, outputs=out, name='model3')\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = build_model1(train1.shape[1], train2.shape[1], target.shape[1])\nmodel1.compile(optimizer=Adam(3 * 1e-3), loss='binary_crossentropy', metrics=[logloss])\nmodel1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"his1 = model1.fit([train1, train2], target, batch_size=64, epochs=max_epoch, validation_split=.2,\n                  shuffle=False, callbacks=[lr_decay, early_stop], verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = build_model2(train3.shape[1], target.shape[1])\nmodel2.compile(optimizer=Adam(1e-1), loss='binary_crossentropy', metrics=[logloss])\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"his2 = model2.fit(train3, target, batch_size=64, epochs=max_epoch, validation_split=.2,\n                  shuffle=False, callbacks=[lr_decay, early_stop], verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3 = build_model3(train3.shape[1], target.shape[1])\nmodel3.compile(optimizer=Adam(.1), loss='binary_crossentropy', metrics=[logloss])\nmodel3.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"his3 = model3.fit(train3, target, batch_size=128, epochs=max_epoch, validation_split=.2,\n                  shuffle=False, callbacks=[lr_decay, early_stop], verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred1 = model1.predict([test1, test2])\npred2 = model2.predict(test3)\npred3 = model3.predict(test3)\n\npred = (pred1 + pred2 + pred3) / 3.0\npred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\nout.iloc[:,1:] = np.clip(pred, p_min, p_max)\n\nout.iloc[test_features['cp_type'] == 'ctl_vehicle', 1:] = 0\nout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}