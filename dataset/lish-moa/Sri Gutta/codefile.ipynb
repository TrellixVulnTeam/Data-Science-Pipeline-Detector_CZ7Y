{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# !python3 -m pip install iterative-stratification\n# !python3 -m pip install tensorflow\n# !python3 -m pip install keras\n# !python3 -m pip install seaborn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nimport tensorflow as tf\nimport keras\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras import Sequential,Model,backend\nfrom tensorflow.keras import layers,regularizers\nfrom tensorflow.keras import callbacks,optimizers,metrics,losses\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train, test, targets and submission file:\ntrain_features = pd.read_csv(f'../input/lish-moa/train_features.csv')\ntrain_target = pd.read_csv(f'../input/lish-moa/train_targets_scored.csv')\ntest_features = pd.read_csv(f'../input/lish-moa/test_features.csv')\nsample_sub = pd.read_csv(f'../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Statistical analysis of data ---------------------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training set of features sample:\")\ntrain_features.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training set of target sample:\")\ntrain_target.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test set of features sample:\")\ntrain_features.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.groupby( ['cp_dose','cp_type','cp_time'] ).agg( ['mean','std'] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Visualization -------------------------------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pie chart showing distribtions of CPtypes of Training data.\ncolors = [\"green\", \"orange\"]\n\nplt.pie(train_features[\"cp_type\"].value_counts(),labels=[\"trt_cp\",\"ctl_vehicle\"],autopct=\"%.2f%%\", colors=colors)\nplt.title(\"Distribution of CPtypes of Training data\")\nplt.show()\n\n# Draw a pie chart about Cpdose of Training data.\nplt.pie(train_features[\"cp_dose\"].value_counts(),labels=[\"D1\",\"D2\"],autopct=\"%.2f%%\", colors=colors)\nplt.title(\"Ratio of CPdose\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Labels per sample.\nplt.figure(figsize=(14,6))\nfeatures = train_target.columns.values[1:]\nsns.countplot(train_target[features].sum(axis=1))\nplt.xlabel('Number of Targets per Sample')\nplt.title('Target Score Counts')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of Cell viability columns\nc_columns = train_features.columns[train_features.columns.str.startswith('c-')]\n\n# Plotting some of the features\nplt.figure(figsize=(12, 12))\ncols =c_columns[0:12]\nfor i, col in enumerate(cols):\n    plt.subplot(4, 4, i + 1)\n    plt.hist(train_features.loc[:, col], bins=100, alpha=1, color='grey');\n    plt.title(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of Cell viability columns\ng_columns = train_features.columns[train_features.columns.str.startswith('g-')]\n\n# Plotting some of the features starting with c\nplt.figure(figsize=(12, 12))\ncols =g_columns[0:12]\nfor i, col in enumerate(cols):\n    plt.subplot(4, 4, i + 1)\n    plt.hist(train_features.loc[:, col], bins=100, alpha=1, color='grey');\n    plt.title(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting correlation Matrix\ncorr = train_features.corr()\n\nkot = corr[corr>=.9]\nplt.figure(figsize=(12,8))\nplt.title(\"Features with correlation > 0.9\")\nsns.heatmap(kot, cmap=\"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Training -----------------------------------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for empty/blank values in our data set\n\ndef get_percentage_missing(series):\n    num = series.isnull().sum()\n    den = len(series)\n    return round(num/den, 2)\n\nprint(\"Column Name\",\"Percentage Missing\")\nfor i in train_features.columns:\n    print(i,\":\\t\",get_percentage_missing(train_features[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = train_features.drop(['sig_id','cp_type','cp_dose','cp_time'],axis=1)\ntest_features = test_features.drop(['sig_id','cp_dose','cp_time'],axis=1)\ntrain_targets_scored = pd.read_csv(f'../input/lish-moa/train_targets_nonscored.csv')\ntrain_targets_scored.pop('sig_id')\nlabels_train = train_targets_scored.values\nnumerical_features= train_features.columns[train_features.dtypes!=\"object\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfeatures = train_features.columns.str.contains('c-')\ngfeatures = train_features.columns.str.contains('g-')\n\ndef get_features(X_train,X_test):\n    n_gfeatures = 20\n    n_cfeatures = 100     \n    pca_cfeatures = PCA(n_components = n_cfeatures)\n    pca_gfeatures = PCA(n_components = n_gfeatures)\n    X_train_pca_gfeatures = pca_gfeatures.fit_transform(X_train[:,gfeatures])\n    X_train_pca_cfeatures = pca_cfeatures.fit_transform(X_train[:,cfeatures])\n    X_test_pca_gfeatures = pca_gfeatures.transform(X_test[:,gfeatures])\n    X_test_pca_cfeatures = pca_cfeatures.transform(X_test[:,cfeatures])\n    X_train_c_mean = X_train[:,cfeatures].mean(axis=1)\n    X_test_c_mean = X_test[:,cfeatures].mean(axis=1)    \n    X_train_g_mean = X_train[:,gfeatures].mean(axis=1)\n    X_test_g_mean = X_test[:,gfeatures].mean(axis=1)\n    X_train = np.concatenate((X_train,X_train_pca_gfeatures,X_train_pca_cfeatures,X_train_c_mean[:,np.newaxis]\n                            ,X_train_g_mean[:,np.newaxis]),axis=1)\n    X_test = np.concatenate((X_test,X_test_pca_gfeatures,X_test_pca_cfeatures,X_test_c_mean[:,np.newaxis],\n                           X_test_g_mean[:,np.newaxis]),axis=1)\n    \n    # Standardizing data\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    \n    return X_train, X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_labels = train_targets_scored.shape[1]\nn_train = train_features.shape[0]\nn_test = test_features.shape[0]\n# Prediction Thresholds\np_min = 4E-4\np_max = 0.9\n\ndef custom_logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -backend.mean(y_true * backend.log(y_pred) + (1-y_true) * backend.log(1-y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction & Logloss -----------------------------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Definition for prediction\ndef Define_model_prediction(n_features, n_features_2, n_labels, label_smoothing = 0.0005):    \n    input_1 = layers.Input(shape = (n_features,), name = 'Input1')\n    input_2 = layers.Input(shape = (n_features_2,), name = 'Input2')\n\n    part_1 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(512, activation=\"elu\",kernel_initializer='glorot_normal',\n        kernel_regularizer=regularizers.l1_l2(l1=1e-7, l2=1e-6)), \n        layers.BatchNormalization(),\n        layers.Dense(256, activation = \"relu\")\n        ],name='part1') \n\n    input_3 = part_1(input_1)\n    input_3_concat = layers.Concatenate()([input_2, input_3])\n\n    part_2 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(512, activation=\"relu\",kernel_initializer='glorot_normal',\n        kernel_regularizer=regularizers.l1_l2(l1=1e-7, l2=1e-6)), \n        layers.BatchNormalization(),\n        layers.Dense(512, \"elu\"),\n        layers.BatchNormalization(),\n        layers.Dense(256, \"relu\"),\n        layers.BatchNormalization(),\n        layers.Dense(256, \"elu\")\n        ],name='part2')\n\n    input_4 = part_2(input_3_concat)\n    input_4_avg = layers.Average()([input_3, input_4]) \n\n    part_3 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(1024, activation=\"relu\",kernel_initializer='glorot_normal',\n        kernel_regularizer=regularizers.l1_l2(l1=1e-7, l2=1e-6)), \n        layers.BatchNormalization(),\n        layers.Dense(512, \"elu\"),\n        layers.BatchNormalization(),\n        layers.Dense(512, \"relu\"),\n        layers.BatchNormalization(),\n        layers.Dense(256, \"elu\")\n        ],name='part3')\n    input_5 = part_3(input_4_avg)\n    input_5_avg = layers.Average()([input_4, input_5]) \n\n    \n    part_4 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(512, activation=\"relu\",kernel_initializer='glorot_normal',\n        kernel_regularizer=regularizers.l1_l2(l1=1e-7, l2=1e-6)), \n        layers.BatchNormalization(),\n        layers.Dense(512, \"elu\"),\n        layers.BatchNormalization(),\n        layers.Dense(256, \"relu\"),\n        layers.BatchNormalization(),\n        layers.Dense(256, \"elu\")\n        ],name='part4')\n    input_6 = part_4(input_5_avg)\n    input_6_avg = layers.Average()([input_5, input_6]) \n\n    \n    part_5 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dense(256, kernel_initializer='glorot_normal', activation='relu',\n        kernel_regularizer=regularizers.l1_l2(l1=1e-6, l2=1e-5),\n        ),\n        layers.BatchNormalization(),\n        layers.Dense(n_labels, kernel_initializer='glorot_normal', activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dense(n_labels, activation=\"sigmoid\")\n        ],name='part5')\n\n    output = part_5(input_6_avg)\n\n\n    model = Model(inputs = [input_1, input_2], outputs = output)\n    model.compile(optimizer='adam', loss=losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n                  metrics=custom_logloss)\n    \n    return model\n\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"n_seeds = 5\nnp.random.seed(4545)\nn_folds = 5\nseeds = np.random.randint(0,100,size=n_seeds)\ny_pred = np.zeros((n_test,n_labels))\n\nfor seed in seeds:\n    fold = 0\n    kf = KFold(n_splits=n_folds,shuffle=True,random_state=seed)\n    for train, test in kf.split(train_features):\n        X_train, X_test = get_features(train_features.iloc[train].values,train_features.iloc[test].values)\n        _,unknown = get_features(train_features.iloc[train].values,test_features.drop('cp_type',axis=1).values)\n        \n        allcols_train = train_features.iloc[train][numerical_features].values\n        allcols_test = train_features.iloc[test][numerical_features].values\n        unknown_2 = test_features[numerical_features].values\n        \n        y_train = labels_train[train]\n        y_test = labels_train[test]\n        n_features = X_train.shape[1]\n        n_features_2 = allcols_train.shape[1]\n\n        model = Define_model_prediction(n_features, n_features_2, n_labels)\n        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_custom_logloss',  mode='min', min_lr=1E-6, factor=0.1, patience=2,)\n        early_stopping = callbacks.EarlyStopping(monitor='val_custom_logloss', min_delta=1E-6, patience=10, mode='min',restore_best_weights=True)\n        hist = model.fit([X_train,allcols_train],y_train, batch_size=128, epochs=12,verbose=1,validation_data = ([X_test,allcols_test],y_test),\n                         callbacks=[reduce_lr, early_stopping])\n        \n        # Run prediction\n        y_pred += model.predict([unknown,unknown_2])/(n_folds*n_seeds)\n                       \n        # Plotting logloss\n        plt.plot(hist.history['custom_logloss'], color='red')\n        plt.plot(hist.history['val_custom_logloss'], color='black')\n        plt.title('Model Accuracy - Logloss')\n        plt.ylabel('Accuracy')\n        plt.xlabel('Epoch')\n        plt.legend(['Train', 'Validation'], loc='upper right')\n        plt.show()\n        \n        #plotting loss\n        plt.plot(hist.history['loss'], color='red')\n        plt.plot(hist.history['val_loss'], color='black')\n        plt.title('Model Accuracy - loss')\n        plt.ylabel('Accuracy')\n        plt.xlabel('Epoch')\n        plt.legend(['Train', 'Validation'], loc='upper right')\n        plt.show()\n\n        fold += 1        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# References:\n# data-flair.training/blogs/keras-models/\n# alanpryorjr.com/visualizations/seaborn/heatmap/heatmap/\n# keras.io/api/models/sequential/\n# keras.io/api/optimizers/\n# arxiv.org/abs/1412.6980\n# ruder.io/optimizing-gradient-descent/\n# tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau\n# ENPM809N - Lecture 4 by Deogratias Kibira: Exploratory Data Analysis; Principle Component Analysis\n\n# Referred submissions & discussions on kaggle to understand the approach for the problem:\n# kaggle.com/roshankumarg/extensive-guide-to-exploratory-data-analysis\n# kaggle.com/arjunsehajpal/mechanism-of-action-exploratory-data-analysis\n# kaggle.com/pankajdubey87/ensemble-nn-and-xgboost\n# kaggle.com/fchollet/moa-keras-kerastuner-best-practices\n# kaggle.com/ravy101/drug-moa-tf-keras-starter\n# kaggle.com/c/lish-moa/discussion/201051\n# kaggle.com/c/lish-moa/discussion/200992","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}