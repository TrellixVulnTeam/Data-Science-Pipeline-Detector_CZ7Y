{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dftrain = pd.read_csv(\"/kaggle/input/lish-moa/train_features.csv\")\ndftest = pd.read_csv(\"/kaggle/input/lish-moa/test_features.csv\")\ndf = pd.concat([dftrain,dftest])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(dftrain['cp_type']) \n# TREATMENT STATUS - trt_cp -> Treated & ctl_vehicle -> control","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(dftrain['cp_dose'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(dftrain['cp_time']) # TREATMENT TIME","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\ntrain_target.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train_target.drop(['sig_id'],axis=1).sum(axis=0).sort_values(ascending=False).reset_index()\nx.columns = ['Protein/Enzyme','non-zero-records']\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top = x.head(20)\nplt.figure(figsize=(20,8))\nplt.title(\"Top 20 Protein/Enzyme Entries\")\nsns.barplot(top['Protein/Enzyme'],top['non-zero-records'])\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bot = x.tail(20)\nplt.figure(figsize=(20,8))\nplt.title(\"Bottom 20 Protein/Enzyme Entries\")\nsns.barplot(bot['Protein/Enzyme'],bot['non-zero-records'])\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train_target.drop(['sig_id'],axis=1).sum(axis=0).sort_values(ascending=False).reset_index()\nx.columns = ['Protein/Enzyme','non-zero-records']\nx['count'] = x['non-zero-records'] * 100 / len(train_target)\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top = x.head(40)\nplt.figure(figsize=(20,8))\nplt.title(\"Top 40 Protein/Enzyme Entries by Overall percentage\")\nsns.barplot(top['Protein/Enzyme'],top['count'])\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bot = x.tail(20)\nplt.figure(figsize=(20,8))\nplt.title(\"Bottom 20 Protein/Enzyme Entries by Overall percentage\")\nsns.barplot(bot['Protein/Enzyme'],bot['count'])\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train_target.drop(['sig_id'],axis=1).astype(bool).sum(axis=1).reset_index()\nx.columns = ['row','count']\nx = x.groupby(['count'])['row'].count().reset_index()\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x['count'],x['row'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.pie(x,values=100 * x['row']/len(train_target),names='count',\n      title='Number of activations in targets for every sample (Percent)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_columns = dftrain.columns.to_list()\ng_list = [i for i in train_columns if i.startswith('g-')]\nc_list = [i for i in train_columns if i.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = g_list + c_list\nimport random\nforcorr = [columns[random.randint(0,len(columns)-1)] for i in range(30)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forcorr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrdata = df[forcorr]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(24,14))\nplt.title(\"Correlation Matrix for Randomly selected 30 Features\")\nsns.heatmap(corrdata.corr(),annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\nstart = time.time()\ncols = ['cp_time'] + columns\nall_columns = []\nfor i in range(0, len(cols)):\n    for j in range(i+1, len(cols)):\n        if abs(dftrain[cols[i]].corr(dftrain[cols[j]])) > 0.9:\n            all_columns.append(cols[i])\n            all_columns.append(cols[j])\n\nprint(time.time()-start)\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"highcorrdata = df[all_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(24,14))\nplt.title(\"Correlation Matrix for Highly Correlated features\")\nsns.heatmap(highcorrdata.corr())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_columns = train_target.columns.tolist()\ntarget_columns.remove('sig_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_matrix = pd.DataFrame()\nfor t_col in train_target.columns:\n    corr_list = list()\n    if t_col == 'sig_id':\n        continue\n    for col in columns:\n        res = dftrain[col].corr(train_target[t_col])\n        corr_list.append(res)\n    correlation_matrix[t_col] = corr_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_matrix['train_features'] = columns\ncorrelation_matrix = correlation_matrix.set_index('train_features')\ncorrelation_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targetcols = train_target.columns.tolist()\n# target_columns.remove('sig_id')\nforanalysis = [target_columns[random.randint(0,len(target_columns)-1)] for m in range(5)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"foranalysis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"currentcols = correlation_matrix[foranalysis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"currentcols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coldf = pd.DataFrame()\ntr_first_cols = list()\ntr_second_cols = list()\ntarcols = list()\nfor col in currentcols.columns:\n    tarcols.append(col)\n    tr_first_cols.append(currentcols[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(2).values[0])\n    tr_second_cols.append(currentcols[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(2).values[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coldf['column'] = tarcols\ncoldf['train_1_column'] = tr_first_cols\ncoldf['train_2_column'] = tr_second_cols\ncoldf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scatterplot(coldf,index):\n    analysis = pd.DataFrame()\n    analysis['color'] = train_target[coldf.iloc[index]['column']]\n    analysis['x'] = dftrain[coldf.iloc[index]['train_1_column']]\n    analysis['y'] = dftrain[coldf.iloc[index]['train_2_column']]\n    analysis.columns = ['color',coldf.iloc[index]['train_1_column'],coldf.iloc[index]['train_2_column']]\n    analysis['size'] = 1\n    analysis.loc[analysis['color'] == 1, 'size'] = 10\n    fig = px.scatter(\n        analysis, \n        x=coldf.iloc[index]['train_1_column'], \n        y=coldf.iloc[index]['train_2_column'], \n        color=\"color\", \n        size='size', \n        height=800,\n        title='Scatter plot for ' + coldf.iloc[index]['column']\n    )\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatterplot(coldf, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatterplot(coldf, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatterplot(coldf, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatterplot(coldf,3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatterplot(coldf, 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODELLING\n\nI have followed this video by Abhishek Thakur for the code given below !\nIts an amazing approach i got to learn a lot :D\nhttps://www.youtube.com/watch?v=VRVit0-0AXE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored = pd.read_csv(\"/kaggle/input/lish-moa/train_targets_scored.csv\")\ntrain_targets_nonscored = pd.read_csv(\"/kaggle/input/lish-moa/train_targets_nonscored.csv\")\ntrain_features = pd.read_csv(\"/kaggle/input/lish-moa/train_features.csv\")\ntest_features = pd.read_csv(\"/kaggle/input/lish-moa/test_features.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/lish-moa/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features[:1][[col for col in train_features.columns if 'g-' in col]].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs = train_features[:1][[col for col in train_features.columns if 'g-' in col]].values.reshape(-1,1)\ngs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(gs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train_features['g-0'],color='red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_features.loc[:,\"kfold\"] = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.concat([train_features,pd.get_dummies(train_features['cp_time'],prefix = 'cp_time')],axis=1)\ntrain_features = pd.concat([train_features,pd.get_dummies(train_features['cp_type'],prefix = 'cp_type')],axis=1)\ntrain_features = pd.concat([train_features,pd.get_dummies(train_features['cp_dose'],prefix = 'cp_dose')],axis=1)\ntrain_features = train_features.drop(columns=['cp_time','cp_type','cp_dose'],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"       \nclass MoADataset:\n    def __init__(self, dataset, targets):\n        self.dataset = dataset\n        self.targets = targets\n\n    def __len__(self):\n        return self.dataset.shape[0]\n\n    def __getitem__(self, item):\n        return {\n            \"x\": torch.tensor(self.dataset[item, :], dtype=torch.float),\n            \"y\": torch.tensor(self.targets[item, :], dtype=torch.float),\n        }\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    \nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(num_features, 1024),\n            nn.BatchNorm1d(1024),\n            nn.Dropout(0.3),\n            nn.PReLU(),\n            nn.Linear(1024, 1024),\n            nn.BatchNorm1d(1024),\n            nn.Dropout(0.4),\n            nn.PReLU(),\n            nn.Linear(1024, 1024),\n            nn.BatchNorm1d(1024),\n            nn.Dropout(0.4),\n            nn.PReLU(),\n            nn.Linear(1024, num_targets),\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pytorch-lightning","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pytorch_lightning as pl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    \nclass MoADataModule(pl.LightningDataModule):\n    def __init__(self, hparams, data, targets):\n        super().__init__()\n        self.hparams = hparams\n        self.data = data\n        self.targets = targets\n\n    def prepare_data(self):\n        pass\n\n    def setup(self, stage=None):\n\n        train_data, valid_data, train_targets, valid_targets = train_test_split(self.data, self.targets,\n                                                                                test_size=0.2, random_state=42)\n        self.train_dataset = MoADataset(dataset=train_data.iloc[:, 1:].values,\n                                         targets=train_targets.iloc[:, 1:].values)\n        self.valid_dataset = MoADataset(dataset=valid_data.iloc[:, 1:].values,\n                                         targets=valid_targets.iloc[:, 1:].values)\n    \n    def train_dataloader(self):\n        train_loader = torch.utils.data.DataLoader(\n            self.train_dataset,\n            batch_size=1024,\n            num_workers=0,\n            shuffle=True,\n        )\n        return train_loader\n\n    def val_dataloader(self):\n        valid_loader = torch.utils.data.DataLoader(\n            self.valid_dataset,\n            batch_size=1024,\n            num_workers=0,\n            shuffle=False,\n        )\n\n        return valid_loader\n\n    def test_dataloader(self):\n        return None\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LitMoA(pl.LightningModule):\n    def __init__(self, hparams, model):\n        super(LitMoA, self).__init__()\n        self.hparams = hparams\n        self.model = model\n        self.criterion = nn.BCEWithLogitsLoss()\n                \n    def forward(self, x):\n        return self.model(x)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                               patience=3, threshold=0.00001, mode=\"min\", verbose=True)\n        return ([optimizer],\n                [{'scheduler': scheduler, 'interval': 'epoch', 'monitor': 'valid_loss'}])\n        \n\n    def training_step(self, batch, batch_idx):\n        data = batch['x']\n        target = batch['y']\n        out = self(data)\n        loss = self.criterion(out, target)\n        logs = {'train_loss': loss}        \n        return {'loss': loss, 'log': logs, 'progress_bar': logs}\n    \n    def training_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n        logs = {'train_loss': avg_loss}\n        return {'log': logs, 'progress_bar': logs}\n    \n    def validation_step(self, batch, batch_idx):\n        data = batch['x']\n        target = batch['y']\n        out = self(data)\n        loss = self.criterion(out, target)\n        \n        logs = {'valid_loss': loss}\n        \n        return {'loss': loss, 'log': logs, 'progress_bar': logs}\n    \n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n        logs = {'valid_loss': avg_loss}\n        return {'log': logs, 'progress_bar': logs}\n    \n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = pl.Trainer(gpus=1,max_epochs=50,weights_summary='full')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net = Model(879, 206) # number of features, number of targets\nmodel = LitMoA(hparams={}, model=net)\ndm = MoADataModule(hparams={}, data=train_features, targets=train_targets_scored)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.fit(model,dm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features = pd.concat([test_features, pd.get_dummies(test_features['cp_time'], prefix='cp_time')], axis=1)\ntest_features = pd.concat([test_features, pd.get_dummies(test_features['cp_dose'], prefix='cp_dose')], axis=1)\ntest_features = pd.concat([test_features, pd.get_dummies(test_features['cp_type'], prefix='cp_type')], axis=1)\ntest_features = test_features.drop(['cp_type', 'cp_time', 'cp_dose'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestMoADataset:\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __len__(self):\n        return self.dataset.shape[0]\n\n    def __getitem__(self, item):\n        return {\n            \"x\": torch.tensor(self.dataset[item, :], dtype=torch.float),\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = TestMoADataset(dataset=test_features.iloc[:, 1:].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loader = torch.utils.data.DataLoader(\n            test_dataset,\n            batch_size=1024,\n            num_workers=0,\n            shuffle=False,\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.zeros((test_features.shape[0], 206))\ninference_model = model.model\ninference_model.eval()\nfor ind, batch in enumerate(test_loader):\n    p = inference_model(batch['x'])[0].detach().cpu().numpy()\n    predictions[ind * 1024:(ind + 1) * 1024] = p","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features1 = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\ns = pd.DataFrame({'sig_id': test_features1['sig_id'].values})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train_targets_scored.columns[1:].tolist():\n    s[col] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s.loc[:, train_targets_scored.columns[1:]] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features1.loc[test_features1['cp_type'] =='ctl_vehicle', 'sig_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s.loc[s['sig_id'].isin(test_features1.loc[test_features1['cp_type'] =='ctl_vehicle', 'sig_id']), train_targets_scored.columns[1:]] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.model.state_dict(), 'model.pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}