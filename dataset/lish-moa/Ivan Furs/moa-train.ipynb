{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport datetime\nimport random\nimport os\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CFG"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"config = {\n          'folds':18, 'lr': 0.00028, 'seed': 666, \n          'batch size': 128,    \n          'inf_in':7, 'inf_out':32,\n          'g_in': 256, 'env2': 256, 'env3': 512, \n          'g_in': 772, 'g_h1': 256, 'g_h2': 256, 'g_out': 2048, 'g_dropout':0.3015,\n          'c_in': 100, 'c_h1': 256, 'c_h2': 256, 'c_out': 128, 'c_dropout':0.3015,\n          'merge_out':2048, 'classes_h1': 4096, 'classes_h2': 512, 'classes':206, 'dropout':0.3015, \n         }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SEED"},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    \n    \n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    \n\nseed_everything(666)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LOAD DATA"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.DataFrame(pd.read_csv('../input/lish-moa/train_features.csv'))\ntrain_targets_nonscored = pd.DataFrame(pd.read_csv('../input/lish-moa/train_targets_nonscored.csv'))\ntrain_targets_scored = pd.DataFrame(pd.read_csv('../input/lish-moa/train_targets_scored.csv'))\ntest_features = pd.DataFrame(pd.read_csv('../input/lish-moa/test_features.csv'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Class Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"cp_type={'trt_cp':np.array([1,0]), 'ctl_vehicle':np.array([0,1])}\ncp_time={24:np.array([0,0,1]), 48:np.array([0,1,0]), 72:np.array([1,0,0])}\ncp_dose={'D1':np.array([1,0]), 'D2':np.array([0,1])}\n\n\nclass Dataset:\n    def __init__(self, x, y):        \n        self.x = x      \n        self.y = y.set_index('sig_id')\n                \n\n    def __getitem__(self, item):  \n        row = self.x.iloc[item].values\n        \n        \n        y = self.y.loc[row[0]]\n        xg = np.float32(row[4:4+772])        \n        xc = np.float32(row[4+772:])\n        X = np.float32(np.concatenate([cp_type[row[1]], cp_time[row[2]], cp_dose[row[3]], xg, xc]))       \n        Y = np.float32(y.values)        \n        \n        return X, Y\n\n\n    def __len__(self):\n        return len(self.x)\n    \nclass DatasetTest:\n    def __init__(self, x):        \n        self.x = x      \n                \n\n    def __getitem__(self, item):  \n        row = self.x.iloc[item].values\n        \n        X = np.float32(np.concatenate([cp_type[row[1]], cp_time[row[2]], cp_dose[row[3]], row[4:]]))\n        id_ = row[0]\n        \n        return X, id_\n\n\n    def __len__(self):\n        return len(self.x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Class MOAModel"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1), self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n\ndef init_layer(layer):\n    nn.init.xavier_uniform_(layer.weight)\n\n    if hasattr(layer, \"bias\"):\n        if layer.bias is not None:\n            layer.bias.data.fill_(0.)\n\ndef init_bn(bn):\n    bn.bias.data.fill_(0.)\n    bn.weight.data.fill_(1.0)    \n\n\nclass Mish(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        x = x * (torch.tanh(F.softplus(x)))\n        return x\n\n\nclass SigmoidSoftplus(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        x = torch.sigmoid(x*F.softplus(x))\n        return x\n\n    \n\n    \nclass FC(nn.Module):\n    def __init__(self, in_features, h1, h2, out_features, drop, layer_end=True):\n        \n        super().__init__()\n        self.l1   = nn.Linear(in_features, h1)\n        self.bn1  = nn.BatchNorm1d(h1)\n        self.act1 = nn.LogSigmoid()            \n        self.dr1  = nn.Dropout(drop)\n        \n        self.l2 = nn.Linear(h1, h2)\n        \n        if layer_end == True:\n            self.bn2  = nn.BatchNorm1d(h2)\n            self.act2 = nn.LogSigmoid()            \n            self.dr2  = nn.Dropout(drop)  \n            \n            self.l3   = nn.utils.weight_norm(nn.Linear(h2, out_features))            \n        else:\n            self.l3 = None\n        \n        init_layer(self.l1)      \n        init_layer(self.l2)  \n        if layer_end:\n            init_layer(self.l3)\n            \n        init_bn(self.bn1)\n        if layer_end:\n            init_bn(self.bn2)\n        \n        \n    def forward(self, x):\n        x = self.l1(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        x = self.dr1(x)\n        \n        x = self.l2(x)\n        \n        if self.l3 is not None:\n            x = self.bn2(x)\n            x = self.act2(x)\n            x = self.dr2(x)\n\n            x = self.l3(x)\n        return x        \n        \nclass ENV(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        \n        self.bn_inf = nn.BatchNorm1d(config['inf_in'])      \n        \n        self.inf = nn.Linear(config['inf_in'], config['inf_out'])        \n        self.g = FC(config['g_in'], config['g_h1'], config['g_h2'], config['g_out'], config['g_dropout'], True)\n        self.c = FC(config['c_in'], config['c_h1'], config['c_h2'], config['c_out'], config['c_dropout'], True)\n        \n        self.merge = nn.utils.weight_norm(nn.Linear(config['g_out'] + config['c_out'] + 32, config['merge_out']))\n        \n        init_bn(self.bn_inf)\n        init_layer(self.inf)\n\n    def forward(self, x):  \n        x_0 = x\n        x1 = x[:, :7]\n        x2 = x[:, 7:7+772]        \n        x3 = x[:, 7+772:]\n        \n        inf = self.inf(self.bn_inf(x1))           \n        g = self.g(x2)\n        c = self.c(x3)           \n\n        x = torch.cat([g, c, inf], 1)     \n        x = self.merge(x)  \n        \n        return x        \n\nclass MOAModel(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        \n        self.env  = ENV(config)\n        self.act1 = SigmoidSoftplus()\n        \n        self.fc = FC(config['merge_out'], config['classes_h1'], config['classes_h2'], config['classes'], config['dropout'], False)\n        #self.mish = Mish()\n\n        self.bn2  = nn.BatchNorm1d(config['classes_h2'])\n        self.act2 = nn.LogSigmoid()        \n        self.dr2  = nn.Dropout(config['dropout'])\n        self.l2   = nn.utils.weight_norm(nn.Linear(config['classes_h2'], config['classes']))\n               \n        init_bn(self.bn2)    \n        init_layer(self.l2)    \n\n    def forward(self, x):\n        x = self.env(x)\n        \n        #x = x*x.sigmoid()\n        #x = self.mish(x)\n        x = self.act1(x) # <---?  \n        \n        b = x.size(0)\n        x = x.reshape(b, -1)\n        \n        x = self.fc(x)\n        \n        x = self.bn2(x)\n        x = self.act2(x)\n        x = self.dr2(x)\n        x = self.l2(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_idx = np.stack([train_targets_scored.sum()[1:].index, train_targets_scored.sum()[1:].values], 1)\nidxs_sort = np.argsort(class_idx[:, 1])\n\nsort_classes = []\nfor cl in class_idx[:, 0][idxs_sort]:\n    sort_classes += [cl]\n\n\nFOLDS = config['folds']\ndataset_train = []\ndataset_valid = []\nfolds = {}\ntd = train_targets_scored.copy()\nfor cl in sort_classes:    \n    v = td[td[cl] == 1].sample(frac=1).reset_index(drop=True)    \n    k = np.int32(np.ceil(len(v)/FOLDS))\n    x = np.arange(FOLDS)\n    x = np.tile(x, k)\n    for k, d in zip(v['sig_id'].values, x[:len(v)]):        \n        folds[k] = d\n    \n    td = td[td[cl] != 1].copy()\n    \n    #print(cl)\nv = td[td.sum(1) == 0]\nk = np.int32(np.ceil(len(v)/FOLDS))\nx = np.arange(FOLDS)\nx = np.tile(x, k)\nfor k, d in zip(v['sig_id'].values, x[:len(v)]):        \n    folds[k] = d\n    \nscored = train_targets_scored.copy()\ndef func(v):\n    return folds[v]\nscored['fold'] = train_targets_scored['sig_id'].apply(func)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_train = train_features[scored['fold'] != config['folds'] - 1].reset_index(drop=True)\ndataset_valid = train_features[scored['fold'] == config['folds'] - 1].reset_index(drop=True)\n\nprint(len(dataset_train), len(dataset_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = config[\"batch size\"]\ndts_train = Dataset(dataset_train, train_targets_scored)\ndts_valid = Dataset(dataset_valid, train_targets_scored)\n\ndts_test  = DatasetTest(test_features)\ntrain_loader =  torch.utils.data.DataLoader(dts_train, \n                                            batch_size=BATCH_SIZE, \n                                            shuffle=True, \n                                            sampler=None, \n                                            #collate_fn=collate_fn, \n                                            num_workers=4)\nvalid_loader =  torch.utils.data.DataLoader(dts_valid, \n                                            batch_size=1, \n                                            shuffle=False, \n                                            sampler=None, \n                                            #collate_fn=collate_fn, \n                                            num_workers=4)\nprint(len(train_loader), len(valid_loader))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = MOAModel(config)\nmodel = model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#criteric1 = SmoothBCEwLogits(smoothing = 0.001)\ncriteric1 = nn.BCEWithLogitsLoss()\ncriteric2 = nn.BCEWithLogitsLoss()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\nscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1,  last_epoch=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GaussianNoise(nn.Module):\n    \"\"\"Gaussian noise regularizer.\n\n    Args:\n        sigma (float, optional): relative standard deviation used to generate the\n            noise. Relative means that it will be multiplied by the magnitude of\n            the value your are adding the noise to. This means that sigma can be\n            the same regardless of the scale of the vector.\n        is_relative_detach (bool, optional): whether to detach the variable before\n            computing the scale of the noise. If `False` then the scale of the noise\n            won't be seen as a constant but something to optimize: this will bias the\n            network to generate vectors with smaller values.\n    \"\"\"\n\n    def __init__(self, sigma=0.1, is_relative_detach=True):\n        super().__init__()\n        self.sigma = sigma\n        self.is_relative_detach = is_relative_detach\n        self.noise = torch.tensor(0).to(device)\n\n    def forward(self, x):\n        if self.training and self.sigma != 0:\n            scale = self.sigma * x.detach() if self.is_relative_detach else self.sigma * x\n            sampled_noise = self.noise.repeat(*x.size()).float().normal_() * scale\n            x = x + sampled_noise\n        return x \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mixup_data(x, y, alpha=1.0, use_cuda=True):\n    if alpha > 0.:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1.\n    batch_size = x.size()[0]\n    if use_cuda:\n        index = torch.randperm(batch_size).cuda()\n    else:\n        index = torch.randperm(batch_size)\n\n    mixed_x = lam * x + (1 - lam) * x[index,:]\n    y_a, y_b = y, y[index]\n    \n    return mixed_x, y_a, y_b, lam\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_loss = np.inf\nepoch_start = 1\nmodel_name_best = ''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything(config['seed'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(datetime.datetime.now())\n\nEARLY_STOPPING_STEPS = 30\nEARLY_STOP = True\n\nearly_stopping_steps = EARLY_STOPPING_STEPS\nearly_step = 0\n    \nnoise = GaussianNoise(0.33)\nhistory = []\nfor epoch in range(epoch_start, 15000+1):\n    model.train()\n    loss_total = 0.0\n    count = 0\n    for b_idx, xy in enumerate(train_loader):\n        X = torch.FloatTensor(xy[0])\n        Y = torch.FloatTensor(xy[1])\n        \n        X = X.to(device)\n        Y = Y.to(device)\n              \n        for param in model.parameters():\n            param.grad = None\n        \n            \n        k = 6 #np.random.randint(2)\n        if k == 0:            \n            pred = model(X)   \n            loss = criteric1(pred, Y)  \n            \n        if k == 1:\n            mixed_x, y_a, y_b, lam = mixup_data(X, Y, 0.4)\n            pred = model(mixed_x)    \n            loss = lam * criteric1(pred, y_a) + (1 - lam) * criteric1(pred, y_b) \n            \n        if k == 2:   \n            X = noise(X)\n            pred = model(X)   \n            loss = criteric1(pred, Y) \n            \n        if k == 3:  \n            mixed_x, y_a, y_b, lam = mixup_data(X, Y, 0.4)\n            mixed_x = noise(mixed_x)\n            pred = model(mixed_x)    \n            loss = lam * criteric1(pred, y_a) + (1 - lam) * criteric1(pred, y_b) \n\n        if k == 4:  \n            X = noise(X)\n            mixed_x, y_a, y_b, lam = mixup_data(X, Y, 0.4)\n            mixed_x = noise(mixed_x)\n            pred = model(mixed_x)    \n            loss = lam * criteric1(pred, y_a) + (1 - lam) * criteric1(pred, y_b) \n            \n\n        if k == 5:              \n            mixed_x, y_a, y_b, lam = mixup_data(X, Y, 0.4)\n            mixed_x = noise(mixed_x)\n            pred = model(mixed_x)    \n            loss = lam * criteric1(pred, y_a) + (1 - lam) * criteric1(pred, y_b) \n            \n            \n        if k == 6:   \n            X = noise(X)\n            X = noise(X)\n            pred = model(X)   \n            loss = criteric1(pred, Y) \n         \n        loss.backward()      \n        if b_idx % 1 == 0:\n            optimizer.step()  \n        \n        scheduler.step(epoch + b_idx / len(train_loader))\n        \n        pred = torch.sigmoid(pred)\n        \n        loss_total += loss.item()\n        count += 1\n        \n        loss_toat_mean = loss_total/count\n\n    \n    print(f\"{datetime.datetime.now()}: {epoch}. Train | loss={(loss_toat_mean):.6f}\")       \n    \n    model.eval()\n    loss_total = 0.0\n    count = 0\n    \n    for b_idx, xy in enumerate(valid_loader):\n        X = torch.FloatTensor(xy[0])\n        Y = torch.FloatTensor(xy[1])#\n        X = X.to(device)\n        Y = Y.to(device)\n        \n        with torch.no_grad():            \n            pred = model(X)\n        loss = criteric2(pred, Y)\n        \n        loss_total += loss.item()\n        count += 1\n        \n        pred = torch.sigmoid(pred)        \n        loss_toat_mean = loss_total/count        \n        \n    print(f\"{datetime.datetime.now()}: {epoch}. Valid | loss={(loss_toat_mean):.6f}\")\n    \n    if best_loss >= loss_total/count :        \n        print(f\"\\tSave: {best_loss} -> {(loss_total/count)}\")\n        \n        best_loss = loss_total/count\n        if os.path.isfile(model_name_best):\n            os.remove(model_name_best)\n        model_name_best = f'{best_loss}_model_base.pt'\n        torch.save(model.state_dict(), model_name_best)\n        \n        early_step = 0\n        \n    elif(EARLY_STOP == True):            \n        early_step += 1\n        if (early_step >= early_stopping_steps):\n            break\n            \n    epoch_start += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'BEST: {best_loss})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msd = torch.load(f'{best_loss}_model_base.pt', map_location=device)\nmodel.load_state_dict(msd)\nmodel = model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 1\ntest_features = pd.DataFrame(pd.read_csv('../input/lish-moa/test_features.csv'))\n\ntest_loading =  torch.utils.data.DataLoader(DatasetTest(test_features), \n                                            batch_size=BATCH_SIZE, \n                                            shuffle=False, \n                                            sampler=None, \n                                            num_workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored = pd.DataFrame(pd.read_csv('../input/lish-moa/train_targets_scored.csv'))\ntrain_targets_scored","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features = pd.DataFrame(pd.read_csv('../input/lish-moa/test_features.csv'))\nsample_submission = pd.DataFrame(pd.read_csv('../input/lish-moa/sample_submission.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns=train_targets_scored.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(datetime.datetime.now())\nres = []\nmodel.eval()\nfor b_idx, xy in enumerate(test_loading):\n    X = torch.FloatTensor(xy[0])#[None, :]\n    X = X.to(device)\n\n    \n    with torch.no_grad():\n        pred = model(X)\n        pred = torch.sigmoid(pred)\n\n    for i in range(len(pred)):\n        pr = pred[i].cpu().data.numpy()\n        pr = np.clip(pr,0.0005,0.999)\n        res.append([xy[1][i]] + pr.tolist())\n    \nprint(len(res))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res_test = pd.DataFrame(res, columns=train_targets_scored.columns)\nres_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols = train_targets_scored.drop('sig_id', axis=1).columns.values.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sample_submission.drop(columns=target_cols).merge(res_test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## End"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}