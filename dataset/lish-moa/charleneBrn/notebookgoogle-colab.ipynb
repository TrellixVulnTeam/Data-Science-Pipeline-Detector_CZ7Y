{"cells":[{"metadata":{"id":"n8p6PUY-JM8J","outputId":"631a6764-72be-4f4e-b401-8da5200f824a","trusted":true},"cell_type":"code","source":"# INSTALL NECESSARY MODULES \n#!pip install -q -U keras-tuner","execution_count":null,"outputs":[]},{"metadata":{"id":"1wCDzAx9MKkF","outputId":"34b7d8cb-d92c-4f82-a528-1b4829d9a904","trusted":true},"cell_type":"code","source":"import os\nimport datetime\nimport IPython\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport kerastuner as kt\nimport matplotlib.pyplot as plt\n\n\nfrom tensorflow import feature_column\nfrom tensorflow.keras import layers\n#from google.colab import drive\nfrom tensorflow.keras import regularizers\nfrom tensorflow import keras\n\n# drive.mount('/content/drive', force_remount=True)\n# os.chdir(\"/content/drive/My Drive/Deep Learning/RENDUS-Groupe/Project\")\n# !ls\n\n# tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"id":"8oIxS6jYMKkG"},"cell_type":"markdown","source":"# Read metadata about our data"},{"metadata":{"id":"dNGidX2Bnx3L","trusted":true},"cell_type":"code","source":"# GET TESTS IDS\ntargets = pd.read_csv(\"/kaggle/input/lish-moa/test_features.csv\")\nids = targets.pop('sig_id')","execution_count":null,"outputs":[]},{"metadata":{"id":"o-b_BQSoMKkG","outputId":"78781bc0-de06-47a3-98ee-5f079dba1690","trusted":true},"cell_type":"code","source":"# HAVE PREVIEW OF METADATA\nfeatures = pd.read_csv(\"/kaggle/input/lish-moa/train_features.csv\", nrows=10)\ntargets = pd.read_csv(\"/kaggle/input/lish-moa/train_targets_scored.csv\", nrows=10)\n\ncolumns = targets.columns\n\ncols_features = features.columns\ncols_targets = targets.columns\n\nnum_features = len(cols_features)\nnum_targets = len(cols_targets)\n\nprint(\"Number of features:\" , num_features)\nprint(\"Number of targets:\" , num_targets)","execution_count":null,"outputs":[]},{"metadata":{"id":"mTU7ZAI7MKkH"},"cell_type":"markdown","source":"# Reading data using tf.data.experimental.CsvDataset"},{"metadata":{"id":"ojKt-d96MKkH","trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\nfeatures_types = [str(), str(), str(), str()] + [float()]*(num_features-4)\ntargets_types = [str()] + [float()]*(num_targets-1)\n\nfeatures = tf.data.experimental.CsvDataset(\"/kaggle/input/lish-moa/train_features.csv\",\n                                           record_defaults=features_types,\n                                           #select_cols\n                                           header=True)\n\ntargets = tf.data.experimental.CsvDataset(\"/kaggle/input/lish-moa/train_targets_scored.csv\",\n                                          record_defaults=targets_types,\n                                          header=True)\n\ntest= tf.data.experimental.CsvDataset(\"/kaggle/input/lish-moa/test_features.csv\",\n                                          record_defaults=features_types,\n                                          header=True)\n\ndataset = tf.data.Dataset.zip((features, targets))","execution_count":null,"outputs":[]},{"metadata":{"id":"OmI1DuIJMKkH","outputId":"3666f285-60ad-4031-e652-66280d2f723e","trusted":true},"cell_type":"code","source":"# split dataset into train and val\ndataset_size = dataset.reduce(0, lambda x, _: x + 1).numpy()\n\ntrain_size = int(0.7*dataset_size)\nval_size = dataset_size - train_size\n\ntrain = dataset.take(train_size)\nval = dataset.skip(train_size)\nval = dataset.take(val_size)\n\n#We get the training size dataset and the validation size dataset\ntrain_size = train.reduce(0, lambda x, _: x + 1).numpy()\nval_size = val.reduce(0, lambda x, _: x + 1).numpy()\n\ntest_size = test.reduce(0, lambda x, _: x + 1).numpy()\ntest = dataset.take(test_size)\n\nprint(\"Full dataset size:\", dataset_size)\nprint(\"Train dataset size:\", train_size)\nprint(\"Val dataset size:\", val_size)\nprint(\"Test size\", test_size)","execution_count":null,"outputs":[]},{"metadata":{"id":"KB5nbee_MKkH","trusted":true},"cell_type":"code","source":"def _preprocess_line(features, targets):\n    # Pack the result into a dictionary\n    features = dict(zip(cols_features, features))\n    features.pop('sig_id')\n    targets = tf.stack(targets[1:])\n    \n    return features, targets\n\ndef _preprocess_line_bis(features, targets):\n    # Pack the result into a dictionary but keep sig_id\n    features = dict(zip(cols_features, features))\n    targets = tf.stack(targets[1:])\n    \n    return features, targets\n\ntrain = train.map(_preprocess_line)\ntrain = train.shuffle(train_size,seed = 123)\ntrain = train.batch(BATCH_SIZE)\n\nval = val.map(_preprocess_line)\nval = val.shuffle(val_size,seed = 123)\nval = val.batch(BATCH_SIZE)\n\ntest = test.map(_preprocess_line_bis)\ntest = test.batch(BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"id":"qlDynELpNdfd"},"cell_type":"markdown","source":"# Features Engineering"},{"metadata":{"id":"8huj-KVCNiEd","trusted":true},"cell_type":"code","source":"feature_columns =  []\n\n# A utility method to create a feature column\n# and to transform a batch of data\ndef demo(feature_column):\n  feature_layer = layers.DenseFeatures(feature_column)\n  feature_columns.append(feature_column)","execution_count":null,"outputs":[]},{"metadata":{"id":"Nx1vheILOR_4","trusted":true},"cell_type":"code","source":"# HANDLE CATEGORICAL COLUMN\ncategorical_columns=['cp_time','cp_type','cp_dose']\ncat_cp_time = feature_column.categorical_column_with_vocabulary_list( categorical_columns[0], ['24', '48', '72'])\ncat_cp_dose = feature_column.categorical_column_with_vocabulary_list(categorical_columns[1],['D1','D2'])\ncat_cp_type = feature_column.categorical_column_with_vocabulary_list(categorical_columns[2],['trt_cp','ctl_vehicle'])\n\ncat_one_encod_cp_time = feature_column.indicator_column(cat_cp_time)\ncat_one_encod_cp_dose = feature_column.indicator_column(cat_cp_dose)\ncat_one_encod_cp_type = feature_column.indicator_column(cat_cp_type)\n\ndemo(cat_one_encod_cp_time)\ndemo(cat_one_encod_cp_dose)\ndemo(cat_one_encod_cp_type)","execution_count":null,"outputs":[]},{"metadata":{"id":"G9sejTtoOX7p","trusted":true},"cell_type":"code","source":"#HANDLE NUMERICAL VALUE\nL=[]\nfor batch, label in train.take(1):\n    L.append(list(batch.keys()))\n\nnumerical_columns=L[0]\nnumerical_columns.remove('cp_time')\nnumerical_columns.remove('cp_type')\nnumerical_columns.remove('cp_dose')\n\nfor i in range(len(numerical_columns)):\n    num = feature_column.numeric_column(numerical_columns[i])\n    demo(num)","execution_count":null,"outputs":[]},{"metadata":{"id":"UFsRJzn8OrTp","trusted":true},"cell_type":"code","source":"# CREATE INPUT LAYER OF FEATURES\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)","execution_count":null,"outputs":[]},{"metadata":{"id":"vjBzBUgURwAs"},"cell_type":"markdown","source":"# Baseline Modeling & HYPERPARAMETER TUNING"},{"metadata":{"id":"bMaPlxirF7ct","trusted":true},"cell_type":"code","source":"# DEFINE A MODEL WRAPPER FOR HP TUNING\n\nmodel = keras.Sequential()\n#hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4, 1e-5]) # tune lr\n#n_layers = hp.Int('num_layers', min_value = 2, max_value = 10, step =1) # tune nb of layers\n\n#FEATURE LAYERS + BATCH NORRMALIZATION\nmodel.add(feature_layer)\nmodel.add(tf.keras.layers.BatchNormalization(momentum=0.8))\n\n# TUNE NUMBER OF LAYERS\n\nmodel.add(layers.Dense(units=800,activation='relu')) # tune nb of neurons\nmodel.add(keras.layers.Dropout(0.1)) # tune dropout\nmodel.add(tf.keras.layers.BatchNormalization(momentum=0.8)) #NORM BATCH AFTER EACH DENSE LAYER\n\nmodel.add(layers.Dense(units=950,activation='relu')) # tune nb of neurons\nmodel.add(keras.layers.Dropout(0.1)) # tune dropout\nmodel.add(tf.keras.layers.BatchNormalization(momentum=0.8)) #NORM BATCH AFTER EACH DENSE LAYER\n\n# OUTPUT LAYER\nmodel.add(keras.layers.Dense(206, activation=tf.nn.sigmoid))  \n\n# COMPILE MODEL\nmodel.compile(optimizer = keras.optimizers.Adam(learning_rate = 0.001),\n            loss = 'binary_crossentropy',\n            metrics = ['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"nMkObdMejj7k","outputId":"0040f209-f485-4422-ce37-e4d61840d9e6","trusted":true},"cell_type":"code","source":"# Build the model with the optimal hyperparameters and train it on the data\n#model = tuner.hypermodel.build(best_hps)\nhistory =model.fit(train, epochs = 30, validation_data = val)","execution_count":null,"outputs":[]},{"metadata":{"id":"nFF9OCFaRec9","outputId":"ba89f7fa-f20d-4c14-e81b-5351bcac9be1","trusted":true},"cell_type":"code","source":"# TAKE A LOOK AT OUR MODEL\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"56zxaPswatBy","outputId":"a4c3dd60-4225-43b7-b330-50be04d27944","trusted":true},"cell_type":"code","source":"#model.predict\npredictions_test = model.predict(test)\nprint(predictions_test.shape)\npredictions_test_pd = pd.DataFrame(predictions_test)\nprint(type(predictions_test_pd))\npredictions_test_pd.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"NrMuTDSSuFyb","outputId":"8f84af12-55cc-4bf0-9252-24312fc97858","trusted":true},"cell_type":"code","source":"print(ids.shape)\nids_pd = pd.DataFrame(ids)\nprint(type(ids_pd))\nids_pd.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"rdTBzbYQyXw_","outputId":"d83e5d81-7c99-4e94-fca1-6d4576862f9e","trusted":true},"cell_type":"code","source":"predictions_test_pd.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"hFmCcV422Umz","outputId":"1bd2b460-df8e-496e-bda5-b4b56faec187","trusted":true},"cell_type":"code","source":"\ncolumns_name = list(columns)\nprint(columns_name)\nprint(len(columns_name))","execution_count":null,"outputs":[]},{"metadata":{"id":"GwSVx4MOz8eK","outputId":"4304eb52-de94-4144-936a-063abb925b27","trusted":true},"cell_type":"code","source":"predictions_test_pd.insert(0,\"sig_id\",ids_pd)\npredictions_test_pd.columns =columns_name\npredictions_test_pd.head()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"H1gI2z3LmWxS","trusted":true},"cell_type":"code","source":"predictions_test_pd.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"PC1dxg8Va-YM","outputId":"dc457116-e2b1-40ab-c628-985dc5a08a56","trusted":true},"cell_type":"code","source":"predictions_test[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"lVBtbcbdZEoB"},"cell_type":"markdown","source":"# Model Variance & Bias Analysis"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}