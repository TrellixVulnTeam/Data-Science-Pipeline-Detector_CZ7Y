{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false},"cell_type":"code","source":"import os\nimport random\nimport time\nimport json\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.base import BaseEstimator\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer\nfrom sklearn.metrics import fbeta_score\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import regularizers\nimport tensorflow.keras.backend as K\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import sys\nsys.path += ['/kaggle/input/iterstrat', '/kaggle/input/tabnet']\n\nfrom ml_stratifiers import MultilabelStratifiedKFold\nfrom tabnet.stacked_tabnet import StackedTabNetClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def set_random_seeds(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nset_random_seeds(43)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"X_full = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\nY_scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\nY_nonscored = pd.read_csv('/kaggle/input/lish-moa/train_targets_nonscored.csv')\ndrug = pd.read_csv('/kaggle/input/lish-moa/train_drug.csv')\n\nX_test_full = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\nsig_id = X_test_full['sig_id'].values.reshape((-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"cat_cols = ['cp_type', 'cp_time', 'cp_dose']\nnumerical_cols = [c for c in X_full.columns if c not in ['sig_id'] + cat_cols]\nlabel_cols = [c for c in Y_scored.columns if c != 'sig_id']\n\nnum_features = X_full.shape[1] - 1\nnum_labels = Y_scored.shape[1] - 1\nnum_nonscored_labels = Y_nonscored.shape[1] - 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def logloss(Y_true, Y_preds, label_smoothing=0):\n    Y_true = Y_true.astype(np.float32)\n    return tf.reduce_mean(keras.losses.binary_crossentropy(Y_true, Y_preds, label_smoothing=label_smoothing)).numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def add_outputs(Y, Y_preds, X=None, idx_list=None, col=None):\n    if X is not None:\n        if col is None:\n            Y.loc[X['cp_type'] == 'trt_cp', :] += Y_preds\n        else:\n            Y.loc[X['cp_type'] == 'trt_cp', Y.columns[col]] += Y_preds\n    elif idx_list is not None:\n        if col is None:\n            Y.iloc[idx_list, :] += Y_preds\n        else:\n            Y.iloc[idx_list, col] += Y_preds\n    else:\n        raise Exception(\"Must provide X or idx_list\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def balance_class_weights(Y_true, class_weights=(0.7, 1.6)):\n    n_labels = Y_true.shape[1]\n    weights = np.empty([n_labels, 2])\n    for i in range(n_labels):\n        if class_weights == 'balanced':\n            total = len(Y_true[:, i])\n            pos = np.sum(Y_true[:, i])\n            neg = total - pos\n            weight_for_0 = (1 / neg)*(total)/2.0 \n            weight_for_1 = (1 / pos)*(total)/2.0\n            weights[i] = [weight_for_0, weight_for_1]\n        else:\n            weights[i] = np.array(class_weights)\n    return weights\n\n\ndef weighted_binary_crossentropy(weights, label_smoothing=0):\n    def weighted_loss(Y_true, Y_pred):\n        bce = tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing)\n        return K.mean(\n            (weights[:, 0]**(1 - Y_true)) * (weights[:, 1]**(Y_true)) * bce(Y_true, Y_pred),\n            axis=-1\n        )\n    return weighted_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def preprocess(df, targets=None):\n    df = df.copy()\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    df = df.drop(columns='sig_id')\n    \n    if targets is not None:\n        targets = targets.copy()\n        targets = targets[df['cp_type'] == 0].reset_index(drop=True)\n\n    df = df[df['cp_type'] == 0].reset_index(drop=True)\n    \n    if targets is None:\n        return df\n    return df, targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"X_train, Y_train = preprocess(X_full, Y_scored)\nX_test = preprocess(X_test_full)\nY_train_nonscored = Y_nonscored.drop(columns='sig_id')[X_full['cp_type'] == 'trt_cp'].reset_index(drop=True)\ninitial_bias = -np.log(Y_train.mean(axis=0).values)\nnonscored_initial_bias = -np.log(Y_train_nonscored.mean(axis=0).values + 1e-6)\n\nY_train_with_drugs = Y_train.merge(drug, on='sig_id', how='left') \nY_train = Y_train.drop(columns='sig_id')\nvc = Y_train_with_drugs.drug_id.value_counts()\nvc1 = vc.loc[(vc == 6) | (vc == 12) | (vc == 18)].index.sort_values()\nvc2 = vc.loc[(vc != 6) & (vc != 12) & (vc != 18)].index.sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def select_features(X):\n    return X.iloc[:, [\n        1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n        18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n        32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n        47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n        61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n        74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n       102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n       115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n       129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n       144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n       158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n       171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n       198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n       213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n       227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n       240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n       267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n       281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n       295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n       310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n       324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n       337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n       350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n       363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n       405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n       419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n       432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n       447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n       463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n       476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n       490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n       506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n       522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n       538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n       552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n       571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n       586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n       600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n       618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n       631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n       645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n       660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n       673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n       686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n       701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n       718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n       733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n       748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n       762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n       775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n       789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n       804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n       821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n       837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n       854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n       870, 871, 872, 873, 874\n    ]].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"gene_cols = [c for c in X_test.columns if c.startswith('g-')]\ncell_cols = [c for c in X_test.columns if c.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"import joblib\n\nqtransform = joblib.load('/kaggle/input/qtransform/qtransform.joblib')\n\nX_test_selected = select_features(X_test)\nX_test_qtrans =  np.concatenate([X_test_selected[:, :2], qtransform.transform(X_test_selected[:, 2:])], axis=1)\n\npca_qtrans_2 = joblib.load('/kaggle/input/qtransform/pca_qtrans.joblib')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"std_scaler_2 = joblib.load('/kaggle/input/qtransform/std_scaler_2.joblib')\ngene_pca_2 = joblib.load('/kaggle/input/qtransform/gene_pca_2.joblib')\ncell_pca_2 = joblib.load('/kaggle/input/qtransform/cell_pca_2.joblib')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nkmeans_g = joblib.load('/kaggle/input/qtransform/kmeans_g.joblib')\nkmeans_c = joblib.load('/kaggle/input/qtransform/kmeans_c.joblib')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DRUG CV"},{"metadata":{},"cell_type":"markdown","source":"## 1. NN"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def preprocess_nn(X):\n    X_pca = pca_qtrans_2.transform(X)\n    return np.concatenate([X, X_pca], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_nn = preprocess_nn(X_test_qtrans)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class MultiLabelNN(BaseEstimator):\n\n    def __init__(self,\n                 n_input_features=num_features,\n                 n_output_labels=num_labels,\n                 n_hidden_units=2048):\n        self.n_input_features = n_input_features\n        self.n_output_labels = n_output_labels\n        self.n_hidden_units = n_hidden_units\n        self._model_nn = None\n        \n    def _create_nn_model(self, loss_fn=None, loss_fn_aux=None):\n        output_bias = keras.initializers.Constant(initial_bias)\n        aux_output_bias = keras.initializers.Constant(nonscored_initial_bias)\n        \n        inp = keras.layers.Input(self.n_input_features)\n        gauss_noise = keras.layers.GaussianNoise(0.001)(inp)\n        norm_1 = keras.layers.BatchNormalization()(gauss_noise)\n\n        dense_1 = tfa.layers.WeightNormalization(keras.layers.Dense(\n            self.n_hidden_units // 2,\n            activation='elu',\n            kernel_initializer='he_normal',\n            kernel_regularizer=regularizers.l2(1e-5)\n        ))(norm_1)\n        norm_2 = keras.layers.BatchNormalization()(dense_1)\n        dropout_1 = keras.layers.Dropout(0.4)(norm_2)\n        \n        dense_2 = tfa.layers.WeightNormalization(keras.layers.Dense(\n            self.n_hidden_units,\n            activation='elu',\n            kernel_initializer='he_normal',\n            kernel_regularizer=regularizers.l2(1e-5)\n        ))(dropout_1)\n        norm_3 = keras.layers.BatchNormalization()(dense_2)\n        dropout_2 = keras.layers.Dropout(0.4)(norm_3)\n        \n        dropout_3 = keras.layers.Dropout(0.5)(norm_2)\n        dense_3 = tfa.layers.WeightNormalization(keras.layers.Dense(\n            self.n_hidden_units // 2,\n            activation='relu',\n            kernel_regularizer=regularizers.l2(1e-5)\n        ))(dropout_3)\n        norm_4 = keras.layers.BatchNormalization()(dense_3)\n        dropout_4 = keras.layers.Dropout(0.4)(norm_4)\n\n        output_1 = tfa.layers.WeightNormalization(keras.layers.Dense(\n            self.n_output_labels,\n            activation='sigmoid',\n            bias_initializer=output_bias\n        ), name='main')(dropout_4)\n        output_2 = tfa.layers.WeightNormalization(keras.layers.Dense(\n            Y_train_nonscored.shape[1],\n            activation='sigmoid',\n            bias_initializer=aux_output_bias\n        ), name='aux')(dropout_2)\n        \n        model_nn = keras.models.Model(inputs=inp, outputs=[output_1, output_2])\n        model_nn.compile(\n            loss=[\n                keras.losses.BinaryCrossentropy(label_smoothing=2e-4) if loss_fn is None else loss_fn,\n                keras.losses.BinaryCrossentropy(label_smoothing=2e-4) if loss_fn_aux is None else loss_fn_aux\n            ],\n            optimizer=keras.optimizers.Nadam(learning_rate=0.01),\n            metrics=['binary_crossentropy'],\n            loss_weights=[0.9, 0.1]\n        )\n        return model_nn\n        \n    def fit(self, X, Y, X_valid, Y_valid, Y_aux, Y_aux_valid, max_epochs=100, save_weights=True):\n        Y = tf.cast(Y, tf.float32)\n        Y_valid = tf.cast(Y_valid, tf.float32)\n        Y_aux = tf.cast(Y_aux, tf.float32)\n        Y_aux_valid = tf.cast(Y_aux_valid, tf.float32)\n\n        class_weights = balance_class_weights(Y.numpy(), class_weights=(0.7, 2.0))\n        loss_fn = weighted_binary_crossentropy(class_weights, label_smoothing=1e-3)\n        class_weights_aux = balance_class_weights(Y_aux.numpy(), class_weights=(0.7, 1.4))\n        loss_fn_aux = weighted_binary_crossentropy(class_weights_aux, label_smoothing=1e-3)\n        self._model_nn = self._create_nn_model(loss_fn=loss_fn, loss_fn_aux=loss_fn_aux)\n\n        learning_rate_cb = keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            patience=3, verbose=1, factor=0.1, min_lr=1e-8\n        )\n        early_stop_cb = keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            min_delta=0.0001, patience=10,\n            restore_best_weights=False\n        )\n        checkpoint_cb = keras.callbacks.ModelCheckpoint(\n            f'model_nn/weights_{repeat}_{fold}.h5',\n            monitor='val_main_binary_crossentropy',\n            save_best_only=True,\n            save_weights_only=True\n        )\n        callbacks = [early_stop_cb, learning_rate_cb, checkpoint_cb] if save_weights else [early_stop_cb, learning_rate_cb]\n\n        history = self._model_nn.fit(\n            X, [Y, Y_aux],\n            batch_size=128,\n            epochs=max_epochs,\n            validation_data=(X_valid, [Y_valid, Y_aux_valid]),\n            callbacks=callbacks,\n            shuffle=True,\n            verbose=0\n        )\n        return history\n    \n    def predict(self, X):\n        return self._model_nn.predict(X)\n    \n    def score(self, X, Y_true):\n        Y_preds = self.predict(X)[0]\n        return logloss(Y_true, Y_preds)\n    \n    def save_weights(self, repeat, fold):\n        self._model_nn.save_weights('model_nn/weights_{}_{}.h5'.format(repeat, fold))\n        \n    def load_weights(self, repeat, fold, pretrained=True):\n        if self._model_nn is None:\n            self._model_nn = self._create_nn_model()\n        if pretrained:\n            base_path = '/kaggle/input/model-nn-drug-cv/'\n        else:\n            base_path = '/kaggle/working/'\n        self._model_nn.load_weights(base_path + 'model_nn/weights_{}_{}.h5'.format(repeat, fold))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-output":false},"cell_type":"code","source":"n_repeats = 5\nn_folds = 7\nY_test_preds_nn = pd.DataFrame(np.zeros((X_test_full.shape[0], num_labels)), columns=label_cols)\n\nfor repeat in range(n_repeats):\n    for fold in range(n_folds):\n        print('** Repeat {}/{}. Fold {}/{}'.format(repeat + 1, n_repeats, fold + 1, n_folds))\n        K.clear_session()\n        model_nn = MultiLabelNN(n_input_features=X_test_nn.shape[1])\n        model_nn.load_weights(repeat, fold)\n        test_preds = model_nn.predict(X_test_nn)[0]\n        add_outputs(Y_test_preds_nn, test_preds, X=X_test_full)\n        \n        del model_nn\n\nY_test_preds_nn.loc[X_test_full['cp_type'] == 'trt_cp', :] /= (n_folds * n_repeats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. TabNet"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def preprocess_tabnet(X, X_qtrans):\n    X_pca = pca_qtrans_2.transform(X_qtrans)\n    g_std = X[gene_cols].std(axis=1).values.reshape(-1, 1)\n    g_kurt = X[gene_cols].kurtosis(axis=1).values.reshape(-1, 1)\n    g_skew = X[gene_cols].skew(axis=1).values.reshape(-1, 1)\n    c_std = X[cell_cols].std(axis=1).values.reshape(-1, 1)\n    c_kurt = X[cell_cols].kurtosis(axis=1).values.reshape(-1, 1)\n    c_skew = X[cell_cols].skew(axis=1).values.reshape(-1, 1)\n    X_std = pd.DataFrame(std_scaler_2.transform(X[numerical_cols]), columns=numerical_cols)\n    return np.concatenate([\n        X_qtrans, X_pca,\n        g_std, g_kurt, g_skew,\n        c_std, c_kurt, c_skew,\n    ], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"X_test_tabnet = preprocess_tabnet(X_test, X_test_qtrans)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class MultiLabelTabNet(BaseEstimator):\n\n    def __init__(self, n_input_features, n_output_labels):\n        self.n_input_features = n_input_features\n        self.n_output_labels = n_output_labels\n        self.estimator_ = None\n        \n    def _create_model(self, loss_fn=None, output_bias=None):\n        if output_bias is not None:\n            output_bias = keras.initializers.Constant(output_bias)\n\n        clf = StackedTabNetClassifier(\n            feature_columns=None, \n            num_classes=1024,\n            num_layers=1,\n            feature_dim=2048,\n            output_dim=1024,\n            num_features=self.n_input_features,\n            num_decision_steps=1,\n            relaxation_factor=1.5,\n            sparsity_coefficient=1e-5,\n            norm_type='batch',\n            num_groups=-1,\n            output_activation='relu'\n        )\n        model = keras.models.Sequential([\n            keras.layers.InputLayer(self.n_input_features),\n            keras.layers.Dropout(0.5),\n            keras.layers.GaussianNoise(0.001),\n            clf,\n            keras.layers.Dropout(0.7),\n            tfa.layers.WeightNormalization(keras.layers.Dense(\n                self.n_output_labels,\n                activation='sigmoid',\n                bias_initializer=output_bias\n            ))\n        ])\n        model.compile(\n            loss=keras.losses.BinaryCrossentropy(label_smoothing=2e-3) if loss_fn is None else loss_fn,\n            optimizer=tfa.optimizers.AdamW(weight_decay=2.6e-5),\n            metrics=['binary_crossentropy']\n        )\n        return model\n        \n    def fit(self, X, Y, X_valid, Y_valid, repeat, fold, max_epochs=200):\n        Y = tf.cast(Y, tf.float32)\n        Y_valid = tf.cast(Y_valid, tf.float32)\n        class_weights = balance_class_weights(Y.numpy(), class_weights=(0.5, 15.0))\n        loss_fn = weighted_binary_crossentropy(class_weights, label_smoothing=1e-3)\n        self.estimator_ = self._create_model(loss_fn=loss_fn, output_bias=initial_bias)\n\n        learning_rate_cb = keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss', min_delta=1e-5,\n            patience=3, verbose=1, factor=0.1, min_lr=1e-7\n        )\n        early_stop_cb = keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            min_delta=1e-5, patience=10, restore_best_weights=False\n        )\n        checkpoint_cb = keras.callbacks.ModelCheckpoint(\n            f'model_tabnet/weights_{repeat}_{fold}.h5',\n            monitor='val_binary_crossentropy',\n            save_best_only=True,\n            save_weights_only=True\n        )\n\n        history = self.estimator_.fit(\n            X, Y,\n            batch_size=128,\n            epochs=max_epochs,\n            validation_data=(X_valid, Y_valid),\n            callbacks=[early_stop_cb, learning_rate_cb, checkpoint_cb],\n            shuffle=True,\n            verbose=0\n        )\n        return history\n    \n    def predict(self, X):\n        return self.estimator_.predict(X)\n    \n    def score(self, X, Y_true):\n        Y_preds = self.predict(X)\n        return logloss(Y_true, Y_preds)\n    \n    def save_weights(self, repeat, fold):\n        self.estimator_.save_weights('model_tabnet/weights_{}_{}.h5'.format(repeat, fold))\n        \n    def load_weights(self, repeat, fold, pretrained=True):\n        if self.estimator_ is None:\n            self.estimator_ = self._create_model()\n        if pretrained:\n            base_path = '/kaggle/input/model-tabnet-drug-cv/'\n        else:\n            base_path = './'\n        self.estimator_.load_weights(base_path + f'model_tabnet/weights_{repeat}_{fold}.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"n_repeats = 5\nn_folds = 7\nY_test_preds_tabnet = pd.DataFrame(np.zeros((X_test_full.shape[0], num_labels)), columns=label_cols)\n\nfor repeat in range(n_repeats):\n    for fold in range(n_folds):\n        print('** Repeat {}/{}. Fold {}/{}'.format(repeat + 1, n_repeats, fold + 1, n_folds))\n        K.clear_session()\n        model_tabnet = MultiLabelTabNet(\n            n_input_features=X_test_tabnet.shape[1],\n            n_output_labels=Y_train.shape[1]\n        )\n        model_tabnet.load_weights(repeat, fold)\n        test_preds = model_tabnet.predict(X_test_tabnet)\n        add_outputs(Y_test_preds_tabnet, test_preds, X=X_test_full)\n        \n        del model_tabnet\n\nY_test_preds_tabnet.loc[X_test_full['cp_type'] == 'trt_cp', :] /= (n_folds * n_repeats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. ResNet"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def preprocess_resnet(X):\n    X_scaled = pd.DataFrame(std_scaler_2.transform(X[numerical_cols]), columns=numerical_cols)\n    X_genes = gene_pca_2.transform(X_scaled[gene_cols])\n    X_cells = cell_pca_2.transform(X_scaled[cell_cols])\n    return select_features(X), np.concatenate([X_genes, X_cells], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"X_test_resnet_1, X_test_resnet_2 = preprocess_resnet(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class MultiLabelResNet(BaseEstimator):\n\n    def __init__(self, n_input_1, n_input_2, n_output_labels):\n        self.n_input_1 = n_input_1\n        self.n_input_2 = n_input_2\n        self.n_output_labels = n_output_labels\n        self.estimator_ = None\n        \n    def _create_model(self, initial_bias, loss_fn=None):\n        output_bias = keras.initializers.Constant(initial_bias)\n        \n        inp_1 = keras.layers.Input(self.n_input_1, name='raw_inp')\n        inp_2 = keras.layers.Input(self.n_input_2, name='pca_inp')\n        gauss_2 = keras.layers.GaussianNoise(1e-4)(inp_2)\n\n        head_1 = keras.models.Sequential([\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.2),\n            tfa.layers.WeightNormalization(keras.layers.Dense(512, activation='elu')),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.5),\n            tfa.layers.WeightNormalization(keras.layers.Dense(256, activation='elu'))\n        ], name='head1') \n\n        seq_1 = head_1(inp_1)\n        seq_1_inp_concat = keras.layers.Concatenate()([gauss_2, seq_1])\n\n        head_2 = keras.models.Sequential([\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.4),\n            tfa.layers.WeightNormalization(keras.layers.Dense(512, \"relu\")),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.5),\n            tfa.layers.WeightNormalization(keras.layers.Dense(512, \"elu\")),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.5),\n            tfa.layers.WeightNormalization(keras.layers.Dense(256, \"relu\")),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.1),\n            tfa.layers.WeightNormalization(keras.layers.Dense(256, \"elu\"))\n        ], name='head2')\n\n        seq_2 = head_2(seq_1_inp_concat)\n        seq_1_seq_2_avg = keras.layers.Average()([seq_1, seq_2]) \n\n        head_3 = keras.models.Sequential([\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.1),\n            tfa.layers.WeightNormalization(keras.layers.Dense(256, activation='relu')),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.1),\n            tfa.layers.WeightNormalization(keras.layers.Dense(206, activation='relu')),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.1),\n            tfa.layers.WeightNormalization(keras.layers.Dense(self.n_output_labels, bias_initializer=output_bias, activation='sigmoid'))\n        ], name='head3')\n\n        output = head_3(seq_1_seq_2_avg)\n        \n        model = keras.models.Model(inputs=[inp_1, inp_2], outputs=output)\n        model.compile(\n            loss=keras.losses.BinaryCrossentropy(label_smoothing=2e-3) if loss_fn is None else loss_fn,\n            optimizer=tfa.optimizers.AdamW(learning_rate=0.02, weight_decay=1e-5),\n            metrics=['binary_crossentropy'],\n        )\n        return model\n\n    def fit(self, X, Y, X_valid, Y_valid, repeat, fold, max_epochs=100, save_weights=True, initial_bias=initial_bias, transfer_model=None):\n        Y = tf.cast(Y, tf.float32)\n        Y_valid = tf.cast(Y_valid, tf.float32)\n\n        class_weights = balance_class_weights(Y.numpy(), class_weights=(0.5, 2.0))\n        loss_fn = weighted_binary_crossentropy(class_weights, label_smoothing=2e-3)\n        self.estimator_ = self._create_model(initial_bias, loss_fn=loss_fn)\n        if transfer_model is not None:\n            self.copy_weights(transfer_model)\n\n        learning_rate_cb = keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            patience=3, verbose=1, factor=0.1, min_lr=1e-8\n        )\n        early_stop_cb = keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            min_delta=0.0001, patience=15,\n            restore_best_weights=False\n        )\n        checkpoint_cb = keras.callbacks.ModelCheckpoint(\n            f'model_resnet/weights_{repeat}_{fold}.h5',\n            monitor='val_binary_crossentropy',\n            save_best_only=True,\n            save_weights_only=True\n        )\n        callbacks = [early_stop_cb, learning_rate_cb, checkpoint_cb] if save_weights else [early_stop_cb, learning_rate_cb]\n\n        history = self.estimator_.fit(\n            X, Y,\n            batch_size=128,\n            epochs=max_epochs,\n            validation_data=(X_valid, Y_valid),\n            callbacks=callbacks,\n            shuffle=True,\n            verbose=0\n        )\n        return history\n\n    def predict(self, X):\n        return self.estimator_.predict(X)\n    \n    def score(self, X, Y_true):\n        Y_preds = self.predict(X)\n        return logloss(Y_true, Y_preds)\n    \n    def save_weights(self, repeat, fold):\n        self.estimator_.save_weights('model_resnet/weights_{}_{}.h5'.format(repeat, fold))\n        \n    def load_weights(self, repeat, fold, pretrained=True):\n        if self.estimator_ is None:\n            self.estimator_ = self._create_model(initial_bias)\n        if pretrained:\n            base_path = '/kaggle/input/model-resnet-drug-cv/'\n        else:\n            base_path = './'\n        self.estimator_.load_weights(base_path + 'model_resnet/weights_{}_{}.h5'.format(repeat, fold))\n        \n    def copy_weights(self, from_model):\n        for to_layer, from_layer in zip(self.estimator_.layers[:-1], from_model.estimator_.layers[:-1]):\n            to_layer.set_weights(from_layer.get_weights())\n        for to_layer, from_layer in zip(self.estimator_.layers[-1].layers[:-1], from_model.estimator_.layers[-1].layers[:-1]):\n            to_layer.set_weights(from_layer.get_weights())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"n_repeats = 7\nn_folds = 7\nY_test_preds_resnet = pd.DataFrame(np.zeros((X_test_full.shape[0], num_labels)), columns=label_cols)\nK.clear_session()\n\nfor repeat in range(n_repeats):\n    for fold in range(n_folds):\n        print('** Repeat {}/{}. Fold {}/{}'.format(repeat + 1, n_repeats, fold + 1, n_folds))\n        model_resnet = MultiLabelResNet(\n            n_input_1=X_test_resnet_1.shape[1],\n            n_input_2=X_test_resnet_2.shape[1],\n            n_output_labels=Y_train.shape[1]\n        )\n        model_resnet.load_weights(repeat, fold)\n        test_preds = model_resnet.predict([X_test_resnet_1, X_test_resnet_2])\n        add_outputs(Y_test_preds_resnet, test_preds, X=X_test_full)\n        \n        del model_resnet\n     \nY_test_preds_resnet.loc[X_test_full['cp_type'] == 'trt_cp', :] /= (n_folds * n_repeats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Blending"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"weights = [0.35, 0.40, 0.25]\nprint(sum(weights))\nassert(abs(sum(weights) - 1) < 1e-6)\n\nfinal_preds_drug_cv = (\n    Y_test_preds_nn * weights[0] +\n    Y_test_preds_tabnet * weights[1] +\n    Y_test_preds_resnet * weights[2]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Raw CV"},{"metadata":{},"cell_type":"markdown","source":"## 1. NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_nn(X):\n    X_pca = pca_qtrans_2.transform(X)\n    return np.concatenate([X, X_pca], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_nn = preprocess_nn(X_test_qtrans)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MultiLabelNN(BaseEstimator):\n\n    def __init__(self,\n                 n_input_features=num_features,\n                 n_output_labels=num_labels,\n                 n_hidden_units=2048):\n        self.n_input_features = n_input_features\n        self.n_output_labels = n_output_labels\n        self.n_hidden_units = n_hidden_units\n        self._model_nn = None\n        \n    def _create_nn_model(self, loss_fn=None, loss_fn_aux=None):\n        output_bias = keras.initializers.Constant(initial_bias)\n        aux_output_bias = keras.initializers.Constant(nonscored_initial_bias)\n        \n        inp = keras.layers.Input(self.n_input_features)\n        gauss_noise = keras.layers.GaussianNoise(0.001)(inp)\n        norm_1 = keras.layers.BatchNormalization()(gauss_noise)\n\n        dense_1 = tfa.layers.WeightNormalization(keras.layers.Dense(\n            self.n_hidden_units // 2,\n            activation='elu',\n            kernel_initializer='he_normal',\n            kernel_regularizer=regularizers.l2(1e-5)\n        ))(norm_1)\n        norm_2 = keras.layers.BatchNormalization()(dense_1)\n        dropout_1 = keras.layers.Dropout(0.4)(norm_2)\n        \n        dense_2 = tfa.layers.WeightNormalization(keras.layers.Dense(\n            self.n_hidden_units,\n            activation='elu',\n            kernel_initializer='he_normal',\n            kernel_regularizer=regularizers.l2(1e-5)\n        ))(dropout_1)\n        norm_3 = keras.layers.BatchNormalization()(dense_2)\n        dropout_2 = keras.layers.Dropout(0.4)(norm_3)\n        \n        dropout_3 = keras.layers.Dropout(0.5)(norm_2)\n        dense_3 = tfa.layers.WeightNormalization(keras.layers.Dense(\n            self.n_hidden_units // 2,\n            activation='relu',\n            kernel_regularizer=regularizers.l2(1e-5)\n        ))(dropout_3)\n        norm_4 = keras.layers.BatchNormalization()(dense_3)\n        dropout_4 = keras.layers.Dropout(0.4)(norm_4)\n\n        output_1 = tfa.layers.WeightNormalization(keras.layers.Dense(\n            self.n_output_labels,\n            activation='sigmoid',\n            bias_initializer=output_bias\n        ), name='main')(dropout_4)\n        output_2 = tfa.layers.WeightNormalization(keras.layers.Dense(\n            Y_train_nonscored.shape[1],\n            activation='sigmoid',\n            bias_initializer=aux_output_bias\n        ), name='aux')(dropout_2)\n        \n        model_nn = keras.models.Model(inputs=inp, outputs=[output_1, output_2])\n        model_nn.compile(\n            loss=[\n                keras.losses.BinaryCrossentropy(label_smoothing=2e-4) if loss_fn is None else loss_fn,\n                keras.losses.BinaryCrossentropy(label_smoothing=2e-4) if loss_fn_aux is None else loss_fn_aux\n            ],\n            optimizer=keras.optimizers.Nadam(learning_rate=0.01),\n            metrics=['binary_crossentropy'],\n            loss_weights=[0.9, 0.1]\n        )\n        return model_nn\n        \n    def fit(self, X, Y, X_valid, Y_valid, Y_aux, Y_aux_valid, max_epochs=100, save_weights=True):\n        Y = tf.cast(Y, tf.float32)\n        Y_valid = tf.cast(Y_valid, tf.float32)\n        Y_aux = tf.cast(Y_aux, tf.float32)\n        Y_aux_valid = tf.cast(Y_aux_valid, tf.float32)\n\n        class_weights = balance_class_weights(Y.numpy(), class_weights=(0.7, 2.0))\n        loss_fn = weighted_binary_crossentropy(class_weights, label_smoothing=1e-3)\n        class_weights_aux = balance_class_weights(Y_aux.numpy(), class_weights=(0.7, 1.4))\n        loss_fn_aux = weighted_binary_crossentropy(class_weights_aux, label_smoothing=1e-3)\n        self._model_nn = self._create_nn_model(loss_fn=loss_fn, loss_fn_aux=loss_fn_aux)\n\n        learning_rate_cb = keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            patience=3, verbose=1, factor=0.1, min_lr=1e-8\n        )\n        early_stop_cb = keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            min_delta=0.0001, patience=10,\n            restore_best_weights=False\n        )\n        checkpoint_cb = keras.callbacks.ModelCheckpoint(\n            f'model_nn/weights_{repeat}_{fold}.h5',\n            monitor='val_main_binary_crossentropy',\n            save_best_only=True,\n            save_weights_only=True\n        )\n        callbacks = [early_stop_cb, learning_rate_cb, checkpoint_cb] if save_weights else [early_stop_cb, learning_rate_cb]\n\n        history = self._model_nn.fit(\n            X, [Y, Y_aux],\n            batch_size=128,\n            epochs=max_epochs,\n            validation_data=(X_valid, [Y_valid, Y_aux_valid]),\n            callbacks=callbacks,\n            shuffle=True,\n            verbose=0\n        )\n        return history\n    \n    def predict(self, X):\n        return self._model_nn.predict(X)\n    \n    def score(self, X, Y_true):\n        Y_preds = self.predict(X)[0]\n        return logloss(Y_true, Y_preds)\n    \n    def save_weights(self, repeat, fold):\n        self._model_nn.save_weights('model_nn/weights_{}_{}.h5'.format(repeat, fold))\n        \n    def load_weights(self, repeat, fold, pretrained=True):\n        if self._model_nn is None:\n            self._model_nn = self._create_nn_model()\n        if pretrained:\n            base_path = '/kaggle/input/model-nn/'\n        else:\n            base_path = './working/'\n        self._model_nn.load_weights(base_path + 'model_nn/weights_{}_{}.h5'.format(repeat, fold))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_repeats = 5\nn_folds = 7\nY_test_preds_nn = pd.DataFrame(np.zeros((X_test_full.shape[0], num_labels)), columns=label_cols)\n\nfor repeat in range(n_repeats):\n    for fold in range(n_folds):\n        print('** Repeat {}/{}. Fold {}/{}'.format(repeat + 1, n_repeats, fold + 1, n_folds))\n        K.clear_session()\n        model_nn = MultiLabelNN(n_input_features=X_test_nn.shape[1])\n        model_nn.load_weights(repeat, fold)\n        test_preds = model_nn.predict(X_test_nn)[0]\n        add_outputs(Y_test_preds_nn, test_preds, X=X_test_full)\n        \n        del model_nn\n     \nY_test_preds_nn.loc[X_test_full['cp_type'] == 'trt_cp', :] /= (n_folds * n_repeats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. TabNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_tabnet(X, X_qtrans):\n    X_pca = pca_qtrans_2.transform(X_qtrans)\n    g_std = X[gene_cols].std(axis=1).values.reshape(-1, 1)\n    g_kurt = X[gene_cols].kurtosis(axis=1).values.reshape(-1, 1)\n    g_skew = X[gene_cols].skew(axis=1).values.reshape(-1, 1)\n    c_std = X[cell_cols].std(axis=1).values.reshape(-1, 1)\n    c_kurt = X[cell_cols].kurtosis(axis=1).values.reshape(-1, 1)\n    c_skew = X[cell_cols].skew(axis=1).values.reshape(-1, 1)\n    X_std = pd.DataFrame(std_scaler_2.transform(X[numerical_cols]), columns=numerical_cols)\n    g_cluster = kmeans_g.predict(X_std[gene_cols]).reshape(-1, 1)\n    c_cluster = kmeans_c.predict(X_std[cell_cols]).reshape(-1, 1)\n    return np.concatenate([\n        X_qtrans, X_pca,\n        g_std, g_kurt, g_skew,\n        c_std, c_kurt, c_skew,\n        g_cluster, c_cluster\n    ], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_tabnet = preprocess_tabnet(X_test, X_test_qtrans)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MultiLabelTabNet(BaseEstimator):\n\n    def __init__(self, n_input_features, n_output_labels):\n        self.n_input_features = n_input_features\n        self.n_output_labels = n_output_labels\n        self.estimator_ = None\n        \n    def _create_model(self, loss_fn=None, output_bias=None):\n        if output_bias is not None:\n            output_bias = keras.initializers.Constant(output_bias)\n\n        clf = StackedTabNetClassifier(\n            feature_columns=None,\n            num_classes=1024,\n            num_layers=1,\n            feature_dim=2048,\n            output_dim=1024,\n            num_features=self.n_input_features,\n            num_decision_steps=1,\n            relaxation_factor=1.5,\n            sparsity_coefficient=1e-5,\n            num_groups=-1,\n            output_activation='relu'\n        )\n        model = keras.models.Sequential([\n            keras.layers.InputLayer(self.n_input_features),\n            keras.layers.Dropout(0.5),\n            keras.layers.GaussianNoise(0.001),\n            clf,\n            keras.layers.Dropout(0.7),\n            tfa.layers.WeightNormalization(keras.layers.Dense(\n                self.n_output_labels,\n                activation='sigmoid',\n                bias_initializer=output_bias\n            ))\n        ])\n        model.compile(\n            loss=keras.losses.BinaryCrossentropy(label_smoothing=2e-3) if loss_fn is None else loss_fn,\n            optimizer=tfa.optimizers.AdamW(weight_decay=2.6e-5),\n            metrics=['binary_crossentropy']\n        )\n        return model\n        \n    def fit(self, X, Y, X_valid, Y_valid, repeat, fold, max_epochs=200):\n        Y = tf.cast(Y, tf.float32)\n        Y_valid = tf.cast(Y_valid, tf.float32)\n        class_weights = balance_class_weights(Y.numpy(), class_weights=(0.5, 15.0))\n        loss_fn = weighted_binary_crossentropy(class_weights, label_smoothing=2e-3)\n        self.estimator_ = self._create_model(loss_fn=loss_fn, output_bias=initial_bias)\n\n        learning_rate_cb = keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss', min_delta=1e-5,\n            patience=3, verbose=1, factor=0.1, min_lr=1e-7\n        )\n        early_stop_cb = keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            min_delta=1e-5, patience=10, restore_best_weights=False\n        )\n        checkpoint_cb = keras.callbacks.ModelCheckpoint(\n            f'model_tabnet/weights_{repeat}_{fold}.h5',\n            monitor='val_binary_crossentropy',\n            save_best_only=True,\n            save_weights_only=True\n        )\n\n        history = self.estimator_.fit(\n            X, Y,\n            batch_size=128,\n            epochs=max_epochs,\n            validation_data=(X_valid, Y_valid),\n            callbacks=[early_stop_cb, learning_rate_cb, checkpoint_cb],\n            shuffle=True,\n            verbose=2\n        )\n        return history\n    \n    def predict(self, X):\n        return self.estimator_.predict(X)\n    \n    def score(self, X, Y_true):\n        Y_preds = self.predict(X)\n        return logloss(Y_true, Y_preds)\n    \n    def save_weights(self, repeat, fold):\n        self.estimator_.save_weights('model_tabnet/weights_{}_{}.h5'.format(repeat, fold))\n        \n    def load_weights(self, repeat, fold, pretrained=True):\n        if self.estimator_ is None:\n            self.estimator_ = self._create_model()\n        if pretrained:\n            base_path = '/kaggle/input/model-tabnet/'\n        else:\n            base_path = './'\n        self.estimator_.load_weights(base_path + f'model_tabnet/weights_{repeat}_{fold}.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_repeats = 5\nn_folds = 7\nY_test_preds_tabnet = pd.DataFrame(np.zeros((X_test_full.shape[0], num_labels)), columns=label_cols)\n\nfor repeat in range(n_repeats):\n    for fold in range(n_folds):\n        print('** Repeat {}/{}. Fold {}/{}'.format(repeat + 1, n_repeats, fold + 1, n_folds))\n        K.clear_session()\n        model_tabnet = MultiLabelTabNet(\n            n_input_features=X_test_tabnet.shape[1],\n            n_output_labels=Y_train.shape[1]\n        )\n        model_tabnet.load_weights(repeat, fold)\n        test_preds = model_tabnet.predict(X_test_tabnet)\n        add_outputs(Y_test_preds_tabnet, test_preds, X=X_test_full)\n        \n        del model_tabnet\n\nY_test_preds_tabnet.loc[X_test_full['cp_type'] == 'trt_cp', :] /= (n_folds * n_repeats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. ResNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_resnet(X):\n    X_scaled = pd.DataFrame(std_scaler_2.transform(X[numerical_cols]), columns=numerical_cols)\n    X_genes = gene_pca_2.transform(X_scaled[gene_cols])\n    X_cells = cell_pca_2.transform(X_scaled[cell_cols])\n    return select_features(X), np.concatenate([X_genes, X_cells], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_resnet_1, X_test_resnet_2 = preprocess_resnet(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MultiLabelResNet(BaseEstimator):\n\n    def __init__(self, n_input_1, n_input_2, n_output_labels):\n        self.n_input_1 = n_input_1\n        self.n_input_2 = n_input_2\n        self.n_output_labels = n_output_labels\n        self.estimator_ = None\n        \n    def _create_model(self, initial_bias, loss_fn=None):\n        output_bias = keras.initializers.Constant(initial_bias)\n        \n        inp_1 = keras.layers.Input(self.n_input_1, name='raw_inp')\n        inp_2 = keras.layers.Input(self.n_input_2, name='pca_inp')\n        gauss_2 = keras.layers.GaussianNoise(1e-4)(inp_2)\n\n        head_1 = keras.models.Sequential([\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.2),\n            tfa.layers.WeightNormalization(keras.layers.Dense(512, activation='elu')),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.5),\n            tfa.layers.WeightNormalization(keras.layers.Dense(256, activation='elu'))\n        ], name='head1') \n\n        seq_1 = head_1(inp_1)\n        seq_1_inp_concat = keras.layers.Concatenate()([gauss_2, seq_1])\n\n        head_2 = keras.models.Sequential([\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.4),\n            tfa.layers.WeightNormalization(keras.layers.Dense(512, \"relu\")),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.5),\n            tfa.layers.WeightNormalization(keras.layers.Dense(512, \"elu\")),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.5),\n            tfa.layers.WeightNormalization(keras.layers.Dense(256, \"relu\")),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.1),\n            tfa.layers.WeightNormalization(keras.layers.Dense(256, \"elu\"))\n        ], name='head2')\n\n        seq_2 = head_2(seq_1_inp_concat)\n        seq_1_seq_2_avg = keras.layers.Average()([seq_1, seq_2]) \n\n        head_3 = keras.models.Sequential([\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.1),\n            tfa.layers.WeightNormalization(keras.layers.Dense(256, activation='relu')),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.1),\n            tfa.layers.WeightNormalization(keras.layers.Dense(206, activation='relu')),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.1),\n            tfa.layers.WeightNormalization(keras.layers.Dense(self.n_output_labels, bias_initializer=output_bias, activation='sigmoid'))\n        ], name='head3')\n\n        output = head_3(seq_1_seq_2_avg)\n        \n        model = keras.models.Model(inputs=[inp_1, inp_2], outputs=output)\n        model.compile(\n            loss=keras.losses.BinaryCrossentropy(label_smoothing=2e-3) if loss_fn is None else loss_fn,\n            optimizer=tfa.optimizers.AdamW(learning_rate=0.02, weight_decay=1e-5),\n            metrics=['binary_crossentropy'],\n        )\n        return model\n\n    def fit(self, X, Y, X_valid, Y_valid, repeat, fold, max_epochs=100, save_weights=True, initial_bias=initial_bias, transfer_model=None):\n        Y = tf.cast(Y, tf.float32)\n        Y_valid = tf.cast(Y_valid, tf.float32)\n\n        class_weights = balance_class_weights(Y.numpy(), class_weights=(0.5, 2.0))\n        loss_fn = weighted_binary_crossentropy(class_weights, label_smoothing=2e-3)\n        self.estimator_ = self._create_model(initial_bias, loss_fn=loss_fn)\n        if transfer_model is not None:\n            self.copy_weights(transfer_model)\n\n        learning_rate_cb = keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            patience=3, verbose=1, factor=0.1, min_lr=1e-8\n        )\n        early_stop_cb = keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            min_delta=0.0001, patience=15,\n            restore_best_weights=False\n        )\n        checkpoint_cb = keras.callbacks.ModelCheckpoint(\n            f'model_resnet/weights_{repeat}_{fold}.h5',\n            monitor='val_binary_crossentropy',\n            save_best_only=True,\n            save_weights_only=True\n        )\n        callbacks = [early_stop_cb, learning_rate_cb, checkpoint_cb] if save_weights else [early_stop_cb, learning_rate_cb]\n\n        history = self.estimator_.fit(\n            X, Y,\n            batch_size=128,\n            epochs=max_epochs,\n            validation_data=(X_valid, Y_valid),\n            callbacks=callbacks,\n            shuffle=True,\n            verbose=0\n        )\n        return history\n    \n    def predict(self, X):\n        return self.estimator_.predict(X)\n    \n    def score(self, X, Y_true):\n        Y_preds = self.predict(X)\n        return logloss(Y_true, Y_preds)\n    \n    def save_weights(self, repeat, fold):\n        self.estimator_.save_weights('model_resnet/weights_{}_{}.h5'.format(repeat, fold))\n        \n    def load_weights(self, repeat, fold, pretrained=True):\n        if self.estimator_ is None:\n            self.estimator_ = self._create_model(initial_bias)\n        if pretrained:\n            base_path = '/kaggle/input/model-resnet/'\n        else:\n            base_path = './'\n        self.estimator_.load_weights(base_path + 'model_resnet/weights_{}_{}.h5'.format(repeat, fold))\n        \n    def copy_weights(self, from_model):\n        for to_layer, from_layer in zip(self.estimator_.layers[:-1], from_model.estimator_.layers[:-1]):\n            to_layer.set_weights(from_layer.get_weights())\n        for to_layer, from_layer in zip(self.estimator_.layers[-1].layers[:-1], from_model.estimator_.layers[-1].layers[:-1]):\n            to_layer.set_weights(from_layer.get_weights())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_repeats = 7\nn_folds = 7\nY_test_preds_resnet = pd.DataFrame(np.zeros((X_test_full.shape[0], num_labels)), columns=label_cols)\n\nfor repeat in range(n_repeats):\n    for fold in range(n_folds):\n        print('** Repeat {}/{}. Fold {}/{}'.format(repeat + 1, n_repeats, fold + 1, n_folds))\n        model_resnet = MultiLabelResNet(\n            n_input_1=X_test_resnet_1.shape[1],\n            n_input_2=X_test_resnet_2.shape[1],\n            n_output_labels=Y_train.shape[1]\n        )\n        model_resnet.load_weights(repeat, fold)\n        test_preds = model_resnet.predict([X_test_resnet_1, X_test_resnet_2])\n        add_outputs(Y_test_preds_resnet, test_preds, X=X_test_full)\n        \n        del model_resnet\n \nY_test_preds_resnet.loc[X_test_full['cp_type'] == 'trt_cp', :] /= (n_folds * n_repeats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Blending"},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = [0.22, 0.43, 0.35]\nprint(sum(weights))\nassert(abs(sum(weights) - 1) < 1e-6)\n\nfinal_preds_raw_cv = (\n    Y_test_preds_nn * weights[0] +\n    Y_test_preds_tabnet * weights[1] +\n    Y_test_preds_resnet * weights[2]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(np.corrcoef(final_preds_drug_cv.values.flatten(), final_preds_raw_cv.values.flatten()))\n# plt.scatter(final_preds_drug_cv.values.flatten(), final_preds_raw_cv.values.flatten(), alpha=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"final_preds = 0.4 * final_preds_drug_cv + 0.6 * final_preds_raw_cv\nfinal_preds.loc[X_test_full['cp_type'] != 'trt_cp', :] = 0\nfinal_preds = np.concatenate([sig_id, final_preds], axis=1)\nfinal_preds = pd.DataFrame(final_preds, columns=Y_scored.columns)\nfinal_preds.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}