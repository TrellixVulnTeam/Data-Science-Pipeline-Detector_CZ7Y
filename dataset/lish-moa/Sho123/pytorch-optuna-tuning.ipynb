{"cells":[{"metadata":{},"cell_type":"markdown","source":"# If you like, pls upvote and check my other kernel\nhttps://www.kaggle.com/utkukubilay/notebooks"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import QuantileTransformer\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=1903):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nclass MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    \n    \ndef train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        #print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n#'dropout_rate1': 0.17921150354824822, \n# 'dropout_rate2': 0.3918194448036252,\n# 'dropout_rate3': 0.2357391436892218, \n# 'dropout_rate4': 0.33573309986814803, '\n#0.3036410351072955, \n#'dropout_rate2': 0.21090449239303233, \n#'dropout_rate3': 0.17015128906288302,\n#'dropout_rate4': 0.27918373084339765,\n#'hidden_2': 920.0,\n#'hidden_3': 450.0}.\n\n'''\nparameters: {'dropout_rate1': 0.34033737675224757,\n'dropout_rate2': 0.3033653487000471,\n'hidden_2': 820.0}. Best is trial 15 with value: 0.014656699010826262.\n\n'''\n\n'''   \nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        #self.dropout1 = nn.Dropout(0.34033737675224757)  #21\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.3033653487000471)  #24\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        #self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n\n        \n        #self.batch_norm2_5 = nn.BatchNorm1d(hidden_2)\n        #self.dropout2_5 = nn.Dropout(0.17015128906288302)   #0.25\n        #self.dense2_5 = nn.utils.weight_norm(nn.Linear(hidden_2, hidden_3))\n        \n        #self.batch_norm2_6 = nn.BatchNorm1d(hidden_3)\n        #self.dropout2_6 = nn.Dropout(dropout4)\n        #self.dense2_6 = nn.utils.weight_norm(nn.Linear(hidden_3, hidden_4))        \n\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)   #hidden_3\n        self.dropout3 = nn.Dropout(0.27918373084339765)  #25\n        #self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_3, num_targets))\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n        \n        #self.resnet = nn.Sequential(*list(resnet.children())[:-3])\n\n    \n    def forward(self, x):\n        #x = self.resnet(x)\n        x = self.batch_norm1(x)\n        #x = self.dropout1(x)\n        #x = F.relu(self.dense1(x))\n        x = F.leaky_relu(self.dense1(x))\n\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        #x = F.relu(self.dense2(x))\n        x = F.leaky_relu(self.dense2(x))\n\n        \n        \n        #x = self.batch_norm2_5(x)\n        #x = self.dropout2_5(x)\n        #x = F.relu(self.dense2_5(x))\n        #x = F.leaky_relu(self.dense2_5(x))\n\n        \n        #x = self.batch_norm2_6(x)\n        #x = self.dropout2_6(x)\n        #x = F.relu(self.dense2_6(x))        \n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x\n        \n'''\nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.hidden_size = [1500, 1250, 1000, 750]\n        self.dropout_value = [0.5, 0.35, 0.3, 0.25]\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n        \n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_value[0])\n        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_value[1])\n        self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n\n        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n        self.dropout4 = nn.Dropout(self.dropout_value[2])\n        self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n\n        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n        self.dropout5 = nn.Dropout(self.dropout_value[3])\n        self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.leaky_relu(self.dense3(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = F.leaky_relu(self.dense4(x))\n\n        x = self.batch_norm5(x)\n        x = self.dropout5(x)\n        x = self.dense5(x)\n        return x\n\n    \n\n    \n\ndef process_data(data):\n    \n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    \n    return data\n\nseed_everything(seed=1903)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor col in (GENES + CELLS):\n\n    #kurt = max(kurtosis(train_features[col]), kurtosis(test_features[col]))\n    #QuantileTransformer_n_quantiles = n_quantile_for_kurt(kurt, calc_QT_par_kurt(QT_n_quantile_min, QT_n_quantile_max))\n    #transformer = QuantileTransformer(n_quantiles=QuantileTransformer_n_quantiles,random_state=0, output_distribution=\"normal\")\n    \n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")   # from optimal commit 9\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dimensionality Reduction with LDA 11/21 "},{"metadata":{},"cell_type":"markdown","source":"## Dimensionality Reduction with LSA  11/21"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n\nfrom sklearn.decomposition import TruncatedSVD\n\n# GENES\n\n#n_comp_GENES = 363\n#n_comp_CELLS = 60\nn_comp = 600    #363  #29  40  50  41-42  51-52\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (TruncatedSVD(n_comp, random_state=1903).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\n#CELLS\nn_comp = 50  #60  #4   7\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = ( TruncatedSVD(n_comp, random_state=1903).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\n\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dimensionality Reduction with PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#'''\n# GENES\n\n#n_comp_GENES = 363\n#n_comp_CELLS = 60\nn_comp = 600    #363  #29  40  50  41-42  51-52\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=1903).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\n#CELLS\nn_comp = 50  #60  #4   7\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=1903).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\n#'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_thresh = VarianceThreshold(threshold=0.8)\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## kmeans other people idea 11/23"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\ndef fe_cluster(train, test, n_clusters_g = 35, n_clusters_c = 5, SEED = 123):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_features ,test_features=fe_cluster(train_features,test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe_stats(train, test):\n    \n    features_g = GENES\n    features_c = CELLS\n    \n    for df in train, test:\n        df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n        df['c52_c42'] = df['c-52'] * df['c-42']\n        df['c13_c73'] = df['c-13'] * df['c-73']\n        df['c26_c13'] = df['c-23'] * df['c-13']\n        df['c33_c6']  = df['c-33'] * df['c-6']\n        df['c11_c55'] = df['c-11'] * df['c-55']\n        df['c38_c63'] = df['c-38'] * df['c-63']\n        df['c38_c94'] = df['c-38'] * df['c-94']\n        df['c13_c94'] = df['c-13'] * df['c-94']\n        df['c4_c52']  = df['c-4'] * df['c-52']\n        df['c4_c42']  = df['c-4'] * df['c-42']\n        df['c13_c38'] = df['c-13'] * df['c-38']\n        df['c55_c2']  = df['c-55'] * df['c-2']\n        df['c55_c4']  = df['c-55'] * df['c-4']\n        df['c4_c13']  = df['c-4'] * df['c-13']\n        df['c82_c42'] = df['c-82'] * df['c-42']\n        df['c66_c42'] = df['c-66'] * df['c-42']\n        df['c6_c38']  = df['c-6'] * df['c-38']\n        df['c2_c13']  = df['c-2'] * df['c-13']\n        df['c62_c42'] = df['c-62'] * df['c-42']\n        df['c90_c55'] = df['c-90'] * df['c-55']  #--→\n        df['c11_c63'] = df['c-11'] * df['c-63']\n        df['c90_c13'] = df['c-90'] * df['c-13']\n        df['c6_c55']  = df['c-6'] * df['c-55']\n        \n        \n        for feature in features_c:\n             df[f'{feature}_squared'] = df[feature] ** 2     \n                \n        for feature in gsquarecols:\n            df[f'{feature}_squared'] = df[feature] ** 2        \n        \n    return train, test\n\ntrain_features,test_features=fe_stats(train_features,test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_scored.columns]\n\ntrain = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)\n\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n#new create \ntrain = train.merge(train_targets, on='sig_id')\n\ntarget = train[train_targets.columns]\n\ntrain = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)\n\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n#new create\nfolds = train.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=4)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)\n\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = train.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=7)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3  #3\nWEIGHT_DECAY = 1e-5   #5\nNFOLDS = 7  #4\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False   #True\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=1500   #1024 1500\n\n\n\nhidden_2 =  820  #950  #920   #950  #750     780\n#hidden_3 = 630  #450    #630  #530   520   440\n#hidden_4 = 340\n\n#hidden_2 = 740  #750     780\n#hidden_3 = 390  #530   520   440\n#hidden_4 = 370\n\n#dropout_rate1': 0.23918606843969956,\n# 'dropout_rate2': 0.26413891205897316,\n# 'dropout_rate3': 0.20789442728099686,\n# 'dropout_rate4': 0.221083001443697,\n# 'hidden_2': 710.0,\n# 'hidden_3': 500.0\n\n\n#'dropout_rate1': 0.17921150354824822, \n# 'dropout_rate2': 0.3918194448036252,\n# 'dropout_rate3': 0.2357391436892218, \n# 'dropout_rate4': 0.33573309986814803, '\n#  hidden_2': 740.0, \n# 'hidden_3': 390.0,","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    seed_everything(seed)\n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size\n        \n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.05, div_factor=1.5e3, \n                                              max_lr=1e-2, epochs=EPOCHS, \n                                              steps_per_epoch=len(trainloader))\n    \n    #loss_fn = nn.BCEWithLogitsLoss()\n    loss_fn = SmoothBCEwLogits(smoothing =0.001)\n\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_fn, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):#,hidden_4,):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)#,hidden_4)\n        \n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n\n#--------------------- optuna dataset ----------------------\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=1903):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nclass MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n\n    \ndef train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        #print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n    \n\n\n    \nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size,dropout1,dropout2):#,hidden_2):#,dropout3,dropout4,\n                 #hidden_2,hidden_3):#,hidden_4):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        #self.dropout1 = nn.Dropout(dropout1)  #21\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(dropout1)  #24\n        #self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_2))\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n\n        \n        #self.batch_norm2_5 = nn.BatchNorm1d(hidden_2)\n        #self.dropout2_5 = nn.Dropout(dropout3)   #0.25\n        #self.dense2_5 = nn.utils.weight_norm(nn.Linear(hidden_2, hidden_3))\n        \n        #self.batch_norm2_6 = nn.BatchNorm1d(hidden_3)\n        #self.dropout2_6 = nn.Dropout(dropout4)\n        #self.dense2_6 = nn.utils.weight_norm(nn.Linear(hidden_3, hidden_4))        \n\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(dropout2)  #25\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        \n        x = self.batch_norm1(x)\n        #x = self.dropout1(x)\n        #x = F.relu(self.dense1(x))\n        x = F.leaky_relu(self.dense1(x))\n\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        #x = F.relu(self.dense2(x))\n        x = F.leaky_relu(self.dense2(x))\n\n        \n        \n        #x = self.batch_norm2_5(x)\n        #x = self.dropout2_5(x)\n        #x = F.relu(self.dense2_5(x))\n        #x = F.leaky_relu(self.dense2_5(x))\n\n        \n        #x = self.batch_norm2_6(x)\n        #x = self.dropout2_6(x)\n        #x = F.relu(self.dense2_6(x))        \n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        return x\n\ndef process_data(data):\n    \n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    \n    return data\n\nseed_everything(seed=1903)\n\n\n\n\ndef run_training(fold, seed,dropout1,dropout2):#,hidden_2):#,dropout3,dropout4,hidden_2,hidden_3):#,hidden_4):\n    seed_everything(seed)\n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n        dropout1=dropout1,\n        dropout2=dropout2,\n        #dropout3=dropout3,\n        #dropout4=dropout4,\n        #hidden_2=hidden_2,\n        #hidden_3=hidden_3\n        #hidden_4=hidden_4\n        \n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.05, div_factor=1.5e3, \n                                              max_lr=1e-2, epochs=EPOCHS, \n                                              steps_per_epoch=len(trainloader))\n    \n    #loss_fn = nn.BCEWithLogitsLoss()\n    loss_fn = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_fn, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n        dropout1=dropout1,\n        dropout2=dropout2,\n        #dropout3=dropout3,\n        #dropout4=dropout4,\n        #hidden_2=hidden_2,\n        #hidden_3=hidden_3\n       # hidden_4=hidden_4\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions\n\n\n\n\n\ndef run_k_fold(NFOLDS, seed,dropout1,dropout2):#,hidden_2):#,dropout3,dropout4,hidden_2,hidden_3):#,hidden_4,):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed,dropout1,dropout2)#,hidden_2)#,dropout3,dropout4,hidden_2,hidden_3)#,hidden_4)\n        \n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions\n    \n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.nn.modules.loss import _WeightedLoss\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    \nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n            \n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SEED = [1903, 1881]\nSEED = [0, 1, 2, 3, 4, 5, 6]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ / target.shape[1]\n    \nprint(\"CV log_loss: \", score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n#-------------------------------- optuna ------------------------\ndef objective(trial):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    #hidden_size      =  int(trial.suggest_discrete_uniform('hidden_size',1100,1500,50))  #780,1000,20    580,830,20\n\n    dropout_rate1 =  trial.suggest_uniform('dropout_rate1', 0.1, 0.4)\n    dropout_rate2 =  trial.suggest_uniform('dropout_rate2', 0.1, 0.4)\n    #dropout_rate3 =  trial.suggest_uniform('dropout_rate3', 0.1, 0.4)\n    #dropout_rate4 =  trial.suggest_uniform('dropout_rate4', 0.1, 0.4)\n    #hidden_2      =  int(trial.suggest_discrete_uniform('hidden_2',500,1500,50))  #580,830,20\n    #hidden_3      =  int(trial.suggest_discrete_uniform('hidden_3',330,700,20))  #330,500,20\n    #hidden_4     =  int(trial.suggest_discrete_uniform('hidden_4',300,500,100))\n    #weight_decay  =  trial.suggest_loguniform('weight_decay', 1e-10, 1e-3)\n\n\n\n        \n    SEED = [1903, 1881]\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n\n    for seed in SEED:\n        oof_, predictions_ = run_k_fold(NFOLDS, seed,\n                                        #hidden_size,\n                                        dropout_rate1,\n                                        dropout_rate2,\n                                        #dropout_rate3,\n                                        #dropout_rate4,\n                                        #hidden_2,\n                                        #hidden_3,\n                                        #hidden_4\n                                        \n                                       )\n        oof += oof_ / len(SEED)\n        predictions += predictions_ / len(SEED)\n\n    train[target_cols] = oof\n    test[target_cols] = predictions\n    \n    valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n    #valid_results = train_targets.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n\n\n    y_true = train_targets_scored[target_cols].values\n    y_pred = valid_results[target_cols].values\n\n    score = 0\n    for i in range(len(target_cols)):\n        score_ = log_loss(y_true[:, i], y_pred[:, i])\n        score += score_ / target.shape[1]\n\n    #print(\"CV log_loss: \", score)\n\n    return score\n    \n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n\nimport optuna\n\nTRIAL_SIZE = 30\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=TRIAL_SIZE)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#memo\n#次回やること\n#hidden_size 固定でdropoutのみをtuning","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nparameters: {'dropout_rate1': 0.34033737675224757,\n'dropout_rate2': 0.3033653487000471,\n'hidden_2': 820.0}. Best is trial 15 with value: 0.014656699010826262.\n\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n\nstudy.best_params\n\n'dropout_rate1': 0.15437954477482482, \n'dropout_rate2': 0.22534119361959673, \n'dropout_rate3': 0.1368538728077269, \n'dropout_rate4': 0.19597838532007975, \n'hidden_2': 720.0, \n'hidden_3': 410.0\n}. Best is trial 26 with value: 0.014628153529320521.\n\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n\ndropout_rate1': 0.23918606843969956,\n 'dropout_rate2': 0.26413891205897316,\n 'dropout_rate3': 0.20789442728099686,\n 'dropout_rate4': 0.221083001443697,\n 'hidden_2': 710.0,\n 'hidden_3': 500.0\n \n \n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nthe latest parameters for optuna 11/07\n\n\nTrial 5 finished with value: 0.01477755571674698 and parameters: {\n'dropout_rate1': 0.3036410351072955, \n'dropout_rate2': 0.21090449239303233, \n'dropout_rate3': 0.17015128906288302,\n'dropout_rate4': 0.27918373084339765,\n'hidden_2': 920.0,\n'hidden_3': 450.0}.\nBest is trial 0 with value: 0.014703993215866778.\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nparameters: {'dropout_rate1': 0.17921150354824822, \n             'dropout_rate2': 0.3918194448036252,\n             'dropout_rate3': 0.2357391436892218, \n             'dropout_rate4': 0.33573309986814803, '\n             hidden_2': 740.0, 'hidden_3': 390.0,\n             'weight_decay': 1.8479208843258598e-06}.\n    Best is trial 5 with value: 0.014696440237529178.\n    \n    \n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n\ndef run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n   \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions\n    \n    \n'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}