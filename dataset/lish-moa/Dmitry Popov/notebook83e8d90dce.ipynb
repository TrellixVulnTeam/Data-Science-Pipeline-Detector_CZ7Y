{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-10-22T16:29:52.373945Z","iopub.status.busy":"2020-10-22T16:29:52.373166Z","iopub.status.idle":"2020-10-22T16:29:54.470326Z","shell.execute_reply":"2020-10-22T16:29:54.469159Z"},"papermill":{"duration":2.114249,"end_time":"2020-10-22T16:29:54.470452","exception":false,"start_time":"2020-10-22T16:29:52.356203","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\nimport sklearn.preprocessing\nfrom sklearn.metrics import log_loss\nfrom tqdm.notebook import tqdm\nimport operator\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport random\nimport os\nimport sys\nfrom pathlib import Path\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nfrom sklearn.utils import check_random_state\nfrom sklearn.utils.validation import _num_samples, check_array\nfrom sklearn.utils.multiclass import type_of_target\n\nfrom sklearn.model_selection._split import _BaseKFold, _RepeatedSplits, \\\n    BaseShuffleSplit, _validate_shuffle_split\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 42\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nset_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom joblib import Parallel, delayed\nfrom scipy.interpolate import interp1d\nfrom scipy.special import erf, erfinv\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils.validation import FLOAT_DTYPES, check_array, check_is_fitted\n\n\nclass GaussRankScaler(BaseEstimator, TransformerMixin):\n    \"\"\"Transform features by scaling each feature to a normal distribution.\n    Parameters\n        ----------\n        epsilon : float, optional, default 1e-4\n            A small amount added to the lower bound or subtracted\n            from the upper bound. This value prevents infinite number\n            from occurring when applying the inverse error function.\n        copy : boolean, optional, default True\n            If False, try to avoid a copy and do inplace scaling instead.\n            This is not guaranteed to always work inplace; e.g. if the data is\n            not a NumPy array, a copy may still be returned.\n        n_jobs : int or None, optional, default None\n            Number of jobs to run in parallel.\n            ``None`` means 1 and ``-1`` means using all processors.\n        interp_kind : str or int, optional, default 'linear'\n           Specifies the kind of interpolation as a string\n            ('linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',\n            'previous', 'next', where 'zero', 'slinear', 'quadratic' and 'cubic'\n            refer to a spline interpolation of zeroth, first, second or third\n            order; 'previous' and 'next' simply return the previous or next value\n            of the point) or as an integer specifying the order of the spline\n            interpolator to use.\n        interp_copy : bool, optional, default False\n            If True, the interpolation function makes internal copies of x and y.\n            If False, references to `x` and `y` are used.\n        Attributes\n        ----------\n        interp_func_ : list\n            The interpolation function for each feature in the training set.\n        \"\"\"\n\n    def __init__(self, epsilon=1e-4, copy=True, n_jobs=None, interp_kind='linear', interp_copy=False):\n        self.epsilon = epsilon\n        self.copy = copy\n        self.interp_kind = interp_kind\n        self.interp_copy = interp_copy\n        self.fill_value = 'extrapolate'\n        self.n_jobs = n_jobs\n\n    def fit(self, X, y=None):\n        \"\"\"Fit interpolation function to link rank with original data for future scaling\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The data used to fit interpolation function for later scaling along the features axis.\n        y\n            Ignored\n        \"\"\"\n        X = check_array(X, copy=self.copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n\n        self.interp_func_ = Parallel(n_jobs=self.n_jobs)(delayed(self._fit)(x) for x in X.T)\n        return self\n\n    def _fit(self, x):\n        x = self.drop_duplicates(x)\n        rank = np.argsort(np.argsort(x))\n        bound = 1.0 - self.epsilon\n        factor = np.max(rank) / 2.0 * bound\n        scaled_rank = np.clip(rank / factor - bound, -bound, bound)\n        return interp1d(\n            x, scaled_rank, kind=self.interp_kind, copy=self.interp_copy, fill_value=self.fill_value)\n\n    def transform(self, X, copy=None):\n        \"\"\"Scale the data with the Gauss Rank algorithm\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The data used to scale along the features axis.\n        copy : bool, optional (default: None)\n            Copy the input X or not.\n        \"\"\"\n        check_is_fitted(self, 'interp_func_')\n\n        copy = copy if copy is not None else self.copy\n        X = check_array(X, copy=copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n\n        X = np.array(Parallel(n_jobs=self.n_jobs)(delayed(self._transform)(i, x) for i, x in enumerate(X.T))).T\n        return X\n\n    def _transform(self, i, x):\n        return erfinv(self.interp_func_[i](x))\n\n    def inverse_transform(self, X, copy=None):\n        \"\"\"Scale back the data to the original representation\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data used to scale along the features axis.\n        copy : bool, optional (default: None)\n            Copy the input X or not.\n        \"\"\"\n        check_is_fitted(self, 'interp_func_')\n\n        copy = copy if copy is not None else self.copy\n        X = check_array(X, copy=copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n\n        X = np.array(Parallel(n_jobs=self.n_jobs)(delayed(self._inverse_transform)(i, x) for i, x in enumerate(X.T))).T\n        return X\n\n    def _inverse_transform(self, i, x):\n        inv_interp_func = interp1d(self.interp_func_[i].y, self.interp_func_[i].x, kind=self.interp_kind,\n                                   copy=self.interp_copy, fill_value=self.fill_value)\n        return inv_interp_func(erf(x))\n\n    @staticmethod\n    def drop_duplicates(x):\n        is_unique = np.zeros_like(x, dtype=bool)\n        is_unique[np.unique(x, return_index=True)[1]] = True\n        return x[is_unique]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"import ctypes\nctypes.cdll.LoadLibrary('caffe2_nvrtc.dll')"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_min = 1e-15\np_max = 1 - p_min\n\ndef score(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    y_pred = np.clip(y_pred, p_min, p_max)\n    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = pd.read_csv('../input/lish-moa/train_features.csv', index_col='sig_id')\ntest_Y = pd.read_csv('../input/lish-moa/sample_submission.csv', index_col='sig_id')\ntrain_Y = pd.read_csv('../input/lish-moa/train_targets_scored.csv', index_col='sig_id', dtype={f: test_Y.dtypes[f] for f in test_Y})\ntest_X = pd.read_csv('../input/lish-moa/test_features.csv', index_col='sig_id')\ndrug_ids = pd.read_csv('../input/lish-moa/train_drug.csv', index_col='sig_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.cp_time = train_X.cp_time / 24\ntest_X.cp_time = test_X.cp_time / 24\n\ntrain_X['real_drug'] = train_X.cp_type == 'trt_cp'\ntest_X['real_drug'] = test_X.cp_type == 'trt_cp'\n\nt = train_X.cp_dose.copy()\ntrain_X.drop(columns=['cp_dose', 'cp_type'], inplace=True)\ntrain_X['cp_dose'] = 1\ntrain_X.loc[(t == 'D2'), 'cp_dose'] = 2\n\nt = test_X.cp_dose.copy()\ntest_X.drop(columns=['cp_dose', 'cp_type'], inplace=True)\ntest_X['cp_dose'] = 1\ntest_X.loc[(t == 'D2'), 'cp_dose'] = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nfolds = 12\nnstarts = 1\nnepochs = 50\nbatch_size = 128\nval_batch_size = batch_size * 4\ncriterion = nn.BCELoss()\nkfold = GroupKFold(n_splits=nfolds)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T16:30:02.71643Z","iopub.status.busy":"2020-10-22T16:30:02.715242Z","iopub.status.idle":"2020-10-22T16:30:02.717724Z","shell.execute_reply":"2020-10-22T16:30:02.718219Z"},"papermill":{"duration":0.019882,"end_time":"2020-10-22T16:30:02.718337","exception":false,"start_time":"2020-10-22T16:30:02.698455","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class Dataset_my(Dataset):\n    def __init__(self, df, targets, mode='train'):\n        self.mode = mode\n        #self.feats = feats_idx\n        #self.data = df[:, feats_idx]\n        self.data = df\n        if mode=='train':\n            self.targets = targets\n    \n    def __getitem__(self, idx):\n        if self.mode == 'train':\n            return torch.FloatTensor(self.data[idx]), torch.FloatTensor(self.targets[idx])\n        elif self.mode == 'test':\n            return torch.FloatTensor(self.data[idx]), 0\n        \n    def __len__(self):\n        return len(self.data)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T16:30:02.755896Z","iopub.status.busy":"2020-10-22T16:30:02.754888Z","iopub.status.idle":"2020-10-22T16:37:28.495451Z","shell.execute_reply":"2020-10-22T16:37:28.496258Z"},"papermill":{"duration":445.769273,"end_time":"2020-10-22T16:37:28.4965","exception":false,"start_time":"2020-10-22T16:30:02.727227","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def run_CV_for_model(cur_model, model_num, train_X_loc, train_Y_loc, test_X_loc, labels_loc):\n    set_seed(seed)\n    if len(train_X_loc) > len(labels_loc):\n        t = len(labels_loc)\n        labels_loc = np.resize(labels_loc, train_X_loc.shape[0])\n        labels_loc[t:] = placebo_label\n        \n    for n, (tr, te) in enumerate(kfold.split(train_Y_loc, train_Y_loc, groups=labels_loc)):\n        print(f'Train fold {n+1}')\n        xtrain, xval = train_X_loc[tr], train_X_loc[te]\n        ytrain, yval = train_Y_loc[tr], train_Y_loc[te]\n\n        train_set = Dataset_my(xtrain, ytrain)\n        val_set = Dataset_my(xval, yval)\n\n        dataloaders = {\n            'train': DataLoader(train_set, batch_size=batch_size, shuffle=True),\n            'val': DataLoader(val_set, batch_size=val_batch_size, shuffle=False)\n        }\n\n        model = cur_model(train_X_loc.shape[1]).to(device)\n        Path(f'./saved_params/model{model_num}').mkdir(parents=True, exist_ok=True)\n        checkpoint_path = f'./saved_params/model{model_num}/repeat_{1}_Fold_{n+1}.pt'\n        optimizer = optim.Adam(model.parameters(), weight_decay=1e-5)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, eps=1e-4, verbose=True)\n        best_loss = {'train': np.inf, 'val': np.inf}\n\n        for epoch in range(nepochs):\n            epoch_loss = {'train': 0.0, 'val': 0.0}\n\n            for phase in ['train', 'val']:\n                if phase == 'train':\n                    model.train()\n                else:\n                    model.eval()\n\n                running_loss = 0.0\n\n                for i, (x, y) in enumerate(dataloaders[phase]):\n                    x, y = x.to(device), y.to(device)\n\n                    optimizer.zero_grad()\n\n                    with torch.set_grad_enabled(phase=='train'):\n                        preds = model(x)\n                        loss = criterion(preds, y)\n\n                        if phase=='train':\n                            loss.backward()\n                            optimizer.step()\n\n                    running_loss += loss.item() / len(dataloaders[phase])\n\n                epoch_loss[phase] = running_loss\n\n            print(\"Epoch {}/{}   -   loss: {:5.5f}   -   val_loss: {:5.5f}\".format(epoch+1, nepochs, epoch_loss['train'], epoch_loss['val']))\n\n            scheduler.step(epoch_loss['val'])\n\n            if epoch_loss['val'] < best_loss['val']:\n                best_loss = epoch_loss\n                torch.save(model.state_dict(), checkpoint_path)\n    return best_loss","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T16:37:29.873707Z","iopub.status.busy":"2020-10-22T16:37:29.872605Z","iopub.status.idle":"2020-10-22T16:37:34.418673Z","shell.execute_reply":"2020-10-22T16:37:34.419312Z"},"papermill":{"duration":4.694514,"end_time":"2020-10-22T16:37:34.419497","exception":false,"start_time":"2020-10-22T16:37:29.724983","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def run_predict_for_model(cur_model, model_num, test_X_loc):\n    preds = np.zeros((test_X_loc.shape[0], test_Y.shape[1], nfolds))\n    \n    for n in range(nfolds):\n        test_set = Dataset_my(test_X_loc, None, mode='test')\n        dataloader = DataLoader(test_set, batch_size=val_batch_size, shuffle=False)\n        \n        checkpoint_path = f'../input/notebook83e8d90dce/saved_params/model{model_num}/repeat_{1}_Fold_{n+1}.pt'\n        model = cur_model(test_X_loc.shape[1]).to(device)\n        model.load_state_dict(torch.load(checkpoint_path))\n        model.eval()\n        \n        fold_preds = []\n        for i, (x, y) in enumerate(dataloader):\n            x = x.to(device)\n\n            with torch.no_grad():\n                fold_preds.append(model(x))\n            \n        fold_preds = torch.cat(fold_preds, dim=0).cpu().numpy()\n        preds[:, :, n] = fold_preds\n    preds = preds.mean(axis=2)\n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T16:30:02.686535Z","iopub.status.busy":"2020-10-22T16:30:02.684652Z","iopub.status.idle":"2020-10-22T16:30:02.687231Z","shell.execute_reply":"2020-10-22T16:30:02.687689Z"},"papermill":{"duration":0.025318,"end_time":"2020-10-22T16:30:02.687824","exception":false,"start_time":"2020-10-22T16:30:02.662506","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class Model4(nn.Module):\n    def __init__(self, num_columns):\n        super(Model4, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_columns)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_columns, 2048))\n        \n        self.batch_norm2 = nn.BatchNorm1d(2048)\n        self.dropout2 = nn.Dropout(0.5)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(2048, 1024))\n        \n        self.batch_norm3 = nn.BatchNorm1d(1024)\n        self.dropout3 = nn.Dropout(0.5)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(1024, 206))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = torch.sigmoid(self.dense3(x))\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-22T16:30:01.961409Z","iopub.status.busy":"2020-10-22T16:30:01.960089Z","iopub.status.idle":"2020-10-22T16:30:02.200795Z","shell.execute_reply":"2020-10-22T16:30:02.199707Z"},"papermill":{"duration":0.257815,"end_time":"2020-10-22T16:30:02.200918","exception":false,"start_time":"2020-10-22T16:30:01.943103","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"t = test_Y[test_X['real_drug'] == False].copy()\nfor f in t:\n    t[f] = 0\ntrain_Y4 = train_Y.reset_index(drop=True).append(t)\ntrain_X4 = train_X.reset_index(drop=True)\ntest_X4 = test_X\n\nall_X4 = train_X4.append(test_X4).drop(columns=['real_drug'])\n\nfeatures_g = [col for col in train_X4.columns if 'g-' in col]\nfeatures_c = [col for col in train_X4.columns if 'c-' in col]\n\nall_X4['g_sum'] = all_X4[features_g].sum(axis = 1)\nall_X4['g_mean'] = all_X4[features_g].mean(axis = 1)\nall_X4['g_std'] = all_X4[features_g].std(axis = 1)\nall_X4['g_kurt'] = all_X4[features_g].kurtosis(axis = 1)\nall_X4['g_skew'] = all_X4[features_g].skew(axis = 1)\nall_X4['c_sum'] = all_X4[features_c].sum(axis = 1)\nall_X4['c_mean'] = all_X4[features_c].mean(axis = 1)\nall_X4['c_std'] = all_X4[features_c].std(axis = 1)\nall_X4['c_kurt'] = all_X4[features_c].kurtosis(axis = 1)\nall_X4['c_skew'] = all_X4[features_c].skew(axis = 1)\nall_X4['gc_sum'] = all_X4[features_g + features_c].sum(axis = 1)\nall_X4['gc_mean'] = all_X4[features_g + features_c].mean(axis = 1)\nall_X4['gc_std'] = all_X4[features_g + features_c].std(axis = 1)\nall_X4['gc_kurt'] = all_X4[features_g + features_c].kurtosis(axis = 1)\nall_X4['gc_skew'] = all_X4[features_g + features_c].skew(axis = 1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = GaussRankScaler()\nall_X4 = scaler.fit_transform(all_X4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_transformer = PCA(687)\nall_X4 = pca_transformer.fit_transform(all_X4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X4 = all_X4[:train_X4.shape[0]]\ntest_X4 = all_X4[train_X4.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X4 = np.vstack([train_X4, test_X4[test_X['real_drug'] == False]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha_smoothing = 0.001\ntrain_Y4 = train_Y4.values\ntrain_Y4 = (1 - alpha_smoothing) * train_Y4 + alpha_smoothing * train_Y4.mean(axis=1)[:, None]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = sklearn.preprocessing.LabelEncoder().fit(drug_ids)\ndrug_labels = encoder.transform(drug_ids)\nplacebo_label = encoder.transform(['cacb2b860'])[0]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"run_CV_for_model(Model4, 4, train_X4, train_Y4, test_X4, drug_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_P4 = run_predict_for_model(Model4, 4, train_X4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_P4.shape, train_Y4.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score(train_Y4, train_P4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_P4[:train_X.shape[0]][train_X['real_drug'] == False] = 0\ntrain_P4[train_X.shape[0]] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score(train_Y4, train_P4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_P = run_predict_for_model(Model4, 4, test_X4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_Y[list(test_Y.columns)] = test_P\ntest_Y[test_X['real_drug'] == False] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_Y.values.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(test_Y.values ** 0.5).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l = 0.5\nr = 1\nwhile r - l > 0.000001:\n    m = (l + r) / 2\n    if (test_Y.values ** m).mean() > 0.003785:\n        l = m\n    else:\n        r = m","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(test_Y.values ** r).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_Y[list(test_Y.columns)] = test_Y ** r","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_Y.values.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_Y.values.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_Y.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}