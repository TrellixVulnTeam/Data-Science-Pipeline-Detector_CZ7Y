{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Importing libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow.keras.backend as K\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.losses import BinaryCrossentropy\nimport tensorflow_addons as tfa\n\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nfrom sklearn import preprocessing\n\n\nfrom tqdm.notebook import tqdm\n\nimport math\nfrom sklearn.preprocessing import StandardScaler\npd.options.display.max_columns = None\nimport tensorflow_addons as tfa","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading train and test data"},{"metadata":{},"cell_type":"markdown","source":"train_features.csv - Features for the training set. Features g- signify gene expression data, and c- signify cell viability data. cp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs; cp_time and cp_dose indicate treatment duration (24, 48, 72 hours) and dose (high or low).\n\ntest_features.csv - Features for the test data. You must predict the probability of each scored MoA for each row in the test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ntest_features = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\n\ntrain_features['dataset'] = 'train'\ntest_features['dataset'] = 'test'\n\ndf = pd.concat([train_features, test_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of rows in training set:', train_features.shape[0])\nprint('Number of columns in training set:', train_features.shape[1] - 1)\n\nprint('Number of rows in test set:', test_features.shape[0])\nprint('Number of columns in test set:', test_features.shape[1] - 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that we have 872 float features 1 integer (cp_time) and 3 categorical (sig_id, cp_type and cp_dose)."},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\nsample_submission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categories Visualization"},{"metadata":{},"cell_type":"markdown","source":"Here we are going to check categorical features:\n\n- Features g- signify gene expression data.\n\n- Features c- signify cell viability data.\n\n- cp_type indicates samples treated with a compound, trt_cp samples treated with the compounds.\n\n- cp_vehicle or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs.\n\n- cp_time and cp_dose indicate treatment duration (24, 48, 72 hours) and dose (high or low)."},{"metadata":{"trusted":true},"cell_type":"code","source":"cp_width = 500\ncp_height = 400\nscatter_size = 600","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds = df.groupby(['cp_type', 'dataset'])['sig_id'].count().reset_index()\nds.columns = ['cp_type', 'dataset', 'count']\n\nfig = px.bar(\n    ds, \n    x='cp_type', \n    y=\"count\", \n    color='dataset',\n    barmode='group',\n    orientation='v', \n    title='cp_type train/test counts', \n    width=cp_width,\n    height=cp_height\n)\n\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds = df.groupby(['cp_time', 'dataset'])['sig_id'].count().reset_index()\nds.columns = [\n    'cp_time', \n    'dataset', \n    'count'\n]\n\nfig = px.bar(\n    ds, \n    x='cp_time', \n    y=\"count\", \n    color='dataset',\n    barmode='group',\n    orientation='v', \n    title='cp_time train/test counts', \n    width=cp_width,\n    height=cp_height\n)\n\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds = df.groupby(['cp_dose', 'dataset'])['sig_id'].count().reset_index()\nds.columns = [\n    'cp_dose', \n    'dataset', \n    'count'\n]\n\nfig = px.bar(\n    ds, \n    x='cp_dose', \n    y=\"count\", \n    color='dataset',\n    barmode='group',\n    orientation='v', \n    title='cp_dose train/test counts', \n    width=cp_width,\n    height=cp_height\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,5))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((1,2),(0,0))\nsns.countplot(x='cp_type', data=train_features, palette='rainbow', alpha=0.75)\nplt.title('Train: Control and treated samples', fontsize=15, weight='bold')\n#first row sec col\nax1 = plt.subplot2grid((1,2),(0,1))\nsns.countplot(x='cp_dose', data=train_features, palette='Purples', alpha=0.75)\nplt.title('Train: Treatment Doses: Low and High',weight='bold', fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.distplot( train_features['cp_time'], color='red', bins=5)\nplt.title(\"Train: Treatment duration \", fontsize=15, weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion:\n\n- Few control samples.\n- The low and high doses were applied equally.\n- 3 treatment durations: 24h, 48h and 72h."},{"metadata":{},"cell_type":"markdown","source":"### Gene and cell features distribution"},{"metadata":{},"cell_type":"markdown","source":"Some distribution of randomly selected columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_columns = train_features.columns.to_list()\ng_list = [i for i in train_columns if i.startswith('g-')]\nc_list = [i for i in train_columns if i.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_set_histograms(plot_list, title):\n    fig = make_subplots(rows=6, cols=2)\n    traces = [\n        go.Histogram(x=train_features[col], nbinsx=100, name=col) for col in plot_list\n    ]\n\n    for i in range(len(traces)):\n        fig.append_trace(\n            traces[i], \n            (i // 2) + 1, \n            (i % 2) + 1\n        )\n\n    fig.update_layout(\n        title_text=title,\n        height=1000,\n        width=800\n    )\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_list = [\n    g_list[np.random.randint(0, len(g_list)-1)] for i in range(50)\n]\nplot_list = list(set(plot_list))[:12]\nplot_set_histograms(plot_list, 'Randomly selected gene expression features distributions')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_list = [\n    c_list[np.random.randint(0, len(c_list)-1)] for i in range(50)\n]\nplot_list = list(set(plot_list))[:12]\nplot_set_histograms(plot_list, 'Randomly selected cell expression features distributions')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training features correlation"},{"metadata":{},"cell_type":"markdown","source":"Let's see some correlation between randomly selected variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = g_list + c_list\nfor_correlation = list(set([columns[np.random.randint(0, len(columns)-1)] for i in range(200)]))[:40]\ndata = df[for_correlation]\n\nf = plt.figure(figsize=(18, 18))\nplt.matshow(data.corr(), fignum=f.number)\nplt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=50)\nplt.yticks(range(data.shape[1]), data.columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=13)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time taken to find pairs of features with high correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ncols = ['cp_time'] + columns\nall_columns = []\nfor i in range(0, len(cols)):\n    for j in range(i+1, len(cols)):\n        if abs(train_features[cols[i]].corr(train_features[cols[j]])) > 0.9:\n            all_columns = all_columns + [cols[i], cols[j]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_columns = list(set(all_columns))\nprint('Number of columns:', len(all_columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In total we have 35 columns that have correlation with at least another 1 higher than 0.9. Let's visualize them."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[all_columns]\n\nf = plt.figure(figsize=(18, 18))\nplt.matshow(data.corr(), fignum=f.number)\nplt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=50)\nplt.yticks(range(data.shape[1]), data.columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=12, cols=3)\n\ntraces = [\n    go.Histogram(x=train_features[col], nbinsx=100, name=col) for col in all_columns\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i // 3) + 1, \n        (i % 3) + 1\n    )\n\nfig.update_layout(\n    title_text='Highly correlated features',\n    height=1200\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Targets analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\nprint('Number of rows: ', train_targets.shape[0])\nprint('Number of cols: ', train_targets.shape[1])\n\ntrain_targets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train_targets.drop(['sig_id'], axis=1).sum(axis=0).sort_values().reset_index()\nx.columns = [\n    'column', \n    'nonzero_records'\n]\nx = x.tail(50)\n\nfig = px.bar(\n    x, \n    x='nonzero_records', \n    y='column', \n    orientation='h', \n    title='Columns with the higher number of positive samples (top 50)', \n    width=800,\n    height=1000\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train_targets.drop(['sig_id'], axis=1).sum(axis=0).sort_values(ascending=False).reset_index()\nx.columns = [\n    'column', \n    'nonzero_records'\n]\nx = x.tail(50)\n\nfig = px.bar(\n    x, \n    x='nonzero_records', \n    y='column', \n    orientation='h', \n    title='Columns with the lowest number of positive samples (top 50)', \n    width=800,\n    height=1000 \n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that at least 50 target columns have number of positive samples less than 20 (about 0.1%)"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train_targets.drop(['sig_id'], axis=1).sum(axis=0).sort_values(ascending=False).reset_index()\nx.columns = ['column', 'count']\nx['count'] = x['count'] * 100 / len(train_targets)\n\nfig = px.bar(\n    x, \n    x='column', \n    y='count', \n    orientation='v', \n    title='Percent of positive records for every column in target', \n    width=1200,\n    height=800 \n)\n\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The biggest number of positive samples for 1 target column is 3.5%. So we deal here with highly imbalanced data."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train_targets.drop(['sig_id'], axis=1).astype(bool).sum(axis=1).reset_index()\ndata.columns = ['row', 'count']\ndata = data.groupby(['count'])['row'].count().reset_index()\n\nfig = px.bar(\n    data, \n    y=data['row'], \n    x=\"count\", \n    title='Number of activations in targets for every sample', \n    width=800, \n    height=500\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train_targets.drop(['sig_id'], axis=1).astype(bool).sum(axis=1).reset_index()\ndata.columns = ['row', 'count']\ndata = data.groupby(['count'])['row'].count().reset_index()\n\nfig = px.pie(\n    data, \n    values=100 * data['row'] / len(train_targets), \n    names=\"count\", \n    title='Number of activations in targets for every sample (Percent)', \n    width=800, \n    height=500\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see here that about 40% of sample have zeros in all columns and more than 50% have only one active target column."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train and Targets correlations"},{"metadata":{},"cell_type":"markdown","source":"Time taken to find the most correlated features for every target column."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ncorrelation_matrix = pd.DataFrame()\n\nfor t_col in train_targets.columns:\n    corr_list = list()\n    if t_col == 'sig_id':\n        continue\n    for col in columns:\n        res = train_features[col].corr(train_targets[t_col])\n        corr_list.append(res)\n    correlation_matrix[t_col] = corr_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_matrix['train_features'] = columns\ncorrelation_matrix = correlation_matrix.set_index('train_features')\n\ncorrelation_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see what is the higher value (absolute) of correlation for target columns with every column from train set. Every column on chart is max correlation of current target column with all of columns from training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"maxCol=lambda x: max(x.min(), x.max(), key=abs)\n\nhigh_scores = correlation_matrix.apply(maxCol, axis=0).reset_index()\nhigh_scores.columns = [\n    'column', \n    'best_correlation'\n]\n\nfig = px.bar(\n    high_scores, \n    x='column', \n    y=\"best_correlation\", \n    orientation='v', \n    title='Best correlation with train columns for every target column', \n    width=1200,\n    height=800\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's see what columns from training set have the higher number of \"high\" correlations with target columns. Every row from chart means that column A N times has the best value of correlation with different target columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"col_df = pd.DataFrame()\ntr_cols = list()\ntar_cols = list()\n\nfor col in correlation_matrix.columns:\n    tar_cols.append(col)\n    tr_cols.append(\n        correlation_matrix[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(1).values[0]\n    )\n\ncol_df['column'] = tar_cols\ncol_df['train_best_column'] = tr_cols\n\ntotal_scores = pd.merge(high_scores, col_df)\n\ntotal_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_features = total_scores['train_best_column'].value_counts().reset_index().sort_values('train_best_column')\ncount_features.columns = ['column', 'count']\ncount_features = count_features.tail(33)\n\nfig = px.bar(\n    count_features, \n    x='count', \n    y=\"column\", \n    orientation='h', \n    title='Columns from training set with number of high correlations with target columns', \n    width=800,\n    height=700\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target columns and pairs of highly correlated features.\n\nLet's select some random columns and see how they deal with pairs of the highly correlated features."},{"metadata":{"trusted":true},"cell_type":"code","source":"target_columns = train_targets.columns.tolist()\ntarget_columns.remove('sig_id')\nfor_analysis = [\n    target_columns[np.random.randint(0, len(target_columns)-1)] for i in range(5)\n]\ncurrent_corr = correlation_matrix[for_analysis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_df = pd.DataFrame()\ntr_first_cols = list()\ntr_second_cols = list()\ntar_cols = list()\n\nfor col in current_corr.columns:\n    tar_cols.append(col)\n    tr_first_cols.append(\n        current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(2).values[0]\n    )\n    tr_second_cols.append(\n        current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(2).values[1]\n    )\n\ncol_df['column'] = tar_cols\ncol_df['train_1_column'] = tr_first_cols\ncol_df['train_2_column'] = tr_second_cols\n\ncol_df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_scatter(col_df, index):\n    analysis = pd.DataFrame()\n    analysis['color'] = train_targets[col_df.iloc[index]['column']]\n    analysis['x'] = train_features[col_df.iloc[index]['train_1_column']]\n    analysis['y'] = train_features[col_df.iloc[index]['train_2_column']]\n    analysis.columns = [\n        'color', \n        col_df.iloc[index]['train_1_column'], \n        col_df.iloc[index]['train_2_column']\n    ]\n    analysis['size'] = 1\n    analysis.loc[analysis['color'] == 1, 'size'] = 12\n\n    fig = px.scatter(\n        analysis, \n        x=col_df.iloc[index]['train_1_column'], \n        y=col_df.iloc[index]['train_2_column'], \n        color=\"color\", \n        size='size', \n        width=scatter_size,\n        height=scatter_size,\n        title='Scatter plot for ' + col_df.iloc[index]['column']\n    )\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_scatter(col_df, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_scatter(col_df, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_scatter(col_df, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's do the same but for 3d plots."},{"metadata":{"trusted":true},"cell_type":"code","source":"for_analysis = [\n    target_columns[np.random.randint(0, len(target_columns)-1)] for i in range(5)\n]\ncurrent_corr = correlation_matrix[for_analysis]\n\ncol_df = pd.DataFrame()\ntr_first_cols = list()\ntr_second_cols = list()\ntr_third_cols = list()\ntar_cols = list()\n\nfor col in current_corr.columns:\n    tar_cols.append(col)\n    tr_first_cols.append(\n        current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(3).values[0]\n    )\n    tr_second_cols.append(\n        current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(3).values[1]\n    )\n    tr_third_cols.append(\n        current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(3).values[2]\n    )\n\ncol_df['column'] = tar_cols\ncol_df['train_1_column'] = tr_first_cols\ncol_df['train_2_column'] = tr_second_cols\ncol_df['train_3_column'] = tr_third_cols\n\ncol_df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_3dscatter(col_df, index):\n    analysis = pd.DataFrame()\n    analysis['color'] = train_targets[col_df.iloc[index]['column']]\n    analysis['x'] = train_features[col_df.iloc[index]['train_1_column']]\n    analysis['y'] = train_features[col_df.iloc[index]['train_2_column']]\n    analysis['z'] = train_features[col_df.iloc[index]['train_3_column']]\n    analysis.columns = [\n        'color', \n        col_df.iloc[index]['train_1_column'], \n        col_df.iloc[index]['train_2_column'], \n        col_df.iloc[index]['train_3_column']\n    ]\n    analysis['size'] = 1\n    analysis.loc[analysis['color'] == 1, 'size'] = 20\n\n    fig = px.scatter_3d(\n        analysis, \n        x=col_df.iloc[index]['train_1_column'], \n        y=col_df.iloc[index]['train_2_column'],\n        z=col_df.iloc[index]['train_3_column'], \n        color=\"color\", \n        size='size', \n        height=scatter_size,\n        width=scatter_size,\n        title='Scatter plot for ' + col_df.iloc[index]['column']\n    )\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_3dscatter(col_df, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_3dscatter(col_df, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_3dscatter(col_df, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can extract several group names from target column names. Looks like that last term in column name is definition of a group. Let's extact them and visualize groups with number of columns > 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"last_term = dict()\n\nfor item in target_columns:\n    try:\n        last_term[item.split('_')[-1]] += 1\n    except:\n        last_term[item.split('_')[-1]] = 1\n\nlast_term = pd.DataFrame(last_term.items(), columns=['group', 'count'])\nlast_term = last_term.sort_values('count')\nlast_term = last_term[last_term['count']>1]\nlast_term['count'] = last_term['count'] * 100 / 206\n\nfig = px.bar(\n    last_term, \n    x='count', \n    y=\"group\", \n    orientation='h', \n    title='Groups in target columns (Percent from all target columns)', \n    width=800,\n    height=500\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of activation for 1 sample in every group"},{"metadata":{"trusted":true},"cell_type":"code","source":"answer = list()\n\nfor group in last_term.group.tolist():\n    agent_list = list()\n    for item in target_columns:\n        if item.split('_')[-1] == group:\n            agent_list.append(item)\n    agent_df = train_targets[agent_list]\n    data = agent_df.astype(bool).sum(axis=1).reset_index()\n    answer.append(data[0].max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds = pd.DataFrame()\nds['group'] = last_term.group.tolist()\nds['max_value'] = answer\n\nfig = px.bar(\n    ds, \n    x='max_value', \n    y=\"group\", \n    orientation='h', \n    title='Maximum number of active columns in 1 sample for every group', \n    width=800,\n    height=500\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that for groups activator, agent, blocker maximum number of active columns in sample is 1."},{"metadata":{},"cell_type":"markdown","source":"### Targets & Train features dependecies"},{"metadata":{},"cell_type":"markdown","source":"Let's check target columns with categorical columns from training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = train_features[['cp_type', 'cp_time', 'cp_dose']]\ntar = train_targets.copy()\ntar = tar.drop(['sig_id'], axis=1)\nanalysis = pd.concat([categories, tar], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for category in analysis['cp_dose'].unique().tolist():\n    \n    number = 0\n    cols = list()\n    \n    for col in analysis.columns:\n        if col in ['cp_type', 'cp_time', 'cp_dose']:\n            continue\n        if len(analysis[analysis['cp_dose'] == category][col].value_counts()) == 1:\n            number += 1\n            cols.append(col)\n\n    print(category, '. Number of columns with 1 unique value: ', number, '. Columns: ', cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check problematic columns for dp_dose = 2."},{"metadata":{"trusted":true},"cell_type":"code","source":"analysis[analysis['cp_dose'] == 'D2']['atp-sensitive_potassium_channel_antagonist'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"analysis[analysis['cp_dose']=='D2']['erbb2_inhibitor'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for category in analysis['cp_time'].unique().tolist():\n    \n    number = 0\n    cols = list()\n    \n    for col in analysis.columns:\n        if col in ['cp_type', 'cp_time', 'cp_dose']:\n            continue\n        if len(analysis[analysis['cp_time']==category][col].value_counts()) == 1:\n            number += 1\n            cols.append(col)\n\n    print(category, '. Number of columns with 1 unique value: ', number, '. Columns: ', cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check problematic columns for cp_time = 24 and 72."},{"metadata":{"trusted":true},"cell_type":"code","source":"analysis[analysis['cp_time'] == 24]['erbb2_inhibitor'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"analysis[analysis['cp_time'] == 72]['erbb2_inhibitor'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"analysis[analysis['cp_time'] == 24]['atp-sensitive_potassium_channel_antagonist'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"analysis[analysis['cp_time'] == 72]['atp-sensitive_potassium_channel_antagonist'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for category in analysis['cp_type'].unique().tolist():\n    \n    number = 0\n    cols = list()\n    \n    for col in analysis.columns:\n        if col in ['cp_type', 'cp_time', 'cp_dose']:\n            continue\n        if len(analysis[analysis['cp_type'] == category][col].value_counts()) == 1:\n            number += 1\n            cols.append(col)\n\n    print(category, '. Number of columns with 1 unique value: ', number, '. Columns: ', cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"analysis[analysis['cp_type']=='ctl_vehicle']['igf-1_inhibitor'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that for column cp_type all records where value is ctl_vehicle for all targets are 0. The same picture for cp_time == 72 ana == 24, but only for 2 target columns and for cp_dose == D2 also for 2 target columns."},{"metadata":{},"cell_type":"markdown","source":"### Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train_features.append(test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    df = df.copy()\n    #df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.drop(['cp_type'], axis=1, inplace=True)\n    df.drop(['dataset'],axis=1,inplace=True)\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})    \n    df = pd.get_dummies(df, columns=['cp_time','cp_dose'])\n    del df['sig_id']\n    return df\n\ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit scaler to joint train and test data\nscaler = preprocessing.MinMaxScaler()\nscaler.fit(train.append(test))\n\ntrain_trans = scaler.transform(train)\ntest_trans = scaler.transform(test)\n\ntrain = pd.DataFrame(train_trans, columns=train.columns)\ntest = pd.DataFrame(test_trans, columns=test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"somthing_rate = 1e-15\nP_MIN = somthing_rate\nP_MAX = 1 - P_MIN\n\ndef loss_fn(yt, yp):\n    yp = np.clip(yp, P_MIN, P_MAX)\n    return log_loss(yt, yp, labels=[0,1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(num_columns, actv='relu'):\n    model = tf.keras.Sequential([tf.keras.layers.Input(num_columns)])\n                \n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.2))\n    model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(1024, activation=actv)))\n    \n    if actv == 'elu':\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.AlphaDropout(0.2))\n        model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(512, kernel_initializer='lecun_normal', activation='selu')))\n    else:\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dropout(0.2))\n        model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(1024, activation=actv))) \n\n    #============ Final Layer =================\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation=\"sigmoid\")))\n    \n    model.compile(optimizer=tfa.optimizers.AdamW(lr = 1e-3, weight_decay = 1e-5, clipvalue = 756), \n                  loss=BinaryCrossentropy(label_smoothing=somthing_rate),\n                  )\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use All feats as top feats\ntop_feats = [i for i in range(train.shape[1])]\nprint(\"Top feats length:\",len(top_feats))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mod = create_model(len(top_feats))\nmod.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def metric(y_true, y_pred):\n    metrics = []\n    for _target in train_targets.columns:\n        metrics.append(loss_fn(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float)))\n    return np.mean(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_STARTS = 14\nS_STARTS = int(N_STARTS/2) \n\nres_relu = train_targets.copy()\nres_elu = train_targets.copy()\nres_relu.loc[:, train_targets.columns] = 0\nres_elu.loc[:, train_targets.columns] = 0\n\nss_relu = sample_submission.copy()\nss_elu = sample_submission.copy()\nss_relu.loc[:, train_targets.columns] = 0\nss_elu.loc[:, train_targets.columns] = 0\n\n#ss.loc[:, train_targets.columns] = 0\nss_dict = {}\n\nhistorys = dict()\n\ntf.random.set_seed(42)\nfor seed in range(N_STARTS):\n    for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits=7, random_state=seed, shuffle=True).split(train_targets, train_targets)):\n        print(f\"======{train_targets.values[tr].shape}========{train_targets.values[te].shape}=====\")\n        \n        if seed < S_STARTS: # every actv. will train for 7 times seed.\n            print(f'Seed: {seed} => Fold: {n} ==> (RELU MODEL)')\n            model = create_model(len(top_feats), actv='relu')\n        else:\n            print(f'Seed: {seed} => Fold: {n} ==> (ELU MODEL)')\n            model = create_model(len(top_feats), actv='elu')\n\n        \n        checkpoint_path = f'repeat:{seed}_Fold:{n}.hdf5'\n        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.2, min_lr=1e-6, patience=4, verbose=1, mode='auto')\n        cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 1, save_best_only = True,\n                                     save_weights_only = True, mode = 'auto')\n        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", restore_best_weights=True, patience= 10, verbose = 1)\n        \n        history = model.fit(train.values[tr][:, top_feats],\n                  train_targets.values[tr],\n                  validation_data=(train.values[te][:, top_feats], train_targets.values[te]),\n                  epochs=60, batch_size=128,\n                  callbacks=[reduce_lr_loss, cb_checkpt, early], verbose=2\n                 )\n        \n        historys[f'history_{seed+1}'] = history\n        print(\"Model History Saved.\")\n        \n        model.load_weights(checkpoint_path)\n        \n        test_predict = model.predict(test.values[:, top_feats])\n        val_predict = model.predict(train.values[te][:, top_feats])\n\n        if seed < S_STARTS: \n            ss_relu.loc[:, train_targets.columns] += test_predict\n            res_relu.loc[te, train_targets.columns] += val_predict\n        else:\n            ss_elu.loc[:, train_targets.columns] += test_predict\n            res_elu.loc[te, train_targets.columns] += val_predict\n            \n        print(f'OOF Metric For SEED {seed} => FOLD {n} : {metric(train_targets.loc[te, train_targets.columns], pd.DataFrame(val_predict, columns=train_targets.columns))}')\n        print('+-' * 10)\n        \nss_relu.loc[:, train_targets.columns] /= ((n+1) * S_STARTS)\nres_relu.loc[:, train_targets.columns] /= S_STARTS\n\nss_elu.loc[:, train_targets.columns] /= ((n+1) * S_STARTS)\nres_elu.loc[:, train_targets.columns] /= S_STARTS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show Model loss in plots\n\nfor k,v in historys.items():\n    loss = []\n    val_loss = []\n    loss.append(v.history['loss'][:40])\n    val_loss.append(v.history['val_loss'][:40])\n    \nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15, 6))\nplt.plot(np.mean(loss, axis=0))\nplt.plot(np.mean(val_loss, axis=0))\nplt.yscale('log')\nplt.yticks(ticks=[1,1e-1,1e-2])\nplt.xlabel('Epochs')\nplt.ylabel('Average Logloss')\nplt.legend(['Training','Validation'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'OOF Metric (relu): {metric(train_targets, res_relu)}')\nprint(f'OOF Metric (elu): {metric(train_targets, res_elu)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_elu.to_csv('submission_elu.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_relu.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}