{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Installing dependencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os, re, random, gc\nfrom tqdm import tqdm\nfrom glob import glob\n\nfrom datetime import datetime\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.preprocessing import QuantileTransformer, LabelEncoder\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.utils.data import Dataset\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Config"},{"metadata":{"trusted":true},"cell_type":"code","source":"class config:\n    \n    ###############\n    # Training\n    ###############\n    \n    num_folds = 5\n    \n    num_workers = 8\n    batch_size = 128\n    num_epochs = 30\n    \n    ###############\n    # LR scheduling\n    ###############\n    \n    step_scheduler = True\n    lr = 1e-4\n    \n    ###############\n    # Miscellaneous\n    ###############\n    \n    seed = 2020\n    verbose = True\n    verbose_step = 5\n    \n    seeds = [0, 42]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data manipulation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_labels(df):\n    \n    le = LabelEncoder()\n    cat_feats = ['cp_time', 'cp_dose', 'cp_type']\n    \n    for feat in cat_feats:\n        df[feat] = le.fit_transform(df[feat])\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '../input/lish-moa/'\n\nTRAIN_FEATURES_PATH = DATA_PATH + 'train_features.csv'\nTEST_FEATURES_PATH = DATA_PATH + 'test_features.csv'\nTRAIN_TARGETS_PATH  = DATA_PATH + 'train_targets_scored.csv'\n\n# Loading data\n\nfeatures_df = pd.read_csv(TRAIN_FEATURES_PATH)\ntargets_df = pd.read_csv(TRAIN_TARGETS_PATH)\n\ntest_features_df = pd.read_csv(TEST_FEATURES_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding features\n\nfeatures_df = encode_labels(features_df)\ntest_features_df = encode_labels(test_features_df)\n\n# Remove control groups from features_df\n\nfeatures_df = features_df[features_df['cp_type'] != 0]\ntargets_df = targets_df.loc[features_df.index]\n\nfeatures_df = features_df.reset_index(drop=True)\ntargets_df = targets_df.reset_index(drop=True)\n\nassert len(features_df) == len(targets_df), 'Dataframes do not have the same length'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming feature distributions\n\ngene_features = [x for x in list(features_df.columns) if 'g-' in x]\ncell_features = [x for x in list(features_df.columns) if 'c-' in x]\ncat_features = ['cp_type', 'cp_time', 'cp_dose']\n\nfor col in gene_features + cell_features:\n    transformer = QuantileTransformer()\n    \n    vec_len = len(features_df[col].values)\n    vec_len_test = len(test_features_df[col].values)\n    raw_vec = features_df[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    features_df[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features_df[col] = transformer.transform(test_features_df[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(config.seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n    \n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset"},{"metadata":{},"cell_type":"markdown","source":"### Dataset class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoADataset(Dataset):\n    def __init__(self, features, targets=None, train=True):\n        super().__init__()\n        self.features = features.values\n        self.train = train\n        \n        if self.train:\n            self.targets = targets.values\n                \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, item):\n        feats = self.features[item, :].astype(np.float32)\n        \n        if self.train: \n            \n            targets = self.targets[item, 1:].astype(np.float32) \n            \n            return {\n                'features': torch.tensor(feats, dtype=torch.float),\n                'targets': torch.tensor(targets, dtype=torch.float),\n            }\n        else: \n            return {'features': torch.tensor(feats, dtype=torch.float)}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BaselineModel(nn.Module):\n    def __init__(self):\n        super(BaselineModel, self).__init__()\n        \n        self.num_features = len(gene_features + cell_features + cat_features)\n        \n        self.block1 = nn.Sequential(\n            nn.BatchNorm1d(self.num_features),\n            nn.Dropout(0.2),\n            nn.utils.weight_norm(nn.Linear(self.num_features, 2048)),\n            nn.ReLU(),\n        )\n        \n        self.block2 = nn.Sequential(\n            nn.BatchNorm1d(2048),\n            nn.Dropout(0.5),\n            nn.utils.weight_norm(nn.Linear(2048, 1024)),\n            nn.ReLU(),\n        )\n        \n        self.block3 = nn.Sequential(\n            nn.BatchNorm1d(1024),\n            nn.Dropout(0.5),\n            nn.utils.weight_norm(nn.Linear(1024, 206)),\n        )\n    \n    def forward(self,\n                inputs):\n        \n        \n        x = self.block1(inputs)\n        x = self.block2(x)\n            \n        return self.block3(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitter"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Fitter:\n    def __init__(self, model, seed, fold, device, config):\n        self.config = config\n        self.model = model\n        self.seed = seed\n        self.device = device\n        self.fold = fold\n                        \n        self.epoch = 0\n        \n        self.history = {\n            'train_history_loss': [],\n            'val_history_loss': [],\n        }\n        \n        self.base_dir = './'\n        self.log_path = f'{self.base_dir}/log.txt'\n        \n        self.best_loss = float('inf')\n        \n        self.optimizer = torch.optim.Adam(\n            self.model.parameters(),\n            weight_decay=1e-5\n        )\n        \n        self.scheduler = lr_scheduler.ReduceLROnPlateau(\n            self.optimizer,\n            mode='min',\n            factor=0.1,\n            patience=3,\n            eps=1e-4,\n            verbose=True\n        )\n        \n        self.criterion = nn.BCEWithLogitsLoss().to(self.device)\n        self.log(f'Fitter prepared. Training on {self.device}')\n    \n    def fit(self, train_loader, valid_loader):\n        \n        for epoch in range(self.config.num_epochs):\n            \n            if self.config.verbose:\n                lr = self.optimizer.param_groups[0]['lr']\n                timestamp = datetime.utcnow().isoformat()\n                self.log(f'\\n{timestamp}\\nLR: {lr}\\n')\n            \n            t = time.time()\n            train_loss = self.train_one_epoch(train_loader)\n            self.history['train_history_loss'].append(train_loss.avg)\n            \n            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, ' + \\\n                     f'loss: {train_loss.avg:.5f}, ' + \\\n                     f'time: {(time.time() - t):.5f}')\n            self.save(f'{self.base_dir}/last-checkpoint.bin')\n            \n            t = time.time()\n            val_loss, y_oof = self.validation_one_epoch(valid_loader)\n            self.history['val_history_loss'].append(val_loss.avg)\n            \n            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, ' + \\\n                     f'val_loss: {val_loss.avg:.5f}, ' + \\\n                     f'time: {(time.time() - t):.5f}')\n            \n            self.scheduler.step(val_loss.avg)\n            \n            if val_loss.avg < self.best_loss:\n                self.best_loss = val_loss.avg\n                self.model.eval()\n                self.save(f'{self.base_dir}/best-loss-fold-{str(self.fold)}-seed-{str(self.seed)}.bin')\n            \n            self.epoch += 1 \n        \n        return y_oof\n    \n    def train_one_epoch(self, train_loader):\n        self.model.train()\n        \n        loss_score = AverageMeter()\n        \n        t = time.time()\n        \n        for step, data in enumerate(train_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Train Step {step}/{len(train_loader)}, ' + \\\n                        f'loss: {loss_score.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            \n            features = data['features']\n            targets = data['targets']\n            \n            features = features.to(self.device)\n            targets = targets.to(self.device).float()\n                \n            batch_size = features.shape[0]\n            \n            for p in self.model.parameters(): p.grad = None\n                \n            outputs = self.model(\n                features\n            )\n                \n            loss = self.criterion(outputs, targets)\n            loss.backward()\n                \n            loss_score.update(\n                loss.detach().item(), \n                batch_size\n            )\n                \n            self.optimizer.step()\n        \n        return loss_score\n\n    def validation_one_epoch(self, valid_loader):\n        self.model.eval()\n        \n        preds = []\n        \n        loss_score = AverageMeter()\n        \n        t = time.time()\n        \n        for step, data in enumerate(valid_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Val Step {step}/{len(valid_loader)}, ' + \\\n                        f'loss: {loss_score.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            \n            features = data['features']\n            targets = data['targets']\n            \n            features = features.to(self.device)\n            targets = targets.to(self.device).float()\n            \n            batch_size = features.shape[0]\n            \n            with torch.no_grad():\n                outputs = self.model(\n                    features\n                )\n                loss = self.criterion(outputs, targets)\n                loss_score.update(loss.detach().item(), batch_size)\n                \n                preds.append(\n                    torch.sigmoid(outputs).detach().cpu().numpy()\n                )\n        \n        return loss_score, np.concatenate(preds)\n    \n    def save(self, path):\n        self.model.eval()\n        \n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'best_loss': self.best_loss,\n            'epoch': self.epoch,\n            'history': self.history,\n        }, path)\n    \n    def load(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_summary_loss = checkpoint['best_loss']\n        self.epoch = checkpoint['epoch'] + 1\n        self.history = checkpoint['history']\n        \n    def log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')\n                \n    def print_history(self):\n        plt.figure(figsize=(15,5))\n        \n        plt.plot(\n            np.arange(self.config.num_epochs),\n            self.history['train_history_loss'],\n            '-o',\n            label='Train loss',\n            color='#ff7f0e'\n        )\n        \n        plt.plot(\n            np.arange(self.config.num_epochs),\n            self.history['val_history_loss'],\n            '-o',\n            label='Val loss',\n            color='#1f77b4'\n        )\n        \n        x = np.argmin(self.history['val_history_loss'])\n        y = np.min(self.history['val_history_loss'])\n        \n        plt.ylim(0, 0.03)\n        \n        xdist = plt.xlim()[1] - plt.xlim()[0]\n        ydist = plt.ylim()[1] - plt.ylim()[0]\n        \n        plt.scatter(x, y, s=200, color='#1f77b4')\n        \n        plt.text(\n            x-0.03*xdist,\n            y-0.13*ydist,\n            'min loss\\n%.5f'%y,\n            size=14\n        )\n        \n        plt.ylabel('Loss', size=14)\n        plt.xlabel('Epoch', size=14)\n        \n        plt.legend(loc=2)\n        \n        plt.title(f'FOLD {self.fold + 1}',size=18)\n        \n        plt.legend(loc=3)\n        plt.show()  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Engine"},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_preds = np.zeros((len(features_df), 206, 3))\n\ndevice = torch.device(\n    'cuda' if torch.cuda.is_available() else 'cpu'\n)\n\nall_features = cat_features + gene_features + cell_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, seed in enumerate(config.seeds):\n    kfold = MultilabelStratifiedKFold(config.num_folds, shuffle=True, random_state=seed)\n    \n    X = features_df[all_features].values\n    y = targets_df.values\n    \n    for fold, (trn_, val_) in enumerate(kfold.split(X, y)):\n        \n        # Model\n        model = BaselineModel().to(device)\n        \n        # Data\n        X_train = features_df[all_features].loc[trn_].reset_index(drop=True)\n        X_valid = features_df[all_features].loc[val_].reset_index(drop=True)\n        \n        y_train = targets_df.loc[trn_].reset_index(drop=True)\n        y_valid = targets_df.loc[val_].reset_index(drop=True)\n        \n        # Dataset\n        train_dataset = MoADataset(X_train, y_train)\n        valid_dataset = MoADataset(X_valid, y_valid)\n        \n        # Dataloader\n        train_loader = torch.utils.data.DataLoader(\n            train_dataset,\n            batch_size=config.batch_size,\n            pin_memory=True,\n            drop_last=True,\n            shuffle=True,\n            num_workers=config.num_workers\n        )\n\n        valid_loader = torch.utils.data.DataLoader(\n            valid_dataset,\n            batch_size=config.batch_size,\n            num_workers=config.num_workers,\n            shuffle=False,\n            pin_memory=True,\n            drop_last=False,\n        )\n        \n        # Fitter\n        fitter = Fitter(model, seed, fold, device, config)\n\n        y_oof = fitter.fit(train_loader, valid_loader) \n        oof_preds[val_, :, i] = y_oof\n\n        fitter.print_history()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CV score"},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_preds = np.mean(oof_preds, axis=2)\n\ntarget_cols = list(targets_df.columns)\ntarget_cols.remove('sig_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_score = 0\ny_true = targets_df[target_cols].values\n\nfor i in range(oof_preds.shape[1]):\n    _score = log_loss(y_true[:,i], oof_preds[:,i])\n    oof_score += _score / y_true.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('OOF CV score', oof_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestConfig:\n    \n    ###############\n    # Verbosity   #\n    ###############\n    verbose = True\n    verbose_step = 1\n        \n    ###############\n    # Data loader #\n    ###############\n    \n    data_loader_params = dict(\n        batch_size=128,\n        num_workers=8,\n        pin_memory=False,\n        drop_last=False,\n        shuffle=False,\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_PATHS = glob('./*.bin')\nMODEL_PATHS.remove('./last-checkpoint.bin')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ctl_indices = test_features_df[test_features_df['cp_type'] == 'ctl_vehicle'].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Predictor:\n    def __init__(self, model, df, device, config):\n        self.model = model\n        self.df = df\n        self.device = device\n        self.config = config\n        \n        self.base_dir = './'\n        self.log_path = f'{self.base_dir}/log.txt'\n        \n        self._log(f'Predictor prepared. Predicting on {self.device}.')\n    \n    def predict(self):\n        if self.config.verbose:\n            timestamp = datetime.utcnow().isoformat()\n            self._log(f'\\n{timestamp}\\n')\n        \n        t = time.time()\n        \n        dataset = MoADataset(\n            self.df,\n            train=False,\n        )\n        \n        loader = torch.utils.data.DataLoader(\n            dataset,\n            **self.config.data_loader_params,\n        )\n        \n        probabilities = self._predict_one_loader(loader)\n        self._log(f'Inference done. Time: {(time.time() - t):.5f}')\n                \n        return probabilities\n\n    def _predict_one_loader(self, test_loader):\n        self.model.eval()\n        \n        t = time.time()\n        \n        probabilities = []\n        \n        for step, data in enumerate(test_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Inference Step {step}/{len(test_loader)}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            \n            features = data['features']\n        \n            features = features.to(self.device)\n                        \n            with torch.no_grad():\n                outputs = self.model(\n                    features\n                )\n                \n                probabilities.extend(\n                    torch.sigmoid(outputs).data.cpu().numpy().tolist()\n                )\n        \n        return np.array(probabilities)\n    \n    def _log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_predictions = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, model_path in enumerate(MODEL_PATHS):\n    print(f'Predicting with model #{i+1}')\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    model = BaselineModel().to(device)\n    \n    model.load_state_dict(\n        torch.load(\n            model_path, \n            map_location=torch.device('cpu')\n        )['model_state_dict']\n    )\n    \n    # Inference\n    predictor = Predictor(model, test_features_df[all_features], device, TestConfig)\n    predictions = predictor.predict()\n    \n    final_predictions.append(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probabilities = np.mean(final_predictions, axis=0)\nprint(probabilities.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probabilities[ctl_indices, :] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.read_csv('../input/lish-moa/sample_submission.csv')\nsub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.loc[:, 1:] = probabilities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}