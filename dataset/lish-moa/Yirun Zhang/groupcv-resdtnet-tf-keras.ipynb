{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-09-06T18:32:08.877939Z","iopub.status.busy":"2020-09-06T18:32:08.877228Z","iopub.status.idle":"2020-09-06T18:32:14.972831Z","shell.execute_reply":"2020-09-06T18:32:14.972235Z"},"papermill":{"duration":6.107558,"end_time":"2020-09-06T18:32:14.972944","exception":false,"start_time":"2020-09-06T18:32:08.865386","status":"completed"},"tags":[],"id":"aI8-T6rjZD5_","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport os\nimport gc\nimport math\nimport random\nimport datetime\nimport numpy as np\nimport pandas as pd\nfrom joblib import dump, load\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nimport tensorflow_addons as tfa\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import log_loss\nfrom hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope\nfrom tqdm.notebook import tqdm\nfrom time import time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_folds(num_starts, num_splits):\n    \n    folds = []\n    \n    # LOAD FILES\n    train_feats = pd.read_csv('../input/lish-moa/train_features.csv')\n    scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\n    drug = pd.read_csv('/kaggle/input/lish-moa/train_drug.csv')\n    scored = scored.loc[train_feats['cp_type'] == 'trt_cp', :]\n    drug = drug.loc[train_feats['cp_type'] == 'trt_cp', :]\n    targets = scored.columns[1:]\n    scored = scored.merge(drug, on = 'sig_id', how = 'left') \n\n    # LOCATE DRUGS\n    vc = scored.drug_id.value_counts()\n    vc1 = vc.loc[vc <= 18].index.sort_values()\n    vc2 = vc.loc[vc > 18].index.sort_values()\n    \n    for seed in range(num_starts):\n\n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}; dct2 = {}\n        skf = MultilabelStratifiedKFold(n_splits = num_splits, shuffle = True, random_state = seed)\n        tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n        for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[targets])):\n            dd = {k:fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits = num_splits, shuffle = True, random_state = seed)\n        tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop = True)\n        for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[targets])):\n            dd = {k:fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        scored['fold'] = scored.drug_id.map(dct1)\n        scored.loc[scored.fold.isna(),'fold'] =\\\n            scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\n        scored.fold = scored.fold.astype('int8')\n        folds.append(scored.fold.values)\n        \n        del scored['fold']\n        \n    return np.stack(folds)","execution_count":null,"outputs":[]},{"metadata":{"id":"EHkVsCptoNfT"},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-09-06T18:32:14.997376Z","iopub.status.busy":"2020-09-06T18:32:14.996643Z","iopub.status.idle":"2020-09-06T18:32:20.482365Z","shell.execute_reply":"2020-09-06T18:32:20.483409Z"},"papermill":{"duration":5.505878,"end_time":"2020-09-06T18:32:20.48361","exception":false,"start_time":"2020-09-06T18:32:14.977732","status":"completed"},"tags":[],"id":"5DxCXDYIZD6C","trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\n\nss = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\ncols = [c for c in ss.columns.values if c != 'sig_id']\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n#     df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    return df\n\ndef log_loss_metric(y_true, y_pred):\n    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    return - np.mean(y_true * np.log(y_pred_clip) + (1 - y_true) * np.log(1 - y_pred_clip))\n\ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']\ndel train_targets_nonscored['sig_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer\n\nqt = QuantileTransformer(output_distribution = 'normal', random_state = 42)\nqt.fit(pd.concat([pd.DataFrame(train[GENES+CELLS]), pd.DataFrame(test[GENES+CELLS])]))\ntrain[GENES+CELLS] = qt.transform(train[GENES+CELLS])\ntest[GENES+CELLS] = qt.transform(test[GENES+CELLS])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n# GENES\nn_comp_genes = 600  #<--Update\n\ndata = pd.concat([pd.DataFrame(train[GENES]), pd.DataFrame(test[GENES])])\npca_genes = PCA(n_components=n_comp_genes, random_state = 42)\ndata2 = pca_genes.fit_transform(data[GENES])\ntrain2 = data2[:train.shape[0]]; test2 = data2[-test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp_genes)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp_genes)])\n\ntrain = pd.concat((train, train2), axis=1)\ntest = pd.concat((test, test2), axis=1)\n\n#CELLS\nn_comp_cells = 50  #<--Update\n\ndata = pd.concat([pd.DataFrame(train[CELLS]), pd.DataFrame(test[CELLS])])\npca_cells = PCA(n_components=n_comp_cells, random_state = 42)\ndata2 = pca_cells.fit_transform(data[CELLS])\ntrain2 = data2[:train.shape[0]]; test2 = data2[-test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp_cells)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp_cells)])\n\ntrain = pd.concat((train, train2), axis=1)\ntest = pd.concat((test, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n\nvar_thresh = VarianceThreshold(0.8)  #<-- Update\ndata = train.append(test)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 3:])\n\ntrain_transformed = data_transformed[ : train.shape[0]]\ntest_transformed = data_transformed[-test.shape[0] : ]\n\ntrain = pd.DataFrame(train[['cp_type','cp_time','cp_dose']].values.reshape(-1, 3),\\\n            columns=['cp_type','cp_time','cp_dose'])\n\ntrain = pd.concat([train, pd.DataFrame(train_transformed)], axis=1)\n\ntest = pd.DataFrame(test[['cp_type','cp_time','cp_dose']].values.reshape(-1, 3),\\\n            columns=['cp_type','cp_time','cp_dose'])\n\ntest = pd.concat([test, pd.DataFrame(test_transformed)], axis=1)\n\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets = train_targets.loc[train['cp_type'] == 0].reset_index(drop = True)\ntrain_targets_nonscored = train_targets_nonscored.loc[train['cp_type'] == 0].reset_index(drop = True)\ntrain = train.loc[train['cp_type'] == 0].reset_index(drop = True)\n\nprint(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_feats = np.arange(1, train.shape[1])\nprint(top_feats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_pseudo = pd.read_csv('../input/drug-sub/submission_pbest.csv').drop('sig_id', axis = 1)\npseudo_targets = ss_pseudo.loc[test['cp_type'] == 0, cols].values\npseudo_train = test.loc[test['cp_type'] == 0, test.columns].values","execution_count":null,"outputs":[]},{"metadata":{"id":"l9086lcvni30"},"cell_type":"markdown","source":"# Model Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"class GroupLinear(tf.keras.layers.Layer):\n    def __init__(self, in_dim, out_dim, group, **kwargs):\n        super(GroupLinear, self).__init__()\n        self.reshape = tf.keras.layers.Reshape((group, in_dim // group))\n        self.linear = tf.keras.layers.Dense(out_dim // group)\n        self.flatten = tf.keras.layers.Flatten()\n        \n    def call(self, x):\n        x = self.reshape(x)\n        x = self.linear(x)\n        x = self.flatten(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(num_columns, num_labels, hidden_units, dropout_rates, num_groups):\n    \n    inp = tf.keras.layers.Input(shape = (num_columns, ), name = 'inp')\n    \n    x0 = tf.keras.Sequential(\n        [\n            tf.keras.layers.BatchNormalization(name = 'bn0'), \n            tf.keras.layers.Dropout(dropout_rates[0], name = 'dp0'), \n            tf.keras.layers.Dense(hidden_units[0], name = 'd0'), \n            GroupLinear(hidden_units[0], hidden_units[1], num_groups[0], name = 'gl0'), \n            tf.keras.layers.Activation(tf.keras.activations.swish, name = 'a0'), \n            tf.keras.layers.Dropout(dropout_rates[1], name = 'dp1'), \n        ], \n        name = 'x0')(inp)\n    \n    x1 = tf.keras.Sequential(\n        [\n            tf.keras.layers.Dense(hidden_units[2], name = 'd1'),\n            GroupLinear(hidden_units[2], hidden_units[1], num_groups[1], name = 'gl1'),\n            tf.keras.layers.BatchNormalization(name = 'bn2'),\n            tf.keras.layers.Activation(tf.keras.activations.swish, name = 'a1'),\n            tf.keras.layers.Dropout(dropout_rates[2], name = 'dp2'), \n        ],\n        name = 'r0')(x0)\n    x1 = tf.keras.layers.Add(name = 'add0')([x0, x1])\n    \n    x2 = tf.keras.Sequential(\n        [\n            tf.keras.layers.Dense(hidden_units[3], name = 'd2'),\n            GroupLinear(hidden_units[3], hidden_units[1], num_groups[2], name = 'gl2'),\n            tf.keras.layers.BatchNormalization(name = 'bn3'),\n            tf.keras.layers.Activation(tf.keras.activations.swish, name = 'a2'),\n            tf.keras.layers.Dropout(dropout_rates[3], name = 'dp3'), \n        ],\n        name = 'r1')(x1)\n    x2 = tf.keras.layers.Add(name = 'add1')([x1, x2])\n    \n    x3 = tf.keras.Sequential(\n        [\n            tf.keras.layers.Dense(hidden_units[4], name = 'd3'),\n            GroupLinear(hidden_units[4], hidden_units[1], num_groups[3], name = 'gl3'),\n            tf.keras.layers.BatchNormalization(name = 'bn4'),\n            tf.keras.layers.Activation(tf.keras.activations.swish, name = 'a3'),\n            tf.keras.layers.Dropout(dropout_rates[4], name = 'dp4'), \n        ],\n        name = 'r2')(x2)\n    x3 = tf.keras.layers.Add(name = 'add2')([x2, x3])\n        \n    x = tf.keras.layers.Dense(num_labels, \n                              bias_initializer = tf.keras.initializers.Constant(6.3), \n                              name = f'output_d{num_labels}')(x3)\n    out = tf.keras.layers.Activation('sigmoid', name = f'output_a{num_labels}')(x)\n    \n    model = tf.keras.models.Model(inputs = inp, outputs = out)\n    model.compile(optimizer = tfa.optimizers.AdamW(weight_decay = 1e-5, learning_rate = 1e-3),\n                  loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.0008), \n                  metrics = tf.keras.losses.BinaryCrossentropy(name = 'mean_loss'), \n                 )\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"id":"ol334uu2oSUO"},"cell_type":"markdown","source":"# Train Model"},{"metadata":{"execution":{"iopub.execute_input":"2020-09-06T18:32:20.945454Z","iopub.status.busy":"2020-09-06T18:32:20.942912Z","iopub.status.idle":"2020-09-06T19:16:12.825686Z","shell.execute_reply":"2020-09-06T19:16:12.826367Z"},"papermill":{"duration":2631.901658,"end_time":"2020-09-06T19:16:12.826538","exception":false,"start_time":"2020-09-06T18:32:20.92488","status":"completed"},"tags":[],"id":"DpIxnqicZD6S","trusted":true},"cell_type":"code","source":"def train_model(X_train, Y_train_2, Y_nonscored, features, folds, model_name, save_path, num_seeds, \n                num_splits, model_params, X_test = None, sample_sub_path = None, pseudo_labeling = True, verbose = 0):\n    start_time_all = time()\n    oof = Y_train_2.copy()\n    oof.loc[:, Y_train_2.columns] = 0\n    if X_test is not None:\n        sub = pd.read_csv(sample_sub_path)\n        sub.loc[:, Y_train_2.columns] = 0\n    else:\n        sub = None\n    for nums, seed in enumerate(range(num_seeds)):\n        start_time_seed = time()\n        tf.random.set_seed(seed)\n        np.random.seed(seed)\n        random.seed(seed) \n        mean_score = 0\n#         skf = MultilabelStratifiedKFold(n_splits = num_splits, random_state = seed, shuffle = True)\n        for n, foldno in enumerate(set(folds[nums])):\n            start_time_fold = time()\n            tr = folds[nums] != foldno\n            te = folds[nums] == foldno\n            x_tr, x_val = X_train.values[tr][:, features], X_train.values[te][:, features]\n            y_tr, y_val = Y_train_2.values[tr], Y_train_2.values[te]\n            if pseudo_labeling:\n                x_tr = np.concatenate([x_tr, pseudo_train[:, features]])\n                y_tr = np.concatenate([y_tr, pseudo_targets])\n\n            if X_test is not None:\n                x_tt = X_test.values[:, features]\n                \n            x_tr_ns, x_val_ns = X_train.values[tr][:, features], X_train.values[te][:, features]\n            y_tr_ns, y_val_ns = Y_nonscored.values[tr], Y_nonscored.values[te]\n            \n            ckp_path = save_path + f'{model_name}_Seed_{seed}_Fold_{n}.hdf5'\n            # Nonscored\n            model = create_model(x_tr.shape[1], 402, **model_params)            \n            rlr = ReduceLROnPlateau(monitor = 'val_mean_loss', factor = 0.1, patience = 3, verbose = verbose, \n                                    min_delta = 1e-4, mode = 'min')\n            ckp = ModelCheckpoint(ckp_path, monitor = 'val_mean_loss', verbose = 0, \n                                  save_best_only = True, save_weights_only = True, mode = 'min')\n            es = EarlyStopping(monitor = 'val_mean_loss', min_delta = 1e-4, patience = 10, mode = 'min', \n                               baseline = None, restore_best_weights = True, verbose = 0)\n            model.fit(x_tr_ns, y_tr_ns, validation_data = (x_val_ns, y_val_ns), epochs = 1000, batch_size = 128,\n                      callbacks = [rlr, ckp, es], verbose = verbose)\n            K.clear_session()\n            \n            # Scored\n            model = create_model(x_tr.shape[1], 206, **model_params)            \n            model.load_weights(ckp_path, by_name = True)\n            rlr = ReduceLROnPlateau(monitor = 'val_mean_loss', factor = 0.1, patience = 3, \n                                    verbose = verbose, min_delta = 1e-4, mode = 'min')\n            ckp = ModelCheckpoint(ckp_path, monitor = 'val_mean_loss', verbose = 0, \n                                  save_best_only = True, save_weights_only = True, mode = 'min')\n            es = EarlyStopping(monitor = 'val_mean_loss', min_delta = 1e-4, patience = 10, mode = 'min', \n                               baseline = None, restore_best_weights = True, verbose = 0)\n            history = model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs = 1000, \n                                batch_size = 128, callbacks=[rlr, ckp, es], verbose = verbose)\n            hist = pd.DataFrame(history.history)\n            model.load_weights(ckp_path)\n        \n            val_predict = model.predict(x_val)\n            fold_score = hist['val_mean_loss'].min()\n#             fold_score = log_loss_metric(y_val, val_predict)\n            mean_score += fold_score / num_splits\n            oof.loc[te, Y_train_2.columns] += val_predict / num_seeds\n            if X_test is not None:\n                test_predict = model.predict(x_tt)\n                sub.loc[:, Y_train_2.columns] += test_predict / (num_splits * num_seeds)\n            print(f'[{str(datetime.timedelta(seconds = time() - start_time_fold))[0:7]}] {model_name} Seed {seed}, Fold {n}:', fold_score)\n            \n            del model\n            x = gc.collect()\n            K.clear_session()\n\n        print(f'[{str(datetime.timedelta(seconds = time() - start_time_seed))[0:7]}] {model_name} Seed {seed} Mean Score:', mean_score)\n        \n    oof.loc[X_train['cp_type'] == 1, Y_train_2.columns] = 0\n    overall_score = log_loss_metric(Y_train_2.values, oof[Y_train_2.columns].values)\n    print(f'[{str(datetime.timedelta(seconds = time() - start_time_all))[0:7]}] {model_name} OOF Score:', overall_score)\n    if X_test is not None:\n        sub.loc[X_test['cp_type'] == 1, Y_train_2.columns] = 0\n    return overall_score, oof, sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_STARTS = 3\nN_SPLITS = 5\n\nfolds_split = create_folds(N_STARTS, N_SPLITS)\nprint(folds_split)","execution_count":null,"outputs":[]},{"metadata":{"id":"YpJRV0O5cxV8","executionInfo":{"status":"ok","timestamp":1602953542287,"user_tz":-60,"elapsed":4372657,"user":{"displayName":"Yirun Zhang","photoUrl":"","userId":"05891579764514658952"}},"outputId":"bd1f6ab2-ca29-4eaf-aff0-c678b1b7f494","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"model_params = [{'hidden_units': [128, 128, 512, 512, 128],  \n                 'dropout_rates': [0.3782307809831188, 0.49668628489698075, 0.2152127463143538, 0.4146007588097611, 0.32126943893048127],  \n                 'num_groups': [16, 8, 8, 8],\n                }, ]\n\nfor m in range(len(model_params)):\n    print(model_params[m])\n    Pseudo_Labeling = True\n    VERBOSE = 0\n    model_name = f'ResDT{m}'\n    save_path = ''\n    sample_sub_path = '../input/lish-moa/sample_submission.csv'\n    oof_score, res, ss = train_model(train, train_targets, train_targets_nonscored, top_feats, folds_split, model_name, save_path, \n                                     N_STARTS, N_SPLITS, model_params[m], test, sample_sub_path, Pseudo_Labeling, VERBOSE)\n    np.save(f'{model_name}_oof.npy', res[cols].values)\n    np.save(f'{model_name}_sub.npy', ss[cols].values)\n    ss.to_csv(f'submission_GroupCV_PBestPre_{model_name}.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_ensemble(X_train, Y_train_2, folds, model_name, save_path, num_seeds, \n                   num_splits, model_params, X_test = None, sample_sub_path = None, verbose = 0):\n    start_time_all = time()\n    oof = Y_train_2.copy()\n    oof.loc[:, Y_train_2.columns] = 0\n    if X_test is not None:\n        sub = pd.read_csv(sample_sub_path)\n        sub.loc[:, Y_train_2.columns] = 0\n    else:\n        sub = None\n    for nums, seed in enumerate(range(num_seeds)):\n        start_time_seed = time()\n        tf.random.set_seed(seed)\n        np.random.seed(seed)\n        random.seed(seed) \n        mean_score = 0\n#         skf = MultilabelStratifiedKFold(n_splits = num_splits, random_state = seed, shuffle = True)\n        for n, foldno in enumerate(set(folds[nums])):\n            start_time_fold = time()\n            tr = folds[nums] != foldno\n            te = folds[nums] == foldno\n            x_tr, x_val = X_train[tr], X_train[te]\n            y_tr, y_val = Y_train_2.values[tr], Y_train_2.values[te]\n\n            if X_test is not None:\n                x_tt = X_test\n\n            ckp_path = save_path + f'{model_name}_Seed_{seed}_Fold_{n}.hdf5'\n            model = create_model(x_tr.shape[1], 206, **model_params)            \n            rlr = ReduceLROnPlateau(monitor = 'val_mean_loss', factor = 0.1, patience = 3, \n                                    verbose = verbose, min_delta = 1e-4, mode = 'min')\n            ckp = ModelCheckpoint(ckp_path, monitor = 'val_mean_loss', verbose = verbose, \n                                  save_best_only = True, save_weights_only = True, mode = 'min')\n            es = EarlyStopping(monitor = 'val_mean_loss', min_delta = 1e-4, patience = 10, mode = 'min', \n                               baseline = None, restore_best_weights = True, verbose = verbose)\n            history = model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs = 1000, \n                                batch_size = 128, callbacks=[rlr, ckp, es], verbose = verbose)\n            hist = pd.DataFrame(history.history)\n            model.load_weights(ckp_path)\n            \n            val_predict = model.predict(x_val)\n            fold_score = hist['val_mean_loss'].min()\n            # fold_score = log_loss_metric(y_val, val_predict)\n            mean_score += fold_score / num_splits\n            oof.loc[te, Y_train_2.columns] += val_predict / num_seeds\n            if X_test is not None:\n                test_predict = model.predict(x_tt)\n                sub.loc[:, Y_train_2.columns] += test_predict / (num_splits * num_seeds)\n            print(f'[{str(datetime.timedelta(seconds = time() - start_time_fold))[0:7]}] {model_name} Seed {seed}, Fold {n}:', fold_score)\n            \n            del model\n            x = gc.collect()\n            K.clear_session()\n\n        print(f'[{str(datetime.timedelta(seconds = time() - start_time_seed))[0:7]}] {model_name} Seed {seed} Mean Score:', mean_score)\n        \n    oof.loc[train['cp_type'] == 1, Y_train_2.columns] = 0\n    overall_score = log_loss_metric(Y_train_2.values, oof[Y_train_2.columns].values)\n    print(f'[{str(datetime.timedelta(seconds = time() - start_time_all))[0:7]}] {model_name} OOF Score:', overall_score)\n    if X_test is not None:\n        sub.loc[test['cp_type'] == 1, Y_train_2.columns] = 0\n    return overall_score, oof, sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # oof1 = np.load('../input/moa-mlp/Model0_oof.npy')\n# # oof2 = np.load('../input/moa-mlp/Model1_oof.npy')\n# # oof3 = np.load('../input/moa-mlp/Model2_oof.npy')\n\n# # sub1 = np.load('../input/moa-mlp/Model0_sub.npy')\n# # sub2 = np.load('../input/moa-mlp/Model1_sub.npy')\n# # sub3 = np.load('../input/moa-mlp/Model2_sub.npy')\n\n# oof1 = np.load('./Model0_oof.npy')\n# oof2 = np.load('./Model1_oof.npy')\n# oof3 = np.load('./Model2_oof.npy')\n\n# sub1 = np.load('./Model0_sub.npy')\n# sub2 = np.load('./Model1_sub.npy')\n# sub3 = np.load('./Model2_sub.npy')\n\n# train_new = np.concatenate([oof1, oof2, oof3], axis = 1)\n# test_new = np.concatenate([sub1, sub2, sub3], axis = 1)\n\n# print('OOF 1:', log_loss_metric(train_targets.values, oof1))\n# print('OOF 2:', log_loss_metric(train_targets.values, oof2))\n# print('OOF 3:', log_loss_metric(train_targets.values, oof3))\n# print('Blend OOF:', log_loss_metric(train_targets.values, 0.33 * oof1 + 0.33 * oof2 + 0.34 * oof3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_params = {'hidden_units': [1920, 768],  \n#                 'dropout_rates': [0.36130273975713795, 0.38130486900003896, 0.44485672673556004], \n#                 }\n\n# N_STARTS = 3\n# N_SPLITS = 5\n# VERBOSE = 0\n# model_name = 'EModel_Stack'\n# save_path = ''\n# sample_sub_path = '../input/lish-moa/sample_submission.csv'\n# oof_score, res, ss = train_ensemble(train_new, train_targets, folds_split, model_name, save_path, \n#                                     N_STARTS, N_SPLITS, model_params, test_new, sample_sub_path, VERBOSE)\n# np.save(f'{model_name}_oof.npy', res[cols].values)\n# np.save(f'{model_name}_sub.npy', ss[cols].values)\n# ss.to_csv('submission_GroupCV_MLP.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperopt"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_onefold_model(X_train, Y_train_2, features, folds, model_name, save_path, num_seeds, \n                        num_splits, model_params, X_test = None, sample_sub_path = None, verbose = 0):\n    start_time_all = time()\n    oof = Y_train_2.copy()\n    oof.loc[:, Y_train_2.columns] = 0\n    if X_test is not None:\n        sub = pd.read_csv(sample_sub_path)\n        sub.loc[:, Y_train_2.columns] = 0\n    else:\n        sub = None\n    for nums, seed in enumerate(range(num_seeds)):\n        start_time_seed = time()\n        tf.random.set_seed(seed)\n        np.random.seed(seed)\n        random.seed(seed) \n        mean_score = 0\n#         skf = MultilabelStratifiedKFold(n_splits = num_splits, random_state = seed, shuffle = True)\n        for n, foldno in enumerate(set(folds[nums])):\n            start_time_fold = time()\n            tr = folds[nums] != foldno\n            te = folds[nums] == foldno\n            x_tr, x_val = X_train.values[tr][:, features], X_train.values[te][:, features]\n            y_tr, y_val = Y_train_2.values[tr], Y_train_2.values[te]\n\n            if X_test is not None:\n                x_tt = X_test.values[:, features]\n\n            ckp_path = save_path + f'{model_name}_Seed_{seed}_Fold_{n}.hdf5'\n            model = create_model(x_tr.shape[1], 206, **model_params)\n            rlr = ReduceLROnPlateau(monitor = 'val_mean_loss', factor = 0.1, patience = 3, \n                                    verbose = verbose, min_delta = 1e-4, mode = 'min')\n            ckp = ModelCheckpoint(ckp_path, monitor = 'val_mean_loss', verbose = verbose, \n                                  save_best_only = True, save_weights_only = True, mode = 'min')\n            es = EarlyStopping(monitor = 'val_mean_loss', min_delta = 1e-4, patience = 10, mode = 'min', \n                               baseline = None, restore_best_weights = True, verbose = verbose)\n            history = model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs = 1000, \n                                batch_size = 128, callbacks=[rlr, ckp, es], verbose = verbose)\n            hist = pd.DataFrame(history.history)\n            model.load_weights(ckp_path)\n            \n            val_predict = model.predict(x_val)\n            fold_score = hist['val_mean_loss'].min()\n            # fold_score = log_loss_metric(y_val, val_predict)\n            mean_score += fold_score / num_splits\n            oof.loc[te, Y_train_2.columns] += val_predict / num_seeds\n            if X_test is not None:\n                test_predict = model.predict(x_tt)\n                sub.loc[:, Y_train_2.columns] += test_predict / (num_splits * num_seeds)\n            print(f'[{str(datetime.timedelta(seconds = time() - start_time_fold))[0:7]}] {model_name} Seed {seed}, Fold {n}:', fold_score)\n            \n            del model\n            x = gc.collect()\n            K.clear_session()\n\n            break\n        break\n    return fold_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def train_onefold_ensemble(X_train, Y_train_2, folds, model_name, save_path, num_seeds, \n#                            num_splits, model_params, X_test = None, sample_sub_path = None, verbose = 0):\n#     start_time_all = time()\n#     oof = Y_train_2.copy()\n#     oof.loc[:, Y_train_2.columns] = 0\n#     if X_test is not None:\n#         sub = pd.read_csv(sample_sub_path)\n#         sub.loc[:, Y_train_2.columns] = 0\n#     else:\n#         sub = None\n#     for nums, seed in enumerate(range(num_seeds)):\n#         start_time_seed = time()\n#         tf.random.set_seed(seed)\n#         np.random.seed(seed)\n#         random.seed(seed) \n#         mean_score = 0\n# #         skf = MultilabelStratifiedKFold(n_splits = num_splits, random_state = seed, shuffle = True)\n#         for n, foldno in enumerate(set(folds[nums])):\n#             start_time_fold = time()\n#             tr = folds[nums] != foldno\n#             te = folds[nums] == foldno\n#             x_tr, x_val = X_train[tr], X_train[te]\n#             y_tr, y_val = Y_train_2.values[tr], Y_train_2.values[te]\n\n#             if X_test is not None:\n#                 x_tt = X_test\n\n#             ckp_path = save_path + f'{model_name}_Seed_{seed}_Fold_{n}.hdf5'\n#             model = create_model(x_tr.shape[1], 206, **model_params)\n#             rlr = ReduceLROnPlateau(monitor = 'val_mean_loss', factor = 0.1, patience = 3, \n#                                     verbose = verbose, min_delta = 1e-4, mode = 'min')\n#             ckp = ModelCheckpoint(ckp_path, monitor = 'val_mean_loss', verbose = verbose, \n#                                   save_best_only = True, save_weights_only = True, mode = 'min')\n#             es = EarlyStopping(monitor = 'val_mean_loss', min_delta = 1e-4, patience = 10, mode = 'min', \n#                                baseline = None, restore_best_weights = True, verbose = verbose)\n#             history = model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs = 1000, \n#                                 batch_size = 128, callbacks=[rlr, ckp, es], verbose = verbose)\n#             hist = pd.DataFrame(history.history)\n#             model.load_weights(ckp_path)\n            \n#             val_predict = model.predict(x_val)\n#             fold_score = hist['val_mean_loss'].min()\n#             # fold_score = log_loss_metric(y_val, val_predict)\n#             mean_score += fold_score / num_splits\n#             oof.loc[te, Y_train_2.columns] += val_predict / num_seeds\n#             if X_test is not None:\n#                 test_predict = model.predict(x_tt)\n#                 sub.loc[:, Y_train_2.columns] += test_predict / (num_splits * num_seeds)\n#             print(f'[{str(datetime.timedelta(seconds = time() - start_time_fold))[0:7]}] {model_name} Seed {seed}, Fold {n}:', fold_score)\n            \n#             del model\n#             x = gc.collect()\n#             K.clear_session()\n\n#             break\n#         break\n#     return fold_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def optimise(params):\n    print(params)\n    hidden_units = []\n    dropout_rates = []\n    num_groups = []\n    for key in params.keys():\n        if 'hidden_unit' in key:\n            hidden_units.append(params[key])\n        elif 'dropout_rate' in key:\n            dropout_rates.append(params[key])\n        elif 'num_groups' in key:\n            num_groups.append(params[key])\n\n    N_STARTS = 1\n    N_SPLITS = 5\n    model_name = 'HModel'\n    save_path = ''\n    model_params = {'hidden_units': hidden_units,  \n                    'dropout_rates': dropout_rates, \n                    'num_groups': num_groups, \n                   }\n    oof_score = train_onefold_model(train, train_targets, top_feats, folds_split, model_name, save_path, \n                                    N_STARTS, N_SPLITS, model_params)\n    return oof_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def optimise_ensemble(params):\n#     print(params)\n# #     hidden_units = [params['hidden_unit_1'], params['hidden_unit_2']]\n# #     dropout_rates = [params['dropout_rate_1'], params['dropout_rate_2'], params['dropout_rate_3']]\n#     hidden_units = []\n#     dropout_rates = []\n#     for key in params.keys():\n#         if 'hidden_unit' in key:\n#             hidden_units.append(params[key])\n#         elif 'dropout_rate' in key:\n#             dropout_rates.append(params[key])\n\n#     N_STARTS = 1\n#     N_SPLITS = 5\n#     model_name = 'HModel'\n#     save_path = ''\n#     model_params = {'hidden_units': hidden_units,  \n#                     'dropout_rates': dropout_rates, \n#                     'n_highway_layers': params['n_highway_layers'], \n#                    }\n#     oof_score = train_onefold_ensemble(train_new, train_targets, folds_split, model_name, save_path, \n#                                        N_STARTS, N_SPLITS, model_params)\n#     return oof_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# N_STARTS = 3\n# N_SPLITS = 5\n\n# folds_split = create_folds(N_STARTS, N_SPLITS)\n# print(folds_split)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# param_space = {'hidden_unit_1': scope.int(hp.quniform('hidden_unit_1', 128, 512, 128)), \n#                'hidden_unit_2': scope.int(hp.quniform('hidden_unit_2', 128, 512, 128)), \n#                'hidden_unit_3': scope.int(hp.quniform('hidden_unit_3', 128, 512, 128)),\n#                'hidden_unit_4': scope.int(hp.quniform('hidden_unit_4', 128, 512, 128)),\n#                'hidden_unit_5': scope.int(hp.quniform('hidden_unit_5', 128, 512, 128)),\n#                'dropout_rate_1': hp.uniform('dropout_rate_1', 0.1, 0.5),  \n#                'dropout_rate_2': hp.uniform('dropout_rate_2', 0.1, 0.5),\n#                'dropout_rate_3': hp.uniform('dropout_rate_3', 0.1, 0.5), \n#                'dropout_rate_4': hp.uniform('dropout_rate_4', 0.1, 0.5),\n#                'dropout_rate_5': hp.uniform('dropout_rate_5', 0.1, 0.5),\n#                'num_groups_1': hp.choice('num_groups_1', [4, 8, 16]), \n#                'num_groups_2': hp.choice('num_groups_2', [4, 8, 16]),\n#                'num_groups_3': hp.choice('num_groups_3', [4, 8, 16]),\n#                'num_groups_4': hp.choice('num_groups_4', [4, 8, 16]),\n#               }\n\n# trials = Trials()\n\n# hopt = fmin(fn = optimise, \n#             space = param_space, \n#             algo = tpe.suggest, \n#             max_evals = 50, \n#             trials = trials, \n#            )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dump(trials, 'trials_DT.pkl', compress = True)\n# print(hopt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scores = []\n# sets = []\n# for item in trials.trials:\n#     scores.append(item['result']['loss'])\n#     sets.append(item['misc']['vals'])\n\n# max_values = np.argsort(scores)[:10]\n# for m in max_values:\n#     print(scores[m])\n#     print(sets[m])\n#     print('-' * 50)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}