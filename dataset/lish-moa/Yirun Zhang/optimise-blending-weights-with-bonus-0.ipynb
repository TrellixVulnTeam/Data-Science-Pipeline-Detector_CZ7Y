{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Model Blending Weights Optimisation\n\nThis demo shows how to use [scipy.optimize][1] to optimise your model blending weights using your models' OOFs.\n\n**UPDATE:** Getting rid of the penalty term by using 'SLSQP' solver with a relatively small tolerance and Jacobian matrix.\n\n**UPDATE:** Calculate the gradients with paper and pencil to accelerate the optimisation...\n\n**UPDATE:** Add numba gradient function\n\n[1]: https://docs.scipy.org/doc/scipy/reference/optimize.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install autograd --quiet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import datetime\nimport pandas as pd\nfrom time import time\n# from autograd import grad\n# import autograd.numpy as np\nimport numpy as np\nfrom numba import njit\nfrom scipy.optimize import minimize, fsolve","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Objective Function and Gradients\n\n$$\nF = -\\frac{1}{NM}\\sum_{m=1}^{M}\\sum_{i=1}^{N}\\left[ y_{i,m}{\\rm log}\\left( \\sum_{k=1}^{K}w_{k}\\hat{y}_{i,m,k} \\right) + \\left( 1 - y_{i,m} \\right) {\\rm log}\\left( 1 - \\sum_{k=1}^{K}w_{k}\\hat{y}_{i,m,k} \\right) \\right],\n$$\n\n$$\n\\frac{\\partial F}{\\partial w_{k}} = -\\frac{1}{NM}\\sum_{m=1}^{M}\\sum_{i=1}^{N}\\left[ \\frac{-y_{i,m}\\hat{y}_{i,m,k}+\\hat{y}_{i,m,k}^{2}w_{k}+\\hat{y}_{i,m,k}\\sum_{j=1, j\\neq k}^{K}\\left( w_{j}\\hat{y}_{i,m,j}\\right)}{\\hat{y}_{i,m,k}^{2}w_{k}^{2}+2\\hat{y}_{i,m,k}\\sum_{j=1, j\\neq k}^{K}\\left( w_{j}\\hat{y}_{i,m,j}\\right)w_{k}-\\hat{y}_{i,m,k}w_{k}+\\left(\\sum_{j=1, j\\neq k}^{K}\\left( w_{j}\\hat{y}_{i,m,j}\\right)\\right)^{2}-\\sum_{j=1, j\\neq k}^{K}\\left( w_{j}\\hat{y}_{i,m,j}\\right)} \\right], \\quad k = 1, ..., K.\n$$"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CPMP's logloss from https://www.kaggle.com/c/lish-moa/discussion/183010\ndef log_loss_numpy(y_pred):\n    y_true_ravel = np.asarray(y_true).ravel()\n    y_pred = np.asarray(y_pred).ravel()\n    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    loss = np.where(y_true_ravel == 1, - np.log(y_pred), - np.log(1 - y_pred))\n    return loss.mean()\n\ndef func_numpy_metric(weights):\n    oof_blend = np.tensordot(weights, oof, axes = ((0), (0)))\n    return log_loss_numpy(oof_blend)\n\ndef grad_func(weights):\n    oof_clip = np.clip(oof, 1e-15, 1 - 1e-15)\n    gradients = np.zeros(oof.shape[0])\n    for i in range(oof.shape[0]):\n        a, b, c = y_true, oof_clip[i], np.zeros((oof.shape[1], oof.shape[2]))\n        for j in range(oof.shape[0]):\n            if j != i:\n                c += weights[j] * oof_clip[j]\n        gradients[i] = -np.mean((-a*b+(b**2)*weights[i]+b*c)/((b**2)*(weights[i]**2)+2*b*c*weights[i]-b*weights[i]+(c**2)-c))\n    return gradients\n\n@njit\ndef grad_func_jit(weights):\n    oof_clip = np.minimum(1 - 1e-15, np.maximum(oof, 1e-15))\n    gradients = np.zeros(oof.shape[0])\n    for i in range(oof.shape[0]):\n        a, b, c = y_true, oof_clip[i], np.zeros((oof.shape[1], oof.shape[2]))\n        for j in range(oof.shape[0]):\n            if j != i:\n                c += weights[j] * oof_clip[j]\n        gradients[i] = -np.mean((-a*b+(b**2)*weights[i]+b*c)/((b**2)*(weights[i]**2)+2*b*c*weights[i]-b*weights[i]+(c**2)-c))\n    return gradients","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model OOF Scores\n\nHere are my oof scores. You may use your own oof scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = pd.read_csv('../input/lish-moa/train_targets_scored.csv', index_col = 'sig_id').values\n\noof_dict = {'Model 1': '../input/moa-oof-demo/oof1.npy', \n            'Model 2': '../input/moa-oof-demo/oof2.npy', \n            'Model 3': '../input/moa-oof-demo/oof3.npy'\n           }\n\noof = np.zeros((len(oof_dict), y_true.shape[0], y_true.shape[1]))\nfor i in range(oof.shape[0]):\n    oof[i] = np.load(list(oof_dict.values())[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nlog_loss_scores = {}\nfor n, key in enumerate(oof_dict.keys()):\n    score_oof = log_loss_numpy(oof[n])\n    log_loss_scores[key] = score_oof\n    print(f'{key} CV:\\t', score_oof)\nprint('-' * 50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test Numba Gradient Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_weights = np.array([1 / oof.shape[0]] * oof.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%timeit -r 10 grad_func(test_weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%timeit -r 10 grad_func_jit(test_weights)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Blending Weights Optimisation\n\nProviding jac is optional because scipy uses its own 2-point finite difference estimation for the Jacobian matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"tol = 1e-10\ninit_guess = [1 / oof.shape[0]] * oof.shape[0]\nbnds = [(0, 1) for _ in range(oof.shape[0])]\ncons = {'type': 'eq', \n        'fun': lambda x: np.sum(x) - 1, \n        'jac': lambda x: [1] * len(x)}\n\nprint('Inital Blend OOF:', func_numpy_metric(init_guess))\nstart_time = time()\nres_scipy = minimize(fun = func_numpy_metric, \n                     x0 = init_guess, \n                     method = 'SLSQP', \n                     jac = grad_func_jit, # grad_func \n                     bounds = bnds, \n                     constraints = cons, \n                     tol = tol)\nprint(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] Optimised Blend OOF:', res_scipy.fun)\nprint('Optimised Weights:', res_scipy.x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Check the sum of all weights:', np.sum(res_scipy.x))\nif np.sum(res_scipy.x) - 1 <= tol:\n    print('Great! The sum of all weights equals to 1!')\nelse:\n    print('Manual adjustion is needed to modify the weights.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bonus (Lagrange Multiplier)\n\nCongratulations! You have found this bonus. In this section, I optimise the blending weights in a more mathematical way using Lagrange Multiplier method. The following equation is the minimisation problem that we want to solve:\n\n$$\n\\begin{align}\n\\min_{w_{1}, w_{2},..., w_{K}} \\quad &-\\frac{1}{NM}\\sum_{m=1}^{M}\\sum_{i=1}^{N}\\left[ y_{i,m}{\\rm log}\\left( \\sum_{k=1}^{K}w_{k}\\hat{y}_{i,m,k} \\right) + \\left( 1 - y_{i,m} \\right) {\\rm log}\\left( 1 - \\sum_{k=1}^{K}w_{k}\\hat{y}_{i,m,k} \\right) \\right], \\qquad {\\rm (1)} \\\\\ns.t. \\quad &\\sum_{k=1}^{k}w_{k} = 1, \\qquad {\\rm (1a)} \\\\\n& 0 \\leqslant w_{k} \\leqslant 1, \\quad k = 1, ..., K, \\qquad {\\rm (1b)}\n\\end{align}\n$$\n\nwhere $N$ is the number of 'sigid' observations in the test data $(i = 1, ...,N)$;\n\n$M$ is the number of scored MoA targets $(m = 1, ...,M)$;\n\n$w_{k}$ is the blending weight for the $k$th model's prediction results $(k = 1, ...,K)$; \n\n$\\hat{y}_{i,m,k}$ is the $k$th model's predicted probability of the $m$th positive MoA response for the $n$th 'sigid'; \n\n$y_{i,m}$ is the groundtruth of the $m$th positive MoA response for the $n$th 'sigid', 1 for a positive response, 0 otherwise; \n\n${\\rm log}(.)$ is the natural (base e) logarithm.\n\nAccording to the [Extreme Value Thereom][1], Constraint (1b) indicates Eq. (1) has absolute maximum and minimum values. We apply the [Lagrange Multiplier][2] method to this optimsiation problem. The new optimisation problem is expressed as follows:\n\n$$\n\\begin{align}\n\\min_{w_{1}, w_{2},..., w_{K}} \\quad &L = -\\frac{1}{NM}\\sum_{m=1}^{M}\\sum_{i=1}^{N}\\left[ y_{i,m}{\\rm log}\\left( \\sum_{k=1}^{K}w_{k}\\hat{y}_{i,m,k} \\right) + \\left( 1 - y_{i,m} \\right) {\\rm log}\\left( 1 - \\sum_{k=1}^{K}w_{k}\\hat{y}_{i,m,k} \\right) \\right] - \\lambda\\left(\\sum_{k=1}^{K}w_{k} - 1\\right), \\qquad {\\rm (2)} \\\\\ns.t. \\quad &0 \\leqslant w_{k} \\leqslant 1, \\quad k = 1, ..., K, \\qquad {\\rm (2b)}\n\\end{align}\n$$\n\nwhere $\\lambda$ is the Lagrange multiplier.\n\nThe [Karush–Kuhn–Tucker condition][3] for the optimal solution is:\n$$\n\\left\\{\\begin{matrix}\n\\frac{\\partial L}{\\partial w_{k}} = 0, & k = 1, ..., K, \\\\ \n\\frac{\\partial L}{\\partial \\lambda} = 0, & \n\\end{matrix}\\right., \\qquad {\\rm (3)}\n$$\n\nFrom Eq. (3), we end up with $K+1$ equations that equal zero, we can simply use [autograd][4] to calculate the partial derivatives and [scipy.optimize.fsolve][5] to get the optimal solution.\n\n[1]: https://en.wikipedia.org/wiki/Extreme_value_theorem\n[2]: https://en.wikipedia.org/wiki/Lagrange_multiplier\n[3]: https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions\n[4]: https://github.com/HIPS/autograd\n[5]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fsolve.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def Lagrange_func(params):\n#     w1, w2, w3, _lambda = params\n#     oof_blend = w1 * oof[0] + w2 * oof[1] + w3 * oof[2]\n#     return log_loss_numpy(oof_blend) - _lambda * (w1 + w2 + w3 - 1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# grad_L = grad(Lagrange_func)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def Lagrange_obj(params):\n#     w1, w2, w3, _lambda = params\n#     dLdw1, dLdw2, dLdw3, dLdlam = grad_L(params)\n#     return [dLdw1, dLdw2, dLdw3, w1 + w2 + w3 - 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start_time = time()\n# w1, w2, w3, _lambda = fsolve(Lagrange_obj, [0.3, 0.3, 0.4, 1.0])\n# print(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] Optimised Weights:', [w1, w2, w3])\n# oof_b = w1 * oof[0] + w2 * oof[1] + w3 * oof[2]\n# print('Optimised Blend OOF:', log_loss_numpy(oof_b))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('Check Condition (1a):', w1 + w2 + w3)\n# if w1 + w2 + w3 - 1 <= tol:\n#     print('Great! The sum of all weights equals to 1!')\n# else:\n#     print('Manual adjustion is needed to modify the weights.')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}