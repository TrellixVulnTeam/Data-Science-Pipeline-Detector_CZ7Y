{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Kernel Logistic Regression\n\n* **Version 1-3:** Nystroem Kernel Approximation + Logistic Regression\n* **Version 4:** Kernel Ridge Regression + Platt Scaling\n* **Version 5:** Kernel Ridge Regression + Platt Scaling + Tuned Hyperparameter C's\n* **Version 6:** Kernel Ridge Regression + Inter-Target Platt Scaling (CPU) The training time is too long so I cancelled it.\n* **Version 7:** Kernel Ridge Regression + Inter-Target Platt Scaling (GPU)\n* **Version 8:** Kernel Ridge Regression + Platt Scaling (CPU) + Remove Control Group\n* **Version 9-13:** GroupCV + Quantile Transformation + Label Smoothing\n* **Version 14:** Add PCA and VarianceThreshold Feature Selection\n\nIn this example, I play with the kernel logistic regression method. Scikit-Learn does not have kernel logistic regression. Instead, I use kernel ridge regression and platt scaling. According to the [Kernel Ridge Regression][1] document on Scikit-Learn, It should perform as well as SVR.\n\nP.S. The inter-target Platt Scaling means I consider target relationships during Platt Scaling.\n\n[1]: https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\n# Thanks to Chris's RAPIDS dataset, it only takes around 1 min to install offline\n# !cp ../input/rapids/rapids.0.15.0 /opt/conda/envs/rapids.tar.gz\n# !cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\n# sys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\n# sys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\n# sys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n# !cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"kw1VW6DCvgSq","outputId":"030d81e0-579d-463d-b2ed-6c714151a063"},"cell_type":"code","source":"import os\nimport gc\nimport datetime\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\nfrom abc import abstractmethod, ABCMeta\nfrom sklearn.kernel_approximation import Nystroem\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.linear_model import LogisticRegression\n# from cuml import LogisticRegression\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom tqdm.notebook import tqdm\nfrom time import time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_folds(num_starts, num_splits):\n    \n    folds = []\n    \n    # LOAD FILES\n    train_feats = pd.read_csv('../input/lish-moa/train_features.csv')\n    scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\n    drug = pd.read_csv('/kaggle/input/lish-moa/train_drug.csv')\n    scored = scored.loc[train_feats['cp_type'] == 'trt_cp', :]\n    drug = drug.loc[train_feats['cp_type'] == 'trt_cp', :]\n    targets = scored.columns[1:]\n    scored = scored.merge(drug, on = 'sig_id', how = 'left') \n\n    # LOCATE DRUGS\n    vc = scored.drug_id.value_counts()\n    vc1 = vc.loc[vc <= 18].index.sort_values()\n    vc2 = vc.loc[vc > 18].index.sort_values()\n    \n    for seed in range(num_starts):\n\n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}; dct2 = {}\n        skf = MultilabelStratifiedKFold(n_splits = num_splits, shuffle = True, random_state = seed)\n        tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n        for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[targets])):\n            dd = {k:fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits = num_splits, shuffle = True, random_state = seed)\n        tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop = True)\n        for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[targets])):\n            dd = {k:fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        scored['fold'] = scored.drug_id.map(dct1)\n        scored.loc[scored.fold.isna(),'fold'] =\\\n            scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\n        scored.fold = scored.fold.astype('int8')\n        folds.append(scored.fold.values)\n        \n        del scored['fold']\n        \n    return np.stack(folds)","execution_count":null,"outputs":[]},{"metadata":{"id":"dSVuPpi2vgSv"},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"UvG3N1HHvgSv"},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\n\nss_krr = pd.read_csv('../input/lish-moa/sample_submission.csv')\nss_lr = ss_krr.copy()\n\ncols = [c for c in ss_krr.columns.values if c != 'sig_id']\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"JItYfC6jvgSy"},"cell_type":"code","source":"def preprocess(df):\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 0.5, 72: 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    return df\n\ndef log_loss_metric(y_true, y_pred):\n    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    return - np.mean(y_true * np.log(y_pred_clip) + (1 - y_true) * np.log(1 - y_pred_clip))\n\ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']\ndel train_targets_nonscored['sig_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer\n\nqt = QuantileTransformer(n_quantiles = 100, output_distribution = 'normal', random_state = 42)\nqt.fit(pd.concat([pd.DataFrame(train[GENES+CELLS]), pd.DataFrame(test[GENES+CELLS])]))\ntrain[GENES+CELLS] = qt.transform(train[GENES+CELLS])\ntest[GENES+CELLS] = qt.transform(test[GENES+CELLS])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n# GENES\nn_comp_genes = 600\n\ndata = pd.concat([pd.DataFrame(train[GENES]), pd.DataFrame(test[GENES])])\npca_genes = PCA(n_components=n_comp_genes, random_state = 42)\ndata2 = pca_genes.fit_transform(data[GENES])\ntrain2 = data2[:train.shape[0]]; test2 = data2[-test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp_genes)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp_genes)])\n\ntrain = pd.concat((train, train2), axis=1)\ntest = pd.concat((test, test2), axis=1)\n\n#CELLS\nn_comp_cells = 50\n\ndata = pd.concat([pd.DataFrame(train[CELLS]), pd.DataFrame(test[CELLS])])\npca_cells = PCA(n_components=n_comp_cells, random_state = 42)\ndata2 = pca_cells.fit_transform(data[CELLS])\ntrain2 = data2[:train.shape[0]]; test2 = data2[-test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp_cells)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp_cells)])\n\ntrain = pd.concat((train, train2), axis=1)\ntest = pd.concat((test, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n\nvar_thresh = VarianceThreshold(0.8)\ndata = train.append(test)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 3:])\n\ntrain_transformed = data_transformed[ : train.shape[0]]\ntest_transformed = data_transformed[-test.shape[0] : ]\n\ntrain = pd.DataFrame(train[['cp_type','cp_time','cp_dose']].values.reshape(-1, 3),\\\n            columns=['cp_type','cp_time','cp_dose'])\n\ntrain = pd.concat([train, pd.DataFrame(train_transformed)], axis=1)\n\ntest = pd.DataFrame(test[['cp_type','cp_time','cp_dose']].values.reshape(-1, 3),\\\n            columns=['cp_type','cp_time','cp_dose'])\n\ntest = pd.concat([test, pd.DataFrame(test_transformed)], axis=1)\n\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets = train_targets.loc[train['cp_type'] == 0].reset_index(drop = True)\ntrain_targets_nonscored = train_targets_nonscored.loc[train['cp_type'] == 0].reset_index(drop = True)\ntrain = train.loc[train['cp_type'] == 0].reset_index(drop = True)\n\nprint(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_feats = np.arange(1, train.shape[1])\nprint(top_feats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_SPLITS = 7\nLBS = 0.0008\nfolds = create_folds(1, N_SPLITS)\nprint(folds)","execution_count":null,"outputs":[]},{"metadata":{"id":"0eDJ68r-vgTA"},"cell_type":"markdown","source":"# Kernel Logistic Regression"},{"metadata":{"trusted":true,"id":"qiCub3F5vgTA","outputId":"a409642d-80cd-4fdb-d21b-586422655f38"},"cell_type":"code","source":"res_krr = train_targets.copy()\nss_krr.loc[:, train_targets.columns] = 0\nres_krr.loc[:, train_targets.columns] = 0\n\n# for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits = N_SPLITS, random_state = 0, shuffle = True).split(train_targets, train_targets)):\nfor n, foldno in enumerate(set(folds[0])):\n    start_time = time()\n    tr = folds[0] != foldno\n    te = folds[0] == foldno\n\n    x_tr, x_val = train.values[tr][:, top_feats], train.values[te][:, top_feats]\n    y_tr, y_val = train_targets.astype(float).values[tr], train_targets.astype(float).values[te]\n    x_tt = test.values[:, top_feats]\n    \n    # Label Smoothing\n    y_tr = y_tr * (1 - LBS) + 0.5 * LBS\n    \n    model = KernelRidge(alpha = 80, kernel = 'rbf')\n    model.fit(x_tr, y_tr)\n\n    ss_krr.loc[:, train_targets.columns] += model.predict(x_tt) / N_SPLITS\n    fold_pred = model.predict(x_val)\n    res_krr.loc[te, train_targets.columns] += fold_pred\n    fold_score = log_loss_metric(train_targets.loc[te].values, fold_pred)\n    print(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] KRR: Fold {n}:', fold_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"dgIrzdQZvgTC","outputId":"a2ab66fb-1783-45aa-b1ef-6a6f719dce6a"},"cell_type":"code","source":"print(f'Model OOF Metric: {log_loss_metric(train_targets.values, res_krr.values)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Platt Scaling\n\nTrain a Logistic Regression model to calibrate the results of Kernel Ridge Regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_new = res_krr[cols].values\nx_tt_new = ss_krr[cols].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res_lr = train_targets.copy()\nss_lr.loc[:, train_targets.columns] = 0\nres_lr.loc[:, train_targets.columns] = 0\n\nfor tar in tqdm(range(train_targets.shape[1])):\n\n#     start_time = time()\n    targets = train_targets.values[:, tar]\n\n    if targets.sum() >= N_SPLITS:\n\n        skf = StratifiedKFold(n_splits = N_SPLITS, random_state = 0, shuffle = True)\n\n        for n, (tr, te) in enumerate(skf.split(targets, targets)):\n\n            x_tr, x_val = X_new[tr, tar].reshape(-1, 1), X_new[te, tar].reshape(-1, 1)\n            y_tr, y_val = targets[tr], targets[te]\n\n            model = LogisticRegression(penalty = 'none', max_iter = 1000)\n            model.fit(x_tr, y_tr)\n            ss_lr.loc[:, train_targets.columns[tar]] += model.predict_proba(x_tt_new[:, tar].reshape(-1, 1))[:, 1] / N_SPLITS\n            res_lr.loc[te, train_targets.columns[tar]] += model.predict_proba(x_val)[:, 1]\n\n    score = log_loss(train_targets.loc[:, train_targets.columns[tar]].values, res_lr.loc[:, train_targets.columns[tar]].values)\n#     print(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] LR Target {tar}:', score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'LR OOF Metric: {log_loss_metric(train_targets.values, res_lr.values)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save('klr_oof.npy', res_lr[cols].values)\nnp.save('klr_sub.npy', ss_lr[cols].values)","execution_count":null,"outputs":[]},{"metadata":{"id":"4i2yuxNCvgTV"},"cell_type":"markdown","source":"# Submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_lr.loc[test['cp_type'] == 1, train_targets.columns] = 0\nss_lr.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}