{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Loading files:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nimport keras\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras import Sequential,Model,backend\nfrom tensorflow.keras import layers,regularizers\nfrom tensorflow.keras import callbacks,optimizers,metrics,losses\n\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\n\n# Utility methods\ndef plotFeatures(columns, plt):\n    cols =columns[0:12]\n    for i, col in enumerate(cols):\n        plt.subplot(5, 5, i + 1)\n        plt.hist(train_features.loc[:, col], bins=200, alpha=1);\n        plt.title(col)\n        \ndef getMissingPercent(series):\n    num = series.isnull().sum()\n    den = len(series)\n    return round(num/den, 2)\n\ndef getFeatures(X_train,X_test):\n    \n    ngFeatures = 20\n    ncFeatures = 100\n    \n    pcFeatures = PCA(n_components = ncFeatures)\n    pgFeatures = PCA(n_components = ngFeatures)\n\n    X_train_pgFeatures = pgFeatures.fit_transform(X_train[:,gfeatures])\n    X_train_pcFeatures = pcFeatures.fit_transform(X_train[:,cfeatures])\n    X_test_pgFeatures = pgFeatures.transform(X_test[:,gfeatures])\n    X_test_pcFeatures = pcFeatures.transform(X_test[:,cfeatures])\n    \n    X_train_c_mean = X_train[:,cfeatures].mean(axis=1)\n    X_test_c_mean = X_test[:,cfeatures].mean(axis=1)\n    \n    X_train_g_mean = X_train[:,gfeatures].mean(axis=1)\n    X_test_g_mean = X_test[:,gfeatures].mean(axis=1)\n    \n    X_train = np.concatenate((X_train,X_train_pgFeatures,X_train_pcFeatures,X_train_c_mean[:,np.newaxis]\n                            ,X_train_g_mean[:,np.newaxis]),axis=1)\n    X_test = np.concatenate((X_test,X_test_pgFeatures,X_test_pcFeatures,X_test_c_mean[:,np.newaxis],\n                           X_test_g_mean[:,np.newaxis]),axis=1)\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    \n    return X_train, X_test\n\ndef lossFunction(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -backend.mean(y_true * backend.log(y_pred) + (1-y_true) * backend.log(1-y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv(f'../input/lish-moa/train_features.csv')\ntrain_target = pd.read_csv(f'../input/lish-moa/train_targets_scored.csv')\n\ntest_features = pd.read_csv(f'../input/lish-moa/test_features.csv')\nsample_sub = pd.read_csv(f'../input/lish-moa/sample_submission.csv')\n\nprint(\"Sample Training set features\")\ntrain_features.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Sample Training set target\")\ntrain_target.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.groupby( ['cp_dose','cp_type','cp_time'] ).agg( ['mean','std'] )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization of the Data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summing the columsn across as the target is one hot encoded\nfig = plt.figure(figsize=(14,6))\nplt.bar(train_target.iloc[:,1:].sum(axis=0).sort_values(ascending=False)[:25].index, \n            train_target.iloc[:,1:].sum(axis=0).sort_values(ascending=False)[:25].values)\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing features that start with c\ncCols = train_features.columns[train_features.columns.str.startswith('c-')]\nplt.figure(figsize=(15, 15))\nplotFeatures(cCols, plt)\n\n# Viewing features that start with g\ngCols = train_features.columns[train_features.columns.str.startswith('g-')]\nplt.figure(figsize=(15, 15))\nplotFeatures(gCols, plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Matrix\ncorr = train_features.corr()\n# Taking features with higher correlation\nplt.figure(figsize=(12,8))\nsns.heatmap(corr[corr>=.9])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Training and Predictions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for NaNs\nfor i in train_features.columns:\n    print(i,getMissingPercent(train_features[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_features = train_features.drop(['sig_id','cp_type','cp_dose','cp_time'],axis=1)\ntest_features = test_features.drop(['sig_id','cp_dose','cp_time'],axis=1)\n\ntrain_targets_scored = pd.read_csv(f'../input/lish-moa/train_targets_scored.csv')\ntrain_targets_scored.pop('sig_id')\nlabels_train = train_targets_scored.values\n\n# Considering numerical feature only\nnum_features= train_features.columns[train_features.dtypes!=\"object\"]\nnum_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfeatures = train_features.columns.str.contains('c-')\ngfeatures = train_features.columns.str.contains('g-')\n\nn_labels = train_targets_scored.shape[1]\nn_train = train_features.shape[0]\nn_test = test_features.shape[0]\n\np_min = 5E-5\np_max = 0.95","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"def sequenceInitOne(input):\n    seq = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(512), \n        layers.BatchNormalization(),\n        layers.Dense(256)\n        ]) \n    return seq(input)\n\ndef sequenceInitTwo(input):\n    seq = Sequential([\n            layers.BatchNormalization(),\n            layers.Dropout(0.3),\n            layers.Dense(512), \n            layers.BatchNormalization(),\n            layers.Dense(512),\n            layers.BatchNormalization(),\n            layers.Dense(256),\n            layers.BatchNormalization(),\n            layers.Dense(256)\n            ])\n    return seq(input)\n\ndef sequenceInitThree(input):\n    seq = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(1024), \n        layers.BatchNormalization(),\n        layers.Dense(512),\n        layers.BatchNormalization(),\n        layers.Dense(512),\n        layers.BatchNormalization(),\n        layers.Dense(256)\n        ])\n    return seq(input)\n\ndef sequenceInitFour(input):\n    seq = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(512), \n        layers.BatchNormalization(),\n        layers.Dense(512),\n        layers.BatchNormalization(),\n        layers.Dense(256),\n        layers.BatchNormalization(),\n        layers.Dense(256)\n        ])\n    return seq(input)\n\ndef sequenceInitFive(input):\n    seq = Sequential([\n        layers.BatchNormalization(),\n        layers.Dense(256),\n        layers.BatchNormalization(),\n        layers.Dense(n_labels),\n        layers.BatchNormalization(),\n        layers.Dense(n_labels)\n        ])\n    return seq(input)\n\ndef modelling (n_features, n_features_2, n_labels, opt, label_smoothing = 0.0005):    \n    input_1 = layers.Input(shape = (n_features,), name = 'Input1')\n    input_2 = layers.Input(shape = (n_features_2,), name = 'Input2')\n    input_3 = sequenceInitOne(input_1)\n    input_3_concat = layers.Concatenate()([input_2, input_3])\n\n    input_4 = sequenceInitTwo(input_3_concat)\n    input_4_avg = layers.Average()([input_3, input_4]) \n\n    input_5 = sequenceInitThree(input_4_avg)\n    input_5_avg = layers.Average()([input_4, input_5]) \n\n    input_6 = sequenceInitFour(input_5_avg)\n    input_6_avg = layers.Average()([input_5, input_6]) \n    \n    output = sequenceInitFive(input_6_avg)\n\n    model = Model(inputs = [input_1, input_2], outputs = output)\n    model.compile(optimizer=opt, loss=losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n                  metrics=lossFunction)\n    \n    return model\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for optimizer in ['adam','adagrad','SGD']:\n    X_train, X_test = getFeatures(train_features.values,train_features.values)\n    _,unknown = getFeatures(train_features.values,\n                                       test_features.drop('cp_type',axis=1).values)\n\n    allcols_train = train_features[num_features].values\n    allcols_test = train_features[num_features].values\n    unknown_2 = test_features[num_features].values\n\n    y_train = labels_train\n    y_test = labels_train\n    n_features = X_train.shape[1]\n    n_features_2 = allcols_train.shape[1]\n\n    model = modelling(n_features, n_features_2, n_labels, opt=optimizer)\n    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_lossFunction',\n                                            mode='min', factor=0.1, patience=2)\n    early_stopping = callbacks.EarlyStopping(monitor='val_lossFunction',\n                                             patience=10,\n                                             mode='min',restore_best_weights=True)\n    hist = model.fit([X_train,allcols_train],\n                     y_train, batch_size=128,\n                     epochs=3,verbose=1,\n                     validation_data = ([X_test,allcols_test],y_test),\n                     callbacks=[reduce_lr, early_stopping])\n\n    print(model.evaluate([X_test,allcols_test],y_test)[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_seeds = 5\nnp.random.seed(4545)\nn_folds = 5\nseeds = np.random.randint(0,100,size=n_seeds)\ny_pred = np.zeros((n_test,n_labels))\n\nfor seed in seeds:\n    fold = 0\n    kf = KFold(n_splits=n_folds,shuffle=True,random_state=seed)\n    for train, test in kf.split(train_features):\n        X_train, X_test = getFeatures(train_features.iloc[train].values,\n                                       train_features.iloc[test].values)\n        _,unknown = getFeatures(train_features.iloc[train].values,\n                                   test_features.drop('cp_type',axis=1).values)\n        \n        allcols_train = train_features.iloc[train][num_features].values\n        allcols_test = train_features.iloc[test][num_features].values\n        unknown_2 = test_features[num_features].values\n        \n        y_train = labels_train[train]\n        y_test = labels_train[test]\n        n_features = X_train.shape[1]\n        n_features_2 = allcols_train.shape[1]\n\n        model = modelling(n_features, n_features_2, n_labels, opt='adam')\n        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_lossFunction', mode='min', factor=0.1, patience=2)\n        early_stopping = callbacks.EarlyStopping(monitor='val_lossFunction',patience=10, mode='min',restore_best_weights=True)\n        hist = model.fit([X_train,allcols_train],y_train, batch_size=128, epochs=15,verbose=1,validation_data = ([X_test,allcols_test],y_test),\n                         callbacks=[reduce_lr, early_stopping])\n        \n        y_pred += model.predict([unknown,unknown_2])/(n_folds*n_seeds)\n\n        plt.plot(hist.history['lossFunction'])\n        plt.plot(hist.history['val_lossFunction'])\n        plt.title('model accuracy')\n        plt.ylabel('accuracy')\n        plt.xlabel('epoch')\n        plt.legend(['train', 'val'], loc='upper right')\n        plt.show()\n        \n        plt.plot(hist.history['loss'])\n        plt.plot(hist.history['val_loss'])\n        plt.title('model accuracy')\n        plt.ylabel('accuracy')\n        plt.xlabel('epoch')\n        plt.legend(['train', 'val'], loc='upper right')\n        plt.show()\n        fold += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub.iloc[:,1:] = y_pred\nsample_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}