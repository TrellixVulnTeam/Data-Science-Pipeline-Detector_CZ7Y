{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nfrom sklearn.svm import SVC\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:09:08.655157Z","iopub.execute_input":"2022-04-06T09:09:08.655718Z","iopub.status.idle":"2022-04-06T09:09:09.861386Z","shell.execute_reply.started":"2022-04-06T09:09:08.655584Z","shell.execute_reply":"2022-04-06T09:09:09.860783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TrainFeat = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\nTrainTargetScored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\nTestFeat = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\nSubmission = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:10:21.764011Z","iopub.execute_input":"2022-04-06T09:10:21.764507Z","iopub.status.idle":"2022-04-06T09:10:29.16428Z","shell.execute_reply.started":"2022-04-06T09:10:21.764474Z","shell.execute_reply":"2022-04-06T09:10:29.16353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ENCODING\nTrainFeat['cp_type'].replace(['ctl_vehicle', 'trt_cp'], [0,1], inplace=True)\nTrainFeat['cp_dose'].replace(['D1', 'D2'], [0,1], inplace=True)\n\nTestFeat['cp_type'].replace(['ctl_vehicle', 'trt_cp'], [0,1], inplace=True)\nTestFeat['cp_dose'].replace(['D1', 'D2'], [0,1], inplace=True)\n\nX=TrainFeat.drop([\"sig_id\"], axis =1)\ny=TrainTargetScored.drop([\"sig_id\"], axis =1)\nX_TestFeat=TestFeat.drop([\"sig_id\"], axis =1)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:11:03.470934Z","iopub.execute_input":"2022-04-06T09:11:03.471215Z","iopub.status.idle":"2022-04-06T09:11:03.522771Z","shell.execute_reply.started":"2022-04-06T09:11:03.471185Z","shell.execute_reply":"2022-04-06T09:11:03.5216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X AND y CONCATENATION\ndf_concat = pd.concat([X, y], axis=1)\n#  DETECTION OF TARGETS WITH SUM <=10\nv=np.array(y.sum())\npos = []\nx = 10 #The required element\nfor i in range(len(v)):\n    if v[i] <= x:\n        pos.append(i)\n        \n# OVER-SAMPLING FOR TARGETS WITH SUM <=10       \nfrom sklearn.utils import resample\nu=0\nj=len(X.columns) #875 colonnes\nfor i in pos:\n    not_MoA= df_concat[df_concat.iloc[:,i+j]==0]\n    MoA= df_concat[df_concat.iloc[:,i+j]==1]\n    u=u+1\n    # upsample minority\n    MoA_upsampled = resample(MoA,\n                          replace=True, # sample with replacement\n                          n_samples=200, # match number in majority class\n                          random_state=0) # reproducible results\n    # combine majority and upsampled minority\n    if u==1: upsampled = pd.concat([not_MoA, MoA, MoA_upsampled])\n    else: upsampled = pd.concat([upsampled , MoA_upsampled])","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:11:23.813989Z","iopub.execute_input":"2022-04-06T09:11:23.814291Z","iopub.status.idle":"2022-04-06T09:11:27.434122Z","shell.execute_reply.started":"2022-04-06T09:11:23.814256Z","shell.execute_reply":"2022-04-06T09:11:27.431784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate input features and targets\nX_up=upsampled[upsampled.columns[0:875]]\ny_up=upsampled[upsampled.columns[875:1081]]       \nfrom sklearn.utils import resample\nu=0\nj=len(X.columns) #875 colonnes\nfor i in pos:\n    not_MoA= df_concat[df_concat.iloc[:,i+j]==0]\n    MoA= df_concat[df_concat.iloc[:,i+j]==1]\n    u=u+1\n    # upsample minority\n    MoA_upsampled = resample(MoA,\n                          replace=True, # sample with replacement\n                          n_samples=200, # match number in majority class\n                          random_state=0) # reproducible results\n    # combine majority and upsampled minority\n    if u==1: upsampled = pd.concat([not_MoA, MoA, MoA_upsampled])\n    else: upsampled = pd.concat([upsampled , MoA_upsampled])\n\n# Separate input features and target\nX_up=upsampled[upsampled.columns[0:875]]\ny_up=upsampled[upsampled.columns[875:1081]]\n\n# Number of additional samples\na=len(X_up)-len(X)\nprint (X.shape)\nprint (X_up.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:12:06.682082Z","iopub.execute_input":"2022-04-06T09:12:06.683699Z","iopub.status.idle":"2022-04-06T09:12:09.927135Z","shell.execute_reply.started":"2022-04-06T09:12:06.683597Z","shell.execute_reply":"2022-04-06T09:12:09.92656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def classif2 (base, X, y, X_TestFeat):\n    losses_NN=[] \n    kf = KFold(n_splits=3,random_state=100, shuffle=True)\n    np.random.seed(1010)    \n    X2=X.to_numpy()\n    y2=y.to_numpy()\n    X_TestFeat2=X_TestFeat.to_numpy()\n    for train_index, test_index in kf.split(X2):\n        X_train, X_test = X2[train_index], X2[test_index]\n        y_train, y_test = y2[train_index], y2[test_index]         \n        model = MultiOutputClassifier(base)\n        train = model.fit(X_train, y_train)\n        predicted=np.transpose(np.array(train.predict_proba(X_test))[:, :,-1])\n        predictedTestFeat=np.transpose(np.array(train.predict_proba(X_TestFeat2))[:, :,-1])      \n        loss=log_loss(np.ravel(y_test), np.ravel(predicted))      \n        print('Loss: '+str(loss)) \n        losses_NN.append(loss)\n    print('Average Loss: '+str(np.average(losses_NN))) \n    return train, predictedTestFeat","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:12:35.256005Z","iopub.execute_input":"2022-04-06T09:12:35.25638Z","iopub.status.idle":"2022-04-06T09:12:35.263402Z","shell.execute_reply.started":"2022-04-06T09:12:35.25635Z","shell.execute_reply":"2022-04-06T09:12:35.262919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SCV\nbase = SVC (C=1e-3, random_state=0, kernel='linear', probability=True, decision_function_shape= 'ovo' )\nSCV_CV = classif2 (base, X_up, y_up, X_TestFeat)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:13:02.522049Z","iopub.execute_input":"2022-04-06T09:13:02.522319Z","iopub.status.idle":"2022-04-06T13:03:06.340385Z","shell.execute_reply.started":"2022-04-06T09:13:02.522289Z","shell.execute_reply":"2022-04-06T13:03:06.338843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictedTestFeat = SCV_CV[1]\nPred_OverS_SVC = pd.DataFrame(predictedTestFeat, columns = Submission.columns[1:])\nPred_OverS_SVC.insert(0,'sig_id', Submission.loc[:,'sig_id'])\nPred_OverS_SVC.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:05:43.161123Z","iopub.execute_input":"2022-04-06T13:05:43.162924Z","iopub.status.idle":"2022-04-06T13:05:44.652004Z","shell.execute_reply.started":"2022-04-06T13:05:43.162837Z","shell.execute_reply":"2022-04-06T13:05:44.650964Z"},"trusted":true},"execution_count":null,"outputs":[]}]}