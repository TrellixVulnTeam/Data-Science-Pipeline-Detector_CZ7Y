{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom datetime import datetime\nfrom tensorflow import keras\nfrom tensorflow import feature_column\nfrom tensorflow.keras import layers, regularizers\n\nprint(tf.__version__)\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n\nwith tf.device(\"gpu:0\"):\n   print(\"tf.keras code in this scope will run on GPU\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load dataset\nx_train = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ny_train = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\nx_test = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\n\ncols_x = x_train.columns\ncols_y = y_train.columns\n\nnum_x = len(cols_x)\nnum_y = len(cols_y)\n\nprint(f\"Features: {num_x}\")\nprint(f\"Labels: {num_y}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"default_records_features = [tf.string, tf.string, tf.string, tf.string] + [tf.float32] * (num_x-4)\ndefault_records_targets =  [tf.string] + [tf.float32] *(num_y-1)\n\ntrain_features = tf.data.experimental.CsvDataset(\"/kaggle/input/lish-moa/train_features.csv\",\n                                           record_defaults=default_records_features,\n                                           #select_cols\n                                           header=True)\n\ntrain_targets = tf.data.experimental.CsvDataset(\"/kaggle/input/lish-moa/train_targets_scored.csv\",\n                                          record_defaults=default_records_targets,\n                                          header=True)\n\ntest_features = tf.data.experimental.CsvDataset(\"/kaggle/input/lish-moa/test_features.csv\",\n                                           record_defaults=default_records_features,\n                                           header=True)\n\ntrain_dataset = tf.data.Dataset.zip((train_features, train_targets))\ntest_dataset = tf.data.Dataset.zip((test_features,train_targets))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split dataset into train and val\ndataset_size = len(x_train)\ntrain_size = int(0.7*dataset_size)\nval_size = dataset_size - train_size\ntest_size = len(x_test)\n\ntrain = train_dataset.take(train_size)\nval = train_dataset.skip(train_size)\nval = train_dataset.take(val_size)\n\nprint(\"Full dataset size:\", dataset_size)\nprint(\"Train dataset size:\", train_size)\nprint(\"Val dataset size:\", val_size)\nprint(\"Test dataset size:\", test_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 1024\n\ndef _preprocess_train(features, targets):\n    features = dict(zip(cols_x, features))\n    features.pop('sig_id')\n    targets = tf.stack(targets[1:])\n    return features, targets\n\ndef _preprocess_test(features, targets):\n    features = dict(zip(cols_x, features))\n    features.pop('sig_id')\n    return features\n\ntrain_ds = train.map(_preprocess_train).shuffle(buffer_size=num_x).batch(BATCH_SIZE)\nval_ds = val.map(_preprocess_train).batch(BATCH_SIZE)\ntest_ds = test_dataset.map(_preprocess_test).batch(BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_columns = []\nfeature_layer_inputs = {}\n\n# numeric cols\nfor num_col in list(cols_x[4:]):\n    feature_layer_inputs[num_col] = keras.Input(shape=(1,), name=num_col)\n    mean = x_train[num_col].mean()\n    std = x_train[num_col].std()\n    feature_columns.append(feature_column.numeric_column(num_col, normalizer_fn=lambda x: (x - mean) / std))\n\n# indicator_columns\nfor col_name in list(cols_x[1:4]):\n    feature_layer_inputs[col_name] = keras.Input(shape=(1,), name=col_name,  dtype=tf.string)\n    categorical_column = feature_column.categorical_column_with_vocabulary_list(col_name, x_train[col_name].unique().astype(str))\n    indicator_column = feature_column.indicator_column(categorical_column)\n    feature_columns.append(indicator_column)\n    \nfeature_layer_inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_size = num_y-1\n\nfeature_layer = layers.DenseFeatures(feature_columns)\nfeature_layer_outputs = feature_layer(feature_layer_inputs)\nx = layers.Dense(832)(feature_layer_outputs)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(448)(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU(0.5)(x)\nx = layers.Dropout(0.5)(x)\nout = layers.Dense(output_size, activation='sigmoid')(x)\n\nmodel = keras.Model([v for v in feature_layer_inputs.values()], outputs=out)\n\nopt = keras.optimizers.Adam(learning_rate=0.005863958845877649)\nmodel.compile(loss='binary_crossentropy', optimizer=opt)\n\ndef set_mode(model, train_mode):\n    for i in range(0, len(model.layers)):\n        model.layers[i].training = train_mode\n    model.compile(loss='binary_crossentropy', optimizer=opt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model\nearly_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\nhistory_callback = keras.callbacks.History()\n\nmodel.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=100,\n    callbacks=[early_stopping_callback, history_callback]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure, ax = plt.subplots()\nax.plot(history_callback.history['loss'], label='loss')\nax.plot(history_callback.history['val_loss'], label='val_loss')\nax.grid(True)\nax.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions\nwith tf.device(\"gpu:0\"):\n    set_mode(model, train_mode=False)\n    prediction = model.predict(test_ds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(prediction)\nsig_id = x_test.pop('sig_id').values\n\ndf.insert(0, 'sig_id', sig_id)\ndf.columns = ['sig_id'] + cols_y.tolist()[1:]\ndf.to_csv('/kaggle/working/submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}