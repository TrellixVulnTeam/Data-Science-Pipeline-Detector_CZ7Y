{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-26T12:20:05.607603Z","iopub.execute_input":"2021-05-26T12:20:05.607992Z","iopub.status.idle":"2021-05-26T12:20:05.621591Z","shell.execute_reply.started":"2021-05-26T12:20:05.607959Z","shell.execute_reply":"2021-05-26T12:20:05.620378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_probability as tfp\n\n\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:20:05.623871Z","iopub.execute_input":"2021-05-26T12:20:05.624217Z","iopub.status.idle":"2021-05-26T12:20:05.63574Z","shell.execute_reply.started":"2021-05-26T12:20:05.624181Z","shell.execute_reply":"2021-05-26T12:20:05.634723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\n\n\nThe genes expressions in a cell are linked to its environment, i.e. if a cell is stimulated from miscellaneous factors of it's environement then with intracellular transduction of the signal, this cell's genome is stimulated.\nOf course, this is a really poor modelling of the problem, but it will help us explain the general idea and goals of this paper.\n\nThe goal is predicting the mechanism of action of molecules through records of 772 genes expressions and cell survivability markers.\n\nIn machine learning, given the data and answers of a problematic, we are trying to get its rules in order to predict further answers. In this competition, given the cellular response of a stimulus we are trying to get the \"cellular rules\" that implied its response, i.e we are trying to find the mechanism of action of this stimulus given the genes expressions.\n\n\n### A. A stimulus implies a cellular response, thus a modulation of the genes expressions:\n\nLet's think of the molecule we are trying to predict its MoA from as the \"environment stimulus\" we sent to the cell.\nIf the cell properly receives the stimulus then it will implies a cellular response. A cellular response we will be able to monitor through its gene expression.\nThis is obviously false, but let's resolve 2 little problematics:\n\n\n    a) A properly received stimulus: What is it?\n    \nIf the stimulus is not strong enough or if the cell don't have any receptor to perceive this signal, then it's trying to hear imperceptible sounds or to listen to ultrasound with human ears. The stimulus must be strong enough and needs specific organs of perception (=Receptors) to be perceived.\nThus, a cell will implies a cellular response we can monitor if the molecule is presented to the cell in a sufficient concentration and only if the cell has receptors able to \"perceive\" this molecule.\nThis idea of the receptor-molecule perception will be detailed in B.1)\nLet's modelize this idea of received stimulus as a function of the stimulus s: f(s) = ReLU(r*s + r') -> r in R(0,+), r' in R-. If no receptors then r=0\nWe will simplify this model by defining a valid stimulus (vS) which is a properly received stimulus: f(vS) = vS\n\n\n    b) Monitoring the cellular response: Why ?\n\nThe theory in this paper is: if a stimulus is received it will implies a cellular response, and we will monitor it through gene expression.\nThis is obviously false but we will define a valid CR (=cellular response) as a CR we can monitor through gene expression.\nWe will also extend the definition of a valid stimulus as a properly received stimulus that implies a valid CR (vCR).\n\n\n\n    c) Studying the case where a stimulus can be perceived by several different receptors :\n\nIf a vS is properly perceived by several different types of receptors(different receptors implying different vCR), this means we can modelize the total of vCR as a vector.\nThus VCR will be the notation of a vector stacking vertically all vCR.\n\n\n    d) Pathway: Studying the case where one specific receptor can implies several (positif or negative) CR and define what a positive or negative CR is.\n    \nWe modelized that a receptor will implies a vCR but a vS can activate the receptor or inhibate it. Thus activating a receptor will be a positive vS and inhibating it will be a negative vS.\nIf we imagine a single receptor able to implies several CR, we can start speaking of pathways.\nA pathway(PW) will be an internal mechanism that will implies a specific vCR. Activating or Inhibiting a PW will implies positively or negatively modulating genes expressions. \n\n\n\n### B. Stimulus, Cellular response, pathways and Mechanisms of Action\n\n\n    1) Stimulus and Cellular response: the Molecule-Receptor perception\n    \nA receptor is able to perceive a molecule because of several factors (3D layout of the molecule, Electronegativity, polarity). A molecule is able to be \"perceived\" by several types of receptors.\nBecause of its 3D layout and physico-chemical properties a molecule will be able to implies a VCR.\nWe can imagine that studying VCR is, in a way, studying the 3D layout and physico-chemical properties of a molecule.\nThen when we will try to predict MoA from data, we will try to predict this properties from the molecule.\n\nThe properties of a stimulus is what implies the molecule-receptor perception and the type of the receptor is what implies a vCR.\n\nA vCR is implied by several positive or negative activation of intracellular PW that will implies regulation and modulation of the genes expressions. Regulation and modulation of the genes expressions being what we monitor and defining what a vCR is.\n\n    2) Chain Reaction\n\nThe CR and PW we talk about where downstream. We went from the stimulus through the receptor, then by several pathways we finally modulate the genes expression. This was downward or downstream.\n\nActivating a gene will implies DNA transcription, then RNA transcription into Proteins. It's a very simplified model.\nThis proteins could be perceived by intracellular receptors or modulate the activity of other proteins implied in a specific PW or even be itself another receptor. Thus a protein will modulate the activity of PW, i.e. activating a gene will modulate the expression of other genes.\n\nThus the vCR is the sum of all gene modulation implied by a chain reaction of several gene modulation.\n\n### C. What is a CoordiNET and why ?\n\n    1) CoordiNET?\n    \n\n    2) Why?\n    \nLet's imagine the gene expressions in our data as a time-series, i.e. we will have the level of expression of a first gene and after this the level of expression of a second gene modified because of the first one.\nThe problem of MoA will be very easy to check. Because we will find obviously high level of correlation of several following genes and we will be able to say \"hey this molecule is negatively modulating this genes, those genes are implied in pro-inflammatory response, the mechanism of action of this molecule could be an inhibitor of NF-κB.\n\n\n### Going farther\n\nThis MOA predictor is obviously a promising tool for High-Throughput Screening.\nBut I think it could be even more interesting, given a specific tissue and a specific disease, we could check the overall genes expression of those cells and then being able with several drugs to bring back the gene expression to a physiological level.\nThis means, instead of taking normal drugs, a specific cocktail of drugs in little concentration could bring back a degenerate homeostasis to a normal state.","metadata":{}},{"cell_type":"markdown","source":"# Ideas\n\nFirst trained ConvNet -> We cluster by drugs structure with train_drug.csv\n\nWe freeze the convnet and train Dense layers on train_targets_scored","metadata":{}},{"cell_type":"markdown","source":"There is 729! permutations possible, we can't try them all! #about 10¹⁷⁷² permutations\nWe have to find a better way to get a good layout.\n\n### I. First attempt : Correlation matrix\n    Version: 0, 1\n    Correlation matrix seems promising but highly computational! We face overfitting and it would take too much time to try fixing it.\n\n    \n### II. Compute Γ and position genes in a 27x27 or 9x9x9 matrix -- Algorithmic solution\n    Matrices dimension: 27*27=9*9*9=729\n    ---> We will either select the 729 best genes or compute PCA\n\n    Let X a tensor of shape (729,1), the representation of the 729 genes vertically stacked.\n    Let xᵢ, xⱼ respectively the i-th and j-th gene of X.\n    Let γᵢ,ⱼ the correlation value of xᵢ, xⱼ.\n    ∀i in [0:728], Γᵢ = (Σⱼ₌₀⁷²⁸|γᵢ,ⱼ|)-1\n    Then get the indices that would sort the array of Γᵢs.\n    \n    Next idea, genes with high Γ are more likely correlated to other genes than low Γ genes.\n    Thus, highly correlated genes means genes that modulate the expression of each others.\n    We will put low Γ genes in the corners and borders of the matrix because they are correlated to less genes.\n    \n    All positions of the matrix must be ranked: A position with high rank means that it's a position where its correlation has a high influence on its neighbores\n    \n    The position ranks and Γ ranks will help us filling all position with genes\n    \n    Limitations: obviously we are badly positioning the genes because we didn't care about the correlation between each xᵢ, xⱼ\n    \n    \n### III. Use Γ and a neural network to find the perfect coordinates of the genes in a 27x27 or 9x9x9 matrix -- CoordiNET\n    Find a way to make a model trying to find the 3 dimensional layout of a set of 729 flatten features.\n    Consume to much ressources and loss function was not adapted: 32 minutes per epoch with very high loss values.\n    \n# IV. Auto-coordinating layers + Residual blocks","metadata":{}},{"cell_type":"markdown","source":"## TO DO:\n- Try a CNN with Residual blocks\n- Try average pooling\n- Try PCA\n- DRUGS CLUSTERING\n- Correlation matrix with only the non-zero rows","metadata":{}},{"cell_type":"code","source":"##################### HYPER PARAMS #####################\nGENES_REGEX = '^g-*|^c-'\nN_SPLITS = 3\n\nBATCH_SIZE=128\nEPOCHS=200\n\nLEARNING_RATE = 4e-1\nLABEL_SMOOTHING = 1e-5\n\nRLR_PATIENCE = 4\nRLR_FACTOR = .7\nES_PATIENCE = 20\n########################################################","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:20:05.637386Z","iopub.execute_input":"2021-05-26T12:20:05.63801Z","iopub.status.idle":"2021-05-26T12:20:05.645456Z","shell.execute_reply.started":"2021-05-26T12:20:05.637972Z","shell.execute_reply":"2021-05-26T12:20:05.644475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##################### HYPER PARAMS #####################\nREGULARIZATION = 1\n\nINPUT_SHAPE = 729\nAUTOCOORD_L1, AUTOCOORD_L2 = 1e-3, 1e-2\nAUTO_SIZE = 9**3\nRESHAPE_SIZE = (9,9,9,1)\nAUTO_DROPOUT = 0\n\nCONV_FILTER = 2\nCONV_DROPOUT = .2\nRESBLOCK_L1, RESBLOCK_L2 = 1e-12, 1e-11\n\nDENSE_L1, DENSE_L2 = 1e-9, 1e-8\nDENSE_DROPOUT = .4\n\nOUTPUT_SIZE = 206\n########################################################","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:20:05.64734Z","iopub.execute_input":"2021-05-26T12:20:05.647917Z","iopub.status.idle":"2021-05-26T12:20:05.655214Z","shell.execute_reply.started":"2021-05-26T12:20:05.647873Z","shell.execute_reply":"2021-05-26T12:20:05.654259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'''##################### HYPER PARAMS #####################\\n\n      BATCH_SIZE = {BATCH_SIZE} \\n\n      EPOCHS = {EPOCHS}\\n\n      N_SPLITS = {N_SPLITS}\\n\n      LABEL_SMOOTHING = {LABEL_SMOOTHING}\\n\n      GENES_REGEX = {GENES_REGEX}\\n\n      RLR_PATIENCE = {RLR_PATIENCE}\\n\n      RLR_FACTOR = {RLR_FACTOR}\\n\n      ES_PATIENCE = {ES_PATIENCE}\\n\n########################################################\n      ''')\n\nprint(f'''##################### HYPER PARAMS #####################\\n\n      LEARNING_RATE = {LEARNING_RATE}\\n\n      REGULARIZATION = {REGULARIZATION}\\n\n      INPUT_SHAPE = {INPUT_SHAPE}\\n\n      AUTOCOORD_L1, AUTOCOORD_L2 = {AUTOCOORD_L1}, {AUTOCOORD_L2}\\n\n      AUTO_SIZE = {AUTO_SIZE}\\n\n      RESHAPE_SIZE = {RESHAPE_SIZE}\\n\n      AUTO_DROPOUT = {AUTO_DROPOUT}\\n\n      CONV_FILTER = {CONV_FILTER}\\n\n      CONV_DROPOUT = {CONV_DROPOUT}\\n\n      RESBLOCK_L1, RESBLOCK_L2 = {RESBLOCK_L1}, {RESBLOCK_L2}\\n\n      DENSE_L1, DENSE_L2 = {DENSE_L1}, {DENSE_L2}\\n\n      DENSE_DROPOUT = {DENSE_DROPOUT}\\n\n      OUTPUT_SIZE = {OUTPUT_SIZE}\\n\n########################################################\n      ''')","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:20:05.659794Z","iopub.execute_input":"2021-05-26T12:20:05.660137Z","iopub.status.idle":"2021-05-26T12:20:05.669527Z","shell.execute_reply.started":"2021-05-26T12:20:05.660111Z","shell.execute_reply":"2021-05-26T12:20:05.66865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data showcase","metadata":{}},{"cell_type":"code","source":"def head(numpy_array):\n    return pd.DataFrame(numpy_array)\n\ntrain_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_features.head()\n\ngenes = train_features.filter(regex=(GENES_REGEX)).to_numpy()\nhead(genes)\n\n\ntargets = pd.read_csv('../input/lish-moa/train_targets_scored.csv').drop(['sig_id'],axis=1).to_numpy()\nnon_scored_targets = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv').drop(['sig_id'],axis=1).to_numpy()\ndrug_targets = pd.get_dummies(pd.read_csv('../input/lish-moa/train_drug.csv').drop(['sig_id'],axis=1)).to_numpy()\nall_targets = np.concatenate([targets, non_scored_targets], axis=-1)\n\ntrain_features.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:20:05.671003Z","iopub.execute_input":"2021-05-26T12:20:05.671626Z","iopub.status.idle":"2021-05-26T12:20:09.278405Z","shell.execute_reply.started":"2021-05-26T12:20:05.671588Z","shell.execute_reply":"2021-05-26T12:20:09.277365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing","metadata":{}},{"cell_type":"code","source":"def cleanFeatures(features):\n    features_ = features.copy()\n    cp_type = {'trt_cp': 0, 'ctl_vehicle': 1}\n    cp_dose = {'D1': 0, 'D2': 1}\n    features_['cp_type'] = features_['cp_type'].map(cp_type)\n    features_['cp_dose'] = features_['cp_dose'].map(cp_dose)\n    features_ = pd.get_dummies(features_, columns=['cp_time'])\n    features_.drop(['sig_id'], inplace=True, axis=1)\n    return features_\n\nX_clean = cleanFeatures(train_features)\nX_clean","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:20:09.281584Z","iopub.execute_input":"2021-05-26T12:20:09.282028Z","iopub.status.idle":"2021-05-26T12:20:09.556654Z","shell.execute_reply.started":"2021-05-26T12:20:09.281984Z","shell.execute_reply":"2021-05-26T12:20:09.555303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rankGauss(x):\n    from scipy.special import erfinv\n    N = x.shape[0]\n    temp = x.argsort(axis=1)\n    rank_x = temp.argsort(axis=1) / N\n    rank_x -= rank_x.mean()\n    rank_x *= 2\n    efi_x = erfinv(rank_x)\n    efi_x -= efi_x.mean()\n    return efi_x","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:20:09.559578Z","iopub.execute_input":"2021-05-26T12:20:09.559983Z","iopub.status.idle":"2021-05-26T12:20:09.567318Z","shell.execute_reply.started":"2021-05-26T12:20:09.559941Z","shell.execute_reply":"2021-05-26T12:20:09.565851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"REGEX = '^cp_*'\n\nX = rankGauss(genes)\nother_X = X_clean.filter(regex=(REGEX)).to_numpy()  #X_clean['cp_type'].to_numpy()\n\nX.shape, other_X.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:20:09.569053Z","iopub.execute_input":"2021-05-26T12:20:09.569491Z","iopub.status.idle":"2021-05-26T12:20:12.145055Z","shell.execute_reply.started":"2021-05-26T12:20:09.569448Z","shell.execute_reply":"2021-05-26T12:20:12.144258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.preprocessing import quantile_transform\n# def quantileTransform(X, seed=51):\n#     X_ = X.copy()\n#     X_ = quantile_transform(X_, n_quantiles=100,output_distribution='normal', random_state=seed, axis=0)\n#     X_ = pd.DataFrame(X_)\n#     return X_\n# train = quantileTransform(X_clean)\n# test = quantileTransform(test)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:20:12.14656Z","iopub.execute_input":"2021-05-26T12:20:12.146953Z","iopub.status.idle":"2021-05-26T12:20:12.152973Z","shell.execute_reply.started":"2021-05-26T12:20:12.146919Z","shell.execute_reply":"2021-05-26T12:20:12.151991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.decomposition import PCA\n\n# pca = PCA(n_components=729)\n# pca.fit(train.append(test))\n# train = pca.transform(train)\n# test = pca.transform(test)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:20:12.154507Z","iopub.execute_input":"2021-05-26T12:20:12.155139Z","iopub.status.idle":"2021-05-26T12:20:12.164133Z","shell.execute_reply.started":"2021-05-26T12:20:12.155096Z","shell.execute_reply":"2021-05-26T12:20:12.163313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Iterative stratification","metadata":{}},{"cell_type":"code","source":"from shutil import copyfile\ncopyfile(src = \"../input/minimalistic-v2/ml_stratifiers.py\", dst = \"../working/ml_stratifiers.py\")\nfrom ml_stratifiers import MultilabelStratifiedKFold\n\n\n\ndef getMskfDataset(X, other_X, y, all_y, n_splits=N_SPLITS, SEED=7):\n    mskf = MultilabelStratifiedKFold(n_splits=n_splits, random_state=SEED, shuffle=True) # test_size=0.2\n    for i, (train_idx, val_idx) in enumerate(mskf.split(X, y)):\n        X_train, other_X_train, y_train = X[train_idx], other_X[train_idx], y[train_idx] # np.expand_dims(other_X[train_idx],1)\n        X_val, other_X_val, y_val = X[val_idx], other_X[val_idx], y[val_idx] # np.expand_dims(other_X[val_idx],1)\n        \n        X_train_0, other_X_train_0, y_train_0 = X[train_idx], other_X[train_idx], all_y[train_idx]\n        X_val_0, other_X_val_0, y_val_0 = X[val_idx], other_X[val_idx], all_y[val_idx]\n        \n        X_train_1, other_X_train_1, y_train_1 = X[train_idx], other_X[train_idx], y[train_idx]\n        X_val_1, other_X_val_1, y_val_1 = X[val_idx], other_X[val_idx], y[val_idx]\n        yield X_train_0, other_X_train_0, y_train_0, X_val_0, other_X_val_0, y_val_0, X_train_1, other_X_train_1, y_train_1, X_val_1, other_X_val_1, y_val_1","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:20:12.167578Z","iopub.execute_input":"2021-05-26T12:20:12.167906Z","iopub.status.idle":"2021-05-26T12:20:12.180761Z","shell.execute_reply.started":"2021-05-26T12:20:12.167855Z","shell.execute_reply":"2021-05-26T12:20:12.179996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Augmentation","metadata":{}},{"cell_type":"markdown","source":"Data augmentation with over- and under-correlated features","metadata":{}},{"cell_type":"code","source":"# def dataAugmentation(X, y, randomness=.3):\n#     dim0 =X.shape[0]\n#     dim1 = X.shape[1]\n#     dim2 = X.shape[2]\n#     dim3 = X.shape[3]\n#     for i in range(dim0):\n#         translation_matrix_1 = np.random.rand(dim1, dim2, dim3)    \n#         translation_matrix_2 = np.random.rand(dim1, dim2, dim3) \n#         randint_matrix = np.random.randint(-1,1, (dim1, dim2, dim3))\n#         X_i = X[i] + X[i]*(randomness*translation_matrix_1) + (randomness*(translation_matrix_2*randint_matrix))\n#         yield X_i, y[i]","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:20:12.182156Z","iopub.execute_input":"2021-05-26T12:20:12.182561Z","iopub.status.idle":"2021-05-26T12:20:12.190349Z","shell.execute_reply.started":"2021-05-26T12:20:12.182521Z","shell.execute_reply":"2021-05-26T12:20:12.189416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ds_to_augment = tf.data.Dataset.from_tensor_slices((X_ds,targets))\n\n# aug_ds = tf.data.Dataset.from_generator(\n#     dataAugmentation, args=[ds_to_augment._tensors[0], ds_to_augment._tensors[1]],\n#     output_signature=(\n#         tf.TensorSpec(shape=(729,1), dtype=tf.float64),\n#         tf.TensorSpec(shape=(206), dtype=tf.int64))\n# ).batch(128)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:20:12.193068Z","iopub.execute_input":"2021-05-26T12:20:12.19358Z","iopub.status.idle":"2021-05-26T12:20:12.199651Z","shell.execute_reply.started":"2021-05-26T12:20:12.193547Z","shell.execute_reply":"2021-05-26T12:20:12.198628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ConvNET","metadata":{}},{"cell_type":"code","source":"class AutoCoordBlock(tf.keras.layers.Layer):\n    def __init__(self, dense_size=AUTO_SIZE, trainable=True):\n        super(AutoCoordBlock, self).__init__()\n        self.dense_size= dense_size\n        self.l2 = tf.keras.regularizers.L2(l2=AUTOCOORD_L1)\n        self.l1 = tf.keras.regularizers.L1(l1=AUTOCOORD_L2*1e-7)\n        self.dense = tf.keras.layers.Dense(self.dense_size, kernel_regularizer=self.l2 ,bias_regularizer=self.l1, trainable=trainable)\n        self.batch_norm = tf.keras.layers.BatchNormalization(trainable=trainable)\n        self.relu = tf.keras.layers.ReLU()\n        self.dropout = tf.keras.layers.Dropout(AUTO_DROPOUT)\n        \n        self.reshape= tf.keras.layers.Reshape(RESHAPE_SIZE)\n    \n    @tf.function\n    def call(self, x, training=False):\n        x = self.dense(x, training=training)\n        x = self.batch_norm(x,training=training) \n        x = self.relu(x, training=training)\n        x = self.reshape(x, training=training)\n        x =  self.dropout(x, training=training) \n        return x\n\n\nclass ResBlock(tf.keras.layers.Layer):\n    def __init__(self, pre_filter_size, k3_filter_size, k1_filter_size, padding='same', is_first=False, trainable=True):\n        super(ResBlock, self).__init__()\n        self.padding = padding\n        self.l2 = tf.keras.regularizers.L2(l2=RESBLOCK_L1)\n        self.l1 = tf.keras.regularizers.L1(l1=RESBLOCK_L2)\n        self.is_first = is_first\n        self.dropout = tf.keras.layers.Dropout(CONV_DROPOUT)\n        \n        if not is_first:\n            self.pre_filter_size = pre_filter_size        \n            self.pre_conv = tf.keras.layers.Conv3D(self.pre_filter_size, (1,1,1), padding=self.padding, kernel_regularizer=self.l2 ,bias_regularizer=self.l1, trainable=trainable)\n            self.batch_norm_pre = tf.keras.layers.BatchNormalization(trainable=trainable)\n            self.relu_pre = tf.keras.layers.ReLU()\n\n            self.k1_filter_size = k1_filter_size\n            self.conv_k1 = tf.keras.layers.Conv3D(self.k1_filter_size, (5,5,5), padding=self.padding, kernel_regularizer=self.l2 ,bias_regularizer=self.l1, trainable=trainable)\n            self.batch_norm_k1 = tf.keras.layers.BatchNormalization(trainable=trainable)\n            self.relu_k1 = tf.keras.layers.ReLU()\n        \n        self.k3_filter_size = k3_filter_size\n        self.conv_k3 = tf.keras.layers.Conv3D(self.k3_filter_size, (3,3,3), padding=self.padding, kernel_regularizer=self.l2 ,bias_regularizer=self.l1, trainable=trainable)     \n        self.batch_norm_k3 = tf.keras.layers.BatchNormalization(trainable=trainable)       \n        self.relu_k3 = tf.keras.layers.ReLU()\n        \n        self.concat = tf.keras.layers.Concatenate(axis=-1)\n    \n    @tf.function\n    def call(self, x, training=False):\n        if not self.is_first:\n            x = self.pre_conv(x, training=training)\n            x = self.batch_norm_pre(x,training=training)\n            x = self.relu_pre(x, training=training)\n            x = self.dropout(x, training = training)\n\n            x_k1 = self.conv_k1(x, training=training)\n            x_k1 = self.batch_norm_k1(x_k1, training=training) \n            x_k1 = self.relu_k1(x_k1, training=training)\n            x_k1 = self.dropout(x, training = training)\n        \n        x_k3 = self.conv_k3(x, training=training)\n        x_k3 = self.batch_norm_k3(x_k3,training=training)\n        x_k3 = self.relu_k3(x_k3, training=training)\n        x_k3 = self.dropout(x, training = training)\n        \n        if not self.is_first:\n            x_ = self.concat([x_k3, x_k1, x])\n        else: \n            x_ = self.concat([x_k3, x])\n        return x_\n\n    \nclass ReduceBlock(tf.keras.layers.Layer):\n    def __init__(self, trainable=True):\n        super(ReduceBlock, self).__init__()\n        self.l2 = tf.keras.regularizers.L2(l2=RESBLOCK_L1)\n        self.l1 = tf.keras.regularizers.L1(l1=RESBLOCK_L2)\n        self.padding = 'valid'\n        self.flatten = tf.keras.layers.Flatten()\n        self.dropout = tf.keras.layers.Dropout(CONV_DROPOUT)\n        \n        self.filter_size_0 = 8*CONV_FILTER\n        self.conv_0 = tf.keras.layers.Conv3D(self.filter_size_0, (3,3,3), padding=self.padding, kernel_regularizer=self.l2 ,bias_regularizer=self.l1, trainable=trainable)     \n        self.batch_norm_0 = tf.keras.layers.BatchNormalization(trainable=trainable)       \n        self.relu_0 = tf.keras.layers.ReLU()\n        \n        self.filter_size_1 = 16*CONV_FILTER\n        self.conv_1 = tf.keras.layers.Conv3D(self.filter_size_1, (3,3,3), padding=self.padding, kernel_regularizer=self.l2 ,bias_regularizer=self.l1, trainable=trainable)     \n        self.batch_norm_1 = tf.keras.layers.BatchNormalization(trainable=trainable)       \n        self.relu_1 = tf.keras.layers.ReLU()\n        \n        self.filter_size_2 = 32*CONV_FILTER\n        self.conv_2 = tf.keras.layers.Conv3D(self.filter_size_2, (3,3,3), padding=self.padding, kernel_regularizer=self.l2 ,bias_regularizer=self.l1, trainable=trainable)     \n        self.batch_norm_2 = tf.keras.layers.BatchNormalization(trainable=trainable)       \n        self.relu_2 = tf.keras.layers.ReLU()\n        \n        self.concat = tf.keras.layers.Concatenate(axis=-1)\n    \n    @tf.function\n    def call(self, x, training=False):\n        x_ = self.flatten(x)\n        x = self.conv_0(x, training = training)\n        x = self.batch_norm_0(x, training = training)\n        x = self.relu_0(x, training = training)\n        x = self.dropout(x, training = training)\n        \n        x = self.conv_1(x, training = training)\n        x = self.batch_norm_1(x, training = training)\n        x = self.relu_1(x, training = training)\n        x = self.dropout(x, training = training)\n        \n        x = self.conv_2(x, training = training)\n        x = self.batch_norm_2(x, training = training)\n        x = self.relu_2(x, training = training)\n        x = self.dropout(x, training = training)\n        x = self.flatten(x)\n        return self.concat([x, x_])\n    \n    \nclass DenseBlock(tf.keras.layers.Layer):\n    def __init__(self, dense_size=512, x_is_flat=False, dropout=DENSE_DROPOUT):\n        super(DenseBlock, self).__init__()\n        self.l2 = tf.keras.regularizers.L2(l2=DENSE_L1)\n        self.l1 = tf.keras.regularizers.L1(l1=DENSE_L2)\n        self.x_is_flat = x_is_flat\n        if not x_is_flat:\n            self.flatten = tf.keras.layers.Flatten()\n        self.concat = tf.keras.layers.Concatenate(axis=1)\n        self.dense = tf.keras.layers.Dense(dense_size, kernel_regularizer=self.l2 ,bias_regularizer=self.l1)\n        self.batch_norm = tf.keras.layers.BatchNormalization()\n        self.relu = tf.keras.layers.ReLU()\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        \n    @tf.function\n    def call(self, x, other_x, training=False):\n        if not self.x_is_flat:\n            x_ = self.concat([self.flatten(x), tf.cast(other_x, 'float')])\n        else:\n            x_ = self.concat([x, tf.cast(other_x, 'float')])\n        x_ = self.dense(x_, training=training)\n        x_ = self.batch_norm(x_,training=training) \n        x_ = self.relu(x_, training=training)\n        x_ = self.dropout(x_, training=training) \n        return x_\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:20:12.201468Z","iopub.execute_input":"2021-05-26T12:20:12.202115Z","iopub.status.idle":"2021-05-26T12:20:12.240931Z","shell.execute_reply.started":"2021-05-26T12:20:12.202073Z","shell.execute_reply":"2021-05-26T12:20:12.240176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AutoCoordNet(tf.keras.Model):\n    def __init__(self, output_size=OUTPUT_SIZE, everything_trainable=True):\n        super(AutoCoordNet, self).__init__()\n        self.auto_block = AutoCoordBlock(trainable=everything_trainable)\n        self.res_block_0 = ResBlock(None, 1*CONV_FILTER, None, is_first=True, trainable=everything_trainable)\n        self.res_block_1 = ResBlock(2*CONV_FILTER, 2*CONV_FILTER, 1*CONV_FILTER, trainable=everything_trainable)\n        self.res_block_2 = ResBlock(4*CONV_FILTER, 4*CONV_FILTER, 2*CONV_FILTER, trainable=everything_trainable)\n        self.reduce_block = ReduceBlock(trainable=everything_trainable)\n        self.dense0 = DenseBlock(x_is_flat=True)\n        self.dense1 = DenseBlock(dense_size=256, x_is_flat=True)\n        self.output_layer = tf.keras.layers.Dense(output_size, activation='sigmoid')\n        self.history = {'loss':[], 'val_loss':[] }\n        \n        self.loss_metrics = tf.keras.metrics.BinaryCrossentropy()\n\n#     @tf.function\n    def train(self, train_ds, validation_ds, epochs, reduce_lr, early_stopping):\n        self.onTrainBegin(reduce_lr, early_stopping)\n        for epoch in range(1,epochs+1):\n            self.loss_metrics.reset_states()\n            start_time = time.time()\n            for step,(x_batch, other_x_batch, y_batch) in enumerate(train_ds):           \n                self.train_step(x_batch, other_x_batch, y_batch)\n            loss = self.loss_metrics.result().numpy()\n            self.history['loss'].append(loss)\n\n            self.loss_metrics.reset_states()\n            for (val_x_batch, val_other_x_batch, val_y_batch) in validation_ds:\n                self.val_step(val_x_batch, val_other_x_batch, val_y_batch)\n            val_loss = self.loss_metrics.result().numpy()\n            self.history['val_loss'].append(val_loss)\n\n            print(f\"Epoch {epoch}/{epochs} -\",\"loss: %.4f,\"%(float(loss)),\"val_loss: %.4f,\"%(float(val_loss)), \"time spent: %.2fs.\"%(time.time()-start_time))\n            self.onEpochEnd(epoch, val_loss)\n            \n            if self.stop_training == True:\n                return self.history\n        return self.history\n                \n#     @tf.function\n    def predict(self, ds):\n        y = []\n        for (x_batch, other_x_batch) in ds:\n            logits = self(x_batch, other_x_batch, training=False)\n            y.extend(logits.numpy())\n        return np.array(y)\n        \n    @tf.function    \n    def train_step(self, x, other_x, y):\n        with tf.GradientTape() as tape:\n            y_pred = self(x, other_x, training=True)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n        gradients = tape.gradient(loss, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n        self.loss_metrics.update_state(y, y_pred)\n    \n    @tf.function\n    def val_step(self, x, other_x, y):\n        val_logits = self(x, other_x, training=False)\n        self.loss_metrics.update_state(y, val_logits)\n\n    \n    @tf.function\n    def call(self, x, other_x, training=False):\n        x = self.auto_block(x, training=training)\n        x = self.res_block_0(x, training=training)\n        x = self.res_block_1(x, training=training)\n        x = self.res_block_2(x, training=training)\n        x = self.reduce_block(x, training=training)\n        x = self.dense0(x, other_x, training=training)\n        x = self.dense1(x, other_x, training=training)\n        return self.output_layer(x, training=training)\n    \n    @tf.function\n    def onTrainBegin(self, reduce_lr, early_stopping):\n        self.reduce_lr, self.early_stopping = reduce_lr, early_stopping\n        self.reduce_lr.set_model(self), self.early_stopping.set_model(self)\n        self.reduce_lr.on_train_begin(), self.early_stopping.on_train_begin()\n    \n    def auto_train(self, trainable):\n        self.auto_block.trainable = trainable\n        \n    def res_train(self, trainable):\n        self.res_block_0.trainable = trainable    \n        self.res_block_1.trainable = trainable\n        self.res_block_2.trainable = trainable\n        self.reduce_block.trainable = trainable\n\n    def dense_train(self, trainable):\n        self.dense0.trainable = trainable\n        self.dense1.trainable = trainable\n        self.output_layer.trainable = trainable\n        \n#     @tf.function\n    def onEpochEnd(self, epoch, val_loss):\n        logs = {'val_loss': val_loss}\n        self.reduce_lr.on_epoch_end(epoch,logs), self.early_stopping.on_epoch_end(epoch,logs)\n    \n    def setOutputSize(self, size):\n        self.output_layer = tf.keras.layers.Dense(size, activation='sigmoid')","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:20:12.242419Z","iopub.execute_input":"2021-05-26T12:20:12.242824Z","iopub.status.idle":"2021-05-26T12:20:12.270051Z","shell.execute_reply.started":"2021-05-26T12:20:12.242786Z","shell.execute_reply":"2021-05-26T12:20:12.269282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_features = pd.read_csv('../input/lish-moa/test_features.csv')\ntest_genes = test_features.filter(regex=(GENES_REGEX)).to_numpy()\ntest_clean = cleanFeatures(test_features)\nX_test = rankGauss(test_genes)\nother_X_test = test_clean.filter(regex=(REGEX)).to_numpy() # REGEX = '^cp_*'\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test, other_X_test)).batch(BATCH_SIZE)\n# X_test.shape, other_X_test.shape, test_ds","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:20:12.271372Z","iopub.execute_input":"2021-05-26T12:20:12.272422Z","iopub.status.idle":"2021-05-26T12:20:13.179701Z","shell.execute_reply.started":"2021-05-26T12:20:12.272377Z","shell.execute_reply":"2021-05-26T12:20:13.178832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\ndef plotHistory(model):\n    loss = model.history['loss']\n    val_loss = model.history['val_loss']\n\n    epochs = range(len(loss))\n    start_epoch=5\n\n    plt.plot(epochs[start_epoch:], loss[start_epoch:], 'r', label='Training Loss')\n    plt.plot(epochs[start_epoch:], val_loss[start_epoch:], 'b', label='Validation Loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n    plt.show()\n    model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:20:13.181361Z","iopub.execute_input":"2021-05-26T12:20:13.181767Z","iopub.status.idle":"2021-05-26T12:20:13.192372Z","shell.execute_reply.started":"2021-05-26T12:20:13.181725Z","shell.execute_reply":"2021-05-26T12:20:13.191464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getCallbacks():\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        mode='min',\n        factor=RLR_FACTOR,\n        patience=RLR_PATIENCE,\n        verbose=2)\n    early_stopping = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        mode='min',\n        patience=ES_PATIENCE,\n        restore_best_weights=True,\n        verbose=2)\n    return reduce_lr, early_stopping","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:20:13.194119Z","iopub.execute_input":"2021-05-26T12:20:13.194625Z","iopub.status.idle":"2021-05-26T12:20:13.203045Z","shell.execute_reply.started":"2021-05-26T12:20:13.194586Z","shell.execute_reply":"2021-05-26T12:20:13.202033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getInfer():\n    stratified_ds_generator = getMskfDataset(X, other_X, targets, all_targets) \n    preds=[] \n    i = 0\n    for (X_train_0, other_X_train_0, y_train_0, X_val_0, other_X_val_0, y_val_0, X_train_1, other_X_train_1, y_train_1, X_val_1, other_X_val_1, y_val_1) in stratified_ds_generator: \n        i+=1\n        print(f'FOLD {i} starting...')\n#         train_ds_0 = tf.data.Dataset.from_tensor_slices((X_train_0, other_X_train_0, y_train_0)).batch(BATCH_SIZE)\n#         val_ds_0 = tf.data.Dataset.from_tensor_slices((X_val_0, other_X_val_0, y_val_0)).batch(BATCH_SIZE)\n#         model_0 = AutoCoordNet(output_size=all_targets.shape[-1])\n#         model_0.compile(loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = LABEL_SMOOTHING),\n#                         optimizer = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE, momentum=.9))\n#         rlr, es = getCallbacks()\n#         model_0.train(train_ds_0, val_ds_0, epochs=EPOCHS, reduce_lr=rlr, early_stopping=es)\n#         plotHistory(model_0)\n#         model_0.save_weights('model_weights')\n        \n        train_ds_1 = tf.data.Dataset.from_tensor_slices((X_train_1, other_X_train_1, y_train_1)).batch(BATCH_SIZE)\n        val_ds_1 = tf.data.Dataset.from_tensor_slices((X_val_1, other_X_val_1, y_val_1)).batch(BATCH_SIZE)\n        model_1 = AutoCoordNet()\n#         model_1.load_weights('model_weights', by_name=False)\n        model_1.setOutputSize(OUTPUT_SIZE)\n        model_1.compile(loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = LABEL_SMOOTHING),\n                        optimizer = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE, momentum=.9))\n        rlr, es = getCallbacks()\n        model_1.train(train_ds_1, val_ds_1, epochs=EPOCHS, reduce_lr=rlr, early_stopping=es)\n        plotHistory(model_1)        \n        pred = model_1.predict(test_ds)\n        preds.append(pred)  \n        \n    infer = np.zeros((test_features.shape[0],206))\n    for pred_ in preds:\n        infer += pred_\n    infer /= N_SPLITS\n    return infer","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:20:13.20493Z","iopub.execute_input":"2021-05-26T12:20:13.205332Z","iopub.status.idle":"2021-05-26T12:20:13.217291Z","shell.execute_reply.started":"2021-05-26T12:20:13.205293Z","shell.execute_reply":"2021-05-26T12:20:13.216296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"infer = getInfer()\nctl_idx = np.where(test_features.iloc[:,1].to_numpy() == 'ctl_vehicle')\ninfer[ctl_idx] = 0","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:20:13.218605Z","iopub.execute_input":"2021-05-26T12:20:13.218987Z","iopub.status.idle":"2021-05-26T12:35:37.925585Z","shell.execute_reply.started":"2021-05-26T12:20:13.218945Z","shell.execute_reply":"2021-05-26T12:35:37.922807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsub = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\n\nsub.iloc[:,1:] = infer\n\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:35:37.926975Z","iopub.status.idle":"2021-05-26T12:35:37.927461Z"},"trusted":true},"execution_count":null,"outputs":[]}]}