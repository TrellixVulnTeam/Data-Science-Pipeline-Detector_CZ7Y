{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About the Competition\n\n**<font color='red'>Q. What is Mechanism of Action (MoA) of a drug ? And why is it important ?</font>**<br><br>\nToday, with the advent of more powerful technologies, drug discovery has changed from the serendipitous approaches of the past to a more targeted model based on an understanding of the underlying biological mechanism of a disease. In this new framework, scientists seek to identify a protein target associated with a disease and develop a molecule that can modulate that protein target. As a shorthand to describe the biological activity of a given molecule, scientists assign a label referred to as mechanism-of-action or MoA for short.\n\n**<font color='red'>Q. How do we determine the MoAs of a new drug?</font>**<br><br>\nIn this competition, you will have access to a unique dataset that combines gene expression and cell viability data. The data is based on a new technology that measures simultaneously (within the same samples) human cellsâ€™ responses to drugs in a pool of 100 different cell types (thus solving the problem of identifying ex-ante, which cell types are better suited for a given drug). In addition, you will have access to MoA annotations for more than 5,000 drugs in this dataset.\n\n**<font color='red'>Q.How to evaluate the accuracy of a solution?</font>**<br><br>\nBased on the MoA annotations, the accuracy of solutions will be evaluated on the average value of the logarithmic loss function applied to each drug-MoA annotation pair.\n\nTo know more about evaluation metric , [click here](https://www.kaggle.com/c/lish-moa/overview/evaluation)"},{"metadata":{},"cell_type":"markdown","source":"## Domain Knowledge\n\nSome domain knowledge can be gained from watching the below video and from additionally watching [here](https://www.youtube.com/watch?v=UMxsZdVrA7A&ab_channel=PharmaStudy)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\nHTML('<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/PGzT3cTPah8\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></center>')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Version Update\n1. **V1,2,3 : Built the NN model with 5 folds + minor bug fixes**\n - OOF score : 0.02702\n \n2. **V4,5 : Top features extracted using feature engg.**\n - OOF score : 0.02719\n \n3. **V6 : Early Stopping applied with EPOCHS=500**\n - OOF score : 0.01617\n\n4. **V7 : Scaling performed on training and test data**\n - OOF score : 0.01609\n \n5. **V8,9 : Lookahead and LR scheduler added.**\n - OOF score : 0.01609\n \n6. **V10 : Reduce LR plateau used instead of LR scheduler**\n - OOF score : 0.01604\n \n7. **V11,12 : Experimented with no. of units in each layer + minor changes**\n - OOF score : 0.01547\n \n8. **V13 : Model checkpoint + MultilabelStratifiedKFold added**\n - OOF score : 0.01631"},{"metadata":{},"cell_type":"markdown","source":"### Bring in the relevant packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau, ModelCheckpoint\nimport tensorflow_addons as tfa\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seed Everythig !!\ndef seed_everything(seed=2020): \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"SEED = 42\nNFOLDS = 5\nEPOCHS = 50\nBATCH_SIZE = 128\nID = 'sig_id'\nroot = '../input/lish-moa/'\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### A glimpse of data.."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(root + 'train_features.csv')\ntarget = pd.read_csv(root + 'train_targets_scored.csv')\ntest = pd.read_csv(root + 'test_features.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.read_csv(root + 'sample_submission.csv')\nsub_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Top Features - Feature selection\n\nI have taken this list of top features from [DmitryS' notebook](https://www.kaggle.com/simakov/keras-multilabel-neural-network-v1-2). You can go and check it out."},{"metadata":{"trusted":true},"cell_type":"code","source":"top_feats = [  1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n        18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n        32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n        47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n        61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n        74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n       102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n       115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n       129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n       144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n       158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n       171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n       198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n       213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n       227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n       240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n       267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n       281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n       295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n       310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n       324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n       337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n       350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n       363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n       405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n       419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n       432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n       447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n       463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n       476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n       490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n       506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n       522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n       538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n       552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n       571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n       586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n       600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n       618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n       631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n       645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n       660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n       673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n       686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n       701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n       718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n       733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n       748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n       762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n       775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n       789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n       804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n       821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n       837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n       854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n       870, 871, 872, 873, 874]\nprint(len(top_feats))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# keep the ID column separate\ntrain_id = train[ID]\ntest_id = test[ID]\nsub_id = sub_df[ID]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Keeping the important features only\nimportant_cols = []\ntrain_cols = train.columns\nfor i in range(len(train_cols)):\n    if i in top_feats:\n        important_cols.append(train_cols[i])\nprint(len(important_cols))\n\ntrain = train[important_cols]\ntest = test[important_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    _df = df.copy()\n    _df['cp_type'] = _df['cp_type'].apply(lambda x : 1 if x == 'ctl_vehicle' else 0)\n    _df['cp_dose'] = _df['cp_dose'].apply(lambda x : 1 if x == 'D2' else 0)\n    return _df\n\ntrain = preprocess(train)\ntest = preprocess(test)\n\ndel target[ID]\n\ntarget = target.loc[train['cp_type']==0].reset_index(drop=True)\ntrain = train.loc[train['cp_type']==0].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combine both train and test for Scaling purpose. We will use a Standard scaler which will standardize features by removing the mean and scaling to unit variance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# ===== SCALING ===== #\n\ntrain['WHERE'] = 'train'\ntest['WHERE'] = 'test'\ntemp = train.append(test)\n\nscaler = StandardScaler()\ntemp.iloc[:, :-1] = scaler.fit_transform(temp.iloc[:, :-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separate train and test data\ntrain = temp.loc[temp.WHERE == 'train']\ntest = temp.loc[temp.WHERE == 'test']\ndel train['WHERE']\ndel test['WHERE']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Neural Network Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define callbacks\n\ndef get_early_stopper():\n    earlyStop = EarlyStopping( monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto',\n        baseline=None, restore_best_weights=True)\n    return earlyStop\n\n\ndef get_lr_callback(batch_size = 64, plot = False):\n    \"\"\"Returns a lr_scheduler callback which is used for training.\n    Feel free to change the values below!\n    \"\"\"\n    lr_start   = 0.001\n    lr_max     = 0.001 * BATCH_SIZE # higher batch size --> higher lr\n    lr_min     = 0.00001\n    # 30% of all epochs are used for ramping up the LR and then declining starts\n    lr_ramp_ep = EPOCHS * 0.3\n    lr_sus_ep  = 0\n    lr_decay   = 0.9\n\n    def lr_scheduler(epoch):\n            if epoch < lr_ramp_ep:\n                lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n\n            elif epoch < lr_ramp_ep + lr_sus_ep:\n                lr = lr_max\n\n            else:\n                lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n\n            return lr\n    \n    if plot == False:\n        # get the Keras-required callback with our LR for training\n        lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler,verbose = 2)\n        return lr_callback \n    \n    else: \n        return lr_scheduler\n\n    \ndef reduce_lr_on_plateau():\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3,\n                                  epsilon = 1e-4, mode = 'min', verbose=1)\n    return reduce_lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_model(l): \n    model = tf.keras.Sequential([\n        Input(l),\n        BatchNormalization(),\n        Dropout(0.2),\n        tfa.layers.WeightNormalization(Dense(2048, activation=\"elu\")),\n        BatchNormalization(),\n        Dropout(0.5),\n        tfa.layers.WeightNormalization(Dense(1024, activation=\"elu\")),\n        BatchNormalization(),\n        Dropout(0.5),\n        tfa.layers.WeightNormalization(Dense(206, activation=\"sigmoid\")),\n    ])\n\n    model.compile(loss='binary_crossentropy',\n                 #optimizer = tf.keras.optimizers.Adam(),\n                 #optimizer = tfa.optimizers.LazyAdam(0.001),\n                 optimizer = tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period=10),\n                 #optimizer = tf.keras.optimizers.SGD(),\n                  metrics=[\"accuracy\"]\n                 )\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net = make_model(len(train.columns))\nnet.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fold preparation & Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_features = target.columns\n_input = train.shape[1]\noof_preds = np.zeros((train.shape[0], 206))\nsub_df.loc[:, y_features] = 0\n\ntrain = train.values\ntarget = target.values\ntest = test.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I tried both **Learning Rate Scheduler** and **ReduceLROnPlateau** callbacks. The latter seems to give better results on this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"N_START = 7\ntf.random.set_seed(SEED)\n\nfor seed in range(N_START):\n    print(f\"\\nSEED {seed+1}\\n\")\n    for fold, (tr_idx, val_idx) in enumerate(MultilabelStratifiedKFold(n_splits=NFOLDS, random_state=42, \n                                                                       shuffle=True).split(train, target)):\n        print(f\"\\nFOLD {fold+1}\\n\")\n        X_train = train[tr_idx]\n        X_val = train[val_idx]\n        y_train = target[tr_idx]\n        y_val = target[val_idx]\n\n        net = make_model(_input)\n\n        checkpoint_path = f'repeat:{seed}_Fold:{fold}.hdf5'\n        cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n                                         save_weights_only = True, mode = 'min')\n\n        net.fit(X_train, y_train,\n                batch_size = BATCH_SIZE,\n                epochs = EPOCHS,\n                validation_data = (X_val, y_val),\n                #callbacks = [get_early_stopper(), get_lr_callback(BATCH_SIZE)],\n                callbacks = [get_early_stopper(), reduce_lr_on_plateau(), cb_checkpt],\n                verbose=2\n               )\n\n        net.load_weights(checkpoint_path)\n        # net.evaluate() returns loss values and metric values\n        print(\"[INFO] Train : \", net.evaluate(X_train, y_train, batch_size=BATCH_SIZE, verbose=0, return_dict=True))\n        print(\"[INFO] Validation : \", net.evaluate(X_val, y_val, batch_size=BATCH_SIZE, verbose=0, return_dict=True))\n        print(\"[INFO] Predicting val...\")\n        oof_preds[val_idx] = net.predict(X_val, batch_size=BATCH_SIZE, verbose=0)\n        print(\"[INFO] Predicting test...\")\n        sub_df.loc[:, y_features] += net.predict(test, batch_size=BATCH_SIZE, verbose=0) / NFOLDS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### OOF Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def score(y_true, y_preds):\n    metric = []\n    for col in range(target.shape[1]):\n        metric.append(log_loss(y_true[:, col], y_preds[:, col].astype('float'), labels=[0,1]))\n    return np.mean(metric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"metric = score(target, oof_preds)\nprint(f\"OOF Metric : {metric}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color='blue'>Thanks a lot for reading. I hope this notebook helped you (...even a tiny bit). If you liked it, an UPVOTE is highly appreciated. If you are interested in more content like this, feel free to follow me! ;)</font>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}