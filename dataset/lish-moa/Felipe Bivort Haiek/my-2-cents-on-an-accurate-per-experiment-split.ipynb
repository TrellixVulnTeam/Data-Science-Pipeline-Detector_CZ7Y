{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"Since the dawn of MOA countless souls have wondered  and mused upon the te separation between experiments. As dusk creeps on this competition, in a last effort to improve the correlation between CV and leaderboad score, a light shines in the darkness.\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"Here I show how to obtain 2 possible groupings using different cluster techiniqes\n    - kmeans\n    - hierarchical clustering\n(Every experiment should include at most 6 trials with different time-dose combinations.)\nI then proceed to use the latter because of its nicer separation properties.\nI encourage those who come after me to try to improve this results and share their thoughts."},{"metadata":{},"cell_type":"markdown","source":"BTW Upvote if you find this useful  :)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys \nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sn\n\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.metrics.pairwise import cosine_similarity,paired_distances ,paired_cosine_distances, pairwise_distances , pairwise_distances_argmin,pairwise_distances_argmin_min\n\nfrom scipy.cluster.hierarchy import dendrogram\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import AgglomerativeClustering ,DBSCAN, KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            \n            # Print current column type\n            #print(\"******************************\")\n            #print(\"Column: \",col)\n            #print(\"dtype before: \",props[col].dtype)\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n            \n            # Print new column type\n           # print(\"dtype after: \",props[col].dtype)\n           # print(\"******************************\")\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n    return props, NAlist\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef train_short_form_loader(feature_file,target_file,extra_target_file=None):\n    '''takes the original target and features and creates a train dataset \n    in col long format'''\n\n\n    train_features = pd.read_csv(feature_file)\n\n    train_targets = pd.read_csv(target_file)\n    train_features,_= reduce_mem_usage(train_features)\n    train_targets,_ = reduce_mem_usage(train_targets)\n\n\n    if extra_target_file is not None:\n        extra_targets = pd.read_csv(extra_target_file)\n        extra_targets,_ = reduce_mem_usage(extra_targets)\n        train_targets = pd.merge(train_targets,extra_targets,on ='sig_id')\n        del extra_targets\n\n    targets = train_targets.columns[1:]\n\n    train_melt=train_targets.merge(train_features,how=\"left\",on=\"sig_id\")\n\n\n    del train_features,train_targets\n\n\n    train_melt.set_index(\"sig_id\",inplace=True)\n\n    #train_melt[\"variable\"]= train_melt[\"variable\"].astype('category')\n    train_melt[\"cp_type\"]= train_melt[\"cp_type\"].astype('category')\n    train_melt[\"cp_dose\"]= train_melt[\"cp_dose\"].astype('category')\n\n    return train_melt , targets.to_list()\n\n\n\ndef test_short_form_loader(feature_file):\n    '''takes the original target and features and creates a train dataset \n    in col long format'''\n\n\n    train_features = pd.read_csv(feature_file)\n\n    #train_targets = pd.read_csv(target_file)\n    train_features,_= reduce_mem_usage(train_features)\n    #train_targets,_ = reduce_mem_usage(train_targets)\n\n    train_melt =  train_features.copy()\n    del train_features\n\n\n    train_melt.set_index(\"sig_id\",inplace=True)\n\n    #train_melt[\"variable\"]= train_melt[\"variable\"].astype('category')\n    train_melt[\"cp_type\"]= train_melt[\"cp_type\"].astype('category')\n    train_melt[\"cp_dose\"]= train_melt[\"cp_dose\"].astype('category')\n\n    return train_melt ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train,target_cols = train_short_form_loader('../input/lish-moa/train_features.csv','../input/lish-moa/train_targets_scored.csv','../input/lish-moa/train_targets_nonscored.csv')\n\ntrain.cp_type.unique()\n\ntrain =  train.loc[train.cp_type=='trt_cp']\n\ntrain.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_clus = train.drop(['cp_time','cp_dose','cp_type'],axis=1).copy()\n\n#here we eight target more, the idea is that  trials from the same experiment should have similar targets\n\nTARGETS_WEIGHT = 1000\n\ntrain_clus[target_cols] = train_clus[target_cols] *TARGETS_WEIGHT","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans=KMeans(n_clusters=3000,max_iter=100)\n\n\nkmeans.fit(train_clus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle as pk\n\npk.dump(kmeans,open('first_kmean_try.pk','wb'))\n\nresults_train_clus = train_clus.copy()\nresults_train_clus['kmeans1-clusters'] = kmeans.predict(train_clus)\n\nresults_train_clus['kmeans1-clusters'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"good_clusters = results_train_clus['kmeans1-clusters'].value_counts().loc[results_train_clus['kmeans1-clusters'].value_counts() < 7].index.to_list()\n\ntrain['kmeans1-clusters'] = results_train_clus['kmeans1-clusters']\n\ngood_cluster_duplicates= train.loc[train['kmeans1-clusters'].isin(good_clusters),['kmeans1-clusters','cp_time','cp_dose'] ].groupby('kmeans1-clusters').apply(lambda x: x.duplicated().sum())\n\nplt.hist(good_cluster_duplicates)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(results_train_clus['kmeans1-clusters'].value_counts(),bins=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"unluckily the number of sig_ids per clusters does not peak at 5 or 6 as expected."},{"metadata":{},"cell_type":"markdown","source":"# Hierarchical clustering"},{"metadata":{},"cell_type":"markdown","source":"## distance matrix creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"distance_matrix= pairwise_distances(train_clus,metric='l1')\n\ndistance_matrix.shape\n\ndistance_matrix.tofile('distance_matrix_l1_pure')\n\ndistance_matrix\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_partitions ={}\n\nfor key,group in train.groupby(['cp_dose','cp_time']):\n    \n    train_partitions[key]=group.drop(['cp_dose','cp_time'],axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we create a same time-dose penalization to add to the whole distance matrix,"},{"metadata":{"trusted":true},"cell_type":"code","source":"SAME_TIME_DOSE_PENALIZATION = 2000\npenalization_same_group= train.apply(lambda x: SAME_TIME_DOSE_PENALIZATION * train.index.isin(train_partitions[tuple(x[['cp_dose','cp_time']].values.tolist())].index),axis=1)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"penalization_same_group.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distance_matrix = distance_matrix.reshape((len(train),len(train)))\n\nmat_penalization = np.concatenate(penalization_same_group.values).reshape((len(train),len(train))) - 2000 * np.eye(len(train),len(train))\n\ntotal_matrix = distance_matrix + mat_penalization\n\ntotal_matrix.tofile('distance_matrix_l1_WsamegroupPenalization2')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nresults_train_clus = train_clus.copy()\nresults_train_clus['aggclus1-clusters'] = aggclus.fit_predict(total_matrix)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_train_clus = pd.read_csv('../input/moafoldsmiscelanea/aggclus3_results.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nresults_train_clus['aggclus1-clusters'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(results_train_clus['aggclus1-clusters'].value_counts(),bins=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can well see hierarchical clustering using the distance matrix with the same time-dose penalization  gives a better distribution, in the sense that the majority of clusters ( experiments) involve 5 or 6 sig-ids (trials)"},{"metadata":{},"cell_type":"markdown","source":"Unluckily I can not run the fll code on a kernel and each distance matrix file weights 3 GB so please tell me in the comments if you know of a way to create the distance matrix sequentially, or exploit some trick. "},{"metadata":{},"cell_type":"markdown","source":"Happy Kaggling !"},{"metadata":{},"cell_type":"markdown","source":"P.S. : check my kernel on how to use this for Group kfolding"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}