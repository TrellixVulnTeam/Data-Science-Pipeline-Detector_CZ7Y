{"cells":[{"metadata":{},"cell_type":"markdown","source":"## <font color = \"green\">Installing Libraries</font>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"%%capture\n!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet\n# Iterative Stratification\n!pip install /kaggle/input/iterative-stratification/iterative-stratification-master/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color = \"green\">Loading Libraries</font>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"### General ###\nimport os\nimport sys\nimport copy\nimport tqdm\nimport pickle\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsys.path.append(\"../input/rank-gauss\")\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n\n### Data Wrangling ###\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom gauss_rank_scaler import GaussRankScaler\n\n### Data Visualization ###\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use(\"fivethirtyeight\")\n\n### Machine Learning ###\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\n### Deep Learning ###\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.nn import functional as F\nfrom torch.nn.modules.loss import _WeightedLoss\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n# Tabnet \nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n### Make prettier the prints ###\nfrom colorama import Fore\nc_ = Fore.CYAN\nm_ = Fore.MAGENTA\nr_ = Fore.RED\nb_ = Fore.BLUE\ny_ = Fore.YELLOW\ng_ = Fore.GREEN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color = \"green\">Reproducibility</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 42\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nset_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color = \"green\">Configuration</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters\ndata_path = \"../input/lish-moa/\"\nno_ctl = True\nscale = \"rankgauss\"\nvariance_threshould = 0.7\ndecompo = \"PCA\"\nncompo_genes = 100\nncompo_cells = 20\nencoding = \"dummy\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color = \"green\">Loading the Data</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(data_path + \"train_features.csv\")\n#train.drop(columns = [\"sig_id\"], inplace = True)\n\ntargets = pd.read_csv(data_path + \"train_targets_scored.csv\")\n#train_targets_scored.drop(columns = [\"sig_id\"], inplace = True)\n\n#train_targets_nonscored = pd.read_csv(data_path + \"train_targets_nonscored.csv\")\n\ntest = pd.read_csv(data_path + \"test_features.csv\")\n#test.drop(columns = [\"sig_id\"], inplace = True)\n\nsubmission = pd.read_csv(data_path + \"sample_submission.csv\")\ndrug_id = pd.read_csv(data_path + \"train_drug.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <font color = \"seagreen\">Preprocessing and Feature Engineering</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"if no_ctl:\n    # cp_type == ctl_vehicle\n    print(b_, \"not_ctl\")\n    ctl_vehicle_train = train[\"cp_type\"] != \"ctl_vehicle\"\n    train = train[train[\"cp_type\"] != \"ctl_vehicle\"]\n    test = test[test[\"cp_type\"] != \"ctl_vehicle\"]\n    targets = targets.iloc[train.index]\n    train.reset_index(drop = True, inplace = True)\n    test.reset_index(drop = True, inplace = True)\n    targets.reset_index(drop = True, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color = \"green\">Distributions Before Rank Gauss and PCA</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in train.columns if col.startswith(\"g-\")]\nCELLS = [col for col in train.columns if col.startswith(\"c-\")]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color = \"green\">Distributions of the Train Set</font>"},{"metadata":{},"cell_type":"markdown","source":"### <font color = \"green\">Distributions of the Test Set</font>"},{"metadata":{},"cell_type":"markdown","source":"## <font color = \"green\">Rank Gauss Process</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all = pd.concat([train, test], ignore_index = True)\ncols_numeric = [feat for feat in list(data_all.columns) if feat not in [\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"]]\nmask = (data_all[cols_numeric].var() >= variance_threshould).values\ntmp = data_all[cols_numeric].loc[:, mask]\ndata_all = pd.concat([data_all[[\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"]], tmp], axis = 1)\ncols_numeric = [feat for feat in list(data_all.columns) if feat not in [\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale_minmax(col):\n    return (col - col.min()) / (col.max() - col.min())\n\ndef scale_norm(col):\n    return (col - col.mean()) / col.std()\n\nif scale == \"boxcox\":\n    print(b_, \"boxcox\")\n    data_all[cols_numeric] = data_all[cols_numeric].apply(scale_minmax, axis = 0)\n    trans = []\n    for feat in cols_numeric:\n        trans_var, lambda_var = stats.boxcox(data_all[feat].dropna() + 1)\n        trans.append(scale_minmax(trans_var))\n    data_all[cols_numeric] = np.asarray(trans).T\n    \nelif scale == \"norm\":\n    print(b_, \"norm\")\n    data_all[cols_numeric] = data_all[cols_numeric].apply(scale_norm, axis = 0)\n    \nelif scale == \"minmax\":\n    print(b_, \"minmax\")\n    data_all[cols_numeric] = data_all[cols_numeric].apply(scale_minmax, axis = 0)\n    \nelif scale == \"rankgauss\":\n    ### Rank Gauss ###\n    print(b_, \"Rank Gauss\")\n    scaler = GaussRankScaler()\n    data_all[cols_numeric] = scaler.fit_transform(data_all[cols_numeric])\n    \nelse:\n    pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color = \"green\">Principal Component Analysis</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# PCA\nif decompo == \"PCA\":\n    print(b_, \"PCA\")\n    GENES = [col for col in data_all.columns if col.startswith(\"g-\")]\n    CELLS = [col for col in data_all.columns if col.startswith(\"c-\")]\n    \n    pca_genes = PCA(n_components = ncompo_genes,\n                    random_state = seed).fit_transform(data_all[GENES])\n    pca_cells = PCA(n_components = ncompo_cells,\n                    random_state = seed).fit_transform(data_all[CELLS])\n    \n    pca_genes = pd.DataFrame(pca_genes, columns = [f\"pca_g-{i}\" for i in range(ncompo_genes)])\n    pca_cells = pd.DataFrame(pca_cells, columns = [f\"pca_c-{i}\" for i in range(ncompo_cells)])\n    data_all = pd.concat([data_all, pca_genes, pca_cells], axis = 1)\nelse:\n    pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color = \"green\">One Hot</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding\nif encoding == \"lb\":\n    print(b_, \"Label Encoding\")\n    for feat in [\"cp_time\", \"cp_dose\"]:\n        data_all[feat] = LabelEncoder().fit_transform(data_all[feat])\nelif encoding == \"dummy\":\n    print(b_, \"One-Hot\")\n    data_all = pd.get_dummies(data_all, columns = [\"cp_time\", \"cp_dose\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in data_all.columns if col.startswith(\"g-\")]\nCELLS = [col for col in data_all.columns if col.startswith(\"c-\")]\n\nfor stats in tqdm.tqdm([\"sum\", \"mean\", \"std\", \"kurt\", \"skew\"]):\n    data_all[\"g_\" + stats] = getattr(data_all[GENES], stats)(axis = 1)\n    data_all[\"c_\" + stats] = getattr(data_all[CELLS], stats)(axis = 1)    \n    data_all[\"gc_\" + stats] = getattr(data_all[GENES + CELLS], stats)(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color = \"green\">Distributions After Rank Gauss and PCA</font>"},{"metadata":{},"cell_type":"markdown","source":"### <font color = \"green\">Distributions of \"data_all\"</font>"},{"metadata":{},"cell_type":"markdown","source":"We can confirme that the shapes of data got close to the normal distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_df and test_df\nfeatures_to_drop = [\"sig_id\", \"cp_type\"]\ntry:\n    data_all.drop(features_to_drop, axis = 1, inplace = True)\nexcept:\n    pass\ntry:\n    targets.drop(\"sig_id\", axis = 1, inplace = True)\nexcept:\n    pass\ntrain_df = data_all[: train.shape[0]]\ntrain_df.reset_index(drop = True, inplace = True)\n# The following line it's a bad practice in my opinion, targets on train set\n#train_df = pd.concat([train_df, targets], axis = 1)\ntest_df = data_all[train_df.shape[0]: ]\ntest_df.reset_index(drop = True, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"{b_}train_df.shape: {r_}{train_df.shape}\")\nprint(f\"{b_}test_df.shape: {r_}{test_df.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test_df.values\nprint(f\"{b_}X_test.shape: {r_}{X_test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <font color = \"seagreen\">Modeling</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LogitsLogLoss(Metric):\n    \"\"\"\n    LogLoss with sigmoid applied\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        \"\"\"\n        Compute LogLoss of predictions.\n\n        Parameters\n        ----------\n        y_true: np.ndarray\n            Target matrix or vector\n        y_score: np.ndarray\n            Score matrix or vector\n\n        Returns\n        -------\n            float\n            LogLoss of predictions vs targets.\n        \"\"\"\n        logits = 1 / (1 + np.exp(-y_pred))\n        aux = (1 - y_true) * np.log(1 - logits + 1e-15) + y_true * np.log(logits + 1e-15)\n        return np.mean(-aux)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <font color = \"seagreen\">Training</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\nFOLDS = 5\n\ndef create_fold():\n    from sklearn.model_selection import KFold\n    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\n    # LOAD FILES\n    train_feats = pd.read_csv('../input/lish-moa/train_features.csv')\n    scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\n    drug = pd.read_csv('/kaggle/input/lish-moa/train_drug.csv')\n    # scored = scored.loc[train_feats['cp_type'] == 'trt_cp', :]\n    # drug = drug.loc[train_feats['cp_type'] == 'trt_cp', :]\n    targets = scored.columns[1:]\n    scored = scored.merge(drug, on='sig_id', how='left') \n\n    # LOCATE DRUGS\n    vc = scored.drug_id.value_counts()\n    vc1 = vc.loc[vc<=18].index.sort_values()\n    vc2 = vc.loc[vc>18].index.sort_values()\n\n    # STRATIFY DRUGS 18X OR LESS\n    dct1 = {}; dct2 = {}\n    skf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, \n              random_state=SEED)\n    tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n        dd = {k:fold for k in tmp.index[idxV].values}\n        dct1.update(dd)\n\n    # STRATIFY DRUGS MORE THAN 18X\n    skf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, \n              random_state=SEED)\n    tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\n    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n        dd = {k:fold for k in tmp.sig_id[idxV].values}\n        dct2.update(dd)\n\n    # ASSIGN FOLDS\n    scored['fold'] = scored.drug_id.map(dct1)\n    scored.loc[scored.fold.isna(),'fold'] =\\\n        scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\n    scored.fold = scored.fold.astype('int8')\n    scored = scored.loc[ctl_vehicle_train]\n    return scored\n\nscored = create_fold()\nscored","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scored = scored.drop([\"sig_id\", \"drug_id\"], axis=1).reset_index(drop=True)\ntrain_df = train_df.reset_index(drop=True)\nscored","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if encoding == \"lb\":\n    categoricals = [train_df.columns.get_loc(\"cp_time\"), train_df.columns.get_loc(\"cp_dose\")]\nelse:\n    categoricals = [train_df.columns.get_loc(x) for x in train_df.columns if \"cp_\" in x]\ncategoricals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_EPOCH = 400\n# n_d and n_a are different from the original work, 32 instead of 24\n# This is the first change in the code from the original\ntabnet_params = dict(\n    n_d = 32,\n    n_a = 32,\n    n_steps = 1,\n    gamma = 1.3,\n    lambda_sparse = 0,\n    optimizer_fn = optim.Adam,\n    optimizer_params = dict(lr = 2e-2, weight_decay=1e-5),\n    mask_type = \"entmax\",\n    scheduler_params = dict(\n        mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n    scheduler_fn = ReduceLROnPlateau,\n    seed = seed,\n    verbose = 5,\n    cat_idxs = categoricals\n)\n\nscores_auc_all = []\ntest_cv_preds = []\n\n\noof_preds = []\noof_targets = []\nscores = []\nscores_auc = []\nfor i in range(FOLDS):\n    print(b_,\"FOLDS: \", r_, i + 1)\n    print(g_, '*' * 60, c_)\n    \n    idx = scored[\"fold\"] == i\n    not_idx = scored[\"fold\"] != i\n    scored_df = scored.drop(\"fold\", axis=1)\n    X_train, y_train = train_df.values[not_idx, :], scored_df.values[not_idx, :]\n    X_val, y_val = train_df.values[idx, :], scored_df.values[idx, :]\n    ### Model ###\n    model = TabNetRegressor(**tabnet_params)\n        \n    ### Fit ###\n    # Another change to the original code\n    # virtual_batch_size of 32 instead of 128\n    model.fit(\n        X_train = X_train,\n        y_train = y_train,\n        eval_set = [(X_val, y_val)],\n        eval_name = [\"val\"],\n        eval_metric = [\"logits_ll\"],\n        max_epochs = MAX_EPOCH,\n        patience = 30,\n        batch_size = 1024, \n        virtual_batch_size = 32,\n        num_workers = 1,\n        drop_last = False,\n        # To use binary cross entropy because this is not a regression problem\n        loss_fn = F.binary_cross_entropy_with_logits\n    )\n    print(y_, '-' * 60)\n    \n    ### Predict on validation ###\n    preds_val = model.predict(X_val)\n    # Apply sigmoid to the predictions\n    preds = 1 / (1 + np.exp(-preds_val))\n    score = np.min(model.history[\"val_logits_ll\"])\n    \n    ### Save OOF for CV ###\n    oof_preds.append(preds_val)\n    oof_targets.append(y_val)\n    scores.append(score)\n    \n    ### Predict on test ###\n    preds_test = model.predict(X_test)\n    test_cv_preds.append(1 / (1 + np.exp(-preds_test)))\n\noof_preds_all = np.concatenate(oof_preds)\noof_targets_all = np.concatenate(oof_targets)\ntest_preds_all = np.stack(test_cv_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aucs = []\nfor task_id in range(oof_preds_all.shape[1]):\n    aucs.append(roc_auc_score(y_true = oof_targets_all[:, task_id],\n                              y_score = oof_preds_all[:, task_id]\n                             ))\nprint(f\"{b_}Overall AUC: {r_}{np.mean(aucs)}\")\nprint(f\"{b_}Average CV: {r_}{np.mean(scores)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The worst CV value that i achive**"},{"metadata":{},"cell_type":"markdown","source":"# <font color = \"seagreen\">Submission</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_feat = [col for col in submission.columns if col not in [\"sig_id\"]]\n# To obtain the same lenght of test_preds_all and submission\ntest = pd.read_csv(data_path + \"test_features.csv\")\nsig_id = test[test[\"cp_type\"] != \"ctl_vehicle\"].sig_id.reset_index(drop = True)\ntmp = pd.DataFrame(test_preds_all.mean(axis = 0), columns = all_feat)\ntmp[\"sig_id\"] = sig_id\n\nsubmission = pd.merge(test[[\"sig_id\"]], tmp, on = \"sig_id\", how = \"left\")\nsubmission.fillna(0, inplace = True)\n\n#submission[all_feat] = tmp.mean(axis = 0)\n\n# Set control to 0\n#submission.loc[test[\"cp_type\"] == 0, submission.columns[1:]] = 0\nsubmission.to_csv(\"submission.csv\", index = None)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"{b_}submission.shape: {r_}{submission.shape}\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}