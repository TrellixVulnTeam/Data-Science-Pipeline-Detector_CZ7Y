{"cells":[{"metadata":{},"cell_type":"markdown","source":"## What is working:\n\nVersion 2:\n- PCA\n- PCA concat train and test (better than PCA for seperate sets)\n- Swish activation (instead of tanh)\n- Adam (rather than sgd)\n\nVersion 8:\n- Lookahead RAdam\n- Do not use \"cp_type\" column\n- Relu (better than Swish)\n- Weight Normalization\n- `ctl_vehicle` data is all zeros\n- Label smoothing\n- Batch Normalization\n\nVersion 9:\n- Increase model's nodes\n- np.clid(y_pred, 0.001, 0.999)\n\n## What is not working:\n- Quantile transformation (5% and 95%)\n- Tanh activation\n\n## Current best version: 8"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- train_features.csv - Features for the training set. Features `g-` signify gene expression data, and `c-` signify cell viability data. `cp_type` indicates samples treated with a compound (`cp_vehicle`) or with a control perturbation (`ctrl_vehicle`); control perturbations have no MoAs; `cp_time` and `cp_dose` indicate treatment duration (24, 48, 72 hours) and `dose` (high or low).\n- train_targets_scored.csv - The binary MoA targets that are scored.\n- train_targets_nonscored.csv - Additional (optional) binary MoA responses for the training data. These are not predicted nor scored.\n- test_features.csv - Features for the test data. You must predict the probability of each scored MoA for each row in the test data.\n- sample_submission.csv - A submission file in the correct format."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/lish-moa/train_features.csv\")\ntest=pd.read_csv(\"../input/lish-moa/test_features.csv\")\nsub=pd.read_csv(\"../input/lish-moa/sample_submission.csv\")\ntrain_target=pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum(axis=0).sum(), train.isnull().sum(axis=1).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.nunique(dropna=False).sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # This code is used to check duplicate columns (if any). It runs for a long time: the result is None, so avoid running this cell\n\n# train_factorized = pd.DataFrame(index=train.index)\n# for col in tqdm.notebook.tqdm(train.columns):\n#     train_factorized[col] = train[col].map(train[col].value_counts())\n\n\n# dup_cols = {}\n\n# for i, c1 in enumerate(tqdm_notebook(train_factorized.columns)):\n#     for c2 in train_factorized.columns[i + 1:]:\n#         if c2 not in dup_cols and np.all(train_factorized[c1] == train_factorized[c2]):\n#             dup_cols[c2] = c1\n            \n# dup_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for classes distribution\nlimit = 0\nfor col in tqdm_notebook(train_target.columns):\n    if col != \"sig_id\":\n        print(train_target[col].value_counts())\n    limit+=1\n    if limit >= 15:\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ctlVehicle_idx = train[\"cp_type\"] != \"ctl_vehicle\"\ntrain = train.loc[ctlVehicle_idx].reset_index(drop=True)\ntrain = train.drop(\"cp_type\", axis=1)\ntrain_target = train_target.loc[ctlVehicle_idx].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_g = list(train.columns[3:775])\nfeatures_c = list(train.columns[775:875])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [train, test]:\n    df['g-sum'] = df[features_g].sum(axis = 1)\n    df['g-mean'] = df[features_g].mean(axis = 1)\n    df['g-std'] = df[features_g].std(axis = 1)\n    df['g-kurt'] = df[features_g].kurtosis(axis = 1)\n    df['g-skew'] = df[features_g].skew(axis = 1)\n    df['c-sum'] = df[features_c].sum(axis = 1)\n    df['c-mean'] = df[features_c].mean(axis = 1)\n    df['c-std'] = df[features_c].std(axis = 1)\n    df['c-kurt'] = df[features_c].kurtosis(axis = 1)\n    df['c-skew'] = df[features_c].skew(axis = 1)\n    df['gc-sum'] = df[features_g + features_c].sum(axis = 1)\n    df['gc-mean'] = df[features_g + features_c].mean(axis = 1)\n    df['gc-std'] = df[features_g + features_c].std(axis = 1)\n    df['gc-kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n    df['gc-skew'] = df[features_g + features_c].skew(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Robust Scaler and PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"gcols = [g for g in train.columns if \"g-\" in g]\nccols = [c for c in train.columns if \"c-\" in c]\ncpcols = [cp for cp in train.columns if \"cp_\" in cp]\ngccols = [gc for gc in train.columns if \"gc-\" in gc]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nctlVehicle_test = test[\"cp_type\"] == \"ctl_vehicle\"\ntest = test.drop(\"cp_type\", axis=1)\n\nenc = LabelEncoder()\nfor col in train[cpcols]:\n    train[col] = enc.fit_transform(train[col])\n    \nenc = LabelEncoder()\nfor col in test[cpcols]:\n    test[col] = enc.fit_transform(test[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import RobustScaler\n\nrs = RobustScaler(quantile_range=(5, 95))\nfeatures = train.columns[3:]\nrs.fit(pd.concat([train[features], test[features]], axis=0))\ntrain[features] = rs.transform(train[features])\ntest[features] = rs.transform(test[features])\n\n\ng_pca = PCA(n_components=0.95, random_state=42)\nc_pca = PCA(n_components=0.95, random_state=42)\n\n\ntrain_test_g_concat = pd.concat([train[gcols], test[gcols]], axis=0)\ntrain_test_c_concat = pd.concat([train[ccols], test[ccols]], axis=0)\ng_pca.fit(train_test_g_concat)\nc_pca.fit(train_test_c_concat)\n\ntrain_gtrans = pd.DataFrame(g_pca.transform(train[gcols]), columns=[\"g_PCA\" + str(i) for i in range(g_pca.n_components_)], index=train.index)\ntest_gtrans = pd.DataFrame(g_pca.transform(test[gcols]), columns=[\"g_PCA\" + str(i) for i in range(g_pca.n_components_)], index=test.index)\n\ntrain_ctrans = pd.DataFrame(c_pca.transform(train[ccols]), columns=[\"c_PCA\" + str(i) for i in range(c_pca.n_components_)], index=train.index)\ntest_ctrans = pd.DataFrame(c_pca.transform(test[ccols]), columns=[\"c_PCA\" + str(i) for i in range(c_pca.n_components_)], index=test.index)\n\ng_pca.n_components_, c_pca.n_components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([train_gtrans, train_ctrans, train[cpcols], train[gccols]], axis=1)\ntest = pd.concat([test_gtrans, test_ctrans, test[cpcols], test[gccols]], axis=1)\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target = train_target.drop(\"sig_id\", axis=1)\ntrain_target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow_addons as tfa\n\n\ndef create_model():\n    model = tf.keras.Sequential([\n        tfa.layers.WeightNormalization(L.Dense(train.shape[1], input_shape=(train.shape[1],))),\n        L.BatchNormalization(),\n        tfa.layers.WeightNormalization(L.Dense(1024, activation=\"relu\")),\n        L.BatchNormalization(),\n        L.Dropout(0.3),\n        tfa.layers.WeightNormalization(L.Dense(1024, activation=\"relu\")),\n        L.BatchNormalization(),\n        L.Dropout(0.2),\n        tfa.layers.WeightNormalization(L.Dense(train_target.shape[1], activation=\"sigmoid\"))\n    ])\n    \n    sgd = tf.keras.optimizers.SGD()\n    adamw = tfa.optimizers.AdamW(weight_decay = 1e-5)\n    adam = tf.keras.optimizers.Adam()\n    radam = tfa.optimizers.RectifiedAdam()\n    lookahead_radam = tfa.optimizers.Lookahead(radam)\n    \n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-15), optimizer=lookahead_radam, metrics=[\"binary_crossentropy\"])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\npredictions = []\nkf = KFold(shuffle=True, random_state=42)\nfor fold_id, (train_idx, valid_idx) in enumerate(kf.split(train)):\n    model = create_model()\n    history = model.fit(train.iloc[train_idx], train_target.iloc[train_idx], \n              validation_data=(train.iloc[valid_idx], train_target.iloc[valid_idx]),\n             epochs=50,\n             verbose=2,\n             callbacks=[\n    tf.keras.callbacks.ReduceLROnPlateau(),\n    tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n    tf.keras.callbacks.ModelCheckpoint(\"model_fold\" + str(fold_id) + \".h5\", save_best_only=True, save_weights_only=True)\n])\n    print(\"Fold ID: {}, train loss: {}, valid loss: {}\".format(fold_id, min(history.history[\"loss\"]), min(history.history[\"val_loss\"])))\n    model.load_weights(\"model_fold\" + str(fold_id) + \".h5\")\n    predictions.append(model.predict(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = np.average(predictions, axis=0)\npred = np.clip(pred, 0.001, 0.999)\npred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")\nsub.loc[:, 1:] = pred\nsub.loc[ctlVehicle_test, sub.columns != \"sig_id\"] = 0\n\n# sub.loc[:, 1:] = tf.keras.utils.normalize(pred)\nsub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}