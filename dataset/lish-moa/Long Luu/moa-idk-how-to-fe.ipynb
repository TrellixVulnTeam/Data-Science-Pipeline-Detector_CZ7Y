{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Current best version: 12"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- train_features.csv - Features for the training set. Features `g-` signify gene expression data, and `c-` signify cell viability data. `cp_type` indicates samples treated with a compound (`cp_vehicle`) or with a control perturbation (`ctrl_vehicle`); control perturbations have no MoAs; `cp_time` and `cp_dose` indicate treatment duration (24, 48, 72 hours) and `dose` (high or low).\n- train_targets_scored.csv - The binary MoA targets that are scored.\n- train_targets_nonscored.csv - Additional (optional) binary MoA responses for the training data. These are not predicted nor scored.\n- test_features.csv - Features for the test data. You must predict the probability of each scored MoA for each row in the test data.\n- sample_submission.csv - A submission file in the correct format."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/lish-moa/train_features.csv\")\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ctlVehicle_idx = train[\"cp_type\"] != \"ctl_vehicle\"\ntrain = train.loc[ctlVehicle_idx].reset_index(drop=True)\ntrain = train.drop(\"cp_type\", axis=1)\ntrain_target = train_target.loc[ctlVehicle_idx].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gcols = [g for g in train.columns if \"g-\" in g]\nccols = [c for c in train.columns if \"c-\" in c]\ncpcols = [cp for cp in train.columns if \"cp_\" in cp]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/lish-moa/test_features.csv\")\nctlVehicle_test = test[\"cp_type\"] == \"ctl_vehicle\"\ntest = test.drop(\"cp_type\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train.iloc[:, 3:].columns:\n    percent = train[col].quantile([0.01, 0.99]).values\n    train[col] = np.clip(train[col], percent[0], percent[1])\n\nfor col in test.iloc[:, 3:].columns:\n    percent = test[col].quantile([0.01, 0.99]).values\n    test[col] = np.clip(test[col], percent[0], percent[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append(\"../input/rank-gauss\")\nfrom gauss_rank_scaler import GaussRankScaler\nscaler = GaussRankScaler()\ntrain[gcols + ccols] = scaler.fit_transform(train[gcols + ccols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ntrain[\"cp_time\"]=train[\"cp_time\"].map({24: 1, 48: 2, 72: 3})\ntrain[\"cp_dose\"] = train[\"cp_dose\"].map({\"D1\": 1, \"D2\": 2})\ntest[\"cp_time\"]= test[\"cp_time\"].map({24: 1, 48: 2, 72: 3})\ntest[\"cp_dose\"]=test[\"cp_dose\"].map({\"D1\": 1, \"D2\": 2})\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler2 = GaussRankScaler()\ntest[gcols + ccols] = scaler2.fit_transform(test[gcols + ccols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\ng_pca = PCA(n_components=0.9)\nc_pca = PCA(n_components=0.9)\n\ntrain_test_g_concat = pd.concat([train[gcols], test[gcols]], axis=0)\ntrain_test_c_concat = pd.concat([train[ccols], test[ccols]], axis=0)\ng_pca.fit(train_test_g_concat)\nc_pca.fit(train_test_c_concat)\n\ntrain_gtrans = pd.DataFrame(g_pca.transform(train[gcols]), columns=[\"g_PCA\" + str(i) for i in range(g_pca.n_components_)], index=train.index)\ntest_gtrans = pd.DataFrame(g_pca.transform(test[gcols]), columns=[\"g_PCA\" + str(i) for i in range(g_pca.n_components_)], index=test.index)\n\ntrain_ctrans = pd.DataFrame(c_pca.transform(train[ccols]), columns=[\"c_PCA\" + str(i) for i in range(c_pca.n_components_)], index=train.index)\ntest_ctrans = pd.DataFrame(c_pca.transform(test[ccols]), columns=[\"c_PCA\" + str(i) for i in range(c_pca.n_components_)], index=test.index)\n\ng_pca.n_components_, c_pca.n_components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([train_gtrans, train_ctrans, train[cpcols]], axis=1)\ntest = pd.concat([test_gtrans, test_ctrans, test[cpcols]], axis=1)\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target = train_target.drop(\"sig_id\", axis=1)\ntrain_target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import tensorflow as tf\n# import math\n# def conf_penalty_loss(y_true, y_pred):\n#     beta = .0001\n#     y_pred = tf.clip_by_value(y_pred, 1e-15, 1 - 1e-5)\n#     epsilon = 0.01\n#     # calculating squared difference between target and predicted values \n#     bce = y_true * K.log(y_pred+epsilon) + (1-y_true) * K.log(1-y_pred+epsilon)\n#     bce_logits = y_true * K.log(y_true + epsilon) + (1-y_true) * K.log(1-y_pred + epsilon)\n#     loss = bce - beta * bce_logits\n#     return -bce","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow_addons as tfa\n\nuse_custom_loss = False\ndef create_shallow_model():\n    model = tf.keras.Sequential([\n        tfa.layers.WeightNormalization(L.Dense(train.shape[1], input_shape=(train.shape[1],))),\n        L.BatchNormalization(),\n        L.Dropout(0.2),\n        tfa.layers.WeightNormalization(L.Dense(128, activation=\"relu\")),\n        L.BatchNormalization(),\n        L.Dropout(0.2),\n        tfa.layers.WeightNormalization(L.Dense(train_target.shape[1], activation=\"sigmoid\"))\n    ])\n    \n    sgd = tf.keras.optimizers.SGD()\n    adamw = tfa.optimizers.AdamW(weight_decay = 0.0001)\n    adam = tf.keras.optimizers.Adam()\n    radam = tfa.optimizers.RectifiedAdam()\n    lookahead_radam = tfa.optimizers.Lookahead(radam)\n    lookahead_adamw = tfa.optimizers.Lookahead(adamw)\n    \n    if use_custom_loss:\n        model.compile(loss=conf_penalty_loss, optimizer=adam)\n    else:\n        model.compile(loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-15), optimizer=adam)\n    return model\n\ndef create_mid_model():\n    model = tf.keras.Sequential([\n        tfa.layers.WeightNormalization(L.Dense(train.shape[1], input_shape=(train.shape[1],))),\n        L.BatchNormalization(),\n        tfa.layers.WeightNormalization(L.Dense(512, activation=\"relu\")),\n        L.BatchNormalization(),\n        L.Dropout(0.3),\n        tfa.layers.WeightNormalization(L.Dense(512, activation=\"relu\")),\n        L.BatchNormalization(),\n        L.Dropout(0.3),\n        tfa.layers.WeightNormalization(L.Dense(train_target.shape[1], activation=\"sigmoid\"))\n    ])\n    \n    sgd = tf.keras.optimizers.SGD()\n    adamw = tfa.optimizers.AdamW(weight_decay = 0.0001)\n    adam = tf.keras.optimizers.Adam()\n    radam = tfa.optimizers.RectifiedAdam()\n    lookahead_radam = tfa.optimizers.Lookahead(radam)\n    lookahead_adamw = tfa.optimizers.Lookahead(adamw)\n    if use_custom_loss:\n        model.compile(loss=conf_penalty_loss, optimizer=lookahead_adamw)\n    else:\n        model.compile(loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-15), optimizer=lookahead_adamw)\n    return model\n\ndef create_deep_model():\n    model = tf.keras.Sequential([\n        tfa.layers.WeightNormalization(L.Dense(train.shape[1], input_shape=(train.shape[1],))),\n        L.BatchNormalization(),\n        tfa.layers.WeightNormalization(L.Dense(512, activation=\"selu\")),\n        L.BatchNormalization(),\n        L.Dropout(0.42),\n        tfa.layers.WeightNormalization(L.Dense(512, activation=\"relu\")),\n        L.BatchNormalization(),\n        L.Dropout(0.42),\n        tfa.layers.WeightNormalization(L.Dense(256, activation=\"relu\")),\n        L.BatchNormalization(),\n        L.Dropout(0.42),\n        tfa.layers.WeightNormalization(L.Dense(256, activation=\"elu\")),\n        L.BatchNormalization(),\n        L.Dropout(0.1),\n        tfa.layers.WeightNormalization(L.Dense(train_target.shape[1], activation=\"sigmoid\"))\n    ])\n    \n    sgd = tf.keras.optimizers.SGD()\n    adamw = tfa.optimizers.AdamW(weight_decay = 0.0001)\n    adam = tf.keras.optimizers.Adam()\n    radam = tfa.optimizers.RectifiedAdam()\n    lookahead_radam = tfa.optimizers.Lookahead(radam)\n    lookahead_adamw = tfa.optimizers.Lookahead(adamw)\n    if use_custom_loss:\n        model.compile(loss=conf_penalty_loss, optimizer=lookahead_radam)\n    else:\n        model.compile(loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-15), optimizer=lookahead_radam)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import KFold\n\npredictions = []\nkf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ntf.random.set_seed(42)\nfor fold_id, (train_idx, valid_idx) in enumerate(kf.split(train, train_target)):\n    model1 = create_shallow_model()\n    model2 = create_mid_model()\n    model3 = create_deep_model()\n    \n    history1 = model1.fit(train.iloc[train_idx], train_target.iloc[train_idx], batch_size=32,\n              validation_data=(train.iloc[valid_idx], train_target.iloc[valid_idx]),\n             epochs=100,\n             verbose=2,\n             callbacks=[\n    tf.keras.callbacks.ReduceLROnPlateau(),\n    tf.keras.callbacks.EarlyStopping(patience=7, restore_best_weights=True),\n    tf.keras.callbacks.ModelCheckpoint(\"model1_fold\" + str(fold_id) + \".h5\", save_best_only=True, save_weights_only=True)\n])\n    print(\"Model 1, Fold ID: {}, train loss: {}, valid loss: {}\".format(fold_id, min(history1.history[\"loss\"]), min(history1.history[\"val_loss\"])))\n    history2 = model2.fit(train.iloc[train_idx], train_target.iloc[train_idx], batch_size=32,\n              validation_data=(train.iloc[valid_idx], train_target.iloc[valid_idx]),\n             epochs=100,\n             verbose=2,\n             callbacks=[\n    tf.keras.callbacks.ReduceLROnPlateau(),\n    tf.keras.callbacks.EarlyStopping(patience=7, restore_best_weights=True),\n    tf.keras.callbacks.ModelCheckpoint(\"model2_fold\" + str(fold_id) + \".h5\", save_best_only=True, save_weights_only=True)\n])\n    print(\"Model 2, Fold ID: {}, train loss: {}, valid loss: {}\".format(fold_id, min(history2.history[\"loss\"]), min(history2.history[\"val_loss\"])))\n    history3 = model3.fit(train.iloc[train_idx], train_target.iloc[train_idx], batch_size=32,\n              validation_data=(train.iloc[valid_idx], train_target.iloc[valid_idx]),\n             epochs=100,\n             verbose=2,\n             callbacks=[\n    tf.keras.callbacks.ReduceLROnPlateau(),\n    tf.keras.callbacks.EarlyStopping(patience=7, restore_best_weights=True),\n    tf.keras.callbacks.ModelCheckpoint(\"model3_fold\" + str(fold_id) + \".h5\", save_best_only=True, save_weights_only=True)\n])\n    \n    print(\"Model 3, Fold ID: {}, train loss: {}, valid loss: {}\".format(fold_id, min(history3.history[\"loss\"]), min(history3.history[\"val_loss\"])))\n    \n    model1.load_weights(\"model1_fold\" + str(fold_id) + \".h5\")\n    model2.load_weights(\"model2_fold\" + str(fold_id) + \".h5\")\n    model3.load_weights(\"model3_fold\" + str(fold_id) + \".h5\")\n    pred1 = model1.predict(test)\n    pred2 = model2.predict(test)\n    pred3 = model3.predict(test)\n    predictions.append(np.average([pred1, pred2, pred3], weights=[0.15, 0.7, 0.15], axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = np.mean(predictions, axis=0)\n# pred = np.clip(pred, 0.001, 0.999)\npred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")\nsub.loc[:, 1:] = pred\nsub.loc[ctlVehicle_test, sub.columns != \"sig_id\"] = 0\n\n# sub.loc[:, 1:] = tf.keras.utils.normalize(pred)\nsub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}