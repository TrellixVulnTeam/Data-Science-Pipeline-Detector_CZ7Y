{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# TabNet\n!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.1-py3-none-any.whl pytorch-tabnet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# project \nimport sys\nsys.path.append('../input/moa-models')\nsys.path.append('../input/moa-scripts')\nfrom pytorch import *\nseed_everything(seed=42)\n\nfrom moa import *\nfrom moa_pipeline import *\nfrom metrics import logloss\nfrom validation import mlp_oof\n\n# np, pd\nimport numpy as np\nimport pandas as pd \nfrom scipy import stats \n\n# misc\nimport warnings\nfrom tqdm.auto import tqdm\nimport os\nimport random\nimport warnings \nimport copy\nimport joblib\nimport gc \nfrom functools import partial\nimport glob \n\n# sklearn\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\n\n# torch \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# viz \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\nwarnings.simplefilter('ignore')\n\n# Tabnet \nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n!ls ../input/moa-pipelines","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## load raw data\nX, y, genes, cells, classnames, features, X_test, test_control, submission = load()\n\n## load splits\nsplits = sorted(glob.glob(\"../input/moa-pipelines/x*.pkl\"))\nfolds, pipes = list(zip(*[joblib.load(split) for split in splits]))\nprint(splits)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tabnet_oof(splits, seeds, pretrained_pipes, tabnet_params, train_params, stop=-1):\n    S = len(seeds)\n    oof_train = np.zeros((len(X), y.shape[1]), dtype=np.float64)\n    oof_test  = np.zeros((len(X_test[~test_control]), y.shape[1]), dtype=np.float64)\n    for split, (folds, seed) in enumerate(zip(splits, seeds)):\n        # loop over kf\n        nfolds = len(set(folds))\n        folds_oof = np.zeros((len(X), y.shape[1]), dtype=np.float64)\n        for fold, (t, v) in enumerate(index2folds(folds)):\n            # loop over folds\n            pipe = pretrained_pipes[split][fold]\n            # apply pipe\n            xtrain, xvalid, xtest = X[t].copy(), X[v].copy(), X_test[~test_control].copy()\n            xtrain = pipe.transform(xtrain)\n            xvalid = pipe.transform(xvalid)\n            xtest  = pipe.transform(xtest)\n            model = TabNetRegressor(seed=seed, **tabnet_params)\n            model.fit(X_train = xtrain, y_train = y[t].copy(), eval_set = [(xvalid, y[v].copy())], **train_params)\n            fold_preds = sigmoid(model.predict(xvalid))\n            test_preds = sigmoid(model.predict(xtest))\n            folds_oof[v] += fold_preds\n            oof_test += (test_preds / nfolds)\n            # model.save_model(f\"split{split}_seed{seed}_fold{fold}\")\n            torch.save(model.network.state_dict(), f\"split{split}_seed{seed}_fold{fold}.pth\")\n            if fold == stop: break\n        oof_train += folds_oof\n    oof_train /= S\n    oof_test  /= S\n    # joblib.dump(oof_train, \"oof_train.pkl\")\n    return oof_train, oof_test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LogitsLogLoss(Metric):\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n    def __call__(self, y_true, y_pred):\n        return logloss(y_true, sigmoid(y_pred))\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\nMAX_EPOCH  = 200\nMASK_TYPE = \"entmax\"\nN = 36\nN_STEPS = 1\nBATCH_SIZE = 1024\nBATCH_SIZE_V = 32\nLR = 2e-2\nLAMBDA_SPARSE = 0\nN_IND = 2\nN_SHARED = 2\nMOMENTUM = 0.02\nPL_FACTOR = 0.9\nrdpt_params = dict(mode = \"min\", patience = 5, min_lr = 1e-5, factor = PL_FACTOR)\n\ntabnet_params = dict(\n    n_d = N,\n    n_a = N,\n    n_steps = N_STEPS,\n    lambda_sparse = LAMBDA_SPARSE,\n    mask_type = MASK_TYPE,\n    n_independent = N_IND,\n    n_shared = N_SHARED,\n    momentum = MOMENTUM,\n    optimizer_fn = optim.Adam,\n    optimizer_params = dict(lr=LR, weight_decay=1e-5),\n    gamma = 1.3,\n    scheduler_params = rdpt_params,\n    scheduler_fn = optim.lr_scheduler.ReduceLROnPlateau,\n    verbose = 10\n)\n\ntrain_params = dict(\n    eval_name = [\"val\"],\n    eval_metric = [\"logits_ll\"],\n    max_epochs = MAX_EPOCH,\n    batch_size = BATCH_SIZE, \n    virtual_batch_size = BATCH_SIZE_V,\n    patience = 20,\n    num_workers = 1,\n    drop_last = False,\n    loss_fn = F.binary_cross_entropy_with_logits\n    #loss_fn = LabelSmoothingCrossEntropy(1e-6, True, 5e-5)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [ # folds/seeds/pipes\n    folds,\n    list(range(len(folds))),\n    pipes\n]\noof_train, oof_test = tabnet_oof(*data, tabnet_params, train_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(oof_train.mean(), oof_test.mean(), y.mean())\noof_result(y, oof_train, oof_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = join_control_group(oof_test, test_control, submission, classnames)\nsubmit_preds(prediction, submission, test_control, classnames)\npd.read_csv(\"submission.csv\").iloc[:5, :5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}