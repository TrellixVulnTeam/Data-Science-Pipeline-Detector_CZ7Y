{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append(\"../input/keras-one-cycle-lr/\")\nsys.path.append(\"../input/keras-one-cycle-lr/one_cycle_lr\")\n\n!pip install ../input/keras-one-cycle-lr","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"sys.path.append('../input/moa-scripts')\nfrom moa import load_datasets, preprocess, split, submit, submit_preds\nfrom metrics import logloss\nfrom oof import OOFTrainer\nfrom ml_stratifiers import MultilabelStratifiedKFold\n\n# np, pd\nimport numpy as np \nimport pandas as pd \n\n# misc\nimport warnings\nwarnings.simplefilter('ignore')\nfrom glob import glob\nfrom tqdm.auto import tqdm\nimport os\nimport random\nimport copy\nimport joblib\nimport gc \nfrom functools import partial\n\n# viz\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# tf\nimport tensorflow as tf\n\n# ml\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\n\n# seed before keras import\ndef seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = '0'\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything()\n\n# import keras\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom one_cycle_lr import one_cycle_scheduler\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.config.optimizer.set_jit(True)\n\n# from tensorflow.keras.mixed_precision import experimental as mixed_precision\n# policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n# mixed_precision.set_policy(policy)\n# # if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train, target, y_nonscored, test, submission = load_datasets(\"../input/lish-moa\")\nX, y, test, test_control = preprocess(train, target, test, standard=False, onehot=True)\nX, y, X_holdout, y_holdout, split_index, index, classnames, features = split(X, y, drop_rare_labels=False)\ngenes = [i for i,col in enumerate(features) if col.startswith('g-')]\ncells = [i for i,col in enumerate(features) if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PCA\nn = len(X)\nc1, c2 = 64, 16\npca1,pca2 = PCA(c1), PCA(c2)\npca1_res  = pca1.fit_transform(np.concatenate([X, test]))\npca2_res  = pca2.fit_transform(np.concatenate([X, test]))\nX = np.concatenate([X, pca1_res[:n], pca2_res[:n]], axis=1)\ntest = np.concatenate([test, pca1_res[n:], pca2_res[n:]], axis=1)\nfeatures = list(features)\nfeatures += [f'pca_genes_{i}' for i in range(c1)] + [f'pca_cells_{i}' for i in range(c2)]\nassert len(features) == X.shape[1] == test.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# variance feature selection\nfrom sklearn.feature_selection import VarianceThreshold\nvar = VarianceThreshold(threshold=0.5)\nvar.fit(np.concatenate([X, test])[:, 4:])\nmask = np.concatenate([[True]*4, var.get_support()])\nX = X[:, mask]\ntest = test[:, mask]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef weighted_logloss_keras(w=0.5):\n    alpha = K.variable(w)\n    def loss(y_true, y_pred):\n        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n        return - 2 * K.mean( ( alpha * y_true * K.log(y_pred) + (1 - alpha) * (1 - y_true) * (K.log(1 - y_pred))) )\n    return loss\n\ndef logloss_keras(y_true, y_pred):\n    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n    return - K.mean( y_true * K.log(y_pred) + (1 - y_true) * (K.log(1 - y_pred)) )\n\nmax_norm = tf.keras.constraints.max_norm\nhe_uniform = lambda scale, seed: tf.keras.initializers.VarianceScaling(mode='fan_in', distribution='uniform', scale=scale, seed=seed)\nconstant = lambda x: tf.keras.initializers.Constant(value=x)\nl2_reg = lambda x: tf.keras.regularizers.l2(x)\n\nclass DenseBlock(L.Layer):\n    def __init__(self, \n                 dropout_rate=0.5, \n                 dense=(1024, 6.0, 1e-5, None), \n                 prelu=(0.25, 0.0, None), \n                 clipnorm=None,\n                 seed=42):\n        super(DenseBlock, self).__init__()\n        \n        self.batch_norm = L.BatchNormalization()\n        self.dropout = L.Dropout(dropout_rate)\n        self.dense = L.Dense(\n            dense[0], activation=None, \n            kernel_initializer=he_uniform(dense[1], seed), \n            kernel_regularizer=l2_reg(dense[2]), \n            kernel_constraint =max_norm(dense[3]) if dense[3] else None\n            )\n        # self.activation = L.PReLU(\n        #     alpha_initializer=constant(prelu[0]),\n        #     alpha_regularizer=l2_reg(prelu[1]), \n        #     alpha_constraint =max_norm(prelu[2]) if prelu[2] else None\n        #     )\n        self.activation = L.ReLU()\n        self.clipnorm = tf.keras.constraints.MaxNorm(clipnorm) if clipnorm else None\n        \n    def call(self, inputs):\n        x = self.batch_norm(inputs)\n        x = self.dropout(x)\n        x = self.dense(x)\n        x = self.activation(x)\n        if self.clipnorm:\n            x = self.clipnorm(x)\n        return x\n\ndef create_model(num_columns, \n                 \n                 dropout=(0.25, 0.5), \n                 hidden=(1024, 1024), \n                 he_scale=6.0, \n                 kernel=(1e-5, None),\n                 activation=(0.25, 0.0, None),\n\n                 top_layer_bias=1.0,\n                 clipnorm=None,\n\n                 learning_rate=1e-3, \n                 weight_decay=1e-5, \n                 lookahead_sync=7,\n                 label_smoothing=1e-6,\n\n                 l2top=1,\n\n                 seed=42, \n                 metrics=[]):\n    \n    # input \n    model = tf.keras.Sequential()\n    model.add(L.Input(num_columns))\n    \n    # dense blocks\n    for i in range(len(hidden)):\n        d = dropout[0] if i == 0 else dropout[1]\n        model.add(DenseBlock\n                  (dropout_rate=d, \n                   dense=(hidden[i], he_scale, kernel[0], kernel[1]), \n                   prelu=activation,\n                   clipnorm=clipnorm,\n                   seed=seed\n                   )\n                  )\n        \n    # top layer\n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(dropout[1]))\n    model.add(tfa.layers.WeightNormalization(L.Dense(\n        206, activation=\"sigmoid\", \n        kernel_initializer=he_uniform(he_scale, seed),\n        bias_initializer=constant(top_layer_bias),\n        kernel_regularizer=l2_reg(kernel[0]) if l2top else None\n        )))\n    \n    # compile\n    model.compile(\n        optimizer=tfa.optimizers.Lookahead(tfa.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay), sync_period=lookahead_sync), \n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing), \n        metrics=metrics\n        )\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel_params = dict(\n    dropout         =(0.05, 0.65), \n    hidden          =(1024, 1024),\n    kernel          =(3e-6, None),\n    activation      =(0.05, 6e-6, 4),\n    he_scale        =6,\n    clipnorm        =None,\n    label_smoothing =0,\n    weight_decay    =1e-5,\n    l2top           =1,\n    top_layer_bias  =1.0,\n    learning_rate   =5e-4, \n    lookahead_sync  =7,\n    seed=42\n    # metrics=[logloss_keras]\n)\nscheduler_params = dict(\n    max_lr=1e-2, \n    pct_start=0.2, \n    start_div=1e4, \n    end_div=1e5\n)\nfit_params = dict(\n    epochs=25, \n    batch_size=96, \n    verbose=0\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\ndef crossval(X, y, X_test, \n             model_params, scheduler_params, fit_params, \n             splits=[(MultilabelStratifiedKFold, 5)], \n             seeds=[42], \n             fold_mask=None,\n             verbose=1, \n             pseudo=-1\n            ):\n    \n    oof = np.zeros((len(X_test), y.shape[1]), dtype=np.float64)\n    preds = np.zeros_like(y, dtype=np.float64)\n    history = []\n    for seed in seeds:\n        seed_everything(seed)\n        model_params['seed'] = seed\n        \n        # split cv\n        for kf, n_splits in splits:\n            cv = kf(n_splits=n_splits, shuffle=True, random_state=seed)\n            for fold, (train_index, valid_index) in enumerate(cv.split(X, y)):\n                if fold_mask and fold not in fold_mask:\n                    continue\n                iter_id = f'{str(kf).split(\".\")[-1][:-2]}-seed{seed}-fold{fold+1}'\n                if verbose: print(iter_id, end=': ')\n                X_train, y_train, X_valid, y_valid = X[train_index].copy(), y[train_index].copy(), X[valid_index].copy(), y[valid_index].copy()\n\n                # fit model\n                model = create_model(X.shape[1], **model_params)\n                checkpoint_path = f'{iter_id}.h5'\n                cb_checkpt = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n                ocp = one_cycle_scheduler.OneCycleScheduler(verbose=0, **scheduler_params)\n                model.fit(X_train, y_train, \n                          validation_data=(X_valid, y_valid),\n                          callbacks=[cb_checkpt, ocp], \n                          **fit_params)\n                model.load_weights(checkpoint_path)\n\n                # pseudo labels\n                if pseudo > 0:\n                    if verbose: print(f'{logloss_keras(y_valid, model.predict(X_valid)):.5f}', end=' ')\n                    pseudo_preds = model.predict(X_test)[~test_control]\n                    fit_params_p = fit_params.copy(); fit_params_p['epochs'] = pseudo\n                    model.fit(X_test[~test_control], pseudo_preds, \n                              validation_data=(X_valid, y_valid),\n                              callbacks=[cb_checkpt], **fit_params_p)\n                    model.load_weights(checkpoint_path)\n                \n                # predict\n                fold_preds = model.predict(X_valid)\n                preds[valid_index] += fold_preds\n                oof += model.predict(X_test)\n                val_loss = logloss_keras(y_valid, fold_preds)\n                history.append(val_loss)\n                if verbose: print(f'{val_loss:.5f}')\n                \n    tries = len(seeds) * len(splits)\n    folds = len(seeds) * sum([n for _, n in splits])\n    preds /= tries\n    oof /= folds\n    return preds, oof, np.array(history)\n\n# ------------------------------------------------------------------------------------------\npseudo_epochs = 5\nsplits = [(MultilabelStratifiedKFold, 6)]\nseeds = range(5)\n# seeds=[0]\nfold_mask = None\npreds, oof, history = crossval(X, y, test, model_params, scheduler_params, fit_params=fit_params, splits=splits, seeds=seeds, fold_mask=fold_mask, pseudo=pseudo_epochs)\nprint(history.mean(), history.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joblib.dump(preds, 'train.pkl')\njoblib.dump(oof, 'oof.pkl')\nsubmit_preds(oof, submission, test_control, classnames)\npd.read_csv('submission.csv').iloc[:5, :5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}