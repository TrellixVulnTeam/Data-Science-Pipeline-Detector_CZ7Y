{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction and Credits","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Hello Kagglers. It's been a while since I have been active here on Kaggle. \n\nThis notebook is my TabNet based submission to MoA competition.\nThe code is a MoA kinda version of [Tanul](https://www.kaggle.com/tanulsingh077)'s TabNet Notebook [here](https://www.kaggle.com/tanulsingh077/achieving-sota-results-with-tabnet).\n\nIt scores **0.02252** on the current public leaderboard\n\nThe data preprocessing and loading code is borrowed from[ Rob](https://www.kaggle.com/robikscube)'s notebook [here](https://www.kaggle.com/robikscube/mechanisms-of-action-moa-prediction-starter)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### *Drop an upvote if this looks good. I believe there are lots of tuning that can be done and will update you on the same :)*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/pytorch-tabnet/pytorch_tabnet-1.2.0-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom itertools import cycle\nimport torch\nfrom pytorch_tabnet.tab_model import TabNetClassifier,TabNetRegressor\npd.set_option('max_columns', 50)\nplt.style.use('seaborn-dark')\ncolor_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we can look at the data format. Everything is stored as a CSV, and the largest file is only 150MB.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -GFlash --color ../input/lish-moa/","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"ss = pd.read_csv('../input/lish-moa/sample_submission.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\ntrain_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features\nThe start of the column:\n- `g-` signify gene expression data\n- `c-` signify cell viability data.\n- `cp_type` indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs; \n- `cp_time` and `cp_dose` indicate treatment duration (24, 48, 72 hours) and dose (high or low).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"GENE_COLS = [c for c in train_features.columns if c[:2] == 'g-']\nCELL_COLS = [c for c in train_features.columns if c[:2] == 'c-']\nprint('Number of gene columns:', len(GENE_COLS))\nprint('Number of cell columns:', len(CELL_COLS))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation Criteria","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For every `sig_id` you will be predicting the probability that the sample had a positive response for each <MoA> target. For N sig_id rows and M <MoA> targets, you will be making N×M predictions. Submissions are scored by the log loss:\n\n\n$$ \\text{score} = - \\frac{1}{M}\\sum_{m=1}^{M} \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_{i,m} \\log(\\hat{y}_{i,m}) + (1 - y_{i,m}) \\log(1 - \\hat{y}_{i,m})\\right] $$\n\n- \\(N\\) is the number of sig_id observations in the test data (\\(i=1,…,N\\))\n- \\(M\\) is the number of scored MoA targets (\\(m=1,…,M\\))\n- \\( \\hat{y}_{i,m} \\) is the predicted probability of a positive MoA response for a sig_id\n- \\( y_{i,m} \\) is the ground truth, 1 for a positive response, 0 otherwise\n- \\( log() \\) is the natural (base e) logarithm\n    \nNote: the actual submitted predicted probabilities are replaced with max(min(p,1−10−15),10−15). A smaller log loss is better.\n\n\n    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\ndef kaggle_metric_np(targets, preds):\n    \"\"\"\n    Kaggle metric for MoA competition targets and preds\n    in numpy format.\n    \"\"\"\n    assert targets.shape[1] == 206\n    assert preds.shape[1] == 206\n    metrics = []\n    for t in range(206):\n        metrics.append(log_loss(targets[:, t], preds[:, t], labels=[0, 1]))\n    return np.mean(metrics)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multiclass Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n# from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.svm import LinearSVC \nfrom sklearn.metrics import log_loss\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nLABEL_ENCODE_COLS = ['cp_type','cp_time','cp_dose']\nfor l in LABEL_ENCODE_COLS:\n    le = LabelEncoder()\n    train_features[f'{l}_le'] = le.fit_transform(train_features[l])\n    test_features[f'{l}_le'] = le.transform(test_features[l])\n\nFEATURES = GENE_COLS + CELL_COLS + ['cp_type_le','cp_time_le','cp_dose_le']\nTARGETS = [t for t in train_targets_scored.columns if t != 'sig_id']\n\ndf = train_features[FEATURES]\ntest_df = test_features[FEATURES]\ny = train_targets_scored[TARGETS]\n\n\n\n# X = train_features[FEATURES].values\n# X_test = test_features[FEATURES].values\n# y = train_targets_scored[TARGETS].values\n\n# Needed\n# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)\n\n\n\n# Not needed\n# X_full = np.concatenate([X, X_test])\n\n# Standard Scale\n# scale = StandardScaler()\n# scale.fit(X_full)\n# X_train = scale.transform(X_train)\n# X_val = scale.transform(X_val)\n# X_test = scale.transform(X_test)\n\n# # Apply PCA\n# pca = PCA(n_components=100, svd_solver='full')\n# pca.fit(X_full)\n# X_train = pca.transform(X_train)\n# X_val = pca.transform(X_val)\n# X_test = pca.transform(X_test)\n# print(X_train.shape, X_val.shape, X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nNUM_FOLDS=5\n\ndf = df.dropna().reset_index(drop=True)\ndf[\"kfold\"] = -1\ny = y.dropna().reset_index(drop=True)\ny[\"kfold\"] = -1\n\ndf = df.sample(frac=1,random_state=2020).reset_index(drop=True)\ny = y.sample(frac=1,random_state=2020).reset_index(drop=True)\n\nkf = KFold(n_splits=NUM_FOLDS)\n\nfor fold, (trn_, val_) in enumerate(kf.split(X=df, y=y)):\n    df.loc[val_, 'kfold'] = fold\n    y.loc[val_,'kfold'] = fold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.loc[val_,'kfold']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = np.zeros((test_df.shape[0],len(TARGETS), NUM_FOLDS))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=FEATURES\ntarget_features = TARGETS\ndef run(fold):\n    df_train = df[df.kfold != fold]\n    df_valid = df[df.kfold == fold]\n    \n    X_train = df_train[features].values\n    Y_train = y[y.kfold!=fold][TARGETS].values\n#     Y_train = df_train[target_features].values\n    \n    X_valid = df_valid[features].values\n    Y_valid = y[y.kfold==fold][TARGETS].values\n#     Y_valid = df_valid[target_features].values\n    \n    y_oof = np.zeros((df_valid.shape[0],len(target_features)))   # Out of folds validation\n    \n    print(\"--------Training Begining for fold {}-------------\".format(fold+1))\n     \n    model.fit(X_train = X_train,\n             y_train = Y_train,\n             X_valid = X_valid,\n             y_valid = Y_valid,\n             max_epochs = 1000,\n             patience =70)\n              \n    \n    print(\"--------Validating For fold {}------------\".format(fold+1))\n    \n    y_oof = model.predict(X_valid)\n    y_test[:,:,fold] = model.predict(test_df.values)\n    \n    val_score = kaggle_metric_np(Y_valid,y_oof)\n    \n    print(\"Validation score: {:<8.5f}\".format(val_score))\n    \n    # VISUALIZTION\n    plt.figure(figsize=(12,6))\n    plt.plot(model.history['train']['loss'])\n    plt.plot(model.history['valid']['loss'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(\"ignore\")\n\n# clf = OneVsRestClassifier(SVC(probability=True))\nmodel = TabNetRegressor(n_d=64,\n                       n_a=64,\n                       n_steps=8,\n                       gamma=1.9,\n                       n_independent=4,\n                       n_shared=5,\n                       seed=2020,\n                       optimizer_fn = torch.optim.Adam,\n                       scheduler_params = {\"milestones\": [150,250,300,350,400,450],'gamma':0.2},\n                       scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)\n# clf.fit(X_train = X_train,\n#              y_train = y_train,\n#              X_valid = X_val,\n#              y_valid = y_val,\n#              max_epochs = 1000,\n#              patience =70)\n# clf.fit(X_train,y_train)\n# pred_train = clf.predict_proba(X_train)\n# pred_val = clf.predict_proba(X_val)\n# pred_test = clf.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(fold=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(fold=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(fold=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(fold=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(fold=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run(fold=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run(fold=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = y_test.mean(axis=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame(y_test, columns=TARGETS)\nsub['sig_id'] = test_features['sig_id'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.shape, ss.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}