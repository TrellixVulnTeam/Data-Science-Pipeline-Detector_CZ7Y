{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-10-27T20:46:39.645304Z","iopub.status.busy":"2020-10-27T20:46:39.644429Z","iopub.status.idle":"2020-10-27T20:46:50.752872Z","shell.execute_reply":"2020-10-27T20:46:50.752011Z"},"papermill":{"duration":11.128924,"end_time":"2020-10-27T20:46:50.753045","exception":false,"start_time":"2020-10-27T20:46:39.624121","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom functools import partial\nfrom collections import defaultdict\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader, random_split, Subset\n\nimport catalyst\nfrom catalyst import dl\n\nimport plotly.express as px\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import VarianceThreshold\n\nimport os\nimport sys\n\nimport plotly.express as px\n\nif os.path.exists(\"/kaggle\"):\n    DATA_DIRECTORY = f\"/kaggle/input/lish-moa\"\n    \n    sys.path.append(\"/kaggle/input/iterative-stratification/iterative-stratification-master/\")\n\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\nelse:\n    DATA_DIRECTORY = f\"data\"\n    \nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nRANDOM_SEED = 42\n\ndef file_path(filename):\n    global DATA_DIRECTORY\n    return os.path.join(DATA_DIRECTORY, filename)\n\ntorch.manual_seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-10-27T20:46:50.784706Z","iopub.status.busy":"2020-10-27T20:46:50.783258Z","iopub.status.idle":"2020-10-27T20:46:56.65402Z","shell.execute_reply":"2020-10-27T20:46:56.653402Z"},"papermill":{"duration":5.887515,"end_time":"2020-10-27T20:46:56.654148","exception":false,"start_time":"2020-10-27T20:46:50.766633","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train = pd.read_csv(file_path(\"train_features.csv\")).sort_values(by='sig_id')\ntargets = pd.read_csv(file_path(\"train_targets_scored.csv\")).sort_values(by='sig_id')\ndf_target_ns = pd.read_csv(file_path(\"train_targets_nonscored.csv\")).sort_values(by='sig_id')\ntest = pd.read_csv(file_path(\"test_features.csv\"))\nsubmission = test[['sig_id']].assign(**targets.iloc[:, 1:].mean())","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T20:46:56.682616Z","iopub.status.busy":"2020-10-27T20:46:56.681524Z","iopub.status.idle":"2020-10-27T20:46:56.729103Z","shell.execute_reply":"2020-10-27T20:46:56.72854Z"},"papermill":{"duration":0.064234,"end_time":"2020-10-27T20:46:56.729217","exception":false,"start_time":"2020-10-27T20:46:56.664983","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"mask = test['cp_type'] != 'ctl_vehicle'\nsubmission.iloc[~mask, 1:] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = train\ndf_test = test\ndf_target_s = targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer = QuantileTransformer(n_quantiles=100,random_state=42, output_distribution=\"normal\")\n\ndef preprocess(df):\n    df['cp_time'] = df['cp_time'].map({24:1, 48:2, 72:3})\n    df['cp_dose'] = df['cp_dose'].map({'D1':0, 'D2':1})\n    g_features = [cols for cols in df.columns if cols.startswith('g-')]\n    c_features = [cols for cols in df.columns if cols.startswith('c-')]\n    for col in (g_features + c_features):\n        vec_len = len(df[col].values)\n        raw_vec = df[col].values.reshape(vec_len, 1)\n        transformer.fit(raw_vec)\n        df[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    return df\n\n\nX = preprocess(df_train)\nX_test = preprocess(df_test)\n\n\ny = df_target_s.drop('sig_id', axis=1)\ny0 =  df_target_ns.drop('sig_id', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Please see reference 3 for this part\ng_features = [cols for cols in X.columns if cols.startswith('g-')]\nn_comp = 0.95\n\ndata = pd.concat([pd.DataFrame(X[g_features]), pd.DataFrame(X_test[g_features])])\ndata2 = (PCA(0.95, random_state=42).fit_transform(data[g_features]))\ntrain2 = data2[:X.shape[0]]\ntest2 = data2[-X_test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_g-{i}' for i in range(data2.shape[1])])\ntest2 = pd.DataFrame(test2, columns=[f'pca_g-{i}' for i in range(data2.shape[1])])\n\nX = pd.concat((X, train2), axis=1)\nX_test = pd.concat((X_test, test2), axis=1)\n\nc_features = [cols for cols in X.columns if cols.startswith('c-')]\nn_comp = 0.95\n\ndata = pd.concat([pd.DataFrame(X[c_features]), pd.DataFrame(X_test[c_features])])\ndata2 = (PCA(0.95, random_state=42).fit_transform(data[c_features]))\ntrain2 = data2[:X.shape[0]]\ntest2 = data2[-X_test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_c-{i}' for i in range(data2.shape[1])])\ntest2 = pd.DataFrame(test2, columns=[f'pca_c-{i}' for i in range(data2.shape[1])])\n\nX = pd.concat((X, train2), axis=1)\nX_test = pd.concat((X_test, test2), axis=1)\nfrom sklearn.feature_selection import VarianceThreshold\n\nvar_thresh = VarianceThreshold(0.8)  \ndata = X.append(X_test)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : X.shape[0]]\ntest_features_transformed = data_transformed[-X_test.shape[0] : ]\n\n\nX = pd.DataFrame(X[['sig_id','cp_type', 'cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\nX = pd.concat([X, pd.DataFrame(train_features_transformed)], axis=1)\n\n\nX_test = pd.DataFrame(X_test[['sig_id','cp_type', 'cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\nX_test = pd.concat([X_test, pd.DataFrame(test_features_transformed)], axis=1)\n\ndisplay(X.head(2))\nprint(X.shape)\ndisplay(X_test.head(2))\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\ndef fe_cluster(train, test, n_clusters_g = 35, n_clusters_c = 5, SEED = 239):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\nX ,X_test=fe_cluster(X,X_test)\ndisplay(X.head(2))\nprint(X.shape)\ndisplay(X_test.head(2))\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe_stats(train, test):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    for df in train, test:\n        df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n    return train, test\n\nX,X_test=fe_stats(X,X_test)\ndisplay(X.head(2))\nprint(X.shape)\ndisplay(X_test.head(2))\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y0 = y0[X['cp_type'] == 'trt_cp'].reset_index(drop = True)\ny = y[X['cp_type'] == 'trt_cp'].reset_index(drop = True)\nX = X[X['cp_type'] == 'trt_cp'].reset_index(drop = True)\nX.drop(['cp_type','sig_id'], axis=1, inplace=True)\nX_test = X_test[X_test['cp_type'] == 'trt_cp'].reset_index(drop = True)\nX_test.drop(['cp_type','sig_id'], axis=1, inplace=True)\n\nprint('New data shape', X.shape)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T20:46:56.762677Z","iopub.status.busy":"2020-10-27T20:46:56.757418Z","iopub.status.idle":"2020-10-27T20:46:57.169876Z","shell.execute_reply":"2020-10-27T20:46:57.169251Z"},"papermill":{"duration":0.430336,"end_time":"2020-10-27T20:46:57.170021","exception":false,"start_time":"2020-10-27T20:46:56.739685","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def basic_preprocess(X, y=None, delete_veh=True, pca=None):\n    if delete_veh:\n        mask = np.where(X['cp_type'] == 'ctl_vehicle')[0]\n    else:\n        mask = []\n    X.drop(mask, inplace=True)\n    X.reset_index(drop=True, inplace=True)\n    if y is not None:\n        y.drop(mask, inplace=True)\n        y.reset_index(drop=True, inplace=True)\n        y.drop(columns='sig_id', inplace=True)\n    X.drop(columns=['cp_type', 'sig_id'], inplace=True)\n    X['cp_dose'] = ((X['cp_dose'] == 'D2').astype(np.int) - 0.5) * 2\n    X['cp_time1'] = ((X['cp_time'] == 24).astype(np.int) - 0.5) * 2\n    X['cp_time2'] = ((X['cp_time'] == 48).astype(np.int) - 0.5) * 2\n    X.drop(columns='cp_time', inplace=True)\n    \n    if y is not None:\n        return X, y\n    return X\n\ndef preprocess(train, targets, test, pca=None, scaler=None, label_smoothing=0, threshold=0.2):\n    train, targets = basic_preprocess(train, targets)\n    test = basic_preprocess(test)\n    \n    if pca:\n        pca = PCA()\n        mask = train.columns.str.startswith('g-')\n        train.loc[:, mask] = pca.fit_transform(train.loc[:, mask])\n        test.loc[:, mask] = pca.transform(test.loc[:, mask])\n        \n        variance_thresholder = VarianceThreshold(threshold)\n        train_features = variance_thresholder.fit_transform(train.loc[:, mask])\n        train.drop(columns=train.columns[mask], inplace=True)\n        train = train.merge(pd.DataFrame(train_features, columns=[f'gp-{i}' for i in range(train_features.shape[1])]), left_index=True, right_index=True)\n        test_features = variance_thresholder.transform(test.loc[:, mask])\n        test.drop(columns=test.columns[mask], inplace=True)\n        test = test.merge(pd.DataFrame(test_features, columns=[f'gp-{i}' for i in range(test_features.shape[1])]), left_index=True, right_index=True)\n\n        mask = train.columns.str.startswith('c-')\n        train.loc[:, mask] = pca.fit_transform(train.loc[:, mask])\n        test.loc[:, mask] = pca.transform(test.loc[:, mask])\n        \n        variance_thresholder = VarianceThreshold(threshold)\n        train_features = variance_thresholder.fit_transform(train.loc[:, mask])\n        train.drop(columns=train.columns[mask], inplace=True)\n        train = train.merge(pd.DataFrame(train_features, columns=[f'cp-{i}' for i in range(train_features.shape[1])]), left_index=True, right_index=True)\n        test_features = variance_thresholder.transform(test.loc[:, mask])\n        test.drop(columns=test.columns[mask], inplace=True)\n        test = test.merge(pd.DataFrame(test_features, columns=[f'cp-{i}' for i in range(test_features.shape[1])]), left_index=True, right_index=True)\n        \n    if scaler:\n        mask = train.columns.str.startswith('g-')\n        train.loc[:, mask] = scaler.fit_transform(train.loc[:, mask])\n        test.loc[:, mask] = scaler.transform(test.loc[:, mask])\n\n        mask = train.columns.str.startswith('c-')\n        train.loc[:, mask] = scaler.fit_transform(train.loc[:, mask])\n        test.loc[:, mask] = scaler.transform(test.loc[:, mask])\n    \n    if label_smoothing:\n        targets.clip(label_smoothing, 1 - label_smoothing, inplace=True)\n    \n    return train, targets, test\n\nlabel_smoothing = 3e-4\n# train, targets, test = preprocess(train, targets, test, pca=False, label_smoothing=0)\n# print(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T20:46:57.229072Z","iopub.status.busy":"2020-10-27T20:46:57.227986Z","iopub.status.idle":"2020-10-27T20:46:57.339014Z","shell.execute_reply":"2020-10-27T20:46:57.340809Z"},"papermill":{"duration":0.145208,"end_time":"2020-10-27T20:46:57.341019","exception":false,"start_time":"2020-10-27T20:46:57.195811","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"X = torch.tensor(X.values.astype(np.float), dtype=torch.float)\ny = torch.tensor(y.values.astype(np.float), dtype=torch.float)\nX_t = torch.tensor(X_test.values.astype(np.float), dtype=torch.float)\n\ndataset = TensorDataset(X, y)\ntest_dataset = TensorDataset(X_t, torch.zeros(X_t.shape[0], y.shape[1], dtype=y.dtype))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T20:46:57.39461Z","iopub.status.busy":"2020-10-27T20:46:57.393774Z","iopub.status.idle":"2020-10-27T20:46:57.398845Z","shell.execute_reply":"2020-10-27T20:46:57.399706Z"},"papermill":{"duration":0.039812,"end_time":"2020-10-27T20:46:57.399878","exception":false,"start_time":"2020-10-27T20:46:57.360066","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class CrafterModel(nn.Module):\n    def __init__(self, input_size, output_size, label_clip=label_smoothing):\n        super().__init__()\n        self.model = nn.Sequential()\n        if label_clip:\n            self.clip_min = torch.log(torch.tensor(label_clip)) - torch.log(torch.tensor(1 - label_clip))\n        else:\n            self.clip_min = -float(\"inf\")\n        \n        hidden_sizes = [1536, 2048, output_size]\n        dropouts = [0.2, 0.55, 0.55]\n        assert len(hidden_sizes) == len(dropouts)\n        for i, (hidden_size, dropout) in enumerate(zip(hidden_sizes, dropouts), start=1):\n            self.model.add_module(f\"batch_norm{i}\", nn.BatchNorm1d(input_size))\n            if i != 1:\n                self.model.add_module(f\"dropout{i}\", nn.Dropout(dropout))\n            linear_layer = torch.nn.utils.weight_norm(nn.Linear(input_size, hidden_size))\n            self.model.add_module(f\"linear{i}\", linear_layer)\n            \n            if i != len(hidden_sizes):\n                self.model.add_module(f\"activation{i}\", nn.ELU())\n            \n            input_size = hidden_size\n    \n    def forward(self, input):\n        input = self.model.forward(input)\n        input = torch.clamp(input, self.clip_min, -self.clip_min)\n        return input\n    \nclass CustomRunner(dl.SupervisedRunner):\n    loss = nn.BCEWithLogitsLoss(reduction='mean')\n    min_pred = np.log(1e-15) - np.log(1 - 1e-15)\n    \n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n    \n    def _handle_batch(self, batch):\n        y_pred = self.model(batch['features'])\n        y_pred = torch.clamp(y_pred, CustomRunner.min_pred, 1 - CustomRunner.min_pred)\n        loss = CustomRunner.loss(y_pred, batch['targets'])\n        self.batch_metrics.update({\"loss\": loss})\n        \nclass EpochMetricSaverCallback(dl.Callback):\n    def __init__(self, metrics=None):\n        super().__init__(dl.CallbackOrder.Logging)\n        self.metrics_log = defaultdict(lambda: [])\n        self.metrics = metrics\n        \n    def on_epoch_end(self, runner):\n        metrics = self.metrics\n        if metrics is None:\n            metrics = runner.epoch_metrics\n        for x in metrics:\n            self.metrics_log[x].append(runner.epoch_metrics[x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(train_dataset, valid_dataset, num_epochs=50):\n    loaders = {\n        \"train\": DataLoader(train_dataset, batch_size=128, shuffle=True),\n        \"valid\": DataLoader(valid_dataset, batch_size=1024, shuffle=False)\n    }\n    \n    input_size = train_dataset[0][0].shape[0]\n    output_size = train_dataset[0][1].shape[0]\n    \n    model = CrafterModel(input_size, output_size, label_clip=0)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, eps=1e-4)\n    logdir = \"./logs\"\n    metric_save = EpochMetricSaverCallback()\n    \n    runner = CustomRunner()\n\n    runner.train(\n        model=model,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        loaders=loaders,\n        num_epochs=num_epochs,\n        logdir=logdir,\n        callbacks=[\n            dl.EarlyStoppingCallback(10),\n            metric_save\n        ],\n        verbose=True,\n        load_best_on_end=True\n    )\n    \n    return model, metric_save.metrics_log\n\ndef predict(model, dataset):\n    runner = dl.SupervisedRunner()\n    results = runner.predict_loader(\n        model=model,\n        loader=DataLoader(dataset, batch_size=1024)\n    )\n\n    total_results = []\n    for x in results:\n        total_results.append(torch.sigmoid(x['logits']).to('cpu'))\n    total_results = torch.cat(total_results)\n    return total_results","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"models = []\npredictions = []\nmetric_logs = []\nvalid_losses = []\n\nseeds = [42]\nn_splits = 5\n\nfor seed in seeds:\n    for train_idx, valid_idx in tqdm(MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed).split(dataset, y), total=n_splits):\n        train_dataset = Subset(dataset, train_idx)\n        valid_dataset = Subset(dataset, valid_idx)\n\n        model, metric_log = train_model(train_dataset, valid_dataset)\n        models.append(model.to('cpu'))\n        metric_logs.append(metric_log)\n        val_loss = min(metric_log['valid_loss'])\n        print(f\"VAL LOSS: {val_loss}\")\n        valid_losses.append(val_loss)\n        predictions.append(predict(model, test_dataset))\n    \npredictions = torch.stack(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(valid_losses).describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"count\t5.000000  \nmean\t0.016427  \nstd\t0.000232  \nmin\t0.016092  \n25%\t0.016293  \n50%\t0.016495  \n75%\t0.016624  \nmax\t0.016632  "},{"metadata":{"trusted":false},"cell_type":"code","source":"total_results = predictions.mean(dim=0)\ntotal_results.clamp_(label_smoothing, 1 - label_smoothing)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":1.280895,"end_time":"2020-10-27T20:47:41.39679","exception":false,"start_time":"2020-10-27T20:47:40.115895","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Best train loss: 0.0144  \nBest valid loss: 0.01672   \nReal loss: 0.01894  \nnum_epochs: 15  "},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T20:47:46.26311Z","iopub.status.busy":"2020-10-27T20:47:46.261843Z","iopub.status.idle":"2020-10-27T20:47:46.700857Z","shell.execute_reply":"2020-10-27T20:47:46.699266Z"},"papermill":{"duration":1.560581,"end_time":"2020-10-27T20:47:46.700992","exception":false,"start_time":"2020-10-27T20:47:45.140411","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"submission.iloc[np.where(mask)[0], 1:] = total_results","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-27T20:47:48.927933Z","iopub.status.busy":"2020-10-27T20:47:48.927075Z","iopub.status.idle":"2020-10-27T20:47:51.296907Z","shell.execute_reply":"2020-10-27T20:47:51.296155Z"},"papermill":{"duration":3.472884,"end_time":"2020-10-27T20:47:51.297044","exception":false,"start_time":"2020-10-27T20:47:47.82416","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False, float_format='%.16f')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}