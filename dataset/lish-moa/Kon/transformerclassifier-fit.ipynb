{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\nsys.path.append(\"../input/adabeliefoptimizer/pypi_packages/adabelief_tf0.1.0\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\nsys.path.append(\"../input/iterative-stratification/iterative-stratification-master\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pickle\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow\n\n\ndef build_callbacks(\n    model_path, factor=0.1, mode=\"auto\", monitor=\"val_loss\", patience=0, verbose=0\n):\n    early_stopping = tf.keras.callbacks.EarlyStopping(\n        mode=mode, monitor=monitor, patience=patience, verbose=verbose\n    )\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        model_path, mode=mode, monitor=monitor, save_best_only=True, verbose=verbose\n    )\n    reduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n        factor=factor, monitor=monitor, mode=mode, verbose=verbose\n    )\n\n    return [early_stopping, model_checkpoint, reduce_lr_on_plateau]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_addons as tfa\nfrom adabelief_tf import AdaBeliefOptimizer\n\n\ndef build_transformer_classifier(\n    input_dim,\n    output_dim,\n    bias_initializer=\"zeros\",\n    d_model=128,\n    dff=256,\n    label_smoothing=0.0,\n    n_heads=8,\n    n_layers=5,\n    optimizer_params=None,\n    pretrained_model_path=None,\n    rate=0.0,\n):\n    inputs = tf.keras.layers.Input(shape=input_dim)\n\n    x = inputs\n    x = tf.keras.layers.Reshape((1, input_dim))(x)\n    x = TransformerEncoder(n_layers - 2, d_model, n_heads, dff, rate=rate)(x)[:, 0, :]\n    x = tfa.layers.WeightNormalization(\n        tf.keras.layers.Dense(output_dim, bias_initializer=bias_initializer)\n    )(x)\n\n    outputs = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n\n    if optimizer_params is None:\n        optimizer_params = {}\n\n    if pretrained_model_path is not None:\n        model.load_weights(pretrained_model_path, by_name=True)\n\n    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing)\n    optimizer = AdaBeliefOptimizer(**optimizer_params)\n\n    model.compile(loss=loss, optimizer=optimizer)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_row_statistics(X, prefix=\"\"):\n    Xt = pd.DataFrame()\n\n    for agg_func in [\n        # \"min\",\n        # \"max\",\n        \"mean\",\n        \"std\",\n        \"kurtosis\",\n        \"skew\",\n    ]:\n        Xt[f\"{prefix}{agg_func}\"] = X.agg(agg_func, axis=1)\n\n    return Xt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n\ndef score(Y, Y_pred, eps=1e-15, label_smoothing=0.0):\n    Y = np.asarray(Y)\n    Y = np.ravel(Y)\n\n    if label_smoothing > 0.0:\n        Y = Y * (1.0 - label_smoothing) + 0.5 * label_smoothing\n\n    Y_pred = np.asarray(Y_pred)\n    Y_pred = np.ravel(Y_pred)\n    Y_pred = np.clip(Y_pred, eps, 1.0 - eps)\n\n    return -np.mean(Y * np.log(Y_pred) + (1.0 - Y) * np.log(1.0 - Y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport random as rn\n\nimport tensorflow as tf\nimport numpy as np\n\n\ndef set_seed(seed=0):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    rn.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n    graph = tf.compat.v1.get_default_graph()\n    session_conf = tf.compat.v1.ConfigProto(\n        inter_op_parallelism_threads=1, intra_op_parallelism_threads=1\n    )\n    sess = tf.compat.v1.Session(graph=graph, config=session_conf)\n\n    tf.compat.v1.keras.backend.set_session(sess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator\nfrom sklearn.base import TransformerMixin\n\n\nclass ClippedFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, copy=True, high=0.99, low=0.01):\n        self.copy = copy\n        self.high = high\n        self.low = low\n\n    def fit(self, X, y=None):\n        self.data_max_ = X.quantile(q=self.high)\n        self.data_min_ = X.quantile(q=self.low)\n\n        return self\n\n    def transform(self, X):\n        if self.copy:\n            X = X.copy()\n\n        X.clip(self.data_min_, self.data_max_, axis=1, inplace=True)\n\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# https://arxiv.org/abs/1905.04899\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass Cutmix(tf.keras.utils.Sequence):\n    def __init__(self, X, y=None, batch_size=32, alpha=1.0):\n        self.X = np.asarray(X)\n\n        if y is None:\n            self.y = y\n        else:\n            self.y = np.asarray(y)\n\n        self.batch_size = batch_size\n        self.alpha = alpha\n\n    def __getitem__(self, i):\n        X_batch = self.X[i * self.batch_size : (i + 1) * self.batch_size]\n\n        n_samples, n_features = self.X.shape\n        batch_size = X_batch.shape[0]\n        shuffle = np.random.choice(n_samples, batch_size)\n\n        l = np.random.beta(self.alpha, self.alpha)\n        mask = np.random.choice([0.0, 1.0], size=n_features, p=[1.0 - l, l])\n        X_shuffle = self.X[shuffle]\n        X_batch = mask * X_batch + (1.0 - mask) * X_shuffle\n\n        if self.y is None:\n            return X_batch, None\n\n        y_batch = self.y[i * self.batch_size : (i + 1) * self.batch_size]\n        y_shuffle = self.y[shuffle]\n        y_batch = l * y_batch + (1.0 - l) * y_shuffle\n\n        return X_batch, y_batch\n\n    def __len__(self):\n        n_samples = self.X.shape[0]\n\n        return int(np.ceil(n_samples / self.batch_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn.model_selection._split import _BaseKFold\n\n\nclass MultilabelStratifiedGroupKFold(_BaseKFold):\n    def __init__(self, n_splits=5, random_state=None, shuffle=False):\n        super().__init__(n_splits=n_splits, random_state=random_state, shuffle=shuffle)\n\n    def _iter_test_indices(self, X=None, y=None, groups=None):\n        cv = MultilabelStratifiedKFold(\n            n_splits=self.n_splits,\n            random_state=self.random_state,\n            shuffle=self.shuffle,\n        )\n\n        value_counts = groups.value_counts()\n        regluar_indices = value_counts.loc[\n            (value_counts == 6) | (value_counts == 12) | (value_counts == 18)\n        ].index.sort_values()\n        irregluar_indices = value_counts.loc[\n            (value_counts != 6) & (value_counts != 12) & (value_counts != 18)\n        ].index.sort_values()\n\n        group_to_fold = {}\n        tmp = y.groupby(groups).mean().loc[regluar_indices]\n\n        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n            group_to_fold.update({group: fold for group in tmp.index[test]})\n\n        sample_to_fold = {}\n        tmp = y.loc[groups.isin(irregluar_indices)]\n\n        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n            sample_to_fold.update({sample: fold for sample in tmp.index[test]})\n\n        folds = groups.map(group_to_fold)\n        is_na = folds.isna()\n        folds[is_na] = folds[is_na].index.map(sample_to_fold).values\n\n        for i in range(self.n_splits):\n            yield np.where(folds == i)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\n\ndef gelu(x):\n    cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n\n    return x * cdf\n\n\ndef point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential(\n        [\n            tf.keras.layers.Dense(dff, activation=gelu),  # (batch_size, seq_len, dff)\n            tf.keras.layers.Dense(d_model),  # (batch_size, seq_len, d_model)\n        ]\n    )\n\n\ndef scaled_dot_product_attention(q, k, v, mask):\n    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n    # scale matmul_qk\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n    # add the mask to the scaled tensor.\n    if mask is not None:\n        scaled_attention_logits += mask * -1e09\n\n    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n    attention_weights = tf.nn.softmax(\n        scaled_attention_logits, axis=-1\n    )  # (..., seq_len_q, seq_len_k)\n\n    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n    return output, attention_weights\n\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        self.depth = d_model // self.num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\n\n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        scaled_attention, attention_weights = scaled_dot_product_attention(\n            q, k, v, mask\n        )\n\n        scaled_attention = tf.transpose(\n            scaled_attention, perm=[0, 2, 1, 3]\n        )  # (batch_size, seq_len_q, num_heads, depth)\n\n        concat_attention = tf.reshape(\n            scaled_attention, (batch_size, -1, self.d_model)\n        )  # (batch_size, seq_len_q, d_model)\n\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n        return output, attention_weights\n\n\nclass EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super().__init__()\n\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-06)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-06)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask):\n        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(\n            out1 + ffn_output\n        )  # (batch_size, input_seq_len, d_model)\n\n        return out2\n\n\nclass TransformerEncoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1, **kwargs):\n        super().__init__(**kwargs)\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.dff = dff\n        self.rate = rate\n\n        self.embedding = tfa.layers.WeightNormalization(\n            tf.keras.layers.Dense(self.d_model)\n        )\n\n        self.enc_layers = [\n            EncoderLayer(self.d_model, self.num_heads, self.dff, self.rate)\n            for _ in range(self.num_layers)\n        ]\n\n        self.dropout = tf.keras.layers.Dropout(self.rate)\n\n    def get_config(self):\n        config = super().get_config().copy()\n\n        config.update(\n            {\n                \"num_layers\": self.num_layers,\n                \"d_model\": self.d_model,\n                \"num_heads\": self.num_heads,\n                \"dff\": self.dff,\n                \"rate\": self.rate,\n            }\n        )\n\n        return config\n\n    def call(self, x, training, mask=None):\n        seq_len = tf.shape(x)[1]\n\n        x = self.embedding(x)\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n\n        return x  # (batch_size, input_seq_len, d_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtype = {\"cp_type\": \"category\", \"cp_dose\": \"category\"}\nindex_col = \"sig_id\"\n\ntrain_features = pd.read_csv(\n    \"../input/lish-moa/train_features.csv\", dtype=dtype, index_col=index_col\n)\nX = train_features.select_dtypes(\"number\")\nY = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\", index_col=index_col)\ngroups = pd.read_csv(\n    \"../input/lish-moa/train_drug.csv\", index_col=index_col, squeeze=True\n)\n\ncolumns = Y.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_prefix = \"c-\"\ng_prefix = \"g-\"\nc_columns = X.columns.str.startswith(c_prefix)\ng_columns = X.columns.str.startswith(g_prefix)\nX_stats_c = compute_row_statistics(X.loc[:, c_columns], prefix=c_prefix)\nX_stats_g = compute_row_statistics(X.loc[:, g_columns], prefix=g_prefix)\n\nwith open(\"../input/preprocessor-fit/clipped_features.pkl\", \"rb\") as f:\n    clipped_features = pickle.load(f)\n\nX = clipped_features.transform(X)\n\nX = pd.concat([X, X_stats_c, X_stats_g], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size, n_features = X.shape\n_, n_classes = Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyperparameters\nalpha = 1.0\nbatch_size = 4096\nfactor = 0.5\nn_seeds = 5\nn_splits = 5\npatience = 30\nshuffle = True\nparams = {\n    \"d_model\": 64,\n    \"dff\": 256,\n    \"label_smoothing\": 5e-04,\n    \"n_heads\": 16,\n    \"n_layers\": 5,\n    \"optimizer_params\": {\"beta_1\": 0.75, \"lr\": 0.03},\n    \"rate\": 0.3,\n}\nfit_params = {\"epochs\": 1_000, \"verbose\": 0}\n\nwith open(\"params.pkl\", \"wb\") as f:\n    pickle.dump(params, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nbias_initializer = -Y.mean(axis=0).apply(np.log).values\nbias_initializer = tf.keras.initializers.Constant(bias_initializer)\n\nY_pred = np.zeros((train_size, n_classes))\nY_pred = pd.DataFrame(Y_pred, columns=Y.columns, index=Y.index)\n\nfor i in range(n_seeds):\n    set_seed(seed=i)\n\n    cv = MultilabelStratifiedGroupKFold(\n        n_splits=n_splits, random_state=i, shuffle=shuffle\n    )\n\n    for j, (train, valid) in enumerate(cv.split(X, Y[columns], groups)):\n        model_path = f\"model_seed_{i}_fold_{j}.h5\"\n\n        model = build_transformer_classifier(\n            n_features,\n            n_classes,\n            bias_initializer=bias_initializer,\n            **params,\n        )\n\n        generator = Cutmix(\n            X.iloc[train], Y.iloc[train], alpha=alpha, batch_size=batch_size\n        )\n        callbacks = build_callbacks(model_path, factor=factor, patience=patience)\n        history = model.fit(\n            generator,\n            callbacks=callbacks,\n            validation_data=(X.iloc[valid], Y.iloc[valid]),\n            **fit_params,\n        )\n\n        model.load_weights(model_path)\n\n        Y_pred.iloc[valid] += model.predict(X.iloc[valid]) / n_seeds\n\nY_pred[train_features[\"cp_type\"] == \"ctl_vehicle\"] = 0.0\n\nwith open(\"Y_pred.pkl\", \"wb\") as f:\n    pickle.dump(Y_pred[columns], f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score(Y[columns], Y_pred[columns])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# b 0.017613745637092890: vanilla\n# + 0.020231773279606454: remove cutmix\n# - 0.016933955369047540: batch_size=256\n# - 0.016561983004116220: batch_size=512\n# - 0.016317078077301540: batch_size=1024\n# - 0.016197360264813016: batch_size=2048\n# - 0.016143128480625435: batch_size=4096\n# + 0.016290473858185200: batch_size=8192\n# + 0.016169000906222634: lr=0.01\n# + 0.017920056100902890: lr=0.1\n# - 0.016141199647868334: cutmix(alpha=2.0)\n# - 0.016122704124989867: cutmix(alpha=1.0)\n# + 0.016145613363032430: cutmix(alpha=0.5)\n# + 0.016154231642736275: label_smoothing=1e-04\n# + 0.016151331836921672: label_smoothing=1e-03\n# - 0.016051994765583076: rate=0.35\n# - 0.016007689076553790: rate=0.3\n# + 0.016050162884695393: rate=0.25\n# - 0.015961052459868758: d_model=64\n# + 0.016032246860115885: d_model=32\n# + 0.016004713614897498: dff=128\n# + 0.015986869746250267: dff=512\n# + 0.016011309773467880: n_layers=6\n# + 0.016051247327988002: n_layers=4\n# - 0.015956038639902700: n_heads=16\n# + 0.020065607207388930: remove weight normalization\n# b 0.015961052459868758: n_heads=16\n# - 0.015938192644412540: apply weight normalization to an embedding layer\n# + 0.016249224488928680: activation=\"elu\"\n# + 0.015938192644412540: n_heads=32\n# + 0.015961652661258500: apply weight normalization to MultiAheadAttention\n# - 0.015932305370398952: add rowstatistics\n# - 0.015919330570302686: rowstatistics -> clipped_features -> concat\n# + 0.016104782998983973: add nonscored targets as additional targets\n# + 0.015925919237608416: add only highly correlated additional targets\n# + 0.015986798159799236: rowstatistics(max, min)\n# + 0.015961425133573657: polynomiral features after clipped features\n# + 0.015944889832617482: polynomiral features before clipped features\n# - 0.015918330358751717: beta_1=0.8\n# - 0.015897521669603114: beta_1=0.75\n# + 0.015919002373462714: beta_1=0.7\n# + 0.019501834012630444: beta_2=0.95\n# + 0.017315020687719890: beta_2=0.99","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}