{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\nsys.path.append(\"../input/customtabnet\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\nsys.path.append(\"../input/rank-gauss\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\nsys.path.append(\"../input/tabnet\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport pickle\n\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport torch\nimport tqdm\nfrom gauss_rank_scaler import GaussRankScaler\nfrom pytorch_tabnet.tab_model import TabNetRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_row_statistics(X, prefix=\"\"):\n    Xt = pd.DataFrame()\n\n    for agg_func in [\n        # \"min\",\n        # \"max\",\n        \"mean\",\n        \"std\",\n        \"kurtosis\",\n        \"skew\",\n    ]:\n        Xt[f\"{prefix}{agg_func}\"] = X.agg(agg_func, axis=1)\n\n    return Xt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator\nfrom sklearn.base import TransformerMixin\n\n\nclass ClippedFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, copy=True, high=0.99, low=0.01):\n        self.copy = copy\n        self.high = high\n        self.low = low\n\n    def fit(self, X, y=None):\n        self.data_max_ = X.quantile(q=self.high)\n        self.data_min_ = X.quantile(q=self.low)\n\n        return self\n\n    def transform(self, X):\n        if self.copy:\n            X = X.copy()\n\n        X.clip(self.data_min_, self.data_max_, axis=1, inplace=True)\n\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\n\nclass DynamicNet(object):\n    def __init__(self, c0, lr):\n        self.models = []\n        self.c0 = c0\n        self.lr = lr\n        self.boost_rate = nn.Parameter(\n            torch.tensor(lr, requires_grad=True, device=device)\n        )\n\n    def add(self, model):\n        self.models.append(model)\n\n    def parameters(self):\n        params = []\n\n        for m in self.models:\n            params.extend(m.parameters())\n\n        params.append(self.boost_rate)\n\n        return params\n\n    def zero_grad(self):\n        for m in self.models:\n            m.zero_grad()\n\n    def to_cuda(self):\n        for m in self.models:\n            m.cuda()\n\n    def to_eval(self):\n        for m in self.models:\n            m.eval()\n\n    def to_train(self):\n        for m in self.models:\n            m.train(True)\n\n    def forward(self, x):\n        if len(self.models) == 0:\n            batch = x.shape[0]\n            c0 = np.repeat(self.c0.detach().cpu().numpy().reshape(1, -1), batch, axis=0)\n            c0 = torch.Tensor(c0).cuda() if device == \"cuda\" else torch.Tensor(c0)\n\n            return None, c0\n\n        middle_feat_cum = None\n        prediction = None\n\n        with torch.no_grad():\n            for m in self.models:\n                if middle_feat_cum is None:\n                    middle_feat_cum, prediction = m(x, middle_feat_cum)\n                else:\n                    middle_feat_cum, pred = m(x, middle_feat_cum)\n                    prediction += pred\n\n        return middle_feat_cum, self.c0 + self.boost_rate * prediction\n\n    def forward_grad(self, x):\n        if len(self.models) == 0:\n            batch = x.shape[0]\n            c0 = np.repeat(self.c0.detach().cpu().numpy().reshape(1, -1), batch, axis=0)\n\n            return None, torch.Tensor(c0).cuda()\n\n        # at least one model\n        middle_feat_cum = None\n        prediction = None\n\n        for m in self.models:\n            if middle_feat_cum is None:\n                middle_feat_cum, prediction = m(x, middle_feat_cum)\n            else:\n                middle_feat_cum, pred = m(x, middle_feat_cum)\n                prediction += pred\n\n        return middle_feat_cum, self.c0 + self.boost_rate * prediction\n\n    @classmethod\n    def from_file(cls, path, builder, device):\n        d = torch.load(path, map_location=device)\n        net = DynamicNet(d[\"c0\"], d[\"lr\"])\n        net.boost_rate = d[\"boost_rate\"]\n\n        for stage, m in enumerate(d[\"models\"]):\n            submod = builder(stage)\n\n            submod.load_state_dict(m)\n            net.add(submod)\n\n        return net\n\n    def to_file(self, path):\n        models = [m.state_dict() for m in self.models]\n        d = {\n            \"models\": models,\n            \"c0\": self.c0,\n            \"lr\": self.lr,\n            \"boost_rate\": self.boost_rate,\n        }\n\n        torch.save(d, path)\n\n    def predict(self, X, device, batch_size=32):\n        self.to_eval()\n\n        for m in self.models:\n            m.to(device)\n\n        dataset = TestDataset(X)\n        data_loader = DataLoader(dataset, batch_size=batch_size)\n\n        predictions = []\n\n        with torch.no_grad():\n            for data in data_loader:\n                x = data[\"x\"].to(device)\n                _, pred = self.forward(x)\n\n                predictions.append(pred.sigmoid().detach().cpu().numpy())\n\n        return np.concatenate(predictions)\n\n\nclass MLP_2HL(nn.Module):\n    def __init__(self, dim_in, dim_hidden, dim_out, sparse=False, bn=True):\n        super().__init__()\n\n        self.bn2 = nn.BatchNorm1d(dim_in)\n\n        self.layer1 = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(dim_in, dim_hidden),\n            nn.ReLU(),\n            nn.BatchNorm1d(dim_hidden),\n            nn.Dropout(0.4),\n            nn.Linear(dim_hidden, dim_hidden),\n        )\n        self.layer2 = nn.Sequential(\n            nn.ReLU(),\n            nn.Linear(dim_hidden, dim_out),\n        )\n\n    def forward(self, x, lower_f):\n        if lower_f is not None:\n            x = torch.cat([x, lower_f], dim=1)\n            x = self.bn2(x)\n\n        middle_feat = self.layer1(x)\n        out = self.layer2(middle_feat)\n\n        return middle_feat, out\n\n    @classmethod\n    def get_model(cls, stage, dim_in, dim_hidden, dim_out):\n        if stage != 0:\n            dim_in += dim_hidden\n\n        return MLP_2HL(dim_in, dim_hidden, dim_out)\n\n\nclass TestDataset(object):\n    def __init__(self, X):\n        self.X = np.asarray(X)\n\n    def __len__(self):\n        return self.X.shape[0]\n\n    def __getitem__(self, i):\n        return {\"x\": torch.tensor(self.X[i], dtype=torch.float)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom pytorch_tabnet.metrics import Metric\n\n\nclass LogitsLogLoss(Metric):\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        logits = 1 / (1 + np.exp(-y_pred))\n        aux = (1 - y_true) * np.log(1 - logits + 1e-15) + y_true * np.log(\n            logits + 1e-15\n        )\n\n        return -np.mean(aux)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\n\ndef predict_with_pytorch_model(model, X, device, batch_size=32):\n    model.eval()\n\n    dataset = TestDataset(X)\n    data_loader = DataLoader(dataset, batch_size=batch_size)\n\n    predictions = []\n\n    with torch.no_grad():\n        for data in data_loader:\n            inputs = data[\"x\"].to(device)\n            outputs = model(inputs)\n\n        predictions.append(outputs.sigmoid().detach().cpu().numpy())\n\n    return np.concatenate(predictions)\n\n\nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size, rate=0.2619422201258426):\n        super().__init__()\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n\n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(rate)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(rate)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n\n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n\n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tabnet import StackedTabNet\n\n\nclass StackedTabNetClassifier(tf.keras.Model):\n    def __init__(\n        self,\n        num_classes,\n        batch_momentum=0.98,\n        epsilon=1e-05,\n        feature_columns=None,\n        feature_dim=64,\n        norm_type=\"group\",\n        num_decision_steps=5,\n        num_features=None,\n        num_groups=2,\n        num_layers=1,\n        output_dim=64,\n        relaxation_factor=1.5,\n        sparsity_coefficient=1e-05,\n        virtual_batch_size=None,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        self.stacked_tabnet = StackedTabNet(\n            feature_columns,\n            batch_momentum=batch_momentum,\n            epsilon=epsilon,\n            feature_dim=feature_dim,\n            norm_type=norm_type,\n            num_decision_steps=num_decision_steps,\n            num_features=num_features,\n            num_groups=num_groups,\n            num_layers=num_layers,\n            output_dim=output_dim,\n            relaxation_factor=relaxation_factor,\n            sparsity_coefficient=sparsity_coefficient,\n            virtual_batch_size=virtual_batch_size,\n        )\n\n        self.classifier = tf.keras.layers.Dense(\n            num_classes, activation=\"sigmoid\", use_bias=False\n        )\n\n    def call(self, inputs, training=None):\n        x = self.stacked_tabnet(inputs, training=training)\n\n        return self.classifier(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tabnet import TabNet\n\n\nclass TabNetClassifier(tf.keras.Model):\n    def __init__(\n        self,\n        num_classes,\n        batch_momentum=0.98,\n        epsilon=1e-05,\n        feature_columns=None,\n        feature_dim=64,\n        norm_type=\"group\",\n        num_decision_steps=5,\n        num_features=None,\n        num_groups=1,\n        output_dim=64,\n        relaxation_factor=1.5,\n        sparsity_coefficient=1e-05,\n        virtual_batch_size=None,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        self.tabnet = TabNet(\n            feature_columns,\n            batch_momentum=batch_momentum,\n            epsilon=epsilon,\n            feature_dim=feature_dim,\n            norm_type=norm_type,\n            num_decision_steps=num_decision_steps,\n            num_features=num_features,\n            num_groups=num_groups,\n            output_dim=output_dim,\n            relaxation_factor=relaxation_factor,\n            sparsity_coefficient=sparsity_coefficient,\n            virtual_batch_size=virtual_batch_size,\n            **kwargs\n        )\n\n        self.classifier = tf.keras.layers.Dense(\n            num_classes, activation=\"sigmoid\", use_bias=False\n        )\n\n    def call(self, inputs, training=None):\n        x = self.tabnet(inputs, training=training)\n\n        return self.classifier(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\n\ndef gelu(x):\n    cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n\n    return x * cdf\n\n\ndef point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential(\n        [\n            tf.keras.layers.Dense(dff, activation=gelu),  # (batch_size, seq_len, dff)\n            tf.keras.layers.Dense(d_model),  # (batch_size, seq_len, d_model)\n        ]\n    )\n\n\ndef scaled_dot_product_attention(q, k, v, mask):\n    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n    # scale matmul_qk\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n    # add the mask to the scaled tensor.\n    if mask is not None:\n        scaled_attention_logits += mask * -1e09\n\n    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n    attention_weights = tf.nn.softmax(\n        scaled_attention_logits, axis=-1\n    )  # (..., seq_len_q, seq_len_k)\n\n    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n    return output, attention_weights\n\n\nclass EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super().__init__()\n\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-06)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-06)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask):\n        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(\n            out1 + ffn_output\n        )  # (batch_size, input_seq_len, d_model)\n\n        return out2\n\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        self.depth = d_model // self.num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\n\n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        scaled_attention, attention_weights = scaled_dot_product_attention(\n            q, k, v, mask\n        )\n\n        scaled_attention = tf.transpose(\n            scaled_attention, perm=[0, 2, 1, 3]\n        )  # (batch_size, seq_len_q, num_heads, depth)\n\n        concat_attention = tf.reshape(\n            scaled_attention, (batch_size, -1, self.d_model)\n        )  # (batch_size, seq_len_q, d_model)\n\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n        return output, attention_weights\n\n\nclass TransformerEncoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1, **kwargs):\n        super().__init__(**kwargs)\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.dff = dff\n        self.rate = rate\n\n        self.embedding = tfa.layers.WeightNormalization(\n            tf.keras.layers.Dense(self.d_model)\n        )\n\n        self.enc_layers = [\n            EncoderLayer(self.d_model, self.num_heads, self.dff, self.rate)\n            for _ in range(self.num_layers)\n        ]\n\n        self.dropout = tf.keras.layers.Dropout(self.rate)\n\n    def get_config(self):\n        config = super().get_config().copy()\n\n        config.update(\n            {\n                \"d_model\": self.d_model,\n                \"dff\": self.dff,\n                \"num_layers\": self.num_layers,\n                \"num_heads\": self.num_heads,\n                \"rate\": self.rate,\n            }\n        )\n\n        return config\n\n    def call(self, x, training, mask=None):\n        seq_len = tf.shape(x)[1]\n\n        x = self.embedding(x)\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n\n        return x  # (batch_size, input_seq_len, d_model)\n\n\nclass TransformerEncoderByZhang(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1, **kwargs):\n        super().__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.dff = dff\n        self.rate = rate\n\n        self.embedding = tf.keras.layers.Dense(self.d_model)\n\n        self.enc_layers = [\n            EncoderLayer(self.d_model, self.num_heads, self.dff, self.rate)\n            for _ in range(self.num_layers)\n        ]\n\n        self.dropout = tf.keras.layers.Dropout(self.rate)\n\n    def get_config(self):\n        config = super().get_config().copy()\n        config.update(\n            {\n                \"d_model\": self.d_model,\n                \"dff\": self.dff,\n                \"num_layers\": self.num_layers,\n                \"num_heads\": self.num_heads,\n                \"rate\": self.rate,\n            }\n        )\n\n        return config\n\n    def call(self, x, training, mask=None):\n        seq_len = tf.shape(x)[1]\n\n        x = self.embedding(x)\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n\n        return x  # (batch_size, input_seq_len, d_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"../input/preprocessor-fit/clipped_features.pkl\", \"rb\") as f:\n    clipped_features = pickle.load(f)\n\nwith open(\"../input/preprocessor-fit/counts.pkl\", \"rb\") as f:\n    counts = pickle.load(f)\n\nwith open(\"../input/mlp-for-ensemble/kaggle_upload/stacked_tabnet_params.pkl\", \"rb\") as f:\n    params_stacked_tabnet = pickle.load(f)\n\nwith open(\"../input/tabnetclassifier-fit/params.pkl\", \"rb\") as f:\n    params_tabnet = pickle.load(f)\n\nwith open(\"../input/votingclassifier-fit-without-lightgm-and-svm/weights.pkl\", \"rb\") as f:\n    weights = pickle.load(f)\n\n# with open(\"../input/votingclassifier-fit-without-lightgm-and-svm/thresholds.pkl\", \"rb\") as f:\n#     thresholds = pickle.load(f)\n\nwith open(\"../input/pytorch-mlp-tabnet-many-fe-train/rank_gauss.pkl\", \"rb\") as f:\n    scalers = pickle.load(f)\n\nwith open(\"../input/pytorch-mlp-tabnet-many-fe-train/gen_pca.pkl\", \"rb\") as f:\n    pca_g = pickle.load(f)\n\nwith open(\"../input/pytorch-mlp-tabnet-many-fe-train/cel_pca.pkl\", \"rb\") as f:\n    pca_c = pickle.load(f)\n\nwith open(\"../input/pytorch-mlp-tabnet-many-fe-train/var_thresh.pkl\", \"rb\") as f:\n    selector = pickle.load(f)\n\n# with open(\"../input/21-tabnet-fit/ica_gene.pickle\", \"rb\") as f:\n#     ica_g = pickle.load(f)\n\n# with open(\"../input/21-tabnet-fit/ica_cell.pickle\", \"rb\") as f:\n#     ica_c = pickle.load(f)\n\n# with open(\"../input/21-tabnet-fit/rankgauss.pickle\", \"rb\") as f:\n#     scaler = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtype = {\"cp_type\": \"category\", \"cp_dose\": \"category\"}\nindex_col = \"sig_id\"\n\nY = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\", index_col=index_col)\ntest_features = pd.read_csv(\n    \"../input/lish-moa/test_features.csv\", dtype=dtype, index_col=index_col\n)\n\ncolumns = Y.columns\nindex = test_features.index\n\nc_prefix = \"c-\"\ng_prefix = \"g-\"\nc_columns = test_features.columns.str.startswith(c_prefix)\nc_columns = list(test_features.columns[c_columns])\ng_columns = test_features.columns.str.startswith(g_prefix)\ng_columns = list(test_features.columns[g_columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test_features.select_dtypes(\"number\").copy()\nX_test_stats_c = compute_row_statistics(X_test.loc[:, c_columns], prefix=c_prefix)\nX_test_stats_g = compute_row_statistics(X_test.loc[:, g_columns], prefix=g_prefix)\n\nX_test_clipped = clipped_features.transform(X_test)\n\nX_test = pd.concat([X_test_clipped, X_test_stats_c, X_test_stats_g], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_hirune924 = test_features[g_columns + c_columns].copy()\n\nfor column in g_columns + c_columns:\n    X_test_hirune924[column] = scalers[column].transform(X_test_hirune924[[column]])\n\nX_test_pca_g = pca_g.transform(X_test_hirune924[g_columns])\nX_test_pca_c = pca_c.transform(X_test_hirune924[c_columns])\n\nX_test_hirune924 = np.concatenate(\n    [X_test_hirune924, X_test_pca_g, X_test_pca_c], axis=1\n)\n\nX_test_hirune924 = selector.transform(X_test_hirune924)\nX_test_hirune924 = pd.DataFrame(X_test_hirune924, index=index)\n\nX_test_hirune924 = pd.concat(\n    [test_features[[\"cp_time\", \"cp_dose\"]], X_test_hirune924], axis=1\n)\nX_test_hirune924 = pd.get_dummies(X_test_hirune924, columns=[\"cp_time\", \"cp_dose\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_test_ynishi = test_features.copy()\n# X_test_ynishi[\"cp_type\"] = X_test_ynishi[\"cp_type\"].map({\"trt_cp\": 0, \"ctl_vehicle\": 1})\n# X_test_ynishi[\"cp_dose\"] = X_test_ynishi[\"cp_dose\"].map({\"D1\": 0, \"D2\": 1})\n\n# vt_label = pd.read_csv(\"../input/98-preparation-set/vt_label.csv\", index_col=0)\n# vt_label = vt_label.iloc[1, 1:].dropna()\n# selected_c_columns = vt_label.str.startswith(c_prefix)\n# selected_c_columns = list(vt_label[selected_c_columns])\n# selected_g_columns = vt_label.str.startswith(g_prefix)\n# selected_g_columns = list(vt_label[selected_g_columns])\n\n# X_test_ica_g = ica_g.transform(X_test_ynishi[selected_g_columns])\n# X_test_ica_g = pd.DataFrame(X_test_ica_g, index=index)\n# X_test_ica_c = ica_c.transform(X_test_ynishi[selected_c_columns])\n# X_test_ica_c = pd.DataFrame(X_test_ica_c, index=index)\n\n# X_test_stats = pd.DataFrame(index=index)\n\n# for agg_func in [\"std\", \"kurt\", \"skew\"]:\n#     X_test_stats[f\"g-{agg_func}\"] = X_test_ynishi[selected_g_columns].agg(\n#         agg_func, axis=1\n#     )\n#     X_test_stats[f\"c-{agg_func}\"] = X_test_ynishi[selected_c_columns].agg(\n#         agg_func, axis=1\n#     )\n#     X_test_stats[f\"gc-{agg_func}\"] = X_test_ynishi[\n#         selected_g_columns + selected_c_columns\n#     ].agg(agg_func, axis=1)\n\n# X_test_scaled = scaler.transform(X_test_ynishi[selected_g_columns + selected_c_columns])\n# X_test_scaled = pd.DataFrame(\n#     X_test_scaled, columns=selected_g_columns + selected_c_columns, index=index\n# )\n\n# X_test_n_effective_features = (X_test_scaled[selected_g_columns].abs() > 2.0).sum(\n#     axis=1\n# )\n\n# X_test_bins = pd.DataFrame(index=index)\n\n# cell_bins = pd.read_csv(f\"../input/21-tabnet-fit/cell_bins.csv\", index_col=0)\n# cell_bins.iloc[0] = -np.inf\n# cell_bins.iloc[-1] = np.inf\n# n_bins = cell_bins.shape[0] - 1\n\n# for column in selected_c_columns:\n#     X_test_bins[f\"bin-{column}\"] = pd.cut(\n#         X_test_scaled[column], cell_bins[column], labels=range(n_bins)\n#     )\n\n# X_test_ynishi = pd.concat(\n#     [\n#         X_test_ynishi[[\"cp_type\", \"cp_time\", \"cp_dose\"]],\n#         X_test_scaled,\n#         X_test_ica_g,\n#         X_test_ica_c,\n#         X_test_stats,\n#         X_test_n_effective_features,\n#         X_test_bins,\n#     ],\n#     axis=1,\n# )\n\n# X_test_ynishi = X_test_ynishi.astype(\"float\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_columns = [\n    \"g-0\",\n    \"g-7\",\n    \"g-8\",\n    \"g-10\",\n    \"g-13\",\n    \"g-17\",\n    \"g-20\",\n    \"g-22\",\n    \"g-24\",\n    \"g-26\",\n    \"g-28\",\n    \"g-29\",\n    \"g-30\",\n    \"g-31\",\n    \"g-32\",\n    \"g-34\",\n    \"g-35\",\n    \"g-36\",\n    \"g-37\",\n    \"g-38\",\n    \"g-39\",\n    \"g-41\",\n    \"g-46\",\n    \"g-48\",\n    \"g-50\",\n    \"g-51\",\n    \"g-52\",\n    \"g-55\",\n    \"g-58\",\n    \"g-59\",\n    \"g-61\",\n    \"g-62\",\n    \"g-63\",\n    \"g-65\",\n    \"g-66\",\n    \"g-67\",\n    \"g-68\",\n    \"g-70\",\n    \"g-72\",\n    \"g-74\",\n    \"g-75\",\n    \"g-79\",\n    \"g-83\",\n    \"g-84\",\n    \"g-85\",\n    \"g-86\",\n    \"g-90\",\n    \"g-91\",\n    \"g-94\",\n    \"g-95\",\n    \"g-96\",\n    \"g-97\",\n    \"g-98\",\n    \"g-100\",\n    \"g-102\",\n    \"g-105\",\n    \"g-106\",\n    \"g-112\",\n    \"g-113\",\n    \"g-114\",\n    \"g-116\",\n    \"g-121\",\n    \"g-123\",\n    \"g-126\",\n    \"g-128\",\n    \"g-131\",\n    \"g-132\",\n    \"g-134\",\n    \"g-135\",\n    \"g-138\",\n    \"g-139\",\n    \"g-140\",\n    \"g-142\",\n    \"g-144\",\n    \"g-145\",\n    \"g-146\",\n    \"g-147\",\n    \"g-148\",\n    \"g-152\",\n    \"g-155\",\n    \"g-157\",\n    \"g-158\",\n    \"g-160\",\n    \"g-163\",\n    \"g-164\",\n    \"g-165\",\n    \"g-170\",\n    \"g-173\",\n    \"g-174\",\n    \"g-175\",\n    \"g-177\",\n    \"g-178\",\n    \"g-181\",\n    \"g-183\",\n    \"g-185\",\n    \"g-186\",\n    \"g-189\",\n    \"g-192\",\n    \"g-194\",\n    \"g-195\",\n    \"g-196\",\n    \"g-197\",\n    \"g-199\",\n    \"g-201\",\n    \"g-202\",\n    \"g-206\",\n    \"g-208\",\n    \"g-210\",\n    \"g-213\",\n    \"g-214\",\n    \"g-215\",\n    \"g-220\",\n    \"g-226\",\n    \"g-228\",\n    \"g-229\",\n    \"g-235\",\n    \"g-238\",\n    \"g-241\",\n    \"g-242\",\n    \"g-243\",\n    \"g-244\",\n    \"g-245\",\n    \"g-248\",\n    \"g-250\",\n    \"g-251\",\n    \"g-254\",\n    \"g-257\",\n    \"g-259\",\n    \"g-261\",\n    \"g-266\",\n    \"g-270\",\n    \"g-271\",\n    \"g-272\",\n    \"g-275\",\n    \"g-278\",\n    \"g-282\",\n    \"g-287\",\n    \"g-288\",\n    \"g-289\",\n    \"g-291\",\n    \"g-293\",\n    \"g-294\",\n    \"g-297\",\n    \"g-298\",\n    \"g-301\",\n    \"g-303\",\n    \"g-304\",\n    \"g-306\",\n    \"g-308\",\n    \"g-309\",\n    \"g-310\",\n    \"g-311\",\n    \"g-314\",\n    \"g-315\",\n    \"g-316\",\n    \"g-317\",\n    \"g-320\",\n    \"g-321\",\n    \"g-322\",\n    \"g-327\",\n    \"g-328\",\n    \"g-329\",\n    \"g-332\",\n    \"g-334\",\n    \"g-335\",\n    \"g-336\",\n    \"g-337\",\n    \"g-339\",\n    \"g-342\",\n    \"g-344\",\n    \"g-349\",\n    \"g-350\",\n    \"g-351\",\n    \"g-353\",\n    \"g-354\",\n    \"g-355\",\n    \"g-357\",\n    \"g-359\",\n    \"g-360\",\n    \"g-364\",\n    \"g-365\",\n    \"g-366\",\n    \"g-367\",\n    \"g-368\",\n    \"g-369\",\n    \"g-374\",\n    \"g-375\",\n    \"g-377\",\n    \"g-379\",\n    \"g-385\",\n    \"g-386\",\n    \"g-390\",\n    \"g-392\",\n    \"g-393\",\n    \"g-400\",\n    \"g-402\",\n    \"g-406\",\n    \"g-407\",\n    \"g-409\",\n    \"g-410\",\n    \"g-411\",\n    \"g-414\",\n    \"g-417\",\n    \"g-418\",\n    \"g-421\",\n    \"g-423\",\n    \"g-424\",\n    \"g-427\",\n    \"g-429\",\n    \"g-431\",\n    \"g-432\",\n    \"g-433\",\n    \"g-434\",\n    \"g-437\",\n    \"g-439\",\n    \"g-440\",\n    \"g-443\",\n    \"g-449\",\n    \"g-458\",\n    \"g-459\",\n    \"g-460\",\n    \"g-461\",\n    \"g-464\",\n    \"g-467\",\n    \"g-468\",\n    \"g-470\",\n    \"g-473\",\n    \"g-477\",\n    \"g-478\",\n    \"g-479\",\n    \"g-484\",\n    \"g-485\",\n    \"g-486\",\n    \"g-488\",\n    \"g-489\",\n    \"g-491\",\n    \"g-494\",\n    \"g-496\",\n    \"g-498\",\n    \"g-500\",\n    \"g-503\",\n    \"g-504\",\n    \"g-506\",\n    \"g-508\",\n    \"g-509\",\n    \"g-512\",\n    \"g-522\",\n    \"g-529\",\n    \"g-531\",\n    \"g-534\",\n    \"g-539\",\n    \"g-541\",\n    \"g-546\",\n    \"g-551\",\n    \"g-553\",\n    \"g-554\",\n    \"g-559\",\n    \"g-561\",\n    \"g-562\",\n    \"g-565\",\n    \"g-568\",\n    \"g-569\",\n    \"g-574\",\n    \"g-577\",\n    \"g-578\",\n    \"g-586\",\n    \"g-588\",\n    \"g-590\",\n    \"g-594\",\n    \"g-595\",\n    \"g-596\",\n    \"g-597\",\n    \"g-599\",\n    \"g-600\",\n    \"g-603\",\n    \"g-607\",\n    \"g-615\",\n    \"g-618\",\n    \"g-619\",\n    \"g-620\",\n    \"g-625\",\n    \"g-628\",\n    \"g-629\",\n    \"g-632\",\n    \"g-634\",\n    \"g-635\",\n    \"g-636\",\n    \"g-638\",\n    \"g-639\",\n    \"g-641\",\n    \"g-643\",\n    \"g-644\",\n    \"g-645\",\n    \"g-646\",\n    \"g-647\",\n    \"g-648\",\n    \"g-663\",\n    \"g-664\",\n    \"g-665\",\n    \"g-668\",\n    \"g-669\",\n    \"g-670\",\n    \"g-671\",\n    \"g-672\",\n    \"g-673\",\n    \"g-674\",\n    \"g-677\",\n    \"g-678\",\n    \"g-680\",\n    \"g-683\",\n    \"g-689\",\n    \"g-691\",\n    \"g-693\",\n    \"g-695\",\n    \"g-701\",\n    \"g-702\",\n    \"g-703\",\n    \"g-704\",\n    \"g-705\",\n    \"g-706\",\n    \"g-708\",\n    \"g-711\",\n    \"g-712\",\n    \"g-720\",\n    \"g-721\",\n    \"g-723\",\n    \"g-724\",\n    \"g-726\",\n    \"g-728\",\n    \"g-731\",\n    \"g-733\",\n    \"g-738\",\n    \"g-739\",\n    \"g-742\",\n    \"g-743\",\n    \"g-744\",\n    \"g-745\",\n    \"g-749\",\n    \"g-750\",\n    \"g-752\",\n    \"g-760\",\n    \"g-761\",\n    \"g-764\",\n    \"g-766\",\n    \"g-768\",\n    \"g-770\",\n    \"g-771\",\n    \"c-0\",\n    \"c-1\",\n    \"c-2\",\n    \"c-3\",\n    \"c-4\",\n    \"c-5\",\n    \"c-6\",\n    \"c-7\",\n    \"c-8\",\n    \"c-9\",\n    \"c-10\",\n    \"c-11\",\n    \"c-12\",\n    \"c-13\",\n    \"c-14\",\n    \"c-15\",\n    \"c-16\",\n    \"c-17\",\n    \"c-18\",\n    \"c-19\",\n    \"c-20\",\n    \"c-21\",\n    \"c-22\",\n    \"c-23\",\n    \"c-24\",\n    \"c-25\",\n    \"c-26\",\n    \"c-27\",\n    \"c-28\",\n    \"c-29\",\n    \"c-30\",\n    \"c-31\",\n    \"c-32\",\n    \"c-33\",\n    \"c-34\",\n    \"c-35\",\n    \"c-36\",\n    \"c-37\",\n    \"c-38\",\n    \"c-39\",\n    \"c-40\",\n    \"c-41\",\n    \"c-42\",\n    \"c-43\",\n    \"c-44\",\n    \"c-45\",\n    \"c-46\",\n    \"c-47\",\n    \"c-48\",\n    \"c-49\",\n    \"c-50\",\n    \"c-51\",\n    \"c-52\",\n    \"c-53\",\n    \"c-54\",\n    \"c-55\",\n    \"c-56\",\n    \"c-57\",\n    \"c-58\",\n    \"c-59\",\n    \"c-60\",\n    \"c-61\",\n    \"c-62\",\n    \"c-63\",\n    \"c-64\",\n    \"c-65\",\n    \"c-66\",\n    \"c-67\",\n    \"c-68\",\n    \"c-69\",\n    \"c-70\",\n    \"c-71\",\n    \"c-72\",\n    \"c-73\",\n    \"c-74\",\n    \"c-75\",\n    \"c-76\",\n    \"c-77\",\n    \"c-78\",\n    \"c-79\",\n    \"c-80\",\n    \"c-81\",\n    \"c-82\",\n    \"c-83\",\n    \"c-84\",\n    \"c-85\",\n    \"c-86\",\n    \"c-87\",\n    \"c-88\",\n    \"c-89\",\n    \"c-90\",\n    \"c-91\",\n    \"c-92\",\n    \"c-93\",\n    \"c-94\",\n    \"c-95\",\n    \"c-96\",\n    \"c-97\",\n    \"c-98\",\n    \"c-99\",\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, n_classes = Y.shape\ntest_size, n_clipped_features = X_test_clipped.shape\n_, n_features = X_test.shape\n_, n_features_hirune924 = X_test_hirune924.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyperparameters\nbatch_size = 32\nn_seeds = 5\nn_splits = 5\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = Y.mean()\nmean = np.asarray(mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndenominator = counts.sum(axis=0)\n\ncounts /= denominator\n\n_, n_models = weights.shape\nY_preds = np.zeros((n_models, test_size, n_classes))\n\nfor i in tqdm.trange(n_seeds):\n    for j in tqdm.trange(n_splits):\n        #\n        # anonamename\n        #\n        # model = tf.keras.models.load_model(\n        #     f\"../input/mlp-for-ensemble/kaggle_upload/2l/model_seed_{i}_fold_{j}.h5\",\n        #     compile=False,\n        # )\n\n        # Y_preds[0] += counts[i * n_splits + j] * model.predict(\n        #     X_test_clipped, batch_size=batch_size\n        # )\n\n        # tf.keras.backend.clear_session()\n\n        # model = tf.keras.models.load_model(\n        #     f\"../input/mlp-for-ensemble/kaggle_upload/3l_v2/model_seed_{i}_fold_{j}.h5\",\n        #     compile=False,\n        # )\n\n        # Y_preds[1] += counts[i * n_splits + j] * model.predict(\n        #     X_test_clipped, batch_size=batch_size\n        # )\n\n        # tf.keras.backend.clear_session()\n\n        # model = tf.keras.models.load_model(\n        #     f\"../input/mlp-for-ensemble/kaggle_upload/4l/model_seed_{i}_fold_{j}.h5\",\n        #     compile=False,\n        # )\n\n        # Y_preds[2] += counts[i * n_splits + j] * model.predict(\n        #     X_test_clipped, batch_size=batch_size\n        # )\n\n        # tf.keras.backend.clear_session()\n\n        # model = tf.keras.models.load_model(\n        #     f\"../input/mlp-for-ensemble/kaggle_upload/5l/model_seed_{i}_fold_{j}.h5\",\n        #     compile=False,\n        # )\n\n        # Y_preds[3] += counts[i * n_splits + j] * model.predict(\n        #     X_test_clipped, batch_size=batch_size\n        # )\n\n        # tf.keras.backend.clear_session()\n\n        model = tf.keras.models.load_model(\n            f\"../input/mlp-for-ensemble/kaggle_upload/rs/model_seed_{i}_fold_{j}.h5\",\n            compile=False,\n        )\n\n        Y_preds[0] += counts[i * n_splits + j] * model.predict(\n            [X_test_clipped, X_test_clipped[selected_columns]], batch_size=batch_size\n        )\n\n        tf.keras.backend.clear_session()\n\n        model = StackedTabNetClassifier(\n            num_classes=n_classes,\n            num_features=n_clipped_features,\n            **params_stacked_tabnet,\n        )\n\n        # https://stackoverflow.com/questions/63658086/tensorflow-2-0-valueerror-while-loading-weights-from-h5-file\n        model(np.zeros((1, n_clipped_features)))\n\n        model.load_weights(\n            f\"../input/mlp-for-ensemble/kaggle_upload/StackedTabNet/model_seed_{i}_fold_{j}.h5\"\n        )\n\n        Y_preds[1] += counts[i * n_splits + j] * model.predict(\n            X_test_clipped, batch_size=batch_size\n        )\n\n        tf.keras.backend.clear_session()\n\n        # model = DynamicNet.from_file(\n        #     f\"../input/moa-grownet/{j}FOLD_{i}_.pth\",\n        #     lambda stage: MLP_2HL.get_model(\n        #         stage,\n        #         n_clipped_features,\n        #         512,\n        #         n_classes,\n        #     ),\n        #     device,\n        # )\n\n        # Y_preds[6] += counts[i * n_splits + j] * model.predict(\n        #     X_test_clipped, device, batch_size=test_size\n        # )\n\n        # with open(\n        #     f\"../input/moa-lgbmclassifier-classifierchain/model_seed_{i}_fold_{j}.jlb\",\n        #     \"rb\",\n        # ) as f:\n        #     model = joblib.load(f)\n\n        # Y_preds[7] += counts[i * n_splits + j] * model.predict_proba(X_test_clipped)\n\n        # for k, column in enumerate(columns):\n        #     with open(\n        #         f\"../input/moa-lightgbm/model_seed_{i}_fold_{j}_{column}.jlb\",\n        #         \"rb\",\n        #     ) as f:\n        #         model = joblib.load(f)\n\n        #     Y_preds[2, :, k] += counts[i * n_splits + j, k] * model.predict(\n        #         X_test_clipped\n        #     )\n\n        # for k, column in enumerate(columns):\n        #     model_dir = {\n        #         0: \"../input/moa-rapids-svm-seed01\",\n        #         1: \"../input/moa-rapids-svm-seed01\",\n        #         2: \"../input/fork-of-moa-rapids-svm-seed23\",\n        #         3: \"../input/fork-of-moa-rapids-svm-seed23\",\n        #         4: \"../input/fork-of-moa-rapids-svm-seed4\",\n        #     }\n\n        #     model_path = f\"{model_dir[i]}/model_seed_{i}_fold_{j}_{column}.jlb\"\n\n        #     if os.path.exists(model_path):\n        #         with open(model_path, \"rb\") as f:\n        #             model = joblib.load(f)\n\n        #         Y_preds[3, :, k] += (\n        #             counts[i * n_splits + j, k]\n        #             * model.predict_proba(X_test_clipped)[:, 1]\n        #         )\n\n        #     else:\n        #         Y_preds[3, :, k] += counts[i * n_splits + j, k] * mean[k]\n\n        #\n        # ari hiro\n        #\n        model = tf.keras.models.load_model(\n            f\"../input/transformer-fit/Transformer_{i}_{j}.hdf5\",\n            compile=False,\n            custom_objects={\"TransformerEncoder\": TransformerEncoderByZhang},\n        )\n\n        Y_preds[2] += counts[i * n_splits + j] * model.predict(\n            X_test_clipped, batch_size=batch_size\n        )\n\n        tf.keras.backend.clear_session()\n\n        #\n        # hirune924\n        #\n        model = Model(\n            num_features=n_features_hirune924, num_targets=n_classes, hidden_size=1_500\n        )\n\n        model.load_state_dict(\n            torch.load(\n                f\"../input/pytorch-mlp-tabnet-many-fe-train/FOLD{j}_SEED{i}.pth\",\n                map_location=torch.device(device),\n            )\n        )\n        model.to(device)\n\n        Y_preds[3] += counts[i * n_splits + j] * predict_with_pytorch_model(\n            model, X_test_hirune924, device, batch_size=test_size\n        )\n\n        with open(\n            f\"../input/pytorch-mlp-tabnet-many-fe-train/tabnet_FOLD{j}_SEED{i}.pkl\",\n            \"rb\",\n        ) as f:\n            model = pickle.load(f)\n\n        Y_pred = model.predict(X_test_hirune924.values)\n        Y_pred = 1.0 / (1.0 + np.exp(-Y_pred))\n        Y_preds[4] += counts[i * n_splits + j] * Y_pred\n\n        with open(\n            f\"../input/pytorch-tabnet-pretraining-step3-many-fe-train/tabnet_FOLD{j}_SEED{i}.pkl\",\n            \"rb\",\n        ) as f:\n            model = pickle.load(f)\n\n        Y_pred = model.predict(X_test_hirune924.values)\n        Y_pred = 1.0 / (1.0 + np.exp(-Y_pred))\n        Y_preds[5] += counts[i * n_splits + j] * Y_pred\n\n        #\n        # Kon\n        #\n        # model = tf.keras.models.load_model(\n        #     f\"../input/lstmclassifier-fit/model_seed_{i}_fold_{j}.h5\",\n        #     compile=False,\n        # )\n\n        # Y_preds[12] += counts[i * n_splits + j] * model.predict(\n        #     X_test, batch_size=batch_size\n        # )\n\n        # tf.keras.backend.clear_session()\n\n        # model = tf.keras.models.load_model(\n        #     f\"../input/mlpclassifier-fit/model_seed_{i}_fold_{j}.h5\",\n        #     compile=False,\n        # )\n\n        # Y_preds[13] += counts[i * n_splits + j] * model.predict(\n        #     X_test, batch_size=batch_size\n        # )\n\n        # tf.keras.backend.clear_session()\n\n        model = tf.keras.models.load_model(\n            f\"../input/resnetclassifier-fit/model_seed_{i}_fold_{j}.h5\",\n            compile=False,\n        )\n\n        Y_preds[6] += counts[i * n_splits + j] * model.predict(\n            X_test, batch_size=batch_size\n        )\n\n        tf.keras.backend.clear_session()\n\n        # model = TabNetClassifier(\n        #     num_classes=n_classes, num_features=n_features, **params_tabnet\n        # )\n\n        # # https://stackoverflow.com/questions/63658086/tensorflow-2-0-valueerror-while-loading-weights-from-h5-file\n        # model(np.zeros((1, n_features)))\n\n        # model.load_weights(f\"../input/tabnetclassifier-fit/model_seed_{i}_fold_{j}.h5\")\n\n        # Y_preds[9] += counts[i * n_splits + j] * model.predict(\n        #     X_test, batch_size=batch_size\n        # )\n\n        # tf.keras.backend.clear_session()\n\n        # model = tf.keras.models.load_model(\n        #     f\"../input/transformerclassifier-fit/model_seed_{i}_fold_{j}.h5\",\n        #     compile=False,\n        #     custom_objects={\"TransformerEncoder\": TransformerEncoder},\n        # )\n\n        # Y_preds[16] += counts[i * n_splits + j] * model.predict(\n        #     X_test, batch_size=batch_size\n        # )\n\n        # tf.keras.backend.clear_session()\n\n        #\n        # ynishi\n        #\n        # model = TabNetRegressor()\n\n        # model.load_model(f\"../input/21-tabnet-fit/model_seed_{i}_fold_{j}.zip\")\n\n        # Y_pred = model.predict(X_test_ynishi.values)\n        # Y_pred = 1.0 / (1.0 + np.exp(-Y_pred))\n        # Y_preds[16] += counts[i * n_splits + j] * Y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = pd.DataFrame(columns=Y.columns, index=X_test.index)\n\nfor i, column in enumerate(columns):\n    Y_pred[column] = np.tensordot(weights[i], Y_preds[:, :, [i]], axes=(0, 0))\n    # Y_pred.loc[Y_pred[column] < 1e-04, column] = 0.0\n    # Y_pred.loc[Y_pred[column] > 1.0 - 1e-04, column] = 1.0\n\nY_pred[test_features[\"cp_type\"] == \"ctl_vehicle\"] = 0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred[columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred[columns].to_csv(\"submission.csv\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}