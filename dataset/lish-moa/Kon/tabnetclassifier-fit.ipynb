{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\nsys.path.append(\"../input/adabeliefoptimizer/pypi_packages/adabelief_tf0.1.0\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\nsys.path.append(\"../input/iterative-stratification/iterative-stratification-master\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\nsys.path.append(\"../input/tabnet\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pickle\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom adabelief_tf import AdaBeliefOptimizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow\n\n\ndef build_callbacks(\n    model_path, factor=0.1, mode=\"auto\", monitor=\"val_loss\", patience=0, verbose=0\n):\n    early_stopping = tf.keras.callbacks.EarlyStopping(\n        mode=mode, monitor=monitor, patience=patience, verbose=verbose\n    )\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        model_path, mode=mode, monitor=monitor, save_best_only=True, verbose=verbose\n    )\n    reduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n        factor=factor, monitor=monitor, mode=mode, verbose=verbose\n    )\n\n    return [early_stopping, model_checkpoint, reduce_lr_on_plateau]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_row_statistics(X, prefix=\"\"):\n    Xt = pd.DataFrame()\n\n    for agg_func in [\n        # \"min\",\n        # \"max\",\n        \"mean\",\n        \"std\",\n        \"kurtosis\",\n        \"skew\",\n    ]:\n        Xt[f\"{prefix}{agg_func}\"] = X.agg(agg_func, axis=1)\n\n    return Xt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n\ndef score(Y, Y_pred, eps=1e-15, label_smoothing=0.0):\n    Y = np.asarray(Y)\n    Y = np.ravel(Y)\n\n    if label_smoothing > 0.0:\n        Y = Y * (1.0 - label_smoothing) + 0.5 * label_smoothing\n\n    Y_pred = np.asarray(Y_pred)\n    Y_pred = np.ravel(Y_pred)\n    Y_pred = np.clip(Y_pred, eps, 1.0 - eps)\n\n    return -np.mean(Y * np.log(Y_pred) + (1.0 - Y) * np.log(1.0 - Y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport random as rn\n\nimport tensorflow as tf\nimport numpy as np\n\n\ndef set_seed(seed=0):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    rn.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n    graph = tf.compat.v1.get_default_graph()\n    session_conf = tf.compat.v1.ConfigProto(\n        inter_op_parallelism_threads=1, intra_op_parallelism_threads=1\n    )\n    sess = tf.compat.v1.Session(graph=graph, config=session_conf)\n\n    tf.compat.v1.keras.backend.set_session(sess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator\nfrom sklearn.base import TransformerMixin\n\n\nclass ClippedFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, copy=True, high=0.99, low=0.01):\n        self.copy = copy\n        self.high = high\n        self.low = low\n\n    def fit(self, X, y=None):\n        self.data_max_ = X.quantile(q=self.high)\n        self.data_min_ = X.quantile(q=self.low)\n\n        return self\n\n    def transform(self, X):\n        if self.copy:\n            X = X.copy()\n\n        X.clip(self.data_min_, self.data_max_, axis=1, inplace=True)\n\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://arxiv.org/abs/1905.04899\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass Cutmix(tf.keras.utils.Sequence):\n    def __init__(self, X, y=None, batch_size=32, alpha=1.0):\n        self.X = np.asarray(X)\n\n        if y is None:\n            self.y = y\n        else:\n            self.y = np.asarray(y)\n\n        self.batch_size = batch_size\n        self.alpha = alpha\n\n    def __getitem__(self, i):\n        X_batch = self.X[i * self.batch_size : (i + 1) * self.batch_size]\n\n        n_samples, n_features = self.X.shape\n        batch_size = X_batch.shape[0]\n        shuffle = np.random.choice(n_samples, batch_size)\n\n        l = np.random.beta(self.alpha, self.alpha)\n        mask = np.random.choice([0.0, 1.0], size=n_features, p=[1.0 - l, l])\n        X_shuffle = self.X[shuffle]\n        X_batch = mask * X_batch + (1.0 - mask) * X_shuffle\n\n        if self.y is None:\n            return X_batch, None\n\n        y_batch = self.y[i * self.batch_size : (i + 1) * self.batch_size]\n        y_shuffle = self.y[shuffle]\n        y_batch = l * y_batch + (1.0 - l) * y_shuffle\n\n        return X_batch, y_batch\n\n    def __len__(self):\n        n_samples = self.X.shape[0]\n\n        return int(np.ceil(n_samples / self.batch_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn.model_selection._split import _BaseKFold\n\n\nclass MultilabelStratifiedGroupKFold(_BaseKFold):\n    def __init__(self, n_splits=5, random_state=None, shuffle=False):\n        super().__init__(n_splits=n_splits, random_state=random_state, shuffle=shuffle)\n\n    def _iter_test_indices(self, X=None, y=None, groups=None):\n        cv = MultilabelStratifiedKFold(\n            n_splits=self.n_splits,\n            random_state=self.random_state,\n            shuffle=self.shuffle,\n        )\n\n        value_counts = groups.value_counts()\n        regluar_indices = value_counts.loc[\n            (value_counts == 6) | (value_counts == 12) | (value_counts == 18)\n        ].index.sort_values()\n        irregluar_indices = value_counts.loc[\n            (value_counts != 6) & (value_counts != 12) & (value_counts != 18)\n        ].index.sort_values()\n\n        group_to_fold = {}\n        tmp = y.groupby(groups).mean().loc[regluar_indices]\n\n        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n            group_to_fold.update({group: fold for group in tmp.index[test]})\n\n        sample_to_fold = {}\n        tmp = y.loc[groups.isin(irregluar_indices)]\n\n        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n            sample_to_fold.update({sample: fold for sample in tmp.index[test]})\n\n        folds = groups.map(group_to_fold)\n        is_na = folds.isna()\n        folds[is_na] = folds[is_na].index.map(sample_to_fold).values\n\n        for i in range(self.n_splits):\n            yield np.where(folds == i)[0]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# import tensorflow as tf\n# from tabnet import StackedTabNet\n\n\n# class StackedTabNetClassifier(tf.keras.Model):\n#     def __init__(\n#         self,\n#         num_classes,\n#         batch_momentum=0.98,\n#         epsilon=1e-05,\n#         feature_columns=None,\n#         feature_dim=64,\n#         norm_type=\"group\",\n#         num_decision_steps=5,\n#         num_features=None,\n#         num_groups=2,\n#         num_layers=1,\n#         output_dim=64,\n#         relaxation_factor=1.5,\n#         sparsity_coefficient=1e-05,\n#         virtual_batch_size=None,\n#         **kwargs\n#     ):\n#         super().__init__(**kwargs)\n\n#         self.stacked_tabnet = StackedTabNet(\n#             feature_columns,\n#             batch_momentum=batch_momentum,\n#             epsilon=epsilon,\n#             feature_dim=feature_dim,\n#             norm_type=norm_type,\n#             num_decision_steps=num_decision_steps,\n#             num_features=num_features,\n#             num_groups=num_groups,\n#             num_layers=num_layers,\n#             output_dim=output_dim,\n#             relaxation_factor=relaxation_factor,\n#             sparsity_coefficient=sparsity_coefficient,\n#             virtual_batch_size=virtual_batch_size,\n#         )\n\n#         self.classifier = tf.keras.layers.Dense(\n#             num_classes, activation=\"sigmoid\", use_bias=False\n#         )\n\n#     def call(self, inputs, training=None):\n#         x = self.stacked_tabnet(inputs, training=training)\n\n#         return self.classifier(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tabnet import TabNet\n\n\nclass TabNetClassifier(tf.keras.Model):\n    def __init__(\n        self,\n        num_classes,\n        batch_momentum=0.98,\n        epsilon=1e-05,\n        feature_columns=None,\n        feature_dim=64,\n        norm_type=\"group\",\n        num_decision_steps=5,\n        num_features=None,\n        num_groups=1,\n        output_dim=64,\n        relaxation_factor=1.5,\n        sparsity_coefficient=1e-05,\n        virtual_batch_size=None,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        self.tabnet = TabNet(\n            feature_columns,\n            batch_momentum=batch_momentum,\n            epsilon=epsilon,\n            feature_dim=feature_dim,\n            norm_type=norm_type,\n            num_decision_steps=num_decision_steps,\n            num_features=num_features,\n            num_groups=num_groups,\n            output_dim=output_dim,\n            relaxation_factor=relaxation_factor,\n            sparsity_coefficient=sparsity_coefficient,\n            virtual_batch_size=virtual_batch_size,\n            **kwargs\n        )\n\n        self.classifier = tf.keras.layers.Dense(\n            num_classes, activation=\"sigmoid\", use_bias=False\n        )\n\n    def call(self, inputs, training=None):\n        x = self.tabnet(inputs, training=training)\n\n        return self.classifier(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtype = {\"cp_type\": \"category\", \"cp_dose\": \"category\"}\nindex_col = \"sig_id\"\n\ntrain_features = pd.read_csv(\n    \"../input/lish-moa/train_features.csv\", dtype=dtype, index_col=index_col\n)\nX = train_features.select_dtypes(\"number\")\nY = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\", index_col=index_col)\ngroups = pd.read_csv(\n    \"../input/lish-moa/train_drug.csv\", index_col=index_col, squeeze=True\n)\n\ncolumns = Y.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_prefix = \"c-\"\ng_prefix = \"g-\"\nc_columns = X.columns.str.startswith(c_prefix)\ng_columns = X.columns.str.startswith(g_prefix)\nX_stats_c = compute_row_statistics(X.loc[:, c_columns], prefix=c_prefix)\nX_stats_g = compute_row_statistics(X.loc[:, g_columns], prefix=g_prefix)\n\nwith open(\"../input/preprocessor-fit/clipped_features.pkl\", \"rb\") as f:\n    clipped_features = pickle.load(f)\n\nX = clipped_features.transform(X)\n\nX = pd.concat([X, X_stats_c, X_stats_g], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size, n_features = X.shape\n_, n_classes = Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyperparameters\nalpha = 4.0\nbatch_size = 8\nfactor = 0.5\nlabel_smoothing = 1e-03\nlr = 0.001\nn_seeds = 5\nn_splits = 5\npatience = 30\nshuffle = True\nparams = {\n    \"batch_momentum\": 0.95,\n    \"feature_dim\": 512,\n    \"norm_type\": \"batch\",\n    \"num_decision_steps\": 1,\n}\nfit_params = {\"epochs\": 1_000, \"verbose\": 0}\n\nwith open(\"params.pkl\", \"wb\") as f:\n    pickle.dump(params, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nY_pred = np.zeros((train_size, n_classes))\nY_pred = pd.DataFrame(Y_pred, columns=Y.columns, index=Y.index)\n\nfor i in range(n_seeds):\n    set_seed(seed=i)\n\n    cv = MultilabelStratifiedGroupKFold(\n        n_splits=n_splits, random_state=i, shuffle=shuffle\n    )\n\n    for j, (train, valid) in enumerate(cv.split(X, Y[columns], groups)):\n        model_path = f\"model_seed_{i}_fold_{j}.h5\"\n\n        model = TabNetClassifier(\n            num_classes=n_classes, num_features=n_features, **params\n        )\n        loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing)\n        optimizer = AdaBeliefOptimizer(learning_rate=lr)\n\n        model.compile(loss=loss, optimizer=optimizer)\n\n        generator = Cutmix(\n            X.iloc[train], Y.iloc[train], alpha=alpha, batch_size=batch_size\n        )\n        callbacks = build_callbacks(model_path, factor=factor, patience=patience)\n        history = model.fit(\n            generator,\n            callbacks=callbacks,\n            validation_data=(X.iloc[valid], Y.iloc[valid]),\n            **fit_params,\n        )\n\n        model.load_weights(model_path)\n\n        Y_pred.iloc[valid] += model.predict(X.iloc[valid]) / n_seeds\n\nY_pred[train_features[\"cp_type\"] == \"ctl_vehicle\"] = 0.0\n\nwith open(\"Y_pred.pkl\", \"wb\") as f:\n    pickle.dump(Y_pred[columns], f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score(Y[columns], Y_pred[columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# b 0.016500749844765163: vanilla\n# - 0.016497676039512712: lr=0.001\n# + 0.016648792505992370: lr=0.01\n# + 0.016514646176541175: lr=0.0003\n# + 0.016576238532578630: label_smoothing=1e-04\n# - 0.016459734021787725: label_smoothing=1e-03\n# + 0.016714446685325107: label_smoothing=3e-03\n# + 0.016595114418360840: batch_size=64\n# - 0.016416163897717113: batch_size=16\n# - 0.016376464826077570: batch_size=8\n# - 0.016264005364634404: norm_type=\"batch\"\n# + 0.016288912869945646: batch_size=16\n# + 0.016583924007269300: batch_size=4\n# - 0.016212486949183844: feature_dim=256\n# - 0.016182858549420427: feature_dim=512\n# + 0.016206945018426098: feature_dim=1024\n# + 0.016326808537440018: output_dim=128\n# + 0.016206255579305613: output_dim=32\n# + 0.021095031830998465: n_decsion_steps=2\n# + 0.016182858549420427: relaxation_factor=1.0\n# + 0.016182858549420427: relaxation_factor=0.5\n# - 0.016146143023995250: cutmix(alpha=1.0)\n# + 0.016154592789026014: fixed_cutmix(alpha=1.0)\n# - 0.016133952316455563: batch_momentum=0.95\n# + 0.016154628705148800: batch_momentum=0.9\n# + 0.016151426430633047: batch_momentum=0.9, remove rowstatistics\n# - 0.016101229987361220: remove rowstatistics\n# + 0.016101229987361220: sparsity_coefficient=1e-04\n# + 0.016186363339304583: batch_size=128\n# + 0.016233864670764358: cutmix(alpha=0.5)\n# - 0.016064267354577305: cutmix(alpha=2.0)\n# - 0.015954724221242774: cutmix(alpha=4.0)\n# + 0.015986972892065680: cutmix(alpha=8.0)\n# b 0.015992065599939043: remove additional targets\n# + 0.017293994271024533: remove additional targets, feature_dim=8, output_dim=4\n# + 0.016365092229987350: remove additional targets, feature_dim=16, output_dim=8\n# + 0.016064325148839542: remove additional targets, feature_dim=32, output_dim=16\n# + 0.016007137125710537: use_bias=True\n# + 0.016130240781653944: apply weight normalization to classifier\n# + 0.015992407110539293: rowstatistics -> clipped_features -> concat","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}