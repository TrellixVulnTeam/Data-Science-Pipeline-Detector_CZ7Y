{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\nsys.path.append(\"../input/adabeliefoptimizer/pypi_packages/adabelief_tf0.1.0\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\nsys.path.append(\"../input/iterative-stratification/iterative-stratification-master\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pickle\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow\n\n\ndef build_callbacks(\n    model_path, factor=0.1, mode=\"auto\", monitor=\"val_loss\", patience=0, verbose=0\n):\n    early_stopping = tf.keras.callbacks.EarlyStopping(\n        mode=mode, monitor=monitor, patience=patience, verbose=verbose\n    )\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        model_path, mode=mode, monitor=monitor, save_best_only=True, verbose=verbose\n    )\n    reduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n        factor=factor, monitor=monitor, mode=mode, verbose=verbose\n    )\n\n    return [early_stopping, model_checkpoint, reduce_lr_on_plateau]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_addons as tfa\nfrom adabelief_tf import AdaBeliefOptimizer\n\n\ndef build_mlp_classifier(\n    input_dim,\n    output_dim,\n    activation=\"relu\",\n    bias_initializer=\"zeros\",\n    kernel_initializer=\"glorot_uniform\",\n    label_smoothing=0.0,\n    momentum=0.99,\n    n_layers=3,\n    n_units=\"auto\",\n    pretrained_model_path=None,\n    optimizer_params=None,\n    rate=0.0,\n    skip=False,\n):\n    if n_units == \"auto\":\n        n_units = 0.5 * (input_dim + output_dim)\n\n    inputs = tf.keras.layers.Input(shape=input_dim, name=\"I\")\n\n    x = inputs\n\n    for i in range(n_layers - 2):\n        x = tfa.layers.WeightNormalization(\n            tf.keras.layers.Dense(n_units, kernel_initializer=kernel_initializer),\n            name=f\"WN{i + 1}\",\n        )(x)\n        x = tf.keras.layers.BatchNormalization(momentum=momentum, name=f\"BN{i + 1}\")(x)\n\n        if skip and i > 0 and i % 2 == 0:\n            x = x + shortcut\n\n        x = tf.keras.layers.Activation(activation, name=f\"A{i + 1}\")(x)\n        x = tf.keras.layers.Dropout(rate, name=f\"D{i + 1}\")(x)\n\n        if skip and i % 2 == 0:\n            shortcut = x\n\n    x = tfa.layers.WeightNormalization(\n        tf.keras.layers.Dense(output_dim, bias_initializer=bias_initializer)\n    )(x)\n\n    outputs = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n\n    if optimizer_params is None:\n        optimizer_params = {}\n\n    if pretrained_model_path is not None:\n        model.load_weights(pretrained_model_path, by_name=True)\n\n    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing)\n    optimizer = AdaBeliefOptimizer(**optimizer_params)\n\n    model.compile(loss=loss, optimizer=optimizer)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_row_statistics(X, prefix=\"\"):\n    Xt = pd.DataFrame()\n\n    for agg_func in [\n        # \"min\",\n        # \"max\",\n        \"mean\",\n        \"std\",\n        \"kurtosis\",\n        \"skew\",\n    ]:\n        Xt[f\"{prefix}{agg_func}\"] = X.agg(agg_func, axis=1)\n\n    return Xt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n\ndef score(Y, Y_pred, eps=1e-15, label_smoothing=0.0):\n    Y = np.asarray(Y)\n    Y = np.ravel(Y)\n\n    if label_smoothing > 0.0:\n        Y = Y * (1.0 - label_smoothing) + 0.5 * label_smoothing\n\n    Y_pred = np.asarray(Y_pred)\n    Y_pred = np.ravel(Y_pred)\n    Y_pred = np.clip(Y_pred, eps, 1.0 - eps)\n\n    return -np.mean(Y * np.log(Y_pred) + (1.0 - Y) * np.log(1.0 - Y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport random as rn\n\nimport tensorflow as tf\nimport numpy as np\n\n\ndef set_seed(seed=0):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    rn.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n    graph = tf.compat.v1.get_default_graph()\n    session_conf = tf.compat.v1.ConfigProto(\n        inter_op_parallelism_threads=1, intra_op_parallelism_threads=1\n    )\n    sess = tf.compat.v1.Session(graph=graph, config=session_conf)\n\n    tf.compat.v1.keras.backend.set_session(sess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator\nfrom sklearn.base import TransformerMixin\n\n\nclass ClippedFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, copy=True, high=0.99, low=0.01):\n        self.copy = copy\n        self.high = high\n        self.low = low\n\n    def fit(self, X, y=None):\n        self.data_max_ = X.quantile(q=self.high)\n        self.data_min_ = X.quantile(q=self.low)\n\n        return self\n\n    def transform(self, X):\n        if self.copy:\n            X = X.copy()\n\n        X.clip(self.data_min_, self.data_max_, axis=1, inplace=True)\n\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# https://arxiv.org/abs/1905.04899\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass Cutmix(tf.keras.utils.Sequence):\n    def __init__(self, X, y=None, batch_size=32, alpha=1.0):\n        self.X = np.asarray(X)\n\n        if y is None:\n            self.y = y\n        else:\n            self.y = np.asarray(y)\n\n        self.batch_size = batch_size\n        self.alpha = alpha\n\n    def __getitem__(self, i):\n        X_batch = self.X[i * self.batch_size : (i + 1) * self.batch_size]\n\n        n_samples, n_features = self.X.shape\n        batch_size = X_batch.shape[0]\n        shuffle = np.random.choice(n_samples, batch_size)\n\n        l = np.random.beta(self.alpha, self.alpha)\n        mask = np.random.choice([0.0, 1.0], size=n_features, p=[1.0 - l, l])\n        X_shuffle = self.X[shuffle]\n        X_batch = mask * X_batch + (1.0 - mask) * X_shuffle\n\n        if self.y is None:\n            return X_batch, None\n\n        y_batch = self.y[i * self.batch_size : (i + 1) * self.batch_size]\n        y_shuffle = self.y[shuffle]\n        y_batch = l * y_batch + (1.0 - l) * y_shuffle\n\n        return X_batch, y_batch\n\n    def __len__(self):\n        n_samples = self.X.shape[0]\n\n        return int(np.ceil(n_samples / self.batch_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn.model_selection._split import _BaseKFold\n\n\nclass MultilabelStratifiedGroupKFold(_BaseKFold):\n    def __init__(self, n_splits=5, random_state=None, shuffle=False):\n        super().__init__(n_splits=n_splits, random_state=random_state, shuffle=shuffle)\n\n    def _iter_test_indices(self, X=None, y=None, groups=None):\n        cv = MultilabelStratifiedKFold(\n            n_splits=self.n_splits,\n            random_state=self.random_state,\n            shuffle=self.shuffle,\n        )\n\n        value_counts = groups.value_counts()\n        regluar_indices = value_counts.loc[\n            (value_counts == 6) | (value_counts == 12) | (value_counts == 18)\n        ].index.sort_values()\n        irregluar_indices = value_counts.loc[\n            (value_counts != 6) & (value_counts != 12) & (value_counts != 18)\n        ].index.sort_values()\n\n        group_to_fold = {}\n        tmp = y.groupby(groups).mean().loc[regluar_indices]\n\n        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n            group_to_fold.update({group: fold for group in tmp.index[test]})\n\n        sample_to_fold = {}\n        tmp = y.loc[groups.isin(irregluar_indices)]\n\n        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n            sample_to_fold.update({sample: fold for sample in tmp.index[test]})\n\n        folds = groups.map(group_to_fold)\n        is_na = folds.isna()\n        folds[is_na] = folds[is_na].index.map(sample_to_fold).values\n\n        for i in range(self.n_splits):\n            yield np.where(folds == i)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtype = {\"cp_type\": \"category\", \"cp_dose\": \"category\"}\nindex_col = \"sig_id\"\n\ntrain_features = pd.read_csv(\n    \"../input/lish-moa/train_features.csv\", dtype=dtype, index_col=index_col\n)\nX = train_features.select_dtypes(\"number\")\nY = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\", index_col=index_col)\ngroups = pd.read_csv(\n    \"../input/lish-moa/train_drug.csv\", index_col=index_col, squeeze=True\n)\n\ncolumns = Y.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_prefix = \"c-\"\ng_prefix = \"g-\"\nc_columns = X.columns.str.startswith(c_prefix)\ng_columns = X.columns.str.startswith(g_prefix)\nX_stats_c = compute_row_statistics(X.loc[:, c_columns], prefix=c_prefix)\nX_stats_g = compute_row_statistics(X.loc[:, g_columns], prefix=g_prefix)\n\nwith open(\"../input/preprocessor-fit/clipped_features.pkl\", \"rb\") as f:\n    clipped_features = pickle.load(f)\n\nX = clipped_features.transform(X)\n\nX = pd.concat([X, X_stats_c, X_stats_g], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size, n_features = X.shape\n_, n_classes = Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyperparameters\nalpha = 4.0\nbatch_size = 32\nfactor = 0.5\nn_seeds = 5\nn_splits = 5\npatience = 30\nshuffle = True\nparams = {\n    \"activation\": \"elu\",\n    \"kernel_initializer\": \"he_normal\",\n    \"label_smoothing\": 5e-04,\n    \"n_layers\": 7,\n    \"n_units\": 256,\n    \"optimizer_params\": {\"beta_1\": 0.85, \"lr\": 0.03},\n    \"rate\": 0.3,\n    \"skip\": True,\n}\nfit_params = {\"epochs\": 1_000, \"verbose\": 0}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nbias_initializer = -Y.mean(axis=0).apply(np.log).values\nbias_initializer = tf.keras.initializers.Constant(bias_initializer)\n\nY_pred = np.zeros((train_size, n_classes))\nY_pred = pd.DataFrame(Y_pred, columns=Y.columns, index=Y.index)\n\nfor i in range(n_seeds):\n    set_seed(seed=i)\n\n    cv = MultilabelStratifiedGroupKFold(\n        n_splits=n_splits, random_state=i, shuffle=shuffle\n    )\n\n    for j, (train, valid) in enumerate(cv.split(X, Y[columns], groups)):\n        model_path = f\"model_seed_{i}_fold_{j}.h5\"\n\n        model = build_mlp_classifier(\n            n_features,\n            n_classes,\n            bias_initializer=bias_initializer,\n            **params,\n        )\n\n        generator = Cutmix(\n            X.iloc[train], Y.iloc[train], alpha=alpha, batch_size=batch_size\n        )\n        callbacks = build_callbacks(model_path, factor=factor, patience=patience)\n        history = model.fit(\n            generator,\n            callbacks=callbacks,\n            validation_data=(X.iloc[valid], Y.iloc[valid]),\n            **fit_params,\n        )\n\n        model.load_weights(model_path)\n\n        Y_pred.iloc[valid] += model.predict(X.iloc[valid]) / n_seeds\n\nY_pred[train_features[\"cp_type\"] == \"ctl_vehicle\"] = 0.0\n\nwith open(\"Y_pred.pkl\", \"wb\") as f:\n    pickle.dump(Y_pred[columns], f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score(Y[columns], Y_pred[columns])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# b 0.015994246952747954: vanilla\n# + 0.016008211277595052: batch_momentum=0.98\n# - 0.015810368099750786: cutmix(alpha=1.0)\n# - 0.015788791741863668: remove first dropout layer, cutmix(alpha=1.0)\n# + 0.016917430610109030: test-time augmentation (cutmix + gaussian noise)\n# - 0.015782911285632180: remove rowstatistics\n# + 0.016236778363784050: add quantiletransformer\n# - 0.015780689018469560: stddev=0.4\n# + 0.015798678302832290: stddev=0.35\n# + 0.015848971743793760: cutmix(alpha=0.5)\n# + 0.015795163474415612: cutmix(alpha=2.0)\n# - 0.015747588913558980: rate=0.3\n# + 0.015759213663024532: rate=0.35\n# + 0.015802250529421566: remove ctl_vehicle from train & valid\n# + 0.015824030991552742: remove ctl_vehicle from train & valid, stddev=0.4*std\n# + 0.016150020012321364: stddev=0.0, remove cutmix\n# + 0.015784536333436300: stddev=0.0\n# + 0.015789385925020200: stddev=0.0, add cp_type & cp_dose\n# + 0.015764240674257066: stddev=0.0, cutmix(alpha=2.0)\n# - 0.015732600356490125: stddev=0.0, cutmix(alpha=4.0)\n# + 0.015793021822842125: stddev=0.0, cutmix(alpha=8.0)\n# + 0.015751460390131804: stddev=0.1*std, stddev[\"cptime\"] = 0.0, cutmix(alpha=4.0)\n# + 0.015761022293541458: stddev=0.1*std, cutmix(alpha=4.0)\n# + 0.015758034499447827: stddev=0.2*std, cutmix(alpha=4.0)\n# + 0.015867841830930873: add quantiletransformer(n_quantiles=100)\n# + 0.015797839642251572: add absolute features\n# - 0.015708352883603460: remove additional targets\n# + 0.015832151243559488: add c-square features\n# b 0.015708352883603467: remove additional targets\n# + 0.015819876183296120: remove clipped features, add quantiletransformer(n_quantiles=100), fit(train+test)\n# + 0.015807305230095343: quantiletransformer(n_quantiles=100) -> clipped features, fit(train+test)\n# + 0.015764452580356664: add additional targets\n# - 0.015705591305375046: rowstatistics -> clipped_features -> concat\n# + 0.015716916509029560: add only highly correlated additional targets\n# + 0.015708715240629875: beta_1=0.75\n# + 0.015710850169124463: beta_1=0.8\n# - 0.015679506881320745: beta_1=0.85","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}