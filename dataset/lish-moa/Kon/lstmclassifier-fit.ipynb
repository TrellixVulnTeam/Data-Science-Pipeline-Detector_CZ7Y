{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\nsys.path.append(\"../input/adabeliefoptimizer/pypi_packages/adabelief_tf0.1.0\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\nsys.path.append(\"../input/iterative-stratification/iterative-stratification-master\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pickle\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow\n\n\ndef build_callbacks(\n    model_path, factor=0.1, mode=\"auto\", monitor=\"val_loss\", patience=0, verbose=0\n):\n    early_stopping = tf.keras.callbacks.EarlyStopping(\n        mode=mode, monitor=monitor, patience=patience, verbose=verbose\n    )\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        model_path, mode=mode, monitor=monitor, save_best_only=True, verbose=verbose\n    )\n    reduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n        factor=factor, monitor=monitor, mode=mode, verbose=verbose\n    )\n\n    return [early_stopping, model_checkpoint, reduce_lr_on_plateau]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_addons as tfa\nfrom adabelief_tf import AdaBeliefOptimizer\n\n\ndef build_lstm_classifier(\n    input_dim,\n    output_dim,\n    bias_initializer=\"zeros\",\n    label_smoothing=0.0,\n    lr=1e-03,\n    momentum=0.99,\n    n_layers=3,\n    n_units=\"auto\",\n    pretrained_model_path=None,\n    rate=0.0,\n):\n    if n_units == \"auto\":\n        n_units = 0.5 * (input_dim + output_dim)\n\n    inputs = tf.keras.layers.Input(shape=input_dim)\n\n    x = inputs\n    x = tf.keras.layers.Reshape((1, input_dim))(x)\n\n    initial_state = None\n\n    for i in range(n_layers - 3):\n        x, h, c = tf.keras.layers.LSTM(\n            n_units, dropout=rate, return_sequences=True, return_state=True\n        )(x, initial_state=initial_state)\n\n        initial_state = [h, c]\n\n    x = tf.keras.layers.LSTM(n_units, dropout=rate)(x, initial_state=initial_state)\n    x = tf.keras.layers.BatchNormalization(momentum=momentum)(x)\n    x = tfa.layers.WeightNormalization(\n        tf.keras.layers.Dense(output_dim, bias_initializer=bias_initializer)\n    )(x)\n\n    outputs = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n\n    if pretrained_model_path is not None:\n        model.load_weights(pretrained_model_path, by_name=True)\n\n    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing)\n    optimizer = AdaBeliefOptimizer(learning_rate=lr)\n\n    model.compile(loss=loss, optimizer=optimizer)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n\ndef compute_row_statistics(X, prefix=\"\"):\n    Xt = pd.DataFrame()\n\n    for agg_func in [\n        # \"min\",\n        # \"max\",\n        \"mean\",\n        \"std\",\n        \"kurtosis\",\n        \"skew\",\n    ]:\n        Xt[f\"{prefix}{agg_func}\"] = X.agg(agg_func, axis=1)\n\n    return Xt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n\ndef score(Y, Y_pred, eps=1e-15, label_smoothing=0.0):\n    Y = np.asarray(Y)\n    Y = np.ravel(Y)\n\n    if label_smoothing > 0.0:\n        Y = Y * (1.0 - label_smoothing) + 0.5 * label_smoothing\n\n    Y_pred = np.asarray(Y_pred)\n    Y_pred = np.ravel(Y_pred)\n    Y_pred = np.clip(Y_pred, eps, 1.0 - eps)\n\n    return -np.mean(Y * np.log(Y_pred) + (1.0 - Y) * np.log(1.0 - Y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport random as rn\n\nimport tensorflow as tf\nimport numpy as np\n\n\ndef set_seed(seed=0):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    rn.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n    graph = tf.compat.v1.get_default_graph()\n    session_conf = tf.compat.v1.ConfigProto(\n        inter_op_parallelism_threads=1, intra_op_parallelism_threads=1\n    )\n    sess = tf.compat.v1.Session(graph=graph, config=session_conf)\n\n    tf.compat.v1.keras.backend.set_session(sess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator\nfrom sklearn.base import TransformerMixin\n\n\nclass ClippedFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, copy=True, high=0.99, low=0.01):\n        self.copy = copy\n        self.high = high\n        self.low = low\n\n    def fit(self, X, y=None):\n        self.data_max_ = X.quantile(q=self.high)\n        self.data_min_ = X.quantile(q=self.low)\n\n        return self\n\n    def transform(self, X):\n        if self.copy:\n            X = X.copy()\n\n        X.clip(self.data_min_, self.data_max_, axis=1, inplace=True)\n\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# https://arxiv.org/abs/1905.04899\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass Cutmix(tf.keras.utils.Sequence):\n    def __init__(self, X, y=None, batch_size=32, alpha=1.0):\n        self.X = np.asarray(X)\n\n        if y is None:\n            self.y = y\n        else:\n            self.y = np.asarray(y)\n\n        self.batch_size = batch_size\n        self.alpha = alpha\n\n    def __getitem__(self, i):\n        X_batch = self.X[i * self.batch_size : (i + 1) * self.batch_size]\n\n        n_samples, n_features = self.X.shape\n        batch_size = X_batch.shape[0]\n        shuffle = np.random.choice(n_samples, batch_size)\n\n        l = np.random.beta(self.alpha, self.alpha)\n        mask = np.random.choice([0.0, 1.0], size=n_features, p=[1.0 - l, l])\n        X_shuffle = self.X[shuffle]\n        X_batch = mask * X_batch + (1.0 - mask) * X_shuffle\n\n        if self.y is None:\n            return X_batch, None\n\n        y_batch = self.y[i * self.batch_size : (i + 1) * self.batch_size]\n        y_shuffle = self.y[shuffle]\n        y_batch = l * y_batch + (1.0 - l) * y_shuffle\n\n        return X_batch, y_batch\n\n    def __len__(self):\n        n_samples = self.X.shape[0]\n\n        return int(np.ceil(n_samples / self.batch_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn.model_selection._split import _BaseKFold\n\n\nclass MultilabelStratifiedGroupKFold(_BaseKFold):\n    def __init__(self, n_splits=5, random_state=None, shuffle=False):\n        super().__init__(n_splits=n_splits, random_state=random_state, shuffle=shuffle)\n\n    def _iter_test_indices(self, X=None, y=None, groups=None):\n        cv = MultilabelStratifiedKFold(\n            n_splits=self.n_splits,\n            random_state=self.random_state,\n            shuffle=self.shuffle,\n        )\n\n        value_counts = groups.value_counts()\n        regluar_indices = value_counts.loc[\n            (value_counts == 6) | (value_counts == 12) | (value_counts == 18)\n        ].index.sort_values()\n        irregluar_indices = value_counts.loc[\n            (value_counts != 6) & (value_counts != 12) & (value_counts != 18)\n        ].index.sort_values()\n\n        group_to_fold = {}\n        tmp = y.groupby(groups).mean().loc[regluar_indices]\n\n        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n            group_to_fold.update({group: fold for group in tmp.index[test]})\n\n        sample_to_fold = {}\n        tmp = y.loc[groups.isin(irregluar_indices)]\n\n        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n            sample_to_fold.update({sample: fold for sample in tmp.index[test]})\n\n        folds = groups.map(group_to_fold)\n        is_na = folds.isna()\n        folds[is_na] = folds[is_na].index.map(sample_to_fold).values\n\n        for i in range(self.n_splits):\n            yield np.where(folds == i)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtype = {\"cp_type\": \"category\", \"cp_dose\": \"category\"}\nindex_col = \"sig_id\"\n\ntrain_features = pd.read_csv(\n    \"../input/lish-moa/train_features.csv\", dtype=dtype, index_col=index_col\n)\nX = train_features.select_dtypes(\"number\")\nY = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\", index_col=index_col)\ngroups = pd.read_csv(\n    \"../input/lish-moa/train_drug.csv\", index_col=index_col, squeeze=True\n)\n\ncolumns = Y.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_prefix = \"c-\"\ng_prefix = \"g-\"\nc_columns = X.columns.str.startswith(c_prefix)\ng_columns = X.columns.str.startswith(g_prefix)\nX_stats_c = compute_row_statistics(X.loc[:, c_columns], prefix=c_prefix)\nX_stats_g = compute_row_statistics(X.loc[:, g_columns], prefix=g_prefix)\n\nwith open(\"../input/preprocessor-fit/clipped_features.pkl\", \"rb\") as f:\n    clipped_features = pickle.load(f)\n\nX = clipped_features.transform(X)\n\nX = pd.concat([X, X_stats_c, X_stats_g], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size, n_features = X.shape\n_, n_classes = Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyperparameters\nalpha = 16.0\nbatch_size = 128\nfactor = 0.5\nn_seeds = 5\nn_splits = 5\npatience = 30\nshuffle = True\nparams = {\n    \"label_smoothing\": 1e-03,\n    \"lr\": 0.03,\n    \"n_layers\": 6,\n    \"n_units\": 256,\n    \"rate\": 0.3,\n}\nfit_params = {\"epochs\": 1_000, \"verbose\": 0}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nbias_initializer = -Y.mean(axis=0).apply(np.log).values\nbias_initializer = tf.keras.initializers.Constant(bias_initializer)\n\nY_pred = np.zeros((train_size, n_classes))\nY_pred = pd.DataFrame(Y_pred, columns=Y.columns, index=Y.index)\n\nfor i in range(n_seeds):\n    set_seed(seed=i)\n\n    cv = MultilabelStratifiedGroupKFold(\n        n_splits=n_splits, random_state=i, shuffle=shuffle\n    )\n\n    for j, (train, valid) in enumerate(cv.split(X, Y[columns], groups)):\n        model_path = f\"model_seed_{i}_fold_{j}.h5\"\n\n        model = build_lstm_classifier(\n            n_features,\n            n_classes,\n            bias_initializer=bias_initializer,\n            **params,\n        )\n\n        generator = Cutmix(\n            X.iloc[train], Y.iloc[train], alpha=alpha, batch_size=batch_size\n        )\n        callbacks = build_callbacks(model_path, factor=factor, patience=patience)\n        history = model.fit(\n            generator,\n            callbacks=callbacks,\n            validation_data=(X.iloc[valid], Y.iloc[valid]),\n            **fit_params,\n        )\n\n        model.load_weights(model_path)\n\n        Y_pred.iloc[valid] += model.predict(X.iloc[valid]) / n_seeds\n\nY_pred[train_features[\"cp_type\"] == \"ctl_vehicle\"] = 0.0\n\nwith open(\"Y_pred.pkl\", \"wb\") as f:\n    pickle.dump(Y_pred[columns], f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score(Y[columns], Y_pred[columns])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# b 0.016535478855028290: vanilla\n# + 0.016583661311213683: batch_size=256\n# + 0.016554469669890960: batch_size=64\n# + 0.016614357198732378: alpha=2.0\n# - 0.016517857278749600: alpha=8.0\n# - 0.016480213755700985: alpha=16.0\n# + 0.016490287447389285: alpha=32.0\n# - 0.016445262957650890: n_units=512\n# - 0.016442914855588953: n_units=256\n# + 0.016481135781868165: n_units=128\n# - 0.016408826234988187: rate=0.35\n# - 0.016389895312250647: rate=0.3\n# + 0.016419288531026564: rate=0.25\n# - 0.016385561737988254: n_layers=5\n# - 0.016337023086601830: n_layers=6\n# + 0.016368735571229004: n_layers=7\n# + 0.016356041026459045: n_layers=8\n# + 0.016338646247728454: momentum=0.98\n# + 0.016373313365994645: lr=0.01\n# + 0.016598675549378180: lr=0.1\n# + 0.016466978587419256: label_smoothing=1e-04\n# - 0.016318224000278620: label_smoothing=1e-03\n# + 0.016623421282517090: label_smoothing=3e-03","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}