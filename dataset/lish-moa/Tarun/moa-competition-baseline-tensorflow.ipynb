{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Mechanisms of Action (MoA) Prediction"},{"metadata":{},"cell_type":"markdown","source":"## importing necessary libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport tensorflow as tf\nimport sklearn.model_selection as ms\nimport sklearn.preprocessing as prep\nimport seaborn as sns\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nimport sklearn.metrics as metric","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Helper Classes and Functions\n\n- CrossValidation: Splitting data into training and validation sets\n- EncodeCategories: Encode categorical variables\n- TFSimpleDataset: Create tfdataset input pipeline from data\n- write_submission: Write submission dataframe"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class CrossValidation:\n    def __init__(self, df, target_cols, shuffle,random_state=0):\n        self.df = df\n        self.target_cols = target_cols\n        self.random_state = 0\n        if shuffle is True:\n            self.df = df.sample(frac=1).reset_index(drop=True)\n\n    def hold_out_split(self,percent,stratify=True):\n        if stratify:\n            y = self.df[self.target_cols]\n            train,val = ms.train_test_split(self.df,test_size=percent/100,\n                stratify=y, random_state=self.random_state)\n            return train,val\n        size = len(self.df) - int(len(self.df)*(percent/100))\n        train = self.df.iloc[:size,:]\n        val = self.df.iloc[size:,:]\n        return train,val\n\n    def kfold_split(self, splits, stratify=None):\n        if stratify is not None:\n            if type('stratify')==\"str\":\n                kf = ms.StratifiedKFold(n_splits=splits)\n                y = self.df[stratify].values\n                for train, val in kf.split(X=self.df,y=y):\n                    t = self.df.iloc[train,:]\n                    v = self.df.iloc[val, :]\n                    yield t,v\n            else:\n                kf = MultilabelStratifiedKFold(n_splits=splits)\n                y = self.df[stratify].values\n                for train, val in kf.split(X=self.df,y=y):\n                    t = self.df.iloc[train,:]\n                    v = self.df.iloc[val, :]\n                    yield t,v\n        else:\n            kf = ms.KFold(n_splits=splits)\n            for train, val in kf.split(X=self.df):\n                t = self.df.iloc[train,:]\n                v = self.df.iloc[val, :]\n                yield t,v","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class EncodeCategories:\n    def __init__(self, df, encode_cols, encoding_type, \n        handle_na=False, na_placeholder=\"NaN\"):\n        self.df = df\n        self.encode_cols = encode_cols\n        self.encoding_type = encoding_type\n        self.handle_na = handle_na\n\n        self.label_encoders = {}\n        self.binary_encoders = {}\n        self.one_hot_encoder = None\n        self.na_placeholder = na_placeholder\n\n        if self.handle_na:\n            self.df = self.__handle_missing_category(self.df, \n                placeholder=na_placeholder)\n\n    def __handle_missing_category(self, df, placeholder=\"NaN\"):\n        for cat in self.encode_cols:\n            df.loc[:, cat] = df.loc[:, cat].astype(str).fillna(placeholder)\n        return df\n\n    def __label_encoder_fit(self,df, cat):\n        le = prep.LabelEncoder()\n        le.fit(self.df[cat].values)\n        self.label_encoders[cat] = le\n\n    def __label_encoder_transform(self,df, cat):\n        return self.label_encoders[cat].transform(df[cat].values)\n\n    def __binary_encoder_fit(self,df, cat):\n        lbl = prep.LabelBinarizer()\n        lbl.fit(self.df[cat].values)\n        self.binary_encoders[cat] = lbl\n\n    def __binary_encoder_transform(self,df, cat):\n        return self.binary_encoders[cat].transform(df[cat].values)\n\n    def __one_hot_fit(self,df, sparse=False):\n        ohe = prep.OneHotEncoder(sparse=sparse)\n        ohe.fit(self.df[self.encode_cols].values)\n        self.one_hot_encoder = ohe\n\n    def __one_hot_transform(self,df, cat):\n        return self.one_hot_encoder.transform(df[cat].values)\n\n    def __label_encoder(self, df, fit=True):\n        for cat in self.encode_cols:\n            if fit:\n                self.__label_encoder_fit(df,cat)\n            df.loc[:,cat] = self.__label_encoder_transform(df,cat)\n        return df\n\n    def __binary_encoder(self, df, fit=True):\n        for cat in self.encode_cols:\n            if fit:\n                self.__binary_encoder_fit(df,cat)\n            val = self.__binary_encoder_transform(df, cat)\n            df = df.drop(cat, axis=1)\n            for i in range(val.shape[1]):\n                new_col_name = f\"{cat}_bin_{i}\"\n                df[new_col_name] = val[:, i]\n        return df\n\n    def __one_hot_encoder(self, df, sparse=False, fit=True):\n        if fit:\n            self.__one_hot_fit(df, sparse)\n        val = self.__one_hot_transform(df, self.encode_cols)\n        for cat in self.encode_cols:\n            df = df.drop(cat, axis=1)\n            for i in range(val.shape[1]):\n                new_col_name = f\"{cat}_ohe_{i}\"\n                df[new_col_name] = val[:, i]\n        return df\n\n    def fit(self):\n        if self.encoding_type == \"label\":\n            for cat in self.encode_cols:\n                self.__label_encoder_fit(self.df,cat)\n        elif self.encoding_type == \"binary\":\n            for cat in self.encode_cols:\n                self.__binary_encoder_fit(self.df,cat)\n        elif self.encoding_type == \"onehot\":\n            self.__one_hot_fit(self.df, False)\n        elif self.encoding_type == \"onehot_sparse\":\n            self.__one_hot_fit(self.df, True)\n        else:\n            raise Exception(\"specified encoding type not defined\")\n\n    def fit_transform(self):\n        df = self.df.copy(deep=True)\n        if self.encoding_type == \"label\":\n            return self.__label_encoder(df)\n        elif self.encoding_type == \"binary\":\n            return self.__binary_encoder(df)\n        elif self.encoding_type == \"onehot\":\n            return self.__one_hot_encoder(df)\n        elif self.encoding_type == \"onehot_sparse\":\n            return self.__one_hot_encoder(df, True)\n        else:\n            raise Exception(\"specified encoding type not defined\")\n\n    def transform(self,dataframe):\n        if self.handle_na:\n            dataframe = self.__handle_missing_category(dataframe, \n                placeholder=self.na_placeholder)\n        df = dataframe.copy(deep=True)\n        if self.encoding_type == \"label\":\n            return self.__label_encoder(df, fit=False)\n        elif self.encoding_type == \"binary\":\n            return self.__binary_encoder(df, fit=False)\n        elif self.encoding_type == \"onehot\":\n            return self.__one_hot_encoder(df, sparse=False, fit=False)\n        elif self.encoding_type == \"onehot_sparse\":\n            return self.__one_hot_encoder(df, sparse=True, fit=False)\n        else:\n            raise Exception(\"specified encoding type not defined\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TFSimpleDataset:\n    def __init__(self,batch_size, repeat,\n        drop_remainder_in_batch=False, \n        num_parallel_calls=tf.data.experimental.AUTOTUNE,\n        buffer_size=tf.data.experimental.AUTOTUNE):\n        self.batch_size = batch_size\n        self.drop_remainder = drop_remainder_in_batch\n        self.num_parallel_calls = num_parallel_calls\n        self.buffer_size = buffer_size\n        self.repeat = repeat\n\n    def create_dataset(self, X, Y=None):\n        datasetX = tf.data.Dataset.from_tensor_slices(X)\n        if Y is not None :\n            datasetY = tf.data.Dataset.from_tensor_slices(Y)\n            dataset = tf.data.Dataset.zip((datasetX,datasetY))\n        else:\n            dataset = datasetX\n        dataset = dataset.batch(self.batch_size, \n            drop_remainder=self.drop_remainder)\n        if self.repeat:\n            dataset = dataset.repeat()\n        dataset = dataset.prefetch(buffer_size=self.buffer_size)\n        return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def write_submission(preds):\n    sub_pred = preds.transpose()\n    submission = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")\n    for i, col in enumerate(target_cols):\n        submission[col]= sub_pred[i]\n    return submission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading, visualizing and setting up data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv(\"../input/lish-moa/train_features.csv\")\ntest_features = pd.read_csv(\"../input/lish-moa/test_features.csv\")\ntrain_targets = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('cp_type',data=train_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('cp_dose',data=train_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('cp_time',data=train_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('cp_type',data=test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('cp_dose',data=test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('cp_time',data=test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_targets.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Encode Categorical variables into onehot"},{"metadata":{"trusted":true},"cell_type":"code","source":"enc = EncodeCategories(train_features, encode_cols=[\"cp_type\", \"cp_dose\", \"cp_time\"], \n                       encoding_type=\"onehot\")\ntrain_features = enc.fit_transform()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"encoding categories of test data too"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = enc.transform(test_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Merge target dataframe with training features on basis of their respective id"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_features.merge(train_targets, on=\"sig_id\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop(\"sig_id\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = test_df.drop(\"sig_id\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"list of target columns in dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols = [t for t in train_targets.columns if not t==\"sig_id\"]\nprint(len(target_cols))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"list of input columns in dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_features = [t for t in train_features.columns if not t==\"sig_id\"]\nprint(len(input_features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting data into folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = CrossValidation(train_df, target_cols, shuffle=True, random_state=11)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining model, callbacks and metrices"},{"metadata":{"trusted":true},"cell_type":"code","source":"metrices = [\"AUC\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model(hidden_layers_units):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=len(input_features)),\n        tf.keras.layers.Dense(len(input_features)),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.LeakyReLU(),\n    ])\n    for units in hidden_layers_units:\n        model.add(tf.keras.layers.Dense(units))\n        model.add(tf.keras.layers.Dropout(0.2))\n        model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Dense(len(target_cols), activation=\"sigmoid\"))\n    model.compile(loss=\"binary_crossentropy\", metrics=metrices, optimizer=\"adam\")\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_START = 0.00001\nLR_MAX = 0.00005\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 0\nLR_EXP_DECAY = .8\n\ndef change_lr(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_callbacks(fold):\n    model_checkpointer = tf.keras.callbacks.ModelCheckpoint(\n        f\"best_model{fold}.h5\",\n        monitor=\"val_auc\",\n        verbose=1,\n        save_best_only=True,\n        mode=\"max\"\n    )\n    early_stop = tf.keras.callbacks.EarlyStopping(\n        monitor=\"val_auc\",\n        min_delta=0,\n        patience=5,\n        verbose=1,\n        mode=\"max\"\n    )\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(change_lr, verbose=True)\n    \n    return [model_checkpointer,early_stop,lr_callback]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Parameters to tune"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=128\nepochs=100\nhidden_layers=[1200]\nnum_folds=5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_dataset_obj = TFSimpleDataset(batch_size=batch_size,\n                                repeat=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_fold_models=[]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training model with 5 folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"f=1\nval_auc = []\ntrain_auc = []\nfor train, val in cv.kfold_split(splits=num_folds, stratify= target_cols):\n    print(\"Fold: \",f)\n    trainX = train.iloc[:,:len(input_features)].values\n    trainY = train.iloc[:,len(input_features):].values\n    valX = val.iloc[:,:len(input_features)].values\n    valY = val.iloc[:,len(input_features):].values\n    train_dataset = tf_dataset_obj.create_dataset(X=trainX, Y=trainY)\n    val_dataset = tf_dataset_obj.create_dataset(X=valX,Y=valY)\n    model = get_model(hidden_layers)\n    k_fold_models.append(model)\n    callbacks = get_callbacks(f)\n    history = model.fit(train_dataset, epochs=epochs,\n              validation_data = val_dataset,\n              callbacks = callbacks)\n    val_auc.append(np.max(history.history['val_auc']))\n    train_auc.append(np.max(history.history['auc']))\n    f+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking up model max validation AUC"},{"metadata":{"trusted":true},"cell_type":"code","source":"for val_acc in val_auc:\n    print(val_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for train_acc in train_auc:\n    print(train_acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average AUC"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.mean(val_auc))\nprint(np.mean(train_auc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading best weights for inferencing"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, model in enumerate(k_fold_models):\n    weights = f\"best_model{i+1}.h5\"\n    model.load_weights(weights)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble Methods\n\n- mean_ensemble: Ensemble models prediction by averaging predictions from different models\n- max_ensemble: Ensemble models prediction by taking max prediction from different models\n- ensemble_accuracy: Check AUC score of ensembled predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_ensemble(models, dataset):\n    predictions = []\n    for model in models:\n        prediction = model.predict(dataset, verbose=1)\n        predictions.append(prediction)\n    predictions = np.mean(predictions, axis=0)\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def max_ensemble(models, dataset):\n    predictions = []\n    for model in models:\n        prediction = model.predict(dataset, verbose=1)\n        predictions.append(prediction)\n    predictions = np.max(predictions, axis=0)\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ensemble_accuracy(models, ensemble_fn, num_folds=5):\n    f=1\n    val_fold_auc = []\n    train_fold_auc = []\n    for train, val in cv.kfold_split(splits=num_folds, stratify= target_cols):\n        print(\"Fold: \",f)\n        trainX = train.iloc[:,:len(input_features)].values\n        trainY = train.iloc[:,len(input_features):].values\n        valX = val.iloc[:,:len(input_features)].values\n        valY = val.iloc[:,len(input_features):].values\n        train_dataset = tf_dataset_obj.create_dataset(X=trainX)\n        val_dataset = tf_dataset_obj.create_dataset(X=valX)\n        prediction_train = ensemble_fn(models, train_dataset)\n        metric_train = tf.keras.metrics.AUC()\n        metric_train.update_state(trainY, prediction_train)\n        train_fold_auc.append(metric_train.result().numpy())\n\n        prediction_val = ensemble_fn(models, val_dataset)\n        metric_val = tf.keras.metrics.AUC()\n        metric_val.update_state(valY, prediction_val)\n        val_fold_auc.append(metric_val.result().numpy())\n        f+=1\n    return train_fold_auc, val_fold_auc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking ensembled AUC with respect to best model"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"train_auc, val_auc= ensemble_accuracy(k_fold_models,mean_ensemble, num_folds=5)\nprint(\"\\n======AUC=========\")\nprint(\"Train AUC\", train_auc)\nprint(\"Validation AUC: \",val_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"train_auc, val_auc= ensemble_accuracy(k_fold_models,max_ensemble, num_folds=5)\nprint(\"\\n======AUC=========\")\nprint(\"Train AUC\", train_auc)\nprint(\"Validation AUC: \",val_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"best_val_model = k_fold_models[np.argmax(val_auc)]\ntrain_auc, val_auc= ensemble_accuracy([best_val_model],mean_ensemble, num_folds=5)\nprint(\"\\n======AUC=========\")\nprint(\"Train AUC\", train_auc)\nprint(\"Validation AUC: \",val_auc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test data predictions for submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = tf_dataset_obj.create_dataset(X=test_df.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mean_predictions = mean_ensemble(k_fold_models,test_dataset)\nmax_predictions = max_ensemble(k_fold_models,test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(mean_predictions.shape)\nprint(max_predictions.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# submission_mean = write_submission(mean_predictions)\n# submission_mean.to_csv(\"submission.csv\", index=False)\n# submission_mean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"submission_max = write_submission(max_predictions)\nsubmission_max.to_csv(\"submission.csv\", index=False)\nsubmission_max.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}