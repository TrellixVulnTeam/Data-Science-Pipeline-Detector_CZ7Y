{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom tqdm import tqdm\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom sklearn.linear_model import LogisticRegression\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features = pd.read_csv('../input/lish-moa/test_features.csv')\ntest_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_scored.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = train_features.drop(columns = ['sig_id'])\ny_test = pd.DataFrame(test_features['sig_id'])\ntest_features = test_features.drop(columns = ['sig_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_columns = ['cp_type', 'cp_time', 'cp_dose']\ncat_train_features = train_features[cat_columns]\ncat_test_features = test_features[cat_columns]\ntrain_features = train_features.drop(columns = cat_columns)\ntest_features = test_features.drop(columns = cat_columns)\nX1_train = np.array(train_features)\nX1_test = np.array(test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"st = StandardScaler()\nX1_train = st.fit_transform(X1_train)\nX1_test = st.transform(X1_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot = OneHotEncoder()\nX2_train = one_hot.fit_transform(cat_train_features)\nX2_test = one_hot.transform(cat_test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.concatenate((X2_train.toarray(), X1_train), 1)\nX_test = np.concatenate((X2_test.toarray(), X1_test), 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epoch(model, optimizer, train_loader, criterion, device, scheduler):\n    \"\"\"\n    for each batch \n    performs forward and backward pass and parameters update \n    \n    Input:\n    model: instance of model (example defined above)\n    optimizer: instance of optimizer (defined above)\n    train_loader: instance of DataLoader\n    \n    Returns:\n    nothing\n    \n    Do not forget to set net to train mode!\n    \"\"\"\n    model.train()\n    for x_batch, y_batch in train_loader:\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n\n        optimizer.zero_grad()\n        output = model(x_batch)\n        \n        loss = criterion(output, y_batch)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n    \n\ndef evaluate_loss(loader, model, criterion, device):\n    \"\"\"\n    Evaluates loss and accuracy on the whole dataset\n    \n    Input:\n    loader:  instance of DataLoader\n    model: instance of model (examle defined above)\n    \n    Returns:\n    (loss, accuracy)\n    \n    Do not forget to set net to eval mode!\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        cumloss, cumacc = 0, 0\n        num_objects = 0\n        model.eval()\n        for x_batch, y_batch in loader:\n            x_batch = x_batch.to(device)\n            y_batch = y_batch.to(device)\n            output = model(x_batch)\n            loss = criterion(output, y_batch)\n            cumloss += loss.item()\n            num_objects += len(x_batch)\n    return cumloss / num_objects\n    \n    \ndef train(model, opt, train_loader, test_loader, criterion, n_epochs, device, scheduler, verbose=True):\n    \"\"\"\n    Performs training of the model and prints progress\n    \n    Input:\n    model: instance of model (example defined above)\n    opt: instance of optimizer \n    train_loader: instance of DataLoader\n    test_loader: instance of DataLoader (for evaluation)\n    n_epochs: int\n    \n    Returns:\n    4 lists: train_log, train_acc_log, val_log, val_acc_log\n    with corresponding metrics per epoch\n    \"\"\"\n    train_log = []\n    val_log = []\n\n    for epoch in range(n_epochs):\n        train_epoch(model, opt, train_loader, criterion, device, scheduler)\n        train_loss = evaluate_loss(train_loader, \n                                                  model, criterion, \n                                                  device)\n        #val_loss = evaluate_loss(test_loader, model, \n        #                                      criterion, device)\n\n        train_log.append(train_loss)\n\n        #val_log.append(val_loss)\n        \n        if verbose:\n             print ('Epoch', epoch+1, '/', n_epochs, 'Loss (train): ', train_loss)\n            \n    return train_log#, val_log","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Data(torch.utils.data.Dataset):\n    def __init__(self, X, Y):\n        self.X = X\n        self.Y = Y\n        \n    def __getitem__(self, idx):\n        return torch.Tensor((self.X)[idx, :]), torch.Tensor((self.Y)[idx, :])\n    \n    def __len__(self):\n        return self.X.shape[0]\n    \nclass DataTest(torch.utils.data.Dataset):\n    def __init__(self, X):\n        self.X = X\n        \n    def __getitem__(self, idx):\n        return torch.Tensor(self.X[idx, :])\n    \n    def __len__(self):\n        return self.X.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if torch.cuda.is_available():\n    device = 'cuda'\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SimpleNet(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(SimpleNet, self).__init__()\n        self.bn1 = nn.BatchNorm1d(hidden_size)\n        self.layer1 = nn.Linear(num_features, hidden_size)\n        \n        self.bn2 = nn.BatchNorm1d(hidden_size)\n        self.layer2 = nn.Linear(hidden_size, hidden_size)\n        \n        self.layer3 = nn.Linear(hidden_size, num_targets)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.layer1(x)))\n        x = F.relu(self.bn2(self.layer2(x)))\n        return self.layer3(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = np.array(train_targets_scored[train_targets_scored.columns[1:]])\ntrain_data = Data(X_train, y_train)\ntest_data = DataTest(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss()\nmodel = SimpleNet(879, 206, 1024)\nmodel.to(device)\noptimizer = optim.Adam(model.parameters(), lr = 0.001)\nscheduler = optim.lr_scheduler.StepLR(optimizer = optimizer, gamma = 0.5, step_size=55)\nloss_stat = train(model, optimizer, train_loader, test_loader, criterion, n_epochs = 20, device = device, scheduler = scheduler, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    tensors = []\n    for x_batch in test_loader:\n        #x_batch.to(device)\n        #print(x_batch.shape)\n        output = model(x_batch.to(device))\n        tensors.append(output)\n    y_pred = torch.cat(tensors, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = (nn.Sigmoid()(y_pred.cpu())).numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nfor column in train_targets_scored.columns[1:]:\n    y_test[column] = pred[:, i]\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}