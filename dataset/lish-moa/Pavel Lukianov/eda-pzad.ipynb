{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nfrom tqdm.notebook import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features = pd.read_csv('../input/lish-moa/test_features.csv')\ntest_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(train_features['cp_type'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(train_features['cp_time'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(train_features['cp_dose'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cp_type = np.array(train_features['cp_type'])\nprint('Train')\nprint('count of trt_cp:', (cp_type == 'trt_cp').sum())\nprint('count of ctl_vehicle:', (cp_type == 'ctl_vehicle').sum())\ncp_type = np.array(test_features['cp_type'])\nprint('Test')\nprint('count of trt_cp:', (cp_type == 'trt_cp').sum())\nprint('count of ctl_vehicle:', (cp_type == 'ctl_vehicle').sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Classes are disbalanced"},{"metadata":{"trusted":true},"cell_type":"code","source":"cp_time = np.array(train_features['cp_time'])\nprint('Train')\nprint('count of 24:', (cp_time == 24).sum())\nprint('count of 48:', (cp_time == 48).sum())\nprint('count of 72:', (cp_time == 72).sum())\ncp_time = np.array(test_features['cp_time'])\nprint('Test')\nprint('count of 24:', (cp_time == 24).sum())\nprint('count of 48:', (cp_time == 48).sum())\nprint('count of 72:', (cp_time == 72).sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cp_dose = np.array(train_features['cp_dose'])\nprint('Train')\nprint('count of D1:', (cp_dose == 'D1').sum())\nprint('count of D2:', (cp_dose == 'D2').sum())\ncp_dose = np.array(test_features['cp_dose'])\nprint('Test')\nprint('count of D1:', (cp_dose == 'D1').sum())\nprint('count of D2:', (cp_dose == 'D2').sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train =  train_features.merge(train_targets, on = 'sig_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = ['cp_type']\nd.extend(train_targets.columns[1:])\ntrain1 = train[d]\nar = np.array(train1[train1['cp_type'] == 'ctl_vehicle'])[:, 1:]\n(ar != 0).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### !!! All targets on (cp_type = ctl_vehicle) == 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"d = ['cp_type']\nd.extend(train_targets.columns[1:])\ntrain1 = train[d]\nar = np.array(train1[train1['cp_type'] == 'trt_cp'])[:, 1:]\n(ar != 0).sum() / (ar.shape[0] * ar.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A lot of zero values"},{"metadata":{},"cell_type":"markdown","source":"Continue research"},{"metadata":{"trusted":true},"cell_type":"code","source":"d = ['cp_type', 'cp_time']\nd.extend(train_targets.columns[1:])\ntrain1 = train[d]\nar = np.array(train1[(train1['cp_type'] == 'trt_cp') & (train1['cp_time'] == 24)])[:, 2:]\n(ar != 0).sum() / (ar.shape[0] * ar.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = ['cp_type', 'cp_time']\nd.extend(train_targets.columns[1:])\ntrain1 = train[d]\nar = np.array(train1[(train1['cp_type'] == 'trt_cp') & (train1['cp_time'] == 48)])[:, 2:]\n(ar != 0).sum() / (ar.shape[0] * ar.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = ['cp_type', 'cp_time']\nd.extend(train_targets.columns[1:])\ntrain1 = train[d]\nar = np.array(train1[(train1['cp_type'] == 'trt_cp') & (train1['cp_time'] == 72)])[:, 2:]\n(ar != 0).sum() / (ar.shape[0] * ar.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = ['cp_type', 'cp_dose']\nd.extend(train_targets.columns[1:])\ntrain1 = train[d]\nar = np.array(train1[(train1['cp_type'] == 'trt_cp') & (train1['cp_dose'] == 'D1')])[:, 2:]\n(ar != 0).sum() / (ar.shape[0] * ar.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = ['cp_type', 'cp_dose']\nd.extend(train_targets.columns[1:])\ntrain1 = train[d]\nar = np.array(train1[(train1['cp_type'] == 'trt_cp') & (train1['cp_dose'] == 'D2')])[:, 2:]\n(ar != 0).sum() / (ar.shape[0] * ar.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = ['cp_type', 'cp_time', 'cp_dose']\nd.extend(train_targets.columns[1:])\ntrain1 = train[d]\nar = np.array(train1[(train1['cp_type'] == 'trt_cp') & (train1['cp_time'] == 72) & (train1['cp_dose'] == 'D2')])[:, 3:]\n(ar != 0).sum() / (ar.shape[0] * ar.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In all parts of data percent of non-zero targets is about the same"},{"metadata":{},"cell_type":"markdown","source":"Look at histograms of random features"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20, 8))\nplt.subplot(241)\nn = np.random.randint(772)\nplt.hist(train_features['g-' + str(n)], bins = 70)\nplt.title('Histogram(' + 'g-' + str(n) + ')', fontsize = 16)\nplt.subplot(242)\nn = np.random.randint(772)\nplt.hist(train_features['g-' + str(n)], bins = 70)\nplt.title('Histogram(' + 'g-' + str(n) + ')', fontsize = 16)\nplt.subplot(243)\nn = np.random.randint(772)\nplt.hist(train_features['g-' + str(n)], bins = 70)\nplt.title('Histogram(' + 'g-' + str(n) + ')', fontsize = 16)\nplt.subplot(244)\nn = np.random.randint(772)\nplt.hist(train_features['g-' + str(n)], bins = 70)\nplt.title('Histogram(' + 'g-' + str(n) + ')', fontsize = 16)\nplt.subplot(245)\nn = np.random.randint(772)\nplt.hist(train_features['g-' + str(n)], bins = 70)\nplt.title('Histogram(' + 'g-' + str(n) + ')', fontsize = 16)\nplt.subplot(246)\nn = np.random.randint(772)\nplt.hist(train_features['g-' + str(n)], bins = 70)\nplt.title('Histogram(' + 'g-' + str(n) + ')', fontsize = 16)\nplt.subplot(247)\nn = np.random.randint(772)\nplt.hist(train_features['g-' + str(n)], bins = 70)\nplt.title('Histogram(' + 'g-' + str(n) + ')', fontsize = 16)\nplt.subplot(248)\nn = np.random.randint(772)\nplt.hist(train_features['g-' + str(n)], bins = 70)\nplt.title('Histogram(' + 'g-' + str(n) + ')', fontsize = 16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20, 8))\nplt.subplot(241)\nn = np.random.randint(100)\nplt.hist(train_features['c-' + str(n)], bins = 70)\nplt.title('Histogram(' + 'c-' + str(n) + ')', fontsize = 16)\nplt.subplot(242)\nn = np.random.randint(100)\nplt.hist(train_features['c-' + str(n)], bins = 70)\nplt.title('Histogram(' + 'c-' + str(n) + ')', fontsize = 16)\nplt.subplot(243)\nn = np.random.randint(100)\nplt.hist(train_features['c-' + str(n)], bins = 70)\nplt.title('Histogram(' + 'c-' + str(n) + ')', fontsize = 16)\nplt.subplot(244)\nn = np.random.randint(100)\nplt.hist(train_features['c-' + str(n)], bins = 70)\nplt.title('Histogram(' + 'c-' + str(n) + ')', fontsize = 16)\nplt.subplot(245)\nn = np.random.randint(100)\nplt.hist(train_features['c-' + str(n)], bins = 70)\nplt.title('Histogram(' + 'c-' + str(n) + ')', fontsize = 16)\nplt.subplot(246)\nn = np.random.randint(100)\nplt.hist(train_features['c-' + str(n)], bins = 70)\nplt.title('Histogram(' + 'c-' + str(n) + ')', fontsize = 16)\nplt.subplot(247)\nn = np.random.randint(100)\nplt.hist(train_features['c-' + str(n)], bins = 70)\nplt.title('Histogram(' + 'c-' + str(n) + ')', fontsize = 16)\nplt.subplot(248)\nn = np.random.randint(100)\nplt.hist(train_features['c-' + str(n)], bins = 70)\nplt.title('Histogram(' + 'c-' + str(n) + ')', fontsize = 16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look at scatter of random features and random targets"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (25, 8))\nplt.subplot(241)\nn = np.random.randint(100)\nm = np.random.randint(1, 207)\nplt.scatter(train_features['c-' + str(n)], train_targets[train_targets.columns[m]])\nplt.title('Scatter(' + 'c-' + str(n) + ' - ' + train_targets.columns[m] +')', fontsize = 12)\nplt.subplot(242)\nn = np.random.randint(100)\nm = np.random.randint(1, 207)\nplt.scatter(train_features['c-' + str(n)], train_targets[train_targets.columns[m]])\nplt.title('Scatter(' + 'c-' + str(n) + ' - ' + train_targets.columns[m] +')', fontsize = 12)\nplt.subplot(243)\nn = np.random.randint(100)\nm = np.random.randint(1, 207)\nplt.scatter(train_features['c-' + str(n)], train_targets[train_targets.columns[m]])\nplt.title('Scatter(' + 'c-' + str(n) + ' - ' + train_targets.columns[m] +')', fontsize = 12)\nplt.subplot(244)\nn = np.random.randint(100)\nm = np.random.randint(1, 207)\nplt.scatter(train_features['c-' + str(n)], train_targets[train_targets.columns[m]])\nplt.title('Scatter(' + 'c-' + str(n) + ' - ' + train_targets.columns[m] +')', fontsize = 12)\nplt.subplot(245)\nn = np.random.randint(100)\nm = np.random.randint(1, 207)\nplt.scatter(train_features['c-' + str(n)], train_targets[train_targets.columns[m]])\nplt.title('Scatter(' + 'c-' + str(n) + ' - ' + train_targets.columns[m] +')', fontsize = 12)\nplt.subplot(246)\nn = np.random.randint(100)\nm = np.random.randint(1, 207)\nplt.scatter(train_features['c-' + str(n)], train_targets[train_targets.columns[m]])\nplt.title('Scatter(' + 'c-' + str(n) + ' - ' + train_targets.columns[m] +')', fontsize = 12)\nplt.subplot(247)\nn = np.random.randint(100)\nm = np.random.randint(1, 207)\nplt.scatter(train_features['c-' + str(n)], train_targets[train_targets.columns[m]])\nplt.title('Scatter(' + 'c-' + str(n) + ' - ' + train_targets.columns[m] +')', fontsize = 12)\nplt.subplot(248)\nn = np.random.randint(100)\nm = np.random.randint(1, 207)\nplt.scatter(train_features['c-' + str(n)], train_targets[train_targets.columns[m]])\nplt.title('Scatter(' + 'c-' + str(n) + ' - ' + train_targets.columns[m] +')', fontsize = 12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (25, 8))\nplt.subplot(241)\nn = np.random.randint(772)\nm = np.random.randint(1, 207)\nplt.scatter(train_features['g-' + str(n)], train_targets[train_targets.columns[m]])\nplt.title('Scatter(' + 'g-' + str(n) + ' - ' + train_targets.columns[m] +')', fontsize = 12)\nplt.subplot(242)\nn = np.random.randint(772)\nm = np.random.randint(1, 207)\nplt.scatter(train_features['g-' + str(n)], train_targets[train_targets.columns[m]])\nplt.title('Scatter(' + 'g-' + str(n) + ' - ' + train_targets.columns[m] +')', fontsize = 12)\nplt.subplot(243)\nn = np.random.randint(772)\nm = np.random.randint(1, 207)\nplt.scatter(train_features['g-' + str(n)], train_targets[train_targets.columns[m]])\nplt.title('Scatter(' + 'g-' + str(n) + ' - ' + train_targets.columns[m] +')', fontsize = 12)\nplt.subplot(244)\nn = np.random.randint(772)\nm = np.random.randint(1, 207)\nplt.scatter(train_features['g-' + str(n)], train_targets[train_targets.columns[m]])\nplt.title('Scatter(' + 'g-' + str(n) + ' - ' + train_targets.columns[m] +')', fontsize = 12)\nplt.subplot(245)\nn = np.random.randint(772)\nm = np.random.randint(1, 207)\nplt.scatter(train_features['g-' + str(n)], train_targets[train_targets.columns[m]])\nplt.title('Scatter(' + 'g-' + str(n) + ' - ' + train_targets.columns[m] +')', fontsize = 12)\nplt.subplot(246)\nn = np.random.randint(772)\nm = np.random.randint(1, 207)\nplt.scatter(train_features['g-' + str(n)], train_targets[train_targets.columns[m]])\nplt.title('Scatter(' + 'g-' + str(n) + ' - ' + train_targets.columns[m] +')', fontsize = 12)\nplt.subplot(247)\nn = np.random.randint(772)\nm = np.random.randint(1, 207)\nplt.scatter(train_features['g-' + str(n)], train_targets[train_targets.columns[m]])\nplt.title('Scatter(' + 'g-' + str(n) + ' - ' + train_targets.columns[m] +')', fontsize = 12)\nplt.subplot(248)\nn = np.random.randint(772)\nm = np.random.randint(1, 207)\nplt.scatter(train_features['g-' + str(n)], train_targets[train_targets.columns[m]])\nplt.title('Scatter(' + 'g-' + str(n) + ' - ' + train_targets.columns[m] +')', fontsize = 12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = ['cp_type', 'cp_time']\nd.extend(train_targets.columns[1:])\ntrain1 = train[d]\nar = np.array(train1[(train1['cp_type'] == 'trt_cp')])[:, 2:]\n(ar != 0).sum() / (ar.shape[0] * ar.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = ['cp_type', 'cp_time']\nd.extend(train_targets.columns[1:])\ntrain1 = train[d]\nar = np.array(train1[(train1['cp_type'] == 'trt_cp')])[:, 2:]\n(ar != 1).sum() / (ar.shape[0] * ar.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(ar == 0).sum() / (ar.shape[0] * ar.shape[1]) + (ar == 1).sum() / (ar.shape[0] * ar.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ar.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Print percent of values 0 and 1 for each feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"n = train_targets.shape[0]\nfor x in train_targets.columns[1:]:\n    ar1 = np.array(train_targets[x])\n    print(x + ':' + ' ' * (50 - len(x)), np.round((ar1 == 0).sum() / n, 5), np.round((ar1 == 1).sum() / n, 5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Add additional features"},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/kushal1506/moa-pytorch-feature-engineering-0-01846"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_g = ['g-' + str(i) for i in range(772)]\nfeatures_c = ['c-' + str(i) for i in range(100)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in train_features, test_features:\n    df['g_sum'] = df[features_g].sum(axis = 1)\n    df['g_mean'] = df[features_g].mean(axis = 1)\n    df['g_median'] = df[features_g].median(axis = 1)\n    df['g_std'] = df[features_g].std(axis = 1)\n    df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n    df['g_skew'] = df[features_g].skew(axis = 1)\n    df['c_sum'] = df[features_c].sum(axis = 1)\n    df['c_mean'] = df[features_c].mean(axis = 1)\n    df['c_std'] = df[features_c].std(axis = 1)\n    df['c_median'] = df[features_c].median(axis = 1)\n    df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n    df['c_skew'] = df[features_c].skew(axis = 1)\n    df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n    df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n    df['gc_std'] = df[features_g + features_c].std(axis = 1)\n    df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n    df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n    df['gc_median'] = df[features_g + features_c].median(axis = 1)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Model\n\nA lot of code is from kaggle.com/nicohrubec/pytorch-multilabel-neural-network/"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets = train_targets[train_features['cp_type'] == 'trt_cp']\ntrain_features = train_features[train_features['cp_type'] == 'trt_cp']\ntrain_features = train_features.drop(columns = ['sig_id', 'cp_type'])\ntest1_features = test_features.copy()\ntest_features = test_features.drop(columns = ['sig_id', 'cp_type'])\ncat_columns = ['cp_time', 'cp_dose']\ncat_train_features = train_features[cat_columns]\ncat_test_features = test_features[cat_columns]\ntrain_features = train_features.drop(columns = cat_columns)\ntest_features = test_features.drop(columns = cat_columns)\ntrain = np.array(train_features)\ntest = np.array(test_features)\ntargets = train_targets.columns\ntrain_targets = np.array(train_targets[train_targets.columns[1:]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 128\nval_batch_size = batch_size * 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot = OneHotEncoder()\nX2_train = one_hot.fit_transform(cat_train_features)\nX2_test = one_hot.transform(cat_test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = np.concatenate((X2_train.toarray(), train), 1)\ntest = np.concatenate((X2_test.toarray(), test), 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_folds = 7\nn_starts = 1\nn_epochs = 70\nn_targets = train_targets.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.BCELoss()\nkfold = MultilabelStratifiedKFold(n_splits = 7, random_state = 42, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SimpleNet(nn.Module):\n    def __init__(self, num_columns):\n        super(SimpleNet, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_columns)\n        self.dropout1 = nn.Dropout(0.4)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_columns, 1024))\n        \n        self.batch_norm2 = nn.BatchNorm1d(1024)\n        self.dropout2 = nn.Dropout(0.4)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(1024, 1024))\n        \n        self.batch_norm3 = nn.BatchNorm1d(1024)\n        self.dropout3 = nn.Dropout(0.4)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(1024, 206))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.sigmoid(self.dense3(x))\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Dataset1(Dataset):\n    def __init__(self, df, targets, mode='train'):\n        self.mode = mode\n        self.df = df\n        if mode == 'train':\n            self.targets = targets\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        if self.mode == 'train':\n            return torch.FloatTensor(self.df[index]), torch.FloatTensor(self.targets[index])\n        elif self.mode == 'test':\n            return torch.FloatTensor(self.df[index]), 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if torch.cuda.is_available():\n    device = 'cuda'\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for n, (tr, te) in enumerate(kfold.split(train_targets, train_targets)):\n    print('Train fold:', n + 1)\n    xtrain, xval = train[tr], train[te]\n    ytrain, yval = train_targets[tr], train_targets[te]\n\n    train_set = Dataset1(xtrain, ytrain)\n    val_set = Dataset1(xval, yval)\n\n    dataloaders = {\n        'train': DataLoader(train_set, batch_size=batch_size, shuffle=True),\n        'val': DataLoader(val_set, batch_size=val_batch_size, shuffle=False)\n    }\n\n    model = SimpleNet(895).to(device)\n    checkpoint_path = f'repeat:{0}_Fold:{n+1}.pt'\n    optimizer = optim.Adam(model.parameters(), weight_decay=1e-5)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, eps=1e-4, verbose=True)\n    best_loss = {'train': np.inf, 'val': np.inf}\n\n    for epoch in range(n_epochs):\n        epoch_loss = {'train': 0.0, 'val': 0.0}\n\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n\n            running_loss = 0.0\n\n            for i, (x, y) in enumerate(dataloaders[phase]):\n                x, y = x.to(device), y.to(device)\n\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase=='train'):\n                    preds = model(x)\n                    loss = criterion(preds, y)\n\n                    if phase=='train':\n                        loss.backward()\n                        optimizer.step()\n\n                running_loss += loss.item() / len(dataloaders[phase])\n\n            epoch_loss[phase] = running_loss\n\n        print(\"Epoch {}/{}   -   loss: {:5.5f}   -   val_loss: {:5.5f}\".format(epoch + 1, n_epochs, epoch_loss['train'], epoch_loss['val']))\n\n        scheduler.step(epoch_loss['val'])\n\n        if epoch_loss['val'] < best_loss['val']:\n            best_loss = epoch_loss\n            torch.save(model.state_dict(), checkpoint_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof = np.zeros((train.shape[0], n_targets - 1))\noof_targets = np.zeros((train.shape[0], n_targets - 1))\npreds = np.zeros((test.shape[0], n_targets - 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_log_loss(y_true, y_pred):\n    metrics = []\n    for i, target in enumerate(targets[1:]):\n        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1]))\n    return np.mean(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_targets = []\nseed_oof = []\nseed_preds = np.zeros((test.shape[0], n_targets, n_folds))\n\nfor n, (tr, te) in enumerate(kfold.split(train_targets, train_targets)):\n    xval, yval = train[te], train_targets[te]\n    fold_preds = []\n\n    val_set = Dataset1(xval, yval)\n    test_set = Dataset1(test, None, mode='test')\n\n    dataloaders = {\n        'val': DataLoader(val_set, batch_size=val_batch_size, shuffle=False),\n        'test': DataLoader(test_set, batch_size=val_batch_size, shuffle=False)\n    }\n\n    checkpoint_path = f'repeat:{0}_Fold:{n+1}.pt'\n    model = SimpleNet(895).to(device)\n    model.load_state_dict(torch.load(checkpoint_path))\n    model.eval()\n\n    for phase in ['val', 'test']:\n        for i, (x, y) in enumerate(dataloaders[phase]):\n            if phase == 'val':\n                x, y = x.to(device), y.to(device)\n            elif phase == 'test':\n                x = x.to(device)\n\n            with torch.no_grad():\n                batch_preds = model(x)\n\n                if phase == 'val':\n                    seed_targets.append(y)\n                    seed_oof.append(batch_preds)\n                elif phase == 'test':\n                    fold_preds.append(batch_preds)\n\n    fold_preds = torch.cat(fold_preds, dim=0).cpu().numpy()\n    seed_preds[:, :, n] = fold_preds\n\nseed_targets = torch.cat(seed_targets, dim=0).cpu().numpy()\nseed_oof = torch.cat(seed_oof, dim=0).cpu().numpy()\nseed_preds = np.mean(seed_preds, axis=2)\n\nprint(\"Overall score is {:5.5f}\".format(mean_log_loss(seed_targets, seed_oof)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[targets[1:]] = seed_preds\nsubmission.loc[test1_features['cp_type']=='ctl_vehicle', targets[1:]] = 0\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}