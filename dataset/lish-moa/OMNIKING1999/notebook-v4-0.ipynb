{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import RMSprop,Adam\nimport tensorflow_addons as tfa\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_selection import VarianceThreshold\nfrom tensorflow.keras import layers,regularizers,Sequential,Model,backend,callbacks,optimizers,metrics,losses\nimport tensorflow as tf\nimport sys\nimport json\nsys.path.append('../input/itration/iterative_stratification-0.1.6-py3-none-any.whl')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\nnon_ctl_idx = df.loc[df['cp_type']!='ctl_vehicle'].index.to_list()\ndf = df.drop(['sig_id','cp_type','cp_dose','cp_time'],axis=1)\ndata_X_test = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\nnon_ctl_data_X_test = data_X_test.loc[data_X_test['cp_type'] =='ctl_vehicle'].index.to_list()\ndata_X_test = data_X_test.drop(['sig_id','cp_type','cp_dose','cp_time'],axis=1)\n\npretrain = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\npretrain = pretrain.drop('sig_id',axis=1)\npretrain = pretrain.values\npretrain = pretrain[non_ctl_idx]\n\ndada = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ndada = dada.drop('sig_id',axis=1)\ndf_label = dada.values\ndf = df.iloc[non_ctl_idx]\ndf_label = df_label[non_ctl_idx]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qq = df.columns\n\nscaler = preprocessing.MinMaxScaler()\n\ndata_train = scaler.fit_transform(df)\n\ndata_test = scaler.transform(data_X_test)\n\ndf = pd.DataFrame(data = data_train,columns = qq)\ndata_X_test = pd.DataFrame(data = data_test,columns = qq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\njson_file_path = '../input/mainfile/main_predictors.json'\n\nwith open(json_file_path, 'r') as j:\n    predictors = json.loads(j.read())\n    predictors = predictors['start_predictors']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ncs = df.columns.str.contains('c-')\ngs = df.columns.str.contains('g-')\n\ndef preprocessor(train,test):\n    \n    \n    n_gs = 2 \n    n_cs = 100 \n    \n    pca_cs = PCA(n_components = n_cs)\n    pca_gs = PCA(n_components = n_gs)\n\n    train_pca_gs = pca_gs.fit_transform(train[:,gs])\n    train_pca_cs = pca_cs.fit_transform(train[:,cs])\n    test_pca_gs = pca_gs.transform(test[:,gs])\n    test_pca_cs = pca_cs.transform(test[:,cs])\n\n    \n    train_c_mean = train[:,cs].mean(axis=1)\n    test_c_mean = test[:,cs].mean(axis=1)\n    train_g_mean = train[:,gs].mean(axis=1)\n    test_g_mean = test[:,gs].mean(axis=1)\n    \n    \n    train = np.concatenate((train,train_pca_gs,train_pca_cs,train_c_mean[:,np.newaxis]\n                            ,train_g_mean[:,np.newaxis]),axis=1)\n    test = np.concatenate((test,test_pca_gs,test_pca_cs,test_c_mean[:,np.newaxis],\n                           test_g_mean[:,np.newaxis]),axis=1)\n    \n    scaler = preprocessing.StandardScaler()\n\n    train = scaler.fit_transform(train)\n\n    test = scaler.transform(test)\n    \n    return train, test\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\np_min = 0.0005\np_max = 0.9995\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(n_features, n_features_2, n_labels, label_smoothing = 0.0005):\n    input_pri = tf.keras.layers.Input(shape = (n_features,), name = 'Input1') \n    input_sec = tf.keras.layers.Input(shape = (n_features_2,), name = 'Input2')\n        \n    mouth_1 = tf.keras.Sequential([\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(512, activation=\"relu\"),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(256, activation=\"relu\"),\n        ],name='Head1') \n    \n    input_tir = mouth_1(input_pri)\n    \n    num_columns = 703\n    aa = tf.keras.layers.Concatenate()([input_sec, input_tir]),\n    \n    mouth_2 = tf.keras.Sequential([\n        tf.keras.layers.Input(num_columns),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(512, activation=\"relu\")\n        ],name='Head2')\n    \n    bb = mouth_2(aa)\n    aa_1 = tf.keras.layers.Concatenate()([input_sec, input_tir ,bb]), \n    \n    num_columns_1 = 1215\n    \n    stomach_1 = tf.keras.Sequential([\n        tf.keras.layers.Input(num_columns_1),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(512, activation=\"elu\"),\n        tf.keras.layers.BatchNormalization(), \n        tf.keras.layers.Dense(256, activation=\"relu\")  \n    ],name='stomach_1')\n    \n    cc = stomach_1(aa_1)\n    aa_2 = tf.keras.layers.Concatenate()([bb,cc]),    \n    \n    num_columns_2 = 768\n    \n    stomach_2 = tf.keras.Sequential([\n        tf.keras.layers.Input(num_columns_2),\n        tf.keras.layers.BatchNormalization(), \n        tf.keras.layers.Dense(256, activation=\"relu\")  \n    ],name='stomach_2')\n    \n    dd = stomach_2(aa_2)\n    aa_3 = tf.keras.layers.Concatenate()([bb,cc,dd]),\n    \n    num_columns_3 = 1024\n    \n    \n    stomach_3 = tf.keras.Sequential([\n        tf.keras.layers.Input(num_columns_3),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(256, kernel_initializer='lecun_normal', activation='selu',name='last_frozen'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(206, kernel_initializer='lecun_normal', activation='selu')\n        ],name='stomach_3')\n\n    output_1 = stomach_3(aa_3)\n    \n    #output_2 = tf.keras.Sequential([\n        #tf.keras.layers.BatchNormalization(),\n        #tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation=\"sigmoid\"))  \n    #],name = 'end')\n    \n    #output = output_2(output_1)\n    \n    \n    model = Model(inputs = [input_pri, input_sec], outputs = output_1)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tra, val = preprocessor(df.values,df.values)\ntrash = df.loc[:,predictors].values\nmodel = create_model(tra.shape[1], trash.shape[1], df_label.shape[1])\nmodel.summary()\ntf.keras.utils.plot_model(model,show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\n\nfrom keras import backend \nn_seeds = 2\nnp.random.seed(1)\n#seeds = np.random.randint(0,100,size=n_seeds)\n#seeds = [23, 228, 1488, 1998, 2208, 2077, 404]\nseeds = [23,1998]\nn_labels = df_label.shape[1]\nn_train = df.shape[0]\nn_test = data_X_test.shape[0]\nn_folds = 10\ny_pred = np.zeros((n_test,n_labels))\noof = tf.constant(0.0)\nhists = []\n\n\nfor seed in seeds:\n    fold = 0\n    print(fold)\n    mlkf = KFold(n_splits=n_folds,shuffle=True,random_state=seed)\n    for n,(train, test) in enumerate(mlkf.split(df,df_label)):\n        tf.print('OOF score is ',oof)\n        print(f'Fold {n}')\n        \n        file_name = ('model weights of seed '+ str(seed)+' and fold '+str(fold)+'.h5')\n        \n        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.5, verbose=1,mode='auto',patience=2, min_lr=1E-5)\n        early_st = tf.keras.callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=6, verbose=1, mode='auto',baseline=None, restore_best_weights=True)\n        callbacks = tf.keras.callbacks.ModelCheckpoint(filepath = file_name, monitor='val_logloss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto')\n        \n        \n        train_pri, validation_pri = preprocessor(df.iloc[train].values,\n                                       df.iloc[test].values)\n        _,data_test_pri = preprocessor(df.iloc[train].values,\n                                   data_X_test.values)\n        \n        train_sec = df.iloc[train][predictors].values\n        validation_sec = df.iloc[test][predictors].values\n        data_test_sec = data_X_test[predictors].values\n        \n        pretrain_y = pretrain[train]\n        prevalidation_y = pretrain[test]\n\n        train_y = df_label[train]\n        validation_y = df_label[test]\n        \n        n_features = train_pri.shape[1]\n        n_features_2 = train_sec.shape[1]\n        \n        ##pretraing\n        n_labels_pre = pretrain.shape[1]\n        \n        tf.keras.callbacks.Callback()\n            \n        base_model = create_model(n_features, n_features_2, n_labels_pre)\n        \n        output_2 = tf.keras.Sequential([\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.Dense(n_labels_pre, activation=\"sigmoid\")],name = 'end')\n        \n        output_2_2 = output_2(base_model.output)\n        \n        model = Model(base_model.input,output_2_2)\n     \n        model.compile(optimizer=tf.optimizers.Adam(),loss=losses.BinaryCrossentropy(label_smoothing=0.0005),metrics=logloss)\n\n        hist = model.fit([train_pri,train_sec],pretrain_y, batch_size=128, epochs=200,verbose=1,validation_data = ([validation_pri,validation_sec],prevalidation_y),callbacks =[reduce_lr,callbacks,early_st])\n        \n        model.load_weights(file_name)\n        \n        loss = model.evaluate([validation_pri,validation_sec], prevalidation_y, verbose=2)  \n        \n        \n        ##maintraing\n        \n        file_name_1 = ('model weights of seed '+ str(seed)+' and fold '+str(fold)+'.h5')\n        \n        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.1, verbose=1,mode='auto',patience=2, min_lr=1E-5)\n        early_st = tf.keras.callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=3, verbose=1, mode='auto',baseline=None, restore_best_weights=True)\n        callbacks = tf.keras.callbacks.ModelCheckpoint(filepath = file_name_1, monitor='val_logloss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto')\n        \n        \n        n_labels_pre = train_y.shape[1]\n        \n        new_model = create_model(n_features, n_features_2, n_labels_pre)\n\n        sz = len(base_model.layers)\n        for k in range(sz):\n            w = model.layers[k].get_weights()\n            new_model.layers[k].set_weights(w)\n        \n        output_3 = tf.keras.Sequential([\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.Dropout(0.3),\n            tf.keras.layers.Dense(n_labels_pre, activation=\"sigmoid\")])\n        \n        output_33 = output_3(new_model.output)\n        \n        \n        model_1 = Model(new_model.input,output_33)\n        \n        model_1.summary()\n     \n        model_1.compile(optimizer=tf.optimizers.Adam(),loss=losses.BinaryCrossentropy(label_smoothing=0.0005),metrics=logloss)\n\n        hist = model_1.fit([train_pri,train_sec],train_y, batch_size=128, epochs=200,verbose=1,validation_data = ([validation_pri,validation_sec],validation_y),callbacks =[reduce_lr,callbacks,early_st])\n        \n        model_1.load_weights(file_name_1)\n        \n        loss = model_1.evaluate([validation_pri,validation_sec], validation_y, verbose=2)  \n                     \n        \n        \n        ## finetraining \n        \n        for layer in model_1.layers[:11]:\n            layer.trainable = False\n            \n        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.1, verbose=1,mode='auto',patience=2, min_lr=1E-5)\n        early_st = tf.keras.callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=4, verbose=1, mode='auto',baseline=None, restore_best_weights=True)\n            \n        model_1.compile(optimizer=tf.optimizers.Adadelta(learning_rate=0.001/3),loss=losses.BinaryCrossentropy(label_smoothing=0.0005),metrics=logloss)\n\n        hist = model_1.fit([train_pri,train_sec],train_y, batch_size=128, epochs=200,verbose=1,validation_data = ([validation_pri,validation_sec],validation_y),callbacks =[reduce_lr,callbacks,early_st])\n            \n    \n        \n        # Save Model\n        model_1.save('_seed_'+str(seed)+'_fold_'+str(fold))\n\n        # OOF Score\n        y_val = model_1.predict([validation_pri,validation_sec])\n        oof += logloss(tf.constant(validation_y,dtype=tf.float32),tf.constant(y_val,dtype=tf.float32))/(n_folds*n_seeds)\n\n        # Run prediction\n        y_pred += model_1.predict([data_test_pri,data_test_sec])/(n_folds*n_seeds)\n        \n        fold += 1\n        \n        gc.collect()\n        tf.keras.backend.clear_session()\n        del train_pri\n        del train_sec\n        del validation_pri\n        del validation_sec\n        del train_y\n        del validation_y\n        del data_test_pri\n        del data_test_sec\n        \n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.print('OOF score is ',oof)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.utils.plot_model(model_1,show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfinal = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\nfinal.iloc[:,1:] = np.clip(y_pred,p_min,p_max)\nfinal.iloc[non_ctl_data_X_test,1:] = 0\n\n\nfinal.to_csv('submission.csv', index=False)\nfinal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}