{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_features=pd.read_csv(\"/kaggle/input/lish-moa/train_features.csv\")\ntrain_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features=pd.read_csv(\"/kaggle/input/lish-moa/test_features.csv\")\ntest_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**One hot encoding**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#The dataset has categorical data, but the model needs something numerical. So let's apply one hot encoding here.\ntrain_features.cp_type.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features['cp_type'] =train_features.cp_type.map(lambda x:0 if x == 'trt_cp' else 1)\ntest_features['cp_type'] =test_features.cp_type.map(lambda x:0 if x == 'trt_cp' else 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.cp_dose.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features['cp_dose'] =train_features.cp_dose.map(lambda x:0 if x =='D1' else 1)\ntest_features['cp_dose'] =test_features.cp_dose.map(lambda x:0 if x =='D1' else 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"replace_values = {24:1, 48:2, 72: 3}\ntrain_features['cp_time'] =train_features['cp_time'].map(replace_values)\ntest_features['cp_time'] =test_features['cp_time'].map(replace_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check cleaned table\ntrain_features_new=train_features.iloc[:, 1:]\ntrain_features_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored=pd.read_csv(\"/kaggle/input/lish-moa/train_targets_scored.csv\")\ntrain_targets_scored.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove sig_id for computing in a later stage\ntrain_targets_new=train_targets_scored.iloc[:, 1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate sum per column and find the top 100s. (The top 100 most significant factors?)\ntrain_targets_scored = train_targets_scored.set_index('sig_id')\ntrain_targets_scored.sum().nlargest(70)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored.sum().nsmallest(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored.sum(axis=1).nsmallest(200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored.sum().nlargest(20).plot.bar(figsize=(18,15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features_new=test_features.iloc[:, 1:]\ntest_features_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **Normalization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalize dataset ((x-min)/(max-min))\n\nnormalized_train_features=(train_features_new-train_features_new.min())/(train_features_new.max()-train_features_new.min())\nnormalized_test_features=(test_features_new-test_features_new.min())/(test_features_new.max()-test_features_new.min())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PCA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n#PCA will hold 80% of the variance and the number of components required to capture 80% variance will be used\npca = PCA(0.8)\npca.fit(normalized_train_features)\n\nPCA(copy=True, iterated_power='auto', n_components=0.8, random_state=42,\n  svd_solver='auto', tol=0.0, whiten=False)\nprint(pca.n_components_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pca.transform(normalized_train_features.values)\nX_test = pca.transform(normalized_test_features.values)\ny = train_targets_new.values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extremely sloooooow!\n\nfrom skmultilearn.model_selection import IterativeStratification\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import precision_score,recall_score, confusion_matrix, classification_report, accuracy_score, f1_score, log_loss\n\n#from sklearn.model_selection import train_test_split\ny = train_targets_new.values\n\ncnt=0\naccu_losses=[]\nk_fold = IterativeStratification(n_splits=5, order=1)\nfor train_index, val_index in k_fold.split(X, y):\n\n    X_train, X_val = X[train_index], X[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    clf =OneVsRestClassifier(LogisticRegression(solver='lbfgs',penalty='l2'), n_jobs=-1)\n\n    clf.fit(X_train, y_train)  \n \n    # Making a prediction on the test set \n\n    pred_train =clf.predict_proba(X_train)\n    pred_val = clf.predict_proba(X_val)\n    pred_test = clf.predict_proba(X_test)\n  \n   \n    # Evaluating the model\n       \n    # Evaluating the model\n    loss = log_loss(np.ravel(y_val), np.ravel(pred_val))\n    print (\"Fold\", cnt, \"loss value is:\",loss)\n    accu_losses.append(loss)\n    cnt+=1\nprint('mean of loss', np.mean(accu_losses))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('mean of loss', np.mean(accu_losses))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samp = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samp.iloc[:,1:] = pred_test\nsamp.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}