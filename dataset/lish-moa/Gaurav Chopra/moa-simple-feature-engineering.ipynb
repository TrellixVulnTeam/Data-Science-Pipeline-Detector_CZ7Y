{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://www.yourgenome.org/sites/default/files/illustrations/diagram/gene_expression_transcription_yourgenome.png\">\n\n<h6><center>Image credit: Genome Research Limited</center></h6>\n<h1><center>Mechanism of Action (MoA) Prediction</center></h1>"},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThis notebook is an introduction in how to generate features for the [Mechanisms of Action (MoA)](https://www.kaggle.com/c/lish-moa) in python. We will first go simple feature generation and then go through advance feature generation and preprocessing.\n\nIf you like it, feel free to upvote :)\n\nLet’s get started!\n\n**Importing libraries and reading the dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Understanding the data\n\nIn this competition, our aim is to predict the the Mechanism of Action (MoA) response(s) of different samples (`sig_id`) in `test_features.csv`, given information about the responses of 100 different types of human cells to various drugs in `train_features.csv`.\n\nFor the samples (`sig_id`) in `train_features.csv` we are also given additional (optional) binary MoA responses that we don’t need to predict `test_targets_nonscored.csv`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load data\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\ntrain_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# copy the original dataframe for feature generation\ntest_features1 = test_features.copy()\ntrain_features1 = train_features.copy()\ntrain_targets_nonscored1 = train_targets_nonscored.copy()\ntrain_targets_scored1 = train_targets_scored.copy()\nsample_submission1 = sample_submission.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# take a look at train_features.csv before digging deeper\ntrain_features.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### train_features.csv - what does it tell us\n\n> **Observations**:\n* we have 23814 observation and 207 features\n* `sig_id`: samples (sig_id) unique for each Mechanism of Action (MoA) response(s).\n* `cp_type`: tell us about samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle);\n* `cp_time`: how long it take for samples to generate Mechanism of Action (MoA) response(s)\n* `cp_dose`: indicate whether sample treated with high or low dose.\n\n*PS: columns starting with `g-` signify \"gene expression data\", and `c-` signify signify cell viability data.*\n\n> **Hypothesis**:\n* Since, we are given different feature types we will first try to use common feature generation methods to generate some new features and then we will move on advanced feature generation using domain knowledge about the features and the task.\n\n\n# Feature Engineering\n\nFeature generation is a process of creating new features. It helps us by making model training more simple and effective. Sometimes, we can engineer these features using by looking at data types, otherwise we require domain knowledge to create new features.\n\nFeature Engineering also depends on model that you are going to use CART (Classification and Regression Trees), Neural Networks or Gradient Boosting Decision Trees. Make sure to preprocess the features in a way that model can understand them. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract columns containing numerical values\nfloat_cols = train_features.select_dtypes(include=[np.float]).columns\n# for each column create a new column emthusiasing floating point value\nfor col in float_cols:\n    train_features[col] = train_features[col].apply(np.exp)\n    \ntrain_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how much days does it take for samples to show (MoA) response(s)\ntrain_features['days'] = (train_features['cp_time']/24).astype(np.int)\ntest_features['days'] = (test_features['cp_time']/24).astype(np.int)\ntrain_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly, we can create features indicating time `minutes`, and `seconds`."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features['minutes'] = train_features['cp_time']*60\ntest_features['minutes'] = test_features['cp_time']*60\ntrain_features['seconds'] = train_features['cp_time']*3600\ntest_features['seconds'] = test_features['cp_time']*3600\ntrain_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For gene expression data and cell viability data, we can taking the floating point value to show our model much more clearer difference in values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract columns containing numerical values\nfloat_cols = train_features.select_dtypes(include=[np.float]).columns\n# for each column create a new column emthusiasing floating point value\nfor col in float_cols:\n    train_features['frac_'+col] = train_features[col].apply(lambda x: x%1)\n    test_features['frac_'+col] = test_features[col].apply(lambda x: x%1)\n    \ntrain_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For categorical features, we can generate feature based on interaction between different features types. For example, sample treated with high dose and showed MoA response(s) in 24 hours, 48 hours and 72 hours respectively."},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature interaction between days and cp_dose\ntrain_features['cp_time_dose'] = train_features['cp_time'].astype(str)+train_features['cp_dose']\ntest_features['cp_time_dose'] = test_features['cp_time'].astype(str)+test_features['cp_dose']\n\ntrain_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Advance Feature Generation\n\nTo understand the data and explore it deeper, we need to do EDA. Since, @artgor, @headsortails and @isaienkov have already perform EDA, I will only discuss plots that will help us generate new features and connect with them to the domain knowledge so that we can comprehend the process of generating features based on EDA. Please go through this links for important parts of EDA process.\n\n**Useful Links:**\n* [Explorations of Action - MoA EDA](https://www.kaggle.com/headsortails/explorations-of-action-moa-eda)\n* [Mechanisms of Action (MoA) Prediction. EDA](https://www.kaggle.com/isaienkov/mechanisms-of-action-moa-prediction-eda)"},{"metadata":{},"cell_type":"markdown","source":"**Let's separate gene expression data and cell viability data and study them individually.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in train_features1.columns if col.startswith('g-')]\nCELLS = [col for col in train_features1.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# study gene expression data\ng_train_features = train_features1[GENES]\ng_test_features = test_features1[GENES]\ng_train_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# study cell viability data\nc_train_features = train_features1[CELLS]\nc_test_features = test_features1[CELLS]\nc_train_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(g_train_features.iloc[0].sort_values(), '.');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, gene expression data is not normal. It shows logits values are distributed across gene expression data as feature values.\n\nA simple data transformation can solve the problem. This is one of the awesome things you can learn in statistical books: in case of logits , exp transformations usually works well. \n\nHowever, if we go to [Useful Links](https://www.kaggle.com/c/lish-moa/overview/useful-links) and study [Corsello et al. “Discovering the anticancer potential of non-oncology drugs by systematic viability profiling,” Nature Cancer](https://doi.org/10.1038/s43018-019-0018-6). In extended data section, it states\n>  Median Fluorescence Intensity (MFI) values are calculated from fluorescence values for each replicate-condition-cell line combination and are log2-transformed. \n\nSo, more advanced revearse engineering technique for log2-transform can be applied.\n\n<img src=\"https://raw.githubusercontent.com/gauravchopracg/share_code/master/HeLa%20cell%20line%20dose%E2%80%93response%20curves%20with%20PDE3A%20genetic%20loss.png\">\n\n*(work in progress)*"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ng_exp_train = g_train_features.copy()\ng_exp_test = g_test_features.copy()\ncols = g_exp_train.columns\n\nfor col in cols:\n    g_exp_train[col] = np.exp(g_exp_train[col])\n    g_exp_test[col] = np.exp(g_exp_test[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(g_exp_train.iloc[0].sort_values(), '.');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Preprocessing\n\nFeature Preprocessing highly depends on model that you are going to use and it's dependence with the target variable. For example, if we are using [Catboost](https://catboost.ai/) all we need to do is generate new feature and it's built-in functions will try to find new feature interactions based on target variable that we have to predict whereas if we are using Neural Network as our model, we need to explore dependencies between our model and target variable. Depending on its nature linear or non-linear after that we need to preprocess it accordingly. \n\nThis part of notebook has been taken from [Feature Engineering Techniques](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575). Please upvote that discussion as ideas were taken from there."},{"metadata":{},"cell_type":"markdown","source":"Frequency Encoding\n\nFrequency encoding is a powerful technique that allows different models to see whether column values are rare or common."},{"metadata":{"trusted":true},"cell_type":"code","source":"# copy of dataframes to apply label encoding\ntrain_features_fe = train_features1.copy()\ntest_features_fe = test_features1.copy()\n\ntemp = train_features_fe['cp_dose'].value_counts().to_dict()\ntrain_features_fe['cp_dose_counts'] = train_features_fe['cp_dose'].map(temp)\ntrain_features_fe.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aggregations / Group Statistics\n\nProviding models with group statistics allows them to determine if a value is common or rare for a particular group. You calculate group statistics by providing pandas with 3 variables. You give it the group, variable of interest, and type of statistic. For example, "},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = train_features_fe.groupby('cp_dose')['g-0'].agg(['mean']).rename({'mean':'g-0_cp_dose_mean'},axis=1)\ntrain_features_fe = pd.merge(train_features_fe,temp,on='cp_dose',how='left')\ntrain_features_fe.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**These are actually all the feature i generated and experiemented with your imagination to create as many features you can.**"},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection\n\nFeature Selection is not necessary most of time , when i experimented with catboot classifier and split neural network in both cases removing features reduces both the cv and lb score, but i haven't tested all the models. So, make sure to generate feature interation in data, then preprocess your features based on the model after that check if feature selection helps.\n\n\nbelow code cell has been taken from [amazing notebook by simakov - keras Multilabel Neural Network v1.2](https://www.kaggle.com/simakov/keras-multilabel-neural-network-v1-2)"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Source: https://www.kaggle.com/simakov/keras-multilabel-neural-network-v1-2\n# add seed and change create_model\n\n'''\nfrom typing import Tuple, List, Callable, Any\n\nfrom sklearn.utils import check_random_state  # type: ignore\n\n### from eli5\ndef iter_shuffled(X, columns_to_shuffle=None, pre_shuffle=False,\n                  random_state=None):\n    rng = check_random_state(random_state)\n\n    if columns_to_shuffle is None:\n        columns_to_shuffle = range(X.shape[1])\n\n    if pre_shuffle:\n        X_shuffled = X.copy()\n        rng.shuffle(X_shuffled)\n\n    X_res = X.copy()\n    for columns in tqdm(columns_to_shuffle):\n        if pre_shuffle:\n            X_res[:, columns] = X_shuffled[:, columns]\n        else:\n            rng.shuffle(X_res[:, columns])\n        yield X_res\n        X_res[:, columns] = X[:, columns]\n\n\n\ndef get_score_importances(\n        score_func,  # type: Callable[[Any, Any], float]\n        X,\n        y,\n        n_iter=5,  # type: int\n        columns_to_shuffle=None,\n        random_state=None\n    ):\n    rng = check_random_state(random_state)\n    base_score = score_func(X, y)\n    scores_decreases = []\n    for i in range(n_iter):\n        scores_shuffled = _get_scores_shufled(\n            score_func, X, y, columns_to_shuffle=columns_to_shuffle,\n            random_state=rng, base_score=base_score\n        )\n        scores_decreases.append(scores_shuffled)\n\n    return base_score, scores_decreases\n\n\n\ndef _get_scores_shufled(score_func, X, y, base_score, columns_to_shuffle=None,\n                        random_state=None):\n    Xs = iter_shuffled(X, columns_to_shuffle, random_state=random_state)\n    res = []\n    for X_shuffled in Xs:\n        res.append(-score_func(X_shuffled, y) + base_score)\n    return res\n\ndef metric(y_true, y_pred):\n    metrics = []\n    for i in range(y_pred.shape[1]):\n        if y_true[:, i].sum() > 1:\n            metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float)))\n    return np.mean(metrics)   \n\nperm_imp = np.zeros(train.shape[1])\nall_res = []\nfor n, (tr, te) in enumerate(KFold(n_splits=7, random_state=0, shuffle=True).split(train_targets)):\n    print(f'Fold {n}')\n\n    model = create_model(len(train.columns))\n    checkpoint_path = f'repeat:{seed}_Fold:{n}.hdf5'\n    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n    cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n                                     save_weights_only = True, mode = 'min')\n    model.fit(train.values[tr],\n                  train_targets.values[tr],\n                  validation_data=(train.values[te], train_targets.values[te]),\n                  epochs=35, batch_size=128,\n                  callbacks=[reduce_lr_loss, cb_checkpt], verbose=2\n                 )\n        \n    model.load_weights(checkpoint_path)\n        \n    def _score(X, y):\n        pred = model.predict(X)\n        return metric(y, pred)\n\n    base_score, local_imp = get_score_importances(_score, train.values[te], train_targets.values[te], n_iter=1, random_state=0)\n    all_res.append(local_imp)\n    perm_imp += np.mean(local_imp, axis=0)\n    print('')\n    \ntop_feats = np.argwhere(perm_imp < 0).flatten()\ntop_feats\n'''\n\n# print(top_feats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thanks and I hope you discovered something new while reading it. In this competition, Laboratory for Innovation Science at Harvard, presented us with a dataset to develop an algorithm to predict a compound’s MoA given its cellular signature which will help scientists advance the drug discovery process. I hope to contribute as my exam's are still going I'll be updating this notebook whenever it's possible. Happy Learning!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}