{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#Implementation of Swap Noise to create robust DAEs.  This yields one trained embedder per fold per split.\n#The encoders are then used downstream for extracting embeddings from the data.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#HYPERPARAMETERS\nSWAP_PERC = .15\nSPLITS = 5 \nEPOCHS = 70 \nBATCH_SIZE = 128\nN_STARTS = 7","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport pickle\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nfrom tqdm.notebook import tqdm\nimport os\nimport gc\n\nfrom scipy.special import erfinv as sp_erfinv\nfrom sklearn.preprocessing import QuantileTransformer #from https://www.kaggle.com/kushal1506/moa-pytorch-0-01859-rankgauss-pca-nn/comments#PCA-features-+-Existing-features","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\n\nss = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    df = df.copy()\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24:-1, 48:0, 72:1})\n    del df['sig_id']\n    return df\n\ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']\n\n#Deleting all control samples.  These have no MoA so makes sense...\ntrain_targets = train_targets.loc[train['cp_type']==0].reset_index(drop=True)\ntrain = train.loc[train['cp_type']==0].reset_index(drop=True)\ntest = test.loc[test['cp_type']==0].reset_index(drop=True)\n\nprint(train.shape, test.shape)\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from time import time\ndef rankGauss(train, test):\n    train_rg = train.copy()\n    test_rg = test.copy()\n    transformer = QuantileTransformer(n_quantiles=1000,random_state=0, output_distribution=\"normal\")\n    \n    for col in train.columns[3:]:\n        vec_len = len(train_rg[col].values)\n        vec_len_test = len(test_rg[col].values)\n        raw_vec = train_rg[col].values.reshape(vec_len, 1)\n        transformer.fit(raw_vec)\n\n        train_rg[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n        test_rg[col] = transformer.transform(test_rg[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]        \n    return train_rg, test_rg\n\nstart = time()\ntrain_rg, test_rg = rankGauss(train, test)\nprint(f'{time() - start :.2f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n#The heart of the kernel.  This generator, SwapNoise, replaces swap_perc of \n# data with noise generated from its distribution.  The noise for all columns\n# but the 1st 2 will be Gaussian(0,1);  \n# the 1st will be drawn from [-1, 0, 1] (cp_time);\n# the 2rd from [0,1] (cp_dose).\n###############################################################################\nclass SwapNoise(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, data, swap_perc = .15, batch_size = 128, shuffle=True):\n        'Initialization'\n        self.batch_size = batch_size\n        self.data = data\n        self.swap_perc = swap_perc\n\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return self.data.shape[0] // self.batch_size\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        #Generate indexes of the batch\n        indices = [i for i in range(index*self.batch_size, (index+1)*self.batch_size)]\n        #print(indices)\n        noisy = self.swap_noise(self.data[indices])\n        \n        return (noisy), self.data[indices]\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        if self.shuffle == True:\n            np.random.shuffle(self.data)\n    def get_data(self):\n        return self.data\n    def swap_noise(self, temp):\n        'Generates data containing batch_size samples' \n        \n        # Add Swap_noise by row for 'g' and 'c' features.  These are all 'normal' due to rankGauss\n        for index in range(temp.shape[0]):            \n            num_swaps = np.random.binomial(temp.shape[1], self.swap_perc, size=1)# how many swaps this row\n            if num_swaps ==0:\n                num_swaps=1\n            swap_rvs = np.random.normal(size=num_swaps) #new values to be swapped in \n            positions_in_row = np.random.choice(a= range(2,temp.shape[1]) , size=num_swaps, replace=False)# new positions to be swapped in\n            temp[index, positions_in_row] = swap_rvs#swapping\n        \n        #Adding Swap_Noise by column for 'cp_time'.  These have distr of [-1,0,1]\n        num_swaps = np.random.binomial(temp.shape[0], self.swap_perc, size=1)\n        if num_swaps ==0:\n            num_swaps=1\n        swap_rvs = np.random.choice(a= np.array([-1,0,1]), size=num_swaps, replace=True) #new values to be swapped in \n        positions_in_col = np.random.choice(a= np.array(range(temp.shape[0])), size=num_swaps, replace=False)# new positions to be swapped in\n        #print(positions_in_col)\n        temp[positions_in_col, 0] = swap_rvs#swapping new values for cp_time\n\n\n        #Adding Noise by column for 'cp_dose'.  These have distr of [0,1]\n        num_swaps = np.random.binomial(temp.shape[0], self.swap_perc, size=1)\n        if num_swaps ==0:\n            num_swaps=1\n        swap_rvs = np.random.choice(a= np.array([0,1]), size=num_swaps, replace=True) #new values to be swapped in \n        positions_in_col = np.random.choice(a= np.array(range(temp.shape[0])), size=num_swaps, replace=False)# new positions to be swapped in\n        temp[positions_in_col, 1] = swap_rvs#swapping new values for cp_time\n        #print(temp)\n        \n        return temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Batch_Drop_Dense(x, layer_size, drop_rate, activation = 'relu'):\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(drop_rate)(x)\n        x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(layer_size, activation=activation))(x)\n        return x\n    \n###################################################################\n#Residual Block that keeps the inputs as the same size as the \n# incoming layer.\n###################################################################\ndef Residual_Block_same(prev_nonActivations, size, drop_rate, lv):\n    x = tf.keras.layers.BatchNormalization()(prev_nonActivations)\n    x = tf.keras.activations.relu(x)\n    x = tf.keras.layers.Dropout(drop_rate)(x)\n    x = tf.keras.layers.Dense(size)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.activations.relu(x)\n    x = tf.keras.layers.Dropout(drop_rate)(x)\n    x = tf.keras.layers.Dense(size)(x)\n    return tf.keras.layers.Add(name=f'Residuals_same{lv}')([x, prev_nonActivations])\n\n###################################################################\n#Residual Block where the size of the network changes.\n###################################################################\ndef Residual_Block_diff(prev_nonActivations, size, drop_rate, lv):\n    resized_prev = tf.keras.layers.Dense(size)(prev_nonActivations)\n\n    x = tf.keras.layers.BatchNormalization()(prev_nonActivations)\n    x = tf.keras.activations.relu(x)\n    x = tf.keras.layers.Dropout(drop_rate)(x)\n    x = tf.keras.layers.Dense(size)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.activations.relu(x)\n    x = tf.keras.layers.Dropout(drop_rate)(x)\n    x = tf.keras.layers.Dense(size)(x)\n    return tf.keras.layers.Add(name=f'Residuals_diff{lv}')([x, resized_prev])\n\ndef create_DAE(num_columns, middle=1024):\n    inp = tf.keras.layers.Input(num_columns)\n    \n    #x = tf.keras.layers.Dropout(.15)(inp) Replaced with .15 swapnoise from the generator\n    x = tf.keras.layers.Dense(middle)(inp)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.activations.relu(x)\n    \n    x = Residual_Block_same(x, middle, .2, 1)\n    x = Residual_Block_diff(x, num_columns, .2, 2)\n\n    model = tf.keras.Model(inputs=inp, outputs=x)\n    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period=10),\n                  loss='mse', \n                  )\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Minor check to make sure the architecture works.  Run a toy DAE \nBATCH_SIZE = 128\ntrain_gen = SwapNoise(train.values[:,1:], swap_perc = .15, batch_size = BATCH_SIZE, shuffle=True)\nval_gen = SwapNoise(test.values[:,1:], swap_perc = .15, batch_size = BATCH_SIZE, shuffle=True)\n\nmodel = create_DAE(train.shape[1]-1, 1092)\nmodel.fit(train_gen, validation_data = val_gen, epochs =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TRAINING THE AUTOENCODERS.  ONE PER SPLIT\ntf.random.set_seed(42)\n\nfor seed in range(N_STARTS):\n    for fold, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits=SPLITS, random_state=seed, shuffle=True).split(train_targets, train_targets)):\n        for EMBEDDING_DIMS in [1024]:\n\n            tf.keras.backend.clear_session()\n            model_AE = create_DAE(train.shape[1]-1, EMBEDDING_DIMS)\n            checkpoint_path = f'FEATURE_AE_repeat_{seed}_Fold:{fold}_dim{EMBEDDING_DIMS}.hdf5'\n            reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n            cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n                                         save_weights_only = True, mode = 'min')\n            \n            train_gen = SwapNoise(np.concatenate((train.values[tr][:, 1:], test.values[:, 1:]), axis=0), \n                                  swap_perc = SWAP_PERC, batch_size = BATCH_SIZE, shuffle=True)\n            val_gen = SwapNoise(train.values[te][:, 1:], \n                                  swap_perc = SWAP_PERC, batch_size = BATCH_SIZE, shuffle=True)\n            h = model_AE.fit(train_gen, validation_data=val_gen,\n                      epochs=EPOCHS, batch_size=BATCH_SIZE,\n                      callbacks=[reduce_lr_loss, cb_checkpt], verbose=2\n                     )\n            model_AE.load_weights(checkpoint_path)\n            embedder = tf.keras.Model(inputs = model_AE.input, outputs = model_AE.get_layer(name='Residuals_same1').output)\n\n            os.remove(checkpoint_path)\n            tf.keras.models.save_model(embedder, checkpoint_path)\n            del train_gen, val_gen; gc.collect()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}