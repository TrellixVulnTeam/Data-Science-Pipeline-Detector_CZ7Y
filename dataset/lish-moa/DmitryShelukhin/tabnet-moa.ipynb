{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport time\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.backend as B\n\nimport tensorflow_addons as tfa\n\nfrom keras.models import Model\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.utils import shuffle\nfrom sklearn.mixture import BayesianGaussianMixture\nfrom sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_folder = '../input/lish-moa/'\ntrain=pd.read_csv(data_folder + '/train_features.csv')\ntest=pd.read_csv(data_folder + '/test_features.csv')\n\ntargets=pd.read_csv(data_folder + '/train_targets_scored.csv')\ntargets_ns = pd.read_csv(data_folder + '/train_targets_nonscored.csv')\n\ndrug_table = pd.read_csv(data_folder + '/train_drug.csv')\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train rows:', len(train.index))\nprint('Test rows:', len(test.index))\n\ntag = 'tag'\ntrain[tag] = 'train'\ntest[tag] = 'test'\n\ntotal=pd.concat([train, test])\nprint('Total rows:', len(total.index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = train.columns.to_list()\n\nc_columns = [col for col in columns if col.startswith('c-')]\nprint('Number of c_cols:', len(c_columns))\ng_columns = [col for col in columns if col.startswith('g-')]\nprint('Number of g_cols:', len(g_columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_col='sig_id'\n\ntype_col='cp_type'\ntime_col='cp_time'\ndose_col='cp_dose'\n\nprint('Type:', total[type_col].unique())\nprint('Time:', total[time_col].unique())\nprint('Dose', total[dose_col].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles=total.loc[total[type_col]=='ctl_vehicle']\nprint('Total vehicles:',len(vehicles.index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets_agg = targets.agg([sum])\ntargets_agg.drop(['sig_id'], axis=1,inplace=True)\ntarget_col = targets_agg.columns\nnp.sort(targets_agg.values)\nzero_targets = targets_agg[targets_agg.columns[(targets_agg<10).any()]]\nzero_targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drug_id = 'drug_id'\ndrug_targets = pd.concat([drug_table[drug_id], targets], axis = 1)\ngrouped = drug_targets.groupby(drug_id)\ndrug_examples = grouped[drug_id].count()\n\ntrain_ids = train[id_col]\ntrain_nums = grouped[drug_id].transform('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total2 = total.copy()\ntargets2 = targets.copy()\n\n# exclude zero targets (underrepresented columns)\n#targets2.drop(zero_targets.columns, axis=1, inplace=True)\n\n# add non-scored targets\nscored_columns = targets2.columns\nscored_columns = scored_columns.drop('sig_id')\nnum_scored = len(scored_columns)\ntargets2 = pd.concat([targets2, targets_ns], axis = 1)\n\n# Change cp_dose\ntotal2[dose_col]=total2[dose_col].map({'D1':0, 'D2':1})\n\n# Change cp_time\ndef onehot_time(x):\n    return [(x==24).astype(int), (x==48).astype(int), (x==72).astype(int)]\ntotal2['t1'], total2['t2'], total2['t3'] = onehot_time(total2[time_col])\n\n# Drop cp_time\ntotal2.drop([time_col], axis=1, inplace=True)\n\n# Remove tags\ntotal2.drop([tag], axis=1, inplace=True)\n\n# Remove vehicles\nvehicles_mask = total2[type_col] == 'ctl_vehicle'\nn_train = len(targets2.index)\n\nvehicles = total2[vehicles_mask]\ntotal2 = total2[~vehicles_mask]\ntargets2 = targets2[~vehicles_mask[:n_train]]\ntrain_nums2 = train_nums[~vehicles_mask[:n_train]]\ntrain_nums2 = np.array(train_nums2.to_list())\n\n\n# Drop cp_type\ntotal2.drop([type_col], axis=1, inplace=True)\nvehicles.drop([type_col], axis=1, inplace=True)\n\n# Remove indexes\ndef rm_index(df):\n    df.reset_index(drop=True, inplace=True)\n    df.drop([id_col], axis=1, inplace=True)\n\nrm_index(total2)\nrm_index(targets2)\nrm_index(vehicles)\n\ntotal2.head()\ntargets2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale_total(total2, scaler = None):\n    #TODO: find better way\n    if scaler is None:\n        scaler = MinMaxScaler()\n        scaler.fit(total2)\n\n    total2 = pd.DataFrame(scaler.transform(total2))\n    #total2 = scaler.transform(total2)\n    return total2, scaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_scaled, scaler = scale_total(total2)\nvehicles_scaled, _ = scale_total(vehicles, scaler)\ntotal_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = None\ndef set_seed(s):\n    tf.random.set_seed(seed)\n    return s\nseed = set_seed(42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_proc=total_scaled.values\ntrain_targets=targets2[scored_columns]\ntrains_without_veh = len(train_targets.index)\ncorr_train_set = total_proc[:trains_without_veh]\n#test_set = total_proc[trains_without_veh:]\n\nnum_columns = corr_train_set.shape[1]\nnum_targets = train_targets.shape[1]\nprint('Train data', corr_train_set.shape)\nprint('Train targets', train_targets.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CorrLoss(tf.keras.losses.Loss):\n    def __init__(self, mu, sigma, moving_mean, moving_variance, *args, **kwargs):\n        super(CorrLoss, self).__init__(*args, **kwargs)\n        self.mu = mu\n        self.b = sigma * moving_variance\n        self.moving_mean = moving_mean\n        #self.moving_variance = moving_variance\n    \n    def __call__(self, y_true, y_pred, *args, **kwargs):\n        a = tf.reduce_mean((y_true - self.mu) * (y_pred - self.moving_mean), axis = 0)\n        result = tf.math.divide(a, self.b)\n        result = tf.math.abs(1 - result)\n        #print('Here', result.shape, self.b.shape)\n        return tf.reduce_mean(result)\n\ndef zero_loss(y_true, y_pred):\n    return 0#tf.zeros([tf.shape(y_pred)[0]], dtype=tf.float32)\n\ndef create_corr_model(num_inputs, num_outputs, mu, sigma):\n    in_layer = L.Input(num_inputs)\n\n    #model.add(tfa.layers.WeightNormalization(L.Dense(int(part*num_inputs), \n    #                                                 activation='sigmoid')))\n    base_layer = in_layer#L.BatchNormalization()(in_layer)#in_layer\n    \n    hidden = L.Dense(1024, use_bias = False)(base_layer)\n    hidden = L.BatchNormalization()(hidden)\n    hidden = L.Activation('tanh')(hidden)\n    \n    out_layer = L.Dense(num_outputs, name='out', use_bias = False)(hidden)\n    \n    hidden = L.Dense(1024, use_bias = False)(base_layer)\n    hidden = L.BatchNormalization()(hidden)\n    hidden = L.Activation('tanh')(hidden)\n    \n    out_layer += L.Dense(num_outputs, name='out1', use_bias = False)(hidden)\n    \n    out_layer = L.Activation('sigmoid', name='final')(out_layer)\n    bn = L.BatchNormalization(name = 'bn')\n    out2 = bn(out_layer)\n    moving_mean = bn.variables[2]\n    moving_variance = bn.variables[3]\n\n    model = Model(in_layer, [out_layer, out2])\n    model.compile(optimizer=tf.keras.optimizers.Adam(0.001),#tfa.optimizers.AdamW(learning_rate = 0.001, weight_decay = 0.0005)\n                 loss = [CorrLoss(mu, sigma, moving_mean, moving_variance), zero_loss])\n    #model.compile(optimizer=tf.keras.optimizers.Adam(0.00035), loss=tf.keras.losses.MeanSquaredError())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def do_train(X, y):\n    num_inputs = X.shape[1]\n    num_outputs = y.shape[1]\n\n    mu = np.mean(y, axis = 0)\n    sigma = np.std(y, axis = 0)\n    \n#kf=KFold(2, shuffle=True, random_state=seed)\n    id_range = np.arange(X.shape[0])\n    train_ids, val_ids= train_test_split(id_range, test_size=0.4, random_state=seed)\n\n    checkpoint_filepath = 'corr_checkpoint.h5'\n    corr_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        filepath=checkpoint_filepath,\n        metric = 'val_final_loss',\n        save_weights_only=True,\n        save_best_only=True)\n    \n    model = create_corr_model(num_inputs, num_outputs, mu, sigma)\n    #model.fit(X[train_ids],\n    #          y[train_ids],\n    #          validation_data=(X[val_ids], y[val_ids]),\n    #          callbacks = [corr_checkpoint_callback],\n    #          epochs=200, batch_size=2048)\n\n    return model, (train_ids, val_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_model, (corr_train_ids, corr_val_ids) = do_train(corr_train_set, train_targets.values.astype(np.float))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_pca = 64\npca = PCA(n_components=n_pca)\npca_features = pca.fit_transform(total_scaled.values)\npca_columns = ['pca-{:03d}'.format(i) for i in range(n_pca)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mixture = BayesianGaussianMixture(n_components = 64, verbose = 2)\n#mixture.fit(total_scaled)\n#mix_features = mixture.predict_proba(total_scaled)\n#mix_columns = ['mix-{:03d}'.format(i) for i in range(mix_features.shape[1])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#kmeans = KMeans(128)\n#kmeans.fit(total_scaled)\n#kmeans_features = kmeans.transform(total_scaled)\n#kmeans_columns = ['kmeans-{:03d}'.format(i) for i in range(kmeans_features.shape[1])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_total = pd.DataFrame(total_proc, columns = total2.columns)\n#added_features = np.concatenate([pca_features, mix_features, kmeans_features], axis = 1)\n#added_columns = pca_columns + mix_columns + kmeans_columns\nadded_features = pca_features\nadded_columns = pca_columns\nnew_total = pd.concat([new_total, pd.DataFrame(added_features, columns = added_columns)], axis = 1)\ntrain_targets=targets2[scored_columns]\ntrain_set = new_total.loc[:trains_without_veh-1]\ntest_set = new_total.loc[trains_without_veh:]\n\nnum_columns = train_set.shape[1]\nnum_targets = train_targets.shape[1]\nprint('Train data', train_set.shape)\nprint('Train targets', train_targets.shape)\nprint('Test set', test_set.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# partially from https://github.com/google-research/google-research/blob/master/tabnet/tabnet_model.py\ndef glu(act, n_units):\n  \"\"\"Generalized linear unit nonlinear activation.\"\"\"\n  return act[:, :n_units] * tf.nn.sigmoid(act[:, n_units:])\n\ndef dense_bn_glu(input_size, num_features, dropout = 0.5):\n        in_layer = L.Input(input_size)\n        base_layer = L.Dropout(dropout)(in_layer)#in_layer\n        layer = L.Dense(2 * num_features)(base_layer)\n        layer = L.BatchNormalization()(layer)\n        layer = glu(layer, num_features)\n        \n        return Model(in_layer, layer)\n\ndef combine_models(model1, model2):\n    model2_output = model2(model1.output)\n    return Model(model1.input, model2_output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logic handling TabNet\ndef create_TabNet2_model(num_columns, n_steps = 4, dropout = 0.5):\n    num_shared = 1\n    num_unique = 2\n    num_features = 2*num_scored\n    num_output_features = num_scored\n    num_steps = n_steps\n    relaxation = 1.5\n    \n    input_layer = L.Input(num_columns)\n    input_base = L.BatchNormalization()(input_layer)\n    masked_input = input_base\n    output_agg = []#tf.ones([0, num_scored, 1])#np.empty([0, num_scored, 1], dtype=np.float32)#0\n    agg_compl_mask = 1\n    \n    mask_agg = [] # for sparse loss\n    epsilon = 1e-6\n    \n    # Static mask\n    output_mask = np.random.randint(2, size=[1, num_scored, num_steps])\n    output_mask_compensation = num_steps / (np.sum(output_mask, axis=2, keepdims = True) + 1e-6)\n    output_mask = output_mask * output_mask_compensation\n    \n    #Learned mask agg\n    out_agg_compl_mask = 1\n    out_mask_agg = []\n    \n    \n    shared_model = dense_bn_glu(num_columns, num_features, dropout)\n    for i in range(num_shared-1):\n        shared_model = combine_models(shared_model, \n                                      dense_bn_glu(num_features, num_features, dropout))\n\n    def feature_transformer(num_layers, num_features, num_shared_features = None):\n        cur_model = shared_model\n        if (num_shared_features is None):\n            num_shared_features = num_features\n        for i in range(num_layers):\n            cur_model = combine_models(cur_model,\n                                       dense_bn_glu(num_shared_features, num_features, dropout))\n        return cur_model\n\n    def attentive_transformer(num_features, input_data):\n        hidden = L.Dense(num_features)(input_data)\n        hidden = L.BatchNormalization()(hidden)\n        return hidden\n    \n    #control_features = feature_transformer(num_unique, num_features)(masked_input)\n    cur = feature_transformer(num_unique, num_features)(masked_input)\n    coef_features = cur[:,num_output_features:]\n    for i in range(num_steps):\n        mask = attentive_transformer(num_columns, coef_features)\n        mask *= agg_compl_mask\n        mask = L.Activation('sigmoid')(mask)\n        mask_agg.append(tf.expand_dims(mask, axis = 2))\n        agg_compl_mask *= (relaxation - mask)\n        masked_input = mask * input_base\n        \n        cur = feature_transformer(num_unique, num_features)(masked_input)\n        coef_features = cur[:,num_output_features:]\n        output_features = tf.expand_dims(L.Activation('sigmoid')(cur[:,:num_output_features]), axis = 2)\n        #output_agg = tf.concat([output_agg, (1.0/num_steps)*output_features], axis = 2)\n        \n        # Learned out mask START\n        out_mask = attentive_transformer(num_scored, coef_features)\n        out_mask *= out_agg_compl_mask\n        out_mask = L.Activation('sigmoid')(out_mask)\n        out_agg_compl_mask *= (relaxation - out_mask)\n        out_mask = tf.expand_dims(out_mask, axis = 2)\n        out_mask_agg.append(out_mask)\n        # Learned out mask END\n        \n        output_agg.append((1.0/num_steps)*output_features)\n    \n    output = tf.concat(output_agg, axis=2)\n    \n    #Use random output mask\n    #output = L.Dropout(0.5, noise_shape=[None, 1, num_steps])(output)\n    \n    #Use static output mask\n    #output = output_mask * output\n    \n    #Use learned output mask\n    out_mask = tf.concat(out_mask_agg, axis = 2)\n    mask_loss = L.Lambda(lambda x: x, name='mask')(out_mask)\n    out_mask_compensation = num_steps / (tf.reduce_sum(out_mask, axis=2, keepdims = True) + epsilon)\n    out_mask = out_mask_compensation*out_mask#L.Lambda(lambda x:  * x, name='mask_norm')(out_mask)\n    output = out_mask * output\n    \n    # Calc sparse loss\n    mask_total = tf.concat(mask_agg, axis = 2)\n    sparse_val = -mask_total*tf.math.log(mask_total+epsilon)\n    sparse_val = tf.reduce_sum(sparse_val, axis = 1) # sum on dimensions\n    sparse_loss = L.Lambda(lambda x:tf.reduce_mean(x), name='sparse')(sparse_val)\n    \n    \n    output = L.Lambda(lambda x:tf.reduce_sum(x, axis = 2), name='output')(output)\n    \n    return Model(input_layer, [output, mask_loss, sparse_loss])#, mask_loss, sparse_loss])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dummy_loss(y_true, y_pred):\n    return y_pred\n\nbase_lr = 0.01\n\ndef compile_model(model, lr = base_lr):\n    focal_loss = tfa.losses.SigmoidFocalCrossEntropy()\n    binary_loss = tf.keras.losses.BinaryCrossentropy()\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr), \n                  loss=[binary_loss, binary_loss, dummy_loss],\n                  metrics={'output':focal_loss},\n                  loss_weights=[1.0, 0.0003, 0.0000001]\n                  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def smooth_labels(labels, factor=0.1):\n    # smooth the labels\n    labels *= (1.0 - factor)\n    labels += (factor / labels.shape[1])\n    # returned the smoothed labels\n    return labels\n\ndef get_mask_loss_data(train_shape, val_shape, f=np.zeros):\n    return f(train_shape), f(val_shape)\n\ndef get_sparse_loss_data(train_shape, val_shape):\n    return get_mask_loss_data(train_shape, val_shape, np.zeros)\n\nclass Cycle_LR():\n    def __init__(self, base_lr, cycle_start, period):\n        self.base_lr = base_lr\n        self.cycle_start = cycle_start\n        self.period = period\n        \n        self.alpha1 = 0.3 * base_lr\n        self.alpha2 = 0.03 * self.alpha1\n\n    def get_lr(self, epoch, cur):\n        if epoch == 35:\n            print('Cur lr', cur)\n            return self.base_lr * 0.33\n        if epoch > self.cycle_start:\n            t = ((epoch - self.cycle_start) % self.period) / self.period\n            cur = self.alpha2*t + self.alpha1 * (1-t)\n            print('New lr:', cur)\n        return cur","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_folds = 1\nnum_steps = 8\nid_range = np.arange(train_targets.shape[0])\n\ntarget_vals = train_targets.values[:,:num_scored].astype(float)\n\ndef train_classifier(train_set, model_filename, epochs = 150, lr = base_lr, dropout = 0.5):\n    #kf=KFold(num_folds, shuffle=True, random_state=seed)\n    for k, (train_ids, test_ids) in enumerate([(id_range,id_range)]):#kf.split(train_set,target_vals)):\n        classifier = create_TabNet2_model(train_set.shape[1], num_steps, dropout) #create_TabNet_model()\n        compile_model(classifier, lr)\n\n        checkpoint_filepath = model_filename + '_checkpoint{:02d}.h5'.format(k)\n        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n            filepath=checkpoint_filepath,\n            metric = 'val_output_loss',\n            save_weights_only=True,\n            save_best_only=True)\n\n        cycle_lr = Cycle_LR(lr, 70, 10)\n        lr_schedule = tf.keras.callbacks.LearningRateScheduler(cycle_lr.get_lr, verbose=0)\n\n        #test_without_veh = test_ids[~vehicles_mask.values[test_ids]]\n        mask_train, mask_val = get_mask_loss_data([*(target_vals[train_ids].shape), num_steps], \n                                                   [*(target_vals[test_ids].shape), num_steps], np.zeros)\n        sparse_train, sparse_val = get_sparse_loss_data([target_vals[train_ids].shape[0], 1], [target_vals[test_ids].shape[0], 1])\n\n        train_true_vals = [smooth_labels(target_vals[train_ids])]\n        val_true_vals = [target_vals[test_ids]]\n\n        train_true_vals.append(mask_train)\n        val_true_vals.append(mask_val)\n\n        train_true_vals.append(sparse_train)\n        val_true_vals.append(sparse_val)\n        \n        sample_weights = train_nums2[train_ids]\n        classifier.fit(train_set[train_ids],\n              train_true_vals,\n              sample_weight = 8.0 / sample_weights,\n              validation_data=(train_set[test_ids], val_true_vals),\n              callbacks=[model_checkpoint_callback, lr_schedule],\n              epochs=epochs, batch_size=256)\n        return classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_model_filename = 'tabnet_classifier'\ncorr_train_ids, corr_val_ids = train_test_split(id_range, test_size=0.4, random_state=seed + 1)\nc_model = train_classifier(train_set.values, c_model_filename, 80, lr = 0.03, dropout = 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict by all models from folds\ndef make_prediction(test_set):\n    results_c = []\n    results_g = []\n    for k in range(num_folds):\n        c_model.load_weights(c_model_filename + '_checkpoint{:02d}.h5'.format(k))\n        results_c.append(c_model.predict(test_set)[0])\n        \n        #g_model.load_weights(g_model_filename + '_checkpoint{:02d}.h5'.format(k))\n        #results_g.append(g_model.predict(test_set[g_columns])[0])\n        \n    #result = 0.5*(np.mean(results_g, axis = 0) + np.mean(results_c, axis = 0))\n    result = np.mean(results_c, axis = 0)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(data_folder + '/sample_submission.csv')\nsample_submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_no_veh = ~vehicles_mask[n_train:]\nsample_submission.loc[:, scored_columns]=0\nresults = make_prediction(test_set)\nsample_submission.loc[submission_no_veh, scored_columns]=results\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}