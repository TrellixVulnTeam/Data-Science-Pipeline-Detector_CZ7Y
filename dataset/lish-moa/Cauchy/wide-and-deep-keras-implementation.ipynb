{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NumericalFeatureEncoder:\n    \"\"\"\n    Class encodes numerical features using tensorflow feature_columns\n    \"\"\"\n\n    def __init__(self, features):\n        self.features = features\n\n    def encode(self, X=None):\n        \"\"\"\n        Set inputs for numerical features\n        \"\"\"\n        numerical_inputs, feature_encoders = {}, {}\n\n        for feature in self.features:\n            numerical_inputs[feature] = tf.keras.Input(shape=(1,), name=feature, dtype=tf.float32)\n            feature_encoders[feature] = tf.feature_column.numeric_column(feature)\n        return numerical_inputs, [feature for _, feature in feature_encoders.items()]\n\n\nclass CategoricalFeatureEncoder:\n    \"\"\"\n    Class encodes Categorical features using tensorflow feature_columns\n    \"\"\"\n    def __init__(self, features=None):\n        self.features = features\n\n    def encode(self, X=None):\n        \"\"\"\n        Set inputs and catergorical vocab list\n        \"\"\"\n        feature_vocab_list, categorical_inputs, feature_encoders = {}, {}, {}\n\n        for feature in self.features:\n            categorical_inputs[feature] = tf.keras.Input(shape=(1,), name=feature, dtype=tf.string)\n            feature_vocab_list[feature] = tf.feature_column.categorical_column_with_vocabulary_list(feature, X[feature].unique().tolist())\n            feature_encoders[feature] = tf.feature_column.indicator_column(feature_vocab_list[feature])\n        return categorical_inputs, [feature for _, feature in feature_encoders.items()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeatureTransformer:\n    \"\"\"\n    Feature encoder specifically for Wide and Deep network\n    \"\"\"\n    def __init__(self, gene_features=None, cell_features=None, categorical_features=None):\n        self.gene_features = gene_features\n        self.cell_features = cell_features\n        self.categorical_features = categorical_features\n\n    def transform(self, X):\n        gene_feature_inputs, gene_feature_encoders = NumericalFeatureEncoder(self.gene_features).encode(X) \n        cell_feature_inputs, cell_feature_encoders = NumericalFeatureEncoder(self.cell_features).encode(X) \n        categorical_inputs, categorical_feature_encoders = CategoricalFeatureEncoder(self.categorical_features).encode(X)\n\n        feature_layer_inputs = {\n                                **gene_feature_inputs,\n                                **cell_feature_inputs,\n                                **categorical_inputs\n                                }\n \n        return feature_layer_inputs, categorical_feature_encoders, gene_feature_encoders, cell_feature_encoders","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def wide_and_deep(inputs, linear_feature_columns, gene_feature_columns, cell_feature_columns, total_classes):\n    metrics = [tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n               tf.keras.metrics.AUC(name='auc')]\n\n    gene_deep = tf.keras.layers.DenseFeatures(gene_feature_columns)(inputs)\n    gene_deep = tf.keras.layers.BatchNormalization()(gene_deep)\n    cell_deep = tf.keras.layers.DenseFeatures(cell_feature_columns)(inputs)\n    cell_deep = tf.keras.layers.BatchNormalization()(cell_deep)\n\n    for numnodes in [256]:\n        gene_deep = tf.keras.layers.Dense(numnodes, activation='relu', kernel_initializer='he_normal')(gene_deep)\n        gene_deep = tf.keras.layers.Dropout(0.25)(gene_deep)\n\n    for numnodes in [256]:\n        cell_deep = tf.keras.layers.Dense(numnodes, activation='relu', kernel_initializer='he_normal')(cell_deep)\n        cell_deep = tf.keras.layers.Dropout(0.25)(cell_deep)\n\n    deep = tf.keras.layers.concatenate([gene_deep, cell_deep])\n\n    for numnodes in [256, 64]:\n        deep = tf.keras.layers.Dense(numnodes, activation='relu', kernel_initializer='he_normal')(deep)\n        deep = tf.keras.layers.Dropout(0.25)(deep)\n\n    wide = tf.keras.layers.DenseFeatures(linear_feature_columns)(inputs)\n    both = tf.keras.layers.concatenate([deep, wide])\n    output = tf.keras.layers.Dense(total_classes, activation='sigmoid')(both)\n    model = tf.keras.Model(inputs=[v for v in inputs.values()], outputs=output)\n    model.compile(\n        loss=tf.keras.losses.BinaryCrossentropy(),\n        optimizer=tf.keras.optimizers.RMSprop(lr=0.0005),\n        metrics=metrics)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TFDataTransformer:\n    \"\"\"Transform pandas data frame to tensorflow data set\n    \"\"\"\n    def __init__(self):\n        pass\n\n    def data_frame_to_dataset(self, data_frame_features=None, data_frame_labels=None):\n        \"\"\"Transform from data frame to data_set\n\n        Args:\n            data_frame ([type], optional): [description]. Defaults to None.\n            labels ([type], optional): [description]. Defaults to None.\n        \"\"\"\n        dataset = tf.data.Dataset.from_tensor_slices((dict(data_frame_features), data_frame_labels.values)).shuffle(1000)\n        return dataset       \n\n    def transform(self, data_frame_features=None, data_frame_labels=None):\n        \"\"\"transform data\n\n        Args:\n            data_frame_features ([type], optional): [description]. Defaults to None.\n            data_frame_labels ([type], optional): [description]. Defaults to None.\n        \"\"\"\n        return self.data_frame_to_dataset(data_frame_features, data_frame_labels)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\ntrain_data_features = pd.read_csv(\"/kaggle/input/lish-moa/train_features.csv\").drop('sig_id', axis=1)\ntest_data_features = pd.read_csv(\"/kaggle/input/lish-moa/test_features.csv\").drop('sig_id', axis=1)\ntrain_data_features['cp_time'] = train_data_features['cp_time'].map(str)\ntest_data_features['cp_time'] = test_data_features['cp_time'].map(str)\n\nraw_labels = pd.read_csv(\"/kaggle/input/lish-moa/train_targets_scored.csv\").drop('sig_id', axis=1)\ntest_data_features_copy = test_data_features.copy()\ndata_types = train_data_features.dtypes\n\n\nCATEGORICAL_FEATURES = ['cp_type', 'cp_dose', 'cp_time']\nNUMERICAL_FEATURES = data_types[data_types=='float64'].index.tolist()\nNUMERICAL_FEATURES_GENE = [feature for feature in NUMERICAL_FEATURES if 'g' in feature]\nNUMERICAL_FEATURES_CELL = [feature for feature in NUMERICAL_FEATURES if 'c' in feature]\n\ntrain_data_features[NUMERICAL_FEATURES_GENE] = (train_data_features[NUMERICAL_FEATURES_GENE]-train_data_features[NUMERICAL_FEATURES_GENE].mean(axis=0))/train_data_features[NUMERICAL_FEATURES_GENE].std(axis=0)\ntrain_data_features[NUMERICAL_FEATURES_CELL] = (train_data_features[NUMERICAL_FEATURES_CELL]-train_data_features[NUMERICAL_FEATURES_CELL].mean(axis=0))/train_data_features[NUMERICAL_FEATURES_CELL].std(axis=0)\n\ntest_data_features[NUMERICAL_FEATURES_GENE] = (test_data_features[NUMERICAL_FEATURES_GENE]-test_data_features[NUMERICAL_FEATURES_GENE].mean(axis=0))/test_data_features[NUMERICAL_FEATURES_GENE].std(axis=0)\ntest_data_features[NUMERICAL_FEATURES_CELL] = (test_data_features[NUMERICAL_FEATURES_CELL]-test_data_features[NUMERICAL_FEATURES_CELL].mean(axis=0))/test_data_features[NUMERICAL_FEATURES_CELL].std(axis=0)\n\nX_train, X_test, y_train, y_test = train_test_split(train_data_features, raw_labels)\n\ntrain_dataset = TFDataTransformer().transform(X_train, y_train).batch(BATCH_SIZE)\nval_dataset = TFDataTransformer().transform(X_test, y_test).batch(BATCH_SIZE)\ntest_dataset = tf.data.Dataset.from_tensor_slices(dict(test_data_features)).batch(BATCH_SIZE)\n\nfeature_layer_inputs, categorical_feature_encoders, gene_feature_encoders, cell_feature_encoders = FeatureTransformer(gene_features=NUMERICAL_FEATURES_GENE,\n                                                                                                                      cell_features=NUMERICAL_FEATURES_CELL,\n                                                                                                                      categorical_features=CATEGORICAL_FEATURES)\\\n                                                                                                   .transform(X_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nearly_stopping = tf.keras.callbacks.EarlyStopping(**{'monitor': 'val_loss',\n                                                     'mode': 'min',\n                                                     'verbose': 1,\n                                                     'patience': 5})\n\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(**{'filepath': '/tmp/best_model',\n                                                         'monitor': 'val_loss',\n                                                         'mode': 'min',\n                                                         'verbose': 1,\n                                                         'save_weights_only': True,\n                                                         'save_best_only': True})\n    \n\nmodel = wide_and_deep(feature_layer_inputs, categorical_feature_encoders, gene_feature_encoders, cell_feature_encoders, total_classes=raw_labels.shape[-1])\nmodel.fit(train_dataset,\n          epochs=20,\n          validation_data=val_dataset,\n          callbacks=[early_stopping, model_checkpoint],\n          )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights('/tmp/best_model')\n\ndef submit_run():\n    submit_data_frame = pd.read_csv(\"/kaggle/input/lish-moa/sample_submission.csv\")\n    submit_data_frame.iloc[:, 1:] = model.predict(test_dataset)\n    submit_data_frame.to_csv('submission.csv', index=False)\n\n\nsubmit_run()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}