{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport xgboost\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn import model_selection\nfrom category_encoders import CountEncoder\nfrom sklearn.pipeline import Pipeline\n\n\nfrom sklearn.multioutput import MultiOutputClassifier\n\nimport sklearn\nimport warnings\nfrom sklearn.preprocessing import QuantileTransformer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n# %% [markdown]\n# **train set before using RankGauss**\n# It may be a too simple idea, it appears that the gene expression data and cell viability data 、\n# can be controlled by the experimenter, so it is safe to assume that these data are independent of each other.\n# \n# Also, since the shape of the distribution is close to normal distribution to begin with, 、\n# I don't think there is much of a problem if it is forced to be transformed into a Gaussian distribution.\n# %% [code]\n#RankGauss\nfor col in (GENES + CELLS):\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    test_vec = test_features[col].values.reshape(vec_len_test, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_vec).reshape(1, vec_len_test)[0]\n# %% [markdown]\n# We can confirme that the shapes of data got close to the normal distribution.\n# \n# **train set after using RankGauss**\n# %% [code]\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)\n# %% [markdown]\n# It appears that we were able to transform the distribution of each data to resemble a normal distribution, as intended.\n# \n# So, let's enter the data into the benchmarking method to see the improvement.\n\n# # PCA features + Existing features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_comp = 600  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\nn_comp = 50  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\nprint(train_features.shape)\n# %% [code]\n# # feature Selection using Variance Encoding\nfrom sklearn.feature_selection import VarianceThreshold\nvar_thresh = VarianceThreshold(0.8)  #<-- Update\ndata = train_features.append(test_features)\n\nfeature_cols = train_features.columns.values[4:]\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\ntrain_features_str = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n    \ntrain_features_new = pd.concat([train_features_str,pd.DataFrame(train_features_transformed, columns=feature_cols[var_thresh.get_support()])], axis=1)\ntest_features_str = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\ntest_features_new = pd.concat([test_features_str,pd.DataFrame(test_features_transformed, columns=feature_cols[var_thresh.get_support()])], axis=1)\n\n# print(train_features_new.shape)\n# %% [code]\ntrain_all = train_features_new.merge(train_targets_scored, on='sig_id')\ntrt_cp_index = train_all[train_all['cp_type']!='ctl_vehicle'].index\nctl_vehicle_index = train_all[train_all['cp_type']=='ctl_vehicle'].index\ntrain_crl = train_all[train_all['cp_type']=='ctl_vehicle'].reset_index(drop=True)\ntrain_trt = train_all[train_all['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntrain_nc_all = train_features_new.merge(train_targets_nonscored, on='sig_id')\ntrain_nc_crl = train_nc_all[train_nc_all['cp_type']=='ctl_vehicle'].reset_index(drop=True)\ntrain_nc_trt = train_nc_all[train_nc_all['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntest_all = test_features_new.merge(sample_submission,on='sig_id')\ntest_crl = test_all[test_all['cp_type']=='ctl_vehicle'].reset_index(drop=True)\ntest_trt = test_all[test_all['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train_trt[train_targets_scored.columns]\ntrain_trt = train_trt.drop('cp_type', axis=1)\ntest_trt = test_trt.drop('cp_type', axis=1)\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n#%%\nfolds = train_trt.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=7)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train_trt, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)\n\nprint(train_trt.shape)\nprint(folds.shape)\nprint(test_trt.shape)\nprint(target.shape)\nprint(sample_submission.shape)\n# %% [code]\n# # Preprocessing steps\ndef process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data\n# %% [code]\n# # CV folds\nfeature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)\n\ngen_cols = [c for c in feature_cols if 'g-' == c[:2]]\ncell_cols = [c for c in feature_cols if 'c-' == c[:2]]\npca_gen_cols = [c for c in feature_cols if 'pca_G' == c[:5]]\npca_cell_cols = [c for c in feature_cols if 'pca_C' == c[:5]]\nohe_cols = [c for c in feature_cols if c not in gen_cols+cell_cols+pca_gen_cols+pca_cell_cols]\nprint(len(gen_cols), len(cell_cols), len(pca_gen_cols), len(pca_cell_cols),len(ohe_cols))\n\n\n# HyperParameters\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7           \nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=1500","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %% [code]\n# # Dataset Classes\nclass MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n\ndef train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n#         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    \n    return final_loss\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\nimport torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n\nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.2619422201258426)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.2619422201258426)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x\n\nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            # true_dist = pred.data.clone()\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    \n\ndef run_training(fold, seed):\n    \n\n       \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test_trt)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n   \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions\n\ndef run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(folds), len(target_cols)))\n    predictions = np.zeros((len(test_trt), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model_MH(nn.Module):\n    def __init__(self, num_gen_features, num_cell_features,num_targets, hidden_size):\n        super(Model_MH, self).__init__()\n        self.gen_batch_norm1 = nn.BatchNorm1d(num_gen_features)\n        self.gen_dropout1 = nn.Dropout(0.2)\n        self.gen_dense1 = nn.utils.weight_norm(nn.Linear(num_gen_features, hidden_size))\n        self.gen_batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.gen_dropout2 = nn.Dropout(0.3)\n        self.gen_dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.cell_batch_norm1 = nn.BatchNorm1d(num_cell_features)\n        self.cell_dropout1 = nn.Dropout(0.2)\n        self.cell_dense1 = nn.utils.weight_norm(nn.Linear(num_cell_features, int(hidden_size/2)))\n        self.cell_batch_norm2 = nn.BatchNorm1d(int(hidden_size/2))\n        self.cell_dropout2 = nn.Dropout(0.3)\n        self.cell_dense2 = nn.utils.weight_norm(nn.Linear(int(hidden_size/2), int(hidden_size/2)))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size + int(hidden_size/2) )\n        self.dropout3 = nn.Dropout(0.25)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size + int(hidden_size/2), num_targets))\n        \n        \n    \n    def forward(self, gen_x, cell_x):\n#         print(cell_x.shape)\n        gen_x = self.gen_batch_norm1(gen_x)\n        gen_x = self.gen_dropout1(gen_x)\n        gen_x = F.relu(self.gen_dense1(gen_x))\n        gen_x = self.gen_batch_norm2(gen_x)\n        gen_x = self.gen_dropout2(gen_x)\n        gen_x = F.relu(self.gen_dense2(gen_x))\n\n        cell_x = self.cell_batch_norm1(cell_x)\n        cell_x = self.cell_dropout1(cell_x)\n        cell_x = F.relu(self.cell_dense1(cell_x))\n        cell_x = self.cell_batch_norm2(cell_x)\n        cell_x = self.cell_dropout2(cell_x)\n        cell_x = F.relu(self.cell_dense2(cell_x))\n                \n        x = torch.cat((gen_x,cell_x),dim=1)\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        return x    \n\nclass MoADataset_MH:\n    def __init__(self, gen_features, cell_features, targets):\n        self.gen_features = gen_features\n        self.cell_features = cell_features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.gen_features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'gen_x' : torch.tensor(self.gen_features[idx, :], dtype=torch.float),\n            'cell_x' : torch.tensor(self.cell_features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset_MH:\n    \n    def __init__(self, gen_features, cell_features):\n        self.gen_features = gen_features\n        self.cell_features = cell_features\n        \n    def __len__(self):\n        return (self.gen_features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'gen_x' : torch.tensor(self.gen_features[idx, :], dtype=torch.float),\n            'cell_x' : torch.tensor(self.cell_features[idx, :], dtype=torch.float)  \n        }\n        return dct\n\nimport time\ndef train_fn_MH(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        gen_x, cell_x, targets = data['gen_x'].to(device), data['cell_x'].to(device), data['y'].to(device)\n        outputs = model(gen_x, cell_x)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    \n    return final_loss\n\ndef valid_fn_MH(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        gen_x, cell_x, targets = data['gen_x'].to(device), data['cell_x'].to(device),  data['y'].to(device)\n        outputs = model(gen_x, cell_x)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn_MH(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        gen_x, cell_x = data['gen_x'].to(device), data['cell_x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(gen_x, cell_x)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n\ndef run_training_MH(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test_trt)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n\n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n\n    gen_x_train, cell_x_train, y_train  = train_df[gen_cols + pca_gen_cols + ohe_cols].values, train_df[cell_cols + pca_cell_cols + ohe_cols].values, train_df[target_cols].values\n    gen_x_valid, cell_x_valid, y_valid =  valid_df[gen_cols + pca_gen_cols + ohe_cols].values, valid_df[cell_cols + pca_cell_cols + ohe_cols].values, valid_df[target_cols].values\n    #     print(\"len target_cols\", len(target_cols), train_df[target_cols].shape, train_df[target_cols].reset_index(drop=True).shape)\n    #     print(\"shapes\", x_train.shape, y_train.shape, x_valid.shape, y_valid.shape)\n\n    train_dataset = MoADataset_MH(gen_x_train, cell_x_train, y_train)\n    valid_dataset = MoADataset_MH(gen_x_valid, cell_x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n#     print('# of target cols', len(target_cols))\n    \n    model = Model_MH(\n        num_gen_features=len(gen_cols + pca_gen_cols + ohe_cols),\n        num_cell_features=len(cell_cols + pca_cell_cols + ohe_cols),\n        num_targets=len(target_cols),\n        hidden_size=hidden_size\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        start = time.time()\n        train_loss = train_fn_MH(model, optimizer,scheduler, loss_fn, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn_MH(model, loss_fn, validloader, DEVICE)\n        elapse = time.time() - start\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss} {elapse:.2f} seconds\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    gen_x_test, cell_x_test = test_[gen_cols + pca_gen_cols + ohe_cols].values, test_[cell_cols + pca_cell_cols + ohe_cols].values\n    testdataset = TestDataset_MH(gen_x_test, cell_x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model_MH(\n        num_gen_features=len(gen_cols + pca_gen_cols + ohe_cols),\n        num_cell_features=len(cell_cols + pca_cell_cols + ohe_cols),\n        num_targets=len(target_cols),\n        hidden_size=hidden_size\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn_MH(model, testloader, DEVICE)\n    \n    return oof, predictions\n\ndef run_k_fold_MH(NFOLDS, seed):\n    oof = np.zeros((len(folds), len(target_cols)))\n    predictions = np.zeros((len(test_trt), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training_MH(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = [0] #<-- Update\noof_nn = np.zeros((len(folds), len(target_cols)))\npredictions_nn = np.zeros((len(test_trt), len(target_cols)))\n\nfor seed in SEED:   \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof_nn += oof_ / len(SEED)\n    predictions_nn += predictions_ / len(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = [0] #<-- Update\noof_nn_mh = np.zeros((len(folds), len(target_cols)))\npredictions_nn_mh = np.zeros((len(test_trt), len(target_cols)))\n\nfor seed in SEED:   \n    oof_, predictions_ = run_k_fold_MH(NFOLDS, seed)\n    oof_nn_mh += oof_ / len(SEED)\n    predictions_nn_mh += predictions_ / len(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_trt_nn = train_trt.copy()\ntrain_trt_nn[target_cols] = oof_nn\ntest_trt_nn = test_trt.copy()\ntest_trt_nn[target_cols] = predictions_nn\n\ntrain_trt_nn_mh = train_trt.copy()\ntrain_trt_nn_mh[target_cols] = oof_nn_mh\ntest_trt_nn_mh = test_trt.copy()\ntest_trt_nn_mh[target_cols] = predictions_nn_mh\n#%%\ndef out_sampe_result(train_trt_result):\n    valid_results = train_targets_scored.drop(columns=target_cols).merge(train_trt_result[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n    \n    return valid_results\n\ndef valid_score(valid_results):\n    \n    y_true = train_targets_scored[target_cols].values\n    y_pred = valid_results[target_cols].values\n    \n    score = 0\n    for i in range(len(target_cols)):\n        score_ = log_loss(y_true[:, i], y_pred[:, i])\n        score += score_ / target.shape[1]\n        \n    print(\"CV log_loss: \", score)\n\ndef log_loss_metric(y_true, y_pred):\n    metrics = []\n    for _target in train_targets_scored.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels = [0,1]))\n    return np.mean(metrics)\n#%%\ndf_valid_nn = out_sampe_result(train_trt_nn)[['sig_id']+target_cols]\ndf_valid_nn_mh = out_sampe_result(train_trt_nn_mh)[['sig_id']+target_cols]\nvalid_score(df_valid_nn)\nvalid_score(df_valid_nn_mh)\n\n# df_valid_nn.to_csv('df_valid_nn_11292020.csv',index = False)\n# df_valid_nn_mh.to_csv('df_valid_nn_mh_11292020.csv', index =False)\n# %% [code]\nsub_nn = sample_submission.drop(columns=target_cols).merge(test_trt_nn[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n# sub_nn.to_csv('submission_nn_11292020.csv', index=False)\nsub_nn_md = sample_submission.drop(columns=target_cols).merge(test_trt_nn_mh[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLDS_xgb = 5\n\nDATA_DIR = '../input/lish-moa/'\ntrain_xgb = pd.read_csv(DATA_DIR + 'train_features.csv')\nlbl1 = preprocessing.LabelEncoder()\nlbl1.fit(train_xgb['cp_type'].astype(str))\nlbl1_dict = dict(zip(lbl1.classes_, range(len(lbl1.classes_))))\ntrain_xgb['cp_type'] = lbl1.transform(train_xgb['cp_type'].astype(str))\nlbl2 = preprocessing.LabelEncoder()\nlbl2.fit(train_xgb['cp_dose'].astype(str))\ntrain_xgb['cp_dose'] = lbl2.transform(train_xgb['cp_dose'].astype(str))\n\ntargets_xgb = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\ntest_xgb = pd.read_csv(DATA_DIR + 'test_features.csv')\nsub_xgb = pd.read_csv(DATA_DIR + 'sample_submission.csv')\ntest_xgb['cp_type'] = lbl1.transform(test_xgb['cp_type'].astype(str))\ntest_xgb['cp_dose'] = lbl2.transform(test_xgb['cp_dose'].astype(str))\n\nX_xgb = train_xgb.iloc[:,1:].to_numpy()\nX_test_xgb = test_xgb.iloc[:,1:].to_numpy()\ny_xgb = targets_xgb.iloc[:,1:].to_numpy() \n# %% [code]\nclassifier = MultiOutputClassifier(XGBClassifier(tree_method='gpu_hist'))\n\nclf = Pipeline([('encode', CountEncoder(cols=[0, 2])),\n                ('classify', classifier)\n               ])\n\nparams = {'classify__estimator__colsample_bytree': 0.6522,\n          'classify__estimator__gamma': 3.6975,\n          'classify__estimator__learning_rate': 0.0503,\n          'classify__estimator__max_delta_step': 2.0706,\n          'classify__estimator__max_depth': 10,\n          'classify__estimator__min_child_weight': 31.5800,\n          'classify__estimator__n_estimators': 166,\n          'classify__estimator__subsample': 0.8639\n         }\n\n_ = clf.set_params(**params)\n\n# ## Train the model\n# \n# Framing this problem as a binary classification problem has the disadvantage that you need to train as many models as you have classes. \\\n# For this problem this means training 206 models per fold, for the large number of features included in this dataset this may take a long time...\n#%%\noof_xgb = np.zeros(y_xgb.shape)\npredictions_xgb = np.zeros((test_xgb.shape[0], y_xgb.shape[1]))\noof_xgb_losses = []\nkf = KFold(n_splits=NFOLDS_xgb)\nfor fn, (trn_idx, val_idx) in enumerate(kf.split(X_xgb, y_xgb)):\n    print('Starting fold: ', fn)\n    X_train, X_val = X_xgb[trn_idx], X_xgb[val_idx]\n    y_train, y_val = y_xgb[trn_idx], y_xgb[val_idx]\n    \n    # drop where cp_type==ctl_vehicle (baseline)\n    ctl_mask = X_train[:,0]==lbl1_dict['ctl_vehicle']\n    X_train = X_train[~ctl_mask,:]\n    y_train = y_train[~ctl_mask]\n    \n    clf.fit(X_train, y_train)\n    val_preds = clf.predict_proba(X_val) # list of preds per class\n    val_preds = np.array(val_preds)[:,:,1].T # take the positive class\n    oof_xgb[val_idx] = val_preds\n    \n    loss = log_loss(np.ravel(y_val), np.ravel(val_preds))\n    oof_xgb_losses.append(loss)\n    preds = clf.predict_proba(X_test_xgb)\n    preds = np.array(preds)[:,:,1].T # take the positive class\n    predictions_xgb += preds / NFOLDS_xgb\n    \nprint(oof_xgb_losses)\nprint('Mean OOF loss across folds', np.mean(oof_xgb_losses))\nprint('STD OOF loss across folds', np.std(oof_xgb_losses))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%\n# set control train preds to 0\ncontrol_mask = train_xgb['cp_type']=='ctl_vehicle'\noof_xgb[control_mask] = 0\nprint('OOF log loss: ', log_loss(np.ravel(y_xgb), np.ravel(oof_xgb)))\n\n# ## Analysis of OOF preds\n# set control test preds to 0\ncontrol_mask = test_xgb['cp_type']=='ctl_vehicle'\npredictions_xgb[control_mask] = 0\n\n# create the submission file\nsub_xgb.iloc[:,1:] = predictions_xgb\n# sub_xgb.to_csv('submission_xgb.csv', index=False)\n#%%\ndf_valid_xgb = train_targets_scored.copy()\ndf_valid_xgb[target_cols] = oof_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def drop_id(df):\n    if 'sig_id' in df.columns:\n        df.drop(columns = ['sig_id'],inplace=True)\n    \n    return df\n\nX_stack = pd.concat([drop_id(df_valid_nn),drop_id(df_valid_nn_mh),drop_id(df_valid_xgb)],axis = 1)\nX_stack = X_stack.to_numpy()\n\nX_test = pd.concat([drop_id(sub_nn),drop_id(sub_nn_md),drop_id(sub_xgb)],axis =1)\nX_test = X_test.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\ndef search_model(train_x, train_y, est, param_grid, n_jobs, cv, refit=False):\n    # Grid Search for the best model,\n    model = model_selection.GridSearchCV(estimator=est,\n                                         param_grid=param_grid,\n                                         scoring='neg_log_loss',\n                                         verbose=1,\n                                         n_jobs=n_jobs,\n                                         iid=True,\n                                         refit=refit,\n                                         cv=cv)\n    # Fit Grid Search Model,\n    model.fit(train_x, train_y)\n    print(\"est score: %0.3f\" % (model.best_score_))\n    print(\"Best parameters set:\" , model.best_params_)\n    print(\"Scores:\" , model.cv_results_)\n    return model\n\n\nparam_grid = {\n              \"hidden_layer_sizes\":[50]\n              }\n\nmodel = search_model(X_stack\n                                         , y_xgb\n                                         , MLPClassifier()\n                                         , param_grid\n                                         , n_jobs=-1\n                                         , cv=4\n                                         , refit=True)   \n\nprint (\"best subsample:\", model.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_final = pd.read_csv(DATA_DIR + 'sample_submission.csv')\npredictions_final = model.predict_proba(X_test)\nsub_final.iloc[:,1:] = predictions_final\nsub_final.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}