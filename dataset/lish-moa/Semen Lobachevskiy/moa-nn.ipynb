{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.layers import Dropout, Input, Dense\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input/lish-moa')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\n#train_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n#submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train_features.shape:', train_features.shape)\nprint('train_targets.shape:', train_targets.shape)\nprint('test_features.shape:', test_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_features.merge(train_targets, on='sig_id')\ntarget_cols = [c for c in train_targets.columns if c not in ['sig_id']]\ncols = target_cols + ['cp_type']\ntrain[cols].groupby('cp_type').sum().sum(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_features.shape, test_features.shape)\nx_train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\nx_test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\nprint(x_train.shape, x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Формирование тестовой и валидационной выборки\nnum_train_samples = int(0.8 * len(train_features))\n\nfull_train_features_ids = train_features.pop('sig_id')\nfull_test_features_ids = test_features.pop('sig_id')\ntrain_targets.pop('sig_id')\n\nfull_train_features_df = train_features.copy()\nfull_train_targets_df = train_targets.copy()\n\nval_features_df = train_features[num_train_samples:]\ntrain_features_df = train_features[:num_train_samples]\nval_targets_df = train_targets[num_train_samples:]\ntrain_targets_df = train_targets[:num_train_samples]\n\nprint('Total training samples:', len(full_train_features_df))\nprint('Training split samples:', len(train_features_df))\nprint('Validation split samples:', len(val_features_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = list(train_features_df)\ncategorical_feature_names = ['cp_type', 'cp_dose']\nnumerical_feature_names = [name for name in feature_names if name not in categorical_feature_names]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#мерджим числовые фичи\ndef merge_numerical_features(feature_dict):\n    categorical_features = {name: feature_dict[name] for name in categorical_feature_names}\n    numerical_features = tf.stack([tf.cast(feature_dict[name], 'float32') for name in numerical_feature_names])\n    feature_dict = categorical_features\n    feature_dict.update({'numerical_features': numerical_features})\n    return feature_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_ds = tf.data.Dataset.from_tensor_slices(dict(train_features_df))\ntrain_features_ds = train_features_ds.map(lambda x: merge_numerical_features(x))\ntrain_targets_ds = tf.data.Dataset.from_tensor_slices(np.array(train_targets_df))\ntrain_ds = tf.data.Dataset.zip((train_features_ds, train_targets_ds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_train_features_ds = tf.data.Dataset.from_tensor_slices(dict(full_train_features_df))\nfull_train_features_ds = full_train_features_ds.map(lambda x: merge_numerical_features(x))\nfull_train_targets_ds = tf.data.Dataset.from_tensor_slices(np.array(full_train_targets_df))\nfull_train_ds = tf.data.Dataset.zip((full_train_features_ds, full_train_targets_ds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_features_ds = tf.data.Dataset.from_tensor_slices(dict(val_features_df))\nval_features_ds = val_features_ds.map(lambda x: merge_numerical_features(x))\nval_targets_ds = tf.data.Dataset.from_tensor_slices(np.array(val_targets_df))\nval_ds = tf.data.Dataset.zip((val_features_ds, val_targets_ds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds = train_ds.shuffle(1024).batch(64).prefetch(8)\nfull_train_ds = full_train_ds.shuffle(1024).batch(64).prefetch(8)\nval_ds = val_ds.batch(64).prefetch(8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_train_ds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding\nfrom tensorflow.keras.layers.experimental.preprocessing import StringLookup\n\ndef encode_numerical_feature(feature, name, dataset):\n    # Create a Normalization layer for our feature\n    normalizer = Normalization()\n\n    # Prepare a Dataset that only yields our feature\n    feature_ds = dataset.map(lambda x, y: x[name])\n\n    # Learn the statistics of the data\n    normalizer.adapt(feature_ds)\n\n    # Normalize the input feature\n    encoded_feature = normalizer(feature)\n    return encoded_feature\n\n\ndef encode_categorical_feature(feature, name, dataset):\n    # Create a Lookup layer which will turn strings into integer indices\n    index = StringLookup()\n\n    # Prepare a Dataset that only yields our feature\n    feature_ds = dataset.map(lambda x, y: x[name])\n\n    # Learn the set of possible feature values and assign them a fixed integer index\n    index.adapt(feature_ds)\n\n    # Turn the values into integer indices\n    encoded_feature = index(feature)\n\n    # Create a CategoryEncoding for our integer indices\n    encoder = CategoryEncoding(output_mode=\"binary\")\n\n    # Prepare a dataset of indices\n    feature_ds = feature_ds.map(index)\n\n    # Learn the space of possible indices\n    encoder.adapt(feature_ds)\n\n    # Apply one-hot encoding to our indices\n    encoded_feature = encoder(encoded_feature)\n    return encoded_feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_inputs = []\nall_encoded_features = []\n\nprint('Processing categorical features...')\nfor name in categorical_feature_names:\n    inputs = keras.Input(shape=(1,), name=name, dtype='string')\n    encoded = encode_categorical_feature(inputs, name, train_ds)\n    all_inputs.append(inputs)\n    all_encoded_features.append(encoded)\n\nprint('Processing numerical features...')\nnumerical_inputs = keras.Input(shape=(len(numerical_feature_names),), name='numerical_features')\nencoded_numerical_features = encode_numerical_feature(numerical_inputs, 'numerical_features', train_ds)\n\nall_inputs.append(numerical_inputs)\nall_encoded_features.append(encoded_numerical_features)\nfeatures = layers.Concatenate()(all_encoded_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(all_inputs)\nprint(all_encoded_features)\nprint(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Формирование модели НС и вывод её структуры в консоль\n\nx = layers.Dropout(0.2)(features)\noutputs = layers.Dense(206, activation='sigmoid')(x)\nmodel = keras.Model(all_inputs, outputs)\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Компиляция НС с оптимизацией по Adam и криетриям- категориальная кросс-энтропия\n#model.compile(optimizer='adam',\n#loss='categorical_crossentropy',\n #              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Второй вариант\nmodel.compile(optimizer=keras.optimizers.RMSprop(),\n                    loss=keras.losses.BinaryCrossentropy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(full_train_ds, epochs=10, validation_data=val_ds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(full_train_ds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}