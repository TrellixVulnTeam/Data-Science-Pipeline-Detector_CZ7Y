{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Source \n\n- Pytorch 1.6 : https://pytorch.org/docs/stable/\n- iterative-stratification : https://github.com/trent-b/iterative-stratification for stratified K fold multilabel"},{"metadata":{},"cell_type":"markdown","source":"# Approach :\nInference script : \nhttps://www.kaggle.com/ludovick/inference-moa-baseline-mlp-kfold-10/edit/run/41997446\n\nNeural Network to classify a multi labels tasks with pytorch\n- Stratified K Fold (10 folds) or shufflesplit\n- BCE Loss\n- optional labels are used for the training, not for inference though (after filtering)\n- gradient accumulation (not tested yet)\n- version 14 : add weight_norm from https://www.kaggle.com/nicohrubec/pytorch-multilabel-neural-network"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install iterative-stratification\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nimport os\nimport random\nimport sys\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsubmission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\nremove_vehicle = True\n\nif remove_vehicle:\n    train_features = train.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n    train_targets_scored = train_targets_scored.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n    train_targets_nonscored = train_targets_nonscored.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\nelse:\n    train_features = train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the different values than each category can take and if there are nan/inf values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the number of categorical features for train/test\ncol_features = list(train_features.columns)[1:]\nprint(train_features[col_features[0]].value_counts())\nprint(test_features[col_features[0]].value_counts())\nprint(train_features[col_features[1]].value_counts())\nprint(test_features[col_features[1]].value_counts())\nprint(train_features[col_features[2]].value_counts())\nprint(test_features[col_features[2]].value_counts())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check nan value and inf value ?\nprint(test_features[col_features].isna().sum().values.sum())\nprint(np.isinf(test_features[col_features[3:]].values).sum()) # only for numerical value\nprint(train_features[col_features].isna().values.sum())\nprint(np.isinf(train_features[col_features[3:]].values).sum()) # only for numerical value","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Targets data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the labels are hightly imbalanced, it may be necessary to use a weighted loss function to help the model ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ratio for each label\n\ndef get_ratio_labels(df):\n    columns = list(df.columns)\n    columns.pop(0)\n    ratios = []\n    toremove = []\n    for c in columns:\n        counts = df[c].value_counts()\n        if len(counts) != 1:\n            ratios.append(counts[0]/counts[1])\n        else:\n            toremove.append(c)\n    print(f\"remove {len(toremove)} columns\")\n    \n    for t in toremove:\n        columns.remove(t)\n    return columns, np.array(ratios).astype(np.int32)\n\ncolumns, ratios = get_ratio_labels(train_targets_scored)\nprint(ratios)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_nonscored","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_nonscored, ratios_nonscored = get_ratio_labels(train_targets_nonscored)\nprint(ratios_nonscored)\nprint(len(columns_nonscored), len(ratios_nonscored))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### seems that some optional labels have only one labels, so we discard them"},{"metadata":{},"cell_type":"markdown","source":"# Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_features[col_features[3:]].max().values.max())\nprint(train_features[col_features[3:]].min().values.min())\nprint(test_features[col_features[3:]].min().values.min())\nprint(test_features[col_features[3:]].max().values.max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we could later normalize our numerical value but we will see that for another version"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(col_features[3:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataloader"},{"metadata":{"trusted":true},"cell_type":"code","source":"mapping = {\"cp_type\":{\"trt_cp\": 1, \"ctl_vehicle\":2},\n               \"cp_time\":{48:1, 72:2, 24:3},\n               \"cp_dose\":{\"D1\":1, \"D2\":2}}\n\ndef transform_data(train, test, col, mapping, normalize=True, removed_vehicle=False):\n    \"\"\"\n        the first 3 columns represents categories, the others numericals features\n    \"\"\"\n\n    \n    if removed_vehicle:\n        categories_tr = np.stack([ train[c].apply(lambda x: mapping[c][x]).values for c in col[1:3]], axis=1)\n        categories_test = np.stack([ test[c].apply(lambda x: mapping[c][x]).values for c in col[1:3]], axis=1)\n    else:\n        categories_tr = np.stack([ train[c].apply(lambda x: mapping[c][x]).values for c in col[:3]], axis=1)\n        categories_test = np.stack([ test[c].apply(lambda x: mapping[c][x]).values for c in col[:3]], axis=1)\n    \n    max_ = 10.\n    min_ = -10.\n   \n    if removed_vehicle:\n        numerical_tr = train[col[3:]].values\n        numerical_test = test[col[3:]].values\n    else:\n        numerical_tr = train[col[3:]].values\n        numerical_test = test[col[3:]].values\n    if normalize:\n        numerical_tr = (numerical_tr-min_)/(max_ - min_)\n        numerical_test = (numerical_test-min_)/(max_ - min_)\n    return categories_tr, categories_test, numerical_tr, numerical_test\ncol_features = list(train_features.columns)[1:]\ncat_tr, cat_test, numerical_tr, numerical_test = transform_data(train_features, test_features, col_features, mapping, normalize=False, removed_vehicle=remove_vehicle)\ntargets_tr = train_targets_scored[columns].values.astype(np.float32)\ntargets2_tr = train_targets_nonscored[columns_nonscored].values.astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MOADataset(Dataset):\n    def __init__(self, x_cats, x_nums, y=None, y2=None):\n        self.cats = x_cats\n        self.nums = x_nums\n        self.y = y\n        self.y2 = y2\n        \n    def __len__(self):\n        return len(self.cats)\n\n    def __getitem__(self, index):\n        x1 = torch.as_tensor(self.cats[index], dtype=torch.long)\n        x2 = torch.as_tensor(self.nums[index], dtype=torch.float)\n        \n        if self.y is not None:\n            label = torch.as_tensor(self.y[index], dtype=torch.float)\n            if self.y2 is not None:\n                label2 = torch.as_tensor(self.y2[index], dtype=torch.float)\n                return x1, x2, label, label2\n            return  x1, x2, label\n        return  x1, x2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MOA_MLP(nn.Module):\n    def __init__(self, num_cats=[2,3,2] , cats_emb_size=[2,2,2], num_numericals=872, hidden_size_numericals=2048,\n                num_class=206, aux=None):\n        super().__init__()\n        self.cat_emb1 = nn.Embedding(num_cats[0], cats_emb_size[0], padding_idx=0)\n        self.cat_emb2 = nn.Embedding(num_cats[1], cats_emb_size[1], padding_idx=0)\n        #self.cat_emb3 = nn.Embedding(num_cats[2], cats_emb_size[2], padding_idx=0)\n\n        self.norms = nn.BatchNorm1d(sum(cats_emb_size) +num_numericals)\n        self.dropout = nn.Dropout(0.2)\n        \n        self.proj = nn.utils.weight_norm(nn.Linear(sum(cats_emb_size) + num_numericals, hidden_size_numericals))\n        self.norm_proj = nn.BatchNorm1d(hidden_size_numericals)\n        self.dropout2 = nn.Dropout(0.5)\n        \n        hd_1 = hidden_size_numericals//2\n        hd_2 = hd_1//2\n        self.extractor = nn.Sequential(nn.utils.weight_norm(nn.Linear(hidden_size_numericals, hd_1)),\n                                        nn.PReLU(),\n                                        nn.BatchNorm1d(hd_1),\n                                        nn.Dropout(0.5),\n                                        #nn.utils.weight_norm(nn.Linear(hd_1, hd_2)),\n                                        #nn.PReLU(),\n                                        #nn.BatchNorm1d(hd_2),\n                                        #nn.Dropout(0.5)\n        )\n        self.cls = nn.utils.weight_norm(nn.Linear(hd_1, num_class))\n        self.cls_aux=None\n        if aux is not None:\n            self.cls_aux = nn.utils.weight_norm(nn.Linear(hd_1, aux))\n    def forward(self, x_cat, x_num):\n        cat_features = torch.cat([self.cat_emb1(x_cat[:,0]), self.cat_emb2(x_cat[:,1])], dim=1)\n        all_features = torch.cat([cat_features, x_num], dim=1)\n        all_features = self.norms(all_features)\n        all_features = self.dropout(all_features)\n        \n        proj_features = self.proj(all_features)\n        proj_features = self.norm_proj(F.relu(proj_features))\n        proj_features = self.dropout2(proj_features )\n        \n        \n        \n        features_reduced = self.extractor(proj_features)\n        \n        outputs = self.cls(features_reduced)\n        if self.cls_aux is not None:\n            outputs2 = self.cls_aux(features_reduced)\n            return outputs, outputs2\n        return outputs\n    \nclass MOA_MLPv2(nn.Module):\n    def __init__(self, num_cats=[2,3,2] , cats_emb_size=[2,2,2], num_numericals=872, hidden_size_numericals=2048,\n                num_class=206, aux=None):\n        super().__init__()\n        self.cat_emb1 = nn.Embedding(num_cats[0], cats_emb_size[0], padding_idx=0)\n        self.cat_emb2 = nn.Embedding(num_cats[1], cats_emb_size[1], padding_idx=0)\n        self.cat_emb3 = nn.Embedding(num_cats[2], cats_emb_size[2], padding_idx=0)\n\n        self.projection_numericals = nn.Linear(num_numericals, hidden_size_numericals)\n        self.norm_numericals = nn.BatchNorm1d(hidden_size_numericals)\n        self.dropout = nn.Dropout(0.5)\n        \n        self.proj = nn.Linear(sum(cats_emb_size) + hidden_size_numericals, 2048)\n        self.norm_proj = nn.BatchNorm1d(2048)\n        \n        hd_1 = hidden_size_numericals//2\n        hd_2 = hd_1//2\n        self.extractor = nn.Sequential(nn.Linear(2048, hd_1),\n                                       nn.ReLU(),\n                                      nn.BatchNorm1d(hd_1),\n                                      nn.Dropout(0.25),\n                                      nn.Linear(hd_1, hd_2),\n                                      nn.ReLU(),\n                                      nn.BatchNorm1d(hd_2),\n                                      nn.Dropout(0.25))\n        \n        self.cls = nn.Linear(hd_2, num_class)\n        self.cls_aux=None\n        if aux is not None:\n            self.cls_aux = nn.Linear(hd_2, aux)\n    def forward(self, x_cat, x_num):\n        cat_features = torch.cat([self.cat_emb1(x_cat[:,0]), self.cat_emb2(x_cat[:,1]), self.cat_emb3(x_cat[:,2])], dim=1)\n        \n        num_features = self.projection_numericals(x_num)\n        num_features = self.norm_numericals(F.relu(num_features))\n        \n        all_features = torch.cat([cat_features, num_features], dim=1)\n        all_features = self.dropout(all_features)\n\n        all_features = F.relu(self.proj(all_features))\n        all_features = self.norm_proj(all_features)\n        \n        features_reduced = self.extractor(all_features)\n        \n        outputs = self.cls(features_reduced)\n        if self.cls_aux is not None:\n            outputs2 = self.cls_aux(features_reduced)\n            return outputs, outputs2\n        return outputs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.cuda.amp import GradScaler, autocast\ndef train_one_epoch(model, dataloader, cfg, optimizer, loss_fn, loss_fn_aux=None, accumulation=1, with_aux_class=False, verbose=True):\n    model.train()\n    scaler = GradScaler()\n    optimizer.zero_grad()\n    N = 0.\n    total_loss = 0.\n    t=tqdm(dataloader, disable=~verbose)\n    for i, batch in enumerate(t):\n        \n        x1 = batch[0]\n        x2 = batch[1]\n        labels = batch[2]\n        \n        x1 = x1.to(cfg.device)\n        x2 = x2.to(cfg.device)\n        labels = labels.to(cfg.device)\n        \n        if with_aux_class:\n            labels2 = batch[3]\n            labels2 = labels2.to(cfg.device)\n            \n        with autocast(cfg.use_apex):\n            if with_aux_class:\n                outputs, outputs2 = model(x1, x2)\n                loss1 = loss_fn(outputs, labels).mean(0).mean()\n                loss2 = loss_fn_aux(outputs2, labels2).mean(0).mean()\n                loss = loss1 + 0.5*loss2\n\n            else:\n                outputs = model(x1, x2)\n                loss = loss_fn(outputs, labels).mean(0).mean()\n        \n        N += len(x1)\n        total_loss += (loss.item() * len(x1))  \n        \n        if cfg.use_apex:\n            loss = loss/accumulation\n            scaler.scale(loss).backward()\n        else:\n            loss = loss/accumulation\n            loss.backward()\n\n\n\n        \n        if (i+1)%accumulation == 0 or i-1 == len(dataloader):\n            if cfg.use_apex:\n                scaler.step(optimizer)\n\n                # Updates the scale for next iteration.\n                scaler.update()\n                optimizer.zero_grad()\n            else:                \n                optimizer.step()\n                optimizer.zero_grad()\n\n\n            t.set_description(\"Loss : {0}\".format(total_loss/N))\n            t.refresh()\n            \n            \ndef evals(model, dataloader, cfg, loss_fn, loss_fn_aux=None, with_aux_class=False, verbose=True):\n    model.eval()\n    N = 0.\n    total_loss = 0.\n\n    y_preds = []\n    y_targets = []\n    t=tqdm(dataloader, disable=~verbose)\n    with torch.no_grad():\n        for i, batch in enumerate(t):\n\n            x1 = batch[0]\n            x2 = batch[1]\n            labels = batch[2]\n\n            x1 = x1.to(cfg.device)\n            x2 = x2.to(cfg.device)\n            labels = labels.to(cfg.device)\n\n            if with_aux_class:\n                labels2 = batch[3]\n                labels2 = labels2.to(cfg.device)\n\n            with autocast(cfg.use_apex):\n                if with_aux_class:\n                    outputs, outputs2 = model(x1, x2)\n                    loss1 = loss_fn(outputs, labels).mean(0).mean()\n                    loss2 = loss_fn_aux(outputs2, labels2).mean(0).mean()\n                    loss = loss1 #+ 0.5*loss2\n\n                else:\n                    outputs = model(x1, x2)\n                    loss = loss_fn(outputs, labels).mean(0).mean()\n\n            N += len(x1)\n            total_loss += (loss.item() * len(x1))  \n\n            t.set_description(\"Loss : {0}\".format(total_loss/N))\n            t.refresh()\n            \n            y_preds.append(torch.sigmoid(outputs).detach().cpu().numpy())\n            y_targets.append(labels.detach().cpu().numpy())\n    y_preds = np.concatenate(y_preds, axis=0)\n    y_targets = np.concatenate(y_targets, axis=0)\n    score = log_loss_multi(y_targets, y_preds)\n    #print(\"Logloss = \", score)\n    return y_preds, y_targets, score\n\n\n\ndef inference_fn(model, dataloader, cfg, with_aux_class=True, verbose=True):\n    model.eval()\n    N = 0.\n    \n    y_preds = []\n    with torch.no_grad():\n        for i, batch in enumerate(tqdm(dataloader, disable=~verbose)):\n\n            x1 = batch[0]\n            x2 = batch[1]\n            \n\n            x1 = x1.to(cfg.device)\n            x2 = x2.to(cfg.device)\n\n            \n\n            with autocast(cfg.use_apex):\n                if with_aux_class:\n                    outputs, outputs2 = model(x1, x2)\n                    \n                else:\n                    outputs = model(x1, x2)\n\n            \n            y_preds.append(torch.sigmoid(outputs).detach().cpu().numpy())\n    y_preds = np.concatenate(y_preds, axis=0)\n    return y_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_loss_score(actual, predicted,  eps=1e-15):\n\n        \"\"\"\n        :param predicted:   The predicted probabilities as floats between 0-1\n        :param actual:      The binary labels. Either 0 or 1.\n        :param eps:         Log(0) is equal to infinity, so we need to offset our predicted values slightly by eps from 0 or 1\n        :return:            The logarithmic loss between between the predicted probability assigned to the possible outcomes for item i, and the actual outcome.\n        \"\"\"\n\n        \n        p1 = actual * np.log(predicted+eps)\n        p0 = (1-actual) * np.log(1-predicted+eps)\n        loss = p0 + p1\n\n        return -loss.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.log(0)*0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_loss_multi(y_true, y_pred):\n    M = y_true.shape[1]\n    results = np.zeros(M)\n    for i in range(M):\n        results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n    return results.mean()\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fold(fold, model, tr_dataloader, val_dataloader, cfg, optimizer, reducer, loss_fn, loss_fn_aux=None, accumulation=1, with_aux_class=False):\n    best_score = np.inf\n    best_preds = None\n    best_targets = None\n    for e in range(cfg.EPOCHS):\n        train_one_epoch(model, tr_dataloader, cfg, optimizer, loss_fn, loss_fn_aux=loss_fn_aux, accumulation=accumulation, with_aux_class=with_aux_class, verbose=cfg.verbose)\n        preds, targets, score = evals(model, val_dataloader, cfg, loss_fn, loss_fn_aux=loss_fn_aux, with_aux_class=with_aux_class, verbose=cfg.verbose)\n        reducer.step(score)\n        if score < best_score:\n            print(\"## Epochs {0} : Improvement from {1} to {2}\".format(e, best_score, score))\n            best_score = score\n            best_preds = preds\n            best_targets= targets\n            torch.save(model.state_dict(), cfg.save_name + f\"_{fold}.pth\")\n    print(\"## FOLD {0} : best results : {1}\".format(fold, best_score))\n    return best_preds, best_targets, score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference_fold(folds, model, test_loader,cfg):\n    preds = []\n    for fold in range(folds):\n        name = cfg.save_name + f\"_{fold}.pth\"\n        model.load_state_dict(torch.load(name))\n        p = inference_fn(model, test_loader, cfg)\n        preds.append(p)\n    \n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_targets(targets):\n    ### check if targets are all binary in training set\n    \n    for i in range(targets.shape[1]):\n        if len(np.unique(targets[:,i])) != 2:\n            return False\n    return True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def auc_multi(y_true, y_pred):\n    M = y_true.shape[1]\n    results = np.zeros(M)\n    for i in range(M):\n        try:\n            results[i] = roc_auc_score(y_true[:,i], y_pred[:,i])\n        except:\n            pass\n    return results.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# script"},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_labels(A):\n    w = np.zeros(A.shape[1])\n    for i in range(A.shape[1]):\n        if len(np.unique(A[:,i])) == 2:\n            w[i] = 1\n    return w.reshape(1, -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Config(object):\n    def __init__(self):\n        self.num_class = targets_tr.shape[1]\n        self.aux_class = targets2_tr.shape[1]\n        self.use_apex=False\n        self.verbose=False\n        #\n        self.batch_size = 128\n        self.device = \"cpu\"\n        self.SPLITS = 10\n        self.EPOCHS = 100\n        # Parameters model\n        self.num_cats=[3+1,2+1] if remove_vehicle else [2+1,3+1,2+1] \n        self.cats_emb_size=[1]* cat_tr.shape[1] #to choose\n        self.num_numericals= len(col_features[3:])\n        self.hidden_size_numericals=1024 # to choose\n        self.num_numericals= numerical_tr.shape[1]\n        self.hidden_size_numericals=2048 # to choose\n        self.num_ensembling = 1\n        # save\n        self.seed = 42\n        self.save_name = f\"MOA_mlp-KFOLD{self.SPLITS}\"\n        \n        self.strategy = \"KFOLD\" # or \ncfg = Config()\ncfg.with_aux_class = True if cfg.aux_class is not None else False\nprint(cfg.num_class, cfg.aux_class,cfg.with_aux_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")#, pos_weight=torch.as_tensor(ratios))\nloss_fn_aux = nn.BCEWithLogitsLoss(reduction=\"none\")#, pos_weight=torch.as_tensor(ratios))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = MOADataset(cat_test, numerical_test)\ntest_dataloader= DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if cfg.strategy == \"KFOLD\":\n    oof_preds_all = []\n    oof_targets_all = []\n    scores_all = []\n    scores_auc_all = []\n    preds_test = []\n    masks = []\n    for seed in range(cfg.num_ensembling):\n        mskf = MultilabelStratifiedKFold(n_splits=cfg.SPLITS, random_state=cfg.seed+seed, shuffle=True)\n        oof_preds = []\n        oof_targets = []\n        scores = []\n        scores_auc = []\n        p = []\n        temp_mask = []\n        for j, (train_idx, val_idx) in enumerate(mskf.split(np.zeros(len(cat_tr)), targets_tr)):\n            print(\"FOLDS : \", j)\n\n            ## model\n\n            model = MOA_MLP(num_cats=cfg.num_cats , cats_emb_size=cfg.cats_emb_size, num_numericals=cfg.num_numericals, hidden_size_numericals=cfg.hidden_size_numericals,\n                        num_class=cfg.num_class, aux=cfg.aux_class).to(cfg.device)\n            \n            optimizer = optim.Adam(model.parameters(), lr = 5e-3, weight_decay=1e-5, amsgrad=True)#optim.SGD(model.parameters(), lr=1e-2, weight_decay=5e-4, momentum=0.9, nesterov=True) \n            reducer = ReduceLROnPlateau(optimizer, mode='min', factor=0.1,threshold=1e-3, patience=3,  min_lr=5e-6, eps=1e-08, verbose=True)\n            ## Create dataset then dataloader\n            if cfg.with_aux_class:\n                train_dataset = MOADataset(cat_tr[train_idx], numerical_tr[train_idx], y=targets_tr[train_idx], y2=targets2_tr[train_idx])\n                val_dataset = MOADataset(cat_tr[val_idx], numerical_tr[val_idx], y=targets_tr[val_idx], y2=targets2_tr[val_idx])\n            else:\n                train_dataset = MOADataset(cat_tr[train_idx], numerical_tr[train_idx], y=targets_tr[train_idx])\n                val_dataset = MOADataset(cat_tr[val_idx], numerical_tr[val_idx], y=targets_tr[val_idx] )\n            train_dataloader =    DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n            val_dataloader =    DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=2)\n\n            ## train fold\n            preds , targets, score = train_fold(f\"_{j}_{seed}\", model, train_dataloader, val_dataloader, cfg, optimizer, reducer, loss_fn, \n                                                loss_fn_aux=loss_fn_aux, accumulation=1, with_aux_class=cfg.with_aux_class)\n\n            ## save oof to compute the CV later\n            temp_mask.append(check_labels(targets_tr[train_idx]))\n            oof_preds.append(preds)\n            oof_targets.append(targets)\n            scores.append(score)\n            scores_auc.append(auc_multi(targets,preds))\n            p.append(inference_fn(model, test_dataloader, cfg,verbose=False))\n\n        oof_preds_all.append(np.concatenate(oof_preds))\n        oof_targets_all.append(np.concatenate(oof_targets))\n        scores_all.append(scores)\n        scores_auc_all.append(scores_auc)\n        preds_test.append(np.array(p))\n        masks.append(np.stack(temp_mask))\n        \n\n    preds_test = np.stack(preds_test)\n    oof_preds_all = np.stack(oof_preds_all)\n    oof_targets_all = np.stack(oof_targets_all)\n    scores_all = np.stack(scores_all)\n    scores_auc_all = np.stack(scores_auc_all)\n    masks = np.stack(masks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if cfg.strategy == \"KFOLD\":\n    for i in range(oof_preds_all.shape[0]):\n        print(\"CV score : \", log_loss_multi(oof_targets_all[i], oof_preds_all[i]))\n        print(\"auc mean : \", sum(scores_auc_all[i])/len(scores_auc_all[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if cfg.strategy != \"KFOLD\":\n    i = 0\n    mskf = MultilabelStratifiedShuffleSplit(n_splits=1000, test_size=0.1, random_state=0)\n    oof_preds = []\n    oof_targets = []\n    scores = []\n    scores_auc = []\n    for j, (train_idx, val_idx) in enumerate(mskf.split(np.zeros(len(cat_tr)), targets_tr)):\n        if i == cfg.SPLITS:\n            break\n            \n        if not check_targets(targets_tr[train_idx]):\n            continue\n        print(\"FOLDS : \", i, j)\n\n        ## model\n\n        model = MOA_MLPv2(num_cats=cfg.num_cats , cats_emb_size=cfg.cats_emb_size, num_numericals=cfg.num_numericals, hidden_size_numericals=cfg.hidden_size_numericals,\n                    num_class=cfg.num_class, aux=cfg.aux_class)\n        optimizer = optim.Adam(model.parameters(), lr = 1e-3, amsgrad=True)#optim.SGD(model.parameters(), lr=1e-2, weight_decay=5e-4, momentum=0.9, nesterov=True) \n        reducer = ReduceLROnPlateau(optimizer, mode='min', factor=0.1,threshold=1e-3, patience=3,  min_lr=1e-5, eps=1e-08, verbose=True)\n        ## Create dataset then dataloader\n        if cfg.with_aux_class:\n            train_dataset = MOADataset(cat_tr[train_idx], numerical_tr[train_idx], y=targets_tr[train_idx], y2=targets2_tr[train_idx])\n            val_dataset = MOADataset(cat_tr[val_idx], numerical_tr[val_idx], y=targets_tr[val_idx], y2=targets2_tr[val_idx])\n        else:\n            train_dataset = MOADataset(cat_tr[train_idx], numerical_tr[train_idx], y=targets_tr[train_idx])\n            val_dataset = MOADataset(cat_tr[val_idx], numerical_tr[val_idx], y=targets_tr[val_idx] )\n        train_dataloader =    DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n        val_dataloader =    DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=2)\n\n        ## train fold\n        preds , targets, score = train_fold(i, model, train_dataloader, val_dataloader, cfg, optimizer, reducer, loss_fn, \n                                            loss_fn_aux=loss_fn_aux, accumulation=1, with_aux_class=cfg.with_aux_class)\n\n        ## save oof to compute the CV later\n        oof_preds.append(preds)\n        oof_targets.append(targets)\n        scores.append(score)\n        scores_auc.append(auc_multi(targets,preds))\n        i+=1\n        #break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if cfg.strategy != \"KFOLD\":\n    oof_preds = np.concatenate(oof_preds)\n    oof_targets = np.concatenate(oof_targets)\n    print(\"CV score : \", log_loss_multi(oof_targets, oof_preds))\n    print(\"auc mean : \", sum(scores_auc)/len(scores_auc))\n    print(oof_preds.shape, oof_targets.shape, targets_tr.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_test2 = preds_test.sum(1).sum(0)/masks.sum(1).sum(0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[columns] = preds_test2#preds_test.mean(1).mean(0)\nsubmission.loc[test_features['cp_type']=='ctl_vehicle', submission.columns[1:]] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}