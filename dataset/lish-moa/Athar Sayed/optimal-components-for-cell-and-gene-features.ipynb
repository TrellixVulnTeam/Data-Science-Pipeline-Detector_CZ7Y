{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Intro :\n1. We need to reduce dimensionality of cell and gene features !\n2. Most public Kernels pick a Number and do Pca , while this method works well there is another much more reliable and most probably .\n3. The Method can be found [Here](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2896/pca-for-dimensionality-reduction-not-visualization/2/module-2-data-science-exploratory-data-analysis-and-data-visualization)\n4. Do let me know in the comments if it offers some improvements! Also please do upvote kernel if you find it useful!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\nimport os\nimport gc\nimport pickle\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom tqdm.notebook import tqdm\nfrom time import time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\ntrain_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\n\nss0 = pd.read_csv('../input/lish-moa/sample_submission.csv') \nss = pd.read_csv('../input/lish-moa/sample_submission.csv') \n\ncols = [c for c in ss.columns.values if c != 'sig_id']\nGENES = [col for col in train_features.columns if col.startswith('g-')] # gene columns\nCELLS = [col for col in train_features.columns if col.startswith('c-')] # cell Columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    return df\n\ndef metric(y_true, y_pred):\n    metrics = []\n    for _target in train_targets.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n    return np.mean(metrics)\n\ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']\ndel train_targets_nonscored['sig_id']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Optimal Components for Gene Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n# Quantile Transform is used to Map Each Feature to a Normal Distribution \n# This transformation tends to spread out the most frequent values and also reduces impact of Outliers\nstd_scalar = StandardScaler()\n# Fit Quantile to both Train and Test\nstd_scalar.fit(pd.concat([pd.DataFrame(train[GENES]), pd.DataFrame(test[GENES])]))\n# Transform Train and Test Cell and Gene Features Individually\ntrain[GENES] = std_scalar.transform(train[GENES])\ntest[GENES] = std_scalar.transform(test[GENES])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[GENES].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\npca = PCA()\npca.n_components = 772\ndata = pd.concat([pd.DataFrame(train[GENES]), pd.DataFrame(test[GENES])])\npca_data = pca.fit_transform(data)\npercentage_var_explained = pca.explained_variance_ / np.sum(pca.explained_variance_);\n\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\n\nplt.clf()\nplt.plot(cum_var_explained, linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. We can see just by using 400 Components of Gene Features we are able to Explain 90% Variance\n2. By using 500 features we can explain 95 % of Variance"},{"metadata":{},"cell_type":"markdown","source":"## 2. Optimal Components for Cell Features "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n# Quantile Transform is used to Map Each Feature to a Normal Distribution \n# This transformation tends to spread out the most frequent values and also reduces impact of Outliers\nstd_scalar = StandardScaler()\n# Fit Quantile to both Train and Test\nstd_scalar.fit(pd.concat([pd.DataFrame(train[CELLS]), pd.DataFrame(test[CELLS])]))\n# Transform Train and Test Cell and Gene Features Individually\ntrain[CELLS] = std_scalar.transform(train[CELLS])\ntest[CELLS] = std_scalar.transform(test[CELLS])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\npca = PCA()\npca.n_components = 100\ndata = pd.concat([pd.DataFrame(train[CELLS]), pd.DataFrame(test[CELLS])])\npca_data = pca.fit_transform(data)\npercentage_var_explained = pca.explained_variance_ / np.sum(pca.explained_variance_);\n\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\n\nplt.clf()\nplt.plot(cum_var_explained, linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. We can see that by using 80 principal components we can explain nearly more than 98 % of Variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}