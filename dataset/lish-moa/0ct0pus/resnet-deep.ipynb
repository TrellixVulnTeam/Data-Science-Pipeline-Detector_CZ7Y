{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\nimport gc,os\nfrom time import time\nimport datetime,random\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold,GroupKFold\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import QuantileTransformer\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset,TensorDataset, DataLoader,RandomSampler\n#from pytorch_tabnet.tab_model import TabNetRegressor\n#from pytorch_tabnet.metrics import Metric as TabNet_Metric\n\nwarnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"root = './'\nid_name = 'sig_id'\nvariance_threshould = 0.7\nncompo_genes = 600\nncompo_cells = 50\nseed=817119","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nSeed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Metric(labels,preds):\n    labels = np.array(labels)\n    preds = np.array(preds)\n    metric = 0\n    for i in range(labels.shape[1]):\n        metric += (-np.mean(labels[:,i]*np.log(np.maximum(preds[:,i],1e-15))+(1-labels[:,i])*np.log(np.maximum(1-preds[:,i],1e-15))))\n    return metric/labels.shape[1]\n\ndef Write_log(logFile,text,isPrint=True):\n    if isPrint:\n        print(text)\n    logFile.write(text)\n    logFile.write('\\n')\n    return None\n\nfrom scipy.special import erfinv\ndef Rank_gauss_encoding(df,cols):\n    for col in cols:\n        print('rank gauss encoding:',col)\n        tmp = df[col].rank().values - 1.0 #rank(method=\"dense\"),默认rank方法是中值法，dense是连续法(都是从1开始)\n        tmp = (tmp / tmp.max())  * 0.998 + 0.001\n        efi = np.sqrt(2.0)*erfinv(tmp)\n        efi = efi - efi.mean()\n        df[col] = np.float16(efi)\n    return df\n\ndef rationalApproximation(t):\n    c = [2.515517, 0.802853, 0.010328]\n    d = 1.432788, 0.189269, 0.001308\n    return t - ((c[2]*t + c[1])*t + c[0]) / (((d[2]*t + d[1])*t + d[0])*t + 1.0)\n\ndef normalCDFInverse(p):\n    if (p <= 0.0 or p >= 1.0):\n        assert(False)\n    if (p < 0.5):\n        return -rationalApproximation(np.sqrt(-2.0*np.log(p)) )\n    return rationalApproximation(np.sqrt(-2.0*np.log(1-p)) )\n\ndef vdErfInvSingle(x):\n    if x == 0.0:\n        return 0.0\n    elif x < 0.0:\n        return -normalCDFInverse(-x)*0.7\n    else:\n        return normalCDFInverse(x)*0.7\n\ndef rankGauss(df, features):\n    df_size = df.shape[0]\n    for f in features:\n        if len(set(df[f])) == 1:\n            df[f] = 0\n        elif len(set(df[f])) == 2:\n            vals = sorted(list(set(df[f])))\n            df[f] = df[f].replace(vals,[0,1])\n        else:\n            df[\"rank\"] = (df[f].rank(method=\"min\")-1.0)/df_size*0.998+0.001\n            df[f] = df[\"rank\"].apply(lambda x:vdErfInvSingle(x))\n            df[f] = df[f] - df[f].mean()\n        print(\"feature:%s rankGauss over\"%(f))\n    if \"rank\" in df.columns.values.tolist():\n        df.drop([\"rank\"],axis=1,inplace=True)\n    print(\"rankGauss over\")\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"files = ['../input/lish-moa/test_features.csv', \n         '../input/lish-moa/train_targets_scored.csv',\n         '../input/lish-moa/train_features.csv',\n         '../input/lish-moa/train_targets_nonscored.csv',\n         '../input/lish-moa/train_drug.csv',\n         '../input/lish-moa/sample_submission.csv']\n\ntest = pd.read_csv(files[0])\ntrain_target = pd.read_csv(files[1])\ntrain = pd.read_csv(files[2])\ntrain_nonscored = pd.read_csv(files[3])\ntrain_drug = pd.read_csv(files[4])\nsub = pd.read_csv('../input/lish-moa/sample_submission.csv')\n#train_cs = pd.read_csv('../input/moamodel/train_cs.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genes = [col for col in train.columns if col.startswith(\"g-\")]\ncells = [col for col in train.columns if col.startswith(\"c-\")]\n\nfeatures = genes + cells\ntargets = [col for col in train_target if col!='sig_id']\n#targets_ns=[col for col in train_nonscored if col!='sig_id']+[col for col in train_target if col!='sig_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ori_train = train.copy()\nctl_train = train.loc[train['cp_type']=='ctl_vehicle'].append(test.loc[test['cp_type']=='ctl_vehicle']).reset_index(drop=True)\nctl_train2 = train.loc[train['cp_type']=='ctl_vehicle'].reset_index(drop=True)\n\nori_test = test.copy()\nctl_test = test.loc[test['cp_type']=='ctl_vehicle'].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Feature(df):\n    for col in ['cp_time','cp_dose']:\n        tmp = pd.get_dummies(df[col],prefix=col)\n        df = pd.concat([df,tmp],axis=1)\n        df.drop([col],axis=1,inplace=True)\n    for col in genes:\n        if df[col].std() < 1.0:\n            df.drop([col],axis=1,inplace=True)\n            print(col)\n        genes.remove(col)\n  \n    df[genes+cells] = df[genes+cells]/10.0\n    df['gene_gt_0'] = (df[genes]>0.).mean(axis=1)\n    df['gene_lt_0'] = (df[genes]<0.).mean(axis=1)\n    df['cell_gt_0'] = (df[cells]>0.).mean(axis=1)\n    df['cell_lt_0'] = (df[cells]<0.).mean(axis=1)\n    df['gene_gt_0.7'] = (df[genes]>0.7).mean(axis=1)\n    df['gene_lt_0.7'] = (df[genes]<0.7).mean(axis=1)\n    df['cell_gt_0.7'] = (df[cells]>0.7).mean(axis=1)\n    df['cell_lt_0.7'] = (df[cells]<0.7).mean(axis=1)\n    df['gene_gt_0.9'] = (df[genes]>0.9).mean(axis=1)\n    df['gene_lt_0.9'] = (df[genes]<0.9).mean(axis=1)\n    df['cell_gt_0.9'] = (df[cells]>0.9).mean(axis=1)\n    df['cell_lt_0.9'] = (df[cells]<0.9).mean(axis=1)\n    #df['gene_abs_gt_0.7'] = (df[genes].abs()>0.7).mean(axis=1) * 5\n    #df['cell_abs_gt_0.7'] = (df[cells].abs()>0.7).mean(axis=1) * 5\n    #df = rankGauss(df,genes+cells)\n    pca_genes = PCA(n_components = ncompo_genes,\n                    random_state = seed).fit_transform(df[genes])\n    pca_cells = PCA(n_components = ncompo_cells,\n                    random_state = seed).fit_transform(df[cells])\n    pca_genes = pd.DataFrame(pca_genes, columns = [f\"pca_g-{i}\" for i in range(ncompo_genes)])\n    pca_cells = pd.DataFrame(pca_cells, columns = [f\"pca_c-{i}\" for i in range(ncompo_cells)])\n    df = pd.concat([df, pca_genes, pca_cells], axis = 1)\n    pca_cols = [col for col in df.columns if 'pca' in col]\n    for col in pca_cols:\n        df[col] = df[col] / df[col].abs().max( )\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Feature1(df):\n    '''df['gene_gt_0'] = (df[genes]>0.).mean(axis=1)\n    df['gene_lt_0'] = (df[genes]<0.).mean(axis=1)\n    df['cell_gt_0'] = (df[cells]>0.).mean(axis=1)\n    df['cell_lt_0'] = (df[cells]<0.).mean(axis=1)\n    df['gene_gt_0.7'] = (df[genes]>7.).mean(axis=1)\n    df['gene_lt_0.7'] = (df[genes]<-7.).mean(axis=1)\n    df['cell_gt_0.7'] = (df[cells]>7.).mean(axis=1)\n    df['cell_lt_0.7'] = (df[cells]<-7.).mean(axis=1)\n    df['gene_gt_0.9'] = (df[genes]>9.).mean(axis=1)\n    df['gene_lt_0.9'] = (df[genes]<-9.).mean(axis=1)\n    df['cell_gt_0.9'] = (df[cells]>9.).mean(axis=1)\n    df['cell_lt_0.9'] = (df[cells]<-9.).mean(axis=1)'''\n    for col in tqdm(genes+cells):\n        transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution='normal')\n        transformer.fit(df[:train.shape[0]][col].values.reshape(-1,1))\n        df[col] = transformer.transform(df[col].values.reshape(-1,1)).reshape(1,-1)[0]\n    pca_genes = PCA(n_components = ncompo_genes,\n                    random_state = 42).fit_transform(df[genes])\n    pca_cells = PCA(n_components = ncompo_cells,\n                    random_state = 42).fit_transform(df[cells])\n    pca_genes = pd.DataFrame(pca_genes, columns = [f\"pca_g-{i}\" for i in range(ncompo_genes)])\n    pca_cells = pd.DataFrame(pca_cells, columns = [f\"pca_c-{i}\" for i in range(ncompo_cells)])\n    df = pd.concat([df, pca_genes, pca_cells], axis = 1)\n    \n    nor_var_col = [col for col in df.columns if col in ['sig_id','cp_type','cp_time','cp_dose'] or '_gt_' in col or '_lt_' in col]\n    \n    var_thresh = VarianceThreshold(0.8)\n    var_cols = [col for col in df.columns if col not in ['sig_id','cp_type','cp_time','cp_dose'] and '_gt_' not in col and '_lt_' not in col]\n    var_data = var_thresh.fit_transform(df[var_cols])\n    df = pd.concat([df[nor_var_col],pd.DataFrame(var_data)],axis=1)\n    '''for col in df.columns:\n        if col not in nor_var_col:\n            df[col] /= df[col].abs().max()'''\n    for col in ['cp_time','cp_dose']:\n        tmp = pd.get_dummies(df[col],prefix=col)\n        df = pd.concat([df,tmp],axis=1)\n        df.drop([col],axis=1,inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Feature2(df):\n    '''df['gene_gt_0'] = (df[genes]>0.).mean(axis=1)\n    df['gene_lt_0'] = (df[genes]<0.).mean(axis=1)\n    df['cell_gt_0'] = (df[cells]>0.).mean(axis=1)\n    df['cell_lt_0'] = (df[cells]<0.).mean(axis=1)\n    df['gene_gt_0.7'] = (df[genes]>7.).mean(axis=1)\n    df['gene_lt_0.7'] = (df[genes]<-7.).mean(axis=1)\n    df['cell_gt_0.7'] = (df[cells]>7.).mean(axis=1)\n    df['cell_lt_0.7'] = (df[cells]<-7.).mean(axis=1)\n    df['gene_gt_0.9'] = (df[genes]>9.).mean(axis=1)\n    df['gene_lt_0.9'] = (df[genes]<-9.).mean(axis=1)\n    df['cell_gt_0.9'] = (df[cells]>9.).mean(axis=1)\n    df['cell_lt_0.9'] = (df[cells]<-9.).mean(axis=1)'''\n    #df['gene_2'] = (df[genes]**2).mean(axis=1)\n    #df['cell_2'] = (df[cells]**2).mean(axis=1)\n    transformers={}\n    for col in tqdm(genes+cells):\n        transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution='normal')\n        transformer.fit(df[:train.shape[0]][col].values.reshape(-1,1))\n        df[col] = transformer.transform(df[col].values.reshape(-1,1)).reshape(1,-1)[0]\n        transformers[col]=transformer\n    gene_pca = PCA(n_components = ncompo_genes,\n                    random_state = 42).fit(df[genes])\n    pca_genes = gene_pca.transform(df[genes])\n    cell_pca = PCA(n_components = ncompo_cells,\n                    random_state = 42).fit(df[cells])\n    pca_cells = cell_pca.transform(df[cells])\n    pca_genes = pd.DataFrame(pca_genes, columns = [f\"pca_g-{i}\" for i in range(ncompo_genes)])\n    pca_cells = pd.DataFrame(pca_cells, columns = [f\"pca_c-{i}\" for i in range(ncompo_cells)])\n    df = pd.concat([df, pca_genes, pca_cells], axis = 1)\n    \n    '''nor_var_col = [col for col in df.columns if col in ['sig_id','cp_type','cp_time','cp_dose'] or '_gt_' in col or '_lt_' in col]\n    \n    var_thresh = VarianceThreshold(0.8)\n    var_cols = [col for col in df.columns if col not in ['sig_id','cp_type','cp_time','cp_dose'] and '_gt_' not in col and '_lt_' not in col]\n    var_data = var_thresh.fit_transform(df[var_cols])\n    df = pd.concat([df[nor_var_col],pd.DataFrame(var_data)],axis=1)'''\n    for col in ['cp_time','cp_dose']:\n        tmp = pd.get_dummies(df[col],prefix=col)\n        df = pd.concat([df,tmp],axis=1)\n        df.drop([col],axis=1,inplace=True)\n    return df,transformers,gene_pca,cell_pca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Feature3(df):\n    '''df['gene_gt_0'] = (df[genes]>0.).mean(axis=1)\n    df['gene_lt_0'] = (df[genes]<0.).mean(axis=1)\n    df['cell_gt_0'] = (df[cells]>0.).mean(axis=1)\n    df['cell_lt_0'] = (df[cells]<0.).mean(axis=1)\n    df['gene_gt_0.7'] = (df[genes]>7.).mean(axis=1)\n    df['gene_lt_0.7'] = (df[genes]<-7.).mean(axis=1)\n    df['cell_gt_0.7'] = (df[cells]>7.).mean(axis=1)\n    df['cell_lt_0.7'] = (df[cells]<-7.).mean(axis=1)\n    df['gene_gt_0.9'] = (df[genes]>9.).mean(axis=1)\n    df['gene_lt_0.9'] = (df[genes]<-9.).mean(axis=1)\n    df['cell_gt_0.9'] = (df[cells]>9.).mean(axis=1)\n    df['cell_lt_0.9'] = (df[cells]<-9.).mean(axis=1)'''\n    #df['gene_2'] = (df[genes]**2).mean(axis=1)\n    #df['cell_2'] = (df[cells]**2).mean(axis=1)\n    transformers={}\n    for col in tqdm(genes+cells):\n        transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution='normal')\n        transformer.fit(df[:train.shape[0]][col].values.reshape(-1,1))\n        df[col] = transformer.transform(df[col].values.reshape(-1,1)).reshape(1,-1)[0]\n        transformers[col]=transformer\n    gene_pca = PCA(n_components = ncompo_genes,\n                    random_state = 42).fit(df[genes])\n    pca_genes = gene_pca.transform(df[genes])\n    cell_pca = PCA(n_components = ncompo_cells,\n                    random_state = 42).fit(df[cells])\n    pca_cells = cell_pca.transform(df[cells])\n    pca_genes = pd.DataFrame(pca_genes, columns = [f\"pca_g-{i}\" for i in range(ncompo_genes)])\n    pca_cells = pd.DataFrame(pca_cells, columns = [f\"pca_c-{i}\" for i in range(ncompo_cells)])\n    df = pd.concat([df, pca_genes, pca_cells], axis = 1)\n    \n    nor_var_col = [col for col in df.columns if col in ['sig_id','cp_type','cp_time','cp_dose'] or '_gt_' in col or '_lt_' in col]\n    \n    var_thresh = VarianceThreshold(0.8)\n    var_cols = [col for col in df.columns if col not in ['sig_id','cp_type','cp_time','cp_dose'] and '_gt_' not in col and '_lt_' not in col]\n    var_data = var_thresh.fit_transform(df[var_cols])\n    df = pd.concat([df[nor_var_col],pd.DataFrame(var_data)],axis=1)\n    for col in ['cp_time','cp_dose']:\n        tmp = pd.get_dummies(df[col],prefix=col)\n        df = pd.concat([df,tmp],axis=1)\n        df.drop([col],axis=1,inplace=True)\n    return df,transformers,gene_pca,cell_pca,var_thresh","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tt = train.append(test).reset_index(drop=True)\ntt,transformers,gene_pca,cell_pca,var_thresh = Feature3(tt)\ntrain = tt[:train.shape[0]]\ntest = tt[train.shape[0]:].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if 1:\n    train_target = train_target.loc[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n    #train_nonscored = train_nonscored.loc[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n    #train_drug = train_drug.loc[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n    ori_train = ori_train.loc[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n    train = train.loc[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class resnetModel(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size,ispretrain=False):\n        super(resnetModel, self).__init__()\n        self.ispretrain=ispretrain\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        \n        self.batch_norm2 = nn.BatchNorm1d(num_features+hidden_size)\n        self.dropout2 = nn.Dropout(0.2619422201258426)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(num_features+hidden_size, hidden_size))\n        self.batch_norm20 = nn.BatchNorm1d(hidden_size)\n        self.dropout20 = nn.Dropout(0.2619422201258426)\n        self.dense20 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n\n        self.batch_norm3 = nn.BatchNorm1d(2*hidden_size)\n        self.dropout3 = nn.Dropout(0.2619422201258426)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(2*hidden_size, hidden_size))\n        self.batch_norm30 = nn.BatchNorm1d(hidden_size)\n        self.dropout30 = nn.Dropout(0.2619422201258426)\n        self.dense30 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        #self.batch_norm6 = nn.BatchNorm1d(2*hidden_size)\n        #self.dropout6 = nn.Dropout(0.2619422201258426)\n        #self.dense6 = nn.utils.weight_norm(nn.Linear(2*hidden_size, hidden_size))\n        #self.batch_norm60 = nn.BatchNorm1d(hidden_size)\n        #self.dropout60 = nn.Dropout(0.2619422201258426)\n        #self.dense60 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm4 = nn.BatchNorm1d(2*hidden_size)\n        self.dropout4 = nn.Dropout(0.2619422201258426)\n        if self.ispretrain:\n          self.dense4 = nn.utils.weight_norm(nn.Linear(2*hidden_size, num_targets))\n        else:\n          self.dense5 = nn.utils.weight_norm(nn.Linear(2*hidden_size, num_targets))\n    \n    def forward(self, x):\n        x1 = self.batch_norm1(x)\n        x1 = F.leaky_relu(self.dense1(x1))\n        x = torch.cat([x,x1],1)\n        \n        x2 = self.batch_norm2(x)\n        x2 = self.dropout2(x2)\n        x2 = F.leaky_relu(self.dense2(x2))\n        x2 = self.batch_norm20(x2)\n        x2 = self.dropout20(x2)\n        x2 = F.leaky_relu(self.dense20(x2))\n        x = torch.cat([x1,x2],1)\n\n        x3 = self.batch_norm3(x)\n        x3 = self.dropout3(x3)\n        x3 = F.leaky_relu(self.dense3(x3))\n        x3 = self.batch_norm30(x3)\n        x3 = self.dropout30(x3)\n        x3 = F.leaky_relu(self.dense30(x3))\n        x = torch.cat([x2,x3],1)\n        \n        #x4 = self.batch_norm6(x)\n        #x4 = self.dropout6(x4)\n        #x4 = F.leaky_relu(self.dense6(x4))\n        #x4 = self.batch_norm60(x4)\n        #x4 = self.dropout60(x4)\n        #x4 = F.leaky_relu(self.dense60(x4))\n        #x4 = torch.cat([x3,x4],1)\n        \n        x4 = self.batch_norm4(x)\n        x4 = self.dropout4(x4)\n        if self.ispretrain:\n          x4 = self.dense4(x4)\n        else:\n          x4 = self.dense5(x4)\n        return x4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class resnetModel2(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(resnetModel, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        \n        self.batch_norm2 = nn.BatchNorm1d(num_features+hidden_size)\n        self.dropout2 = nn.Dropout(0.2619422201258426)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(num_features+hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(2*hidden_size)\n        self.dropout3 = nn.Dropout(0.2619422201258426)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(2*hidden_size, hidden_size))\n        \n        self.batch_norm4 = nn.BatchNorm1d(2*hidden_size)\n        self.dropout4 = nn.Dropout(0.2619422201258426)\n        self.dense4 = nn.utils.weight_norm(nn.Linear(2*hidden_size, num_targets))\n    \n    def forward(self, x):\n        x1 = self.batch_norm1(x)\n        x1 = F.leaky_relu(self.dense1(x1))\n        x = torch.cat([x,x1],1)\n        \n        x2 = self.batch_norm2(x)\n        x2 = self.dropout2(x2)\n        x2 = F.leaky_relu(self.dense2(x2))\n        x = torch.cat([x1,x2],1)\n        \n        x3 = self.batch_norm3(x)\n        x3 = self.dropout3(x3)\n        x3 = self.dense3(x3)\n        x3 = torch.cat([x2,x3],1)\n        \n        x3 = self.batch_norm4(x3)\n        x3 = self.dropout4(x3)\n        x3 = self.dense4(x3)\n        return x3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Ctl_augment(train,target,include_test=0):\n    if include_test==0:\n        ctl_aug=ctl_train2.copy()\n    if include_test==1:\n        ctl_aug=ctl_train.copy()\n    aug_trains = []\n    aug_targets = []\n    for t in [24,48,72]:\n        for d in ['D1','D2']:\n            for _ in range(3):\n                train1 = train.loc[(train['cp_time']==t)&(train['cp_dose']==d)]\n                target1 = target.loc[(train['cp_time']==t)&(train['cp_dose']==d)]\n                ctl1 = ctl_aug.loc[(ctl_aug['cp_time']==t)&(ctl_aug['cp_dose']==d)].sample(train1.shape[0],replace=True)\n                ctl2 = ctl_aug.loc[(ctl_aug['cp_time']==t)&(ctl_aug['cp_dose']==d)].sample(train1.shape[0],replace=True)\n                train1[genes+cells] = train1[genes+cells].values + ctl1[genes+cells].values - ctl2[genes+cells].values\n                aug_train = train1.merge(target1,how='left',on='sig_id')\n                aug_trains.append(aug_train[['cp_time','cp_dose']+genes+cells])\n                aug_targets.append(aug_train[targets])\n    df = pd.concat(aug_trains).reset_index(drop=True)\n    target = pd.concat(aug_targets).reset_index(drop=True)\n    for col in tqdm(genes+cells):\n        df[col] = transformers[col].transform(df[col].values.reshape(-1,1)).reshape(1,-1)[0]\n    pca_genes = gene_pca.transform(df[genes])\n    pca_cells = cell_pca.transform(df[cells])\n    pca_genes = pd.DataFrame(pca_genes, columns = [f\"pca_g-{i}\" for i in range(ncompo_genes)])\n    pca_cells = pd.DataFrame(pca_cells, columns = [f\"pca_c-{i}\" for i in range(ncompo_cells)])\n    df = pd.concat([df, pca_genes, pca_cells], axis = 1)\n    for col in ['cp_time','cp_dose']:\n        tmp = pd.get_dummies(df[col],prefix=col)\n        df = pd.concat([df,tmp],axis=1)\n        df.drop([col],axis=1,inplace=True)\n    xs = df[train_cols].values\n    ys = target[targets]\n    #ys_ns = target[targets_ns]\n    return xs,ys#,ys_ns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Ctl_augment_new(train,target,include_test=0):\n    if include_test==0:\n        ctl_aug=ctl_train2.copy()\n    if include_test==1:\n        ctl_aug=ctl_train.copy()\n    aug_trains = []\n    aug_targets = []\n    for _ in range(3):\n          train1 = train.copy()\n          target1 = target.copy()\n          ctl1 = ctl_train.sample(train1.shape[0],replace=True).reset_index(drop=True)#.loc[(ctl_train['cp_time']==t)&(ctl_train['cp_dose']==d)]\n          ctl2 = ctl_train.sample(train1.shape[0],replace=True).reset_index(drop=True)\n\n          ctl3 = ctl_train.sample(train1.shape[0],replace=True).reset_index(drop=True)#.loc[(ctl_train['cp_time']==t)&(ctl_train['cp_dose']==d)]\n          ctl4 = ctl_train.sample(train1.shape[0],replace=True).reset_index(drop=True)\n          mask_index1 = list(np.random.choice(ctl3.index.tolist(),int(ctl3.shape[0]*0.4),replace=False))\n          ctl3.loc[mask_index1,genes+cells] = 0.0\n          ctl4.loc[mask_index1,genes+cells] = 0.0\n\n          ctl5 = ctl_train.sample(train1.shape[0],replace=True).reset_index(drop=True)#.loc[(ctl_train['cp_time']==t)&(ctl_train['cp_dose']==d)]\n          ctl6 = ctl_train.sample(train1.shape[0],replace=True).reset_index(drop=True)\n          mask_index2 = list(np.random.choice(list(set(ctl5.index)-set(mask_index1)),int(ctl5.shape[0]*0.3),replace=False))\n          ctl5.loc[mask_index1+mask_index2,genes+cells] = 0.0\n          ctl6.loc[mask_index1+mask_index2,genes+cells] = 0.0\n\n          train1[genes+cells] = train1[genes+cells].values + ctl1[genes+cells].values - ctl2[genes+cells].values \\\n                              + ctl3[genes+cells].values - ctl4[genes+cells].values + ctl5[genes+cells].values - ctl6[genes+cells].values\n\n          aug_train = train1.merge(target1,how='left',on='sig_id')\n          aug_trains.append(aug_train[['cp_time','cp_dose']+genes+cells])\n          aug_targets.append(aug_train[targets])\n\n    df = pd.concat(aug_trains).reset_index(drop=True)\n    target = pd.concat(aug_targets).reset_index(drop=True)\n    for col in tqdm(genes+cells):\n        df[col] = transformers[col].transform(df[col].values.reshape(-1,1)).reshape(1,-1)[0]\n    pca_genes = gene_pca.transform(df[genes])\n    pca_cells = cell_pca.transform(df[cells])\n    pca_genes = pd.DataFrame(pca_genes, columns = [f\"pca_g-{i}\" for i in range(ncompo_genes)])\n    pca_cells = pd.DataFrame(pca_cells, columns = [f\"pca_c-{i}\" for i in range(ncompo_cells)])\n    df = pd.concat([df, pca_genes, pca_cells], axis = 1)\n    \n    nor_var_col = [col for col in df.columns if col in ['sig_id','cp_type','cp_time','cp_dose'] or '_gt_' in col or '_lt_' in col]\n    var_cols = [col for col in df.columns if col not in ['sig_id','cp_type','cp_time','cp_dose'] and '_gt_' not in col and '_lt_' not in col]\n    var_data = var_thresh.transform(df[var_cols])\n    df = pd.concat([df[nor_var_col],pd.DataFrame(var_data)],axis=1)\n\n    for col in ['cp_time','cp_dose']:\n        tmp = pd.get_dummies(df[col],prefix=col)\n        df = pd.concat([df,tmp],axis=1)\n        df.drop([col],axis=1,inplace=True)\n    xs = df[train_cols].values\n    ys = target[targets]\n    #ys_ns = target[targets_ns]\n    return xs,ys#,ys_ns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets,noise=0.1,val=0):\n        self.features = features\n        self.targets = targets\n        self.noise = noise\n        self.val = val\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        sample = self.features[idx, :].copy()\n        \n        if 0 and np.random.rand()<0.3 and not self.val:\n            sample = self.swap_sample(sample)\n        \n        dct = {\n            'x' : torch.tensor(sample, dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \n    def swap_sample(self,sample):\n            #print(sample.shape)\n            num_samples = self.features.shape[0]\n            num_features = self.features.shape[1]\n            if len(sample.shape) == 2:\n                batch_size = sample.shape[0]\n                random_row = np.random.randint(0, num_samples, size=batch_size)\n                for i in range(batch_size):\n                    random_col = np.random.rand(num_features) < self.noise\n                    #print(random_col)\n                    sample[i, random_col] = self.features[random_row[i], random_col]\n            else:\n                batch_size = 1\n          \n                random_row = np.random.randint(0, num_samples, size=batch_size)\n               \n            \n                random_col = np.random.rand(num_features) < self.noise\n                #print(random_col)\n                #print(random_col)\n       \n                sample[ random_col] = self.features[random_row, random_col]\n                \n            return sample\n\nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.2619422201258426)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n        self.dropout4 = nn.Dropout(0.2619422201258426)\n        self.dense4 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.2619422201258426)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = F.leaky_relu(self.dense4(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cs_df = pd.read_csv('../input/moa-cv-file/cv_fold.csv')\n\n#cs_df['most_cs_same_target'] = cs_df['most_cs_same_target'] + 10*(cs_df['cs']//0.1)\ncs_df=cs_df[cs_df['sig_id'].isin(train.sig_id)].reset_index(drop=True)\ndevice = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS1 = 29\nEPOCHS = 23\ntrn_loss_=[]\ndef train_and_predict(features, sub, aug,  folds=5, seed=817119,lr=1/90.0/3.5*3,weight_decay=1e-5/3):\n    dnn_oof = train[['sig_id']]\n    oof = train[['sig_id']]\n    for t in targets:\n        dnn_oof[t] = 0.0\n        oof[t] = 0.0\n    preds = []\n    test_X = test[features].values\n    test_data_loader = DataLoader(dataset=TensorDataset(torch.Tensor(test_X)),batch_size=1024,shuffle=False)\n    \n    #aug_test_X = Ctl_augment_test(ori_test)\n    #aug_test_data_loader = DataLoader(dataset=TensorDataset(torch.Tensor(aug_test_X)),batch_size=1024,shuffle=False)\n    eval_train_loss=0\n    for fold, (trn_ind, val_ind) in enumerate(StratifiedKFold(n_splits = folds, shuffle=True, random_state=seed)\\\n                                              .split(train, cs_df['most_cs_same_target'])):\n        #if fold!=4:\n            #continue\n        train_X = train.loc[trn_ind,features].values\n        #aug_X = aug_df[features].values\n        \n        train_Y = train_target.loc[trn_ind,targets].values\n        eval_train_Y = train_target.loc[trn_ind,targets].values\n        #train_Y_ns = train_target.loc[trn_ind,targets_ns].values\n        #aug_Y = aug_df[targets].values\n        if 0:\n            #aug_X,aug_Y = Mix_augment(ori_train.loc[trn_ind],train_target.loc[trn_ind,targets])\n            aug_X,aug_Y = Ctl_augment(ori_train.loc[trn_ind],train_target.loc[trn_ind],include_test=1)\n            train_X = np.concatenate([train_X,aug_X],axis=0)\n            train_Y = np.concatenate([train_Y,aug_Y],axis=0)\n            #train_Y_ns = np.concatenate([train_Y_ns,aug_Y_ns],axis=0)\n        \n        valid_X = train.loc[val_ind,features].values\n        valid_Y = train_target.loc[val_ind,targets].values\n        #valid_Y_ns = train_target.loc[val_ind,targets_ns].values\n        #if aug:\n            #aug_X,aug_Y = Mix_augment(ori_train.loc[trn_ind],train_target.loc[trn_ind,targets])\n        #    aug_X,aug_Y = Ctl_augment(ori_train.loc[val_ind],train_target.loc[val_ind],include_test=1)\n        #    valid_X = np.concatenate([valid_X,aug_X],axis=0)\n        #    valid_Y = np.concatenate([valid_Y,aug_Y],axis=0)\n        #############################################\n        #             pretrain model                #\n        #############################################\n        '''\n        train_dataset = MoADataset(train_X, train_Y_ns)\n        valid_dataset = MoADataset(valid_X, valid_Y_ns,val=1)\n        \n        train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n        valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1024, shuffle=False)\n    \n    \n        model = Model(len(features),len(targets_ns),1500,ispretrain=True)\n        model.to(device)\n        optimizer = torch.optim.Adam(model.parameters(),betas=(0.9, 0.99), lr=1e-3, weight_decay=weight_decay,eps=1e-5)\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=lr, epochs=20, steps_per_epoch=len(train_data_loader))\n        \n        loss_fn = nn.BCEWithLogitsLoss()\n        loss_tr = SmoothBCEwLogits(smoothing =0.001)\n        \n        best_valid_metric = 1e9\n        not_improve_epochs = 0\n        for epoch in range(20):\n            # train\n            train_loss = 0.0\n            train_num = 0\n            for data in (train_data_loader):\n                optimizer.zero_grad()\n                x,y = data['x'].to(device),data['y'].to(device)\n                outputs = model(x)\n                loss = loss_tr(outputs, y)\n                loss.backward()\n                optimizer.step()\n                scheduler.step()\n                train_num += x.shape[0]\n                train_loss += (loss.item()*x.shape[0])\n                \n            train_loss /= train_num\n            # eval\n            model.eval()\n            valid_loss = 0.0\n            valid_num = 0\n            for data in (valid_data_loader):\n                x,y = data['x'].to(device),data['y'].to(device)\n                outputs = model(x)\n                loss = loss_fn(outputs, y)\n                valid_num += x.shape[0]\n                valid_loss += (loss.item()*x.shape[0])\n            valid_loss /= valid_num\n            \n            if valid_loss < best_valid_metric:\n                torch.save(model.state_dict(),'./model_resnet_fold%s.ckpt'%fold)\n                not_improve_epochs = 0 \n                best_valid_metric = valid_loss\n                print('[epoch %s] lr: %.6f, train_loss: %.6f, valid_metric: %.6f'%(epoch,optimizer.param_groups[0]['lr'],train_loss,valid_loss))\n                trn_loss_.append(train_loss)\n            else:\n                not_improve_epochs += 1\n                print('[epoch %s] lr: %.6f, train_loss: %.6f, valid_metric: %.6f, NIE +1 ---> %s'%(epoch,optimizer.param_groups[0]['lr'],train_loss,valid_loss,not_improve_epochs))\n                if not_improve_epochs >= 50:\n                    break\n            model.train()'''\n        #############################################\n        #         pretrain model finish             #\n        #############################################\n    \n        #train_dataset = MoADataset(train_X, train_Y)\n        valid_dataset = MoADataset(valid_X, valid_Y,val=1)\n        eval_train_dataset = MoADataset(train_X, eval_train_Y)\n        \n        eval_train_data_loader = torch.utils.data.DataLoader(eval_train_dataset, batch_size=128, shuffle=False)\n        valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1024, shuffle=False)\n    \n    \n        model = Model(len(features),len(targets),1500)\n        model.to(device)\n        #state_dict = torch.load('./model_resnet_fold%s.ckpt'%fold, torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") )\n        #model.load_state_dict(state_dict,strict=False)\n        aug_X,aug_Y = Ctl_augment_new(ori_train.loc[trn_ind],train_target.loc[trn_ind],include_test=1)\n        train_X_ = np.concatenate([train_X,aug_X],axis=0)\n        train_Y_ = np.concatenate([train_Y,aug_Y],axis=0)\n        train_dataset = MoADataset(train_X_, train_Y_)\n        train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n\n        optimizer = torch.optim.Adam(model.parameters(),betas=(0.9, 0.99), lr=1e-3, weight_decay=weight_decay,eps=1e-5)\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=lr, epochs=EPOCHS1, steps_per_epoch=len(train_data_loader))\n        \n        loss_fn = nn.BCEWithLogitsLoss()\n        loss_tr = SmoothBCEwLogits(smoothing =0.0005)\n        \n        best_valid_metric = 1e9\n        not_improve_epochs = 0\n        for epoch in range(EPOCHS1):\n            # train\n            train_loss = 0.0\n            train_num = 0\n            for data in (train_data_loader):\n                optimizer.zero_grad()\n                x,y = data['x'].to(device),data['y'].to(device)\n                outputs = model(x)\n                loss = loss_tr(outputs, y)\n                loss.backward()\n                optimizer.step()\n                scheduler.step()\n                train_num += x.shape[0]\n                train_loss += (loss.item()*x.shape[0])\n                \n            train_loss /= train_num\n            # eval\n            model.eval()\n            valid_loss = 0.0\n            valid_num = 0\n            for data in (valid_data_loader):\n                x,y = data['x'].to(device),data['y'].to(device)\n                outputs = model(x)\n                loss = loss_fn(outputs, y)\n                valid_num += x.shape[0]\n                valid_loss += (loss.item()*x.shape[0])\n            valid_loss /= valid_num\n            t_preds = []\n            for data in (test_data_loader):\n                x = data[0].to(device)\n                with torch.no_grad():\n                    outputs = model(x)\n                t_preds.extend(list(outputs.sigmoid().cpu().detach().numpy()))\n            pred_mean = np.mean(t_preds)\n            if valid_loss < best_valid_metric:\n                torch.save(model.state_dict(),'./model_dnn0005_fold%s'%fold+'_'+str(seed)+'.ckpt')\n                not_improve_epochs = 0 \n                best_valid_metric = valid_loss\n                print('[epoch %s] lr: %.6f, train_loss: %.6f, valid_metric: %.6f, pred_mean:%.6f'%(epoch,optimizer.param_groups[0]['lr'],train_loss,valid_loss,pred_mean))\n                trn_loss_.append(train_loss)\n            else:\n                not_improve_epochs += 1\n                print('[epoch %s] lr: %.6f, train_loss: %.6f, valid_metric: %.6f, pred_mean:%.6f, NIE +1 ---> %s'%(epoch,optimizer.param_groups[0]['lr'],train_loss,valid_loss,pred_mean,not_improve_epochs))\n                if not_improve_epochs >= 50:\n                    break\n            model.train()\n            if epoch!=EPOCHS1-1:\n                aug_X,aug_Y = Ctl_augment_new(ori_train.loc[trn_ind],train_target.loc[trn_ind],include_test=1)\n                train_X_ = np.concatenate([train_X,aug_X],axis=0)\n                train_Y_ = np.concatenate([train_Y,aug_Y],axis=0)\n                train_dataset = MoADataset(train_X_, train_Y_)\n                train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n\n        #model = resnetModel(len(features),len(targets),1500)\n        #model.to(device)\n        state_dict = torch.load('./model_dnn0005_fold%s'%fold+'_'+str(seed)+'.ckpt', torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") )\n        model.load_state_dict(state_dict)\n        model.eval()\n        valid_preds = []\n        for data in tqdm(valid_data_loader):\n            x,y = data['x'].to(device),data['y'].to(device)\n            with torch.no_grad():\n                outputs = model(x)\n            valid_preds.extend(list(outputs.cpu().detach().numpy()))\n        dnn_oof.loc[val_ind,targets] = 1 / (1+np.exp(-np.array(valid_preds)))\n        t_preds = []\n        for data in tqdm(test_data_loader):\n            x = data[0].to(device)\n            with torch.no_grad():\n                outputs = model(x)\n            t_preds.extend(list(outputs.sigmoid().cpu().detach().numpy()))\n        print(np.mean(t_preds))\n        \n        train_preds=[]\n        for data in (eval_train_data_loader):\n            x =  data['x'].to(device)\n            with torch.no_grad():\n                outputs = model(x)\n            train_preds.extend(list(outputs.sigmoid().cpu().detach().numpy()))\n        train_loss = Metric(eval_train_Y,train_preds)\n        eval_train_loss += train_loss\n        print('eval_train_loss:',train_loss) \n        \n        preds.append(t_preds)\n    sub[targets] = np.array(preds).mean(axis=0)\n    return dnn_oof,oof,sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cols = [col for col in train.columns if col not in ['sig_id','cp_type','cs','most_cs_ind','most_cs_same_target']]\n#train_cols = sorted(train_cols)\nlen(train_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Seed_everything(0)\n#dnn_oof,oof,sub = train_and_predict(train_cols,sub.copy(),aug=True,seed=0,lr=1/90.0/2,weight_decay=1e-5/2.7)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Seed_everything(1)\ndnn_oof,oof,sub = train_and_predict(train_cols,sub.copy(),aug=True,seed=1,lr=1/90.0/3.5*8,weight_decay=1e-6)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs = []\nfor seed in [2]:\n    Seed_everything(seed)\n    outputs.append(train_and_predict(train_cols,sub.copy(),aug=True,seed=seed,lr=1/90.0/3.5*8,weight_decay=1e-6))\n    #outputs.append(train_and_predict(train_cols,sub.copy(),aug=True,seed=seed,lr=1/90.0/2,weight_decay=1e-5/2.7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for output in outputs:\n    \n    dnn_oof[targets] += output[0][targets]\n    sub[targets] += output[2][targets]\ndnn_oof[targets] /= (1+len(outputs))\nsub[targets] /= (1+len(outputs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_metric = Metric(train_target[targets].values,dnn_oof[targets].values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_metric\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.loc[test['cp_type']=='ctl_vehicle',targets] = 0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dnn_oof.to_csv('./oof.csv',index=False)\nsub.to_csv('./submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target[targets].mean().mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub[targets].mean().mean()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dnn_oof[targets].mean().mean()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}