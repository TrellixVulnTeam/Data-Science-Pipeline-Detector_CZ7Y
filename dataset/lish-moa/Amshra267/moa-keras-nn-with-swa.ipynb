{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Few things I tried**\n\n* Using Add SWA(stochastic weights averaging between different folds in kfold), ReduceLROnPlateau, GroupNormalization\n* Uses Gelu as activation function\n* Remove cp_type = ctrl_vehicle during training\n* Averaging over six random seeds","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n# popular EDA libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom sklearn import *\nfrom sklearn.model_selection import *\nfrom sklearn.preprocessing import *\nfrom sklearn.decomposition import PCA\nimport statsmodels.api as sm\nfrom tqdm.notebook import tqdm\nimport gc\n\n# model related libraries\n\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_addons as tfa\nimport numpy as np\nfrom keras.layers import Dense, BatchNormalization, Input\nfrom keras.models import Model\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix, log_loss \nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\ntf.random.set_seed(2) # for reproducible results\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/lish-moa/train_features.csv')\ntargets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntargets_non_scored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ntest = pd.read_csv('../input/lish-moa/test_features.csv')\nsample = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\n# let's see few rows of both train and test\ntrain.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for test data\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# targets corresponding to train data\ntargets_scored.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shapes of data\n\nprint('train data shape - ',train.shape)\nprint('test data shape - ',test.shape)\nprint('Different MoA labels - ',targets_scored.shape[1]-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Description of Data\n\n* Train data contains\n    - cp_type - indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have **no MoAs(Will remove for training Model)**\n    - cp_time and cp_dose indicate treatment duration (24, 48, 72 hours) and dose (high or low)\n      \n* We have to predict the probabilities corresponding to 206 MoAs\n* Metric Used - Average Log loss","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Firstly we remove any label whose cp_type belong to ctrl_vehicle as it doesn't contain MoAs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the unique values what they are\nprint(np.unique(train['cp_type']))\n\n\n#let's see the distribution of persons in these classes\ntrain['cp_type'].value_counts().plot(kind='bar',figsize=[10,3])\ntrain['cp_type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if labels for 'ctl_vehicle' are all 0.\ntrain1 = train.merge(targets_scored, on='sig_id')\ntarget_cols = [c for c in targets_scored.columns if c not in ['sig_id']]\ncols = target_cols + ['cp_type']\ntrain1[cols].groupby('cp_type').sum().sum(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Labels for ctrl_vehicle are all 0","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# constrcut train&test data except 'cp_type'=='ctl_vehicle' data\nprint(train.shape, test.shape)\ntrain1 = train1[train1['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest1 = test[test['cp_type']!='ctl_vehicle'].reset_index(drop=True)\nprint(train1.shape, test1.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Concatenate Train and test\n\nIt is better to concatenate train and test to perform same transformation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.concat([train1, test1],sort=False, ignore_index= True)\ndataset.head()\n\n# the values corresponding to test data labels become NaNs after concatenation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying one hot encoding to the categorical_features\n\nobj_cols =  list(dataset.select_dtypes(include = 'object').columns[1:])   # not taking sig_id\nobj_cols.append('cp_time')\ndataset = pd.get_dummies(dataset, columns = obj_cols)\nobj_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice! we prepared the data let's slicing train and test back","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# firstly drop sig_id as the necessary test_id is already presented in test1\n\ndataset.drop('sig_id', axis=1, inplace = True)\n\n# slicing\ntarget_cols = targets_scored.columns[1:]  # not icluding sig_id\nfeature_cols = [c for c in dataset.columns if c not in target_cols ]\nx_train = dataset[feature_cols][0:len(train1)]  # columns other than cols\ny_train = dataset[target_cols][0:len(train1)]\nx_test = dataset[feature_cols][len(train1):]\n\nx_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODEL BUILDING","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef nn_model(input_shape):\n\n    inputs = Input(shape = input_shape)\n    x = tf.keras.layers.BatchNormalization()(inputs)\n    x = tf.keras.layers.Dense(1024, activation= tfa.activations.gelu)(inputs)\n    x = tfa.layers.GroupNormalization(groups = 32)(x)\n    x = tf.keras.layers.Dense(512, activation= tfa.activations.gelu)(x)\n    x = tfa.layers.GroupNormalization(groups = 16)(x)\n    x = tf.keras.layers.Dense(256, activation= tfa.activations.gelu)(x)\n    x = tfa.layers.GroupNormalization(groups = 8)(x)\n    outputs = tf.keras.layers.Dense(206,activation='sigmoid')(x)\n\n    # model\n    return tf.keras.models.Model(inputs,outputs)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't need to pass any metric here because the loss we are using is actually a type of average log loss","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nN_STARTS = 6\n\n\nval_pred = y_train.copy()\n\n# making an array for rows same as x_test\ntest_pred = np.zeros((x_test.shape[0],206))\n\nval_pred.loc[:, y_train.columns] = 0\n\nfor seed in tqdm(range(N_STARTS)):\n    for n, (tr, tv) in enumerate(KFold(n_splits=7, random_state=seed, shuffle=True).split(y_train)):\n        print(f'Fold {n}')\n        \n        \n        model = nn_model(len(x_train.columns))\n        \n        # using Stochastic Weight averaging\n        model.compile(optimizer=tfa.optimizers.SWA(tf.optimizers.Adam(lr = 0.001), start_averaging = 9, average_period = 6),\n                      loss='binary_crossentropy', metrics = None )\n        \n        # Callbacks\n        \n        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n        # for saving best weights after each\n        file_path = str(n) + \"weights.best.hdf5\"\n        \n        \n        # stochastic weight averaging uses average model checkpoint\n        avg_checkpoint = tfa.callbacks.AverageModelCheckpoint(filepath= file_path, monitor='val_loss', save_best_only=True,verbose=2,update_weights=True,mode='min')\n        \n        # early stoping\n        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience= 5)\n        \n        history = model.fit(x_train.values[tr],\n                  y_train.values[tr],\n                  validation_data=(x_train.values[tv], y_train.values[tv]),\n                  epochs=20, batch_size=64,\n                  callbacks=[reduce_lr_loss, avg_checkpoint, early]\n                 )\n        \n        \n        # loading best weights for prediction\n        model.load_weights(file_path)\n\n        test_predict = model.predict(x_test.values)\n        val_predict = model.predict(x_train.values[tv])\n        \n        test_pred += test_predict\n        val_pred.loc[tv, y_train.columns] += val_predict\n        print('')\n        \n        \ntest_pred /= ((n+1) * N_STARTS)\nval_pred.loc[:, y_train.columns] /= N_STARTS        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction on Test Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The predictions we are making on x_test is only for those when 'cp_type'= !'ctl_vehicle'\nwe need to add those in the original data making all other 0","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# making predictions on test data\n# firstly making a zero array of test data label shape\n\npred_array = np.zeros((test.shape[0],sample.shape[1]-1))\n\n# Replacing those rows\n# where 'cp_type'= !'ctl_vehicle'\npred_rows = [ c for c in sample.sig_id.values if c in test1.sig_id.values]\n\n# collecting indexes of these rows\nindex_pred =  sample[sample.sig_id.isin(pred_rows)][sample.columns[1:]].index\n\n\n# and now for other rows replace it with pred\n\nc = 0\nfor i in list(index_pred):\n    pred_array[i,:] = test_pred[c,:]\n    c +=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submitting file\n\nsample[sample.columns[1:]] = pred_array\n\n\nsample.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you find this notebook helpful, feel free to **Upvote**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}