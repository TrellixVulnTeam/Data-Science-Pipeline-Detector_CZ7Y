{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport random\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_names = ['train_features', 'train_targets_scored', 'train_targets_nonscored', 'test_features', 'sample_submission']\n\ndf = {}\nfor name in df_names:\n    df[name] = pd.read_csv(f\"../input/lish-moa/{name}.csv\", index_col=0)\n    print(f\"{name}: {df[name].shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full = pd.concat([df['train_features'], df['train_targets_scored'].add_prefix('scored_'), df['train_targets_nonscored'].add_prefix('nonscored_')], axis=1)\ndf_full['count_scored'] = df_full.filter(regex='^scored_').sum(axis=1)\ndf_full['count_nonscored'] = df_full.filter(regex='^nonscored_').sum(axis=1)\ndf_full['count_total'] = df_full['count_scored'] + df_full['count_nonscored']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_target_cols = list(df_full.filter(regex='^(scored|nonscored)_').columns)\nprint(\"length of total targets, scored and non scored:\", len(total_target_cols))\ndf_target_combine = df_full[[\"cp_type\"]+total_target_cols].groupby(total_target_cols).size().reset_index(name='Combination_Count').sort_values('Combination_Count', ascending=False).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_target_combine['Combination_Label'] = df_target_combine.index\ndf_target_combine = df_target_combine[['Combination_Label', 'Combination_Count'] + total_target_cols]\ndf_target_combine.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transition = df_target_combine[['Combination_Label']+total_target_cols].set_index('Combination_Label')\ntransition.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full_combine = pd.merge(df_full, df_target_combine, on=total_target_cols, how='left')\ndf_full_combine.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full_trt = df_full_combine[df_full_combine.cp_type !='ctl_vehicle']\ndf_train = df_full_trt.drop(columns= total_target_cols+['count_scored',\n       'count_nonscored', 'count_total', \n       'Combination_Count'] ).reset_index(drop = True).drop('cp_type', axis = 1)\n\n#df_test = df['test_features'][df['test_features']['cp_type']!='ctl_vehicle'].reset_index(drop = True).drop('cp_type', axis = 1)\ndf_test = df['test_features'].drop('cp_type', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combination_label_target = df_full_trt.Combination_Label.reset_index(drop = True)\ndf_target_OHE = pd.get_dummies(combination_label_target, prefix='combo')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_target_OHE.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CV fold"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=5, random_state=2020, shuffle= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = df_train.copy()\n\nfor f, (train_index, val_index)  in enumerate(skf.split(X= df_train, y=combination_label_target)):\n    folds.loc[val_index, 'kfold'] = int(f)\n    \nfolds['kfold'] =folds['kfold'].astype(int)\nfolds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DataSet Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n    #the __len__ and __getitem__ are for torch.utils.data.DataLoader to load batches into neural networks.    \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    # this is to tell the model that it is in train mode, thus use batch normalization and dropout.\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        \n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    #model.eval() is to disable batch normalization and dropout.\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n        \n        #this saves memory and accelerate the running time as we don't need to keep memory for the gradients.\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n   \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.3)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.2)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(data):\n    \n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = [c for c in process_data(folds).columns if c!='Combination_Label']\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\ntarget_cols = [c for c in df_target_OHE.columns]\nprint(len(feature_cols), len(target_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=2020)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 5\nEARLY_STOPPING_STEPS = 20\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=df_target_OHE.shape[1]\nhidden_size=1024","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Single_fold_training"},{"metadata":{"trusted":true},"cell_type":"code","source":"#need to modify the dataframe names\ndef run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(df_test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    train_target = df_target_OHE.iloc[trn_idx,:]\n    val_target = df_target_OHE.iloc[val_idx,:]\n    \n    #x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    #x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n\n    x_train, y_train = train_df[feature_cols].values, train_target.values\n    x_valid, y_valid =  valid_df[feature_cols].values,val_target.values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    #optimizer = torch.optim.RMsprop(model.parameters(), lr=LEARNING_RATE, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n    \n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(df_train), df_target_OHE.shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_fn, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \"\"\"\"\n    as a first step we train the combination targets and\n    just put the combination back using transition matrix of size (696* 608)\n    \n    Second step would be to use transition targets\n    \"\"\"\n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    #predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = np.zeros((len(test_), df_target_OHE.shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(df_train), df_target_OHE.shape[1]))\n    predictions = np.zeros((len(df_test), df_target_OHE.shape[1] ))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = [42,2020,2,27]\n#SEED = [2]\noof = np.zeros((len(df_train),df_target_OHE.shape[1] ))\npredictions = np.zeros((len(df_test), df_target_OHE.shape[1]))\n\nfor seed in SEED:\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n\n#train[target_cols] = oof\n#test[target_cols] = predictions\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalize probabilities\nfor i in range(oof.shape[0]):\n    oof[i,:] = oof[i,:]/np.sum(oof[i,:])\n    \noof_original_target = np.matmul(oof, transition.to_numpy() )\nprediction_original_target = np.matmul(predictions,transition.to_numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate train log loss\ntrain_log_loss = 0\nnum_of_scored_targets = df['train_targets_scored'].shape[1]\n\ntrt_index = df_full_combine[df_full_combine.cp_type !='ctl_vehicle'].index\nfor i in range(num_of_scored_targets):\n    y_true = df['train_targets_scored'].iloc[trt_index,i].values\n    y_pred = oof_original_target[:,i]\n    train_log_loss += log_loss(y_true, y_pred)/num_of_scored_targets\ntrain_log_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_original_target.shape\nscored_target_cols = df['sample_submission'].columns\nsub = df['sample_submission'].copy()\nsub[scored_target_cols] = prediction_original_target[:,:len(scored_target_cols)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['test_features'].reset_index(drop = True, inplace = True)\ntest_ctrl_index = df['test_features'][df['test_features'].cp_type == 'ctl_vehicle'].index\n\nsub.iloc[test_ctrl_index,:] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prob_clip(df, floor, ceiling):\n    for c in range(df.shape[1]):\n        df.iloc[:,c] = np.where(df.iloc[:,c]< bar, bar, df.iloc[:,c])\n        df.iloc[:,c] = np.where(df.iloc[:,c] > ceiling, ceiling, df.iloc[:,c])\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_clipped = prob_clip(sub, 0.00015, 0.995)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_clipped.to_csv('submission.csv',index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}