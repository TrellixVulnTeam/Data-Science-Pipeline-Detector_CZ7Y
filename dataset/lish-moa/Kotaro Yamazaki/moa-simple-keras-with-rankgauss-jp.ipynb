{"cells":[{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 概要\n\nKerasを使うのは初めてで自分用のメモとして公開します\n\n\n1. カテゴリ変数の削除\nカテゴリ関数ctl_vehicleは常に目的変数0となることが知られているので学習に不要 Reason: notebook\n\n2. Keras を使った単純な深層学習\n・　BatchNormalization とDropOut層\n・　アーリーストッピング\n・　学習率のスケジュール\n・　32バッチサイズ\nhttps://arxiv.org/abs/1804.07612\n↑バッチサイズは32くらいがいいよって論文\n→バッチサイズをおおきくした方が良い結果が出る\n\n\n3. KN-FOLD\nAdd Data からデータセットを加え、MultilabelStratifiedKFold のインポート\n\n4. RankGauss\nRankGauss で前処理\n\n\n### 参考資料\n- https://www.kaggle.com/simakov/keras-multilabel-neural-network-v1-2\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.preprocessing import QuantileTransformer\nimport tensorflow_addons as tfa\nfrom sklearn.metrics import log_loss\n\nimport sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.read_csv('../input/lish-moa/train_features.csv')\ny_train = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\nX_test = pd.read_csv('../input/lish-moa/test_features.csv')\n\nss = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RankGauss\n# 数値変数を順位に変換したあと順位を保ったまま半ば無理やり正規分布になるように変換する手法\ndef rank_gauss(df):\n    for col in df.columns:\n        transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')\n        vec_len = len(df[col].values)\n        raw_vec = df[col].values.reshape(vec_len, 1)\n        transformer.fit(raw_vec)\n\n        # 変換後のデータで各列を置換\n        df[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    df = df.copy()\n    # カテゴリ変数を数値変換\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    \n    df = rank_gauss(df)\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"カテゴリ関数ctl_vehicleは常に目的変数0となることが知られているので学習に不要\nReason: [notebook](https://www.kaggle.com/demetrypascal/t-test-pca-rfe-logistic-regression)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# cp_type == 0 のみ利用\n# あわせてyも同じように落とす\ny_train = y_train.loc[X_train['cp_type']=='trt_cp'].reset_index(drop=True)\nX_train = X_train.loc[X_train['cp_type']=='trt_cp'].reset_index(drop=True)\n\ntrain = preprocess(X_train)\ntest = preprocess(X_test)\ndel y_train['sig_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(num_columns):\n    model = keras.models.Sequential([\n        keras.layers.Input(num_columns),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dropout(rate=0.2),\n        tfa.layers.WeightNormalization(keras.layers.Dense(2048, activation='relu')),\n        \n        keras.layers.BatchNormalization(),\n        keras.layers.Dropout(rate=0.2),\n        tfa.layers.WeightNormalization(keras.layers.Dense(1024, activation='relu')),\n        \n        keras.layers.BatchNormalization(),\n        keras.layers.Dropout(rate=0.5),\n        tfa.layers.WeightNormalization(keras.layers.Dense(512, activation='relu')),                            \n        \n        # == final layer == \n         keras.layers.BatchNormalization(),\n         keras.layers.Dropout(rate=0.5),\n         tfa.layers.WeightNormalization(keras.layers.Dense(206, activation='sigmoid'))\n    ])\n    model.compile(\n        optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(),sync_period=10),\n        loss='binary_crossentropy'\n    )\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_feats = [i for i in range(train.shape[1])]\n# len(train.columns) と同じ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 現在のエポックを引数として学習率を返す関数を定義\n# それまでの学習散るに0.1 ** (epoch/20)をかけていて学習率は指数関数的な減衰をする\n# keras.optimizer.schedules を使う方法もある\n\ndef exponential_delay_fn(epoch):\n    return 0.01 * 0.1 ** (epoch/20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_STARTS = 7\nK_FOLD = 7\nBATCH_NUM = 128\nEPOCH_NUM = 35\ntf.random.set_seed(1)\nss.loc[:, y_train.columns] = 0\nres = y_train.copy()\nres.loc[:, y_train.columns] = 0\n\nhistorys = dict()\n\n# N_STARS * KFOLD　回学習する\n# MultilabelStratifielsKFold はランダムシャッフルをしてくれるgroup-k-fold のようなライブラリ\nfor seed in range(N_STARTS):\n    for n, (tr, te) in enumerate(\n        MultilabelStratifiedKFold(n_splits=K_FOLD,\n                                  random_state=seed, shuffle=True).split(train, train)):\n        print(f\"--{train.values[tr].shape}--{train.values[te].shape}--\")\n        print(f\"Seed: {seed}, Fold: {n}\")\n        \n        # モデル作成\n        model = create_model(len(top_feats))\n        \n        # === コールバック関数の設定 ===\n        checkpoint_path = f'repeated:{seed}_Fold:{n}.hdf5'\n        \n        # スケジュール関数を引数としてLearningRateScheduler コールバックを作りそのコールバックをfit()メソッドに渡す\n        # model.fit(...., callback=[exponential_delay_fn(epoch)])のように使う\n\n        lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_delay_fn)\n\n        # ベストなモデルをcheckpoint_path に保存しておいてくれる設定\n        checkpoint_cb = keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True)\n        # early_stopping のコールバック関数\n        # val_loss を監視\n        # modeは上書きするときの設定、基本的にautoにしておけばOK\n        early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n        \n        # ===========================\n        \n        history = model.fit(train.values[tr][:, top_feats],\n                  y_train.values[tr],\n                  validation_data=(train.values[te][:, top_feats], y_train.values[te]),\n                  epochs=EPOCH_NUM, \n                  batch_size=BATCH_NUM, \n                  callbacks=[checkpoint_cb, early_stopping_cb, lr_scheduler], \n                  verbose=2 #　エポックごとに1行のログを出力\n                 )\n        \n        historys[f'history_{seed+1}'] = history\n        \n        model.load_weights(checkpoint_path)\n        test_predict = model.predict(test.values[:, top_feats])\n        \n        # 自己評価用にvalidationモデルに対する予測も保存しておく\n        val_predict = model.predict(train.values[te][:, top_feats]) \n        \n        ss.loc[:, y_train.columns] += test_predict\n        res.loc[te, y_train.columns] += val_predict\n        print('')\n\n# 最終的に足し合わされているので試行回数分の平均を取る\nss.loc[:, y_train.columns] /= ((n+1) * N_STARTS)\n# val についてはN_STARTS分\nres.loc[:, y_train.columns] /= N_STARTS\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show Model loss in plots\n#　訓練の可視化\n# https://keras.io/ja/visualization/\n\nfor k,v in historys.items():\n    loss = []\n    val_loss = []\n    loss.append(v.history['loss'][:40])\n    val_loss.append(v.history['val_loss'][:40])\n    \nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15, 6))\nplt.plot(np.mean(loss, axis=0))\nplt.plot(np.mean(val_loss, axis=0))\nplt.yscale('log')\nplt.yticks(ticks=[1,1e-1,1e-2])\nplt.xlabel('Epochs')\nplt.ylabel('Average Logloss')\nplt.legend(['Training','Validation'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def metric(y_true, y_pred):\n    metrics = []\n    for _target in y_train.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float)))\n    return np.mean(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# OOF (Out of Fold)とは、k-Fold などでデータを分割した際に学習に使わなかったデータを指す\n# OOF に対してlog_loss の平均を出力\nprint(f'OOF Metric :{metric(y_train, res)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 予測時には省いていたcp_type=1 のカラムは強制的に0を代入する\n# でももともと全部0っぽい\nss.loc[test['cp_type']==1, y_train.columns] = 0\nss.to_csv('submission1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del historys","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBOOST\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom category_encoders import CountEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.multioutput import MultiOutputClassifier\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.read_csv('../input/lish-moa/train_features.csv')\ny_train = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\nX_test = pd.read_csv('../input/lish-moa/test_features.csv')\n\n# cp_type == 0 のみ利用\n# あわせてyも同じように落とす\ny_train = y_train.loc[X_train['cp_type']=='trt_cp'].reset_index(drop=True)\nX_train = X_train.loc[X_train['cp_type']=='trt_cp'].reset_index(drop=True)\n\ntrain = preprocess(X_train)\ntest = preprocess(X_test)\ndel y_train['sig_id']\n\n# drop id col\nX = train.to_numpy()\nX_test = test.to_numpy()\ny = y_train.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclassifier = MultiOutputClassifier(XGBClassifier(tree_method='gpu_hist'))\n\nclf = Pipeline([('encode', CountEncoder(cols=[0, 2])),\n                ('classify', classifier)\n               ])\n\nparams = {'classify__estimator__colsample_bytree': 0.6522,\n          'classify__estimator__gamma': 3.6975,\n          'classify__estimator__learning_rate': 0.0503,\n          'classify__estimator__max_delta_step': 2.0706,\n          'classify__estimator__max_depth': 10,\n          'classify__estimator__min_child_weight': 31.5800,\n          'classify__estimator__n_estimators': 166,\n          'classify__estimator__subsample': 0.8639\n         }\n\n_ = clf.set_params(**params)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"control_mask = train['cp_type']=='ctl_vehicle'\noof_preds[control_mask] = 0\n\nprint('OOF log loss: ', log_loss(np.ravel(y), np.ravel(oof_preds)))"},{"metadata":{"trusted":true},"cell_type":"code","source":"N = 7\noof_preds = np.zeros(y.shape)\ntest_preds = np.zeros((test.shape[0], y.shape[1]))\noof_losses = []\nkf = KFold(n_splits=N)\nfor fn, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n    print('Starting fold: ', fn)\n    X_train, X_val = X[trn_idx], X[val_idx]\n    y_train, y_val = y[trn_idx], y[val_idx]\n\n    \n    clf.fit(X_train, y_train)\n    val_preds = clf.predict_proba(X_val) # list of preds per class\n    val_preds = np.array(val_preds)[:,:,1].T # take the positive class\n    oof_preds[val_idx] = val_preds\n    \n    loss = log_loss(np.ravel(y_val), np.ravel(val_preds))\n    oof_losses.append(loss)\n    preds = clf.predict_proba(X_test)\n    preds = np.array(preds)[:,:,1].T # take the positive class\n    test_preds += preds / N\n    \nprint(oof_losses)\nprint('Mean OOF loss across folds', np.mean(oof_losses))\nprint('STD OOF loss across folds', np.std(oof_losses))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set control train preds to 0\ncontrol_mask = train['cp_type']=='ctl_vehicle'\noof_preds[control_mask] = 0\n\nprint('OOF log loss: ', log_loss(np.ravel(y), np.ravel(oof_preds)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the submission file\nsub.iloc[:,1:] = test_preds\nsub.to_csv('submission2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensumble"},{"metadata":{"trusted":true},"cell_type":"code","source":"stack = (oof_preds + res)/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stack.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stacked_test_pred = np.columns_stack(test_preds, ss)\n# meta_model_pred = meta_model.predict(stacked_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# meta_model_pred.to_csv('submission_stacked,csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assert(len(meta_model_pred) == len(ss))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}