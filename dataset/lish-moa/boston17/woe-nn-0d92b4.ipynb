{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.decomposition import PCA\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_dtypes = {\"cp_type\": \"category\",\"cp_dose\": \"category\"}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv',dtype = train_features_dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features['train'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features = pd.read_csv('/kaggle/input/lish-moa/test_features.csv',dtype = train_features_dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features['train'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = pd.concat([train_features,test_features],axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_features,test_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col, col_dtype in train_features_dtypes.items():\n    if col_dtype == \"category\":\n        temp[col] = temp[col].cat.codes.astype(\"int16\")\n        temp[col] -= temp[col].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = temp[temp['train'] == 1].copy()\ntest_features = temp[temp['train'] == 0].copy()\ndel temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = train_features.drop(columns='train')\ntest_features = test_features.drop(columns='train')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features[['cp_type']].info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(train_targets_scored.columns)[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.merge(train_features,train_targets_scored,how='inner',on='sig_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = sample_submission.copy()\nfor col in submission.columns[1:]:\n    submission[col].values[:] = 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cols = list(train_features.drop('sig_id',axis=1).columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cols[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_cols = list(train_targets_scored.drop('sig_id',axis=1).columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"stack = pd.concat([train_features,test_features],axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stack.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\ndef fe_woe(y):\n    \n    new_X_cols = X_cols.copy()\n    df1 = df.copy()\n    test_features1 = test_features.copy()\n    for i in X_cols[3:]:  #['c-98'] \n        print(i)\n        quack, bins =  pd.qcut(stack[i], 10, retbins=True, duplicates='drop')\n        df1[i + '_cut'] = pd.cut(df1[i], bins=bins, labels=False, include_lowest = True)\n        woe = df1.groupby([i + '_cut'])[y].agg(['count','sum']).reset_index()\n        woe['non_event'] = woe['count'] - woe['sum']\n        woe['pct_event'] = woe['sum'] / woe['sum'].sum() \n        woe['pct_non_event'] = np.where(woe['non_event'].sum() == 0, 0, woe['non_event'] / woe['non_event'].sum())\n        woe[i + '_woe'] =  np.where((woe['non_event'].sum() == 0), 0, np.log((woe['pct_event'] + 0.5) / (woe['pct_non_event'] + 0.5)))\n        woe[i + '_iv'] =  (woe['pct_event'] - woe['pct_non_event']) * woe[i + '_woe']\n        iv =woe[i + '_iv'].sum()\n        #print(iv)\n        if iv >= 0.15:\n            try_1 = woe[[i + '_cut',i + '_woe']].to_dict()\n            #df = df.merge(woe[[i + '_cut',i + '_woe']],how='left',on=[i + '_cut'])\n            df1[i + '_woe'] = df1[i + '_cut'].map(try_1[i + '_woe'])\n            df1 = df1.drop(i + '_cut',axis=1)\n            test_features1[i + '_cut'] = pd.cut(test_features1[i], bins=bins, labels=False, include_lowest = True)  #\n            #test_features = test_features.merge(woe[[i + '_cut',i + '_woe']],how='left',on=[i + '_cut'])\n            test_features1[i + '_woe'] = test_features1[i + '_cut'].map(try_1[i + '_woe'])\n            test_features1 = test_features1.drop(i + '_cut',axis=1)\n            new_X_cols = new_X_cols + [i + '_woe']\n        else:\n            1==1\n            \n    #scaler = StandardScaler()\n    #df1 = scaler.fit_transform(df1[new_X_cols])\n    df1 = scale(df1[new_X_cols],new_X_cols)\n    # X_test_scaler = scaler.transform(X_test)\n    #test_features1 = scaler.transform(test_features1[new_X_cols]) \n    test_features1 = scale(test_features1[new_X_cols],new_X_cols)\n    \n    return df1, test_features1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale(a1,cols):\n    scaler = StandardScaler()\n\n    a2 = scaler.fit_transform(a1)\n    a3 = pd.DataFrame(a2)\n    a3.columns = cols\n\n    return a3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model"},{"metadata":{},"cell_type":"markdown","source":"The core data structures of Keras are layers and models.\nThe Keras functional API is a way to create models that are more flexible than the tf.keras.Sequential API. The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs.\n\nThe main idea is that a deep learning model is usually a directed acyclic graph (DAG) of layers. So the functional API is a way to build graphs of layers.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow.keras.backend as K\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.losses import BinaryCrossentropy\nimport tensorflow_addons as tfa\ndef create_model(num_columns, actv='relu'):\n    model = tf.keras.Sequential([tf.keras.layers.Input(num_columns)])\n                \n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.2))\n    model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(1024, activation=actv)))\n    \n    if actv == 'elu':\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.AlphaDropout(0.2))\n        model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(512, kernel_initializer='lecun_normal', activation='selu')))\n    else:\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dropout(0.2))\n        model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(1024, activation=actv))) \n\n    #============ Final Layer =================\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation=\"sigmoid\")))\n    \n    model.compile(optimizer=tfa.optimizers.AdamW(lr = 1e-3, weight_decay = 1e-5, clipvalue = 756), \n                  loss=BinaryCrossentropy(label_smoothing=1e-15),\n                  )\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nfolds=5\n\n# prepare split\nkf = KFold(n_splits = nfolds)\n\n# base model definition throught sklearn Pipeline\npca = PCA(n_components = 300)\n# svm0 = SVC(C = 0.1,probability =True)\n\n\n# base_model = Pipeline(steps=[('pca', pca), ('nn', model)])\n\n#mo_base = MultiOutputClassifier(model, n_jobs=-1) #base_model\nxtrain = df[X_cols]#.head(1000)  set to small value for testing code\nytrain = df[y_cols]#.head(1000) set to small value for testing code\n\nxtest = test_features[X_cols]\n\n#standard scaler\nxtrain = scale(xtrain,X_cols)\nxtest = scale(xtest,X_cols)\n\n# storage matrices for OOF / test predictions\nprval = np.zeros(ytrain.shape)\n#kfold cv \nz = pd.DataFrame()\nfor (ff, (id0, id1)) in enumerate(kf.split(xtrain)):\n     \n    x0, x1 = xtrain.loc[id0], xtrain.loc[id1]\n    y0, y1 = np.array(ytrain.loc[id0]), np.array(ytrain.loc[id1])\n    \n    # fix for empty columns\n    check_for_empty_cols = np.where(y0.sum(axis = 0) == 0)[0]\n    if len(check_for_empty_cols):\n        y0[0,check_for_empty_cols] = 1\n    \n    model1 = create_model(len([i for i in range(x0.shape[1])]))\n    # fit model\n    model1.fit(x0,y0) #fails here\n    \n    # predicitons\n    prv = model1.predict_proba(x1)  #[:, 1] see note below, this does not appear to work on a multioutput scenario\n    prf = model1.predict_proba(xtest)  #[:, 1]\n    \n    z = pd.concat([z,pd.DataFrame(prv)],axis=0)\n    \n    submission[y_cols] += prf / nfolds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# keras.utils.plot_model(model1, \"my_first_model.png\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xtrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len([i for i in range(x0.shape[1])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x0.shape[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_loss_metric_ind(y_true, y_pred):\n    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    loss = - np.mean(np.mean(y_true * np.log(y_pred_clip) + (1 - y_true) * np.log(1 - y_pred_clip)))\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prval_df = pd.DataFrame(z)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prval_df.columns = y_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#highlight the worst performing models\nperf_check = []\nfor i in y_cols:\n    perf_check.append((i,log_loss_metric_ind(ytrain[i], prval_df[i])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame(perf_check)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nresults.columns = ['target','log_loss']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#worst performing models\nresults.sort_values('log_loss',ascending=False).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results['log_loss'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.sort_values('log_loss',ascending=False).head(10)['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# #from sklearn.model_selection import StratifiedKFold\n\n\n# #clf = make_pipeline(StandardScaler(), SVC(kernel='rbf', probability =True))\n# #clf = SVC(kernel='rbf',C= 0.001,  probability =True) # Platt calibration\n# z1 = pd.DataFrame()\n# for y in results.sort_values('log_loss',ascending=False).head(10)['target'] : #y_cols ['5-alpha_reductase_inhibitor','11-beta-hsd1_inhibitor']\n#     df1, test_features1 = fe_woe(y)\n#     #df1, test_features1 = scale()\n#     print(y)\n#     print(\"step1 passed\")\n\n#     for (ff, (id0, id1)) in enumerate(kf.split(df1)):\n\n#         x0, x1 = df1.loc[id0], df1.loc[id1]\n#         y0, y1 = np.array(ytrain.loc[id0]), np.array(ytrain.loc[id1])\n\n#         # fix for empty columns\n#         check_for_empty_cols = np.where(y0.sum(axis = 0) == 0)[0]\n#         if len(check_for_empty_cols):\n#             y0[0,check_for_empty_cols] = 1\n\n\n#         print(\"step2 passed\")\n\n#         #clf.fit(X_train, y_train)\n#         #pred = clf.predict_proba(test_features1)\n#         #submission[y] += pred[:,1] / splits\n#         model2 = create_model(len([i for i in range(x0.shape[1])]))\n#         print(\"step3 passed\")\n#         model2.fit(x0,y0) \n#         print(\"step4 passed\")\n#         pred = model2.predict_proba(test_features1)\n#         prv1 = model2.predict_proba(x1)  #[:, 1]\n#         z1 = pd.concat([z1,pd.DataFrame(prv1)],axis=0)\n#         print(\"step5 passed\")\n#         submission[y] = 0\n#         print(\"step6 passed\")\n#         submission[y] += np.array(pd.DataFrame(pred)[ytrain.columns.get_loc(y)]) / nfolds #[:,1]\n#         print(\"step7 passed\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[i].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#test_export = test.loc[:, ['id', 'target']]\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}