{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Mechanism of Actions Prediction using Autoencoders and Multi-Layer Perceptron\n\nIn this notebook, we will predict the mechanisms of action of different drugs using the data provided by lish. \n\nWe shall use PCA to compress and extract more predicatbility from our features and categorical embeddings for categorical variables.\n\nWe will first be building an autoencoder for encoding the information available for our features into a feature vector. On top of these embeddings, we will build a multi-label classifier which will basically be an MLP model. So, let's get started.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!export CUDA_LAUNCH_BLOCKING=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import required libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport random\n\n# For building multi-layer perceptron\nimport torch\nimport torch.nn as nn\nimport torch.nn.utils.weight_norm as wnrm\nimport torch.nn.functional as F\n\n# For preprocessing & manipulating data\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\n# For plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use(\"fivethirtyeight\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nprint(\"Imported all necessary libraries.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read the data \nbase_path = \"/kaggle/input/lish-moa/\"\nread_data = lambda x: pd.read_csv(f\"{base_path}/{x}\")\n\ntrain_features = read_data(\"train_features.csv\")\ntest_features  = read_data(\"test_features.csv\")\ntrain_targets  = read_data(\"train_targets_scored.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets.head(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We do not need the id values for analysis, so let's store them and drop them from the main dataframe. \n\nPS: Please verify that the target and features are in the same order in both train_features and train_targets datasets before dropping them off."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ids = train_features.sig_id.copy()\ntrain_target_ids = train_targets.sig_id.copy()\ntest_ids = test_features.sig_id.copy()\n\nassert list(train_ids) == list(train_target_ids)\n\ntrain_features = train_features.drop(columns = [\"sig_id\"])\ntrain_targets = train_targets.drop(columns = [\"sig_id\"])\ntest_features = test_features.drop(columns = [\"sig_id\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have three categorical variables cp_type, cp_time and cp_dose. Let's look at each of them in more detail."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train_features.cp_type);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are two groups of people, the treatment group and the control group. The control group is generally kept aside and nothing substantial should happen in this group, let's see the targets of the control group entries."},{"metadata":{"trusted":true},"cell_type":"code","source":"control_group = list(train_features[train_features.cp_type == \"ctl_vehicle\"].index)\ncontrol_group_targets = train_targets.iloc[control_group, :]\nnp.any(control_group_targets.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As presumed, none of the target entries of the control group evaluate to 1, which means this group is simply not informative for our analysis; we can get rid of this column.\n\nIn the test set, wherever we have `cp_type` category as `ctl_vehicle` we can manually override the predictions for all MoAs to be zeros. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = train_features[train_features.cp_type != \"ctl_vehicle\"]\ntrain_targets = train_targets.iloc[list(train_features.index), :]\n\ntrain_features.reset_index(inplace = True, drop = True)\ntrain_targets.reset_index(inplace = True, drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = train_features.drop(columns = [\"cp_type\"])\ntest_cp_types = test_features.cp_type.copy()\ntest_features = test_features.drop(columns = [\"cp_type\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train_features.cp_dose);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like equal proportions of people were assigned doses of strength `D1` and `D2` respectively. Let's label encode them."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features[\"cp_dose\"] = np.where(train_features.cp_dose == \"D1\", 1, 0)\ntest_features[\"cp_dose\"]  = np.where(test_features.cp_dose == \"D1\", 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train_features.cp_time);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar to cp_dose, it looks like an equal proportion of people who were administered the doses were monitored for 24, 48 and 72 hours respectively. Let's label encode these as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"cp_time_enc = LabelEncoder()\ntrain_features[\"cp_time\"] = cp_time_enc.fit_transform(train_features.cp_time)\ntest_features[\"cp_time\"] = cp_time_enc.transform(test_features.cp_time)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot a random subset of all the columns in genetype and have a look at their distributions in the trainset."},{"metadata":{"trusted":true},"cell_type":"code","source":"gene_cols = [x for x in list(train_features.columns) if \"g-\" in x]\nrandom.seed(10)\ngene_cols = random.sample(gene_cols, 10)\n\nfig, axes_ = plt.subplots(2, 5, figsize = (15, 6), sharey=True)\nfor idx, col in enumerate(gene_cols):\n    \n    r, c = idx // 5, idx % 5\n    sns.distplot(train_features[col], ax = axes_[r][c])\n    axes_[r][c].set_title(col, fontsize = 24)\n\nfig.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like most of the attributes are normally distributed with a mean of zero with a spread of values between (-10, 10) respectively. Let's look at the columns particular to cell type."},{"metadata":{"trusted":true},"cell_type":"code","source":"gene_cols = [x for x in list(train_features.columns) if \"c-\" in x]\nrandom.seed(10)\ngene_cols = random.sample(gene_cols, 10)\n\nfig, axes_ = plt.subplots(2, 5, figsize = (15, 6), sharey=True)\nfor idx, col in enumerate(gene_cols):\n    \n    r, c = idx // 5, idx % 5\n    sns.distplot(train_features[col], ax = axes_[r][c])\n    axes_[r][c].set_title(col, fontsize = 24)\n\nfig.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These features seem to have a bimodal distribution with 0 and -10 being the two potential modes for most of the features with 0 being the primary mode."},{"metadata":{},"cell_type":"markdown","source":"Since most of the numerical features are already constrained within comparatively close ranges, we will try working with them without worrying about any covariate shifts that might occur (as the effect will be minimal)."},{"metadata":{},"cell_type":"markdown","source":"Let's split the data into train and validation sets. This will help us check the veracity of our model on data it hasn't encountered during training."},{"metadata":{"trusted":true},"cell_type":"code","source":"trX, vlX, trY, vlY = train_test_split(train_features, train_targets, random_state = 2, test_size = .2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reset the indices after creating the split\ng = lambda x: x.reset_index(drop = True)\ntrX, vlX, trY, vlY = map(g, [trX, vlX, trY, vlY])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a dataset class to feed the data to pytorch models"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = list(train_features.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use PCA for compressing and extracting relevant information from all the continuous features."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_comps = 650\npca_object = PCA(n_components = n_comps)\nprint(\"Fitting PCA object\")\npca_object.fit(trX[num_cols]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2, 1, figsize = (20, 10))\nprint(\"First twenty components and the percent of variance explained by them.\")\nprint(np.cumsum(pca_object.explained_variance_ratio_)[:20])\nsns.lineplot(y = pca_object.explained_variance_ratio_, x = range(n_comps), ax = ax[0])\nax[0].set_title(\"% of explained variance using PCA on train features\", fontsize = 25)\n\nsns.lineplot(y = np.cumsum(pca_object.explained_variance_ratio_), x = range(n_comps), ax = ax[1])\nax[1].set_title(\"% of explained variance cumulative using PCA on train features\", fontsize = 25)\nfig.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that as expected, there's the first two components which account for most of the explained variance and others marginally contribute to the variance explained. But since we have a gpu at our disposal, we shall choose to keep all the components as they won't harm the predictability in any way..."},{"metadata":{},"cell_type":"markdown","source":"We can also use umap which is another compression technique in order to extract predictability out of our data. Let's try that as well. Let's use the same number of components as the PCA object to fit our umap object also."},{"metadata":{"trusted":true},"cell_type":"code","source":"# umap_object = umap.UMAP(n_components = n_comps, n_neighbors = 50)\n# print(\"Fitting UMAP object\")\n# umap_object.fit(trX[num_cols]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Extracting pca components from train, validation and test datasets.\")\n\ng = lambda x: pd.DataFrame(pca_object.transform(x[num_cols]), columns = [f\"pca_{i}\" for i in range(n_comps)])\n# h = lambda x: pd.DataFrame(umap_object.transform(x[num_cols]), columns = [f\"umap_{i}\" for i in range(n_comps)])\n\ntrX_pca = g(trX)\nvlX_pca = g(vlX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Concatenating pca components to train, validation and test datasets.\")\nj = lambda x, y, z: pd.concat([x, y, z])\n\ntrX = pd.concat([trX, trX_pca], axis = 1)\nvlX = pd.concat([vlX, vlX_pca], axis = 1)\n\nnum_cols = num_cols + [f\"pca_{i}\" for i in range(n_comps)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SS = StandardScaler()\ntrX = pd.DataFrame(SS.fit_transform(trX[num_cols].values), columns = num_cols)\nvlX = pd.DataFrame(SS.transform(vlX[num_cols].values), columns = num_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For autoencoders, we need to introduce some noise in the dataset from which it needs to learn to denoise the input."},{"metadata":{"trusted":true},"cell_type":"code","source":"noise_ = np.random.normal(0, 0.02, trX.shape)\ntrX_noised = trX.values + noise_\n\nvl_noise = np.random.normal(0, 0.02, vlX.shape)\nvlX_noised = vlX.values + vl_noise","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Denoising Auto Encoder for feature compression"},{"metadata":{"trusted":true},"cell_type":"code","source":"class moaAutoEncoderDataset(torch.utils.data.Dataset):\n    \"\"\"\n    A dataset class to load the data for feeding it to pytorch models during training\n    \"\"\"\n    def __init__(self, X, X_N):\n        self.X = X.copy().astype(np.float32) # Output\n        self.X_noise = X_N.copy().astype(np.float32) # Noise Input\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        return self.X_noise[idx, :], self.X[idx, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create train and valid datasets\ntrain_ds = moaAutoEncoderDataset(trX.values, trX_noised)\nvalid_ds = moaAutoEncoderDataset(vlX.values, vlX_noised)\n\nbatch_size = 128\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nvalid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=batch_size, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create your model configuration for AutoEncoder here"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoding_dim = 450\nclass moaAutoEncoder(nn.Module):\n    \"\"\"\n    Pytorch multiperceptron model\n    \"\"\"\n    def __init__(self, input_dim):\n        \"\"\"\n        Given embedding size and the number of continuous variables, initializes different layers of the model\n        \"\"\"\n        super(moaAutoEncoder, self).__init__()\n        \n        # Define the initial embedding layer and batchnorm layer for input continuous columns\n                \n        self.fc1 = nn.Linear(in_features = input_dim, out_features = 800)\n        self.act1 = nn.ELU()\n#         self.bn1 = nn.BatchNorm1d(num_features = 800)\n#         self.do1 = nn.Dropout(p = .3)\n        \n        self.fc_encoder = nn.Linear(in_features = 800, out_features = encoding_dim)\n        self.act2 = nn.ELU()\n#         self.bn_encoder = nn.BatchNorm1d(num_features = encoding_dim)\n#         self.do_encoder = nn.Dropout(p = .15)\n        \n        self.fc2 = nn.Linear(in_features = encoding_dim, out_features = 800)\n        self.act3 = nn.ELU()\n#         self.bn2 = nn.BatchNorm1d(num_features = 800)\n#         self.do2 = nn.Dropout(p = .3)\n        \n        self.fc_decoder = nn.Linear(in_features = 800, out_features = input_dim)\n        self.fc_decoder_act = nn.ELU()\n        \n        \n    def forward(self, ip):\n        \"\"\"\n        Implement forward pass through the network\n        \"\"\"\n                \n        # Pass the input through the first expander layers\n        x = self.act1(self.fc1(ip))\n        \n        # Pass through the bottleneck\n        x = self.act2(self.fc_encoder(x))\n        \n        # Pass through the end expander layers\n        x = self.act3(self.fc2(x))\n        \n        # Pass through the output layer\n        op = self.fc_decoder_act(self.fc_decoder(x))\n        \n        return op\n    \n    def encode(self, ip):\n        \"\"\"\n        Forward pass only when doing prediction\n        \"\"\"\n        with torch.no_grad():\n            \n            # Pass the input through the first expander layers\n            x = self.act1(self.fc1(ip))\n\n            # Pass through the bottleneck\n            encoding = self.fc_encoder(x)\n            \n        return encoding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_dim = trX.shape[1]\nautoencoderModel = moaAutoEncoder(input_dim)\nautoencoderModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if gpu is available for training and utilize if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\" \nautoencoderModel = autoencoderModel.to(device);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Hyperparams for AutoEncoder\nn_epochs = 750\nlearning_rate = 8e-4\n\n# Use the BCEWithLogitsLoss function. It will apply sigmoid activation to output and use Cross Entropy loss using\n# log sum exp trick\nAE_loss_func = nn.MSELoss()\n\n# Use Adam as the optimizer with an initial learning rate specified above with a small weight decay for regularization\noptim = torch.optim.Adadelta(autoencoderModel.parameters(), lr = learning_rate, weight_decay = 5e-5)\n\n# Use Learning rate schedular to reduce the Learning rate to a third if validation \n# loss has plateaued for about 10 epochs. Set a lower bound of 1e-5 for learning rate\n# If an update is made then print it to the output (verbose = True)\n# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience = 5, verbose = True, min_lr=1e-5, factor = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Loop for AutoEncoder\nfor epoch in range(1, n_epochs + 1):\n    \n    train_loss = 0\n    valid_loss = 0\n    n_train_samples = len(train_dl)\n    n_valid_samples = len(valid_dl)\n    \n    # Train the model on training data\n    for x_noise, x_real in train_dl:       \n        # Move the batches to gpu if available\n        x_noise, x_real = x_noise.to(device), x_real.to(device)\n        \n        # Zero out the accumulated gradients\n        optim.zero_grad()\n        \n        # Get the inputs and outputs from the forward pass\n        ops = autoencoderModel(x_noise)\n        \n        # Compute the loss\n        loss = AE_loss_func(ops, x_real)\n        \n        # Backpropagate the losses\n        loss.backward()\n        optim.step()\n        \n        # Accumulate the losses\n        train_loss += loss.item()\n    \n    # Evaluate the model performance on validation data\n    for x_noise, x_real in valid_dl:\n        # Move the batches to gpu if available\n        x_noise, x_real = x_noise.to(device), x_real.to(device)\n        \n        with torch.no_grad():\n            # Get the inputs and outputs from the forward pass\n            ops = autoencoderModel(x_noise)\n\n            # Compute the loss\n            loss = AE_loss_func(ops, x_real)\n        \n        # Accumulate the loss\n        valid_loss += loss.item()\n    \n#     scheduler.step(valid_loss / n_valid_samples)\n    \n    train_loss = round(train_loss / n_train_samples, 6)\n    valid_loss = round(valid_loss / n_valid_samples, 6)\n    \n    print(f\"Epoch {str(epoch):<3}/{str(n_epochs):<3} | Train Loss: {str(train_loss):<8}| Validation Loss: {str(valid_loss):<8}\")\ntorch.save(autoencoderModel.state_dict(), f\"autoencoder_weights.pth\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encode train and test datasets using the Autoencoder to latent dimension"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a train and test dataloader and extract the encodings from the autoencoder\npca_compression_cols = list(train_features.columns)\n\n# Get the train features in a dataframe with the pca components\narr = pca_object.transform(train_features[pca_compression_cols])\ntemp_df = pd.DataFrame(arr, columns = [f\"pca_{i}\" for i in range(n_comps)])\ntrain_features_df = pd.concat([train_features, temp_df], axis = 1)\n\n# Get the test features in a dataframe with the pca components\narr = pca_object.transform(test_features[pca_compression_cols])\ntemp_df = pd.DataFrame(arr, columns = [f\"pca_{i}\" for i in range(n_comps)])\ntest_features_df = pd.concat([test_features, temp_df], axis = 1)\n\n# Create train and test dataloaders\ntrain_features_ds = moaAutoEncoderDataset(train_features_df.values, train_features_df.values)\ntest_features_ds = moaAutoEncoderDataset(test_features_df.values, test_features_df.values)\n\ntrain_features_dl = torch.utils.data.DataLoader(train_features_ds, batch_size = batch_size, shuffle = False)\ntest_features_dl = torch.utils.data.DataLoader(test_features_ds, batch_size = batch_size, shuffle = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_encodings = []\nfor x, _ in train_features_dl:\n    x = x.to(device)\n    embeddings = autoencoderModel.encode(x)\n    train_encodings.append(embeddings.to(\"cpu\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_encodings = []\nfor x, _ in test_features_dl:\n    x = x.to(device)\n    embeddings = autoencoderModel.encode(x)\n    test_encodings.append(embeddings.to(\"cpu\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train_feats = pd.DataFrame(torch.cat(train_encodings).numpy(), columns = [f\"AE_ft{i}\" for i in range(encoding_dim)])\nnew_test_feats = pd.DataFrame(torch.cat(test_encodings).numpy(), columns = [f\"AE_ft{i}\" for i in range(encoding_dim)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train_feats = pd.concat([train_features_df, new_train_feats], axis = 1)\nnew_test_feats = pd.concat([test_features_df, new_test_feats], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building a Multi-Layer Perceptron for Classification"},{"metadata":{},"cell_type":"markdown","source":"## Create an indexer for Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train_df = pd.concat([new_train_feats, train_targets], axis = 1)\ntarget_cols = list(train_targets.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def multifold_indexer(train_df, target_cols, n_splits = 8, random_state = 10):\n    folds = train_df.copy()\n\n    mlskf = MultilabelStratifiedKFold(n_splits=n_splits,random_state=random_state)\n    \n    folds['kfold'] = 0\n    \n    for f, (t_idx, v_idx) in enumerate(mlskf.split(X = train_df, y=train_df[target_cols])):\n        folds.iloc[v_idx,-1] = int(f)\n\n    folds['kfold'] = folds['kfold'].astype(int)\n    \n    return folds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create a dataset class for loading data"},{"metadata":{"trusted":true},"cell_type":"code","source":"class moaMLPDataset(torch.utils.data.Dataset):\n    \"\"\"\n    A dataset class to load the data for feeding it to pytorch models during training\n    \"\"\"\n    def __init__(self, X, target_cols = None, dtype = \"train\"):\n        X = X.copy()\n        self.dtype = dtype\n        \n        if dtype == \"train\":\n            self.X = X.drop(columns = target_cols).values.astype(np.float32) \n            self.y = X[target_cols].values.astype(np.float32)\n        else:\n            self.X = X.values.astype(np.float32)\n        \n    def __len__(self):\n        return self.X.shape[0]\n    \n    def __getitem__(self, idx):\n        \n        if self.dtype == \"train\":\n            op = (self.X[idx, :], self.y[idx, :])\n        else:\n            op = (self.X[idx, :], -1)\n        \n        return op","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define a Multi-Layer Perceptron Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class moaModel(nn.Module):\n    \"\"\"\n    Pytorch multiperceptron model\n    \"\"\"\n    def __init__(self, ip_dim, fc_dims, op_dim):\n        \"\"\"\n        Given embedding size and the number of continuous variables, initializes different layers of the model\n        \"\"\"\n        super(moaModel, self).__init__()\n                        \n        # Define the architecture with hidden layers\n        fc_dims = [ip_dim] + fc_dims\n        middle_layers = []\n        middle_layers.append(nn.BatchNorm1d(num_features = ip_dim))\n        for ip, op in zip(fc_dims[:-1], fc_dims[1:]):\n            middle_layers.append(wnrm(nn.Linear(in_features = ip, out_features = op)))\n            middle_layers.append(nn.LeakyReLU())\n            middle_layers.append(nn.BatchNorm1d(num_features = op))\n            middle_layers.append(nn.Dropout(p = 0.25))\n        \n        self.hidden = nn.Sequential(*middle_layers)\n        \n        # Define the output layer\n        self.op = wnrm(nn.Linear(in_features = fc_dims[-1], out_features = op_dim))\n    \n    def forward(self, X):\n        \"\"\"\n        Implement forward pass through the network\n        \"\"\"\n        op = self.op(self.hidden(X))\n        return op\n    \n    def predict(self, X):\n        \"\"\"\n        Forward pass only when doing prediction\n        \"\"\"\n        with torch.no_grad():\n            predictions = self.forward(X)\n        return predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define the Training Loss Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"class smoothLoss(nn.Module):\n    def __init__(self, reduction = \"mean\", smoothing = 0):\n        super(smoothLoss, self).__init__()\n        self.reduction = reduction\n        assert (smoothing >= 0) & (smoothing < 1)\n        self.smoothing = smoothing\n    \n    def forward(self, ip, target):\n        # Smooth the target labels\n        with torch.no_grad():\n            target = target * (1 - self.smoothing) + 0.5 * self.smoothing\n        \n        loss = F.binary_cross_entropy_with_logits(ip, target)\n        \n        if self.reduction == \"mean\":\n            loss = loss.mean()\n        else:\n            loss = loss.sum()\n        \n        return loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define the training loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_validate(model, train_dl, valid_dl, train_loss_func, valid_loss_func, optimizer, sched, device):\n    \n    model.to(device)\n    \n    train_batches = len(train_dl)\n    valid_batches = len(valid_dl)\n\n    train_ls = 0\n    valid_ls = 0\n\n\n    # Train for one epoch\n    for X, y in train_dl:\n        # Clear the accumulated gradients\n        optimizer.zero_grad()\n\n        # Perform the forward pass operation\n        X, targs = X.to(device), y.to(device)\n        op = model(X)\n\n        # Backpropagate the errors through the network\n        tr_loss = train_loss_func(op, targs)\n        tr_loss.backward()\n        optimizer.step()\n        train_ls += tr_loss.item()\n\n    # Check the performance on valiation data\n    valid_preds = []\n    for X, y in valid_dl:\n        X, targs = X.to(device), y.to(device)\n        op = model.predict(X)\n        valid_preds.append(F.sigmoid(op).cpu())\n        vls = valid_loss_func(op, targs)\n        valid_ls += vls.item()\n\n    train_ls = round(train_ls / train_batches, 6)\n    valid_ls = round(valid_ls / valid_batches, 6)\n    \n    valid_preds = torch.cat(valid_preds).numpy()\n    \n    # Check if validation loss is reducing, if not, reduce the learning rate\n    sched.step(valid_ls)\n\n#         print(f\"Epoch {str(epoch):<3}/{str(n_epochs):<3} | Train Loss: {str(train_ls):<8}| Validation Loss: {str(valid_ls):<8}\")\n    return (train_ls, valid_ls, valid_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define a function for doing inference on test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_test(model, test_dl, device):\n    model.to(device)\n    predictions = []\n    for x, _ in test_dl:\n        x = x.to(device)\n        with torch.no_grad():\n            predictions.append(F.sigmoid(model(x)).cpu())\n    predictions = torch.cat(predictions).numpy()\n    return predictions    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create a model instance"},{"metadata":{"trusted":true},"cell_type":"code","source":"ip_dim = new_train_feats.shape[1]\nfc_dims = [600, 512, 256]\nop_dim = train_targets.shape[1]\nmyClassifierModel = moaModel(ip_dim, fc_dims, op_dim)\n\nprint(myClassifierModel)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define training hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seed everything for reproducibility\ndef seed_everything(seed=42):\n    random.seed(seed)\n    #os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_FOLDS = 7\nRANDOM_STATE = 42\nN_EPOCHS_PER_FOLD = 20\nBATCH_SIZE = 128\nWEIGHT_DECAY = 1e-5\nLEARNING_RATE = 1.5e-3\nMIN_LR = 8e-4\n\n# Seed everything\nseed_everything(RANDOM_STATE)\n\n# Create a dataset with the cross-validation indices\ndf_with_folds = multifold_indexer(new_train_df, target_cols ,N_FOLDS, RANDOM_STATE)\n\n# Check if a gpu is available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# For training use the smooth loss func defined above\ntrain_loss_func = smoothLoss(smoothing = 0.001)\n# train_loss_func = nn.BCEWithLogitsLoss()\n\n# Use the BCEWithLogitsLoss function for validation. It will apply sigmoid activation to output\n# and use Cross Entropy loss using log sum exp trick\nvalid_loss_func = nn.BCEWithLogitsLoss()\n\n# Use Adam as the optimizer with an initial learning rate specified above with a small weight decay for regularization\noptimizer = torch.optim.AdamW(myClassifierModel.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n\n# Use Learning rate schedular to reduce the Learning rate to a third if validation \n# loss has plateaued for about 5 epochs. Set a lower bound of 1e-5 for learning rate\n# If an update is made then print it to the output (verbose = True)\nsched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 10, verbose = True, min_lr = MIN_LR, factor = 0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the MLP"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_history = {}\nbest_loss = np.inf\n\n# Create a test dataloader for inferring the predictions after each fold\ntest_ds = moaMLPDataset(new_test_feats, dtype = \"test\")\ntest_dl = torch.utils.data.DataLoader(test_ds, batch_size = BATCH_SIZE, shuffle = False)\n\n# Create an out of folds predictions and an inference predictions array\noof_predictions = np.zeros((new_train_feats.shape[0], len(target_cols)))\ninference_predictions = np.zeros((new_test_feats.shape[0], len(target_cols)))\nrandom_states = [10, 42, 73, 1729, 31415]\n\nfor random_state in random_states:\n    df_with_folds = multifold_indexer(new_train_df, target_cols ,N_FOLDS, random_state)\n    for fold in np.unique(df_with_folds.kfold):\n\n        # Using the fold indices, split the data into train and validation\n        tr = df_with_folds[df_with_folds.kfold == fold].copy().reset_index(drop = True)\n        vl = df_with_folds[df_with_folds.kfold != fold].copy()\n        vl_indices = list(vl.index)                   \n        vl = vl.reset_index(drop = True)\n\n        tr = tr.drop(columns = [\"kfold\"])\n        vl = vl.drop(columns = [\"kfold\"])\n\n        # Create train and validation dataloaders\n        train_ds = moaMLPDataset(tr, target_cols)\n        valid_ds = moaMLPDataset(vl, target_cols)\n\n        train_dl = torch.utils.data.DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = True)\n        valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size = BATCH_SIZE, shuffle = False)\n\n        # Keep the training history in memory\n        if f\"FOLD_{fold}\" not in loss_history:\n            loss_history[f\"FOLD_{fold}\"] = {}\n            loss_history[f\"FOLD_{fold}\"][\"train_loss\"] = []\n            loss_history[f\"FOLD_{fold}\"][\"valid_loss\"] = []\n\n        # Do the actual training\n        for epoch in range(1, N_EPOCHS_PER_FOLD + 1):\n            train_ls, valid_ls, valid_preds = train_validate(myClassifierModel, train_dl, valid_dl, train_loss_func, valid_loss_func, optimizer, sched, device)\n            print(f\"Fold: {str(fold):<3}| Epoch: {str(epoch):<3}| Train Loss: {str(round(train_ls, 5)):<6}| Valid Loss: {str(round(valid_ls, 5)):<6}\")\n            loss_history[f\"FOLD_{fold}\"][\"train_loss\"].append(train_ls)\n            loss_history[f\"FOLD_{fold}\"][\"valid_loss\"].append(valid_ls)\n\n            # Save the parameters of the best model until now\n            if valid_ls < best_loss:\n                best_loss = valid_ls\n                torch.save(myClassifierModel.state_dict(), f\"best_model.pth\")\n            \n            # Add the validation predictions to the out of folds dataframe\n            oof_predictions[vl_indices, :] += valid_preds\n        \n        # Do inference and append it to the inference predictions dataset\n        print(\"Adding inference predictions\")\n        inference_predictions += predict_test(myClassifierModel, test_dl, device)\n\n    # Save the parameters of the final model\n    torch.save(myClassifierModel.state_dict(), f\"final_model.pth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Averaging all the inference predictions across all folds and random states breaks.\")\ninference_predictions /= (len(random_states) * N_FOLDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Averaging out of folds predictions across all epochs and random states breaks\")\noof_predictions /= (len(random_states) * N_EPOCHS_PER_FOLD)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Look at the training curves for different folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_losses(ax, record, fold):\n    df = pd.DataFrame(record)\n    sns.lineplot(df.index, df.train_loss, ax = ax)\n    sns.lineplot(df.index, df.valid_loss, ax = ax)\n    ax.set_xlabel(\"Epochs\", fontsize = 12)\n    ax.set_ylabel(\"Loss\", fontsize = 12)\n    ax.set_title(f\"Training Curve for {fold}\", fontsize = 15)\n    ax.legend([\"Train Loss\", \"Valid Loss\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use(\"fivethirtyeight\")\nfig, ax = plt.subplots(2, 3, figsize = (15,9), sharex = True)\nax = np.ravel(ax)\n\n# Plot the loss curves for different training folds\nfor fold, axis in zip(loss_history, ax):\n    rec = loss_history[fold]\n    plot_losses(axis, rec, fold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate the performance on the whole dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the log loss on the entire dataframe\nscore = 0\nfor i in range(len(target_cols)):\n    true = train_targets.values[:, i]\n    preds = oof_predictions[:, i]/N_FOLDS\n    \n    score_ = log_loss(true, preds, eps = 1e-6)\n    if not np.isnan(score_):\n        score += score_ \n    else:\n        print(i)\n    \nscore /= len(target_cols)\n\nprint(f\"Loss on the entire dataframe: {round(score, 5)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Infer on the test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = pd.DataFrame(inference_predictions, columns = target_cols)\npredictions[\"sig_id\"] = test_ids\n\ncols = [\"sig_id\"] + list(predictions.columns)[:-1]\npredictions = predictions[cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Manually change predictions of control group to zeros"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get all the control group ids\ncontrol_ids = list(test_cp_types[test_cp_types == \"ctl_vehicle\"].index)\n\n# Modify predictions of control group records\nmodified_predictions = pd.DataFrame(np.zeros_like(predictions.iloc[control_ids]), columns = predictions.columns)\nmodified_predictions.index = control_ids\npredictions.iloc[control_ids] = modified_predictions\npredictions.sig_id = test_ids\n\n# Save the predictions\npredictions.to_csv(\"/kaggle/working/submission.csv\", index = False)\npredictions.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}