{"cells":[{"metadata":{},"cell_type":"markdown","source":"# https://www.kaggle.com/thehemen/pytorch-transfer-learning-with-k-folds-by-drug-ids by Vladimir Zhuravlev \n# https://www.kaggle.com/thehemen/pytorch-transfer-learning-with-k-folds-by-drug-ids by Vladimir Zhuravlev \n# https://www.kaggle.com/thehemen/pytorch-transfer-learning-with-k-folds-by-drug-ids by Vladimir Zhuravlev \n# https://www.kaggle.com/thehemen/pytorch-transfer-learning-with-k-folds-by-drug-ids by Vladimir Zhuravlev \n# https://www.kaggle.com/thehemen/pytorch-transfer-learning-with-k-folds-by-drug-ids by Vladimir Zhuravlev \n# https://www.kaggle.com/thehemen/pytorch-transfer-learning-with-k-folds-by-drug-ids by Vladimir Zhuravlev \n# https://www.kaggle.com/thehemen/pytorch-transfer-learning-with-k-folds-by-drug-ids by Vladimir Zhuravlev \n# https://www.kaggle.com/thehemen/pytorch-transfer-learning-with-k-folds-by-drug-ids by Vladimir Zhuravlev \n# https://www.kaggle.com/thehemen/pytorch-transfer-learning-with-k-folds-by-drug-ids by Vladimir Zhuravlev \n# https://www.kaggle.com/thehemen/pytorch-transfer-learning-with-k-folds-by-drug-ids by Vladimir Zhuravlev \n# https://www.kaggle.com/thehemen/pytorch-transfer-learning-with-k-folds-by-drug-ids by Vladimir Zhuravlev \n# https://www.kaggle.com/thehemen/pytorch-transfer-learning-with-k-folds-by-drug-ids by Vladimir Zhuravlev "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-10-30T16:26:44.160903Z","iopub.status.busy":"2020-10-30T16:26:44.160036Z","iopub.status.idle":"2020-10-30T16:26:45.599387Z","shell.execute_reply":"2020-10-30T16:26:45.598462Z"},"executionInfo":{"elapsed":6322,"status":"ok","timestamp":1605114236516,"user":{"displayName":"Vladimir Zhuravlev","photoUrl":"","userId":"16372324542816680996"},"user_tz":-420},"id":"pfi5SZ_kU50f","outputId":"adaffd70-a7f7-49e6-ee61-20961c80c5d4","papermill":{"duration":1.50169,"end_time":"2020-10-30T16:26:45.599507","exception":false,"start_time":"2020-10-30T16:26:44.097817","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n\nfrom datetime import timedelta\nfrom time import time\nfrom types import SimpleNamespace\n\nimport joblib\n \nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\n \nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install /kaggle/input/iterative-stratification/iterative-stratification-master/\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-10-30T16:26:45.771303Z","iopub.status.busy":"2020-10-30T16:26:45.770451Z","iopub.status.idle":"2020-10-30T16:26:45.778663Z","shell.execute_reply":"2020-10-30T16:26:45.779209Z"},"executionInfo":{"elapsed":79185,"status":"ok","timestamp":1605114309438,"user":{"displayName":"Vladimir Zhuravlev","photoUrl":"","userId":"16372324542816680996"},"user_tz":-420},"id":"PcmCrDMHU50n","outputId":"8d65d771-547c-47c1-b374-dbc0f8d67247","papermill":{"duration":0.050111,"end_time":"2020-10-30T16:26:45.779337","exception":false,"start_time":"2020-10-30T16:26:45.729226","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"data_dir = '../input/lish-moa/'\nos.listdir(data_dir)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:26:53.34316Z","iopub.status.busy":"2020-10-30T16:26:53.342367Z","iopub.status.idle":"2020-10-30T16:26:53.346789Z","shell.execute_reply":"2020-10-30T16:26:53.346264Z"},"id":"YbP28luJU50t","papermill":{"duration":0.048987,"end_time":"2020-10-30T16:26:53.346888","exception":false,"start_time":"2020-10-30T16:26:53.297901","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def set_columns(state, train_features):\n    state.GENES = [col for col in train_features.columns if col.startswith('g-')]\n    state.CELLS = [col for col in train_features.columns if col.startswith('c-')]\n\n    print('GENES: {}'.format(state.GENES[:10]))\n    print('CELLS: {}'.format(state.CELLS[:10]))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:42.973092Z","iopub.status.busy":"2020-10-30T16:27:42.9717Z","iopub.status.idle":"2020-10-30T16:27:42.976302Z","shell.execute_reply":"2020-10-30T16:27:42.978092Z"},"id":"J80ID9aaU518","papermill":{"duration":0.596642,"end_time":"2020-10-30T16:27:42.978321","exception":false,"start_time":"2020-10-30T16:27:42.381679","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def mix_seed(z):\n    # splitmix32\n    z = (z + 0x9e3779b9) & 0xffffffff\n    z = ((z ^ (z >> 15))*0x85ebca6b) & 0xffffffff\n    z = ((z ^ (z >> 13))*0xc2b2ae35) & 0xffffffff\n    return z ^ (z >> 16)\n\n\ndef set_parameters(state):\n    # HyperParameters\n    state.SEEDS = list(map(mix_seed, range(10)))\n    state.NFOLDS = 7\n    state.DRUG_THRESH = 18\n\n    state.DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n    state.EPOCHS = 24\n    state.BATCH_SIZE = 128\n\n    state.WEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\n    state.MAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\n    state.DIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\n    state.PCT_START = 0.1","execution_count":null,"outputs":[]},{"metadata":{"id":"-gsULXIEVs1V"},"cell_type":"markdown","source":"# RankGauss"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:02.692315Z","iopub.status.busy":"2020-10-30T16:27:02.691473Z","iopub.status.idle":"2020-10-30T16:27:12.807731Z","shell.execute_reply":"2020-10-30T16:27:12.807143Z"},"id":"XmZk8BSxU509","papermill":{"duration":10.171313,"end_time":"2020-10-30T16:27:12.807963","exception":false,"start_time":"2020-10-30T16:27:02.63665","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def set_rank_gauss(state, train_features):\n    state.col_to_transformer = {}\n    for col in (state.GENES + state.CELLS):\n        transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n        raw_vec = train_features[col].values.reshape(-1, 1)\n        transformer.fit(raw_vec)\n        state.col_to_transformer[col] = transformer # TODO deepcopy\n\n\ndef apply_rank_gauss(state, data_set):\n    for col, transformer in state.col_to_transformer.items():\n        data_set[col] = transformer.transform(data_set[col].values.reshape(-1, 1)).ravel()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:12.990981Z","iopub.status.busy":"2020-10-30T16:27:12.990145Z","iopub.status.idle":"2020-10-30T16:27:12.996237Z","shell.execute_reply":"2020-10-30T16:27:12.995593Z"},"id":"6J2XxF2lU51A","papermill":{"duration":0.057375,"end_time":"2020-10-30T16:27:12.996346","exception":false,"start_time":"2020-10-30T16:27:12.938971","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\ndef set_seed(state):\n    state.SEED_VALUE = 42\n    seed_everything(seed=state.SEED_VALUE)","execution_count":null,"outputs":[]},{"metadata":{"id":"zqqSsd-0U51O","papermill":{"duration":0.071318,"end_time":"2020-10-30T16:27:21.173019","exception":false,"start_time":"2020-10-30T16:27:21.101701","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# PCA features + Existing features"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:21.327068Z","iopub.status.busy":"2020-10-30T16:27:21.32619Z","iopub.status.idle":"2020-10-30T16:27:34.313177Z","shell.execute_reply":"2020-10-30T16:27:34.312497Z"},"id":"_7XdDzbRU51P","papermill":{"duration":13.068568,"end_time":"2020-10-30T16:27:34.313298","exception":false,"start_time":"2020-10-30T16:27:21.24473","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def set_pca_genes(state, train_features, test_features):\n    n_comp = 600\n    GENES = state.GENES\n    SEED_VALUE = state.SEED_VALUE\n    \n    data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n    transformer = PCA(n_components=n_comp, random_state=SEED_VALUE)\n    transformer.fit(data[GENES])\n    \n    state.pca_genes_n_comp = n_comp\n    state.pca_genes = transformer\n\n\ndef apply_pca_genes(state, data_set):\n    n_comp = state.pca_genes_n_comp\n    GENES = state.GENES\n    transformer = state.pca_genes\n    \n    data2 = transformer.transform(data_set[GENES])\n    data2 = pd.DataFrame(data2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n    data2_features = pd.concat((data_set, data2), axis=1)\n    print('GENES data2_features: {}'.format(data2_features.shape))\n    return data2_features","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:21.327068Z","iopub.status.busy":"2020-10-30T16:27:21.32619Z","iopub.status.idle":"2020-10-30T16:27:34.313177Z","shell.execute_reply":"2020-10-30T16:27:34.312497Z"},"id":"_7XdDzbRU51P","papermill":{"duration":13.068568,"end_time":"2020-10-30T16:27:34.313298","exception":false,"start_time":"2020-10-30T16:27:21.24473","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def set_pca_cells(state, train_features, test_features):\n    n_comp = 50\n    CELLS = state.CELLS\n    SEED_VALUE = state.SEED_VALUE\n    \n    data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n    transformer = PCA(n_components=n_comp, random_state=SEED_VALUE)\n    transformer.fit(data[CELLS])\n    \n    state.pca_cells_n_comp = n_comp\n    state.pca_cells = transformer\n\n\ndef apply_pca_cells(state, data_set):\n    n_comp = state.pca_cells_n_comp\n    CELLS = state.CELLS\n    transformer = state.pca_cells\n    \n    data2 = transformer.transform(data_set[CELLS])\n    data2 = pd.DataFrame(data2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n    data2_features = pd.concat((data_set, data2), axis=1)\n    print('CELLS data2_features: {}'.format(data2_features.shape))\n    return data2_features","execution_count":null,"outputs":[]},{"metadata":{"id":"00hCtGDsU51W","papermill":{"duration":0.051098,"end_time":"2020-10-30T16:27:35.401617","exception":false,"start_time":"2020-10-30T16:27:35.350519","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# feature Selection using Variance Encoding"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:35.517783Z","iopub.status.busy":"2020-10-30T16:27:35.51697Z","iopub.status.idle":"2020-10-30T16:27:36.626838Z","shell.execute_reply":"2020-10-30T16:27:36.625742Z"},"executionInfo":{"elapsed":121315,"status":"ok","timestamp":1605114351712,"user":{"displayName":"Vladimir Zhuravlev","photoUrl":"","userId":"16372324542816680996"},"user_tz":-420},"id":"ri4YPSgxU51X","outputId":"7a1f41b1-5c9f-4f2f-af02-d98dc5f63823","papermill":{"duration":1.173308,"end_time":"2020-10-30T16:27:36.626956","exception":false,"start_time":"2020-10-30T16:27:35.453648","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def set_var_thresh(state, train_features, test_features):\n    var_thresh = VarianceThreshold(0.8)\n    data = train_features.append(test_features)\n    var_thresh.fit(data.iloc[:, 4:])\n    state.var_thresh = var_thresh\n\n\ndef apply_var_thresh(state, data_set):\n    xcols = ['sig_id','cp_type','cp_time','cp_dose']\n    var_thresh = state.var_thresh\n    data_features_transformed = var_thresh.transform(data_set.iloc[:, 4:])\n    data_features = pd.DataFrame(data_set[xcols].values.reshape(-1, 4), columns=xcols)\n    data_features = pd.concat([data_features, pd.DataFrame(data_features_transformed)], axis=1)\n    print('VAR THRESH data_features: {}'.format(data_features.shape))\n    return data_features","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:36.73753Z","iopub.status.busy":"2020-10-30T16:27:36.736157Z","iopub.status.idle":"2020-10-30T16:27:37.084383Z","shell.execute_reply":"2020-10-30T16:27:37.084913Z"},"id":"0YYLua3IU51a","papermill":{"duration":0.408004,"end_time":"2020-10-30T16:27:37.085091","exception":false,"start_time":"2020-10-30T16:27:36.677087","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def apply_train_extensions(train_set, train_targets_scored, train_targets_nonscored, train_drug):\n    train = train_set.merge(train_targets_scored, on='sig_id')\n    train = train.merge(train_targets_nonscored, on='sig_id')\n    train = train.merge(train_drug, on='sig_id')\n    print('EXTENSION train: {}'.format(train.shape))\n    return train\n\n\ndef apply_cleaning(data_set):\n    data_set = data_set[data_set['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n    data_set = data_set.drop('cp_type', axis=1)\n    print('CLEANING data_set: {}'.format(data_set.shape))\n    return data_set\n\n\ndef apply_dummies(data_set):\n    # former process_data\n    data_set = pd.get_dummies(data_set, columns=['cp_time', 'cp_dose'])\n    print('DUMMIES data_set: {}'.format(data_set.shape))    \n    return data_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_meta(state, train, train_targets_scored, train_targets_nonscored):\n    target_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\n    aux_target_cols = [x for x in train_targets_nonscored.columns if x != 'sig_id']\n    all_target_cols = target_cols + aux_target_cols\n\n    num_targets = len(target_cols)\n    num_aux_targets = len(aux_target_cols)\n    num_all_targets = len(all_target_cols)\n\n    print('META num_targets: {}'.format(num_targets))\n    print('META num_aux_targets: {}'.format(num_aux_targets))\n    print('META num_all_targets: {}'.format(num_all_targets))\n    \n    state.target_cols = target_cols\n    state.aux_target_cols = aux_target_cols\n    state.all_target_cols = all_target_cols\n    state.num_targets = num_targets\n    state.num_aux_targets = num_aux_targets\n    state.num_all_targets = num_all_targets\n    \n    feature_cols = [c for c in train.columns if c not in all_target_cols]\n    feature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id', 'drug_id']]\n    \n    state.feature_cols = feature_cols\n    state.num_features = len(feature_cols)\n    print(\"META num_features\", state.num_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_cv_folds(state, train_set):\n    DRUG_THRESH = state.DRUG_THRESH\n    SEEDS = state.SEEDS\n    NFOLDS = state.NFOLDS\n    target_cols = state.target_cols\n    \n    vc = train_set.drug_id.value_counts()\n    vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n    vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n\n    for seed_id in SEEDS:\n        kfold_col = 'kfold_{}'.format(seed_id)\n\n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}\n        dct2 = {}\n\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train_set.groupby('drug_id')[target_cols].mean().loc[vc1]\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train_set.loc[train_set.drug_id.isin(vc2)].reset_index(drop=True)\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        train_set[kfold_col] = train_set.drug_id.map(dct1)\n        train_set.loc[train_set[kfold_col].isna(), kfold_col] = train_set.loc[train_set[kfold_col].isna(), 'sig_id'].map(dct2)\n        train_set[kfold_col] = train_set[kfold_col].astype('int8')\n\n    return train_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_all(state, train_features, test_features,\n            train_targets_scored, train_targets_nonscored, train_drug):\n    \"\"\" Create input transformations. \"\"\"\n    train_features = train_features.copy()\n    print(f\"ALL train_features {train_features.shape!r}\")\n    test_features = test_features.copy()\n    print(f\"ALL test_features {test_features.shape!r}\")\n    \n    # parameteres\n    set_parameters(state)\n    \n    # column meta\n    set_columns(state, train_features)\n    \n    # quantile transform\n    set_rank_gauss(state, train_features)\n    apply_rank_gauss(state, train_features)\n    apply_rank_gauss(state, test_features)\n    \n    # seed here for some reason\n    set_seed(state)\n    \n    # pca features\n    set_pca_genes(state, train_features, test_features)\n    train_features = apply_pca_genes(state, train_features)\n    test_features = apply_pca_genes(state, test_features)\n    \n    set_pca_cells(state, train_features, test_features)\n    train_features = apply_pca_cells(state, train_features)\n    test_features = apply_pca_cells(state, test_features)\n    \n    # variance threshold\n    set_var_thresh(state, train_features, test_features)\n    train_features = apply_var_thresh(state, train_features)\n    test_features = apply_var_thresh(state, test_features)\n    \n    # add targets & drug\n    train = apply_train_extensions(train_features, train_targets_scored, train_targets_nonscored, train_drug)\n    \n    # strip out waste\n    train = apply_cleaning(train)\n    test = apply_cleaning(test_features)\n    \n    # dummies\n    train = apply_dummies(train)\n    test = apply_dummies(test)\n    \n    # put other variables into state\n    set_meta(state, train, train_targets_scored, train_targets_nonscored)\n    \n    # persist\n    joblib.dump(state, \"state_object.joblib\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_all(state, data_set):\n    data_set = data_set.copy()\n    apply_rank_gauss(state, data_set)\n    data_set = apply_pca_genes(state, data_set)\n    data_set = apply_pca_cells(state, data_set)\n    data_set = apply_var_thresh(state, data_set)\n    data_set = apply_cleaning(data_set)\n    data_set = apply_dummies(data_set)\n    print(f\"APPLY data_set {data_set.shape!r}\")\n    return data_set\n\n\ndef apply_all_train(state, data_set,\n                    train_targets_scored, train_targets_nonscored, train_drug):\n    data_set = data_set.copy()\n    apply_rank_gauss(state, data_set)\n    data_set = apply_pca_genes(state, data_set)\n    data_set = apply_pca_cells(state, data_set)\n    data_set = apply_var_thresh(state, data_set)\n    data_set = apply_train_extensions(data_set, train_targets_scored, train_targets_nonscored, train_drug)\n    data_set = apply_cleaning(data_set)\n    data_set = make_cv_folds(state, data_set)\n    data_set = apply_dummies(data_set)\n    print(f\"APPLY TRAIN data_set {data_set.shape!r}\")\n    return data_set","execution_count":null,"outputs":[]},{"metadata":{"id":"-4PxBYMVU51p","papermill":{"duration":0.053045,"end_time":"2020-10-30T16:27:41.168745","exception":false,"start_time":"2020-10-30T16:27:41.1157","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Dataset Classes"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:41.290886Z","iopub.status.busy":"2020-10-30T16:27:41.289986Z","iopub.status.idle":"2020-10-30T16:27:41.293444Z","shell.execute_reply":"2020-10-30T16:27:41.292871Z"},"id":"Oe4Fyj0rU51q","papermill":{"duration":0.069081,"end_time":"2020-10-30T16:27:41.293559","exception":false,"start_time":"2020-10-30T16:27:41.224478","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)\n        }\n        \n        return dct\n\n\nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n\n        return dct","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:41.419307Z","iopub.status.busy":"2020-10-30T16:27:41.415376Z","iopub.status.idle":"2020-10-30T16:27:41.421889Z","shell.execute_reply":"2020-10-30T16:27:41.422529Z"},"id":"tIvXQTmIU51s","papermill":{"duration":0.074766,"end_time":"2020-10-30T16:27:41.422664","exception":false,"start_time":"2020-10-30T16:27:41.347898","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n\n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    return final_loss, valid_preds\n\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    return preds","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:41.544517Z","iopub.status.busy":"2020-10-30T16:27:41.543653Z","iopub.status.idle":"2020-10-30T16:27:41.546741Z","shell.execute_reply":"2020-10-30T16:27:41.546261Z"},"id":"xEI0c6n2U51u","papermill":{"duration":0.069859,"end_time":"2020-10-30T16:27:41.546868","exception":false,"start_time":"2020-10-30T16:27:41.477009","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            \n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"id":"2YOmMelYU51x","papermill":{"duration":0.055406,"end_time":"2020-10-30T16:27:41.65766","exception":false,"start_time":"2020-10-30T16:27:41.602254","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Model"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:41.785992Z","iopub.status.busy":"2020-10-30T16:27:41.784898Z","iopub.status.idle":"2020-10-30T16:27:41.787531Z","shell.execute_reply":"2020-10-30T16:27:41.788132Z"},"id":"NuBU5W3lU51y","papermill":{"duration":0.076376,"end_time":"2020-10-30T16:27:41.78827","exception":false,"start_time":"2020-10-30T16:27:41.711894","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(Model, self).__init__()\n        self.hidden_size = [1504, 1248, 992, 736] # XXXGBoost change (round to multiple of 32)\n        self.dropout_value = [0.5, 0.35, 0.3, 0.25]\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n        \n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_value[0])\n        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_value[1])\n        self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n\n        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n        self.dropout4 = nn.Dropout(self.dropout_value[2])\n        self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n\n        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n        self.dropout5 = nn.Dropout(self.dropout_value[3])\n        self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.leaky_relu(self.dense3(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = F.leaky_relu(self.dense4(x))\n\n        x = self.batch_norm5(x)\n        x = self.dropout5(x)\n        x = self.dense5(x)\n        return x\n\n\nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n            \n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    ","execution_count":null,"outputs":[]},{"metadata":{"id":"NJaTGiY4uJRP","trusted":true},"cell_type":"code","source":"class FineTuneScheduler:\n    def __init__(self, epochs, device):\n        self.epochs = epochs\n        self.device = device\n        self.epochs_per_step = 0\n        self.frozen_layers = []\n\n    def copy_without_top(self, model, num_features, num_targets, num_targets_new):\n        self.frozen_layers = []\n\n        model_new = Model(num_features, num_targets)\n        model_new.load_state_dict(model.state_dict())\n\n        # Freeze all weights\n        for name, param in model_new.named_parameters():\n            layer_index = name.split('.')[0][-1]\n\n            if layer_index == 5:\n                continue\n\n            param.requires_grad = False\n\n            # Save frozen layer names\n            if layer_index not in self.frozen_layers:\n                self.frozen_layers.append(layer_index)\n\n        self.epochs_per_step = max(1, self.epochs // len(self.frozen_layers))\n\n        # Replace the top layers with another ones\n        model_new.batch_norm5 = nn.BatchNorm1d(model_new.hidden_size[3])\n        model_new.dropout5 = nn.Dropout(model_new.dropout_value[3])\n        model_new.dense5 = nn.utils.weight_norm(nn.Linear(model_new.hidden_size[-1], num_targets_new))\n        model_new.to(self.device)\n        return model_new\n\n    def step(self, epoch, model):\n        if len(self.frozen_layers) == 0:\n            return\n\n        if epoch % self.epochs_per_step == 0:\n            last_frozen_index = self.frozen_layers[-1]\n            \n            # Unfreeze parameters of the last frozen layer\n            for name, param in model.named_parameters():\n                layer_index = name.split('.')[0][-1]\n\n                if layer_index == last_frozen_index:\n                    param.requires_grad = True\n\n            del self.frozen_layers[-1]  # Remove the last layer as unfrozen","execution_count":null,"outputs":[]},{"metadata":{"id":"0hidUPH8U51-","papermill":{"duration":0.149218,"end_time":"2020-10-30T16:27:43.27826","exception":false,"start_time":"2020-10-30T16:27:43.129042","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Single fold training"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:43.533519Z","iopub.status.busy":"2020-10-30T16:27:43.532417Z","iopub.status.idle":"2020-10-30T16:27:43.586422Z","shell.execute_reply":"2020-10-30T16:27:43.585623Z"},"id":"JHTrp23wU51_","papermill":{"duration":0.213528,"end_time":"2020-10-30T16:27:43.586582","exception":false,"start_time":"2020-10-30T16:27:43.373054","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def run_training(state, fold_id, seed_id, train_, test_):\n    feature_cols = state.feature_cols\n    target_cols = state.target_cols\n    all_target_cols = state.all_target_cols\n    num_features = state.num_features\n    num_all_targets = state.num_all_targets\n    num_targets = state.num_targets\n    BATCH_SIZE = state.BATCH_SIZE\n    PCT_START = state.PCT_START\n    DIV_FACTOR = state.DIV_FACTOR\n    WEIGHT_DECAY = state.WEIGHT_DECAY\n    MAX_LR = state.MAX_LR\n    EPOCHS = state.EPOCHS\n    DEVICE = state.DEVICE\n    \n    seed_everything(seed_id)\n    \n    kfold_col = f'kfold_{seed_id}'\n    trn_idx = train_[train_[kfold_col] != fold_id].index\n    val_idx = train_[train_[kfold_col] == fold_id].index\n    \n    train_df = train_[train_[kfold_col] != fold_id].reset_index(drop=True)\n    valid_df = train_[train_[kfold_col] == fold_id].reset_index(drop=True)\n    \n    def train_model(model, tag_name, target_cols_now, fine_tune_scheduler=None):\n        x_train, y_train  = train_df[feature_cols].values, train_df[target_cols_now].values\n        x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols_now].values\n        \n        train_dataset = MoADataset(x_train, y_train)\n        valid_dataset = MoADataset(x_valid, y_valid)\n\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY[tag_name])\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n                                                  steps_per_epoch=len(trainloader),\n                                                  pct_start=PCT_START,\n                                                  div_factor=DIV_FACTOR[tag_name], \n                                                  max_lr=MAX_LR[tag_name],\n                                                  epochs=EPOCHS)\n        \n        loss_fn = nn.BCEWithLogitsLoss()\n        loss_tr = SmoothBCEwLogits(smoothing=0.001)\n\n        oof = np.zeros((len(train_), len(target_cols_now)))\n        best_loss = np.inf\n        \n        for epoch in range(EPOCHS):\n            if fine_tune_scheduler is not None:\n                fine_tune_scheduler.step(epoch, model)\n\n            train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n            print(f\"SEED: {seed_id}, FOLD: {fold_id}, {tag_name}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}\")\n\n            if np.isnan(valid_loss):\n                break\n            \n            if valid_loss < best_loss:\n                best_loss = valid_loss\n                oof[val_idx] = valid_preds\n                torch.save(model.state_dict(), f\"{tag_name}_SEED{seed_id}_FOLD{fold_id}_.pth\")\n\n        return oof\n\n    fine_tune_scheduler = FineTuneScheduler(EPOCHS, DEVICE)\n\n    pretrained_model = Model(num_features, num_all_targets)\n    pretrained_model.to(DEVICE)\n\n    # Train on scored + nonscored targets\n    train_model(pretrained_model, 'ALL_TARGETS', all_target_cols)\n\n    # Load the pretrained model with the best loss\n    pretrained_model = Model(num_features, num_all_targets)\n    pretrained_model.load_state_dict(torch.load(f\"ALL_TARGETS_SEED{seed_id}_FOLD{fold_id}_.pth\"))\n    pretrained_model.to(DEVICE)\n\n    # Copy model without the top layer\n    final_model = fine_tune_scheduler.copy_without_top(pretrained_model, num_features, num_all_targets, num_targets)\n\n    # Fine-tune the model on scored targets only\n    oof = train_model(final_model, 'SCORED_ONLY', target_cols, fine_tune_scheduler)\n\n    # Load the fine-tuned model with the best loss\n    model = Model(num_features, num_targets)\n    model.load_state_dict(torch.load(f\"SCORED_ONLY_SEED{seed_id}_FOLD{fold_id}_.pth\"))\n    model.to(DEVICE)\n\n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    predictions = inference_fn(model, testloader, DEVICE)\n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:43.804473Z","iopub.status.busy":"2020-10-30T16:27:43.803598Z","iopub.status.idle":"2020-10-30T16:27:43.808545Z","shell.execute_reply":"2020-10-30T16:27:43.809223Z"},"id":"GxfyrnG4U52C","papermill":{"duration":0.143086,"end_time":"2020-10-30T16:27:43.809403","exception":false,"start_time":"2020-10-30T16:27:43.666317","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def run_k_fold(state, seed_id, train, test):\n    target_cols = state.target_cols\n    NFOLDS = state.NFOLDS\n    \n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold_id in range(NFOLDS):\n        oof_, pred_ = run_training(state, fold_id, seed_id, train, test)\n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:43.965401Z","iopub.status.busy":"2020-10-30T16:27:43.964403Z","iopub.status.idle":"2020-10-30T17:02:14.476596Z","shell.execute_reply":"2020-10-30T17:02:14.475472Z"},"executionInfo":{"elapsed":4373947,"status":"ok","timestamp":1605118604618,"user":{"displayName":"Vladimir Zhuravlev","photoUrl":"","userId":"16372324542816680996"},"user_tz":-420},"id":"Tix4uSLRU52D","outputId":"7fba4206-ad29-4c55-960a-14abc3ecebdd","papermill":{"duration":2070.583824,"end_time":"2020-10-30T17:02:14.476733","exception":false,"start_time":"2020-10-30T16:27:43.892909","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def run_over_seeds(state, train_features, test_features, train_targets_scored, train_targets_nonscored, train_drug):\n    target_cols = state.target_cols\n    SEEDS = state.SEEDS\n    NFOLDS = state.NFOLDS\n    \n    train = apply_all_train(state, train_features, train_targets_scored, train_targets_nonscored, train_drug)\n    test = apply_all(state, test_features)\n    \n    # Averaging on multiple SEEDS\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n\n    time_begin = time()\n\n    for seed_id in SEEDS:\n        oof_, predictions_ = run_k_fold(state, seed_id, train, test)\n        oof += oof_ / len(SEEDS)\n        predictions += predictions_ / len(SEEDS)\n\n    time_diff = time() - time_begin\n    print(timedelta(seconds=time_diff))\n\n    train[target_cols] = oof\n    test[target_cols] = predictions\n    \n    # cross-validation score\n    valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n    y_true = train_targets_scored[target_cols].values\n    y_pred = valid_results[target_cols].values\n\n    score = 0.0\n\n    for i in range(len(target_cols)):\n        score += log_loss(y_true[:, i], y_pred[:, i])\n\n    print(\"CV log_loss: \", score / y_pred.shape[1])\n    joblib.dump((train, test), \"train_test.joblib\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-10-30T16:26:45.864219Z","iopub.status.busy":"2020-10-30T16:26:45.863163Z","iopub.status.idle":"2020-10-30T16:26:53.257922Z","shell.execute_reply":"2020-10-30T16:26:53.257217Z"},"id":"m7vLwXCDU50r","papermill":{"duration":7.440368,"end_time":"2020-10-30T16:26:53.258045","exception":false,"start_time":"2020-10-30T16:26:45.817677","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv(data_dir + 'train_features.csv')\ntrain_targets_scored = pd.read_csv(data_dir + 'train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(data_dir + 'train_targets_nonscored.csv')\ntrain_drug = pd.read_csv(data_dir + 'train_drug.csv')\ntest_features = pd.read_csv(data_dir + 'test_features.csv')\nsample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n\nprint('train_features: {}'.format(train_features.shape))\nprint('train_targets_scored: {}'.format(train_targets_scored.shape))\nprint('train_targets_nonscored: {}'.format(train_targets_nonscored.shape))\nprint('train_drug: {}'.format(train_drug.shape))\nprint('test_features: {}'.format(test_features.shape))\nprint('sample_submission: {}'.format(sample_submission.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xxxtorch = SimpleNamespace()\nset_all(xxxtorch, train_features, test_features, train_targets_scored, train_targets_nonscored, train_drug)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_over_seeds(xxxtorch, train_features, test_features, train_targets_scored, train_targets_nonscored, train_drug)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}