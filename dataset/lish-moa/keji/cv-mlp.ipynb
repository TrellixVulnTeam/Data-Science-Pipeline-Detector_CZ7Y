{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n \nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold\n \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n \nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-10-30T16:26:44.160903Z","iopub.status.busy":"2020-10-30T16:26:44.160036Z","iopub.status.idle":"2020-10-30T16:26:45.599387Z","shell.execute_reply":"2020-10-30T16:26:45.598462Z"},"executionInfo":{"elapsed":6322,"status":"ok","timestamp":1605114236516,"user":{"displayName":"Vladimir Zhuravlev","photoUrl":"","userId":"16372324542816680996"},"user_tz":-420},"id":"pfi5SZ_kU50f","outputId":"adaffd70-a7f7-49e6-ee61-20961c80c5d4","papermill":{"duration":1.50169,"end_time":"2020-10-30T16:26:45.599507","exception":false,"start_time":"2020-10-30T16:26:44.097817","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install /kaggle/input/iterative-stratification/iterative-stratification-master/\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nseed_everything(seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessor_nn_transfer():\n    variance_threshould = 0.8\n    ncompo_genes = 600\n    ncompo_cells = 50\n    \n    data_dir = '../input/lish-moa/'\n    train_features = pd.read_csv(data_dir + 'train_features.csv')\n    train_targets_scored = pd.read_csv(data_dir + 'train_targets_scored.csv')\n    train_targets_nonscored = pd.read_csv(data_dir + 'train_targets_nonscored.csv')\n    train_drug = pd.read_csv(data_dir + 'train_drug.csv')\n    test_features = pd.read_csv(data_dir + 'test_features.csv')\n    sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n    \n    \n    train_features = train_features[train_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n    test_features = test_features[test_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n    # drop cp_type\n    train_features = train_features.drop('cp_type', axis=1)\n    test_features = test_features.drop('cp_type', axis=1)\n\n    \n    GENES = [col for col in train_features.columns if col.startswith('g-')]\n    CELLS = [col for col in train_features.columns if col.startswith('c-')]\n    \n    # Rank Gauss\n    for col in (GENES + CELLS):\n        transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n        vec_len = len(train_features[col].values)\n        vec_len_test = len(test_features[col].values)\n        raw_vec = train_features[col].values.reshape(vec_len, 1)\n        transformer.fit(raw_vec)\n\n        train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n        test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n    \n    \n    # PCA\n    # g\n    pca_genes = PCA(n_components = ncompo_genes, random_state = seed)\n    pca_genes_train = pca_genes.fit_transform(train_features[GENES])\n    # c\n    pca_cells = PCA(n_components = ncompo_cells, random_state = seed)\n    pca_cells_train = pca_cells.fit_transform(train_features[CELLS])\n    #train\n    pca_genes_train = pd.DataFrame(pca_genes_train, columns = [f\"pca_g-{i}\" for i in range(ncompo_genes)])\n    pca_cells_train = pd.DataFrame(pca_cells_train, columns = [f\"pca_c-{i}\" for i in range(ncompo_cells)])\n    train_features = pd.concat([train_features, pca_genes_train, pca_cells_train], axis = 1)\n    #test\n    pca_genes_test = pca_genes.transform(test_features[GENES])\n    pca_cells_test = pca_cells.transform(test_features[CELLS])\n    \n    pca_genes_test = pd.DataFrame(pca_genes_test, columns = [f\"pca_g-{i}\" for i in range(ncompo_genes)])\n    pca_cells_test = pd.DataFrame(pca_cells_test, columns = [f\"pca_c-{i}\" for i in range(ncompo_cells)])\n    test_features = pd.concat([test_features, pca_genes_test, pca_cells_test], axis = 1)\n\n    \n    # 特征选择\n    cols_numeric = [feat for feat in list(train_features.columns) if feat not in [\"sig_id\", \"cp_time\", \"cp_dose\"]]\n    mask = (train_features[cols_numeric].var() >= variance_threshould).values\n    tmp = train_features[cols_numeric].loc[:, mask]\n    train_features = pd.concat([train_features[[\"sig_id\", \"cp_time\", \"cp_dose\"]], tmp], axis = 1)\n    cols_numeric = [feat for feat in list(train_features.columns) if feat not in [\"sig_id\", \"cp_time\", \"cp_dose\"]]\n    test_features = pd.concat([test_features[[\"sig_id\", \"cp_time\", \"cp_dose\"]], test_features.loc[:,cols_numeric]], axis = 1)\n    \n    # 离散值处理\n    train_features = pd.get_dummies(train_features, columns = ['cp_time', 'cp_dose'])\n    test_features = pd.get_dummies(test_features, columns = ['cp_time', 'cp_dose'])\n\n    train = train_features.merge(train_targets_scored, on='sig_id')\n    train = train.merge(train_targets_nonscored, on='sig_id')\n    train = train.merge(train_drug, on='sig_id')\n    test = test_features\n    \n    # 获取目标类型 列\n    target_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\n    num_targets = len(target_cols) # 目标类型长度\n    \n    return train, test, num_targets, num_aux_targets, num_all_targets, target_cols, aux_target_cols, all_target_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test, num_targets, num_aux_targets, num_all_targets, target_cols, aux_target_cols, all_target_cols = preprocessor_nn_transfer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset 加载类","metadata":{"id":"-4PxBYMVU51p","papermill":{"duration":0.053045,"end_time":"2020-10-30T16:27:41.168745","exception":false,"start_time":"2020-10-30T16:27:41.1157","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)}\n        \n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {'x' : torch.tensor(self.features[idx, :], dtype=torch.float)}\n\n        return dct","metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:41.290886Z","iopub.status.busy":"2020-10-30T16:27:41.289986Z","iopub.status.idle":"2020-10-30T16:27:41.293444Z","shell.execute_reply":"2020-10-30T16:27:41.292871Z"},"id":"Oe4Fyj0rU51q","papermill":{"duration":0.069081,"end_time":"2020-10-30T16:27:41.293559","exception":false,"start_time":"2020-10-30T16:27:41.224478","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 训练数据\ndef train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        # 梯度清零\n        optimizer.zero_grad()\n        # 转移数据到GPU\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        # 模型输出\n        outputs = model(inputs)\n        # 计算损失\n        loss = loss_fn(outputs, targets)\n        # 计算梯度\n        loss.backward()\n        # 反向传播\n        optimizer.step()\n        # 提前停止\n        scheduler.step()\n        # 损失值求和\n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader) # 求平均\n    return final_loss\n\n# 交叉验证\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    # 加载数据\n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    for data in dataloader:\n        inputs = data['x'].to(device)\n        with torch.no_grad():\n            outputs = model(inputs)\n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    return preds","metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:41.419307Z","iopub.status.busy":"2020-10-30T16:27:41.415376Z","iopub.status.idle":"2020-10-30T16:27:41.421889Z","shell.execute_reply":"2020-10-30T16:27:41.422529Z"},"id":"tIvXQTmIU51s","papermill":{"duration":0.074766,"end_time":"2020-10-30T16:27:41.422664","exception":false,"start_time":"2020-10-30T16:27:41.347898","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"id":"2YOmMelYU51x","papermill":{"duration":0.055406,"end_time":"2020-10-30T16:27:41.65766","exception":false,"start_time":"2020-10-30T16:27:41.602254","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# 构建神经网络\nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(Model, self).__init__()\n        # 网络参数\n        self.hidden_size = [1500, 1250, 1000, 750]\n        self.dropout_value = [0.5, 0.35, 0.3, 0.25]\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n        \n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_value[0])\n        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_value[1])\n        self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n\n        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n        self.dropout4 = nn.Dropout(self.dropout_value[2])\n        self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n\n        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n        self.dropout5 = nn.Dropout(self.dropout_value[3])\n        self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.leaky_relu(self.dense3(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = F.leaky_relu(self.dense4(x))\n\n        x = self.batch_norm5(x)\n        x = self.dropout5(x)\n        x = self.dense5(x)\n        return x","metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:41.785992Z","iopub.status.busy":"2020-10-30T16:27:41.784898Z","iopub.status.idle":"2020-10-30T16:27:41.787531Z","shell.execute_reply":"2020-10-30T16:27:41.788132Z"},"id":"NuBU5W3lU51y","papermill":{"duration":0.076376,"end_time":"2020-10-30T16:27:41.78827","exception":false,"start_time":"2020-10-30T16:27:41.711894","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_cols = [c for c in train.columns if c not in all_target_cols] # 取出 非目标的 特征\nfeature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id', 'drug_id']]\nnum_features = len(feature_cols)","metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:42.134029Z","iopub.status.busy":"2020-10-30T16:27:42.13261Z","iopub.status.idle":"2020-10-30T16:27:42.325649Z","shell.execute_reply":"2020-10-30T16:27:42.326231Z"},"executionInfo":{"elapsed":126139,"status":"ok","timestamp":1605114356736,"user":{"displayName":"Vladimir Zhuravlev","photoUrl":"","userId":"16372324542816680996"},"user_tz":-420},"id":"lcFRstDpU515","outputId":"68d18d46-efda-46d4-bfe4-d641bcbc5fe8","papermill":{"duration":0.256088,"end_time":"2020-10-30T16:27:42.32637","exception":false,"start_time":"2020-10-30T16:27:42.070282","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 超参数\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 24\nBATCH_SIZE = 128\n\nWEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6} # 权重衰减\nMAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3} # 最大学习率\nDIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2} # 正则因子\nPCT_START = 0.1 ","metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:42.973092Z","iopub.status.busy":"2020-10-30T16:27:42.9717Z","iopub.status.idle":"2020-10-30T16:27:42.976302Z","shell.execute_reply":"2020-10-30T16:27:42.978092Z"},"id":"J80ID9aaU518","papermill":{"duration":0.596642,"end_time":"2020-10-30T16:27:42.978321","exception":false,"start_time":"2020-10-30T16:27:42.381679","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 模型结构\nmodel = Model(num_features, num_all_targets)","metadata":{"executionInfo":{"elapsed":126118,"status":"ok","timestamp":1605114356745,"user":{"displayName":"Vladimir Zhuravlev","photoUrl":"","userId":"16372324542816680996"},"user_tz":-420},"id":"7d2R5qElopmx","outputId":"747b3d73-b306-48da-f488-9d7e146ce5dd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 单次训练","metadata":{"id":"0hidUPH8U51-","papermill":{"duration":0.149218,"end_time":"2020-10-30T16:27:43.27826","exception":false,"start_time":"2020-10-30T16:27:43.129042","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\n# 进行K折交叉验证\ndef make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n    vc = train.drug_id.value_counts()\n    vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n    vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n\n    for seed_id in SEEDS:\n        kfold_col = 'kfold_{}'.format(seed_id)\n        \n        dct1 = {}\n        dct2 = {}\n\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        train[kfold_col] = train.drug_id.map(dct1)\n        train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n        train[kfold_col] = train[kfold_col].astype('int8')\n        \n    return train\n\nSEEDS = list(range(42,49))\nNFOLDS = 10\nDRUG_THRESH = 18\n\ntrain = make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 开始训练\ndef run_training(fold_id, seed_id):\n    seed_everything(seed_id)\n    \n    train_ = train\n    test_ = test\n    \n    kfold_col = f'kfold_{seed_id}'\n    trn_idx = train_[train_[kfold_col] != fold_id].index\n    val_idx = train_[train_[kfold_col] == fold_id].index\n    \n    train_df = train_[train_[kfold_col] != fold_id].reset_index(drop=True)\n    valid_df = train_[train_[kfold_col] == fold_id].reset_index(drop=True)\n    \n    def train_model(model, tag_name, target_cols_now, fine_tune_scheduler=None):\n        x_train, y_train  = train_df[feature_cols].values, train_df[target_cols_now].values\n        x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols_now].values\n        \n        train_dataset = MoADataset(x_train, y_train)\n        valid_dataset = MoADataset(x_valid, y_valid)\n\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY[tag_name])\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n                                                  steps_per_epoch=len(trainloader),\n                                                  pct_start=PCT_START,\n                                                  div_factor=DIV_FACTOR[tag_name], \n                                                  max_lr=MAX_LR[tag_name],\n                                                  epochs=EPOCHS)\n        \n        loss_fn = nn.BCEWithLogitsLoss()\n        loss_tr = SmoothBCEwLogits(smoothing=0.001)\n\n        oof = np.zeros((len(train), len(target_cols_now)))\n        best_loss = np.inf\n        \n        for epoch in range(EPOCHS):\n            if fine_tune_scheduler is not None:\n                fine_tune_scheduler.step(epoch, model)\n\n            train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n            print(f\"SEED: {seed_id}, FOLD: {fold_id}, {tag_name}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}\")\n\n            if np.isnan(valid_loss):\n                break\n            \n            if valid_loss < best_loss:\n                best_loss = valid_loss\n                oof[val_idx] = valid_preds\n                torch.save(model.state_dict(), f\"{tag_name}_SEED_{seed_id}_FOLD{fold_id}_.pth\")\n\n        return oof\n\n    model = Model(num_features, num_targets)\n    model.load_state_dict(torch.load(f\"../input/nn-transferlearning-7seeds4248-10folds/SCORED_ONLY_SEED_{seed_id}_FOLD{fold_id}_.pth\"))\n    model.to(DEVICE)\n\n    #--------------------- 预测 ---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    predictions = np.zeros((len(test_), num_targets))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return 0.02, predictions","metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:43.533519Z","iopub.status.busy":"2020-10-30T16:27:43.532417Z","iopub.status.idle":"2020-10-30T16:27:43.586422Z","shell.execute_reply":"2020-10-30T16:27:43.585623Z"},"id":"JHTrp23wU51_","papermill":{"duration":0.213528,"end_time":"2020-10-30T16:27:43.586582","exception":false,"start_time":"2020-10-30T16:27:43.373054","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_k_fold(NFOLDS, seed_id):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold_id in range(NFOLDS):\n        oof_, pred_ = run_training(fold_id, seed_id)\n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions","metadata":{"execution":{"iopub.execute_input":"2020-10-30T16:27:43.804473Z","iopub.status.busy":"2020-10-30T16:27:43.803598Z","iopub.status.idle":"2020-10-30T16:27:43.808545Z","shell.execute_reply":"2020-10-30T16:27:43.809223Z"},"id":"GxfyrnG4U52C","papermill":{"duration":0.143086,"end_time":"2020-10-30T16:27:43.809403","exception":false,"start_time":"2020-10-30T16:27:43.666317","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '../input/lish-moa/'\ntrain_targets_scored = pd.read_csv(data_dir + 'train_targets_scored.csv')","metadata":{"execution":{"iopub.execute_input":"2020-10-30T17:02:15.499323Z","iopub.status.busy":"2020-10-30T17:02:15.49831Z","iopub.status.idle":"2020-10-30T17:02:15.526114Z","shell.execute_reply":"2020-10-30T17:02:15.526649Z"},"executionInfo":{"elapsed":4373987,"status":"ok","timestamp":1605118604695,"user":{"displayName":"Vladimir Zhuravlev","photoUrl":"","userId":"16372324542816680996"},"user_tz":-420},"id":"venT86yMU52F","outputId":"c6b013a7-b986-4677-e12b-6a9e965f69fb","papermill":{"duration":0.549441,"end_time":"2020-10-30T17:02:15.526794","exception":false,"start_time":"2020-10-30T17:02:14.977353","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\ny_pred = np.clip(y_pred, 0.0005, 0.9995)\n\nscore = 0\n\nfor i in range(len(target_cols)):\n    score += log_loss(y_true[:, i], y_pred[:, i])\n\nprint(\"CV log_loss: \", score / y_pred.shape[1])","metadata":{"execution":{"iopub.execute_input":"2020-10-30T17:02:17.645921Z","iopub.status.busy":"2020-10-30T17:02:17.644851Z","iopub.status.idle":"2020-10-30T17:02:19.277871Z","shell.execute_reply":"2020-10-30T17:02:19.27717Z"},"executionInfo":{"elapsed":4375017,"status":"ok","timestamp":1605118605755,"user":{"displayName":"Vladimir Zhuravlev","photoUrl":"","userId":"16372324542816680996"},"user_tz":-420},"id":"j1s9_62IU52L","outputId":"afd34d19-0e09-4f4f-efeb-427648cc0f51","papermill":{"duration":2.164653,"end_time":"2020-10-30T17:02:19.277983","exception":false,"start_time":"2020-10-30T17:02:17.11333","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\nsub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.execute_input":"2020-10-30T17:02:21.416721Z","iopub.status.busy":"2020-10-30T17:02:21.415189Z","iopub.status.idle":"2020-10-30T17:02:23.900023Z","shell.execute_reply":"2020-10-30T17:02:23.898834Z"},"id":"Wl2DgUjxU52R","papermill":{"duration":3.028268,"end_time":"2020-10-30T17:02:23.900162","exception":false,"start_time":"2020-10-30T17:02:20.871894","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"saved_path_name = '../input/nn-transferlearning-7seeds4248-10folds/'\noof_TabNet_all = np.load(saved_path_name + 'oof_nn_transfer_all.npy')\nprint(oof_TabNet_all.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}