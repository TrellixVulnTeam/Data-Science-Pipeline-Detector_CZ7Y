{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nimport random\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n#ptimizer=tfa.optimizers.AdamW(lr = 2e-3, weight_decay = 1e-5, clipvalue = 700)\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD, Adam, RMSprop\nfrom keras.layers import Dropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import EarlyStopping,ReduceLROnPlateau\nfrom keras.optimizers import SGD, Adam, RMSprop\n\nimport tensorflow_addons as tfa\n\nfrom keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.merge import concatenate\n\n\nfrom sklearn.metrics import log_loss\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterativestratification/')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=999):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(49)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '../input/lish-moa/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv(PATH+'train_features.csv')\ntest_features =pd.read_csv(PATH+'test_features.csv')\ntrain_targets_nonscored =pd.read_csv(PATH+'train_targets_nonscored.csv')\ntrain_targets_scored =pd.read_csv(PATH+'train_targets_scored.csv')\nsample_submission =pd.read_csv(PATH+'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###################Select Feature#######################\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nID = ['sig_id']\nCP = ['cp_type','cp_time','cp_dose']\nG = [x for x in train_features.columns if 'g-' in x]\nC = [x for x in train_features.columns if 'c-' in x]\nX = train_features[C+G].values\n\nTargets = train_targets_scored.columns[1:].tolist()\nY = train_targets_scored[Targets].values[:,1:]\nmodel = ExtraTreesClassifier(n_estimators=100)\nmodel.fit(X, Y)\nprint(model.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sel_feat= model.feature_importances_.argsort()[-200:][::-1]\nZ=train_features[C+G]\nname_sel_feat= Z.iloc[:,sel_feat].columns\n###################Select Feature#######################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let us check data types of train_features\ntrain_features.select_dtypes(include=['object','category','int','float']).dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Which are the object datatypes\ntrain_features.select_dtypes(include=['object']).dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This means we have have cp_type and cp_dose as categorical variables, also cp_time which has only three values 24, 48 and 72\ntr_features= [col for col in train_features.columns if col!='sig_id']\nprint(\"Length of train features list without 'sig_id' is:\", len(tr_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now use label encoder to convert train_features and test_features df together\nfor train_feat in ['cp_type', 'cp_dose','cp_time']:\n    le = LabelEncoder()\n    le.fit(list(train_features[train_feat].astype(str).values) + list(test_features[train_feat].astype(str).values))\n    train_features[train_feat] = le.transform(list(train_features[train_feat].astype(str).values))\n    test_features[train_feat] = le.transform(list(test_features[train_feat].astype(str).values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let us keep on train_feat columns for NN\n#\ntr_features = name_sel_feat\ntrain = train_features[tr_features]\ntest= test_features[tr_features]\ntrain_target = train_targets_scored.drop(['sig_id'], axis=1)\ntrain_target_aux = train_targets_nonscored.drop(['sig_id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#So label encoding is done, let us move towards building NN\n#We are using StandardScalar to transform all features and feed a balanced input to the neural net\n\nsc = StandardScaler()\n#train = sc.fit_transform(train)\n#test = sc.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len_X =int(train.shape[1])\nprint(len_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_feat_len= train_target.shape[1]\ntarget_aux_len= train_target_aux.shape[1]\n#print(target_feat_len,train_target_aux)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## You can play around with network architecture and check what works\n\n#Using functional API, we will create a model with two outputs. First for scored columns and second for nonscored columns.\n#Nonscored output is forcing model to consider this targets and I \"hope\" that it will produce more better accuracy\n#Using batch normalization and high dropouts so that we prevent overfitting\n\n#9/11 added tfa.layers.WeightNormalization\n\ndef getModel2(input_dim,target_feature_length,target_auxiliary_length):\n    visible = Input(shape=(input_dim,))\n    hidden1 = tfa.layers.WeightNormalization(Dense(875, activation='relu'))(visible)\n    batchnorm1= BatchNormalization()(hidden1)\n    dropout1= Dropout(0.5)(batchnorm1)\n    hidden2 = tfa.layers.WeightNormalization(Dense(1750, activation='relu'))(dropout1)\n    batchnorm2= BatchNormalization()(hidden2)\n    dropout2= Dropout(0.5)(batchnorm2)\n    hidden3 = tfa.layers.WeightNormalization(Dense(100, activation='relu'))(dropout2)\n    batchnorm3= BatchNormalization(name=\"batchnorm3\")(hidden3)\n    dropout3= Dropout(0.5,name=\"dropout3\")(batchnorm3)    \n    output_2 = tfa.layers.WeightNormalization(Dense(target_auxiliary_length, activation='sigmoid',name=\"ouput_2\"))(dropout3)\n    \n    hidden4 = tfa.layers.WeightNormalization(Dense(206, activation='relu',name=\"hidden4\"))(output_2)\n    batchnorm4= BatchNormalization(name=\"batchnorm4\")(hidden4)\n    dropout4= Dropout(0.5,name=\"dropout4\")(batchnorm4)\n    \n    concat1 = tf.keras.layers.Concatenate(name=\"concate_nonscored_feat\")([output_2, dropout4])\n    \n    output1 = tfa.layers.WeightNormalization(Dense(target_feature_length, activation='sigmoid', name=\"outputscore\"))(concat1)\n    output2 = tfa.layers.WeightNormalization(Dense(target_auxiliary_length, activation='sigmoid',name=\"outputnonscore\"))(concat1)\n    model = Model(inputs=visible, outputs=[output1,output2])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a sample model and check summay\nmodelNN= getModel2(len_X,target_feat_len,target_aux_len)\nmodelNN.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def metric(y_true, y_pred):\n    #print('y_true:', y_true.head(3))\n    #print('y_pred:',y_pred.head(3))\n    metrics = []\n    for _target in train_target.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n    return np.mean(metrics)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot model and delete from memory\nplot_model(modelNN, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\ndel modelNN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n#from sklearn.model_selection import MultilabelStratifiedKFold\nNFOLD = 10\n\nBATCH_SIZE=16\nEPOCHS=100\n\npout = np.zeros((test_features.shape[0], target_feat_len))\npaux = np.zeros((test_features.shape[0], target_aux_len))\nfoldmetric=0\n#Train is already a numpy as it went through StandardScalar\ntrain_features = train\n#train_targets needs to be converted to numpy\ntrain_targets = train_target\ntrain_target_aux_np=train_target_aux\n\npred = np.zeros((train_features.shape[0], target_feat_len))\n\ncnt=0\nkf = KFold(n_splits=NFOLD)\n#kf = MultilabelStratifiedKFold(n_splits=NFOLD,shuffle=True, random_state=49)\nfor tr_idx, val_idx in kf.split(train_features):\n#for tr_idx, val_idx in enumerate(kf.split(train_targets,train_targets)):\n    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_delta=0.0001, mode='auto')\n    cnt += 1\n    print(f\"FOLD {cnt}\")\n    #net = getModel(len_X,target_feat_len)\n    net= getModel2(len_X,target_feat_len,target_aux_len)\n    #net.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    net.compile(optimizer = tfa.optimizers.AdamW(lr = 2e-3, weight_decay = 1e-5, clipvalue = 700), loss = 'binary_crossentropy', metrics = ['accuracy'])\n    net.fit(train_features.iloc[tr_idx], [train_targets.iloc[tr_idx],train_target_aux_np.iloc[tr_idx]], batch_size=BATCH_SIZE, epochs=EPOCHS, \n            validation_data=(train_features.iloc[val_idx], [train_targets.iloc[val_idx],train_target_aux_np.iloc[val_idx]]), verbose=0, callbacks=[early_stopping,reduce_lr_loss])\n    print(\"Training for this fold\", net.evaluate(train_features.iloc[tr_idx], [train_targets.iloc[tr_idx],train_target_aux_np.iloc[tr_idx]], verbose=0, batch_size=BATCH_SIZE))\n    print(\"Validation for this fold\", net.evaluate(train_features.iloc[val_idx], [train_targets.iloc[val_idx],train_target_aux_np.iloc[val_idx]], verbose=0, batch_size=BATCH_SIZE))\n    print(\"Predict on validation data for this fold\")\n    pred[val_idx] = net.predict(train_features.iloc[val_idx], batch_size=BATCH_SIZE, verbose=0)[0]\n    fold_metric = metric(train_targets.iloc[val_idx], pd.DataFrame(pred[val_idx],columns=train_target.columns))\n    print(f'OOF Metric log_loss for this FOLD {cnt} : {fold_metric}')\n    foldmetric += fold_metric/ NFOLD\n    print(\"Average metric is:\",foldmetric)\n    #print(f'OOF Metric log_loss for this FOLD {cnt} : {metric(train_targets[val_idx], pred[val_idx])}')\n    print(\"Predict test for nonscored targets\")\n    paux += net.predict(test, batch_size=BATCH_SIZE, verbose=0)[1] / NFOLD\n    print(\"Predict test with scored targets\")\n    pout += net.predict(test, batch_size=BATCH_SIZE, verbose=0)[0] / NFOLD","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(data=pout, columns=train_target.columns)\nsubpaux = pd.DataFrame(data=paux, columns=train_target_aux.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.insert(0, column = 'sig_id', value=sample_submission['sig_id'])\nsubpaux.insert(0, column = 'sig_id', value=sample_submission['sig_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.loc[test_features['cp_type'] == 1, train_target.columns] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nsubpaux.to_csv('subaux.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(submission.shape)\nprint(subpaux.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}