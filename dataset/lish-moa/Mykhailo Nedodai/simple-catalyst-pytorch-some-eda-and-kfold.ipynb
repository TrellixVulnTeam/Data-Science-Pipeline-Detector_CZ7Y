{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 500)\n\nfrom collections import OrderedDict\nimport numpy as np\nfrom matplotlib.pylab import plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom catalyst.contrib.nn.schedulers import OneCycleLRWithWarmup\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom catalyst.dl.callbacks import CriterionCallback\nimport catalyst.dl.utils as utils\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom catalyst.dl import SupervisedRunner\nfrom catalyst.utils import set_global_seed\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\n\nss = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    return df\n\ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''mscl = MinMaxScaler()\nscale_cols = list(train.columns[3:])\ntrain[scale_cols] = mscl.fit_transform(train[scale_cols].values)\ntest[scale_cols] = mscl.transform(test[scale_cols].values)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_pca(train, text=\"PCA\", algo = 'PCA', size = 2):\n    \"\"\"Function visualizating PCA/TSNE\"\"\"\n\n    plt.figure(figsize=(20,8))\n    if algo == 'PCA':\n        pca = PCA(n_components = 2,copy=False)\n    elif algo == 'TSNE':\n        pca = TSNE(n_components = 2)\n    else:\n        print('Unknown algo, using PCA...')\n        pca = PCA(n_components = 2, copy=False)\n        \n    train_pca = pca.fit_transform(train)\n\n    plt.scatter(train_pca[:,0], train_pca[:,1], edgecolor='none', alpha=0.9,\n            cmap=plt.cm.get_cmap('seismic', size))\n    plt.title(text)\n    plt.xlabel('component 1')\n    plt.ylabel('component 2')\n    plt.colorbar()\n\nplot_pca(train.values, algo = \"TSNE\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scale_cols = list(train.columns[3:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_data(train, test):\n    vlas = train.columns.values\n    vlas_t = test.columns.values\n    plt.figure(figsize=(20,8))\n    plt.title(\"Distribution of mean values per row in the train and test set\")\n    sns.distplot(train[vlas].mean(axis=1),color=\"green\", kde=True,bins=100, label='train')\n    sns.distplot(test[vlas_t].mean(axis=1), color='red', kde=True, bins=100, label='test')\n    plt.legend()\n    plt.show()\n    \n    plt.figure(figsize=(20,8))\n    plt.title(\"Distribution of std values per row in the train and test set\")\n    sns.distplot(train[vlas].std(axis=1),color=\"green\", kde=True,bins=100, label='train')\n    sns.distplot(test[vlas_t].std(axis=1), color='red', kde=True, bins=100, label='test')\n    plt.legend()\n    plt.show()\n    \n    plt.figure(figsize=(20,8))\n    plt.title(\"Distribution of max values per row in the train and test set\")\n    sns.distplot(train[vlas].max(axis=1), color=\"green\", kde=True, bins=100, label='train')\n    sns.distplot(test[vlas_t].max(axis=1), color='red', kde=True, bins=100, label='test')\n    plt.legend()\n    plt.show()\n    \n    plt.figure(figsize=(20,8))\n    plt.title(\"Distribution of min values per row in the train and test set\")\n    sns.distplot(train[vlas].min(axis=1), color=\"green\", kde=True, bins=100, label='train')\n    sns.distplot(test[vlas_t].min(axis=1), color='red', kde=True, bins=100, label='test')\n    plt.legend()\n    plt.show()\n    \nplot_data(train[scale_cols], test[scale_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train, X_test, y_train, y_test = train_test_split(train, train_targets, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, train_dataframe, train_target):\n        self.train_df = train_dataframe\n        self.train_target = train_target\n        \n    def __len__(self):\n        return len(self.train_df)\n    \n    def __getitem__(self, index):\n        \n        label = self.train_df[index]\n        target = self.train_target[index]\n        \n        label = torch.FloatTensor(label)\n        target = torch.FloatTensor(target)\n        \n        return {\"label\":label, \"target\":target}\n        \nclass TestDataset(Dataset):\n    def __init__(self, test_df):\n        self.test_df = test_df\n        \n    def __len__(self):\n        return len(self.test_df)\n    \n    def __getitem__(self, index):\n        \n        label = self.test_df[index]\n        label = torch.FloatTensor(label)\n        \n        return {\"label\":label}                                 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n       \n        self.linear1  = nn.Linear(875, 875) # 168\n        self.batchn1  = nn.BatchNorm1d(875)\n        #self.dropout1 = nn.Dropout(0.2)\n        self.prelu1   = nn.PReLU()\n        self.linear2  = nn.Linear(875, 512)\n        self.batchn2  = nn.BatchNorm1d(512)\n        \n        self.prelu3   = nn.PReLU()\n        self.linear3  = nn.Linear(512, 256)\n        self.batchn3  = nn.BatchNorm1d(256)\n        #self.dropout2 = nn.Dropout(0.2)\n        self.prelu4   = nn.PReLU()\n        self.linear5  = nn.Linear(256, 206)\n        \n        \n    \n        \n    def forward(self, x):\n        \n        x = self.linear1(x)\n        x = self.batchn1(x)\n        #x = self.dropout1(x)\n        x = self.prelu1(x)\n        x = self.linear2(x)\n        x = self.batchn2(x)\n        x = self.prelu3(x)\n        x = self.linear3(x)\n        x = self.batchn3(x)\n        #x = self.dropout2(x)\n        x = self.prelu4(x)\n        \n        out = self.linear5(x)   \n        \n        return out\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Classifier()\n#model.to(\"cuda\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\ncriterion = {\n    \"bce\": nn.BCEWithLogitsLoss(),\n}\n#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\nscheduler = OneCycleLRWithWarmup(\n    optimizer,\n    num_steps=2,\n    lr_range=(0.05, 0.0005),\n    warmup_steps=2,\n    momentum_range=(0.85, 0.95))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"runner = SupervisedRunner(\n    input_key=\"label\",\n    output_key = \"pred\",\n    input_target_key = \"target\"\n    #device = \"cuda\"\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=10)\ntest_ds = TestDataset(test.values)\n\ntest_dict = OrderedDict()\ntest_dl = DataLoader(test_ds, batch_size = 1, shuffle = False, num_workers = 1)\n\ntest_dict[\"test\"] = test_dl\n\npredictions = np.zeros((len(test), 206))\n\nfor fold_, (train_index, test_index) in enumerate(kf.split(train)):\n    train_ds = TrainDataset(train.iloc[train_index].values, train_targets.iloc[train_index].values)\n    valid_ds = TrainDataset(train.iloc[test_index].values, train_targets.iloc[test_index].values)\n\n    batch = 64\n\n    train_dl = DataLoader(train_ds, batch_size=batch, shuffle=True, num_workers=1)\n    valid_dl = DataLoader(valid_ds, batch_size=batch, shuffle=False, num_workers=1)\n    \n    data = OrderedDict()\n    data[\"train\"] = train_dl\n    data[\"valid\"] = valid_dl\n    \n    print(\"Fold idx:{}\".format(fold_ + 1))\n    \n    runner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler = scheduler,\n    callbacks=[\n        CriterionCallback(\n            #fields = ['image'],\n            input_key=\"target\",\n            output_key=\"pred\",\n            criterion_key='bce',\n            prefix='loss',\n        ),],\n    loaders=data,\n    logdir=\"run\",\n    load_best_on_end=True,\n    num_epochs=30,\n    verbose=True,)\n    \n    pred = np.vstack(list(map(\n    lambda x: x[\"pred\"].sigmoid().cpu().numpy(), \n    runner.predict_loader(loader=test_dict[\"test\"])\n    )))\n    \n    predictions += pred / kf.n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.plot_metrics(\n    logdir=\"../working\", \n    # specify which metrics we want to plot\n    metrics=[\"loss\", \"metric\"]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss[ss.columns[1:]] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}