{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers as L\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Sequential","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/lish-moa/train_features.csv')\ntest_df = pd.read_csv('../input/lish-moa/test_features.csv')\ntrain_target_df = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\nsample_sub = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\ntarget_cols = train_target_df.columns[1:]\nN_TARGETS = len(target_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.at[train_df['cp_type'].str.contains('ctl_vehicle'),train_df.filter(regex='-.*').columns] = 0.0\n\ntest_df.at[test_df['cp_type'].str.contains('ctl_vehicle'),test_df.filter(regex='-.*').columns] = 0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_df(df, target=False):\n    \n    \n    scaler = MinMaxScaler()\n    df[\"cp_time\"]=scaler.fit_transform(df[\"cp_time\"].values.reshape(-1, 1))\n    \n    df[\"cp_dose\"]=(df[\"cp_dose\"]==\"D1\").astype(int)\n    df[\"cp_type\"]=(df[\"cp_type\"]==\"trt_cp\").astype(int)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = preprocess_df(train_df.drop([\"sig_id\"], axis=1))\ny_train = train_target_df.drop([\"sig_id\"], axis=1)\n\nx_test = preprocess_df(test_df.drop([\"sig_id\"], axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n# plt.figure(figsize=(12, 8))\n# sns.heatmap(x_train[:30].corr())\n# plt.title('Pairwise correlations of the first 50 gene features')\n# plt.show()\n#Correlation matrix for Variables\ncell=train_df.loc[:, train_df.columns.str.startswith('c-')]\ncorr = cell.corr(method='pearson')\n# corr\nf, ax = plt.subplots(figsize=(25, 25))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.8, cbar_kws={\"shrink\": .5})\n\nax = sns.heatmap(corr,linewidths=0.8,cmap=cmap)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Capping outliers\ndef cap_outliers(col):\n    col[col>3]=3\n    col[col<-3]=-3\n    return col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats #Outlier Analysis & Removal\n\n#Filtering all the numeric columns\nnumcols=x_train._get_numeric_data().columns\nall_data_num=x_train.loc[:,numcols]\nall_data_num=x_train.iloc[:,1:]\n\n#z=np.abs(stats.zscore(all_data_num['g-0']))\n#Calculate Z Scores for all the variables. \nall_data_num=x_train.apply(stats.zscore)\n\n#Cap the outliers\nall_data_num=all_data_num.apply(cap_outliers)\n#all_data_num.describe()\n#z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pca_application(df,n_components,pattern):\n    df_p=df.loc[:, df.columns.str.startswith(pattern)]\n    x = StandardScaler().fit_transform(df_p)\n    pca = PCA(n_components=n_components)\n    principalComponents = pca.fit_transform(x)\n    principalDf = pd.DataFrame(data = principalComponents)\n    return principalDf,pca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler #Scaling variables\nfrom sklearn.decomposition import PCA #Dimensionality Reduction\n\n#Calculate principal components separately for GE & CV columns\nprincipalDf_g,pca_g=pca_application(x_train,200,'g-')\nprincipalDf_c,pca_c=pca_application(x_train,30,'c-')\n\n#principalDf_g","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.cumsum(pca_g.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.title('Cumulative Explained Variance for Gene Expression Variable PCAs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.cumsum(pca_c.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.title('Cumulative Explained Variance for Cell Viability Variable PCAs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = x_train.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(upper[column] > 0.90)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = y_train.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nto_drop_y = [column for column in upper.columns if any(upper[column] > 0.90)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(to_drop_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xx_train = x_train.drop(x_train[to_drop], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yy_train = y_train.drop(y_train[to_drop_y], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow_addons as tfa","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_keras_model(input_dim=875, output_dim=206):\n    \n    model = Sequential()\n    model.add(tfa.layers.WeightNormalization((L.Dense(512, input_dim=875, activation=\"elu\"))))\n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(0.5))\n    model.add(tfa.layers.WeightNormalization(L.Dense(256, activation=\"elu\")))\n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(0.3))\n    model.add(tfa.layers.WeightNormalization(L.Dense(256, activation=\"elu\")))\n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(0.3))\n    model.add(tfa.layers.WeightNormalization(L.Dense(256, activation=\"elu\")))\n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(0.3))\n    model.add(tfa.layers.WeightNormalization(L.Dense(206, activation=\"sigmoid\")))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = get_keras_model()\n# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n             loss=\"binary_crossentropy\",\n             metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def multi_log_loss(y_true, y_pred):\n    losses = []\n    for col in y_true.columns:\n        losses.append(log_loss(y_true.loc[:, col], y_pred.loc[:, col]))\n    return np.mean(losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit(x_train, y_train, epochs=80)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = pd.DataFrame({\"loss\": hist.history['accuracy'], \"val_loss\": hist.history['loss'] })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hist.history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ps = model.predict(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ps_df = y_train.copy()\nps_df.iloc[:, : ] = ps\n\ntr_score = multi_log_loss(y_train, ps_df)\n\nprint(f\"Train score: {tr_score}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = sample_sub.copy()\ntest_preds[target_cols] = 0\n\ntest_preds.loc[:,target_cols] = model.predict(x_test)\n\ntest_preds.loc[x_test['cp_type'] == 0, target_cols] = 0\ntest_preds.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}