{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_targets_scored = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\ntrain_targets_nonscored = pd.read_csv(\"../input/lish-moa/train_targets_nonscored.csv\")\ntrain_features = pd.read_csv(\"../input/lish-moa/train_features.csv\")\ntest_features = pd.read_csv(\"../input/lish-moa/test_features.csv\")\nsample_submission = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check if number of unique Ids is equal to the row number"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features['sig_id'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2 types of cp_type"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.cp_type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3 types of time"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.cp_time.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2 types of dose"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.cp_dose.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored.sum()[1:].sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs = train_features[:1][[col for col in train_features.columns if 'g-' in col]].values.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(gs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(sorted(gs))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check data distribution in c columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features['c-0'].plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model"},{"metadata":{},"cell_type":"markdown","source":"> Split the data\n>> It is a multi-label classification problem, thus, to choose how to split based off of that"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install iterative-stratification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\ndf = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\ndf.loc[:, \"kfold\"] = -1\ndf = df.sample(frac=1).reset_index(drop=True)\ntargets = df.drop('sig_id', axis=1).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mskf = MultilabelStratifiedKFold(n_splits=5)\nfor fold_, (trn_, val_) in enumerate(mskf.split(X=df, y=targets)):\n    df.loc[val_, \"kfold\"] = fold_\n\ndf.to_csv(\"train_folds.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pytorch Model"},{"metadata":{},"cell_type":"markdown","source":"Neural Networks is chosen coz of the fact that we should predict on different targets at the same time. "},{"metadata":{},"cell_type":"markdown","source":"In case of the usage of any kind of different boosting algorithms like LightGBM, XGBoost, then we have to build the model for each of the >200 targets. Thus, Neural Nets"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoaDataset:\n    def __init__(self, dataset, features):\n        self.dataset = dataset\n        self.features = features\n    \n    def __len__(self):\n        return self.dataset.shape[0]\n    \n    def __getitem__(self, item):\n        return {\n            \"x\": torch.tensor(self.dataset[item, :], dtype=torch.float),\n            \"y\": torch.tensor(self.features[item, :], dtype=torch.float)\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Engine:\n#     Model, optimizer, and device are fixed, \n#     thus, they are in the init function\n    def __init__(self, model, optimizer, device):\n        self.model = model\n        self.optimizer = optimizer\n        self.device = device\n    \n    @staticmethod\n    def loss_fn(targets, outputs):\n        return nn.BCEWithLogitsLoss()(outputs, targets)\n#     data (batches) can change, thus, data, and model..., \n#     are in different functions\n    def train(self, data_loader):\n        self.model.train()\n        final_loss = 0\n        for data in data_loader:\n            self.optimizer.zero_grad()\n            inputs = data[\"x\"].to(self.device)\n            targets = data[\"y\"].to(self.device)\n            outputs = self.model(inputs)\n            loss = self.loss_fn(targets, outputs)\n            loss.backward()\n            self.optimizer.step()\n            final_loss += loss.item()\n        return final_loss / len(data_loader)\n\n#     validation\n    def validate(self, data_loader):\n        self.model.eval()\n        final_loss = 0\n        for data in data_loader:\n            inputs = data[\"x\"].to(self.device)\n            targets = data[\"y\"].to(self.device)\n            outputs = self.model(inputs)\n            loss = self.loss_fn(targets, outputs)\n            final_loss += loss.item()\n        return final_loss / len(data_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nUSE_GPU = True\n\nif USE_GPU and torch.cuda.is_available():\n    print('using device: cuda')\nelse:\n    print('using device: cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(num_features, 256),\n            nn.BatchNormalization(256),\n            nn.Dropout(0.3),\n            nn.Linear(num_features, 256),\n            nn.BatchNormalization(256),\n            nn.Dropout(0.3),\n            nn.Linear(num_features, 256),\n            nn.BatchNormalization(256),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_targets)\n        )\n    \n    def forward(self, x):\n        x = self.model(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = \"cuda\"\nEPOCHS = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_dummies(data, column):\n    ohe = pd.get_dummies(data[column])\n    ohe_columns = [f\"{column}_{c}\" for c in ohe.columns]\n    ohe.columns = ohe_columns\n    data = data.drop(column, axis=1)\n    data = data.join(ohe)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(df):\n    df = add_dummies(df, \"cp_time\")\n    df = add_dummies(df, \"cp_dose\")\n    df = add_dummies(df, \"cp_type\")\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold):\n    df = pd.read_csv(\"../input/lish-moa/train_features.csv\")\n    df = process_data(df)\n    folds = pd.read_csv(\"../working/train_folds.csv\")\n    \n    targets = folds.drop([\"sig_id\", \"kfold\"], axis=1).columns\n    features = df.drop(\"sig_id\", axis=1).columns\n    \n    df = df.merge(folds, on=\"sig_id\", how=\"left\")\n    train_df = df[df.kfold != fold].reset_index(drop=True)\n    valid_df = df[df.kfold == fold].reset_index(drop=True)\n    \n    x_train = train_df[features].to_array()\n    x_valid = valid_df[features].to_array()\n                     \n    y_train = train_df[features].to_array()\n    y_valid = valid_df[targets].to_array()\n    \n    train_dataset = MoaDataset(x_train, y_train)\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=1024, num_workers=8\n    )\n                     \n    valid_dataset = MoaDataset(x_valid, y_valid)\n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset, batch_size=1024, num_workers=8\n    )\n    model = utils.ModelX(\n        num_features = x_train.shape[1],\n        num_targets = y_train.shape[1],\n        num_layers = params[\"num_layers\"],\n        hidden_size = params[\"hidden_size\"],\n        dropout =  params[\"dropout\"]\n    )\n    model.to(DEVICE)\n    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, patience=3, threshold=0.00001, mode=\"min\", verbose=True)\n                     \n    eng = Engine(\n        model, optimizer, device=DEVICE\n    )\n    best_loss = np.inf\n    early_stopping = 10\n    early_stopping_counter = 0\n    for _ in range(EPOCHS):\n        train_loss = engine.train(train_loader)\n        valid_loss = engine.train(valid_loader)\n        scheduler.step(valid_loss)\n        print(f\"{fold}, {epoch}, {train_loss}, {valid_loss}\")\n        if valid_loss < best_loss:\n            best_loss = valid_loss\n            if save_model:\n                torch.save(model.state_dict(), f\"model{fold}.bin\")\n        else:\n            early_stopping_counter += 1\n        if early_stopping_counter > early_stopping:\n            break\n    return best_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import optuna\nclass ModelX(nn.Module):\n    def __init__(self, num_features, num_targets, num_layers, hidden_size, dropout):\n        super().__init__()\n        layers = []\n        for _ in range(num_layers):\n            if len(layers) == 0:\n                layers.append(nn.Linear(num_features, hidden_size))\n                layers.append(nn.BatchNorm1d(hidden_size))\n                layers.append(nn.Dropout(dropout))\n                nn.ReLU()\n            else:\n                layers.append(nn.Linear(hidden_size, hidden_size))\n                layers.append(nn.BatchNorm1d(hidden_size))\n                layers.append(nn.Dropout(dropout))\n                nn.ReLU()\n        layers.append(nn.Linear(hidden_size, num_targets))\n        self.model = nn.Sequential(*layers)\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}