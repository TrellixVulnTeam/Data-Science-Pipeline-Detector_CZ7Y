{"cells":[{"metadata":{},"cell_type":"markdown","source":"## ２つのモデルだけを平均　"},{"metadata":{"_uuid":"4b275534-e9fc-4851-b107-c019446174c0","_cell_guid":"17fef6b1-b7f2-4f92-a428-9ab2c8da956b","trusted":true},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n# [Mechanisms of Action (MoA) Prediction](https://www.kaggle.com/c/lish-moa)"},{"metadata":{"_uuid":"89b50957-8e3e-46e9-bbf4-843156c6bdc5","_cell_guid":"f92ea90b-f2dc-4c78-835c-635df5323cc7","trusted":true},"cell_type":"markdown","source":"### I use the notebook [Pytorch CV|0.0145| LB| 0.01839 |](https://www.kaggle.com/riadalmadani/pytorch-cv-0-0145-lb-0-01839) from [riadalmadani](https://www.kaggle.com/riadalmadani) as a basis and will try to tune its various parameters."},{"metadata":{"_uuid":"075cec22-d9e8-4e97-9ace-ecfc5730cab0","_cell_guid":"dc871e7d-9a6c-48d1-af6c-1588392c2ef4","trusted":true},"cell_type":"markdown","source":"# Acknowledgements\n\n* [MoA | Pytorch | 0.01859 | RankGauss | PCA | NN](https://www.kaggle.com/kushal1506/moa-pytorch-0-01859-rankgauss-pca-nn)\n* [[MoA] Pytorch NN+PCA+RankGauss](https://www.kaggle.com/nayuts/moa-pytorch-nn-pca-rankgauss)\n* [Pytorch CV|0.0145| LB| 0.01839 |](https://www.kaggle.com/riadalmadani/pytorch-cv-0-0145-lb-0-01839)\n* [[New Baseline] Pytorch | MoA](https://www.kaggle.com/namanj27/new-baseline-pytorch-moa)\n* [Deciding (n_components) in PCA](https://www.kaggle.com/kushal1506/deciding-n-components-in-pca)\n* [Titanic - Featuretools (automatic FE&FS)](https://www.kaggle.com/vbmokin/titanic-featuretools-automatic-fe-fs)\n* tuning and visualization from [Higher LB score by tuning mloss - upgrade & visual](https://www.kaggle.com/vbmokin/higher-lb-score-by-tuning-mloss-upgrade-visual)\n* [Data Science for tabular data: Advanced Techniques](https://www.kaggle.com/vbmokin/data-science-for-tabular-data-advanced-techniques)"},{"metadata":{"_uuid":"4396add6-a11d-49ba-a6ab-65fac249c21b","_cell_guid":"ccf252fa-8a28-4ae4-b163-f04c8fdbc3d1","trusted":true},"cell_type":"markdown","source":"### My upgrade:\n\n* PCA parameters\n* Feature Selection methods\n* Dropout\n* Structuring of the notebook\n* Tuning visualization\n* Number of folds\n\nI used the code from sources (please see above). But I am planning to develop this notebook. There are still promising areas for improvement and research of parameters."},{"metadata":{"_uuid":"d290fe24-3a7a-4c9b-9bf4-ae8d2ada8c90","_cell_guid":"d9fddac3-6a4b-4e4b-9a24-a259f0fc713b","trusted":true},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0.1\"></a>\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [My upgrade](#2)\n    -  [Commit now](#2.1)\n    -  [Previous commits](#2.2)\n    -  [Parameters and LB score visualization](#2.3)\n1. [Download data](#3)\n1. [FE & Data Preprocessing](#4)\n    - [RankGauss](#4.1)\n    - [Seed](#4.2)    \n    - [PCA features](#4.3)\n    - [Feature selection](#4.4)\n    - [CV folds](#4.5)\n    - [Dataset Classes](#4.6)\n    - [Smoothing](#4.7)\n    - [Preprocessing](#4.8)\n1. [Modeling](#5)\n1. [Prediction & Submission](#6)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cab3cf9d-b2c6-45a9-a139-589d721242ab","_cell_guid":"e2803845-a9b8-427d-999d-a535a01027b7","trusted":true},"cell_type":"markdown","source":"## 1. Import libraries<a class=\"anchor\" id=\"1\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"95ad1aab-49c6-49a4-96c7-daf7399f208b","_cell_guid":"35bca189-f29a-4cf1-ae9d-c90e9f3d8199","trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterativestratification')\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport os\nimport copy\nimport gc\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nos.listdir('../input/lish-moa')\n\npd.set_option('max_columns', 2000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a4c77e3-d520-442a-8534-d6c47ea5b580","_cell_guid":"69eea2c4-378c-4aa2-a889-8d685a680aab","trusted":true},"cell_type":"markdown","source":"## 2. My upgrade <a class=\"anchor\" id=\"2\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"69b0538f-7ee0-4672-944b-784e450429e1","_cell_guid":"1f48ea76-a02f-406d-8bd7-750fb965cab0","trusted":true},"cell_type":"markdown","source":"### 2.1. Commit now <a class=\"anchor\" id=\"2.1\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"15136571-51c6-4669-bafb-8b5163a8e035","_cell_guid":"4684bbb8-4588-4851-b0c2-9a6c7059c4f6","trusted":true},"cell_type":"code","source":"n_comp_GENES = 463\nn_comp_CELLS = 60\nVarianceThreshold_for_FS = 0.9\nDropout_Model = 0.25\nLEARNING_RATE_NEW = 5e-4\nprint('n_comp_GENES', n_comp_GENES, 'n_comp_CELLS', n_comp_CELLS, 'total', n_comp_GENES + n_comp_CELLS)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf3d3fd6-df5b-42ff-b23e-f2cc3b34c5ea","_cell_guid":"a7af9b1b-5281-4f90-a36e-8a7898be94d8","trusted":true},"cell_type":"markdown","source":"### 2.2 Previous commits <a class=\"anchor\" id=\"2.2\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"9ae5981c-b592-47e8-bc11-5e7ef936283c","_cell_guid":"a6250dcf-edb4-4d8b-8f0c-4c3079651c09","trusted":true},"cell_type":"code","source":"commits_df = pd.DataFrame(columns = ['n_commit', 'n_comp_GENES', 'n_comp_CELLS', 'train_features','VarianceThreshold_for_FS', 'Dropout_Model', 'LB_score', 'CV_logloss'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f9b0098-5d44-4a41-b57d-10524722e6b6","_cell_guid":"ffe82d84-3673-4bd6-9612-53fce5b5bcd2","trusted":true},"cell_type":"markdown","source":"### Commit 0 (parameters from https://www.kaggle.com/riadalmadani/pytorch-cv-0-0145-lb-0-01839, commit 8)"},{"metadata":{"_uuid":"b63e82f8-95b5-4534-ad9d-7756891c5ce5","_cell_guid":"3264fa9a-a623-48e5-8407-8ca9b813bd5e","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"n=0\ncommits_df.loc[n, 'n_commit'] = 0                       # Number of commit\ncommits_df.loc[n, 'n_comp_GENES'] = 600                 # Number of output features for PCA for g-features\ncommits_df.loc[n, 'n_comp_CELLS'] = 50                  # Number of output features for PCA for c-features\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.8     # Threshold for VarianceThreshold for feature selection\ncommits_df.loc[n, 'train_features'] = 1245              # Number features in the training dataframe after FE and before modeling\ncommits_df.loc[n, 'Dropout_Model'] = 0.2619422201258426 # Dropout in Model\ncommits_df.loc[n, 'CV_logloss'] = 0.01458269555140327   # Result CV logloss metrics\ncommits_df.loc[n, 'LB_score'] = 0.01839                 # LB score after submitting","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a824b2b2-5298-44b2-b90b-41d7c5746035","_cell_guid":"f720d381-63b1-416f-9df8-ade69d60feed","trusted":true},"cell_type":"markdown","source":"### Commit 4"},{"metadata":{"_uuid":"155afbce-0e3c-4364-ab19-af19ff23313f","_cell_guid":"713cf268-a965-4ec8-b637-f62dbf790aee","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"n=1\ncommits_df.loc[n, 'n_commit'] = 4\ncommits_df.loc[n, 'n_comp_GENES'] = 610\ncommits_df.loc[n, 'n_comp_CELLS'] = 55\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.82\ncommits_df.loc[n, 'train_features'] = 1240\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'CV_logloss'] =  0.014584545081734047\ncommits_df.loc[n, 'LB_score'] = 0.01839","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a90b1708-4eae-45f6-ac0c-350d889cc760","_cell_guid":"4eb609fd-3604-48c2-b8d8-4fa956a44263","trusted":true},"cell_type":"markdown","source":"### Commit 5"},{"metadata":{"_uuid":"47df4247-a7c9-4887-8154-ef67d5822549","_cell_guid":"b88453be-f19b-40ad-9ac9-b49076e5ea05","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"n=2\ncommits_df.loc[n, 'n_commit'] = 5\ncommits_df.loc[n, 'n_comp_GENES'] = 670\ncommits_df.loc[n, 'n_comp_CELLS'] = 67\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.67\ncommits_df.loc[n, 'train_features'] = 1298\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'CV_logloss'] =  0.014588561242139069\ncommits_df.loc[n, 'LB_score'] = 0.01840","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"800416fb-cd91-423e-a44a-9877f77525f5","_cell_guid":"0033f8d5-36c2-4d4b-984f-8a70ff27bb39","trusted":true},"cell_type":"markdown","source":"### Commit 6"},{"metadata":{"_uuid":"7827e3fe-c499-4a45-b4be-9992e1ab90d5","_cell_guid":"662265fe-97fd-4ed0-9d46-0c0d20c4daa4","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"n=3\ncommits_df.loc[n, 'n_commit'] = 6\ncommits_df.loc[n, 'n_comp_GENES'] = 450\ncommits_df.loc[n, 'n_comp_CELLS'] = 45\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.67\ncommits_df.loc[n, 'train_features'] = 1297\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'CV_logloss'] =  0.014586229676302227\ncommits_df.loc[n, 'LB_score'] = 0.01840","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45fc4ccc-6725-4976-b1dc-5ab55ed6dadb","_cell_guid":"bb81cfd3-0e43-4429-8ad4-fe691c9e7207","trusted":true},"cell_type":"markdown","source":"### Commit 9"},{"metadata":{"_uuid":"7e2eee55-65a6-4491-879a-e5c532a8fc6e","_cell_guid":"e439c16d-9670-4402-ac8e-5e59d4c179e0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"n=4\ncommits_df.loc[n, 'n_commit'] = 9\ncommits_df.loc[n, 'n_comp_GENES'] = 463\ncommits_df.loc[n, 'n_comp_CELLS'] = 60\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.9\ncommits_df.loc[n, 'train_features'] = 1219\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'CV_logloss'] =  0.014572358066092783\ncommits_df.loc[n, 'LB_score'] = 0.01839","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fd4985e-ba9b-4c44-92df-62bd69629f7e","_cell_guid":"48e9efcf-1bf5-480e-b27c-de7441ca89e1","trusted":true},"cell_type":"markdown","source":"### Commit 10"},{"metadata":{"_uuid":"1ca35880-aa3b-41c5-99b0-61ee7b7ddb23","_cell_guid":"d94b92c2-3d45-4896-9c81-e1cf019470b7","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"n=5\ncommits_df.loc[n, 'n_commit'] = 10\ncommits_df.loc[n, 'n_comp_GENES'] = 463\ncommits_df.loc[n, 'n_comp_CELLS'] = 80\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.92\ncommits_df.loc[n, 'train_features'] = 1214\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'CV_logloss'] =  0.014571552074579226\ncommits_df.loc[n, 'LB_score'] = 0.01841","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdd20027-7b06-4f5a-9295-ed6eb8c236d5","_cell_guid":"c533483b-a283-495c-a48f-813e6aef0885","trusted":true},"cell_type":"markdown","source":"### Commit 12"},{"metadata":{"_uuid":"4a229333-531b-4f40-9dcb-e392b67fdeb6","_cell_guid":"d18dccd2-0476-41b1-a395-e4b49b7c0a4e","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"n=6\ncommits_df.loc[n, 'n_commit'] = 12\ncommits_df.loc[n, 'n_comp_GENES'] = 450\ncommits_df.loc[n, 'n_comp_CELLS'] = 65\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.9\ncommits_df.loc[n, 'train_features'] = 1219\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'CV_logloss'] = 0.01458043214513875\ncommits_df.loc[n, 'LB_score'] = 0.01840","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66fcbc38-1891-4cae-8b1a-3ace5e1470fd","_cell_guid":"8c351e13-94bc-4240-b910-798a7251df51","trusted":true},"cell_type":"markdown","source":"### Commit 13"},{"metadata":{"_uuid":"a977b3be-1275-4941-b1cc-e73cfffc1d00","_cell_guid":"30b61c79-836d-4209-87b9-46f4689d5202","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"n=7\ncommits_df.loc[n, 'n_commit'] = 13\ncommits_df.loc[n, 'n_comp_GENES'] = 463\ncommits_df.loc[n, 'n_comp_CELLS'] = 60\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.9\ncommits_df.loc[n, 'train_features'] = 1219\ncommits_df.loc[n, 'Dropout_Model'] = 0.4\ncommits_df.loc[n, 'CV_logloss'] = 0.014625250378417162\ncommits_df.loc[n, 'LB_score'] = 0.01844","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cac8e7f3-20e0-4dbf-a58c-fb16edc93ee5","_cell_guid":"6d642fbf-a49d-4eaa-b90c-7f254f9c85c4","trusted":true},"cell_type":"markdown","source":"### Commit 14"},{"metadata":{"_uuid":"cc8ec0e7-d243-4b9a-9507-0c5ee1d1094b","_cell_guid":"4d77c864-6896-484e-97a7-cd7b1bf4a1ec","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"n=8\ncommits_df.loc[n, 'n_commit'] = 14\ncommits_df.loc[n, 'n_comp_GENES'] = 463\ncommits_df.loc[n, 'n_comp_CELLS'] = 60\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.01\ncommits_df.loc[n, 'train_features'] = 1604\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'CV_logloss'] = 0.014713482787703418\ncommits_df.loc[n, 'LB_score'] = 0.01849","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a25b1bc8-81bf-4eab-ac05-9b9b23a3d8a7","_cell_guid":"3cdbb981-5064-44d4-b1ef-59624331c6c8","trusted":true},"cell_type":"markdown","source":"### Commit 18"},{"metadata":{"_uuid":"044ce17a-bebb-48ac-8beb-8386d78d8492","_cell_guid":"38095515-a03d-499c-92f4-0e7677100ea5","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"n=9\ncommits_df.loc[n, 'n_commit'] = 18\ncommits_df.loc[n, 'n_comp_GENES'] = 363\ncommits_df.loc[n, 'n_comp_CELLS'] = 60\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.9\ncommits_df.loc[n, 'train_features'] = 1219\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'CV_logloss'] = 0.014568689235607534\ncommits_df.loc[n, 'LB_score'] = 0.01841","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4b97634-b17d-4a18-8e79-38e7060ea9ae","_cell_guid":"92d5c30d-da7f-40ce-8d01-1b507452af1e","trusted":true},"cell_type":"markdown","source":"### Commit 19"},{"metadata":{"_uuid":"b682af2a-f27f-4988-a2c6-dcc1c310c05a","_cell_guid":"c63b3a0b-fafd-4d16-9b2a-dfa83b15ce51","trusted":true},"cell_type":"code","source":"n=10\ncommits_df.loc[n, 'n_commit'] = 19\ncommits_df.loc[n, 'n_comp_GENES'] = 550\ncommits_df.loc[n, 'n_comp_CELLS'] = 60\ncommits_df.loc[n, 'VarianceThreshold_for_FS'] = 0.91\ncommits_df.loc[n, 'train_features'] = 1218\ncommits_df.loc[n, 'Dropout_Model'] = 0.25\ncommits_df.loc[n, 'CV_logloss'] = 0.014577509066710863\ncommits_df.loc[n, 'LB_score'] = 0.01841","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"764427ad-78f3-4eb1-8d68-4b04e207e2b7","_cell_guid":"9331674a-84d6-4ea6-b581-13a8f17ef85b","trusted":true},"cell_type":"markdown","source":"### 2.3 Parameters and LB score visualization <a class=\"anchor\" id=\"2.3\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"b92c5c84-46d3-4f87-abc0-4bba39f8b409","_cell_guid":"aa217dd4-de41-4508-8773-0a34ebee71b9","trusted":true},"cell_type":"code","source":"commits_df['n_comp_total'] = commits_df['n_comp_GENES'] + commits_df['n_comp_CELLS']\ncommits_df['seed'] = 42\ncommits_df['l_rate'] = 1e-3","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7db2636c-8589-411f-b26b-c4c8d870810f","_cell_guid":"80a8e9e8-382d-4195-9bdb-4709ecec4d94","trusted":true},"cell_type":"code","source":"# Find and mark minimun value of LB score\ncommits_df['LB_score'] = pd.to_numeric(commits_df['LB_score'])\ncommits_df = commits_df.sort_values(by=['LB_score', 'CV_logloss'], ascending = True).reset_index(drop=True)\ncommits_df['min'] = 0\ncommits_df.loc[0, 'min'] = 1\ncommits_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87735194-16bb-44fb-b5ee-a62268a482f9","_cell_guid":"c0fb44cc-2bdc-418d-9c09-7f68d908a6b7","trusted":true},"cell_type":"code","source":"commits_df.sort_values(by=['CV_logloss'], ascending = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a90d7aa-7378-44ba-9428-3748bfa1a8b8","_cell_guid":"c603a4cb-ba85-4386-98cd-a6ab78966768","trusted":true},"cell_type":"code","source":"# Interactive plot with results of parameters tuning\nfig = px.scatter_3d(commits_df, x='n_comp_GENES', y='n_comp_CELLS', z='LB_score', color = 'min', \n                    symbol = 'Dropout_Model',\n                    title='Parameters and LB score visualization of MoA solutions')\nfig.update(layout=dict(title=dict(x=0.1)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5871c979-3313-45b1-97fb-f3ed92b926d8","_cell_guid":"c890e926-b7bb-454a-bf5f-e183f66e010e","trusted":true},"cell_type":"code","source":"# Interactive plot with results of parameters tuning\nfig = px.scatter_3d(commits_df, x='train_features', y='VarianceThreshold_for_FS', z='LB_score', color = 'min', \n                    symbol = 'seed',\n                    title='Parameters and LB score visualization of MoA solutions')\nfig.update(layout=dict(title=dict(x=0.1)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ff99cea-4bf4-4efd-a8d2-8b699763922e","_cell_guid":"9e2ba16c-aff3-4551-82bf-b02210c9fb87","trusted":true},"cell_type":"code","source":"# Interactive plot with results of parameters tuning\nfig = px.scatter_3d(commits_df, x='train_features', y='CV_logloss', z='LB_score', color = 'min', \n                    symbol = 'l_rate',\n                    title='Parameters and LB score visualization of MoA solutions')\nfig.update(layout=dict(title=dict(x=0.1)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02594170-7e5e-4df9-ab15-a80904a28262","_cell_guid":"58c8b38e-9b7a-4d7e-8004-e6176029da22","trusted":true},"cell_type":"code","source":"# Interactive plot with results of parameters tuning\ncommits_df_1841 = commits_df[commits_df.LB_score <= 0.01841]\nfig = px.scatter_3d(commits_df_1841, x='train_features', y='CV_logloss', z='LB_score', color = 'min', \n                    symbol = 'l_rate',\n                    title='Parameters and LB score visualization of MoA solutions')\nfig.update(layout=dict(title=dict(x=0.1)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41855d9b-9f63-467e-8a82-6ec9e41e8872","_cell_guid":"8d10bb42-f808-477e-8c58-2cd3f7aa14ee","trusted":true},"cell_type":"markdown","source":"### It is recommended:\n* **n_comp_GENES** smaller, \n* **n_comp_CELLS** more,\n* **VarianceThreshold_for_FS** more, so that **train_features** is less."},{"metadata":{"_uuid":"265d35f0-7829-4958-8542-03b4a2a8a4b9","_cell_guid":"ed333efd-626f-4486-9e03-e3fc0a0e30d1","trusted":true},"cell_type":"markdown","source":"## 3. Download data<a class=\"anchor\" id=\"3\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"c8bae1c3-b7e0-459e-818f-ca306dda80e2","_cell_guid":"c0fb6b31-562f-47fb-9cc5-a85f274ba85c","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2dbe95dd-6bf6-40a5-a1d0-814e04047068","_cell_guid":"fe618d09-921a-466c-b65d-c349df139908","trusted":true},"cell_type":"markdown","source":"## 4. FE & Data Preprocessing <a class=\"anchor\" id=\"4\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"3b0110a3-a520-4cf4-a3df-5b0df18d3a0b","_cell_guid":"b8868b16-1d9c-4b49-8d7f-945421a69ac3","trusted":true},"cell_type":"code","source":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffe8419d-7d81-451f-a9f9-bec94109536a","_cell_guid":"12d374ef-720f-43f7-a0ae-989896a438fb","trusted":true},"cell_type":"markdown","source":"### 4.1 RankGauss<a class=\"anchor\" id=\"4.1\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"05389fca-8aab-4383-a13e-3c8d06563a38","_cell_guid":"c9f8a0bd-7166-4eb8-ac8e-a4a236487aa5","trusted":true},"cell_type":"code","source":"# RankGauss - transform to Gauss\n\nfor col in (GENES + CELLS):\n\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"609e8815-f0d1-4c5c-b760-840fc545b298","_cell_guid":"f81bedeb-029d-48f4-bf69-f0fabad38fa0","trusted":true},"cell_type":"markdown","source":"### 4.2 Seed<a class=\"anchor\" id=\"4.2\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"466a1496-41ca-457c-b17f-add6cbcf4971","_cell_guid":"649916cb-4461-4283-bd63-1a28e9695c27","trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7258eed-4970-432a-87c5-570ff597d6db","_cell_guid":"52cf3a85-5cdf-4118-bdbf-f9a57e85c92e","trusted":true},"cell_type":"markdown","source":"### 4.3 PCA features<a class=\"anchor\" id=\"4.3\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"011165d6-467f-4180-b142-ea71a4d9d0c1","_cell_guid":"5f559ace-ac36-4b7e-9742-b9374f03952f","trusted":true},"cell_type":"code","source":"len(GENES)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36c4d0e5-1bf1-47e9-980f-608ab49b4d88","_cell_guid":"6fe49d91-03e4-46b0-8c2b-8b8692de370d","trusted":true},"cell_type":"code","source":"# GENES\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp_GENES, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp_GENES)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp_GENES)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c330365-1143-4564-aa28-05ba825e3446","_cell_guid":"ff2da03f-5588-4615-98d5-6cb42e1413d2","trusted":true},"cell_type":"code","source":"len(CELLS)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2eba01b-e042-483d-84b6-5b73a9a13831","_cell_guid":"fa570173-de0a-4bd3-8a7b-fd31c245e9e9","trusted":true},"cell_type":"code","source":"# CELLS\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp_CELLS, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp_CELLS)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp_CELLS)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68627520-51e6-4f2e-8a8a-216282d3c570","_cell_guid":"4e74094e-948f-4f5a-a22d-c334441f438b","trusted":true},"cell_type":"code","source":"train_features.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afd578c4-9e7c-4f33-9fc9-771f1d2acf97","_cell_guid":"2bbf606e-30f5-4fbd-9d24-a31fbe2bb203","trusted":true},"cell_type":"code","source":"train_features.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a41ce9c9-4c5b-4957-bb91-0f0be0f4e5d1","_cell_guid":"f6f2932f-1d90-45e2-b471-ad0a13028b7c","trusted":true},"cell_type":"markdown","source":"### 4.4 Feature selection<a class=\"anchor\" id=\"4.4\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"03ddc51d-b28e-42da-b793-ee220f8350dd","_cell_guid":"18031031-d5d3-43cf-bcbd-d8a6654ff53f","trusted":true},"cell_type":"code","source":"data = train_features.append(test_features)\ndata","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a270cd35-a834-4037-be6b-c6fb123e4a6c","_cell_guid":"e897eee4-896c-4bd9-8cbf-a5e54d2cdc0a","trusted":true},"cell_type":"code","source":"var_thresh = VarianceThreshold(VarianceThreshold_for_FS)\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de3de22e-8a7e-424a-ab2a-6ecd5f1c0473","_cell_guid":"38bd7a59-37a5-4c25-b306-920323847dd5","trusted":true},"cell_type":"code","source":"train_features.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1873f777-7d0b-43d8-99c4-62d4a9fa319c","_cell_guid":"05e13b2b-0f47-4635-a758-5e5a4cae795b","trusted":true},"cell_type":"code","source":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_scored.columns]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3121cb4a-cd20-4a09-b318-19d4a75af829","_cell_guid":"3862e36a-a410-4994-89dd-5f628f4c024a","trusted":true},"cell_type":"code","source":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"548418cc-0a38-4987-9256-977060b00049","_cell_guid":"45f64871-8a77-47f2-93e4-0d8113c9626a","trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7af1b7b3-8768-4c64-ab1e-e9400ded9328","_cell_guid":"86fc363c-8d7a-4593-a340-5cf1942101e3","trusted":true},"cell_type":"code","source":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f8d9eab-726b-4b17-af2f-2c8bb199c370","_cell_guid":"cefcd10c-bf8d-45a3-a355-f67b9ed18e29","trusted":true},"cell_type":"markdown","source":"### 4.5 CV folds<a class=\"anchor\" id=\"4.5\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"a02486bc-cd9f-45d2-9fa0-93adae5e8978","_cell_guid":"a2bfeade-e2c4-4e95-8c78-7b5a9d4969c0","trusted":true},"cell_type":"code","source":"folds = train.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=7)\n#mskf = MultilabelStratifiedKFold(n_splits=2) #変更\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)\nfolds","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"493bf907-c0ac-4eec-a7d2-48b7c16e3c0c","_cell_guid":"7d38ad7b-9705-4f40-8a41-dfb7d0746842","trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\nprint(sample_submission.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2883130c-0592-49fa-910a-07fa81974206","_cell_guid":"56b6c9d2-afb7-4104-98b5-91570f2846d9","trusted":true},"cell_type":"markdown","source":"### 4.6 Dataset Classes<a class=\"anchor\" id=\"4.6\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"4811b007-e94f-40e1-9cd9-71a1f2e140d3","_cell_guid":"18f03f3d-a393-46e5-926b-279f794bd911","trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecf88e32-1a00-45cb-850c-f1cebe91595d","_cell_guid":"a1a887f5-051e-4b55-92a2-294ef048e006","trusted":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"988772f2-f6df-4a28-861f-7a9593b4b24b","_cell_guid":"b30644a7-8c14-49aa-a051-8a4ceca781a0","trusted":true},"cell_type":"markdown","source":"### 4.7 Smoothing<a class=\"anchor\" id=\"4.7\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"638a8e99-665e-4b79-9360-4c82c0912f43","_cell_guid":"e05c92fd-dafc-4dda-b87e-eedf04ac9c83","trusted":true},"cell_type":"code","source":"class SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f22dfa9-de6c-4265-919c-ac38ffe4c5ca","_cell_guid":"fcd03233-bb23-49d4-8bf9-528b57e43df3","trusted":true},"cell_type":"markdown","source":"### 4.8 Preprocessing<a class=\"anchor\" id=\"4.8\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"2f6a46b3-ead3-4493-ad55-908db2da2184","_cell_guid":"4f36aedd-9759-4701-a475-975b29d3df9a","trusted":true},"cell_type":"code","source":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5f180d8-5255-4254-9112-450655c76321","_cell_guid":"2635f1ff-d4cf-4b6b-ab0d-c308b830b83c","trusted":true},"cell_type":"code","source":"feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6691f88f-d964-4f96-94a3-2c4c37939623","_cell_guid":"5055686b-341a-4a8a-9e57-6139638226e9","trusted":true},"cell_type":"markdown","source":"## 5. Modeling<a class=\"anchor\" id=\"5\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"3ba793a6-8b1a-4f18-b572-9d7a03ce009a","_cell_guid":"a5c8152e-c84f-48f8-8cd0-3a7812383e70","trusted":true},"cell_type":"code","source":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\n#EPOCHS = 2 #変更\nBATCH_SIZE = 128\nLEARNING_RATE = LEARNING_RATE_NEW\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7\n#NFOLDS = 2 #変更\n\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=1500","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21d5d968-8c89-41fe-a0d7-9b6138ba5e8d","_cell_guid":"6d0a1d3e-6b5d-48f5-95d2-f5d4a465ae14","trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(Dropout_Model)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(Dropout_Model)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x\n    \nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dd18701-0c82-4c56-ab7c-a58bc28572b3","_cell_guid":"dffe9a6b-11a9-4836-bcc2-30c1d640437b","trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n   \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"100bd7a7-2942-4082-a1d1-968f57a4ea16","_cell_guid":"fe1106bd-87df-409c-abb5-ca9bd60c8587","trusted":true},"cell_type":"markdown","source":"## 6. Prediction & Submission <a class=\"anchor\" id=\"6\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"bef08f7f-4cc4-404f-aee4-e1ff2d971416","_cell_guid":"d24d24ef-4d21-4ef6-8de5-0248dade24c8","trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ba7827b-9599-484e-af9f-5e7624d6e57c","_cell_guid":"8c0f87f5-9f7f-475a-9c24-b1a618861c88","trusted":true},"cell_type":"code","source":"# Averaging on multiple SEEDS\n\nSEED = [0, 1, 2, 3, 4, 5, 6]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65de059f-e82e-4c00-86fe-c08d3a3cee04","_cell_guid":"bd052b7b-b017-4c8c-97d8-c4689b7b00d0","trusted":true},"cell_type":"code","source":"train_targets_scored","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"888f9da0-d5d7-431b-855b-08d158e96c2e","_cell_guid":"d10d2ba7-b009-4b15-8ff7-66f1d60f4947","trusted":true},"cell_type":"code","source":"len(target_cols)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbe3598f-d99c-46cd-ba4e-e206cd1d2a60","_cell_guid":"1aa7479d-12e0-4b78-bcb9-5495bcddf997","trusted":true},"cell_type":"code","source":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ / target.shape[1]\n    \nprint(\"CV log_loss: \", score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c9fb04c-d7a2-41b4-b2b0-76ac6e6acdb2","_cell_guid":"12522ff6-db3b-4ad9-9104-22ac40dac3c1","trusted":true},"cell_type":"code","source":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebe1cc53-64a9-465e-8f1b-97f535c553ef","_cell_guid":"6529d2df-57c6-4c89-a506-ae72d104e96d","trusted":true},"cell_type":"code","source":"sub.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ee9cfe2-2ffc-4a21-acf4-754a919ff356","_cell_guid":"5f113a4b-dde0-457a-9a52-22f404eea9c5","trusted":true},"cell_type":"markdown","source":"[Go to Top](#0)"},{"metadata":{},"cell_type":"markdown","source":"# # MoA Predictions 🧬: Overfitting with TabNet Ver7\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet\n!pip install /kaggle/input/iterative-stratification/iterative-stratification-master/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### General ###\nimport os\nimport sys\nimport copy\nimport tqdm\nimport pickle\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsys.path.append(\"../input/rank-gauss\")\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n\n### Data Wrangling ###\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom gauss_rank_scaler import GaussRankScaler\n\n### Data Visualization ###\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use(\"fivethirtyeight\")\n\n### Machine Learning ###\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\n### Deep Learning ###\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.nn import functional as F\nfrom torch.nn.modules.loss import _WeightedLoss\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n# Tabnet \nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n### Make prettier the prints ###\nfrom colorama import Fore\nc_ = Fore.CYAN\nm_ = Fore.MAGENTA\nr_ = Fore.RED\nb_ = Fore.BLUE\ny_ = Fore.YELLOW\ng_ = Fore.GREEN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 42\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nset_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters\ndata_path = \"../input/lish-moa/\"\nno_ctl = True\nscale = \"rankgauss\"\nvariance_threshould = 0.7\ndecompo = \"PCA\"\nncompo_genes = 80\nncompo_cells = 10\nencoding = \"dummy\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(data_path + \"train_features.csv\")\n#train.drop(columns = [\"sig_id\"], inplace = True)\n\ntargets = pd.read_csv(data_path + \"train_targets_scored.csv\")\n#train_targets_scored.drop(columns = [\"sig_id\"], inplace = True)\n\n#train_targets_nonscored = pd.read_csv(data_path + \"train_targets_nonscored.csv\")\n\ntest = pd.read_csv(data_path + \"test_features.csv\")\n#test.drop(columns = [\"sig_id\"], inplace = True)\n\nsubmission = pd.read_csv(data_path + \"sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if no_ctl:\n    # cp_type == ctl_vehicle\n    print(b_, \"not_ctl\")\n    train = train[train[\"cp_type\"] != \"ctl_vehicle\"]\n    test = test[test[\"cp_type\"] != \"ctl_vehicle\"]\n    targets = targets.iloc[train.index]\n    train.reset_index(drop = True, inplace = True)\n    test.reset_index(drop = True, inplace = True)\n    targets.reset_index(drop = True, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def distributions(num, graphs, items, features, gorc):\n    \"\"\"\n    Plot the distributions of gene expression or cell viability data\n    \"\"\"\n    for i in range(0, num - 1, 7):\n        if i >= 3:\n            break\n        idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n        fig, axs = plt.subplots(1, 7, sharey = True)\n        for k, item in enumerate(idxs):\n            if item >= items:\n                break\n            graph = sns.distplot(train[features].values[:, item], ax = axs[k])\n            graph.set_title(f\"{gorc}-{item}\")\n            graphs.append(graph)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in train.columns if col.startswith(\"g-\")]\nCELLS = [col for col in train.columns if col.startswith(\"c-\")]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gnum = train[GENES].shape[1]\ngraphs = []\n\ndistributions(gnum, graphs, 771, GENES, \"g\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnum = train[CELLS].shape[1]\ngraphs = []\n\ndistributions(cnum, graphs, 100, CELLS, \"c\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gnum = test[GENES].shape[1]\ngraphs = []\n\ndistributions(gnum, graphs, 771, GENES, \"g\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnum = test[CELLS].shape[1]\ngraphs = []\n\ndistributions(cnum, graphs, 100, CELLS, \"c\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all = pd.concat([train, test], ignore_index = True)\ncols_numeric = [feat for feat in list(data_all.columns) if feat not in [\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"]]\nmask = (data_all[cols_numeric].var() >= variance_threshould).values\ntmp = data_all[cols_numeric].loc[:, mask]\ndata_all = pd.concat([data_all[[\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"]], tmp], axis = 1)\ncols_numeric = [feat for feat in list(data_all.columns) if feat not in [\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale_minmax(col):\n    return (col - col.min()) / (col.max() - col.min())\n\ndef scale_norm(col):\n    return (col - col.mean()) / col.std()\n\nif scale == \"boxcox\":\n    print(b_, \"boxcox\")\n    data_all[cols_numeric] = data_all[cols_numeric].apply(scale_minmax, axis = 0)\n    trans = []\n    for feat in cols_numeric:\n        trans_var, lambda_var = stats.boxcox(data_all[feat].dropna() + 1)\n        trans.append(scale_minmax(trans_var))\n    data_all[cols_numeric] = np.asarray(trans).T\n    \nelif scale == \"norm\":\n    print(b_, \"norm\")\n    data_all[cols_numeric] = data_all[cols_numeric].apply(scale_norm, axis = 0)\n    \nelif scale == \"minmax\":\n    print(b_, \"minmax\")\n    data_all[cols_numeric] = data_all[cols_numeric].apply(scale_minmax, axis = 0)\n    \nelif scale == \"rankgauss\":\n    ### Rank Gauss ###\n    print(b_, \"Rank Gauss\")\n    scaler = GaussRankScaler()\n    data_all[cols_numeric] = scaler.fit_transform(data_all[cols_numeric])\n    \nelse:\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PCA\nif decompo == \"PCA\":\n    print(b_, \"PCA\")\n    GENES = [col for col in data_all.columns if col.startswith(\"g-\")]\n    CELLS = [col for col in data_all.columns if col.startswith(\"c-\")]\n    \n    pca_genes = PCA(n_components = ncompo_genes,\n                    random_state = seed).fit_transform(data_all[GENES])\n    pca_cells = PCA(n_components = ncompo_cells,\n                    random_state = seed).fit_transform(data_all[CELLS])\n    \n    pca_genes = pd.DataFrame(pca_genes, columns = [f\"pca_g-{i}\" for i in range(ncompo_genes)])\n    pca_cells = pd.DataFrame(pca_cells, columns = [f\"pca_c-{i}\" for i in range(ncompo_cells)])\n    data_all = pd.concat([data_all, pca_genes, pca_cells], axis = 1)\nelse:\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding\nif encoding == \"lb\":\n    print(b_, \"Label Encoding\")\n    for feat in [\"cp_time\", \"cp_dose\"]:\n        data_all[feat] = LabelEncoder().fit_transform(data_all[feat])\nelif encoding == \"dummy\":\n    print(b_, \"One-Hot\")\n    data_all = pd.get_dummies(data_all, columns = [\"cp_time\", \"cp_dose\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in data_all.columns if col.startswith(\"g-\")]\nCELLS = [col for col in data_all.columns if col.startswith(\"c-\")]\n\nfor stats in tqdm.tqdm([\"sum\", \"mean\", \"std\", \"kurt\", \"skew\"]):\n    data_all[\"g_\" + stats] = getattr(data_all[GENES], stats)(axis = 1)\n    data_all[\"c_\" + stats] = getattr(data_all[CELLS], stats)(axis = 1)    \n    data_all[\"gc_\" + stats] = getattr(data_all[GENES + CELLS], stats)(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def distributions(num, graphs, items, features, gorc):\n    \"\"\"\n    Plot the distributions of gene expression or cell viability data\n    \"\"\"\n    for i in range(0, num - 1, 7):\n        if i >= 3:\n            break\n        idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n        fig, axs = plt.subplots(1, 7, sharey = True)\n        for k, item in enumerate(idxs):\n            if item >= items:\n                break\n            graph = sns.distplot(data_all[features].values[:, item], ax = axs[k])\n            graph.set_title(f\"{gorc}-{item}\")\n            graphs.append(graph)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gnum = data_all[GENES].shape[1]\ngraphs = []\n\ndistributions(gnum, graphs, 771, GENES, \"g\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnum = data_all[CELLS].shape[1]\ngraphs = []\n\ndistributions(cnum, graphs, 100, CELLS, \"c\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"data_all.pickle\", \"wb\") as f:\n    pickle.dump(data_all, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_df and test_df\nfeatures_to_drop = [\"sig_id\", \"cp_type\"]\ndata_all.drop(features_to_drop, axis = 1, inplace = True)\ntry:\n    targets.drop(\"sig_id\", axis = 1, inplace = True)\nexcept:\n    pass\ntrain_df = data_all[: train.shape[0]]\ntrain_df.reset_index(drop = True, inplace = True)\n# The following line it's a bad practice in my opinion, targets on train set\n#train_df = pd.concat([train_df, targets], axis = 1)\ntest_df = data_all[train_df.shape[0]: ]\ntest_df.reset_index(drop = True, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"{b_}train_df.shape: {r_}{train_df.shape}\")\nprint(f\"{b_}test_df.shape: {r_}{test_df.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test_df.values\nprint(f\"{b_}X_test.shape: {r_}{X_test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_EPOCH = 200\n#MAX_EPOCH = 2 #変更\n\n# n_d and n_a are different from the original work, 32 instead of 24\n# This is the first change in the code from the original\ntabnet_params = dict(\n    n_d = 32,\n    n_a = 32,\n    n_steps = 1,\n    gamma = 1.3,\n    lambda_sparse = 0,\n    optimizer_fn = optim.Adam,\n    optimizer_params = dict(lr = 2e-2, weight_decay = 1e-5),\n    mask_type = \"entmax\",\n    scheduler_params = dict(\n        mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n    scheduler_fn = ReduceLROnPlateau,\n    seed = seed,\n    verbose = 10\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LogitsLogLoss(Metric):\n    \"\"\"\n    LogLoss with sigmoid applied\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        \"\"\"\n        Compute LogLoss of predictions.\n\n        Parameters\n        ----------\n        y_true: np.ndarray\n            Target matrix or vector\n        y_score: np.ndarray\n            Score matrix or vector\n\n        Returns\n        -------\n            float\n            LogLoss of predictions vs targets.\n        \"\"\"\n        logits = 1 / (1 + np.exp(-y_pred))\n        aux = (1 - y_true) * np.log(1 - logits + 1e-15) + y_true * np.log(logits + 1e-15)\n        return np.mean(-aux)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_auc_all = []\ntest_cv_preds = []\n\nNB_SPLITS = 10 # 7\n#NB_SPLITS = 2 #変更\n\nmskf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, random_state = 0, shuffle = True)\n\noof_preds = []\noof_targets = []\nscores = []\nscores_auc = []\nfor fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train_df, targets)):\n    print(b_,\"FOLDS: \", r_, fold_nb + 1)\n    print(g_, '*' * 60, c_)\n    \n    X_train, y_train = train_df.values[train_idx, :], targets.values[train_idx, :]\n    X_val, y_val = train_df.values[val_idx, :], targets.values[val_idx, :]\n    ### Model ###\n    model = TabNetRegressor(**tabnet_params)\n        \n    ### Fit ###\n    # Another change to the original code\n    # virtual_batch_size of 32 instead of 128\n    model.fit(\n        X_train = X_train,\n        y_train = y_train,\n        eval_set = [(X_val, y_val)],\n        eval_name = [\"val\"],\n        eval_metric = [\"logits_ll\"],\n        max_epochs = MAX_EPOCH,\n        patience = 20,\n        batch_size = 1024, \n        virtual_batch_size = 32,\n        num_workers = 1,\n        drop_last = False,\n        # To use binary cross entropy because this is not a regression problem\n        loss_fn = F.binary_cross_entropy_with_logits\n    )\n    print(y_, '-' * 60)\n    \n    ### Predict on validation ###\n    preds_val = model.predict(X_val)\n    # Apply sigmoid to the predictions\n    preds = 1 / (1 + np.exp(-preds_val))\n    score = np.min(model.history[\"val_logits_ll\"])\n    \n    ### Save OOF for CV ###\n    oof_preds.append(preds_val)\n    oof_targets.append(y_val)\n    scores.append(score)\n    \n    ### Predict on test ###\n    preds_test = model.predict(X_test)\n    test_cv_preds.append(1 / (1 + np.exp(-preds_test)))\n\noof_preds_all = np.concatenate(oof_preds)\noof_targets_all = np.concatenate(oof_targets)\ntest_preds_all = np.stack(test_cv_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aucs = []\nfor task_id in range(oof_preds_all.shape[1]):\n    aucs.append(roc_auc_score(y_true = oof_targets_all[:, task_id],\n                              y_score = oof_preds_all[:, task_id]\n                             ))\nprint(f\"{b_}Overall AUC: {r_}{np.mean(aucs)}\")\nprint(f\"{b_}Average CV: {r_}{np.mean(scores)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_feat = [col for col in submission.columns if col not in [\"sig_id\"]]\n# To obtain the same lenght of test_preds_all and submission\ntest = pd.read_csv(data_path + \"test_features.csv\")\nsig_id = test[test[\"cp_type\"] != \"ctl_vehicle\"].sig_id.reset_index(drop = True)\ntmp = pd.DataFrame(test_preds_all.mean(axis = 0), columns = all_feat)\ntmp[\"sig_id\"] = sig_id\n\nsubmission = pd.merge(test[[\"sig_id\"]], tmp, on = \"sig_id\", how = \"left\")\nsubmission.fillna(0, inplace = True)\n\n#submission[all_feat] = tmp.mean(axis = 0)\n\n# Set control to 0\n#submission.loc[test[\"cp_type\"] == 0, submission.columns[1:]] = 0\nsubmission.to_csv(\"submission2.csv\", index = None)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"{b_}submission2.shape: {r_}{submission.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # # pca_var_cv_simple_nn fold6\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master/')\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport os\nfrom matplotlib import pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nsns.set_style('ticks')\nsns.set_context(\"poster\")\nsns.set_palette('colorblind')\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\nfrom torch.utils.data import Dataset, DataLoader\nimport warnings\nwarnings.filterwarnings('ignore')\n# os.listdir('../input/lish-moa')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (20.0, 10.0)\ndevice = ('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'device': device,\n          'n_comp_g': 450, \n          'n_comp_c': 45, \n          'var_thresh': 0.67,\n          'epochs': 25,\n          'batch_size': 128,\n          'lr': 1e-3,\n          'weight_decay': 1e-5, \n          #'n_folds': 7, \n          'n_folds': 6, #変更 \n          \n          'early_stopping_steps': 10,\n          'early_stop': False,\n          'in_size': None,\n          'out_size': None,\n          'hidden_size': 1500}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv') # ../input/lish-moa/\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv') # ../input/lish-moa/\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv') # ../input/lish-moa/\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv') # ../input/lish-moa/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_features = [col for col in train_features.columns if col.startswith('g-')]\nc_features = [col for col in train_features.columns if col.startswith('c-')]\n\ng_c_features = g_features + c_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trans_train_features = transformer.fit_transform(train_features[g_c_features])\ntrans_test_features = transformer.transform(test_features[g_c_features])\n\ntrans_train_df = pd.DataFrame(trans_train_features, columns = g_c_features)\ntrans_test_df = pd.DataFrame(trans_test_features, columns = g_c_features)\n\ntrain_features = pd.concat([train_features.drop(columns=g_c_features), trans_train_df], axis=1)\ntest_features = pd.concat([test_features.drop(columns=g_c_features), trans_test_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_sample = random.sample(g_features, 3)\nc_sample = random.sample(c_features, 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = ['navy', 'r', 'g']\nfor col, color in zip(g_sample, colors):\n    plt.hist(test_features[col], bins=50, alpha=0.5, label=col)\n    plt.axvline(np.median(test_features[col]), linewidth=3, color=color, label='median_{}'.format(col))\nplt.xlim(-7, 7)\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = ['navy', 'r', 'g']\nfor col, color in zip(c_sample, colors):\n    plt.hist(test_features[col], bins=50, alpha=0.5, label=col)\n    plt.axvline(np.median(test_features[col]), linewidth=3, color=color, label='median_{}'.format(col))\nplt.xlim(-7, 7)\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transfrom_all_data(transformer, train, test, feature_list):\n    \n    data = pd.concat([train[feature_list], test[feature_list]], axis=0).reset_index(drop=True)\n    n = train.shape[0]\n    \n    data_trans = transformer.fit_transform(data)\n    train_trans = data_trans[:n, :]\n    test_trans = data_trans[n:, :]\n    return train_trans, test_trans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_pca_features(n_comp, train, test, feature_list, name, normalize=False, scaler=None):\n    \n    pca = PCA(n_comp)\n    \n    train_pca, test_pca = transfrom_all_data(pca, train, test, feature_list)\n    \n    if normalize and scaler is not None:\n        train_pca = scaler.fit_transform(train_pca)\n        test_pca = scaler.transform(test_pca)\n    \n    for i in range(n_comp):\n        train['{0}_{1}'.format(name, i)] = train_pca[:, i]\n        test['{0}_{1}'.format(name, i)] = test_pca[:, i]\n        \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(data):\n    data['cp_time'] = data['cp_time'].map({24:0, 48:1, 72:2})\n    data['cp_dose'] = data['cp_dose'].map({'D1':0, 'D2':1})\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features, test_features = make_pca_features(params['n_comp_g'], train_features, test_features, g_features, 'g_pca')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features, test_features = make_pca_features(params['n_comp_c'], train_features, test_features, c_features, 'c_pca')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_thresh = VarianceThreshold(params['var_thresh'])\nto_thresh = train_features.columns[4:]\ncat_features = train_features.columns[:4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_thresh, test_thresh = transfrom_all_data(var_thresh, train_features, test_features, to_thresh)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.concat([train_features[cat_features], pd.DataFrame(train_thresh)], axis=1)\ntest_features = pd.concat([test_features[cat_features], pd.DataFrame(test_thresh)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_mask = train_features['cp_type'] != 'ctl_vehicle'\ntrain_sig_ids = train_features.loc[train_mask]['sig_id']\ntrain = train_features.loc[train_mask].reset_index(drop=True)\n\ntest_mask = test_features['cp_type'] != 'ctl_vehicle'\ntest_sig_ids = test_features.loc[test_mask]['sig_id']\ntest = test_features.loc[test_mask].reset_index(drop=True)\n\ntrain_target_sigids = train_targets[['sig_id']]\ny_true  = train_targets.copy()\n\ntrain_targets = train_targets[train_targets['sig_id'].isin(train_sig_ids)].reset_index(drop=True)\ntrain_targets.drop(columns=['sig_id'], inplace=True)\ntrain_targets.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params['in_size'] = train.shape[1] - 2\nparams['out_size'] = train_targets.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mskf = MultilabelStratifiedKFold(n_splits=params['n_folds'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = train.copy()\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=train_targets)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TabularDataset:\n    \n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n    \n    def __len__(self):\n        return(self.X.shape[0])\n    \n    def __getitem__(self, i):\n        \n        X_i = torch.tensor(self.X[i, :], dtype=torch.float)\n        y_i = torch.tensor(self.y[i, :], dtype=torch.float)\n        \n        return X_i, y_i\n    \n    \n\nclass TabularDatasetTest:\n    \n    def __init__(self, X):\n        self.X = X\n    \n    def __len__(self):\n        return(self.X.shape[0])\n    \n    def __getitem__(self, i):\n        \n        X_i = torch.tensor(self.X[i, :], dtype=torch.float)        \n        return X_i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_func(model, optimizer, scheduler, loss_func, dataloader, device):\n    \n    train_loss = 0\n    \n    model.train()  \n    for inputs, labels in dataloader:        \n        optimizer.zero_grad()\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        loss = loss_func(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        train_loss += loss.item()\n        \n    train_loss /= len(dataloader)\n    \n    return train_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def valid_func(model, loss_func, dataloader, device):\n    \n    model.eval()\n    \n    valid_loss = 0\n    valid_preds = []\n    \n    for inputs, labels in dataloader:   \n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        loss = loss_func(outputs, labels)\n        \n        valid_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    valid_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return valid_loss, valid_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data.to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.25)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.25)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = preprocess(folds.drop(columns = ['sig_id', 'cp_type']))\n    \n    train_mask = train['kfold'] != fold\n    valid_idc = train.loc[~train_mask].index\n    \n    X_train = train.loc[train_mask].reset_index(drop=True)\n    y_train = train_targets.loc[train_mask].reset_index(drop=True)\n\n    \n    X_val = train.loc[~train_mask].reset_index(drop=True)\n    y_val = train_targets.loc[~train_mask].reset_index(drop=True)\n    \n    X_train.drop(columns=['kfold'], inplace=True)\n    X_val.drop(columns=['kfold'], inplace=True)\n    \n    test_ = preprocess(test.drop(columns = ['sig_id', 'cp_type']))\n\n    \n    train_ds = TabularDataset(X_train.values, y_train.values)\n    valid_ds = TabularDataset(X_val.values, y_val.values)\n    test_ds = TabularDatasetTest(test_.values)\n    \n    train_dl = DataLoader(train_ds, batch_size=params['batch_size'], shuffle=True)\n    valid_dl = DataLoader(valid_ds, batch_size=params['batch_size'], shuffle=False)\n    test_dl = DataLoader(test_ds, batch_size=params['batch_size'], shuffle=False)\n    \n    \n    model = Model(num_features=params['in_size'], num_targets=params['out_size'], \n                  hidden_size=params['hidden_size'] )\n    \n    model.to(params['device'])\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=params['epochs'], steps_per_epoch=len(train_dl))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing=0.001)\n    \n    early_stopping_steps = params['early_stopping_steps']\n    early_step = 0\n   \n    oof = np.zeros((train.shape[0], params['out_size']))\n    best_loss = np.inf\n    \n    for epoch in range(params['epochs']):\n        \n        train_loss = train_func(model, optimizer,scheduler, loss_tr, train_dl, params['device'])\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_func(model, loss_fn, valid_dl, params['device'])\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[valid_idc] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif(params['early_stop'] == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n\n    \n    model = Model(num_features=params['in_size'], num_targets=params['out_size'], \n                  hidden_size=params['hidden_size'] )\n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(params['device'])\n    \n    \n    predictions = np.zeros((test.shape[0], params['out_size']))\n    predictions = inference_fn(model, test_dl, params['device'])\n    \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(n_folds, seed):\n    oof = np.zeros((train.shape[0], params['out_size']))\n    predictions = np.zeros((test.shape[0], params['out_size']))\n    \n    for fold in range(n_folds):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / n_folds\n        oof += oof_\n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#seeds = [0, 1, 2, 3, 4, 5, 6]\nseeds = [0, 1, 2, 3, 4, 5] #変更\n\noof = np.zeros((train.shape[0], params['out_size']))\npredictions = np.zeros((test.shape[0], params['out_size']))\n\nfor seed in seeds:\n    \n    oof_, predictions_ = run_k_fold(params['n_folds'], seed)\n    oof += oof_ / len(seeds)\n    predictions += predictions_ / len(seeds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_results = pd.concat([train_target_sigids[train_target_sigids['sig_id'].isin(train_sig_ids)].reset_index(drop=True), pd.DataFrame(oof)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_results = pd.concat([test[['sig_id']], pd.DataFrame(predictions, columns = sample_submission.columns[1:])], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_full = train_target_sigids.merge(valid_results, on='sig_id', how='left').fillna(0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = y_true.drop(columns=['sig_id']).values\ny_pred = valid_full.drop(columns=['sig_id']).values\n\nscore = 0\nfor i in range(y_true.shape[1]):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ / y_true.shape[1]\n    \nprint(\"CV log_loss: \", score)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sample_submission[['sig_id']].merge(test_results, on='sig_id', how='left').fillna(0)\nsub.to_csv('submission3.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"平均をとる"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub1 = pd.read_csv('submission.csv')\nsub2 = pd.read_csv('submission2.csv')\nsub3 = pd.read_csv('submission3.csv')\n\nsub_id = sub1[['sig_id']] #id部分のみを抜き出し\n\nsub1 = sub1[sub1.columns[sub1.columns != 'sig_id']] #数値のみを抜き出し \nsub2 = sub2[sub2.columns[sub2.columns != 'sig_id']] #数値のみを抜き出し\nsub3 = sub3[sub3.columns[sub3.columns != 'sig_id']] #数値のみを抜き出し\n\nsub4 = (sub1 + sub2 + sub3) / 3 #平均\n\nsub_mean = pd.concat([sub_id, sub4], axis=1) #idと数値を結合\n\nsub_mean.to_csv('submission.csv', index=False)\nsub_mean.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}