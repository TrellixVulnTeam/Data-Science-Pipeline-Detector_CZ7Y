{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.preprocessing import QuantileTransformer, OneHotEncoder\nfrom sklearn.pipeline import Pipeline,FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import SelectKBest, VarianceThreshold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterativestratificationmaster/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 30","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Import Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"FILE_DIR = '../input/lish-moa/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(FILE_DIR+'train_features.csv')\ntest_df = pd.read_csv(FILE_DIR+'test_features.csv')\ntarget_df = pd.read_csv(FILE_DIR+'train_targets_scored.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('shape of train_df:{}'.format(train_df.shape))\nprint('shape of test_df:{}'.format(test_df.shape))\nprint('shape of target_df:{}'.format(target_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENE = [ col for col in train_df.columns.tolist() if col.startswith('g-')]\nCELL = [col for col in train_df.columns.to_list() if col.startswith('c-')]\nCAT = [col for col in train_df.columns.tolist() if col.startswith('cp_')]\nTARGET = [col for col in target_df.columns.to_list()[1:]]\nprint('Length of GENE: {}'.format(len(GENE)))\nprint('Length of CELL: {}'.format(len(CELL)))\nprint('Length of CAT: {}'.format(len(CAT)))\nprint('Length of TARGET: {}'.format(len(TARGET)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if there is na in train_df\ntrain_df.isna().any().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if there is na in train_df\ntarget_df.isna().any().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. EDA"},{"metadata":{},"cell_type":"markdown","source":"## 2.1 training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig ,axs = plt.subplots(1,3,figsize=(14,4))\nplt.subplots_adjust(left=-0.1,right=1.1,bottom=-0.1,top=1.1)\nfor i,col in enumerate(CAT):\n    sns.countplot(x=col, data=train_df,ax=axs[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw random gene data \nN = 6 \ncol = random.choices(GENE,k=N)\nplot_df = train_df.loc[:,col]\nsns.pairplot(plot_df,diag_kind='hist')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N = 6 \ncol = random.choices(CELL,k=N)\nplot_df = train_df.loc[:,col]\nsns.pairplot(plot_df,diag_kind='hist')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Target data "},{"metadata":{"trusted":true},"cell_type":"code","source":"N=6\n\ncol = random.choices(TARGET,k=N)\n\nfig =plt.figure(figsize=(14,8))\n\nfor i,v in enumerate(col):\n    plt.subplot(2,3,i+1)\n    sns.countplot(x=v,data=target_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt_df = pd.DataFrame(target_df.iloc[:,1:].sum(axis=0).sort_values())\nplt_df.tail(20).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(target_df[TARGET].sum(axis=0)).sum()/206","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Merge Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop ctl_vehicle\ndrop_inx = train_df[train_df['cp_type']=='ctl_vehicle'].index.tolist()\ntrain_df = train_df.drop(index=drop_inx,axis=0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop ctl_vehicle\ntarget_df = target_df.drop(index=drop_inx,axis=0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_df = test_df.loc[test_df['cp_type']!='ctl_vehicle',:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.1 Start Merging"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df = train_df.join(target_df.set_index('sig_id'),on='sig_id',how='inner')\ndata_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.drop('sig_id',axis=1,inplace=True)\ndata_df.drop('cp_type',axis=1,inplace=True)\ndata_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_moa_index = test_df[test_df['cp_type']=='ctl_vehicle'].index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4 PCA "},{"metadata":{"trusted":true},"cell_type":"code","source":"N =500\npca_pipe = Pipeline([\n    ('qt',QuantileTransformer(output_distribution='normal')),\n    ('pca',PCA(n_components=N))\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_comp_num(data,n,threshold):\n    \n    '''return specific component_num over variance explaination ratio'''\n    \n    pca_pipe = Pipeline([\n        ('qt',QuantileTransformer(output_distribution='normal')),\n        ('pca',PCA(n_components=n))\n    ])\n    \n    pca_pipe.fit_transform(data)\n    #check explained ratio\n    ratio = pca_pipe.steps[1][1].explained_variance_ratio_.cumsum()\n    for i,v in enumerate(ratio):\n        if v>threshold:\n            d=i+1\n            break\n    return d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_GENE = get_comp_num(data_df.loc[:,GENE],n=700,threshold=0.85)\nd_GENE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_CELL = get_comp_num(data_df.loc[:,CELL],n=90,threshold=0.85)\n\nd_CELL","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Preprocess"},{"metadata":{"trusted":true},"cell_type":"code","source":"gene_pipe = Pipeline([\n    ('qt',QuantileTransformer(output_distribution='normal')),\n    ('pca',PCA(n_components= d_GENE))\n])\n\ngene_union = FeatureUnion([\n    ('pca',gene_pipe),\n    ('qt',QuantileTransformer(output_distribution='normal'))\n])\n\ngene_variance = Pipeline([\n    ('union',gene_union),\n    ('var',VarianceThreshold(0.8))\n])\n\ncell_pipe = Pipeline([\n    ('qt',QuantileTransformer(output_distribution='normal')),\n    ('pca',PCA(n_components= d_CELL))\n])\n\ncell_union = FeatureUnion([\n    ('pca',cell_pipe),\n    ('qt',QuantileTransformer(output_distribution='normal'))\n])\n\ncell_variance = Pipeline([\n    ('union',cell_union),\n    ('var',VarianceThreshold(0.8))\n])\n\ncat_pipe = Pipeline([\n    ('ohc',OneHotEncoder())\n])\n\nCAT = ['cp_time','cp_dose']\npre_pipe = ColumnTransformer([\n    ('GENE',gene_variance,GENE),\n    ('CELL',cell_variance,CELL),\n    ('CAT',cat_pipe,CAT)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ff = pre_pipe.fit_transform(data_df.loc[:,CAT+GENE+CELL])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ff.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Callbacks"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import backend \nlabel_smooth = 0.001\np_min = 0*(1-label_smooth)+label_smooth/2\np_max = 1*(1-label_smooth)+label_smooth/2\n# p_min =0.001\n# p_max = 0.999\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3.1 initial bias "},{"metadata":{"trusted":true},"cell_type":"code","source":"total = data_df.shape[0]\npos = data_df[TARGET].sum(axis=0).values\nneg = (pos-total)*(-1)\n\ninitial_bias = np.log(pos/neg)\n\ninit = tf.constant_initializer(initial_bias)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1 build NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = []\n# lr \n# def exponential_decay(lr0, s):\n#     def exponential_decay_fn(epoch):\n#         return lr0 * 0.1**(epoch / s) \n#     return exponential_decay_fn\n# exponential_decay_fn = exponential_decay(lr0=0.01, s=5)\n# lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n# callbacks.append(lr_scheduler)\n\n# def scheduler(epoch, lr):\n#     if epoch < 5:\n#         return lr\n#     else:\n#         return lr * tf.math.exp(-0.1)\n# lr_scheduler = keras.callbacks.LearningRateScheduler(scheduler,verbose=0)\n# callbacks.append(lr_scheduler)\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_logloss', \n                                                 factor=0.1, \n                                                 verbose=0,\n                                                 mode='min',\n                                                 patience=5,\n                                                 min_lr=1e-7)\ncallbacks.append(reduce_lr)\n\n# check point \ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",monitor='val_logloss', save_best_only=True)\ncallbacks.append(checkpoint_cb)\n\n\n# early stop\nearly_st = tf.keras.callbacks.EarlyStopping(monitor='val_logloss',\n                                            min_delta=1E-5,\n                                            patience=7,\n                                            verbose=0,\n                                            mode='min',\n                                            baseline=None,\n                                            restore_best_weights=True)\ncallbacks.append(early_st)\n\n# lr record \n# class LearningRateLoggingCallback(tf.keras.callbacks.Callback):\n#     def on_epoch_end(self, epoch):\n#         lr = self.model.optimizer.lr\n#         tf.summary.scalar('learning rate', data=lr, step=epoch)\n        \n# callbacks.append(LearningRateLoggingCallback())        \n# Tensorboard \n# root_logdir = os.path.join(os.curdir, \"my_logs\")\n\n# def get_run_logdir(): \n#     import time\n#     run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\") \n#     return os.path.join(root_logdir, run_id)\n\n# run_logdir = get_run_logdir()\n    \n# tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n# callbacks.append(tensorboard_cb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow_addons as tfa\n\ndef build_NN(n_hiddens,n_neurons,drop_rate=0.2,inputshape=447,smooth_rate=0.001,lr=3e-4):\n    \n    adam = tfa.optimizers.Lookahead(tf.optimizers.Adam(learning_rate=lr),sync_period = 10)\n    #adam = tf.optimizers.Adam(learning_rate=lr,beta_1=0.9,beta_2=0.999)\n    \n    model = keras.models.Sequential()\n    \n    model.add(keras.layers.Input(shape=(inputshape,)))\n    \n    for i in range(n_hiddens):\n        \n        model.add(keras.layers.BatchNormalization())\n        \n#         model.add(tfa.layers.WeightNormalization(keras.layers.Dense(n_neurons,activation=\"relu\", \n#                                      kernel_initializer=\"he_normal\",\n#                                      kernel_regularizer=keras.regularizers.l2(0.01))))\n        \n        model.add(keras.layers.Dense(n_neurons,activation=\"relu\", \n                                      kernel_initializer=\"he_normal\",\n                                      kernel_regularizer=keras.regularizers.l2(0.01)))\n        \n        model.add(keras.layers.Dropout(rate = drop_rate))\n    \n      \n    model.add(keras.layers.Dense(206,activation='sigmoid',bias_initializer=init))\n    \n    model.compile(loss = keras.losses.BinaryCrossentropy(label_smoothing=smooth_rate),\n                  optimizer=adam,metrics=logloss)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"21948/6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_folds = 6\nmlkf = MultilabelStratifiedKFold(n_splits=n_folds,shuffle=True,random_state=SEED)\nresults = []\noof = tf.constant(0.0)\npred = tf.constant(0.0)\nnp.random.seed(SEED)\nn_seeds=2\nseeds = np.random.randint(0,100,size=n_seeds)\nfor seed in seeds:\n    mskf = MultilabelStratifiedKFold(n_splits=n_folds,shuffle=True,random_state=seed)\n    for n,(train, test) in enumerate(mlkf.split(data_df.loc[:,CAT+GENE+CELL],data_df[TARGET])):\n            print('fold{}'.format(n))\n            train_x = pre_pipe.fit_transform(data_df.loc[train,CAT+GENE+CELL])\n            val_x = pre_pipe.transform(data_df.loc[test,CAT+GENE+CELL])\n            test_x = pre_pipe.transform(test_df.loc[:,CAT+GENE+CELL])\n            train_y = data_df.loc[train,TARGET]\n            val_y = data_df.loc[test,TARGET]\n\n\n            model = build_NN(n_hiddens=3, \n                             n_neurons=512,\n                             lr=0.0003,\n                             drop_rate=0.3,\n                             inputshape=train_x.shape[1])\n\n            hist = model.fit(train_x,train_y, \n                             batch_size=128,\n                             epochs=200,\n                             validation_data = (val_x,val_y),\n                             callbacks =callbacks,\n                             verbose=1\n                             # test \n\n                            )\n            pred_y = model.predict(val_x)\n            oof += logloss(tf.constant(val_y,dtype=tf.float32),tf.constant(pred_y,dtype=tf.float32))/(n_folds*n_seeds)\n            print(oof)\n            pred+= (pred_y/(n_folds*n_seeds))\n            results.append(model.predict(test_x)/(n_folds*n_seeds))\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"0.018790415","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=np.zeros_like(results[0])\nfor i in results:\n    a+=i\n\n#a= tf.clip_by_value(i,p_min,p_max).numpy()\n\nsubmission_df = pd.DataFrame(a,columns=TARGET)\nsubmission_df = pd.concat([test_df.loc[:,['sig_id']],submission_df],axis=1)\nsubmission_df.iloc[no_moa_index,1:] =0.0\nsubmission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred = pd.DataFrame(pred,columns=TARGET)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bce = tf.keras.losses.BinaryCrossentropy()\n# ll=[]\n# for i in range(len(TARGET)):\n#     ll.append(bce(val_y.iloc[:,[i]],pred_y.iloc[:,[i]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ll = np.array(ll)\n\n# ll = pd.DataFrame(ll,index=TARGET,columns=['bce'])\n\n# ll['bce'].sort_values(ascending=False).head(30).plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with open('loss_feature.pkl','wb') as f:\n#     pickle.dump(ll,f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}