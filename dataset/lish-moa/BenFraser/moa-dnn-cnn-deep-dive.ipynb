{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Mechanisms of Action DNN Deep-Dive\n\nWithin this notebook we'll explore a range of techniques and model architectures, some standard, whilst others are unconventional. The focus is Deep neural networks and variants thereof, rather than classical tabular models or tree-based ensembles. \n\nWe'll start by performing some analysis of the given dataset, followed by defining a range of preprocessing and feature engineering functions to help provide a diverse set of training and testing data for our models.\n\nAfter this, we'll craft some standard deep neural networks and explore what impact different subsets of data, hyper-parameters and techniques have on the cross-validation performance. With these results then established as a base mark, we start exploring some further unconventional models. This includes some simple convolutional neural networks, and also more complex hybrid tabular & convolutional neural networks. \n\nYou might be skeptical as to why we'd ever throw a convolutional neural network at a tabular data challenge such as this, and the answer is simple: Because we can, and in fact its not as bad as you'd think! \n\n**Table of Contents:**\n\n1. [Imports](#imports)\n2. [EDA](#EDA)\n3. [Data Preparation and Preprocessing](#data-preprocessing)\n4. [Model Production and Evaluation](#model-production) \n    - 4.1. [Hold-out Validation with Tuned 3-layer ANN](#standard-3layer)\n    - 4.2. [K-Folds Cross-Validation](#kfolds-cross-validation)\n    - 4.3. [Evaluation of Feature Engineered Data](#feature-engineered-evaluation)\n    - 4.4. [Evaluation of Dimensionally Reduced Features](#dimensionally-reduced-evaluation)\n    - 4.5. [Evaluation of Oversampled Data](#oversampled-evaluation)\n5. [Composite Neural Networks](#composite-models)\n    - 5.1. [Validation of a single 1D ConvNet](#single-Conv1d)\n    - 5.2 [Multi-input ConvNet](#multi-input-convnet)\n    - 5.3 [Dense and Convolutional Composite Multi-Input Model](#composite-model)\n6. [Test Set Predictions](#test-predictions)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"imports\"></a>\n\n## 1. Import dependencies and load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\n\nimport keras\nimport keras.backend as K\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.initializers import Constant\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import Dense, Embedding, Flatten, LSTM, GRU, \\\n        SpatialDropout1D, Bidirectional, Conv1D, MaxPooling1D, BatchNormalization\nfrom keras.models import Sequential, load_model\nfrom keras import models\nfrom keras import layers\n\nimport pickle\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import log_loss, silhouette_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_predict, \\\n        cross_validate, cross_val_score, KFold\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier, MultiOutputRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(keras.__version__) \nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"input_dir = '/kaggle/input/lish-moa'\ntrain_features = pd.read_csv(os.path.join(input_dir, 'train_features.csv'))\ntrain_targets_scored = pd.read_csv(os.path.join(input_dir, 'train_targets_scored.csv'))\ntrain_targets_nonscored = pd.read_csv(os.path.join(input_dir, 'train_targets_nonscored.csv'))\ntest_features = pd.read_csv(os.path.join(input_dir, 'test_features.csv'))\n\ntrain_features.shape, train_targets_scored.shape, train_targets_nonscored.shape, test_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have our main input features (train_features.csv), which is high-dimensional tabular data containing a mixture of categorical and numerical features. We then have our train targets, which consists of 206 different output classes for each data instance. Its important to note that these output labels are not mutually exclusive, and it is possible to get multiple outputs for each data instance. Therefore, this problem is a multi-output classification problem, and not just a multiclass classification problem. \n\nIn contrast to a normal binary classification task, this type of multi-label problem becomes much more difficult in terms of producing and fine-tuning a classification model. "},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"EDA\"></a>\n## 2. Basic Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = ['cp_type', 'cp_time', 'cp_dose']\n\nplt.figure(figsize=(14,4))\n\nfor idx, col in enumerate(cat_cols):\n    plt.subplot(int(f'13{idx + 1}'))\n    labels = train_features[col].value_counts().index.values\n    vals = train_features[col].value_counts().values\n    sns.barplot(x=labels, y=vals)\n    plt.xlabel(f'{col}', weight='bold')\n    plt.ylabel('Count', weight='bold')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For 'cp_type', the 'ctl_vehicle' refers to samples treated with a control perturbation. For control perturbations, our targets are all zero, since they have no Mechanism of Action (MoA).\n\nTo deal with this, a good strategy could be to identify samples that are ctl_vehicle (through training a classification model or simply using the feature as its in the test data!), and set all of these to zero. We can then process the test set accordingly, by first setting all test instance targets to zero if its a ctl_vehicle, followed by processing all of the others normally using our trained model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# select all indices when 'cp_type' is 'ctl_vehicle'\nctl_vehicle_idx = (train_features['cp_type'] == 'ctl_vehicle')\n\n# evaluate number of 1s we have in the total train scores when cp_type = ctl_vehicle\ntrain_targets_scored.loc[ctl_vehicle_idx].iloc[:, 1:].sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The total sum is zero, which confirms the statement above on all targets being zero for cases where cp_type is ctl_vehicle. The best thing to do with this is simply fill our targets for zero when this is the case.\n\nWe could also remove all of these from the training set, however there are arguments for and against this in practice. If we remove them, we could be witholding valuable zero case data from our models, and for new data our model might struggle to predict these cases accordingly. On the other hand, it is a lot of extra data, which could just serve to unnecessarily complicate our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# take a copy of all our training sig_ids for reference\ntrain_sig_ids = train_features['sig_id'].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop cp_type column since we no longer need it\nX = train_features.drop(['sig_id', 'cp_type'], axis=1).copy()\nX = X.loc[~ctl_vehicle_idx].copy()\n\ny = train_targets_scored.drop('sig_id', axis=1).copy()\ny = y.loc[~ctl_vehicle_idx].copy()\n\nX.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data has already been normalised using quantile normalisation, and so is not in its natural form as we see it."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.distplot(X.iloc[:, 2:].mean())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.distplot(y.mean())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.distplot(train_targets_nonscored.mean())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.sum().sort_values()[:30].plot.bar(figsize=(18,6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some output classes only have 1 instance in the entire training set. This is problematic and is no where near enough data if we expect our models to effectively make predictions across the whole range of targets. Imbalanced dataset techniques such as minority class over-sampling may have to be introduced, which may help our models generalise better to new data."},{"metadata":{},"cell_type":"markdown","source":"#### Plotting all gene / cell features for random samples:"},{"metadata":{},"cell_type":"markdown","source":"Lets quickly assess how our cell data looks when plotted over all features for random instances:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feats = X.iloc[:, :2].copy()\nX_cell_v = X.iloc[:, -100:].copy()\nX_gene_e = X.iloc[:, 2:772].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_features(X, y, selected_idx, features_type, figsize=(14,10)):\n    x_range = range(1, X.shape[1] + 1)\n    \n    fig = plt.figure(figsize=(14,10))\n    \n    for i, idx in enumerate(selected_idx):\n        ax = fig.add_subplot(selected_idx.shape[0], 1, i + 1)\n        vals = X.iloc[idx].values\n    \n        if (y.iloc[idx] == 1).sum():\n            output_labels = list(y.iloc[idx][y.iloc[idx] == 1].index.values)\n        \n            labels = \" \".join(output_labels)\n        else:\n            labels = \"None (all labels zero)\"\n        \n        sns.lineplot(x_range, vals)\n        plt.title(f\"Row {idx}, Labels: {labels}\", weight='bold')\n        plt.xlim(0.0, X.shape[1])\n        plt.grid()\n\n    plt.xlabel(f\"{features_type}\", weight='bold', size=14)\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets plot some random rows from our data\nrandom_idx = np.random.randint(X.shape[0], size=(5,))\n\nplot_features(X_cell_v, y, random_idx, features_type='Cell Features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly some rows vary substancially in terms of their value range, and therefore it is worth standardising this data prior to training our models."},{"metadata":{},"cell_type":"markdown","source":"Now lets do the same for our gene features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_features(X_gene_e, y, random_idx, features_type='Gene Features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have some noticeable peaks throughout the features for some of the above instances. It could be worth plotting a range of data instances with the same output labels against one another, and compare their peaks. If they correlate in one or more areas, this could be insightful for developing further features with our dataset.\n\nLets now repeat above, but for data instances with the same output label(s)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# select an output label to plot associated training features\nchosen_label = 'btk_inhibitor'\nchosen_rows = y.loc[y[chosen_label] == 1]\nchosen_feats = X_gene_e.loc[y[chosen_label] == 1]\n\n# select random rows from those available above for the chosen label\nrandom_idx = np.random.choice(range(0, chosen_rows.shape[0]), size=(5,), replace=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_features(chosen_feats, chosen_rows, random_idx, features_type='Gene Features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets also look at the mean and standard deviation of this feature:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_mean_std(dataframe, feature_name, features_type, figsize=(14,6), alpha=0.3):\n    \"\"\" Plot rolling mean and standard deviation for given dataframe \"\"\"\n    \n    plt.figure(figsize=figsize)\n    \n    x_range = range(1, dataframe.shape[1] + 1)\n    \n    chosen_rows = y.loc[y[feature_name] == 1]\n    chosen_feats = dataframe.loc[y[feature_name] == 1]\n    \n    means = chosen_feats.mean()\n    stds = chosen_feats.std()\n    \n    plt.plot(x_range, means, label=feature_name)    \n    plt.fill_between(x_range, means - stds, means + stds, \n                         alpha=alpha)\n\n    plt.title(f'{features_type}: {feature_name} - Mean & Standard Deviation', weight='bold')\n    \n    plt.xlim(0.0, dataframe.shape[1])\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_mean_std(X_gene_e, 'btk_inhibitor', 'Gene Features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets repeat for some different output labels:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# select an output label to plot associated training features\nchosen_label = 'histamine_receptor_antagonist'\nchosen_rows = y.loc[y[chosen_label] == 1]\nchosen_feats = X_gene_e.loc[y[chosen_label] == 1]\n\n# select random rows from those available above for the chosen label\nrandom_idx = np.random.choice(range(0, chosen_rows.shape[0]), size=(5,))\n\nplot_features(chosen_feats, chosen_rows, random_idx, features_type='Gene Features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_mean_std(X_gene_e, 'histamine_receptor_antagonist', 'Gene Features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# select an output label to plot associated training features\nchosen_label = 'free_radical_scavenger'\nchosen_rows = y.loc[y[chosen_label] == 1]\nchosen_feats = X_gene_e.loc[y[chosen_label] == 1]\n\n# select random rows from those available above for the chosen label\nrandom_idx = np.random.choice(range(0, chosen_rows.shape[0]), size=(5,))\n\nplot_features(chosen_feats, chosen_rows, random_idx, features_type='Gene Features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_mean_std(X_gene_e, 'free_radical_scavenger', 'Gene Features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This analysis highlights the potential for performing advanced feature engineering. We could process the trends of gene and/or cell features like we might with a time-series dataset, and use these features to supplement models that use the features in their standard form."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"data-preprocessing\"></a>\n## 3. Preprocessing and data preparation"},{"metadata":{},"cell_type":"markdown","source":"This will be relatively simple and will include:\n- Standardisation of all numerical features.\n- Creation of embeddings or encodings for our categorical variables.\n- Removal of unwanted / unnecessary columns.\n\nWe'll define a simple class to perform these actions for us on both the training and test data."},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Definition of a class for preprocessing:"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MOAPreprocessor:\n    \n    def __init__(self, cat_features, num_features, remove_cp_type=False):\n        self.cat_features = cat_features\n        self.num_features = num_features\n        self.std_scaler = StandardScaler()\n        self.remove_cp_type = remove_cp_type\n        \n    def preprocess_data(self, X, test=False):\n        \n        # take a copy of sig ids for reference\n        sig_ids = X.loc[:, 'sig_id']\n        \n        #  remove ctl_vehicle if selected\n        if self.remove_cp_type and not test:\n            ctl_vehicle_idx = (X['cp_type'] == 'ctl_vehicle')\n            data_df = X.loc[~ctl_vehicle_idx].copy()\n            data_df.reset_index(inplace=True, drop=True)\n        else:\n            data_df = X.copy()\n        \n        # subsets of categorical and numerical\n        X_cat = data_df.loc[:, self.cat_features]\n        \n        # standardise our cp_time column values\n        X_cat['cp_time'] = (data_df['cp_time'] / 24.0) - 2\n        \n        # one-hot encode our categorical features\n        X_cat = pd.get_dummies(X_cat)\n        \n        # select our numerical features\n        X_num = data_df.loc[:, self.num_features]\n        \n        if not test:\n            # fit parameters of our scaler and transform train\n            X_num[self.num_features] = self.std_scaler.fit_transform(X_num)\n            \n            # add train sig ids to class instance\n            self.train_sig_ids = sig_ids.copy()\n            \n        else:\n            # transform test set\n            X_num[self.num_features] = self.std_scaler.transform(X_num)\n            \n            # add test sig ids to class instance\n            self.test_sig_ids = sig_ids.copy()\n            \n        return pd.concat([X_cat, X_num], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cat_features = ['cp_time', 'cp_dose', 'cp_type']\ncat_features = ['cp_time', 'cp_dose']\n\n# define non-numeric cols to form list of numeric cols\nnon_num_tuple = ('cp_time', 'cp_dose', 'cp_type', 'sig_id')\nnum_features = [x for x in train_features.columns.values if not x.startswith(non_num_tuple)]\n\ndata_processor = MOAPreprocessor(cat_features, num_features, remove_cp_type=True)\nX_train_full = data_processor.preprocess_data(train_features)\nX_test = data_processor.preprocess_data(test_features, test=True)\n\n# convert our numeric data into float32 prior to tensorflow\nX_train_full = X_train_full.astype('float32')\nX_test = X_test.astype('float32')\n\nX_train_full.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_full.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also need to format our labels accordingly:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_labels(X, y):\n    \"\"\" Format our output labels appropriately \"\"\"\n    # remove sig id from labels\n    labels = y.drop('sig_id', axis=1).copy()\n\n    # remove cp_type if selected\n    if data_processor.remove_cp_type:\n        ctl_vehicle_idx = (X['cp_type'] == 'ctl_vehicle')\n        labels = labels.loc[~ctl_vehicle_idx].copy()\n        labels.reset_index(inplace=True, drop=True)\n    \n    return labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can use this to process both our scored and non-scored labels:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = process_labels(train_features, train_targets_scored)\ny_nonscored = process_labels(train_features, train_targets_nonscored)\ny.shape, y_nonscored.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good, we can now move on to additional feature processing and feature engineering."},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Preprocessing of Cell and Gene features"},{"metadata":{},"cell_type":"markdown","source":"In order to use them as additional features, lets extract the gene and cell features from our training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def gene_and_cell_feats(data):\n    X_gene = data.iloc[:, 3:775].copy()\n    X_cell = data.iloc[:, 775:].copy()\n    return X_gene, X_cell","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_gene, X_train_cell = gene_and_cell_feats(X_train_full)\nX_test_gene, X_test_cell = gene_and_cell_feats(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Preparation of Cell and Gene features for Convolutional Neural Network Input"},{"metadata":{},"cell_type":"markdown","source":"Before being able to feed our data splits into a CNN model, its preferable to have it in a specific form. In particular, for our use-case, we need our features to be of the current format for a 1D Conv Net model: (num_instances, num_features, 1)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def conv1d_preprocess(data):\n    processed = data.astype('float32').values\n    processed = processed.reshape(data.shape[0], data.shape[1], 1)\n    return processed\n\n# if not done so already - convert to float32 and reshape for ConvNet\nX_train_gene, X_train_cell = gene_and_cell_feats(X_train_full)\nX_test_gene, X_test_cell = gene_and_cell_feats(X_test)\n\nX_train_gene = conv1d_preprocess(X_train_gene)\nX_train_cell = conv1d_preprocess(X_train_cell)\nX_test_gene = conv1d_preprocess(X_test_gene)\nX_test_cell = conv1d_preprocess(X_test_cell)\nX_train_gene.shape, X_train_cell.shape, X_test_gene.shape, X_test_cell.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have these sets of features that can easily be fed into a composite CNN network. These features can be fed into the model, in addition to any other data we may want to provide our model with."},{"metadata":{},"cell_type":"markdown","source":"### 3.3 Feature engineering some common statistics and relationships from our data"},{"metadata":{},"cell_type":"markdown","source":"In addition to this, lets engineer some common features to use in our model, e.g. mean and std of cell and gene data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_num_feats(num_data, feature_type):\n    \"\"\" Find a range of numerical statistics across the row\n        of each instance in the given numerical data \"\"\"\n    \n    col_names = [f\"{feature_type}_{x}\" for x in ['mean', 'std', 'max', 'min']]\n    \n    means = num_data.mean(axis=1)\n    stds = num_data.std(axis=1)\n    maxs = num_data.max(axis=1)\n    mins = num_data.min(axis=1)\n    \n    result = np.c_[means, stds, maxs, mins]\n    return pd.DataFrame(result, columns=col_names)\n\n# obtain aux features for both train and test\naux_cell_train = create_num_feats(X_train_cell, feature_type='cell')\naux_gene_train = create_num_feats(X_train_gene, feature_type='gene')\naux_cell_test = create_num_feats(X_test_cell, feature_type='cell')\naux_gene_test = create_num_feats(X_test_gene, feature_type='gene')\n\n# create new dataframes of non-gene and non-cell features plus those above\nX_train_misc = X_train_full.iloc[:, :3].copy()\nX_train_misc.reset_index(inplace=True, drop=True)\nX_train_misc = pd.concat([X_train_misc, aux_cell_train, aux_gene_train], axis=1)\n\nX_test_misc = X_test.iloc[:, :3].copy()\nX_test_misc.reset_index(inplace=True, drop=True)\nX_test_misc = pd.concat([X_test_misc, aux_cell_test, aux_gene_test], axis=1)\nX_train_misc.shape, X_test_misc.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets also use these features to create extended training & test sets, which include these extra engineered features in addition to the original features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_ext = pd.concat([X_train_full, aux_cell_train, aux_gene_train], axis=1)\nX_test_ext = pd.concat([X_test, aux_cell_test, aux_gene_test], axis=1)\nX_train_ext.shape, X_test_ext.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good - we now have a variety of data-sets to try out on various model types in the next few sections."},{"metadata":{},"cell_type":"markdown","source":"### 3.4 Creation of a function to over-sample our minority classes"},{"metadata":{},"cell_type":"markdown","source":"We have a huge imbalance of data within this challenge. To help counter this, we can apply a range of imbalanced data processing techniques, however the application of most of these is extremely difficult due to the nature of the multi-label classification problem. A simple solution to this, which we'll use, is just simple oversampling of our training data.\n\nIt's important we dont perform any form of over-sampling before performing splits of training data as part of a cross validation strategy, otherwise we'll introduce duplicate instances into our training and evaluation process. This in turn will give us an overly optimistic estimate of the generalisation performance of our models."},{"metadata":{"trusted":true},"cell_type":"code","source":"def oversample_data(X, y, top_minority=200, min_class_count=30):\n    \"\"\" Oversample the lowest n represented classes from the provided data \n        and labels, ensuring we have a minimum number of counts \"\"\"\n    \n    # gather the top n under-represented classes to oversample\n    top_n_minority = y.sum().sort_values()[:top_minority].index.values\n    extra_features = pd.DataFrame()\n    extra_labels = pd.DataFrame()\n    \n    for column in top_n_minority:\n        class_count = int(y[column].sum())\n        class_count_diff = min_class_count - class_count\n    \n        if class_count_diff > 1 and class_count:\n            \n            # find instance idxs where class is 1\n            positive_idxs = y[column] == 1\n        \n            for iteration in range(int(np.ceil(class_count_diff / class_count))):\n                \n                # get random feature and label corresponding to class\n                rand_feature = X[positive_idxs].sample(class_count)\n                rand_label = y[positive_idxs].sample(class_count)\n                extra_features = extra_features.append(rand_feature, ignore_index=True)\n                extra_labels = extra_labels.append(rand_label, ignore_index=True)\n    \n    oversampled_X = X.append(extra_features, ignore_index=False)\n    oversampled_y = y.append(extra_labels, ignore_index=False)\n    \n    return oversampled_X, oversampled_y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets test the functionality of the above function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_oversamp, y_oversamp = oversample_data(X_train_full, y)\n\nprint(f\"Original sizes: \\n- X training: {X_train_full.shape} \\n- y: {y.shape}\\n\") \nprint(f\"New oversampled sizes: \\n- X training: {X_oversamp.shape} \\n- y: {y_oversamp.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_oversamp.sum().sort_values()[:30].plot.bar(figsize=(18,6))\nplt.ylabel(\"Class Counts\", weight='bold')\nplt.title(\"Count of minority classes\", weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good, we can now apply this into our training if / when required."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"model-production\"></a>\n## 4. Model production and evaluation of a Tuned Deep NN"},{"metadata":{},"cell_type":"markdown","source":"Lets split our data randomly for an initial validation set. Ideally we'd perform a multi-label stratified split here, but due to issues with limited numbers of class instances and imbalance across the dataset, we'll avoid it for now.\n\nOnce we've refined some of the basic hyper-parameters, we'll perform more in-depth optimisation and evaluation using K-Folds cross validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# choose a larger subset for this evaluation\nX_train, X_val, y_train, y_val = train_test_split(X_train_full, y, test_size=0.2, shuffle=True)\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"standard-3layer\"></a>\n### 4.1 Model production - ANN with three hidden layers on single hold-out validation set"},{"metadata":{},"cell_type":"markdown","source":"Lets first create a tuned ANN without any Convolutional layers, so that we've got something to compare against.\n\nWe'll create a three layered model with ELU activations, He normal weight initialisation, dropout regularisation, and Adam optimisation with a suitable learning rate decay."},{"metadata":{"trusted":true},"cell_type":"code","source":"def ann_model_1(dropout=True, dropout_val=0.45, lr=2e-3, \n                output_shape=206, input_feat_dim=X_train_full.shape[1]):\n    \"\"\" Create a basic Deep NN for classification \"\"\"\n    model = models.Sequential()\n    \n    model.add(layers.Dense(512, activation='elu', input_shape=(input_feat_dim,), \n                           kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    if dropout:\n        model.add(layers.Dropout(dropout_val))\n    model.add(layers.Dense(256, activation='elu', kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    if dropout:\n        model.add(layers.Dropout(dropout_val))\n    model.add(layers.Dense(256, activation='elu', kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    if dropout:\n        model.add(layers.Dropout(dropout_val))\n        \n    # output layer\n    model.add(layers.Dense(206, activation='sigmoid'))\n        \n    model.compile(optimizer=keras.optimizers.Adam(lr=lr), \n                  loss='binary_crossentropy', metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluating a suitable learning rate for this model:"},{"metadata":{},"cell_type":"markdown","source":"Lets create a custom callback for exploring the best learning rates for our models:"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LearningRateComparison(keras.callbacks.Callback):\n    def __init__(self, factor):\n        self.factor = factor\n        \n        # arrays to store current rate and associated loss\n        self.lr_rates = []\n        self.losses = []\n        \n    def on_batch_end(self, batch, logs):\n        self.lr_rates.append(K.get_value(self.model.optimizer.lr))\n        self.losses.append(logs[\"loss\"])\n        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train for one or two epochs with our learning rate comparison:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define custom learning rate scheduler to compare loss across many learning rates\ncustom_lr = LearningRateComparison(factor=1.0025)\n\nmodel_1 = ann_model_1(dropout=True, lr=1e-3)\n\n# train model for 10 epochs\nhistory = model_1.fit(X_train, y_train, epochs=10, \n                      batch_size=64, validation_data=(X_val, y_val), \n                      callbacks=[custom_lr])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.lineplot(custom_lr.lr_rates, custom_lr.losses)\nplt.gca().set_xscale('log')\nplt.hlines(min(custom_lr.losses), min(custom_lr.lr_rates), max(custom_lr.lr_rates), \n           linestyle='dashed')\nplt.axis([min(custom_lr.lr_rates), 1.0, 0, custom_lr.losses[0]])\nplt.xlabel(\"Learning rate\", weight='bold', size=13)\nplt.ylabel(\"Loss\", weight='bold', size=13)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our loss begins to increase after around $ 1 \\times 10^{-1} $, however this is quite high for an initial learning rate. We'll try a learning rate that is reasonably lower than this, but still high enough to provide sufficient training and convergence times."},{"metadata":{},"cell_type":"markdown","source":"#### Training our model on the training set and evaluating on the hold-out validation split "},{"metadata":{},"cell_type":"markdown","source":"For this we'll make use of learning rate decay with a scheduler, along with an early stop callback so that we can obtain the best model found throughout the training process."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_1 = ann_model_1(dropout=True, lr=1e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def schedule_lr_rate(epoch, lr):\n    \"\"\" Use initial learning rate for 20 epochs and then\n        decrease it exponentially \"\"\"\n    if epoch < 20:\n        return lr\n    else:\n        return lr * tf.math.exp(-0.1)\n\n# create our lr scheduler - use reduceLRonPlat below - better performance\n#lr_scheduler = tf.keras.callbacks.LearningRateScheduler(schedule_lr_rate)\n\n# create learning rate scheduler\nlr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, \n                                                   patience=3, verbose=0, \n                                                   min_delta=0.0001, mode='min')\n\n# create an early stopper callback\nearly_stopper = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n\n# list of callbacks to use\ntrg_callbacks = [early_stopper, lr_scheduler]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"history = model_1.fit(X_train, y_train, epochs=75, \n                      batch_size=64, validation_data=(X_val, y_val), \n                      callbacks=trg_callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_history_results(history, metric='accuracy', figsize=(16,6)):\n    \"\"\" Helper function for plotting history from keras model \"\"\"\n    \n    # gather desired features\n    trg_loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    trg_acc = history.history[f'{metric}']\n    val_acc = history.history[f'val_{metric}']\n    epochs = range(1, len(trg_acc) + 1)\n\n    # plot losses and accuracies for training and validation \n    fig = plt.figure(figsize=figsize)\n    ax = fig.add_subplot(1, 2, 1)\n    plt.plot(epochs, trg_loss, marker='o', label='Training Loss')\n    plt.plot(epochs, val_loss, marker='x', label='Validation Loss')\n    plt.title(\"Training / Validation Loss\")\n    ax.set_ylabel(\"Loss\")\n    ax.set_xlabel(\"Epochs\")\n    plt.legend(loc='best')\n\n    ax = fig.add_subplot(1, 2, 2)\n    plt.plot(epochs, trg_acc, marker='o', label=f'Training {metric}')\n    plt.plot(epochs, val_acc, marker='^', label=f'Validation {metric}')\n    plt.title(f\"Training / Validation {metric}\")\n    ax.set_ylabel(f\"{metric}\")\n    ax.set_xlabel(\"Epochs\")\n    plt.legend(loc='best')\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history_results(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds = model_1.predict(X_val)\nloss = keras.losses.BinaryCrossentropy()(y_val, val_preds)\nprint(f'Validation Log Loss: {loss.numpy():.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not too shabby, however we didn't put much (if any!) effort into forming our hold out validation splits, so its difficult to say for certain how well our model is actually performing. Instead, lets evaluate it on a range of validation splits using K-folds cross validation instead. This is more computationally intensive, but a much better way of gauging how well our model actually performs."},{"metadata":{},"cell_type":"markdown","source":"Before we move on, just out of interest, lets see how we improve / impact our score on the validation set when we clip our prediction values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_min = 0.001\npred_max = 0.999\n\nclipped_val_preds = np.clip(val_preds, pred_min, pred_max)\nloss = keras.losses.BinaryCrossentropy()(y_val, clipped_val_preds)\nprint(f'Validation Log Loss (with clipped preds): {loss.numpy():.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's actually worse in this case. However, this could be something to keep in mind and try again for different sets of results. Let's also see, just the curiosity, how our log loss is impacted when we set a threshold and round our predictions to hard classified labels:"},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = 0.5\nrounded_preds = np.where(val_preds > threshold, 1.0, 0.0)\nloss = keras.losses.BinaryCrossentropy()(y_val, rounded_preds)\nprint(f'Validation Log Loss (with rounded preds): {loss.numpy():.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"kfolds-cross-validation\"></a>\n### 4.2 Improvements on evaluation of our ANN model - K Folds Cross Validation"},{"metadata":{},"cell_type":"markdown","source":"We'll use 7 folds of cross-validation, and for each fold we'll evaluate a new model trained on the respective training and validation splits. In addition, we can also make test predictions during each fold with each model, which can then be combined into an overall ensemble of predictions. In general, this approach should give us a better performance due to the mix of training data our different models are using."},{"metadata":{"trusted":true},"cell_type":"code","source":"def perform_cross_validation(X_full, y_full, model_func, nfolds=7, lr=1e-3, epochs=100, \n                             batch_size=64, callbacks=trg_callbacks, test_set=None, \n                             print_scores=True, random_state=12):\n    \"\"\" Perform cross-validation and return histories, validation losses and averaged \n    test set predictions across all folds. \"\"\"\n    model_histories = []\n    model_losses = []\n    test_preds = np.zeros((test_features.shape[0], \n                           train_targets_scored.shape[1] - 1))\n    \n    k_folds = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n\n    for train_idx, val_idx in tqdm(k_folds.split(X_full, y_full)):\n        train_split = X_full.iloc[train_idx].copy()\n        train_labels = y_full.iloc[train_idx].astype(np.float64).copy()\n        val_split = X_full.iloc[val_idx].copy()\n        val_labels = y_full.iloc[val_idx].astype(np.float64).copy()\n    \n        fold_model = model_func(lr=lr)\n    \n        # train model for 100 epochs with early stopping\n        temp_history = fold_model.fit(train_split, train_labels, \n                                epochs=epochs, batch_size=batch_size, \n                                verbose=0, validation_data=(val_split, val_labels), \n                                callbacks=[callbacks])\n    \n        model_histories.append(temp_history)\n        model_val_preds = fold_model.predict(val_split)\n        model_log_loss = keras.losses.BinaryCrossentropy()(val_labels, \n                                                       model_val_preds).numpy()\n        model_losses.append(model_log_loss)\n        if print_scores:\n            print(f'Current Fold Validation Loss: {model_log_loss:.4f}')\n    \n        # test preds if selected\n        if test_set:\n            temp_test_preds = fold_model.predict(test_set)\n            test_preds += (temp_test_preds / nfolds)\n\n    model_losses = np.array(model_losses)\n    \n    return model_histories, model_losses, test_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets perform this a few times with different random seeds, and see what the results yield:"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_states = [12, 24, 37, 42]\ntest_preds = np.zeros((test_features.shape[0], \n                       train_targets_scored.shape[1] - 1))\n\nfor iteration, rand_state in enumerate(random_states):\n    print(f\"Performing Iteration {iteration + 1} of Cross-Validations.\\n\")\n    _, iter_losses, iter_preds = perform_cross_validation(X_train_full, y, ann_model_1, \n                                                          random_state=rand_state)\n    \n    print(f\"Iteration {iteration + 1} mean loss over all folds: \"\n          f\"{iter_losses.mean():.4f} +/- {iter_losses.std():.4f}\\n\")\n    \n    test_preds +- (iter_preds / len(random_states))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Combining the predictions from cross-validation into an overall averaged set of test predictions\n\nSince we removed all instances with cp_type == ctl_vehicle from our training data, we will need to adjust our test set predictions so that the targets are always zero for these instances."},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_test_preds(test_preds, original_test_feats, clip_preds=False, \n                       pred_max=0.999, pred_min=0.001):\n    \"\"\" Adjust our test set predictions by replacing all class preds\n        with zero when cp_type == ctl_vehicle \"\"\"\n    corrected_preds = test_preds.copy()\n    test_sig_ids = original_test_feats['sig_id'].copy()\n    test_ctl_vehicle_idx = (original_test_feats['cp_type'] == 'ctl_vehicle')\n    corrected_preds[test_sig_ids[test_ctl_vehicle_idx].index.values] = 0\n\n    if clip_preds:\n        corrected_preds[:, 1:] = np.clip(corrected_preds[:, 1:], pred_min, pred_max)\n    \n    return corrected_preds, test_sig_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds, test_sig_ids = process_test_preds(test_preds, test_features, clip_preds=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_submission = pd.DataFrame({'sig_id' : test_sig_ids})\ntest_preds_df = pd.DataFrame(test_preds, columns=train_targets_scored.columns[1:])\ntest_submission = pd.concat([test_submission, test_preds_df], axis=1)\ntest_submission.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great, this can now be submitted if desired. Alternatively, we can try to improve on these results using more complex architectures and data processing."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"feature-engineered-evaluation\"></a>\n### 4.3 Evaluating our model with the additional engineered features"},{"metadata":{},"cell_type":"markdown","source":"Lets investigate whether our performance improves or not when we make use of the additional features engineered earlier. This is as simple as swapping out our original full training set (X_train_full) for the extended training set (X_train_ext) in our cross-validation code:"},{"metadata":{"trusted":true},"cell_type":"code","source":"N_FOLDS = 7\nk_folds = KFold(n_splits=N_FOLDS, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_histories = []\nmodel_losses = []\ntest_preds_ext = np.zeros((test_features.shape[0], \n                       train_targets_scored.shape[1] - 1))\n\nfor train_idx, val_idx in tqdm(k_folds.split(X_train_ext, y)):\n    train_split = X_train_ext.iloc[train_idx].copy()\n    train_labels = y.iloc[train_idx].astype(np.float64).copy()\n    val_split = X_train_ext.iloc[val_idx].copy()\n    val_labels = y.iloc[val_idx].astype(np.float64).copy()\n    \n    temp_model = ann_model_1(dropout=True, lr=1e-3, input_feat_dim=X_train_ext.shape[1])\n    \n    # train model for 100 epochs with early stopping\n    temp_history = temp_model.fit(train_split, train_labels, \n                            epochs=75, batch_size=64, verbose=0,\n                            validation_data=(val_split, val_labels), \n                                  callbacks=[trg_callbacks])\n    \n    model_histories.append(temp_history)\n    \n    model_val_preds = temp_model.predict(val_split)\n    \n    model_log_loss = keras.losses.BinaryCrossentropy()(val_labels, model_val_preds).numpy()\n    print(f'Current Fold Validation Loss: {model_log_loss:.4f}')\n    \n    model_losses.append(model_log_loss)\n    \n    temp_test_preds = temp_model.predict(X_test_ext)\n    test_preds_ext += (temp_test_preds / N_FOLDS)\n\nmodel_losses = np.array(model_losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Mean loss across all folds: {model_losses.mean():.4f} +/- {model_losses.std():.4f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems in this case the additional columns actually decrease performance, although this is debatable with such a small margin. We would need to repeat this many times to know for certain, using different randomised splits of folds. Never the less, we did not obtain the boost in performance we hoped, although it would be naive to expect such a simple feature engineering approach to yield anything spectacular."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"dimensionally-reduced-evaluation\"></a>\n### 4.4 Evaluating performance with dimensionally reduced features"},{"metadata":{},"cell_type":"markdown","source":"For completeness, along with testing extra features above, we'll also evaluate the effects of dimensionally reducing some of our data. We'll take our cell and gene features, and simply reduce them whilst retaining most of the variance using PCA. We'll then apply K-Folds cross validation again and see what impacts it has on performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create our pca transformers for each\ngene_pca = PCA(n_components=0.95)\ncell_pca = PCA(n_components=0.95)\n\n# fit transform training set with 95% variance retained\nX_gene_rd = gene_pca.fit_transform(X_train_gene.reshape(X_train_gene.shape[:-1]))\nX_cell_rd = cell_pca.fit_transform(X_train_cell.reshape(X_train_cell.shape[:-1])) \nX_train_rd = X_train_full.iloc[:, :3].copy()\nX_train_rd = np.concatenate([X_train_rd, X_gene_rd, X_cell_rd], axis=1)\n\n# transform test set using pca transformers from above\ntest_gene_rd = gene_pca.transform(X_test_gene.reshape(X_test_gene.shape[:-1]))\ntest_cell_rd = cell_pca.transform(X_test_cell.reshape(X_test_cell.shape[:-1]))\nX_test_rd = X_test.iloc[:, :3].copy()\nX_test_rd = np.concatenate([X_test_rd, test_gene_rd, test_cell_rd], axis=1)\n\n# verify both feature dimensions match\nX_train_rd.shape, X_test_rd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_histories = []\nmodel_losses = []\ntest_preds_rd = np.zeros((test_features.shape[0], \n                       train_targets_scored.shape[1] - 1))\n\nfor train_idx, val_idx in tqdm(k_folds.split(X_train_rd, y)):\n    train_split = X_train_rd[train_idx].copy()\n    train_labels = y.iloc[train_idx].astype(np.float64).copy()\n    val_split = X_train_rd[val_idx].copy()\n    val_labels = y.iloc[val_idx].astype(np.float64).copy()\n    \n    temp_model = ann_model_1(dropout=True, lr=1e-3, input_feat_dim=X_train_rd.shape[1])\n    \n    # train model for 100 epochs with early stopping\n    temp_history = temp_model.fit(train_split, train_labels, \n                            epochs=75, batch_size=64, verbose=0,\n                            validation_data=(val_split, val_labels), \n                                  callbacks=[trg_callbacks])\n    \n    model_histories.append(temp_history)\n    \n    model_val_preds = temp_model.predict(val_split)\n    \n    model_log_loss = keras.losses.BinaryCrossentropy()(val_labels, model_val_preds).numpy()\n    print(f'Current Fold Validation Loss: {model_log_loss:.4f}')\n    \n    model_losses.append(model_log_loss)\n    \n    temp_test_preds = temp_model.predict(X_test_rd)\n    test_preds_rd += (temp_test_preds / N_FOLDS)\n\nmodel_losses = np.array(model_losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Mean loss across all folds: {model_losses.mean():.4f} +/- {model_losses.std():.4f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not too bad, although the performance is no improvement over what we had originally with the standard features. Dimensionality reduction techniques tend to work well with complex data problems, particularly when there are a large number of redundant and correlated features. \n\nIf you train a simple linear model or other classical model on this dataset, then it is likely you'll obtain a good improvement on the final performance if you perform dimensionality reduction. However, in the case of a Deep NN with a huge number of model parameters, we dont actually obtain an improvement, since the model is more than capable of distilling the complexity contained within the original high-dimensional dataset. All we obtain in this case is a slightly faster computational time during training due to having less dimensions."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"oversampled-evaluation\"></a>\n### 4.5 Evaluating performance when we apply cross validation with oversampling of minority classes"},{"metadata":{},"cell_type":"markdown","source":"Finally, in addition to testing our ANN network on various dataset combinations and parts thereof, we'll also try a kind of oversampling technique on our training data as we train each model on our cross-validation folds. Its unlikely this will be reflected in a better validation score, since we're particularly focussing on minority classes. However, it could result in a better generalisation performance of our model when exposed to new and totally unseen data, such as that in the public test set and hidden (held-out) test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"N_FOLDS = 7\nk_folds = KFold(n_splits=N_FOLDS, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_histories = []\nmodel_losses = []\ntest_preds_os = np.zeros((test_features.shape[0], \n                          train_targets_scored.shape[1] - 1))\n\nfor train_idx, val_idx in tqdm(k_folds.split(X_train_full, y)):\n    train_split = X_train_full.iloc[train_idx].copy()\n    train_labels = y.iloc[train_idx].astype(np.float64).copy()\n    val_split = X_train_full.iloc[val_idx].copy()\n    val_labels = y.iloc[val_idx].astype(np.float64).copy()\n    \n    # oversample our data for training split\n    train_split.reset_index(inplace=True, drop=True)\n    train_labels.reset_index(inplace=True, drop=True)\n    train_split, train_labels = oversample_data(train_split, \n                                                train_labels)\n    \n    print(f\"Size of training split: {train_split.shape}\")\n    \n    temp_model = ann_model_1(dropout=True, lr=1e-3)\n    \n    # train model for 100 epochs with early stopping\n    temp_history = temp_model.fit(train_split, train_labels, \n                            epochs=75, batch_size=64, verbose=0,\n                            validation_data=(val_split, val_labels), \n                                  callbacks=[trg_callbacks])\n    \n    model_histories.append(temp_history)\n    \n    model_val_preds = temp_model.predict(val_split)\n    \n    model_log_loss = keras.losses.BinaryCrossentropy()(val_labels, model_val_preds).numpy()\n    print(f'Current Fold Validation Loss: {model_log_loss:.4f}')\n    \n    model_losses.append(model_log_loss)\n    \n    temp_test_preds = temp_model.predict(X_test)\n    test_preds_os += (temp_test_preds / N_FOLDS)\n\nmodel_losses = np.array(model_losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Mean loss across oversampled folds: {model_losses.mean():.4f} +/- {model_losses.std():.4f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history_results(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We've seen that our performance is relatively consistent above, regardless of whether we include the additional features, conduct over-sampling or apply dimensionality reduction. \n\nTo enhanced our individual predictions obtained above, we'll combine them all into one overall ensemble set of predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"dnn_test_preds = test_preds + test_preds_ext + test_preds_rd + test_preds_os / 4.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a rather crude way of combining our predictions from this section, but it will suffice for the purpose of this work."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"composite-models\"></a>\n## 5. Trying something novel - ANN with supplementary 1D Conv inputs"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"single-Conv1d\"></a>\n### 5.1 ConvNet experiment - validate that a single 1D ConvNet can learn and predict using the data"},{"metadata":{},"cell_type":"markdown","source":"Just to make sure we're not completely wasting time, lets first establish that a basic 1D ConvNet can actually learn from our data and make meaningful predictions. We'll form a simple model that takes as input our gene numerical features, and train it to make predictions on our scored training labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"def single_conv1d(num_features, dropout_val=0.45, lr=1e-3):\n    model = Sequential()\n    model.add(layers.Input(shape=(num_features, 1)))\n    model.add(layers.Conv1D(filters=64, kernel_size=7))\n    model.add(layers.MaxPooling1D(5))\n    model.add(layers.Conv1D(filters=32, kernel_size=5))\n    model.add(layers.MaxPooling1D(5))\n    model.add(layers.Conv1D(filters=16, kernel_size=2))\n    model.add(layers.Flatten())\n    \n    # follow-up dense layers\n    model.add(Dense(256, activation='elu', kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(layers.Dropout(dropout_val))\n    \n    # output layer\n    model.add(Dense(206, activation='sigmoid'))\n    \n    model.compile(optimizer=keras.optimizers.Adam(lr=lr), \n                  loss=['binary_crossentropy'], \n                  metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cell_cnn = single_conv1d(num_features=X_train_gene.shape[1])\ncell_cnn.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see if this model even has any chance of working - we'll train for 50 epochs and see how it performs using just the Gene features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an early stopper callback\nearly_stopper = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n\n# create learning rate scheduler\nlr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, \n                                                   patience=3, verbose=0, \n                                                   min_delta=0.0001, mode='min')\n\n# list of callbacks to use\ntrg_callbacks = [early_stopper, lr_scheduler]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model for 100 epochs with early stopping\nhistory = cell_cnn.fit(X_train_gene, y, epochs=50, batch_size=64, verbose=1,\n                              validation_split=0.2, callbacks=trg_callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history_results(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performance might not be the best, and training time takes noticeably longer than a dense network, but this model definitely has the potential to learn and make predictions. This is reinforcing, and means that there is scope to form more complex models that blend both convolutional neural networks and dense neural network layers together.\n\nWe'll explore this next."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"multi-input-convnet\"></a>\n### 5.2 Combine gene and cell data using a multi-input Convolutional Neural Network"},{"metadata":{},"cell_type":"markdown","source":"We'll make a model that takes the gene and cell data in seperately and processes them using 1D convolutional layers. Simultaneously, we'll take in the categorical features from the original data, plus some common engineered features from the numerical data.\n\nWe can do this using the previous features we engineered from the data numerical features. In addition, we already processed our gene and cell data into a form amenable for CNNs."},{"metadata":{"trusted":true},"cell_type":"code","source":"def multi_input_cnn(dropout_val=0.45, lr=1e-3):\n    \"\"\" Composite ANN with multiple inputs, including the cell, gene and\n        other general features. \"\"\"\n    # define input layers\n    main_input = layers.Input(shape=X_train_misc.shape[1])\n    gene_input = layers.Input(shape=(X_train_gene.shape[1], 1))\n    cell_input = layers.Input(shape=(X_train_cell.shape[1], 1))\n    \n    # define main input processing layers\n    input_batch_norm = BatchNormalization()(main_input)\n    hidden_1 = layers.Dense(128, activation='elu', \n                            kernel_initializer='he_normal')(input_batch_norm)\n    batch_norm_1 = BatchNormalization()(hidden_1)\n    dropout_1 = layers.Dropout(dropout_val)(batch_norm_1)\n    \n    # define gene convolutional layers\n    gene_conv1 = layers.Conv1D(filters=64, kernel_size=7)(gene_input)\n    gene_max_pool_1 = layers.MaxPooling1D(5)(gene_conv1)\n    gene_conv2 = layers.Conv1D(filters=32, kernel_size=5)(gene_max_pool_1)\n    gene_max_pool_2 = layers.MaxPooling1D(5)(gene_conv2)\n    gene_conv3 = layers.Conv1D(filters=16, kernel_size=2)(gene_max_pool_2)\n    gene_flatten = layers.Flatten()(gene_conv3)\n    \n    # define cell convolutional layers\n    cell_conv1 = layers.Conv1D(filters=64, kernel_size=7)(cell_input)\n    cell_max_pool_1 = layers.MaxPooling1D(5)(cell_conv1)\n    cell_conv2 = layers.Conv1D(filters=32, kernel_size=5)(cell_max_pool_1)\n    cell_max_pool_2 = layers.MaxPooling1D(5)(cell_conv2)\n    cell_conv3 = layers.Conv1D(filters=16, kernel_size=2)(cell_max_pool_2)\n    cell_flatten = layers.Flatten()(cell_conv3)\n    \n    # combine our multiple inputs into one\n    concat_layer = layers.concatenate([dropout_1, gene_flatten, \n                                       cell_flatten])\n    \n    # follow-up dense layers\n    hidden_2 = layers.Dense(256, activation='elu', \n                            kernel_initializer='he_normal')(concat_layer)\n    batch_norm_2 = BatchNormalization()(hidden_2)\n    dropout_2 = layers.Dropout(dropout_val)(batch_norm_2)\n    \n    hidden_3 = layers.Dense(256, activation='elu', \n                            kernel_initializer='he_normal')(dropout_2)\n    batch_norm_3 = BatchNormalization()(hidden_3)\n    dropout_3 = layers.Dropout(dropout_val)(batch_norm_3)\n    \n    # main output for our scored labels\n    output_1 = layers.Dense(206, activation='sigmoid', \n                            name='scored_output')(dropout_3)\n    \n    model = keras.Model(inputs=[main_input, gene_input, cell_input], \n                        outputs=[output_1])\n    \n    model.compile(optimizer=keras.optimizers.Adam(lr=lr), \n                  loss=['binary_crossentropy'], metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multifeature_cnn = multi_input_cnn()\n\n# lets visualise our model to ensure it looks correct\nkeras.utils.plot_model(multifeature_cnn, \"composite_model.png\", show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an early stopper callback\nearly_stopper = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n\n# create learning rate scheduler\nlr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, \n                                                   patience=3, verbose=0, \n                                                   min_delta=0.0001, mode='min')\n\n# list of callbacks to use\ntrg_callbacks = [early_stopper, lr_scheduler]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model for 50 epochs w/ early stop. Ensure we use three inputs:\nhistory = multifeature_cnn.fit([X_train_misc, X_train_gene, X_train_cell], y, \n                               epochs=50, batch_size=64, verbose=1,\n                               validation_split=0.2, callbacks=trg_callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history_results(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performance is not amazing in this case, but when we consider that we are mostly using the gene and cell data fed into multiple 1D convolutional layers, its certainly not bad!"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"composite-model\"></a>\n### 5.3 Produce a finalised composite network with both CNN features and full tabular features "},{"metadata":{},"cell_type":"markdown","source":"This will be more complicated, and will make use of our full original dataset, along with our gene and cell features being fed into seperate 1D convolutional layers. We'll branch these three inputs seperately and then combine them later in the network, and make use of the output to form our final predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"def ann_convnet_composite(dropout_val=0.45, lr=1e-3):\n    \"\"\" Composite ANN with multiple inputs, including the full tabular\n        data and the gene and cell features fed into 1D Conv layers \"\"\"\n    # define our input layers\n    main_input = layers.Input(shape=X_train_full.shape[1])\n    gene_input = layers.Input(shape=(X_train_gene.shape[1], 1))\n    cell_input = layers.Input(shape=(X_train_cell.shape[1], 1))\n    \n    # define main input processing layers\n    input_batch_norm = BatchNormalization()(main_input)\n    hidden_1 = layers.Dense(512, activation='elu', \n                            kernel_initializer='he_normal')(input_batch_norm)\n    batch_norm_1 = BatchNormalization()(hidden_1)\n    dropout_1 = layers.Dropout(dropout_val)(batch_norm_1)\n    \n    # define gene convolutional layers\n    gene_conv1 = layers.Conv1D(filters=64, kernel_size=7)(gene_input)\n    gene_max_pool_1 = layers.MaxPooling1D(5)(gene_conv1)\n    gene_conv2 = layers.Conv1D(filters=32, kernel_size=5)(gene_max_pool_1)\n    gene_max_pool_2 = layers.MaxPooling1D(5)(gene_conv2)\n    gene_conv3 = layers.Conv1D(filters=16, kernel_size=2)(gene_max_pool_2)\n    gene_flatten = layers.Flatten()(gene_conv3)\n    \n    # define cell convolutional layers\n    cell_conv1 = layers.Conv1D(filters=64, kernel_size=7)(cell_input)\n    cell_max_pool_1 = layers.MaxPooling1D(5)(cell_conv1)\n    cell_conv2 = layers.Conv1D(filters=32, kernel_size=5)(cell_max_pool_1)\n    cell_max_pool_2 = layers.MaxPooling1D(5)(cell_conv2)\n    cell_conv3 = layers.Conv1D(filters=16, kernel_size=2)(cell_max_pool_2)\n    cell_flatten = layers.Flatten()(cell_conv3)\n    \n    # combine our multiple inputs into one\n    concat_layer = layers.concatenate([dropout_1, gene_flatten, \n                                       cell_flatten])\n    \n    # follow-up dense layers\n    hidden_2 = layers.Dense(256, activation='elu', \n                            kernel_initializer='he_normal')(concat_layer)\n    batch_norm_2 = BatchNormalization()(hidden_2)\n    dropout_2 = layers.Dropout(dropout_val)(batch_norm_2)\n    \n    hidden_3 = layers.Dense(256, activation='elu', \n                            kernel_initializer='he_normal')(dropout_2)\n    batch_norm_3 = BatchNormalization()(hidden_3)\n    dropout_3 = layers.Dropout(dropout_val)(batch_norm_3)\n    \n    # main output for our scored labels\n    output_1 = layers.Dense(206, activation='sigmoid', \n                            name='scored_output')(dropout_3)\n    \n    model = keras.Model(inputs=[main_input, gene_input, cell_input], \n                        outputs=[output_1])\n    \n    model.compile(optimizer=keras.optimizers.Adam(lr=lr), \n                  loss=['binary_crossentropy'], metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets create our model and visualise its structure\ncomposite_model = ann_convnet_composite(dropout_val=0.45, lr=2e-3)\nkeras.utils.plot_model(composite_model, \"composite_model.png\", show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train one model to confirm our model works:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an early stopper callback\nearly_stopper = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n\n# create learning rate scheduler\nlr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, \n                                                   patience=3, verbose=0, \n                                                   min_delta=0.0001, mode='min')\n\n# list of callbacks to use\ntrg_callbacks = [early_stopper, lr_scheduler]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"composite_model = ann_convnet_composite(dropout_val=0.45, lr=2e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model for 100 epochs with early stopping\nhistory = composite_model.fit([X_train_full, X_train_gene, X_train_cell], y, \n                              epochs=75, batch_size=64, verbose=1,\n                              validation_split=0.2, callbacks=trg_callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate our predictions on entire training just for verification it works\nval_preds = composite_model.predict([X_train_full, X_train_gene, X_train_cell])\nscore = keras.losses.BinaryCrossentropy()(y, val_preds)\nprint(f'Log Loss on entire training set: {score.numpy():.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history_results(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar performance to what we achieved before with just a simple ANN.\n\nLets evaluate this further and in greater depth through K-Folds Cross Validation:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_histories = []\nmodel_losses = []\ncomposite_test_preds = np.zeros((test_features.shape[0], \n                                 train_targets_scored.shape[1] - 1))\n\nfor train_idx, val_idx in tqdm(k_folds.split(X_train_full, y)):\n    # obtain train and val splits\n    train_split = X_train_full.iloc[train_idx].copy()\n    train_labels = y.iloc[train_idx].astype(np.float64).copy()\n    val_split = X_train_full.iloc[val_idx].copy()\n    val_labels = y.iloc[val_idx].astype(np.float64).copy()\n    \n    # gene and cell training / validation splits\n    train_cell = X_train_cell[train_idx].copy()\n    train_gene = X_train_gene[train_idx].copy()\n    val_cell = X_train_cell[val_idx].copy()\n    val_gene = X_train_gene[val_idx].copy()\n    \n    composite_nn = ann_convnet_composite(dropout_val=0.45, lr=2e-3)\n    \n    # train initially on the nonscored labels\n    history = composite_nn.fit([train_split, train_gene, train_cell], \n                               train_labels, epochs=75, batch_size=64, verbose=0, \n                               validation_data=([val_split, val_gene, val_cell], \n                                                val_labels,), callbacks=[trg_callbacks])\n    \n    model_histories.append(history)\n\n    # find log loss for our main output\n    model_val_preds = composite_nn.predict([val_split, val_gene, val_cell])\n    model_log_loss = keras.losses.BinaryCrossentropy()(val_labels, model_val_preds).numpy()\n    model_losses.append(model_log_loss)\n    print(f'Current Fold Scored Validation Loss: {model_log_loss:.4f}')\n    \n    # make predictions on test set for each fold\n    temp_test_preds = composite_nn.predict([X_test, X_test_gene, X_test_cell])\n    composite_test_preds += (temp_test_preds / N_FOLDS)\n\n# convert results to np array\nmodel_losses = np.array(model_losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Mean loss across all folds: {model_losses.mean():.4f} +/- {model_losses.std():.4f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"test-predictions\"></a>\n## 6. Test Set Predictions - Combining the performance of the best models"},{"metadata":{},"cell_type":"markdown","source":"Lets make a submission to the competition using the test predictions obtained earlier. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# choose our final predictions to submit\n#final_preds = (dnn_test_preds + composite_test_preds) / 2.0\nfinal_preds = test_preds.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before submitting this, we will need to adjust our test set predictions so that the targets are always zero for instances where cp_type refers to a controlled experiment:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_test_preds(test_preds, original_test_feats, clip_preds=False, \n                       pred_max=0.999, pred_min=0.001):\n    \"\"\" Adjust our test set predictions by replacing all class preds\n        with zero when cp_type == ctl_vehicle \"\"\"\n    corrected_preds = test_preds.copy()\n    test_sig_ids = original_test_feats['sig_id'].copy()\n    test_ctl_vehicle_idx = (original_test_feats['cp_type'] == 'ctl_vehicle')\n    corrected_preds[test_sig_ids[test_ctl_vehicle_idx].index.values] = 0\n\n    if clip_preds:\n        corrected_preds[:, 1:] = np.clip(corrected_preds[:, 1:], pred_min, pred_max)\n    \n    return corrected_preds, test_sig_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_preds, test_sig_ids = process_test_preds(final_preds, test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_submission = pd.DataFrame({'sig_id' : test_sig_ids})\ntest_preds_df = pd.DataFrame(processed_preds, columns=train_targets_scored.columns[1:])\ntest_submission = pd.concat([test_submission, test_preds_df], axis=1)\ntest_submission.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this in the correct format, we can now save it and make a basic submission for the competition:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}