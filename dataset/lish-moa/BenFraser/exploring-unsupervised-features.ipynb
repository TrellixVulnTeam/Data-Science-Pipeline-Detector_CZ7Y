{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Unsupervised Learning and Exploration of MoA Gene and Cell Features"},{"metadata":{},"cell_type":"markdown","source":"This notebook attempts to explore the MoA data through various clustering and unsupervised learning techniques. It then follows up on this by making a set of predictions on the test set for this multi-label classification problem.\n\nIn its default state, the dataset has a large number of dimensions, many of which are redundant and potentially related to one-another. Discovering of insights through clustering and unsupervised techniques can be useful for performing feature engineering and discovering hidden relationships within our data. During the basic work in this notebook, we isolate both the gene and cell data, and investigate the optimal number of clusters using common clustering techniques, such as KMeans, DBSCAN, t-SNE (for visualisation), and Gaussian Mixture Models. \n\nAfter performing this exploration, some simple linear models are produced and evaluated in terms of their performance with different sub-sets of clustered features. This could be extended to many different model types, but for the purpose of this short notebook only one simple model is tested.\n\n**Table of Contents:**\n\n1. [Imports](#imports)\n2. [EDA](#EDA)\n3. [KMeans Clustering and t-SNE Visualisation](#clustering-one)\n4. [PCA, t-SNE and DBSCAN Clustering](#clustering-two)\n5. [Model Production and Evaluation](#model-production)\n6. [Test Set Predictions](#test-predictions)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"imports\"></a>\n## 1. Import dependencies and data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nimport numpy as np\nimport seaborn as sns\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import log_loss, silhouette_score\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_predict, cross_validate, cross_val_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier, MultiOutputRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"input_dir = '/kaggle/input/lish-moa'\ntrain_features = pd.read_csv(os.path.join(input_dir, 'train_features.csv'))\ntrain_targets_scored = pd.read_csv(os.path.join(input_dir, 'train_targets_scored.csv'))\ntrain_targets_nonscored = pd.read_csv(os.path.join(input_dir, 'train_targets_nonscored.csv'))\ntest_features = pd.read_csv(os.path.join(input_dir, 'test_features.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape, train_targets_scored.shape, train_targets_nonscored.shape, test_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"EDA\"></a>\n## 2. Basic Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = ['cp_type', 'cp_time', 'cp_dose']\n\nplt.figure(figsize=(16,4))\n\nfor idx, col in enumerate(cat_cols):\n    plt.subplot(int(f'13{idx + 1}'))\n    labels = train_features[col].value_counts().index.values\n    vals = train_features[col].value_counts().values\n    sns.barplot(x=labels, y=vals)\n    plt.xlabel(f'{col}')\n    plt.ylabel('Count')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For 'cp_type', the 'ctl_vehicle' refers to samples treated with a control perturbation. For control perturbations, our targets are all zero, since they have no Mechanism of Action (MoA).\n\nTo deal with this, a good strategy could be to identify samples that are ctl_vehicle (through training a classification model or simply using the feature as its in the test data!), and set all of these to zero. We can then process the test set accordingly, by first setting all test instance targets to zero if its a ctl_vehicle, followed by processing all of the others normally using our trained model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# select all indices when 'cp_type' is 'ctl_vehicle'\nctl_vehicle_idx = (train_features['cp_type'] == 'ctl_vehicle')\n\n# evaluate number of 1s we have in the total train scores when cp_type = ctl_vehicle\ntrain_targets_scored.loc[ctl_vehicle_idx].iloc[:, 1:].sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The total sum is zero, which confirms the statement above on all targets being zero for cases where cp_type is ctl_vehicle. The best thing to do with this is simply fill our targets for zero when this is the case.\n\nWe shall also remove all of these from the training set, since there is no need to unnecessarily complicate our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# take a copy of all our training sig_ids for reference\ntrain_sig_ids = train_features['sig_id'].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop cp_type column since we no longer need it\nX = train_features.drop(['sig_id', 'cp_type'], axis=1).copy()\nX = X.loc[~ctl_vehicle_idx].copy()\n\ny = train_targets_scored.drop('sig_id', axis=1).copy()\ny = y.loc[~ctl_vehicle_idx].copy()\n\nX.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feats = X.iloc[:, :2].copy()\nX_cell_v = X.iloc[:, -100:].copy()\nX_gene_e = X.iloc[:, 2:772].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feats.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cell_v.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_gene_e.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(X_cell_v)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(X_gene_e)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting all gene / cell features for random samples:"},{"metadata":{},"cell_type":"markdown","source":"Lets quickly assess how our cell data looks when plotted over all features for random instances:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feats = X.iloc[:, :2].copy()\nX_cell_v = X.iloc[:, -100:].copy()\nX_gene_e = X.iloc[:, 2:772].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_features(X, y, selected_idx, features_type, figsize=(14,10)):\n    x_range = range(1, X.shape[1] + 1)\n    \n    fig = plt.figure(figsize=(14,10))\n    \n    for i, idx in enumerate(selected_idx):\n        ax = fig.add_subplot(selected_idx.shape[0], 1, i + 1)\n        vals = X.iloc[idx].values\n    \n        if (y.iloc[idx] == 1).sum():\n            output_labels = list(y.iloc[idx][y.iloc[idx] == 1].index.values)\n        \n            labels = \" \".join(output_labels)\n        else:\n            labels = \"None (all labels zero)\"\n        \n        sns.lineplot(x_range, vals)\n        plt.title(f\"Row {idx}, Labels: {labels}\", weight='bold')\n        plt.xlim(0.0, X.shape[1])\n        plt.grid()\n\n    plt.xlabel(f\"{features_type}\", weight='bold', size=14)\n    plt.tight_layout()\n    plt.show()\n    \n    \ndef plot_mean_std(dataframe, feature_name, features_type, figsize=(14,6), alpha=0.3):\n    \"\"\" Plot rolling mean and standard deviation for given dataframe \"\"\"\n    \n    plt.figure(figsize=figsize)\n    \n    x_range = range(1, dataframe.shape[1] + 1)\n    \n    chosen_rows = y.loc[y[feature_name] == 1]\n    chosen_feats = dataframe.loc[y[feature_name] == 1]\n    \n    means = chosen_feats.mean()\n    stds = chosen_feats.std()\n    \n    plt.plot(x_range, means, label=feature_name)    \n    plt.fill_between(x_range, means - stds, means + stds, \n                         alpha=alpha)\n\n    plt.title(f'{features_type}: {feature_name} - Mean & Standard Deviation', weight='bold')\n    \n    plt.xlim(0.0, dataframe.shape[1])\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets plot some random rows from our data\nrandom_idx = np.random.randint(X.shape[0], size=(5,))\n\nplot_features(X_cell_v, y, random_idx, features_type='Cell Features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly some rows vary substancially in terms of their value range, and therefore it is worth standardising this data prior to training our models.\n\nNow lets do the same for our gene features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_features(X_gene_e, y, random_idx, features_type='Gene Features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have some noticeable peaks throughout the features for some of the above instances. It could be worth plotting a range of data instances with the same output labels against one another, and compare their peaks. If they correlate in one or more areas, this could be insightful for developing further features with our dataset.\n\nLets now repeat above, but for data instances with the same output label(s)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# select an output label to plot associated training features\nchosen_label = 'btk_inhibitor'\nchosen_rows = y.loc[y[chosen_label] == 1]\nchosen_feats = X_gene_e.loc[y[chosen_label] == 1]\n\n# select random rows from those available above for the chosen label\nrandom_idx = np.random.choice(range(0, chosen_rows.shape[0]), size=(5,), replace=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_features(chosen_feats, chosen_rows, random_idx, features_type='Gene Features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets also look at the mean and standard deviation of this feature:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_mean_std(X_gene_e, 'btk_inhibitor', 'Gene Features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets repeat this process for some different output labels:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# select an output label to plot associated training features\nchosen_label = 'histamine_receptor_antagonist'\nchosen_rows = y.loc[y[chosen_label] == 1]\nchosen_feats = X_gene_e.loc[y[chosen_label] == 1]\n\n# select random rows from those available above for the chosen label\nrandom_idx = np.random.choice(range(0, chosen_rows.shape[0]), size=(5,))\n\nplot_features(chosen_feats, chosen_rows, random_idx, features_type='Gene Features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_mean_std(X_gene_e, 'histamine_receptor_antagonist', 'Gene Features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# select an output label to plot associated training features\nchosen_label = 'free_radical_scavenger'\nchosen_rows = y.loc[y[chosen_label] == 1]\nchosen_feats = X_gene_e.loc[y[chosen_label] == 1]\n\n# select random rows from those available above for the chosen label\nrandom_idx = np.random.choice(range(0, chosen_rows.shape[0]), size=(5,))\n\nplot_features(chosen_feats, chosen_rows, random_idx, features_type='Gene Features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_mean_std(X_gene_e, 'free_radical_scavenger', 'Gene Features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This analysis highlights the potential for performing advanced feature engineering, such as using the trends of gene and/or cell features as additional features to our models. We could use such features to supplement the existing data in its standard form. We could also investigate the relationships of our unsupervised work to these types of trends for different features."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"clustering-one\"></a>\n## 3. Clustering of our splits of features"},{"metadata":{},"cell_type":"markdown","source":"To speed up our clustering significantly, we'll only use a random subset of the total data, else it will take an extremely long time for some of our exploration, e.g. Gaussian Mixture models."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_sample = X.sample(10000, random_state=12)\nX_cell_v = X_sample.iloc[:, -100:].copy()\nX_gene_e = X_sample.iloc[:, 2:772].copy()\nX_cell_gene = X_sample.iloc[:, 2:].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Clustering and exploring Cell features using KMeans"},{"metadata":{"trusted":true},"cell_type":"code","source":"k_range = [x for x in range(1, 25, 1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time k_kmeans = [KMeans(n_clusters=k, random_state=12).fit(X_cell_v) for k in k_range]\ninertias = [model.inertia_ for model in k_kmeans]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nsns.lineplot(k_range, inertias)\nsns.scatterplot(k_range, inertias)\nplt.xlabel(\"Clusters, $k$\", fontsize=14, weight='bold')\nplt.ylabel(\"Inertia\", fontsize=14, weight='bold')\nplt.grid()\nplt.xlim(0.0, 15.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a clear elbow located at k=2, which represents the optimal value of k to choose in this case. In general however, this might not always be the best choice, but somewhere around this point is usually a good start. We could experiment with both k=2 and k=3 and see what yields better results. \n\nWe can also evaluate this further using the silhouette score, which in practice can be a more effective technique. The only downside is the computational complexity, which we need to consider carefully if we want to evaluate a wide range of k values."},{"metadata":{"trusted":true},"cell_type":"code","source":"%time silhouette_scores = [silhouette_score(X_cell_v, model.labels_) for model in k_kmeans[1:]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 6))\nsns.lineplot(k_range[1:], silhouette_scores)\nsns.scatterplot(k_range[1:], silhouette_scores)\nplt.xlabel(\"Clusters, $k$\", fontsize=14, weight='bold')\nplt.ylabel(\"Silhoutte Score\", fontsize=14, weight='bold')\nplt.grid()\nplt.xlim(0.0, 15.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we dont want to use too few clusters (we need a lot of information to provide insights for the 206 output classes), we can compromise with this and select around 4 clusters."},{"metadata":{},"cell_type":"markdown","source":"#### Visualisation of these clusters using T-SNE"},{"metadata":{"trusted":true},"cell_type":"code","source":"cell_k = 10\nkmeans = KMeans(n_clusters=cell_k)\nkm_cell_feats = kmeans.fit_transform(X_cell_v)\nkmeans_cell_labels = kmeans.predict(X_cell_v)\n\nkm_cell_feats.shape, kmeans_cell_labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(verbose=1, perplexity=100, n_jobs=-1)\n%time X_cell_embedded = tsne.fit_transform(km_cell_feats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sns settings\nsns.set(rc={'figure.figsize':(14,10)})\npalette = sns.hls_palette(cell_k, l=.4, s=.8)\n\n# plot t-SNE with annotations from k-means clustering\nsns.scatterplot(X_cell_embedded[:,0], X_cell_embedded[:,1], \n                hue=kmeans_cell_labels, legend='full', palette=palette)\nplt.title('t-SNE on our Cell data with K-Means Clustered labels', weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Optional extra exploration for interest - Fitment of a Gaussian Mixture Model to our data\n\nLets also try and fit a Gaussian Mixture model to our data. This is more difficult due to the very slow computation time, and so its essential that we use only our subset of data, rather than the entire training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"k_range = [x for x in range(1, 10)]\nk_range.extend([x for x in range(10, 21, 2)])\naic_scores = []\nbic_scores = []\n\nfor k in tqdm(k_range):\n    gm_k = GaussianMixture(n_components=k, n_init=10, random_state=12).fit(X_cell_v)\n    aic_scores.append(gm_k.aic(X_cell_v))\n    bic_scores.append(gm_k.bic(X_cell_v))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The computational time increases significantly as the number of clusters increases in this case."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nsns.lineplot(k_range, aic_scores, color=\"tab:blue\", label='AIC')\nsns.scatterplot(k_range, aic_scores, color=\"tab:blue\")\n\nsns.lineplot(k_range, bic_scores, color=\"tab:green\", label='BIC')\nsns.scatterplot(k_range, bic_scores, color=\"tab:blue\")\n\nplt.xlabel(\"Clusters, $k$\", fontsize=14, weight='bold')\nplt.ylabel(\"Information Criterion\", fontsize=14, weight='bold')\nplt.legend()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"AIC minimum at {k_range[np.argmin(aic_scores)]} clusters.\")\nprint(f\"BIC minimum at {k_range[np.argmin(bic_scores)]} clusters.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AIC appears to keep decreasing after 4 clusters, but not at a significant amount. You can see that the rate at which it decreases slows considerably after it has reached 4 clusters. In addition, the Bayesian Information Criterion (BIC), seems to have the best score at 4 clusters, and then worsens as we increase clusters beyond this amount.\n\nThus, 4 clusters is probably a reasonable initial choice for the number of clusters in our model in this case."},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Clustering and exploring Gene features using KMeans"},{"metadata":{"trusted":true},"cell_type":"code","source":"k_range = [x for x in range(1, 25, 1)]\nk_range.extend([50, 100, 150, 200, 250])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time k_kmeans = [KMeans(n_clusters=k, random_state=12).fit(X_gene_e) for k in k_range]\ninertias = [model.inertia_ for model in k_kmeans]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nsns.lineplot(k_range, inertias)\nsns.scatterplot(k_range, inertias)\nplt.xlabel(\"Clusters, $k$\", fontsize=14, weight='bold')\nplt.ylabel(\"Inertia\", fontsize=14, weight='bold')\nplt.grid()\nplt.xlim(0.0, 25.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time silhouette_scores = [silhouette_score(X_gene_e, model.labels_) for model in k_kmeans[1:]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 6))\nsns.lineplot(k_range[1:], silhouette_scores)\nsns.scatterplot(k_range[1:], silhouette_scores)\nplt.xlabel(\"Clusters, $k$\", fontsize=14, weight='bold')\nplt.ylabel(\"Silhoutte Score\", fontsize=14, weight='bold')\nplt.grid()\nplt.xlim(0.0, 15.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualisation of these clusters using t-SNE"},{"metadata":{"trusted":true},"cell_type":"code","source":"gene_k = 6\nkmeans = KMeans(n_clusters=gene_k)\nkm_gene_feats = kmeans.fit_transform(X_gene_e)\nkmeans_gene_labels = kmeans.predict(X_gene_e)\n\nkm_gene_feats.shape, kmeans_gene_labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(verbose=1, perplexity=100, n_jobs=-1)\n%time X_gene_embedded = tsne.fit_transform(km_gene_feats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sns settings\nsns.set(rc={'figure.figsize':(14,10)})\npalette = sns.hls_palette(gene_k, l=.4, s=.8)\n\n# plot t-SNE with annotations from k-means clustering\nsns.scatterplot(X_gene_embedded[:,0], X_gene_embedded[:,1], \n                hue=kmeans_gene_labels, legend='full', palette=palette)\nplt.title('t-SNE with labels obtained from K-Means Clustering', weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Optional extra exploration for interest - Gaussian Mixture Model estimation\n\nSimilarly to previous, lets assess the performance of clustering using a Gaussian Mixture model."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_tf_gene = PCA(n_components=0.90)\nX_gene_e_red = pca_tf_gene.fit_transform(X_gene_e)\nprint(f\"Original data: {X_gene_e.shape} \\nPCA Reduced data: {X_gene_e_red.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_range = [x for x in range(1, 11)]\nk_range.extend([12, 15, 30, 50])\ngene_aic_scores = []\ngene_bic_scores = []\n\nfor k in tqdm(k_range):\n    gm_k = GaussianMixture(n_components=k, n_init=10, random_state=12).fit(X_gene_e_red)\n    gene_aic_scores.append(gm_k.aic(X_gene_e_red))\n    gene_bic_scores.append(gm_k.bic(X_gene_e_red))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nsns.lineplot(k_range, gene_aic_scores, color=\"tab:blue\", label='AIC')\nsns.scatterplot(k_range, gene_aic_scores, color=\"tab:blue\")\n\nsns.lineplot(k_range, gene_bic_scores, color=\"tab:green\", label='BIC')\nsns.scatterplot(k_range, gene_bic_scores, color=\"tab:blue\")\n\nplt.xlabel(\"Clusters, $k$\", fontsize=14, weight='bold')\nplt.ylabel(\"Information Criterion\", fontsize=14, weight='bold')\nplt.title(\"Gene Features Gaussian Mixture Model Clustering\")\nplt.legend()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the Bayesian Information Criterion (BIC) penalises model complexity much more, which leads to BIC steadily increasing as we increase from 2 clusters. AIC on the other hand, continues to improve as we increase the number of clusters. \n\nFrom these criteria alone, it is not straightforward to choose the optimal number of clusters in this case."},{"metadata":{"trusted":true},"cell_type":"code","source":"aic_arr = np.array(gene_aic_scores)\nbic_arr = np.array(gene_bic_scores)\ntotal = aic_arr + bic_arr\nprint(f\"Cluster number with minimum sum of AIC and BIC: {k_range[np.argmin(total)]}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For our gene data, a good value of k clusters to choose could be around 8 in this case, since it provides a good compromise between both BIC and AIC."},{"metadata":{},"cell_type":"markdown","source":"### 3.3 KMeans Clustering on combined cell and gene data using KMeans"},{"metadata":{"trusted":true},"cell_type":"code","source":"k_range = [x for x in range(1, 25, 1)]\nk_range.extend([50, 100, 150, 200, 250])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time k_kmeans = [KMeans(n_clusters=k, random_state=12).fit(X_cell_gene) for k in k_range]\ninertias = [model.inertia_ for model in k_kmeans]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.lineplot(k_range, inertias)\nsns.scatterplot(k_range, inertias)\nplt.xlabel(\"Clusters, $k$\", fontsize=14, weight='bold')\nplt.ylabel(\"Inertia\", fontsize=14, weight='bold')\nplt.grid()\nplt.xlim(0.0, 50.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time silhouette_scores = [silhouette_score(X_cell_gene, model.labels_) for model in k_kmeans[1:]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 6))\nsns.lineplot(k_range[1:], silhouette_scores)\nsns.scatterplot(k_range[1:], silhouette_scores)\nplt.xlabel(\"Clusters, $k$\", fontsize=14, weight='bold')\nplt.ylabel(\"Silhoutte Score\", fontsize=14, weight='bold')\nplt.grid()\nplt.xlim(0.0, 15.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Overall, I think KMeans struggles to cluster our data effectively and into any meaningful splits. We could probably do better through applying a more complex clustering algorithm, such as a variation of Kernal PCA, or spectral clustering. Despite this, we'll use this work with KMeans clustering to produce a basic pipeline and compare how it impacts / improves our performance on the given problem."},{"metadata":{},"cell_type":"markdown","source":"#### Visualisation of combined clusters using t-SNE"},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_k = 4\nkmeans = KMeans(n_clusters=combined_k)\nkm_comb_feats = kmeans.fit_transform(X_cell_gene)\nkmeans_comb_labels = kmeans.predict(X_cell_gene)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(verbose=1, perplexity=100, n_jobs=-1)\n%time X_comb_embedded = tsne.fit_transform(km_comb_feats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sns settings\nsns.set(rc={'figure.figsize':(14,10)})\npalette = sns.hls_palette(combined_k, l=.4, s=.8)\n\n# plot t-SNE with annotations from k-means clustering\nsns.scatterplot(X_comb_embedded[:,0], X_comb_embedded[:,1], \n                hue=kmeans_comb_labels, legend='full', palette=palette)\nplt.title('t-SNE with labels obtained from K-Means Clustering', weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"clustering-two\"></a>\n## 4. DBSCAN Clustering on our data"},{"metadata":{},"cell_type":"markdown","source":"As an experiment we'll perform DBSCAN clustering on our data, and visualise our clusters on a t-SNE 2-D projection of our dimensionality reduced data (obtained using PCA for convenience)."},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Dimensionality reduction using PCA and t-SNE"},{"metadata":{},"cell_type":"markdown","source":"First of all, lets reduce the dimensionality of our gene and combined data (since they are high-dimensional), and transform it to 2-dimensions using t-SNE:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_gene = PCA(n_components=0.99)\npca_combined = PCA(n_components=0.99)\n\nX_gene_e_rd = pca_gene.fit_transform(X_gene_e)\nX_cell_gene_rd = pca_combined.fit_transform(X_cell_gene)\n\nX_gene_e_rd.shape, X_cell_gene_rd.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets reduce these to 2-dimensions using t-SNE:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_cell = TSNE(verbose=1, perplexity=100, n_jobs=-1)\ntnse_gene = TSNE(verbose=1, perplexity=100, n_jobs=-1)\ntnse_combined = TSNE(verbose=1, perplexity=100, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time X_cell_v_tsne = tsne_cell.fit_transform(X_cell_v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time X_gene_e_tsne = tnse_gene.fit_transform(X_gene_e_rd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time X_cell_gene_tsne = tnse_gene.fit_transform(X_gene_e_rd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,5))\nax = fig.add_subplot(1, 3, 1)\nsns.scatterplot(X_cell_v_tsne[:,0], X_cell_v_tsne[:,1], legend='full')\nax.set_title('Cell Features t-SNE', weight='bold')\n\nax = fig.add_subplot(1, 3, 2)\nsns.scatterplot(X_gene_e_tsne[:,0], X_gene_e_tsne[:,1], legend='full', color='tab:orange')\nax.set_title('Gene Features t-SNE', weight='bold')\n\nax = fig.add_subplot(1, 3, 3)\nsns.scatterplot(X_cell_gene_tsne[:,0], X_cell_gene_tsne[:,1], legend='full', color='tab:red')\nax.set_title('Combined Gene and Cell Features t-SNE', weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Applying DBSCAN to our data"},{"metadata":{},"cell_type":"markdown","source":"Now lets apply DBSCAN to our data and attempt to cluster it. The issue with high-dimensional data is that as our dimensionality grows, the more everything tends to becoming an outlier, which is referred to as the curse of dimensionality. This is especially true for density-based techniques such as DBSCAN, and so reducing our dimensions to a lower number of features first is generally required. If we dont do this, we'll end up with an unreasonable number of outliers within our results, regardless of the amount we tweak the epsilon and number of sample parameters.\n\nWe'll apply basic PCA first to provide us with a low number of dimensions, and then apply DBSCAN. Rather than keeping 90%-95% variance like above, we'll have to reduce this considerably further, since density estimations can struggle significantly after around 10 dimensions."},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_dbs_gene_pca = PCA(n_components=10)\npre_dbs_cell_pca = PCA(n_components=10)\npre_dbs_comb_pca = PCA(n_components=10)\n\ncell_reduced = pre_dbs_gene_pca.fit_transform(X_gene_e)\ngene_reduced = pre_dbs_cell_pca.fit_transform(X_cell_v)\ncombined_reduced = pre_dbs_comb_pca.fit_transform(X_cell_gene)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dbscan_cell = DBSCAN(eps=13, min_samples=5)\ndbscan_cell.fit(cell_reduced)\nnp.unique(dbscan_cell.labels_, return_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dbscan_gene = DBSCAN(eps=3, min_samples=4)\ndbscan_gene.fit(gene_reduced)\nnp.unique(dbscan_gene.labels_, return_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dbscan_comb = DBSCAN(eps=3, min_samples=5)\ndbscan_comb.fit(combined_reduced)\nnp.unique(dbscan_comb.labels_, return_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(17,6))\ncell_palette = sns.hls_palette(len(np.unique(dbscan_cell.labels_)), l=.4, s=.8)\nax = fig.add_subplot(1, 3, 1)\nsns.scatterplot(X_cell_v_tsne[:,0], X_cell_v_tsne[:,1], \n                hue=dbscan_cell.labels_, legend='full', palette=cell_palette)\nax.set_title('Cell t-SNE & DBSCAN Clusters', weight='bold')\n\nax = fig.add_subplot(1, 3, 2)\ngene_palette = sns.hls_palette(len(np.unique(dbscan_gene.labels_)), l=.4, s=.8)\nsns.scatterplot(X_gene_e_tsne[:,0], X_gene_e_tsne[:,1], color='tab:orange',\n                hue=dbscan_gene.labels_, legend='full', palette=gene_palette)\nax.set_title('Gene t-SNE & DBSCAN Clusters', weight='bold')\n\nax = fig.add_subplot(1, 3, 3)\ncomb_palette = sns.hls_palette(len(np.unique(dbscan_comb.labels_)), l=.4, s=.8)\nsns.scatterplot(X_cell_gene_tsne[:,0], X_cell_gene_tsne[:,1], color='tab:red',\n                hue=dbscan_comb.labels_, legend='full', palette=comb_palette)\nax.set_title('Combined Gene and Cell t-SNE & DBSCAN Clusters', weight='bold')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfortunately, our DBSCAN results were not great in this case. Perhaps the chosen method of dimensionality reduction using PCA was not a great choice, and has resulted in the poor results we see above. Better choices could be randomised and/or non-dimensional forms of clustering, such as kernal PCA or spectral clustering."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"model-production\"></a>\n## 5. Basic Pipeline and Evaluation of Models with clustered features"},{"metadata":{},"cell_type":"markdown","source":"We'll create a basic pipeline that combines our clustered (dimensionally reduced) features. For clarity, this will contain the clustered features from the cell data, the clustered features from the gene data, and also the clustered data from a combination both combined (which may produce additional different clusters than either alone).\n\nWith this data, we can perform evaluate of performance with the following configurations:\n\n- Original Data cross-validation\n- Individual Clustered data cross-validation\n- Combined clustered data cross-validation\n- Original data + variations of the clustered data\n\nWe'll use the optimal clusters for each of these as identified previously."},{"metadata":{"trusted":true},"cell_type":"code","source":"# standardise our numerical features data prior to clustering\nstd_scaler = StandardScaler()\nX.iloc[:, 2:] = std_scaler.fit_transform(X.iloc[:, 2:].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cell_kmeans = KMeans(n_clusters=4)\ngene_kmeans = KMeans(n_clusters=4)\ncomb_kmeans = KMeans(n_clusters=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets transform each of these splits accordingly. We'll one-hot encode the categorical features, and cluster the numerical with the optimal cluster numbers found above:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encode our categorical features\nX_cats = X.iloc[:, :2].copy()\nX_cats['cp_time'] = X_cats['cp_time'].astype('object')\nX_cats = pd.get_dummies(X_cats)\n\n# obtain our splits for gene and cell data\nX_cell_gene = X.iloc[:, 2:].copy()\nX_cell = X.iloc[:, -100:].copy()\nX_gene = X.iloc[:, 2:772].copy()\n\nX_cats.shape, X_cell_gene.shape, X_cell_v.shape, X_gene_e.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time X_cell_rd = cell_kmeans.fit_transform(X_cell)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time X_gene_rd = gene_kmeans.fit_transform(X_gene)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time X_cell_gene_rd = comb_kmeans.fit_transform(X_cell_gene)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cell_rd.shape, X_gene_rd.shape, X_cell_gene_rd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine all of our features into one\ncat_feats = list(X_cats.columns.values)\ncell_feats = [f\"cell_clust_{x}\" for x in range(1, X_cell_rd.shape[1] + 1)]\ngene_feats = [f\"gene_clust_{x}\" for x in range(1, X_gene_rd.shape[1] + 1)]\ncombined_feats = [f\"cell_gene_clust_{x}\" for x in range(1, X_cell_gene_rd.shape[1] + 1)]\n\ncombined = np.c_[X_cats, X_cell_rd, X_gene_rd, X_cell_gene_rd]\nX_all_rd = pd.DataFrame(combined, columns=cat_feats + cell_feats + gene_feats + combined_feats)\nX_all_rd.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Case 0 (Benchmark) - Linear Regression model on original processed features"},{"metadata":{"trusted":true},"cell_type":"code","source":"original = np.c_[X_cats, X_cell, X_gene]\nX_original = pd.DataFrame(original, columns= cat_feats + \n                          list(X_cell.columns.values) + \n                          list(X_gene.columns.values))\nX_original.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate using cross-validation\nlin_reg = LinearRegression()\nlr_val_preds_0 = cross_val_predict(lin_reg, X_original, y, cv=5)\n\n# in order to effective work out log loss, we need to flatten both arrays before computing log loss\nlr_log_loss = log_loss(np.ravel(y), np.ravel(lr_val_preds_0))\nprint(f\"Log loss for our Linear Regression Model: {lr_log_loss:.5f}\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Case 1: All clustered features together"},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate using cross-validation\nlin_reg = LinearRegression()\nlr_val_preds_1 = cross_val_predict(lin_reg, X_all_rd, y, cv=5)\n\n# in order to effective work out log loss, we need to flatten both arrays before computing log loss\nlr_log_loss = log_loss(np.ravel(y), np.ravel(lr_val_preds_1))\nprint(f\"Log loss for our Linear Regression Model: {lr_log_loss:.5f}\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model_1 = LinearRegression().fit(X_all_rd, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This log loss is much better than our original log loss on the entire dataset (Case 0 above).\n\nLets see how a tree based classifier does (extra trees classifier in this case):"},{"metadata":{"trusted":true},"cell_type":"code","source":"#et_clf = ExtraTreesClassifier(n_jobs=-1)\n#%time et_val_preds = cross_val_predict(et_clf, X_all_rd, y, cv=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# in order to effective work out log loss, we need to flatten both arrays before computing log loss\n#et_log_loss = log_loss(np.ravel(y), np.ravel(et_val_preds))\n#print(f\"Log loss for Extra Trees Classifier: {et_log_loss:.5f}\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The log loss appears to be much worse on our extra trees classifier in this case. We've likely reduced too much information from our data as a result of the clustering performed, which has a tendency to reduce the performance of more complex model types such as random forests, gradient boosting and deep neural networks."},{"metadata":{},"cell_type":"markdown","source":"### Case 2: Clustered Features with Original Features"},{"metadata":{},"cell_type":"markdown","source":"We'll now combine the original features (one-hot encoded cat columns, unclustered cell and gene data, combined with the clustered cell and gene data).\n\nDue to the large number of dimensions of this case, we'll just experiment with a quick linear regression model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_combined = np.c_[X_cats, X_cell, X_gene, X_cell_rd, X_gene_rd]\nX_extended = pd.DataFrame(all_combined, columns=(cat_feats + list(X_cell.columns.values) +\n                                                 list(X_gene.columns.values)+ cell_feats + gene_feats))\n\nX_extended.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_extended.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate using cross-validation\nlin_reg = LinearRegression()\nlr_val_preds_2 = cross_val_predict(lin_reg, X_extended, y, cv=5)\n\n# in order to effective work out log loss, we need to flatten both arrays before computing log loss\nlr_log_loss = log_loss(np.ravel(y), np.ravel(lr_val_preds_2))\nprint(f\"Log loss for our Linear Regression Model: {lr_log_loss:.5f}\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the case of our linear regressor, performance is actually worse when we include the additional features from the original dataframe. It's likely a simple linear regression model is not complex enough to exploit the large number of features effectively."},{"metadata":{},"cell_type":"markdown","source":"### Case 3: Only the individual clustered columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"clustered = np.c_[X_cats, X_cell_rd, X_gene_rd]\nX_clustered = pd.DataFrame(clustered, columns= cat_feats + cell_feats + gene_feats)\n\nX_clustered.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate using cross-validation\nlin_reg = LinearRegression()\nlr_val_preds_3 = cross_val_predict(lin_reg, X_clustered, y, cv=5)\n\n# in order to effective work out log loss, we need to flatten both arrays before computing log loss\nlr_log_loss = log_loss(np.ravel(y), np.ravel(lr_val_preds_3))\nprint(f\"Log loss for our Linear Regression Model: {lr_log_loss:.5f}\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model_2 = LinearRegression().fit(X_clustered, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The log loss is actually lowest when we use a smaller subset of clustered features."},{"metadata":{},"cell_type":"markdown","source":"### Case 4: Only combined cell and gene clustered features"},{"metadata":{"trusted":true},"cell_type":"code","source":"clustered = np.c_[X_cats, X_cell_gene_rd]\nX_clustered = pd.DataFrame(clustered, columns= cat_feats + combined_feats)\n\nX_clustered.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate using cross-validation\nlin_reg = LinearRegression()\nlr_val_preds_4 = cross_val_predict(lin_reg, X_clustered, y, cv=5)\n\n# in order to effective work out log loss, we need to flatten both arrays before computing log loss\nlr_log_loss = log_loss(np.ravel(y), np.ravel(lr_val_preds_4))\nprint(f\"Log loss for our Linear Regression Model: {lr_log_loss:.5f}\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model_3 = LinearRegression().fit(X_clustered, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Case 5: Ensemble of our different clustered linear regression models"},{"metadata":{},"cell_type":"markdown","source":"Lets combine all of our previous models together (except Case 2 which was poor), and see how well it fares:"},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_val_preds = (lr_val_preds_1 + lr_val_preds_3 + lr_val_preds_4) / 3.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# in order to effective work out log loss, we need to flatten both arrays before computing log loss\ncomb_log_loss = log_loss(np.ravel(y), np.ravel(avg_val_preds))\nprint(f\"Log loss for our Linear Regression Model: {comb_log_loss:.5f}\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This performance is best overall combined to either of the attempts above. It's worth repeating this work for the test set and making a prediction accordingly."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"test-predictions\"></a>\n## 6. Test set predictions"},{"metadata":{},"cell_type":"markdown","source":"### 6.1 Preprocess our test set as required"},{"metadata":{"trusted":true},"cell_type":"code","source":"# take a copy of all our training sig_ids for reference\ntest_sig_ids = test_features['sig_id'].copy()\n\n# select all indices when 'cp_type' is 'ctl_vehicle'\ntest_ctl_vehicle_idx = (test_features['cp_type'] == 'ctl_vehicle')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test_features.drop(['sig_id', 'cp_type'], axis=1).copy()\n\n# standardise our test set numerical features\nX_test.iloc[:, 2:] = std_scaler.fit_transform(X_test.iloc[:, 2:].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_cat = X_test.iloc[:, :2].copy()\nX_test_cat['cp_time'] = X_test_cat['cp_time'].astype('object')\nX_test_cat = pd.get_dummies(X_test_cat)\n\nX_test_cell = X_test.iloc[:, -100:].copy()\nX_test_gene = X_test.iloc[:, 2:772].copy()\nX_test_cell_gene = X_test.iloc[:, 2:].copy()\n\nX_test_cat.shape, X_test_cell.shape, X_test_gene.shape, X_test_cell_gene.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transform our test set using the kmean clusters found earler:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_cell_rd = cell_kmeans.transform(X_test_cell)\nX_test_gene_rd = gene_kmeans.transform(X_test_gene)\nX_test_cell_gene_rd = comb_kmeans.transform(X_test_cell_gene)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.2 Form our model variations and make predictions"},{"metadata":{},"cell_type":"markdown","source":"#### Model 1 - All clustered features together"},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine all of our features into one\ntest_combined = np.c_[X_test_cat, X_test_cell_rd, X_test_gene_rd, X_test_cell_gene_rd]\nX_test_1 = pd.DataFrame(test_combined, columns=cat_feats + cell_feats + gene_feats + combined_feats)\n\n# make predicts on this data using model 1 (trained previously)\nmodel_1_preds = lr_model_1.predict(X_test_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 2 - Only individual clustered columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_clustered = np.c_[X_test_cat, X_test_cell_rd, X_test_gene_rd]\nX_test_2 = pd.DataFrame(test_clustered, columns= cat_feats + cell_feats + gene_feats)\n\n# make predicts on this data using model 2 (trained previously)\nmodel_2_preds = lr_model_2.predict(X_test_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 3 - Only combined cell and gene clustered features"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_clust_comb = np.c_[X_test_cat, X_test_cell_gene_rd]\nX_test_3 = pd.DataFrame(test_clust_comb, columns= cat_feats + combined_feats)\n\n# make predicts on this data using model 3 (trained previously)\nmodel_3_preds = lr_model_3.predict(X_test_3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Average our individual model predictions into one final set"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = (model_1_preds + model_2_preds + model_3_preds) / 3.0\ntest_preds.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Final tuning of our predictions"},{"metadata":{},"cell_type":"markdown","source":"We now need to update all of the predictions for cp_type == ctl_vehicle so that they are zero."},{"metadata":{"trusted":true},"cell_type":"code","source":"# change all cp_type == ctl_vehicle predictions to zero\ntest_preds[test_sig_ids[test_ctl_vehicle_idx].index.values] = 0\n\n# confirm all values now sum to zero for these instances\ntest_preds[test_sig_ids[test_ctl_vehicle_idx].index.values].sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also have many values outside the range of 0 and 1, since we've used a regression model. Since our output results should be probabilities, we need to set any values greater than 1 to 1, and any negative values to zero."},{"metadata":{"trusted":true},"cell_type":"code","source":"# we have some values above 1 and below 0 - this needs amending since probs should only be 0-1\ntest_preds[test_preds > 1.0] = 1.0\ntest_preds[test_preds < 0.0] = 0.0\n\n# confirm these values are all corrected\ntest_preds.max(), test_preds.min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = pd.DataFrame(test_preds, columns=train_targets_scored.columns.values[1:])\ntest_submission = pd.DataFrame({'sig_id' : test_sig_ids})\ntest_submission[test_preds.columns] = test_preds\ntest_submission.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save our submission as csv\ntest_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}