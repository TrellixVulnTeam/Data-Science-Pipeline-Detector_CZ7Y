{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\nfrom tensorflow_addons.layers import WeightNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set seed\ndef seed_everything(seed = 42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(seed = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading in all the data files\ntrain_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ntest_features = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\ntrain_targets_scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('/kaggle/input/lish-moa/train_targets_nonscored.csv')\nsample_submission = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind_te = test_features[test_features['cp_type']=='ctl_vehicle'].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    d = df.copy()\n    # change cp_dose: D1 -> 0, D2 -> 1\n    d['cp_dose'] = d['cp_dose'].map({'D1':0, 'D2':1})\n    # change cp_time: 24 -> 0, 48 -> 1, 72 -> 2\n    d['cp_time'] = d['cp_time']//24-1\n    # change cp_type: trt_cp -> 1, ctl_vehicle -> 0\n    d['cp_type'] = d['cp_type'].map({'ctl_vehicle':0, 'trt_cp':1})\n    # drop the sig_id column\n    d.drop(columns = ['sig_id'], inplace = True)\n    return d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = preprocess(train_features)\nX_test = preprocess(test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape, X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train_targets_scored.copy()\n# drop the sig_id column\ny_train.drop(columns = ['sig_id'], inplace = True)\ny_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stop training when the validation log loss metric has stopped decreasing for 3 epochs.\ndef callbacks():\n    early_stopping = EarlyStopping(monitor = 'val_logloss',\n                                   patience = 3,\n                                   mode = 'min',\n                                   restore_best_weights = True)\n    return early_stopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the neural network model\ndef create_model(num_columns):\n    model = Sequential()\n    model.add(Input(num_columns))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    model.add(WeightNormalization(Dense(2048, activation=\"relu\")))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(WeightNormalization(Dense(1024, activation=\"relu\")))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(WeightNormalization(Dense(512, activation=\"relu\")))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(WeightNormalization(Dense(206, activation=\"sigmoid\")))\n    model.compile(optimizer = Adam(), loss = 'binary_crossentropy')\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feats = [0,1,2,.......,873]\nfeats = np.arange(0,X_train.shape[1],1)\ninp_size = 874","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a sample df to store predicted values\nsample = sample_submission.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = y_train.copy()\nsample.loc[:, y_train.columns] = 0\nres.loc[:, y_train.columns] = 0\nprint(sample.shape, res.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from time import time\nimport datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_loss_metric(y_true, y_pred):\n    metrics = []\n    for _target in y_train.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels = [0,1]))\n    return np.mean(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = []\nres_preds = []\nnp.random.seed(seed=42)\nn_splits = 5\nn_top = 10\nn_round = 1\n\nfor seed in range(n_round):\n    split_cols = np.random.choice(feats, inp_size, replace = False)\n    res.loc[:, y_train.columns] = 0\n    sample.loc[:, y_train.columns] = 0\n    for n, (tr, te) in enumerate(KFold(n_splits = n_splits, random_state = seed, shuffle = True).split(X_train, y_train)):\n        start_time = time()\n        x_tr = X_train.values[tr][:, split_cols]\n        x_val = X_train.values[te][:, split_cols]\n        y_tr, y_val = y_train.values[tr], y_train.values[te]\n        x_tt = X_test.values[:, split_cols]   \n        for num in range(n_top):\n            model = create_model(inp_size)\n            model.fit(x_tr, y_tr,validation_data=(x_val, y_val), epochs = 25, batch_size = 128,\n                      callbacks = callbacks(), verbose = 0)\n            sample.loc[:, y_train.columns] += model.predict(x_tt, batch_size = 128)/(n_splits*n_top)\n            res.loc[te, y_train.columns] += model.predict(x_val, batch_size = 128)/(n_top)\n        loss = log_loss_metric(y_train.loc[te,y_train.columns], res.loc[te, y_train.columns])\n        print(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}], Seed {seed}, Fold {n}:', loss)\n        K.clear_session()\n        del model\n    # set prediction 0 for ctl_vehicle observations\n    sample.loc[ind_te, y_train.columns] = 0\n    test_preds.append(sample.copy())    \n    res_preds.append(res.copy())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(res_preds), len(test_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aa = [1.0]\nsample.loc[:, y_train.columns] = 0\nfor i in range(n_round):\n    sample.loc[:, y_train.columns] += aa[i] * test_preds[i].loc[:, y_train.columns]\nsample.loc[ind_te, y_train.columns] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(sample.head())\n# write sample to submission.csv file\nsample.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}