{"cells":[{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/customtabnet')\nfrom pytorch_tabnet_custom.tab_model import TabNetRegressor as TabNetRegressorCustom","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterativestratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"os.listdir('../input/lish-moa')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ngroups = pd.read_csv(\"../input/lish-moa/train_drug.csv\", index_col=\"sig_id\", squeeze=True)\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn.model_selection._split import _BaseKFold\n\n\nclass MultilabelStratifiedGroupKFold(_BaseKFold):\n    def __init__(self, n_splits=5, random_state=None, shuffle=False):\n        super().__init__(n_splits=n_splits, random_state=random_state, shuffle=shuffle)\n\n    def _iter_test_indices(self, X=None, y=None, groups=None):\n        cv = MultilabelStratifiedKFold(\n            n_splits=self.n_splits,\n            random_state=self.random_state,\n            shuffle=self.shuffle,\n        )\n\n        value_counts = groups.value_counts()\n        regluar_indices = value_counts.loc[\n            (value_counts == 6) | (value_counts == 12) | (value_counts == 18)\n        ].index.sort_values()\n        irregluar_indices = value_counts.loc[\n            (value_counts != 6) & (value_counts != 12) & (value_counts != 18)\n        ].index.sort_values()\n\n        group_to_fold = {}\n        tmp = y.groupby(groups).mean().loc[regluar_indices]\n\n        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n            group_to_fold.update({group: fold for group in tmp.index[test]})\n\n        sample_to_fold = {}\n        tmp = y.loc[groups.isin(irregluar_indices)]\n\n        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n            sample_to_fold.update({sample: fold for sample in tmp.index[test]})\n\n        folds = groups.map(group_to_fold)\n        is_na = folds.isna()\n        folds[is_na] = folds[is_na].index.map(sample_to_fold).values\n\n        for i in range(self.n_splits):\n            yield np.where(folds == i)[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'll check distributions of g-* and c-* of train and test set. They are spiky distribution rather than normal distribution. Regardless of the train and test, they look be in the same shape."},{"metadata":{},"cell_type":"markdown","source":"**train set before using RankGauss**"},{"metadata":{"trusted":true},"cell_type":"code","source":"gnum = train_features[GENES].shape[1]\ngraphs = []\n\nfor i in range(0, gnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=771:\n            break\n        graph = sns.distplot(train_features[GENES].values[:,item], ax=axs[k])\n        graph.set_title(f\"g-{item}\")\n        graphs.append(graph)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnum = train_features[CELLS].shape[1]\ngraphs = []\n\nfor i in range(0, cnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=100:\n            break\n        graph = sns.distplot(train_features[CELLS].values[:,item], ax=axs[k])\n        graph.set_title(f\"c-{item}\")\n        graphs.append(graph)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**test set before using RankGauss**"},{"metadata":{"trusted":true},"cell_type":"code","source":"gnum = test_features[GENES].shape[1]\ngraphs = []\n\nfor i in range(0, gnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=771:\n            break\n        graph = sns.distplot(test_features[GENES].values[:,item], ax=axs[k])\n        graph.set_title(f\"g-{item}\")\n        graphs.append(graph)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnum = test_features[CELLS].shape[1]\ngraphs = []\n\nfor i in range(0, cnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=100:\n            break\n        graph = sns.distplot(test_features[CELLS].values[:,item], ax=axs[k])\n        graph.set_title(f\"c-{item}\")\n        graphs.append(graph)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It may be a too simple idea, it appears that the gene expression data and cell viability data can be controlled by the experimenter, so it is safe to assume that these data are independent of each other.\n\nAlso, since the shape of the distribution is close to normal distribution to begin with, I don't think there is much of a problem if it is forced to be transformed into a Gaussian distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"#RankGauss\nrank_gauss = {}\nfor col in (GENES + CELLS):\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    ##raw_vec = train_features[col].values.reshape(vec_len, 1)\n    raw_vec = pd.concat([train_features,test_features])[col].values.reshape(vec_len + vec_len_test, 1)\n    transformer.fit(raw_vec)\n    rank_gauss[col] = transformer\n    \npickle.dump(rank_gauss, open('rank_gauss.pkl', 'wb'))\nrank_gauss = pickle.load(open('rank_gauss.pkl', 'rb'))\n\nfor col in (GENES + CELLS):\n    transformer = rank_gauss[col]\n    train_features[col] = transformer.transform(train_features[col].values.reshape(vec_len, 1)).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can confirme that the shapes of data got close to the normal distribution.\n\n**train set after using RankGauss**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gnum = train_features[GENES].shape[1]\ngraphs = []\n\nfor i in range(0, gnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=771:\n            break\n        graph = sns.distplot(train_features[GENES].values[:,item], ax=axs[k])\n        graph.set_title(f\"g-{item}\")\n        graphs.append(graph)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnum = train_features[CELLS].shape[1]\ngraphs = []\n\nfor i in range(0, cnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=100:\n            break\n        graph = sns.distplot(train_features[CELLS].values[:,item], ax=axs[k])\n        graph.set_title(f\"c-{item}\")\n        graphs.append(graph)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**test set after using RankGauss**"},{"metadata":{"trusted":true},"cell_type":"code","source":"gnum = test_features[GENES].shape[1]\ngraphs = []\n\nfor i in range(0, gnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=771:\n            break\n        graph = sns.distplot(test_features[GENES].values[:,item], ax=axs[k])\n        graph.set_title(f\"g-{item}\")\n        graphs.append(graph)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnum = test_features[CELLS].shape[1]\ngraphs = []\n\nfor i in range(0, cnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=100:\n            break\n        graph = sns.distplot(test_features[CELLS].values[:,item], ax=axs[k])\n        graph.set_title(f\"c-{item}\")\n        graphs.append(graph)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears that we were able to transform the distribution of each data to resemble a normal distribution, as intended.\n\nSo, let's enter the data into the benchmarking method to see the improvement."},{"metadata":{},"cell_type":"markdown","source":"# PCA features + Existing features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GENES\nn_comp = 600  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n#data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ngen_pca = PCA(n_components=n_comp, random_state=42).fit(data[GENES])\n\npickle.dump(gen_pca, open('gen_pca.pkl', 'wb'))\ngen_pca = pickle.load(open('gen_pca.pkl', 'rb'))\n\ndata2 = (gen_pca.transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CELLS\nn_comp = 50  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n#data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\n\ncel_pca = PCA(n_components=n_comp, random_state=42).fit(data[CELLS])\n\npickle.dump(cel_pca, open('cel_pca.pkl', 'wb'))\ncel_pca = pickle.load(open('cel_pca.pkl', 'rb'))\n\ndata2 = (cel_pca.transform(data[CELLS]))\n\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# feature Selection using Variance Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n\n\nvar_thresh = VarianceThreshold(0.8)  #<-- Update\ndata = train_features.append(test_features)\n#data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\nvar_thresh = var_thresh.fit(data.iloc[:, 4:])\n\npickle.dump(var_thresh, open('var_thresh.pkl', 'wb'))\nvar_thresh = pickle.load(open('var_thresh.pkl', 'rb'))\n\ndata_transformed = var_thresh.transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train.merge(groups, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_scored.columns]\ngroups = train['drug_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CV folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = train.copy()\n\n#msgkf = MultilabelStratifiedGroupKFold(n_splits=7)\n\n#for f, (t_idx, v_idx) in enumerate(msgkf.split(X=train, y=target, groups=groups)):\n#    folds.loc[v_idx, 'kfold'] = int(f)\n\n#folds['kfold'] = folds['kfold'].astype(int)\nfolds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\nprint(sample_submission.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            # true_dist = pred.data.clone()\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_EPOCH=200\ntabnet_params = dict(n_d=32, n_a=32, n_steps=3, gamma=1.3,\n                     lambda_sparse=0, optimizer_fn=torch.optim.Adam,\n                     optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n                     mask_type='entmax',\n                     scheduler_params=dict(mode=\"min\",\n                                           patience=5,\n                                           min_lr=1e-5,\n                                           factor=0.9,),\n                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                     verbose=10,\n                     )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\nfrom pytorch_tabnet_custom.metrics import Metric\nfrom sklearn.metrics import roc_auc_score, log_loss\n\nclass LogitsLogLoss(Metric):\n    \"\"\"\n    LogLoss with sigmoid applied\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        \"\"\"\n        Compute LogLoss of predictions.\n\n        Parameters\n        ----------\n        y_true: np.ndarray\n            Target matrix or vector\n        y_score: np.ndarray\n            Score matrix or vector\n\n        Returns\n        -------\n            float\n            LogLoss of predictions vs targets.\n        \"\"\"\n        logits = 1 / (1 + np.exp(-y_pred))\n        aux = (1-y_true)*np.log(1-logits+1e-15) + y_true*np.log(logits+1e-15)\n        return np.mean(-aux)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing steps"},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id','drug_id']]\nlen(feature_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 5       \nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=1500\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Single fold training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_tabnet_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        seed_everything(seed)\n        print(f\"FOLD: {fold}\")\n\n        train_data = process_data(folds)\n        test_data = process_data(test)\n\n        trn_idx = train_data[train_data['kfold'] != fold].index\n        val_idx = train_data[train_data['kfold'] == fold].index\n\n        train_df = train_data[train_data['kfold'] != fold].reset_index(drop=True)\n        valid_df = train_data[train_data['kfold'] == fold].reset_index(drop=True)\n\n        x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n        x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n        \n        model = TabNetRegressorCustom(seed=seed, **tabnet_params)\n        \n        model.fit(X_train=x_train,\n                  y_train=y_train,\n                  eval_set=[(x_valid, y_valid)],\n                  eval_name = [\"val\"],\n                  eval_metric = [\"logits_ll\"],\n                  max_epochs=MAX_EPOCH,\n                  patience=20, batch_size=1024, virtual_batch_size=128,\n                  num_workers=1, drop_last=False,\n                  # use binary cross entropy as this is not a regression problem\n                  loss_fn=torch.nn.functional.binary_cross_entropy_with_logits,\n                  pretrain='pretrained.pth')\n\n        pickle.dump(model, open(f\"tabnet_FOLD{fold}_SEED{seed}.pkl\", 'wb'))\n        model = pickle.load(open(f\"tabnet_FOLD{fold}_SEED{seed}.pkl\", 'rb'))\n        \n        preds_val = model.predict(x_valid)\n        # Apply sigmoid to the predictions\n        preds =  1 / (1 + np.exp(-preds_val))\n        score = np.min(model.history[\"val_logits_ll\"])\n    #     name = cfg.save_name + f\"_fold{fold_nb}\"\n    #     model.save_model(name)\n        ## save oof to compute the CV later\n        oof[val_idx] = preds\n        #scores.append(score)\n        #--------------------- PREDICTION---------------------\n        x_test = test_data[feature_cols].values  \n        preds_test = model.predict(x_test)\n        predictions += ((1 / (1 + np.exp(-preds_test))) / NFOLDS)\n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_tabnet_custom.pretraining import PreTrainingModel\n\nfolds = train.copy()\nmskf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=2020)\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\nfolds['kfold'] = folds['kfold'].astype(int)\n    \nfold = 0\ntrain_data = process_data(folds)\ntest_data = process_data(test)\n\ntrn_idx = train_data[train_data['kfold'] != fold].index\nval_idx = train_data[train_data['kfold'] == fold].index\n\ntrain_df = train_data[train_data['kfold'] != fold].reset_index(drop=True)\nvalid_df = train_data[train_data['kfold'] == fold].reset_index(drop=True)\n\nx_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\nx_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n\nx_test = test_data[feature_cols].values\n\ntrain_test = np.concatenate([x_train, x_test])\n\nclf = PreTrainingModel(\n    input_dim=train_test.shape[1],\n    output_dim=train_test.shape[1],\n    n_d=32, n_a=32, n_steps=3, gamma=1.3,\n    lambda_sparse=0, optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n    mask_type='entmax',\n    scheduler_params=dict(mode=\"min\",\n                        patience=5,\n                        min_lr=1e-5,\n                        factor=0.9,),\n    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n    verbose=10, # \"sparsemax\"\n    )\n\n\nclf.fit(X_train=train_test,\n        y_train=train_test,\n        eval_set=[(x_valid, x_valid)],\n        eval_name = [\"val\"],\n        max_epochs=2000,\n        patience=20, batch_size=1024, virtual_batch_size=128,\n        num_workers=1, drop_last=False)\n\nfrom collections import OrderedDict\nstate_dict = clf.network.state_dict()\nnew_state_dict = OrderedDict()\nfor key, value in state_dict.items():\n    if key not in ['encoder.final_mapping.weight']:\n        new_key = 'tabnet.'+key\n        new_state_dict[new_key] = value\n#torch.save(clf.network.state_dict(), 'pretrained.pth')\ntorch.save(new_state_dict, 'pretrained.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Averaging on multiple SEEDS\n\nSEED = [0, 1, 2, 3, 4] \n\ntab_oof = np.zeros((len(train), len(target_cols)))\ntab_predictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    folds = train.copy()\n    mskf = MultilabelStratifiedGroupKFold(n_splits=NFOLDS, shuffle=True, random_state=seed)\n    for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target, groups=groups)):\n        folds.loc[v_idx, 'kfold'] = int(f)\n    folds['kfold'] = folds['kfold'].astype(int)\n    \n    oof_, predictions_ = run_tabnet_k_fold(NFOLDS, seed)\n    \n    tab_oof += oof_ / len(SEED)\n    tab_predictions += predictions_ / len(SEED)\n\ntrain[target_cols] = tab_oof\ntest[target_cols] = tab_predictions\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(target_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ / target.shape[1]\n    \nprint(\"CV log_loss: \", score)\nvalid_results.to_csv('oof.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_results.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_loss_metric(y_true, y_pred):\n    metrics = []\n    for _target in train_targets_scored.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels = [0,1]))\n    return np.mean(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.shape","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}