{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # load module "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing useful libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Adding iterative-stratification \n# Select add data from the right menu and search for iterative-stratification, then add it to your kernel.\nimport sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\n#from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\n\nfrom time import time\nimport datetime\nimport gc\n\nimport numpy as np\nimport pandas as pd \n\n# ML tools \nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nimport tensorflow as tf \nfrom tensorflow.keras import layers\nimport tensorflow.keras.backend as K\nfrom sklearn.metrics import log_loss\nfrom tensorflow_addons.layers import WeightNormalization\n# Setting random seeds\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Visualization tools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('white')\nsns.set(font_scale=1.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 1 : Data Check"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ndisplay(df_train.head(3))\nprint('train data size', df_train.shape)\n\ndf_target_ns = pd.read_csv('/kaggle/input/lish-moa/train_targets_nonscored.csv')\ndisplay(df_target_ns.head(3))\nprint('train target nonscored size', df_target_ns.shape)\n\n\ndf_target_s = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ndisplay(df_target_s.head(3))\nprint('train target scored size', df_target_s.shape)\n\n\ndf_test = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\ndisplay(df_test.head(3))\nprint('test data size', df_test.shape)\n\ndf_sample = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\ndisplay(df_sample.head(3))\nprint('sample submission size', df_sample.shape)\n\nprint(type(df_target_s))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # train_features : df_train"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_copy= df_train.copy()\nprint(type(train_copy))\ntrain_copy['target_71'] = df_target_s.iloc[:,72] \nprint(train_copy['target_71'])\nprint(df_target_s.columns[72])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # train_targets_nonscored : df_target_ns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_target_ns_copy= df_target_ns.copy()\nprint(type(df_target_ns_copy))\nprint(df_target_ns_copy.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_target_ns_copy.drop('sig_id', axis=1, inplace=True)\nn_row = 30\nn_col = 4 \nn_sub = 1   \nfig = plt.figure(figsize=(20,50))\nplt.subplots_adjust(left=-0.3, right=1.3,bottom=-0.3,top=1.3)\n\nfor i in np.random.choice(np.arange(0,df_target_ns_copy.shape[1],1),n_row):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.countplot(y=df_target_ns_copy.iloc[:, i],palette='nipy_spectral',orient='h')\n    \n    plt.legend()                    \n    n_sub+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_target_ns_copy.sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\ndf_target_ns_copy.sum().sort_values()[-20:].plot(kind='barh',color='mediumseagreen')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # train_targets_scored : df_target_s"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_s_copy=df_target_s.copy()\nprint(type(target_s_copy))\nprint(target_s_copy.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_s_copy.drop('sig_id', axis=1, inplace=True)\nn_row = 30\nn_col = 4 \nn_sub = 1   \nfig = plt.figure(figsize=(20,50))\nplt.subplots_adjust(left=-0.3, right=1.3,bottom=-0.3,top=1.3)\n\nfor i in np.random.choice(np.arange(0,target_s_copy.shape[1],1),n_row):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.countplot(y=target_s_copy.iloc[:, i],palette='nipy_spectral',orient='h')\n    \n    plt.legend()                    \n    n_sub+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_s_copy.sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\ntarget_s_copy.sum().sort_values()[-20:].plot(kind='barh',color='mediumseagreen')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2 : Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# 2-1 : Preprocessing for 772 gene expression features"},{"metadata":{"trusted":true},"cell_type":"code","source":"ind_tr = df_train[df_train['cp_type']=='ctl_vehicle'].index\nind_te = df_test[df_test['cp_type']=='ctl_vehicle'].index\nprint(ind_tr[:5])\nprint(ind_te[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\ntransformer = QuantileTransformer(n_quantiles=100,random_state=42, output_distribution=\"normal\")\n\ndef preprocess(df):\n    df['cp_time'] = df['cp_time'].map({24:1, 48:2, 72:3})\n    df['cp_dose'] = df['cp_dose'].map({'D1':0, 'D2':1})\n    df['cp_type'] = df['cp_type'].map({'trt_cp':0, 'ctl_vehicle':1})\n    g_features = [cols for cols in df.columns if cols.startswith('g-')]\n    c_features = [cols for cols in df.columns if cols.startswith('c-')]\n    for col in (g_features + c_features):\n        vec_len = len(df[col].values)\n        raw_vec = df[col].values.reshape(vec_len, 1)\n        transformer.fit(raw_vec)\n        df[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    return df\n\nX_train = preprocess(df_train)\nX_test = preprocess(df_test)\n\ndisplay(X_train.head(5))\nprint('Train data size', X_train.shape)\ndisplay(X_test.head(3))\nprint('Test data size', X_test.shape)\n\ny = df_target_s.drop('sig_id', axis=1)\ndisplay(y.head(3))\nprint('target size', y.shape)\ny0 =  df_target_ns.drop('sig_id', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Please see reference 3 for this part\ng_features = [cols for cols in X_train.columns if cols.startswith('g-')]\nn_comp = 0.95\nprint(g_features[:20])\nprint(len(g_features))\n\ng_data = pd.concat([pd.DataFrame(X_train[g_features]), pd.DataFrame(X_test[g_features])])\nprint(g_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # PCA transformation "},{"metadata":{"trusted":true},"cell_type":"code","source":"g_data2 = (PCA(0.95, random_state=42).fit_transform(g_data))\ng_data2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_train2 = g_data2[:X_train.shape[0]]\nprint(g_train2.shape)\nprint(type(g_data2))\ng_test2 = g_data2[-X_test.shape[0]:]\nprint(g_test2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_train2 = pd.DataFrame(g_train2, columns=[f'pca_g-{i}' for i in range(g_data2.shape[1])])\ng_test2 = pd.DataFrame(g_test2, columns=[f'pca_g-{i}' for i in range(g_test2.shape[1])])\n\ndisplay(g_train2.head(3))\nprint('train data PCA',g_train2.shape)\n\ndisplay(g_test2.head(3))\nprint('test data PCA',g_test2.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # merge original data and PCA tansformed data for gene expression data"},{"metadata":{"trusted":true},"cell_type":"code","source":"g_X_train = pd.concat((X_test[g_features],g_train2), axis=1)\ng_X_test = pd.concat((X_test[g_features], g_test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(g_X_train.head(3))\nprint('train data PCA + train original data',g_X_train.shape)\n\ndisplay(g_X_test.head(3))\nprint('test data PCA + test original data',g_X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2-2 : Preprocessing for 100 cell viability features"},{"metadata":{"trusted":true},"cell_type":"code","source":"c_features = [cols for cols in X_train.columns if cols.startswith('c-')]\nn_comp = 0.95\nprint(len(c_features))\n\nc_data = pd.concat([pd.DataFrame(X_train[c_features]), pd.DataFrame(X_test[c_features])])\nc_data2 = (PCA(0.95, random_state=42).fit_transform(c_data[c_features]))\nc_train2 = c_data2[:X_train.shape[0]]\nc_test2 = c_data2[-X_test.shape[0]:]\n\nc_train2 = pd.DataFrame(c_train2, columns=[f'pca_c-{i}' for i in range(c_data2.shape[1])])\nc_test2 = pd.DataFrame(c_test2, columns=[f'pca_c-{i}' for i in range(c_data2.shape[1])])\n\ndisplay(c_data.head(3))\nprint('cell viability data',c_data.shape)\n\ndisplay(c_data2)\nprint('PCA cell viability data',c_data2.shape)\n\ndisplay(c_train2.head(3))\nprint('train data PCA',c_train2.shape)\n\ndisplay(c_test2.head(3))\nprint('test data PCA',c_test2.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_X_train = pd.concat((X_train[c_features], c_train2), axis=1)\nc_X_test = pd.concat((X_test[c_features], c_test2), axis=1)\n\ndisplay(c_X_train.head(3))\nprint('train data PCA + train original data of cell viability',c_X_train.shape)\n\ndisplay(c_X_test.head(3))\nprint('test data PCA + test original data of cell viability',c_X_test.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # integration of gene expression features and cell viability features "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.concat((g_X_train,c_X_train), axis=1)\nX_test = pd.concat((g_X_test, c_X_test), axis=1)\n\ndisplay(X_train.head(3))\nprint('train data PCA + train original data of gene expression & cell viability',X_train.shape)\n\ndisplay(X_test.head(3))\nprint('test data PCA + test original data of gene expression & cell viability',X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n\nvar_thresh = VarianceThreshold(0.8)  \ndata = X_train.append(X_test)\ndata_transformed = var_thresh.fit_transform(data)\n\ntrain_features = data_transformed[:X_train.shape[0]]\ntest_features = data_transformed[-X_test.shape[0] : ]\n\ndisplay(train_features)\nprint('train data PCA + train original data of gene expression & cell viability',train_features.shape)\n\ndisplay(test_features)\nprint('test data PCA + test original data of gene expression & cell viability',test_features.shape)\n\nprint(type(data))\nprint(data.shape)\nprint(type(train_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_features=pd.DataFrame(train_features,columns=[f'feature_{i}' for i in range(train_features.shape[1])])\n\ndf_train_features['sig_id']=df_train['sig_id']\n\ndf_test_features=pd.DataFrame(test_features,columns=[f'feature_{i}' for i in range(test_features.shape[1])])\n\ndf_test_features['sig_id']=df_test['sig_id']\n\n\ndisplay(df_train_features.head(5))\nprint('train features data', df_train_features.shape)\n\n\ndisplay(df_test_features.head(5))\nprint('test features data', df_test_features.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # cp_dose, cp_time, cp_type information"},{"metadata":{"trusted":true},"cell_type":"code","source":"#X = pd.DataFrame(X[['sig_id','cp_type', 'cp_time','cp_dose']].values.reshape(-1, 4),\\\n#                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\np_train=pd.DataFrame(df_train[['sig_id','cp_type', 'cp_time','cp_dose']].values.reshape(-1,4),columns=['sig_id','cp_type','cp_time','cp_dose'])\np_test=pd.DataFrame(df_test[['sig_id','cp_type', 'cp_time','cp_dose']].values.reshape(-1,4),columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ndisplay(p_train.head(3))\nprint('train data dosing information',p_train.shape)\n\ndisplay(p_test.head(3))\nprint('test data dosing information',p_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_fp_train=pd.merge(df_train_features,p_train,on='sig_id')\nX_fp_test=pd.merge(df_test_features,p_test,on='sig_id')\n\ndisplay(X_fp_train.head(3))\nprint('train data dosing information + feature data',X_fp_train.shape)\n\ndisplay(X_fp_test.head(3))\nprint('test data dosing information + feature data',X_fp_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Label Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"y=df_target_s\ny0=df_target_ns\n\ndisplay(y.head(3))\nprint('target scored data label',y.shape)\n\ndisplay(y0.head(3))\nprint('target non-scored data label',y0.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle \nimport os\n\nprint(os.getcwd())\n\nwith open('X_fp_train','wb') as web:\n    pickle.dump(X_fp_train,web)\n\nwith open('X_fp_test','wb') as web:\n    pickle.dump(X_fp_test,web)  \n    \nwith open('y','wb') as web:\n    pickle.dump(y,web)\n\nwith open('y0','wb') as web:\n    pickle.dump(y0,web)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MLP Model Define"},{"metadata":{},"cell_type":"markdown","source":"https://machinelearningmastery.com/multi-label-classification-with-deep-learning/"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('X_fp_test','rb') as web:\n    X_fp_test=pickle.load(web)\n    \nwith open('X_fp_train','rb') as web:\n    X_fp_train=pickle.load(web)\n    \nwith open('y','rb') as web:\n    y=pickle.load(web)\n    \nwith open('y0','rb') as web:\n    y0=pickle.load(web)\n    \nX_fp_train=X_fp_train.drop('sig_id',axis=1)\nX_fp_test=X_fp_test.drop('sig_id',axis=1)\ny=y.drop('sig_id',axis=1)\ny0=y0.drop('sig_id',axis=1)\n\ndisplay(X_fp_train.head(3))\ndisplay(X_fp_test.head(3))\ndisplay(y.head(3))\ndisplay(y0.head(3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feats = np.arange(0,X_fp_train.shape[1],1)\ninp_size = int(np.ceil(1* len(feats)))\nprint(inp_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_fp_train.shape[1]\ny.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_min = 0.001\np_max = 0.999\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import layers,models\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -K.mean(y_true*K.log(y_pred) + (1-y_true)*K.log(1-y_pred))\n\ndef create_model1(num_cols, hid_layers, dropout_rate, num_cols_y):\n    \n    inp1 = tf.keras.layers.Input(shape = (num_cols, ))\n    x1 = tf.keras.layers.BatchNormalization()(inp1)\n    activations=['selu','relu','swish']\n    for i, units in enumerate(hid_layers):\n        x1 = tf.keras.layers.Dense(units, activation=activations[i])(x1)\n        x1 = tf.keras.layers.Dropout(dropout_rate)(x1)\n        x1 = tf.keras.layers.BatchNormalization()(x1)\n    \n    x1 = tf.keras.layers.Dense(num_cols_y,activation='sigmoid')(x1)\n    model = tf.keras.models.Model(inputs= inp1, outputs= x1)\n    \n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.000656575),\n                 loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001), metrics=logloss)\n    \n    return model \n\ndef create_model2():\n    \n    model=models.Sequential()\n    \n\n    # First Layer\n    model.add(layers.Dense(\n    800,\n    input_shape=(1042,),\n        activation='relu'\n    ))\n    \n    model.add(layers.Dropout(\n    0.5))\n    \n    # Second Layer\n    model.add(layers.Dense(\n    600,\n    activation='relu'\n    ))\n    \n    model.add(layers.Dropout(\n    0.5))\n    \n    # Third Layer\n    model.add(layers.Dense(\n    400,\n    activation='relu'\n    ))\n    \n    model.add(layers.Dropout(\n    0.5))\n    \n    # Output Layer\n    model.add(layers.Dense(\n        y.shape[1],\n        activation='sigmoid'\n    ))\n    \n    model.compile(optimizer ='adam',\n                 loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model \n\nmodel=create_model2()\n\nmodel.summary()\n\nmodel2=create_model1(1042,2,0.5,206)\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2=create_model1(X_fp_train.shape[1],2,0.5,y.shape[1])\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install iterative-stratification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nnp.random.seed(seed=42)\nn_split = 5\n\nfor n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits = n_split, random_state = 29, shuffle = True).split(X_fp_train, y)):\n    print(tr,te)\n    \nprint(tr)\nprint(len(tr))\nprint(len(te))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=[]\nfor k, (train, test) in enumerate(MultilabelStratifiedKFold(n_splits = 5, random_state = 30, shuffle = True).split(X_fp_train, y)):\n    \n    x_tr = X_fp_train.astype('float64').values[train]\n    x_val = X_fp_train.astype('float64').values[test]\n    \n    y_tr, y_val = y.astype(float).values[train], y.astype(float).values[test]\n    \n    model = create_model2()\n    \n    result=model.fit(x_tr,y_tr,validation_data=(x_val, y_val), epochs = 100, batch_size = 128,verbose = 1)\n    \n    results.append(result)\n\n        \n        ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}