{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#Prep the environment","metadata":{"id":"rI3nysLTv3H1"}},{"cell_type":"code","source":"!pip install ../input/nnmodels/joblib-1.0.1-py3-none-any.whl\n!pip install ../input/nnmodels/threadpoolctl-2.2.0-py3-none-any.whl\n!pip install ../input/nnmodels/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl\n!pip install ../input/nnmodels/tqdm-4.62.0-py2.py3-none-any.whl\n!pip install ../input/nnmodels/numpy-1.21.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n!pip install ../input/nnmodels/scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl\n!pip install ../input/nnmodels/torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ../input/nnmodels/pytorch_tabnet-3.1.1-py3-none-any.whl","metadata":{"id":"rQ9XVL5KUinU","execution":{"iopub.status.busy":"2021-08-06T09:56:17.211367Z","iopub.execute_input":"2021-08-06T09:56:17.21218Z","iopub.status.idle":"2021-08-06T09:57:10.875179Z","shell.execute_reply.started":"2021-08-06T09:56:17.212113Z","shell.execute_reply":"2021-08-06T09:57:10.872824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#TF stuff\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\nimport keras_tuner as kt\nfrom tensorflow import keras\n\n\n#XGBoost\nfrom xgboost import XGBClassifier\n\n#Scikit\nfrom sklearn.metrics import log_loss\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score, log_loss\n\n#TabNet\nimport torch\nfrom torch.nn.modules.loss import _WeightedLoss\nfrom torch.nn import BCEWithLogitsLoss\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom pytorch_tabnet.multitask import TabNetMultiTaskClassifier\nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n\n# Scipy\nfrom scipy.special import expit, logit","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import normalize ","metadata":{"id":"pb7t8AJeZNat","execution":{"iopub.status.busy":"2021-08-06T09:11:11.25159Z","iopub.execute_input":"2021-08-06T09:11:11.251923Z","iopub.status.idle":"2021-08-06T09:11:11.255628Z","shell.execute_reply.started":"2021-08-06T09:11:11.251892Z","shell.execute_reply":"2021-08-06T09:11:11.254692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Reading and understanding the data","metadata":{"id":"MFlrX9Wbwax4"}},{"cell_type":"markdown","source":"Main training and target data commponents","metadata":{"id":"W_Abqt-WwpTt"}},{"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')","metadata":{"id":"l85bOEYrwLDz","outputId":"9a29f2ba-472f-44a1-d6ef-7c0da50ade54","execution":{"iopub.status.busy":"2021-08-06T09:13:04.816943Z","iopub.execute_input":"2021-08-06T09:13:04.81727Z","iopub.status.idle":"2021-08-06T09:13:08.322364Z","shell.execute_reply.started":"2021-08-06T09:13:04.817243Z","shell.execute_reply":"2021-08-06T09:13:08.321492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Encoding the categorical variables","metadata":{"id":"2l8jmLtOxTNy"}},{"cell_type":"code","source":"train_features_enc = pd.get_dummies(train_features, columns=['cp_type', 'cp_dose'], drop_first=True)\nprint(train_features_enc.head())","metadata":{"id":"1EO5QdSKwrpw","outputId":"dd22b8a8-1c96-474c-dc44-31d817a4890a","execution":{"iopub.status.busy":"2021-08-06T09:13:08.323978Z","iopub.execute_input":"2021-08-06T09:13:08.324361Z","iopub.status.idle":"2021-08-06T09:13:08.486413Z","shell.execute_reply.started":"2021-08-06T09:13:08.324318Z","shell.execute_reply":"2021-08-06T09:13:08.485478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the independent variables set **X** and the dependent one **y**\n","metadata":{"id":"tmpX81_wxsJe"}},{"cell_type":"code","source":"X = train_features_enc.iloc[:,1:].to_numpy()\ny = train_targets.iloc[:,1:].to_numpy() ","metadata":{"id":"eD16Acydw1tY","execution":{"iopub.status.busy":"2021-08-06T09:13:12.417548Z","iopub.execute_input":"2021-08-06T09:13:12.417951Z","iopub.status.idle":"2021-08-06T09:13:12.572189Z","shell.execute_reply.started":"2021-08-06T09:13:12.417918Z","shell.execute_reply":"2021-08-06T09:13:12.571207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Applying PCA on the independent variables X","metadata":{"id":"beDP8AnMVv8h"}},{"cell_type":"code","source":"pca = PCA()\npca.fit(X)\n\ncum_sum_sv2 = np.cumsum(pca.explained_variance_ratio_)\n\npca = PCA(n_components = 50)\nX_trim = pca.fit_transform(X)\n#Concatenation of the PCA features and the original ones\nX_pca = np.concatenate([X_trim, X],axis=1)","metadata":{"id":"HSNPUx6yV4n-","execution":{"iopub.status.busy":"2021-08-06T09:13:14.315088Z","iopub.execute_input":"2021-08-06T09:13:14.315437Z","iopub.status.idle":"2021-08-06T09:13:19.416779Z","shell.execute_reply.started":"2021-08-06T09:13:14.315404Z","shell.execute_reply":"2021-08-06T09:13:19.415755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preprocessing of the data: Normalization of the independent variables","metadata":{"id":"Ui3VhnupyF7R"}},{"cell_type":"code","source":"X_norm = normalize(X, axis=0, norm='max')\nX_norm_pca = normalize(X_pca, axis=0, norm='max')","metadata":{"id":"NnET6FjnyBRF","execution":{"iopub.status.busy":"2021-08-06T09:13:19.418023Z","iopub.execute_input":"2021-08-06T09:13:19.418286Z","iopub.status.idle":"2021-08-06T09:13:19.806895Z","shell.execute_reply.started":"2021-08-06T09:13:19.418262Z","shell.execute_reply":"2021-08-06T09:13:19.80596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Adding useful functions**\n","metadata":{"id":"dKp_dnVvzy_y"}},{"cell_type":"code","source":"# A convenient plotting function as we train models\ndef plot_hist(hist, last = None):\n    if last == None:\n        last = len(hist.history[\"loss\"])\n    plt.plot(hist.history[\"loss\"][-last:])\n    plt.plot(hist.history[\"val_loss\"][-last:])\n    plt.title(\"model accuracy\")\n    plt.ylabel(\"accuracy\")\n    plt.xlabel(\"epoch\")\n    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n    plt.show()","metadata":{"id":"noN2GXwgysfh","execution":{"iopub.status.busy":"2021-08-06T09:13:20.407066Z","iopub.execute_input":"2021-08-06T09:13:20.407408Z","iopub.status.idle":"2021-08-06T09:13:20.413327Z","shell.execute_reply.started":"2021-08-06T09:13:20.407378Z","shell.execute_reply":"2021-08-06T09:13:20.412095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Models implementation","metadata":{"id":"GNFSWLEs0PZf"}},{"cell_type":"markdown","source":"###Model 1: NN 4 layer","metadata":{"id":"qsHH-Yo8EaW4"}},{"cell_type":"code","source":"def l4_model(input_shape, no_classes, lr):\n    inputs = tf.keras.Input(shape=input_shape)\n    x = layers.Dense(128, activation='sigmoid')(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.2)(x)\n    x = layers.Dense(128, activation='sigmoid')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.2)(x)\n    x = layers.Dense(128, activation='sigmoid')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.2)(x)\n    x = layers.Dense(128, activation='sigmoid')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.2)(x)\n    outputs = layers.Dense(no_classes, activation='sigmoid')(x)\n    model = tf.keras.Model(inputs, outputs)\n    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate = lr), metrics=['binary_crossentropy'])\n    return model\n\n### CV setup for NN\n\"\"\"\nlosses_NN=[]\nkf = KFold(n_splits=10)\ntf.random.set_seed(1010)\nnp.random.seed(1010)\n\nfor train_index, test_index in kf.split(X_norm_pca):\n    X_train, X_test = X_norm_pca[train_index], X_norm_pca[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    control_vehicle_mask = X_train[:,-2] == 0\n    X_train = X_train[~control_vehicle_mask,:]\n    y_train = y_train[~control_vehicle_mask]\n\n    nnclf = l4_model((925,),206,0.0005)\n    hist = nnclf.fit(X_train, y_train, batch_size=512, epochs=50, validation_data=(X_test, y_test), verbose=0)\n    plot_hist(hist, last = 20)\n\n    preds = nnclf.predict(X_test) # list of preds per class\n\n    control_mask = X_test[:,-2]==0\n    preds[control_mask] = 0\n\n    loss = log_loss(np.ravel(y_test), np.ravel(preds))\n    print('Loss: '+str(loss))\n    losses_NN.append(loss)\n\nprint('Average Loss: '+str(np.average(losses_NN))) \n\"\"\"","metadata":{"id":"N8sROe_AEZGn","outputId":"6d0810be-6858-428f-a1f8-e124509f9e5c","execution":{"iopub.status.busy":"2021-08-06T09:13:29.438121Z","iopub.execute_input":"2021-08-06T09:13:29.438462Z","iopub.status.idle":"2021-08-06T09:14:00.37167Z","shell.execute_reply.started":"2021-08-06T09:13:29.438433Z","shell.execute_reply":"2021-08-06T09:14:00.369419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###Model 2: Residual Neural Network with three layers and fixed parameters","metadata":{"id":"QaR7_k8U0UhO"}},{"cell_type":"code","source":"def l3_res_model(input_shape, no_classes, lr):\n    inputs = tf.keras.Input(shape=input_shape)\n    x = layers.Dense(128, activation='sigmoid')(inputs)\n    x = layers.BatchNormalization()(x)\n    b_1 = layers.Dropout(0.2)(x)\n    x = layers.Dense(128, activation='sigmoid')(b_1)\n    x = layers.BatchNormalization()(x)\n    b_2 = layers.Dropout(0.2)(x)\n    x = layers.Dense(128, activation='sigmoid')(b_2)\n    x = layers.BatchNormalization()(x)\n    b_3 = layers.Dropout(0.2)(x)\n    tot_op = tf.keras.layers.add([b_1, b_2, b_3])\n    outputs = layers.Dense(no_classes, activation='sigmoid')(tot_op)\n    model = tf.keras.Model(inputs, outputs)\n    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate = lr), metrics=['binary_crossentropy'])\n    return model\n\n### CV setup for NN\n\"\"\"\nlosses_NN=[]\nkf = KFold(n_splits=10)\ntf.random.set_seed(1010)\nnp.random.seed(1010)\n\nfor train_index, test_index in kf.split(X_norm_pca):\n    X_train, X_test = X_norm_pca[train_index], X_norm_pca[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    control_vehicle_mask = X_train[:,-2] == 0\n    X_train = X_train[~control_vehicle_mask,:]\n    y_train = y_train[~control_vehicle_mask]\n\n    nnclf = l3_res_model((925,),206,0.0005)\n    hist = nnclf.fit(X_train, y_train, batch_size=512, epochs=50, validation_data=(X_test, y_test), verbose=0)\n    plot_hist(hist, last=20)\n\n    preds = nnclf.predict(X_test) # list of preds per class\n\n    control_mask = X_test[:,-2]==0\n    preds[control_mask] = 0\n\n    loss = log_loss(np.ravel(y_test), np.ravel(preds))\n    print('Loss: '+str(loss))\n    losses_NN.append(loss)\n\nprint('Average Loss: '+str(np.average(losses_NN))) \n\"\"\"","metadata":{"id":"ZeDgqie6yu99","outputId":"a4921f9e-a22f-424c-f063-2b19373f6746","execution":{"iopub.status.busy":"2021-08-06T09:14:09.618254Z","iopub.execute_input":"2021-08-06T09:14:09.618596Z","iopub.status.idle":"2021-08-06T09:14:38.415744Z","shell.execute_reply.started":"2021-08-06T09:14:09.618552Z","shell.execute_reply":"2021-08-06T09:14:38.41392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TabNet Classifier","metadata":{"id":"4Yaj4NpDU0tP"}},{"cell_type":"code","source":"# helper class that manually implements logloss for TabNet\nclass LogitsLogLoss(Metric):\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        logits = expit(y_pred)\n        aux = (1 - y_true) * np.log(1 - logits + 1e-15) + y_true * np.log(logits + 1e-15)\n        return np.mean(-aux)","metadata":{"id":"oT1uojHmU36T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### CV setup for TabNet\n\"\"\"\nlosses_tabnet=[]\nkf = KFold(n_splits=3)\ntf.random.set_seed(1010)\nnp.random.seed(1010)\n\nfor train_index, test_index in kf.split(X_norm_pca):\n    X_train_cv, X_test_cv = X_norm_pca[train_index], X_norm_pca[test_index]\n    y_train_cv, y_test_cv = y[train_index], y[test_index]\n\n    control_vehicle_mask = X_train_cv[:,-2] == 0\n    X_train_cv = X_train_cv[~control_vehicle_mask,:]\n    y_train_cv = y_train_cv[~control_vehicle_mask]\n    \n    clf = TabNetRegressor(optimizer_fn = optim.Adam,\n      optimizer_params = dict(lr = 2e-2, weight_decay = 1e-5),\n      n_steps = 1, gamma = 1.3, lambda_sparse = 0, n_d = 32, n_a = 32,\n      scheduler_params = dict(mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n      scheduler_fn = ReduceLROnPlateau, verbose=10, seed=1010)\n\n    clf.fit(\n      X_train = X_train_cv,\n      y_train = y_train_cv,\n      eval_set = [(X_test_cv, y_test_cv)],\n      eval_name = [\"val\"],\n      eval_metric = [\"logits_ll\"],\n      max_epochs = 200,\n      patience = 50,\n      batch_size = 1024, \n      virtual_batch_size = 32,\n      num_workers = 1,\n      loss_fn = BCEWithLogitsLoss()\n    )\n    preds_val = clf.predict(X_test_cv)\n    preds = expit(preds_val)\n\n    control_mask = X_test_cv[:,-2]==0\n    preds[control_mask] = 0\n\n    loss = log_loss(np.ravel(y_test_cv), np.ravel(preds))\n    print('Loss: '+str(loss))\n    losses_tabnet.append(loss)\n\nprint('Average Loss: '+str(np.average(losses_tabnet)))\n\"\"\"","metadata":{"id":"t7krVGSKVAgp","outputId":"77e3b440-c0a3-41aa-93d9-270ab28649aa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Prediction over all the dataset","metadata":{"id":"2uxQPtTUd7Lt"}},{"cell_type":"code","source":"#L4 simple NN \nnnclf_l4 = l4_model((925,),206,0.0005)\nhist_l4 = nnclf_l4.fit(X_norm_pca, y, batch_size=512, epochs=50, verbose=0)\npreds_l4 = nnclf_l4.predict(X_norm_pca) # list of preds per class\n\n#L3 Residual\nnnclfl3_r = l3_res_model((925,),206,0.0005)\nhist_l3_r = nnclfl3_r.fit(X_norm_pca, y, batch_size=512, epochs=50, verbose=0)\npreds_l3_r = nnclfl3_r.predict(X_norm_pca) # list of preds per class\n\n#Tab net\nclf = TabNetRegressor(optimizer_fn = optim.Adam,\n  optimizer_params = dict(lr = 2e-2, weight_decay = 1e-5),\n  n_steps = 1, gamma = 1.3, lambda_sparse = 0, n_d = 32, n_a = 32,\n  scheduler_params = dict(mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n  scheduler_fn = ReduceLROnPlateau, verbose=10, seed=1010)\n\nclf.fit(\n  X_train = X_norm_pca,\n  y_train = y,\n  eval_metric = [\"logits_ll\"],\n  max_epochs = 200,\n  patience = 50,\n  batch_size = 1024, \n  virtual_batch_size = 32,\n  num_workers = 1,\n  loss_fn = BCEWithLogitsLoss()\n)\npreds_val = clf.predict(X_norm_pca)\npreds_tabnet = expit(preds_val)\n\n","metadata":{"id":"-HHgl_fGd_e8","outputId":"ee46a52a-2d06-4ac0-e613-dfd9f0a7bc8c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a_opt, b_opt, c_opt = 1/3,1/3,1/3\npreds_ensemble = a_opt*preds_l4 + b_opt*preds_l3_r + c_opt*preds_tabnet\nlog_loss(np.ravel(y), np.ravel(a_opt*preds_l4 + b_opt*preds_l3_r + c_opt*preds_tabnet)) ","metadata":{"id":"DAFiKc1blZ49","outputId":"38ce5462-a2e2-4abd-8daf-568e86a9c3f8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final prediction","metadata":{"id":"7tLA1tQ0neXv"}},{"cell_type":"code","source":"#Getting Data\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\ntest_features_enc = pd.get_dummies(test_features, columns=['cp_type', 'cp_dose'], drop_first=True)\n\nX_test = test_features_enc.iloc[:,1:].to_numpy()\n\n\n#PCA\nX_trim_test = pca.transform(X_test)\n\n#Concatenation of the PCA features and the original ones\nX_pca_test = np.concatenate([X_trim_test, X_test],axis=1)\n\n#Data normalization\nX_norm_test = normalize(X_test, axis=0, norm='max')\nX_norm_pca_test = normalize(X_pca_test, axis=0, norm='max')\n","metadata":{"id":"1wf37oebnigX","execution":{"iopub.status.busy":"2021-08-06T09:15:23.528504Z","iopub.execute_input":"2021-08-06T09:15:23.528852Z","iopub.status.idle":"2021-08-06T09:15:24.604257Z","shell.execute_reply.started":"2021-08-06T09:15:23.528823Z","shell.execute_reply":"2021-08-06T09:15:24.602879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#L4 simple NN \npreds_l4_test = nnclf_l4.predict(X_norm_pca_test) # list of preds per class\n\n#L3 Residual\npreds_l3_r_test = nnclfl3_r.predict(X_norm_pca_test) # list of preds per class\n\n#Tab net\npreds_val_test = clf.predict(X_norm_pca_test)\npreds_tabnet_test = expit(preds_val_test)\n\n#Ensembling the model resuls as the mean of the three models\na_opt, b_opt, c_opt = 1/3,1/3,1/3\npreds_ensemble_test = a_opt*preds_l4_test + b_opt*preds_l3_r_test + c_opt*preds_tabnet_test","metadata":{"id":"CW3-LhRzsty2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_df = pd.DataFrame(preds_ensemble_test, columns=train_targets.columns[1:], index=test_features[\"sig_id\"].values)\nsubmit_df.index.name = \"sig_id\"\nsubmit_df.to_csv(\"submission.csv\")","metadata":{"id":"tXDqq7B-BEux"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"4hknErb7BcrF"},"execution_count":null,"outputs":[]}]}