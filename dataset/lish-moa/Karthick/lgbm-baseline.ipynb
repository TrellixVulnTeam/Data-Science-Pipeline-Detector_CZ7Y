{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport time\nimport logging\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, QuantileTransformer\nfrom sklearn.compose import make_column_selector, make_column_transformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multioutput import ClassifierChain, MultiOutputClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.decomposition import PCA\nfrom tqdm.notebook import tqdm\n\n\nfrom skmultilearn.model_selection import IterativeStratification\nimport category_encoders as ce\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv', index_col='sig_id')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv', index_col='sig_id')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv', index_col='sig_id')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv', index_col='sig_id')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv', index_col='sig_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TransformerLog():\n    @property\n    def log(self):\n        return \"{} transform\".format(type(self).__name__)\n    \ndef timeit(method):\n    def timed(*args, **kw):\n        ts = time.time()\n        result = method(*args, **kw)\n        te = time.time()\n        if 'log_time' in kw:\n            name = kw.get('log_name', method.__name__.upper())\n            kw['log_time'][name] = int((te - ts) * 1000)\n        else:\n            logging.info(\"\\t {} method took {:2.1f}s\".format(method.__name__, (te - ts)))\n        return result\n\n    return timed\n    \nclass TypeSelector(BaseEstimator, TransformerMixin, TransformerLog):\n    def __init__(self, dtype):\n        self.dtype = dtype\n\n    def fit(self, X, y=None):\n        return self\n\n    @timeit\n    def transform(self, X):\n        assert isinstance(X, pd.DataFrame)\n        return X.select_dtypes(include=[self.dtype])\n    \nclass FeatureSelector(BaseEstimator, TransformerMixin, TransformerLog):\n    def __init__(self, feature_names):\n        self._feature_names = feature_names\n\n\n    def fit(self, X, y=None):\n        return self\n\n    @timeit\n    def transform(self, X, y=None):\n        logging.info(\"{} - {}\".format(self.log, self._feature_names))\n        assert isinstance(X, pd.DataFrame)\n\n        try:\n            return X[self._feature_names]\n        except KeyError:\n            cols_error = list(set(self._feature_names) - set(X.columns))\n            raise KeyError(\"The DataFrame does not include the columns: %s\" % cols_error)\n\nclass OrdinalEncoder(BaseEstimator, TransformerMixin, TransformerLog):\n    def __init__(self, feature_name, mapping_dict):\n        self._feature_name = feature_name\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        logging.info(\"{} - {}\".format(self.log, self._feature_names))\n        assert isinstance(X, pd.Series)\n        \n        try:\n            return X[self._feature_name].map(mapping_dict)\n        except KeyError:\n            raise KeyError(f\"The DataFrame does not have the column: {self._feature_name}\")\n            \nclass ClassifierChainEnsemble(BaseEstimator, ClassifierMixin):\n    def __init__(self, base_classifier=None):\n        self.base_classifier = base_classifier\n        self.chains = [ClassifierChain(self.base_classifier, \n                                       order='random', \n                                       random_state=i) \n                       for i in range(3)]\n    \n    def fit(self, X_train, y_train):\n        for chain in tqdm(self.chains):\n            chain.fit(X_train, y_train)\n        return self\n\n    def predict(self, X_test):\n\n        y_pred_chains = np.array([chain.predict(X_test) for chain in tqdm(self.chains)])\n        \n        \n        return y_pred_chains.mean(axis=0)\n    \n    def predict_proba(self, X_test):\n\n        y_pred_chains = np.array([chain.predict_proba(X_test) for chain in tqdm(self.chains)])\n        \n        \n        return y_pred_chains.mean(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = ['cp_type', 'cp_dose']\n\ncp_time_features = ['cp_time']\n\ng_features = [col for col in train_features.columns if col.startswith('g-')]\nc_features = [col for col in train_features.columns if col.startswith('c-')]\n\nnumerical_features = cp_time_features + g_features + c_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_pipeline = Pipeline(steps=[('cat_selector', FeatureSelector(categorical_features)), \n                                       ('onehot_encoder', ce.OneHotEncoder())\n                                      ])\n\nnumerical_pipeline = Pipeline(steps=[('numeric_selector', FeatureSelector(numerical_features)), \n                                     ('ordinal_encoder', ce.OrdinalEncoder())\n                                    ])\n\n\npreprocessing_pipeline = FeatureUnion(transformer_list=[('categorical_pipeline', categorical_pipeline), \n                                                         ('numerical_pipeline', numerical_pipeline)], \n                                       n_jobs=-1,\n                                       verbose=True,\n                                      )\n\nlgbm = LGBMClassifier(boosting_type='dart', \n                      objective = 'binary',\n                      bagging_freq=3,\n                      learning_rate=0.2496,\n                      num_leaves=511,\n                      max_depth=9,\n                      metric= 'binary_logloss',\n                      verbosity= 0,\n                      reg_alpha= 0.4,\n                      reg_lambda= 0.6,\n                      random_state= 123,\n                      class_weight='balanced',\n                      subsample=0.4578,\n                      bagging_fraction=0.7697,\n                     )\n\nfull_pipeline = Pipeline(steps=[('features', preprocessing_pipeline),\n                                ('scaler', StandardScaler()),\n                                ('log_reg_model', ClassifierChainEnsemble(base_classifier=lgbm))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_FOLDS = 4\nk_fold = IterativeStratification(n_splits=N_FOLDS, order=1)\n\ntest_preds = np.zeros((test_features.shape[0], train_targets_scored.shape[1]))\n\nfor fold, (train_indices, valid_indices) in enumerate(k_fold.split(train_features, train_targets_scored)):\n    \n    X_train = train_features.iloc[train_indices]\n    y_train = train_targets_scored.iloc[train_indices]\n    \n    X_valid = train_features.iloc[valid_indices]\n    y_valid = train_targets_scored.iloc[valid_indices]\n\n    full_pipeline.fit(X_train, y_train)\n    \n    train_loss = log_loss(np.ravel(y_train), np.ravel(full_pipeline.predict_proba(X_train)))\n    valid_loss = log_loss(np.ravel(y_valid), np.ravel(full_pipeline.predict_proba(X_valid)))\n    \n    test_preds += full_pipeline.predict_proba(test_features)\n    \n    print(f'Fold {fold} - Train loss : {train_loss} , Test loss : {valid_loss}')\n    \ntest_preds /= N_FOLDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.loc[:,:] = np.clip(test_preds,0.0005,0.999)\nsample_submission.loc[test_features.cp_type=='ctl_vehicle',sample_submission.columns] = 0\nsample_submission.to_csv('submission.csv',index=False)\nsample_submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}