{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# V2 : apply Rank Gauss, del controlled\n# V3 : apply Rank Gauss only\n# V5 : True Rank Gauss","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss, confusion_matrix, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom scipy.special import erfinv as sp_erfinv\n\n\ndef rank_gauss(data):\n    epsilon = 1e-6\n\n    for k in tqdm(GENES + CELLS):\n        r_cpu = data.loc[:,k].argsort().argsort()\n        r_cpu = (r_cpu/r_cpu.max()-0.5)*2 \n        r_cpu = np.clip(r_cpu,-1+epsilon,1-epsilon)\n        r_cpu = sp_erfinv(r_cpu) \n        data.loc[:,k] = r_cpu * np.sqrt(2)  \n    return data\n\n\ntrain_features = rank_gauss(train_features)\ntest_features = rank_gauss(test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# New train data\n# data_train = train_features[train_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)[GENES+CELLS]\ndata_train = train_features[GENES+CELLS]\n\n# New Test data\n# data_test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)[GENES+CELLS]\ndata_test = test_features[GENES+CELLS]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(CELLS), len(GENES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Config:\n    NUM_FOLDS = 5\n    SEED = 718\n    TRAIN_BATCH_SIZE = 128\n    VALID_BATCH_SIZE = 128\n    EPOCHS = 100\n    \n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        \n        \nclass DenoisingAutoEncoder(nn.Module):\n    def __init__(self, in_out_channels, hidden_channels):\n        super(DenoisingAutoEncoder, self).__init__()\n        self.encoder=nn.Sequential(\n                        nn.Linear(in_out_channels, hidden_channels),\n                        nn.ReLU(True),\n                        nn.Linear(hidden_channels, hidden_channels//2),\n                        nn.ReLU(True),\n                        nn.Linear(hidden_channels//2, hidden_channels//4),\n                        nn.ReLU(True)\n                        )\n\n        self.decoder=nn.Sequential(\n                        nn.Linear(hidden_channels//4, hidden_channels//2),\n                        nn.ReLU(True),\n                        nn.Linear(hidden_channels//2, hidden_channels),\n                        nn.ReLU(True),\n                        nn.Linear(hidden_channels, in_out_channels),\n                        # nn.Sigmoid(),\n                        )\n\n    def forward(self,x):\n        x=self.encoder(x)\n        x=self.decoder(x)   \n        return x\n        \n        \nclass DAEDataset:\n    def __init__(self, ids, ys=None):\n        self.ids = ids\n        self.ys = ys\n    \n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, item):\n        \n        if self.ys is not None:\n\n            return {\n                'ids': torch.tensor(self.ids[item], dtype=torch.float32),\n                'targets': torch.tensor(self.ys[item], dtype=torch.float32),\n            }\n        \n        else:\n            return {\n                'ids': torch.tensor(self.ids[item], dtype=torch.float32),\n            }\n        \n        \ndef loss_fn(logits, targets):\n    loss_fct = nn.MSELoss()\n    loss = loss_fct(logits, targets)\n    return loss\n\n\ndef train_fn(data_loader, model, optimizer, device, scheduler=None):\n    model.train()\n    losses = AverageMeter()\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    \n    y_true = []\n    y_pred = []\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"].to(device, dtype=torch.float32)        \n        targets = d[\"targets\"].to(device, dtype=torch.float32)\n\n        model.zero_grad()\n        \n        outputs = model(targets)\n        \n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        y_true.append(targets.cpu().detach().numpy())\n        y_pred.append(outputs.cpu().detach().numpy())\n\n        losses.update(loss.item(), targets.size(0))\n        tk0.set_postfix(loss=losses.avg)\n\n\ndef valid_fn(data_loader, model, device, scheduler=None):\n    model.eval()\n    losses = AverageMeter()\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    \n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for bi, d in enumerate(tk0):\n            ids = d[\"ids\"].to(device, dtype=torch.float32)        \n            targets = d[\"targets\"].to(device, dtype=torch.float32)\n\n            outputs = model(targets)\n\n            loss = loss_fn(outputs, targets)\n\n            y_true.append(targets.cpu().detach().numpy())\n            y_pred.append(outputs.cpu().detach().numpy())\n\n            losses.update(loss.item(), targets.size(0))\n            tk0.set_postfix(loss=losses.avg)\n\n    y_true = np.concatenate(y_true, 0)\n    y_pred = np.concatenate(y_pred, 0)\n    return loss\n\n\ndef test_fn(data_loader, model, device):\n    model.eval()\n    \n    preds = []\n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total=len(data_loader))\n        for bi, d in enumerate(tk0):\n            ids = d[\"ids\"].to(device, dtype=torch.float32)\n            outputs = model(ids)\n            outputs = outputs.cpu().detach().numpy()\n            preds.append(outputs)\n            \n    preds = np.concatenate(preds, 0)\n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GENES","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = DAEDataset(\n            ids=data_train[GENES].values,\n            ys=data_train[GENES].values,\n        )\n    \ntrain_loader = torch.utils.data.DataLoader(\n            train_dataset,\n            shuffle=True,\n            batch_size=Config.TRAIN_BATCH_SIZE,\n            num_workers=0, \n            pin_memory=True\n        )\n\nval_dataset = DAEDataset(\n            ids=data_test[GENES].values,\n            ys=data_test[GENES].values,\n        )\n    \nval_loader = torch.utils.data.DataLoader(\n            val_dataset,\n            shuffle=False,\n            batch_size=Config.VALID_BATCH_SIZE,\n            num_workers=0, \n            pin_memory=True\n        )\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = DenoisingAutoEncoder(len(GENES), 512)\nmodel = model.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30, eta_min=1e-6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = 0\nmin_loss = 999\npatience = 10\n\nfor epoch in range(1, Config.EPOCHS + 1):\n\n    print(\"Starting {} epoch...\".format(epoch))\n\n    train_fn(train_loader, model, optimizer, device)\n    val_loss = valid_fn(val_loader, model, device)\n    scheduler.step()\n    \n    if val_loss < min_loss:\n        min_loss = val_loss\n        best_epoch = epoch\n        torch.save(model.state_dict(), 'GENES_Reconstruction_model.pth')\n        print(\"save model at min valid loss={} on epoch={}\".format(min_loss, best_epoch))\n        p = 0 \n\n    if p > 0: \n        print(f'val loss is not updated while {p} epochs of training')\n    p += 1\n    if p > patience:\n        print(f'Early Stopping')\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ae_genes_train_dataset = DAEDataset(\n            ids=data_train[GENES].values,\n        )\n\nae_genes_test_dataset = DAEDataset(\n            ids=data_test[GENES].values,\n        )\n\nae_genes_train_loader = torch.utils.data.DataLoader(\n            ae_genes_train_dataset,\n            shuffle=False,\n            batch_size=Config.VALID_BATCH_SIZE,\n            num_workers=0, \n            pin_memory=True\n        )\n\nae_genes_test_loader = torch.utils.data.DataLoader(\n            ae_genes_test_dataset,\n            shuffle=False,\n            batch_size=Config.VALID_BATCH_SIZE,\n            num_workers=0, \n            pin_memory=True\n        )\n\nmodel.load_state_dict(torch.load('GENES_Reconstruction_model.pth'))\nae_genes_train_reconstruction = test_fn(ae_genes_train_loader, model, device)\nae_genes_test_reconstruction = test_fn(ae_genes_test_loader, model, device)\n\nprint(ae_genes_train_reconstruction.shape)\nprint(ae_genes_test_reconstruction.shape)\n\nautoencoder_error = mean_squared_error(data_test[GENES].values, ae_genes_test_reconstruction)\nprint(\"GENES reconstruction error is \" + str(autoencoder_error))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CELLS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = DAEDataset(\n            ids=data_train[CELLS].values,\n            ys=data_train[CELLS].values,\n        )\n    \ntrain_loader = torch.utils.data.DataLoader(\n            train_dataset,\n            shuffle=True,\n            batch_size=Config.TRAIN_BATCH_SIZE,\n            num_workers=0, \n            pin_memory=True\n        )\n\nval_dataset = DAEDataset(\n            ids=data_test[CELLS].values,\n            ys=data_test[CELLS].values,\n        )\n    \nval_loader = torch.utils.data.DataLoader(\n            val_dataset,\n            shuffle=False,\n            batch_size=Config.VALID_BATCH_SIZE,\n            num_workers=0, \n            pin_memory=True\n        )\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = DenoisingAutoEncoder(len(CELLS), 64)\nmodel = model.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30, eta_min=1e-6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = 0\nmin_loss = 999\npatience = 10\n\nfor epoch in range(1, Config.EPOCHS + 1):\n\n    print(\"Starting {} epoch...\".format(epoch))\n\n    train_fn(train_loader, model, optimizer, device)\n    val_loss = valid_fn(val_loader, model, device)\n    scheduler.step()\n    \n    if val_loss < min_loss:\n        min_loss = val_loss\n        best_epoch = epoch\n        torch.save(model.state_dict(), 'CELLS_Reconstruction_model.pth')\n        print(\"save model at min valid loss={} on epoch={}\".format(min_loss, best_epoch))\n        p = 0 \n\n    if p > 0: \n        print(f'val loss is not updated while {p} epochs of training')\n    p += 1\n    if p > patience:\n        print(f'Early Stopping')\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ae_cells_train_dataset = DAEDataset(\n            ids=data_train[CELLS].values,\n        )\n\nae_cells_test_dataset = DAEDataset(\n            ids=data_test[CELLS].values,\n        )\n\nae_cells_train_loader = torch.utils.data.DataLoader(\n            ae_cells_train_dataset,\n            shuffle=False,\n            batch_size=Config.VALID_BATCH_SIZE,\n            num_workers=0, \n            pin_memory=True\n        )\n\nae_cells_test_loader = torch.utils.data.DataLoader(\n            ae_cells_test_dataset,\n            shuffle=False,\n            batch_size=Config.VALID_BATCH_SIZE,\n            num_workers=0, \n            pin_memory=True\n        )\n\nmodel.load_state_dict(torch.load('CELLS_Reconstruction_model.pth'))\nae_cells_train_reconstruction = test_fn(ae_cells_train_loader, model, device)\nae_cells_test_reconstruction = test_fn(ae_cells_test_loader, model, device)\n\nprint(ae_cells_train_reconstruction.shape)\nprint(ae_cells_test_reconstruction.shape)\n\nautoencoder_error = mean_squared_error(data_test[CELLS].values, ae_cells_test_reconstruction)\nprint(\"CELLS reconstruction error is \" + str(autoencoder_error))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}