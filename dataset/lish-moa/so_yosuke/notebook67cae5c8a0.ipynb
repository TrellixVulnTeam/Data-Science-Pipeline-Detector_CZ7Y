{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\n#category_encodersは様々なカテゴリ特徴量(通常のものからバイナリ、OneHot、ハッシング\n#など様々)をいくつかの変換方法でNumeric型の特徴量に変換\n#最近のものでsklearn.preprocessingより良さげで、OneHot化も4行くらいでできちゃう\nfrom category_encoders import CountEncoder\n#PipelineはscikitAPIの機能の一つ。Estimator(データから学習する機能。fitなど)の処理を\n#まとめて実行可能\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import log_loss\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.multioutput import MultiOutputClassifier\n\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\nNFOLDS = 5\nDATA_DIR = '/kaggle/input/lish-moa/'\nnp.random.seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_DIR = '/kaggle/input/lish-moa/'\ntrain = pd.read_csv(DATA_DIR + 'train_features.csv')\ntargets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n\ntest = pd.read_csv(DATA_DIR + 'test_features.csv')\nsub = pd.read_csv(DATA_DIR + 'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sigIDのカラムを消し、ndarray化\nX = train.iloc[:,1:].to_numpy()\nX_test = test.iloc[:,1:].to_numpy()\ny = targets.iloc[:,1:].to_numpy() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#データを確認\nprint(train.shape, targets.shape, test.shape, sub.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#薬物VS薬物を溶かすのに使用した溶剤で、vehicleがその溶剤\ntrain.cp_type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#分類モデル\n#時間によってはXGBClassifier()引数にtree_method='gpu_hist'いれてAWSかGCPでGPU使用\n#category_encodersのCounterEncoder(cols=リスト)→エンコードする列のリストで\n#カラムの特徴量をその特徴の集合数の数値でカテゴリ数値化??orシンプルにリストの0番目と2番目をエンコード\n#pipelineは変換(fit,transform)を順番に一度で行う機能でpipeline(ステップ)→ステップは\n#リストに、学習機にかけるfit前データと最終的な学習器をそれぞれタプルで設定する\n#こんな風に>>> pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n#このオブジェクトにはいくつかメソッドもある\nclassifier = MultiOutputClassifier(XGBClassifier(tree_method='gpu_hist'))\nclf = Pipeline([('encode', CountEncoder(cols=[0,2])),\n                ('classify', classifier)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pipelineオブジェにさらにパラメータ設定\n#'モデル__estimator__通常のパラ設定'\nparams = {'classify__estimator__colsample_bytree': 0.6522,#各ステージの決定木の使う特徴量の割合\n          'classify__estimator__gamma': 3.6975,\n          'classify__estimator__learning_rate': 0.0503,\n          'classify__estimator__max_delta_step': 2.0706,\n          'classify__estimator__max_depth': 10,\n          'classify__estimator__min_child_weight': 31.5800,\n          'classify__estimator__n_estimators': 166,\n          'classify__estimator__subsample': 0.8639\n         }\n_ = clf.set_params(**params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#モデルの学習(目的変数のクラス数206個を学習させる必要があるためかなり時間かかる)\n\n#oofはout of foldで交差検証の分割で学習に使わなかったデータを指す\noof_preds = np.zeros(y.shape)\n#testを3982行206列に揃えて全て0に\ntest_preds = np.zeros((test.shape[0], y.shape[1]))\noof_losses = []\n#K-Fold交差検証でデータをk個に分けてn個を訓練用、残りk-n個をテスト用にわける\n#分けられたn個のデータが絶対1回はテスト用として使われるようn回検定する\n#n_splitはデータの分割数つまりk\nkf = KFold(n_splits=NFOLDS)#NFOLDS=5\n\n#k-fold.split(X, y=None, groups=None)で分割データを訓練、テストセットに代入\n#trn_idx(訓練用),val_idx(テスト用)にしてループ\nfor fn, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n    print('Starting fold: ', fn)\n    X_train, X_val = X[trn_idx], X[val_idx]\n    y_train, y_val = y[trn_idx], y[val_idx]\n    \n    #vehicleは直接的に関係なさそうなので削除\n    ctl_mask = X_train[:,0] == 'ctl_vehicle'\n    #~(チルダ)は2進数のビット反転が基本だがpandasでは集合Notの役割\n    X_train = X_train[~ctl_mask, :]\n    y_train = y_train[~ctl_mask]\n    \n    clf.fit(X_train, y_train)\n    #predict_proba(予測したいイテラブルデータ)で最終推定器ゆえサンプル数とクラス数のarray返す\n    #つまりテストデータでfit,transformしクラスあたりの推定リストを返す\n    val_preds = clf.predict_proba(X_val)\n    #陽性クラスを抽出\n    #array(奥行,行,列)ゆえ1の列が陽性??\n    val_preds = np.array(val_preds)[:,:,1].T\n    #0でまっさらなoof_predsの検証データに相当するarrayに前行で推測した陽性クラスをいれる\n    oof_preds[val_idx] = val_preds\n    \n    #sklearn.metrics.log_loss(正解y_true,予測y_pred)で交差エントロピーを使った評価\n    #np.ravelはflatten関数みたく多次元を1次元配列にして返すがflattenと違い元データを変更して返す点に注意\n    loss = log_loss(np.ravel(y_val), np.ravel(val_preds))\n    oof_losses.append(loss)\n    #テストデータも同様に予測かける\n    preds = clf.predict_proba(X_test)\n    preds = np.array(preds)[:,:,1].T\n    test_preds += preds / NFOLDS\n    \n    print(oof_losses)\n    print('Mean OOF loss across folds', np.mean(oof_losses))\n    print('STD OOF loss across folds', np.std(oof_losses))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ctl_vehicleを学習に考慮しないようにするためoof_preds内のvehicleタイプを再度0に設定\ncontrol_mask = train['cp_type']=='ctl_vehicle'\noof_preds[control_mask] = 0\n#正解y(train_targets_scored)と新たに設定した予測oof_predsで交差エントロピーの損失計算\nprint('OOF log loss: ', log_loss(np.ravel(y), np.ravel(oof_preds)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#テストデータでも同様にvehicleを考慮しないようにする\ncontrol_mask = test['cp_type']=='ctl_vehicle'\n\ntest_preds[control_mask] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#提出ファイルを作成\nsub.iloc[:,1:] = test_preds\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}