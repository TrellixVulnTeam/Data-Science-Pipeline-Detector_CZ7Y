{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import fastai\nimport optuna\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import QuantileTransformer\nimport sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n#from torchsampler import ImbalanceDatasetSampler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2=pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ndf2.shape[1]+206\ndf=pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE=\"cuda\"\nEPOCHS=30\nparams= {'num_layers': 3, 'hidden_size': 1024, 'dropout': 0.4, 'learning_rate': 0.01}\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ndf2=pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ndf=df.merge(df2,on='sig_id',how='inner')\ndf.loc[:,'kfold']=-1\ndf=df.sample(frac=1).reset_index(drop=True)\ntargets=df.drop(\"sig_id\",axis=1).values\nmksf=MultilabelStratifiedKFold(n_splits=2)\nfor f,(trn,val) in enumerate(mksf.split(X=df,y=targets)):\n    df.loc[val,'kfold']=int(f)\ndf.to_csv('train_targets_fold.csv',index=False)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoaDataset:\n    def __init__(self,features,targets):\n        self.features=features\n        self.targets=targets\n        \n    def __len__(self):\n        return self.features.shape[0]\n    def __getitem__(self,item):\n        return {\n            'x':torch.tensor(self.features[item,:],dtype=torch.float),\n            'y':torch.tensor(self.targets[item,:],dtype=torch.float)\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataset:\n    def __init__(self,features):\n        self.features=features\n        \n        \n    def __len__(self):\n        return self.features.shape[0]\n    def __getitem__(self,item):\n        return {\n            'x':torch.tensor(self.features[item,:],dtype=torch.float)\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Function to perform one hot encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"#df=pd.read_csv(\"../input/lish-moa/train_features.csv\")\ndef ohe(df):\n    ohe=pd.get_dummies(df['cp_time'])\n    df=pd.concat([df,ohe],axis=1)\n    ohe=pd.get_dummies(df['cp_type'])\n    df=pd.concat([df,ohe],axis=1)\n    ohe=pd.get_dummies(df['cp_dose'])\n    df=pd.concat([df,ohe],axis=1)\n    df.drop(['cp_type','cp_time','cp_dose'],axis=1,inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Engine Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Engine:\n    def __init__(self,model,optimizer,device):\n        self.model=model\n        self.optimizer=optimizer\n        self.device=device\n    @staticmethod    \n    def loss_fn(targets,outputs):\n        return nn.BCEWithLogitsLoss()(outputs,targets)\n    def train(self,data_loader):\n        self.model.train()\n        final_loss=0\n        for data in data_loader:\n            self.optimizer.zero_grad()\n            inputs=data['x'].to(self.device)\n            targets=data['y'].to(self.device)\n            outputs=self.model(inputs)\n            loss=self.loss_fn(targets,outputs)\n            loss.backward()\n            self.optimizer.step()\n            final_loss+=loss.item()\n        return final_loss/len(data_loader)    \n            \n    def evaluate(self,data_loader):\n        self.model.eval()\n        final_loss=0\n        for data in data_loader:\n            #self.optimizer.zero_grad()\n            inputs=data['x'].to(self.device)\n            targets=data['y'].to(self.device)\n            outputs=self.model(inputs)\n            loss=self.loss_fn(targets,outputs)\n            #loss.backward()\n            #self.optimizer.step()\n            final_loss+=loss.item()\n        return final_loss/len(data_loader)       ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self,nfeatures,ntargets,nlayers,hidden_size,dropout):\n        super().__init__()\n        layers=[]\n        for _ in range(nlayers-1):\n            if len(layers)==0:\n                layers.append(nn.BatchNorm1d(nfeatures))\n                layers.append(nn.Dropout(dropout))\n                layers.append(nn.Linear(nfeatures,hidden_size))\n                layers.append(nn.Tanh())\n                \n                \n                \n            else:\n               \n                layers.append(nn.BatchNorm1d(hidden_size))\n                layers.append(nn.Dropout(dropout))\n                layers.append(nn.Linear(hidden_size,hidden_size))\n                layers.append(nn.Tanh())\n                \n        if len(layers)==0:\n            layers.append(nn.BatchNorm1d(nfeatures))\n            layers.append(nn.Dropout(dropout))\n            layers.append(nn.Linear(nfeatures,ntargets))\n            \n        else :\n            layers.append(nn.BatchNorm1d(hidden_size))\n            layers.append(nn.Dropout(dropout))\n            layers.append(nn.Linear(hidden_size,ntargets))\n        self.model=nn.Sequential(*layers)\n            \n    def forward(self,x):\n        return self.model(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef run_training(fold,params,save_model=True):\n    df=pd.read_csv(\"../input/lish-moa/train_features.csv\")\n    \n    df=ohe(df)\n    \n    targets_df=pd.read_csv(\"./train_targets_fold.csv\")\n    features_columns=df.drop('sig_id',axis=1).columns\n    targets_columns=targets_df.drop(['sig_id','kfold'],axis=1).columns\n    df=df.merge(targets_df,on='sig_id',how='inner')\n    train_df=df[df.kfold!=fold].reset_index(drop=True)\n    valid_df=df[df.kfold==fold].reset_index(drop=True)\n    \n    x_train=train_df[features_columns].to_numpy()\n    x_valid=train_df[features_columns].to_numpy()\n    y_train=train_df[targets_columns].to_numpy()\n    y_valid=train_df[targets_columns].to_numpy()\n    \n    \n    train_dataset=MoaDataset(features=x_train,targets=y_train)\n    valid_dataset=MoaDataset(features=x_valid,targets=y_valid)\n    \n    train_loader=torch.utils.data.DataLoader(train_dataset,batch_size=64,num_workers=8)\n    valid_loader=torch.utils.data.DataLoader(valid_dataset,batch_size=64,num_workers=8)\n    \n    \n    model=Model(\n        nfeatures=x_train.shape[1],\n        ntargets=y_train.shape[1],\n        nlayers=params[\"num_layers\"],\n        hidden_size=params[\"hidden_size\"],\n        dropout=params[\"dropout\"]\n    \n    )\n    model.to(DEVICE)\n    optimizer=torch.optim.Adam(model.parameters(),lr=params[\"learning_rate\"])\n    eng=Engine(model,optimizer,device=DEVICE)\n    best_loss=np.inf\n    \n    early_stopping_iter=10\n    early_stopping_counter=0\n    \n    for epoch in range(EPOCHS):\n        train_loss=eng.train(train_loader)\n        valid_loss=eng.evaluate(valid_loader)\n        print(f\"Fold-{fold},--EPOCH-{epoch},--TRAIN_LOSS-{train_loss},--VAL_LOSS--{valid_loss}\")\n        \n        if best_loss>valid_loss:\n            best_loss=valid_loss\n            if save_model:\n                torch.save(model.state_dict(),f\"model_{fold}.pth\")\n                c=fold\n        else :\n            early_stopping_counter+=1\n            \n        if early_stopping_counter>early_stopping_iter:\n            break\n    return best_loss,c    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#def objective(trial):\n #   params={\n  #      \"num_layers\":trial.suggest_int(\"num_layer\",1,5),\n   #    \"hidden_size\":trial.suggest_int(\"hidden_size\",800,2048),\n    #    \"dropout\":trial.suggest_uniform(\"dropout\",0.1,0.7),\n     #   \"learning_rate\":trial.suggest_loguniform(\"learning_rate\",1e-6,1e-3)\n\n    #}\n    \n   # all_losses=[]\n    #for f_ in range(1):\n     #  temp_loss=run_training(f_,params,save_model=False)\n      # all_losses.append(temp_loss)\n    #return np.mean(all_losses)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#study=optuna.create_study(direction=\"minimize\")\n#study.optimize(objective,n_trials=100)\n#print(\"best trial\")\n#trial_=study.best_trial\n#print(trial_.values)\n#print(trial_.params)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor i in range(1): \n    a,b=run_training(i,params,save_model=True)\n    print(f\"Best Score--{a}--Best Model at fold--{b}\")\n    \n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/lish-moa/test_features.csv\")\ndf=ohe(df)\nfeatures_columns=df.drop('sig_id',axis=1).columns\nx_test=df[features_columns].to_numpy()\ntestdataset = TestDataset(x_test)\ntestloader = torch.utils.data.DataLoader(testdataset, batch_size=1024, shuffle=False)\n    \nmodel=Model(\n        nfeatures=df.shape[1]-1,\n        ntargets=608,\n        nlayers=params[\"num_layers\"],\n        hidden_size=params[\"hidden_size\"],\n        dropout=params[\"dropout\"]\n    \n    \n    \n    )\nmodel.load_state_dict(torch.load(f\"model_{b}.pth\"))\nmodel.to(DEVICE)\n    \npredictions = np.zeros((128, 608))\npredictions = inference_fn(model, testloader, DEVICE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_df=pd.read_csv(\"../input/lish-moa/test_features.csv\")\ntarget=pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntarget=target.drop('sig_id',axis=1).columns\nid=_df.loc[_df['cp_type'] =='ctl_vehicle', 'sig_id']\n\n_df=pd.DataFrame(predictions[:,0:206],columns=list(target),index=_df['sig_id'])\n_df.index[0]\nfor i in range(len(_df.index)):\n    if _df.index[i] in(id):\n        _df.iloc[_df.index[i],train_targets_scored.columns[1:]]=0\n_df.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_df.columns","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}