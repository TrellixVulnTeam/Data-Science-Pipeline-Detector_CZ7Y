{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"IS_KAGGLE_KERNEL = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport datetime\nimport os\n\nimport tensorflow as tf\nfrom tensorflow import keras as keras\nfrom tensorflow.keras import layers, optimizers, activations, losses, backend\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nimport tensorflow_addons as tfa\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.utils import resample, shuffle\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import recall_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import log_loss\n# !pip install iterative_stratification\n# if IS_KAGGLE_KERNEL:\n#     ! pip install \"/kaggle/input/moa-env/joblib-0.17.0-py3-none-any.whl\"\n#     ! pip install \"/kaggle/input/moa-env/iterative_stratification-0.1.6-py3-none-any.whl\"\n!pip install ../input/iterstrat\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def with_input_path(s):\n    src = \"./kaggle/input/lish-moa/\"\n    if IS_KAGGLE_KERNEL:\n        src = src[1:]\n    return os.path.join(src, s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Basic data preprocessing\n\nLoad the data, drop out the ID columns and control columns, replace dosages with numerics, and normalize."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare train data\ndf_train = pd.read_csv(with_input_path(\"train_features.csv\"))\ndf_test = pd.read_csv(with_input_path(\"test_features.csv\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# id is meaningess signifier\ndf_train = df_train.drop(\"sig_id\", axis=1)\ndf_test = df_test.drop(\"sig_id\", axis=1)\n\n# cp_type indicates control (just vehicle) vs. drug. For now, we'll set all control experiments \n# to have zero MoA's before submission. We will therefore ignore this feature in training\ntrain = df_train.copy()\ndf_train = df_train[train[\"cp_type\"] != 'ctl_vehicle'].reset_index(drop=True)\n# train_control_locs = df_train.loc[df_train[\"cp_type\"] == 'ctl_vehicle'].index\ndf_train = df_train.drop(\"cp_type\", axis=1)\n\n# Save these to set control exp MoA's to zero after training\ntest_control_locs = df_test.loc[df_test[\"cp_type\"] == 'ctl_vehicle'].index\ndf_test = df_test.drop(\"cp_type\", axis=1)\n\n# Dosages are strings right now. I don't exactly know the dosages used but we can pretend it was either a single\n# dose or a double dose\ndf_train['cp_dose'].replace('D1', 1, inplace=True)\ndf_train['cp_dose'].replace('D2', 2, inplace=True)\ndf_test['cp_dose'].replace('D1', 1, inplace=True)\ndf_test['cp_dose'].replace('D2', 2, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize train data and test data simultaneously\nscaler = MinMaxScaler(feature_range=(-1, 1))\nX_total = np.vstack((df_train, df_test))\nscaler.fit(X_total)\nX_train = scaler.transform(df_train)\nX_test = scaler.transform(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Compress cell viabilities with PCA since they're highly correlated\nn, _ = X_train.shape\npca = PCA(0.97) # Cutoff at 97% cum. explained variance\ncell_v_pca = pca.fit_transform(X_total[:,-100:])\n\nX_train = np.hstack((X_train[:,:-100], cell_v_pca[:n,:]))\nX_test = np.hstack((X_test[:,:-100], cell_v_pca[n:,:]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare train labels\ndf_targets = pd.read_csv(with_input_path(\"train_targets_scored.csv\"))\ndf_targets = df_targets[train[\"cp_type\"] != 'ctl_vehicle'].reset_index(drop=True)\ny_train = df_targets.drop(\"sig_id\", axis=1).to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n, input_dim = X_train.shape\nprint(input_dim)\nn, num_labels = y_train.shape\nn_test, _ = X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction Clipping Thresholds\n\np_min = 0.001\np_max = 0.999\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred, p_min, p_max)\n    return -backend.mean(y_true * backend.log(y_pred) + (1 - y_true) * backend.log(1 - y_pred))\n\n\ndef make_model(input_dim):    \n    #4 Layer feedforward NN\n    model = keras.Sequential()\n\n    model.add(layers.Input(input_dim))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(0.2))\n    \n    model.add(tfa.layers.WeightNormalization(\n        layers.Dense(2048, activation=\"relu\", name=\"layer1\")))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(.4))\n\n    model.add(tfa.layers.WeightNormalization(\n        layers.Dense(1024, activation=\"relu\", name=\"layer2\")))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(.4))\n    \n    model.add(tfa.layers.WeightNormalization(\n        layers.Dense(512, activation=\"sigmoid\", name=\"layer3\")))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(.4))\n    \n    model.add(tfa.layers.WeightNormalization(\n        layers.Dense(256, activation=\"sigmoid\", name=\"layer4\")))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(.4))\n    \n    model.add(tfa.layers.WeightNormalization(\n        layers.Dense(256, activation=\"relu\", name=\"layer5\")))\n    model.add(layers.Dense(num_labels, activation=\"sigmoid\", name=\"output\"))\n\n    optimizer = optimizers.Adam()\n    loss = losses.BinaryCrossentropy(label_smoothing=0.005)\n    \n    # Early stopping if model converges\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_logloss', min_delta=1e-5, patience=5, verbose=0,\n                                                      mode='min', restore_best_weights=True)\n    model.compile(optimizer=optimizer, loss=loss, metrics=logloss)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 300\nbatch_size = 2300\n\n# CV splits (different seeds)\nn_splits = 7\nseeds = [394, 388, 2772, 105]\nn_seeds = len(seeds)\n\n# Rolling averages for validation scores and test predictions\navg_score = 0\ntest_preds = np.zeros((n_test, num_labels))\n\nhistories = []\n\n\ndf_targets = pd.read_csv(with_input_path(\"train_targets_scored.csv\"))\ndf_targets = df_targets[train[\"cp_type\"] != 'ctl_vehicle'].reset_index(drop=True)\ndf_id = pd.read_csv(with_input_path(\"train_drug.csv\"))\ndf_id = df_id[train[\"cp_type\"] != 'ctl_vehicle'].reset_index(drop=True)\npkg = (df_id, df_targets)\n\nfor i, seed in enumerate(seeds):\n    for j, (train_locs, val_locs) in enumerate(MultilabelStratifiedKFold(n_splits=n_splits, random_state=seed, shuffle=True).split(X_train, y_train)):\n        model = make_model(input_dim=input_dim)\n        X_train_bal = X_train[train_locs]\n        y_train_bal = y_train[train_locs]\n        Xval = X_train[val_locs]\n        yval = y_train[val_locs]\n        reduce_lr_loss = ReduceLROnPlateau(\n            monitor='val_logloss', factor=0.1, patience=5, verbose=1, min_delta=1e-4, mode='min')\n        \n        \n        history_model = model.fit(x=X_train_bal, \n                            y=y_train_bal, \n                            epochs=epochs, \n                            batch_size=batch_size,\n                            validation_data=(Xval, yval), \n                            callbacks=[reduce_lr_loss])\n        histories.append(history_model)\n        y_preds = model.predict(Xval)\n        fold_score = logloss(yval, y_preds)\n        print(\"\\t seed {}, fold {} validation score: {}\".format(i, j, fold_score))\n        avg_score += fold_score / (n_splits * n_seeds)\n\n        # Update test score from this fold/cv\n        test_preds += model.predict(X_test) / (n_splits * n_seeds)\n\ntrg_loss_dnn_orig = history_model.history['loss']\nval_loss_dnn_orig = history_model.history['val_loss']\nepochs = range(1, 301)\nfig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss_dnn_orig, 'r',  linewidth=3, label='Training Loss')\nplt.plot(epochs, val_loss_dnn_orig, 'g',  linewidth=3, label='Validation Loss')\nplt.title(\"Training / Validation Loss\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nax.set_facecolor(\"white\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(with_input_path(\"sample_submission.csv\"))\nsub.iloc[:,1:].shape\nsub.iloc[:,1:] = np.clip(test_preds, p_min, p_max)\nsub.iloc[test_control_locs, 1:] = 0\n\nsub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}