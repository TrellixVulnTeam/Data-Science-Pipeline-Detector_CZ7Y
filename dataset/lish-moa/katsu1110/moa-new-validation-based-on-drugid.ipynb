{"cells":[{"metadata":{},"cell_type":"markdown","source":"<center><h2>MoA | Keras Multilabel Classifier NN | New Starter </h2></center><hr>\n\nWe now have drug ID for the training data, which we can take advantage of for cross-validation. Here I used a great validation strategy ([Drug and MultiLabel Stratification Code](https://www.kaggle.com/c/lish-moa/discussion/195195)) proposed by @cdeotte.\n\nThis kernel can be a good starter using the drug ID."},{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sys.path.append('../input/rank-gauss')\nfrom gauss_rank_scaler import GaussRankScaler","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os, sys\nimport gc\nimport math\nimport random\nfrom tqdm import tqdm\nfrom typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn import linear_model\nimport umap\n\n# keras\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import models\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import utils\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import log_loss\nimport lightgbm as lgb\nfrom tqdm import tqdm\n\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nfrom matplotlib_venn import venn2\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nsns.set_context(\"talk\")\nstyle.use('fivethirtyeight')\npd.options.display.max_columns = None\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Config"},{"metadata":{"trusted":true},"cell_type":"code","source":"N_STARTS = 12 # the number of seed average\nN_SPLITS = 12 # the number of folds\nSEED = 42\nVAR_THRESHOLD = 0.6\nN_COMPONENTS = [360, 40] # g-feats, c-feats\nDROPOUT = 0.24\nPOSTPROCESS = False\nVERBOSE = 0\nBATCH_SIZE = 128\nEPOCHS = 160\nLR = 0.001\nNUM_NEURON = 1024 # the number of neurons in the first layer\nDECAY_FACTOR = 2 # decides the number of neurons in the second layer by dividing 'NUM_NEURON'\nNN_NORM = 'batch' # layer\nNUM_HIDDEN_LAYER = 1 # the number of hidden layer\nAF = 'mish' # name of activation function\n\nDEBUG = False\nif DEBUG:\n    N_STARTS = 1\n    VERBOSE = 2\n    print('DEBUG TRUE!!!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\n\nprint('loading train, test, targets, drugs')\ntrain_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\ndrug = pd.read_csv('../input/lish-moa/train_drug.csv')\n    \nprint('merging drug ID')\ntrain_features = pd.merge(train_features, drug, how='left', on='sig_id')\n\nprint('no ctl')\ntrain_g = train_features['cp_type'] != 'ctl_vehicle'\ncontrol_g = test_features['cp_type'] == 'ctl_vehicle'\ntest_g = test_features['cp_type'] != 'ctl_vehicle'\n\ntest_features = test_features.loc[test_g, :].reset_index(drop=True)\ntrain_features = train_features.loc[train_g, :].reset_index(drop=True)\ntrain_targets = train_targets.loc[train_g, :].reset_index(drop=True)    \ntargets = [f for f in train_targets.columns.values.tolist() if 'sig_id' not in f]\n\nprint('loading non-targets')\ntrain_targets_non = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ntrain_targets_non = train_targets_non.loc[train_g, :].reset_index(drop=True)\nvari = train_targets_non.var().reset_index()\ntrain_targets_non = train_targets_non[[f for f in train_targets_non.columns.values.tolist() if f in vari.loc[vari[0] > 0, 'index'].values.tolist()]]\nnon_targets = [f for f in train_targets_non.columns.values.tolist() if 'sig_id' not in f]\n    \nss = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# variance threshold\ndata_all = pd.concat([train_features, test_features], ignore_index=True)\ncols_numeric = [feat for feat in list(data_all.columns) if feat not in ['sig_id', 'drug_id', 'cp_type', 'cp_time', 'cp_dose']]\nmask = (data_all[cols_numeric].var() >= VAR_THRESHOLD).values\ntmp = data_all[cols_numeric].loc[:, mask]\ndata_all = pd.concat([data_all[['sig_id', 'drug_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    df = df.copy()\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})\n    df = pd.get_dummies(df, columns=['cp_time','cp_dose'])\n    return df\n\ndata_all = preprocess(data_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data_all.shape)\ndata_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_targets.shape)\ntrain_targets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features['drug_id'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Assign Folds\nThis is based on [Drug and MultiLabel Stratification Code](https://www.kaggle.com/c/lish-moa/discussion/195195) proposed by @cdeotte. Thanks a lot for this great implementation."},{"metadata":{"trusted":true},"cell_type":"code","source":"def assign_folds(train, train_targets, targets, seed=SEED):\n    # LOCATE DRUGS\n    scored = train_targets.copy()\n    scored = pd.merge(scored, train[['sig_id', 'drug_id']], how='left', on='sig_id')\n    vc = scored['drug_id'].value_counts()\n#     vc1 = vc.loc[(vc==6)|(vc==12)|(vc==18)].index.sort_values()\n#     vc2 = vc.loc[(vc!=6)&(vc!=12)&(vc!=18)].index.sort_values()\n    vc1 = vc.loc[vc <= 18].index.sort_values()\n    vc2 = vc.loc[vc > 18].index.sort_values()\n\n    # STRATIFY DRUGS 18X OR LESS\n    dct1 = {}; dct2 = {}\n    skf = MultilabelStratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=seed)\n    tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[targets])):\n        dd = {k:fold for k in tmp.index[idxV].values}\n        dct1.update(dd)\n\n    # STRATIFY DRUGS MORE THAN 18X\n    skf = MultilabelStratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=seed)\n    tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\n    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n        dd = {k:fold for k in tmp.sig_id[idxV].values}\n        dct2.update(dd)\n\n    # ASSIGN FOLDS\n    scored['fold'] = scored['drug_id'].map(dct1)\n    scored.loc[scored.fold.isna(),'fold'] =\\\n        scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\n    scored.fold = scored.fold.astype('int8')\n    \n    return scored['fold'].values\n\n# example\nfolds = assign_folds(train_features, train_targets, train_targets.columns.values[1:].tolist(), SEED)\npd.DataFrame(folds).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fold examples\nn = 0\ntr = np.where(folds != n)[0]\nte = np.where(folds == n)[0]\nprint(tr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(te)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Group targets\nJust manually for now..."},{"metadata":{"trusted":true},"cell_type":"code","source":"def target_category_maker(targets):\n    targets_category = {'agonist': [], 'antagonist': [], 'agent': [], 'others': []}\n    for t in targets:\n        if ('_agonist' in t) | ('_activator' in t) | ('_stimulant' in t) | ('_secretagogue' in t) | ('_sensitizer' in t):\n            targets_category['agonist'].append(t)\n        elif ('_antagonist' in t) | ('_inhibitor' in t) | ('_blocker' in t):\n            targets_category['antagonist'].append(t)\n        elif ('_agent' in t) | ('_medium' in t):\n            targets_category['agent'].append(t)\n        else:\n            targets_category['others'].append(t)\n    return targets_category\n\ntargets_category = target_category_maker(targets)\nnon_targets_category = target_category_maker(non_targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# positive label ratio (scored)\nfor k in list(targets_category.keys()):\n    print('----------------------------')\n    print('{} ({:,} features)'.format(k, len(targets_category[k])))\n    print('----------------------------')\n    for t in targets_category[k]:\n        print('{}: {:,} ({:.3f} %) positive.'.format(t, train_targets[t].sum(), 100 * train_targets[t].sum() / train_targets.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# positive label ratio (non-scored)\nfor k in list(non_targets_category.keys()):\n    print('----------------------------')\n    print('{} ({:,} features)'.format(k, len(non_targets_category[k])))\n    print('----------------------------')\n    for t in non_targets_category[k]:\n        print('{}: {:,} ({:.3f} %) positive.'.format(t, train_targets_non[t].sum(), 100 * train_targets_non[t].sum() / train_targets_non.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering (agg, pca)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# categorize feats\ng_feats = [f for f in data_all.columns.values.tolist() if f.startswith('g-')]\nc_feats = [f for f in data_all.columns.values.tolist() if f.startswith('c-')]\ncp_feats = [f for f in data_all.columns.values.tolist() if f.startswith('cp_')]\nprint(len(g_feats), len(c_feats), len(cp_feats))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# agg features\ndef add_stats_feats(df, feat_list, n):\n    # by row\n    df[f'{n}stats-mean'] = df[feat_list].mean(axis=1)\n    df[f'{n}stats-std'] = df[feat_list].std(axis=1)\n    df[f'{n}stats-skew'] = df[feat_list].skew(axis=1)\n    df[f'{n}stats-kurt'] = df[feat_list].kurt(axis=1)\n    df[f'{n}stats-mad'] = df[feat_list].mad(axis=1)\n                \n    return df\n    \ndata_all = add_stats_feats(data_all, g_feats, 'g')\ndata_all = add_stats_feats(data_all, c_feats, 'c')\ndata_all = add_stats_feats(data_all, g_feats+c_feats, 'gc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# PCA features\nscaler = StandardScaler()\ndata_all[g_feats+c_feats] = scaler.fit_transform(data_all[g_feats+c_feats])\n\n# dimensionality reduction\ndef dim_reducer(data_all, feats, n_components=100):\n    trans = PCA(n_components=n_components, random_state=SEED)\n    train_dist = trans.fit_transform(data_all[feats].values)\n    \n    return train_dist\n\ntrain_g = dim_reducer(data_all, g_feats, n_components=N_COMPONENTS[0])\ntrain_c = dim_reducer(data_all, c_feats, n_components=N_COMPONENTS[1])\n\nfor i in range(train_g.shape[1]):\n    data_all[f'g-pca{i+1}'] = train_g[:, i]\nfor i in range(train_c.shape[1]):\n    data_all[f'c-pca{i+1}'] = train_c[:, i]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scaling features for NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"feats = data_all.columns.values.tolist()\ndrops = ['sig_id', 'cp_type', 'drug_id']\nfeats = [f for f in feats if f not in drops]\nprint('{:,} features'.format(len(feats)))\nprint(feats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# combine\nt_feats = [f for f in feats if 'cp_' not in f]\n\n# rank gauss transform\npt = GaussRankScaler()\ndata_all[t_feats] = pt.fit_transform(data_all[t_feats])\n\n# final scaling\nscaler = StandardScaler()\ndata_all[t_feats] = scaler.fit_transform(data_all[t_feats])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = data_all.iloc[:len(train_features)]\ntest = data_all.iloc[len(train_features):]\n\ndel data_all\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.shape)\ntest.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed : int) -> NoReturn :    \n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Activation\nfrom tensorflow.keras.utils import get_custom_objects\n\n# mish\nclass Mish(Activation):\n    '''\n    Mish Activation Function.\n    .. math::\n        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n    Shape:\n        - Input: Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model.\n        - Output: Same shape as the input.\n    Examples:\n        >>> X = Activation('Mish', name=\"conv1_act\")(X_input)\n    '''\n\n    def __init__(self, activation, **kwargs):\n        super(Mish, self).__init__(activation, **kwargs)\n        self.__name__ = 'Mish'\n\ndef mish(inputs):\n    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n\nget_custom_objects().update({'mish': Mish(mish)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_min = 0.001\np_max = 0.999\n\ndef logloss(y_true, y_pred):\n    tf.dtypes.cast(y_true, tf.float32)\n    tf.dtypes.cast(y_pred, tf.float32)\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -K.mean(y_true*K.log(y_pred) + (1-y_true)*K.log(1-y_pred))\n\ndef create_model(num_feats):\n    \"\"\"very simple MLP for now\"\"\"\n\n    # input, first layer\n    inp = layers.Input(shape=(num_feats,), name=\"inp\")\n    x = layers.Dense(NUM_NEURON, activation=AF)(inp)\n    if NN_NORM == 'layer':\n        x = layers.LayerNormalization()(x)    \n    elif NN_NORM == 'batch':\n        x = layers.BatchNormalization()(x)\n    x = layers.Dropout(DROPOUT)(x)\n    \n    # second or later layers\n    for i in range(NUM_HIDDEN_LAYER):\n        x = layers.Dense(NUM_NEURON // DECAY_FACTOR, activation=AF)(inp)\n        if NN_NORM == 'layer':\n            x = layers.LayerNormalization()(x)    \n        elif NN_NORM == 'batch':\n            x = layers.BatchNormalization()(x)\n        x = layers.Dropout(DROPOUT)(x)\n    preds = layers.Dense(206, activation='sigmoid')(x)\n    \n    model = models.Model(inp, preds)\n    \n    opt = optimizers.Adam(lr=LR)\n    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001)\n    model.compile(loss=loss, optimizer=opt, metrics=logloss)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model(len(feats))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import plot_model\nplot_model(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"def metric(y_true, y_pred, targets):\n    metrics = []\n    for _target in targets:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n    return np.mean(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_nfold(train, test, ss, train_targets, feats, targets, seed=SEED, n_splits=N_SPLITS):\n    res = train_targets.copy()\n    ss.loc[:, targets] = 0\n    res.loc[:, targets] = 0\n\n    folds = assign_folds(train, train_targets, targets, seed=seed)\n    historys = dict()\n\n    for n in range(n_splits):\n        # train test split\n        tr = np.where(folds != n)[0]\n        te = np.where(folds == n)[0]\n        \n        print(f\"======{train_targets.values[tr].shape}========{train_targets.values[te].shape}=====\")\n        print(f'Seed: {seed} => Fold: {n}')\n\n        if DEBUG:\n            if n > 0:\n                print(f'Skip fold{n}')\n                continue\n\n        # NN model\n        model = create_model(len(feats))\n\n        # callbacks\n        checkpoint_path = f'repeat{seed}_fold{n}.hdf5'\n        reduce_lr_loss = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, min_lr=1e-5, patience=4, verbose=VERBOSE, mode='min')\n        cb_checkpt = callbacks.ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 2, save_best_only = True,\n                                     save_weights_only = True, mode = 'min')\n        early = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", restore_best_weights=True, patience=8, verbose = VERBOSE)\n        nn_callbacks = [reduce_lr_loss, cb_checkpt, early]\n        \n        # nn datasets\n        x_train = train[feats].values[tr]\n        x_val = train[feats].values[te]\n        x_test = test[feats].values\n        y_train = train_targets[targets].values[tr]\n        y_val = train_targets[targets].values[te]\n        \n        # fit\n        history = model.fit(x_train, y_train, \n                  validation_data=(x_val, y_val),\n                  epochs=EPOCHS, batch_size=BATCH_SIZE,\n                  callbacks=nn_callbacks, verbose=VERBOSE\n                 )\n        historys[f'history_{n}'] = history\n\n        # predict\n        model.load_weights(checkpoint_path)\n        test_predict = model.predict(x_test)\n        val_predict = model.predict(x_val)\n        \n        # assign\n        res.loc[te, targets] = val_predict\n        ss.loc[test_g, targets] += test_predict / n_splits\n\n        print(f'OOF Metric For SEED {seed} => FOLD {n} : {metric(train_targets.loc[te, targets], pd.DataFrame(val_predict, columns=targets), targets)}')\n        print('+-' * 10)\n\n    # average predictions\n    print(f'OOF Metric: {metric(train_targets[targets], res[targets], targets)}')\n    return ss, res, historys","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = train_targets.copy()\nres.loc[:, targets] = 0\nss.loc[:, targets] = 0\n    \n# seed average\nfor s in range(N_STARTS):\n    # kfold\n    ss_tmp = ss.copy()\n    ss_, res_, historys = fit_nfold(train, test, ss_tmp, train_targets, feats, targets, seed=SEED+s**2, n_splits=N_SPLITS)\n    \n    # add\n    ss.loc[:, targets] += ss_[targets].values / N_STARTS\n    res.loc[:, targets] += res_[targets].values / N_STARTS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training & validation loss values\ndef plot_history(history):\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc='upper right', frameon=False)\n    plt.show()\n    \nplot_history(historys[f'history_0'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'OOF Metric: {metric(train_targets, res, targets)}')\n\nif POSTPROCESS:\n    print('post-process...')\n\n    # clip\n    ss.iloc[:,1:] = np.clip(ss.values[:, 1:], p_min, p_max)\n\n    # Set ctl_vehicle to 0\n    ss.iloc[control_g, 1:] = 0\n    \nss.to_csv('submission.csv', index=False)\n\nprint(ss.shape)\nss.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}