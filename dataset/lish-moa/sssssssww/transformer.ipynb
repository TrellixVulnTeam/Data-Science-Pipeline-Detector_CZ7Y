{"cells":[{"metadata":{"_uuid":"a8ac1b70-f8bf-4310-9c1f-7189c0844c45","_cell_guid":"b565a981-ebf7-4f53-994a-6eb5c428e0b0","trusted":true},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n# [Mechanisms of Action (MoA) Prediction](https://www.kaggle.com/c/lish-moa)"},{"metadata":{"_uuid":"54d3dbdb-ecf1-4e94-9f7a-68f4d0d6fd1b","_cell_guid":"b8484537-eb74-4879-9475-f57dc1094eb1","trusted":true},"cell_type":"markdown","source":"## 1. Import libraries<a class=\"anchor\" id=\"1\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"7ff49cdc-a340-4540-9153-e4aa0f45ed19","_cell_guid":"74cb667c-b7fb-4898-87d6-154c1f0fa1fd","trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterativestratification')\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport os\nimport copy\nimport gc\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport scipy.stats as stats\nfrom scipy.stats import kurtosis\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nos.listdir('../input/lish-moa')\n\npd.set_option('max_columns', 2000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d10f62bd-54d4-487c-a44c-ad8ab295cdc2","_cell_guid":"a7ad6fe8-da34-4d8f-b6ec-eef3501c1334","trusted":true},"cell_type":"code","source":"# From optimal commit 9\nn_comp_GENES = 400\nn_comp_CELLS = 50\nVarianceThreshold_for_FS = 0.9\nDropout_Model = 0.25\n#QT_n_quantile_min=50,\n#QT_n_quantile_max=1000,\nprint('n_comp_GENES', n_comp_GENES, 'n_comp_CELLS', n_comp_CELLS, 'total', n_comp_GENES + n_comp_CELLS)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81b6bf97-bb73-4777-bf2d-3025326c885c","_cell_guid":"40f8c352-f3f6-4827-8537-a64aac939cc2","trusted":true},"cell_type":"markdown","source":"## 2. My upgrade <a class=\"anchor\" id=\"2\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"37032e9a-3ae9-45ae-808b-2bdef8b123ea","_cell_guid":"356dc331-01e8-4676-b7c6-35ade3a9a5aa","trusted":true},"cell_type":"markdown","source":"## 3. Download data<a class=\"anchor\" id=\"3\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"37424358-1623-485b-90b7-19aefa419f47","_cell_guid":"d1b1f725-8b1d-4195-b681-e6952d44c20a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87fecf32-c7e8-4342-920d-18e637aa00f4","_cell_guid":"240d4271-4e47-456b-9c4c-d73d819b5ef6","trusted":true},"cell_type":"markdown","source":"## 4. FE & Data Preprocessing <a class=\"anchor\" id=\"4\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"5a175ba7-2e65-444b-9f94-8e62bb92a83b","_cell_guid":"b7b810cc-8425-4726-946a-57df3dc91016","trusted":true},"cell_type":"code","source":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"126d31cc-0d9a-443a-a06c-81d4f0fd9edc","_cell_guid":"cc8f8d0b-5b8c-446e-9b6d-e6e4e4452637","trusted":true},"cell_type":"markdown","source":"### 4.1 RankGauss<a class=\"anchor\" id=\"4.1\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"70d4ceb3-9aef-4688-945a-2654ced0ce69","_cell_guid":"2dafe004-8152-45cb-8806-e16139f636f5","trusted":true},"cell_type":"code","source":"# Search for minimum and maximum values\n# df_kurt = pd.DataFrame(columns=['col','train', 'test'])\n# i = 0\n# for col in (GENES + CELLS):\n#     df_kurt.loc[i, 'col'] = col\n#     df_kurt.loc[i, 'train'] = kurtosis(train_features[col])\n#     df_kurt.loc[i, 'test'] = kurtosis(test_features[col])\n#     i += 1\n# print(df_kurt.min())\n# print(df_kurt.max())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd5022de-73f9-497e-a049-fdb5a94ccc45","_cell_guid":"cdb4a111-0169-4981-82c2-bf4272b491a1","trusted":true},"cell_type":"code","source":"def calc_QT_par_kurt(QT_n_quantile_min=10, QT_n_quantile_max=200):\n    # Calculation parameters of function: n_quantile(kurtosis) = k1*kurtosis + k0\n    # For Train & Test datasets (GENES + CELLS features): minimum kurtosis = 1.53655, maximum kurtosis = 30.4929\n    \n    a = np.array([[1.53655,1], [30.4929,1]])\n    b = np.array([QT_n_quantile_min, QT_n_quantile_max])\n    \n    return np.linalg.solve(a, b)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad8f87bb-dbc6-4f70-b4bf-d14abf8fa513","_cell_guid":"b1601607-07cd-4a4c-91dd-f25b636e3cb7","trusted":true},"cell_type":"code","source":"def n_quantile_for_kurt(kurt, calc_QT_par_kurt_transform):\n    # Calculation parameters of function: n_quantile(kurtosis) = calc_QT_par_kurt_transform[0]*kurtosis + calc_QT_par_kurt_transform[1]\n    return int(calc_QT_par_kurt_transform[0]*kurt + calc_QT_par_kurt_transform[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3f72561-b54b-460e-87cf-efa714fc5bb7","_cell_guid":"161d68e4-fb25-4bee-a77c-1b271badd947","trusted":true},"cell_type":"code","source":"# RankGauss - transform to Gauss\n\nfor col in (GENES + CELLS):\n\n    #kurt = max(kurtosis(train_features[col]), kurtosis(test_features[col]))\n    #QuantileTransformer_n_quantiles = n_quantile_for_kurt(kurt, calc_QT_par_kurt(QT_n_quantile_min, QT_n_quantile_max))\n    #transformer = QuantileTransformer(n_quantiles=QuantileTransformer_n_quantiles,random_state=0, output_distribution=\"normal\")\n    \n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")   # from optimal commit 9\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a4e26d2-8b94-45e8-9b53-3b45c880f51d","_cell_guid":"656ff446-1ee7-4d0c-9404-ea8694fbe056","trusted":true},"cell_type":"markdown","source":"### 4.2 Seed<a class=\"anchor\" id=\"4.2\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"00edcdf0-2cb4-4256-a940-5516b0f65c27","_cell_guid":"5a911a4d-7ecd-48b4-a63c-02074d991315","trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bce4151a-4242-4082-87d2-fbdbebad779d","_cell_guid":"3804f6ed-bb6e-4239-be6e-390560807195","trusted":true},"cell_type":"markdown","source":"### 4.3 PCA features<a class=\"anchor\" id=\"4.3\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"c6e15d92-bdf5-4651-982e-24dce4059ae0","_cell_guid":"e40a820e-e10b-4e54-bb94-181cc42aa9d0","trusted":true},"cell_type":"code","source":"len(GENES)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4535f0ec-bdc2-492c-98e6-3ae61c6c651f","_cell_guid":"11a75934-7aec-4f25-afab-759bc6fed36e","trusted":true},"cell_type":"code","source":"# GENES\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp_GENES, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp_GENES)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp_GENES)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a7d7a39-31fa-4b95-bd4f-4811ab962ad7","_cell_guid":"cd1ccac8-c38f-478f-800e-bae6999b9005","trusted":true},"cell_type":"code","source":"len(CELLS)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fec04fe0-661b-4c4c-8276-a6453552fb48","_cell_guid":"5b190b7c-24c4-4278-85b8-6038aabab062","trusted":true},"cell_type":"code","source":"# CELLS\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp_CELLS, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp_CELLS)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp_CELLS)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"141d4c7f-20fe-48ed-9a62-11c1df88a83a","_cell_guid":"cf595681-49b4-4c41-9b1e-ba6178126e42","trusted":true},"cell_type":"code","source":"train_features.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12a34f7f-2279-4f9f-bd5e-f1a21d28ec97","_cell_guid":"3bdd7c1c-71ee-4cb7-98ed-2a735cc09409","trusted":true},"cell_type":"code","source":"train_features.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f285fcf-96ac-40e9-bd86-428f5a9c6d48","_cell_guid":"e752af43-bea6-475d-a4ab-491cc12a82bf","trusted":true},"cell_type":"markdown","source":"### 4.4 Feature selection<a class=\"anchor\" id=\"4.4\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"65813e6e-744a-48de-bc5f-4e4fdaf4aa8d","_cell_guid":"1365dcd9-f503-4468-9cc3-51fdb895a44a","trusted":true},"cell_type":"code","source":"data = train_features.append(test_features)\ndata","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a819e1f-dfbf-4325-8bbd-5449062cd57c","_cell_guid":"7808e574-fa55-4647-9922-ec0041b2b2bb","trusted":true},"cell_type":"code","source":"var_thresh = VarianceThreshold(VarianceThreshold_for_FS)\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"091f2fbb-8a29-42be-a9f2-fb89d8b95497","_cell_guid":"a0f26960-f0c0-41c3-b592-0eb772bfacc0","trusted":true},"cell_type":"code","source":"train_features.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d67d613-43e5-42f1-b5da-1b9e7e79e4bc","_cell_guid":"30b8b9e2-3618-4008-98cd-000ea5ac56d6","trusted":true},"cell_type":"code","source":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_scored.columns]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf926e7a-b6e8-4c5a-8e02-9309636fa969","_cell_guid":"1c1ec769-1be1-44b2-a4c1-e3bb6c5d51fb","trusted":true},"cell_type":"code","source":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8619c67-56f1-461b-bce2-6b1e295b67af","_cell_guid":"7effa539-171a-408c-ac25-aefe4d676497","trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a02f371-18e7-4432-872c-d53999dd23c9","_cell_guid":"c3554eed-5ea2-4a47-9a2d-35ba217c561a","trusted":true},"cell_type":"code","source":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b3f513e-7bc6-4390-a2f1-8226062ef8c1","_cell_guid":"5726c116-bf1e-4d10-8d61-eee220f4d4ac","trusted":true},"cell_type":"markdown","source":"### 4.5 CV folds<a class=\"anchor\" id=\"4.5\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"b0ea616b-e3b6-4772-8225-e0c777cca3b0","_cell_guid":"add0e4cb-a75b-4ba4-af68-d1e772a079ef","trusted":true},"cell_type":"code","source":"folds = train.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=7)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)\nfolds","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d827b88f-f586-4c61-92be-f5d112a690c8","_cell_guid":"7f820873-dc23-4e38-bbf2-b0f6d4b79ae5","trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\nprint(sample_submission.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f33e56e-c90b-43e9-a767-57f10630943f","_cell_guid":"889e4eb4-7b05-4cf2-841a-5015cdb9d0dd","trusted":true},"cell_type":"markdown","source":"### 4.6 Dataset Classes<a class=\"anchor\" id=\"4.6\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"0957589b-71a5-4939-9982-426a37708e68","_cell_guid":"92db12f7-b9d3-4d09-bf2d-77ce2c1d7c16","trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8082d67-5209-42d9-812c-19bb39cdb3ba","_cell_guid":"d9359eef-6bb2-41bc-8a10-721f4a412000","trusted":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        final_loss += loss.item()\n    \n    final_loss /= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e8cf378-b12c-4ea3-9c91-f86bf6c5cac7","_cell_guid":"35f77c72-3220-492e-b4d5-19ba5759b49d","trusted":true},"cell_type":"markdown","source":"### 4.7 Smoothing<a class=\"anchor\" id=\"4.7\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"55fafd52-8434-482b-be17-8555c70ad4c0","_cell_guid":"8137e3e6-e0ec-4cb6-baab-affc13b56b91","trusted":true},"cell_type":"code","source":"class SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c25493c5-bc28-443b-afa1-6e42412c23ed","_cell_guid":"aea0d567-d706-44ef-9fe8-cdff980f1b9b","trusted":true},"cell_type":"markdown","source":"### 4.8 Preprocessing<a class=\"anchor\" id=\"4.8\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"2d6ed507-32e5-4454-80c7-80e79cc59467","_cell_guid":"6b9620af-f3ee-4414-aee5-4c188389b8a5","trusted":true},"cell_type":"code","source":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd1fe1a3-e153-4228-b5b0-5398111bd07d","_cell_guid":"82099647-5e4c-4c16-a711-70258fe369ba","trusted":true},"cell_type":"code","source":"feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"789ef00d-1409-45df-9d5c-812dd181d00f","_cell_guid":"54fecc33-eeb3-4ce8-9520-462dfcb1a066","trusted":true},"cell_type":"markdown","source":"## 5. Modeling<a class=\"anchor\" id=\"5\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"7864ae3e-3665-429c-907b-050cc594b44e","_cell_guid":"65851a68-b33c-47a8-b5f8-658383ce7f4f","trusted":true},"cell_type":"code","source":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7\nEARLY_STOPPING_STEPS = 8\nEARLY_STOP = True\n\nd_model = 32\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=1500","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a874f6e-72a9-4744-b581-93c6b2cfb3b6","_cell_guid":"c629b7d2-b0b0-4ae6-8796-c32f805c29e3","trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.conv1 = nn.Conv1d(in_channels=1, out_channels=d_model, kernel_size=1)\n        \n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=2, dropout=0.5)\n        # self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3, norm=nn.BatchNorm1d(hidden_size // d_model, eps=1e-5))\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3, norm=nn.LayerNorm(d_model, eps=1e-5))\n        self.conv2 = nn.Conv1d(in_channels=d_model, out_channels=1, kernel_size=1)\n        \n        self.batch_norm2 = nn.BatchNorm1d(num_features * 2)\n        self.dropout2 = nn.Dropout(Dropout_Model)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(num_features * 2, hidden_size))\n\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(Dropout_Model)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n\n    def forward(self, inputs):\n        x = self.batch_norm1(inputs)\n        x = self.conv1(x.unsqueeze(1))\n        x = x.permute(0, 2, 1)\n        x = self.transformer_encoder(x)\n        x = x.permute(0, 2, 1)\n        x = self.conv2(x).squeeze(1)\n        \n        x = torch.cat([x, inputs], 1)\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = self.dense2(x)\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        return x\n    \nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53674ce2-9f1e-4be7-9819-ec01863b1a00","_cell_guid":"1bef1af4-66db-4507-8798-5fcf283df70b","trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY, eps=0.00001)\n    # scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n    #                                           max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min', factor=0.5, patience=2, verbose=True, min_lr=0.00001, eps=0.00001)\n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n   \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    cnt = 0\n    res = False\n    print(f\"FOLD:{fold}\")\n    print(\"*\"*60)\n    init_epoch = EPOCHS\n    for epoch in range(init_epoch):\n\n        train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        scheduler.step(valid_loss)\n        print(f\"EPOCH: {epoch}, train_loss: {train_loss:.5f}, valid_loss: {valid_loss:.5f}\")\n        if train_loss < valid_loss:\n            cnt += 1\n            if valid_loss < best_loss:\n\n                best_loss = valid_loss\n                oof[val_idx] = valid_preds\n                torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n\n            elif(EARLY_STOP == True):\n\n                early_step += 1\n                if (early_step >= early_stopping_steps):\n                    break\n        if cnt > 3:\n            res = True\n        if res == False and epoch > init_epoch - 5:\n            init_epoch += 5\n            \n    \n    #--------------------- PREDICTION---------------------\n    print(\"*\"*6+f\"best_loss: {best_loss:.5f}\"+\"*\"*6)\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5804dba3-dbfa-4cbd-b67f-16dc48063411","_cell_guid":"c31d8597-d0f8-4e97-a834-1a4d96d49109","trusted":true},"cell_type":"markdown","source":"## 6. Prediction & Submission <a class=\"anchor\" id=\"6\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_uuid":"9dc33d39-1116-4ddd-a13b-d7338d8110c5","_cell_guid":"3af4e8b1-5b4d-4462-88c0-027ee6ff1c28","trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d4ef942-538e-4d08-8857-0df29a514d79","_cell_guid":"49cd0f0c-bce7-4611-9cd3-59d3dde7a7c2","trusted":true},"cell_type":"code","source":"# Averaging on multiple SEEDS\n\nSEED = [0, 1, 2, 3]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c72aa747-b52f-4023-97e9-8d4d141f7801","_cell_guid":"f183d960-8045-4ffc-b57a-e0bd1e89447a","trusted":true},"cell_type":"code","source":"train_targets_scored","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2703bc48-29d2-4231-95d8-94128ec036bf","_cell_guid":"24457ec7-ee23-4ed6-9f9f-9902e08c0481","trusted":true},"cell_type":"code","source":"len(target_cols)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d32029a2-3bb7-4402-a8c5-8aa253fe70e1","_cell_guid":"06f24ac1-c37d-408c-9720-11cb72822db8","trusted":true},"cell_type":"code","source":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ / target.shape[1]\n    \nprint(\"CV log_loss: \", score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e83f01e8-bd49-4069-9360-303909a99807","_cell_guid":"acae6a59-f6b4-4478-8d4d-8f9236f04322","trusted":true},"cell_type":"code","source":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ab3d62b-f340-4cee-8b7e-0f655203fead","_cell_guid":"36f9024e-f083-4cba-822f-3728dc17a5d0","trusted":true},"cell_type":"code","source":"sub.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af9d4c66-54de-4ee4-a46e-136491ea1ee9","_cell_guid":"bc006024-2a63-4bcb-83e4-050bc7b28ce3","trusted":true},"cell_type":"markdown","source":"[Go to Top](#0)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}