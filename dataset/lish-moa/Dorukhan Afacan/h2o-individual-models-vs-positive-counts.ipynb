{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport pickle\nfrom tqdm import tqdm\n\nimport h2o\nfrom h2o.automl import H2OAutoML\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I trained 206 individual models with H2O AutoML which includes StackedEnsemble in the model family as well. With a fairly well `max_runtime_secs` the AutoML optimizes over a fixed model and hyperparameter space with k-fold CV and picks the best model on its own **\"Leaderboard\"**. \n\nLater on I stored log loss of best individual models to see for which labels we are having predictions that increase the mean log loss of labels which is the competition metric. \n\nBelow you can make a comparison between positive value counts for each label and logloss of each label's model. Seems like some labels require a seperate effort because even their \"optimized\" model losses are very high compared to the competiton leaderboard."},{"metadata":{"trusted":true},"cell_type":"code","source":"label_df = pd.read_csv('../input/lish-moa/train_targets_scored.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = label_df.columns.difference(['sig_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_counts = {}\nfor i,col in enumerate(labels):\n    pos_count = label_df[col].value_counts()[1]\n    pos_counts[f\"{i}-{col}\"] = pos_count\n    if pos_count<10:\n        print(col,':',pos_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_df = pd.DataFrame({'label':list(pos_counts.keys()), \n              'pos_counts':list(pos_counts.values())})\npos_df_sorted = pos_df.sort_values('pos_counts', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig ,ax = plt.subplots(figsize=(20,30))\nsns.barplot(data=pos_df_sorted, x='pos_counts', y='label',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('../input/h2oleaderboards/LBs.pkl', 'rb') as f:\n    lbs = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\nfor i,val in tqdm(enumerate(lbs.values()), total=206):\n    if type(val) == str:\n        print(val,f\"{i}-{list(lbs.keys())[i]}\")\n        score = np.nan\n    else:\n        score = val['logloss'].values[0]\n    scores.append(score)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_df['logloss'] = scores ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_df_score_sorted = pos_df.sort_values('logloss', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig ,ax = plt.subplots(figsize=(20,30))\nsns.barplot(data=pos_df_score_sorted, x='logloss', y='label',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}