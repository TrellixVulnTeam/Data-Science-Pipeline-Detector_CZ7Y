{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n###  Every drug is a biological molecule(protein,lipid etc.) whenever it enters in any body it need to go through various processes and it is called pharmacokinetics.There is four major steps.\n \n### **1.Adsorption-** In this step the molecule get absorbed in the blood stream.If it is oral then it is absorbed in the abdominal region.If it is injected in blood it directly in the blood stream.\n\n### **2.Distribution-** Every disease have a special pathogen or target.In case of antibiotic it is the pathogen and the drug molecule get adhere to pathogen cells.\n\n### **3.Metabolism-** Now the molecule introduced in the metabolism cycles and starts its action.\n\n### **4.Excretion-** Now its also important for the molecule to get out of the body so in order to minimize side effect on body.\n\n### Through out this all processes the drug molecule need to interect with different molecules and there are various factors that defines the mechanism of action of the drug.\n\n### The motivation of this competition is to find the mechanism of action of the drug through its whole life inside the body with the help of different factors and that we are going to predict as targets.These targets are nothing but chemical properties of these molecules.The features includes the gene expression data and cell viability."},{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Activation,Dense,Dropout,BatchNormalization,Input\nfrom tensorflow.keras.metrics import Recall,Precision\nfrom sklearn.model_selection import train_test_split\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\ntf.random.set_seed(0)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data\n### Now lets talk about the csv files that are provided.The train_features.csv file contain 875 features that will be used in training the data.Next test_features.csv file has the test data that is going to be used in prediction.Third csv file contains optional data.The train_targets_scored.csv contains 206 target columns for the training feature data.\n### So we have two different csv first have features and second contains targets."},{"metadata":{"trusted":true},"cell_type":"code","source":"X =pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ntest=pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\npd.read_csv('/kaggle/input/lish-moa/train_targets_nonscored.csv')\ny =pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\nsample_submission=pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nX = X.iloc[: , 1: ].values\ny = y.iloc[:,1:].values\nle = LabelEncoder()\nX[:,2] = le.fit_transform(X[:,2])\nle = LabelEncoder()\nX[:,0] = le.fit_transform(X[:,0])\nprint(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{},"cell_type":"markdown","source":"### I use holdout validation we can also go with kfold."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.12)\nmodel = Sequential([\n    \n    Input(X_train.shape[1]),\n    layers.BatchNormalization(),\n    Dropout(0.3),\n    Dense(512, activation=tf.nn.leaky_relu),\n    layers.BatchNormalization(),\n    layers.Dropout(0.4),\n    Dense(256, activation=tf.nn.leaky_relu),\n    layers.BatchNormalization(),\n    layers.Dropout(0.4),\n    #Dense(206, activation=\"relu\"),\n    #layers.BatchNormalization(),\n    #layers.Dropout(0.5),\n    Dense(y_train.shape[1], activation =\"sigmoid\")\n    \n    \n])\nmodel.compile(\n    optimizer=Adam(learning_rate=0.0075),\n    loss='binary_crossentropy',\n    metrics=['Recall','Precision']\n)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def func(arg):\n    \n    arg = tf.convert_to_tensor(arg, dtype=tf.float32)\n    return arg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{},"cell_type":"markdown","source":"### Here I am using callback technique that is used to stop the training the model as per requirement.We can stop on the basis of loss ,val_loss,accuracy,Validation accuracy. "},{"metadata":{"trusted":true},"cell_type":"code","source":"class mycallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self,epoch,logs={}):\n        if (logs.get('loss')<=0.0152):\n            self.model.stop_training=True\ncallback= mycallback()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(func(X_train),func(y_train) , verbose=2, epochs=200, \n         validation_data=(func(X_val),func(y_val)),batch_size=32768,callbacks=[callback])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(25,10))\nplt.plot(history.history['recall'])  \nplt.plot(history.history['val_recall'])\nplt.title('model recall')  \nplt.ylabel('recall')  \nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.figure(figsize=(25,10))\nplt.plot(history.history['loss'])  \nplt.plot(history.history['val_loss'])\nplt.title('model loss')  \nplt.ylabel('loss')  \nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.figure(figsize=(25,10))\nplt.plot(history.history['precision'])  \nplt.plot(history.history['val_precision'])\nplt.title('model precison')  \nplt.ylabel('loss')  \nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_final = test.iloc[: , 1: ].values\nle = LabelEncoder()\nX_final[:,2] = le.fit_transform(X_final[:,2])\nle = LabelEncoder()\nX_final[:,0] = le.fit_transform(X_final[:,0])\nprint(X_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(func(X_final))\n\ncolumns = list(sample_submission.columns)\ncolumns.remove('sig_id')\n\nfor i in range(len(columns)):\n    sample_submission[columns[i]] = y_pred[:, i]\n\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Thanks for your time. Please upvote if you like this,I will really appreciate it.\n# Any critisism will be appreciated feel free to comment.Love to get improvement tips."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}