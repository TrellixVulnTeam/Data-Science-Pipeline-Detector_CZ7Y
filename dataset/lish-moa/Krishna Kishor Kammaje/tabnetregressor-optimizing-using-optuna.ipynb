{"cells":[{"metadata":{},"cell_type":"markdown","source":"Original code by @optimo  from https://www.kaggle.com/optimo/tabnetregressor-2-0-train-infer"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/tabnetdevelop/tabnet-develop (1)/tabnet-develop')\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom pytorch_tabnet.tab_model import TabNetRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"import optuna\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nimport pandas as pd \n\nimport os\nimport random\nimport sys\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data and minimal preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_folds(seed_count, fold_count):\n\n    folds = []\n\n    train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n    train_targets_scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\n    train_drug = pd.read_csv('/kaggle/input/lish-moa/train_drug.csv')\n\n\n    # Get rid of \"ctl_vehicle\" from training. \n    # You may comment below lines if you do not want to do it.\n    train_targets_scored = train_targets_scored.loc[train_features['cp_type'] == 'trt_cp', :]\n    train_features = train_features[train_features['cp_type'] == 'trt_cp']\n    \n    train_features_drug = train_features.merge(train_drug, on=\"sig_id\", how='left')\n    \n    # Add drug_id as one of the targets (for stratifying later)\n    targets = train_targets_scored.columns[1:]\n    train_targets_scored = train_targets_scored.merge(train_drug, on='sig_id', how='left') \n\n    # Within in training data, identify indices where drug ids \n    # which are present in more than 18 rows and less than 18 rows \n    vc = train_targets_scored.drug_id.value_counts()\n    vc1 = vc.loc[vc <= 18].index\n    vc2 = vc.loc[vc > 18].index\n\n    # tmp is a dataframe derived from scored targets, where targets are \n    # averaged by drugid (one row per drug id)\n    tmp = train_targets_scored.groupby('drug_id')[targets].mean().loc[vc1]\n    tmp = tmp.reset_index()    \n    tmp = tmp.rename(columns={\"index\":\"drug_id\"})\n    \n    # tmp1 is a dataframe with tagets and drug_id for all drugs that have \n    # repeated more that 18 times in train dataset.\n    # We are stratifying these drugs as among all folds. \n    # Thought here is that such drugs might repeat in public/private test sets as well\n    tmp1 = train_targets_scored[train_targets_scored['drug_id'].isin(vc2)]\n    tmp1 = tmp1.reset_index(drop=True)\n\n    for seed in range(seed_count):\n\n        skf = MultilabelStratifiedKFold(n_splits = fold_count, shuffle = True, random_state = seed)\n        tmp_copy = tmp.copy()\n        tmp1_copy = tmp1.copy()\n        train_indices = train_features_drug[['sig_id', 'drug_id']].copy()\n        \n        for fold,(idxT,idxV) in enumerate(skf.split(X=tmp_copy,y=tmp_copy[targets])):\n            tmp_copy.loc[idxV,\"kfold\"] = fold\n        train_indices = train_indices.merge(tmp_copy[['drug_id', 'kfold']], on='drug_id', how=\"left\")\n\n        for fold,(idxT,idxV) in enumerate(skf.split(X=tmp1_copy,y=tmp1_copy[targets])):\n            tmp1_copy.loc[idxV,\"kfold\"] = fold        \n        train_indices = train_indices.merge(tmp1_copy[['sig_id', 'kfold']], on='sig_id', how=\"left\")\n\n        train_indices['kfold'] = train_indices['kfold_x'].combine_first(train_indices['kfold_y'])        \n        train_indices.drop(['drug_id', 'kfold_x', 'kfold_y'], inplace=True, axis=1) \n        \n        # Add this to the output\n        folds.append(train_indices)       \n\n    return np.stack(folds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"data_path = \"../input/lish-moa/\"\ntrain = pd.read_csv(data_path+'train_features.csv')\n\ntrain_targets_scored = pd.read_csv(data_path+'train_targets_scored.csv')\n\ntest = pd.read_csv(data_path+'test_features.csv')\ntest.drop(columns=[\"sig_id\"], inplace=True)\n\nsubmission = pd.read_csv(data_path+'sample_submission.csv')\n\nremove_vehicle = True\n\nif remove_vehicle:\n    kept_index = train['cp_type']=='trt_cp'\n    train = train.loc[kept_index].reset_index(drop=True)\n    train_targets_scored = train_targets_scored.loc[kept_index].reset_index(drop=True)\n\ntrain[\"cp_type\"] = (train[\"cp_type\"]==\"trt_cp\") + 0\ntrain[\"cp_dose\"] = (train[\"cp_dose\"]==\"D1\") + 0\n\ntest[\"cp_type\"] = (test[\"cp_type\"]==\"trt_cp\") + 0\ntest[\"cp_dose\"] = (test[\"cp_dose\"]==\"D1\") + 0\n\nX_test = test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One seeds, 7 folds\nfolds = 7\nfolded = create_folds(1, folds)\nfolded_data = pd.DataFrame(data=folded[0], columns=[\"sig_id\", \"kfold\"])\ntrain = train.merge(folded_data, on=\"sig_id\", how=\"left\")\ntrain_targets_scored = train_targets_scored.merge(folded_data, on=\"sig_id\", how=\"left\")\ntrain.drop(columns=[\"sig_id\"], inplace=True)\ntrain_targets_scored.drop(columns=[\"sig_id\"], inplace=True)\nmean = np.mean(train, axis=0)\nstd = np.std(train, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define custom metric for valdidation"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\nfrom pytorch_tabnet.metrics import Metric\nfrom sklearn.metrics import roc_auc_score, log_loss\n\nclass LogitsLogLoss(Metric):\n    \"\"\"\n    LogLoss with sigmoid applied\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        \"\"\"\n        Compute LogLoss of predictions.\n\n        Parameters\n        ----------\n        y_true: np.ndarray\n            Target matrix or vector\n        y_score: np.ndarray\n            Score matrix or vector\n\n        Returns\n        -------\n            float\n            LogLoss of predictions vs targets.\n        \"\"\"\n        logits = 1 / (1 + np.exp(-y_pred))\n        aux = (1-y_true)*np.log(1-logits+1e-15) + y_true*np.log(logits+1e-15)\n        return np.mean(-aux)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optuna Objective function with hyperparameters"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Low number of epochs for Optuna\nMAX_EPOCH=15\n\ndef objective(trial):\n\n    mask_type = 'entmax'\n#     n_da = 60\n#     n_steps = 1\n#     gamma = 1.0\n    lambda_sparse = 2.6743669818463933e-05\n    n_shared = 1\n    \n    # all hyperparameters here\n#     mask_type = trial.suggest_categorical(\"mask_type\", [\"entmax\", \"sparsemax\"])\n    n_da = trial.suggest_int(\"n_da\", 44, 64, step=4)\n    n_steps = trial.suggest_int(\"n_steps\", 1, 3, step=1)\n    gamma = trial.suggest_float(\"gamma\", 1., 1.4, step=0.2)\n#     lambda_sparse = trial.suggest_float(\"lambda_sparse\", 1e-6, 1e-3, log=True)\n#     n_shared = trial.suggest_int(\"n_shared\", 1, 3)    \n    batch_size = trial.suggest_int(\"batch_size\", 128, 1024, 128)\n    \n    tabnet_params = dict(n_d=n_da, n_a=n_da, n_steps=n_steps, gamma=gamma,\n                         lambda_sparse=lambda_sparse, optimizer_fn=torch.optim.Adam,\n                         optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n                         mask_type=mask_type, n_shared=n_shared,\n                         scheduler_params=dict(mode=\"min\",\n                                               patience=5,\n                                               min_lr=1e-5,\n                                               factor=0.5,),\n                         scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                         verbose=0,\n                         )\n\n    scores_auc_all= []\n    test_cv_preds = []\n\n    NB_SPLITS = 5\n    mskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, random_state=0, shuffle=True)\n    oof_preds = []\n    oof_targets = []\n    scores = []\n    for fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train, train_targets_scored)):\n        print(\"FOLDS : \", fold_nb)\n\n        X_train, y_train = train[train['kfold'] != fold_nb], train_targets_scored[train_targets_scored[\"kfold\"] != fold_nb]\n        X_val, y_val = train[train['kfold'] == fold_nb], train_targets_scored[train_targets_scored[\"kfold\"] == fold_nb]\n\n        X_train = X_train.reset_index(drop=True)\n        gauss = np.random.normal(0, std/10, (X_train.shape[0], 876))    \n        aug_train_X = X_train + gauss\n        X_train = X_train.append(aug_train_X).reset_index(drop=True)    \n        y_train = y_train.append(y_train)\n\n        X_train = X_train.drop(columns=[\"kfold\"]).values\n        y_train = y_train.drop(columns=[\"kfold\"]).values\n        X_val = X_val.drop(columns=[\"kfold\"]).values\n        y_val = y_val.drop(columns=[\"kfold\"]).values\n    \n        model = TabNetRegressor(**tabnet_params)\n\n        model.fit(X_train=X_train,\n                  y_train=y_train,\n                  eval_set=[(X_val, y_val)],\n                  eval_name = [\"val\"],\n                  eval_metric = [\"logits_ll\"],\n                  max_epochs=MAX_EPOCH,\n                  patience=10, batch_size=1024, virtual_batch_size=batch_size,\n                  num_workers=1, drop_last=False,\n                  # use binary cross entropy as this is not a regression problem\n                  loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n\n        preds_val = model.predict(X_val)\n        # Apply sigmoid to the predictions\n        preds =  1 / (1 + np.exp(-preds_val))\n        score = np.min(model.history[\"val_logits_ll\"])\n\n        ## save oof to compute the CV later\n        oof_preds.append(preds_val)\n        oof_targets.append(y_val)\n        scores.append(score)\n\n        # preds on test\n        preds_test = model.predict(X_test)\n        test_cv_preds.append(1 / (1 + np.exp(-preds_test)))\n\n    oof_preds_all = np.concatenate(oof_preds)\n    oof_targets_all = np.concatenate(oof_targets)\n    test_preds_all = np.stack(test_cv_preds)\n    \n    return np.mean(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### You can run around 95 trials in background mode using GPU, without timing out"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"pruner = optuna.pruners.MedianPruner() \n\nstudy = optuna.create_study(direction=\"minimize\", pruner=pruner)\nstudy.optimize(objective, n_jobs=-1, n_trials=50, gc_after_trial=True, timeout=None)\n\nprint(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize optimization trials"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_contour(study, params=['mask_type',\n                            'n_da',\n                            'n_steps',\n                            'gamma',\n                            'lambda_sparse',\n                            'n_shared'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_slice(study)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}