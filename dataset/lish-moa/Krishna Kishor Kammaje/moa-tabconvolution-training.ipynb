{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Tabular Convolution\nIn Tabular convolution, an innovative approach is used to create an image from a tabular sample. Here is the summaised approach.\n\n1. Choose a sample image (you can consider this as a seed and experiment with various images)\n2. Arrange the input row/sample as a kernel. \n3. Do a Conv2D on the sample image using this kernel. (In this notebook, I do this operation within the PyTorch model itself)\n4. Use the resuting image as a sample and use it in your vision model.\n\nIt has started showing promising results but not yet close to top results. I do not have enough time to do vairous experiments. "},{"metadata":{},"cell_type":"markdown","source":"### Original paper"},{"metadata":{},"cell_type":"markdown","source":"https://www.biorxiv.org/content/10.1101/2020.05.02.074203v1.full"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from pathlib import Path\nimport subprocess\n\nPL_PATH = Path(\"/kaggle/input/pytorch-lightning\")\nsubprocess.call(\n    [\"pip\", \"install\", PL_PATH / \"pytorch_lightning-1.0.2-py3-none-any.whl\"]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nsys.path.append('../input/efficientnetpytorch') ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":false},"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport random\nimport os\nfrom PIL import Image\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom efficientnet_pytorch.utils import MemoryEfficientSwish\nfrom efficientnet_pytorch import EfficientNet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.models as models\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ntrain_drug = pd.read_csv('/kaggle/input/lish-moa/train_drug.csv')\ntest = pd.read_csv('../input/lish-moa/test_features.csv')\ntest.drop(columns=[\"sig_id\"], inplace=True)\nsubmission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"remove_vehicle = True\n\nif remove_vehicle:\n    kept_index = train['cp_type']=='trt_cp'\n    train = train.loc[kept_index].reset_index(drop=True)\n    train_targets_scored = train_targets_scored.loc[kept_index].reset_index(drop=True)\n\ntrain[\"cp_type\"] = (train[\"cp_type\"]==\"trt_cp\") + 0\ntrain[\"cp_dose\"] = (train[\"cp_dose\"]==\"D1\") + 0\n\ntest[\"cp_type\"] = (test[\"cp_type\"]==\"trt_cp\") + 0\ntest[\"cp_dose\"] = (test[\"cp_dose\"]==\"D1\") + 0\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K-Fold stratificaiton including drug_id"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def create_folds(seed_count, fold_count):\n\n    folds = []\n\n    train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n    train_targets_scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\n    train_drug = pd.read_csv('/kaggle/input/lish-moa/train_drug.csv')\n\n    # Get rid of \"ctl_vehicle\" from training. \n    # You may comment below lines if you do not want to do it.\n    train_targets_scored = train_targets_scored.loc[train_features['cp_type'] == 'trt_cp', :]\n    train_features = train_features[train_features['cp_type'] == 'trt_cp']\n    \n    train_features_drug = train_features.merge(train_drug, on=\"sig_id\", how='left')\n    \n    # Add drug_id as one of the targets (for stratifying later)\n    targets = train_targets_scored.columns[1:]\n    train_targets_scored = train_targets_scored.merge(train_drug, on='sig_id', how='left') \n\n    # Within in training data, identify indices where drug ids \n    # which are present in more than 18 rows and less than 18 rows \n    vc = train_targets_scored.drug_id.value_counts()\n    vc1 = vc.loc[vc <= 18].index\n    vc2 = vc.loc[vc > 18].index\n\n    # tmp is a dataframe derived from scored targets, where targets are \n    # averaged by drugid (one row per drug id)\n    tmp = train_targets_scored.groupby('drug_id')[targets].mean().loc[vc1]\n    tmp = tmp.reset_index()    \n    tmp = tmp.rename(columns={\"index\":\"drug_id\"})\n    \n    # tmp1 is a dataframe with tagets and drug_id for all drugs that have \n    # repeated more that 18 times in train dataset.\n    # We are stratifying these drugs as among all folds. \n    # Thought here is that such drugs might repeat in public/private test sets as well\n    tmp1 = train_targets_scored[train_targets_scored['drug_id'].isin(vc2)]\n    tmp1 = tmp1.reset_index(drop=True)\n\n    for seed in range(seed_count):\n\n        skf = MultilabelStratifiedKFold(n_splits = fold_count, shuffle = True, random_state = seed)\n        tmp_copy = tmp.copy()\n        tmp1_copy = tmp1.copy()\n        train_indices = train_features_drug[['sig_id', 'drug_id']].copy()\n        \n        for fold,(idxT,idxV) in enumerate(skf.split(X=tmp_copy,y=tmp_copy[targets])):\n            tmp_copy.loc[idxV,\"kfold\"] = fold\n        train_indices = train_indices.merge(tmp_copy[['drug_id', 'kfold']], on='drug_id', how=\"left\")\n\n        for fold,(idxT,idxV) in enumerate(skf.split(X=tmp1_copy,y=tmp1_copy[targets])):\n            tmp1_copy.loc[idxV,\"kfold\"] = fold        \n        train_indices = train_indices.merge(tmp1_copy[['sig_id', 'kfold']], on='sig_id', how=\"left\")\n\n        train_indices['kfold'] = train_indices['kfold_x'].combine_first(train_indices['kfold_y'])        \n        train_indices.drop(['drug_id', 'kfold_x', 'kfold_y'], inplace=True, axis=1) \n        \n        # Add this to the output\n        folds.append(train_indices)       \n\n    return np.stack(folds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# 1 fold, 5 seed\nfolded = create_folds(1, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"folded_data = pd.DataFrame(data=folded[0], columns=[\"sig_id\", \"kfold\"])\ntrain = train.merge(folded_data, on=\"sig_id\", how=\"left\")\ntrain_targets_scored = train_targets_scored.merge(folded_data, on=\"sig_id\", how=\"left\")\ntrain.drop(columns=[\"sig_id\"], inplace=True)\ntrain_targets_scored.drop(columns=[\"sig_id\"], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalise features"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn import preprocessing\n\nx = train.values #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ndf = pd.DataFrame(x_scaled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert a sample (row of data) into a kernel"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def getKernel(data):\n    \n    # Get the kernel ready for convolution\n    nb_channels = 1\n    demeaned_data = data - np.mean(data)\n    kernel = np.concatenate((demeaned_data, np.zeros(86))).reshape(31,31)\n    kernel = torch.from_numpy(kernel)\n    kernel = kernel.view(1,1,31,31).repeat(1, nb_channels, 1, 1)\n    \n    # Convolution to get a new image\n    return kernel.squeeze(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dataset"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class MoAImageDataset(Dataset):\n    \n    def __init__(self, features, targets=None, transforms=None):\n        self.features = features\n        self.targets = targets\n        self.transforms = transforms\n        \n    def __len__(self):\n        return self.features.shape[0]\n        \n    def __getitem__(self, index):\n        single_row = self.features[index]\n        imageKernel = getKernel(single_row)\n          \n        return {\n            \"x\": imageKernel,\n            \"y\": torch.tensor(self.targets[index, :], dtype=torch.float)\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# mean = (0.485, 0.456, 0.406)\n# std = (0.229, 0.224, 0.225)\n\nmean = (0.485)\nstd = (0.229)\n\ndef get_train_transforms():\n    return A.Compose([\n#             A.HorizontalFlip(p=0.5),\n#             A.VerticalFlip(p=0.5),\n#             A.GaussianBlur(p=0.3),\n            A.Normalize(mean, std, max_pixel_value=1, always_apply=True),\n            ToTensorV2(),\n        ], p=1.0)\n\ndef get_valid_transforms():\n    return A.Compose([\n#             A.Resize(224, 224),\n            A.Normalize(mean, std, max_pixel_value=1, always_apply=True),\n            ToTensorV2(),\n        ], p=1.0)\n\ndef get_tta_transforms():\n    return A.Compose([\n#             A.Resize(224, 224),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.Normalize(mean, std, max_pixel_value=1, always_apply=True),\n            ToTensorV2(),\n        ], p=1.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data module"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class MoADataModule(pl.LightningDataModule):\n    def __init__(self, batch_size=64, fold=0):\n        super().__init__()\n        self.batch_size = batch_size\n        self.fold = fold\n              \n    def setup(self, stage=None):\n        # In multi-GPU training, this method is run on each GPU. \n        # So ideal for each training/valid split\n        \n        X_train, y_train = train[train['kfold'] != fold], train_targets_scored[train_targets_scored[\"kfold\"] != fold]\n        X_val, y_val = train[train['kfold'] == fold], train_targets_scored[train_targets_scored[\"kfold\"] == fold]\n\n        self.train_dataset = MoAImageDataset(X_train.iloc[:,:-1].values, y_train.iloc[:,:-1].values, transforms=get_train_transforms())\n        self.valid_dataset = MoAImageDataset(X_val.iloc[:,:-1].values, y_val.iloc[:,:-1].values, transforms=get_valid_transforms())        \n\n    \n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, self.batch_size, num_workers=4, shuffle=True, pin_memory=True)\n    \n    def val_dataloader(self):\n        return DataLoader(self.valid_dataset, self.batch_size, num_workers=4, shuffle=False, pin_memory=True) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CNN Model (transfer learning)"},{"metadata":{},"cell_type":"markdown","source":"This is the place we are creating the images required for learning. (i.e. converting tabular data to images)"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super().__init__()\n        \n        # Get the base image ready for convolution\n        base_image = plt.imread('../input/global-wheat-detection-512x512/train/026b6f389.jpg')\n        \n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n        normalised_image = self.normalize(torch.from_numpy(base_image/255).permute(2, 0, 1))\n        self.batched_image = normalised_image.unsqueeze(0).to('cuda' if torch.cuda.is_available() else 'cpu')    \n        self.grey_normalize = transforms.Normalize([0.485], [0.225])\n                \n        self.model = EfficientNet.from_pretrained('efficientnet-b0', in_channels=1, num_classes=206)  \n        # Freeze BN layers\n        for name, parameters in self.model.named_parameters():\n            if '_bn' in name:\n                parameters.requires_grad=False\n        \n\n    def forward(self, x):\n        # Get the image ready\n        target_image = F.conv2d(self.batched_image, x.repeat(1,3,1,1))\n        target_image = target_image.permute(1,0,2,3).float()  \n        \n        # Resize to 224\n        target_image = F.interpolate(target_image, 224)\n        target_image = self.grey_normalize(target_image.squeeze(1))\n        target_image = target_image.unsqueeze(1)\n        \n        #Actual forward\n        x = self.model(target_image)\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pytorch Lightning Model"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class PLitMoAModule(pl.LightningModule):\n    def __init__(self, hparams, model):\n        super(PLitMoAModule, self).__init__()\n        self.hparams = hparams\n        self.model = model\n        self.criterion = nn.BCEWithLogitsLoss()\n        \n    def forward(self, x):\n        return self.model(x)\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.hparams[\"lr\"])\n        scheduler = {\"scheduler\": \n                     torch.optim.lr_scheduler.ReduceLROnPlateau(\n                        optimizer, patience=2, \n                        threshold=0.0003, \n                        factor = 0.5,\n                        mode='min', verbose=True),\n                    \"interval\": \"epoch\",\n                    \"monitor\": \"val_loss\"}\n        return [optimizer], [scheduler]\n    \n    def training_step(self, batch, batch_index):\n        features = batch['x']\n        targets = batch['y']\n        out = self(features)\n        loss = self.criterion(out, targets)\n        logs = {\"train_loss\" : loss}\n        return {\"loss\": loss, \"log\": logs, \"progress_bar\": logs}\n    \n    def training_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n        logs = {\"train_loss\": avg_loss}\n        return {\"log\": logs, \"progress_bar\": logs}\n            \n    def validation_step(self, batch, batch_index):\n        features = batch['x']\n        targets = batch['y']\n        out = self(features)\n        loss = self.criterion(out, targets)\n        logs = {\"val_loss\" : loss}\n        return {\"loss\": loss, \"log\": logs, \"progress_bar\": logs}\n    \n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n        logs = {\"val_loss\": avg_loss}\n        return {\"log\": logs, \"progress_bar\": logs}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fold training"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"LR = 0.001\nfor fold in range(5):  \n    \n    checkpoint_callback = ModelCheckpoint(\n        filepath='./models/model_{epoch:02d}', \n        monitor='val_loss', verbose=False, \n        save_last=False, save_top_k=1, save_weights_only=False, \n        mode='min', period=1, prefix='')\n    \n    early_stop_callback = EarlyStopping(\n       monitor='val_loss',\n       min_delta=0.0001,\n       patience=5,\n       verbose=True,\n       mode='min'\n    )\n    \n    trainer = pl.Trainer(gpus=-1 if torch.cuda.is_available() else None, max_epochs=15, checkpoint_callback=checkpoint_callback, callbacks=[early_stop_callback])\n    dm = MoADataModule(fold=fold, batch_size=128)\n    \n    net = Model(875, 206) # Input Features, Output Targets\n    pylitModel = PLitMoAModule(hparams={\"lr\":LR}, model=net)\n    trainer.fit(pylitModel, dm)\n    \n    print(checkpoint_callback.best_model_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction"},{"metadata":{},"cell_type":"markdown","source":"### Submission"},{"metadata":{},"cell_type":"markdown","source":"#### Yet to try"},{"metadata":{},"cell_type":"markdown","source":"- DO not freeze BatchNorm\n- Try different base images\n- Check if the base image is within 0 to 1\n- Try other optimizers like AdamW\n- Try creating base images in a single go\n- Try lower/higher batch size\n- LR scheuling seems to have some good effect. Instead of default 0.1m try 0.5, 0.3 etc"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}