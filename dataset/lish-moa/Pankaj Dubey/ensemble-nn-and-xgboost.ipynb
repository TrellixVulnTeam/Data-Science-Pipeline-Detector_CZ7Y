{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder, QuantileTransformer\nfrom sklearn.model_selection import KFold\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.multioutput import MultiOutputClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import callbacks\nimport keras\nfrom sklearn.metrics import log_loss\nfrom keras.regularizers import L1, L2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drug = pd.read_csv('../input/lish-moa/train_features.csv')\ntarget = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n\n# setting the sig_id column as index\ndrug.set_index('sig_id', inplace= True)\ntarget.set_index('sig_id', inplace= True)\n\ntreat_drug = drug.query('cp_type == \"trt_cp\"')\ntreat_target = target.loc[treat_drug.index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting list of columns names for categorical features, numerical features, gene epxression related features and cell vialbility related features\ncat_cols = drug.select_dtypes(include = 'O').columns.tolist()\nnum_cols = drug.select_dtypes(exclude = 'O').columns.tolist()\ngene_features = [i for i in num_cols if i.startswith('g-')]\ncell_viability = [i for i in num_cols if i.startswith('c-')]\ncat_cols2 = cat_cols + ['cp_time']\nnum_cols2 = num_cols\nnum_cols.remove('cp_time')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data prepocesing i.e label encoding 'cp_dose', 'cp_time' and 'cp_type', or whether to drop vehicle/control treated sample rows\n\nqt = QuantileTransformer()\n\ndef data_preprocessing(dataframe, only_treatment = True, fit = False, transform = False):\n    df = dataframe.copy()\n    if fit:\n        df[num_cols] = qt.fit_transform(df[num_cols])\n    if transform:\n        df[num_cols] = qt.transform(df[num_cols])\n    df[\"cp_dose\"] = df.cp_dose.map({\"D1\": 0, \"D2\":1})\n    df[\"cp_time\"] = df.cp_time.map({24: 0,48: 1, 72: 2})\n    if only_treatment:\n        df = df.drop(\"cp_type\", 1)\n    else:\n        df[\"cp_type\"] = df.cp_type.map({\"trt_cp\": 1, \"ctl_vehicle\":0})\n    return df\n\n\ndrug_cleaned = data_preprocessing(dataframe= drug, only_treatment= False, fit= True, transform= False)\ndrug_treatment = data_preprocessing(dataframe= drug, only_treatment= True,fit= True, transform= False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining NN model to be optimized using Optuna hyperparameter optimization:\n\ndef for_bayes_optimization2(dimension):\n    \n    [dl1,dl2,dl3,dl4,dp1,dp2,dp3,dp4,regu,regu_val,activation,learning_rate] = dimension\n    if (regu == 'l2'):\n        act_reg = keras.regularizers.l2(regu_val)\n    if (regu =='l1'):\n        act_reg = keras.regularizers.l1(regu_val)\n    lr = callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 5, verbose = 0)\n    \n    #x_train,x_val, y_train, y_val = train_test_split(drug_cleaned, target, test_size = 0.3, random_state = 42)\n    es = callbacks.EarlyStopping(monitor = 'val_loss', min_delta = 1e-4, mode = 'min', baseline = 0.3 , \n                                 restore_best_weights=False, patience= 30, verbose = 0)\n    \n    adam = keras.optimizers.Adam(learning_rate = learning_rate)\n    \n    model = Sequential()\n    model.add(Dense(dl1, input_dim = x_train.shape[1], activation = activation, activity_regularizer = act_reg))\n    model.add(Dropout(dp1))\n    model.add(Dense(dl2, activation = activation))\n    model.add(Dropout(dp2))\n    model.add(Dense(dl3, activation = activation))\n    model.add(Dropout(dp3))\n    model.add(Dense(dl4, activation = activation))\n    model.add(Dropout(dp4))\n    model.add(Dense(y_train.shape[1], activation = 'sigmoid'))\n    \n    model.compile(optimizer = adam, loss = 'binary_crossentropy', metrics = ['AUC'])\n    \n    model.fit(x = x_train, y = y_train, validation_data = (x_val, y_val), epochs = 200, batch_size = 128, callbacks = [es], verbose = 0)\n    \n    log_loss_data = log_loss(np.ravel(y_val), np.ravel(model.predict_proba(x_val)), eps = 1e-7) \n    \n    return model # or return log_loss_data (for optuna optimization)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters obtained from Optuna \nbest_set_from_baysian_optimization = [2048, 1982, 708, 470, 0.6067766671093088, 0.1, 0.4973213653064633, 0.5950996340056243, 'l1', 1e-05, 'swish', 0.0001]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prepartion of sample submission file\nsubmission_test = pd.read_csv('../input/lish-moa/test_features.csv')\nsubmission_test_prob = pd.read_csv('../input/lish-moa/sample_submission.csv')\nsubmission_test_cleaned = data_preprocessing(dataframe= submission_test, only_treatment= False, fit= False, transform= True)\nsubmission_test_prob.set_index('sig_id', inplace= True)\nsubmission_test_cleaned.set_index('sig_id', inplace = True)\nsubmission_test_cleaned","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#setting initial prediction for all to zeros\nsubmission_test_prob[:] = np.zeros(submission_test_prob.shape)\nsubmission_test_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For submission_File_prediction\nn_splits = 5\nsub_file = submission_test_cleaned\nsub_file_all_predict = np.zeros(submission_test_prob.shape)\nnn_loss = [] # neural network loss\nxgb_loss = [] # xgb loss\ncombined_loss = [] # loss of ensembel of NN and XGB\nfor seed in [10, 20, 30]: # trying three dfiferent seeds\n\n    for e, (train, val) in enumerate(KFold(n_splits = n_splits, shuffle = True, random_state = seed).split(drug_cleaned, target)):\n        x_train, y_train = drug_cleaned.iloc[train], target.iloc[train]\n        x_val, y_val = drug_cleaned.iloc[val], target.iloc[val]\n\n        model = for_bayes_optimization2(best_set_from_baysian_optimization)\n        \n        nn_predict = model.predict_proba(x_val)\n        \n        sub_file_nn_predict = model.predict_proba(sub_file)\n        nn_loss_temp = log_loss(np.ravel(y_val), np.ravel(nn_predict), eps = 1e-7)\n        nn_loss.append(nn_loss_temp)\n        print(f\"NN_log_loss fold {e}, seed {seed}: \", nn_loss_temp)\n        \n        xgb = MultiOutputClassifier(XGBClassifier(tree_method = 'gpu_hist', n_estimators = 130, max_depth = 3, reg_alpha = 2, min_child_weight = 2,\n                                             gamma = 3, learning_rate = 0.0580666601841646, colsample_bytree = 0.58)) # Parameters obtained after optimization with Optuna\n        xgb.fit(x_train, y_train)\n        \n        xgb_predict = np.array(xgb.predict_proba(x_val))[:,:,1].T\n        xgb_loss_temp = log_loss(np.ravel(y_val), np.ravel(xgb_predict), eps = 1e-7)\n        xgb_loss.append(xgb_loss_temp)\n        \n        sub_file_xgb_predict = np.array(xgb.predict_proba(sub_file))[:,:,1].T\n        avg_sub_file_predict = (sub_file_nn_predict + sub_file_xgb_predict)/2\n        \n        sub_file_all_predict = sub_file_all_predict + avg_sub_file_predict\n        \n        combined_loss_temp  = log_loss(np.ravel(y_val), np.ravel((nn_predict + xgb_predict)/2), eps = 1e-7)\n        combined_loss.append(combined_loss_temp)\n        \n        print(f\"xgb_log_loss fold {e}, seed {seed}: \", xgb_loss_temp)\n        print(f\"combined_loss fold {e}, seed {seed}: \", combined_loss_temp)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Average log loss of NN is :\", np.mean(nn_loss), \" and standard deviation: \", np.std(nn_loss))\nprint(\"Average log loss of Xgboost is :\", np.mean(xgb_loss), \" and standard deviation: \", np.std(xgb_loss))\nprint(\"Combined log loss is :\", np.mean(combined_loss), \" and standard deviation: \", np.std(combined_loss))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_file_all_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_predictions = sub_file_all_predict/(n_splits * 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_test_prob[:] = final_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_test_prob.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_test_prob = np.clip(submission_test_prob, 0.0005, 0.99)\nsubmission_test_prob.iloc[submission_test.query('cp_type == \"ctl_vehicle\"').index] = 0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_test_prob.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_test_prob.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}