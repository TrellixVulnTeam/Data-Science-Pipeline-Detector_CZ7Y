{"cells":[{"metadata":{},"cell_type":"markdown","source":"Special thanks to https://www.kaggle.com/simakov/keras-multilabel-neural-network-v1-2"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport keras\nimport matplotlib.pyplot as plt\nfrom keras.layers import Dense, Activation, Dropout, Flatten, BatchNormalization, Input\nfrom keras.models import Sequential, save_model\nfrom keras.utils import np_utils\nimport tensorflow as tf\nfrom keras.callbacks import EarlyStopping\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTEENN\nfrom sklearn.decomposition import PCA\nfrom sklearn.utils import class_weight\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom sklearn.metrics import log_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"moa_train_feat = pd.read_csv('../input/lish-moa/train_features.csv')\nmoa_train_targ_NS = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\nmoa_train_targ_S = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\nmoa_test = pd.read_csv('../input/lish-moa/test_features.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"moa_train_feat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Types of Over and Undersampling Using imblearn\n\n1) **Random Under Sampling:** We can use the function RandomUnderSampler(sampling_strategy= 'not minority') or 'majority' to adjust the relationship between the minority class and majority class. We can use dict to input a dictionary with keys corresponding to classes and values corresponding to desired # of samples for each class. Finally, the default is to sample without replacement but that can be altered by utilizing replacement=True. Note: For binary classification, you can input a float with the desired minority / majority ratio. \n\n\n**Here we reach a 2 to 1 balance:** \ndefine undersample strategy\n\nundersample = RandomUnderSampler(sampling_strategy='not minority')\n\nsampling_strategy = {0: 10, 1: 15, 2: 20}\n\nrus = RandomUnderSampler(sampling_strategy=sampling_strategy)\n\nfit and apply the transform\n\nX_under, y_under = undersample.fit_resample(X, y)\n\n2) **Random Over Sampling:** \n\nros = RandomOverSampler(random_state=0)\n\nros.fit(X, y)\n\nX_resampled, y_resampled = ros.sample(X, y)\n\n3) **Over Sampling Using SMOTE:** This method takes two minority class points that are close to each other and creates a synthetic point repeatedly until the desired balance is acheived. \n\n**Here we reach a 2 to 1 balance:** \noversample = SMOTE(sampling_strategy='not minority') \nX_over, y_over = oversample.fit_resample(X, y)\n\n\n4) **Nearest Neighbor Under Sampling and SMOTE Over Sampling (SMOTEENN)**: This strategy combines the above approaches to acheive the desired ratio. \n\n**Here we reach a 2 to 1 balance:** \ndefine sampling strategy\nsample = SMOTEENN(sampling_strategy='not minority')\nfit and apply the transform\nX_over, y_over = sample.fit_resample(X, y)\n\n4)Bagging: Bootstrap resampling with replacement from under represented classes. \n\n**Challenges:** \n\n1) Must reduce dimensionality to make k nearest neighbor approach useable. It has trouble with high dimensional data. Use PCA. Note: Using PCA may change the angle rendering Cos similarity unuseable. \n\n2) We ultimately should take into consideration that some observations have multiple classes associated with them. Possible Solution: Expand to a single class representing All Variations.  \n\n3) SMOTE relies on a distance measure. Distance measures do not work well at high dimensions. Vectors are not similar. \n\n4) Repeating categories appearing once increases log loss. "},{"metadata":{"trusted":true},"cell_type":"code","source":"####Drop ID's\nmoa_train_targ_S=moa_train_targ_S.drop(moa_train_targ_S.columns[0],axis=1)\nmoa_train_targ_S.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####Drop ID's\nmoa_train_feat =moa_train_feat.drop(moa_train_feat.columns[0],axis=1)\nmoa_train_feat.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"moa_train_targ_S.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####Drop ID's but Save Test ID's\ntest_id=moa_test['sig_id']\nmoa_test=moa_test.drop(moa_test.columns[0],axis=1)\nmoa_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Remove Control Rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"moa_train_feat=moa_train_feat[moa_train_feat['cp_type'] != 'ctl_vehicle']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indexs_list2=moa_train_feat.index.values.tolist() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"moa_train_targ_S=moa_train_targ_S.iloc[indexs_list2]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df = moa_train_targ_S.sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_no_label=(df==0)\n#df_no_label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####Make List of Indexs\n#moa_train_targ_S[df_no_label]\n#indexs_list2=moa_train_targ_S[df_no_label].index.values.tolist() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#moa_train_targ_S = moa_train_targ_S.drop(indexs_list2) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#moa_train_feat = moa_train_feat.drop(indexs_list2) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#moa_train_targ_S","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#moa_train_feat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# One Hot Code and Remove Low Variance Features"},{"metadata":{},"cell_type":"markdown","source":"**One Hot Code Training Set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"####One Hot Code Train Columns: cp_type and cp_dose\ndummies=moa_train_feat[['cp_type','cp_dose']]\ncat_columns = ['cp_type','cp_dose']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies2=pd.get_dummies(dummies, prefix_sep=\"_\",\n                              columns=cat_columns)\ndummies2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"moa_train_feat['cp_type']=dummies2['cp_type_trt_cp']\nmoa_train_feat['cp_dose']=dummies2['cp_dose_D1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Remove Low Variance Features\nprint(moa_train_feat.shape)\nfrom sklearn import feature_selection as fs\n## Define the variance threhold and fit the threshold to the feature array. \nsel = fs.VarianceThreshold(threshold=.7)\nmoa_train_feat_vt = sel.fit_transform(moa_train_feat)\n\n## Print the support and shape for the transformed features\nprint(sel.get_support())\nprint(moa_train_feat.shape)\n\nmoa_train_feat=moa_train_feat[moa_train_feat.columns[sel.get_support(indices=True)]] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_moa_train_feat=moa_train_feat.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note: Since it doesn't make sense to consider the timing or dose size of a placebo, we are going to multiply the columns by the treatment status and get rid of treatment status all together.** "},{"metadata":{"trusted":true},"cell_type":"code","source":"#dummies2=dummies2.multiply(dummies2['cp_type_trt_cp'], axis=0)\n#dummies2=dummies2.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Remove Categorical Columns\n#moa_train_feat=moa_train_feat.drop(['cp_type','cp_dose'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Insert Dummies\n#dummies2=dummies2[['cp_dose_D1','cp_dose_D2']]\n#one_hot_moa_train_feat=dummies2.join(moa_train_feat)\n#one_hot_moa_train_feat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**One Hot Code Test Set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"####One Hot Code Columns: cp_type and cp_dose\ndummies3=moa_test[['cp_type','cp_dose']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies4=pd.get_dummies(dummies3, prefix_sep=\"_\",\n                              columns=cat_columns)\ndummies4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"moa_test['cp_type']=dummies4['cp_type_trt_cp']\nmoa_test['cp_dose']=dummies4['cp_dose_D1']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_control_group=moa_test['cp_type'] == 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dummies4=dummies4.multiply(dummies4['cp_type_trt_cp'], axis=0)\n#dummies4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#moa_test=moa_test.drop(['cp_type','cp_dose','cp_time'],axis=1)\n#moa_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_feats3=list(sel.get_support(indices=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sel.get_support(indices=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_moa_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Remove Same Variance Threshold Columns from Test Set\n#moa_test=moa_test[moa_test.columns[sel.get_support(indices=True)]] \n#moa_test\none_hot_moa_test=moa_test.iloc[:, top_feats3]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_moa_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dummies4=dummies4[['cp_dose_D1','cp_dose_D2']]\n#one_hot_moa_test=dummies4.join(moa_test)\n#one_hot_moa_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_x=one_hot_moa_train_feat.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_y=moa_train_targ_S.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA Each Group and Add to Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_col_g = [col for col in combined_x if col.startswith('g-')]\ngenes=combined_x[filter_col_g]\ngenes.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_col_c = [col for col in combined_x if col.startswith('c-')]\ncells=combined_x[filter_col_c]\ncells.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_col_c_test = [col for col in one_hot_moa_test if col.startswith('c-')]\ncells_test=one_hot_moa_test[filter_col_c_test]\ncells_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_col_g_test = [col for col in one_hot_moa_test if col.startswith('g-')]\ngenes_test=one_hot_moa_test[filter_col_g_test]\ngenes_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Add PCA Features###\npca_c = PCA(.9)\npca_g = PCA(.9)\n\n#fit PCA on Training Set\npca_c.fit(cells)\npca_g.fit(genes)\n\n### Apply PCA Mapping to Training and Test Set: Converts to a np.array\npca_cells_train = pca_c.transform(cells)\npca_genes_train = pca_g.transform(genes)\npca_cells_test = pca_c.transform(cells_test)\npca_genes_test = pca_g.transform(genes_test)\n\n#####Create Dataframe of PCA Features\nPCA_g_train=pd.DataFrame(pca_genes_train)\nPCA_c_train=pd.DataFrame(pca_cells_train)\nPCA_g_test=pd.DataFrame(pca_genes_test)\nPCA_c_test=pd.DataFrame(pca_cells_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PCA_g_train = PCA_g_train.reset_index()\ndel PCA_g_train['index']\n\nPCA_c_train = PCA_c_train.reset_index()\ndel PCA_c_train['index']\n\nPCA_g_test = PCA_g_test.reset_index()\ndel PCA_g_test['index']\n\nPCA_c_test = PCA_c_test.reset_index()\ndel PCA_c_test['index']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(PCA_g_train.shape)\nprint(PCA_c_train.shape)\nprint(PCA_g_test.shape)\nprint(PCA_c_test.shape)\nprint(one_hot_moa_test.shape)\nprint(combined_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PCA_train=pd.merge(PCA_g_train, PCA_c_train,right_index=True, left_index=True)\nPCA_test=pd.merge(PCA_g_test, PCA_c_test,right_index=True, left_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_moa_test = one_hot_moa_test.reset_index()\ndel one_hot_moa_test['index']\n\ncombined_x = combined_x.reset_index()\ndel combined_x['index']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_moa_test=one_hot_moa_test.join(PCA_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_x=combined_x.join(PCA_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_moa_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#one_hot_moa_train_feat=pd.merge(PCA_g_train, PCA_c_train,right_index=True, left_index=True)\n#one_hot_moa_train_feat=pd.merge(dummies_train,one_hot_moa_train_feat,right_index=True, left_index=True)\n#one_hot_moa_test=pd.merge(PCA_g_test, PCA_c_test,right_index=True, left_index=True)\n#one_hot_moa_test=pd.merge(dummies_test,one_hot_moa_test,right_index=True, left_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adjusted Neural Network Model with Weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"#top_feats2=list(np.array(top_feats))\n#top_feats3=[1]+top_feats2\n#top_feats3=top_feats3[:-1]\n#print(len(top_feats3))\n#top_feats3=top_feats\ntop_feats3=list(sel.get_support(indices=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#combined_x=combined_x.iloc[:, top_feats3]\n#combined_x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_x.iloc[:, 7:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=np.array(combined_x)\ninput_dim=X.shape[1]\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y=np.array(combined_y)\nnum_classes=Y.shape[1]\nY.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow_addons as tfa","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(num_columns):\n    model = Sequential()\n    model.add(Input(num_columns))\n    model.add( BatchNormalization() )\n    model.add( Dropout(0.5))\n    model.add(Dense(units=800, kernel_initializer='glorot_uniform', activation='swish'))\n    model.add( BatchNormalization() )\n    model.add( Dropout(0.5))\n    model.add(Dense(units=400,activation='swish'))\n    model.add( BatchNormalization() )\n    model.add( Dropout(0.5) )\n    model.add(Dense(units=num_classes,activation='sigmoid'))\n    opt = keras.optimizers.Adam(learning_rate=3e-3)\n    model.compile( optimizer=opt, loss='binary_crossentropy')\n    return model\n    \n\n#metrics=[tf.keras.metrics.AUC(name='auc')]\n#tf.keras.metrics.AUC(name='auc')\n#tf.keras.metrics.Recall(name='recall')\n#tf.keras.metrics.Precision(name='precision')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####Get Length of Test\nl=len(one_hot_moa_test)-1\nl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Empty Predictions Set\nss = combined_y.copy()\nss = ss.reset_index()\ndel ss['index']\nss=ss.loc[0:l,:]\nss.loc[:, combined_y.columns] = 0\nss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Empty Validation Set\nres = combined_y.copy()\nres = res.reset_index ()\nres.loc[:, combined_y.columns] = 0\ndel res['index']\nres","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_moa_test.values[:, top_feats3].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_STARTS = 4\nimport tensorflow as tf\ntf.random.set_seed(42)\n\n####This iterates through starts:\n\nfor seed in range(N_STARTS):\n#####This iteraties through folds n, validation indexes te, and train indexes tr:    \n    for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits=5, random_state=seed, shuffle=True).split(combined_y, combined_y)):\n        print(f'Fold {n}')\n    \n        model = create_model(input_dim)\n        #checkpoint_path = f'repeat:{seed}_Fold:{n}.hdf5'\n        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n        #cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n          #                           save_weights_only = True, mode = 'min')\n        \n####This fits the model to each fold and validation set. .values avoids creating a np array:\n\n        model.fit(combined_x.values[tr],\n                  combined_y.values[tr],\n                  validation_data=(combined_x.values[te], combined_y.values[te]),\n                  epochs=28, batch_size=128,\n                  callbacks=[reduce_lr_loss], verbose=2\n                 )\n        \n        #model.load_weights(checkpoint_path)\n####Makes predictions for each fold & seed:\n        test_predict = model.predict(one_hot_moa_test.values[:, :])\n        val_predict = model.predict(combined_x.values[te])\n####Sum Predictions for Each Epoch     \n        ss.loc[:, combined_y.columns] += test_predict\n        res.loc[te, combined_y.columns] += val_predict\n        print('')\n        \n####After all summed, Divide summed predictions by the number of starts times the number of folds:     \nss.loc[:, combined_y.columns] /= ((n+1) * N_STARTS)\nres.loc[:, combined_y.columns] /= N_STARTS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####Estimate Validation Loss of Averaged Results\ndef metric(y_true, y_pred):\n    metrics = []\n    for _target in combined_y.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n    return np.mean(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'OOF Metric: {metric(combined_y, res)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####Set Controls to 0\nss.loc[(test_control_group), combined_y.columns] = 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id=pd.DataFrame(test_id)\nss\ntest_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss=pd.merge(test_id, ss, how='inner', left_index=True, right_index=True)\nss=pd.DataFrame(ss)\nss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Check for nulls\npd.DataFrame(ss.isnull().sum(axis = 0)).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df =pd.DataFrame(ss.describe()).max(axis=1)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}