{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.config.list_physical_devices('GPU')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"test_features = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\ntrain_targets_scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ntrain_targets_non_scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_nonscored.csv')\ntrain_features = pd.read_csv(\"/kaggle/input/lish-moa/train_features.csv\")\nsubmission = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_features.drop('sig_id',axis=1)\ntest = test_features.drop('sig_id',axis=1)\ntarget = train_targets_scored.drop('sig_id',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"std_scaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.cp_type = train.cp_type.map({'trt_cp':1,'ctl_vehicle':0})\ntrain.cp_dose = train.cp_dose.map({'D1':0,'D2':1})\n\ntest.cp_type = test.cp_type.map({'trt_cp':1,'ctl_vehicle':0})\ntest.cp_dose = test.cp_dose.map({'D1':0,'D2':1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_matrix = train.corr().abs()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thresh = 0.9\nto_drop = [column for column in upper_tri.columns if any(upper_tri[column] > thresh)]\nprint(); print(to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_wo_mc = train.drop(to_drop,axis=1)\ntest_wo_mc = test.drop(to_drop,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"std_scaler.fit(train_wo_mc)\ntrain_wo_mc =std_scaler.transform(train_wo_mc)\ntest_wo_mc = std_scaler.transform(test_wo_mc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_log_loss(ytrue,pred):\n    ytrue = ytrue.astype(np.float64)\n    pred = pred.astype(np.float64) \n    score = 0\n    for i in range(ytrue.shape[1]):\n        _score = log_loss(ytrue[:,i], pred[:,i], labels=[0,1])\n        score += _score / ytrue.shape[1]\n        if str(_score) == 'nan':\n            print(ytrue[:,i], pred[:,i])\n            print(_score,score)\n            print(i)\n            break\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model(ip_shape,op_shape):\n    model  = tf.keras.Sequential([\n        layers.Dense(ip_shape*2,input_shape=(ip_shape,)),\n        layers.BatchNormalization(),\n        layers.Activation('selu'),\n        layers.Dropout(.4),\n        layers.Dense(ip_shape*3),\n        layers.BatchNormalization(),\n        layers.Activation('selu'),\n        layers.Dropout(.3),\n        layers.Dense(op_shape,activation='softmax')\n        ])\n    model.compile(optimizer='adam',loss='binary_crossentropy', metrics='accuracy')\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0001,\n  patience=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# kfold = KFold(n_splits=3)\n# Xdata = train_wo_mc.values\n# ydata = target.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def get_mean(data_list):\n#     return sum(data_list)/len(data_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# log_loss_score = []\n# for train_idx, test_idx in kfold.split(Xdata):\n#     xtrain,ytrain = Xdata[train_idx,:],ydata[train_idx,:]\n#     xtest,ytest = Xdata[test_idx,:], ydata[test_idx,:]\n    \n#     model = get_model(xtrain.shape[1],ytrain.shape[1])\n#     history =  model.fit(xtrain,ytrain,validation_data=(xtest,ytest),epochs=50,batch_size=128,verbose=0,callbacks=[early_stop])\n#     y_pred_test = model.predict(xtest)\n#     log_loss_score.append(get_log_loss(ytest,y_pred_test))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sum(log_loss_score)/len(log_loss_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_wo_mc.shape, train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Without K Fold\nval_accuracy = []\nlog_loss_score =[]\ny_pred_all = []\ny_pred_all_test =[]\n#for i in range(8,5,-1):\npca_n = [780,740,660]\nfor i in range(600,100,-20):\n#for i in pca_n:\n    print(\"I  \",i)\n    pca = PCA(n_components=i)\n    X = pca.fit_transform(train_wo_mc)\n    Xt = pca.transform(test_wo_mc)\n    xtrain, xtest,ytrain,ytest = train_test_split(X,target.values,test_size=.2)\n    print(xtrain.shape[1],ytrain.shape[1])\n    model = get_model(xtrain.shape[1],ytrain.shape[1])\n    history = model.fit(xtrain,ytrain,validation_data=(xtest,ytest),epochs=50,batch_size=128,verbose=0,callbacks=[early_stop])\n    val_accuracy.append(history.history['val_accuracy'])\n    y_pred = model.predict(xtest)\n    log_loss_current = get_log_loss(ytest,y_pred)\n    if log_loss_current < .0195:\n        y_pred_test = model.predict(Xt)\n        log_loss_score.append(log_loss_current)\n        y_pred_all.append(y_pred)\n        y_pred_all_test.append(y_pred_test)\n        print(\"Mean Log_Loss\",sum(log_loss_score)/len(log_loss_score))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(log_loss_score)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(len(y_pred_all)+5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_sum = np.sum(y_pred_all,axis=0)/(len(y_pred_all)+5)\nget_log_loss(ytest,y_pred_sum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_test_sum = np.sum(y_pred_all_test,axis=0)/float(len(y_pred_all))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = target.columns.tolist()\n#cols.remove('sig_id')\n\nsubmission.loc[:,cols] = y_pred_test_sum\n\nsubmission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"hi\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}