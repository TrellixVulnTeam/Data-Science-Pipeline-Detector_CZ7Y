{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" <font size=\"4\"> **In this kernel, I show how I tuned the hyperparameters for the tutorial kernel  <a href=\"https://www.kaggle.com/sinamhd9/mechanisms-of-action-moa-tutorial?scriptVersionId=46332122\">[Moa Tutorial]  </a> using Keras Tuner and its Bayesian optimization method.**","metadata":{}},{"cell_type":"code","source":"# Importing useful libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Adding iterative-stratification \n# Select add data from the right menu and search for iterative-stratification, then add it to your kernel.\nimport sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\n\nfrom time import time\nimport gc\n\nimport numpy as np\nimport pandas as pd \n\n# ML tools \nfrom tensorflow.keras.callbacks import EarlyStopping\nimport tensorflow as tf \nimport tensorflow.keras.backend as K\nimport kerastuner as kt\n\n# Setting random seeds\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\n\ndf_target_ns = pd.read_csv('/kaggle/input/lish-moa/train_targets_nonscored.csv')\n\ndf_target_s = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\n\ndf_test = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\n\ndf_sample = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess\n\nPlease see the references for the tutorial notebook.  <a href=\"https://www.kaggle.com/sinamhd9/mechanisms-of-action-moa-tutorial?scriptVersionId=46332122\">[Moa Tutorial]  </a>","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\ntransformer = QuantileTransformer(n_quantiles=100,random_state=42, output_distribution=\"normal\")\n\ndef preprocess(df):\n    df = df.drop('sig_id', axis=1)\n    df['cp_time'] = df['cp_time'].map({24:1, 48:2, 72:3})\n    df['cp_dose'] = df['cp_dose'].map({'D1':0, 'D2':1})\n    g_features = [cols for cols in df.columns if cols.startswith('g-')]\n    c_features = [cols for cols in df.columns if cols.startswith('c-')]\n    for col in (g_features + c_features):\n        vec_len = len(df[col].values)\n        raw_vec = df[col].values.reshape(vec_len, 1)\n        transformer.fit(raw_vec)\n        df[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    return df\n\nX = preprocess(df_train)\nX_test = preprocess(df_test)\n\nprint('Train data size', X.shape)\nprint('Test data size', X_test.shape)\ny = df_target_s.drop('sig_id', axis=1)\nprint('target size', y.shape)\ny0 =  df_target_ns.drop('sig_id', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g_features = [cols for cols in X.columns if cols.startswith('g-')]\nn_comp = 0.95\n\ndata = pd.concat([pd.DataFrame(X[g_features]), pd.DataFrame(X_test[g_features])])\ndata2 = (PCA(0.95, random_state=42).fit_transform(data[g_features]))\ntrain2 = data2[:X.shape[0]]\ntest2 = data2[-X_test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_g-{i}' for i in range(data2.shape[1])])\ntest2 = pd.DataFrame(test2, columns=[f'pca_g-{i}' for i in range(data2.shape[1])])\n\nX = pd.concat((X, train2), axis=1)\nX_test = pd.concat((X_test, test2), axis=1)\n\nc_features = [cols for cols in X.columns if cols.startswith('c-')]\nn_comp = 0.95\n\ndata = pd.concat([pd.DataFrame(X[c_features]), pd.DataFrame(X_test[c_features])])\ndata2 = (PCA(0.95, random_state=42).fit_transform(data[c_features]))\ntrain2 = data2[:X.shape[0]]\ntest2 = data2[-X_test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_c-{i}' for i in range(data2.shape[1])])\ntest2 = pd.DataFrame(test2, columns=[f'pca_c-{i}' for i in range(data2.shape[1])])\n\nX = pd.concat((X, train2), axis=1)\nX_test = pd.concat((X_test, test2), axis=1)\nfrom sklearn.feature_selection import VarianceThreshold\n\nvar_thresh = VarianceThreshold(0.8)  \ndata = X.append(X_test)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 2:])\n\ntrain_features_transformed = data_transformed[ : X.shape[0]]\ntest_features_transformed = data_transformed[-X_test.shape[0] : ]\n\n\nX = pd.DataFrame(X[['cp_type', 'cp_time','cp_dose']].values.reshape(-1, 3),\\\n                              columns=['cp_type','cp_time','cp_dose'])\n\nX = pd.concat([X, pd.DataFrame(train_features_transformed)], axis=1)\n\n\nX_test = pd.DataFrame(X_test[['cp_type', 'cp_time','cp_dose']].values.reshape(-1, 3),\\\n                             columns=['cp_type','cp_time','cp_dose'])\n\nX_test = pd.concat([X_test, pd.DataFrame(test_features_transformed)], axis=1)\n\nprint(X.shape)\nprint(X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\ndef fe_cluster(train, test, n_clusters_g = 35, n_clusters_c = 5, SEED = 239):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\nX ,X_test=fe_cluster(X,X_test)\ndisplay(X.head(2))\nprint(X.shape)\ndisplay(X_test.head(2))\nprint(X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fe_stats(train, test):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    for df in train, test:\n        df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n    return train, test\n\nX,X_test=fe_stats(X,X_test)\ndisplay(X.head(2))\nprint(X.shape)\ndisplay(X_test.head(2))\nprint(X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y0 = y0[X['cp_type'] == 'trt_cp'].reset_index(drop = True)\ny = y[X['cp_type'] == 'trt_cp'].reset_index(drop = True)\nX = X[X['cp_type'] == 'trt_cp'].reset_index(drop = True)\nX.drop(['cp_type'], axis=1, inplace=True)\nX_test.drop(['cp_type'], axis=1, inplace=True)\n\nprint(X.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_min = 0.001\np_max = 0.999\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -K.mean(y_true*K.log(y_pred) + (1-y_true)*K.log(1-y_pred))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"\ndef create_model(hp):\n    num_cols = X.shape[1]\n    inp = tf.keras.layers.Input(shape = (num_cols, ))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    num_dense = hp.Int('num_dense', min_value=0, max_value=3, step=1)\n    for i in range(num_dense):\n        hp_units = hp.Int('units_{i}'.format(i=i), min_value=128, max_value=4096, step=128)\n        hp_drop_rate = hp.Choice('dp_{i}'.format(i=i), values=[0.25,0.3,0.35,0.4,0.45,0.5,0.55, 0.6, 0.65,0.7])\n        hp_activation = hp.Choice('dense_activation_{i}'.format(i=i),values=['relu', 'selu', 'elu', 'swish'])\n        x = tf.keras.layers.Dense(units=hp_units, activation=hp_activation)(x)\n        x = tf.keras.layers.Dropout(hp_drop_rate)(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n    outputs = tf.keras.layers.Dense(206, activation='sigmoid')(x)\n    model = tf.keras.Model(inp, outputs)\n#     hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=5e-3)\n    learning_rate = 1e-3\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n                 loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001), metrics=logloss)\n    \n    return model ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Increase the number of max_trials for getting better resutls. For example, you can use 150 for number of trials and then blend the 5 top models (n_top).","metadata":{}},{"cell_type":"code","source":"feats = np.arange(0,X.shape[1],1)\ninp_size = int(np.ceil(1* len(feats)))\nn_split = 5\nbests=[]\nseeds = [0, 1]\nn_round = len(seeds)\nfor seed in seeds:\n    split_cols = np.random.choice(feats, inp_size, replace=False)\n    for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits = n_split, random_state = seed, shuffle = True).split(X, y)):\n        st = time()\n        tuner = kt.tuners.BayesianOptimization(create_model,\n                     kt.Objective(\"val_logloss\", direction=\"min\"),\n                     max_trials = 10, overwrite=True) \n        start_time = time()\n        x_tr = X.astype('float64').values[tr][:, split_cols]\n        x_val = X.astype('float64').values[te][:, split_cols]\n        y0_tr, y0_val = y0.astype(float).values[tr], y0.astype(float).values[te]\n        y_tr, y_val = y.astype(float).values[tr], y.astype(float).values[te]\n        x_tt = X_test.astype('float64').values[:, split_cols]\n        callbacks=[EarlyStopping(monitor='val_logloss', mode='min', patience=5)]\n        start_time = time()\n        tuner.search(x_tr, y_tr,validation_data=(x_val, y_val),\n                            epochs = 150, batch_size = 128,\n                            verbose = 0, callbacks = callbacks)        \n        n_top = 5\n        best_hps = tuner.get_best_hyperparameters(n_top)\n        end_time = time()\n        bests.append(best_hps)\n        for i in range(n_top):\n            print(best_hps[i].values)\n        print('Seed', seed, 'Fold', n, 'Time elapsed:', \"{:.2f}\".format((end_time-start_time)/60), 'minutes')\n        del tuner","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(bests)) # len(seeds) * n_split\nprint(bests[0][0].values) # Best for fold 0 seed 0 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (bests[0][1].values) # Second best for fold 0 seed 0 \nprint(bests[1][0].values) # Best for fold 1 seed 0 \n# and so on...\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n<a href=\"https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html\" > [1] Hyperparameter tuning with Keras Tuner\n</a> <br>\n<a href=\"https://www.kaggle.com/fchollet/moa-keras-kerastuner-best-practices\" > [2] MoA: Keras + KerasTuner best practices</a>","metadata":{}},{"cell_type":"markdown","source":" <font size=\"6\"> **Please upvote if you liked it :) Thanks!**","metadata":{}},{"cell_type":"markdown","source":"\n","metadata":{}}]}