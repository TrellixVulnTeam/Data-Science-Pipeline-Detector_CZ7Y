{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><center>Mechanisms of action (MoA)\n: Data analysis, visualization, and modeling</center></h1>\n<center><img src=\"https://thumbs.dreamstime.com/z/gene-therapy-tablets-genetic-code-inside-concept-advancement-medicine-treatment-diseases-57708501.jpg\" width=\"60%\"></center>\n","metadata":{}},{"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Quick navigation</center></h2>\n\n    \n    \n* [Problem Description](#1)\n* [Explanatory Data Analysis (EDA)](#2)\n    - [Example data](#21)\n    - [Missing values](#22)\n    - [Features](#23)\n        - [Gene expression features](#231)\n        - [Cell viability features](#232)\n        - [Cp_time and cp_dose](#233)\n    - [Exploring some realationships](#24)\n    - [Targets](#25)\n    - [Preprocessing and feature engineering](#26)\n* [Training](#3)\n    - [Model definition](#31)\n    - [Training and validation](#32)\n    - [Blending](#33)\n* [Evaluation and Summary](#4)\n* [References](#5)\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Problem Description</center></h2>\n\nIt is important to ask the right question before trying to solve it! Can we predict mechanism of action (MoA) of a drug based on gene expression and cell viability data? Or better to ask first, what is mechansim of action? The term mechanism of action means the biochemical interactions through which a drug generates its pharmacological effect. Scientists know many MoAs of drugs, for example, an antidepressant may have a selective serotonin reuptake inhibitor (SSRI), which affects the brain serotonin level. In this project we are going to train a model that classifies drugs based on their biological activity. The dataset consists of different features of gene expression data, and cell viability data as well as multiple targets of mechansim of action (MoA). This problem is a multilabel classification, which means we have multiple targets (not multiple classes). In this project, we will first perform explanatory data analysis and then train a model using deep neural networks with Keras. We will do a bit model evaluation at the end.","metadata":{}},{"cell_type":"code","source":"# Importing useful libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Adding iterative-stratification \n# Select add data from the right menu and search for iterative-stratification, then add it to your kernel.\nimport sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\n\nfrom time import time\nimport datetime\nimport gc\n\nimport numpy as np\nimport pandas as pd \n\n# ML tools \nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nimport tensorflow as tf \nfrom tensorflow.keras import layers\nimport tensorflow.keras.backend as K\nfrom sklearn.metrics import log_loss\nfrom tensorflow_addons.layers import WeightNormalization\n# Setting random seeds\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Visualization tools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('white')\nsns.set(font_scale=1.2)\n\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Explanatory Data Analysis (EDA)</center></h2>\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"21\"></a>\n## Example data\nFirst, we are going to see the train and test data size and some of their examples. Please note there are two different target dataframes, non-scored and scored. The non-scored ones are not used for scoring, but we can make use of them to pretrain our network\n<a href=\"https://www.kaggle.com/kailex/moa-transfer-recipe-with-smoothing\"> [1]</a>\n ","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ndisplay(df_train.head(3))\nprint('train data size', df_train.shape)\n\ndf_target_ns = pd.read_csv('/kaggle/input/lish-moa/train_targets_nonscored.csv')\ndisplay(df_target_ns.head(3))\nprint('train target nonscored size', df_target_ns.shape)\n\n\ndf_target_s = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ndisplay(df_target_s.head(3))\nprint('train target scored size', df_target_s.shape)\n\n\ndf_test = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\ndisplay(df_test.head(3))\nprint('test data size', df_test.shape)\n\ndf_sample = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\ndisplay(df_sample.head(3))\nprint('sample submission size', df_sample.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"22\"></a>\n## Missing values\nLet's see if there are any missing values, and see some information about our data types. ","metadata":{}},{"cell_type":"code","source":"print(df_train.isnull().sum().any()) # True if there are missing values\nprint(df_train.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no missing values; there are 872 float dtypes, 1 integer and 3 objects. Let's print the latter ones.","metadata":{}},{"cell_type":"code","source":"display(df_train.select_dtypes('int64').head(3))\ndisplay(df_train.select_dtypes('object').head(3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"23\"></a>\n## Features\nLet's visualize some features randomly:\n<a id=\"231\"></a>\n### Gene expression features","metadata":{}},{"cell_type":"code","source":"g_features = [cols for cols in df_train.columns if cols.startswith('g-')]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color = ['dimgray','navy','purple','orangered', 'red', 'green' ,'mediumorchid', 'khaki', 'salmon', 'blue','cornflowerblue','mediumseagreen']\n \ncolor_ind=0\nn_row = 6\nn_col = 3\nn_sub = 1 \nplt.rcParams[\"legend.loc\"] = 'upper right'\nfig = plt.figure(figsize=(8,14))\nplt.subplots_adjust(left=-0.3, right=1.3,bottom=-0.3,top=1.3)\nfor i in (np.arange(0,6,1)):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.kdeplot(df_train.loc[:,g_features[i]],color=color[color_ind],shade=True,\n                 label=['mean:'+str('{:.2f}'.format(df_train.loc[:,g_features[i]].mean()))\n                        +'  ''std: '+str('{:.2f}'.format(df_train.loc[:,g_features[i]].std()))])\n    \n    plt.xlabel(g_features[i])\n    plt.legend()                    \n    n_sub+=1\n    color_ind+=1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"232\"></a>\n### Cell viability features","metadata":{}},{"cell_type":"code","source":"c_features = [cols for cols in df_train.columns if cols.startswith('c-')]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_row = 6\nn_col = 3\nn_sub = 1 \nfig = plt.figure(figsize=(8,14))\nplt.subplots_adjust(left=-0.3, right=1.3,bottom=-0.3,top=1.3)\nplt.rcParams[\"legend.loc\"] = 'upper left'\nfor i in (np.arange(0,6,1)):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.kdeplot(df_train.loc[:,c_features[i]],color=color[color_ind],shade=True,\n                 label=['mean:'+str('{:.2f}'.format(df_train.loc[:,c_features[i]].mean()))\n                        +'  ''std: '+str('{:.2f}'.format(df_train.loc[:,c_features[i]].std()))])\n    \n    plt.xlabel(c_features[i])\n    plt.legend()                    \n    n_sub+=1\n    color_ind+=1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems data are somehow normalized and also clipped at -10, 10. Please see this great discussion here: <a href=\"https://www.kaggle.com/c/lish-moa/discussion/184005\"> [2] </a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"233\"></a>\n### Cp_time and cp_dose\n\ncp_time and cp_dose indicate treatment duration (24, 48, 72 hours) and dose (high or low which are D1 and D2).","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(10,4))\nplt.subplots_adjust(right=1.3)\nplt.subplot(1, 2, 1)\nsns.countplot(df_train['cp_time'],palette='nipy_spectral')\nplt.subplot(1, 2, 2)\nsns.countplot(df_train['cp_dose'],palette='nipy_spectral')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see there are almost the same number of examples in each treatment duration and dosage features.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"24\"></a>\n\n## Exploring some relationships","metadata":{}},{"cell_type":"markdown","source":"Next, we can use stripplot to show the relationship of a feature and a target with respect to dosage and time. Since, this is a multilabel probelm, we only show one label here, which is target 71. We will see later this target is contributing the most to the loss. For the feature, we chose two random g and c features. You may wanna do this with other features and labels to get more insight. ","metadata":{}},{"cell_type":"code","source":"train_copy= df_train.copy()\ntrain_copy['target_71'] = df_target_s.iloc[:,72] # sig_id is included\nfig = plt.figure(figsize=(16,8))\nplt.subplots_adjust(right=1.1,top=1.1)\nax1 = fig.add_subplot(121)\nsns.stripplot(data= train_copy , x='cp_time', y= 'g-3',color='red', hue='target_71',ax=ax1)\nax2 = fig.add_subplot(122)\nsns.stripplot(data= train_copy , x='cp_dose', y= 'g-3',color='red', hue='target_71',ax=ax2)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(16,8))\nplt.subplots_adjust(right=1.1,top=1.1)\nax1 = fig.add_subplot(121)\nsns.stripplot(data= train_copy, x='cp_time', y= 'c-1',color='yellow', hue='target_71',ax=ax1)\nax2 = fig.add_subplot(122)\nsns.stripplot(data= train_copy , x='cp_dose', y= 'c-1',color='yellow', hue='target_71',ax=ax2)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Or we can do the same process with the mean of g and c features. For example, here we plot the mean of g and c features with respect to a target, dosage and time. ","metadata":{}},{"cell_type":"code","source":"train_copy['g_mean'] = train_copy.loc[:, g_features].mean(axis=1) \nfig = plt.figure(figsize=(16,10))\nplt.subplots_adjust(right=1.1,top=1.1)\nax1 = fig.add_subplot(121)\nsns.stripplot(data= train_copy , x='cp_time', y= 'g_mean',color='red', hue='target_71',ax=ax1)\nax2 = fig.add_subplot(122)\nsns.stripplot(data= train_copy , x='cp_dose', y= 'g_mean', color='red', hue='target_71',ax=ax2)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_copy['c_mean'] = train_copy.loc[:, c_features].mean(axis=1) \nfig = plt.figure(figsize=(16,10))\nplt.subplots_adjust(right=1.1,top=1.1)\nax1 = fig.add_subplot(121)\nsns.stripplot(data= train_copy, x='cp_time', y= 'c_mean',color='yellow', hue='target_71',ax=ax1)\nax2 = fig.add_subplot(122)\nsns.stripplot(data= train_copy , x='cp_dose', y= 'c_mean', color='yellow', hue='target_71',ax=ax2)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can get some insights from the figures above and apply it in our [Preprocessing](#26) step ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"25\"></a>\n## Targets\nBelow are some scored targets which are used to train the main model. As we can see, the targets are very imbalanced and there are only a few positive examples in some labels. ","metadata":{}},{"cell_type":"code","source":"target_s_copy = df_target_s.copy()\ntarget_s_copy.drop('sig_id', axis=1, inplace=True)\nn_row = 20\nn_col = 4 \nn_sub = 1   \nfig = plt.figure(figsize=(20,50))\nplt.subplots_adjust(left=-0.3, right=1.3,bottom=-0.3,top=1.3)\nfor i in np.random.choice(np.arange(0,target_s_copy.shape[1],1),n_row):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.countplot(y=target_s_copy.iloc[:, i],palette='nipy_spectral',orient='h')\n    \n    plt.legend()                    \n    n_sub+=1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see the 20 largest positive number of labels in the scored targets. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\ntarget_s_copy.sum().sort_values()[-20:].plot(kind='barh',color='mediumseagreen')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And here are some non-scored targets. We can see that some labels do no have positive examples at all.","metadata":{}},{"cell_type":"code","source":"target_ns_copy = df_target_ns.copy()\ntarget_ns_copy.drop('sig_id', axis=1, inplace=True)\nn_row = 20\nn_col = 4 \nn_sub = 1   \nfig = plt.figure(figsize=(20,50))\nplt.subplots_adjust(left=-0.3, right=1.3,bottom=-0.3,top=1.3)\nfor i in np.random.choice(np.arange(0,target_ns_copy.shape[1],1),n_row):\n    plt.subplot(n_row, n_col, n_sub)\n    sns.countplot(y=target_ns_copy.iloc[:, i],palette='magma',orient='h')\n    \n    plt.legend()                    \n    n_sub+=1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And here is the 20 largest positive number of labels in the non-scored targets. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\ntarget_ns_copy.sum().sort_values()[-20:].plot(kind='barh',color='purple')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, there are fewer positive examples in non-scored dataset.","metadata":{}},{"cell_type":"markdown","source":"[](http://)<a id=\"26\"></a>\n## Preprocessing and feature engineering","metadata":{}},{"cell_type":"markdown","source":"The control group is defined as the group in an experiment or study that does not have the desired effect or MoAs here; which means the target labels are zero for them. I will drop the data for this group, and we will later set all predictions of this group to zero.\n\nWe will keep track of the control group (ctl_vehicle) indexes. \nI dropped cp_type column and mapped the values of time and dose features. I performed some feature engineering based on the insights I got from the [Exploring some relationships](#24) part. <br>\nUpdate : I added the methods of Rankgauss scaler and PCA from this great kernel : <a href='https://www.kaggle.com/kushal1506/moa-pytorch-0-01859-rankgauss-pca-nn?scriptVersionId=44558776' >[3] ","metadata":{}},{"cell_type":"code","source":"ind_tr = df_train[df_train['cp_type']=='ctl_vehicle'].index\nind_te = df_test[df_test['cp_type']=='ctl_vehicle'].index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\ntransformer = QuantileTransformer(n_quantiles=100,random_state=42, output_distribution=\"normal\")\n\ndef preprocess(df):\n    df['cp_time'] = df['cp_time'].map({24:1, 48:2, 72:3})\n    df['cp_dose'] = df['cp_dose'].map({'D1':0, 'D2':1})\n    g_features = [cols for cols in df.columns if cols.startswith('g-')]\n    c_features = [cols for cols in df.columns if cols.startswith('c-')]\n    for col in (g_features + c_features):\n        vec_len = len(df[col].values)\n        raw_vec = df[col].values.reshape(vec_len, 1)\n        transformer.fit(raw_vec)\n        df[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    return df\n\nX = preprocess(df_train)\nX_test = preprocess(df_test)\n\ndisplay(X.head(5))\nprint('Train data size', X.shape)\ndisplay(X_test.head(3))\nprint('Test data size', X_test.shape)\ny = df_target_s.drop('sig_id', axis=1)\ndisplay(y.head(3))\nprint('target size', y.shape)\ny0 =  df_target_ns.drop('sig_id', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Please see reference 3 for this part\ng_features = [cols for cols in X.columns if cols.startswith('g-')]\nn_comp = 0.95\n\ndata = pd.concat([pd.DataFrame(X[g_features]), pd.DataFrame(X_test[g_features])])\ndata2 = (PCA(0.95, random_state=42).fit_transform(data[g_features]))\ntrain2 = data2[:X.shape[0]]\ntest2 = data2[-X_test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_g-{i}' for i in range(data2.shape[1])])\ntest2 = pd.DataFrame(test2, columns=[f'pca_g-{i}' for i in range(data2.shape[1])])\n\nX = pd.concat((X, train2), axis=1)\nX_test = pd.concat((X_test, test2), axis=1)\n\nc_features = [cols for cols in X.columns if cols.startswith('c-')]\nn_comp = 0.95\n\ndata = pd.concat([pd.DataFrame(X[c_features]), pd.DataFrame(X_test[c_features])])\ndata2 = (PCA(0.95, random_state=42).fit_transform(data[c_features]))\ntrain2 = data2[:X.shape[0]]\ntest2 = data2[-X_test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_c-{i}' for i in range(data2.shape[1])])\ntest2 = pd.DataFrame(test2, columns=[f'pca_c-{i}' for i in range(data2.shape[1])])\n\nX = pd.concat((X, train2), axis=1)\nX_test = pd.concat((X_test, test2), axis=1)\n\ndisplay(X.head(2))\ndisplay(X_test.head(2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fe_stats(train, test):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    for df in train, test:\n        df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n    return train, test\n\nX,X_test=fe_stats(X,X_test)\ndisplay(X.head(2))\nprint(X.shape)\ndisplay(X_test.head(2))\nprint(X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\ndef fe_cluster(train, test, n_clusters_g = 35, n_clusters_c = 5, SEED = 239):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\nX ,X_test=fe_cluster(X,X_test)\ndisplay(X.head(2))\nprint(X.shape)\ndisplay(X_test.head(2))\nprint(X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n\nvar_thresh = VarianceThreshold(0.8)  \ndata = X.append(X_test)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : X.shape[0]]\ntest_features_transformed = data_transformed[-X_test.shape[0] : ]\n\n\nX = pd.DataFrame(X[['sig_id','cp_type', 'cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\nX = pd.concat([X, pd.DataFrame(train_features_transformed)], axis=1)\n\n\nX_test = pd.DataFrame(X_test[['sig_id','cp_type', 'cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\nX_test = pd.concat([X_test, pd.DataFrame(test_features_transformed)], axis=1)\n\ndisplay(X.head(2))\nprint(X.shape)\ndisplay(X_test.head(2))\nprint(X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y0 = y0[X['cp_type'] == 'trt_cp'].reset_index(drop = True)\ny = y[X['cp_type'] == 'trt_cp'].reset_index(drop = True)\nX = X[X['cp_type'] == 'trt_cp'].reset_index(drop = True)\nX.drop(['cp_type','sig_id'], axis=1, inplace=True)\nX_test.drop(['cp_type','sig_id'], axis=1, inplace=True)\n\nprint('New data shape', X.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center> Training</center></h2>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"31\"></a>\n## Model definition\nHere we define our neural network model which consists of several dense, dropout and batchnorm layers. I used different activations after my dense layers. We first train the network on non-scored targets and then transfer the weights to train another model on the scored targets. Smoothing the labels may prevent the network from becoming over-confident and has some sort of regularization effect <a href=\"https://www.kaggle.com/rahulsd91/moa-label-smoothing\">[4] </a>. It seems this method works well here. I used Keras Tuner to tune the hyperparameters. The details are in this notebook <a href=\"https://www.kaggle.com/sinamhd9/hyperparameter-tuning-with-keras-tuner\">[5] </a>","metadata":{}},{"cell_type":"code","source":"p_min = 0.001\np_max = 0.999\nfrom tensorflow.keras import regularizers\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -K.mean(y_true*K.log(y_pred) + (1-y_true)*K.log(1-y_pred))\n\ndef create_model(num_cols, hid_layers, activations, dropout_rate, lr, num_cols_y):\n    \n    inp1 = tf.keras.layers.Input(shape = (num_cols, ))\n    x1 = tf.keras.layers.BatchNormalization()(inp1)\n\n    for i, units in enumerate(hid_layers):\n        x1 = tf.keras.layers.Dense(units, activation=activations[i])(x1)\n        x1 = tf.keras.layers.Dropout(dropout_rate[i])(x1)\n        x1 = tf.keras.layers.BatchNormalization()(x1)\n    \n    x1 = tf.keras.layers.Dense(num_cols_y,activation='sigmoid')(x1)\n    model = tf.keras.models.Model(inputs= inp1, outputs= x1)\n    \n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=lr),\n                 loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001), metrics=logloss)\n    \n    return model \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hid_layers = [[[512, 768, 896],[384, 640, 1024],[768,768,896],[512, 384, 1024],\n              [512, 640, 640], [640, 896, 1024], [256,640,896],[512,512,768],\n              [512, 384, 896],[512,768,768]],\n             [[512, 768, 896],[384, 640, 1024],[768,768,896],[512, 384, 1024],\n              [512, 640, 640], [640, 896, 1024], [256,640,896],[512,512,768],\n              [512, 384, 896],[512,768,768]],\n              [[1152, 640, 3072],[896, 1024, 3584],[1920,1024,3712],[896, 1024, 3456],\n              [1408, 896, 3456], [1408, 768, 3456], [2176,640,3840],[1664,1280,2688],\n              [1792, 768, 2432],[1280,1664,4096]],\n            [[896, 1792, 3712],[2048, 1024, 1664],[1664,1408,1920],[896, 1408, 3072],\n              [1152, 1152, 3072], [2816, 3072, 3328], [2304,2432,2176],[3968,4096,2816],\n              [1920, 1536, 3072],[128,1920,1664]],\n         [[896, 1152, 1408],[1920, 768, 2176],[2048,2048,1792],[2304, 1664, 512],\n              [768, 384, 512], [640, 1664, 512], [1664,1920,2688],[2432,1664,1536],\n              [640, 896, 2432],[1536,2176,2176]]]\n\ndropout_rate = [[[0.65,0.35,0.35],[0.65,0.35,0.45],[0.7,0.4,0.4],[0.65,0.35,0.45],\n                [0.65,0.35,0.45],[0.7,0.3,0.45],[0.7,0.35,0.4],[0.7,0.4,0.4],\n               [0.7, 0.3, 0.4],[0.65, 0.3, 0.4]],\n               [[0.65,0.35,0.35],[0.65,0.35,0.45],[0.7,0.4,0.4],[0.65,0.35,0.45],\n                [0.65,0.35,0.45],[0.7,0.3,0.45],[0.7,0.35,0.4],[0.7,0.4,0.4],\n               [0.7, 0.3, 0.4],[0.65, 0.3, 0.4]],\n                [[0.7,0.55,0.7],[0.7,0.4,0.6],[0.7,0.5,0.55],[0.7,0.55,0.6],\n                [0.7,0.5,0.65],[0.7,0.5,0.65],[0.7,0.55,0.7],[0.7,0.5,0.65],\n               [0.7, 0.35, 0.6],[0.7, 0.5, 0.7]],\n                [[0.7,0.4,0.7],[0.7,0.7,0.7],[0.7,0.7,0.7],[0.7,0.25,0.7],\n                [0.7,0.4,0.6],[0.7,0.6,0.7],[0.7,0.5,0.7],[0.7,0.7,0.7],\n               [0.7, 0.45, 0.7],[0.7, 0.3, 0.6]],\n             [[0.7,0.7,0.45],[0.7,0.7,0.6],[0.7,0.7,0.4],[0.7,0.7,0.55],\n                [0.65,0.45,0.4],[0.7,0.35,0.5],[0.7,0.7,0.45],[0.7,0.6,0.6],\n               [0.7, 0.7, 0.25],[0.7, 0.7, 0.25]]]\n\nactivations = [[['elu', 'swish', 'selu'], ['selu','swish','selu'], ['selu','swish','selu'],['selu','swish','elu'],\n                ['selu','swish','elu'],['elu','swish','selu'],\n               ['selu','swish','elu'],['selu','elu','selu'],['selu','swish','selu'],\n               ['selu','swish','elu']],\n               [['elu', 'swish', 'selu'], ['selu','swish','selu'], ['selu','swish','selu'],['selu','swish','elu'],\n                ['selu','swish','elu'],['elu','swish','selu'],\n               ['selu','swish','elu'],['selu','elu','selu'],['selu','swish','selu'],\n               ['selu','swish','elu']],\n                [['selu', 'relu', 'swish'], ['selu','relu','swish'], ['selu','relu','swish'],['selu','relu','swish'],\n               ['selu','relu','swish'],['selu','relu','swish'],['selu','relu','swish'],['selu','relu','swish'],\n               ['selu','elu','swish'],['selu','relu','swish']],\n                [['selu', 'elu', 'swish'], ['elu','swish','relu'], ['elu','swish','selu'],['selu','elu','swish'],\n               ['selu','elu','swish'],['selu','elu','swish'],['selu','elu','swish'],['selu','selu','swish'],\n               ['selu','relu','swish'],['elu','elu','swish']],\n            [['selu', 'swish', 'selu'], ['selu','swish','selu'], ['elu','swish','selu'],['selu','swish','selu'],\n               ['selu','relu','relu'],['selu','relu','relu'],['selu','swish','elu'],['selu','swish','relu'],\n               ['elu','swish','selu'],['elu','swish','swish']]]\n\nlr = 5e-4\n\nfeats = np.arange(0,X.shape[1],1)\ninp_size = int(np.ceil(1* len(feats)))\nres = y.copy()\ndf_sample.loc[:, y.columns] = 0\nres.loc[:, y.columns] = 0\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining callbacks\n\ndef callbacks():\n    rlr = ReduceLROnPlateau(monitor = 'val_logloss', factor = 0.2, patience = 3, verbose = 0, \n                                min_delta = 1e-4, min_lr = 1e-6, mode = 'min')\n        \n    ckp = ModelCheckpoint(\"model.h5\", monitor = 'val_logloss', verbose = 0, \n                              save_best_only = True, mode = 'min')\n        \n    es = EarlyStopping(monitor = 'val_logloss', min_delta = 1e-5, patience = 10, mode = 'min', \n                           baseline = None, restore_best_weights = True, verbose = 0)\n    return rlr, ckp, es","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def log_loss_metric(y_true, y_pred):\n    metrics = []\n    for _target in y.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels = [0,1]))\n    return np.mean(metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"32\"></a>\n\n## Training and validation\nWe use Multilabel Stratified KFold with 5 splits which is added in the beginning to the notebook.<br>","metadata":{}},{"cell_type":"code","source":"test_preds = []\nres_preds = []\nnp.random.seed(seed=42)\nn_split = 5\nn_top = 10\nn_round = 1\n\nfor seed in range(n_round):\n\n    split_cols = np.random.choice(feats, inp_size, replace=False)\n    res.loc[:, y.columns] = 0\n    df_sample.loc[:, y.columns] = 0\n    for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits = n_split, random_state = seed, shuffle = True).split(X, y)):\n        \n        start_time = time()\n        x_tr = X.astype('float64').values[tr][:, split_cols]\n        x_val = X.astype('float64').values[te][:, split_cols]\n        y0_tr, y0_val = y0.astype(float).values[tr], y0.astype(float).values[te]\n        y_tr, y_val = y.astype(float).values[tr], y.astype(float).values[te]\n        x_tt = X_test.astype('float64').values[:, split_cols]\n        \n        for num in range(n_top):\n            model = create_model(inp_size, hid_layers[n][num], activations[n][num], dropout_rate[n][num], lr, y0.shape[1])\n            model.fit(x_tr, y0_tr,validation_data=(x_val, y0_val), epochs = 150, batch_size = 128,\n                      callbacks = callbacks(), verbose = 0)\n            model.load_weights(\"model.h5\")\n            model2 = create_model(inp_size, hid_layers[n][num], activations[n][num], dropout_rate[n][num], lr, y.shape[1])\n            for i in range(len(model2.layers)-1):\n                model2.layers[i].set_weights(model.layers[i].get_weights())\n\n            model2.fit(x_tr, y_tr,validation_data=(x_val, y_val),\n                            epochs = 150, batch_size = 128,\n                            callbacks = callbacks(), verbose = 0)\n                       \n            model2.load_weights('model.h5')\n        \n            df_sample.loc[:, y.columns] += model2.predict(x_tt, batch_size = 128)/(n_split*n_top)\n        \n            res.loc[te, y.columns] += model2.predict(x_val, batch_size = 128)/(n_top)\n        \n        oof = log_loss_metric(y.loc[te,y.columns], res.loc[te, y.columns])\n        print(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}], Seed {seed}, Fold {n}:', oof)\n\n        K.clear_session()\n        del model2\n        x = gc.collect()\n\n    df_sample.loc[ind_te, y.columns] = 0\n    \n    test_preds.append(df_sample.copy())\n    \n    res_preds.append(res.copy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"33\"></a>\n\n## Blending","metadata":{}},{"cell_type":"markdown","source":"We blend the results of all models using averaging. In previous versions we used optimization suggested by this notebook <a href='https://www.kaggle.com/gogo827jz/optimise-blending-weights-with-bonus-0'>[6] </a>. It is also good to read this notebook to understand the neural split method. <a href='https://www.kaggle.com/gogo827jz/split-neural-network-approach-tf-keras'>[7] </a> <br>Blending may result in score improvement taking into the effect of all the models with slight differences. ","metadata":{}},{"cell_type":"code","source":"\naa = [1.0]\nres2= res.copy()\nres2.loc[:, y.columns] = 0\nfor i in range(n_round):\n    res2.loc[:, y.columns] += aa[i] * res_preds[i].loc[:, y.columns]\nprint(log_loss_metric(y, res2))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sample.loc[:, y.columns] = 0\nfor i in range(n_round):\n    df_sample.loc[:, y.columns] += aa[i] * test_preds[i].loc[:, y.columns]\ndf_sample.loc[ind_te, y.columns] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df_sample.head())\ndf_sample.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Evaluation and Summary</center></h2>","metadata":{"trusted":true}},{"cell_type":"markdown","source":"In this project, we first examined the data and performed some explanatory data analysis. We then trained a model using deep neural networks on the non-scored targets, transfered the weights and trained the model on scored targets. We then blended the results of different network architectures and initializations. Let's dive deep more into the data and see which label is contributing the most to the overall loss. ","metadata":{}},{"cell_type":"code","source":"y_true = y\ny_preds = res2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses = []\nfor i in range(y.shape[1]):\n    losses.append(log_loss(y.iloc[:,i], res2.iloc[:,i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_loss_ind= np.argmax(losses)\nmax_loss = np.max(losses)\nprint(\"Max loss is\", max_loss,'For index', max_loss_ind,'which is',y.iloc[:,max_loss_ind].name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_max_loss = y.iloc[:,max_loss_ind]\ny_max_loss.value_counts()\n\nsns.countplot(y=y_max_loss,palette='nipy_spectral',orient='h')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see label 71 is contributing the most to the loss. As we saw earlier, this target was also the third top in having the most positive labels. Some may think of using imblearn library to address the imbalance problem. <a href=\"https://www.kaggle.com/sinamhd9/safe-driver-prediction-a-comprehensive-project\">[8] </a> However, this may get complicated for a multilabel problem . <font size=\"4\"> <b>Please leave a comment and share your ideas and let me know if this notebook was useful. Your upvote is appreciated! </b>","metadata":{"trusted":true}},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>References</center></h2>\nAlmost all the things I shared was the ideas I learned from other kernels which are listed below. I would like to express my gratitude to the authors of these kernels who shared their work. Also, the outline for this notebook was inspired by this notebook <a href='https://www.kaggle.com/isaienkov/mechanisms-of-action-moa-prediction-eda'> [9]</a> <br>\n    \n<a href=\"https://www.kaggle.com/kailex/moa-transfer-recipe-with-smoothing\"> [1] MOA: Transfer Recipe with Smoothing</a> <br>\n<a href=\"https://www.kaggle.com/c/lish-moa/discussion/184005\"> [2] Competition Insights </a> <br>\n<a href='https://www.kaggle.com/kushal1506/moa-pytorch-0-01859-rankgauss-pca-nn?scriptVersionId=44558776' >[3] MoA | Pytorch | 0.01859 | RankGauss | PCA | NN </a> <br> \n<a href=\"https://www.kaggle.com/rahulsd91/moa-label-smoothing\">[4] MoA Label Smoothing  <a/> <br>\n<a href=\"https://www.kaggle.com/sinamhd9/hyperparameter-tuning-with-keras-tuner\">[5] Hyperparameter tuning with Keras Tuner </a> <br>\n<a href='https://www.kaggle.com/gogo827jz/optimise-blending-weights-with-bonus-0'>[6] Model Blending Weights Optimisation </a> <br>\n<a href='https://www.kaggle.com/gogo827jz/split-neural-network-approach-tf-keras'>[7] Split Neural Network Approach (TF Keras) </a> <br>\n<a href='https://www.kaggle.com/sinamhd9/safe-driver-prediction-a-comprehensive-project'>[8] Safe driver prediction: A comprehensive project</a> <br>\n<a href='https://www.kaggle.com/isaienkov/mechanisms-of-action-moa-prediction-eda'> [9] Mechanisms of Action (MoA) Prediction. EDA </a>\n","metadata":{}},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}