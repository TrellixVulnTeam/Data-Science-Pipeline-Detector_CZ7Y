{"cells":[{"metadata":{"id":"o0zGNa1z6iA-","executionInfo":{"status":"ok","timestamp":1604416999673,"user_tz":-480,"elapsed":8652,"user":{"displayName":"Vic H","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivJ4FNHP9I2aI3Ca6GsYtTqv3ItiD8gwpxKmC5=s64","userId":"06031028774208724730"}},"outputId":"4e38d682-daac-4418-ad18-8f710e863225","trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nsys.path.append('../input/autograd')\nfrom autograd import grad\nimport autograd.numpy as anp\nfrom scipy.optimize import fsolve\nimport datetime\n\nimport os\nimport gc\nimport math\nfrom time import time\nimport random\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import log_loss\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom torch.utils.tensorboard import SummaryWriter       \nwriter = SummaryWriter('./log')\nif not os.path.exists('./log'):\n    os.makedirs('log')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{"id":"Rxvcwk136iBR","executionInfo":{"status":"ok","timestamp":1604416999677,"user_tz":-480,"elapsed":8646,"user":{"displayName":"Vic H","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivJ4FNHP9I2aI3Ca6GsYtTqv3ItiD8gwpxKmC5=s64","userId":"06031028774208724730"}},"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(42)","execution_count":null,"outputs":[]},{"metadata":{"id":"YahtcUd66iBf","executionInfo":{"status":"ok","timestamp":1604416999678,"user_tz":-480,"elapsed":8643,"user":{"displayName":"Vic H","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivJ4FNHP9I2aI3Ca6GsYtTqv3ItiD8gwpxKmC5=s64","userId":"06031028774208724730"}},"trusted":true},"cell_type":"code","source":"def load_data(path = '../input/lish-moa/'):\n    train_features = pd.read_csv(os.path.join(path, 'train_features.csv'))\n    test_features = pd.read_csv(os.path.join(path, 'test_features.csv'))\n    Y0 = pd.read_csv(os.path.join(path, 'train_targets_nonscored.csv'))\n    Y = pd.read_csv(os.path.join(path, 'train_targets_scored.csv'))\n    submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n    \n    keep_rows = train_features['cp_type']!='ctl_vehicle'\n    train = train_features.copy()\n    train = train[keep_rows].reset_index(drop=True)\n    origin_Y = Y.copy()\n    origin_Y = origin_Y.drop('sig_id', axis=1)\n    Y0 = Y0[keep_rows].reset_index(drop=True) # nonscored pretrain\n    Y0 = Y0.drop('sig_id', axis=1)\n    Y = Y[keep_rows].reset_index(drop=True)\n    Y = Y.drop('sig_id', axis=1)\n    submission.iloc[:,1:] = 0\n\n    # label smoothing\n    Y_smooth = smooth_one_hot(Y, classes=2, smoothing=0.001)\n    Y0_smooth = smooth_one_hot(Y0, classes=2, smoothing=0.001)\n    return train, test_features, Y0, Y, Y0_smooth, Y_smooth, submission, origin_Y","execution_count":null,"outputs":[]},{"metadata":{"id":"76AYtI_DPndP","executionInfo":{"status":"ok","timestamp":1604416999678,"user_tz":-480,"elapsed":8637,"user":{"displayName":"Vic H","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivJ4FNHP9I2aI3Ca6GsYtTqv3ItiD8gwpxKmC5=s64","userId":"06031028774208724730"}},"trusted":true},"cell_type":"code","source":"def smooth_one_hot(Y, classes: int, smoothing=0.001):\n    \"\"\"\n    Y_train : one-hot encoding dataframe\n    if smoothing == 0, it's one-hot method\n    if 0 < smoothing < 1, it's smooth method\n\n    confidence = 1.0 - label_smoothing\n    return y_true * confidence + (label_smoothing / num_classes)\n    \"\"\"\n    assert 0 <= smoothing < 1\n    Y_smooth = Y.copy()\n    confidence = 1.0 - smoothing\n    Y_smooth.replace(1, confidence + smoothing / classes, inplace=True)\n    Y_smooth.replace(0, smoothing/classes, inplace=True)\n    return Y_smooth","execution_count":null,"outputs":[]},{"metadata":{"id":"Dsv0JqTUazlf","executionInfo":{"status":"ok","timestamp":1604416999679,"user_tz":-480,"elapsed":8634,"user":{"displayName":"Vic H","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivJ4FNHP9I2aI3Ca6GsYtTqv3ItiD8gwpxKmC5=s64","userId":"06031028774208724730"}},"trusted":true},"cell_type":"code","source":"def apply_rankgauss(df1, df2, n_quantiles, ouput_distribution):\n    GENES = [col for col in df1.columns if col.startswith('g-')]\n    CELLS = [col for col in df1.columns if col.startswith('c-')]\n    cols = GENES + CELLS\n    \n    qt = QuantileTransformer(n_quantiles=n_quantiles, random_state=0, output_distribution=ouput_distribution)\n    df1[cols] = qt.fit_transform(df1[cols])\n    df2[cols] = qt.transform(df2[cols])\n    return df1, df2\n\n\ndef rank_gauss(train_features, test_features):\n    GENES = [col for col in train_features.columns if col.startswith('g-')]\n    CELLS = [col for col in train_features.columns if col.startswith('c-')]\n    \n    for col in (GENES + CELLS):\n        transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n        vec_len = len(train_features[col].values)\n        vec_len_test = len(test_features[col].values)\n        raw_vec = train_features[col].values.reshape(vec_len, 1)\n        transformer.fit(raw_vec)\n\n        train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n        test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n    return train_features, test_features","execution_count":null,"outputs":[]},{"metadata":{"id":"XQljdxufH0VP","executionInfo":{"status":"ok","timestamp":1604416999679,"user_tz":-480,"elapsed":8630,"user":{"displayName":"Vic H","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivJ4FNHP9I2aI3Ca6GsYtTqv3ItiD8gwpxKmC5=s64","userId":"06031028774208724730"}},"trusted":true},"cell_type":"code","source":"def pre_process_0(train_data, test_data):\n    GENES = [col for col in train_data.columns if col.startswith('g-')]\n    CELLS = [col for col in train_data.columns if col.startswith('c-')]\n    \n    def fe_stats(train, test):\n        for df in [train, test]:\n            df['g_sum'] = df[GENES].sum(axis = 1)\n            df['g_mean'] = df[GENES].mean(axis = 1)\n            df['g_std'] = df[GENES].std(axis = 1)\n            df['g_kurt'] = df[GENES].kurtosis(axis = 1)\n            df['g_skew'] = df[GENES].skew(axis = 1)\n            df['c_sum'] = df[CELLS].sum(axis = 1)\n            df['c_mean'] = df[CELLS].mean(axis = 1)\n            df['c_std'] = df[CELLS].std(axis = 1)\n            df['c_kurt'] = df[CELLS].kurtosis(axis = 1)\n            df['c_skew'] = df[CELLS].skew(axis = 1)\n            df['gc_sum'] = df[GENES + CELLS].sum(axis = 1)\n            df['gc_mean'] = df[GENES + CELLS].mean(axis = 1)\n            df['gc_std'] = df[GENES + CELLS].std(axis = 1)\n            df['gc_kurt'] = df[GENES + CELLS].kurtosis(axis = 1)\n            df['gc_skew'] = df[GENES + CELLS].skew(axis = 1)\n        return train, test\n\n    def c_squared(train, test):\n        for df in [train, test]:\n            for feature in CELLS:\n                df[f'squared_{feature}'] = df[feature] ** 2\n        return train, test\n\n    train_data, test_data = fe_stats(train_data, test_data)\n    train_data, test_data = c_squared(train_data, test_data)\n\n    return train_data, test_data","execution_count":null,"outputs":[]},{"metadata":{"id":"v685PB_Y6iBp","executionInfo":{"status":"ok","timestamp":1604416999680,"user_tz":-480,"elapsed":8628,"user":{"displayName":"Vic H","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivJ4FNHP9I2aI3Ca6GsYtTqv3ItiD8gwpxKmC5=s64","userId":"06031028774208724730"}},"trusted":true},"cell_type":"code","source":"def pre_process(train_data, test_data):\n    GENES = [col for col in train_data.columns if col.startswith('g-')]\n    CELLS = [col for col in train_data.columns if col.startswith('c-')]\n    \n    # drop no use feature\n    train_data = train_data.drop(['cp_type', 'sig_id'], axis=1)\n    test_data = test_data.drop(['cp_type', 'sig_id'], axis=1)\n    \n    # get dummy, one hot encoding\n    train_data = pd.get_dummies(train_data, columns=['cp_time', 'cp_dose'], drop_first=True)\n    test_data = pd.get_dummies(test_data, columns=['cp_time', 'cp_dose'], drop_first=True)\n\n    #train_data, test_data = rank_gauss(train_data, test_data)\n    train_data, test_data = apply_rankgauss(train_data, test_data, 100, 'normal')\n    \n    # PCA from kernel https://www.kaggle.com/ragnar123/moa-dnn-feature-engineering\n    # this improve CV a little but LB doesn't change\n    def create_pca(train, test, colunm, n_components, kind='g'):\n        # add pca to train data\n        pca = PCA(n_components)\n        PCA_data = pca.fit_transform(train[colunm])\n        PCA_data = pd.DataFrame(PCA_data, columns=[f'PCA_{kind}-{i}' for i in range(PCA_data.shape[1])])\n        train = pd.concat((train, PCA_data), axis=1)\n        # train = train.drop(colunm, axis=1)\n        \n        # add pca to test data\n        PCA_data = pca.transform(test[colunm])\n        PCA_data = pd.DataFrame(PCA_data, columns=[f'PCA_{kind}-{i}' for i in range(PCA_data.shape[1])])\n        test = pd.concat((test, PCA_data), axis=1)\n        # test = test.drop(colunm, axis=1)\n        return train, test\n\n    train_data, test_data = create_pca(train_data, test_data, GENES, 45, kind='g')\n    train_data, test_data = create_pca(train_data, test_data, CELLS, 15, kind='c')\n\n    return train_data, test_data","execution_count":null,"outputs":[]},{"metadata":{"id":"7FZo-TRI6iBw","executionInfo":{"status":"ok","timestamp":1604416999680,"user_tz":-480,"elapsed":8623,"user":{"displayName":"Vic H","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivJ4FNHP9I2aI3Ca6GsYtTqv3ItiD8gwpxKmC5=s64","userId":"06031028774208724730"}},"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size=512, init_bias=0):\n        super(Model, self).__init__()\n        \n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.Linear(num_features, hidden_size)\n\n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dense2 = nn.Linear(hidden_size, hidden_size // 2)\n\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size // 2)\n\n        self.dense3 = nn.Linear(hidden_size // 2, num_targets)\n        self.dense3.bias.data = init_bias\n\n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = F.relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dense3(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"id":"W9XE9Mpz6iB5","executionInfo":{"status":"ok","timestamp":1604416999680,"user_tz":-480,"elapsed":8620,"user":{"displayName":"Vic H","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivJ4FNHP9I2aI3Ca6GsYtTqv3ItiD8gwpxKmC5=s64","userId":"06031028774208724730"}},"trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n\n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        dct = {\n            'x': torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y': torch.tensor(self.targets[idx, :], dtype=torch.float)\n        }\n        return dct\n\n\nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n\n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        dct = {\n            'x': torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct","execution_count":null,"outputs":[]},{"metadata":{"id":"K-fPF8i66iCA","executionInfo":{"status":"ok","timestamp":1604416999681,"user_tz":-480,"elapsed":8617,"user":{"displayName":"Vic H","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivJ4FNHP9I2aI3Ca6GsYtTqv3ItiD8gwpxKmC5=s64","userId":"06031028774208724730"}},"trusted":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n\n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        final_loss += loss.item()\n\n    final_loss /= len(dataloader)\n\n    return final_loss\n\ndef BCELoss_with_clip(preds, targets, p_min=0.0001, p_max=0.9999):\n    loss_fn = nn.BCELoss()\n    loss = loss_fn(torch.clamp(preds, p_min, p_max), targets)\n    return loss\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n\n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        \n        loss = BCELoss_with_clip(outputs.sigmoid().detach(), targets)\n        #loss = loss_fn(outputs, targets)\n\n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n\n    return final_loss, valid_preds\n\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n\n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n\n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n    preds = np.concatenate(preds)\n    return preds","execution_count":null,"outputs":[]},{"metadata":{"id":"_Sp4t8lp6iCG","executionInfo":{"status":"ok","timestamp":1604416999681,"user_tz":-480,"elapsed":8614,"user":{"displayName":"Vic H","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivJ4FNHP9I2aI3Ca6GsYtTqv3ItiD8gwpxKmC5=s64","userId":"06031028774208724730"}},"trusted":true},"cell_type":"code","source":"def load_pretrain(model):\n    pretrained_dict = torch.load('model.pth')\n    model_dict = model.state_dict()\n    pretrained_dict =  {k: v for k, v in pretrained_dict.items() if k in model_dict} \n    model_dict.update(pretrained_dict)\n    model.load_state_dict(model_dict)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"id":"AKfUAs2B6iCN","executionInfo":{"status":"ok","timestamp":1604416999682,"user_tz":-480,"elapsed":8612,"user":{"displayName":"Vic H","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivJ4FNHP9I2aI3Ca6GsYtTqv3ItiD8gwpxKmC5=s64","userId":"06031028774208724730"}},"trusted":true},"cell_type":"code","source":"def run_train(seed, X_train, Y_train, Y_train_smooth, x_test, train_index, valid_index):\n    INIT_BIAS = torch.Tensor(np.log(Y_train.values.mean(axis=0)))\n    seed_everything(seed)\n    x_train, x_valid = X_train.iloc[train_index], X_train.iloc[valid_index]\n    y_train, y_valid = Y_train_smooth.iloc[train_index], Y_train.iloc[valid_index]\n\n    train_dataset = MoADataset(x_train.values, y_train.values)\n    valid_dataset = MoADataset(x_valid.values, y_valid.values)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=x_train.shape[1],\n        num_targets=y_train.shape[1],\n        hidden_size=HIDDEN_SIZE,\n        init_bias=INIT_BIAS\n    )\n\n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.05, div_factor=1e4,\n                                              max_lr=0.01, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    loss_fn = nn.BCEWithLogitsLoss()\n\n    oof = np.zeros((X_train.shape[0], Y_train.shape[1]))\n    best_loss = np.inf\n\n    for epoch in range(EPOCHS):\n        train_loss = train_fn(model, optimizer, scheduler, loss_fn, trainloader, DEVICE)\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        if valid_loss < best_loss:\n            best_loss = valid_loss\n            oof[valid_index] = valid_preds\n            torch.save(model.state_dict(), \"model.pth\")\n            print(f\"EPOCH: %d \\t LR: %f \\t train_loss: %f \\t valid_loss: %f, New Best!\" % (epoch, scheduler.get_lr()[0], train_loss, valid_loss))\n        else:\n            print(f\"EPOCH: %d \\t LR: %f \\t train_loss: %f \\t valid_loss: %f\" % (epoch, scheduler.get_lr()[0], train_loss, valid_loss))\n\n    # --------------------- PREDICTION---------------------\n    testdataset = TestDataset(x_test.values)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    model = Model(\n        num_features=x_test.shape[1],\n        num_targets=Y_train.shape[1],\n        hidden_size=HIDDEN_SIZE,\n        init_bias=INIT_BIAS\n    )\n    model.load_state_dict(torch.load(\"model.pth\"))\n    model.to(DEVICE)\n    predictions = inference_fn(model, testloader, DEVICE)\n\n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_folds(df_y, FOLDS, SEED):\n    # LOCATE DRUGS\n    vc = df_y.drug_id.value_counts()    \n    vc1 = vc.loc[(vc==6)|(vc==12)|(vc==18)].index.sort_values()\n    vc2 = vc.loc[(vc!=6)&(vc!=12)&(vc!=18)].index.sort_values()\n    \n    # TARGETS\n    targets = [x for x in df_y.columns if x not in ['sig_id', 'drug_id']]\n\n    # STRATIFY DRUGS 18X OR LESS\n    dct1 = {}; dct2 = {}\n    skf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, \n              random_state=SEED)\n    tmp = df_y.groupby('drug_id')[targets].mean().loc[vc1]\n    for fold, (idxT, idxV) in enumerate(skf.split(tmp, tmp[targets])):\n        dd = {k:fold for k in tmp.index[idxV].values}\n        dct1.update(dd)\n\n    # STRATIFY DRUGS MORE THAN 18X\n    skf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, \n              random_state=SEED)\n    tmp = df_y.loc[df_y.drug_id.isin(vc2)].reset_index(drop=True)\n    for fold, (idxT, idxV) in enumerate(skf.split(tmp, tmp[targets])):\n        dd = {k:fold for k in tmp.sig_id[idxV].values}\n        dct2.update(dd)\n\n    # ASSIGN FOLDS\n    df_y['Fold'] = df_y.drug_id.map(dct1)\n    df_y.loc[df_y.Fold.isna(),'Fold'] = df_y.loc[df_y.Fold.isna(),'sig_id'].map(dct2)\n    df_y.Fold = df_y.Fold.astype('int8')\n    return df_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_loss_numpy(y_pred, y_true):\n    y_true = np.array(y_true).ravel()\n    y_pred = np.array(y_pred).ravel()\n    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    loss = np.where(y_true == 1, -np.log(y_pred), -np.log(1 - y_pred))\n    return loss.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NEW\ndef run_k_fold(seed, X_train, Y_train, Y_train_smooth, NFOLDS):\n    oof = np.zeros((X_train.shape[0], Y_train.shape[1]))\n    predictions = np.zeros((X_test.shape[0], Y_train.shape[1]))\n\n    y_develop = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n    x_develop = pd.read_csv('../input/lish-moa/train_features.csv')\n    keep_rows = x_develop['cp_type']!='ctl_vehicle'\n    y_develop = y_develop[keep_rows]\n    \n    drugs = pd.read_csv('../input/lish-moa/train_drug.csv')\n    y_develop = y_develop.merge(drugs, how='left', on='sig_id')\n    y_develop = create_folds(y_develop, NFOLDS, seed)\n\n    for foldno in np.sort(y_develop['Fold'].unique()):\n        train_index = y_develop[y_develop['Fold']!=foldno].index\n        valid_index = y_develop[y_develop['Fold']==foldno].index\n        print(f\"\\nFold-%d\" % (foldno))\n        print('Train Sample Size: %d, Validation Sample Size: %d' % (len(train_index), len(valid_index)))\n        oof_, pred_ = run_train(seed, X_train, Y_train, Y_train_smooth, X_test, train_index, valid_index)\n\n        predictions += pred_ / NFOLDS\n        oof += oof_\n\n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"id":"8wWS3AhNHfCz","executionInfo":{"status":"ok","timestamp":1604416999683,"user_tz":-480,"elapsed":8606,"user":{"displayName":"Vic H","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivJ4FNHP9I2aI3Ca6GsYtTqv3ItiD8gwpxKmC5=s64","userId":"06031028774208724730"}},"trusted":true},"cell_type":"code","source":"def generate_result_with_clip(oof, predictions, origin_Y, Y_train, ctl_idx, use_ctl=False, p_min=0.0001, p_max=0.9999):\n    # 使用clip计算cv\n    oof = np.clip(oof, p_min, p_max)\n    if use_ctl:\n        num1 = origin_Y.shape[0] - Y_train.shape[0]\n        num2 = Y_train.shape[1]\n        ctl_data = np.zeros((num1, num2))\n        ctl_pd = pd.DataFrame(data=ctl_data, columns=Y_train.columns)\n        Y_train = Y_train.append(ctl_pd, ignore_index=True)\n        oof = np.vstack((oof, ctl_data))\n    score = log_loss_numpy(oof, Y_train.values)\n    print(\"CV log_loss: %f\" % score)\n\n    # generate submission\n    # using clip\n    predictions = np.clip(predictions, p_min, p_max)\n    predictions[ctl_idx] = 0\n    submission.iloc[:, 1:] = predictions\n    submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def optimize_weights(ps, labels):\n    if isinstance(ps, list):\n        ps = anp.stack(ps)\n    \n    weights = anp.random.dirichlet([2]*len(ps),size=1).reshape(len(ps)).tolist() + [1]\n    L = labels.values\n\n    def log_loss_numpy(y_pred, y_true=L):\n        y_true = anp.array(y_true).ravel()\n        y_pred = anp.array(y_pred).ravel()\n        y_pred = anp.clip(y_pred, 1e-15, 1 - 1e-15)\n        loss = anp.where(y_true == 1, -anp.log(y_pred), -anp.log(1 - y_pred))\n        return loss.mean()\n\n\n    def individual_log_loss(ps):\n        for i, p  in enumerate(ps):\n            print(f'Log Loss of M%d: %.7f' % (i, log_loss_numpy(p)))\n\n\n    def calc_oof_blend(ws, ps):\n        return anp.squeeze(anp.matmul(ws.reshape(1, 1, len(ws)), anp.transpose(ps, [1, 0, 2])))\n\n\n    def Lagrange_func(params):\n        ws = params[:-1]\n        _lambda = params[-1]\n        ws = anp.array(ws)\n        oof_blend = calc_oof_blend(ws, ps)\n        return log_loss_numpy(oof_blend) - _lambda * (ws.sum() - 1.)\n\n\n    def Lagrange_obj(params):\n        ws = params[:-1]\n        grad_L = grad(Lagrange_func)\n        pars = grad_L(params)\n        dLdws = pars[:-1]\n        # dldlam = pars[-1]\n        res = anp.append(dLdws, sum(ws) - 1.)\n        return res\n\n\n    individual_log_loss(ps)\n    start_time = time()\n    pars = fsolve(Lagrange_obj, weights)\n    ws = pars[:-1]\n    time_elapsed = time() - start_time\n    print(f'Optimized in %.2fs' % time_elapsed)\n    print('Optimized Weights:', ws)\n    oof_b = calc_oof_blend(ws, ps)\n    optimized_cv = log_loss_numpy(oof_b)\n    print('Optimised Blend OOF Score:', optimized_cv)\n    return ws, optimized_cv","execution_count":null,"outputs":[]},{"metadata":{"id":"I-fdUrRx6iCK","executionInfo":{"status":"ok","timestamp":1604416999684,"user_tz":-480,"elapsed":8601,"user":{"displayName":"Vic H","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivJ4FNHP9I2aI3Ca6GsYtTqv3ItiD8gwpxKmC5=s64","userId":"06031028774208724730"}},"trusted":true},"cell_type":"code","source":"# hyper param\nBATCH_SIZE = 128\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nHIDDEN_SIZE = 512\nEPOCHS = 50\nNFOLDS = 7\nSEED = [14, 16, 77, 1984, 42]","execution_count":null,"outputs":[]},{"metadata":{"id":"GtqrtBQA6iCV","executionInfo":{"status":"ok","timestamp":1604417020784,"user_tz":-480,"elapsed":29698,"user":{"displayName":"Vic H","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivJ4FNHP9I2aI3Ca6GsYtTqv3ItiD8gwpxKmC5=s64","userId":"06031028774208724730"}},"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_pretrain, Y_train, Y_pretrain_smooth, Y_train_smooth, submission, origin_Y = load_data()\nctl_idx = X_test[X_test['cp_type'] == 'ctl_vehicle'].index\nX_train, X_test = pre_process_0(X_train, X_test)\nX_train, X_test = pre_process(X_train, X_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"A8kanucw6iCd","executionInfo":{"status":"ok","timestamp":1604418011196,"user_tz":-480,"elapsed":1020105,"user":{"displayName":"Vic H","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivJ4FNHP9I2aI3Ca6GsYtTqv3ItiD8gwpxKmC5=s64","userId":"06031028774208724730"}},"outputId":"8d6bce12-da4e-4ac0-ca97-7a417f592566","trusted":true},"cell_type":"code","source":"oof = np.empty(shape=(len(SEED), Y_train.shape[0], Y_train.shape[1]))\npredictions = np.empty(shape=(len(SEED), X_test.shape[0], Y_train.shape[1]))\nstart_time = time()\nfor i, seed in enumerate(SEED):\n    print(f'\\n\\nSeed-%d (%d)' % (i, seed))\n    oof_, predictions_ = run_k_fold(seed, X_train, Y_train, Y_train_smooth, NFOLDS)\n    oof[i, :, :] = oof_\n    predictions[i, :, :] = predictions_\n    \nnp.save('Val_pred.npy', oof)\nws, optimized_cv = optimize_weights(oof, Y_train)\npredictions = (predictions.transpose(1, 2, 0) * ws.reshape(-1, len(ws))).sum(axis=-1)\noof = (oof.transpose(1, 2, 0) * ws.reshape(-1, len(ws))).sum(axis=-1)\n\nend_time = time()\nprint(\"total time consume:\", end_time - start_time)","execution_count":null,"outputs":[]},{"metadata":{"id":"pC43gtQl6iCj","executionInfo":{"status":"ok","timestamp":1604418013853,"user_tz":-480,"elapsed":1022756,"user":{"displayName":"Vic H","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivJ4FNHP9I2aI3Ca6GsYtTqv3ItiD8gwpxKmC5=s64","userId":"06031028774208724730"}},"outputId":"7214fff2-296b-4e11-ebbb-cc291240d7ea","trusted":true},"cell_type":"code","source":"generate_result_with_clip(oof, predictions, origin_Y, Y_train, ctl_idx, use_ctl=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}