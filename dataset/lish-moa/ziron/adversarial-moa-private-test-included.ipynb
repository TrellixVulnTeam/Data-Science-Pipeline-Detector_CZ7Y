{"cells":[{"metadata":{},"cell_type":"markdown","source":"Understanding the similarities between train and test distributions is a vital starting point for creating a local validation framework that generalises well on unseen data. In this kernel, let's look at Adversarial validation to compare the distributions of train and test.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn import model_selection, metrics\nimport lightgbm as lgb\n\nimport matplotlib.pyplot as plt\n\nfrom pathlib import Path","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"SEED = 1729\nINPUT_PATH = Path(\"../input/lish-moa/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv(INPUT_PATH/\"train_features.csv\"); print(f\"Train features shape: {train_features.shape}\")\ntest_features = pd.read_csv(INPUT_PATH/\"test_features.csv\"); print(f\"Test features shape: {test_features.shape}\")\n\ntrain_targets = pd.read_csv(INPUT_PATH/\"train_targets_scored.csv\"); print(f\"Train targets shape: {train_targets.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adversarial Validation on public test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build a model that can separate train and test based on the provided features.\ntrain_features[\"is_test\"] = 0\ntest_features[\"is_test\"] = 1\n\npanel = pd.concat([train_features, test_features], sort=False, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cp_type_dict = {\"trt_cp\": 0, \"ctl_vehicle\": 1}\ncp_dose_dict = {\"D1\": 0, \"D2\": 1}\n\npanel[\"cp_type\"] = panel[\"cp_type\"].map(cp_type_dict)\npanel[\"cp_dose\"] = panel[\"cp_dose\"].map(cp_dose_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_for_model = panel.columns[~np.in1d(panel.columns, [\"sig_id\", \"is_test\"])]\nprint(len(columns_for_model))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \"num_leaves\": 128,\n    \"min_data_in_leaf\": 64, \n    \"objective\": \"binary\",\n    \"max_depth\": 6,\n    \"learning_rate\": 0.001,\n    \"min_child_samples\": 64,\n    \"boosting\": \"gbdt\",\n    \"feature_fraction\": 0.9,\n    \"bagging_freq\": 5,\n    \"bagging_fraction\": 0.9 ,\n    \"bagging_seed\": SEED,\n    \"metric\": \"auc\",\n    \"lambda_l1\": 50.0,\n    \"lambda_l2\": 10.0,\n    \"verbosity\": -1\n}\nnum_rounds = 1000\nearly_stopping_rounds = 50\nverbose_eval = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=SEED)\ncv_scores = []\nmodels = []\nfor fold_idx, (dev_idx, val_idx) in enumerate(kf.split(panel)):\n    print(f\"Fold: {fold_idx+1}\")\n    X_dev, y_dev = panel.loc[dev_idx, columns_for_model], panel.loc[dev_idx, \"is_test\"].values\n    X_val, y_val = panel.loc[val_idx, columns_for_model], panel.loc[val_idx, \"is_test\"].values\n    \n    dev_dataset = lgb.Dataset(X_dev, y_dev)\n    val_dataset = lgb.Dataset(X_val, y_val)\n    \n    clf = lgb.train(\n        params,\n        dev_dataset,\n        num_rounds,\n        valid_sets=[dev_dataset, val_dataset],\n        early_stopping_rounds=early_stopping_rounds,\n        verbose_eval=verbose_eval\n    )\n    \n    cv_scores.append(clf.best_score[\"valid_1\"]['auc'])\n    models.append(clf)\n    print(\"\\n\")\n    \nadversarial_validation_auc = np.mean(cv_scores)\nprint(f\"Mean Adversarial AUC: {adversarial_validation_auc:.4f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train and Public test distributions are more or less similar!\nCV and LB should be in sync (Hopefully!)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = models[-1]\nfig, ax = plt.subplots(figsize=(15,15))\nlgb.plot_importance(model, max_num_features=50, importance_type=\"gain\", height=0.8, ax=ax)\nax.grid(False)\nplt.title(\"LightGBM - Feature Importance by gain (Adversarial Validation)\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adversarial Validation on private test data\n\nSince this is a code competition and the private dataset is available only when we submit the kernel to the competition.\nWe can make use of this fact to check a condition that can make a successful submission if the condition is true otherwise the submission would fail.\n\nBelow are the steps to check Adversarial Validation on private dataset:\n\n1. Run the Adversarial Validation just like how we would run on public test data in notebooks.\n2. Check the condition ```adversarial_validation_auc``` is less than 0.55 for private dataset.\n3. Make a submission if the condition satisfies.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(INPUT_PATH/\"sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if adversarial_validation_auc < 0.55:\n    sample_submission.to_csv(\"submission.csv\", index=False)\nelse:\n    pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the submission was successful, we can conclude that train and private test distributions are also similar.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Next steps:\n1. Build a baseline model\n2. Check CV-LB difference & correlation (Random KFold is a reasonable validation framework to start with here)\n3. Improve single model (This is important because, the competition is Code only)\n\nFinally a tabular competition to work on!\n\nHappy Kaggling!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}