{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\n\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_features.shape) # (23814, 876)\nprint(train_targets.shape) # (23814, 207)\nprint(test_features.shape) # (3982, 876)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-processed Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    \"\"\"Returns preprocessed data frame\"\"\"\n    df = df.copy()\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})\n    del df['sig_id']\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets = train_targets.loc[train['cp_type']==0].reset_index(drop=True)\ntrain = train.loc[train['cp_type']==0].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaler.fit(train)\n\n# scale train data\ntrain = scaler.transform(train)\n# scale test data\ntest = scaler.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = [col for col in train_targets.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape) # (21948, 875)\nprint(test.shape) # (3982, 875)\nprint(train_targets.shape) # (21948, 206)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return self.features.shape[0]\n    \n    def __getitem__(self, idx):\n        return {\n            'input': torch.tensor(self.features[idx, :], dtype=torch.float),\n            'target': torch.tensor(self.targets[idx, :], dtype=torch.float)\n        }\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n    \n    def __len__(self):\n        return self.features.shape[0]\n    \n    def __getitem__(self, idx):\n        return {\n            'input': torch.tensor(self.features[idx, :], dtype=torch.float)\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.2) # 0.2\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.2) # 0.2\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.2) # 0.2\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.weight = torch.tensor([0.5]).to(device)\n        \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        \n        x = F.prelu(self.dense1(x), self.weight) # relu -> prelu\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.prelu(self.dense2(x), self.weight) # relu -> prelu\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train = train.values\n#test = test.values\ntrain_targets = train_targets.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_seed(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_dataloaders(num_workers, batch_size, x_train, y_train, x_valid, y_valid):\n    \"\"\"Return training and valid dataloader\"\"\"\n    \n    # load the training and valid datasets\n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n\n    # prepare data loaders\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers)\n\n    # define loaders\n    loader = {\n        \"train\": train_loader,\n        \"valid\": valid_loader\n    }\n    \n    return loader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_testloaders(num_workers, batch_size, x_test):\n    \"\"\"Return test dataloader\"\"\"\n    \n    # load the test datasets\n    test_dataset = TestDataset(x_test)\n    \n    # prepare test loader\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\n    \n    # define loaders\n    loader = {\n        'test': test_loader\n    }\n    return loader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.optim.lr_scheduler import StepLR\n\ndef train_model(n_epochs, loaders, model, optimizer, criterion, device, save_path):\n    \"\"\"Returns a trained model\"\"\"        \n    scheduler = StepLR(optimizer, step_size=2, gamma=0.96)\n    \n    # initialize tracker for minimum validation loss\n    valid_loss_min = np.Inf\n    print(valid_loss_min)\n    for epoch in range(1, n_epochs + 1):\n        # decay Learning Rate\n        scheduler.step()\n        # print(f'Epoch: \\t{epoch}\\tLR: {scheduler.get_lr()}')\n        \n        # initialize variables to monitor training and validation loss\n        train_loss = 0.0\n        valid_loss = 0.0\n        \n        # train the model\n        model.train()\n        \n        #for batch_idx, (data, target) in enumerate(loaders['train']):\n        for data in loaders['train']:\n            data_input, data_target = data['input'].to(device), data['target'].to(device)\n            \n            # initialize weights to zero: clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            \n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data_input)\n\n            # calcuate loss\n            loss = criterion(output, data_target)\n            \n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            \n            # perform a single optimization step\n            optimizer.step()\n            \n            # TODO: scheduler.step()\n            \n            # update running training loss\n            # print(\"train loss : \", loss.item())\n            train_loss += (loss.item() / len(loaders['train']))\n\n        # validate the model\n        model.eval()\n        \n        for data in loaders['valid']:\n            data_input, data_target = data['input'].to(device), data['target'].to(device)\n            \n            # update the average validation loss\n            output = model(data_input)\n            \n            # calculate loss\n            loss = criterion(output, data_target)\n            \n            # update running validation loss\n            # print(\"validation loss : \", loss.item())\n            valid_loss += (loss.item() / len(loaders['valid']))\n        \n        # print training/validation statistics\n        # print(f'Epoch: \\t{epoch}\\tTraining Loss: {train_loss}\\tValidation Loss:{valid_loss}')\n        \n        # save the model if validation loss has descrased\n        if valid_loss < valid_loss_min:\n            print(f'Epoch: \\t{epoch}\\tValidation loss decreased ({valid_loss_min} -> {valid_loss}). Saving the model...')\n            torch.save(model.state_dict(), save_path)\n            valid_loss_min = valid_loss\n\n    # return trained model\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(seed, kfold, batch_size, epochs, learning_rate, weight_decay):\n    set_seed(seed)\n\n    for fold, (train_idx, valid_idx) in enumerate(kfold.split(X=train, y=train_targets)):\n        x_train, x_valid = train[train_idx], train[valid_idx]\n        y_train, y_valid = train_targets[train_idx], train_targets[valid_idx]\n        \n        # get dataloaders\n        dataloaders = get_dataloaders(0, batch_size, x_train, y_train, x_valid, y_valid)\n        \n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        model = Model(875, 206, 1024).to(device)\n        \n        criterion_moa = nn.BCEWithLogitsLoss() # for multi-lable classfication\n        optimizer_moa = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n        # train the model\n        train_model(epochs, dataloaders, model, optimizer_moa, criterion_moa, device, f'models/model_seed_{seed}_fold_{fold}.pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyper parameters\n\nFOLDS = 5\nWORKERS = 0\nBATCH_SIZE = 128\nEPOCHS = 50\nLEARNING_RATE = 0.0002\nWEIGHT_DECAY = 0.00001\nSEED = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%mkdir models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mskf = MultilabelStratifiedKFold(n_splits=FOLDS)\n\nfor seed in range(40, 45):\n    run_training(seed, mskf, BATCH_SIZE, EPOCHS, LEARNING_RATE, WEIGHT_DECAY)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inferencing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference(loaders, model, device):\n    \"\"\"Return a prediction\"\"\"\n    \n    model.eval()\n    preds = []\n    \n    for data in  loaders['test']:\n        data_input = data['input'].to(device)\n        \n        # forward pass: compute predicted outputs by passing inputs to the model\n        with torch.no_grad():\n            output = model(data_input)\n        \n        pred = output.sigmoid().detach().cpu().numpy()\n        preds.append(pred)\n        \n    return np.concatenate(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_inferencing(seed):\n    set_seed(seed)\n    \n    preds = np.zeros((len(test), 206))\n    \n    #for fold, (train_idx, valid_idx) in enumerate(mskf.split(X=train, y=train_targets)):\n    for i in range(0, FOLDS):    \n        # get dataloaders\n        dataloaders = get_testloaders(0, 128, test)\n        \n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        model = Model(875, 206, 1024)\n        \n        model.load_state_dict(torch.load(f'models/model_seed_{seed}_fold_{i}.pt'))\n        model.to(device)\n        \n        pred = inference(dataloaders, model, device)\n        \n        preds += pred\n        \n    preds = preds / FOLDS\n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preds = run_inferencing(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.zeros((len(test), 206))\nfor seed in range(40, 45):\n    preds += run_inferencing(seed)\npreds = preds / 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission[targets] = preds\nsample_submission.loc[test_features['cp_type']=='ctl_vehicle', targets] = 0\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}