{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport IPython\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"y_test = np.random.randint(0,2,(3,4))\ny_pred = np.random.random((3,4))\ny_test,y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_loss(y_test,y_pred):\n    y_test = y_test.astype(np.float16)\n    y_pred = y_pred.astype(np.float16)\n    N,M = y_test.shape\n    a=[]\n    for m in range(M):\n        loss=0\n        for i in range(N):\n            loss -= ((y_test[i,m]*np.log(y_pred[i,m]))+((1.0-y_test[i,m])*np.log(1.0-y_pred[i,m])))\n        loss = loss/N\n        a.append(round(loss,8))\n    return a\na = log_loss(y_test,y_pred)\nprint(a)\nnp.mean(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.losses.categorical_crossentropy(np.transpose(y_test), np.transpose(y_pred)).numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tf.keras.losses.binary_crossentropy(np.transpose(y_test), np.transpose(y_pred)).numpy())\ntf.keras.losses.binary_crossentropy(np.transpose(y_test), np.transpose(y_pred)).numpy().mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# how to improve log loss\n\n# 1. log loss is nothing but entropy.\n# 2. entropy is measure of uncertainty.\n# 3. so a low Log Loss means alow uncertainty/entropy of your model\n\n# The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score, it would be much better to keep our probabilities between 0.05–0.95 so that we are never very sure about our prediction. In this case, we won’t see the massive growth of an error function."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = np.random.randint(0,2,(300,1))\ny_pred = np.random.random((300,1))\na = log_loss(y_test,y_pred)\nprint(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred[0.1>y_pred].shape,y_pred[y_pred>0.9].shape ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = y_pred.clip(0.05,0.95)\nlog_loss(y_test,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = np.random.randint(0,2,(300,3))\ny_pred = np.random.random((300,3))\na = log_loss(y_test,y_pred)\nprint(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = y_pred.clip(0.05,0.95)\nlog_loss(y_test,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Trick is:\n# We need to change all values that less than 0.05 to be equal to 0.05 and all values that are more than 0.95 to be equal to 0.95"},{"metadata":{},"cell_type":"markdown","source":"# secound Trick is (note : if only if you have 100 % accuracy)"},{"metadata":{"trusted":true},"cell_type":"code","source":"a = np.array([1.0,0.0,1.0,0.0,1.0])\np = np.array([0.6,0.3,0.7,0.2,0.9])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.losses.binary_crossentropy(a, p).numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.losses.binary_crossentropy(a, np.where(p,p>=0.5,1).astype(np.float16)).numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# what if you have 80 % accuracy "},{"metadata":{"trusted":true},"cell_type":"code","source":"a = np.array([1.0,0.0,1.0,0.0,1.0])\np = np.array([0.6,0.3,0.7,0.2,0.4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.losses.binary_crossentropy(a, p).numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.losses.binary_crossentropy(a, np.where(p,p>=0.5,1).astype(np.float16)).numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Log loss has very useful property in that it penalizes heavily when the model makes emphatically incorrect labels. In case the model predicts 1 for the label but the truth is 0, you will end up with natural log of (0), which reaches negative infinity. This will greatly increase the log loss term.\n\n# By this better to not to use secound trick. if you use hahaha you know 0.47 is for better than 3.05 :D\n\n# Hope this helps with the intuition."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}