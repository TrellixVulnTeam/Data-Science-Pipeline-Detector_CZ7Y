{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import module"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_value= 0\n\nimport os\nos.environ['PYTHONHASHSEED']=str(seed_value)\n\nimport random\nrandom.seed(seed_value)\n\nimport numpy as np\nnp.random.seed(seed_value)\n\nimport tensorflow as tf\ntf.random.set_seed(seed_value)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feature = pd.read_csv('../input/lish-moa/train_features.csv')\nX_train = train_feature.drop('sig_id', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\nY_train = train_targets_scored.drop('sig_id', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine = pd.concat([X_train, Y_train], axis=1, sort=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_conditional_prob(feature, observation):\n    '''\n    Function for returning a resulting dataframe\n    '''\n    return (combine[[feature, observation]].groupby([feature], as_index=False).\n             mean().\n             sort_values(by=observation, ascending=False))\n\nx_col = X_train.columns\ny_col = Y_train.columns\n\nprint(\"I have an assumsion that if cp_type is ctl_vehicle then y will be all zero\")\nctl_vehicle_is_zero = True\nfor y in y_col:\n    \n    temp = get_conditional_prob(feature=x_col[0], observation=y)\n    if temp[temp['cp_type'] == 'ctl_vehicle'][y][0] > 0:\n        # If y result > 0 was found\n        ctl_vehicle_is_zero = False\n        print(temp)\n    # print(temp[temp['cp_type'] == 'ctl_vehicle'][y][0])\n\nif ctl_vehicle_is_zero:\n    print(\"That's true\")\nelse:\n    print(\"Just Misunderstood\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check significant of cp_time"},{"metadata":{"trusted":true},"cell_type":"code","source":"cp_time_to_MOA = pd.DataFrame()\nimport scipy.stats as stats\n\n\nfor y in y_col:\n    temp = get_conditional_prob(feature=x_col[1], observation=y)\n    temp_2 = temp.T.rename(columns=temp.T.iloc[0]).iloc[1:]\n    cp_time_to_MOA = pd.concat([cp_time_to_MOA, temp_2])\ncp_time_to_MOA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats.ttest_rel(cp_time_to_MOA[24],cp_time_to_MOA[48])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats.ttest_rel(cp_time_to_MOA[24],cp_time_to_MOA[72])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats.ttest_rel(cp_time_to_MOA[48],cp_time_to_MOA[72])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats.f_oneway(cp_time_to_MOA[24], cp_time_to_MOA[48], cp_time_to_MOA[72])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Then, cp_time is very siginificant***"},{"metadata":{},"cell_type":"markdown","source":"## T-testing for cp_dose"},{"metadata":{"trusted":true},"cell_type":"code","source":"cp_dose_to_MOA = pd.DataFrame()\nimport scipy.stats as stats\n\n# stats.f_oneway(cp_time_to_MOA[24], cp_time_to_MOA[48], cp_time_to_MOA[72])\nfor y in y_col:\n    temp = get_conditional_prob(feature=x_col[2], observation=y)\n    temp_2 = temp.T.rename(columns=temp.T.iloc[0]).iloc[1:]\n    cp_dose_to_MOA = pd.concat([cp_dose_to_MOA, temp_2])\ncp_dose_to_MOA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats.ttest_rel(cp_dose_to_MOA['D1'],cp_dose_to_MOA['D2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats.ttest_rel([10,10],[1,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cp_time_to_MOA[24].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[X_train.columns[3:]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_col = Y_train.columns[1]\ncombine.loc[Y_train[Y_train[y_col]==1].index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_zero = True\nfor yc in Y_train.columns:\n    temp = (Y_train[yc] == 0)\n    if type(all_zero) is bool :\n        all_zero = temp\n    else:\n        all_zero = all_zero & temp\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_wraggled = X_train[X_train['cp_type'] != 'ctl_vehicle'][X_train.columns[3:]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train_wraggled = Y_train.loc[X_train_wraggled.index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_wraggled = X_train_wraggled.loc[Y_train_wraggled.index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_wraggled_g = X_train_wraggled[X_train_wraggled.columns[pd.Series(X_train_wraggled.columns).str.startswith('g')]] \n\nX_train_wraggled_c = X_train_wraggled[X_train_wraggled.columns[pd.Series(X_train_wraggled.columns).str.startswith('c')]] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train_wraggled.shape[1])\nprint(X_train_wraggled_g.shape[1])\nprint(X_train_wraggled_c.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train_wraggled[Y_train_wraggled.sum(axis=1)== 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input/moa-lstm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_wraggled_c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n              \ndef calculating_class_weights(y_true):\n    from sklearn.utils.class_weight import compute_class_weight\n    number_dim = np.shape(y_true)[1]\n    weights = np.empty([number_dim, 2])\n    for i in range(number_dim):\n        weights[i] = compute_class_weight('balanced', [0,1], y_true.iloc[:, i])\n    return weights\n\ndef get_weighted_loss(weights):\n    def weighted_loss(y_true, y_pred):\n        return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)\n    return weighted_loss\n\nimport tensorflow as tf\n\ndef f1(y_true, y_pred):\n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2*p*r / (p+r+K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)\n\ndef f1_loss(y_true, y_pred):\n    \n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2*p*r / (p+r+K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return 1 - K.mean(f1)\n\nclass_weights = calculating_class_weights(Y_train_wraggled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_weights_2 = np.copy(class_weights)\nclass_weights[:,0] = 1\nclass_weights[:,1] = 1.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fully connect\nimport keras\nfrom keras import backend as K\nfrom numpy import loadtxt\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping\nfrom keras.layers import Input, Concatenate, concatenate, BatchNormalization\nfrom keras.models import Model\nfrom keras.layers import Dense, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.layers import LSTM, Reshape\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom keras.callbacks import TensorBoard\nfrom tensorflow_addons.layers import WeightNormalization\nfrom functools import partial\nimport tensorflow as tf\n\nmodel_name = \"conv\"\nmodel_name = \"parallel_conv\"\nmodel_name = \"LSTM\"\n\nSPLIT_RATIO = 0.75\n# model_name = 'g_conv'\n# model_name = 'c_lstm'\n#Load pretrained model\nfrom keras.models import load_model\n\n\n\n#Create model function\ndef getModel(model_name):\n    if model_name == 'pretrained':\n        model = load_model('../input/moa-lstm/best_weights (1).hdf5')\n        toBeCompiled = False\n    elif model_name == 'conv':\n        InputLayer = Input(shape=(X_train_wraggled.shape[1], 1))\n        ConvLayer = Conv1D(filters=20,\n                           kernel_size=10,\n                           padding='valid',\n                           activation='relu',\n                           strides=1)(InputLayer)\n        PoolingLayer = GlobalMaxPooling1D()(ConvLayer)\n        OutputLayer = Dense(206, activation='sigmoid')(PoolingLayer)\n        model = Model(inputs=InputLayer, outputs=OutputLayer)\n        toBeCompiled = True\n        \n    elif model_name == 'parallel_conv':\n        InputLayer_g = Input(shape=(X_train_wraggled_g.shape[1], 1))\n        InputLayer_c = Input(shape=(X_train_wraggled_c.shape[1], 1))\n        \n        \n        ConvLayer_g = Conv1D(filters=1200,\n                           kernel_size=50,\n                           padding='valid',\n                           activation='relu',\n                           strides=1)(InputLayer_g)\n        PoolingLayer_g = GlobalMaxPooling1D()(ConvLayer_g)\n        PoolingLayer_g = BatchNormalization()(PoolingLayer_g)\n        PoolingLayer_g = Dropout(0.4)(PoolingLayer_g)\n        \n        ConvLayer_c = Conv1D(filters=1200,\n                           kernel_size=50,\n                           padding='valid',\n                           activation='relu',\n                           strides=1)(InputLayer_c)\n        PoolingLayer_c = GlobalMaxPooling1D()(ConvLayer_c)\n        PoolingLayer_c = BatchNormalization()(PoolingLayer_c)\n        PoolingLayer_c = Dropout(0.4)(PoolingLayer_c)\n        \n        merged = concatenate([PoolingLayer_g, PoolingLayer_c], axis=1)\n        merged = BatchNormalization()(merged)\n        merged = Reshape((2400,1))(merged)\n        \n        merged = WeightNormalization(LSTM(2000))(merged)\n        merged = BatchNormalization()(merged)\n        merged = Dropout(0.2)(merged)\n        \n        merged = WeightNormalization(Dense(1000, activation='relu'))(merged)\n        merged = BatchNormalization()(merged)\n        merged = Dropout(0.2)(merged)\n        \n        merged = WeightNormalization(Dense(1000, activation='linear'))(merged)\n        merged = tf.keras.layers.LeakyReLU(alpha=0.01)(merged)\n        merged = BatchNormalization()(merged)\n        merged = Dropout(0.2)(merged)\n        \n        merged = WeightNormalization(Dense(1000, activation='linear'))(merged)\n        merged = tf.keras.layers.LeakyReLU(alpha=0.01)(merged)\n        merged = BatchNormalization()(merged)\n        merged = Dropout(0.2)(merged)\n        \n        merged = WeightNormalization(Dense(1000, activation='linear'))(merged)\n        merged = tf.keras.layers.LeakyReLU(alpha=0.01)(merged)\n        merged = BatchNormalization()(merged)\n        merged = Dropout(0.2)(merged)\n        \n        merged = WeightNormalization(Dense(1000, activation='linear'))(merged)\n        merged = tf.keras.layers.LeakyReLU(alpha=0.01)(merged)\n        merged = BatchNormalization()(merged)\n        merged = Dropout(0.2)(merged)\n        \n        merged = WeightNormalization(Dense(1000, activation='linear'))(merged)\n        merged = tf.keras.layers.LeakyReLU(alpha=0.01)(merged)\n        merged = BatchNormalization()(merged)\n        merged = Dropout(0.2)(merged)\n        \n        merged = WeightNormalization(Dense(1000, activation='linear'))(merged)\n        merged = tf.keras.layers.LeakyReLU(alpha=0.01)(merged)\n        merged = BatchNormalization()(merged)\n        merged = Dropout(0.2)(merged)\n        \n        OutputLayer = WeightNormalization(Dense(206, activation='sigmoid'))(merged)\n        model = Model(inputs=[InputLayer_g,InputLayer_c], outputs=OutputLayer)\n        toBeCompiled = True\n        \n    elif model_name == 'LSTM':\n        InputLayer_g = Input(shape=(X_train_wraggled_g.shape[1], 1))\n        InputLayer_c = Input(shape=(X_train_wraggled_c.shape[1], 1))\n        \n        LSTM_g = LSTM(X_train_wraggled_g.shape[1])(InputLayer_g)\n        LSTM_g = BatchNormalization()(LSTM_g)\n        LSTM_c = LSTM(X_train_wraggled_c.shape[1])(InputLayer_c)\n        LSTM_c = BatchNormalization()(LSTM_c)\n        merged = concatenate([LSTM_g, LSTM_c], axis=1)\n        merged = BatchNormalization()(merged)\n#         merged = Reshape((400,1))(merged)\n        \n#         merged = WeightNormalization(LSTM(400, dropout = 0.4))(merged)\n#         merged = BatchNormalization()(merged)\n#         merged = Dropout(0.2)(merged)\n        \n#         merged = WeightNormalization(Dense(300, activation='relu'))(merged)\n#         merged = BatchNormalization()(merged)\n#         merged = Dropout(0.4)(merged)\n        \n#         merged = WeightNormalization(Dense(300, activation='relu'))(merged)\n#         merged = BatchNormalization()(merged)\n#         merged = Dropout(0.4)(merged)\n        \n#         merged = WeightNormalization(Dense(300, activation='relu'))(merged)\n#         merged = BatchNormalization()(merged)\n#         merged = Dropout(0.4)(merged)\n        \n#         merged = WeightNormalization(Dense(300, activation='relu'))(merged)\n#         merged = BatchNormalization()(merged)\n#         merged = Dropout(0.4)(merged)\n        \n#         merged = WeightNormalization(Dense(300, activation='relu'))(merged)\n#         merged = BatchNormalization()(merged)\n#         merged = Dropout(0.4)(merged)\n        \n#         merged = WeightNormalization(Dense(300, activation='relu'))(merged)\n#         merged = BatchNormalization()(merged)\n#         merged = Dropout(0.4)(merged)\n        \n#         merged = WeightNormalization(Dense(300, activation='relu'))(merged)\n#         merged = BatchNormalization()(merged)\n#         merged = Dropout(0.4)(merged)\n        \n#         merged = WeightNormalization(Dense(300, activation='relu'))(merged)\n#         merged = BatchNormalization()(merged)\n#         merged = Dropout(0.4)(merged)\n        \n#         merged = WeightNormalization(Dense(300, activation='relu'))(merged)\n#         merged = BatchNormalization()(merged)\n#         merged = Dropout(0.4)(merged)\n        \n        merged = WeightNormalization(Dense(300, activation='relu'))(merged)\n        merged = BatchNormalization()(merged)\n        merged = Dropout(0.4)(merged)\n        \n        OutputLayer = WeightNormalization(Dense(206, activation='sigmoid'))(merged)\n        model = Model(inputs=[InputLayer_g,InputLayer_c], outputs=OutputLayer)\n        toBeCompiled = True\n    return toBeCompiled,model\n\ntoBeCompiled,model = getModel(model_name)\nMETRICS = [\n        'accuracy',\n        \"binary_crossentropy\",\n        f1_m,\n        keras.metrics.TruePositives(name='tp'),\n        keras.metrics.FalsePositives(name='fp'),\n        keras.metrics.TrueNegatives(name='tn'),\n        keras.metrics.FalseNegatives(name='fn'), \n        keras.metrics.Precision(name='precision'),\n        keras.metrics.Recall(name='recall'),\n        keras.metrics.AUC(name='auc'),\n    ]\nif toBeCompiled:\n    model.compile(loss=get_weighted_loss(class_weights),\n                  #loss=\"binary_crossentropy\",\n                  optimizer='adam',\n                  metrics=METRICS)\n\n\n\nif model_name in ['parallel_conv','LSTM']:\n    #Set Checkpoint\n    filepath=\"best_weights.hdf5\"\n    Monitor = 'val_binary_crossentropy'\n    #Monitor = 'binary_crossentropy'\n    checkpoint = ModelCheckpoint(filepath, monitor=Monitor, verbose=1, save_best_only=True, mode='min')\n    early_stop =EarlyStopping(monitor=Monitor, mode = 'min', patience=30)\n    \n    validation = (\n                      [\n                        X_train_wraggled_g.iloc[int(len(X_train_wraggled_g)*SPLIT_RATIO):].values.astype('float32'),\n                        X_train_wraggled_c.iloc[int(len(X_train_wraggled_c)*SPLIT_RATIO):].values.astype('float32')\n                    ],\n                      Y_train_wraggled.iloc[int(len(X_train_wraggled)*SPLIT_RATIO):].values.astype('float32')\n                  )\n    # validation = None\n    \n    model.summary()\n    model.fit([\n                X_train_wraggled_g.iloc[0:int(len(X_train_wraggled_g)*SPLIT_RATIO)].values.astype('float32'),\n                X_train_wraggled_c.iloc[0:int(len(X_train_wraggled_c)*SPLIT_RATIO)].values.astype('float32')\n                ],\n              Y_train_wraggled.iloc[0:int(len(X_train_wraggled)*SPLIT_RATIO)].values.astype('float32'),\n              epochs=600,\n        \n              validation_data= validation,\n              # batch_size=200,\n                shuffle = True,\n              callbacks = [checkpoint,early_stop]\n             )\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train_wraggled.str.str","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Y_predicts = (best_model.predict([X_train_wraggled_g, X_train_wraggled_c]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Y_predicts_df = pd.DataFrame(Y_predicts)\n# Y_predicts_df.columns = Y_train_wraggled.columns\n# Y_predicts_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Y_predicts_df[Y_train_wraggled[]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Y_train_wraggled.index = Y_predicts_df.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Y_predicts_df[(Y_train_wraggled['5-alpha_reductase_inhibitor']==0) & (Y_predicts_df['5-alpha_reductase_inhibitor'] > 0.005)].iloc[0].idxmax()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Y_train_wraggled.loc[1024].idxmax()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# min(Y_predicts_df[(Y_train_wraggled['5-alpha_reductase_inhibitor']==1)]['5-alpha_reductase_inhibitor'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_feature = pd.read_csv('../input/lish-moa/test_features.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model\ntry:\n    best_model = load_model('best_weights.hdf5', custom_objects={'weighted_loss': get_weighted_loss(class_weights), 'f1_m':f1_m})\nexcept OSError:\n    best_model = model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"default_result = {}\nfor y_c in Y_train_wraggled.columns:\n    default_result[y_c] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Post processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicts = []\n\nX_test = test_feature\n\nX_test_wraggled = X_test[X_test.columns[4:]]\n\nX_test_wraggled_g = X_test_wraggled[X_test_wraggled.columns[pd.Series(X_test_wraggled.columns).str.startswith('g')]] \n\nX_test_wraggled_c = X_test_wraggled[X_test_wraggled.columns[pd.Series(X_test_wraggled.columns).str.startswith('c')]] \n\nif model_name in ['conv']:\n    result = model.predict(X_test_wraggled.values)\nelif model_name in ['parallel_conv', 'LSTM']:\n    result = model.predict([X_test_wraggled_g.values, X_test_wraggled_c.values])\nelif model_name in ['g_conv']:\n    result = model.predict([X_test_wraggled_g.values])\nelif model_name in ['c_lstm']:\n    result = model.predict([X_test_wraggled_c.values])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicts_df = pd.DataFrame(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicts_df[test_feature['cp_type'] == \"ctl_vehicle\"] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicts_df.columns = Y_train_wraggled.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicts_df['sig_id']=test_feature['sig_id']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicts_df = predicts_df[['sig_id']+list(Y_train_wraggled.columns)]\npredicts_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicts = pd.DataFrame(predicts_df)\npredicts.to_csv('submission.csv', index=False)\npredicts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Next step"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}