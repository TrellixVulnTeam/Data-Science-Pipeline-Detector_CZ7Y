{"cells":[{"metadata":{},"cell_type":"markdown","source":"import sys\nsys.path.append('../input/iterativestratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('ticks')\nsns.set_context(\"poster\")\nsns.set_palette('colorblind')\nimport warnings\nwarnings.filterwarnings('ignore')\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom torch import nn\nimport torch.nn.functional as F\nimport torchvision\nimport time\nfrom sklearn.model_selection import train_test_split\nimport random\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 0. Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (20.0, 10.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ntest_features = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\ntrain_targets = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_mask = train_features['cp_type'] != 'ctl_vehicle'\ntrain_sig_ids = train_features.loc[train_mask]['sig_id']\ntrain = train_features.loc[train_mask]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_mask = test_features['cp_type'] != 'ctl_vehicle'\ntest_sig_ids = test_features.loc[test_mask]['sig_id']\ntest = test_features.loc[test_mask]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_features = [cols for cols in train.columns if cols.startswith('g-')]\nc_features = [cols for cols in train.columns if cols.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets = train_targets[train_targets['sig_id'].isin(train_sig_ids)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Data preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_pca_features(n_comp, train, test, feature_list, name, normalize=False, scaler=None):\n    \n    pca = PCA(n_comp)\n    train_pca = pca.fit_transform(train[feature_list])\n    test_pca = pca.transform(test[feature_list])\n    \n    if normalize and scaler is not None:\n        train_pca = scaler.fit_transform(train_pca)\n        test_pca = scaler.transform(test_pca)\n    \n    for i in range(n_comp):\n        train['{0}_{1}'.format(name, i)] = train_pca[:, i]\n        test['{0}_{1}'.format(name, i)] = test_pca[:, i]\n        \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(data):\n    data['cp_time'] = data['cp_time'].map({24:0, 48:1, 72:2})\n    data['cp_dose'] = data['cp_dose'].map({'D1':0, 'D2':1})\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_w_pca, test_w_pca = make_pca_features(3, train, test, g_features, 'g_pca', normalize=True, scaler=StandardScaler())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_w_pca, test_w_pca = make_pca_features(2, train_w_pca, test_w_pca, c_features, 'c_pca', normalize=True, scaler=StandardScaler())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(preprocess(train_w_pca.drop(columns = ['sig_id', 'cp_type'])), train_targets.drop(columns = ['sig_id']), test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Model definition"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TabDataset:\n    \n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n    \n    def __len__(self):\n        return(self.X.shape[0])\n    \n    def __getitem__(self, i):\n        \n        X_i = torch.from_numpy(self.X.iloc[i, :].values.astype(np.float32))\n        y_i = torch.from_numpy(self.y.iloc[i, :].values.astype(np.float32))\n        \n        return X_i, y_i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TabDatasetTest:\n    \n    def __init__(self, X):\n        self.X = X\n    \n    def __len__(self):\n        return(self.X.shape[0])\n    \n    def __getitem__(self, i):\n        \n        X_i = torch.from_numpy(self.X.iloc[i, :].values.astype(np.float32))        \n        return X_i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds = TabDataset(X_train, y_train)\nvalid_ds = TabDataset(X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ds = TabDatasetTest(preprocess(test_w_pca.drop(columns = ['sig_id', 'cp_type'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ds[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds[0][1].dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dl = DataLoader(train_ds, batch_size=16, num_workers=8)\nvalid_dl = DataLoader(valid_ds, batch_size=16, num_workers=8)\ntest_dl = DataLoader(test_ds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lin_block(in_size, out_size):\n    return nn.Sequential(\n        nn.BatchNorm1d(in_size), \n        nn.Dropout(0.2),\n        nn.utils.weight_norm(nn.Linear(in_size, out_size))\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, in_size, hidden_size, out_size, num_blocks):\n        super().__init__()\n        \n        self.num_blocks = num_blocks\n        self.dense0 = nn.Sequential(\n        nn.BatchNorm1d(in_size), \n        nn.utils.weight_norm(nn.Linear(in_size, hidden_size))\n    )\n        \n        self.dense_blocks = nn.ModuleList()    \n        for i in range(self.num_blocks):\n            self.dense_blocks.append(lin_block(hidden_size, hidden_size))\n           \n        self.final = nn.Linear(hidden_size, out_size)\n                \n    def forward(self, x): \n\n        x = F.relu(self.dense0(x))\n\n        for i, block in enumerate(self.dense_blocks):                \n            x = F.relu(block(x))\n\n        x = self.final(x)\n        return x            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit(epochs, train_dl, valid_dl, model, loss_func, score_func, optimizer, scheduler):\n    losses, val_losses, scores = [], [], []\n    \n    for epoch in range(epochs):\n        t0 = time.time()\n        train_loss = 0.0\n        valid_loss = 0.0\n#         score = 0\n        \n        model.train()\n        for inputs, labels in train_dl:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            \n            loss = loss_func(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            train_loss += loss.item()\n\n        model.eval();\n        with torch.no_grad():\n            for inputs, labels in valid_dl:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n\n                valid_loss += loss_func(outputs, labels).item()\n#                 score += score_func(outputs, labels).item()\n        \n        train_loss /= len(train_dl)\n        valid_loss /= len(valid_dl)\n#         score      /= len(valid_dl)\n        \n        scheduler.step(valid_loss)\n        \n        print(f'[{epoch + 1}, {time.time() - t0:.1f}] train loss: {train_loss}, val loss: {valid_loss}') # , score: {score:.3f}')\n        losses.append(train_loss)\n        val_losses.append(valid_loss)\n#         scores.append(score)\n                \n    return losses, val_losses #, scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"model = Model(879, 1024, 206, 2)\nmodel.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = y_train.sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w = torch.from_numpy(y_train.shape[0] / weights.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = nn.BCEWithLogitsLoss(pos_weight=w.to(device)) # pos_weight=torch.from_numpy(w).to(device)\noptimizer = torch.optim.Adam(model.parameters(), 1e-2)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. No cross validation"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"losses_train, losses_val = fit(20, train_dl, valid_dl, model, loss, None, optimizer, scheduler=scheduler)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We overfit without the dropout layers"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.plot(range(len(losses_train)), losses_train, label='train');\nplt.plot(range(len(losses_val)), losses_val, label='val');\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Multilabel cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data.to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = inference_fn(model, test_dl, device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame(preds, columns=train_targets.columns[1:])\n\ntest_sig_ids.reset_index(drop=True, inplace=True)\nresults['sig_id'] = test_sig_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_subs = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = sample_subs[['sig_id']].merge(results, on='sig_id', how='left').fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}