{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master/')\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport os\nfrom matplotlib import pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nsns.set_style('ticks')\nsns.set_context(\"poster\")\nsns.set_palette('colorblind')\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\nfrom torch.utils.data import Dataset, DataLoader\nimport warnings\nwarnings.filterwarnings('ignore')\n# os.listdir('../input/lish-moa')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (20.0, 10.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = ('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Data preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'device': device,\n          'n_comp_g': 450, \n          'n_comp_c': 45, \n          'var_thresh': 0.67,\n          'epochs': 25,\n          'batch_size': 128,\n          'lr': 1e-3,\n          'weight_decay': 1e-5, \n          'n_folds': 7, \n          'early_stopping_steps': 10,\n          'early_stop': False,\n          'in_size': None,\n          'out_size': None,\n          'hidden_size': 1500}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv') # ../input/lish-moa/\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv') # ../input/lish-moa/\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv') # ../input/lish-moa/\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv') # ../input/lish-moa/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sample_submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_features = [col for col in train_features.columns if col.startswith('g-')]\nc_features = [col for col in train_features.columns if col.startswith('c-')]\n\ng_c_features = g_features + c_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1. Quantile transform"},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trans_train_features = transformer.fit_transform(train_features[g_c_features])\ntrans_test_features = transformer.transform(test_features[g_c_features])\n\ntrans_train_df = pd.DataFrame(trans_train_features, columns = g_c_features)\ntrans_test_df = pd.DataFrame(trans_test_features, columns = g_c_features)\n\ntrain_features = pd.concat([train_features.drop(columns=g_c_features), trans_train_df], axis=1)\ntest_features = pd.concat([test_features.drop(columns=g_c_features), trans_test_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_sample = random.sample(g_features, 3)\nc_sample = random.sample(c_features, 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = ['navy', 'r', 'g']\nfor col, color in zip(g_sample, colors):\n    plt.hist(test_features[col], bins=50, alpha=0.5, label=col)\n    plt.axvline(np.median(test_features[col]), linewidth=3, color=color, label='median_{}'.format(col))\nplt.xlim(-7, 7)\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"colors = ['navy', 'r', 'g']\nfor col, color in zip(c_sample, colors):\n    plt.hist(test_features[col], bins=50, alpha=0.5, label=col)\n    plt.axvline(np.median(test_features[col]), linewidth=3, color=color, label='median_{}'.format(col))\nplt.xlim(-7, 7)\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2. PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"def transfrom_all_data(transformer, train, test, feature_list):\n    \n    data = pd.concat([train[feature_list], test[feature_list]], axis=0).reset_index(drop=True)\n    n = train.shape[0]\n    \n    data_trans = transformer.fit_transform(data)\n    train_trans = data_trans[:n, :]\n    test_trans = data_trans[n:, :]\n    return train_trans, test_trans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_pca_features(n_comp, train, test, feature_list, name, normalize=False, scaler=None):\n    \n    pca = PCA(n_comp)\n    \n    train_pca, test_pca = transfrom_all_data(pca, train, test, feature_list)\n    \n    if normalize and scaler is not None:\n        train_pca = scaler.fit_transform(train_pca)\n        test_pca = scaler.transform(test_pca)\n    \n    for i in range(n_comp):\n        train['{0}_{1}'.format(name, i)] = train_pca[:, i]\n        test['{0}_{1}'.format(name, i)] = test_pca[:, i]\n        \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(data):\n    data['cp_time'] = data['cp_time'].map({24:0, 48:1, 72:2})\n    data['cp_dose'] = data['cp_dose'].map({'D1':0, 'D2':1})\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features, test_features = make_pca_features(params['n_comp_g'], train_features, test_features, g_features, 'g_pca')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features, test_features = make_pca_features(params['n_comp_c'], train_features, test_features, c_features, 'c_pca')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.3. Variance threshold"},{"metadata":{"trusted":true},"cell_type":"code","source":"var_thresh = VarianceThreshold(params['var_thresh'])\nto_thresh = train_features.columns[4:]\ncat_features = train_features.columns[:4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_thresh, test_thresh = transfrom_all_data(var_thresh, train_features, test_features, to_thresh)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"train_features = pd.concat([train_features[cat_features], pd.DataFrame(train_thresh)], axis=1)\ntest_features = pd.concat([test_features[cat_features], pd.DataFrame(test_thresh)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.4. Control group removal"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_mask = train_features['cp_type'] != 'ctl_vehicle'\ntrain_sig_ids = train_features.loc[train_mask]['sig_id']\ntrain = train_features.loc[train_mask].reset_index(drop=True)\n\ntest_mask = test_features['cp_type'] != 'ctl_vehicle'\ntest_sig_ids = test_features.loc[test_mask]['sig_id']\ntest = test_features.loc[test_mask].reset_index(drop=True)\n\ntrain_target_sigids = train_targets[['sig_id']]\ny_true  = train_targets.copy()\n\ntrain_targets = train_targets[train_targets['sig_id'].isin(train_sig_ids)].reset_index(drop=True)\ntrain_targets.drop(columns=['sig_id'], inplace=True)\ntrain_targets.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target_sigids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params['in_size'] = train.shape[1] - 2\nparams['out_size'] = train_targets.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params['out_size']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params['in_size']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.5. Cross validation split"},{"metadata":{"trusted":true},"cell_type":"code","source":"mskf = MultilabelStratifiedKFold(n_splits=params['n_folds'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"folds = train.copy()\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=train_targets)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folds.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TabularDataset:\n    \n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n    \n    def __len__(self):\n        return(self.X.shape[0])\n    \n    def __getitem__(self, i):\n        \n        X_i = torch.tensor(self.X[i, :], dtype=torch.float)\n        y_i = torch.tensor(self.y[i, :], dtype=torch.float)\n        \n        return X_i, y_i\n    \n    \n\nclass TabularDatasetTest:\n    \n    def __init__(self, X):\n        self.X = X\n    \n    def __len__(self):\n        return(self.X.shape[0])\n    \n    def __getitem__(self, i):\n        \n        X_i = torch.tensor(self.X[i, :], dtype=torch.float)        \n        return X_i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_func(model, optimizer, scheduler, loss_func, dataloader, device):\n    \n    train_loss = 0\n    \n    model.train()  \n    for inputs, labels in dataloader:        \n        optimizer.zero_grad()\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        loss = loss_func(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        train_loss += loss.item()\n        \n    train_loss /= len(dataloader)\n    \n    return train_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def valid_func(model, loss_func, dataloader, device):\n    \n    model.eval()\n    \n    valid_loss = 0\n    valid_preds = []\n    \n    for inputs, labels in dataloader:   \n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        loss = loss_func(outputs, labels)\n        \n        valid_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    valid_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return valid_loss, valid_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data.to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.25)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.25)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = preprocess(folds.drop(columns = ['sig_id', 'cp_type']))\n    \n    train_mask = train['kfold'] != fold\n    valid_idc = train.loc[~train_mask].index\n    \n    X_train = train.loc[train_mask].reset_index(drop=True)\n    y_train = train_targets.loc[train_mask].reset_index(drop=True)\n\n    \n    X_val = train.loc[~train_mask].reset_index(drop=True)\n    y_val = train_targets.loc[~train_mask].reset_index(drop=True)\n    \n    X_train.drop(columns=['kfold'], inplace=True)\n    X_val.drop(columns=['kfold'], inplace=True)\n    \n    test_ = preprocess(test.drop(columns = ['sig_id', 'cp_type']))\n\n    \n    train_ds = TabularDataset(X_train.values, y_train.values)\n    valid_ds = TabularDataset(X_val.values, y_val.values)\n    test_ds = TabularDatasetTest(test_.values)\n    \n    train_dl = DataLoader(train_ds, batch_size=params['batch_size'], shuffle=True)\n    valid_dl = DataLoader(valid_ds, batch_size=params['batch_size'], shuffle=False)\n    test_dl = DataLoader(test_ds, batch_size=params['batch_size'], shuffle=False)\n    \n    \n    model = Model(num_features=params['in_size'], num_targets=params['out_size'], \n                  hidden_size=params['hidden_size'] )\n    \n    model.to(params['device'])\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=params['epochs'], steps_per_epoch=len(train_dl))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing=0.001)\n    \n    early_stopping_steps = params['early_stopping_steps']\n    early_step = 0\n   \n    oof = np.zeros((train.shape[0], params['out_size']))\n    best_loss = np.inf\n    \n    for epoch in range(params['epochs']):\n        \n        train_loss = train_func(model, optimizer,scheduler, loss_tr, train_dl, params['device'])\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_func(model, loss_fn, valid_dl, params['device'])\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[valid_idc] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif(params['early_stop'] == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n\n    \n    model = Model(num_features=params['in_size'], num_targets=params['out_size'], \n                  hidden_size=params['hidden_size'] )\n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(params['device'])\n    \n    \n    predictions = np.zeros((test.shape[0], params['out_size']))\n    predictions = inference_fn(model, test_dl, params['device'])\n    \n    return oof, predictions\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(n_folds, seed):\n    oof = np.zeros((train.shape[0], params['out_size']))\n    predictions = np.zeros((test.shape[0], params['out_size']))\n    \n    for fold in range(n_folds):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / n_folds\n        oof += oof_\n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Averaging on multiple SEEDS\n\nseeds = [0, 1, 2, 3, 4, 5, 6]\noof = np.zeros((train.shape[0], params['out_size']))\npredictions = np.zeros((test.shape[0], params['out_size']))\n\nfor seed in seeds:\n    \n    oof_, predictions_ = run_k_fold(params['n_folds'], seed)\n    oof += oof_ / len(seeds)\n    predictions += predictions_ / len(seeds)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"valid_results = pd.concat([train_target_sigids[train_target_sigids['sig_id'].isin(train_sig_ids)].reset_index(drop=True), pd.DataFrame(oof)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_results = pd.concat([test[['sig_id']], pd.DataFrame(predictions, columns = sample_submission.columns[1:])], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_full = train_target_sigids.merge(valid_results, on='sig_id', how='left').fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"y_true = y_true.drop(columns=['sig_id']).values\ny_pred = valid_full.drop(columns=['sig_id']).values\n\nscore = 0\nfor i in range(y_true.shape[1]):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ / y_true.shape[1]\n    \nprint(\"CV log_loss: \", score)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sample_submission[['sig_id']].merge(test_results, on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}