{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Mechanisms of Action - ElasticNet"},{"metadata":{},"cell_type":"markdown","source":"In this notebook we use elastic net regularization to trian a deep neural network."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import log_loss\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.feature_selection import VarianceThreshold\n\nfrom keras import Sequential\nfrom keras.backend import clear_session\nfrom keras.layers import Dense, Dropout, BatchNormalization, Input\nfrom keras.optimizers import Adam\nfrom keras.losses import BinaryCrossentropy\n\nfrom tensorflow_addons.layers import WeightNormalization\nfrom tensorflow.keras import callbacks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load data\ntrain_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ntrain_non_scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_nonscored.csv')\n\ntrain_targets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Features shape:', train_features.shape)\nprint('Scored targets shape:', train_targets.shape)\nprint('Non-scored targets shape:', train_non_scored.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = train_features.set_index('sig_id')\ntrain_targets = train_targets.set_index('sig_id')\ntrain_non_scored = train_non_scored.set_index('sig_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preparation\nPreprocessing used:\n* Statistical features\n* Variance threshold\n* Scaling\n* PCA\n* Autoencoder features\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#features are in categories\ng_features = [x for x in train_features.columns if x.startswith('g-')]\nc_features = [x for x in train_features.columns if x.startswith('c-')]\nother_features = [x for x in train_features.columns if x not in g_features+c_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encode binary features\ntrain_features['cp_type'] = train_features['cp_type'].map({\n    'trt_cp' : 0,\n    'ctl_vehicle' : 1})\ntrain_features['cp_dose'] = train_features['cp_dose'].map({\n    'D1' : 0,\n    'D2' : 1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_features\ny = train_targets\ny_non_scored = train_non_scored\n\nX = pd.get_dummies(X, columns = ['cp_time'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Statistical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X['g_std'] = X[g_features].std(axis = 1)\nX['g_var'] = X[g_features].var(axis = 1)\nX['g_skew'] = X[g_features].skew(axis = 1)\nX['g_kurt'] = X[g_features].kurtosis(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X['c_std'] = X[c_features].std(axis = 1)\nX['c_var'] = X[c_features].var(axis = 1)\nX['c_skew'] = X[c_features].skew(axis = 1)\nX['c_kurt'] = X[c_features].kurtosis(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stat_features = ['g_std', 'g_var', 'g_skew', 'g_kurt', 'c_std', 'c_var', 'c_skew', 'c_kurt']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Variance Threshold"},{"metadata":{"trusted":true},"cell_type":"code","source":"selector = VarianceThreshold(0.85)\nselector_cols = g_features + c_features + stat_features\nselector.fit(X[selector_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop = [col for col, support in zip(selector_cols, selector.get_support()) if not support]\nX = X.drop(columns = drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#update features\ng_features_train = list(set(X.columns).intersection(set(g_features)))\nc_features_train = list(set(X.columns).intersection(set(c_features)))\nstat_features_train = list(set(X.columns).intersection(set(stat_features)))\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#rescaling\nscale_cols = g_features_train + c_features_train + stat_features_train\nscaler = StandardScaler()\nX[scale_cols] = scaler.fit_transform(X[scale_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rescaling\nscale_cols = g_features_train + c_features_train + stat_features_train\nscaler = StandardScaler()\nX[scale_cols] = scaler.fit_transform(X[scale_cols])\n\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract PCA features\nn_components = 100\npca = PCA(n_components = n_components)\npca_features = pca.fit_transform(X[g_features_train + c_features_train])\n\npca_cols = ['pca_'+str(i) for i in range(n_components)]\nX[pca_cols] = pca_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Autoencoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train an autoencoder then use the output of its encoder to extract features\nauto_input_cols = g_features_train + c_features_train\nlen(auto_input_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auto = tf.keras.Sequential([\n    tf.keras.layers.Dense(400, activation = 'relu', input_shape = (len(auto_input_cols),)),\n    tf.keras.layers.Dense(100, activation = 'relu'),\n    tf.keras.layers.Dense(400, activation = 'relu'),\n    tf.keras.layers.Dense(len(auto_input_cols))\n])\n\nauto.compile(optimizer = tf.keras.optimizers.Adam(),\n            loss = tf.keras.losses.MeanSquaredError())\n\nauto.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auto.fit(X[auto_input_cols], X[auto_input_cols],\n        epochs = 80,\n        batch_size = 128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the auto features from the encoder output\nauto_features = tf.keras.backend.function([auto.input], [auto.layers[1].output])([X[auto_input_cols].to_numpy(), 1])[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#add the auto features\nauto_cols = ['auto_' + str(n) for n in range(auto_features.shape[1])]\nX[auto_cols] = auto_features\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare the submission input data\ntest = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\ntest = test.set_index('sig_id')\n\ntest['cp_type'] = test['cp_type'].map({\n    'trt_cp' : 0,\n    'ctl_vehicle' : 1})\ntest['cp_dose'] = test['cp_dose'].map({\n    'D1' : 0,\n    'D2' : 1})\nX_test = pd.get_dummies(test, columns = ['cp_time'])\n\n#statistical features\nX_test['g_std'] = X_test[g_features].std(axis = 1)\nX_test['g_var'] = X_test[g_features].var(axis = 1)\nX_test['g_skew'] = X_test[g_features].skew(axis = 1)\nX_test['g_kurt'] = X_test[g_features].kurtosis(axis = 1)\nX_test['c_std'] = X_test[c_features].std(axis = 1)\nX_test['c_var'] = X_test[c_features].var(axis = 1)\nX_test['c_skew'] = X_test[c_features].skew(axis = 1)\nX_test['c_kurt'] = X_test[c_features].kurtosis(axis = 1)\n\n#variance threshold\nX_test = X_test.drop(columns = drop)\n\n#scaling\nX_test[scale_cols] = scaler.transform(X_test[scale_cols])\n\n#pca\npca_features = pca.transform(X_test[g_features_train + c_features_train])\nX_test[pca_cols] = pca_features\n\n#auto encoder\nauto_features = tf.keras.backend.function([auto.input], [auto.layers[1].output])([X_test[auto_input_cols].to_numpy(), 1])[0]\nX_test[auto_cols] = auto_features\n\nX_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling\n\nDefine the architecture of the neural network. The architecture and hyperparameters were chosen through repeated rounds of Bayesian optimisations in previous notebooks."},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(input_shape, output_shape, l1 = 1e-7, l2 = 1e-7, r = 1):\n    '''Function to create the network with given amount of regularization.'''\n    \n    reg = tf.keras.regularizers.L1L2(l1 = l1, l2 = l2)\n\n    model = Sequential()\n\n    model.add(Input(input_shape))\n\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n\n    model.add(WeightNormalization(Dense(int(2048*r), activation = 'selu',\n                                        kernel_initializer = 'lecun_normal',\n                                       kernel_regularizer = reg)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(WeightNormalization(Dense(int(1024*r), activation = 'selu',\n                                        kernel_initializer = 'lecun_normal',\n                                       kernel_regularizer = reg)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n\n    model.add(WeightNormalization(Dense(int(512*r), activation = 'selu',\n                                        kernel_initializer = 'lecun_normal',\n                                       kernel_regularizer = reg)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n\n    model.add(WeightNormalization(Dense(int(395*r), activation = 'selu',\n                                        kernel_initializer = 'lecun_normal',\n                                       kernel_regularizer = reg)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n\n    model.add(BatchNormalization())\n    model.add(WeightNormalization(Dense(output_shape, activation = 'sigmoid',\n                                       kernel_regularizer = reg)))\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_metric(y_true, y_pred):\n    '''Competition metric'''\n    \n    eps = tf.constant(1e-7, dtype = tf.float32)\n    \n    y_pred = tf.math.maximum(tf.math.minimum(y_pred, 1-eps),eps)\n    \n    log_loss = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n    return tf.reduce_mean(log_loss)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#callbacks for warmup training\nreduce_lr = callbacks.ReduceLROnPlateau(patience = 5, mode = 'min', monitor = 'val_custom_metric', factor=0.1, epsilon=1e-4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bayesian Optimisation\nOptimize the strengths of regularization and the size of the network with gp_minimize."},{"metadata":{"trusted":true},"cell_type":"code","source":"from skopt import gp_minimize\nfrom skopt.space.space import Real, Integer\nfrom skopt.utils import use_named_args","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dim_l1 = Real(low = 1e-7, high = 3e-7, name = 'l1')\ndim_l2 = Real(low = 9e-9, high = 3e-8, name = 'l2')\ndim_r = Real(low = 0.85, high = 1.15, name = 'r')\ndimensions = [dim_l1, dim_l2, dim_r]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@use_named_args(dimensions = dimensions)\ndef obj_fun(l1, l2, r):\n    \n    clear_session()\n        \n    #warmup model with no regularization\n    model = build_model(X_train.shape[1], y_train.shape[1], l1 = l1, l2 = l2, r = r)\n\n    model.compile(optimizer = Adam(2e-3),\n                  loss = BinaryCrossentropy(label_smoothing = 0.001),\n                  metrics = [custom_metric])\n\n    history = model.fit(X_train, y_train,\n                  batch_size = 128,\n                  epochs = 45,\n                  validation_data = (X_val, y_val),\n                  callbacks = [reduce_lr],\n                  verbose = 0)\n\n    return history.history['val_custom_metric'][-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#opt_result = gp_minimize(obj_fun, dimensions = dimensions, n_calls = 50, x0 = [2e-7, 1.48e-8, 1.0],verbose = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#optimal results\nl1 = 1.3e-07\nl2 = 2.27e-08\nr = 0.946078","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction\n\nBelow is the main loop containing training and inference. Performing a K-fold cross validation over different seeds predicting the test set after fitting to each fold."},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model to each fold and generate predictions\ny_preds = []\nval_log_losses = []\n\nverbose = 0\nbatch_size = 128\nruns = 5\nsplits = 7\nepochs = 45\n\nfor run in range(runs):\n\n    kf = KFold(n_splits = splits, shuffle = True)\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n\n        print('Run {} Fold {}'.format(run, fold))\n\n        #split data\n        X_train, X_val = X.to_numpy()[train_idx], X.to_numpy()[val_idx]\n        y_train, y_val = y.to_numpy()[train_idx], y.to_numpy()[val_idx]\n        y_non_scored_train, y_non_scored_val = y_non_scored.to_numpy()[train_idx], y_non_scored.to_numpy()[val_idx]\n        \n        clear_session()\n        \n        #warmup model with no regularization\n        model = build_model(X_train.shape[1], y_train.shape[1], l1 = l1, l2 = l2, r = r)\n        \n        model.compile(optimizer = Adam(2e-3),\n                      loss = BinaryCrossentropy(label_smoothing = 0.001),\n                      metrics = [custom_metric])\n        \n        history = model.fit(X_train, y_train,\n                      batch_size = batch_size,\n                      epochs = epochs,\n                      validation_data = (X_val, y_val),\n                      callbacks = [reduce_lr],\n                      verbose = verbose)\n        \n        val_log_losses.append(history.history['val_custom_metric'][-1])\n        \n        y_preds.append(model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(val_log_losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.mean(np.array(y_preds), axis = 0) #average the predictions from each fold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission\n\nSubmit the predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(predictions, columns = train_targets.columns)\nsubmission['sig_id'] = test.index\nsubmission = submission[['sig_id']+list(train_targets.columns)]\n\n#set ctl vehicle predictions to 0\nsubmission.loc[list(test.cp_type == 1), train_targets.columns] = 0\n\nprint(submission.shape)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}