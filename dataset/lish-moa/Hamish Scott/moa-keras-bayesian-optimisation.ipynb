{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Mechanism of Action Classification"},{"metadata":{},"cell_type":"markdown","source":"Idea: use Bayesian Optimisation to optimize a neural network. Then fit a network to multiple CV folds and average predictions."},{"metadata":{},"cell_type":"markdown","source":"## Version History\n\n* v6 first submission\n* v9 added line to set ctl_vehicle predictions to 0\n* v11 add weight normalization, switched optimisation to fix the number of layers and optimise number of neurons\n* v12 change architecture and convert to ensemble for prediction"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom keras import Sequential\nfrom keras.backend import clear_session\nfrom keras.layers import Dense, Dropout, BatchNormalization, Input\nfrom keras.optimizers import Adam\n\nfrom tensorflow_addons.layers import WeightNormalization\nfrom tensorflow.keras import callbacks\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import log_loss\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.multioutput import ClassifierChain, MultiOutputClassifier\n\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.utils import use_named_args","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load data\ntrain_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\n\ntrain_features = train_features.set_index('sig_id')\ntrain_targets = train_targets.set_index('sig_id')\n\ntrain_targets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Features shape:', train_features.shape)\nprint('Scored targets shape:', train_targets.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preparation"},{"metadata":{},"cell_type":"markdown","source":"To prepare the data we will choose features recommended by DmitryS https://www.kaggle.com/simakov/keras-multilabel-neural-network-v1-2. We will split the data into training and test sets and perform rescaling."},{"metadata":{"trusted":true},"cell_type":"code","source":"#features are in categories\ng_features = [x for x in train_features.columns if x.startswith('g-')]\nc_features = [x for x in train_features.columns if x.startswith('c-')]\nother_features = [x for x in train_features.columns if x not in g_features+c_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encode binary features\ntrain_features['cp_type'] = train_features['cp_type'].map({\n    'trt_cp' : 0,\n    'ctl_vehicle' : 1})\ntrain_features['cp_dose'] = train_features['cp_dose'].map({\n    'D1' : 0,\n    'D2' : 1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#top features for modelling\ntop_features = [  1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n        18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n        32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n        47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n        61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n        74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n       102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n       115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n       129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n       144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n       158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n       171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n       198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n       213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n       227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n       240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n       267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n       281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n       295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n       310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n       324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n       337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n       350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n       363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n       405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n       419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n       432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n       447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n       463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n       476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n       490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n       506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n       522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n       538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n       552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n       571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n       586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n       600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n       618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n       631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n       645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n       660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n       673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n       686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n       701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n       718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n       733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n       748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n       762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n       775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n       789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n       804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n       821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n       837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n       854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n       870, 871, 872, 873, 874]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare datasets for testing\nn_targets = train_targets.shape[1]\n\nX = train_features.iloc[:, top_features]\ny = train_targets.iloc[:,:n_targets]\n\nX = pd.get_dummies(X, columns = ['cp_time'])\n\nX.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scale_cols = list(set(X.columns).intersection(set(g_features + c_features)))\nscaler = StandardScaler()\nX[scale_cols] = scaler.fit_transform(X[scale_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 5)\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline model"},{"metadata":{},"cell_type":"markdown","source":"We will fit a nerual network with three hidden layers to give us an idea of the strength of the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"#baseline model has two layers of 1000 neurons\nmodel = Sequential()\n\nmodel.add(Input(X_train.shape[1]))\n\nmodel.add(BatchNormalization())\nmodel.add(WeightNormalization(Dense(1000, activation = 'selu', kernel_initializer = 'lecun_normal')))\nmodel.add(Dropout(0.2))\n\nmodel.add(BatchNormalization())\nmodel.add(WeightNormalization(Dense(500, activation = 'selu', kernel_initializer = 'lecun_normal')))\nmodel.add(Dropout(0.4))\n\nmodel.add(BatchNormalization())\nmodel.add(WeightNormalization(Dense(250, activation = 'selu', kernel_initializer = 'lecun_normal')))\nmodel.add(Dropout(0.4))\n\nmodel.add(BatchNormalization())\nmodel.add(WeightNormalization(Dense(y_train.shape[1], activation = 'sigmoid')))\n\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = callbacks.EarlyStopping(patience = 10, restore_best_weights = True)\nreduce_lr = callbacks.ReduceLROnPlateau(patience = 3, mode = 'min', monitor = 'val_loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train,\n         batch_size = 64,\n         epochs = 20,\n         validation_data = (X_test, y_test),\n         callbacks = [early_stopping, reduce_lr])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#metric for this comp is log loss\ndef score(y_test, y_pred):\n    metric = []\n    y_test, y_pred = np.array(y_test), np.array(y_pred)\n    for col in range(y_test.shape[1]):\n        metric.append(log_loss(y_test[:, col], y_pred[:, col].astype('float'), labels = [0, 1]))\n    return np.mean(metric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\ntest_loss = score(y_test, y_pred)\n\ntrain_loss = score(y_train, model.predict(X_train))\n\nprint('Train loss: {}'.format(train_loss))\nprint('Test loss: {}'.format(test_loss))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model performs fairly well however we need to optimise the hyperparameters to improve the test loss."},{"metadata":{},"cell_type":"markdown","source":"# Bayesian Optimisation"},{"metadata":{},"cell_type":"markdown","source":"We will attempt to improve the model using Bayesian Optimisation. We will try to tune the number of hidden layers, number of neurons in the layers (constrained to fall by a factor of two each layer) and the dropout rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to create the model given settings\ndef build_model(hidden_layers, neurons, dropout_rate):\n    model = Sequential()\n\n    model.add(Input(X_train.shape[1]))\n    \n    for i in range(hidden_layers):\n        model.add(BatchNormalization())\n        model.add(WeightNormalization(Dense(neurons // 2**i, activation = 'selu', kernel_initializer = 'lecun_normal')))\n        model.add(Dropout(dropout_rate))\n\n    model.add(BatchNormalization())\n    model.add(Dense(y_train.shape[1], activation = 'sigmoid'))\n\n    model.compile(optimizer = Adam(), loss = 'binary_crossentropy')\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dimensions for each variable to optimise\ndim_hidden_layers = Integer(low = 2, high = 4, name = 'hidden_layers')\ndim_neurons = Integer(low = 1000, high = 2000, name = 'neurons')\ndim_dropout_rate = Real(low = 0.2, high = 0.5, name = 'dropout_rate')\n#dim_learning_rate = Real(low = 1e-4, high = 1e-2, prior = 'log-uniform', name = 'learning_rate')\n#dim_batch_size = Integer(low = 4, high = 16, name = 'batch_size')\n#dim_epochs = Integer(low = 10, high = 20, name = 'epochs')\n\ndimensions = [\n    dim_hidden_layers,\n    dim_neurons,\n    dim_dropout_rate]\n\ndefault = [3, 1024, 0.3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the objective function that skopt will optimise\n@use_named_args(dimensions = dimensions)\ndef obj_fun(hidden_layers, neurons, dropout_rate):\n    model = build_model(\n        hidden_layers = hidden_layers,\n        neurons = neurons,\n        dropout_rate = dropout_rate)\n    \n    history = model.fit(X_train, y_train,\n                        batch_size = 64,\n                        epochs = 20,\n                        validation_data = (X_test, y_test),\n                        callbacks = [early_stopping, reduce_lr])\n    \n    logloss = score(y_test, model.predict(X_test))\n    \n    clear_session()\n    \n    return logloss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clear_session() #good to clear session each time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#perform the optimisation\nopt_result = gp_minimize(func = obj_fun,\n                         dimensions = dimensions,\n                         n_calls = 50,\n                         n_jobs = -1,\n                         x0 = default)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#put the results of the optimisation into a dataframe\nresults = pd.DataFrame(opt_result.x_iters,\n                      columns = [\n                          'hidden_layers',\n                          'neurons',\n                          'dropout_rate'])\nresults['obj_fun'] = opt_result.func_vals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#top 10 results\nresults.sort_values('obj_fun', ascending = True).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results show we have found a better solution than the baseline model. Despite this the results are slightly better than the baseline model. In future versions we may need to expand our search space or run for more optimisations."},{"metadata":{},"cell_type":"markdown","source":"# Submission model"},{"metadata":{},"cell_type":"markdown","source":"We will split the data into 7 folds and fit the model to each. The fitted models will predict the test set and the predictions will be averaged for submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits = 7, shuffle = True, random_state = 69)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare the submission input data\ntest = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\ntest = test.set_index('sig_id')\n\ntest['cp_type'] = test['cp_type'].map({\n    'trt_cp' : 0,\n    'ctl_vehicle' : 1})\ntest['cp_dose'] = test['cp_dose'].map({\n    'D1' : 0,\n    'D2' : 1})\n\nX_test = test.iloc[:, top_features]\nX_test = pd.get_dummies(X_test, columns = ['cp_time'])\nX_test[scale_cols] = scaler.transform(X_test[scale_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model to each fold and store val loss and test predictions\nval_losses = []\ny_preds = []\nfor i, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n    #split data\n    X_train, X_val = X.to_numpy()[train_idx], X.to_numpy()[val_idx]\n    y_train, y_val = y.to_numpy()[train_idx], y.to_numpy()[val_idx]\n    \n    clear_session()\n    #fit model with opt params\n    model = build_model(hidden_layers = opt_result.x[0],\n                        neurons = opt_result.x[1],\n                        dropout_rate = opt_result.x[2])\n\n    history = model.fit(X_train, y_train,\n             batch_size = 64,\n             epochs = 25,\n             validation_data = (X_val, y_val),\n             callbacks = [early_stopping, reduce_lr])\n    \n    val_losses.append(history.history['val_loss'][-1])\n    \n    #test predictions\n    y_pred = model.predict(X_test)\n    y_preds.append(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean val loss: {}'.format(np.mean(val_losses)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.mean(np.array(y_preds), axis = 0) #average the predictions from each fold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{},"cell_type":"markdown","source":"Prepare the file for submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(predictions, columns = train_targets.columns)\nsubmission['sig_id'] = test.index\nsubmission = submission[['sig_id']+list(train_targets.columns)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set ctl vehicle predictions to 0\nsubmission.loc[list(test.cp_type == 1), train_targets.columns] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(submission.shape)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}