{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# %% [code]\nimport os\nimport gc\nimport sys\nimport torch\nimport logging\nimport warnings\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.auto import tqdm as tqdm\nfrom sklearn.metrics import log_loss\n\nsys.path.append('../input/omegaconf')\nfrom omegaconf.omegaconf import DictConfig, OmegaConf\n\nlog = logging.getLogger(__name__)\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 2000)\n\n##########################\n\nsys.path.append('../input/iterativestratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nsys.path.append('../input/src-code0')\nsys.path.append('../input/models0')\n\nfrom src.load_preprocess import load_and_preprocess_data_index\nfrom src.cv.multilabel import DrugAwareMultilabelStratifiedKFold\nfrom src.torch_model_loop import run_k_fold, run_k_fold_nn, run_k_fold_nn_two_head\nfrom src.tree.xgb import get_xgboost\n\nos.listdir('../input/lish-moa')\n\n\ndef change_type(data):\n    for k, v in data.dtypes.items():\n        if v == 'float64':\n            data[k] = data[k].astype('float32')\n        if v == 'int64':\n            data[k] = data[k].astype('int8')\n    return data\n\n\n# @hydra.main(config_path=\"config\", config_name=\"config.yaml\", strict=False)\ndef run():\n    on_kaggle = True  # change me True if you use kaggle\n    pretrain_model = False\n    cfg = OmegaConf.load(f\"{'../input/src-code0' if on_kaggle else './'}/src/test.yaml\")\n    # os.chdir(utils.get_original_cwd())\n    #     log.info(OmegaConf.to_yaml(cfg))\n    cfg['device'] = 'cuda'\n    print(cfg['device'])\n    cfg['list_seed'] = [i for i in range(cfg.model.nseed)]\n    verbose = 1\n    local_path = '../'\n    path = f'../input/lish-moa'\n    path_model = f\"{'/kaggle/input/models0' if pretrain_model else '/kaggle/working' if on_kaggle else '../models'}\"\n    cfg['path_model'] = path_model\n\n    ######################################\n    # data_load and preprocess\n    ######################################\n    data_dict = load_and_preprocess_data_index(cfg, path, pca_append_test=True, variancethreshold_append_test=False, verbose=1)\n\n    CV = DrugAwareMultilabelStratifiedKFold(n_splits=cfg.model.nfolds, shuffle=False, random_state=42)\n    ##################################################\n    # Train\n    ##################################################\n    SEED = [0]\n    oof = np.zeros((len(data_dict['train']), len(data_dict['target_cols'])))\n    predictions = np.zeros((len(data_dict['test']), len(data_dict['target_cols'])))\n    for seed in tqdm(cfg['list_seed'], leave=verbose):\n        return_run_k_fold = run_k_fold_nn(data_dict, cfg, cv=CV, seed=seed, file_prefix='h1', pretrain_model=pretrain_model, verbose=verbose)\n        if not pretrain_model:\n            oof_, predictions_ = return_run_k_fold\n            oof += oof_ / cfg.model.nseed\n        else:\n            predictions_ = return_run_k_fold\n        predictions += predictions_ / cfg.model.nseed / 2\n        gc.collect()\n\n\n        return_run_k_fold = run_k_fold_nn_two_head(data_dict, cfg, cv=CV, seed=seed, file_prefix='m1', pretrain_model=pretrain_model, verbose=verbose)\n        if not pretrain_model:\n            oof_, predictions_ = return_run_k_fold\n            oof += oof_ / cfg.model.nseed\n        else:\n            predictions_ = return_run_k_fold\n        predictions += predictions_ / cfg.model.nseed / 2\n        gc.collect()\n\n\n    train = data_dict['train'].copy()\n    test = data_dict['test'].copy()\n    target = data_dict['target'].copy()\n    feature_cols = data_dict['feature_cols']\n    target_cols = data_dict['target_cols']\n    train_targets_scored = data_dict['train_targets_scored']\n    test_features = data_dict['test_features']\n\n    if not pretrain_model:\n        train[target_cols] = oof\n    test[target_cols] = predictions\n\n    ##################################################\n    # valodation and save\n    ##################################################\n\n    if not pretrain_model:\n        y_true = train_targets_scored[target_cols].values\n        valid_results = train_targets_scored.drop(columns=target_cols).merge(train[target_cols],\n                                                                             on='sig_id', how='left').fillna(0)\n        y_pred = valid_results[target_cols].values\n\n        score = 0\n        for i in range(len(target_cols)):\n            score_ = log_loss(y_true[:, i], y_pred[:, i])\n            score += score_ / len(target_cols)\n\n        print(f\"CV log_loss: {score}\")\n        log.info(f\"CV log_loss: {score}\")\n        log.info(f\"y_true.shape: {y_true.shape}\")\n        log.info(f\"y_pred.shape: {y_pred.shape}\")\n\n    # sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id'] + target_cols], on='sig_id',\n    #                                                         how='left').fillna(0)\n    # sub.to_csv('submission.csv', index=False)\n    # log.info(f\"sub.shape: {sub.shape}\")\n\n    res = test[target_cols]\n    corner_case = test_features[test_features['cp_type'] == 'ctl_vehicle']\n    zeros = np.zeros((corner_case.shape[0], len(target_cols)))\n    corner_case[target_cols] = zeros\n    corner_case = corner_case[target_cols]\n    res = pd.concat([res, corner_case], axis=0)\n\n    res.to_csv('submission.csv')\n    log.info(f\"res.shape: {res.shape}\")\n    log.info(f\"test[target_cols].shape: {test[target_cols].shape}\")\n\n    if not pretrain_model:\n        return score\n    else:\n        return 0\n    \n\nif __name__ == '__main__':\n    run()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}