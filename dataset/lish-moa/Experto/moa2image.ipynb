{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook uses real valued features and their pca transformed values to create a 40 * 40 image, then pass these images to a 3 convolutional layers and concanate its output vector with output of a linear layer which its inputs are cp_time & cp_dose.\nfull process of our work is shown in below image:\n\n<img src=\"https://i.ibb.co/HnK8NDD/cnn.png\" alt=\"cnn\" border=\"0\">"},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:30:45.440737Z","iopub.status.busy":"2020-11-24T19:30:45.439641Z","iopub.status.idle":"2020-11-24T19:31:18.039652Z","shell.execute_reply":"2020-11-24T19:31:18.038519Z"},"papermill":{"duration":32.640313,"end_time":"2020-11-24T19:31:18.039784","exception":false,"start_time":"2020-11-24T19:30:45.399471","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"!pip install /kaggle/input/iterate/iterative-stratification-master\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:18.10374Z","iopub.status.busy":"2020-11-24T19:31:18.102851Z","iopub.status.idle":"2020-11-24T19:31:20.230775Z","shell.execute_reply":"2020-11-24T19:31:20.229527Z"},"papermill":{"duration":2.164728,"end_time":"2020-11-24T19:31:20.230908","exception":false,"start_time":"2020-11-24T19:31:18.06618","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nfrom  torch.autograd import Variable\nfrom torch.optim import lr_scheduler\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nfrom torch.utils import data as torch_data \nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torchvision import transforms, utils\nimport torchvision \nimport torchvision.models as models\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom skimage import io, transform\n\n# from iterate.iterativestratification.iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport itertools\nfrom PIL import Image\nimport random\nimport pandas as pd\nfrom tqdm import tqdm\nimport csv\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport logging\nimport warnings\nimport gc\nwarnings.filterwarnings(\"ignore\")\nimport cv2 as cv\nimport copy\nfrom random import shuffle\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:20.290237Z","iopub.status.busy":"2020-11-24T19:31:20.289387Z","iopub.status.idle":"2020-11-24T19:31:20.296028Z","shell.execute_reply":"2020-11-24T19:31:20.295467Z"},"papermill":{"duration":0.038796,"end_time":"2020-11-24T19:31:20.296143","exception":false,"start_time":"2020-11-24T19:31:20.257347","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"seed = 2020\n\ncudnn.benchmark = False\ncudnn.deterministic = True\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:20.356625Z","iopub.status.busy":"2020-11-24T19:31:20.355295Z","iopub.status.idle":"2020-11-24T19:31:27.242133Z","shell.execute_reply":"2020-11-24T19:31:27.240879Z"},"papermill":{"duration":6.920846,"end_time":"2020-11-24T19:31:27.242276","exception":false,"start_time":"2020-11-24T19:31:20.32143","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"train_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('/kaggle/input/lish-moa/train_targets_nonscored.csv')\ntest_features = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\ntrain_drug = pd.read_csv('/kaggle/input/lish-moa/train_drug.csv')\nsample_submission = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:27.300487Z","iopub.status.busy":"2020-11-24T19:31:27.298579Z","iopub.status.idle":"2020-11-24T19:31:27.301215Z","shell.execute_reply":"2020-11-24T19:31:27.301756Z"},"papermill":{"duration":0.034301,"end_time":"2020-11-24T19:31:27.301889","exception":false,"start_time":"2020-11-24T19:31:27.267588","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:27.364156Z","iopub.status.busy":"2020-11-24T19:31:27.363145Z","iopub.status.idle":"2020-11-24T19:31:36.526358Z","shell.execute_reply":"2020-11-24T19:31:36.524743Z"},"papermill":{"duration":9.199349,"end_time":"2020-11-24T19:31:36.526492","exception":false,"start_time":"2020-11-24T19:31:27.327143","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"#RankGauss\nfor col in (GENES + CELLS):\n\n    transformer = QuantileTransformer(n_quantiles=100,random_state=seed, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:36.593066Z","iopub.status.busy":"2020-11-24T19:31:36.591204Z","iopub.status.idle":"2020-11-24T19:31:36.597039Z","shell.execute_reply":"2020-11-24T19:31:36.596193Z"},"papermill":{"duration":0.045089,"end_time":"2020-11-24T19:31:36.597189","exception":false,"start_time":"2020-11-24T19:31:36.5521","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# import seaborn as sns\n# sns.displot(train_features['g-3'].values)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:36.713907Z","iopub.status.busy":"2020-11-24T19:31:36.668235Z","iopub.status.idle":"2020-11-24T19:31:41.511994Z","shell.execute_reply":"2020-11-24T19:31:41.510713Z"},"papermill":{"duration":4.880402,"end_time":"2020-11-24T19:31:41.512146","exception":false,"start_time":"2020-11-24T19:31:36.631744","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# GENES\nn_comp = 650  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=seed).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:41.576697Z","iopub.status.busy":"2020-11-24T19:31:41.57535Z","iopub.status.idle":"2020-11-24T19:31:42.127422Z","shell.execute_reply":"2020-11-24T19:31:42.126792Z"},"papermill":{"duration":0.590395,"end_time":"2020-11-24T19:31:42.127577","exception":false,"start_time":"2020-11-24T19:31:41.537182","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"#CELLS\nn_comp = 100  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=seed).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:42.187577Z","iopub.status.busy":"2020-11-24T19:31:42.186625Z","iopub.status.idle":"2020-11-24T19:31:42.198216Z","shell.execute_reply":"2020-11-24T19:31:42.197679Z"},"papermill":{"duration":0.045465,"end_time":"2020-11-24T19:31:42.198313","exception":false,"start_time":"2020-11-24T19:31:42.152848","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"dic1 = {24:1, 72:2, 48:3}\ndic2 = {'D1':1, 'D2':2}\n\ntrain_features['cp_time'] = train_features.cp_time.map(dic1)\ntrain_features['cp_dose'] = train_features.cp_dose.map(dic2)\n\ntest_features['cp_time'] = test_features.cp_time.map(dic1)\ntest_features['cp_dose'] = test_features.cp_dose.map(dic2)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:42.255725Z","iopub.status.busy":"2020-11-24T19:31:42.254663Z","iopub.status.idle":"2020-11-24T19:31:42.258002Z","shell.execute_reply":"2020-11-24T19:31:42.257419Z"},"papermill":{"duration":0.033568,"end_time":"2020-11-24T19:31:42.258103","exception":false,"start_time":"2020-11-24T19:31:42.224535","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# bkup_df = train_features.copy()\ncols = train_features.columns","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:42.31748Z","iopub.status.busy":"2020-11-24T19:31:42.316634Z","iopub.status.idle":"2020-11-24T19:31:42.320098Z","shell.execute_reply":"2020-11-24T19:31:42.319463Z"},"papermill":{"duration":0.035531,"end_time":"2020-11-24T19:31:42.320193","exception":false,"start_time":"2020-11-24T19:31:42.284662","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def shuffle_cols(df,cols):\n    colnames     = list(cols)\n    colnames1    = colnames.copy()\n    colnames1    = np.random.RandomState(seed).permutation(colnames1[4:],)\n    colnames[4:] = colnames1\n    #pd.DataFrame({'columns':colnames}).to_csv('columns.csv',index=False)\n    df = df.reindex(columns=colnames)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:42.381565Z","iopub.status.busy":"2020-11-24T19:31:42.380213Z","iopub.status.idle":"2020-11-24T19:31:42.749246Z","shell.execute_reply":"2020-11-24T19:31:42.748674Z"},"papermill":{"duration":0.402073,"end_time":"2020-11-24T19:31:42.749398","exception":false,"start_time":"2020-11-24T19:31:42.347325","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# train_features  = shuffle_cols(train_features,cols)\n# test_features   = shuffle_cols(test_features,cols)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:42.824504Z","iopub.status.busy":"2020-11-24T19:31:42.823568Z","iopub.status.idle":"2020-11-24T19:31:42.826949Z","shell.execute_reply":"2020-11-24T19:31:42.826446Z"},"papermill":{"duration":0.05158,"end_time":"2020-11-24T19:31:42.827045","exception":false,"start_time":"2020-11-24T19:31:42.775465","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import torch.nn.functional as F\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3,stride=1)\n        self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1)\n        self.conv3 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=1,stride=1)\n        self.relu  = nn.ReLU(inplace=True)\n        self.bn1   = nn.BatchNorm2d(num_features=32)\n        self.bn2   = nn.BatchNorm2d(num_features=64)\n        self.bn3   = nn.BatchNorm2d(num_features=128)\n        self.mp    = nn.AvgPool2d(kernel_size=2,stride=1)\n        self.do    = nn.Dropout2d(p=0.3)\n        self.l1    = nn.Linear(139394,700)\n        self.l2    = nn.Linear(700, 206)\n        self.feat1 = nn.Linear(2,4)\n        self.feat2 = nn.Linear(4,2)\n        \n    def forward(self, x , z):\n        x = self.conv_layers1(x)\n        x = self.conv_layers2(x)\n        x = self.conv_layers3(x)\n        N ,_,_,_ = x.size()\n        x = x.view(N,-1)\n        z = self.feat1(z)\n        x = self.do(x)\n        z = self.feat2(z)\n        \n        w = torch.cat((x,z),1)\n        w = w.view(N,-1)\n        print(w.size())\n        w = self.l1(w)\n#         x = self.do(x)\n        w = self.relu(w)\n        w = self.l2(w)\n        return w\n    \n    def conv_layers1(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n#         x = self.bn1(x)\n        x = self.mp(x)\n#         x = self.do(x)\n        return x\n    \n    def conv_layers2(self, x):\n        x = self.conv2(x)\n        x = self.relu(x)\n        x = self.bn2(x)\n        x = self.mp(x)\n#         x = self.do(x)\n        return x\n    \n    def conv_layers3(self, x):\n        x = self.conv3(x)\n        x = self.relu(x)\n#         x = self.do(x)\n#         x = self.bn3(x)\n        x = self.mp(x)\n        return x\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:43.275461Z","iopub.status.busy":"2020-11-24T19:31:43.274559Z","iopub.status.idle":"2020-11-24T19:31:43.279419Z","shell.execute_reply":"2020-11-24T19:31:43.279903Z"},"papermill":{"duration":0.427244,"end_time":"2020-11-24T19:31:43.280073","exception":false,"start_time":"2020-11-24T19:31:42.852829","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"use_gpu = torch.cuda.is_available()\nprint(use_gpu)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:43.360165Z","iopub.status.busy":"2020-11-24T19:31:43.359109Z","iopub.status.idle":"2020-11-24T19:31:48.214374Z","shell.execute_reply":"2020-11-24T19:31:48.213726Z"},"papermill":{"duration":4.905047,"end_time":"2020-11-24T19:31:48.214521","exception":false,"start_time":"2020-11-24T19:31:43.309474","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"weight = train_targets_scored.iloc[:,1:].sum()\nwght = 1-(np.log(weight)/np.mean(weight))\nwght = wght.values\nwght = torch.tensor(wght).cuda()\n# wght","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:48.282552Z","iopub.status.busy":"2020-11-24T19:31:48.281722Z","iopub.status.idle":"2020-11-24T19:31:48.285786Z","shell.execute_reply":"2020-11-24T19:31:48.285316Z"},"papermill":{"duration":0.04367,"end_time":"2020-11-24T19:31:48.285884","exception":false,"start_time":"2020-11-24T19:31:48.242214","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = wght\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:48.351316Z","iopub.status.busy":"2020-11-24T19:31:48.350597Z","iopub.status.idle":"2020-11-24T19:31:48.354412Z","shell.execute_reply":"2020-11-24T19:31:48.354912Z"},"papermill":{"duration":0.042565,"end_time":"2020-11-24T19:31:48.355043","exception":false,"start_time":"2020-11-24T19:31:48.312478","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"class LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            # true_dist = pred.data.clone()\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    ","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:48.426501Z","iopub.status.busy":"2020-11-24T19:31:48.424875Z","iopub.status.idle":"2020-11-24T19:31:48.427749Z","shell.execute_reply":"2020-11-24T19:31:48.428262Z"},"papermill":{"duration":0.045522,"end_time":"2020-11-24T19:31:48.428378","exception":false,"start_time":"2020-11-24T19:31:48.382856","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def convert_to_img(row):\n    image = np.array(row[2:].values,dtype=float)\n    ax = image[4:1604]\n    \n    ax= ax.reshape((40,40))\n#     ax = np.random.shuffle(ax)\n    ax  = (ax - np.min(ax))/(np.max(ax)-np.min(ax))\n#     ax = ax * 255\n    ax = ax.reshape((-1, ax.shape[0], ax.shape[1]))\n    return ax\n\n\nclass myDataset(torch_data.Dataset):\n    def __init__(self, feats, labels):\n        self.features = feats\n        self.labels = labels\n        \n    def __len__(self):\n        return (self.features.shape[0])\n\n    def __getitem__(self, idx):\n        img_name = str(self.features.iloc[idx, 0])\n        img1 = convert_to_img(self.features.iloc[idx, :])\n        feats = self.features.iloc[idx, 2:4]\n        d = {\"img\": img_name}\n        d[\"label\"] = torch.tensor(self.labels.iloc[idx, 1:], dtype=torch.float) \n        d[\"img1\"] = torch.tensor(img1, dtype=torch.float)\n        d[\"feats\"] = torch.tensor(feats, dtype=torch.float)\n\n        \n        return d","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:48.487522Z","iopub.status.busy":"2020-11-24T19:31:48.486686Z","iopub.status.idle":"2020-11-24T19:31:48.491196Z","shell.execute_reply":"2020-11-24T19:31:48.491641Z"},"papermill":{"duration":0.036952,"end_time":"2020-11-24T19:31:48.491754","exception":false,"start_time":"2020-11-24T19:31:48.454802","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:48.552571Z","iopub.status.busy":"2020-11-24T19:31:48.551732Z","iopub.status.idle":"2020-11-24T19:31:48.555688Z","shell.execute_reply":"2020-11-24T19:31:48.555168Z"},"papermill":{"duration":0.037373,"end_time":"2020-11-24T19:31:48.555783","exception":false,"start_time":"2020-11-24T19:31:48.51841","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def create_train(X_train,Y_train):\n    train_dataset = myDataset(feats = X_train, labels=Y_train)\n    trainloader = torch_data.DataLoader(train_dataset, batch_size= 64, shuffle=True)\n    return trainloader\n\ndef create_test(X_test,Y_test):\n    val_dataset   = myDataset(feats = X_test, labels=Y_test)\n    valloader = torch_data.DataLoader(val_dataset, batch_size= 64, shuffle=False)\n    return valloader\n    ","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:48.618849Z","iopub.status.busy":"2020-11-24T19:31:48.613386Z","iopub.status.idle":"2020-11-24T19:31:48.641176Z","shell.execute_reply":"2020-11-24T19:31:48.640659Z"},"papermill":{"duration":0.058568,"end_time":"2020-11-24T19:31:48.641278","exception":false,"start_time":"2020-11-24T19:31:48.58271","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def train(epoches, model,dataloaders,f, num_label=206):\n    since = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    running_loss_train = []\n    running_loss_val = []\n    running_corrects_train = []\n    running_corrects_val = []\n    epoch_acc_hist = {\"train\": [], \"valc\": []}\n    epoch_loss_hist = {\"train\": [], \"val\": []}\n    \n    \n    LEARNING_RATE = 8e-2\n    WEIGHT_DECAY = 1e-3\n    loss_fn = nn.BCEWithLogitsLoss()\n    criterion = SmoothBCEwLogits(smoothing =0.001)\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n                                              pct_start=0.1,\n                                              div_factor=1e3,\n                                              max_lr=1e-2, \n                                              epochs=2, \n                                              steps_per_epoch=len(trainloader))\n    early_stopping_steps = 3\n    early_step = 1\n    \n    min_loss = 9999999\n    print(f\"Fold {f}/{n_splt}\")\n    print(\"- \" * 15)\n    for epoch in range(epoches):\n        print(f\"Epoch {epoch}/{epoches}\")\n\n        for phase in [\"train\", \"val\"]:\n            if phase == \"train\":\n                model.train()\n            else:\n                model.eval()\n\n            running_loss = 0.0\n            running_corrects = 0.0\n            \n            for batch_idx, batch_data in enumerate(tqdm(dataloaders[phase])):\n                img1  = Variable(batch_data[\"img1\"].float().cuda())\n                feats = Variable(batch_data[\"feats\"].float().cuda())\n                label = Variable(batch_data[\"label\"].float().cuda())\n                model.zero_grad()\n\n                with torch.set_grad_enabled(phase == \"train\"):\n                    out = model(img1,feats)\n                    batch_loss = 0.0\n                    batch_loss += criterion(out, label)\n\n                    if phase == \"train\":\n                        batch_loss.backward()\n                        optimizer.step()\n\n                running_loss += batch_loss * img1.size(0)\n                if phase == \"train\":\n                    running_loss_train.append(batch_loss.item())\n                elif phase == \"val\":\n                    running_loss_val.append(batch_loss.item())\n                batch_corrects = 0.0\n\n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_loss_hist[phase].append(epoch_loss)\n\n            print('-------------{} Loss: {:.4f}'.format(phase, epoch_loss))\n            \n \n                \n            if phase == \"val\" and epoch_loss < min_loss:\n                min_loss = epoch_loss\n                best_model_wts = copy.deepcopy(model.state_dict())\n                torch.save({\"current_model_wts\": model.state_dict(),\n                            \"best_model_wts\": best_model_wts,\n                            \"current_epoch\": epoch,\n                            \"optimizer_wts\": optimizer.state_dict(),\n                            \"running_loss_train\": running_loss_train,\n                            \"running_loss_val\": running_loss_val,\n                            \"epoch_loss_hist\": epoch_loss_hist, },\n                            \"moa_model_fold_{}.pth\".format(str(f)))\n        if epoch_loss >= min_loss:\n            early_step = early_step + 1\n        if early_step == early_stopping_steps:\n            break\n                \n    gc.collect()\n    time_elapsed = time.time() - since\n    print('Fold Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print('Min val Loss: {:4f}'.format(min_loss))\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:31:48.875247Z","iopub.status.busy":"2020-11-24T19:31:48.732612Z","iopub.status.idle":"2020-11-24T19:42:41.161862Z","shell.execute_reply":"2020-11-24T19:42:41.160812Z"},"papermill":{"duration":652.494036,"end_time":"2020-11-24T19:42:41.162106","exception":false,"start_time":"2020-11-24T19:31:48.66807","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"n_splt = 6\nmskf = MultilabelStratifiedKFold(n_splits=n_splt)\nepoches = 5\nmodel = None\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train_features, y=train_targets_scored)):\n    train_df = train_features.loc[t_idx,:].reset_index(drop=True)\n    train_y_df = train_targets_scored.loc[t_idx,:].reset_index(drop=True)\n    \n    valid_df = train_features.loc[v_idx,:].reset_index(drop=True)\n    valid_y_df = train_targets_scored.loc[v_idx,:].reset_index(drop=True)\n    \n    trainloader = create_train(train_df,train_y_df)\n    validloader = create_train(valid_df,valid_y_df)\n    \n    dataloaders = {\"train\": trainloader, \"val\": validloader}\n#     dataset_sizes = {\"train\":  len(train_dataset), \"val\": len(val_dataset)}\n    if model != None:\n        del model\n    torch.cuda.empty_cache()\n\n    model = Model()\n    if use_gpu:\n        model = model.cuda()\n        \n    train(epoches, model, dataloaders,f, num_label = 206)\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:42:44.866145Z","iopub.status.busy":"2020-11-24T19:42:44.865064Z","iopub.status.idle":"2020-11-24T19:42:44.874292Z","shell.execute_reply":"2020-11-24T19:42:44.873496Z"},"papermill":{"duration":2.101223,"end_time":"2020-11-24T19:42:44.874476","exception":false,"start_time":"2020-11-24T19:42:42.773253","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def inference_fn(model, dataloader):\n    model.eval()\n    preds  = []\n    labels = []\n    \n    for batch_idx, batch_data in enumerate(tqdm(dataloader)):\n        img1  = Variable(batch_data[\"img1\"].float().cuda())\n        feats = Variable(batch_data[\"feats\"].float().cuda())\n        label = Variable(batch_data[\"label\"].float().cuda())\n#         inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(img1,feats)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        labels.append(label.cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    labels = np.concatenate(labels)\n    return preds,labels","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:42:51.702095Z","iopub.status.busy":"2020-11-24T19:42:51.701151Z","iopub.status.idle":"2020-11-24T19:42:51.703131Z","shell.execute_reply":"2020-11-24T19:42:51.70362Z"},"papermill":{"duration":1.384425,"end_time":"2020-11-24T19:42:51.703745","exception":false,"start_time":"2020-11-24T19:42:50.31932","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"validloader = create_train(test_features,sample_submission)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:42:54.549716Z","iopub.status.busy":"2020-11-24T19:42:54.548775Z","iopub.status.idle":"2020-11-24T19:44:45.761785Z","shell.execute_reply":"2020-11-24T19:44:45.759123Z"},"papermill":{"duration":112.659435,"end_time":"2020-11-24T19:44:45.762095","exception":false,"start_time":"2020-11-24T19:42:53.10266","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"for i in range(n_splt):\n    if model != None:\n        del model\n    torch.cuda.empty_cache()\n    \n    model = Model()\n    model.eval()\n    model.load_state_dict(torch.load(f\"moa_model_fold_{i}.pth\")['current_model_wts'])\n    \n\n    model.to(device)\n\n    preds,labels = inference_fn(model, validloader)\n    preds += preds\nsubmission = preds/n_splt","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:44:49.571504Z","iopub.status.busy":"2020-11-24T19:44:49.570627Z","iopub.status.idle":"2020-11-24T19:44:49.57624Z","shell.execute_reply":"2020-11-24T19:44:49.575648Z"},"papermill":{"duration":1.971433,"end_time":"2020-11-24T19:44:49.576341","exception":false,"start_time":"2020-11-24T19:44:47.604908","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"preds","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-11-24T19:44:53.215866Z","iopub.status.busy":"2020-11-24T19:44:53.210282Z","iopub.status.idle":"2020-11-24T19:44:56.322307Z","shell.execute_reply":"2020-11-24T19:44:56.32176Z"},"papermill":{"duration":5.007153,"end_time":"2020-11-24T19:44:56.322432","exception":false,"start_time":"2020-11-24T19:44:51.315279","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# preds = preds - (np.mean(preds)/3)\npreds[preds<0] = 0.000123\nsample_submission.iloc[:,1:] = preds\nsample_submission = sample_submission.fillna(0)\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head(100)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":1.577298,"end_time":"2020-11-24T19:44:59.547058","exception":false,"start_time":"2020-11-24T19:44:57.96976","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}