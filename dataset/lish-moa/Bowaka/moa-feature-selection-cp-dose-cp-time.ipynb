{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThis notebook aims at exploring the possibilities offers to make feature selection based on the features cp_dose and cp_time.\n\nThe idea is that for a given target, dose and time shall be somehow correlated to active features for that target."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nimport random\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm\n\nimport plotly.graph_objects as go\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score, roc_curve\nfrom lightgbm import LGBMClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/lish-moa/train_features.csv').drop('sig_id', axis=1)\ndf_target = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv').drop('sig_id', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GC = [col for col in df_train if ('g-' in col) | ('c-' in col)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict cp_dosage"},{"metadata":{},"cell_type":"markdown","source":"### Trying to predict each target separately\n\nIn this part, I take only samples related to one target, and I try to predict the dose based on the other training features"},{"metadata":{"trusted":true},"cell_type":"code","source":"rocs = {}\nfeature_importance = {}\n\nfor target in tqdm(df_target.columns):\n    \n    X = df_train.loc[df_target[target]==1, GC]\n    y = df_train.loc[df_target[target]==1, 'cp_dose']\n    lenc = LabelEncoder()\n    y = lenc.fit_transform(y)\n    \n    if (len(X)>= 10) & (len(np.unique(y)) == 2):\n        kf = StratifiedKFold(n_splits=5)\n        preds = []\n        feat_imp = []\n        yy = []\n        for train_ind, test_ind in kf.split(X,y):\n            \n            Xtrain, Xtest, ytrain, ytest = X.iloc[train_ind], X.iloc[test_ind], y[train_ind], y[test_ind]\n            model = LGBMClassifier(n_estimators=100)\n            model.fit(Xtrain, ytrain)\n            p = model.predict_proba(Xtest)[:,1]\n            preds.append(p)\n            yy.append(ytest)\n            feat_imp.append(model.feature_importances_)\n            \n        yy = np.hstack(yy)\n        preds = np.hstack(preds)\n        rocs[target] = roc_auc_score(yy, preds)\n        feature_importance[target] = np.mean(feat_imp,axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### AUC Scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"rocs_serie = pd.Series(rocs).sort_values(ascending = False)\nfig = go.Figure(\n    go.Bar(\n        x = rocs_serie.index,\n        y = rocs_serie.values\n    )\n)\nfig.update_layout(template = 'presentation', title = 'AUC Score')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature importance for targets with AUC > 0.8"},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\nfrom sklearn.decomposition import PCA, KernelPCA\n\nacc_cols = rocs_serie[rocs_serie>0.8].index\nfi_dose = pd.DataFrame(feature_importance, index = GC)\nfi_dose = fi_dose[acc_cols]\n\nfor col in fi_dose.columns:\n    \n    fig = go.Figure(\n        go.Bar(\n            x = fi_dose.index,\n            y = fi_dose[col]\n        )\n    )\n    fig.update_layout(template = 'presentation', title = col, height = 300)\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Top feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_fi = fi_dose.copy()\ntop_fi[top_fi<=10] = 0\ntop_fi[top_fi!=0] = 1\ntop_fi = top_fi.sum(axis=1)/len(top_fi.columns)\ntop_fi = top_fi[top_fi>0].sort_values(ascending = False)[:40]\nfig = go.Figure(\n    go.Bar(\n        x = top_fi.index,\n        y = top_fi.values\n    )\n)\nfig.update_layout(template = 'presentation', title = 'Top Features to predict cp_dose', height = 300)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing \"obvious\" features\n\nFrom the figure above, we see that some features are presents for all targets.\nTo make sure that we keep only features relevant to each target, we run the same approach on all samples tagged as **cp_type = ctr_vehicle** and we will remove the active features from our training set"},{"metadata":{},"cell_type":"markdown","source":"#### Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_train.loc[df_train.cp_type != 'trt_cp', GC]\ny = df_train.loc[df_train.cp_type != 'trt_cp', 'cp_dose']\nlenc = LabelEncoder()\ny = lenc.fit_transform(y)\n\ntarget = 'no_target'\nif (len(X)> 10) & (len(np.unique(y)) == 2):\n\n    Xtrain, Xtest, ytrain, ytest = train_test_split(X,y, test_size = 0.2, stratify = y)\n    model = LGBMClassifier(n_estimators=100)\n    model.fit(Xtrain, ytrain)\n    p = model.predict_proba(Xtest)[:,1]\n    roc = roc_auc_score(ytest, p)\n    \nprint(roc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(roc_auc_score(ytest, p))\nprint(accuracy_score(ytest, np.round(p)))\n\nfi = pd.Series(model.feature_importances_, index = GC)\nfi = fi[fi>10] # keep only high importance features\nfi = fi.sort_values(ascending = False)\nfig = go.Figure(\n    go.Bar(\n        x = fi.index,\n        y = fi.values\n    )\n)\nfig.update_layout(template = 'presentation', title = 'feature importance for cp_type != trt_cp')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So it appears here that the featuress g-307 and g-370 are actually very important, even when there is no actual drugs in the samples.\nWe will rerun the analysis on the targets an remove those \"obvious\" targets"},{"metadata":{},"cell_type":"markdown","source":"#### Removing features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove high feature importances\nbfi = fi[fi>10].index\nGC2 = [elmt for elmt in GC if elmt not in bfi]\nprint(len(GC))\nprint(len(GC2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Active targets\n\nNow that we removed features good at predicting **ctr_vehicle** samples, we try to predict cp_dose for the samples of each target separately"},{"metadata":{},"cell_type":"markdown","source":"#### Models training"},{"metadata":{"trusted":true},"cell_type":"code","source":"rocs = {}\n\nfeature_importance = {}\n\nfor target in tqdm(df_target.columns):\n    \n    X = df_train.loc[df_target[target]==1, GC2]\n    y = df_train.loc[df_target[target]==1, 'cp_dose']\n    lenc = LabelEncoder()\n    y = lenc.fit_transform(y)\n    \n    if (len(X)>= 10) & (len(np.unique(y)) == 2):\n        kf = StratifiedKFold(n_splits=5)\n        preds = []\n        feat_imp = []\n        yy = []\n        for train_ind, test_ind in kf.split(X,y):\n            \n            Xtrain, Xtest, ytrain, ytest = X.iloc[train_ind], X.iloc[test_ind], y[train_ind], y[test_ind]\n            model = LGBMClassifier(n_estimators=100)\n            model.fit(Xtrain, ytrain)\n            p = model.predict_proba(Xtest)[:,1]\n            preds.append(p)\n            yy.append(ytest)\n            feat_imp.append(model.feature_importances_)\n            \n        yy = np.hstack(yy)\n        preds = np.hstack(preds)\n        rocs[target] = roc_auc_score(yy, preds)\n        feature_importance[target] = np.mean(feat_imp,axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ROCs\n\nAbout 30 set of samples reach a ROC > 0.8. For those samples, it is possible to predict correctly cp_dosage based only on the other training_features "},{"metadata":{"trusted":true},"cell_type":"code","source":"rocs_serie = pd.Series(rocs).sort_values(ascending = False)\nfig = go.Figure(\n    go.Bar(\n        x = rocs_serie.index,\n        y = rocs_serie.values\n    )\n)\nfig.update_layout(template = 'presentation', title = 'AUC Score')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### How many samples are included in that batch of high AUC? \n\nThe below figure shows that this analysis might be worth for more than 60% of the total dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"rep = df_target[rocs_serie.index].sum(axis=1).apply(lambda x:1 if x>0 else 0)\nrep = rep[df_train.cp_type == 'trt_cp']\nrep = pd.DataFrame(rep.groupby(rep).count()).rename(columns = {0: 'count'})\nrep['names'] = ['with AUC < 0.8', 'with AUC > 0.8']\nfig = px.pie(rep, values = 'count', names='names')\nfig.update_layout(title = 'Repartion of samples by AUC for target cp_dose', template = 'presentation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature importance\n\nWe select the sets of samples with ROC > 0.8 and plot the feature importance.\nWe notice that the important features are not the same for each target, giving a potential clue that these are the features reacting to the target."},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\nfrom sklearn.decomposition import PCA, KernelPCA\n\nacc_cols = rocs_serie[rocs_serie>0.8].index\nfi_dose = pd.DataFrame(feature_importance, index = GC2)\nfi_dose = fi_dose[acc_cols]\n\nfor col in fi_dose.columns:\n    \n    fig = go.Figure(\n        go.Bar(\n            x = fi_dose.index,\n            y = fi_dose[col]\n        )\n    )\n    fig.update_layout(template = 'presentation', title = col, height = 300)\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### What features comes back the more ?\n\nWe see here that after removing the \"obvious\" features, the frequency of apparition of top features is much lower, which mean that each target has its own feature importance.\nThose might be well related to the the target themselves"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_fi = fi_dose.copy()\ntop_fi[top_fi<=10] = 0\ntop_fi[top_fi!=0] = 1\ntop_fi = top_fi.sum(axis=1)/len(top_fi.columns)\ntop_fi = top_fi[top_fi>0].sort_values(ascending = False)[:40]\nfig = go.Figure(\n    go.Bar(\n        x = top_fi.index,\n        y = top_fi.values\n    )\n)\nfig.update_layout(template = 'presentation', title = '% of time a feature is important for the remaining targets', height = 300)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict cp_time\n\nWe restart as above, but this time trying to predict the cp_time feature. I start directly by removing \"obvious\" targets for cp_time"},{"metadata":{},"cell_type":"markdown","source":"### No target (cp_type == ctr_vehicle)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_train.loc[df_train.cp_type != 'trt_cp', GC]\ny = df_train.loc[df_train.cp_type != 'trt_cp', 'cp_time']\nlenc = LabelEncoder()\ny = lenc.fit_transform(y)\n\ntarget = 'no_target'\nif (len(X)> 10) & (len(np.unique(y)) == 3):\n    Xtrain, Xtest, ytrain, ytest = train_test_split(X,y, test_size = 0.2, stratify = y)\n    model = LGBMClassifier()\n    model.fit(Xtrain, ytrain)\n    p = model.predict_proba(Xtest)\n    roc = roc_auc_score(ytest, p, multi_class = 'ovr')\n    feature_importance = model.feature_importances_\n    \nprint(roc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature importance (ctr_vehicle)"},{"metadata":{"trusted":true},"cell_type":"code","source":"fi = pd.Series(model.feature_importances_, index = GC)\nfi = fi.sort_values(ascending = False)\nfig = go.Figure(\n    go.Bar(\n        x = fi.index,\n        y = fi.values\n    )\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Removing features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove high feature importances\nbfi = fi[fi>10].index\nGC2 = [elmt for elmt in GC if elmt not in bfi]\nprint(len(GC))\nprint(len(GC2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predict cp_time for real targets"},{"metadata":{"trusted":true},"cell_type":"code","source":"rocs = {}\n\nfeature_importance = {}\n\nfor target in tqdm(df_target.columns):\n    \n    X = df_train.loc[df_target[target]==1, GC2]\n    y = df_train.loc[df_target[target]==1, 'cp_time']\n    lenc = LabelEncoder()\n    y = lenc.fit_transform(y)\n    \n    if (len(X)>= 15) & (len(np.unique(y)) == 3):\n        kf = StratifiedKFold(n_splits=5)\n        preds = []\n        feat_imp = []\n        yy = []\n        for train_ind, test_ind in kf.split(X,y):\n            \n            Xtrain, Xtest, ytrain, ytest = X.iloc[train_ind], X.iloc[test_ind], y[train_ind], y[test_ind]\n            model = LGBMClassifier(n_estimators=100)\n            model.fit(Xtrain, ytrain)\n            p = model.predict_proba(Xtest)\n            preds.append(p)\n            yy.append(ytest)\n            feat_imp.append(model.feature_importances_)\n            \n        yy = np.hstack(yy)\n        preds = np.vstack(preds)\n        rocs[target] = roc_auc_score(yy, preds, multi_class='ovr')\n        feature_importance[target] = np.mean(feat_imp,axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ROC"},{"metadata":{"trusted":true},"cell_type":"code","source":"rocs_serie = pd.Series(rocs).sort_values(ascending = False)\nfig = go.Figure(\n    go.Bar(\n        x = rocs_serie.index,\n        y = rocs_serie.values\n    )\n)\nfig.update_layout(template = 'presentation', title = 'AUC Score')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\nfrom sklearn.decomposition import PCA, KernelPCA\n\nacc_cols = rocs_serie[rocs_serie>0.8].index\nfi_time = pd.DataFrame(feature_importance, index = GC2)\nfi_time = fi_time[acc_cols]\n\nfor col in fi_time.columns:\n    \n    fig = go.Figure(\n        go.Bar(\n            x = fi_time.index,\n            y = fi_time[col]\n        )\n    )\n    fig.update_layout(template = 'presentation', title = col, height = 300)\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### What features comes back the more ?\n\nWe see here that after removing the \"obvious\" features, the frequency of apparition of top features is much lower, which mean that each target has its own feature importance.\nThose might be well related to the the target themselves"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_fi = fi_time.copy()\ntop_fi[top_fi<=10] = 0\ntop_fi[top_fi!=0] = 1\ntop_fi = top_fi.sum(axis=1)/len(top_fi.columns)\ntop_fi = top_fi[top_fi>0].sort_values(ascending = False)[:40]\nfig = go.Figure(\n    go.Bar(\n        x = top_fi.index,\n        y = top_fi.values\n    )\n)\nfig.update_layout(template = 'presentation', title = '% of time a feature is important for the remaining targets', height = 300)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Are the same targets detected ?\n\nVariations in **cp_dosage** seems easier to predict compared to **cp_time**\nOn the other end, we see that targets with high ROC score for **cp_time** are all included in targets with high ROC for **cp_dosage**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'total number of targets for target = dose : {len(fi_dose.columns)}')\nprint(fi_dose.columns)\nprint('\\n')\nprint(f'total number of targets for target = time : {len(fi_time.columns)}')\nprint(fi_time.columns)\nprint('\\n')\ncommon = list(set(fi_dose.columns).intersection(set(fi_time.columns)))\nprint('\\n')\nprint('targets in common in both sets:')\nprint(common)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Are the features extracted the same ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in common:\n    fig = go.Figure()\n    fig.add_trace(\n        go.Bar(\n            x = fi_dose.index,\n            y = fi_dose[col],\n            name = 'dose'\n        )   \n    )\n    fig.add_trace(\n        go.Bar(\n            x = fi_time.index,\n            y = fi_time[col],\n            name = 'time'\n        )   \n    )\n    fig.update_layout(template = 'presentation', height = 300, title = col)\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using this method to remove outliers ?"},{"metadata":{},"cell_type":"markdown","source":"Let's visualise the PCA of the features selected by our cp_dose classifier for each target"},{"metadata":{"trusted":true},"cell_type":"code","source":"for target in fi_dose.columns:\n    try:\n        plt.figure(figsize = (25,5))\n        plt.subplot(1,2,1)\n        plt.title(target+ 'importance > 10')\n        test = fi_dose[target]\n        features = test[test>10].index\n\n        sub = df_train.loc[(df_target[target]==1), features]\n        pca = PCA(2)\n        pca.fit(sub)\n\n        \n        for d in df_train.cp_dose.unique():\n            sub_df = df_train.loc[(df_target[target]==1) & (df_train.cp_dose==d), features]\n            xpca = pca.transform(sub_df)\n            plt.scatter(xpca[:,0],xpca[:,1], label = d)\n        plt.legend()\n        plt.subplot(1,2,2)\n        test = fi_dose[target]\n        features = test[test>5].index\n\n        sub = df_train.loc[(df_target[target]==1), features]\n        pca = PCA(2)\n        pca.fit(sub)\n\n        plt.title(target + 'importance > 5')\n        for d in df_train.cp_dose.unique():\n            sub_df = df_train.loc[(df_target[target]==1) & (df_train.cp_dose==d), features]\n            xpca = pca.transform(sub_df)\n            plt.scatter(xpca[:,0],xpca[:,1], label = d)\n        plt.legend()\n        plt.show()\n    except:\n        pass","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}