{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThe purpose of this notebook is to assess if a single NN is the best option to predict the 206 targets\n\n### Primilinary results\n\nIt seems that single target NN outperform the classic multi-target NN for several targets (see below).\nUnfortunately, trying to blend the two models tend to degrade a lot my LB. If anybody as a clue on where this can come from, please let me know !\n\n#### Single Target NN: \n* KFold Logloss: **0.01687**   \n* KFold ROC: **0.779** \n* LB: **0.01929**\n\n#### Multi Target NN:\n \n* KFold Logloss: **0.01714**  \n* KFold ROC: **0.634** \n* LB: **0.01855**\n\n\n### Updates\n\n#### Version 7\nChange average probability of OOF graph from bar to scatter chart\n\n#### Version 8\nAdd predictions from public test set\n\n#### Version 9\nAdd LB Score for the two models\n\n#### Version 11\n- Add analysis for main cluster of target and targets without co-occurence with others\n- Add recall/precision analysis\n\nPlot difference of predictions between train and test set in relative rather than in absolute"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport networkx as nx\n\nimport matplotlib.pyplot as plt\n\nimport random","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part-1: Co-occurences between targets\n\nTrain a multi-target Neural Network might be usefull if targets are correlated with each other ( several samples have the same 1)\nIn this part, I explore the graph of co-occurences and isolated group of targets having co-occurances in the train set. I will use both scored and non-scored targets"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Load data\ntrain_targets_scored = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\ntrain_targets_nonscored = pd.read_csv(\"../input/lish-moa/train_targets_nonscored.csv\")\ntrain_features = pd.read_csv(\"../input/lish-moa/train_features.csv\")\n# Group targets\ntarget_scored = train_targets_scored.drop(\"sig_id\",axis=1)\ntarget_notscored= train_targets_nonscored.drop(\"sig_id\",axis=1)\n\ntarget = pd.concat([target_scored,target_notscored],axis=1)\ntarget = target.loc[:,target.sum()>0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Co-occurences\n\nTo calculate the co-occurences in a fast way, we multiply target by its transposed\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count the number of co-occurence of ones in the target, remove the diag elements\ncommun = target.T@target\n\ncommun.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The diagonal values represent the total number of occurence of one target in the dataset, we are not interested by it, so we set it to 0.\nWe set all co-occurences to 1, they symbolise the connexions between two targets"},{"metadata":{},"cell_type":"markdown","source":"## Visualisation of the network with NetworkX and Plotly"},{"metadata":{"trusted":true},"cell_type":"code","source":"G = nx.from_pandas_adjacency(commun,create_using=nx.DiGraph)\npos = nx.spring_layout(G)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"edge_x = []\nedge_y = []\n\nfor edge in G.edges():\n        \n    x0, y0 = pos[edge[0]]\n    x1, y1 = pos[edge[1]]\n    edge_x.append(x0)\n    edge_x.append(x1)\n    edge_x.append(None)\n    edge_y.append(y0)\n    edge_y.append(y1)\n    edge_y.append(None)\n\nedge_trace = go.Scatter(\n    x=edge_x, y=edge_y,\n    opacity = 0.5,\n    line=dict(width=2, color='black'),\n    hoverinfo='none',\n    mode='lines')\n\nnode_x = []\nnode_y = []\nfor node in G.nodes():\n    x, y = pos[node]\n    node_x.append(x)\n    node_y.append(y)\n    \nnode_trace = go.Scatter(\n    x=node_x, y=node_y,\n    mode='markers',\n    hoverinfo='text',\n    marker=dict(\n        showscale=True,\n        colorscale='Portland',\n        opacity = 0.3,\n        reversescale=True,\n        color=[],\n        size=20,\n        line_width=2))\n\nnode_adjacencies = []\nnode_text = []\nnode_sizes = []\nfor node, adjacencies in enumerate(G.adjacency()):\n    node_adjacencies.append(len(adjacencies[1]))\n    node_text.append(adjacencies[0])\n    node_sizes.append(np.log(target[adjacencies[0]].sum())*5)\n\nnode_trace.marker.color = node_adjacencies\nnode_trace.text = node_text\nnode_trace.marker.size = node_sizes\n\nfig = go.Figure(data=[edge_trace, node_trace],\n             layout=go.Layout(\n                title='<br>Targets Co-occurences Network',\n                titlefont_size=16,\n                showlegend=False,\n                hovermode='closest',\n                margin=dict(b=20,l=5,r=5,t=40),\n                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n                )\n\nfig.update_layout(template = 'presentation', titlefont_size=30)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The network above shows that most of the targets are not related to each others. \nOn the other hand, we observe a big cluster of connected targets in the center"},{"metadata":{},"cell_type":"markdown","source":"## Identify the clusters\n\nI developped this snipper to create group of targets based on the co-occurence matric. There might be ways of doing using networkX directly."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def find_recursive(commun, todo, done, effect, current_group, threshold = 10):\n\n    \"\"\" Function used to group recursively the targets if they have a common sample\n        commun: matrice of co-occurence\n\n        todo: Serie representing the sum of co-occurence per target\n        done: targets already mapped to a group\n        effect: current target checked\n        current_group: the current group in which we add targets\n        threshold: number of minimum co-occurence to make the group\n    \"\"\"\n\n    #List of target remaining to group\n    efflist = todo.index\n\n    #Reduce co-occurence matrix to remaining targets to class\n    sub = commun.loc[efflist,efflist]\n    # Keeping only the co-occurence of the target that we are currently looking at\n    sub = sub[effect]\n    #adding the target to the list of target processed\n    done += [effect]\n    #add the targets with co-occurence above threshold\n    to_add = list(sub[sub>threshold].index)\n\n    #If targets in to_add, recursively check the co-occurence for those new targets to add in the current group\n    if len(to_add):\n        current_group += to_add\n        todo = todo[[elmt for elmt in todo.index if elmt not in done]]\n        for effect in to_add:\n            if effect not in done:\n                todo, done, current_group = find_recursive(commun, todo, done, effect, current_group, threshold)\n        return todo, done, current_group\n\n    #Otherwise, update todo list and return final outputs\n    else:\n        todo = todo[[elmt for elmt in todo.index if elmt not in done]]\n        return todo, done, current_group\n    \n# Initiate todo Serie\ntodo = commun.sum().sort_values(ascending = False)\ngroups = {}\nthreshold = 1\ni = 0\nwhile True:\n    todo, done, current_group = find_recursive(commun, todo, [todo.index[0]], todo.index[0], [todo.index[0]], threshold = threshold)\n    groups[i] = list(set(current_group))\n    i+=1\n    if len(todo) ==0:\n        break\n        \n#Keep only scored target\ngroup_score = {}\nfor g, t in groups.items():\n    elmts = [elmt for elmt in t if elmt in target_scored.columns]\n    if len(elmts):\n        group_score[g] = elmts ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cluster of targets"},{"metadata":{"trusted":true},"cell_type":"code","source":"count = {}\nfor g, t in group_score.items():\n    count['g_'+str(g)] = len(t)\n\ncount = pd.Series(count).sort_values(ascending = False)\n\nfig = go.Figure(\n    go.Bar(\n        x = count.index,\n        y = count.values\n    )\n)\n\nfig.update_layout(template = 'presentation', title = 'Count of Targets per Cluster ( Scored )')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Approx. half of the targets have no co-occurences with other targets"},{"metadata":{},"cell_type":"markdown","source":"# Part 2: OOFs study\n\nIn this part, I am going to study the oofs obtained using 3 seeds - 5folds splits based on [Chris Deotte topic](https://www.kaggle.com/c/lish-moa/discussion/195195)\nThe big advantage of those splits is that it guaranty that same drugs are not in both train and test folds, avoiding some leakages.\n\nI built two types of OOFs: \n- One OOF is made using a NN making multi-target predictions\n- The other OOF are based on NN making single target predictions\n\nI use both ROC_AUC_Score and Log_Loss to evaluate the predictions\n\nI calculate scores based on trt_cp samples only"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\ndef score(y, yp):\n    return - np.mean(y*np.log(yp+10**(-15)) + (1-y)*np.log(1-yp+10**(-15))) \n\n \ntarget = target_scored[train_features.cp_type == 'trt_cp'].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multi-Target Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"oofs_main = []\nGROUP = 'MAINMODEL'\nfor SEED in [0,1]:\n    oof = pd.read_csv(f'../input/moa-oofs/oofs/group{GROUP}_SEED{SEED}.csv', index_col = 0).values\n    oofs_main.append(oof)\n    roc = np.round(roc_auc_score(target.values, oof),3)\n    log_loss = np.round(score(target.values, oof), 5)\n    print(f'Seed {SEED} : roc_auc_score {roc} - log_loss {log_loss}')\n    \noof_main_avg = np.mean(oofs_main, axis= 0)\nroc = np.round(roc_auc_score(target.values, oof_main_avg),3)\nlog_loss = np.round(score(target.values, oof_main_avg), 5)\nprint(f'Average OOFs : roc_auc_score {roc} - log_loss {log_loss}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Single Target Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"oofs_single_target = []\nfor SEED in [0,1]:\n    oof_tot = []\n    for GROUP in range(206):\n        oof = pd.read_csv(f'../input/moa-oofs/oofs/group{GROUP}_SEED{SEED}.csv', index_col = 0).values\n        oof_tot.append(oof)\n        \n    oof_tot = np.hstack(oof_tot)\n    oofs_single_target.append(oof_tot)\n    roc = np.round(roc_auc_score(target.values, oof_tot),3)\n    log_loss = np.round(score(target.values, oof_tot), 5)\n    print(f'Seed {SEED} : roc_auc_score {roc} - log_loss {log_loss}')\n    \noof_single_target_avg = np.mean(oofs_single_target, axis= 0)\nroc = np.round(roc_auc_score(target.values, oof_single_target_avg),3)\nlog_loss = np.round(score(target.values, oof_single_target_avg), 5)\nprint(f'Average OOFs : roc_auc_score {roc} - log_loss {log_loss}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions Statistics\n\nThe graph below shows interesting information.\nFirst the single target model tend to make higher probability prediction than its counterpart.\n\nSecond, we see that the more positive labels, the higher the probabilities"},{"metadata":{"trusted":true},"cell_type":"code","source":"main_mean = oof_main_avg.mean(axis=0)\nsingle_target_mean = oof_single_target_avg.mean(axis=0)\n\nfig = go.Figure()\nfig.add_trace(\n    go.Scatter(\n        x = main_mean,\n        y = single_target_mean,\n        hovertext = target.columns,\n        mode = 'markers',\n        marker = {'size':target.sum()/10},\n        name = 'oofs'\n    )\n)\n\nfig.add_trace(\n    go.Scatter(\n        x = [0,0.05],\n        y = [0,0.05],\n        marker = {'color':'black', 'line' : {'width':0.1}},\n        mode = 'lines',\n        showlegend = False\n    )\n)\n\nfig.update_layout(template = 'presentation', title = 'Average Probability by Type of Model')\nfig.update_yaxes(title = 'Multi Target')\nfig.update_xaxes(title = 'Single Targets')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_logloss = [score(target.values[:,i], oof_main_avg[:,i]) for i in range(206)]\nsingle_target_logloss = [score(target.values[:,i], oof_single_target_avg[:,i]) for i in range(206)]\n\nfig = go.Figure()\nfig.add_trace(\n    go.Bar(\n        x = target.columns,\n        y = main_logloss,\n        name = 'Main Model'\n    )\n)\n\nfig.add_trace(\n    go.Bar(\n        x = target.columns,\n        y = single_target_logloss,\n        name = 'Single Targets Models'\n    )\n)\nfig.update_layout(template = 'presentation', title = 'Logloss by Type of Model per Target')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlations between the predictions of the two models"},{"metadata":{},"cell_type":"markdown","source":"I am pairing the targets from same clusters with same random colors. It allows us to see if the model type perform better on targets from big/small clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"colors_target = {}\n\nfor n, g in group_score.items():\n    if len(g)>1:\n        color = '#' + \"%06x\" % random.randint(0, 0xFFFFFF)\n        for elmt in g:\n            colors_target[elmt] = color","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (30,50))\nfor i in range(206):\n    fig = plt.subplot(18,12,i+1)\n    plt.title(target.columns[i])\n    x2 = oof_main_avg[np.where(target.iloc[:,i]==0)[0],i]\n    y2 = oof_single_target_avg[np.where(target.iloc[:,i]==0)[0],i]\n    plt.scatter(x2, y2, color = 'black', alpha = 0.1)\n    x1 = oof_main_avg[np.where(target.iloc[:,i]==1)[0],i]\n    y1 = oof_single_target_avg[np.where(target.iloc[:,i]==1)[0],i]\n    plt.scatter(x1, y1, color = 'red')\n    if target.columns[i] in colors_target.keys():\n        fig.patch.set_facecolor(f'{colors_target[target.columns[i]]}')\n    plt.xlim(0,1)\n    plt.ylim(0,1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some targets where using mono-target NN seems to perform better"},{"metadata":{},"cell_type":"markdown","source":"As expected, the single target NN perform better on targets that are not part of clusters of targets"},{"metadata":{"trusted":true},"cell_type":"code","source":"toplot = [11,12*3-1,12*7+6,12*14+5]\nplt.figure(figsize = (15,15))\nfor i in range(4):\n    plt.subplot(2,2,i+1)\n    i = toplot[i]\n    plt.title(target.columns[i], size = 20)\n    x2 = oof_main_avg[np.where(target.iloc[:,i]==0)[0],i]\n    y2 = oof_single_target_avg[np.where(target.iloc[:,i]==0)[0],i]\n    plt.scatter(x2, y2, color = 'black', alpha = 0.1, label = 'negatif')\n    x1 = oof_main_avg[np.where(target.iloc[:,i]==1)[0],i]\n    y1 = oof_single_target_avg[np.where(target.iloc[:,i]==1)[0],i]\n    plt.scatter(x1, y1, color = 'red', label = 'positif')\n    plt.xlim(-0.1,1.1)\n    plt.ylim(-0.1,1.1)\n    plt.xlabel('proba multi target', size = 15)\n    plt.ylabel('proba single target', size = 15)\n    if target.columns[i] in colors_target.keys():\n        fig.patch.set_facecolor(f'{colors_target[target.columns[i]]}')\n    plt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 3: Models behavior on \"isolated\" targets\n\nBy isolated I mean targets with no connexions to the others following the co-occurence pattern shown in Part 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"is_target = []\nfor k,v in group_score.items():\n    if len(v)==1:\n        is_target+=v\nprint(f'total of isolated target: {len(is_target)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ROC and LogLoss"},{"metadata":{"trusted":true},"cell_type":"code","source":"st = np.where(target.columns.isin(is_target))[0]\n\noofs_main = []\nGROUP = 'MAINMODEL'\nprint('MAINMODEL')\nfor SEED in [0,1,2]:\n    oof = pd.read_csv(f'../input/moa-oofs/oofs/group{GROUP}_SEED{SEED}.csv', index_col = 0).values\n    oofs_main.append(oof)\n    roc = np.round(roc_auc_score(target.values[:,st], oof[:,st]),3)\n    log_loss = np.round(score(target.values[:,st], oof[:,st]), 5)\n    print(f'Seed {SEED} : roc_auc_score {roc} - log_loss {log_loss}')\n    \noof_main_avg = np.mean(oofs_main, axis= 0)\nroc = np.round(roc_auc_score(target.values[:,st], oof_main_avg[:,st]),3)\nlog_loss = np.round(score(target.values[:,st], oof_main_avg[:,st]), 5)\nprint(f'Average OOFs : roc_auc_score {roc} - log_loss {log_loss}')\n\noofs_single_target = []\nprint('\\nSINGLE TARGETS')\nfor SEED in [0,1]:\n    oof_tot = []\n    for GROUP in range(206):\n        oof = pd.read_csv(f'../input/moa-oofs/oofs/group{GROUP}_SEED{SEED}.csv', index_col = 0).values\n        oof_tot.append(oof)\n        \n    oof_tot = np.hstack(oof_tot)\n    oofs_single_target.append(oof_tot)\n    roc = np.round(roc_auc_score(target.values[:,st], oof_tot[:,st]),3)\n    log_loss = np.round(score(target.values[:,st], oof_tot[:,st]), 5)\n    print(f'Seed {SEED} : roc_auc_score {roc} - log_loss {log_loss}')\n    \noof_single_target_avg = np.mean(oofs_single_target, axis= 0)\nroc = np.round(roc_auc_score(target.values[:,st], oof_single_target_avg[:,st]),3)\nlog_loss = np.round(score(target.values[:,st], oof_single_target_avg[:,st]), 5)\nprint(f'Average OOFs : roc_auc_score {roc} - log_loss {log_loss}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recall & Precision"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score, precision_score\nrecalls_main = {}\nprecisions_main = {}\nrecalls_single = {}\nprecisions_single = {}\nfor GROUP in st:\n    yy = target.iloc[:,GROUP:GROUP+1]\n    name = yy.columns[0]\n    oofmain = oof_main_avg[:,GROUP:GROUP+1]\n    oofsingle = oof_single_target_avg[:,GROUP:GROUP+1]\n    \n    recalls_main[name] = recall_score(yy, np.round(oofmain))\n    precisions_main[name] = precision_score(yy, np.round(oofmain))\n    recalls_single[name] = recall_score(yy, np.round(oofsingle))\n    precisions_single[name] = precision_score(yy, np.round(oofsingle))\n    \nrecalls_main = pd.Series(recalls_main)\nprecisions_main = pd.Series(precisions_main)\nrecalls_single = pd.Series(recalls_single)\nprecisions_single = pd.Series(precisions_single)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(\n    go.Scatter(\n        x = recalls_main,\n        y = precisions_main,\n        mode= 'markers',\n        name = 'MultiTarget NN',\n        text = recalls_main.index,\n        marker = {'size':target[recalls_single.index].sum()/4}\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x = recalls_single,\n        y = precisions_single,\n        mode= 'markers',\n        name = 'SingleTarget NN',\n        text = recalls_single.index,\n        marker = {'size':target[recalls_single.index].sum()/4}\n    )\n)\n\nfig.update_layout(template = 'presentation', title = 'Precision and Recall for \"single\" targets')\nfig.update_yaxes(title = 'Precision')\nfig.update_xaxes(title = 'Recall')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is interesting to note that single target NN outperform the multitarget NN for targets not related to the main cluster of targets"},{"metadata":{},"cell_type":"markdown","source":"# Part 4: Models behaviours on main cluster of targets"},{"metadata":{},"cell_type":"markdown","source":"### ROC and LogLoss"},{"metadata":{"trusted":true},"cell_type":"code","source":"mc = np.where(target.columns.isin(group_score[0]))[0]\noofs_main = []\nGROUP = 'MAINMODEL'\nprint('MAINMODEL')\nfor SEED in [0,1,2]:\n    oof = pd.read_csv(f'../input/moa-oofs/oofs/group{GROUP}_SEED{SEED}.csv', index_col = 0).values\n    oofs_main.append(oof)\n    roc = np.round(roc_auc_score(target.values[:,mc], oof[:,mc]),3)\n    log_loss = np.round(score(target.values[:,mc], oof[:,mc]), 5)\n    print(f'Seed {SEED} : roc_auc_score {roc} - log_loss {log_loss}')\n    \noof_main_avg = np.mean(oofs_main, axis= 0)\nroc = np.round(roc_auc_score(target.values[:,mc], oof_main_avg[:,mc]),3)\nlog_loss = np.round(score(target.values[:,mc], oof_main_avg[:,mc]), 5)\nprint(f'Average OOFs : roc_auc_score {roc} - log_loss {log_loss}')\n\noofs_single_target = []\nprint('\\nSINGLE TARGETS')\nfor SEED in [0,1]:\n    oof_tot = []\n    for GROUP in range(206):\n        oof = pd.read_csv(f'../input/moa-oofs/oofs/group{GROUP}_SEED{SEED}.csv', index_col = 0).values\n        oof_tot.append(oof)\n        \n    oof_tot = np.hstack(oof_tot)\n    oofs_single_target.append(oof_tot)\n    roc = np.round(roc_auc_score(target.values[:,mc], oof_tot[:,mc]),3)\n    log_loss = np.round(score(target.values[:,mc], oof_tot[:,mc]), 5)\n    print(f'Seed {SEED} : roc_auc_score {roc} - log_loss {log_loss}')\n    \noof_single_target_avg = np.mean(oofs_single_target, axis= 0)\nroc = np.round(roc_auc_score(target.values[:,mc], oof_single_target_avg[:,mc]),3)\nlog_loss = np.round(score(target.values[:,mc], oof_single_target_avg[:,mc]), 5)\nprint(f'Average OOFs : roc_auc_score {roc} - log_loss {log_loss}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recall and Precision"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score, precision_score\nrecalls_main = {}\nprecisions_main = {}\nrecalls_single = {}\nprecisions_single = {}\nfor GROUP in mc:\n    yy = target.iloc[:,GROUP:GROUP+1]\n    name = yy.columns[0]\n    oofmain = oof_main_avg[:,GROUP:GROUP+1]\n    oofsingle = oof_single_target_avg[:,GROUP:GROUP+1]\n    \n    recalls_main[name] = recall_score(yy, np.round(oofmain))\n    precisions_main[name] = precision_score(yy, np.round(oofmain))\n    recalls_single[name] = recall_score(yy, np.round(oofsingle))\n    precisions_single[name] = precision_score(yy, np.round(oofsingle))\n    \nrecalls_main = pd.Series(recalls_main)\nprecisions_main = pd.Series(precisions_main)\nrecalls_single = pd.Series(recalls_single)\nprecisions_single = pd.Series(precisions_single)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(\n    go.Scatter(\n        x = recalls_main,\n        y = precisions_main,\n        mode= 'markers',\n        name = 'MultiTarget NN',\n        text = recalls_main.index,\n        marker = {'size':target[recalls_single.index].sum()/10}\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x = recalls_single,\n        y = precisions_single,\n        mode= 'markers',\n        name = 'SingleTarget NN',\n        text = recalls_single.index,\n        marker = {'size':target[recalls_single.index].sum()/10}\n    )\n)\n\nfig.update_layout(template = 'presentation', title = 'Precision and Recall for \"main cluster\" targets')\nfig.update_yaxes(title = 'Precision')\nfig.update_xaxes(title = 'Recall')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 5: Predictions on public test set\n\nI noticed that blending my two models give me poor results. So I want to understand what is happening between train and test set. I just have two seeds available here"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = {}\nfor GROUP in ['MAINMODEL','SINGLETARGET']:\n    pred = []\n    for SEED in [0,1]:\n        p = pd.read_csv(f'../input/outputs-moa-models/preds_public_GROUP{GROUP}_SEED{SEED}.csv', index_col = 0).values\n        pred.append(p)\n    pred = np.mean(pred, axis=0)\n    preds[GROUP] = pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gathering my predictions from another notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_train_pred_main = oof_main_avg.mean(axis=0)\navg_train_pred_single_target = oof_single_target_avg.mean(axis=0)\navg_test_pred_main = preds['MAINMODEL'].mean(axis=0)\navg_test_pred_single_target = preds['SINGLETARGET'].mean(axis=0)\n\ndiff_main = pd.Series((avg_train_pred_main-avg_test_pred_main), \n                      index = target.columns).sort_values(ascending = False)\n\ndiff_main = diff_main[(diff_main>0.0005) | (diff_main<-0.0005)]\ndiff_single_target = pd.Series((avg_train_pred_single_target-avg_test_pred_single_target), \n                      index = target.columns).sort_values(ascending = False)\ndiff_single_target = diff_single_target[(diff_single_target>0.0005) | (diff_single_target<-0.0005)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scatter Plot\n\nThe scatter plot below shows the mean prediction of each target for the trainset and the testset. It is interesting to note that the target prediction distribution can change a lot from train to test set, and it might explain why my single target models underperform on LB"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(\n    go.Scatter(\n        x = avg_train_pred_main,\n        y = avg_test_pred_main,\n        name = 'Multi Target NN',\n        mode = 'markers',\n        text = target.columns,\n        marker = {'size':np.sqrt(target.sum(axis=0))},\n        opacity = 0.8\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x = avg_train_pred_single_target,\n        y = avg_test_pred_single_target,\n        name = 'Single Target NN',\n        mode = 'markers',\n        text = target.columns,\n        marker = {'size':np.sqrt(target.sum(axis=0))},\n        opacity = 0.8\n    )\n)\n\nfig.add_trace(\n    go.Scatter(\n        x = [0,0.05],\n        y = [0,0.05],\n        marker = {'color':'black', 'line' : {'width':0.1}},\n        mode = 'lines',\n        showlegend = False\n    )\n)\n\nfig.update_layout(template = 'presentation', title = 'Mean Prediction by target, differences between train and test set')\nfig.update_xaxes(title = 'Train set')\nfig.update_yaxes(title = 'Test set')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfig.add_trace(\n    go.Bar(\n        x = diff_single_target.index,\n        y = diff_single_target.values,\n        name = 'Single Target NN'\n    )\n)\n\nfig.add_trace(\n    go.Bar(\n        x = diff_main.index,\n        y = diff_main.values,\n        name = 'Multi Targets NN'\n    )\n)\n\nfig.update_layout(template = 'presentation', title = 'Difference of predictions mean between Train and Test Sets')\nfig.update_xaxes(tickfont = {'size':5}, tickangle = 45)\nfig.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}