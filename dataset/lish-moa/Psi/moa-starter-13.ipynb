{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterativestratification')\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport os\nimport copy\nimport gc\nimport time\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom tqdm.notebook import tqdm\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom transformers import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\nfrom sklearn.pipeline import Pipeline\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nos.listdir('../input/lish-moa')\n\npd.set_option('max_columns', 2000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\ntrain = train_features.merge(train_targets_scored, on='sig_id')\n\ndrug = pd.read_csv('/kaggle/input/lish-moa/train_drug.csv')\n\ntrain = train.merge(drug, on=\"sig_id\")\n\ntrain[\"drug_id_cnt\"] = train[\"drug_id\"].map(train[\"drug_id\"].value_counts().to_dict())\n\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget_cols = train_targets_scored.drop('sig_id', axis=1).columns.values.tolist()\n\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\nfeature_cols = [c for c in train_features.columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['fold','sig_id', 'drug_id']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 1337\nFOLDS = 5\n\n# LOCATE DRUGS\nvc = train.drug_id.value_counts()\nvc1 = vc.loc[(vc==6)|(vc==12)|(vc==18)].index.sort_values()\nvc2 = vc.loc[(vc!=6)&(vc!=12)&(vc!=18)].index.sort_values()\n\n# STRATIFY DRUGS 18X OR LESS\ndct1 = {}; dct2 = {}\nskf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\ntmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\nfor fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[target_cols])):\n    dd = {k:fold for k in tmp.index[idxV].values}\n    dct1.update(dd)\n\n# STRATIFY DRUGS MORE THAN 18X\nskf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\ntmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\nfor fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[target_cols])):\n    dd = {k:fold for k in tmp.sig_id[idxV].values}\n    dct2.update(dd)\n\n# ASSIGN FOLDS\ntrain['fold'] = train.drug_id.map(dct1)\ntrain.loc[train.fold.isna(),'fold'] = train.loc[train.fold.isna(),'sig_id'].map(dct2)\ntrain.fold = train.fold.astype('int8')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\nfolds = train[\"fold\"]\ntrain[\"fold\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = []\nfor f in range(FOLDS):\n    trn_idx = np.where(folds.values!=f)[0]\n    val_idx = np.where(folds.values==f)[0]\n    kf.append([trn_idx, val_idx])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass MoADataset:\n    def __init__(self, genes, cells, features, sample_weights=None, targets=None):\n        self.genes = genes\n        self.cells = cells\n        self.features = features\n        self.sample_weights = sample_weights\n        self.targets = targets\n        \n        if self.sample_weights is None:\n            self.sample_weights = np.zeros(len(self.genes))\n        \n        if self.targets is None:\n            self.targets = np.zeros((len(self.genes),1))\n        \n    def __len__(self):\n        return (self.genes.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'genes' : torch.tensor(self.genes[idx,:], dtype=torch.float),\n            'cells' : torch.tensor(self.cells[idx,:], dtype=torch.float),\n            'features': torch.tensor(self.features[idx,:], dtype=torch.float),\n            'sample_weights': torch.tensor(self.sample_weights[idx], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)           \n        }\n        return dct\n\n\nclass FE():\n    \n    def __init__(self):\n        self.fnames = []\n        return\n    \n    def fit(self, df):\n        ''\n        self.ohe = {}\n        for f in [\"cp_time\", \"cp_dose\"]:\n            self.ohe[f] = OneHotEncoder(sparse=False).fit(df[f].values.reshape(-1,1))\n        \n    def transform(self, df, is_train=True):\n        \n        if is_train:\n            y = df[target_cols]\n        else:\n            y = None\n        \n        X = pd.DataFrame()\n        \n        X[GENES] = df[GENES]\n        X[CELLS] = df[CELLS]\n        \n        for f in self.ohe.keys():\n            oh = self.ohe[f].transform(df[f].values.reshape(-1,1))\n            oh = pd.DataFrame(oh, columns=[f\"{f}_{c}\" for c in self.ohe[f].get_feature_names()])\n            X = pd.concat([X, oh], axis=1)\n            self.fnames += list(oh.columns)\n        \n        return X, y\n    \nfe = FE()\n\nfe.fit(pd.concat([train, test], axis=0))\n\n\n\nX_train, Y_train = fe.transform(train)\nX_test, Y_test = fe.transform(test, is_train=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def metric(y, p):\n    loss = 0\n    y_pred_clip = np.clip(p, 1e-15, 1 - 1e-15)\n    for i in range(p.shape[1]):\n        loss += - np.mean(y[:, i] * np.log(y_pred_clip[:, i]) + (1 - y[:, i]) * np.log(1 - y_pred_clip[:, i]))\n    return loss / p.shape[1]\n\ntrain[\"sample_weight\"] = 1\n\n\n\ndef get_activation(act):\n    if act == \"leaky_relu\":\n        return nn.LeakyReLU()\n    if act == \"relu\":\n        return nn.ReLU()\n    if act == \"prelu\":\n        return nn.PReLU()\n    \ndef get_bn(bn, l):\n    if bn == True:\n        return nn.BatchNorm1d(l)\n    else:\n        return nn.Identity()\n\nclass Model(nn.Module):\n    def __init__(self, num_targets):\n        super(Model, self).__init__()\n        \n        hidden1 = 128\n        hidden2 = 256\n        \n        dropout1 = 0.2749655379470675\n        dropout2 = 0.059365935529546235\n        dropout3 = 0.33205409463214886\n        \n        activation1 = \"leaky_relu\"\n        activation2 = \"prelu\"\n        activation3 = \"prelu\"\n        \n        bn1 = True\n        bn2 = False\n        bn3 = False\n        bn4 = True\n        \n        self.genes = nn.Sequential(\n            get_bn(bn1, len(GENES)),\n            nn.utils.weight_norm(nn.Linear(len(GENES), hidden1, bias=True)),\n            get_activation(activation1),\n            get_bn(bn2, hidden1),\n            nn.Dropout(dropout1),\n            #nn.LeakyReLU(),\n            #trial.suggest_categorical('activation1', [nn.LeakyReLU(), nn.ReLU(), nn.PReLU()]),\n        )\n        \n        self.cells = nn.Sequential(\n            get_bn(bn3, len(CELLS)),\n            nn.utils.weight_norm(nn.Linear(len(CELLS), hidden2, bias=True)),\n            get_activation(activation2),\n            get_bn(bn4, hidden2),\n            nn.Dropout(dropout2),\n            #nn.LeakyReLU(),\n            #trial.suggest_categorical('activation2', [nn.LeakyReLU(), nn.ReLU(), nn.PReLU()]),\n            #nn.BatchNorm1d(hidden2),\n        )\n\n#         )\n        \n        \n        self.out = nn.Sequential(\n            #get_bn(bn3, hidden1+hidden2),\n            \n            nn.utils.weight_norm(nn.Linear(hidden1+hidden2, hidden1+hidden2, bias=False)),\n            get_activation(activation3),\n            #nn.LeakyReLU(),\n            #trial.suggest_categorical('activation3', [nn.LeakyReLU(), nn.ReLU(), nn.PReLU()]),\n            \n            nn.BatchNorm1d(hidden1+hidden2),\n            nn.Dropout(dropout3),\n            nn.utils.weight_norm(nn.Linear(hidden1+hidden2, num_targets, bias=True)),\n            #nn.Sigmoid()\n        )\n    \n    def forward(self, data):\n        \n        genes = data[\"genes\"].to(DEVICE)\n        cells = data[\"cells\"].to(DEVICE)\n        #features = data[\"features\"].to(DEVICE)\n        \n#         genes_cnn = torch.max(self.genes_cnn(genes.unsqueeze(1)), dim=2)[0]\n#         cells_cnn = torch.max(self.cells_cnn(cells.unsqueeze(1)), dim=2)[0]\n        \n        genes = self.genes(genes)\n        cells = self.cells(cells)\n        #features = self.features(features)\n        \n        \n        \n        x = torch.cat([genes, cells], dim=1)\n        \n        #x = torch.cat([x, features], dim=1)\n        \n        out = self.out(x)\n        \n        #out = torch.clamp(out, 1e-15, 1 - 1e-15)\n        \n        return out\n    \n\n    \n\n\nEPOCHS = 29\n\nBATCH_SIZE = 512\n\nNFOLDS = 5\nNBAGS = 5\n\nif len(test) > 3624:\n    FULL_FIT = False\nelse:\n    FULL_FIT = False\n\nnum_features=X_train.shape[1]\nnum_targets=Y_train.shape[1]\n\nseed = 42    \n#seed_everything(seed)\n\noof = np.zeros((len(train), num_targets))\n\np_test = []\n\nfor fold, (trn_idx, val_idx) in enumerate(kf):\n    \n#     if fold != 4:\n#         continue\n    \n    if FULL_FIT:\n        trn_idx = np.arange(len(X_train))\n    \n    preds_bag = []\n\n    for bag in range(NBAGS):\n        \n        GENES = [col for col in train_features.columns if col.startswith('g-')]\n        CELLS = [col for col in train_features.columns if col.startswith('c-')]\n        FNAMES = fe.fnames\n\n        x_train, y_train  = X_train.iloc[trn_idx].reset_index(drop=True).copy(), Y_train.values[trn_idx].copy()\n        x_valid, y_valid =  X_train.iloc[val_idx].reset_index(drop=True).copy(), Y_train.values[val_idx].copy()\n        x_test = X_test.copy()\n        \n        \n        \n        \n        pipe = Pipeline([('scaler', QuantileTransformer(n_quantiles=1000, random_state=np.random.randint(10_000), output_distribution=\"normal\"))])\n        #pipe = Pipeline([('scaler', RobustScaler())])\n        pipe.fit(np.concatenate([X_train[GENES], X_test[GENES]], axis=0))\n        x_train[GENES] = pipe.transform(x_train[GENES])\n        x_valid[GENES] = pipe.transform(x_valid[GENES])\n        x_test[GENES] = pipe.transform(x_test[GENES])\n        \n#         n_comp = 600\n#         pca = Pipeline([('pca', PCA(n_components=n_comp)), ('scaler', QuantileTransformer(n_quantiles=100, random_state=np.random.randint(10_000), output_distribution=\"normal\"))])\n#         pca.fit(np.concatenate([x_train[GENES], x_valid[GENES], x_test[GENES]], axis=0))\n#         x_train_pca = pd.DataFrame(pca.transform(x_train[GENES]), columns=[f'pca_G-{i}' for i in range(n_comp)])\n#         x_train = pd.concat([x_train, x_train_pca], axis=1)\n#         x_valid_pca = pd.DataFrame(pca.transform(x_valid[GENES]), columns=[f'pca_G-{i}' for i in range(n_comp)])\n#         x_valid = pd.concat([x_valid, x_valid_pca], axis=1)\n#         x_test_pca = pd.DataFrame(pca.transform(x_test[GENES]), columns=[f'pca_G-{i}' for i in range(n_comp)])\n#         x_test = pd.concat([x_test, x_test_pca], axis=1)\n#         GENES += list(x_train_pca.columns)\n        #, ('var', VarianceThreshold(0.8))\n        pipe = Pipeline([('scaler', RobustScaler(quantile_range=(25.0, 75.0)))])\n        pipe.fit(np.concatenate([X_train[CELLS], X_test[CELLS]], axis=0))\n        x_train[CELLS] = pipe.transform(x_train[CELLS])\n        x_valid[CELLS] = pipe.transform(x_valid[CELLS])\n        x_test[CELLS] = pipe.transform(x_test[CELLS])\n        #x_train[CELLS] = np.exp(x_train[CELLS])\n        #x_valid[CELLS] = np.exp(x_valid[CELLS])\n        \n#         n_comp = 50\n#         pca = PCA(n_components=n_comp)\n#         pca.fit(np.concatenate([x_train[CELLS], x_valid[CELLS], x_test[CELLS]], axis=0))\n#         x_train_pca = pd.DataFrame(pca.transform(x_train[CELLS]), columns=[f'pca_C-{i}' for i in range(n_comp)])\n#         x_train = pd.concat([x_train, x_train_pca], axis=1)\n#         x_valid_pca = pd.DataFrame(pca.transform(x_valid[CELLS]), columns=[f'pca_C-{i}' for i in range(n_comp)])\n#         x_valid = pd.concat([x_valid, x_valid_pca], axis=1)\n#         x_test_pca = pd.DataFrame(pca.transform(x_test[CELLS]), columns=[f'pca_C-{i}' for i in range(n_comp)])\n#         x_test = pd.concat([x_test, x_test_pca], axis=1)\n#         CELLS += list(x_train_pca.columns)\n        \n#         print(len(CELLS))\n\n#         var = VarianceThreshold(0.8)\n#         var.fit(np.concatenate([x_train[GENES]], axis=0))\n#         x_train_genes = var.transform(x_train[GENES])\n#         x_valid_genes = var.transform(x_valid[GENES])  \n#         print(x_train[GENES].shape, x_train_genes.shape)\n        \n#         var = VarianceThreshold(0.8)\n#         var.fit(np.concatenate([x_train[CELLS]], axis=0))\n#         x_train_cells = var.transform(x_train[CELLS])\n#         x_valid_cells = var.transform(x_valid[CELLS])  \n#         print(x_train[CELLS].shape, x_train_cells.shape)\n        \n        #x_train = x_train.values\n        #x_valid = x_valid.values\n        #x_test = x_test.values\n\n        train_dataset = MoADataset(x_train[GENES].values, x_train[CELLS].values, x_train[FNAMES].values, train[\"sample_weight\"].values[trn_idx], y_train)\n        valid_dataset = MoADataset(x_valid[GENES].values, x_valid[CELLS].values, x_valid[FNAMES].values, train[\"sample_weight\"].values[val_idx], y_valid)\n        test_dataset = MoADataset(x_test[GENES].values, x_test[CELLS].values, x_test[FNAMES].values)\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        model = Model(\n            num_targets=num_targets\n        )\n\n        model.to(DEVICE)\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.04647353847564317, weight_decay=8.087569236449597e-06)\n\n        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=len(trainloader)*EPOCHS//2, num_training_steps=len(trainloader)*EPOCHS)\n\n        #loss_fn = nn.BCEWithLogitsLoss()\n        loss_fn = nn.BCEWithLogitsLoss()\n\n        best_loss = np.inf\n        best_preds = None\n\n        for epoch in range(EPOCHS):\n\n            start_time = time.time()\n\n            model.train()\n            train_loss = 0\n\n            progress_bar = tqdm(trainloader, total=len(trainloader), disable=True)\n            for data in progress_bar:\n                optimizer.zero_grad()\n                outputs = model(data)\n                loss = nn.BCEWithLogitsLoss(reduction=\"mean\")(outputs, data['y'].to(DEVICE))\n\n    #             loss = loss * sample_weights.reshape(-1,1)\n    #             loss = loss.sum() / sample_weights.sum()\n\n                loss.backward()\n                optimizer.step()\n\n                if scheduler is not None:\n                    scheduler.step()\n\n                train_loss += loss.item() / len(trainloader)\n                progress_bar.set_description(f\"loss: {loss.item():.2f}\")\n\n            model.eval()\n            valid_loss = 0\n            valid_targets = []\n            valid_preds = []\n\n            for data in validloader:\n                outputs = model(data)\n                loss = loss_fn(outputs, data[\"y\"].to(DEVICE))\n\n                valid_loss += loss.item() / len(validloader)\n                \n                valid_targets.append(data[\"y\"])\n                valid_preds.append(outputs.sigmoid())\n             \n            valid_targets = torch.cat(valid_targets)\n            valid_preds = torch.cat(valid_preds)\n            #print(loss_fn(valid_preds, valid_targets))\n            valid_preds = valid_preds.detach().cpu().numpy()\n                \n            valid_score = metric(y_valid, valid_preds)\n\n            print(f\"Fold {fold} Epoch {epoch} {time.time()-start_time:.2f}s train_loss: {train_loss:.5f}, val_loss: {valid_loss:.5f}, val_score: {valid_score:.5f}\")\n            \n            best_preds = valid_preds\n            \n            \n            \n        print()\n        \n        preds_bag.append(best_preds)\n        \n        bag_score = metric(y_valid, np.mean(preds_bag, axis=0))\n        print(f\"Bag {bag} val_score: {bag_score:.5f}\")\n        print()\n        \n        test_preds = []\n        for data in testloader:\n            outputs = model(data)\n\n            test_preds.append(outputs.sigmoid())\n        test_preds = torch.cat(test_preds).detach().cpu().numpy()\n        p_test.append(test_preds)\n\n    oof[val_idx] = np.mean(preds_bag, axis=0)\n\n    print()\n    #return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metric(Y_train.values, oof)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import scipy as sp\n# from functools import partial\n\n\n# init=[0]*oof.shape[1]\n\n# #init = 1/np.mean(Y_train.values, axis=0)\n\n# def _loss(coef, X, y):\n#     X = X+coef\n#     #print(coef)\n#     loss = metric(y,X)\n#     print(loss)\n#     return loss\n\n# loss_partial = partial(_loss, X=oof, y=Y_train.values)\n# initial_coef = np.array(init).reshape(-1)\n# coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead', options={\"maxiter\": 100})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_test = np.mean(p_test, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[target_cols] = p_test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}