{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Mechanism of Action (MoA) participation kernel"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from time import time\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Deep learning libraries\nimport tensorflow as tf\nfrom keras import Model, models\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, BatchNormalization, Dropout\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import layers,regularizers,Sequential,backend,callbacks,optimizers,metrics,losses\n\n# Utility functions\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.utils import shuffle\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import scale, StandardScaler\n\nfrom kerastuner.tuners import RandomSearch\nfrom kerastuner.engine.hyperparameters import HyperParameters\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nnp.random.seed(7)\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_feat_df = pd.read_csv('../input/lish-moa/train_features.csv')\ntest_feat_df = pd.read_csv('../input/lish-moa/test_features.csv')\n\nscored_train_targets_df = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\nnscored_train_targets_df = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\nsubmission_sample_df = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gene_cols = train_feat_df.columns[4:-100]\ngene_data = train_feat_df[gene_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cell_via_cols = train_feat_df.columns[-100:]\ncell_via_data = train_feat_df[cell_via_cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA gene features"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_gene_data = scale(gene_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca1 = PCA(0.95)\npca1.fit(scaled_gene_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_gene = pca1.transform(scaled_gene_data)\nper_var = np.round(pca1.explained_variance_ratio_* 100, decimals=1)\nlabels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\ntransformed_gene_feats = pd.DataFrame(pca_gene, columns=labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA cell features"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_cell_data = scale(cell_via_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca2 = PCA(.95)\npca2.fit(scaled_cell_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_cell = pca2.transform(scaled_cell_data)\nper_var = np.round(pca2.explained_variance_ratio_* 100, decimals=1)\nlabels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\ntransformed_cell_feats = pd.DataFrame(pca_cell, columns=labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NN Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_data = train_feat_df.drop(columns=['sig_id']+list(gene_cols)+list(cell_via_cols) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_data = Train_data.merge(transformed_gene_feats, how='left', left_index=True, right_index=True)\nTrain_data = Train_data.merge(transformed_cell_feats, how='left', left_index=True, right_index=True)\nTrain_data[['ctl_vehicle','trt_cp']] = pd.get_dummies(Train_data.cp_type)\nTrain_data[['D1','D2']] = pd.get_dummies(Train_data.cp_dose)\nTrain_data.drop(columns=['cp_type','cp_dose'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(Train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_data_scaled = pd.DataFrame(scaler.transform(Train_data))\nTrain_data_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_targets = scored_train_targets_df.drop(columns=['sig_id'])\nTrain_targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ns_Train_targets = nscored_train_targets_df.drop(columns=['sig_id'])\nns_Train_targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_idx = np.random.choice(range(Train_data_scaled.shape[0]), Train_data_scaled.shape[0]//10, replace=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Tuner_train_data = np.delete(np.array(Train_data_scaled), val_idx, 0)\nTuner_train_target = np.delete(np.array(ns_Train_targets), val_idx, 0)\n\nTuner_val_data = np.array(Train_data_scaled)[val_idx,:]\nTuner_val_target = np.array(ns_Train_targets)[val_idx,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_min = 0.001\np_max = 0.999\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(hp):\n    \n    Model = Sequential()\n    Model.add(Dense(hp.Choice('input_units', values=[512,1024,2048]), activation='relu', kernel_initializer='he_normal', input_shape=(Train_data_scaled.shape[1],)))\n    Model.add(Dropout(hp.Float('input_drop', min_value=0.3, max_value=0.9,step=0.1)))\n    Model.add(BatchNormalization())\n              \n    for i in range(hp.Int('nbr_lay', min_value=5, max_value=10, step=1)):\n        Model.add(Dense(hp.Choice(f'dense_{i}_units', values=[256,512,1024]), activation=hp.Choice(f'dense_{i}_act', values=['relu','elu','swish']), kernel_initializer='he_normal'))\n        Model.add(Dropout(hp.Float(f'lay_{i}_drop', min_value=0.2, max_value=0.9,step=0.1)))\n        Model.add(BatchNormalization())\n    \n    Model.add(Dense(402, activation='sigmoid', kernel_initializer='he_normal'))\n    \n    Model.compile(optimizer = 'adam', loss=losses.BinaryCrossentropy(label_smoothing=0.001), metrics=logloss)\n    \n    return Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Tuner = RandomSearch(build_model,\n                    objective='val_loss',\n                    max_trials=77,\n                    executions_per_trial=1,\n                    seed=77,\n                    directory='./')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"Tuner.search(Tuner_train_data, Tuner_train_target,\n             epochs=10, verbose=0,\n             validation_data=(Tuner_val_data, Tuner_val_target))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model_tuned = Tuner.get_best_models(num_models=1)\nModel_tuned[0].summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trials_dir=[]\nfor s in os.listdir('./untitled_project'):\n    if 'trial_' in s:\n        trials_dir.append(s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Trials = []\nfor i in trials_dir:\n    with open(f'./untitled_project/{i}/trial.json', 'r') as handle:\n        parsed = json.load(handle)\n        Trials.append((parsed['hyperparameters']['values'],parsed['score']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_score=1\nfor trial in Trials:\n    if trial[1]<max_score:\n        best_trial = trial\n        max_score = trial[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_trial","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"splits_nbr = 10\nkf = KFold(n_splits = splits_nbr)\nskf = StratifiedKFold(n_splits = splits_nbr, random_state = 10, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model_name(k):\n    return 'Model_'+str(k)+'.h5'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VAL_ACCURACY = []\nVAL_LOSS = []\n\nfold_var = 1\n\nfor train_index, val_index in kf.split(Train_data_scaled,Train_targets):\n    \n    train_data = Train_data_scaled.iloc[train_index]\n    train_target = Train_targets.iloc[train_index]\n    \n    val_data = Train_data_scaled.iloc[val_index]\n    val_target = Train_targets.iloc[val_index]\n    \n    Model = Sequential(name='FFN')\n    Model.add(Dense(best_trial[0]['input_units'], activation='relu', kernel_initializer='he_normal', input_shape=(Train_data_scaled.shape[1],)))\n    Model.add(Dropout(best_trial[0]['input_drop']))\n    Model.add(BatchNormalization())\n    for l in range(best_trial[0]['nbr_lay']):\n        Model.add(Dense(best_trial[0][f'dense_{l}_units'], activation=best_trial[0][f'dense_{l}_act'], kernel_initializer='he_normal'))\n        Model.add(Dropout(best_trial[0][f'lay_{l}_drop']))\n        Model.add(BatchNormalization())\n    \n    Model.add(Dense(206, activation='sigmoid', kernel_initializer='he_normal'))\n\n    Model.compile(optimizer = 'adam', loss=losses.BinaryCrossentropy(label_smoothing=0.001), metrics='accuracy')\n    \n    Checkpoint = tf.keras.callbacks.ModelCheckpoint(get_model_name(fold_var), \n                                                    monitor='val_loss', verbose=1, \n                                                    save_best_only=True, mode='min')\n\n    LR_OnPlat = ReduceLROnPlateau(monitor='val_loss',\n                                  patience = 2,\n                                  cooldown=1,\n                                  verbose=1,\n                                  factor=0.8,\n                                  epsilon=1e-4,\n                                  min_lr=0.000001)\n    \n    results = Model.fit(x=train_data,\n                        y=train_target,\n                        batch_size=64,\n                        validation_data=(val_data,val_target),\n                        epochs=50,\n                        verbose=0,\n                        callbacks=[LR_OnPlat, Checkpoint])\n    \n    Model.load_weights(\"./\"+get_model_name(fold_var))\n    \n    results = Model.evaluate(val_data, val_target, batch_size=128)\n    results = dict(zip(Model.metrics_names,results))\n\n    VAL_LOSS.append(results['loss'])\n\n    tf.keras.backend.clear_session()\n\n    fold_var += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VAL_LOSS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model_1 = load_model('./Model_1.h5')\nModel_2 = load_model('./Model_2.h5')\nModel_3 = load_model('./Model_3.h5')\nModel_4 = load_model('./Model_4.h5')\nModel_5 = load_model('./Model_5.h5')\nModel_6 = load_model('./Model_6.h5')\nModel_7 = load_model('./Model_7.h5')\nModel_8 = load_model('./Model_8.h5')\nModel_9 = load_model('./Model_9.h5')\nModel_10 = load_model('./Model_10.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logloss_np(y_true, y_pred):\n    y_pred = np.clip(y_pred,p_min,p_max)\n    return -np.mean(np.array(y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models=[Model_1,Model_2,Model_3,Model_4,Model_5,Model_6,Model_7,Model_8,Model_9,Model_10]\nval_losses=[]\nfor i in range(1,splits_nbr+1):\n    temp = logloss_np(Train_targets,models[i-1].predict(Train_data_scaled))\n    val_losses.append(temp)\n\nAVG_Val_Loss= np.mean(val_losses)\nprint(f'The average validation log loss: {AVG_Val_Loss}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_data = test_feat_df.drop(columns=['sig_id']+list(gene_cols)+list(cell_via_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_gene_data = test_feat_df[gene_cols]\ntest_cell_via_data = test_feat_df[cell_via_cols]\nscaled_test_gene_data = scale(test_gene_data)\nscaled_test_cell_via_data = scale(test_cell_via_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_test_gene = pca1.transform(scaled_test_gene_data)\npca_test_cell = pca2.transform(scaled_test_cell_via_data)\nlabels_gene_pca = ['PC' + str(x) for x in range(1, pca_test_gene.shape[1]+1)]\nlabels_cell_pca = ['PC' + str(x) for x in range(1, pca_test_cell.shape[1]+1)]\ntransformed_test_gene_feats = pd.DataFrame(pca_test_gene, columns=labels_gene_pca)\ntransformed_test_cell_feats = pd.DataFrame(pca_test_cell, columns=labels_cell_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_data = Test_data.merge(transformed_test_gene_feats, how='left', left_index=True, right_index=True)\nTest_data = Test_data.merge(transformed_test_cell_feats, how='left', left_index=True, right_index=True)\nTest_data[['ctl_vehicle','trt_cp']] = pd.get_dummies(Test_data.cp_type)\nTest_data[['D1','D2']] = pd.get_dummies(Test_data.cp_dose)\nTest_data.drop(columns=['cp_type','cp_dose'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler2 = StandardScaler()\nscaler2.fit(Test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_data_scaled = pd.DataFrame(scaler.transform(Test_data))\nTest_data_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models_pred = np.zeros((Test_data_scaled.shape[0],206))\nfor i in range(1,splits_nbr+1):\n    temp = models[i-1].predict(Test_data_scaled)\n    models_pred += temp\nAVG_test_pred = models_pred/splits_nbr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Prediction = pd.DataFrame(AVG_test_pred, columns=Train_targets.columns)\nsig_id_df = pd.DataFrame(test_feat_df.sig_id)\nPrediction = sig_id_df.merge(Prediction, how='left', left_index=True, right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Prediction.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}