{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style='ticks')\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/lish-moa/train_features.csv')\ndf_test = pd.read_csv('../input/lish-moa/test_features.csv')\ndf_train_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ndf_train_unscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\nprint(df_train.head())\nprint(df_train_scored.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train=pd.concat([df_train,df_test])\ndata_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data_train.columns)\nprint(df_train_scored.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_scored.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of Train Data: ', data_train.shape)\nprint('Shape of Train Scored Data: ', df_train_scored.shape)\nprint('Shape of Train Unscored Data: ', df_train_unscored.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round((data_train.isnull().sum()/len(data_train))*100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train['cp_type'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data_train['cp_type'])\nplt.title('Train: Treatment Doses: Low and High',weight='bold', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data_train['cp_dose'])\nplt.title('Train: Control and treated samples', fontsize=15, weight='bold')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train['cp_type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.cp_type.value_counts(normalize=True).plot(kind='pie', figsize=(12, 5), fontsize=12,\n                                                         title='CP Type', autopct='%1.1f%%')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Datasets for treated and control experiments\ntreated= data_train[data_train['cp_type']=='trt_cp']\ncontrol= data_train[data_train['cp_type']=='ctl_vehicle']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.cp_time.value_counts(normalize=True).plot(kind='bar', figsize=(12, 5), fontsize=14,\n                                                         title='CP Time', xlabel='Time')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"treated","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.cp_dose.value_counts(normalize=True).plot(kind='bar', figsize=(12, 5), fontsize=14, \n                                                         title='CP Dose', xlabel='Dose')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Treatment time datasets\ncp24= data_train[data_train['cp_time']== 24]\ncp48= data_train[data_train['cp_time']== 48]\ncp72= data_train[data_train['cp_time']== 72]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Merge scored and nonscored labels\ndrugs= pd.merge(df_train_scored, df_train_unscored, on='sig_id', how='inner')\ndrugs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Treated drugs:\ntreated_list = treated['sig_id'].to_list()\ndrugs_tr= df_train_scored[df_train_scored['sig_id'].isin(treated_list)]\n\nunscored= df_train_unscored[df_train_unscored['sig_id'].isin(treated_list)]\nscored= df_train_scored[df_train_scored['sig_id'].isin(treated_list)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#adt= All Drugs Treated\nadt= drugs[drugs['sig_id'].isin(treated_list)]\nc_cols = [col for col in df_train.columns if 'c-' in col]\ncells=treated[c_cols]\n\ng_cols = [col for col in df_train.columns if 'g-' in col]\ngenes=treated[g_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_cols = sns.pairplot(data_train[g_cols[:10]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_cols = sns.pairplot(data_train[c_cols[:10]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotd(f1):\n    plt.style.use('seaborn')\n    sns.set_style('whitegrid')\n    fig = plt.figure(figsize=(15,5))\n    #1 rows 2 cols\n    #first row, first col\n    ax1 = plt.subplot2grid((1,2),(0,0))\n    plt.hist(control[f1], bins=4, color='mediumpurple',alpha=0.5)\n    plt.title(f'control: {f1}',weight='bold', fontsize=18)\n    #first row sec col\n    ax1 = plt.subplot2grid((1,2),(0,1))\n    plt.hist(treated[f1], bins=4, color='darkcyan',alpha=0.5)\n    plt.title(f'Treated with compound: {f1}',weight='bold', fontsize=18)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plott(f1):\n    sns.set_style('whitegrid')\n    fig = plt.figure(figsize=(15,5))\n    #1 rows 2 cols\n    #first row, first col\n    ax1 = plt.subplot2grid((1,3),(0,0))\n    plt.hist(cp24[f1], bins=3, color='deepskyblue',alpha=0.5)\n    plt.title(f'Treatment duration 24h: {f1}',weight='bold', fontsize=14)\n    #first row sec col\n    ax1 = plt.subplot2grid((1,3),(0,1))\n    plt.hist(cp48[f1], bins=3, color='lightgreen',alpha=0.5)\n    plt.title(f'Treatment duration 48h: {f1}',weight='bold', fontsize=14)\n    #first row 3rd column\n    ax1 = plt.subplot2grid((1,3),(0,2))\n    plt.hist(cp72[f1], bins=3, color='gold',alpha=0.5)\n    plt.title(f'Treatment duration 72h: {f1}',weight='bold', fontsize=14)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotf(f1, f2, f3, f4):\n    sns.set_style('whitegrid')\n    fig= plt.figure(figsize=(15,10))\n    ax1 = plt.subplot2grid((2,2),(0,0))\n    plt.hist(df_train[f1], bins=3, color='orange', alpha=0.7)\n    plt.title(f1,weight='bold', fontsize=18)\n    plt.yticks(weight='bold')\n    plt.xticks(weight='bold')\n    #first row sec col\n    ax1 = plt.subplot2grid((2,2), (0, 1))\n    plt.hist(df_train[f2], bins=3, alpha=0.7)\n    plt.title(f2,weight='bold', fontsize=18)\n    plt.yticks(weight='bold')\n    plt.xticks(weight='bold')\n    #Second row first column\n    ax1 = plt.subplot2grid((2,2), (1, 0))\n    plt.hist(df_train[f3], bins=3, color='red', alpha=0.7)\n    plt.title(f3,weight='bold', fontsize=18)\n    plt.yticks(weight='bold')\n    plt.xticks(weight='bold')\n    #second row second column\n    ax1 = plt.subplot2grid((2,2), (1, 1))\n    plt.hist(df_train[f4], bins=3, color='green', alpha=0.7)\n    plt.title(f4,weight='bold', fontsize=18)\n    plt.yticks(weight='bold')\n    plt.xticks(weight='bold')\n    return plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ploth(data, w=15, h=9):\n    plt.figure(figsize=(w,h))\n    sns.heatmap(data.corr(), cmap='hot')\n    plt.title('Correlation between the drugs', fontsize=18, weight='bold')\n    return plt.show()\n\n# corrs function: Show dataframe of high correlation between features\ndef corrs(data, col1='Gene 1', col2='Gene 2',rows=5,thresh=0.8, pos=[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35]):\n        #Correlation between genes\n        corre= data.corr()\n         #Unstack the dataframe\n        s = corre.unstack()\n        so = s.sort_values(kind=\"quicksort\", ascending=False)\n        #Create new dataframe\n        so2= pd.DataFrame(so).reset_index()\n        so2= so2.rename(columns={0: 'correlation', 'level_0':col1, 'level_1': col2})\n        #Filter out the coef 1 correlation between the same drugs\n        so2= so2[so2['correlation'] != 1]\n        #Drop pair duplicates\n        so2= so2.reset_index()\n        pos = pos\n        so2= so2.drop(so2.index[pos])\n        so2= so2.drop('index', axis=1)\n        #Show the first 10 high correlations\n        cm = sns.light_palette(\"Red\", as_cmap=True)\n        s = so2.head(rows).style.background_gradient(cmap=cm)\n        print(f\"{len(so2[so2['correlation']>thresh])/2} {col1} pairs have +{thresh} correlation.\")\n        return s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,5))\nax1 = plt.subplot2grid((1,2),(0,0))\nsns.countplot(x='cp_type', data=df_train, palette='rainbow', alpha=0.75)\nplt.title('Train: Control and treated samples', fontsize=15, weight='bold')\nax1 = plt.subplot2grid((1,2),(0,1))\nsns.countplot(x='cp_dose', data=df_train, palette='Purples', alpha=0.75)\nplt.title('Train: Treatment Doses: Low and High',weight='bold', fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_scored = df_train_scored.sum()[1:].sort_values()\ndf_train_scored[:50].plot(kind='barh', title='Least Target Occurances', fontsize='14', figsize=(5, 20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_scored[-50:].plot(kind='barh', title='Most Target Occurences', fontsize='12', figsize=(12, 20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see outliers in genomes.\n\nplt.figure(figsize=(100, 100))\nsns.heatmap(data_train[c_cols].corr())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.distplot( data_train['cp_time'], color='green', bins=5)\nplt.title(\"Train: Treatment Time \", fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotf('c-10', 'c-50', 'c-70', 'c-90')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plott('c-30')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_cols = [col for col in df_train.columns if 'c-' in col]\n#Filter the columns c-\ncells=treated[c_cols]\n#Plot heatmap FOR corelations\nplt.figure(figsize=(12,6))\nsns.heatmap(cells.corr(), cmap='coolwarm', alpha=0.9)\nplt.title('Correlation: Cell viability', fontsize=15, weight='bold')\nplt.xticks(weight='bold')\nplt.yticks(weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc_corr = {}\nuseful_col = c_cols - gc_corr.keys()\nuseful_col = list(useful_col)\nuseful_col = useful_col + ['cp_type', 'cp_dose', 'cp_time']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"useful_col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(useful_col), len(c_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs(cells, 'Cell', 'Cell 2', rows=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs(genes, 'Gene', 'Gene 2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation between drugs\ncorre= genes.corr()\n#Unstack the dataframe\ns = corre.unstack()\nso = s.sort_values(kind=\"quicksort\", ascending=False)\n#Create new dataframe\nso2= pd.DataFrame(so).reset_index()\nso2= so2.rename(columns={0: 'correlation', 'level_0':'Drug 1', 'level_1': 'Drug2'})\n#Filter out the coef 1 correlation between the same drugs\nso2= so2[so2['correlation'] != 1]\n#Drop pair duplicates\nso2= so2.reset_index()\nso2= so2.sort_values(by=['correlation'])\npos = [1,3,5,7,9,11,13,15,17,19,21]\nso2= so2.drop(so2.index[pos])\nso2= so2.round(decimals=4)\nso2=so2.drop('index', axis=1)\nso3=so2.head(4)\n#Show the first 10 high correlations\ncm = sns.light_palette(\"Red\", as_cmap=True)\ns = so2.head().style.background_gradient(cmap=cm)\ns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scored= df_train_scored[df_train_scored['sig_id'].isin(treated_list)]\n\n#Count unique values per column\ncols = drugs_tr.columns.to_list() # specify the columns whose unique values you want here\nuniques = {col: drugs_tr[col].nunique() for col in cols}\nuniques=pd.DataFrame(uniques, index=[0]).T\nuniques=uniques.rename(columns={0:'count'})\nuniques= uniques.drop('sig_id', axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Calculate the mean values\naverage=scored.mean()\naverage=pd.DataFrame(average)\naverage=average.rename(columns={ 0: 'mean'})\naverage['percentage']= average['mean']*100\n#Filter just the drugs with mean >0.01\naverage_filtered= average[average['mean'] > 0.01]\naverage_filtered= average_filtered.reset_index()\naverage_filtered= average_filtered.rename(columns={'index': 'drug'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ctl_vehicle_id = data_train[data_train.cp_type == 'ctl_vehicle']['sig_id']\nctl_vehicle_id = list(ctl_vehicle_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sum_target_cp_type_ctl_vehicle = data_train[df_train.sig_id.isin( ctl_vehicle_id)].sum()[1:].sum()\n\n# print('Training Data - For cp_type - ctl_vehicle total sum of targets : {}'.format(sum_target_cp_type_ctl_vehicle))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train['cp_type'] = data_train.cp_type.map({'trt_cp':0, 'ctl_vehicle':1})\ndf_test['cp_type'] = df_test.cp_type.map({'trt_cp':0, 'ctl_vehicle':1})\n\ndata_train['cp_dose'] = data_train.cp_dose.map({'D1':0, 'D2':1})\ndf_test['cp_dose'] = df_test.cp_dose.map({'D1':0, 'D2':1})\n\ndata_train.cp_type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,5))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((1,2),(0,0))\nsns.countplot(uniques['count'], color='deepskyblue', alpha=0.75)\nplt.title('Unique elements per drug [0,1]', fontsize=15, weight='bold')\n#first row sec col\nax1 = plt.subplot2grid((1,2),(0,1))\nsns.distplot(average['percentage'], color='orange', bins=20)\nplt.title(\"The drugs mean distribution\", fontsize=15, weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,7))\nplt.scatter(average_filtered['percentage'\n                            ].sort_values(), average_filtered\n            ['drug'], color=sns.color_palette('Reds',len(average_filtered)))\nplt.title('Drugs - Higher presence in train samples', weight='bold', fontsize=15)\nplt.xticks(weight='bold')\nplt.yticks(weight='bold')\nplt.xlabel('Percentage', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inhibitors = [col for col in df_train_scored.columns if 'inhibitor' in col]\nactivators = [col for col in df_train_scored.columns if 'activator' in col]\nantagonists = [col for col in df_train_scored.columns if 'antagonist' in col]\nagonists = [col for col in df_train_scored.columns if 'agonist' in col]\nmodulators = [col for col in df_train_scored.columns if 'modulator' in col]\nreceptors = [col for col in df_train_scored.columns if 'receptor' in col]\nreceptors_ago = [col for col in df_train_scored.columns if 'receptor_agonist' in col]\nreceptors_anta = [col for col in df_train_scored.columns if 'receptor_antagonist' in col]\n\n\nlabelss= {'Drugs': ['inhibitors', 'activators', 'antagonists', 'agonists', 'receptors', 'receptors_ago', 'receptors_anta'],\n          'Count':[112,5,32,60, 53, 24, 26]}\n\n\nlabels= pd.DataFrame(labelss)\nlabels=labels.sort_values(by=['Count'])\nplt.figure(figsize=(15,5))\nplt.bar(labels['Drugs'], labels['Count'], color=sns.color_palette('Reds',len(labels)))\nplt.xticks(weight='bold')\nplt.title('Compounds types', weight='bold', fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ploth(drugs_tr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extract unique elements per column\ncols2 = unscored.columns.to_list() # specify the columns whose unique values you want here\nuniques2 = {col: unscored[col].nunique() for col in cols2}\nuniques2=pd.DataFrame(uniques2, index=[0]).T\nuniques2=uniques2.rename(columns={0:'count'})\nuniques2= uniques2.drop('sig_id', axis=0)\n\n#############################\n### PLOT\nplt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,5))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((1,2),(0,0))\nsns.countplot(uniques2['count'], palette='Blues', alpha=0.75)\nplt.title('Un-scored: Unique elements per drug [0,1]', fontsize=13, weight='bold')\n#first row sec col\nax1 = plt.subplot2grid((1,2),(0,1))\nsns.countplot(uniques['count'], color='cyan', alpha=0.75)\nplt.title('Scored: Unique elements per drug [0,1]', fontsize=13, weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"{len(uniques2[uniques2['count']==1])} drugs without ANY mechanism of action in the nonscored dataset\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"average2=unscored.mean()\naverage2=pd.DataFrame(average2)\naverage2=average2.rename(columns={ 0: 'mean'})\naverage2['percentage']= average2['mean']*100\naverage_filtered2= average2[average2['mean'] > 0.01]\naverage_filtered2= average_filtered2.reset_index()\naverage_filtered2= average_filtered2.rename(columns={'index': 'drug'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,5))\nax1 = plt.subplot2grid((1,2),(0,0))\nsns.distplot(average2['percentage'], color='blue', bins=20)\nplt.title('Percentage of the nonscored MoAs in the samples',weight='bold', fontsize=13)\nax1 = plt.subplot2grid((1,2),(0,1))\nsns.distplot(average['percentage'], color='gold', bins=20)\nplt.title('Percentage of the scored MoAs in the samples',weight='bold', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs(adt, 'Drug', 'Drug 2', 15, thresh= 0.7)\ncorre= adt.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Unstack the dataframe\ns = corre.unstack()\nso = s.sort_values(kind=\"quicksort\", ascending=False)\n#Create new dataframe\nso2= pd.DataFrame(so).reset_index()\nso2= so2.rename(columns={0: 'correlation', 'level_0':'Drug 1', 'level_1': 'Drug2'})\n#Filter out the coef 1 correlation between the same drugs\nso2= so2[so2['correlation'] != 1]\n#Drop pair duplicates\nso2= so2.reset_index()\npos = [1,3,5,7,9, 11,13,15,17,19,21,23, 25, 27, 29, 31,33,35, 37, 39, 41, 43, 45]\nso2= so2.drop(so2.index[pos])\n#so2= so2.round(decimals=4)\nso3=so2.head()\n#Show the first 10 high correlations\ncm = sns.light_palette(\"Red\", as_cmap=True)\ns = so2.head(16).style.background_gradient(cmap=cm)\ns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#High correlation adt 22 pairs\nadt15= so2.head(22)\n#Filter the drug names\nadt_1=adt15['Drug 1'].values.tolist()\nadt_2=adt15['Drug2'].values.tolist()\n#Join the 2 lists\nadt3= adt_1 + adt_2\n#Keep unique elements and drop duplicates\nadt4= list(dict.fromkeys(adt3))\n#Filter out the selected drugs from the \"all drugs treated\" adt dataset\nadt5= adt[adt4]\nploth(adt5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,5))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((1,2),(0,0))\nsns.countplot(x='cp_type', data=df_test, palette='rainbow', alpha=0.75)\nplt.title('Test: Control and treated samples', fontsize=15, weight='bold')\n#first row sec col\nax1 = plt.subplot2grid((1,2),(0,1))\nsns.countplot(x='cp_dose', data=df_test, palette='rainbow', alpha=0.75)\nplt.title('Test: Treatment Doses: Low and High',weight='bold', fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13,3))\nsns.distplot( df_train['cp_time'], color='gold', bins=5)\nplt.title(\"Test: Treatment duration \", fontsize=15, weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filter out just the treated samples\ntreated2= df_test[df_test['cp_type']=='trt_cp']\ntreated_list2 = treated2['sig_id'].to_list()\nfull_tr= df_test[df_test['sig_id'].isin(treated_list2)]\n\n#Select the columns c-\nc_cols2 = [col for col in full_tr.columns if 'g-' in col]\n#Filter the columns c-\ncells2=treated2[c_cols2]\n#Plot heatmap\nplt.figure(figsize=(15,6))\nsns.heatmap(cells2.corr(), cmap='coolwarm', alpha=0.9)\nplt.title('Test: Correlation gene expression', fontsize=15, weight='bold')\nplt.xticks(weight='bold')\nplt.yticks(weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation between drugs\ncorre= cells2.corr()\n#Unstack the dataframe\ns = corre.unstack()\nso = s.sort_values(kind=\"quicksort\", ascending=False)\n#Create new dataframe\nso2= pd.DataFrame(so).reset_index()\nso2= so2.rename(columns={0: 'correlation', 'level_0':'Gene 1', 'level_1': 'Gene 2'})\n#Filter out the coef 1 correlation between the same drugs\nso2= so2[so2['correlation'] != 1]\n#Drop pair duplicates\nso2= so2.reset_index()\n#so2= so2.sort_values(by=['correlation'])\npos = [1,3,5,7,9,11,13,15,17,19,21]\nso2= so2.drop(so2.index[pos])\nso2= so2.round(decimals=4)\nso2=so2.drop('index', axis=1)\nso4=so2.head(10)\ncm = sns.light_palette(\"Red\", as_cmap=True)\ns = so2.head(10).style.background_gradient(cmap=cm)\ns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Select the columns c-\nc_cols3 = [col for col in df_test.columns if 'c-' in col]\n#Filter the columns c-\ncells3=treated[c_cols3]\n#Plot heatmap\nplt.figure(figsize=(12,6))\nsns.heatmap(cells3.corr(), cmap='coolwarm', alpha=0.9)\nplt.title('Correlation: Cell viability', fontsize=15, weight='bold')\nplt.xticks(weight='bold')\nplt.yticks(weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n        \n        inp = Input(shape=(X.shape[1], ))\n        batch_norm = BatchNormalization()(inp)\n        \n        x1 = WeightNormalization(Dense(units=1024,\n                                       activation='elu'))(batch_norm)\n        drop = Dropout(0.5)(x1)\n        batch_norm = BatchNormalization()(drop)\n        \n        \n        x1 = WeightNormalization(Dense(640,\n                                       activation='elu'))(batch_norm)\n        drop = Dropout(0.5)(x1)\n        batch_norm = BatchNormalization()(drop)\n        \n        x1 = WeightNormalization(Dense(768,\n                                       activation='elu'))(batch_norm)\n        drop = Dropout(0.5)(x1)\n        batch_norm = BatchNormalization()(drop)        \n\n        x1 = WeightNormalization(Dense(units=256,\n                                       activation='elu'))(batch_norm)\n        drop = Dropout(0.5)(x1)\n        batch_norm = BatchNormalization()(drop)\n        \n        dense = Dense(Y.shape[1], activation='sigmoid')(batch_norm)\n        \n        model = Model(inputs=inp, outputs=dense)\n        \n        model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n        \n        return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_log_loss(Y, val_Y):    \n    full_loss = []\n    for i, col in enumerate(Y.columns):\n        loss = log_loss(Y.loc[:, col].values, val_Y[:, i].astype(float), labels=[0, 1])\n        full_loss.append(loss)\n    loss = np.mean(full_loss)\n   \n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(X, Y, X_Test, folds = 3, seed=3, batch_size = 128, epochs = 50, learning_rate=1e-4):\n    \n    submission = sample_submission.drop('sig_id', axis=1).copy()\n    submission.loc[:, :] = 0\n    \n    mkf = IterativeStratification(n_splits=folds, random_state=seed, shuffle=True)\n\n    final_log_loss = []\n    \n    T_logloss = []\n    V_logloss = []\n    \n    for n in range(seed):\n        \n        for fold, (train_idx, val_idx) in enumerate(mkf.split(X, Y)):\n\n            print('\\n Run - {}, Fold - {}'.format(n, fold))\n\n            X_train = X.loc[train_idx, :]\n            Y_train = Y.loc[train_idx, :]\n\n            X_val = X.loc[val_idx, :]\n            Y_val = Y.loc[val_idx, :]\n\n            model = build_model()\n\n            reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.15, patience=3, verbose=1,\n                                               epsilon=learning_rate, mode='min')\n            \n            early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, mode= 'min')\n            \n            history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n                      callbacks=[reduce_lr_loss, early_stop], validation_data=(X_val, Y_val), verbose=2)\n\n            Train_Pred = model.predict(X_train)\n            Val_pred = model.predict(X_val)\n        \n            Train_Pred = Train_Pred/(seed*(fold+1))\n            Val_pred = Val_pred/(seed*(fold+1))\n            \n            train_logloss = get_log_loss(Y_train, Train_Pred)\n            val_logloss = get_log_loss(Y_val, Val_pred)\n        \n            print('Training Log Loss : {:.6}'.format(train_logloss))\n            print('Validation Log Loss : {:.6}'.format(val_logloss))\n\n            T_logloss.append(train_logloss)\n            V_logloss.append(val_logloss)\n            \n            submission += model.predict(X_Test)\n            \n            submission = submission/((fold+1)*seed)\n    \n        final_T_logloss = np.mean(T_logloss)\n        final_V_logloss = np.mean(V_logloss)\n\n        print('Final Training Log Loss : {:.6}'.format(final_T_logloss))\n        print('Final Validation Log Loss : {:.6}'.format(final_V_logloss))\n    return submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.sig_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_train.drop('sig_id', axis=1)\nY = df_train_unscored.drop('sig_id', axis=1)\n\nX_test = df_test.drop('sig_id', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xseed = 43\n# number of folds for cv\nnfolds = 5\n# number of components to retain from PCA decomposition\nnof_comp = 250\nmodel_name = 'svm'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # create the submission file\n# sub.iloc[:,1:] = Test_preds\n# sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}