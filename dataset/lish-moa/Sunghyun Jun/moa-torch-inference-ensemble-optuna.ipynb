{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install optuna -q","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport random\nimport sys\n\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom time import time\n\nimport optuna\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, optim, Tensor\nfrom torch.utils.data import DataLoader\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import QuantileTransformer\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    # torch.backends.cudnn.benchmark = False\n    # torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map(lambda x: int(x/24 -2)) # -1, 0, 1\n    del df['sig_id']\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mapping_filter(train, test, scored, nonscored, drug):\n    train = preprocess(train)\n    test = preprocess(test)\n        \n    scored = scored.loc[train['cp_type']==0].reset_index(drop=True)\n    nonscored = nonscored.loc[train['cp_type']==0].reset_index(drop=True)\n    drug = drug.loc[train['cp_type']==0].reset_index(drop=True)\n    \n    train = train.loc[train['cp_type']==0].reset_index(drop=True)\n    \n    del scored['sig_id']\n    del nonscored['sig_id']\n    \n    return train, test, scored, nonscored, drug","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def qt_transform(train, test, SEED = 42):\n    qt = QuantileTransformer(n_quantiles=100, random_state=SEED, output_distribution='normal')\n    len_train = train.shape[0]\n    len_test = test.shape[0]\n\n    features_g = train.columns[3:775]\n    features_c = train.columns[775:875]\n    \n    for columns in (features_g, features_c):\n        qt.fit(train[columns])\n        train[columns] = qt.transform(train[columns])\n        test[columns] = qt.transform(test[columns])\n        \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n\n    def __len__(self):\n        return (self.features.shape[0])\n\n    def __getitem__(self, idx):\n        dct = {\n            'x': torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y': torch.tensor(self.targets[idx, :], dtype=torch.float),\n        }\n\n        return dct\n\nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n\n    def __len__(self):\n        return (self.features.shape[0])\n\n    def __getitem__(self, idx):\n        dct = {\n            'x': torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n\n        return dct","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n\n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        final_loss += loss.item()\n\n    final_loss /= len(dataloader)\n    return final_loss\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n\n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n\n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n\n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n\n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n    preds = np.concatenate(preds)\n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            \n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model_2l(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(Model_2l, self).__init__()\n        self.hidden_size = [2400, 800]\n        self.dropout_rate = 0.27\n\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n#         self.dropout1 = nn.Dropout(self.dropout_rate)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, self.hidden_size[0]))\n\n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_rate)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(self.hidden_size[0], self.hidden_size[1]))\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_rate)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(self.hidden_size[1], num_targets))\n\n    def forward(self, x):\n        x = self.batch_norm1(x)\n#         x = self.dropout1(x)\n        x = F.leaky_relu(self.dense1(x))\n\n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model_3l(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(Model_3l, self).__init__()\n        self.hidden_size = [1700, 600, 1000]\n        self.dropout_rate = 0.21\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(self.dropout_rate)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, self.hidden_size[0]))\n\n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_rate)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(self.hidden_size[0], self.hidden_size[1]))\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_rate)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(self.hidden_size[1], self.hidden_size[2]))\n\n        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n        self.dropout4 = nn.Dropout(self.dropout_rate)\n        self.dense4 = nn.utils.weight_norm(nn.Linear(self.hidden_size[2], num_targets))\n\n    def recalibrate_layer(self, layer):\n        if(torch.isnan(layer.weight_v).sum() > 0):\n            print ('recalibrate layer.weight_v')\n            layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n            layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n\n        if(torch.isnan(layer.weight).sum() > 0):\n            print ('recalibrate layer.weight')\n            layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n            layer.weight += 1e-7\n\n\n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        self.recalibrate_layer(self.dense1)\n        x = F.leaky_relu(self.dense1(x))\n\n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        self.recalibrate_layer(self.dense2)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        self.recalibrate_layer(self.dense3)\n        x = F.leaky_relu(self.dense3(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = self.dense4(x)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model_4l(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(Model_4l, self).__init__()\n        self.hidden_size = [1400, 700, 300, 700]\n        self.dropout_rate = 0.16\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(self.dropout_rate)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, self.hidden_size[0]))\n\n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_rate)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(self.hidden_size[0], self.hidden_size[1]))\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_rate)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(self.hidden_size[1], self.hidden_size[2]))\n\n        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n        self.dropout4 = nn.Dropout(self.dropout_rate)\n        self.dense4 = nn.utils.weight_norm(nn.Linear(self.hidden_size[2], self.hidden_size[3]))\n\n        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n        self.dropout5 = nn.Dropout(self.dropout_rate)\n        self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n\n    def recalibrate_layer(self, layer):\n        if(torch.isnan(layer.weight_v).sum() > 0):\n            print ('recalibrate layer.weight_v')\n            layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n            layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n\n        if(torch.isnan(layer.weight).sum() > 0):\n            print ('recalibrate layer.weight')\n            layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n            layer.weight += 1e-7\n\n\n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        self.recalibrate_layer(self.dense1)\n        x = F.leaky_relu(self.dense1(x))\n\n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        self.recalibrate_layer(self.dense2)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        self.recalibrate_layer(self.dense3)\n        x = F.leaky_relu(self.dense3(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        self.recalibrate_layer(self.dense4)\n        x = F.leaky_relu(self.dense4(x))\n\n        x = self.batch_norm5(x)\n        x = self.dropout5(x)\n        x = self.dense5(x)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model_5l(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(Model_5l, self).__init__()\n        self.hidden_size = [1100, 1900, 100, 200, 2000]\n        self.dropout_rate = 0.18\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(self.dropout_rate)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, self.hidden_size[0]))\n\n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_rate)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(self.hidden_size[0], self.hidden_size[1]))\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_rate)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(self.hidden_size[1], self.hidden_size[2]))\n\n        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n        self.dropout4 = nn.Dropout(self.dropout_rate)\n        self.dense4 = nn.utils.weight_norm(nn.Linear(self.hidden_size[2], self.hidden_size[3]))\n\n        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n        self.dropout5 = nn.Dropout(self.dropout_rate)\n        self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], self.hidden_size[4]))\n\n        self.batch_norm6 = nn.BatchNorm1d(self.hidden_size[4])\n        self.dropout6 = nn.Dropout(self.dropout_rate)\n        self.dense6 = nn.utils.weight_norm(nn.Linear(self.hidden_size[4], num_targets))\n\n    def recalibrate_layer(self, layer):\n        if(torch.isnan(layer.weight_v).sum() > 0):\n            print ('recalibrate layer.weight_v')\n            layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n            layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n\n        if(torch.isnan(layer.weight).sum() > 0):\n            print ('recalibrate layer.weight')\n            layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n            layer.weight += 1e-7\n\n\n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        self.recalibrate_layer(self.dense1)\n        x = F.leaky_relu(self.dense1(x))\n\n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        self.recalibrate_layer(self.dense2)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        self.recalibrate_layer(self.dense3)\n        x = F.leaky_relu(self.dense3(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        self.recalibrate_layer(self.dense4)\n        x = F.leaky_relu(self.dense4(x))\n\n        x = self.batch_norm5(x)\n        x = self.dropout5(x)\n        self.recalibrate_layer(self.dense5)\n        x = F.leaky_relu(self.dense5(x))\n\n        x = self.batch_norm6(x)\n        x = self.dropout6(x)\n        x = self.dense6(x)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model_rs(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(Model_rs, self).__init__()\n        self.hidden_size = [700, 900]\n        self.dropout_rate = 0.34\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(self.dropout_rate)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, self.hidden_size[0]))\n\n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0] + num_features)\n        self.dropout2 = nn.Dropout(self.dropout_rate)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(self.hidden_size[0] + num_features, self.hidden_size[1]))\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1] + self.hidden_size[0] + num_features)\n        self.dropout3 = nn.Dropout(self.dropout_rate)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(self.hidden_size[1] + self.hidden_size[0] + num_features, num_targets))\n\n\n    def recalibrate_layer(self, layer):\n\n        if(torch.isnan(layer.weight_v).sum() > 0):\n            print ('recalibrate layer.weight_v')\n            layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n            layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n\n        if(torch.isnan(layer.weight).sum() > 0):\n            print ('recalibrate layer.weight')\n            layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n            layer.weight += 1e-7\n\n    def forward(self, x):\n        x1 = x\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        self.recalibrate_layer(self.dense1)\n        x = F.leaky_relu(self.dense1(x))\n        x = torch.cat((x, x1), dim=1)\n\n        x2 = x\n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        self.recalibrate_layer(self.dense2)\n        x = F.leaky_relu(self.dense2(x))\n        x = torch.cat((x, x2), dim=1)\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_log_loss(y_true, y_pred):\n    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    metrics = []\n    for target in range(206):\n        metrics.append(log_loss(y_true[:, target], y_pred[:, target]))\n    return np.mean(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LogitsLogLoss(Metric):\n    \"\"\"\n    LogLoss with sigmoid applied\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        \"\"\"\n        Compute LogLoss of predictions.\n\n        Parameters\n        ----------\n        y_true: np.ndarray\n            Target matrix or vector\n        y_score: np.ndarray\n            Score matrix or vector\n\n        Returns\n        -------\n            float\n            LogLoss of predictions vs targets.\n        \"\"\"\n        logits = 1 / (1 + np.exp(-y_pred))\n        aux = (1 - y_true) * np.log(1 - logits + 1e-15) + y_true * np.log(logits + 1e-15)\n        return np.mean(-aux)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nscored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\nnonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ndrug = pd.read_csv('../input/lish-moa/train_drug.csv')\n\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape, test_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape, test_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\nFOLDS = 7\nSEEDS = [1, 2, 3, 4, 5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Drug and MultiLabel Stratification\n[https://www.kaggle.com/c/lish-moa/discussion/195195](https://www.kaggle.com/c/lish-moa/discussion/195195)"},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = scored.columns[1:]\nscored = scored.merge(drug, on='sig_id', how='left')\n\n# LOCATE DRUGS\nvc = scored.drug_id.value_counts()\nvc1 = vc.loc[vc<=18].index.sort_values()\nvc2 = vc.loc[vc>18].index.sort_values()\n\n# STRATIFY DRUGS 18X OR LESS\ndct1 = {}; dct2 = {}\nskf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, \n          random_state=SEED)\ntmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\nfor fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n    dd = {k:fold for k in tmp.index[idxV].values}\n    dct1.update(dd)\n\n# STRATIFY DRUGS MORE THAN 18X\nskf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, \n          random_state=SEED)\ntmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\nfor fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n    dd = {k:fold for k in tmp.sig_id[idxV].values}\n    dct2.update(dd)\n\n# ASSIGN FOLDS\nscored['fold'] = scored.drug_id.map(dct1)\nscored.loc[scored.fold.isna(),'fold'] =\\\n    scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\nscored.fold = scored.fold.astype('int8')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Delete ctrl_vehicle and preprocess"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features, test_features, scored, nonscored, drug \\\n= mapping_filter(train_features, test_features, scored, nonscored, drug)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# re-assign fold index"},{"metadata":{"trusted":true},"cell_type":"code","source":"fold_array = scored.fold.values\n\ntrain_index = [0 for i in range(FOLDS)]\nvalid_index = [0 for i in range(FOLDS)]\n\nfor fold in range(FOLDS):\n    train_index[fold] = np.where(fold_array != fold)[0]\n    valid_index[fold] = np.where(fold_array == fold)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scored.drop(columns=['drug_id', 'fold'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gauss rank scale"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features, test_features = qt_transform(train_features, test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_features.values\ntest = test_features.values\ntrain_targets = scored.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET_COLUMNS = [col for col in targets]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nFEATURES = train.shape[1]\nNTARGETS = train_targets.shape[1]\nEARLY_STOP = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blend_oof_pred = []\nblend_test_preds = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# 2l_nn\noof_pred = []\noof_score = []\ntest_preds = np.zeros((len(SEEDS), FOLDS, test.shape[0], NTARGETS))\n\nfor j, SEED in enumerate(SEEDS):\n    set_seed(SEED)\n    \n    models = []\n\n    oof_pred_ = np.zeros((train.shape[0], NTARGETS))\n    for fold in range(FOLDS):\n\n        valid_X = train[valid_index[fold]]\n        valid_y = train_targets[valid_index[fold]]\n\n        valid_ds = MoADataset(valid_X, valid_y)\n\n        validloader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False)\n\n        test_ds = TestDataset(test)\n        testloader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n    \n        filepath = '../input/moa-torch-train-2l-nn/'\n        filename = f'2l_nn_seed_{SEED}_fold_{fold}.pth'\n        checkpoint = os.path.join(filepath, filename)\n    \n        model = Model_2l(num_features=FEATURES,\n                         num_targets=NTARGETS)\n        \n        model.load_state_dict(torch.load(checkpoint))\n\n        model.to(DEVICE)\n\n        models.append(model)\n        \n        loss_fn = nn.BCEWithLogitsLoss()\n        \n        valid_loss, valid_preds = valid_fn(models[fold], loss_fn, validloader, DEVICE)\n        \n        oof_pred_[valid_index[fold]] = valid_preds\n        test_preds[j, fold, :, :] = inference_fn(models[fold], testloader, DEVICE)\n\n    oof_score_ = mean_log_loss(train_targets, oof_pred_)\n    oof_score.append(oof_score_)\n    oof_pred.append(oof_pred_)\n\noof_pred = np.mean(oof_pred, axis = 0, dtype=np.float64)\nmean_preds = np.mean(test_preds, axis = (0, 1), dtype=np.float64)\n    \nseed_log_loss = mean_log_loss(train_targets, oof_pred)\n\nblend_oof_pred.append(oof_pred)\nblend_test_preds.append(mean_preds)\n\nprint('2L_NN')\nfor j, SEED in enumerate(SEEDS):\n    print(f'SEED:{SEED} Our out of folds mean log loss score is {oof_score[j]}')\n    \nprint(f'Our out of folds log loss for our seed blend model is {seed_log_loss}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# 3l_nn\noof_pred = []\noof_score = []\ntest_preds = np.zeros((len(SEEDS), FOLDS, test.shape[0], NTARGETS))\n\nfor j, SEED in enumerate(SEEDS):\n    set_seed(SEED)\n    \n    models = []\n\n    oof_pred_ = np.zeros((train.shape[0], NTARGETS))\n    for fold in range(FOLDS):\n\n        valid_X = train[valid_index[fold]]\n        valid_y = train_targets[valid_index[fold]]\n\n        valid_ds = MoADataset(valid_X, valid_y)\n\n        validloader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False)\n\n        test_ds = TestDataset(test)\n        testloader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n    \n        filepath = '../input/moa-torch-train-3l-nn/'\n        filename = f'3l_nn_seed_{SEED}_fold_{fold}.pth'\n        checkpoint = os.path.join(filepath, filename)\n    \n        model = Model_3l(num_features=FEATURES,\n                         num_targets=NTARGETS)\n        \n        model.load_state_dict(torch.load(checkpoint))\n\n        model.to(DEVICE)\n\n        models.append(model)\n        \n        loss_fn = nn.BCEWithLogitsLoss()\n        \n        valid_loss, valid_preds = valid_fn(models[fold], loss_fn, validloader, DEVICE)\n        \n        oof_pred_[valid_index[fold]] = valid_preds\n        test_preds[j, fold, :, :] = inference_fn(models[fold], testloader, DEVICE)\n\n    oof_score_ = mean_log_loss(train_targets, oof_pred_)\n    oof_score.append(oof_score_)\n    oof_pred.append(oof_pred_)\n\noof_pred = np.mean(oof_pred, axis = 0, dtype=np.float64)\nmean_preds = np.mean(test_preds, axis = (0, 1), dtype=np.float64)\n    \nseed_log_loss = mean_log_loss(train_targets, oof_pred)\n\nblend_oof_pred.append(oof_pred)\nblend_test_preds.append(mean_preds)\n\nprint('3L_NN')\nfor j, SEED in enumerate(SEEDS):\n    print(f'SEED:{SEED} Our out of folds mean log loss score is {oof_score[j]}')\n    \nprint(f'Our out of folds log loss for our seed blend model is {seed_log_loss}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# 4l_nn\noof_pred = []\noof_score = []\ntest_preds = np.zeros((len(SEEDS), FOLDS, test.shape[0], NTARGETS))\n\nfor j, SEED in enumerate(SEEDS):\n    set_seed(SEED)\n    \n    models = []\n\n    oof_pred_ = np.zeros((train.shape[0], NTARGETS))\n    for fold in range(FOLDS):\n\n        valid_X = train[valid_index[fold]]\n        valid_y = train_targets[valid_index[fold]]\n\n        valid_ds = MoADataset(valid_X, valid_y)\n\n        validloader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False)\n\n        test_ds = TestDataset(test)\n        testloader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n    \n        filepath = '../input/moa-torch-train-4l-nn/'\n        filename = f'4l_nn_seed_{SEED}_fold_{fold}.pth'\n        checkpoint = os.path.join(filepath, filename)\n    \n        model = Model_4l(num_features=FEATURES,\n                         num_targets=NTARGETS)\n        \n        model.load_state_dict(torch.load(checkpoint))\n\n        model.to(DEVICE)\n\n        models.append(model)\n        \n        loss_fn = nn.BCEWithLogitsLoss()\n        \n        valid_loss, valid_preds = valid_fn(models[fold], loss_fn, validloader, DEVICE)\n        \n        oof_pred_[valid_index[fold]] = valid_preds\n        test_preds[j, fold, :, :] = inference_fn(models[fold], testloader, DEVICE)\n\n    oof_score_ = mean_log_loss(train_targets, oof_pred_)\n    oof_score.append(oof_score_)\n    oof_pred.append(oof_pred_)\n\noof_pred = np.mean(oof_pred, axis = 0, dtype=np.float64)\nmean_preds = np.mean(test_preds, axis = (0, 1), dtype=np.float64)\n    \nseed_log_loss = mean_log_loss(train_targets, oof_pred)\n\nblend_oof_pred.append(oof_pred)\nblend_test_preds.append(mean_preds)\n\nprint('4L_NN')\nfor j, SEED in enumerate(SEEDS):\n    print(f'SEED:{SEED} Our out of folds mean log loss score is {oof_score[j]}')\n    \nprint(f'Our out of folds log loss for our seed blend model is {seed_log_loss}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# 5l_nn\noof_pred = []\noof_score = []\ntest_preds = np.zeros((len(SEEDS), FOLDS, test.shape[0], NTARGETS))\n\nfor j, SEED in enumerate(SEEDS):\n    set_seed(SEED)\n    \n    models = []\n\n    oof_pred_ = np.zeros((train.shape[0], NTARGETS))\n    for fold in range(FOLDS):\n\n        valid_X = train[valid_index[fold]]\n        valid_y = train_targets[valid_index[fold]]\n\n        valid_ds = MoADataset(valid_X, valid_y)\n\n        validloader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False)\n\n        test_ds = TestDataset(test)\n        testloader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n    \n        filepath = '../input/moa-torch-train-5l-nn/'\n        filename = f'5l_nn_seed_{SEED}_fold_{fold}.pth'\n        checkpoint = os.path.join(filepath, filename)\n    \n        model = Model_5l(num_features=FEATURES,\n                         num_targets=NTARGETS)\n        \n        model.load_state_dict(torch.load(checkpoint))\n\n        model.to(DEVICE)\n\n        models.append(model)\n        \n        loss_fn = nn.BCEWithLogitsLoss()\n        \n        valid_loss, valid_preds = valid_fn(models[fold], loss_fn, validloader, DEVICE)\n        \n        oof_pred_[valid_index[fold]] = valid_preds\n        test_preds[j, fold, :, :] = inference_fn(models[fold], testloader, DEVICE)\n\n    oof_score_ = mean_log_loss(train_targets, oof_pred_)\n    oof_score.append(oof_score_)\n    oof_pred.append(oof_pred_)\n\noof_pred = np.mean(oof_pred, axis = 0, dtype=np.float64)\nmean_preds = np.mean(test_preds, axis = (0, 1), dtype=np.float64)\n    \nseed_log_loss = mean_log_loss(train_targets, oof_pred)\n\nblend_oof_pred.append(oof_pred)\nblend_test_preds.append(mean_preds)\n\nprint('4L_NN')\nfor j, SEED in enumerate(SEEDS):\n    print(f'SEED:{SEED} Our out of folds mean log loss score is {oof_score[j]}')\n    \nprint(f'Our out of folds log loss for our seed blend model is {seed_log_loss}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# rs_nn\noof_pred = []\noof_score = []\ntest_preds = np.zeros((len(SEEDS), FOLDS, test.shape[0], NTARGETS))\n\nfor j, SEED in enumerate(SEEDS):\n    set_seed(SEED)\n    \n    models = []\n\n    oof_pred_ = np.zeros((train.shape[0], NTARGETS))\n    for fold in range(FOLDS):\n\n        valid_X = train[valid_index[fold]]\n        valid_y = train_targets[valid_index[fold]]\n\n        valid_ds = MoADataset(valid_X, valid_y)\n\n        validloader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False)\n\n        test_ds = TestDataset(test)\n        testloader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n    \n        filepath = '../input/moa-torch-train-rs-nn/'\n        filename = f'rs_nn_seed_{SEED}_fold_{fold}.pth'\n        checkpoint = os.path.join(filepath, filename)\n    \n        model = Model_rs(num_features=FEATURES,\n                         num_targets=NTARGETS)\n        \n        model.load_state_dict(torch.load(checkpoint))\n\n        model.to(DEVICE)\n\n        models.append(model)\n        \n        loss_fn = nn.BCEWithLogitsLoss()\n        \n        valid_loss, valid_preds = valid_fn(models[fold], loss_fn, validloader, DEVICE)\n        \n        oof_pred_[valid_index[fold]] = valid_preds\n        test_preds[j, fold, :, :] = inference_fn(models[fold], testloader, DEVICE)\n\n    oof_score_ = mean_log_loss(train_targets, oof_pred_)\n    oof_score.append(oof_score_)\n    oof_pred.append(oof_pred_)\n\noof_pred = np.mean(oof_pred, axis = 0, dtype=np.float64)\nmean_preds = np.mean(test_preds, axis = (0, 1), dtype=np.float64)\n    \nseed_log_loss = mean_log_loss(train_targets, oof_pred)\n\nblend_oof_pred.append(oof_pred)\nblend_test_preds.append(mean_preds)\n\n\nprint('RS_NN')\nfor j, SEED in enumerate(SEEDS):\n    print(f'SEED:{SEED} Our out of folds mean log loss score is {oof_score[j]}')\n    \nprint(f'Our out of folds log loss for our seed blend model is {seed_log_loss}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# TabNet\noof_pred = []\noof_score = []\ntest_preds = np.zeros((len(SEEDS), FOLDS, test.shape[0], NTARGETS))\n\nfor j, SEED in enumerate(SEEDS):\n    set_seed(SEED)\n    \n    models = []\n\n    oof_pred_ = np.zeros((train.shape[0], NTARGETS))\n    for fold in range(FOLDS):\n\n#         train_X = train[train_index[fold]]\n#         train_y = train_targets[train_index[fold]]\n\n        valid_X = train[valid_index[fold]]\n        valid_y = train_targets[valid_index[fold]]\n\n        tabnet_params = dict(\n            n_d = 32,\n            n_a = 32,\n            n_steps = 1,\n            gamma = 1.3,\n            lambda_sparse = 0,\n            mask_type = \"entmax\",\n            optimizer_fn = optim.Adam,\n            optimizer_params = dict(lr = 2e-2, weight_decay = 1e-5),\n            scheduler_params = dict(\n                mode = \"min\", patience = 40, min_lr = 1e-5, factor = 0.9),\n            scheduler_fn = optim.lr_scheduler.ReduceLROnPlateau,\n\n            seed = SEED,\n            verbose = 10\n        )\n\n#         model = TabNetRegressor(**tabnet_params)\n\n#         models.append(model)\n\n#         print(f'{\"=\"*50} seed: {SEED} - fold: {fold+1} of {FOLDS}')\n#         models[fold].fit(X_train = train_X,\n#                          y_train = train_y,\n#                          eval_set = [(valid_X, valid_y)],\n#                          eval_name = [\"val\"],\n#                          eval_metric = [\"logits_ll\"],\n#                          max_epochs = EPOCHS,\n#                          patience = 100,\n#                          batch_size = 1024,\n#                          virtual_batch_size = 32,\n#                          num_workers = 1,\n#                          drop_last = False,\n#                          loss_fn = F.binary_cross_entropy_with_logits)\n        filepath = '../input/moa-torch-train-tabnet/'\n        filename = f'tn_nn_seed_{SEED}_fold_{fold}.zip'\n        checkpoint = os.path.join(filepath, filename)\n#         models[fold].save_model(filename)\n\n        model = TabNetRegressor(**tabnet_params)\n        model.load_model(checkpoint)\n\n        models.append(model)\n\n        models[fold] = model\n\n        valid_preds = models[fold].predict(valid_X)\n#         valid_preds = 1 / (1 + np.exp(-valid_preds))\n        valid_preds = torch.sigmoid(torch.as_tensor(valid_preds)).detach().cpu().numpy()\n        oof_pred_[valid_index[fold]] = valid_preds\n\n#         test_preds[j, fold, :, :] = models[fold].predict(test)\n        test_pred = models[fold].predict(test)\n        test_pred = torch.sigmoid(torch.as_tensor(test_pred)).detach().cpu().numpy()\n        test_preds[j, fold, :, :] = test_pred\n\n    oof_score_ = mean_log_loss(train_targets, oof_pred_)\n    oof_score.append(oof_score_)\n    oof_pred.append(oof_pred_)\n\noof_pred = np.mean(oof_pred, axis = 0, dtype=np.float64)\nmean_preds = np.mean(test_preds, axis = (0, 1), dtype=np.float64)\n    \nseed_log_loss = mean_log_loss(train_targets, oof_pred)\n\nblend_oof_pred.append(oof_pred)\nblend_test_preds.append(mean_preds)\n\nprint('TabNet')\nfor j, SEED in enumerate(SEEDS):\n    print(f'SEED:{SEED} Our out of folds mean log loss score is {oof_score[j]}')\n    \nprint(f'Our out of folds log loss for our seed blend model is {seed_log_loss}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# blend_mean_oof_pred = np.mean(blend_oof_pred, axis = 0, dtype=np.float64)\n# blend_mean_preds = np.mean(blend_test_preds, axis = 0, dtype=np.float64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    w = np.zeros(6, dtype=np.float64)\n    for i in range(6):\n        w[i] = trial.suggest_float(f'w{i}', 0, 1)\n    \n#     w1 = trial.suggest_float('w1', 0, 1)\n#     w2 = trial.suggest_float('w2', 0, 1)\n#     w3 = trial.suggest_float('w3', 0, 1)\n#     w4 = trial.suggest_float('w4', 0, 1)\n#     w5 = trial.suggest_float('w5', 0, 1)\n#     w6 = trial.suggest_float('w6', 0, 1)\n#     sum_w = w1 + w2 + w3 + w4 + w5 + w6\n\n    sum_w = 0\n    for i in range(6):\n        sum_w += w[i]\n\n    weighted_oof = 0\n    for i in range(6):\n        weighted_oof += blend_oof_pred[i] * w[i] / sum_w\n    \n    log_loss = mean_log_loss(train_targets, weighted_oof)\n    \n    return log_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study = optuna.create_study()\nstudy.optimize(objective, n_trials=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_trial","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# blend_log_loss = mean_log_loss(train_targets, blend_mean_oof_pred)\n# print(f'Our out of folds log loss for our ensemble model is {blend_log_loss}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #submit\n# df_test = pd.read_csv('../input/lish-moa/test_features.csv')\n# submission = pd.DataFrame(df_test['sig_id'])\n# submission[TARGET_COLUMNS] = blend_mean_preds\n# # 'cp_type': 'ctl_vehicle': 1\n# submission.loc[df_test['cp_type']=='ctl_vehicle', TARGET_COLUMNS] = 0\n# submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}