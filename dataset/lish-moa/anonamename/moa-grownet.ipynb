{"cells":[{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/tmhrkt/grownet-gradient-boosting-neural-networks"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install adabelief-pytorch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from adabelief_pytorch import AdaBelief","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\n\nsys.path.append(\"../input/iterative-stratification/iterative-stratification-master\")\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport os\nimport copy\nimport random\nimport pickle\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import (\n    StandardScaler,\n    PowerTransformer,\n    QuantileTransformer,\n    OneHotEncoder,\n)\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\nimport time\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATADIR = \"../input/lish-moa\"\n\ntrain_features = pd.read_csv(f\"{DATADIR}/train_features.csv\")\nX = train_features.select_dtypes(\"number\")\ntrain_targets_nonscored = pd.read_csv(f\"{DATADIR}/train_targets_nonscored.csv\")\ntrain_targets_scored = pd.read_csv(f\"{DATADIR}/train_targets_scored.csv\")\ntrain_drug = pd.read_csv(f\"{DATADIR}/train_drug.csv\")\n\ntest_features = pd.read_csv(f\"{DATADIR}/test_features.csv\")\nsample_submission = pd.read_csv(f\"{DATADIR}/sample_submission.csv\")\n\ncolumns = train_targets_scored.iloc[:, 1:].columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \"batch_size\": 256,\n    \"model\": \"MLP_2HL\",\n    \"optimizer\": \"adabelief\",\n    \"lr\": 1e-2,\n    \"weight_decay\": 1e-5,\n    \"n_folds\": 5,\n    \"early_stopping_steps\": 5,\n    \"hidden_size\": 512,\n    \"boost_rate\": 1.0,  # original: 1.0\n    \"num_nets\": 20,  # Number of weak NNs. original: 40 n_estimators?\n    \"epochs_per_stage\": 3,  # Number of epochs to learn the Kth model. original: 1\n    \"correct_epoch\": 1,  # Number of epochs to correct the whole week models original: 1\n    \"model_order\": \"second\",  # You could put \"first\" according to the original implemention, but error occurs. original: \"second\"\n}\nn_seeds = 5\n\n\n#DEBUG = True\nDEBUG = False\nif DEBUG:\n    params[\"batch_size\"] = 1024\n    params[\"num_nets\"] = 3\n    params[\"epochs_per_stage\"] = 1\n    n_seeds = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in train_features.columns if col.startswith(\"g-\")]\nCELLS = [col for col in train_features.columns if col.startswith(\"c-\")]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nseed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator\nfrom sklearn.base import TransformerMixin\n\n\nclass ClippedFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, copy=True, high=0.99, low=0.01):\n        self.copy = copy\n        self.high = high\n        self.low = low\n\n    def fit(self, X, y=None):\n        self.data_max_ = X.quantile(q=self.high)\n        self.data_min_ = X.quantile(q=self.low)\n\n        return self\n\n    def transform(self, X):\n        if self.copy:\n            X = X.copy()\n\n        X.clip(self.data_min_, self.data_max_, axis=1, inplace=True)\n\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clipped_features = ClippedFeatures()\nX = clipped_features.fit_transform(X)\n\nwith open(\"clipped_features.pkl\", \"wb\") as f:\n    pickle.dump(clipped_features, f)\n\ntrain_features[X.columns] = X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature_cols = train_features.columns[4:].tolist()\nfeature_cols = [\"cp_time\"] + train_features.columns[4:].tolist()\nparams[\"feat_d\"] = len(feature_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_features.merge(train_targets_scored, on=\"sig_id\")\ntest = test_features.copy()\ntarget = train[train_targets_scored.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(\"cp_type\", axis=1)\ntest = test.drop(\"cp_type\", axis=1)\n\nif \"sig_id\" in target.columns:\n    target_cols = target.drop(\"sig_id\", axis=1).columns.values.tolist()\nelse:\n    target_cols = target.columns.values.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CV strategy"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterativestratification')\n\nimport numpy as np\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn.model_selection._split import _BaseKFold\n\n\nclass MultilabelGroupStratifiedKFold(_BaseKFold):\n    def __init__(self, n_splits=5, random_state=None, shuffle=False):\n        super().__init__(n_splits=n_splits, random_state=random_state, shuffle=shuffle)\n\n    def _iter_test_indices(self, X=None, Y=None, groups=None):\n        cv = MultilabelStratifiedKFold(\n            n_splits=self.n_splits,\n            random_state=self.random_state,\n            shuffle=self.shuffle,\n        )\n\n        value_counts = groups.value_counts()\n        regular_index = value_counts.loc[\n            (value_counts == 6) | (value_counts == 12) | (value_counts == 18)\n        ].index.sort_values()\n        irregular_index = value_counts.loc[\n            (value_counts != 6) & (value_counts != 12) & (value_counts != 18)\n        ].index.sort_values()\n\n        group_to_fold = {}\n        tmp = Y.groupby(groups).mean().loc[regular_index]\n\n        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n            group_to_fold.update({group: fold for group in tmp.index[test]})\n\n        sample_to_fold = {}\n        tmp = Y.loc[groups.isin(irregular_index)]\n\n        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n            sample_to_fold.update({sample: fold for sample in tmp.index[test]})\n\n        folds = groups.map(group_to_fold)\n        is_na = folds.isna()\n        folds[is_na] = folds[is_na].index.map(sample_to_fold).values\n\n        for i in range(self.n_splits):\n            yield np.where(folds == i)[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset Classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n\n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        dct = {\n            \"x\": torch.tensor(self.features[idx, :], dtype=torch.float),\n            \"y\": torch.tensor(self.targets[idx, :], dtype=torch.float),\n        }\n        return dct\n\n\nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n\n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        dct = {\"x\": torch.tensor(self.features[idx, :], dtype=torch.float)}\n        return dct","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dynamic Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from enum import Enum\n\n\nclass ForwardType(Enum):\n    SIMPLE = 0\n    STACKED = 1\n    CASCADE = 2\n    GRADIENT = 3\n\n\nclass DynamicNet(object):\n    def __init__(self, c0, lr):\n        self.models = []\n        self.c0 = c0\n        self.lr = lr\n        self.boost_rate = nn.Parameter(\n            torch.tensor(lr, requires_grad=True, device=device)\n        )\n\n    def add(self, model):\n        self.models.append(model)\n\n    def parameters(self):\n        params = []\n        for m in self.models:\n            params.extend(m.parameters())\n\n        params.append(self.boost_rate)\n        return params\n\n    def zero_grad(self):\n        for m in self.models:\n            m.zero_grad()\n\n    def to_cuda(self):\n        for m in self.models:\n            m.cuda()\n\n    def to_eval(self):\n        for m in self.models:\n            m.eval()\n\n    def to_train(self):\n        for m in self.models:\n            m.train(True)\n\n    def forward(self, x):\n        if len(self.models) == 0:\n            batch = x.shape[0]\n            c0 = np.repeat(self.c0.detach().cpu().numpy().reshape(1, -1), batch, axis=0)\n            c0 = torch.Tensor(c0).cuda() if device == \"cuda\" else torch.Tensor(c0)\n            return None, c0\n        middle_feat_cum = None\n        prediction = None\n        with torch.no_grad():\n            for m in self.models:\n                if middle_feat_cum is None:\n                    middle_feat_cum, prediction = m(x, middle_feat_cum)\n                else:\n                    middle_feat_cum, pred = m(x, middle_feat_cum)\n                    prediction += pred\n        return middle_feat_cum, self.c0 + self.boost_rate * prediction\n\n    def forward_grad(self, x):\n        if len(self.models) == 0:\n            batch = x.shape[0]\n            c0 = np.repeat(self.c0.detach().cpu().numpy().reshape(1, -1), batch, axis=0)\n            return None, torch.Tensor(c0).cuda()\n        # at least one model\n        middle_feat_cum = None\n        prediction = None\n        for m in self.models:\n            if middle_feat_cum is None:\n                middle_feat_cum, prediction = m(x, middle_feat_cum)\n            else:\n                middle_feat_cum, pred = m(x, middle_feat_cum)\n                prediction += pred\n        return middle_feat_cum, self.c0 + self.boost_rate * prediction\n\n    @classmethod\n    def from_file(cls, path, builder):\n        d = torch.load(path)\n        net = DynamicNet(d[\"c0\"], d[\"lr\"])\n        net.boost_rate = d[\"boost_rate\"]\n        for stage, m in enumerate(d[\"models\"]):\n            submod = builder(stage)\n            submod.load_state_dict(m)\n            net.add(submod)\n        return net\n\n    def to_file(self, path):\n        models = [m.state_dict() for m in self.models]\n        d = {\n            \"models\": models,\n            \"c0\": self.c0,\n            \"lr\": self.lr,\n            \"boost_rate\": self.boost_rate,\n        }\n        torch.save(d, path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Weak Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MLP_2HL(nn.Module):\n    def __init__(self, dim_in, dim_hidden1, dim_hidden2, sparse=False, bn=True):\n        super(MLP_2HL, self).__init__()\n        self.bn2 = nn.BatchNorm1d(dim_in)\n\n        self.layer1 = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(dim_in, dim_hidden1),\n            nn.ReLU(),\n            nn.BatchNorm1d(dim_hidden1),\n            nn.Dropout(0.4),\n            nn.Linear(dim_hidden1, dim_hidden2),\n        )\n        self.layer2 = nn.Sequential(nn.ReLU(), nn.Linear(dim_hidden2, len(target_cols)),)\n\n    def forward(self, x, lower_f):\n        if lower_f is not None:\n            x = torch.cat([x, lower_f], dim=1)\n            x = self.bn2(x)\n        middle_feat = self.layer1(x)\n        out = self.layer2(middle_feat)\n        return middle_feat, out\n\n    @classmethod\n    def get_model(cls, stage, params):\n        if stage == 0:\n            dim_in = params[\"feat_d\"]\n        else:\n            dim_in = params[\"feat_d\"] + params[\"hidden_size\"]\n        model = MLP_2HL(dim_in, params[\"hidden_size\"], params[\"hidden_size\"])\n        return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.nn.modules.loss import _WeightedLoss\n\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction=\"mean\", smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets: torch.Tensor, n_labels: int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1), self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets, self.weight)\n\n        if self.reduction == \"sum\":\n            loss = loss.sum()\n        elif self.reduction == \"mean\":\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_optim(params, lr, weight_decay):\n    optimizer = optim.Adam(params, lr, weight_decay=weight_decay)\n    # optimizer = SGD(params, lr, weight_decay=weight_decay)\n    return optimizer\n\n\ndef get_optim_adabelief(params, lr, weight_decay):\n    optimizer = AdaBelief(\n        params,\n        lr=lr,\n        weight_decay=weight_decay,\n        eps=1e-16,\n        betas=(0.9, 0.999),\n        weight_decouple=True,\n        rectify=False,\n    )\n    return optimizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logloss(net_ensemble, test_loader):\n    loss = 0\n    total = 0\n    loss_f = (\n        nn.BCEWithLogitsLoss()\n    )  # Binary cross entopy loss with logits, reduction=mean by default\n    for data in test_loader:\n        x = data[\"x\"].cuda() if device == \"cuda\" else data[\"x\"]\n        y = data[\"y\"].cuda() if device == \"cuda\" else data[\"y\"]\n        with torch.no_grad():\n            _, out = net_ensemble.forward(x)\n        loss += loss_f(out, y)\n        total += 1\n\n    return loss / total","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"c0_ = np.log(np.mean(train_targets_scored.iloc[:, 1:].values, axis=0))\n\n\ndef train_fn(n_seeds):\n    print(f\"params: {params}\")\n\n    counts = np.empty((n_seeds * params[\"n_folds\"], len(target_cols)))\n    oof = np.zeros((len(train), len(target_cols)))\n\n    for seed in tqdm(range(n_seeds)):\n        seed_everything(seed)\n        print(\"-\" * 100)\n        \n        cv = MultilabelGroupStratifiedKFold(n_splits=params[\"n_folds\"], random_state=seed, shuffle=True)\n        cv_split = cv.split(train[feature_cols], train[target_cols], train_drug[\"drug_id\"])\n        \n        for fold, (trn_idx, val_idx) in enumerate(cv_split):\n            print(\"=\" * 25, f\"fold: {fold}\", \"=\" * 25)\n            \n            x_train, x_val = train[feature_cols].iloc[trn_idx].values, train[feature_cols].iloc[val_idx].values\n            y_train, y_val = train[target_cols].iloc[trn_idx].values, train[target_cols].iloc[val_idx].values\n            \n            counts[seed * params[\"n_folds\"] + fold] = train[target_cols].iloc[trn_idx].sum()\n\n            train_ds = MoADataset(x_train, y_train)\n            val_ds = MoADataset(x_val, y_val)\n            train_loader = DataLoader(\n                train_ds, batch_size=params[\"batch_size\"], shuffle=True\n            )\n            val_loader = DataLoader(\n                val_ds, batch_size=params[\"batch_size\"], shuffle=False\n            )\n\n            best_score = np.inf\n            val_score = best_score\n            best_stage = params[\"num_nets\"] - 1\n\n            c0 = torch.tensor(c0_, dtype=torch.float).to(device)\n            net_ensemble = DynamicNet(c0, params[\"boost_rate\"])\n            loss_f1 = nn.MSELoss(reduction=\"none\")\n            loss_f2 = SmoothBCEwLogits(smoothing=0.001, reduction=\"none\")\n            loss_models = torch.zeros((params[\"num_nets\"], 3))\n\n            all_ensm_losses = []\n            all_ensm_losses_te = []\n            all_mdl_losses = []\n            dynamic_br = []\n\n            lr = params[\"lr\"]\n            L2 = params[\"weight_decay\"]\n\n            early_stop = 0\n            for stage in range(params[\"num_nets\"]):\n                t0 = time.time()\n\n                if params[\"model\"] == \"MLP_2HL_weight_norm\":\n                    model = MLP_2HL_weight_norm.get_model(stage, params)\n                elif params[\"model\"] == \"MLP_2HL_leaky_relu\":\n                    model = MLP_2HL_leaky_relu.get_model(stage, params)\n                else:\n                    model = MLP_2HL.get_model(stage, params)\n                model.to(device)\n\n                if params[\"optimizer\"] == \"adam\":\n                    optimizer = get_optim(model.parameters(), lr, L2)\n                elif params[\"optimizer\"] == \"adabelief\":\n                    optimizer = get_optim_adabelief(model.parameters(), lr, L2)\n\n                net_ensemble.to_train()  # Set the models in ensemble net to train mode\n                stage_mdlloss = []\n                for epoch in range(params[\"epochs_per_stage\"]):\n                    for i, data in enumerate(train_loader):\n                        x = data[\"x\"].to(device)\n                        y = data[\"y\"].to(device)\n                        middle_feat, out = net_ensemble.forward(x)\n                        if params[\"model_order\"] == \"first\":\n                            grad_direction = y / (1.0 + torch.exp(y * out))\n                        else:\n                            h = 1 / (\n                                (1 + torch.exp(y * out)) * (1 + torch.exp(-y * out))\n                            )\n                            grad_direction = y * (1.0 + torch.exp(-y * out))\n                            nwtn_weights = (torch.exp(out) + torch.exp(-out)).abs()\n                        _, out = model(x, middle_feat)\n                        loss = loss_f1(net_ensemble.boost_rate * out, grad_direction)\n                        loss = loss * h\n                        loss = loss.mean()\n                        model.zero_grad()\n                        loss.backward()\n                        optimizer.step()\n                        stage_mdlloss.append(loss.item())\n\n                net_ensemble.add(model)\n                sml = np.mean(stage_mdlloss)\n\n                stage_loss = []\n                lr_scaler = 2\n                # fully-corrective step\n                if stage != 0:\n                    # Adjusting corrective step learning rate\n                    if stage % 3 == 0:\n                        lr /= 2 \n                    optimizer = get_optim(net_ensemble.parameters(), lr / lr_scaler, L2)\n                    for _ in range(params[\"correct_epoch\"]):\n                        for i, data in enumerate(train_loader):\n                            x = data[\"x\"].to(device)\n                            y = data[\"y\"].to(device)\n                            _, out = net_ensemble.forward_grad(x)\n                            loss = loss_f2(out, y).mean()\n                            optimizer.zero_grad()\n                            loss.backward()\n                            optimizer.step()\n                            stage_loss.append(loss.item())\n\n                sl_te = logloss(net_ensemble, val_loader)\n\n                # Store dynamic boost rate\n                dynamic_br.append(net_ensemble.boost_rate.item())\n\n                elapsed_tr = time.time() - t0\n                sl = 0\n                if stage_loss != []:\n                    sl = np.mean(stage_loss)\n\n                all_ensm_losses.append(sl)\n                all_ensm_losses_te.append(sl_te)\n                all_mdl_losses.append(sml)\n                print(\n                    f\"Stage - {stage}, training time: {elapsed_tr: .1f} sec, boost rate: {net_ensemble.boost_rate: .4f}, Training Loss: {sl: .5f}, Val Loss: {sl_te: .5f}\"\n                )\n\n                if device == \"cuda\":\n                    net_ensemble.to_cuda()\n                net_ensemble.to_eval()  # Set the models in ensemble net to eval mode\n\n                # --------------------- Train ---------------------\n                if sl_te < best_score:\n                    best_score = sl_te\n                    best_stage = stage\n                    net_ensemble.to_file(f\"./{fold}FOLD_{seed}_.pth\")\n                    early_stop = 0\n                else:\n                    early_stop += 1\n\n                if early_stop > params[\"early_stopping_steps\"]:\n                    print(\"early stopped!\")\n                    break\n\n            print(f\"Best validation stage: {best_stage}\")\n\n            if params[\"model\"] == \"MLP_2HL_weight_norm\":\n                net_ensemble = DynamicNet.from_file(\n                    f\"./{fold}FOLD_{seed}_.pth\",\n                    lambda stage: MLP_2HL_weight_norm.get_model(stage, params),\n                )\n            elif params[\"model\"] == \"MLP_2HL_leaky_relu\":\n                net_ensemble = DynamicNet.from_file(\n                    f\"./{fold}FOLD_{seed}_.pth\",\n                    lambda stage: MLP_2HL_leaky_relu.get_model(stage, params),\n                )\n            else:\n                net_ensemble = DynamicNet.from_file(\n                    f\"./{fold}FOLD_{seed}_.pth\",\n                    lambda stage: MLP_2HL.get_model(stage, params),\n                )\n            if device == \"cuda\":\n                net_ensemble.to_cuda()\n            net_ensemble.to_eval()\n\n            # --------------------- PREDICTION---------------------\n\n            preds = []\n            with torch.no_grad():\n                for data in val_loader:\n                    x = data[\"x\"].to(device)\n                    _, pred = net_ensemble.forward(x)\n                    preds.append(pred.sigmoid().detach().cpu().numpy())\n            oof[val_idx, :] += np.concatenate(preds) / n_seeds\n\n    train[target_cols] = oof\n\n    val_results = (\n        train_targets_scored.drop(columns=target_cols)\n        .merge(train[[\"sig_id\"] + target_cols], on=\"sig_id\", how=\"left\")\n        .fillna(0)\n    )\n\n    y_true = train_targets_scored[target_cols].values\n    y_pred = val_results[target_cols].values\n\n    score = 0\n    for i in range(len(target_cols)):\n        score_ = log_loss(y_true[:, i], y_pred[:, i])\n        score += score_ / len(target_cols)\n    print(\"CV log_loss \", score)\n   \n    with open(\"counts.pkl\", \"wb\") as f:\n        pickle.dump(counts, f)\n    \n    y_pred = pd.DataFrame(y_pred, index=train[\"sig_id\"], columns=target_cols)\n    with open(\"Y_pred.pkl\", \"wb\") as f:\n        pickle.dump(y_pred, f)\n\n    return score, y_pred","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"%%time\n\nscore, y_pred = train_fn(n_seeds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = r\"Y_pred.pkl\"\nwith open(path, 'rb') as f:\n    Y_pred = pickle.load(f)\nY_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = r\"counts.pkl\"\nwith open(path, 'rb') as f:\n    counts = pickle.load(f)\nprint(counts.shape)\ncounts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# predict test"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv(f\"{DATADIR}/train_features.csv\")\nX = train_features.select_dtypes(\"number\")\n\nsample_submission = pd.read_csv(f\"{DATADIR}/sample_submission.csv\")\ntest_features = pd.read_csv(f\"{DATADIR}/test_features.csv\")\n\ntest = test_features.copy()\nwith open(\"./clipped_features.pkl\", \"rb\") as f:\n    clipped_features = pickle.load(f)\ntest[X.columns] = clipped_features.transform(test[X.columns])\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', None)\n\nx_test = test[feature_cols].values\ntest_ds = TestDataset(x_test)\ntest_loader = DataLoader(test_ds, batch_size=params[\"batch_size\"], shuffle=False)\n\npredictions = np.zeros((len(test), len(target_cols)))\nfor seed in tqdm(range(n_seeds)):\n    seed_everything(seed)\n    \n    for fold in range(params[\"n_folds\"]):\n        if params[\"model\"] == \"MLP_2HL_weight_norm\":\n            net_ensemble = DynamicNet.from_file(\n                f\"./{fold}FOLD_{seed}_.pth\",\n                lambda stage: MLP_2HL_weight_norm.get_model(stage, params),\n            )\n        elif params[\"model\"] == \"MLP_2HL_leaky_relu\":\n            net_ensemble = DynamicNet.from_file(\n                f\"./{fold}FOLD_{seed}_.pth\",\n                lambda stage: MLP_2HL_leaky_relu.get_model(stage, params),\n            )\n        else:\n            net_ensemble = DynamicNet.from_file(\n                f\"./{fold}FOLD_{seed}_.pth\",\n                lambda stage: MLP_2HL.get_model(stage, params),\n            )\n        if device == \"cuda\":\n            net_ensemble.to_cuda()\n        net_ensemble.to_eval()\n\n        preds = []\n        with torch.no_grad():\n            for data in test_loader:\n                x = data[\"x\"].to(device)\n                _, pred = net_ensemble.forward(x)\n                preds.append(pred.sigmoid().detach().cpu().numpy())\n        predictions += np.concatenate(preds) / (params[\"n_folds\"] * n_seeds)\n\nsample_submission[target_cols] = predictions\n\nsample_submission.loc[:, [\"atp-sensitive_potassium_channel_antagonist\", \"erbb2_inhibitor\"]] = 0.000012\n\ntest = test.set_index(\"sig_id\")\nsample_submission = sample_submission.set_index(\"sig_id\")\nsample_submission[test[\"cp_type\"] == \"ctl_vehicle\"] = 0.0\n\nsample_submission.to_csv(\"submission.csv\")\n\ndisplay(sample_submission)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}