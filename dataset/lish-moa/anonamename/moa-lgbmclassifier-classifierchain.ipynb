{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport re\nimport math\nimport pickle\nimport joblib\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.multioutput import MultiOutputClassifier, ClassifierChain\nfrom sklearn.linear_model import LogisticRegression\n\nfrom lightgbm import LGBMClassifier\n\nwarnings.simplefilter('ignore')\npd.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport random as rn\nimport numpy as np\n\n\ndef set_seed(seed=0):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    rn.seed(seed)\n    np.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\n\n\ndef score(Y, Y_pred):\n    _, n_classes = Y.shape\n\n    losses = []\n\n    for j in range(n_classes):\n        loss = log_loss(Y.iloc[:, j], Y_pred.iloc[:, j], labels=[0, 1])\n\n        losses.append(loss)\n\n    return np.mean(losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterativestratification')\n\nimport numpy as np\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn.model_selection._split import _BaseKFold\n\n\nclass MultilabelGroupStratifiedKFold(_BaseKFold):\n    def __init__(self, n_splits=5, random_state=None, shuffle=False):\n        super().__init__(n_splits=n_splits, random_state=random_state, shuffle=shuffle)\n\n    def _iter_test_indices(self, X=None, y=None, groups=None):\n        cv = MultilabelStratifiedKFold(\n            n_splits=self.n_splits,\n            random_state=self.random_state,\n            shuffle=self.shuffle,\n        )\n\n        value_counts = groups.value_counts()\n        regular_index = value_counts.loc[\n            (value_counts == 6) | (value_counts == 12) | (value_counts == 18)\n        ].index.sort_values()\n        irregular_index = value_counts.loc[\n            (value_counts != 6) & (value_counts != 12) & (value_counts != 18)\n        ].index.sort_values()\n\n        group_to_fold = {}\n        tmp = Y.groupby(groups).mean().loc[regular_index]\n\n        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n            group_to_fold.update({group: fold for group in tmp.index[test]})\n\n        sample_to_fold = {}\n        tmp = Y.loc[groups.isin(irregular_index)]\n\n        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n            sample_to_fold.update({sample: fold for sample in tmp.index[test]})\n\n        folds = groups.map(group_to_fold)\n        is_na = folds.isna()\n        folds[is_na] = folds[is_na].index.map(sample_to_fold).values\n\n        for i in range(self.n_splits):\n            yield np.where(folds == i)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator\nfrom sklearn.base import TransformerMixin\n\n\nclass ClippedFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, copy=True, high=0.99, low=0.01):\n        self.copy = copy\n        self.high = high\n        self.low = low\n\n    def fit(self, X, y=None):\n        self.data_max_ = X.quantile(q=self.high)\n        self.data_min_ = X.quantile(q=self.low)\n\n        return self\n\n    def transform(self, X):\n        if self.copy:\n            X = X.copy()\n\n        X.clip(self.data_min_, self.data_max_, axis=1, inplace=True)\n\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n\ndef compute_row_statistics(X, prefix=\"\"):\n    Xt = pd.DataFrame()\n\n    for agg_func in [\n        # \"min\",\n        # \"max\",\n        \"mean\",\n        \"std\",\n        \"kurtosis\",\n        \"skew\",\n    ]:\n        Xt[f\"{prefix}{agg_func}\"] = X.agg(agg_func, axis=1)\n\n    return Xt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtype = {\"cp_type\": \"category\", \"cp_dose\": \"category\"}\nindex_col = \"sig_id\"\n\ntrain_features = pd.read_csv(\n    \"../input/lish-moa/train_features.csv\", dtype=dtype, index_col=index_col\n)\nX = train_features.select_dtypes(\"number\")\nY_nonscored = pd.read_csv(\n    \"../input/lish-moa/train_targets_nonscored.csv\", index_col=index_col\n)\nY = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\", index_col=index_col)\ngroups = pd.read_csv(\n    \"../input/lish-moa/train_drug.csv\", index_col=index_col, squeeze=True\n)\n\ncolumns = Y.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clipped_features = ClippedFeatures()\nX = clipped_features.fit_transform(X)\n\nwith open(\"clipped_features.pkl\", \"wb\") as f:\n    pickle.dump(clipped_features, f)\n\n# アンサンブルのため統計値, nonscoredは入れない \n#c_prefix = \"c-\"\n#g_prefix = \"g-\"\n#c_columns = X.columns.str.startswith(c_prefix)\n#g_columns = X.columns.str.startswith(g_prefix)\n#X_c = compute_row_statistics(X.loc[:, c_columns], prefix=c_prefix)\n#X_g = compute_row_statistics(X.loc[:, g_columns], prefix=g_prefix)\n#X = pd.concat([X, X_c, X_g], axis=1)\n#\n#Y_nonscored = Y_nonscored.loc[:, Y_nonscored.sum(axis=0) > 0]\n#Y = pd.concat([Y, Y_nonscored], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"is_drug_cv = True\nn_splits = 5\nn_seeds = 5\n# LBS = 0.0008  # ラベルスムージングは全然効かないからやめる\nLBS = 0.0\n\nparams = {\n    \"objective\": \"binary\",\n    \"learning_rate\": 0.1,\n    \"num_leaves\": 3,\n    \"max_depth\": 2, \n    \"min_data_in_leaf\": 1367, \n    \"feature_fraction\": 0.65, \n    \"lambda_l1\": 3.6056855520901, \n    \"lambda_l2\": 0.2777268104182044\n}\n\n#DEBUG = True\nDEBUG = False\nif DEBUG:\n    columns = [\n        \"atp-sensitive_potassium_channel_antagonist\",  # 陽性ラベル1個だけ\n        \"erbb2_inhibitor\",  # 陽性ラベル1個だけ\n        \"antiarrhythmic\",  # 陽性ラベル6個だけ\n        \"aldehyde_dehydrogenase_inhibitor\",  # 陽性ラベル7個だけ\n        \"lipase_inhibitor\",  # 陽性ラベル12個だけ\n        \"sphingosine_receptor_agonist\",  # 陽性ラベル25個だけ\n        \"igf-1_inhibitor\",  # 陽性ラベル37個だけ\n        \"potassium_channel_activator\",  # 陽性ラベル55個だけ\n        \"potassium_channel_antagonist\",  # 陽性ラベル98個だけ\n        \"dopamine_receptor_agonist\",  # 陽性ラベル121個だけ\n        \"nfkb_inhibitor\",  # 陽性ラベル832個\n        \"cyclooxygenase_inhibitor\",  # 陽性ラベル435個\n        \"dna_inhibitor\",  # 陽性ラベル402個\n        \"glutamate_receptor_antagonist\",  # 陽性ラベル367個\n        \"tubulin_inhibitor\",  # 陽性ラベル316個\n        \"pdgfr_inhibitor\",  # 陽性ラベル297個\n        \"calcium_channel_blocker\",  # 陽性ラベル281個\n        \"flt3_inhibitor\",  # 陽性ラベル279個\n        \"progesterone_receptor_agonist\",  # 陽性ラベル119個\n        \"hdac_inhibitor\",  # 陽性ラベル106個\n    ]\n    Y = Y[columns]\n\n    #non_columns = [\n    #    \"abc_transporter_expression_enhancer\",  # nonscored class\n    #    \"abl_inhibitor\",  # nonscored class\n    #]\n    #Y_nonscored = Y_nonscored[non_columns]\n    #\n    #Y = pd.concat([Y, Y_nonscored], axis=1)\n    \n    params[\"n_estimators\"] = 2\n    n_seeds = 2\n    n_splits = 2\n    print(f\"DEBUG: {DEBUG}\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size, n_features = X.shape\n_, n_classes_nonscored = Y_nonscored.shape\n_, n_classes = Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ncounts = np.empty((n_seeds * n_splits, n_classes))\n\nY_pred = np.zeros((train_size, n_classes))\nY_pred = pd.DataFrame(Y_pred, columns=Y.columns, index=Y.index)\n\nfor i in range(n_seeds):\n    set_seed(seed=i)\n    \n    if is_drug_cv:\n        cv = MultilabelGroupStratifiedKFold(n_splits=n_splits, random_state=i, shuffle=True)\n        cv_split = cv.split(X, Y, groups)\n    else:\n        MultilabelStratifiedKFold(n_splits=n_splits, random_state=i, shuffle=True)\n        cv_split = cv.split(X, Y)\n\n    order = rn.sample(range(Y.shape[1]), k=Y.shape[1])\n    print(f\"order: {order}\")\n        \n    for j, (trn_idx, val_idx) in tqdm(enumerate(cv_split)):\n        counts[i * n_splits + j] = Y.iloc[trn_idx].sum()\n        \n        X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n        Y_train, Y_val = Y.iloc[trn_idx], Y.iloc[val_idx]\n\n        # Label Smoothing. https://www.kaggle.com/gogo827jz/self-stacking-groupcv-xgboost\n        Y_train = Y_train * (1 - LBS) + 0.5 * LBS\n        \n        clf = ClassifierChain(LGBMClassifier(**params), order=order)\n        clf.fit(X_train, Y_train)\n        val_preds = clf.predict_proba(X_val) # list of preds per class\n        \n        Y_pred.iloc[val_idx] += val_preds / n_seeds\n        \n        joblib.dump(clf, f\"model_seed_{i}_fold_{j}.jlb\", compress=True)\n\nY_pred[train_features[\"cp_type\"] == \"ctl_vehicle\"] = 0.0\n\nwith open(\"counts.pkl\", \"wb\") as f:\n    pickle.dump(counts, f)\n\nwith open(\"Y_pred.pkl\", \"wb\") as f:\n    pickle.dump(Y_pred[columns], f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score(Y[columns], Y_pred[columns])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Platt Scaling\nTrain a Logistic Regression model to calibrate the results\n- https://www.kaggle.com/gogo827jz/kernel-logistic-regression-one-for-206-targets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict_probaでだしたY_predをロジスティク回帰で確率に補正する\n\ncounts = np.empty((n_classes))\n\nX_new = Y_pred.values\nY_cali = Y_pred.copy()\n\nfor tar in tqdm(range(Y.shape[1])):\n    \n    targets = Y.values[:, tar]\n    X_targets = X_new[:, tar]\n    counts[tar] = targets.sum()\n    \n    print(tar, counts[tar])\n\n    if targets.sum() >= n_splits:\n        \n        Y_cali[Y.columns[tar]] = np.zeros((Y_cali.shape[0], ))\n\n        skf = StratifiedKFold(n_splits=n_splits, random_state=0, shuffle=True)\n\n        for n, (tr, te) in enumerate(skf.split(targets, targets)):\n            x_tr, x_val = X_targets[tr].reshape(-1, 1), X_targets[te].reshape(-1, 1)\n            y_tr, y_val = targets[tr], targets[te]\n\n            model = LogisticRegression(penalty=\"none\", max_iter=1000)\n            model.fit(x_tr, y_tr)\n            Y_cali[Y.columns[tar]].iloc[te] += model.predict_proba(x_val)[:, 1]\n            \n            joblib.dump(model, f\"calibrate_model_target_{Y.columns[tar]}.jlb\", compress=True)\n\nwith open(\"counts_calibrate.pkl\", \"wb\") as f:\n    pickle.dump(counts, f)\n\nwith open(\"Y_pred_calibrate.pkl\", \"wb\") as f:\n    pickle.dump(Y_cali[columns], f)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score(Y[columns], Y_cali[columns])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# pkl check"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = r\"counts.pkl\"\nwith open(path, 'rb') as f:\n    counts = pickle.load(f)\ncounts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = r\"counts_calibrate.pkl\"\nwith open(path, 'rb') as f:\n    counts = pickle.load(f)\ncounts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = r\"Y_pred.pkl\"\nwith open(path, 'rb') as f:\n    Y_pred = pickle.load(f)\nY_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = r\"Y_pred_calibrate.pkl\"\nwith open(path, 'rb') as f:\n    Y_pred = pickle.load(f)\nY_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# predict test"},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\nimport pathlib\n\n\ntest_features = pd.read_csv(\n    \"../input/lish-moa/test_features.csv\", dtype=dtype, index_col=index_col,\n)\nX_test = test_features.select_dtypes(\"number\")\n\nwith open(\"./clipped_features.pkl\", \"rb\") as f:\n    clipped_features = pickle.load(f)\nX_test = clipped_features.transform(X_test)\n# アンサンブルのために統計値, nonscoredは入れない\n# X_c = compute_row_statistics(X_test.loc[:, c_columns], prefix=c_prefix)\n# X_g = compute_row_statistics(X_test.loc[:, g_columns], prefix=g_prefix)\n# X_test = pd.concat([X_test, X_c, X_g], axis=1)\n\n# lgbで予測\nY_test_pred = np.zeros((X_test.shape[0], len(columns)))\nY_test_pred = pd.DataFrame(Y_test_pred, columns=columns, index=test_features.index)\nfor i in range(n_seeds):\n    set_seed(seed=i)\n\n    for j in range(n_splits):\n        clf = joblib.load(f\"model_seed_{i}_fold_{j}.jlb\")\n        Y_test_pred += clf.predict_proba(X_test)[:, : len(columns)] / (\n            n_seeds * n_splits\n        )\n\nprint(Y_test_pred.shape)\ndisplay(Y_test_pred)\n\n# lgbの予測値補正\nmodel_paths = glob.glob(f\"./calibrate_model_target_*.jlb\")\nfor model_path in model_paths:\n    target = str(pathlib.Path(model_path).stem).replace(\"calibrate_model_target_\", \"\")\n\n    if target in columns:\n        # print(target)\n        model = joblib.load(model_path)\n        X_targets = Y_test_pred.loc[:, target].values.reshape(-1, 1)\n        Y_test_pred.loc[:, target] = model.predict_proba(X_targets)[:, 1]\n\nprint(Y_test_pred.shape)\ndisplay(Y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\nimport pathlib\n\n\nwith open(\"counts.pkl\", \"rb\") as f:\n    counts = pickle.load(f)\n\ndenominator = counts.sum(axis=0)\n\ncounts /= denominator\nprint(counts)\n\n\ntest_features = pd.read_csv(\n    \"../input/lish-moa/test_features.csv\", dtype=dtype, index_col=index_col,\n)\nX_test = test_features.select_dtypes(\"number\")\n\nwith open(\"./clipped_features.pkl\", \"rb\") as f:\n    clipped_features = pickle.load(f)\nX_test = clipped_features.transform(X_test)\n# アンサンブルのために統計値, nonscoredは入れない\n# X_c = compute_row_statistics(X_test.loc[:, c_columns], prefix=c_prefix)\n# X_g = compute_row_statistics(X_test.loc[:, g_columns], prefix=g_prefix)\n# X_test = pd.concat([X_test, X_c, X_g], axis=1)\n\n# lgbで予測\nY_test_pred = np.zeros((X_test.shape[0], len(columns)))\nY_test_pred = pd.DataFrame(Y_test_pred, columns=columns, index=test_features.index)\nfor i in range(n_seeds):\n    set_seed(seed=i)\n\n    for j in range(n_splits):\n        clf = joblib.load(f\"model_seed_{i}_fold_{j}.jlb\")\n        # マイナークラスの確率下げるためにtrain setのfoldごとの陽性ラベルの割合かける\n        Y_test_pred += counts[i * n_splits + j] * clf.predict_proba(X_test)[:, : len(columns)]\n\nprint(Y_test_pred.shape)\ndisplay(Y_test_pred)\n\n# lgbの予測値補正\nmodel_paths = glob.glob(f\"./calibrate_model_target_*.jlb\")\nfor model_path in model_paths:\n    target = str(pathlib.Path(model_path).stem).replace(\"calibrate_model_target_\", \"\")\n\n    if target in columns:\n        # print(target)\n        model = joblib.load(model_path)\n        X_targets = Y_test_pred.loc[:, target].values.reshape(-1, 1)\n        Y_test_pred.loc[:, target] = model.predict_proba(X_targets)[:, 1]\n\nprint(Y_test_pred.shape)\ndisplay(Y_test_pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}