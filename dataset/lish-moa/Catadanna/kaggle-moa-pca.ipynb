{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import log_loss\n\nfrom catboost import CatBoostRegressor, Pool\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow.keras.callbacks as C\nimport sklearn\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sklearn.__version__)\n\n# epochs: 100  batch size: 1024  nb_hiddens: 256  dropout: 0.1  nb_blocks: 5\n# NEW BEST SCORE :  3.0524902204895814\n# score :  3.0524902204895814\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_COMPONENTS = 50\n#INPUT_SHAPE = 878\nINPUT_SHAPE = N_COMPONENTS\nOUTPUT_SHAPE = 206\nBATCH_SIZE=2048\nEPOCHS=200\nNFOLD=5\nDROPOUT=0.1\nNB_HIDDENS=4096\n# epochs: 100  batch size: 128  nb_hiddens: 4096  dropout: 0.2  nb_blocks: 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\n#df_train_targets_scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ndf_train_targets_scored = pd.read_csv('/kaggle/input/train-with-fold/train_with_fold_y.csv')\ndf_train_targets_nonscored = pd.read_csv('/kaggle/input/lish-moa/train_targets_nonscored.csv')\ndf_test = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\ndf_sub = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\n\nnon_ctl_idx = df_train_features.loc[df_train_features['cp_type']!='ctl_vehicle'].index.to_list()\nlabels_train = df_train_targets_scored.drop(['sig_id', 'fold'],axis=1).values\nprint(labels_train.shape)\nlabels_train = labels_train[non_ctl_idx]\n\nprint(labels_train.shape)\nbias = tf.keras.initializers.Constant(-np.log(labels_train.mean(axis=0)))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Prediction Clipping Thresholds\np_min = 0.001\np_max = 0.999\n\n# Evaluation Metric with clipping and no label smoothing\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -K.mean(y_true*K.log(y_pred) + (1-y_true)*K.log(1-y_pred))\n\ndef make_model():\n    z = L.Input(shape=(INPUT_SHAPE,), name=\"Id\")\n    x = L.Dense(2048, activation=\"relu\", name=\"d1\")(z)\n    x = L.Dropout(0.25)(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dense(1024, activation=\"relu\", name=\"d2\")(x)\n    x = L.Dropout(0.25)(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dense(512, activation=\"relu\", name=\"d3\")(x)\n    x = L.Dropout(0.25)(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dense(OUTPUT_SHAPE, activation=\"sigmoid\", name=\"p1\")(x)\n    \n    model = M.Model(z, x, name=\"MOA\")\n    model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=[tf.keras.metrics.BinaryCrossentropy()])\n    #model.compile(loss=mloss(0.8), optimizer=tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False), metrics=[score])\n    return model\n\ndef make_model_ddb1():\n    z = L.Input(shape=(INPUT_SHAPE,), name=\"Id\")\n    x = L.Dense(4096, activation=\"relu\", name=\"d1\")(z)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    x = L.Dense(OUTPUT_SHAPE, activation=\"sigmoid\", name=\"p1\")(x)\n    \n    model = M.Model(z, x, name=\"MOA\")\n    model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=[tf.keras.metrics.BinaryCrossentropy()])\n    #model.compile(loss=mloss(0.8), optimizer=tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False), metrics=[score])\n    return model\n\ndef make_model_ddb2():\n    z = L.Input(shape=(INPUT_SHAPE,), name=\"Id\")\n    \n    x = L.Dense(NB_HIDDENS, activation=\"relu\", name=\"d1\")(z)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    \n    x = L.Dense(NB_HIDDENS, activation=\"relu\", name=\"d2\")(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    \n    x = L.Dense(NB_HIDDENS, activation=\"relu\", name=\"d3\")(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    \n    #x = L.Dense(OUTPUT_SHAPE, activation=\"sigmoid\", bias_initializer=bias, name=\"p1\")(x)\n    x = L.Dense(OUTPUT_SHAPE, activation=\"sigmoid\", name=\"p1\")(x)\n    \n    model = M.Model(z, x, name=\"MOA\")\n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=\"adam\", metrics=[tf.keras.metrics.BinaryCrossentropy()])\n    #model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001), metrics=logloss)\n    #model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), metrics=logloss)\n    #model.compile(loss=mloss(0.8), optimizer=tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False), metrics=[score])\n    return model\n\ndef make_model_best():\n    z = L.Input(shape=(INPUT_SHAPE,), name=\"Id\")\n    x = L.Dense(NB_HIDDENS, activation=\"relu\", name=\"d1\")(z)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    \n    \n    x = L.Dense(NB_HIDDENS, activation=\"relu\", name=\"d2\")(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    \n    x = L.Dense(NB_HIDDENS, activation=\"relu\", name=\"d3\")(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    \n    \n    x = L.Dense(NB_HIDDENS, activation=\"relu\", name=\"d4\")(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    \n     \n    x = L.Dense(NB_HIDDENS, activation=\"relu\", name=\"d5\")(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    \n    x = L.Dense(OUTPUT_SHAPE, activation=\"sigmoid\", name=\"p1\")(x)\n    \n    model = M.Model(z, x, name=\"MOA\")\n    model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=[tf.keras.metrics.BinaryCrossentropy()])\n    #model.compile(loss=mloss(0.8), optimizer=tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False), metrics=[score])\n    return model\n\ndef make_model_original():\n    z = L.Input(shape=(INPUT_SHAPE,), name=\"Id\")\n    x = L.Dense(100, activation=\"relu\", name=\"d1\")(z)\n    x = L.Dense(100, activation=\"relu\", name=\"d2\")(x)\n    x = L.Dense(100, activation=\"relu\", name=\"d3\")(x)\n    x = L.Dense(OUTPUT_SHAPE, activation=\"sigmoid\", name=\"p1\")(x)\n    \n    model = M.Model(z, x, name=\"MOA\")\n    model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=[tf.keras.metrics.BinaryCrossentropy()])\n    return model\n\ndef preprocess_df(df, str_cols, scaler=None):\n    cols_to_scale = [c for c in df.columns if c not in str_cols]\n    if scaler is None:\n        scaler = MinMaxScaler()\n        scaler.fit(df[cols_to_scale])\n    np_scaled = scaler.transform(df[cols_to_scale])\n    df_scaled =pd.DataFrame(data=np_scaled[0:,0:],\n            index=[i for i in range(np_scaled.shape[0])],\n            columns=cols_to_scale)\n    \n    for c in str_cols:\n        df_scaled[c] = df[c]\n    \n    df = df_scaled\n    df['cp_type'] = df.apply(lambda row: 1 if row['cp_type']=='trt_cp' else 0, axis=1)\n    #df['cp_type_ctl_vehicle'] = df.apply(lambda row: 1 if row['cp_type']=='ctl_vehicle' else 0, axis=1)\n    df['cp_dose_D1'] = df.apply(lambda row: 1 if row['cp_dose']=='D1' else 0, axis=1)\n    df['cp_dose_D2'] = df.apply(lambda row: 1 if row['cp_dose']=='D2' else 0, axis=1)\n    df['cp_time_24'] = df.apply(lambda row: 1 if row['cp_time']==24 else 0, axis=1)\n    df['cp_time_48'] = df.apply(lambda row: 1 if row['cp_time']==48 else 0, axis=1)\n    df['cp_time_72'] = df.apply(lambda row: 1 if row['cp_time']==72 else 0, axis=1)\n    df.drop(['cp_dose', 'cp_time'], axis=1, inplace=True)\n    return (df_scaled, scaler)\n\ndef display_training_curves(training, validation, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])\n    \ndef get_prediction_multifold(X, Y, X_predict, epochs, batch_size):\n    \n    cols = [c for c in Y if c != 'fold']\n    \n    pe = np.zeros(X_predict.shape)\n    pred = np.zeros((X.shape[0], OUTPUT_SHAPE))\n    \n    pe = None\n\n    cnt = 0\n    diff_eval = 3000\n    history = ''\n    ev_tr = np.zeros((OUTPUT_SHAPE, 1))\n    ev_val = np.zeros((OUTPUT_SHAPE, 1))\n    \n    ev_tr = None\n    ev_val = None\n    targets = [c for c in X if c not in ['sig_id', 'fold']]\n    for i in range(NFOLD):\n    #for tr_idx, val_idx in kf.split(X):\n        cnt += 1\n        print(f\"FOLD {cnt}\")\n        \n        net = make_model_ddb2()\n        \n        test_index = Y.loc[Y.fold==i].index\n        train_index = Y.loc[Y.fold!=i].index\n        \n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = Y.loc[train_index, cols], Y.loc[test_index, cols]\n        \n        callback_lr = get_callback_ReduceLROnPlateau()\n        checkpoint = C.ModelCheckpoint(\n            filepath='best_nn_'+str(cnt)+'.h5',\n            save_best_only=True, \n            monitor='val_loss', \n            mode='min')\n        \n        history=net.fit(\n            X_train,\n            y_train, \n            batch_size=batch_size, \n            epochs=epochs, \n            validation_data=(X_test, y_test), \n            callbacks=[callback_lr, checkpoint],\n            verbose=0\n        )\n            \n        net.load_weights('best_nn_'+str(cnt)+'.h5')    \n        \n        preds_val = net.predict(X_test, batch_size=batch_size, verbose=0)\n        #pred[test_index] = preds_val.reshape(preds_val.shape[0],preds_val.shape[2])\n        \n        display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\n        #display_training_curves(history.history['logloss'], history.history['val_logloss'], 'loss', 211)\n        display_training_curves(history.history['binary_crossentropy'], history.history['val_binary_crossentropy'], 'SCORE', 212)\n        #pe += net.predict(X_predict, batch_size=batch_size, verbose=0) / NFOLD\n        \n        preds_test = net.predict(X_predict, batch_size=BATCH_SIZE, verbose=0) / NFOLD\n        \n        if pe is None:\n            pe = preds_test\n        else:\n            pe = np.add(pe, preds_test)\n        break    \n    score = log_loss(Y[cols], pred)\n    print(\"Score : \", score)\n     \n    return (score, pred, pe)\n\ndef get_prediction_multifold_Conv(X, Y, X_predict, epochs, batch_size):\n    cols = [c for c in Y if c != 'fold']\n    \n    pe = np.zeros(X_predict.shape)\n    pred = np.zeros((X.shape[0], OUTPUT_SHAPE))\n\n    X_predict = X_predict.values.reshape((X_predict.shape[0], 1, X_predict.shape[1]))\n    \n    pe = None\n\n    cnt = 0\n    diff_eval = 3000\n    history = ''\n    ev_tr = np.zeros((OUTPUT_SHAPE, 1))\n    ev_val = np.zeros((OUTPUT_SHAPE, 1))\n    \n    ev_tr = None\n    ev_val = None\n    targets = [c for c in X if c not in ['sig_id', 'fold']]\n    for i in range(NFOLD):\n    #for tr_idx, val_idx in kf.split(X):\n        cnt += 1\n        print(f\"FOLD {cnt}\")\n        \n        net = make_model_Conv1D_2()\n        \n        test_index = Y.loc[Y.fold==i].index\n        train_index = Y.loc[Y.fold!=i].index\n        \n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = Y.loc[train_index, cols], Y.loc[test_index, cols]\n        \n        X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n        X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n        y_train = y_train.values.reshape((y_train.shape[0], 1, y_train.shape[1]))\n        y_test = y_test.values.reshape((y_test.shape[0], 1, y_test.shape[1]))\n        \n        callback_lr = get_callback_ReduceLROnPlateau()\n        checkpoint = C.ModelCheckpoint(\n            filepath='best_nn_'+str(cnt)+'.h5',\n            save_best_only=True, \n            monitor='val_loss', \n            mode='min')\n        \n        history=net.fit(\n            X_train,\n            y_train, \n            batch_size=batch_size, \n            epochs=epochs, \n            validation_data=(X_test, y_test), \n            callbacks=[callback_lr, checkpoint],\n            verbose=0\n        )\n            \n        net.load_weights('best_nn_'+str(cnt)+'.h5')    \n        \n        preds_val = net.predict(X_test, batch_size=batch_size, verbose=0)\n        pred[test_index] = preds_val.reshape(preds_val.shape[0],preds_val.shape[2])\n        \n        display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\n        #display_training_curves(history.history['logloss'], history.history['val_logloss'], 'loss', 211)\n        display_training_curves(history.history['binary_crossentropy'], history.history['val_binary_crossentropy'], 'SCORE', 212)\n        #pe += net.predict(X_predict, batch_size=batch_size, verbose=0) / NFOLD\n        \n        preds_test = net.predict(X_predict, batch_size=BATCH_SIZE, verbose=0) / NFOLD\n        pe_to_add = preds_test.reshape(preds_test.shape[0],preds_test.shape[2])\n        if pe is None:\n            pe = pe_to_add\n        else:\n            pe = np.add(pe, pe_to_add)\n            \n        \n    score = log_loss(Y[cols], pred)\n    print(\"Score : \", score)\n\n    return (score, pred, pe)\n\ndef get_callback_ReduceLROnPlateau():\n    callback = C.ReduceLROnPlateau(\n                monitor='val_loss',\n                factor=0.1,\n                patience=3,\n                verbose=0,\n                mode='min',\n                min_delta=0.00001,\n                cooldown=1,\n                min_lr=0,\n            )\n    return callback\n\n\ndef make_model_Conv1D():\n    z = L.Input(shape=(1,INPUT_SHAPE), name=\"Id\")\n    x = L.Conv1D(25, 100, activation='relu', padding=\"same\", input_shape=(1,INPUT_SHAPE))(z)\n    x = L.Dense(OUTPUT_SHAPE, activation=\"sigmoid\", name=\"p1\")(x)\n    \n    model = M.Model(z, x, name=\"MOA\")\n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.01), optimizer=\"adam\", metrics=[tf.keras.metrics.BinaryCrossentropy()])\n    \n    return model\n\ndef make_model_Conv1D_2():\n    z = L.Input(shape=(INPUT_SHAPE,1), name=\"Id\")\n    x = L.Conv1D(100, 400, activation='relu', padding=\"valid\", input_shape=(INPUT_SHAPE,1))(z)\n    x = L.Conv1D(100, 400, activation='relu', padding=\"valid\", input_shape=(INPUT_SHAPE,1))(x)\n    x = L.Conv1D(100, 80, activation='relu', padding=\"valid\", input_shape=(INPUT_SHAPE,1))(x)\n    x = L.Dense(OUTPUT_SHAPE, activation=\"sigmoid\", name=\"p1\")(x)\n    \n    model = M.Model(z, x, name=\"MOA\")\n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001), optimizer=\"adam\", metrics=[tf.keras.metrics.BinaryCrossentropy()])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nprint('df_train_features ', df_train_features.shape)\nprint('df_train_targets_scored ', df_train_targets_scored.shape)\nprint('df_train_targets_nonscored ', df_train_targets_nonscored.shape)\n\nxx = df_train_targets_nonscored.drop('sig_id', axis=1)\nsum_cols = xx.sum(axis=0).sort_values()\ntranche = sum_cols.loc[sum_cols==6]\nprint(sum_cols[200:250])\n\nx = range(sum_cols.shape[0])\nplt.plot(x, sum_cols)\nplt.show()\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = None\nstr_cols = [c for c in df_train_features.columns if df_train_features[c].dtype=='object']\n#str_cols = [c for c in df_test.columns if df_test[c].dtype=='object']\nstr_cols_train = str_cols.copy()\n\n(df_train_features, scaler) = preprocess_df(df_train_features, str_cols, scaler)\n(df_test, sc) = preprocess_df(df_test, str_cols, scaler)\n\ndf_train_temp = df_train_features.copy()\ndf_train_temp['where']='train'\n\ndf_test_temp = df_test.copy()\ndf_test_temp['where']='test'\ndf_all = df_train_temp.append(df_test_temp)\n\npca = PCA(n_components=N_COMPONENTS, whiten=True)\ncols_pca = [c for c in df_all if c not in ['sig_id', 'where']]\ndf_pca = df_all[cols_pca]\ndf_short = pca.fit_transform(df_pca)\n\ndf_all_short = pd.DataFrame(data=df_short).reset_index(drop=True)\n\ndf_all_sliced = df_all.loc[:, ['sig_id', 'where']].reset_index(drop=True)\n\nprint(\"Shape all short\", df_all_short.shape)\n\nprint(\"Shape df all\", df_all_sliced.shape)\n      \ndf_all_short.head()\ndf_all_sliced.head() \n\ndf_all = pd.concat([df_all_sliced, df_all_short], axis=1, ignore_index=True)\nprint(\"Shape all\", df_all.shape)\n\ndf_train = df_all.loc[df_all[1]=='train']\ndf_test = df_all.loc[df_all[1]=='test']\n\ndf_train.drop(1, axis=1, inplace=True)\ndf_test.drop(1, axis=1, inplace=True)\n\ndf_train.rename(columns={0:'sig_id'}, inplace=True)\ndf_test.rename(columns={0:'sig_id'}, inplace=True)\n\n#cat_features = [c for c in str_cols if c != 'sig_id']\ncat_features = []\ndf_train_all = df_train.merge(df_train_targets_scored).reset_index(drop=True)\n\nsub_sig_id = df_sub['sig_id']\nYs = [c for c in df_sub.columns if c != 'sig_id']\nYss = [c for c in df_train_targets_scored.columns if c != 'sig_id']\n\nX = df_train_all.drop(df_sub.columns, axis=1)\nX = X.drop('fold', axis=1)\nX_test = df_test.drop('sig_id', axis=1)\nY = df_train_all.loc[:,Yss]\nsc = df_sub.head(1).copy()\n\nprint(sc.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Y.shape)\nprint(X.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(score, pred, pe) = get_prediction_multifold(X, Y, X_test, EPOCHS, BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\n#display_training_curves(history.history['mse'], history.history['val_mse'], 'SCORE', 212)\n\ndef ff(x):\n    if float(x) < 0.0001:\n        return 0\n    else:\n        return x\nvfunc = np.vectorize(ff)\n#pe = vfunc(pe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pe.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ID = df_test['sig_id'].reset_index(drop=True)\ndf = pd.DataFrame(data=pe, columns=Ys)\ndf.insert(loc=0, column='sig_id', value=ID)\n\ndf.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ID_train = df_train_targets_scored['sig_id'].reset_index(drop=True)\ndft = pd.DataFrame(data=pred, columns=Ys)\ndft.insert(loc=0, column='sig_id', value=ID_train)\n\ndft.to_csv('submission_train.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}