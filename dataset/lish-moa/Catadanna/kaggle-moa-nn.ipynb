{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\n\nimport gc\n\n#from scipy._lib import *\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler, OrdinalEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\n\nfrom scipy.stats import *\n\nfrom catboost import CatBoostRegressor, Pool\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow.keras.callbacks as C\nimport tensorflow.keras.initializers as I\nimport sklearn\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"scikit-learn version : \", sklearn.__version__)\n\n# epochs: 100  batch size: 1024  nb_hiddens: 256  dropout: 0.1  nb_blocks: 5\n# NEW BEST SCORE :  3.0524902204895814\n# score :  3.0524902204895814\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ALGO = 1\n\nINPUT_SHAPE = 879\nOUTPUT_SHAPE = 206\nBATCH_SIZE=2048\nEPOCHS=200\nNFOLD=5\nDROPOUT=0.1\nNB_HIDDENS=4096\nALPHA_LEAKY_RELU = 0.3\n# epochs: 100  batch size: 128  nb_hiddens: 4096  dropout: 0.2  nb_blocks: 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ndf_train_drug = pd.read_csv('/kaggle/input/lish-moa/train_drug.csv')\ndf_train_targets_scored = pd.read_csv('/kaggle/input/train-with-fold/train_with_fold_y.csv')\ndf_train_targets_nonscored = pd.read_csv('/kaggle/input/lish-moa/train_targets_nonscored.csv')\ndf_test = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\ndf_sub = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\n\nnon_ctl_idx = df_train_features.loc[df_train_features['cp_type']!='ctl_vehicle'].index.to_list()\nlabels_train = df_train_targets_scored.drop(['sig_id', 'fold'],axis=1).values\nprint(labels_train.shape)\nlabels_train = labels_train[non_ctl_idx]\n\nprint(labels_train.shape)\nbias = tf.keras.initializers.Constant(-np.log(labels_train.mean(axis=0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_features = df_train_features.merge(df_train_drug, on=\"sig_id\")\n\ndrugs = df_train_drug['drug_id'].values\n# 3289 values\noe = OrdinalEncoder()\ndrug_id_train = df_train_features.loc[:, ['drug_id']]\noe.fit(drug_id_train)\nscaled_drug_id = oe.transform(drug_id_train)\ndf_train_features['drug_id'] = scaled_drug_id + 1\n\ndf_test['drug_id'] = 0\n\ng_cols = [c for c in df_train_features.columns if 'g-' in c]\nc_cols = [c for c in df_train_features.columns if 'c-' in c]\n\n#print(df_train_features.head())\ng_train = df_train_features.loc[:, g_cols]\n\ndf_train_features['g_mean'] = g_train.apply(lambda x: np.mean(x.values, axis=0))\ndf_train_features['c_mean'] = df_train_features.loc[:,c_cols].apply(lambda x: np.mean(x.values, axis=0))\ndf_test['g_mean'] = df_test.loc[:,g_cols].apply(lambda x: np.mean(x.values, axis=0))\ndf_test['c_mean'] = df_test.loc[:,c_cols].apply(lambda x: np.mean(x.values, axis=0))\n\ndf_train_features['g_std'] = g_train.apply(lambda x: np.std(x.values, axis=0))\ndf_train_features['c_std'] = df_train_features.loc[:,c_cols].apply(lambda x: np.std(x.values, axis=0))\ndf_test['g_std'] = df_test.loc[:,g_cols].apply(lambda x: np.std(x.values, axis=0))\ndf_test['c_std'] = df_test.loc[:,c_cols].apply(lambda x: np.std(x.values, axis=0))\n\ndf_train_features['g_sum'] = g_train.apply(lambda x: np.sum(x.values, axis=0))\ndf_train_features['c_sum'] = df_train_features.loc[:,c_cols].apply(lambda x: np.sum(x.values, axis=0))\ndf_test['g_sum'] = df_test.loc[:,g_cols].apply(lambda x: np.sum(x.values, axis=0))\ndf_test['c_sum'] = df_test.loc[:,c_cols].apply(lambda x: np.sum(x.values, axis=0))\n\ndf_train_features['g_var'] = g_train.apply(lambda x: np.var(x.values, axis=0))\ndf_train_features['c_var'] = df_train_features.loc[:,c_cols].apply(lambda x: np.var(x.values, axis=0))\ndf_test['g_var'] = df_test.loc[:,g_cols].apply(lambda x: np.var(x.values, axis=0))\ndf_test['c_var'] = df_test.loc[:,c_cols].apply(lambda x: np.var(x.values, axis=0))\n\ndf_train_features['g_q025'] = df_train_features.loc[:,g_cols].apply(lambda x: np.quantile(x.values, 0.25, axis=0))\ndf_train_features['c_q025'] = df_train_features.loc[:,c_cols].apply(lambda x: np.quantile(x.values, 0.25, axis=0))\ndf_test['g_q025'] = df_test.loc[:,g_cols].apply(lambda x: np.quantile(x.values, 0.25, axis=0))\ndf_test['c_q025'] = df_test.loc[:,c_cols].apply(lambda x: np.quantile(x.values, 0.25, axis=0))\n                                                \ndf_train_features['g_q075'] = df_train_features.loc[:,g_cols].apply(lambda x: np.quantile(x.values, 0.75, axis=0))\ndf_train_features['c_q075'] = df_train_features.loc[:,c_cols].apply(lambda x: np.quantile(x.values, 0.75, axis=0))\ndf_test['g_q075'] = df_test.loc[:,g_cols].apply(lambda x: np.quantile(x.values, 0.75, axis=0))\ndf_test['c_q075'] = df_test.loc[:,c_cols].apply(lambda x: np.quantile(x.values, 0.75, axis=0))\n\ndf_train_features['g_q010'] = df_train_features.loc[:,g_cols].apply(lambda x: np.quantile(x.values, 0.1, axis=0))\ndf_train_features['c_q010'] = df_train_features.loc[:,c_cols].apply(lambda x: np.quantile(x.values, 0.1, axis=0))\ndf_test['g_q010'] = df_test.loc[:,g_cols].apply(lambda x: np.quantile(x.values, 0.1, axis=0))\ndf_test['c_q010'] = df_test.loc[:,c_cols].apply(lambda x: np.quantile(x.values, 0.1, axis=0))\n                                                \ndf_train_features['g_q090'] = df_train_features.loc[:,g_cols].apply(lambda x: np.quantile(x.values, 0.9, axis=0))\ndf_train_features['c_q090'] = df_train_features.loc[:,c_cols].apply(lambda x: np.quantile(x.values, 0.9, axis=0))\ndf_test['g_q090'] = df_test.loc[:,g_cols].apply(lambda x: np.quantile(x.values, 0.9, axis=0))\ndf_test['c_q090'] = df_test.loc[:,c_cols].apply(lambda x: np.quantile(x.values, 0.9, axis=0))\n\ndf_train_features['g_skew'] = df_train_features.loc[:,g_cols].apply(lambda x: skew(x.values))\ndf_train_features['c_skew'] = df_train_features.loc[:,c_cols].apply(lambda x: skew(x.values))\ndf_test['g_skew'] = df_test.loc[:,g_cols].apply(lambda x: skew(x.values, axis=0))\ndf_test['c_skew'] = df_test.loc[:,c_cols].apply(lambda x: skew(x.values, axis=0))   \n\ndf_train_features['g_kurtosis'] = df_train_features.loc[:,g_cols].apply(lambda x: kurtosis(x.values))\ndf_train_features['c_kurtosis'] = df_train_features.loc[:,c_cols].apply(lambda x: kurtosis(x.values))\ndf_test['g_kurtosis'] = df_test.loc[:,g_cols].apply(lambda x: kurtosis(x.values, axis=0))\ndf_test['c_kurtosis'] = df_test.loc[:,c_cols].apply(lambda x: kurtosis(x.values, axis=0))   \n\nfeature_columns = [c for c in df_train_features.columns if c not in ['sig_id', 'fold']]\nINPUT_SHAPE = len(feature_columns)\n\nfeature_columns = ['cp_dose_D1', 'cp_dose_D1', 'cp_time_24', 'cp_time_48', 'cp_time_72',\n                  'g_mean', 'c_mean', 'g_std', 'c_std', 'g_var', 'c_var', 'g_sum', 'c_sum', 'g_q025', 'c_q025', 'g_q075', 'c_q075',\n                   'g_q010', 'c_q010', 'g_q090', 'c_q090', 'g_skew', 'c_skew', 'g_kurtosis', 'c_kurtosis'\n                  ]\n\nINPUT_SHAPE = len(df_train_features.columns)     \nprint(\"Input shape\", INPUT_SHAPE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Prediction Clipping Thresholds\np_min = 0.001\np_max = 0.999\n\n# Evaluation Metric with clipping and no label smoothing\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -K.mean(y_true*K.log(y_pred) + (1-y_true)*K.log(1-y_pred))\n\ndef make_model():\n    z = L.Input(shape=(INPUT_SHAPE,), name=\"Id\")\n    x = L.Dense(2048, activation=\"relu\", name=\"d1\")(z)\n    x = L.Dropout(0.25)(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dense(1024, activation=\"relu\", name=\"d2\")(x)\n    x = L.Dropout(0.25)(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dense(512, activation=\"relu\", name=\"d3\")(x)\n    x = L.Dropout(0.25)(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dense(OUTPUT_SHAPE, activation=\"sigmoid\", name=\"p1\")(x)\n    \n    model = M.Model(z, x, name=\"MOA\")\n    model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=[tf.keras.metrics.BinaryCrossentropy()])\n    #model.compile(loss=mloss(0.8), optimizer=tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False), metrics=[score])\n    return model\n\ndef make_model_ddb1():\n    initializer = I.LecunNormal()\n    z = L.Input(shape=(INPUT_SHAPE,), kernel_initializer=initializer, name=\"Id\")\n    x = L.Dense(4096, activation=\"relu\", name=\"d1\")(z)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    x = L.Dense(OUTPUT_SHAPE, activation=\"sigmoid\", name=\"p1\")(x)\n    \n    model = M.Model(z, x, name=\"MOA\")\n    model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=[tf.keras.metrics.BinaryCrossentropy()])\n    #model.compile(loss=mloss(0.8), optimizer=tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False), metrics=[score])\n    return model\n\ndef make_model_ddb2():\n    initializer = I.LecunNormal()\n    z = L.Input(shape=(INPUT_SHAPE,), name=\"Id\")\n    \n    x = L.Dense(NB_HIDDENS, activation=\"relu\", name=\"d1\")(z)\n    #x = L.Dense(NB_HIDDENS, bias_initializer='ones', activation=\"relu\", name=\"d1\")(z)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    \n    x = L.Dense(NB_HIDDENS, activation=\"relu\", name=\"d2\")(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    \n    x = L.Dense(NB_HIDDENS, activation=\"relu\", name=\"d3\")(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    \n    #x = L.Dense(OUTPUT_SHAPE, activation=\"sigmoid\", bias_initializer=bias, name=\"p1\")(x)\n    x = L.Dense(OUTPUT_SHAPE, activation=\"sigmoid\", name=\"p1\")(x)\n    \n    model = M.Model(z, x, name=\"MOA\")\n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001), optimizer=\"adam\", metrics=[tf.keras.metrics.BinaryCrossentropy()])\n    #model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001), metrics=logloss)\n    #model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), metrics=logloss)\n    #model.compile(loss=mloss(0.8), optimizer=tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False), metrics=[score])\n    return model\n\ndef make_model_ddb3():\n    initializer = I.LecunNormal()\n    z = L.Input(shape=(INPUT_SHAPE,), kernel_initializer=initializer, name=\"Id\")\n    \n    x = L.Dense(NB_HIDDENS, activation=L.LeakyReLU(alpha=ALPHA_LEAKY_RELU), name=\"d1\")(z)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    \n    x = L.Dense(NB_HIDDENS, activation=L.LeakyReLU(alpha=ALPHA_LEAKY_RELU), name=\"d2\")(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    \n    x = L.Dense(NB_HIDDENS, activation=L.LeakyReLU(alpha=ALPHA_LEAKY_RELU), name=\"d3\")(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    \n    x = L.Dense(OUTPUT_SHAPE, activation=\"sigmoid\", bias_initializer=bias, name=\"p1\")(x)\n    \n    model = M.Model(z, x, name=\"MOA\")\n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001), optimizer=\"adam\", metrics=[tf.keras.metrics.BinaryCrossentropy()])\n    #model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001), metrics=logloss)\n    #model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), metrics=logloss)\n    #model.compile(loss=mloss(0.8), optimizer=tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False), metrics=[score])\n    return model\n\ndef make_model_best():\n    z = L.Input(shape=(INPUT_SHAPE,), name=\"Id\")\n    x = L.Dense(NB_HIDDENS, activation=\"relu\", name=\"d1\")(z)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    \n    \n    x = L.Dense(NB_HIDDENS, activation=\"relu\", name=\"d2\")(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    \n    x = L.Dense(NB_HIDDENS, activation=\"relu\", name=\"d3\")(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    \n    \n    x = L.Dense(NB_HIDDENS, activation=\"relu\", name=\"d4\")(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    \n     \n    x = L.Dense(NB_HIDDENS, activation=\"relu\", name=\"d5\")(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(DROPOUT)(x)\n    \n    x = L.Dense(OUTPUT_SHAPE, activation=\"sigmoid\", name=\"p1\")(x)\n    \n    model = M.Model(z, x, name=\"MOA\")\n    model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=[tf.keras.metrics.BinaryCrossentropy()])\n    #model.compile(loss=mloss(0.8), optimizer=tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False), metrics=[score])\n    return model\n\ndef make_model_original():\n    z = L.Input(shape=(INPUT_SHAPE,), name=\"Id\")\n    x = L.Dense(100, activation=\"relu\", name=\"d1\")(z)\n    x = L.Dense(100, activation=\"relu\", name=\"d2\")(x)\n    x = L.Dense(100, activation=\"relu\", name=\"d3\")(x)\n    x = L.Dense(OUTPUT_SHAPE, activation=\"sigmoid\", name=\"p1\")(x)\n    \n    model = M.Model(z, x, name=\"MOA\")\n    model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=[tf.keras.metrics.BinaryCrossentropy()])\n    return model\n\ndef preprocess_df(df, str_cols, scaler=None):\n    cols_to_scale = [c for c in df.columns if c not in str_cols]\n    if scaler is None:\n        #scaler = MinMaxScaler()\n        scaler = RobustScaler()\n        scaler.fit(df[cols_to_scale])\n    np_scaled = scaler.transform(df[cols_to_scale])\n    df_scaled =pd.DataFrame(data=np_scaled[0:,0:],\n            index=[i for i in range(np_scaled.shape[0])],\n            columns=cols_to_scale)\n    \n    for c in str_cols:\n        df_scaled[c] = df[c]\n    \n    df = df_scaled\n    df['cp_type'] = df.apply(lambda row: 1 if row['cp_type']=='trt_cp' else 0, axis=1)\n    #df['cp_type_ctl_vehicle'] = df.apply(lambda row: 1 if row['cp_type']=='ctl_vehicle' else 0, axis=1)\n    df['cp_dose_D1'] = df.apply(lambda row: 1 if row['cp_dose']=='D1' else 0, axis=1)\n    df['cp_dose_D2'] = df.apply(lambda row: 1 if row['cp_dose']=='D2' else 0, axis=1)\n    df['cp_time_24'] = df.apply(lambda row: 1 if row['cp_time']==24 else 0, axis=1)\n    df['cp_time_48'] = df.apply(lambda row: 1 if row['cp_time']==48 else 0, axis=1)\n    df['cp_time_72'] = df.apply(lambda row: 1 if row['cp_time']==72 else 0, axis=1)\n    df.drop(['cp_dose', 'cp_time'], axis=1, inplace=True)\n    return (df_scaled, scaler)\n\ndef display_training_curves(training, validation, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])\n    \ndef get_prediction_multifold(X, Y, X_predict, df_test, Ys):\n    \n    cols = [c for c in Y if c != 'fold']\n    \n    pe = np.zeros((X_predict.shape[0], OUTPUT_SHAPE))\n    pee = np.zeros((X_predict.shape[0], OUTPUT_SHAPE))\n    pred = np.zeros((X.shape[0], OUTPUT_SHAPE))\n    \n    #pe = None\n    #pee = None\n    cnt = 0\n    diff_eval = 3000\n    history = ''\n    ev_tr = np.zeros((OUTPUT_SHAPE, 1))\n    ev_val = np.zeros((OUTPUT_SHAPE, 1))\n    \n    ev_tr = None\n    ev_val = None\n    targets = [c for c in X if c not in ['sig_id', 'fold']]\n    for i in range(NFOLD):\n    #for tr_idx, val_idx in kf.split(X):\n        cnt += 1\n        print(f\"FOLD {cnt}\")\n        \n        #net = make_model()\n        net = make_model_original()\n        \n        test_index = Y.loc[Y.fold==i].index\n        train_index = Y.loc[Y.fold!=i].index\n        \n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = Y.loc[train_index, cols], Y.loc[test_index, cols]\n           \n        callback_lr = get_callback_ReduceLROnPlateau()\n        checkpoint = C.ModelCheckpoint(\n            filepath='best_nn_'+str(cnt)+'.h5',\n            save_best_only=True, \n            monitor='val_loss', \n            mode='min')\n        \n        history=net.fit(\n            X_train,\n            y_train, \n            batch_size=BATCH_SIZE, \n            epochs=EPOCHS, \n            validation_data=(X_test, y_test), \n            callbacks=[callback_lr], #, checkpoint],\n            verbose=0\n        )\n            \n        #net.load_weights('best_nn_'+str(cnt)+'.h5')    \n        \n        yhat_val = net.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n        pred[test_index] = yhat_val\n        #score_fold = log_loss(y_test, yhat_val)\n        #print(\"Score fold \", cnt, \":\", score_fold)\n        \n        display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\n        #display_training_curves(history.history['logloss'], history.history['val_logloss'], 'loss', 211)\n        display_training_curves(history.history['binary_crossentropy'], history.history['val_binary_crossentropy'], 'SCORE', 212)\n        \n        preds_test = net.predict(X_predict, batch_size=BATCH_SIZE, verbose=0) / NFOLD\n        pee_test = net.predict(X_predict, batch_size=BATCH_SIZE, verbose=0)\n        \n        if pe is None:\n            pe = preds_test\n            pee = pee_test\n        else:\n            pe = np.add(pe, preds_test)\n            pee = np.add(pee, pee_test)\n         \n        \n        pp = pee / cnt\n        ID = df_test['sig_id']\n        df = pd.DataFrame(data=pp, columns=Ys)\n        df.insert(loc=0, column='sig_id', value=ID)\n        df.to_csv('submission.csv', index=False)  \n        \n    score = log_loss(Y[cols], pred)\n    print(\"Score : \", score)\n    \n    #ID = df_test['sig_id']\n    #df = pd.DataFrame(data=pe, columns=Ys)\n    #df.insert(loc=0, column='sig_id', value=ID)\n    #df.to_csv('submission.csv', index=False)\n    \n    return (score, pred, pe)\n\n\ndef get_prediction_multifold_reg(X, Y, X_predict, epochs, batch_size, df_test, Ys):\n    \n    cols = [c for c in Y if c != 'fold']\n    \n    pe = np.zeros(X_predict.shape)\n    pred = np.zeros((X.shape[0], OUTPUT_SHAPE))\n    \n    pe = None\n    \n    cnt = 0\n    diff_eval = 3000\n    history = ''\n    ev_tr = np.zeros((OUTPUT_SHAPE, 1))\n    ev_val = np.zeros((OUTPUT_SHAPE, 1))\n    \n    ev_tr = None\n    ev_val = None\n    targets = [c for c in X if c not in ['sig_id', 'fold']]\n    for i in range(NFOLD):\n    #for tr_idx, val_idx in kf.split(X):\n        cnt += 1\n        print(f\"FOLD {cnt}\")\n        \n        net = TabNetMultiTaskClassifier()\n        \n        test_index = Y.loc[Y.fold==i].index\n        train_index = Y.loc[Y.fold!=i].index\n        \n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = Y.loc[train_index, cols], Y.loc[test_index, cols]\n           \n        callback_lr = get_callback_ReduceLROnPlateau()\n        checkpoint = C.ModelCheckpoint(\n            filepath='best_nn_'+str(cnt)+'.h5',\n            save_best_only=True, \n            monitor='val_loss', \n            mode='min')\n        \n        history=net.fit(\n            X_train,\n            y_train, \n            batch_size=batch_size, \n            epochs=epochs, \n            validation_data=(X_test, y_test), \n            callbacks=[callback_lr, checkpoint],\n            verbose=0\n        )\n            \n        net.load_weights('best_nn_'+str(cnt)+'.h5')    \n        \n        pred[test_index] = net.predict(X_test, batch_size=batch_size, verbose=0)\n        \n        display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\n        #display_training_curves(history.history['logloss'], history.history['val_logloss'], 'loss', 211)\n        display_training_curves(history.history['binary_crossentropy'], history.history['val_binary_crossentropy'], 'SCORE', 212)\n        \n        preds_test = net.predict(X_predict, batch_size=BATCH_SIZE, verbose=0) / NFOLD\n        if pe is None:\n            pe = preds_test\n        else:\n            pe = np.add(pe, preds_test)\n    score = log_loss(Y[cols], pred)\n    print(\"Score : \", score)\n    \n    ID = df_test['sig_id']\n    df = pd.DataFrame(data=pe, columns=Ys)\n    df.insert(loc=0, column='sig_id', value=ID)\n    \n    df.to_csv('submission.csv', index=False)\n    \n    return (score, pred, pe)\n\n\ndef get_prediction_multifold_Conv1D(X, Y, X_predict, epochs, batch_size, df_test, Ys):\n    \n    cols = [c for c in Y if c != 'fold']\n    \n    pe = np.zeros(X_predict.shape)\n    pred = np.zeros((X.shape[0], OUTPUT_SHAPE))\n\n    X_predict = X_predict.values.reshape((X_predict.shape[0], 1, X_predict.shape[1]))\n    \n    pe = None\n    \n    cnt = 0\n    diff_eval = 3000\n    history = ''\n    ev_tr = np.zeros((OUTPUT_SHAPE, 1))\n    ev_val = np.zeros((OUTPUT_SHAPE, 1))\n    \n    ev_tr = None\n    ev_val = None\n    targets = [c for c in X if c not in ['sig_id', 'fold']]\n    for i in range(NFOLD):\n    #for tr_idx, val_idx in kf.split(X):\n        cnt += 1\n        print(f\"FOLD {cnt}\")\n        \n        #net = make_model_Conv1D_2()\n        \n        net = make_model_ddb3()\n        \n        test_index = Y.loc[Y.fold==i].index\n        train_index = Y.loc[Y.fold!=i].index\n        \n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = Y.loc[train_index, cols], Y.loc[test_index, cols]\n        \n        X_train = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n        X_test = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n        y_train = y_train.values.reshape((y_train.shape[0], 1, y_train.shape[1]))\n        y_test = y_test.values.reshape((y_test.shape[0], 1, y_test.shape[1]))\n           \n        callback_lr = get_callback_ReduceLROnPlateau()\n        checkpoint = C.ModelCheckpoint(\n            filepath='best_nn_'+str(cnt)+'.h5',\n            save_best_only=True, \n            monitor='val_loss', \n            mode='min')\n        \n        history=net.fit(\n            X_train,\n            y_train, \n            batch_size=batch_size, \n            epochs=epochs, \n            validation_data=(X_test, y_test), \n            callbacks=[callback_lr, checkpoint],\n            verbose=0\n        )\n            \n        net.load_weights('best_nn_'+str(cnt)+'.h5')    \n        \n        preds_val = net.predict(X_test, batch_size=batch_size, verbose=0)\n        pred[test_index] = preds_val.reshape(preds_val.shape[0],preds_val.shape[2]) \n        pred[test_index] = preds_val\n        \n        display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\n        #display_training_curves(history.history['logloss'], history.history['val_logloss'], 'loss', 211)\n        display_training_curves(history.history['binary_crossentropy'], history.history['val_binary_crossentropy'], 'SCORE', 212)\n        #pe += net.predict(X_predict, batch_size=batch_size, verbose=0) / NFOLD\n        \n        preds_test = net.predict(X_predict, batch_size=BATCH_SIZE, verbose=0) / NFOLD\n        pe_to_add = preds_test.reshape(preds_test.shape[0],preds_test.shape[2])\n        pe_to_add = preds_test\n        if pe is None:\n            pe = pe_to_add\n        else:\n            pe = np.add(pe, pe_to_add)\n    score = log_loss(Y[cols], pred)\n    print(\"Score : \", score)\n\n    ID = df_test['sig_id']\n    df = pd.DataFrame(data=pe, columns=Ys)\n    df.insert(loc=0, column='sig_id', value=ID)\n    \n    df.to_csv('submission.csv', index=False)\n    \n    return (score, pred, pe)\n\ndef get_callback_ReduceLROnPlateau():\n    callback = C.ReduceLROnPlateau(\n                monitor='val_loss',\n                factor=0.1,\n                patience=3,\n                verbose=0,\n                mode='min',\n                min_delta=0.00001,\n                cooldown=1,\n                min_lr=0,\n            )\n    return callback\n\ndef make_model_Conv1D():\n    z = L.Input(shape=(1,INPUT_SHAPE), name=\"Id\")\n    x = L.Conv1D(25, 100, activation='relu', padding=\"same\", input_shape=(1,INPUT_SHAPE))(z)\n    x = L.Dense(OUTPUT_SHAPE, activation=\"sigmoid\", name=\"p1\")(x)\n    \n    model = M.Model(z, x, name=\"MOA\")\n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.01), optimizer=\"adam\", metrics=[tf.keras.metrics.BinaryCrossentropy()])\n    \n    return model\n\ndef make_model_Conv1D_2():\n    z = L.Input(shape=(INPUT_SHAPE,1), name=\"Id\")\n    x = L.Conv1D(100, 400, activation='relu', padding=\"valid\", input_shape=(INPUT_SHAPE,1))(z)\n    x = L.Conv1D(100, 400, activation='relu', padding=\"valid\", input_shape=(INPUT_SHAPE,1))(x)\n    x = L.Conv1D(100, 80, activation='relu', padding=\"valid\", input_shape=(INPUT_SHAPE,1))(x)\n    x = L.Dense(OUTPUT_SHAPE, activation=\"sigmoid\", name=\"p1\")(x)\n    \n    model = M.Model(z, x, name=\"MOA\")\n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001), optimizer=\"adam\", metrics=[tf.keras.metrics.BinaryCrossentropy()])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nprint('df_train_features ', df_train_features.shape)\nprint('df_train_targets_scored ', df_train_targets_scored.shape)\nprint('df_train_targets_nonscored ', df_train_targets_nonscored.shape)\n\nxx = df_train_targets_nonscored.drop('sig_id', axis=1)\nsum_cols = xx.sum(axis=0).sort_values()\ntranche = sum_cols.loc[sum_cols==6]\nprint(sum_cols[200:250])\n\nx = range(sum_cols.shape[0])\nplt.plot(x, sum_cols)\nplt.show()\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = None\nstr_cols = [c for c in df_train_features.columns if df_train_features[c].dtype=='object']\n#str_cols = [c for c in df_test.columns if df_test[c].dtype=='object']\nstr_cols_train = str_cols.copy()\n\n(df_train_features, scaler) = preprocess_df(df_train_features, str_cols, scaler)\n(df_test, sc) = preprocess_df(df_test, str_cols, scaler)\n\n#cat_features = [c for c in str_cols if c != 'sig_id']\ncat_features = []\ndf_train_all = df_train_features.merge(df_train_targets_scored).reset_index(drop=True)\n\nsub_sig_id = df_sub['sig_id']\nYs = [c for c in df_sub.columns if c != 'sig_id']\nYss = [c for c in df_train_targets_scored.columns if c != 'sig_id']\n\nif ALGO == 0:\n    X = df_train_all.drop(df_sub.columns, axis=1)\n    X = X.drop('fold', axis=1)\n    X_test = df_test.drop('sig_id', axis=1)\nelif ALGO == 1:\n    X = df_train_all[feature_columns]\n    X_test = df_test[feature_columns]\nY_df = df_train_all.loc[:,Yss]\n\nscaler_rs_Y = RobustScaler()\nscaler_rs_Y.fit(Y_df)\nY_scaled = scaler_rs_Y.transform(Y_df)\nY = pd.DataFrame(data=Y_scaled, columns=Yss).reset_index(drop=True)\n\ndel Y_df\ngc.collect()\n\nsc = df_sub.head(1).copy()\n\nprint(sc.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Y.shape)\nprint(X.shape)\nprint(X_test.shape)\n\nINPUT_SHAPE = X.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(score, pred, pe) = get_prediction_multifold(X, Y, X_test, df_test, Ys)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\n#display_training_curves(history.history['mse'], history.history['val_mse'], 'SCORE', 212)\n\ndef ff(x):\n    if float(x) < 0.0001:\n        return 0\n    else:\n        return x\nvfunc = np.vectorize(ff)\n#pe = vfunc(pe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pe.shape)\npe_scaled = scaler_rs_Y.transform(pe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ID = df_test['sig_id']\ndf = pd.DataFrame(data=pe_scaled, columns=Ys)\ndf.insert(loc=0, column='sig_id', value=ID)\n\ndf.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ID_train = df_train_targets_scored['sig_id']\ndft = pd.DataFrame(data=pred, columns=Ys)\ndft.insert(loc=0, column='sig_id', value=ID_train)\n\ndft.to_csv('submission_train.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}