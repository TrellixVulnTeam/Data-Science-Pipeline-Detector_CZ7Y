{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this approach we use Bag of decision trees.\n**Features:**\n\nSurrouding cells for a given cell.\n\n**Data Augmentation**\n* Background color is detected using custom logic.\n* Custom logic is written to find the colors which need to be excempted from augmentation.\n * All the colors which are present in all the input of training exmaples are marked for excemption.\n * Color which is not present in input but present in output are excepted.\n* For rest of colors all permutations are used to augment.\n\n**Model** \n\n Bag of decision trees are used since, single decision tree have some randomness factor which may give incorrect result.\n \n **Result**\n \n Training: Solved 41/400\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json\nfrom pathlib import Path\nfrom collections import defaultdict\nfrom itertools import product\nfrom matplotlib import colors\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom itertools import combinations,permutations\nfrom sklearn.tree import *\nfrom sklearn import tree\nfrom sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nimport random\nfrom math import floor\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_path = Path(\"/kaggle/input/abstraction-and-reasoning-challenge\")\ntrain_path = data_path/'training'\ntest_path = data_path/'test'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_result(inp,eoup,oup):\n    \"\"\"\n    Plots the first train and test pairs of a specified task,\n    using same color scheme as the ARC app\n    \"\"\"\n    cmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n    norm = colors.Normalize(vmin=0, vmax=9)\n    fig, axs = plt.subplots(1, 3, figsize=(15,15))\n    \n    axs[0].imshow(inp, cmap=cmap, norm=norm)\n    axs[0].axis('off')\n    axs[0].set_title('Input')\n\n    axs[1].imshow(eoup, cmap=cmap, norm=norm)\n    axs[1].axis('off')\n    axs[1].set_title('Output')\n    \n    axs[2].imshow(oup, cmap=cmap, norm=norm)\n    axs[2].axis('off')\n    axs[2].set_title('Model prediction')\n    \n    plt.grid()\n    plt.tight_layout()\n    plt.show()\n    \ndef plot_training(inp,eoup):\n    \"\"\"\n    Plots the first train and test pairs of a specified task,\n    using same color scheme as the ARC app\n    \"\"\"\n    cmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n    norm = colors.Normalize(vmin=0, vmax=9)\n    fig, axs = plt.subplots(1, 2, figsize=(15,15))\n    \n    axs[0].imshow(inp, cmap=cmap, norm=norm)\n    axs[0].axis('off')\n    axs[0].set_title('Input')\n\n    axs[1].imshow(eoup, cmap=cmap, norm=norm)\n    axs[1].axis('off')\n    axs[1].set_title('Output')\n    \n    plt.grid()\n    plt.tight_layout()\n    plt.show()\n\ndef plot_mat(inp, title = \"Input\"):\n    \"\"\"\n    Plots the first train and test pairs of a specified task,\n    using same color scheme as the ARC app\n    \"\"\"\n    cmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n    norm = colors.Normalize(vmin=0, vmax=9)\n    fig, axs = plt.subplots(1, 1, figsize=(5,5))\n    \n    axs.imshow(inp, cmap=cmap, norm=norm)\n    axs.axis('on')\n    axs.set_title(title)\n  \n    plt.grid()\n    plt.tight_layout()\n    plt.show()\n\ndef plot_mats(mats):\n    cmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n    norm = colors.Normalize(vmin=0, vmax=9)\n    fig, axs = plt.subplots(1, len(mats), figsize=(15,15))\n    \n    for i in range(len(mats)):\n        axs[i].imshow(mats[i], cmap=cmap, norm=norm)\n        axs[i].axis('off')\n        axs[i].set_title('Fig: '+str(i))\n    \n    plt.rc('grid', linestyle=\"-\", color='white')\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getiorc(pair):\n    inp = pair[\"input\"]\n    return pair[\"input\"],pair[\"output\"],len(inp),len(inp[0])\n    \ndef getAround(i,j,inp,size=1):\n    #v = [-1,-1,-1,-1,-1,-1,-1,-1,-1]\n    r,c = len(inp),len(inp[0])\n    v = []\n    sc = [0]\n    for q in range(size):\n        sc.append(q+1)\n        sc.append(-(q+1))\n    for idx,(x,y) in enumerate(product(sc,sc)):\n        ii = (i+x)\n        jj = (j+y)\n        v.append(-1)\n        if((0<= ii < r) and (0<= jj < c)):\n            v[idx] = (inp[ii][jj])\n    return v\n\ndef getDiagonal(i,j,r,c):\n    return\n        \n    \ndef getX(inp,i,j,size):\n    z = []\n    n_inp = np.array(inp)\n    z.append(i)\n    z.append(j)\n    r,c = len(inp),len(inp[0])\n    for m in range(5):\n        z.append(i%(m+1))\n        z.append(j%(m+1))\n    z.append(i+j)\n    z.append(i*j)\n#     z.append(i%j)\n#     z.append(j%i)\n    z.append((i+1)/(j+1))\n    z.append((j+1)/(i+1))\n    z.append(r)\n    z.append(c)\n    z.append(len(np.unique(n_inp[i,:])))\n    z.append(len(np.unique(n_inp[:,j])))\n    arnd = getAround(i,j,inp,size)\n    z.append(len(np.unique(arnd)))\n    z.extend(arnd)\n    return z\n\n    \ndef getXAll(inp,i,j):\n    z = []\n    z.append(inp[i][j])\n    \"\"\"\n    z.append(i)\n    z.append(j)\n    for m in range(5):\n        z.append(i%(m+1))\n        z.append(j%(m+1))\n    z.append(i+j)\n    z.append(i*j)\n    z.append((i+1)/(j+1))\n    z.append((j+1)/(i+1))\n    \"\"\"\n    r,c = len(inp),len(inp[0])\n    for i in range(r):\n        for j in range(c):\n            z.append(inp[i][j])\n    return z\n\ndef getXy(inp,oup,size):\n    x = []\n    y = []\n    r,c = len(inp),len(inp[0])\n    for i in range(r):\n        for j in range(c):\n            x.append(getX(inp,i,j,size))\n            y.append(oup[i][j])\n    return x,y\n    \ndef getXyAll(inp,oup):\n    x = []\n    y = []\n    r,c = len(inp),len(inp[0])\n    for i in range(r):\n        for j in range(c):\n            x.append(getXAll(inp,i,j))\n            y.append(oup[i][j])\n    return x,y\n    \ndef getBkgColor(task_json):\n    color_dict = defaultdict(int)\n    \n    for pair in task_json['train']:\n        inp,oup,r,c = getiorc(pair)\n        for i in range(r):\n            for j in range(c):\n                color_dict[inp[i][j]]+=1\n    color = -1\n    max_count = 0\n    for col,cnt in color_dict.items():\n        if(cnt > max_count):\n            color = col\n            max_count = cnt\n    return color","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_num_colors(inp,oup,bl_cols):\n    r,c = len(inp),len(inp[0])\n    return \n\ndef replace(inp,uni,perm):\n    # uni = '234' perm = ['5','7','9']\n    #print(uni,perm)\n    r_map = { int(c):int(s) for c,s in zip(uni,perm)}\n    r,c = len(inp),len(inp[0])\n    rp = np.array(inp).tolist()\n    #print(rp)\n    for i in range(r):\n        for j in range(c):\n            if(rp[i][j] in r_map):\n                rp[i][j] = r_map[rp[i][j]]\n    return rp\n            \n    \ndef augment(inp,oup,bl_cols):\n    cols = \"0123456789\"\n    npr_map = [1,9,72,3024,15120,60480,181440,362880,362880]\n    uni = \"\".join([str(x) for x in np.unique(inp).tolist()])\n    for c in bl_cols:\n        cols=cols.replace(str(c),\"\")\n        uni=uni.replace(str(c),\"\")\n\n    exp_size = len(inp)*len(inp[0])*npr_map[len(uni)]\n#    print(\"cols\",cols,\"uni\",uni,\"exp_size\",exp_size)\n    mod = floor(exp_size/120000)\n    mod = 1 if mod==0 else mod\n    \n    #print(exp_size,mod,len(uni))\n    result = []\n    count = 0\n    for comb in combinations(cols,len(uni)):\n        for perm in permutations(comb):\n            count+=1\n            if(count % mod == 0):\n                result.append((replace(inp,uni,perm),replace(oup,uni,perm)))\n    return result\n            \ndef get_flips(inp,oup):\n    result = []\n    n_inp = np.array(inp)\n    n_oup = np.array(oup)\n    result.append((np.fliplr(inp).tolist(),np.fliplr(oup).tolist()))\n    result.append((np.rot90(np.fliplr(inp),1).tolist(),np.rot90(np.fliplr(oup),1).tolist()))\n    result.append((np.rot90(np.fliplr(inp),2).tolist(),np.rot90(np.fliplr(oup),2).tolist()))\n    result.append((np.rot90(np.fliplr(inp),3).tolist(),np.rot90(np.fliplr(oup),3).tolist()))\n    result.append((np.flipud(inp).tolist(),np.flipud(oup).tolist()))\n    result.append((np.rot90(np.flipud(inp),1).tolist(),np.rot90(np.flipud(oup),1).tolist()))\n    result.append((np.rot90(np.flipud(inp),2).tolist(),np.rot90(np.flipud(oup),2).tolist()))\n    result.append((np.rot90(np.flipud(inp),3).tolist(),np.rot90(np.flipud(oup),3).tolist()))\n    result.append((np.fliplr(np.flipud(inp)).tolist(),np.fliplr(np.flipud(oup)).tolist()))\n    result.append((np.flipud(np.fliplr(inp)).tolist(),np.flipud(np.fliplr(oup)).tolist()))\n    return result\n    \ndef get_4_flips(inp,oup):\n    result = []\n    n_inp = np.array(inp)\n    n_oup = np.array(oup)\n    result.append((np.fliplr(inp).tolist(),np.fliplr(oup).tolist()))\n    result.append((np.flipud(inp).tolist(),np.flipud(oup).tolist()))\n    result.append((np.rot90(inp).tolist(),np.rot90(oup).tolist()))\n    result.append((np.rot90(inp,3).tolist(),np.rot90(oup,3).tolist()))\n    return result\n\ndef gettaskxy(task_json,aug,around_size,bl_cols,flip=True):    \n    X = []\n    Y = []\n    for pair in task_json['train']:\n        inp,oup=pair[\"input\"],pair[\"output\"]\n        tx,ty = getXy(inp,oup,around_size)\n#        tx,ty = getXyAll(inp,oup)\n        X.extend(tx)\n        Y.extend(ty)\n        if(flip):\n            for ainp,aoup in get_flips(inp,oup):\n                tx,ty = getXy(ainp,aoup,around_size)\n#                tx,ty = getXyAll(ainp,aoup)\n                X.extend(tx)\n                Y.extend(ty)\n                if(aug):\n                    augs = augment(ainp,aoup,bl_cols)\n                    for ainp,aoup in augs:\n                        tx,ty = getXy(ainp,aoup,around_size)\n#                        tx,ty = getXyAll(ainp,aoup)\n                        X.extend(tx)\n                        Y.extend(ty)\n        if(aug):\n            augs = augment(inp,oup,bl_cols)\n            for ainp,aoup in augs:\n                tx,ty = getXy(ainp,aoup,around_size)\n#                tx,ty = getXyAll(ainp,aoup)\n                X.extend(tx)\n                Y.extend(ty)\n    return X,Y\n\ndef test_predict(task_json,model,size):\n    inp = task_json['test'][0]['input']\n    eoup = task_json['test'][0]['output']\n    r,c = len(inp),len(inp[0])\n    oup = predict(inp,model,size)\n    return inp,eoup,oup\n\ndef predict(inp,model,size):\n    r,c = len(inp),len(inp[0])\n    oup = np.zeros([r,c],dtype=int)\n    for i in range(r):\n        for j in range(c):\n            x = getX(inp,i,j,size)\n#            x = getXAll(inp,i,j)\n            o = int(model.predict([x]))\n            o = 0 if o<0 else o\n            oup[i][j]=o\n    return oup\n\ndef submit_predict(task_json,model,size):\n    pred_map = {}\n    idx=0\n    for pair in task_json['test']:\n        inp = pair[\"input\"]\n        oup = predict(inp,model,size)\n        pred_map[idx] = oup.tolist()\n        idx+=1\n        plot_result(inp,oup,oup)\n    return pred_map\n\ndef dumb_predict(task_json):\n    pred_map = {}\n    idx=0\n    for pair in task_json['test']:\n        inp = pair[\"input\"]\n        pred_map[idx] = [[0,0],[0,0]]\n        idx+=1\n    return pred_map\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# difference in matrix size\ndef get_size_loss(test, oup):\n    return (len(test) - len(oup)) + (len(test[0]) - len(oup[0]))\n\n# difference in pixel count other then back ground colour if -ve then less pixel or if +ve then extra pixel \n# if both image have same number of pixel with same colour then return 0\n# return first colour of test that is not present in out or colour with different count\ndef get_count_loss(bg_col, test, oup):\n    u_test, uc_test = np.unique(test, return_counts = True)\n    u_oup, uc_oup = np.unique(oup, return_counts = True)\n    for uni_col in u_test:\n        if uni_col == bg_col:\n            continue\n        if uni_col not in u_oup:\n            return uni_col, -1\n        diff = uc_oup[u_oup.index(uni_col)] - uc_test[u_test.index(uni_col)]\n        if diff != 0:\n            return uni_col, diff\n    return -1, -1\n\n# difference in pixel count other then back ground colour if -ve then less pixel or if +ve then extra pixel \n# if both image have same number of pixel with same colour then return 0\n# return first colour of test that is not present in out or colour with different count\ndef get_col_count_loss(bg_col, test, oup):\n    u_test, uc_test = np.unique(test, return_counts = True)\n    u_oup, uc_oup = np.unique(oup, return_counts = True)\n    for uni_col in u_test:\n        if uni_col == bg_col:\n            continue\n        if uni_col not in u_oup:\n            return uni_col, -1\n        diff = uc_oup[u_oup.index(uni_col)] - uc_test[u_test.index(uni_col)]\n        if diff != 0:\n            return uni_col, diff\n    return -1, -1\n\n# at this point we are assuming that input/test and output have same number of colour\n# this function will return error in terms of distance of nearest same colour pixel \n#def get_shift_loss(bg_col, test, oup):\n    \n\ndef get_total_loss(bg_col, test, oup):\n    return get_count_loss(bg_col, test, oup)\n\ndef get_loss(model,task_json,size):\n    total = 0\n    for pair in task_json['train']:\n        inp,oup=pair[\"input\"],pair[\"output\"]\n        eoup = predict(inp,model,size)\n        total+= np.sum((np.array(oup) != np.array(eoup)))\n    return total\n\ndef get_test_loss(model,task_json,size):\n    total = 0\n    for pair in task_json['test']:\n        inp,oup=pair[\"input\"],pair[\"output\"]\n        eoup = predict(inp,model,size)\n        total+= np.sum((np.array(oup) != np.array(eoup)))\n    return total\n\ndef get_a_size(task_json):\n    return 4;\n\ndef get_bl_cols(task_json):\n    result = []\n    bkg_col = getBkgColor(task_json);\n    result.append(bkg_col)\n    # num_input,input_cnt,num_output,output_cnt\n    met_map = {}\n    for i in range(10):\n        met_map[i] = [0,0,0,0]\n        \n    total_ex = 0\n    # for each input and output pair create a map of unique colours and their count\n    for pair in task_json['train']:\n        inp,oup=pair[\"input\"],pair[\"output\"]\n        u,uc = np.unique(inp, return_counts=True)\n        inp_cnt_map = dict(zip(u,uc))\n        u,uc = np.unique(oup, return_counts=True)\n        oup_cnt_map = dict(zip(u,uc))\n        # add unique colours and their count to common map\n        for col,cnt in inp_cnt_map.items():\n            met_map[col][0] = met_map[col][0] + 1\n            met_map[col][1] = met_map[col][1] + cnt\n        for col,cnt in oup_cnt_map.items():\n            met_map[col][2] = met_map[col][2] + 1\n            met_map[col][3] = met_map[col][3] + cnt\n        total_ex+=1\n    # now each row of met_map represent a colour and their total count of all examples of input and output\n    for col,met in met_map.items():\n        num_input,input_cnt,num_output,output_cnt = met\n        # record a colour if it appears in both input and output of all examples\n        if(num_input == total_ex or num_output == total_ex):\n            result.append(col)\n        # record a colour if it is not present in any input but present in atleast one output of all examples\n        elif(num_input == 0 and num_output > 0):\n            result.append(col)\n    \n    result = np.unique(result).tolist()\n    if(len(result) == 10):\n        result.append(bkg_col)\n    return np.unique(result).tolist()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def flattener(pred):\n    str_pred = str([row for row in pred])\n    str_pred = str_pred.replace(', ', '')\n    str_pred = str_pred.replace('[[', '|')\n    str_pred = str_pred.replace('][', '|')\n    str_pred = str_pred.replace(']]', '|')\n    return str_pred\n\ndef combine_preds(tid,pm1,pm3,pm5):\n    result = []\n    for i in range(len(pm1)):\n        tk_s = tid+\"_\"+str(i)\n        str_pred = flattener(pm1[i])+\" \"+flattener(pm3[i])+\" \"+flattener(pm5[i])\n        #print(tk_s,str_pred)\n        result.append([tk_s,str_pred])\n    return result\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\"\"\"\ndef inp_oup_dim_same(task_json):\n    return all([ len(pair[\"input\"]) == len(pair[\"output\"]) and len(pair[\"input\"][0]) == len(pair[\"output\"][0])\n                for pair in task_json['train']])\n    \n\nsolved_task = 0\ntotal_task = 0\ntask_ids = []\ntask_preds = []\nfor task_path in test_path.glob(\"*.json\"):\n    task_json = json.load(open(task_path))\n    tk_id = str(task_path).split(\"/\")[-1].split(\".\")[0]\n    print(tk_id)\n    print(len(task_json['train']))\n    continue\n    if(inp_oup_dim_same(task_json)):\n        a_size = get_a_size(task_json)\n        bl_cols = get_bl_cols(task_json)\n        \n        isflip = False\n        X1,Y1 = gettaskxy(task_json,True,1,bl_cols,isflip)\n        X3,Y3 = gettaskxy(task_json,True,3,bl_cols,isflip)\n        X5,Y5 = gettaskxy(task_json,True,5,bl_cols,isflip)\n        \n        model_1 = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100).fit(X1, Y1)\n        model_3 = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100).fit(X3, Y3)\n        model_5 = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100).fit(X5, Y5)\n        \n        pred_map_1 = submit_predict(task_json,model_1,1)\n        pred_map_3 = submit_predict(task_json,model_3,3)\n        pred_map_5 = submit_predict(task_json,model_5,5)\n        \n        for tks,str_pred in combine_preds(tk_id,pred_map_1,pred_map_3,pred_map_5):\n            task_ids.append(tks)\n            task_preds.append(str_pred)\n            #print(tks,str_pred)\n        solved_task+=1\n        #break\n    else:\n        pred_map_1 = dumb_predict(task_json)\n        pred_map_3 = dumb_predict(task_json)\n        pred_map_5 = dumb_predict(task_json)\n        \n        for tks,str_pred in combine_preds(tk_id,pred_map_1,pred_map_3,pred_map_5):\n            task_ids.append(tks)\n            task_preds.append(str_pred)\n            #print(tks,str_pred)\n        \n    total_task+=1\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" \ndef get_shapes(inp, out, bl_cols):    \n    in_shapes = {}\n    out_shapes = {}\n    r,c = len(inp),len(inp[0])\n    for i in range(r):\n        for j in range(c):\n            if inp[i][j] not in bl_cols:\n                if inp[i][j] in in_shapes:\n                    in_shapes[inp[i][j]].append([i,j])\n                else:\n                    in_shapes.update({inp[i][j]:[[i,j]]})\n    r,c = len(out),len(out[0])\n    for i in range(r):\n        for j in range(c):\n            if out[i][j] not in bl_cols:\n                if out[i][j] in out_shapes:\n                    out_shapes[out[i][j]].append([i,j])\n                else:\n                    out_shapes.update({out[i][j]:[[i,j]]})\n    return in_shapes,out_shapes\n                            \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_shapes_comb(inp, out, bl_cols):\n    in_pixels = [] # colour code, x, y\n    r,c = len(inp),len(inp[0])\n    for i in range(r):\n        for j in range(c):\n            if inp[i][j] != bl_cols[0]:\n                in_pixels.append([inp[i][j],i,j])\n                \n    out_pixels = [] # colour code, x, y\n    r,c = len(out),len(out[0])\n    for i in range(r):\n        for j in range(c):\n            if out[i][j] != bl_cols[0]:\n                out_pixels.append([out[i][j],i,j])\n                \n    in_shape_comb = []\n    print(len(in_pixels))\n    for comb_count in reversed(range(len(in_pixels) - 2, len(in_pixels))):\n        for comb in combinations(in_pixels, comb_count):\n            in_shape_comb.append(comb)\n            \n    out_shape_comb = []\n#    for comb_count in reversed(range(1, len(out_pixels))):\n#        for comb in combinations(out_pixels, comb_count):\n#            out_shape_comb.append(comb)\n    return in_shape_comb,out_shape_comb\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = np.array([ [0, 1, 2, 2, 0, 2, 2, 2],\n               [0, 0, 1, 2, 2, 0, 3, 3],\n               [0, 0, 2, 0, 0, 0, 0, 0],\n               [0, 1, 2, 1, 2, 1, 2, 1]])\n#a = a.tolist()\ns = np.where(a == 2, a, 0).nonzero()\nans = np.unique(a)\nshapes = {}\ns = np.transpose(s).tolist()\nprint(s)\ncount = 0\nfor x,y in s:\n    if not shapes: # if blank first entry\n        shapes.update({count:[[x,y]]})\n    else:\n        near_found = False\n        for key in shapes:\n            shape_pix = shapes[key]\n            if [x - 1, y] in shape_pix or [x, y - 1] in shape_pix or [x - 1, y - 1] in shape_pix:\n                shapes[key].append([x,y])\n                near_found = True\n                break\n        if not near_found:\n            count += 1\n            shapes.update({count:[[x,y]]})\n\nprint(shapes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find same colour shape\ndef find_match_shapes(inp, bl_cols):\n    n_inp = np.array(inp)\n    uni = np.unique(n_inp).tolist()\n    col_shapes = {}\n    uni.remove(bl_cols[0])\n    for col in uni:\n        pos = np.where(n_inp == col, inp, bl_cols[0]).nonzero()\n        pos = np.transpose(pos).tolist()\n        shapes = {}\n        count = 0\n        for x,y in pos:\n            if not shapes: # if blank first entry\n                shapes.update({count:[[x,y]]})\n            else:\n                near_found = False\n                for key in shapes:\n                    shape_pix = shapes[key]\n                    if [x - 1, y] in shape_pix or [x, y - 1] in shape_pix or [x - 1, y - 1] in shape_pix:\n                        shapes[key].append([x,y])\n                        near_found = True\n                        break\n                if not near_found:\n                    count += 1\n                    print(\"new\",count, [x,y])\n                    shapes.update({count:[[x,y]]})   \n        col_shapes.update({col:shapes})\n                    \n    return col_shapes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_predict(inp,model,size):\n    oup = predict(inp,model,size)\n    return inp,oup","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inp_oup_dim_same(task_json):\n    return all([ len(pair[\"input\"]) == len(pair[\"output\"]) and len(pair[\"input\"][0]) == len(pair[\"output\"][0])\n                for pair in task_json['train']])\n\nsolved_task = 0\ntotal_task = 0\nindex = 1\nfor task_path in train_path.glob(\"*.json\"):\n    task_json = json.load(open(task_path))\n    print(index,task_path)\n    index+=1\n    if(inp_oup_dim_same(task_json)):\n        a_size = get_a_size(task_json)\n        print(a_size)\n        bl_cols = get_bl_cols(task_json)\n        print(bl_cols)\n        \"\"\"\n        # Find the max row and column in the training set\n        max_row = 0\n        max_col = 0\n        for pair in task_json['train']:\n            max_row = max(len(pair[\"input\"]), max_row)\n            max_row = max(len(pair[\"output\"]), max_row)\n            max_col = max(len(pair[\"input\"][0]), max_col)\n            max_col = max(len(pair[\"output\"][0]), max_col)\n        for pair in task_json['test']:\n            max_row = max(len(pair[\"input\"]), max_row)\n            max_row = max(len(pair[\"output\"]), max_row)\n            max_col = max(len(pair[\"input\"][0]), max_col)\n            max_col = max(len(pair[\"output\"][0]), max_col)\n            \n        max_row = max(max_row, max_col)\n        max_col = max_row\n                \n        # Add padding to the left and bottom to equalize size of training set\n        for pair in task_json['train']:\n            while len(pair[\"input\"]) < max_row:\n                pair[\"input\"].append([0] * len(pair[\"input\"][0]))\n            while len(pair[\"input\"][0]) < max_col:\n                pair[\"input\"] = [x + [0] for x in pair[\"input\"]]\n            while len(pair[\"output\"]) < max_row:\n                pair[\"output\"].append([0] * len(pair[\"output\"][0]))\n            while len(pair[\"output\"][0]) < max_col:\n                pair[\"output\"] = [x + [0] for x in pair[\"output\"]]\n            \n        # Add padding to the left and bottom to equalize size of training set\n        for pair in task_json['test']:\n            while len(pair[\"input\"]) < max_row:\n                pair[\"input\"].append([0] * len(pair[\"input\"][0]))\n            while len(pair[\"input\"][0]) < max_col:\n                pair[\"input\"] = [x + [0] for x in pair[\"input\"]]\n            while len(pair[\"output\"]) < max_row:\n                pair[\"output\"].append([0] * len(pair[\"output\"][0]))\n            while len(pair[\"output\"][0]) < max_col:\n                pair[\"output\"] = [x + [0] for x in pair[\"output\"]]\n        \"\"\"\n        print(\"Training Data\")\n        print(len(task_json['train']))\n        for pair in task_json['train']:\n            inp,oup = pair[\"input\"],pair[\"output\"]\n#            print(np.array(pair[\"input\"]).shape,np.array(pair[\"output\"]).shape)\n#            plot_training(np.array(pair[\"input\"]),np.array(pair[\"output\"]))\n            in_shapes = find_match_shapes(inp,bl_cols)\n            plot_mat(np.array(pair[\"input\"]))\n            print(\"in_shapes\",in_shapes)\n#            print(\"out_shapes\",out_shapes)\n            count = 0\n            for col in in_shapes:\n                col_shapes = in_shapes[col]\n                for count in col_shapes:\n                    np_inp = np.full((len(inp),len(inp[0])), bl_cols[0])\n                    count_shapes = col_shapes[count]\n                    for pos in count_shapes:\n                        np_inp[pos[0],pos[1]] = col\n                    plot_mat(np_inp, str(count))\n                    count += 1\n                            \n        isflip = False\n#        X1,Y1 = gettaskxy(task_json,flip=True)\n#        X1,Y1 = gettaskxy(task_json,True,5,bl_cols,isflip)\n#        X3,Y3 = gettaskxy(task_json,True,5,bl_cols,isflip)\n#        X5,Y5 = gettaskxy(task_json,True,5,bl_cols,isflip)\n#        print(\"X1\",np.array(X1), np.array(X1).shape)\n#        print(\"\\n\")\n#        print(\"Y1\",np.array(Y1), np.array(Y1).shape)\n        \n#        model_1 = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100).fit(X1, Y1)\n#        model_3 = BaggingClassifier(base_estimator=AdaBoostClassifier(),n_estimators=100).fit(X3, Y3)\n#        model_5 = BaggingClassifier(base_estimator=ExtraTreesClassifier(),n_estimators=100).fit(X5, Y5)\n#        model_5 = BaggingClassifier(base_estimator=LinearSVC(max_iter=5000),n_estimators=100).fit(X5, Y5)\n        \n#        for pair in task_json['train']:\n#            inp,oup = train_predict(pair[\"input\"],model_5,5)\n#            plot_result(inp,np.array(pair[\"output\"]),oup)\n            \n#        if(get_test_loss(model_1,task_json,1) == 0):\n        if False:\n            print(\"Error Loss 1\", get_test_loss(model_1,task_json,5))\n            inp,eoup,oup = test_predict(task_json,model_1,5)\n            plot_result(inp,eoup,oup)\n            solved_task+=1\n#        elif(get_test_loss(model_3,task_json,3) == 0):\n        if False:\n            print(\"Error Loss 3\", get_test_loss(model_3,task_json,5))\n            inp,eoup,oup = test_predict(task_json,model_3,5)\n            plot_result(inp,eoup,oup)\n            solved_task+=1\n#        elif(get_test_loss(model_5,task_json,5) == 0):\n        if False:\n            print(\"Error Loss 5\", get_test_loss(model_5,task_json,5))\n            inp,eoup,oup = test_predict(task_json,model_5,5)\n            plot_result(inp,eoup,oup)\n            solved_task+=1\n#        else:\n        if False:\n            print(\"Error Loss \", get_test_loss(model_5,task_json,5))\n            inp,eoup,oup = test_predict(task_json,model_5,5)\n            plot_result(inp,eoup,oup)\n        total_task+=1\n        if total_task > 0:\n            break\nprint(solved_task,total_task)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sub_df = pd.DataFrame({\"output_id\":task_ids,'output':task_preds})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#sub_df.to_csv(\"submission.csv\", index=None)\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}