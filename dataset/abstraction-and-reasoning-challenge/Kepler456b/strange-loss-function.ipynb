{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is an attempt to combine NN with search. In essence, NN will try to extract structures from the input. In the example we will explore, the structures are the anchors and objects. The object is the object to be copied. The anchor is the second object that acts like a reference point for the copying process. There are two NNs to start, both CNNs, that will output 2 points representing the box in which the anchor or object can be placed. Next, a second level NN will take the anchor and object predicted from previous input and output two parameters, a direction (from 8 different hot coded directions) and spacing. Next, the object will be copied in the given direction and spacing. The image will then be compared with the correct target and the error will be propagated backwards. During training, the identification of anchor and objects can be performed through hardcoded search algorithms.\n\nThere is also a helper function that can look for repeating patterns in the images. This one can be combined with another function that looks for any changes in the pattern. If all pattern objects are going through the same change, that must be an object of attention."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Loading"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import json\nimport os\nfrom os.path import join as path_join\n\n\ndef load_data(path):\n    tasks = pd.Series()\n    for file_path in os.listdir(path):\n        task_file = path_join(path, file_path)\n\n        with open(task_file, 'r') as f:\n            task = json.load(f)\n\n        tasks[file_path[:-5]] = task\n    return tasks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tasks = load_data('../input/abstraction-and-reasoning-challenge/training/')\nevaluation_tasks = load_data('../input/abstraction-and-reasoning-challenge/evaluation/')\ntest_tasks = load_data('../input/abstraction-and-reasoning-challenge/test/')\n\ntrain_tasks.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn.parallel\nimport torch.optim as optim\nimport torch.utils.data\nimport torch.nn as nn\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import Adam\nfrom torch.nn import Conv2d\nfrom torch import FloatTensor, LongTensor\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import Adam\n\nclass CAModel(nn.Module):\n    def __init__(self, num_states):\n        super(CAModel, self).__init__()      \n        self.simple_conv = nn.Sequential(\n            nn.Conv2d(num_states, 32, kernel_size=3, padding=1),\n            nn.ReLU()\n        )\n        self.conv1 = nn.Conv2d(32, num_states, kernel_size=1)\n        self.conv2 = nn.Conv2d(32, num_states+1, kernel_size=1)\n        self.conv3 = nn.Conv2d(32, 1, kernel_size=1)\n        self.conv4 = nn.Conv2d(32, 1, kernel_size=1)\n        self.conv5 = nn.Conv2d(32, 1, kernel_size=1)\n        self.soft = nn.Softmax(dim=1)\n        \n    def forward(self, x, steps = 1):\n        for step in range(steps):\n            x = self.simple_conv(x)\n            y1 = self.conv1(x) #actual output\n            y2 = self.conv2(x) #difference output\n            y3 = self.conv3(x) #object box upper left corner\n            y3 = torch.flatten(y3, start_dim=1)\n            y3 = self.soft(y3)\n            y4 = self.conv4(x) #object box lower right corner\n            y4 = torch.flatten(y4, start_dim=1)\n            y4 = self.soft(y4)\n            y5 = self.conv5(x) #anchor point\n            y5 = torch.flatten(y5, start_dim=1)\n            y5 = self.soft(y5)\n            x = self.soft(y1)\n            \n        return y1, y2, y3, y4, y5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib import colors\n\n\ncmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25', '#530C25'])\nnorm = colors.Normalize(vmin=0, vmax=10)\n    \ndef plot_pictures(pictures, labels):\n    fig, axs = plt.subplots(1, len(pictures), figsize=(2*len(pictures),32))\n    for i, (pict, label) in enumerate(zip(pictures, labels)):\n        axs[i].imshow(np.array(pict), cmap=cmap, norm=norm)\n        axs[i].set_title(label)\n    plt.show()\n    \n\ndef plot_sample(sample, predict=None):\n    if predict is None:\n        plot_pictures([sample['input'], sample['output']], ['Input', 'Output'])\n    else:\n        plot_pictures([sample['input'], sample['output'], predict], ['Input', 'Output', 'Predict'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2 as cv\nimport numpy as np\nfrom matplotlib import pyplot as plt\n        \ndef get_matching_shapes_helper(img, template):\n    matches = []\n    for i in range(img.shape[0]-3):\n        for j in range(img.shape[1]-3):\n            match = True\n            for ti in range(3):\n                for tj in range(3):\n                    if img[i+ti,j+tj] != template[ti,tj]:\n                        match = False\n            if match:\n                matches.append((i,j))                        \n    print(matches)\n       \ndef get_matching_shapes(inp):\n    inp = np.array(inp)\n    print(inp.shape)\n    for j in range(inp.shape[1]-3):\n        for i in range(inp.shape[0]-3):\n            template = np.zeros((3,3))\n            template = inp[i:i+3,j:j+3]\n            if np.sum(template) != 0:\n                plot_pictures([inp, template],['inp', 'template'])\n                get_matching_shapes_helper(inp, template)\n                    \ndef generate_copies_to_right(inp_img, object_coords, anchor_coords, spacing=1):\n    inp_img = np.array(inp_img)\n    \n    img_xlen = inp_img.shape[0]\n    img_ylen = inp_img.shape[1]\n    \n    object_left_x = object_coords[0]%img_ylen\n    object_up_y = object_coords[0]//img_ylen\n    object_rite_x = object_coords[1]%img_ylen\n    object_low_y = object_coords[1]//img_ylen\n    object_width =  object_rite_x - object_left_x\n    object_height = object_up_y - object_low_y\n    anchor_x = anchor_coords%img_ylen\n    anchor_y = anchor_coords//img_ylen\n    \n    object_img = inp_img[object_left_x:object_rite_x,object_low_y:object_up_y]\n    \n    if object_width <= 0 or object_height <= 0:\n        return inp_img\n    elif anchor_x+object_width > img_xlen or anchor_y+object_height > img_ylen:\n        return inp_img\n    elif object_left_x < 0 or object_rite_x > img_xlen or object_low_y < 0 or object_up_y > img_ylen:\n        return inp_img\n    \n    #copying object 1 time to the upper right of the anchor:\n    \n    for i in range(img_xlen):\n        for j in range(img_ylen):\n            if i == anchor_x and j == anchor_y:\n                for ti in range(object_width):\n                    for tj in range(object_height):\n                        copy_value = object_img[ti,tj]\n                        inp_img[i+ti,j+tj]= copy_value\n\n    return inp_img\n\ndef my_weird_loss(trg_img, copy_img):\n    trg_img = np.array(trg_img)\n    loss = 0\n    for i in range(trg_img.shape[0]):\n        for j in range(trg_img.shape[1]):\n            if trg_img[i,j] != copy_img[i,j]:\n                loss += -1\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_pic = train_tasks[155]['train'][1][\"input\"]\nsample_out = train_tasks[155]['train'][1][\"output\"]\nplot_pictures([sample_pic, sample_out], ['sample_input','sample_output'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_matching_shapes(sample_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inp2img(inp):\n    inp = np.array(inp)\n    img = np.full((10, inp.shape[0], inp.shape[1]), 0, dtype=np.uint8)\n    for i in range(10):\n        img[i] = (inp==i)\n    return img\n\ndef target_maker(samin, samout):\n    target = np.full((len(samin), len(samin[0])), 0, dtype=np.uint8)\n    for i in range(len(samin)):\n        for j in range(len(samin[0])):\n            if samin[i][j]==samout[i][j]:\n                target[i,j] = 10  #index 10 is for retained info. see pred_maker.\n            else:\n                target[i,j] = samout[i][j]\n    return target\n\ndef pred_maker(inp, pred, pred_grade):\n    inp = np.asarray(inp)\n    final_pred = np.full((inp.shape[0], inp.shape[1]), 0, dtype=np.uint8)\n    grade_avg = np.mean(pred_grade)\n    threshold = 0.66\n    for i in range(pred.shape[0]):\n        for j in range(pred.shape[1]):\n            if pred[i,j] == 10 or pred_grade[i, j]/grade_avg < threshold: #index=10\n                final_pred[i,j] = inp[i,j]\n            else:\n                final_pred[i,j] = pred[i,j]\n    return final_pred\n    \nclass TaskSolver:        \n    def train(self, task_train, n_epoch=200, max_steps=2):\n        \"\"\"basic pytorch train loop\"\"\"\n        self.net = CAModel(10)\n        criterion = CrossEntropyLoss()\n        optimizer = Adam(self.net.parameters(), lr = 0.01) #(0.1 / (num_steps * 2)))\n            \n        for epoch in range(n_epoch):\n            optimizer.zero_grad()\n            for sample in task_train:\n                #Our target is label - input. This makes it easier.\n                target = target_maker(sample['input'],sample['output'])\n                inputs = FloatTensor(inp2img(sample['input'])).unsqueeze(dim=0)\n                labels1 = LongTensor(sample['output']).unsqueeze(dim=0)\n                labels2 = LongTensor(target).unsqueeze(dim=0)\n                outputs1 = self.net(inputs)[0]\n                outputs2 = self.net(inputs)[1]\n                loss = criterion(outputs1, labels1)\n                loss.backward()\n                loss = criterion(outputs2, labels2)\n                loss.backward()\n                \n                #object multiplying routine:\n                object_upl = self.net(inputs)[2]\n                object_upleft = object_upl.squeeze(dim=0).cpu().detach().numpy().argmax(0)\n                object_lowr = self.net(inputs)[3]\n                object_lowrite = object_lowr.squeeze(dim=0).cpu().detach().numpy().argmax(0)\n                anchor_point = self.net(inputs)[4]\n                anchor_point = anchor_point.squeeze(dim=0).cpu().detach().numpy().argmax(0)\n                object_coords = (object_upleft, object_lowrite)\n                \n                copied_object_img = generate_copies_to_right(sample['input'], object_coords, anchor_point)\n                \n                #shilly loss that deserves to be punished\n                loss = my_weird_loss(sample['output'],copied_object_img)*torch.mean(object_upl)\n                loss.backward()\n                \n                optimizer.step()\n\n        #output_img = outputs.squeeze(dim=0).detach().cpu().numpy().argmax(0)\n        plot_pictures([sample['input'], sample['output'], copied_object_img ], ['Input', 'Label', 'Copies'])\n            \n        #output_img = outputs.detach().numpy()\n        #output_img = np.squeeze(output_img)\n        #output_img = np.argmax(output_img, axis=0)\n        #plot_pictures([sample['input'],target,output_img],['input','target','predict'])\n            \n        return self\n            \n    def predict(self, task_test):\n        predictions = []\n        pred_grades = []\n        with torch.no_grad():\n            for sample in task_test:\n                inputs = FloatTensor(inp2img(sample['input'])).unsqueeze(dim=0)\n                outputs = self.net(inputs)[0]\n                #outputs = self.net(inputs)[1]\n                pred =  outputs.squeeze(dim=0).cpu().numpy().argmax(0)\n                \n                #pred_grade = np.full((pred.shape[0], pred.shape[1]), 0, dtype=np.float64)\n                #for i in range(pred.shape[0]):\n                #    for j in range(pred.shape[1]):\n                #        k = pred[i][j]\n                #        pred_grade[i][j] = (torch.nn.Softmax(dim=1)(outputs)).squeeze(dim=0).cpu().numpy()[k][i][j]\n                \n                #final_pred = pred_maker(sample['input'],pred, pred_grade)\n                final_pred = pred\n                predictions.append(final_pred)\n                \n                #predictions.append(pred)\n                \n        return predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"def input_output_shape_is_same(task):\n    return all([np.array(el['input']).shape == np.array(el['output']).shape for el in task['train']])\n\ndef calk_score(task_test, predict):\n    return [int(np.equal(sample['output'], pred).all()) for sample, pred in zip(task_test, predict)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(tasks):\n    ts = TaskSolver()\n    result = []\n    predictions = []\n    for task in tqdm(tasks):\n        if input_output_shape_is_same(task):\n            ts.train(task['train'])\n            pred = ts.predict(task['test'])\n            score = calk_score(task['test'], pred)\n        else:\n            pred = [el['input'] for el in task['test']]\n            score = [0]*len(task['test'])\n        \n        predictions.append(pred)\n        result.append(score)\n       \n    return result, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_result, train_predictions = evaluate(train_tasks)\ntrain_solved = [any(score) for score in train_result]\n\ntotal = sum([len(score) for score in train_result])\nprint(f\"solved : {sum(train_solved)} from {total} ({sum(train_solved)/total})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation_result, evaluation_predictions = evaluate(evaluation_tasks)\nevaluation_solved = [any(score) for score in evaluation_result]\n\ntotal = sum([len(score) for score in evaluation_result])\nprint(f\"solved : {sum(evaluation_solved)} from {total} ({sum(evaluation_solved)/total})\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize\n\nvisualize solved tasks"},{"metadata":{},"cell_type":"markdown","source":"### train solved tasks"},{"metadata":{"trusted":true},"cell_type":"code","source":"for task, prediction, solved in tqdm(zip(train_tasks, train_predictions, train_solved)):\n    if solved:\n        for i in range(len(task['train'])):\n            plot_sample(task['train'][i])\n            \n        for i in range(len(task['test'])):\n            plot_sample(task['test'][i], prediction[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### evaluation solved tasks"},{"metadata":{"trusted":true},"cell_type":"code","source":"for task, prediction, solved in tqdm(zip(evaluation_tasks, evaluation_predictions, evaluation_solved)):\n    if solved:\n        for i in range(len(task['train'])):\n            plot_sample(task['train'][i])\n            \n        for i in range(len(task['test'])):\n            plot_sample(task['test'][i], prediction[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"def flattener(pred):\n    str_pred = str([row for row in pred])\n    str_pred = str_pred.replace(', ', '')\n    str_pred = str_pred.replace('[[', '|')\n    str_pred = str_pred.replace('][', '|')\n    str_pred = str_pred.replace(']]', '|')\n    return str_pred\n\ndef make_pediction(tasks):\n    ts = TaskSolver()\n    result = pd.Series()\n    for idx, task in tqdm(test_tasks.iteritems()):\n        if input_output_shape_is_same(task):\n            ts.train(task['train'])\n            pred = ts.predict(task['test'])\n        else:\n            pred = [el['input'] for el in task['test']]\n        \n        for i, p in enumerate(pred):\n            result[f'{idx}_{i}'] = flattener(np.array(p).tolist())\n       \n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = make_pediction(test_tasks)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = submission.reset_index()\nsubmission.columns = ['output_id', 'output']\nsubmission.to_csv('submission.csv', index=False)\nsubmission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# All train tasks predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"for task, prediction in tqdm(zip(train_tasks, train_predictions)):\n    if input_output_shape_is_same(task):\n        for i in range(len(task['test'])):\n            plot_sample(task['test'][i], prediction[i])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}