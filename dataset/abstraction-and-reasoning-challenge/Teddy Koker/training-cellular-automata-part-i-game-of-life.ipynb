{"cells":[{"metadata":{},"cell_type":"markdown","source":"[arseny-n](https://www.kaggle.com/arsenynerinovsky/cellular-automata-as-a-language-for-reasoning) showed us that Cellular Automata can be used as a language for solving ARC challenges, and provided us with a few example solutions. In this notebook we'll explore how we can use neural networks to create cellular automata. We'll start with Conway's Game of Life, and then move on to see if we can replicate his results using learned cellular automata. (See [part II](https://www.kaggle.com/teddykoker/training-cellular-automata-part-ii-learn-a-task))."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation, rc\nfrom IPython.display import HTML\n\nrc('animation', html='jshtml')\ndevice = torch.device(\"cuda:0\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction: Conway's Game of Life\n\nBefore we start training models on the ARC, lets see how we can use neural networks to learn a Cellular Automata that we already know: Conway's Game of Life (GOL). We'll start out by creating our input $x$, 10 different random 32x32 grids, then performing one transition step to get our output, $y$. I chose to keep the train set relitively small to reflect the nature of the ARC challenge."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size, width, height = (10, 32, 32)\n\ndef gol(x):\n    # performs one step of Conway's Game of Life\n    # http://jakevdp.github.io/blog/2013/08/07/conways-game-of-life/\n    nbrs_count = sum(np.roll(np.roll(x, i, 0), j, 1)\n                     for i in (-1, 0, 1) for j in (-1, 0, 1)\n                     if (i != 0 or j != 0))\n    return (nbrs_count == 3) | (x & (nbrs_count == 2))\n\n\nx_train = np.random.randint(0, 2, size=(train_size, width, height))\ny_train = np.stack([gol(d) for d in x_train])\nx_train = np.stack([1 - x_train, x_train], axis=1) # one channel per color\n\nx_train = torch.from_numpy(x_train).float().to(device)\ny_train = torch.from_numpy(y_train).to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model\n\nWe'll use a very simple model to recreate the rules of GOL. The first layer pads the edges of the grid by sampling the values from the opposite side. The rest of the layers consists of first a 3x3 convolutional layer, which samples the \"neighborhood\" around each cell, followed by 2 1x1 convolutional layers, which effectively act as fully connected layers for each neighborhood sample. The output of the model is the same as the input, a channel of values representing dead cells, and a channel of values representing alive cells, this allows the output to be repeatedly fed back into the input, peforming one transition each time. This model is inspired in part by [(Gilpin, 2018)](https://arxiv.org/abs/1809.02942)."},{"metadata":{"trusted":true},"cell_type":"code","source":"class CircularPad(nn.Module):\n    def forward(self, x):\n        return F.pad(x, (1, 1, 1, 1), mode=\"circular\")\n    \ngol_model = nn.Sequential(\n    CircularPad(),\n    nn.Conv2d(2, 8, kernel_size=3),\n    nn.ReLU(),\n    nn.Conv2d(8, 8, kernel_size=1),\n    nn.ReLU(),\n    nn.Conv2d(8, 2, kernel_size=1)\n).to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training\n\nWe'll train the model using Cross Entropy Loss for 1000 epochs. We can see that the model converges nicely."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 1000\n\noptimizer = torch.optim.Adam(gol_model.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()\n\nlosses = np.zeros(num_epochs)\nfor e in range(num_epochs):\n    optimizer.zero_grad()\n    y_pred = gol_model(x_train)\n    loss = criterion(y_pred, y_train)\n    losses[e] = loss.item()\n    loss.backward()\n    optimizer.step()\n\nplt.plot(losses)\nprint(f\"Last loss: {losses[-1]:.5f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test: Glider\n\nAn easy way to test our model is to repeatedly pass through a <a href=\"https://en.wikipedia.org/wiki/Glider_(Conway%27s_Life)\">glider</a>. If our model has learned correctly, we should see the glider travel across the grid."},{"metadata":{"trusted":true},"cell_type":"code","source":"glider = np.array([[0,1,0],\n                   [0,0,1],\n                   [1,1,1]])\nstate = np.zeros((10, 10))\nstate[:3,:3] = glider\nstate = np.stack([1 - state, state])\nstate = torch.from_numpy(state).float().unsqueeze(0).to(device)\n\n@torch.no_grad()\ndef animate(i):\n    global state\n    state = torch.softmax(gol_model(state), dim=1)\n    mat.set_data(state.cpu().numpy()[0,0])\n\nfig, ax = plt.subplots()\nmat = ax.matshow(state.cpu().numpy()[0,0], cmap=\"gray\")\nanim = animation.FuncAnimation(fig, animate, frames=100, interval=60)\nHTML(anim.to_jshtml())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like it works! Now we see if we can apply the same principles to solving some of the ARC challenges. ~~Stay tuned for Part II!~~ [Part II](https://www.kaggle.com/teddykoker/training-cellular-automata-part-ii-learn-a-task)."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}