{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The idea:\n\nStudy a context free algorithm that knows nothing about shapes, numbers, lines or planar geometry. \nHowever it assumes that:\n\n1, All the input pictures have the same size (n,k)\n\n2, All the output pictures have the same size (a,b)\n\n3, The color of Output_Picture at pixel (p,q) is given by the color of Input_Picture at pixel (i,j).\n\nThe goal is to find the mapping from (p,q) to the correct (i,j). The algorithm is quite simple:\n\n1, At the training time for given (p,q) it collects all the pairs (i,j) that are possible candidates. \n\n2, At the prediction time it uses majority rule. (computes the colors of Test_Picture at the candidate places).\n"},{"metadata":{},"cell_type":"markdown","source":"# Scores:\n\nTraining: 11/400\n\nEvaluation: 17/400\n\nTest: 0\n\nIt seems there is no free lunch. But still, perhaps somebody can use this as a building block.\n\n\n\n"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nimport json\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport numpy as np\nfrom pathlib import Path\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    print(dirname)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Getting the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = Path('/kaggle/input/abstraction-and-reasoning-challenge/')\ntraining_path = data_path / 'training'\nevaluation_path = data_path / 'evaluation'\ntest_path = data_path / 'test'\ntraining_tasks = sorted(os.listdir(training_path))\neval_tasks = sorted(os.listdir(evaluation_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"T = training_tasks\nTrains = []\nfor i in range(400):\n    task_file = str(training_path / T[i])\n    task = json.load(open(task_file, 'r'))\n    Trains.append(task)\n    \nE = eval_tasks\nEvals= []\nfor i in range(400):\n    task_file = str(evaluation_path / E[i])\n    task = json.load(open(task_file, 'r'))\n    Evals.append(task)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"cmap = colors.ListedColormap(\n    ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n     '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\nnorm = colors.Normalize(vmin=0, vmax=9)\n# 0:black, 1:blue, 2:red, 3:greed, 4:yellow,\n# 5:gray, 6:magenta, 7:orange, 8:sky, 9:brown\nplt.figure(figsize=(5, 2), dpi=200)\nplt.imshow([list(range(10))], cmap=cmap, norm=norm)\nplt.xticks(list(range(10)))\nplt.yticks([])\nplt.show()\n\ndef plot_task(task):\n    n = len(task[\"train\"]) + len(task[\"test\"])\n    fig, axs = plt.subplots(2, n, figsize=(4*n,8), dpi=50)\n    plt.subplots_adjust(wspace=0, hspace=0)\n    fig_num = 0\n    for i, t in enumerate(task[\"train\"]):\n        t_in, t_out = np.array(t[\"input\"]), np.array(t[\"output\"])\n        axs[0][fig_num].imshow(t_in, cmap=cmap, norm=norm)\n        axs[0][fig_num].set_title(f'Train-{i} in')\n        axs[0][fig_num].set_yticks(list(range(t_in.shape[0])))\n        axs[0][fig_num].set_xticks(list(range(t_in.shape[1])))\n        axs[1][fig_num].imshow(t_out, cmap=cmap, norm=norm)\n        axs[1][fig_num].set_title(f'Train-{i} out')\n        axs[1][fig_num].set_yticks(list(range(t_out.shape[0])))\n        axs[1][fig_num].set_xticks(list(range(t_out.shape[1])))\n        fig_num += 1\n    for i, t in enumerate(task[\"test\"]):\n        t_in, t_out = np.array(t[\"input\"]), np.array(t[\"output\"])\n        axs[0][fig_num].imshow(t_in, cmap=cmap, norm=norm)\n        axs[0][fig_num].set_title(f'Test-{i} in')\n        axs[0][fig_num].set_yticks(list(range(t_in.shape[0])))\n        axs[0][fig_num].set_xticks(list(range(t_in.shape[1])))\n        axs[1][fig_num].imshow(t_out, cmap=cmap, norm=norm)\n        axs[1][fig_num].set_title(f'Test-{i} out')\n        axs[1][fig_num].set_yticks(list(range(t_out.shape[0])))\n        axs[1][fig_num].set_xticks(list(range(t_out.shape[1])))\n        fig_num += 1\n    \n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_picture(x):\n    plt.imshow(np.array(x), cmap = cmap, norm = norm)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Defensive_Copy(A): \n    n = len(A)\n    k = len(A[0])\n    L = np.zeros((n,k), dtype = int)\n    for i in range(n):\n        for j in range(k):\n            L[i,j] = 0 + A[i][j]\n    return L.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Create(task, task_id = 0):\n    n = len(task['train'])\n    Input = [Defensive_Copy(task['train'][i]['input']) for i in range(n)]\n    Output = [Defensive_Copy(task['train'][i]['output']) for i in range(n)]\n    Input.append(Defensive_Copy(task['test'][task_id]['input']))\n    return Input, Output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The algorithm:\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"def Recolor(task):\n    Input = task[0]\n    Output = task[1]\n    Test_Picture = Input[-1]\n    Input = Input[:-1]\n    N = len(Input)\n\n    x0 = Input[0]\n    y0 = Output[0]\n    n = len(x0)\n    k = len(x0[0])\n    a = len(y0)\n    b = len(y0[0])\n    for x in Input+[Test_Picture]:\n        if len(x) != n or len(x[0]) != k:\n            return -1\n    for y in Output:\n        if len(y) != a or len(y[0]) != b:\n            return -1\n    List1 = {}\n    List2 = {}\n    \n    for i in range(n):\n        for j in range(k):\n            seq = []\n            for x in Input:\n                seq.append(x[i][j])\n            List1[(i,j)] = seq\n            \n    for p in range(a):\n        for q in range(b):\n            seq1 = []\n            for y in Output:\n                seq1.append(y[p][q])\n           \n            places = []\n            for key in List1:\n                if List1[key] == seq1:\n                    places.append(key) \n                    \n            List2[(p,q)] = places\n            if len(places) == 0:\n                return -1\n                \n    answer = np.zeros((a,b), dtype = int)\n   \n    for p in range(a):\n        for q in range(b):\n            palette = [0,0,0,0,0,0,0,0,0,0]\n            for i, j in List2[(p,q)]:\n                color = Test_Picture[i][j]\n                palette[color]+=1\n            answer[p,q] =  np.argmax(palette)\n            \n    return answer.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results on Training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_examples = []\nfor i in range(400):\n    task = Trains[i]\n    basic_task = Create(task,0)\n    a = Recolor(basic_task)\n  \n    if a != -1 and task['test'][0]['output'] == a:\n        plot_picture(a)\n        plot_task(task)\n        print(i)\n        training_examples.append(i)      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(training_examples))\nprint(training_examples)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results on the evaluation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation_examples = []\n\n\nfor i in range(400):\n    task = Evals[i]\n    basic_task = Create(task,0)\n    a = Recolor(basic_task)\n    \n    if a != -1 and task['test'][0]['output'] == a:\n        plot_picture(a)\n        plot_task(task)\n        evaluation_examples.append(i)        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(evaluation_examples))\nprint(evaluation_examples)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As remarked elsewhere there are some subtle differences between the Evaluation tasks and the Testing tasks and\nthis approach doesn't give a hit on the leaderboard.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(data_path/ 'sample_submission.csv')\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def flattener(pred):\n    str_pred = str([row for row in pred])\n    str_pred = str_pred.replace(', ', '')\n    str_pred = str_pred.replace('[[', '|')\n    str_pred = str_pred.replace('][', '|')\n    str_pred = str_pred.replace(']]', '|')\n    return str_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_grid = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ndisplay(example_grid)\nprint(flattener(example_grid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Solved = []\nProblems = submission['output_id'].values\nProposed_Answers = []\nfor i in  range(len(Problems)):\n    output_id = Problems[i]\n    task_id = output_id.split('_')[0]\n    pair_id = int(output_id.split('_')[1])\n    f = str(test_path / str(task_id + '.json'))\n   \n    with open(f, 'r') as read_file:\n        task = json.load(read_file)\n    \n    n = len(task['train'])\n    Input = [Defensive_Copy(task['train'][j]['input']) for j in range(n)]\n    Output = [Defensive_Copy(task['train'][j]['output']) for j in range(n)]\n    Input.append(Defensive_Copy(task['test'][pair_id]['input']))\n    \n    solution = Recolor([Input, Output])\n   \n    \n    pred = ''\n        \n    if solution != -1:\n        Solved.append(i)\n        pred1 = flattener(solution)\n        pred = pred+pred1+' '\n        \n    if pred == '':\n        pred = flattener(example_grid)\n        \n    Proposed_Answers.append(pred)\n    \nsubmission['output'] = Proposed_Answers\nsubmission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Solved)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission2 = pd.read_csv('submission.csv')\nsubmission2.tail(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission2.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}