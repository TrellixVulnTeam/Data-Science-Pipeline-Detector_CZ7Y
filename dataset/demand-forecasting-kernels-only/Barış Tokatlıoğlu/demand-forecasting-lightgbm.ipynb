{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport warnings\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-15T10:40:48.168299Z","iopub.execute_input":"2022-02-15T10:40:48.168704Z","iopub.status.idle":"2022-02-15T10:40:50.509836Z","shell.execute_reply.started":"2022-02-15T10:40:48.1686Z","shell.execute_reply":"2022-02-15T10:40:50.508782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###\n# Reading Data\n###\ntrain = pd.read_csv('../input/demand-forecasting-kernels-only/train.csv', parse_dates=['date'])\ntest = pd.read_csv('../input/demand-forecasting-kernels-only/test.csv', parse_dates=['date'])\nsample_sub = pd.read_csv('../input/demand-forecasting-kernels-only/sample_submission.csv')\ndf = pd.concat([train, test], sort=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:40:50.511675Z","iopub.execute_input":"2022-02-15T10:40:50.512409Z","iopub.status.idle":"2022-02-15T10:40:51.51755Z","shell.execute_reply.started":"2022-02-15T10:40:50.512372Z","shell.execute_reply":"2022-02-15T10:40:51.516477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How is the sales distribution?\ndf[\"sales\"].describe([0.10, 0.30, 0.50, 0.70, 0.80, 0.90, 0.95, 0.99])\n\n# How many stores are there?\ndf[[\"store\"]].nunique()\n\n# How many items are there?\ndf[[\"item\"]].nunique()\n\n# Are there an equal number of unique items in each store?\ndf.groupby([\"store\"])[\"item\"].nunique()\n\n# So, is there an equal number of sales in each store?\ndf.groupby([\"store\", \"item\"]).agg({\"sales\": [\"sum\"]})\n\n# sales statistics in store-item breakdown\ndf.groupby([\"store\", \"item\"]).agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\"]})","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:40:51.518955Z","iopub.execute_input":"2022-02-15T10:40:51.519239Z","iopub.status.idle":"2022-02-15T10:40:51.865962Z","shell.execute_reply.started":"2022-02-15T10:40:51.519194Z","shell.execute_reply":"2022-02-15T10:40:51.864798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_date_features(df):\n    df['month'] = df.date.dt.month\n    df['day_of_month'] = df.date.dt.day\n    df['day_of_year'] = df.date.dt.dayofyear\n    df['week_of_year'] = df.date.dt.weekofyear\n    df['day_of_week'] = df.date.dt.dayofweek\n    df['year'] = df.date.dt.year\n    df[\"is_wknd\"] = df.date.dt.weekday // 4\n    df['is_month_start'] = df.date.dt.is_month_start.astype(int)\n    df['is_month_end'] = df.date.dt.is_month_end.astype(int)\n    return df\n\ndf = create_date_features(df)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:40:51.868099Z","iopub.execute_input":"2022-02-15T10:40:51.868398Z","iopub.status.idle":"2022-02-15T10:40:53.038081Z","shell.execute_reply.started":"2022-02-15T10:40:51.868357Z","shell.execute_reply":"2022-02-15T10:40:53.037143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################\n# Random Noise\n########################\n\ndef random_noise(dataframe):\n    return np.random.normal(scale=1.6, size=(len(dataframe),))","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:40:53.039372Z","iopub.execute_input":"2022-02-15T10:40:53.039616Z","iopub.status.idle":"2022-02-15T10:40:53.045875Z","shell.execute_reply.started":"2022-02-15T10:40:53.039584Z","shell.execute_reply":"2022-02-15T10:40:53.044794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this point, we add random noise to the variables that we will produce in the sales focus in order to prevent over-learning or to prevent bias. This will add noise prevents over-learning.","metadata":{}},{"cell_type":"code","source":"########################\n# Lag/Shifted Features\n########################\ndf.sort_values(by=['store', 'item', 'date'], axis=0, inplace=True)\n\ndf.groupby([\"store\", \"item\"])['sales'].head()\n\ndf.groupby([\"store\", \"item\"])['sales'].transform(lambda x: x.shift(1))","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:40:53.047166Z","iopub.execute_input":"2022-02-15T10:40:53.047431Z","iopub.status.idle":"2022-02-15T10:40:53.977672Z","shell.execute_reply.started":"2022-02-15T10:40:53.047373Z","shell.execute_reply":"2022-02-15T10:40:53.97665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lag/Shifted means Latency. The time period yt was most affected, from yt-1 to yt-2 in order. Therefore, we assumed that the next day's sale of a store would be affected by the previous day's sale. We will need to derive features for this.","metadata":{}},{"cell_type":"code","source":"def lag_features(dataframe, lags):\n    for lag in lags:\n        dataframe['sales_lag_' + str(lag)] = dataframe.groupby([\"store\", \"item\"])['sales'].transform(\n            lambda x: x.shift(lag)) + random_noise(dataframe)\n    return dataframe\n\n\ndf = lag_features(df, [91, 98, 105, 112, 119, 126, 180, 364, 546, 728, 910, 1092])","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:40:53.979167Z","iopub.execute_input":"2022-02-15T10:40:53.97944Z","iopub.status.idle":"2022-02-15T10:40:56.800502Z","shell.execute_reply.started":"2022-02-15T10:40:53.979407Z","shell.execute_reply":"2022-02-15T10:40:56.799452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################\n# Rolling Mean Features\n########################\ndef roll_mean_features(dataframe, windows):\n    for window in windows:\n        dataframe['sales_roll_mean_' + str(window)] = dataframe.groupby([\"store\", \"item\"])['sales']. \\\n                                                          transform(\n            lambda x: x.shift(1).rolling(window=window, min_periods=10, win_type=\"triang\").mean()) + random_noise(\n            dataframe)\n    return dataframe\n\ndf = roll_mean_features(df, [180, 365, 546, 728, 910])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:40:56.801952Z","iopub.execute_input":"2022-02-15T10:40:56.802244Z","iopub.status.idle":"2022-02-15T10:41:03.655411Z","shell.execute_reply.started":"2022-02-15T10:40:56.80221Z","shell.execute_reply":"2022-02-15T10:41:03.654465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Rolling Mean means moving average. These moving averages carry historical information. We will create them to put Level and Trend components. window represents the number of delay. The entered value takes the delay with itself and calculates the average.\nAt this point, the own value of the observation unit should not be taken into account when calculating the moving average, but in this case, a feature independent of the current value can be produced that can express the trend from the past.","metadata":{}},{"cell_type":"code","source":"########################\n# Exponentially Weighted Mean Features\n########################\ndef ewm_features(dataframe, alphas, lags):\n    for alpha in alphas:\n        for lag in lags:\n            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n                dataframe.groupby([\"store\", \"item\"])['sales'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n    return dataframe\n\nalphas = [0.95, 0.9, 0.8, 0.7, 0.5]\nlags = [91, 98, 105, 112, 180, 270, 365, 546, 728, 910]\n\ndf = ewm_features(df, alphas, lags)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:41:03.656687Z","iopub.execute_input":"2022-02-15T10:41:03.656922Z","iopub.status.idle":"2022-02-15T10:41:21.367704Z","shell.execute_reply.started":"2022-02-15T10:41:03.656891Z","shell.execute_reply":"2022-02-15T10:41:21.36646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We know that the weighted average gives more weight to values close to the past. When the alpha is .99 it will give more weight to the nearest values, when it is 0.1 it will give more weight to the far values. In this example, we observe that as the weight of the past values increases, it approaches the normal mean value.","metadata":{}},{"cell_type":"code","source":"########################\n# One-Hot Encoding\n########################\n\ndf = pd.get_dummies(df, columns=['store', 'item', 'day_of_week', 'month'])","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:41:21.372317Z","iopub.execute_input":"2022-02-15T10:41:21.373056Z","iopub.status.idle":"2022-02-15T10:41:24.068145Z","shell.execute_reply.started":"2022-02-15T10:41:21.373015Z","shell.execute_reply":"2022-02-15T10:41:24.067198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################\n# Converting sales to log(1+sales)\n########################\n\ndf['sales'] = np.log1p(df[\"sales\"].values)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:41:24.069148Z","iopub.execute_input":"2022-02-15T10:41:24.069419Z","iopub.status.idle":"2022-02-15T10:41:24.092982Z","shell.execute_reply.started":"2022-02-15T10:41:24.069388Z","shell.execute_reply":"2022-02-15T10:41:24.092209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we will use lightgbm, one of the tree methods, we may prefer to log the dependent variable in order to make the gradient descent algorithm work faster.","metadata":{}},{"cell_type":"code","source":"#####################################################\n# Model\n#####################################################\n\n########################\n# Custom Cost Function\n########################\ndef smape(preds, target):\n    n = len(preds)\n    masked_arr = ~((preds == 0) & (target == 0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds - target)\n    denom = np.abs(preds) + np.abs(target)\n    smape_val = (200 * np.sum(num / denom)) / n\n    return smape_val\n\ndef lgbm_smape(preds, train_data):\n    labels = train_data.get_label()\n    smape_val = smape(np.expm1(preds), np.expm1(labels))\n    return 'SMAPE', smape_val, False","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:41:24.094151Z","iopub.execute_input":"2022-02-15T10:41:24.094864Z","iopub.status.idle":"2022-02-15T10:41:24.102633Z","shell.execute_reply.started":"2022-02-15T10:41:24.094829Z","shell.execute_reply":"2022-02-15T10:41:24.101631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MAE: mean absolute error\nMAPE: mean absolute percentage error\nSMAPE: Symmetric mean absolute percentage error (adjusted MAPE)\n\nSMAPE (Symmetric Mean Absolute Error) or MAPE allows us to evaluate the errors as a percentage. Returns a metric from 0-100. The expm1() function reverses the log transformation.","metadata":{}},{"cell_type":"code","source":"########################\n# Time-Based Validation Sets\n########################\n\ntrain = df.loc[(df[\"date\"] < \"2017-10-01\"), :]\n\nval = df.loc[(df[\"date\"] >= \"2017-10-01\") & (df[\"date\"] < \"2017-12-31\"), :]\n\ncols = [col for col in train.columns if col not in ['date', 'id', \"sales\", \"year\"]]\n\nY_train = train['sales']\nX_train = train[cols]\n\nY_val = val['sales']\nX_val = val[cols]\n\nY_train.shape, X_train.shape, Y_val.shape, X_val.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:41:24.103603Z","iopub.execute_input":"2022-02-15T10:41:24.104354Z","iopub.status.idle":"2022-02-15T10:41:25.588379Z","shell.execute_reply.started":"2022-02-15T10:41:24.104319Z","shell.execute_reply":"2022-02-15T10:41:25.58728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this point, we cannot separate the data set with train test split, because this function pulls random values from the data, thus distorting the context and structure of the time series. For this, we separate the dataset ourselves.","metadata":{}},{"cell_type":"code","source":"lgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'num_boost_round': 1000,\n              'early_stopping_rounds': 200,\n              'nthread': -1}\n\nlgbtrain = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\nlgbval = lgb.Dataset(data=X_val, label=Y_val, reference=lgbtrain, feature_name=cols)\n\nmodel = lgb.train(lgb_params, lgbtrain,\n                  valid_sets=[lgbtrain, lgbval],\n                  num_boost_round=lgb_params['num_boost_round'],\n                  early_stopping_rounds=lgb_params['early_stopping_rounds'],\n                  feval=lgbm_smape,\n                  verbose_eval=100)\n\ny_pred_val = model.predict(X_val, num_iteration=model.best_iteration)\n\nsmape(np.expm1(y_pred_val), np.expm1(Y_val))","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:41:25.589611Z","iopub.execute_input":"2022-02-15T10:41:25.589836Z","iopub.status.idle":"2022-02-15T10:44:24.401739Z","shell.execute_reply.started":"2022-02-15T10:41:25.589804Z","shell.execute_reply":"2022-02-15T10:44:24.400762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Normally we used lightgbm from within sklearn. Here we used Microsoft's own lightgbm, but for this the model requires its own data type. With lgb.Dataseet, we can convert this data to the desired data type.","metadata":{}},{"cell_type":"code","source":"########################\n# Final Model\n########################\ntrain = df.loc[~df.sales.isna()]\nY_train = train['sales']\nX_train = train[cols]\n\ntest = df.loc[df.sales.isna()]\nX_test = test[cols]\n\nlgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'nthread': -1,\n              \"num_boost_round\": model.best_iteration}","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:44:24.403733Z","iopub.execute_input":"2022-02-15T10:44:24.403993Z","iopub.status.idle":"2022-02-15T10:44:26.008432Z","shell.execute_reply.started":"2022-02-15T10:44:24.403961Z","shell.execute_reply":"2022-02-15T10:44:26.007669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LightGBM dataset\nlgbtrain_all = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\n\nmodel = lgb.train(lgb_params, lgbtrain_all, num_boost_round=model.best_iteration)\n\ntest_preds = model.predict(X_test, num_iteration=model.best_iteration)\n\n# Creat submission\nsubmission_df = test.loc[:, ['id', 'sales']]\nsubmission_df['sales'] = np.expm1(test_preds)\nsubmission_df['id'] = submission_df.id.astype(int)\n\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:44:26.009896Z","iopub.execute_input":"2022-02-15T10:44:26.010602Z","iopub.status.idle":"2022-02-15T10:46:12.087797Z","shell.execute_reply.started":"2022-02-15T10:44:26.010569Z","shell.execute_reply":"2022-02-15T10:46:12.086759Z"},"trusted":true},"execution_count":null,"outputs":[]}]}