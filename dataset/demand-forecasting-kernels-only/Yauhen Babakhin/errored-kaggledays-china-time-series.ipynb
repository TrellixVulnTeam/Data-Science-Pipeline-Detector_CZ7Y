{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action=\"ignore\")\n\nimport datetime\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nplt.style.use(\"ggplot\")\n\n# Seed Everything\nseed = 123\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read data and preprocess date feature"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def read_data(train_path, test_path):\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n    \n    # Set date to the index\n    train['date'] = pd.to_datetime(train['date'])\n    train.set_index([\"date\"], inplace=True)\n\n    test['date'] = pd.to_datetime(test['date'])\n    test.set_index([\"date\"], inplace=True)\n    \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_date_features(df):\n    \n    # Add date-specific features\n    df['dayofweek'] = df.index.dayofweek\n    df['is_weekend'] = df.index.dayofweek // 5\n    df['day'] = df.index.day\n    df['month'] = df.index.month\n    df['year'] = df.index.year\n    df['dayofyear'] = df.index.dayofyear\n    df['weekofyear'] = df.index.weekofyear\n\n    return df\n\n\ndef generate_features(train, test):\n\n    # Get the time delta between last test and last train observations\n    model_delta = max(test.index) - max(train.index)\n\n    # Initialize variables\n    lags = [model_delta.days]\n    window = 7\n    lag_features = []\n\n    # Concatenate train and test together\n    data = pd.concat([train, test], sort=False)\n\n    for lag in lags:\n        feat = data.groupby([\"store\", \"item\"])[[\"sales\"]].rolling(window=window).mean().shift(lag)\n        feat.columns = ['sales_mean_lag_{}'.format(lag)]\n        lag_features.append(feat)\n\n        feat = data.groupby([\"store\", \"item\"])[[\"sales\"]].rolling(window=window).std().shift(lag)\n        feat.columns = ['sales_std_lag_{}'.format(lag)]\n        lag_features.append(feat)\n\n    # Concatenate all features together\n    lag_features = pd.concat(lag_features, axis=1)\n\n    # Add date features to the train and test\n    train = get_date_features(train)\n    test = get_date_features(test)\n\n    # Join lag features to the train and test by (store, item, date)\n    train.set_index([\"store\", \"item\"], append=True, inplace=True)\n    train = train.reorder_levels([\"store\", \"item\", \"date\"])\n    test.set_index([\"store\", \"item\"], append=True, inplace=True)\n    test = test.reorder_levels([\"store\", \"item\", \"date\"])\n\n    train = train.join(lag_features)\n    test = test.join(lag_features)\n\n    # Resetting index back\n    train.reset_index(level=[0, 1], inplace=True)\n    test.reset_index(level=[0, 1], inplace=True)\n    \n    return train, test, model_delta, window","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Machine Learning pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read data\ntrain, test = read_data(train_path=\"../input/demand-forecasting-kernels-only/train.csv\",\n                        test_path=\"../input/demand-forecasting-kernels-only/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look at the data\nplt.figure(figsize=(12, 8))\nplt.plot(train[(train.store==1) & (train.item==1)][\"sales\"].diff())\nplt.title(\"Train data for store 1; item 1\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are multiple approaches for multi-step time series predictions: https://arxiv.org/pdf/1108.3259.pdf\n\n# Generate some features\ntrain, test, model_delta, window = generate_features(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train start date\nstart = min(train.index) + model_delta + datetime.timedelta(days=window)\n\n# Validation start date\nsplit_date = max(train.index) - model_delta\n\n# Keep only non-NA values in the train data\ntrain = train[train.index >= start]\n\n# Train-Validation split\nval_train = train[train.index < split_date]\nvalidation = train[train.index > split_date]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SMAPE metric"},{"metadata":{},"cell_type":"markdown","source":"![smape](https://im0-tub-ru.yandex.net/i?id=2933600500955e25964ec47bc5aebcf0&n=13)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def smape(y_true, y_pred):\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n    diff = 100 * np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    \n    return np.nanmean(diff)\n\n\ndef smape_lightgbm(y_pred, y_true):\n    return \"smape\", smape(y_true, y_pred), False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train a model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select all the features except for the target variable\nfeatures = [x for x in train.columns if x != \"sales\"]\n\nparams = {\n    \"learning_rate\": 0.1,\n    \"objective\": \"regression\",\n    \"metric\": \"None\",\n    \"n_estimators\": 1000,\n    \"colsample_bytree\": 0.9,\n    \"num_leaves\": 32,\n    \"subsample\": 0.8,\n    \"subsample_freq\": 1,\n    \"lambda_l2\": 1,\n}\n\n# Train LightGBM model\nreg = lgb.LGBMRegressor(**params)\nreg = reg.fit(\n    val_train[features],\n    val_train[\"sales\"],\n    eval_set=[(validation[features], validation[\"sales\"])],\n    verbose=100,\n    early_stopping_rounds=200,\n    eval_metric=smape_lightgbm,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate model"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = reg.predict(validation[features])\nvalidation['pred'] = predictions\nprint(\"Validation SMAPE: {:.3f}\".format(smape(validation[\"sales\"], validation[\"pred\"])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\n\nto_plot = validation[(validation.store==1) & (validation.item==1)]\n\nplt.plot(to_plot[\"sales\"], alpha=0.8)\nplt.plot(to_plot[\"pred\"], linestyle=\"--\")\nplt.title(\"True values vs Predicted values for store 1; item 1\")\nplt.legend([\"TRUE VALUES\", \"PREDICTED VALUES\"])\nplt.xticks(None)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Re-train the model on the whole train data\nparams['n_estimators'] = reg.best_iteration_\n\n# Train LightGBM model\nreg = lgb.LGBMRegressor(**params)\nreg = reg.fit(\n    train[features],\n    train[\"sales\"],\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = pd.DataFrame(\n    {\"feature\": val_train[features].columns,\n     \"importance\": reg.feature_importances_}\n)\n\nplt.figure(figsize=(8, 8))\nsns.barplot(\n    data=feature_importances.sort_values(\"importance\", ascending=False).head(10),\n    x=\"importance\",\n    y=\"feature\",\n)\nplt.yticks(fontsize=14)\nplt.title(\"Top 10 features\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make test predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"sales\"] = reg.predict(test[features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nplt.plot(train[(train.store==1) & (train.item==1)][[\"sales\"]])\nplt.plot(test[(test.store==1) & (test.item==1)][[\"sales\"]])\nplt.title(\"Train data and Test predictions for store 1; item 1\")\nplt.legend([\"TRAIN\", \"TEST\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[[\"id\", \"sales\"]].to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}