{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/train.csv')\ntest  = pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/test.csv')\nsamp  = pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.date = pd.to_datetime(test.date)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[:,'date'] = pd.to_datetime(train.loc[:,'date'])\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Store: ',train.store.unique(),'{} stores'.format(len(train.store.unique())))\nprint('Item: ',train.item.unique(),'{} items'.format(len(train.item.unique())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig,ax = plt.subplots(figsize = (20,10))\n\nfor j in range(3):\n    s = np.random.randint(min(train.store),max(train.store))\n    i  = np.random.randint(min(train.item),max(train.item))\n    \n    temp = train.loc[(train.store == s) & (train.item ==i),:]\n        \n    ax.plot(temp.date,temp.sales,label = 'Store {} Item {}'.format(s,i))\n\nax.set_xlabel('datetime',fontsize = 20)\nax.set_ylabel('sales',fontsize = 20)\nax.tick_params(axis='both', which='major', labelsize=20)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ex = train.loc[(train.store == 9) & (train.item ==12),:]\n\nex.loc[:,'day_of_month']  = ex.loc[:,'date'].dt.day\nex.loc[:,'month_of_year'] = ex.loc[:,'date'].dt.month\nex.loc[:,'year']          = ex.loc[:,'date'].dt.year\nex.loc[:,'day_of_year']   = ex.loc[:,'date'].dt.dayofyear\nex.loc[:,'day_of_week']   = ex.loc[:,'date'].dt.dayofweek\n\nex_train = ex.loc[ex.year <  2017,:]\nex_test  = ex.loc[ex.year == 2017,:]\n\nplt.plot(ex_train.date,ex_train.sales)\nplt.plot(ex_test.date,ex_test.sales)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (20,10))\n\nfor year in ex_train.year.unique():\n    temp = ex_train.loc[ex_train.year == year,:]\n    ax.plot(temp.day_of_year,temp.sales,label = year)\n    ax.set_xlabel('Day of a year',fontsize = 20)\n    ax.set_ylabel('Sales',fontsize = 20)\n    ax.tick_params(labelsize = 20)\nplt.legend()  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transformation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## rolling mean","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Null hypothesis: Non Stationarity exists in the series.\n# Alternative Hypothesis: Stationarity exists in the series\n\n# Data: (-1.8481210964862593, 0.35684591783869046, 0, 1954, {'10%': -2.5675580437891359, \n# '1%': -3.4337010293693235, '5%': -2.863020285222162}, 21029.870846458849\n\n# Lets break data one by one.\n# First data point: -1.8481210964862593: Critical value of the data in your case\n# Second data point: 0.35684591783869046: Probability that null hypothesis will not be rejected(p-value)\n# Third data point: 0: Number of lags used in regression to determine t-statistic. So there are no auto correlations going back to '0' periods here.\n# Forth data point: 1954: Number of observations used in the analysis.\n# Fifth data point: {'10%': -2.5675580437891359, '1%': -3.4337010293693235, '5%': -2.863020285222162}: T values corresponding to adfuller test.\n\n\n# Since critical value -1.8>-2.5,-3.4,-2.8 (t-values at 1%,5%and 10% confidence intervals), null hypothesis cannot be rejected. So there is non stationarity in your data\n# Also p-value of 0.35>0.05(if we take 5% significance level or 95% confidence interval), null hypothesis cannot be rejected.\n\n# Hence data is non stationary (that means it has relation with time)\n\n\n\nex_train_rm = ex_train.copy()\nex_train_rm.loc[:,'sales'] = ex_train_rm.loc[:,'sales'].rolling(window = 30).mean()\nplt.plot(ex_train.date,ex_train.sales)\nplt.plot(ex_train_rm.date,ex_train_rm.sales)\n\n\nfrom statsmodels.tsa.stattools import adfuller\n\nadf = adfuller(ex_train.sales,autolag='AIC')\n\nprint(adf[0],\n      adf[1],\n      adf[4])\n\nadf = adfuller(ex_train_rm.sales.dropna(),autolag='AIC')\n\nprint(adf[0],\n      adf[1],\n      adf[4])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Log transform","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ex_train_log = ex_train.copy()\nex_train_log.loc[:,'sales'] = np.log(ex_train_log.loc[:,'sales'])\n\nplt.plot(ex_train.date,ex_train.sales)\nplt.plot(ex_train_log.date,ex_train_log.sales)\n\nfrom statsmodels.tsa.stattools import adfuller\n\nadf = adfuller(ex_train.sales,autolag='AIC')\n\nprint(adf[0],\n      adf[1],\n      adf[4])\n\nadf = adfuller(ex_train_log.sales,autolag='AIC')\n\nprint(adf[0],\n      adf[1],\n      adf[4])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Differenciating","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ex_train_diff= ex_train.copy()\nex_train_diff.loc[:,'sales'] = ex_train_diff.loc[:,'sales'].diff()\n\nplt.plot(ex_train.date,ex_train.sales)\nplt.plot(ex_train_diff.date,ex_train_diff.sales)\n\nfrom statsmodels.tsa.stattools import adfuller\nadf = adfuller(ex_train.sales,autolag='AIC')\n\nprint(adf[0],\n      adf[1],\n      adf[4])\n\nadf = adfuller(ex_train_diff.dropna().sales,autolag='AIC')\n\nprint(adf[0],\n      adf[1],\n      adf[4])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Seasonality and Trend","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa import seasonal\n# seasonal trend residual\ndecompose = seasonal.seasonal_decompose(ex_train.set_index('date')['sales'],model='additive',extrapolate_trend = 'freq',period = 365)\n\nfig,ax = plt.subplots(4,1,figsize = (10,10))\n\n\nax[0].plot(decompose.observed.index,decompose.observed)\nax[1].plot(decompose.observed.index,decompose.trend,linewidth=10 )\nax[2].plot(decompose.observed.index,decompose.seasonal)\nax[3].plot(decompose.observed.index,decompose.resid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## deseson prev season","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"decompose2 = seasonal.seasonal_decompose(decompose.seasonal,model='additive',extrapolate_trend = 'freq',period = 365)\n\nfig,ax = plt.subplots(4,1,figsize = (10,10))\n\n\nax[0].plot(decompose2.observed.index,decompose2.observed)\nax[1].plot(decompose2.observed.index,decompose2.trend )\nax[2].plot(decompose2.observed.index,decompose2.seasonal)\nax[3].plot(decompose2.observed.index,decompose2.resid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compare trend,season reomval with diff","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## diff\nex_train_diff = ex_train.copy()\nex_train_diff.sales = ex_train.sales.diff()\n\n# trend,season,remove\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndecomposed  = seasonal_decompose(ex_train.set_index('date').sales,model = 'additive',period=365,extrapolate_trend = 'freq')\n\nplt.plot(ex_train_diff.date,ex_train_diff.sales)\nplt.plot(ex_train.date,decomposed.resid,color = 'red',linestyle = '-')\n\n# on diff data\nfrom statsmodels.tsa.stattools import adfuller, kpss\nadf = adfuller(ex_train_diff.dropna().sales,autolag='AIC')\nprint('diff',adf[1])\nprint('diff',adf[0],adf[4])\n\n#on resid\nadf = adfuller(decompose.resid,autolag='AIC')\nprint('Trend removal', adf[1])\nprint('Trend removal',adf[0],adf[4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# on row data\nfrom statsmodels.tsa.stattools import adfuller, kpss\nadf = adfuller(ex_train.sales,autolag='AIC')\nprint(adf[1])\nprint(adf[0],adf[4])\n\n#on resid\nadf = adfuller(decompose.resid,autolag='AIC')\nprint(adf[1])\nprint(adf[0],adf[4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import acf,pacf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nacf_50 = acf(ex_train_diff.sales.dropna(), nlags=10)\npacf_50 = pacf(ex_train_diff.sales.dropna(), nlags=10)\n\nfig, axes = plt.subplots(1,2,figsize=(16,3))\nplot_acf(ex_train_diff.sales.dropna(), lags=10, ax=axes[0])\nplot_pacf(ex_train_diff.sales.dropna(),lags=10, ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ex_train = ex_train.set_index('date')[['sales']].resample('D').mean()\nex_test = ex_test.set_index('date')[['sales']].resample('D').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(ex_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\n\nsarima = SARIMAX(ex_train,\n                 order=(1,1,1),\n                 seasonal_order=(1,1,1,7),\n                 freq='D')\nsarima_fit = sarima.fit(disp = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = sarima_fit.plot_diagnostics()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = sarima_fit.get_prediction(start = '2013-01-06',end = '2018-01-01',dynamic = False)\nmean_pred = pred.predicted_mean\nconf_mean = pred.conf_int()\n\nfig1,ax1 = plt.subplots(figsize = (15,10))\n\nax1.plot(ex_train.index,ex_train.values,color = 'black',linestyle = '-',linewidth = 2)\nax1.plot(mean_pred.index,mean_pred.values,color = 'red',linestyle = '-',linewidth = 2)\nax1.plot(conf_mean['2014':].index,conf_mean['2014':],color = 'blue',linestyle = '--',linewidth = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = np.mean(np.abs((ex_train.values-mean_pred.values)))\nprint(rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(sarima_fit.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast = sarima_fit.get_forecast(steps = 365)\nmean_forecast = forecast.predicted_mean\nconf_forecast = forecast.conf_int()\n\nfig1,ax1 = plt.subplots(figsize = (15,10))\n\nax1.plot(ex_test.index,ex_test.values,color = 'black',linestyle = '-',linewidth = 2)\nax1.plot(mean_forecast.index,mean_forecast.values,color = 'red',linestyle = '-',linewidth = 2)\nax1.plot(conf_forecast.index,conf_forecast,color = 'blue',linestyle = '--',linewidth = 1)\n\nprint(np.mean(np.sqrt((ex_test.values - mean_forecast.values)**2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id = test.id\ntest.drop('id',axis = 1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['set'] = 'train'\ntest['set']  = 'test'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train,test])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['day_of_month']     = data.date.dt.day\ndata['month_of_year']   = data.date.dt.month\ndata['year']    = data.date.dt.year\ndata['day_of_year'] = data.date.dt.dayofyear\ndata['day_of_week'] = data.date.dt.dayofweek\ndata['is_weekday'] = data['day_of_week'].apply(lambda x: 1 if x in (6,7) else 0)\ndata['is_month_start']   = data.date.dt.is_month_start.map({False:0,True:1})\ndata['is_month_end']     = data.date.dt.is_month_end.map({False:0,True:1})\n\n\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\nholidays = calendar().holidays(start=data.date.min(), end=data.date.max())\ndata['ia_holiday'] = data.date.isin(holidays).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lags","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grouping = data.groupby(['store','item'])\nlags = [90,91,92,93,94,95,96,97,98,99,100,180,270]\nfor lag in lags:\n    col_name = 'lag-'+str(lag)\n    data[col_name] = grouping.sales.shift(lag)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lags = [90,97,104]\n\nfor lag in lags:\n    col_name = 'rolling_mean-'+str(lag)\n    data[col_name] = grouping.sales.shift(lag).rolling(window=7).mean()\n    \nfor lag in lags:\n    col_name = 'rolling_std-'+str(lag)\n    data[col_name] = grouping.sales.shift(lag).rolling(window=7).std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test  = data.loc[data.set  == 'test',:]\ntrain = data.loc[data.set == 'train',:].dropna()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sales = np.log1p(train.sales)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['date','sales','set'],axis=1).dropna()\ny = train.sales\n\nX_train,X_val = X.loc[X.year < 2017],X.loc[X.year == 2017]\ny_train,y_val = y.loc[X.year < 2017],y.loc[X.year == 2017]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\ntransformer = make_column_transformer(\n    (OneHotEncoder(),['store','item','day_of_week']),\n    (MinMaxScaler(), ['day_of_month','day_of_year']),\n    (StandardScaler(),['lag-90', 'lag-91','lag-92', 'lag-93', 'lag-94', 'lag-95', 'lag-96', 'lag-97', 'lag-98','lag-99', 'lag-100', 'lag-180', 'lag-270',\n                        'rolling_mean-90',\n                        'rolling_mean-97', 'rolling_mean-104', 'rolling_std-90',\n                        'rolling_std-97', 'rolling_std-104']),\n    remainder = 'passthrough'\n)\n\nfrom sklearn.model_selection import TimeSeriesSplit\nimport xgboost as xgb\n\nregressor = xgb.XGBRegressor(n_estimators = 500,\n                             max_depth = 5)\n\npipeline = make_pipeline(transformer,regressor)\n\npipeline.fit(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\npred_val = pipeline.predict(X_val)\npred_train = pipeline.predict(X_train)\n\nprint(mean_absolute_error(y_val,pred_val))\nprint(mean_absolute_error(y_train,pred_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def smape(preds, target):\n    '''\n    Function to calculate SMAPE\n    '''\n    n = len(preds)\n    masked_arr = ~((preds==0)&(target==0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds-target)\n    denom = np.abs(preds)+np.abs(target)\n    smape_val = (200*np.sum(num/denom))/n\n    return smape_val\n\nprint(smape(pred_val,y_val))\nprint(smape(pred_train,y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.fit(X,y)\npred = pipeline.predict(X)\nprint(smape(pred,y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test.copy().drop(['date','sales','set'],axis = 1)\npred_test = np.expm1(pipeline.predict(X_test))\n\nsub = pd.DataFrame({'id':test_id,'sales':np.round(pred_test)})\nsub.to_csv('submission.csv',index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}