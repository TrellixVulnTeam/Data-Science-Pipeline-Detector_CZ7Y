{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-14T14:33:50.484984Z","iopub.execute_input":"2022-02-14T14:33:50.485303Z","iopub.status.idle":"2022-02-14T14:33:50.493374Z","shell.execute_reply.started":"2022-02-14T14:33:50.485269Z","shell.execute_reply":"2022-02-14T14:33:50.492792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport warnings\nimport statsmodels.api as sm\nimport statsmodels.tsa.api as smt\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.metrics import mean_absolute_error\nwarnings.filterwarnings(\"ignore\")\n\n# Farklı mağazalar için 3 aylık item-level sales tahmini.\n# 5 yıllık bir veri setinde 10 farklı mağaza ve 50 farklı item var.\n# Buna göre mağaza-item kırılımında 3 ay sonrasının tahminlerini vermemiz gerekiyor.\n\n# Note : İstatistiksel zaman serilerinde yaklaşımımız şu şekilde olmalı; Yöntemler var ve bu yöntemlerin model dereceleri,\n# diğer ifadesiyle hiperparametreleri var. Olası tüm hiperparametre kombinasyonlarını deneriz, en iyi versiyonlara sahip\n# olan özelliklerle modellerimizi kurarız. Olaya nedensellik bağlamında değil de yüksek tahmin başarısı bağlamında yaklaşıyoruz.\n# SARIMA modeli triple exponential smoothing methoduna benzemektedir.\n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 500)\nwarnings.filterwarnings('ignore')\n\ndef check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\ntrain = pd.read_csv('../input/demand-forecasting-kernels-only/train.csv', parse_dates=['date'])\ntest = pd.read_csv('../input/demand-forecasting-kernels-only/test.csv', parse_dates=['date'])\nsample_sub = pd.read_csv('../input/demand-forecasting-kernels-only/sample_submission.csv')\ndf = pd.concat([train, test], sort=False)\ndf.head()\n#         date  store  item  sales  id\n# 0 2013-01-01      1     1   13.0 NaN\n# 1 2013-01-02      1     1   11.0 NaN\n# 2 2013-01-03      1     1   14.0 NaN\n# 3 2013-01-04      1     1   13.0 NaN\n# 4 2013-01-05      1     1   10.0 NaN\n\ndf['date'].min(), df['date'].max()\n# (Timestamp('2013-01-01 00:00:00'), Timestamp('2018-03-31 00:00:00'))\n\ncheck_df(train)\n# ##################### Shape #####################\n# (913000, 4)\n# ##################### Types #####################\n# date     datetime64[ns]\n# store             int64\n# item              int64\n# sales             int64\n# dtype: object\n# ##################### Head #####################\n#         date  store  item  sales\n# 0 2013-01-01      1     1     13\n# 1 2013-01-02      1     1     11\n# 2 2013-01-03      1     1     14\n# 3 2013-01-04      1     1     13\n# 4 2013-01-05      1     1     10\n# ##################### Tail #####################\n#              date  store  item  sales\n# 912995 2017-12-27     10    50     63\n# 912996 2017-12-28     10    50     59\n# 912997 2017-12-29     10    50     74\n# 912998 2017-12-30     10    50     62\n# 912999 2017-12-31     10    50     82\n# ##################### NA #####################\n# date     0\n# store    0\n# item     0\n# sales    0\n# dtype: int64\n# ##################### Quantiles #####################\n#        0.00  0.05  0.50   0.95   0.99   1.00\n# store   1.0   1.0   5.5   10.0   10.0   10.0\n# item    1.0   3.0  25.5   48.0   50.0   50.0\n# sales   0.0  16.0  47.0  107.0  135.0  231.0\n\ncheck_df(test)\n# ##################### Shape #####################\n# (45000, 4)\n# ##################### Types #####################\n# id                int64\n# date     datetime64[ns]\n# store             int64\n# item              int64\n# dtype: object\n# ##################### Head #####################\n#    id       date  store  item\n# 0   0 2018-01-01      1     1\n# 1   1 2018-01-02      1     1\n# 2   2 2018-01-03      1     1\n# 3   3 2018-01-04      1     1\n# 4   4 2018-01-05      1     1\n# ##################### Tail #####################\n#           id       date  store  item\n# 44995  44995 2018-03-27     10    50\n# 44996  44996 2018-03-28     10    50\n# 44997  44997 2018-03-29     10    50\n# 44998  44998 2018-03-30     10    50\n# 44999  44999 2018-03-31     10    50\n# ##################### NA #####################\n# id       0\n# date     0\n# store    0\n# item     0\n# dtype: int64\n# ##################### Quantiles #####################\n#        0.00     0.05     0.50      0.95      0.99     1.00\n# id      0.0  2249.95  22499.5  42749.05  44549.01  44999.0\n# store   1.0     1.00      5.5     10.00     10.00     10.0\n# item    1.0     3.00     25.5     48.00     50.00     50.0\n\ncheck_df(df)\n# ##################### Shape #####################\n# (958000, 5)\n# ##################### Types #####################\n# date     datetime64[ns]\n# store             int64\n# item              int64\n# sales           float64\n# id              float64\n# dtype: object\n# ##################### Head #####################\n#         date  store  item  sales  id\n# 0 2013-01-01      1     1   13.0 NaN\n# 1 2013-01-02      1     1   11.0 NaN\n# 2 2013-01-03      1     1   14.0 NaN\n# 3 2013-01-04      1     1   13.0 NaN\n# 4 2013-01-05      1     1   10.0 NaN\n# ##################### Tail #####################\n#             date  store  item  sales       id\n# 44995 2018-03-27     10    50    NaN  44995.0\n# 44996 2018-03-28     10    50    NaN  44996.0\n# 44997 2018-03-29     10    50    NaN  44997.0\n# 44998 2018-03-30     10    50    NaN  44998.0\n# 44999 2018-03-31     10    50    NaN  44999.0\n# ##################### NA #####################\n# date          0\n# store         0\n# item          0\n# sales     45000\n# id       913000\n# dtype: int64\n# ##################### Quantiles #####################\n#        0.00     0.05     0.50      0.95      0.99     1.00\n# store   1.0     1.00      5.5     10.00     10.00     10.0\n# item    1.0     3.00     25.5     48.00     50.00     50.0\n# sales   0.0    16.00     47.0    107.00    135.00    231.0\n# id      0.0  2249.95  22499.5  42749.05  44549.01  44999.0\n\ndf['sales'].describe([0.10, 0.30, 0.50, 0.70, 0.80, 0.90, 0.95, 0.99])\n# count    913000.000000\n# mean         52.250287\n# std          28.801144\n# min           0.000000\n# 10%          20.000000\n# 30%          33.000000\n# 50%          47.000000\n# 70%          64.000000\n# 80%          76.000000\n# 90%          93.000000\n# 95%         107.000000\n# 99%         135.000000\n# max         231.000000\n\n# Kaç tane mağaza var ?\ndf[[\"store\"]].nunique()\n# store    10\n\n# Kaç tane item var ?\ndf[[\"item\"]].nunique()\n# item    50\n\n#Her mağaza'da eşit sayıda mı eşsiz item var ?\ndf.groupby([\"store\"])[\"item\"].nunique()\n# store\n# 1     50\n# 2     50\n# 3     50\n# 4     50\n# 5     50\n# 6     50\n# 7     50\n# 8     50\n# 9     50\n# 10    50\n\n# Her mağazada eşit sayıda mı satış var?\ndf.groupby([\"store\", \"item\"]).agg({\"sales\": [\"sum\"]})\n#                sales\n#                  sum\n# store item\n# 1     1      36468.0\n#       2      97050.0\n#       3      60638.0\n#       4      36440.0\n#       5      30335.0\n#               ...\n# 10    46    120601.0\n#       47     45204.0\n#       48    105570.0\n#       49     60317.0\n#       50    135192.0\n# [500 rows x 1 columns]\n\n# Mağaza-item kırılımında satış istatistikleri\ndf.groupby([\"store\", \"item\"]).agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\"]})\n#                sales\n#                  sum       mean median        std\n# store item\n# 1     1      36468.0  19.971522   19.0   6.741022\n#       2      97050.0  53.148959   52.0  15.005779\n#       3      60638.0  33.208105   33.0  10.072529\n#       4      36440.0  19.956188   20.0   6.640618\n#       5      30335.0  16.612815   16.0   5.672102\n#               ...        ...    ...        ...\n# 10    46    120601.0  66.046550   65.0  18.114991\n#       47     45204.0  24.755750   24.0   7.924820\n#       48    105570.0  57.814896   57.0  15.898538\n#       49     60317.0  33.032311   32.0  10.091610\n#       50    135192.0  74.037240   73.0  19.937566\n# [500 rows x 4 columns]\n\n#######################\n# Feature Engineering #\n#######################\n\n# gün\n# hafta\n# yıl\n# hafta içi\n# hafta sonu\n# özel günler\n# haftanın kaçıncı günü\n# ayın kaçıncı günü\n# yılın kaçıncı haftası\n# yılın kaçıncı ayı\n# yılın kaçıncı günü\n\ndf.head()\n#         date  store  item  sales  id\n# 0 2013-01-01      1     1   13.0 NaN\n# 1 2013-01-02      1     1   11.0 NaN\n# 2 2013-01-03      1     1   14.0 NaN\n# 3 2013-01-04      1     1   13.0 NaN\n# 4 2013-01-05      1     1   10.0 NaN\n\n# Zaman odaklı bir özellik yapıyoruz. Örüntü yakalama açısıyla olaylara yaklaşmak gerekir.\ndef create_date_features(df):\n    df['month'] = df.date.dt.month\n    df['day_of_month'] = df.date.dt.day\n    df['day_of_year'] = df.date.dt.dayofyear\n    df['week_of_year'] = df.date.dt.weekofyear\n    df['day_of_week'] = df.date.dt.dayofweek\n    df['year'] = df.date.dt.year\n    df[\"is_wknd\"] = df.date.dt.weekday // 4\n    df['is_month_start'] = df.date.dt.is_month_start.astype(int)\n    df['is_month_end'] = df.date.dt.is_month_end.astype(int)\n    return df\n\ndf = create_date_features(df)\n\ndf.head()\n#         date  store  item  sales  id  month  day_of_month  day_of_year  week_of_year  day_of_week  year  is_wknd  is_month_start  is_month_end\n# 0 2013-01-01      1     1   13.0 NaN      1             1            1             1            1  2013        0               1             0\n# 1 2013-01-02      1     1   11.0 NaN      1             2            2             1            2  2013        0               0             0\n# 2 2013-01-03      1     1   14.0 NaN      1             3            3             1            3  2013        0               0             0\n# 3 2013-01-04      1     1   13.0 NaN      1             4            4             1            4  2013        1               0             0\n# 4 2013-01-05      1     1   10.0 NaN      1             5            5             1            5  2013        1               0             0\n\n\ndf.groupby([\"store\", \"item\", \"month\"]).agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\"]})\n#                    sales\n#                       sum       mean median        std\n# store item month\n# 1     1    1       2125.0  13.709677   13.0   4.397413\n#            2       2063.0  14.631206   14.0   4.668146\n#            3       2728.0  17.600000   17.0   4.545013\n#            4       3118.0  20.786667   20.0   4.894301\n#            5       3448.0  22.245161   22.0   6.564705\n#                    ...        ...    ...        ...\n# 10    50   8      13108.0  84.567742   85.0  15.676527\n#            9      11831.0  78.873333   79.0  15.207423\n#            10     11322.0  73.045161   72.0  14.209171\n#            11     11549.0  76.993333   77.0  16.253651\n#            12      8724.0  56.283871   56.0  11.782529\n# [6000 rows x 4 columns]\n# Aylara göre hangi mağazanın ne kadar satış yaptığı bilgisine erişmiş olduk.\n\n################\n# Random Noise #\n################\n# Veri setinin boyutu kadar bir normal dağılımlı bir gürültü seti oluşturuyoruz. Bu veri setinin boyutu kadar\n# oluşturduğumuz rasgele değerlere oluşturacak olduğumuz yeni özelliklerin üzerine ekliyoruz. Yani rasgele\n# gürültü oluşturuyoruz. Veriye gürültü eklemek modelin aşırı öğrenmesini engelliyor.\ndef random_noise(dataframe):\n    return np.random.normal(scale=1.6, size=(len(dataframe),))\n\n########################\n# Lag/Shifted Features #\n########################\n\n# Geçmiş dönem satış sayılarına ilişkin özellikler üretiyoruz.\n\n# Burada veri setini mağazaya, item'e ve tarihe göre sıralıyoruz.\ndf.sort_values(by=['store', 'item', 'date'], axis=0, inplace=True)\n\ndf[\"sales\"].head(10)\n# 0    13.0\n# 1    11.0\n# 2    14.0\n# 3    13.0\n# 4    10.0\n# 5    12.0\n# 6    10.0\n# 7     9.0\n# 8    12.0\n# 9     9.0\n\ndf[\"sales\"].shift(1).values[0:10]\n# array([nan, 13., 11., 14., 13., 10., 12., 10.,  9., 12.])\n\npd.DataFrame({\"sales\": df[\"sales\"].values[0:10],\n              \"lag1\": df[\"sales\"].shift(1).values[0:10],\n              \"lag2\": df[\"sales\"].shift(2).values[0:10],\n              \"lag3\": df[\"sales\"].shift(3).values[0:10],\n              \"lag4\": df[\"sales\"].shift(4).values[0:10]})\n#    sales  lag1  lag2  lag3  lag4\n# 0   13.0   NaN   NaN   NaN   NaN\n# 1   11.0  13.0   NaN   NaN   NaN\n# 2   14.0  11.0  13.0   NaN   NaN\n# 3   13.0  14.0  11.0  13.0   NaN\n# 4   10.0  13.0  14.0  11.0  13.0\n# 5   12.0  10.0  13.0  14.0  11.0\n# 6   10.0  12.0  10.0  13.0  14.0\n# 7    9.0  10.0  12.0  10.0  13.0\n# 8   12.0   9.0  10.0  12.0  10.0\n# 9    9.0  12.0   9.0  10.0  12.0\n\ndf.groupby([\"store\", \"item\"])[\"sales\"].head()\n# 0         13.0\n# 1         11.0\n# 2         14.0\n# 3         13.0\n# 4         10.0\n#           ...\n# 911174    33.0\n# 911175    37.0\n# 911176    46.0\n# 911177    51.0\n# 911178    41.0\n\ndf.groupby([\"store\", \"item\"])[\"sales\"].transform(lambda x: x.shift(1))\n# 0         NaN\n# 1        13.0\n# 2        11.0\n# 3        14.0\n# 4        13.0\n#          ...\n# 44995     NaN\n# 44996     NaN\n# 44997     NaN\n# 44998     NaN\n# 44999     NaN\n# Name: sales, Length: 958000, dtype: float64\n# Bütün veri setine 1 gecikme uygulamış olduk.\n\ndef lag_features(dataframe, lags):\n    for lag in lags:\n        dataframe['sales_lag_' + str(lag)] = dataframe.groupby([\"store\", \"item\"])['sales'].transform(\n            lambda x: x.shift(lag)) + random_noise(dataframe)\n    return dataframe\n# Bu fonksiyonda dataframe'i verip istediğimiz gecikmeleri belirteceğiz. 15, 30, 90 günlük gecikmeleri belirtip\n# bu gecikmeler için yeni özellikler türetip bu gecikmelerin içerisinde gezeceğiz ve üstüne gürültü ekleyerek\n# veri setine uygulayacak.\n\ndf = lag_features(df, [91, 98, 105, 112, 119, 126, 182, 364, 546, 728])\n# 3 aylık periyota denk gelecek olan mevsimselliği ilgili periyot ve katları olacak şekilde seçtik.\n\ndf.head()\n#         date  store  item  sales  id  month  day_of_month  day_of_year  week_of_year  day_of_week  year  is_wknd  is_month_start  is_month_end  sales_lag_91  sales_lag_98  sales_lag_105  sales_lag_112  sales_lag_119  sales_lag_126  sales_lag_182  sales_lag_364  sales_lag_546  sales_lag_728\n# 0 2013-01-01      1     1   13.0 NaN      1             1            1             1            1  2013        0               1             0           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN\n# 1 2013-01-02      1     1   11.0 NaN      1             2            2             1            2  2013        0               0             0           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN\n# 2 2013-01-03      1     1   14.0 NaN      1             3            3             1            3  2013        0               0             0           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN\n# 3 2013-01-04      1     1   13.0 NaN      1             4            4             1            4  2013        1               0             0           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN\n# 4 2013-01-05      1     1   10.0 NaN      1             5            5             1            5  2013        1               0             0           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN\n\n#########################\n# Rolling Mean Features #\n#########################\n\n# Hareketli ortalamalar trendi gösterir, geçmiş bilgiyi taşırlar.\n\ndf[\"sales\"].head(10)\n# 0    13.0\n# 1    11.0\n# 2    14.0\n# 3    13.0\n# 4    10.0\n# 5    12.0\n# 6    10.0\n# 7     9.0\n# 8    12.0\n# 9     9.0\n\ndf[\"sales\"].rolling(window=2).mean().values[0:10]\n# array([ nan, 12. , 12.5, 13.5, 11.5, 11. , 11. ,  9.5, 10.5, 10.5])\n\npd.DataFrame({\"sales\": df[\"sales\"].values[0:10],\n              \"roll2\": df[\"sales\"].rolling(window=2).mean().values[0:10],\n              \"roll3\": df[\"sales\"].rolling(window=3).mean().values[0:10],\n              \"roll5\": df[\"sales\"].rolling(window=5).mean().values[0:10]})\n#    sales  roll2      roll3  roll5\n# 0   13.0    NaN        NaN    NaN\n# 1   11.0   12.0        NaN    NaN\n# 2   14.0   12.5  12.666667    NaN\n# 3   13.0   13.5  12.666667    NaN\n# 4   10.0   11.5  12.333333   12.2\n# 5   12.0   11.0  11.666667   12.0\n# 6   10.0   11.0  10.666667   11.8\n# 7    9.0    9.5  10.333333   10.8\n# 8   12.0   10.5  10.333333   10.6\n# 9    9.0   10.5  10.000000   10.4\n# Gecikmelerin ortalamasıdır.\n\npd.DataFrame({\"sales\": df[\"sales\"].values[0:10],\n              \"roll2\": df[\"sales\"].shift(1).rolling(window=2).mean().values[0:10],\n              \"roll3\": df[\"sales\"].shift(1).rolling(window=3).mean().values[0:10],\n              \"roll5\": df[\"sales\"].shift(1).rolling(window=5).mean().values[0:10]})\n#    sales  roll2      roll3  roll5\n# 0   13.0    NaN        NaN    NaN\n# 1   11.0    NaN        NaN    NaN\n# 2   14.0   12.0        NaN    NaN\n# 3   13.0   12.5  12.666667    NaN\n# 4   10.0   13.5  12.666667    NaN\n# 5   12.0   11.5  12.333333   12.2\n# 6   10.0   11.0  11.666667   12.0\n# 7    9.0   11.0  10.666667   11.8\n# 8   12.0    9.5  10.333333   10.8\n# 9    9.0   10.5  10.333333   10.6\n\n# shitf uygulayarak geçmiş trendi yakalamaya çalışırken gözlem birimini almayıp ondan öncekini alarak uyguluyoruz.\n\ndef roll_mean_features(dataframe, windows):\n    for window in windows:\n        dataframe['sales_roll_mean_' + str(window)] = dataframe.groupby([\"store\", \"item\"])['sales']. \\\n                                                          transform(\n            lambda x: x.shift(1).rolling(window=window, min_periods=10, win_type=\"triang\").mean()) + random_noise(\n            dataframe)\n    return dataframe\n\n# Üretilen özellikleri veri setinin içerisine yerleştirmek üzere yazılmış bir fonksiyondur.\n# store, item kırılımında sales değişkeninin 1 günlük shift'ini alıp ön tanımlı olarak girecek olduğumuz window'ların\n# her birisi için hesaplama işlemini gerçekleştiriyoruz.\n\ndf = roll_mean_features(df, [365, 546])\n\ndf.head()\n#         date  store  item  sales  id  month  day_of_month  day_of_year  week_of_year  day_of_week  year  is_wknd  is_month_start  is_month_end  sales_lag_91  sales_lag_98  sales_lag_105  sales_lag_112  sales_lag_119  sales_lag_126  sales_lag_182  sales_lag_364  sales_lag_546  sales_lag_728  sales_roll_mean_365  sales_roll_mean_546\n# 0 2013-01-01      1     1   13.0 NaN      1             1            1             1            1  2013        0               1             0           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                  NaN                  NaN\n# 1 2013-01-02      1     1   11.0 NaN      1             2            2             1            2  2013        0               0             0           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                  NaN                  NaN\n# 2 2013-01-03      1     1   14.0 NaN      1             3            3             1            3  2013        0               0             0           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                  NaN                  NaN\n# 3 2013-01-04      1     1   13.0 NaN      1             4            4             1            4  2013        1               0             0           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                  NaN                  NaN\n# 4 2013-01-05      1     1   10.0 NaN      1             5            5             1            5  2013        1               0             0           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                  NaN                  NaN\n\n########################################\n# Exponentially Weighted Mean Features #\n########################################\n# alpha değerleri burada ağırlıklı ortalamadır. 0.99 değeri verdiğimizde yakınındaki değerlere ağrılık verecek,\n# 0.1 olduğunda ise uzağındaki değerlere ağırlık verecektir. Gecikme sayısını 1 verip buna göre üssel ortalamalarını aldık.\npd.DataFrame({\"sales\": df[\"sales\"].values[0:10],\n              \"roll2\": df[\"sales\"].shift(1).rolling(window=2).mean().values[0:10],\n              \"ewm099\": df[\"sales\"].shift(1).ewm(alpha=0.99).mean().values[0:10],\n              \"ewm095\": df[\"sales\"].shift(1).ewm(alpha=0.95).mean().values[0:10],\n              \"ewm07\": df[\"sales\"].shift(1).ewm(alpha=0.7).mean().values[0:10],\n              \"ewm01\": df[\"sales\"].shift(1).ewm(alpha=0.1).mean().values[0:10]})\n#    sales  roll2     ewm099     ewm095      ewm07      ewm01\n# 0   13.0    NaN        NaN        NaN        NaN        NaN\n# 1   11.0    NaN  13.000000  13.000000  13.000000  13.000000\n# 2   14.0   12.0  11.019802  11.095238  11.461538  11.947368\n# 3   13.0   12.5  13.970201  13.855107  13.287770  12.704797\n# 4   10.0   13.5  13.009702  13.042750  13.084686  12.790637\n# 5   12.0   11.5  10.030097  10.152137  10.920146  12.109179\n# 6   10.0   11.0  11.980301  11.907607  11.676595  12.085878\n# 7    9.0   11.0  10.019803  10.095380  10.502722  11.686057\n# 8   12.0    9.5   9.010198   9.054769   9.450748  11.214433\n# 9    9.0   10.5  11.970102  11.852738  11.235259  11.342672\n\ndef ewm_features(dataframe, alphas, lags):\n    for alpha in alphas:\n        for lag in lags:\n            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n                dataframe.groupby([\"store\", \"item\"])['sales'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n    return dataframe\n\nalphas = [0.95, 0.9, 0.8, 0.7, 0.5]\nlags = [91, 98, 105, 112, 180, 270, 365, 546, 728]\n\ndf = ewm_features(df, alphas, lags)\n\ndf.head()\n#         date  store  item  sales  id  month  day_of_month  day_of_year  week_of_year  day_of_week  year  is_wknd  is_month_start  is_month_end  sales_lag_91  sales_lag_98  sales_lag_105  sales_lag_112  sales_lag_119  sales_lag_126  sales_lag_182  sales_lag_364  sales_lag_546  sales_lag_728  sales_roll_mean_365  sales_roll_mean_546  sales_ewm_alpha_095_lag_91  sales_ewm_alpha_095_lag_98  sales_ewm_alpha_095_lag_105  sales_ewm_alpha_095_lag_112  sales_ewm_alpha_095_lag_180  \\\n# 0 2013-01-01      1     1   13.0 NaN      1             1            1             1            1  2013        0               1             0           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                  NaN                  NaN                         NaN                         NaN                          NaN                          NaN                          NaN\n# 1 2013-01-02      1     1   11.0 NaN      1             2            2             1            2  2013        0               0             0           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                  NaN                  NaN                         NaN                         NaN                          NaN                          NaN                          NaN\n# 2 2013-01-03      1     1   14.0 NaN      1             3            3             1            3  2013        0               0             0           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                  NaN                  NaN                         NaN                         NaN                          NaN                          NaN                          NaN\n# 3 2013-01-04      1     1   13.0 NaN      1             4            4             1            4  2013        1               0             0           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                  NaN                  NaN                         NaN                         NaN                          NaN                          NaN                          NaN\n# 4 2013-01-05      1     1   10.0 NaN      1             5            5             1            5  2013        1               0             0           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                  NaN                  NaN                         NaN                         NaN                          NaN                          NaN                          NaN\n#    sales_ewm_alpha_095_lag_270  sales_ewm_alpha_095_lag_365  sales_ewm_alpha_095_lag_546  sales_ewm_alpha_095_lag_728  sales_ewm_alpha_09_lag_91  sales_ewm_alpha_09_lag_98  sales_ewm_alpha_09_lag_105  sales_ewm_alpha_09_lag_112  sales_ewm_alpha_09_lag_180  sales_ewm_alpha_09_lag_270  sales_ewm_alpha_09_lag_365  sales_ewm_alpha_09_lag_546  sales_ewm_alpha_09_lag_728  sales_ewm_alpha_08_lag_91  sales_ewm_alpha_08_lag_98  sales_ewm_alpha_08_lag_105  sales_ewm_alpha_08_lag_112  \\\n# 0                          NaN                          NaN                          NaN                          NaN                        NaN                        NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN\n# 1                          NaN                          NaN                          NaN                          NaN                        NaN                        NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN\n# 2                          NaN                          NaN                          NaN                          NaN                        NaN                        NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN\n# 3                          NaN                          NaN                          NaN                          NaN                        NaN                        NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN\n# 4                          NaN                          NaN                          NaN                          NaN                        NaN                        NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN\n#    sales_ewm_alpha_08_lag_180  sales_ewm_alpha_08_lag_270  sales_ewm_alpha_08_lag_365  sales_ewm_alpha_08_lag_546  sales_ewm_alpha_08_lag_728  sales_ewm_alpha_07_lag_91  sales_ewm_alpha_07_lag_98  sales_ewm_alpha_07_lag_105  sales_ewm_alpha_07_lag_112  sales_ewm_alpha_07_lag_180  sales_ewm_alpha_07_lag_270  sales_ewm_alpha_07_lag_365  sales_ewm_alpha_07_lag_546  sales_ewm_alpha_07_lag_728  sales_ewm_alpha_05_lag_91  sales_ewm_alpha_05_lag_98  sales_ewm_alpha_05_lag_105  \\\n# 0                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN\n# 1                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN\n# 2                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN\n# 3                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN\n# 4                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN\n#    sales_ewm_alpha_05_lag_112  sales_ewm_alpha_05_lag_180  sales_ewm_alpha_05_lag_270  sales_ewm_alpha_05_lag_365  sales_ewm_alpha_05_lag_546  sales_ewm_alpha_05_lag_728\n# 0                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN\n# 1                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN\n# 2                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN\n# 3                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN\n# 4                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN\n\ndf.shape\n# (958000, 71)\n\n####################\n# One-Hot Encoding #\n####################\n\ndf = pd.get_dummies(df, columns=['store', 'item', 'day_of_week', 'month'])\n\ndf.shape\n# (958000, 146)\n\n####################################\n# Converting sales to log(1+sales) #\n####################################\n\ndf['sales'] = np.log1p(df[\"sales\"].values)\n\ndf.head()\n#         date     sales  id  day_of_month  day_of_year  week_of_year  year  is_wknd  is_month_start  is_month_end  sales_lag_91  sales_lag_98  sales_lag_105  sales_lag_112  sales_lag_119  sales_lag_126  sales_lag_182  sales_lag_364  sales_lag_546  sales_lag_728  sales_roll_mean_365  sales_roll_mean_546  sales_ewm_alpha_095_lag_91  sales_ewm_alpha_095_lag_98  sales_ewm_alpha_095_lag_105  sales_ewm_alpha_095_lag_112  sales_ewm_alpha_095_lag_180  sales_ewm_alpha_095_lag_270  \\\n# 0 2013-01-01  2.639057 NaN             1            1             1  2013        0               1             0           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                  NaN                  NaN                         NaN                         NaN                          NaN                          NaN                          NaN                          NaN\n# 1 2013-01-02  2.484907 NaN             2            2             1  2013        0               0             0           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                  NaN                  NaN                         NaN                         NaN                          NaN                          NaN                          NaN                          NaN\n# 2 2013-01-03  2.708050 NaN             3            3             1  2013        0               0             0           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                  NaN                  NaN                         NaN                         NaN                          NaN                          NaN                          NaN                          NaN\n# 3 2013-01-04  2.639057 NaN             4            4             1  2013        1               0             0           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                  NaN                  NaN                         NaN                         NaN                          NaN                          NaN                          NaN                          NaN\n# 4 2013-01-05  2.397895 NaN             5            5             1  2013        1               0             0           NaN           NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN            NaN                  NaN                  NaN                         NaN                         NaN                          NaN                          NaN                          NaN                          NaN\n#    sales_ewm_alpha_095_lag_365  sales_ewm_alpha_095_lag_546  sales_ewm_alpha_095_lag_728  sales_ewm_alpha_09_lag_91  sales_ewm_alpha_09_lag_98  sales_ewm_alpha_09_lag_105  sales_ewm_alpha_09_lag_112  sales_ewm_alpha_09_lag_180  sales_ewm_alpha_09_lag_270  sales_ewm_alpha_09_lag_365  sales_ewm_alpha_09_lag_546  sales_ewm_alpha_09_lag_728  sales_ewm_alpha_08_lag_91  sales_ewm_alpha_08_lag_98  sales_ewm_alpha_08_lag_105  sales_ewm_alpha_08_lag_112  sales_ewm_alpha_08_lag_180  \\\n# 0                          NaN                          NaN                          NaN                        NaN                        NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN                         NaN\n# 1                          NaN                          NaN                          NaN                        NaN                        NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN                         NaN\n# 2                          NaN                          NaN                          NaN                        NaN                        NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN                         NaN\n# 3                          NaN                          NaN                          NaN                        NaN                        NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN                         NaN\n# 4                          NaN                          NaN                          NaN                        NaN                        NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN                         NaN\n#    sales_ewm_alpha_08_lag_270  sales_ewm_alpha_08_lag_365  sales_ewm_alpha_08_lag_546  sales_ewm_alpha_08_lag_728  sales_ewm_alpha_07_lag_91  sales_ewm_alpha_07_lag_98  sales_ewm_alpha_07_lag_105  sales_ewm_alpha_07_lag_112  sales_ewm_alpha_07_lag_180  sales_ewm_alpha_07_lag_270  sales_ewm_alpha_07_lag_365  sales_ewm_alpha_07_lag_546  sales_ewm_alpha_07_lag_728  sales_ewm_alpha_05_lag_91  sales_ewm_alpha_05_lag_98  sales_ewm_alpha_05_lag_105  sales_ewm_alpha_05_lag_112  \\\n# 0                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN\n# 1                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN\n# 2                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN\n# 3                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN\n# 4                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                         NaN                        NaN                        NaN                         NaN                         NaN\n#    sales_ewm_alpha_05_lag_180  sales_ewm_alpha_05_lag_270  sales_ewm_alpha_05_lag_365  sales_ewm_alpha_05_lag_546  sales_ewm_alpha_05_lag_728  store_1  store_2  store_3  store_4  store_5  store_6  store_7  store_8  store_9  store_10  item_1  item_2  item_3  item_4  item_5  item_6  item_7  item_8  item_9  item_10  item_11  item_12  item_13  item_14  item_15  item_16  item_17  item_18  item_19  item_20  item_21  item_22  item_23  item_24  item_25  item_26  item_27  item_28  item_29  item_30  \\\n# 0                         NaN                         NaN                         NaN                         NaN                         NaN        1        0        0        0        0        0        0        0        0         0       1       0       0       0       0       0       0       0       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0\n# 1                         NaN                         NaN                         NaN                         NaN                         NaN        1        0        0        0        0        0        0        0        0         0       1       0       0       0       0       0       0       0       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0\n# 2                         NaN                         NaN                         NaN                         NaN                         NaN        1        0        0        0        0        0        0        0        0         0       1       0       0       0       0       0       0       0       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0\n# 3                         NaN                         NaN                         NaN                         NaN                         NaN        1        0        0        0        0        0        0        0        0         0       1       0       0       0       0       0       0       0       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0\n# 4                         NaN                         NaN                         NaN                         NaN                         NaN        1        0        0        0        0        0        0        0        0         0       1       0       0       0       0       0       0       0       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0\n#    item_31  item_32  item_33  item_34  item_35  item_36  item_37  item_38  item_39  item_40  item_41  item_42  item_43  item_44  item_45  item_46  item_47  item_48  item_49  item_50  day_of_week_0  day_of_week_1  day_of_week_2  day_of_week_3  day_of_week_4  day_of_week_5  day_of_week_6  month_1  month_2  month_3  month_4  month_5  month_6  month_7  month_8  month_9  month_10  month_11  month_12\n# 0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0              0              1              0              0              0              0              0        1        0        0        0        0        0        0        0        0         0         0         0\n# 1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0              0              0              1              0              0              0              0        1        0        0        0        0        0        0        0        0         0         0         0\n# 2        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0              0              0              0              1              0              0              0        1        0        0        0        0        0        0        0        0         0         0         0\n# 3        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0              0              0              0              0              1              0              0        1        0        0        0        0        0        0        0        0         0         0         0\n# 4        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0              0              0              0              0              0              1              0        1        0        0        0        0        0        0        0        0         0         0         0\n\n#########\n# Model #\n#########\n\n########################\n# Custom Cost Function #\n########################\n\n# LightGBM'i optimize ederken iterasyonlarda bakacak olduğumuz şey loss fonksiyondu.\n# MSE: Mean Squared Error\n# RMSE: Root Mena Squared Error\n# MAE: Mean Absolute Error\n# MAPE: Mean Absolute Percentage Error\n# SMAPE: Symmetric Mean Absolute Percentage Error (Adjusted MAPE)\n\n# Gerçek değerlerle, tahmin edilen değerleri kıyaslar, sonrasında bunların mutlak değerini alır ve bunları\n# yüzdelik forma çevirir.\ndef smape(preds, target):\n    n = len(preds)\n    masked_arr = ~((preds == 0) & (target == 0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds - target)\n    denom = np.abs(preds) + np.abs(target)\n    smape_val = (200 * np.sum(num / denom)) / n\n    return smape_val\n\n# Daha öncesinde logaritmik bir dönüşüm yapmıştık. Bu logaritmik dönüşümü geri alacak şekilde MAPE fonksiyonuna\n# bunu uygulayacağız. LightGBM'in anlayabileceği bir şekilde optimize ediyoruz.\ndef lgbm_smape(preds, train_data):\n    labels = train_data.get_label()\n    smape_val = smape(np.expm1(preds), np.expm1(labels))\n    return 'SMAPE', smape_val, False\n\n##############################\n# Time-Based Validation Sets #\n##############################\n\n# Beklenen test tesi 2018'in ilk 3 ayı elimizde de 2017'nin son ayına kadarlık bir eğitim veri setimiz var.\n# 2017'nin son üç ayı bizden beklenen örüntüyle örtüşür mü örtüşmez mi bilemiyoruz dolayısıyla aynı örüntüyü\n# yakalayabileceğimiz başka bir noktaya gitmek istiyoruz. Buradaki tercihimiz doğal olarak 2017'nin ilk 3 ayını\n# validasyon seti olarak ayırıyoruz.\n\ntest.head()\n#    id       date  store  item\n# 0   0 2018-01-01      1     1\n# 1   1 2018-01-02      1     1\n# 2   2 2018-01-03      1     1\n# 3   3 2018-01-04      1     1\n# 4   4 2018-01-05      1     1\n\ntest.tail()\n#           id       date  store  item\n# 44995  44995 2018-03-27     10    50\n# 44996  44996 2018-03-28     10    50\n# 44997  44997 2018-03-29     10    50\n# 44998  44998 2018-03-30     10    50\n# 44999  44999 2018-03-31     10    50\n\ntrain[\"date\"].min(), train[\"date\"].max()\n# (Timestamp('2013-01-01 00:00:00'), Timestamp('2017-12-31 00:00:00'))\n\ntest[\"date\"].min(), test[\"date\"].max()\n# (Timestamp('2018-01-01 00:00:00'), Timestamp('2018-03-31 00:00:00'))\n\n# 2017'nin başına kadar (2016'nın sonuna kadar) train seti.\ntrain = df.loc[(df[\"date\"] < \"2017-01-01\"), :]\n\n# 2017'nin ilk 3 ayını validasyon seti yapıyoruz.\nval = df.loc[(df[\"date\"] >= \"2017-01-01\") & (df[\"date\"] < \"2017-04-01\"), :]\n\n# Bağımsız değişkenlerimizi cols adında bir değişkenin içine koyuyoruz. \"date\", \"id\", \"sales\", \"year\" değişkenleri\n# ile bir işimiz yok.\ncols = [col for col in train.columns if col not in ['date', 'id', \"sales\", \"year\"]]\n\nY_train = train[\"sales\"]\nX_train = train[cols]\n\nY_val = val[\"sales\"]\nX_val = val[cols]\n\n##################\n# LightGBM Model #\n##################\n\n# \"num_leaves\" yaprak sayısını belirtir.\n# \"learning_rate\" öğrenme oranıdır, shrinkage_rate, eta\n# \"feature_fraction\" her iterasyonda gözlemlerin belirli bir oranda mı yoksa hepsini mi göz önünde bulunduralım parametresidir.\n#  rf'nin random subspace özelliği. her iterasyonda rastgele göz önünde bulundurulacak değişken sayısı.\n# \"max_depth\" maximum derinlik.\n# \"verbose\" kaç adımda bir raporlama yapması gerektiğini belirtiyoruz.\n# \"num_boost_round\" n estimators demektir, number of boosting iterations. En az 10000-15000 civarı yapmak lazım.\n# \"early_stopping_round\" validasyon setindeki metrik belirli bir early_stopping_rounds'da ilerlemiyorsa yani\n# hata düşmüyorsa modellemeyi durdur.\n# \"nthread\" işlemcilerin kullanımı ile alakalı. -1 dediğimizde işlemcilerin hepsini kullanır.\n# Hem train süresini kısaltır hem de overfit'e engel olur.\n# LightGBM parameters\n\n# metric mae: l1, absolute loss, mean_absolute_error, regression_l1\n# l2, square loss, mean_squared_error, mse, regression_l2, regression\n# rmse, root square loss, root_mean_squared_error, l2_root\n# mape, MAPE loss, mean_absolute_percentage_error\n\nlgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'num_boost_round': 1000,\n              'early_stopping_rounds': 200,\n              'nthread': -1}\n\nlgbtrain = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\n# Data bölümüne bağımsız değişkenleri label bölümüne bağımlı değişkeni, feature_name argümanına da bağımsız değişkenlerin\n# isimlerini giriyoruz.\n\nlgbval = lgb.Dataset(data=X_val, label=Y_val, reference=lgbtrain, feature_name=cols)\n# Validasyon seti\n\n\n# feval argümanı custom cost function'ı belirtir. Mean Absolute Error değerini yüzdelik bir hata oranı olarak göstermesini belirttik.\nmodel = lgb.train(lgb_params, lgbtrain,\n                  valid_sets=[lgbtrain, lgbval],\n                  num_boost_round=lgb_params[\"num_boost_round\"],\n                  early_stopping_rounds=lgb_params[\"early_stopping_rounds\"],\n                  feval=lgbm_smape,\n                  verbose_eval=100)\n\n# [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016832 seconds.\n# You can set `force_row_wise=true` to remove the overhead.\n# And if memory is not enough, you can set `force_col_wise=true`.\n# Training until validation scores don't improve for 200 rounds\n# [100]\ttraining's l1: 0.172546\ttraining's SMAPE: 17.5961\tvalid_1's l1: 0.171318\tvalid_1's SMAPE: 17.5098\n# [200]\ttraining's l1: 0.142145\ttraining's SMAPE: 14.5582\tvalid_1's l1: 0.145245\tvalid_1's SMAPE: 14.8992\n# [300]\ttraining's l1: 0.136584\ttraining's SMAPE: 14.0017\tvalid_1's l1: 0.140459\tvalid_1's SMAPE: 14.4192\n# [400]\ttraining's l1: 0.134453\ttraining's SMAPE: 13.7893\tvalid_1's l1: 0.138927\tvalid_1's SMAPE: 14.2659\n# [500]\ttraining's l1: 0.133125\ttraining's SMAPE: 13.657\tvalid_1's l1: 0.137614\tvalid_1's SMAPE: 14.1343\n# [600]\ttraining's l1: 0.132192\ttraining's SMAPE: 13.5638\tvalid_1's l1: 0.136639\tvalid_1's SMAPE: 14.0362\n# [700]\ttraining's l1: 0.131487\ttraining's SMAPE: 13.4933\tvalid_1's l1: 0.135878\tvalid_1's SMAPE: 13.9596\n# Did not meet early stopping. Best iteration is:\n# [1000]\ttraining's l1: 0.130023\ttraining's SMAPE: 13.3466\tvalid_1's l1: 0.134515\tvalid_1's SMAPE: 13.8224\n\n# Note : LightGBM'in en önemli hiperparametre optimizasyonu num_boost_round'dur.\n\ny_pred_val = model.predict(X_val, num_iteration=model.best_iteration)\n\n# Daha öncesinde modele sokulan değerlerin logaritması alınarak modele sokulmuştu. Şimdi bunların tersini alıyoruz.\nsmape(np.expm1(y_pred_val), np.expm1(Y_val))\n# 13.822393276127935 hatamız var.\n\n######################\n# Feature Importance #\n######################\n\ndef plot_lgb_importances(model, plot=False, num=10):\n\n    gain = model.feature_importance('gain')\n    feat_imp = pd.DataFrame({'feature': model.feature_name(),\n                             'split': model.feature_importance('split'),\n                             'gain': 100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n    if plot:\n        plt.figure(figsize=(10, 10))\n        sns.set(font_scale=1)\n        sns.barplot(x=\"gain\", y=\"feature\", data=feat_imp[0:25])\n        plt.title('feature')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(feat_imp.head(num))\n\n# 30 değişkeni göstermek için num parametresine 30 değerini girdik.\nplot_lgb_importances(model, num=30)\n#                          feature  split       gain\n# 17           sales_roll_mean_546    920  54.288368\n# 13                 sales_lag_364   1259  13.192058\n# 16           sales_roll_mean_365    632   9.891184\n# 60    sales_ewm_alpha_05_lag_365    364   4.960250\n# 18    sales_ewm_alpha_095_lag_91     78   2.759355\n# 1                    day_of_year    768   2.117360\n# 54     sales_ewm_alpha_05_lag_91     84   1.866880\n# 3                        is_wknd    229   1.215430\n# 123                day_of_week_0    242   1.175220\n# 141                     month_12    305   1.116339\n# 2                   week_of_year    298   0.973359\n# 36     sales_ewm_alpha_08_lag_91     14   0.906002\n# 6                   sales_lag_91     89   0.845151\n# 27     sales_ewm_alpha_09_lag_91     36   0.548153\n# 7                   sales_lag_98     21   0.501067\n# 62    sales_ewm_alpha_05_lag_728    391   0.388387\n# 59    sales_ewm_alpha_05_lag_270    192   0.353259\n# 53    sales_ewm_alpha_07_lag_728     72   0.350212\n# 44    sales_ewm_alpha_08_lag_728     26   0.348790\n# 51    sales_ewm_alpha_07_lag_365     51   0.234604\n# 35    sales_ewm_alpha_09_lag_728     17   0.148732\n# 12                 sales_lag_182    119   0.141630\n# 136                      month_7    132   0.106572\n# 19    sales_ewm_alpha_095_lag_98     10   0.095046\n# 130                      month_1    102   0.093066\n# 129                day_of_week_6     80   0.086785\n# 45     sales_ewm_alpha_07_lag_91     16   0.085298\n# 26   sales_ewm_alpha_095_lag_728      6   0.078847\n# 126                day_of_week_3    112   0.061916\n# 28     sales_ewm_alpha_09_lag_98      5   0.046419\n\n# split argümanı değişkenin ağaç yöntemlerinde kaç bölme işleminde kullanıldığını göstermektedir. gain argümanı ise\n# değişkenin modeli tahmin etmesinde ne kadar kazanç sağladığını göstermektedir.\n\nplot_lgb_importances(model, num=30, plot=True)\n\nlgb.plot_importance(model, max_num_features=20, figsize=(10, 10), importance_type=\"gain\")\nplt.show()\n\n###############\n# Final Model #\n###############\n\n# NA olmayan sales değerleri train setine karşılık gelir.\ntrain = df.loc[~df.sales.isna()]\n\nY_train = train[\"sales\"]\nX_train = train[cols]\n\ntest = df.loc[df.sales.isna()]\nX_test = test[cols]\n\n# dataleakage\n# tüm veride age 0-1 minmax dönüşümü\n# train - test\n\nlgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'nthread': -1,\n              \"num_boost_round\": model.best_iteration}\n\nlgbtrain_all = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\n\nmodel = lgb.train(lgb_params, lgbtrain_all, num_boost_round=model.best_iteration)\n# [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016028 seconds.\n# You can set `force_row_wise=true` to remove the overhead.\n# And if memory is not enough, you can set `force_col_wise=true`.\n\ntest_preds = model.predict(X_test, num_iteration=model.best_iteration)\n\nsubmission_df = test.loc[:, [\"id\", \"sales\"]]\nsubmission_df[\"sales\"] = np.expm1(test_preds)\nsubmission_df[\"id\"] = submission_df.id.astype(int)\nsubmission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df.head(20)\n#     id      sales\n# 0    0  11.784425\n# 1    1  14.145350\n# 2    2  13.632879\n# 3    3  14.270368\n# 4    4  17.758245\n# 5    5  18.004468\n# 6    6  19.996335\n# 7    7  13.258884\n# 8    8  14.792087\n# 9    9  14.818561\n# 10  10  15.148537\n# 11  11  16.874620\n# 12  12  15.801704\n# 13  13  17.523760\n# 14  14  12.808774\n# 15  15  15.357608\n# 16  16  13.822296\n# 17  17  15.709829\n# 18  18  17.256970\n# 19  19  18.706011","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:34:02.058297Z","iopub.execute_input":"2022-02-14T14:34:02.058609Z","iopub.status.idle":"2022-02-14T14:38:23.550023Z","shell.execute_reply.started":"2022-02-14T14:34:02.058557Z","shell.execute_reply":"2022-02-14T14:38:23.54905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}