{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Demand Forecasting using LSTM\n---\n## Problem Statement ~\n>Every retailer must stay on top of planning activity to stand the demand of goods based on needs. A highly accurate demand forecast is the only way retailers can predict which goods are needed for each store location. This will also ensure high availability for customers while maintaining minimal stock risk and support capacity management, store staff labour force planning, etc. <br>\nThe project will use LSTM, which is very suitable for handling time-series data and widely\nused for forecasting purposes.\n\n## Dataset\n> The dataset for this project is available on Kaggle. \n\n**Link:** https://www.kaggle.com/c/demand-forecasting-kernels-only/data?select=train.csv\n\n### Table of ContentsÂ¶\n#### 1. Environment Setup\n#### 2. Dataset Gathering\n#### 3. Exploratory Data Analysis\n#### 4. Dataset Preprocessing\n#### 5. Model Evaluation\n#### 6. Performance Measurement","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# 1. Environment Setup:\n---\n> In this step, we have installed and imported all neccessary libraries required to proceed with the solution to the given problem statement.","metadata":{}},{"cell_type":"code","source":"import math\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:48.506438Z","iopub.execute_input":"2022-06-16T13:58:48.506816Z","iopub.status.idle":"2022-06-16T13:58:50.743537Z","shell.execute_reply.started":"2022-06-16T13:58:48.506718Z","shell.execute_reply":"2022-06-16T13:58:50.74259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Dataset Gathering\n---\n> In this step, we have gathered the dataset from kaggle and have verified its integrity.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/demand-forecasting-kernels-only/train.csv\")\ntest =  pd.read_csv(\"../input/demand-forecasting-kernels-only/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:50.745782Z","iopub.execute_input":"2022-06-16T13:58:50.74612Z","iopub.status.idle":"2022-06-16T13:58:51.055182Z","shell.execute_reply.started":"2022-06-16T13:58:50.746074Z","shell.execute_reply":"2022-06-16T13:58:51.054226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Exploratory Data Analysis\n---\n> In this step, we took a deeper look at the data, and checked if the data is properly gathered in the previous steps.","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:51.05664Z","iopub.execute_input":"2022-06-16T13:58:51.056953Z","iopub.status.idle":"2022-06-16T13:58:51.073647Z","shell.execute_reply.started":"2022-06-16T13:58:51.056912Z","shell.execute_reply":"2022-06-16T13:58:51.072953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:51.07583Z","iopub.execute_input":"2022-06-16T13:58:51.076105Z","iopub.status.idle":"2022-06-16T13:58:51.085655Z","shell.execute_reply.started":"2022-06-16T13:58:51.07607Z","shell.execute_reply":"2022-06-16T13:58:51.084727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:51.087044Z","iopub.execute_input":"2022-06-16T13:58:51.087332Z","iopub.status.idle":"2022-06-16T13:58:51.172608Z","shell.execute_reply.started":"2022-06-16T13:58:51.087298Z","shell.execute_reply":"2022-06-16T13:58:51.171899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:51.174223Z","iopub.execute_input":"2022-06-16T13:58:51.174665Z","iopub.status.idle":"2022-06-16T13:58:51.198365Z","shell.execute_reply.started":"2022-06-16T13:58:51.174626Z","shell.execute_reply":"2022-06-16T13:58:51.197631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Min date from test set: %s' % train['date'].min())\nprint('Max date from test set: %s' % train['date'].max())\nimport datetime\nlag_size = len(test['date'].unique())\nprint('Forecast lag size: ', lag_size)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:51.199479Z","iopub.execute_input":"2022-06-16T13:58:51.199773Z","iopub.status.idle":"2022-06-16T13:58:51.449395Z","shell.execute_reply.started":"2022-06-16T13:58:51.19973Z","shell.execute_reply":"2022-06-16T13:58:51.448634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"daily_sales = train.groupby('date', as_index=False)['sales'].sum()\nprint(daily_sales)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:51.450823Z","iopub.execute_input":"2022-06-16T13:58:51.451079Z","iopub.status.idle":"2022-06-16T13:58:51.549188Z","shell.execute_reply.started":"2022-06-16T13:58:51.451043Z","shell.execute_reply":"2022-06-16T13:58:51.548352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"daily_sales=daily_sales.reset_index()['sales']\nprint(daily_sales)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:51.55051Z","iopub.execute_input":"2022-06-16T13:58:51.550801Z","iopub.status.idle":"2022-06-16T13:58:51.560201Z","shell.execute_reply.started":"2022-06-16T13:58:51.550763Z","shell.execute_reply":"2022-06-16T13:58:51.55934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Overall Daily Sales\n> In this step, we have aggregated the sales value and grouped it by date before finally plotting it.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,13))\nplt.plot(daily_sales, linewidth=1)\nplt.xlabel('Date')\nplt.ylabel('Sales')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:51.564008Z","iopub.execute_input":"2022-06-16T13:58:51.564459Z","iopub.status.idle":"2022-06-16T13:58:51.894948Z","shell.execute_reply.started":"2022-06-16T13:58:51.564409Z","shell.execute_reply":"2022-06-16T13:58:51.894207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Daily Sales by Store\n> In this step, we have taken sub-tables for each store, and then group their sales values by the date. Finally, we have plotted the graph for the sales value for each individual store.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,13))\nlegend = []\nfor i in range(10):\n    store_sales=train.loc[train['store'] == i]\n    store_sales=store_sales.groupby('date', as_index=False)['sales'].sum()\n    store_sales=store_sales.reset_index()['sales']\n    plt.plot(store_sales, linewidth=1)    \n    legend.append(('Store '+str(i+1)))\nplt.xlabel('Date')\nplt.ylabel('Sales')\nplt.legend(legend, loc='upper left', ncol=1, fancybox=True, shadow=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:51.89592Z","iopub.execute_input":"2022-06-16T13:58:51.896203Z","iopub.status.idle":"2022-06-16T13:58:52.575154Z","shell.execute_reply.started":"2022-06-16T13:58:51.896165Z","shell.execute_reply":"2022-06-16T13:58:52.574533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Daily Sales by Item\n> In this step, we have taken sub-tables for each item, and then group their sales values by the date. Finally, we have plotted the graph for the sales value for each individual item.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,13))\nlegend = []\nfor i in range(50):\n    item_sales=train.loc[train['item'] == i]\n    item_sales=item_sales.groupby('date', as_index=False)['sales'].sum()\n    item_sales=item_sales.reset_index()['sales']\n    plt.plot(item_sales, linewidth=1)    \n    legend.append(('Item '+str(i+1)))\nplt.xlabel('Sales')\nplt.ylabel('Date')\nplt.legend(legend, loc='upper right', ncol=1, bbox_to_anchor=[1.005, 1.04], fancybox=True, shadow=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:52.576249Z","iopub.execute_input":"2022-06-16T13:58:52.5767Z","iopub.status.idle":"2022-06-16T13:58:54.271156Z","shell.execute_reply.started":"2022-06-16T13:58:52.576639Z","shell.execute_reply":"2022-06-16T13:58:54.269979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Data Preprocessing:\n---\n> In this step, we have cleaned the data thus obtained for the previous steps before splitting them into training and testing datasets.","metadata":{}},{"cell_type":"markdown","source":"#### Sub-sampling training set to get only the last year of data and reduce training time\n> In this step, we have sub-sampled the training set to only look at last year's data to reduce our training time.","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:54.272482Z","iopub.execute_input":"2022-06-16T13:58:54.272909Z","iopub.status.idle":"2022-06-16T13:58:54.283735Z","shell.execute_reply.started":"2022-06-16T13:58:54.272874Z","shell.execute_reply":"2022-06-16T13:58:54.282829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train[(train['date'] >= '2017-01-01')]\ntrain_gp = train.sort_values('date').groupby(['item', 'store', 'date'], as_index=False)\ntrain_gp = train_gp.agg({'sales':['mean']})\ntrain_gp.columns = ['item', 'store', 'date', 'sales']\ntrain_gp.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:54.285046Z","iopub.execute_input":"2022-06-16T13:58:54.285698Z","iopub.status.idle":"2022-06-16T13:58:54.771807Z","shell.execute_reply.started":"2022-06-16T13:58:54.285651Z","shell.execute_reply":"2022-06-16T13:58:54.770944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gp","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:54.773218Z","iopub.execute_input":"2022-06-16T13:58:54.773688Z","iopub.status.idle":"2022-06-16T13:58:54.789366Z","shell.execute_reply.started":"2022-06-16T13:58:54.773633Z","shell.execute_reply":"2022-06-16T13:58:54.788568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Transforming the data into a time series problem\n\n> In this step, we have tranformed the data into a time series problem so that we can take into account a portion of it and use that to look into the future.","metadata":{}},{"cell_type":"code","source":"def series_to_supervised(data, window=1, lag=1, dropnan=True):\n    cols, names = list(), list()\n    # Input sequence (t-n, ... t-1)\n    for i in range(window, 0, -1):\n        cols.append(data.shift(i))\n        names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n    # Current timestep (t=0)\n    cols.append(data)\n    names += [('%s(t)' % (col)) for col in data.columns]\n    # Target timestep (t=lag)\n    cols.append(data.shift(-lag))\n    names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # Drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:54.790548Z","iopub.execute_input":"2022-06-16T13:58:54.790816Z","iopub.status.idle":"2022-06-16T13:58:54.799212Z","shell.execute_reply.started":"2022-06-16T13:58:54.790778Z","shell.execute_reply":"2022-06-16T13:58:54.798492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Using the current timestep and the last 29 days to forecast 90 days ahead\n\n> In this step, we are utilising the current timestep and the last 29 days, to forecast 90 days into the future.","metadata":{}},{"cell_type":"code","source":"window = 29\nlag = lag_size\nseries = series_to_supervised(train_gp.drop('date', axis=1), window=window, lag=lag)\nseries.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:54.800432Z","iopub.execute_input":"2022-06-16T13:58:54.801184Z","iopub.status.idle":"2022-06-16T13:58:55.211958Z","shell.execute_reply.started":"2022-06-16T13:58:54.801145Z","shell.execute_reply":"2022-06-16T13:58:55.210773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Dropping rows with different item/store values other than the shifted columns\n> In this step, we have dropped any item values or store values which are different from the shifted columns.","metadata":{}},{"cell_type":"code","source":"last_item = 'item(t-%d)' % window\nlast_store = 'store(t-%d)' % window\nseries = series[(series['store(t)'] == series[last_store])]\nseries = series[(series['item(t)'] == series[last_item])]","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:55.2137Z","iopub.execute_input":"2022-06-16T13:58:55.214068Z","iopub.status.idle":"2022-06-16T13:58:55.297098Z","shell.execute_reply.started":"2022-06-16T13:58:55.214015Z","shell.execute_reply":"2022-06-16T13:58:55.296313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing unnecessary columns\ncolumns_to_drop = [('%s(t+%d)' % (col, lag)) for col in ['item', 'store']]\nfor i in range(window, 0, -1):\n    columns_to_drop += [('%s(t-%d)' % (col, i)) for col in ['item', 'store']]\nseries.drop(columns_to_drop, axis=1, inplace=True)\nseries.drop(['item(t)', 'store(t)'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:55.298422Z","iopub.execute_input":"2022-06-16T13:58:55.298812Z","iopub.status.idle":"2022-06-16T13:58:55.323705Z","shell.execute_reply.started":"2022-06-16T13:58:55.29877Z","shell.execute_reply":"2022-06-16T13:58:55.322849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Splitting the dataset into Training and Testing set\n> In this step, we have splitted the datset into training and testing sets, for further development.","metadata":{}},{"cell_type":"code","source":"labels_col = 'sales(t+%d)' % lag_size\nlabels = series[labels_col]\nseries = series.drop(labels_col, axis=1)\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(series, labels.values, test_size=0.4, random_state=42)\nprint('Train set shape', X_train.shape)\nprint('Validation set shape', X_valid.shape)\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:55.325227Z","iopub.execute_input":"2022-06-16T13:58:55.325939Z","iopub.status.idle":"2022-06-16T13:58:55.412267Z","shell.execute_reply.started":"2022-06-16T13:58:55.325899Z","shell.execute_reply":"2022-06-16T13:58:55.411479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_series = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\nX_valid_series = X_valid.values.reshape((X_valid.shape[0], X_valid.shape[1], 1))\nprint('Train set shape', X_train_series.shape)\nprint('Validation set shape', X_valid_series.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:55.41376Z","iopub.execute_input":"2022-06-16T13:58:55.414742Z","iopub.status.idle":"2022-06-16T13:58:55.422999Z","shell.execute_reply.started":"2022-06-16T13:58:55.414665Z","shell.execute_reply":"2022-06-16T13:58:55.421988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Model Evaluation:\n---\n> In this step, we have chosen LSTM layers for our model as it poses the most performance in problems such as these, where even a small amount of data can provide a lot of insight to the model. The LSTM model actually sees the input data as a sequence, so it's able to learn patterns from sequenced data (assuming it exists) better than the other ones, especially patterns from long sequences.","metadata":{}},{"cell_type":"code","source":"model_lstm = Sequential()\nmodel_lstm.add(LSTM(50, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\nmodel_lstm.add(Dense(1))\nmodel_lstm.compile(loss='mse', optimizer='adam')\nmodel_lstm.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:59:16.182255Z","iopub.execute_input":"2022-06-16T13:59:16.182529Z","iopub.status.idle":"2022-06-16T13:59:16.2645Z","shell.execute_reply.started":"2022-06-16T13:59:16.182498Z","shell.execute_reply":"2022-06-16T13:59:16.263728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_lstm.compile(optimizer='adam', loss='mse')","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:59:30.638434Z","iopub.execute_input":"2022-06-16T13:59:30.63904Z","iopub.status.idle":"2022-06-16T13:59:30.65046Z","shell.execute_reply.started":"2022-06-16T13:59:30.638984Z","shell.execute_reply":"2022-06-16T13:59:30.649687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm_history = model_lstm.fit(X_train_series, Y_train, validation_data=(X_valid_series, Y_valid), epochs=40, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:59:36.367665Z","iopub.execute_input":"2022-06-16T13:59:36.367947Z","iopub.status.idle":"2022-06-16T15:43:59.1844Z","shell.execute_reply.started":"2022-06-16T13:59:36.367915Z","shell.execute_reply":"2022-06-16T15:43:59.183512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Performance Measurement\n---\n> In this step, we have evaluated the performance measure of the model.","metadata":{}},{"cell_type":"code","source":"plt.plot(lstm_history.history['loss'])\nplt.plot(lstm_history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train Loss', 'Validation Loss'], fancybox=True, shadow=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:45:39.058051Z","iopub.execute_input":"2022-06-16T15:45:39.058323Z","iopub.status.idle":"2022-06-16T15:45:39.250289Z","shell.execute_reply.started":"2022-06-16T15:45:39.058294Z","shell.execute_reply":"2022-06-16T15:45:39.249535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Performance on Testing Data\n> In this step, we have utilised the test data and made the model predict the values, to validate its performance.","metadata":{}},{"cell_type":"code","source":"# Predicting the prices\npredicted_sales = model_lstm.predict(X_valid_series)\n\n# # We flatten the 2 dimensional array so we can plot it with matplotlib\nY_valid = Y_valid.flatten()\npredicted_sales = predicted_sales.flatten()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:46:01.894367Z","iopub.execute_input":"2022-06-16T15:46:01.894626Z","iopub.status.idle":"2022-06-16T15:46:12.30122Z","shell.execute_reply.started":"2022-06-16T15:46:01.894596Z","shell.execute_reply":"2022-06-16T15:46:12.300444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(Y_valid, color='black', label=f\"Actual Sales\")\nplt.plot(predicted_sales, color= 'green', label=\"Predicted Sales\")\nplt.title(\"Sales vs Predicted Sales\")\nplt.xlabel(\"Days in test period\")\nplt.ylabel(\"Price\")\nplt.legend(fancybox=True, shadow=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:46:36.889835Z","iopub.execute_input":"2022-06-16T15:46:36.890313Z","iopub.status.idle":"2022-06-16T15:46:37.627276Z","shell.execute_reply.started":"2022-06-16T15:46:36.890271Z","shell.execute_reply":"2022-06-16T15:46:37.626564Z"},"trusted":true},"execution_count":null,"outputs":[]}]}