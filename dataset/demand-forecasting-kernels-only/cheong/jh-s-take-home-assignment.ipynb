{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\nThis is my first ever Kaggle Project. As I am only a novice in time-series forecasting and Python Programming, I have relied on a variety of sources for this project. Please let me know of any mistakes I have made and also any good resources I can refer to for me to improve \n\nTask: Given 5 years of store-item sales data, predict 3 months of sales for 50 different items at 10 different stores.\n\nThe arrangement of my kaggle kernel is as follows\n\n1) Preliminary Data Analysis\n\n2) ARIMA Model\n\n3) SARIMA Model  \n\n4) Final thoughts and conclusion ","metadata":{}},{"cell_type":"code","source":"import pandas as pd   #data analysis\nimport numpy as np\nimport matplotlib.pyplot as plt #graph visualisation\nfrom datetime import datetime\nfrom pandas import Series \nimport seaborn as sns  #graph visualisation \n%matplotlib inline\n\nsns.set_style(\"darkgrid\")\nsns.axes_style(\"darkgrid\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# for accuracy and error calculation\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom statsmodels.tsa.seasonal import seasonal_decompose #for conducting stationarity analysis \n# for conducting ARIMA and SARIMA models \nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nimport time \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the datasets","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv(\"../input/demand-forecasting-kernels-only/train.csv\",parse_dates=True,index_col=['date'])\ntest=pd.read_csv(\"../input/demand-forecasting-kernels-only/test.csv\",parse_dates=True,index_col=['date'])\n\n\n# train['date'] = pd.to_datetime(train['date'], format=\"%Y-%m-%d\")\n\n# train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape,test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df = train_df.set_index('date')\ntrain['year'] = train.index.year\ntrain['month'] = train.index.month\ntrain['day'] = train.index.day\ntrain['day_of_week'] = train.index.dayofweek\n\ntrain.head()\n# Monday=0 and Sunday= 6","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['year'] = test.index.year\ntest['month'] = test.index.month\ntest['day'] = test.index.day\ntest['day_of_week'] = test.index.dayofweek\n\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PRELIMINARY DATA ANALYSIS ","metadata":{}},{"cell_type":"code","source":"# sns.lineplot(x=train.index, y=\"sales\",legend = 'full' , data=train[:28])\ntrain['sales'].plot(figsize=(10,8))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A generally increasing trend of total sales can be observed from the graph above, with evidence of a pattern for every year. \n\nWe observe that sales usually peak during the middle of the year,around the June and July period, before decreasing in the second half of the year ","metadata":{}},{"cell_type":"markdown","source":"## Breakdown of sales for each store","metadata":{}},{"cell_type":"code","source":"store_count= len(train['store'].unique())\n# 10 stores in dataset\nfig,axes = plt.subplots(store_count,figsize=(12,13))\n# use a for loop to iterate through all 10 stores and plot the graph of resampled \n# total weekly sales data for each store \nfor i in train['store'].unique():\n    g= train.loc[train['store']==i,'sales'].resample('W').sum()\n    ax= g.plot(ax=axes[i-1])\n    ax.set_ylabel('sales')\n    ax.set_xlabel('year')\nfig.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above graph series further proves our point that the sales have a seasonal pattern. We can see that all stores have similar trends through the duration of the data, albeit with minor differences. With all stores having a seasonal pattern, we can seek to pool them together for our analysis  ","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x=\"day_of_week\", y=\"sales\", data=train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems like Sunday has the highest median sales among all the days ","metadata":{}},{"cell_type":"markdown","source":"## Sales by Store","metadata":{}},{"cell_type":"code","source":"# plot graph of sales over the 5 years\ngraph_sales= sns.FacetGrid(train,col='store',col_order=[1,2,3,4,5,6,7,8,9,10],col_wrap=2)\ngraph_sales.map_dataframe(sns.barplot,\"year\",\"sales\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graph, we can observe that there is a general increasing trend in sales for each store from 2013 to 2017 ","metadata":{}},{"cell_type":"markdown","source":"## Mean Sales by Store","metadata":{}},{"cell_type":"code","source":"# plot mean sales by store  \noverall_sales_by_store= train[['sales','store']].groupby(['store']).mean().plot.bar(figsize=(10,8))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Stores 2 and 8 has the highest mean sales, which could be due to to a variety of reasons, such as being located in an area with heavy customer traffic, or due to better customer services provided","metadata":{}},{"cell_type":"markdown","source":"## Mean Sales by Store by month","metadata":{}},{"cell_type":"code","source":"# Plot average Sales by month for all stores \ntrain[['sales','month']].groupby(['month']).mean().plot.bar(figsize=(10,8))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graph, we can observe that there is general increasing mean sales trend, peaking in the month of July before decreasing for the second half of the year onwards. The top 3 months of Sales are:  July, June and August. \n\nIt is likely that the summer period may be a period of major discounts,such as Summer Sales, or that it is the tourist peak season that could account for the highest sales figures during this period \n\nIt is also worthy to see that there is an increase of sales for all stores in the month of November, which could be due to the store running promotions and campaigns such as Black Friday. \n","metadata":{}},{"cell_type":"markdown","source":"## Plotting relative sales per year for both stores and items ","metadata":{}},{"cell_type":"code","source":"store_sales_trend_year=pd.pivot_table(train,index=\"year\",columns='store',values='sales',aggfunc='mean').values\nplt.figure(figsize=(12,6))\nplt.subplot(1,2,1)\nplt.plot(store_sales_trend_year/store_sales_trend_year.mean(0)[np.newaxis])\nplt.xlabel(\"Year\")\nplt.ylabel(\"relative sales\")\nplt.title(\" store \")\nplt.subplot(1,2,2)\n\nitem_sales_trend_year=pd.pivot_table(train,index=\"year\",columns='item',values='sales',aggfunc='mean').values\nplt.plot(item_sales_trend_year/item_sales_trend_year.mean(0)[np.newaxis])\nplt.xlabel(\"Year\")\nplt.ylabel(\"relative sales\")\nplt.title(\"items\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both stores and items experienced a similar growth trend over the years","metadata":{}},{"cell_type":"markdown","source":"## Relative sales per month for both stores and items","metadata":{}},{"cell_type":"code","source":"store_sales_trend_month=pd.pivot_table(train,index=\"month\",columns='store',values='sales',aggfunc='mean').values\nplt.figure(figsize=(12,6))\nplt.subplot(1,2,1)\nplt.plot(store_sales_trend_month/store_sales_trend_month.mean(0)[np.newaxis])\nplt.xlabel(\"month\")\nplt.ylabel(\"relative sales\")\nplt.title(\" store \")\nplt.subplot(1,2,2)\n\nitem_sales_trend_month=pd.pivot_table(train,index=\"month\",columns='item',values='sales',aggfunc='mean').values\nplt.plot(item_sales_trend_month/item_sales_trend_month.mean(0)[np.newaxis])\nplt.xlabel(\"month\")\nplt.ylabel(\"relative sales\")\nplt.title(\"items\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As with relative sales per year, both store and item sales follow similar trends over the months, with sales increasing during the first half of the year (peaking around June,before decreasing  in the second half)","metadata":{}},{"cell_type":"code","source":"item_sales_trend_day = pd.pivot_table(train, index='day_of_week', columns='item',\n                              values='sales', aggfunc='mean').values\nstore_sales_trend_day = pd.pivot_table(train, index='day_of_week', columns='store',\n                               values='sales', aggfunc='mean').values\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1,2,1)\nplt.plot(store_sales_trend_day / store_sales_trend_day.mean(0)[np.newaxis])\nplt.title(\"Items\")\nplt.xlabel(\"Day of Week\")\nplt.ylabel(\"Relative Sales\")\n\n\nplt.subplot(1,2,2)\nplt.plot(item_sales_trend_day /item_sales_trend_day.mean(0)[np.newaxis])\nplt.title(\"Stores\")\nplt.xlabel(\"Day of Week\")\nplt.ylabel(\"Relative Sales\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graph above, we can see that items and stores seem to have a common pattern over the days of the week.","metadata":{}},{"cell_type":"markdown","source":"From the preliminary analysis of the train data set, we can observe that all stores show similar trends and seasonality in the years,albeit with some difference in sales levels. \n\nThe data looks to be additive in nature due to progressive increment of sales volume . For our time-series forecasting, we will focus our analysis on 1 store-item pair.","metadata":{}},{"cell_type":"markdown","source":"# MODEL BUILDING","metadata":{}},{"cell_type":"markdown","source":"For Time-Series forecasting to be conducted, we first need to ensure that our data to be stationary. Stationary data refers to data where its mean,standard deviation and covariance do not vary with time. This is an important factor to note in time series analysis to prevent any errorneous and misleading analysis conducted. \n\n","metadata":{}},{"cell_type":"markdown","source":"There are 50 items and 10 stores as part of our dataset, which gives us 500 store-item pairs. ARIMA and SARIMA forecasting methods will be used for our prediction. For simplicity purposes, we will use one store-item pair(Store 1 and Item 1 ) to construct our prediction models.","metadata":{}},{"cell_type":"code","source":"# First start with item 1-store 1 pair \nS1_I1=train.loc[(train['store']==1) & (train['item']==1)]\nS1_I1.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"S1_I1['sales'].plot()\n# seasonal trend where it peaks at mid-year with general increasing sales over time ","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will now conduct a time series decomposition to break down time series for  S1-I1 pair to show trend,seasonal and residual components to determine S1-I1's stationarity","metadata":{}},{"cell_type":"code","source":"# use freq-365 due to long term nature of data \nplt.figsize=(50,30)\ndecomposition=seasonal_decompose(S1_I1['sales'],model='additive',freq=365)\nfig=decomposition.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the seasonal decomposition, we can clearly observe a increasing trend and yearly seasonality that exists in the dataset. This is a indication that the data is not stationary. ","metadata":{}},{"cell_type":"code","source":"# A better look at the data trend \ntrend=decomposition.trend\ntrend.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figsize=(50,30)\nseasonal = decomposition.seasonal \nseasonal.plot()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evidence of yearly seasonality with increasing trend","metadata":{}},{"cell_type":"code","source":"residual = decomposition.resid\nresidual.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another way to check for stationarity in the data is to plot moving average and moving standard deviation.","metadata":{}},{"cell_type":"code","source":"# pLot moving average and moving standard deviation to see if it varies in time \nrolmean = S1_I1['sales'].rolling(window=12).mean()\nrolstd = S1_I1['sales'].rolling(window=12).std()\n\nfig = plt.figure(figsize=(12, 8))\norig = plt.plot(S1_I1['sales'], color='blue',label='Original')\nmean = plt.plot(rolmean, color='red', label='Rolling Mean')\nstd = plt.plot(rolstd, color='black', label = 'Rolling Std')\nplt.legend(loc='best')\nplt.title('Rolling Mean & Standard Deviation')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As seen in the graph above, it is clear that the mean follows the sales trend and is not constant. We can also observe that the moving standard deviation is also fluctuating together with the data","metadata":{}},{"cell_type":"markdown","source":" We will now conduct Dickey Fuller test to test the stationarity of the pair. \n\n If the test statistic is less than the critical value, we can reject the null hypothesis (aka the series is stationary). When the test statistic is greater than the critical value, we fail to reject the null hypothesis (which means the series is not stationary).\n\nIn our above example, the test statistic > critical value, which implies that the series is not stationary. This confirms our original observation which we initially saw in the visual test.","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n\ndef adf_test(dataset):\n    dftest=adfuller(dataset,autolag='AIC')\n    print(\"1. Test statistic:\", dftest[0])\n    print(\"2. P-value:\", dftest[1])\n    print(\"3. No of lags:\", dftest[2])\n    print(\"4. No of observations used for ADF Regression and critical values calculation:\", dftest[3])\n    print(\"5. critical values: \")\n    for key,val in dftest[4].items():\n        print(\"\\t\",key,\": \", val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adf_test(S1_I1['sales'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Dicky-Fuller test results shows the test statistic being higher than critical value at 1% in addtion to upwards trend and seasonality observed. The model is considered not to be stationary","metadata":{}},{"cell_type":"markdown","source":"### Use of differencing method to remove any trends in series ","metadata":{}},{"cell_type":"code","source":"# To modify data to obtain stationary pattern\n# to use the differencing method to remove \nfirst_diff= S1_I1.sales-S1_I1.sales.shift(1)\nfirst_diff=first_diff.dropna(inplace=False)\nfirst_diff.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform seasonal decompose on 1st degree differencing series\n# use freq-365 due to long term nature of data \nplt.figsize=(50,40)\ndecomposition=seasonal_decompose(first_diff,model='additive',freq=365)\nfig=decomposition.plot()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be seen that increasing trend is now removed, and that data values have roughly constant mean and standard deviation ","metadata":{}},{"cell_type":"code","source":"# Replot the rolling mean and standard deviation graphs \nrolmean1 = first_diff.rolling(window=12).mean()\nrolstd1 = first_diff.rolling(window=12).std()\n\nfig = plt.figure(figsize=(12, 8))\norig = plt.plot(first_diff, color='blue',label='Original')\nmean = plt.plot(rolmean1, color='red', label='Rolling Mean')\nstd = plt.plot(rolstd1, color='black', label = 'Rolling Std')\nplt.legend(loc='best')\nplt.title('Rolling Mean & Standard Deviation')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Readminister Dicky Fuller test on altered series\nadf_test(first_diff)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now observe that the p-value is now very small and the Test statistic value isless than 1% critical value. We can conclude that the data is now stationary ","metadata":{}},{"cell_type":"markdown","source":"### Plot ACF and PACF function\n\n\n\n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"We now proceed to plot both the Auto Correlation Function (ACF) and Partial-Auto Correlation Function (PACF). The 2 graphs seek to summarise the strength of a relationship between an observation in a time series with observations at prior time steps.","metadata":{}},{"cell_type":"code","source":"# Initial data before first-order differencing  \nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(S1_I1.sales, lags=40, ax=ax1) # \nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(S1_I1.sales, lags=40, ax=ax2)# \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the initial series, can see that there is evident of recurring patterns,after every 7 lags (days). For PACF, there is also consistent trend among the partial autocorrelation,indicating pattern exists and seasonal trend \n","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(first_diff, lags=40, ax=ax1) # \nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(first_diff, lags=40, ax=ax2)# ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 2 ways to determine the p,d,q combinations for our ARIMA model:\n\n1) Plot the ACF and PACF graphs as seen above to determine the most appropriate ratio \n\n2) Fit the trainset into the auto_arima() function and run it to determine the p,d,q combination with the lowest AIC \n\nWe will use option 1) for this kernel. \n\nWe know that d=1 since the train data achieved stationarity after 1st order differencing \n\nFor p, we can determine that p=6 as the AR term becomes significant after 6 time lags \n\nFor q, it is usually determined by the ACF Plot, but we will set it to 0 to prevent the risk of wrong selection which can affect our prediction model \n\nThus the ARIMA combination used will be **ARIMA(6,1,0)**","metadata":{}},{"cell_type":"markdown","source":"## ARIMA MODEL BUILDING ","metadata":{}},{"cell_type":"markdown","source":"ARIMA(Auto-Regressive Integrated Moving Average), is a generalization of the simpler AutoRegressive Moving Average and adds the notion of integration.\n\nIt is made up for the following elements: \n\n**AR: Autoregression**. A model that uses the dependent relationship between an observation and some number of lagged observations.\n\n**I: Integrated.** The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.\n\n**MA: Moving Average.** A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n\nEach of these components are explicitly specified in the model as a parameter. A standard notation is used of ARIMA(p,d,q) where the parameters are substituted with integer values to quickly indicate the specific ARIMA model being used.\n\nThe parameters of the ARIMA model are (p,d,q) and is defined as follows:\n\np: The number of lag observations included in the model, also called the lag order. This is dependent on the number of autoregressive terms (AR). We can find out the required number of AR terms by inspecting the PACF plot\n\nd: The number of times that the raw observations are differenced, also called the degree of differencing\n\nq: Order of moving average i.e number of moving average terms (MA) that shuld go into the ARIMA model. \n\n ","metadata":{}},{"cell_type":"markdown","source":"To build the model, the train-test split approach will be adopted. Data from the S1_P1 data is split into a trainset(consisting of all data up to the last 3 months in the dataset) and a validation set(made up of the last 3 months of data from S1_P1) where the model will be tested. This method enables us to evaluate the performance of the models generated on the different dataset without testing it on the same data used for training, preventing biasedness and ensuring fair evaluation \n\n\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"Train=S1_I1['2013-01-01':'2017-09-30']['sales']\nvalid=S1_I1['2017-10-01':'2017-12-31']['sales']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Train.shape,valid.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid.head(),valid.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\nmodel=ARIMA(Train,freq='D',order=(6,1,0))\nmodel=model.fit()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make prediction on test set\nstart= len(Train)\nend=len(Train)+len(valid)-1\npred=model.predict(start=start,end=end,typ='levels')\nprint(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred.plot(legend=True)\nvalid.plot(legend=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Calculate the errors of the model \nrmse = sqrt(mean_squared_error(valid,pred))\nprint(\"ARIMA model MRSE: {}\".format(rmse)) \n# get rmse of 7.2432","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SMAPE calculations \ndef smape(A, F):\n    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\nprint(\"ARIMA model SMAPE: {:.4}\".format(smape(valid,pred))+\"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MAKE PREDICTIONS ON THE SUBSEQUENT 90 DAYS (testset)","metadata":{}},{"cell_type":"code","source":"model2=ARIMA(S1_I1['sales'],freq='D',order=(6,1,0))\nmodel2=model2.fit()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index_future_dates=pd.date_range(start='2018-01-01',end='2018-03-31')\n# print(index_future_dates)\npred=model2.predict(start=len(S1_I1),end=len(S1_I1)+90,typ='levels').rename('ARIMA Predictions')\n# pred.index=index_future_dates\nprint(pred)\n# print(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test.head()\n# test.tail()\n# test.index\n# index_future_dates=test.index\n# testset runs from 1 Jan 2018 to 31 Mar 2018\n\n\ntest_S1_I1=test.loc[(test['store']==1) & (test['item']==1)]\n# test_S1_I1.head()\ntest_S1_I1['sales']=pred\n# test.loc['store'==1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_S1_I1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arima_submission=pd.DataFrame(data=test_S1_I1,columns=['id','sales']).reset_index(drop=True)\narima_submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Option to save arima model predictions to csv\n# arima_submission.to_csv('arima_submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SARIMA MODEL ","metadata":{}},{"cell_type":"markdown","source":"Since the SARIMA ratios of (P,D,Q) corresponds to that of the ARIMA values, we will use the same values obtained by our ARIMA model for our SARIMA model building, with a season value of 7 to represent a weekly series","metadata":{}},{"cell_type":"code","source":"sarima_model= sm.tsa.statespace.SARIMAX(Train,\n                                        order= (6,1,0),\n                                        seasonal_order=(6,1,0,7),\n                                        enforce_stationarity=False,\n                                        enforce_invertibility=False)\nsarima_model=sarima_model.fit()\nsarima_model.summary()\n# print(model_aic.summary().tables[1])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start= len(Train)\nend=len(Train)+len(valid)-1\npredict=sarima_model.predict(start=start,end=end,typ='levels')\nprint(predict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npredict.plot(legend=True)\nvalid.plot(legend=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the predicted values generated from SARIMA more closely follows the actual sales values in the validation set","metadata":{}},{"cell_type":"code","source":"rmse=sqrt(mean_squared_error(valid,predict))\nrmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def smape(A, F):\n    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\nprint(\"SARIMA model SMAPE: {:.4}\".format(smape(valid,predict))+\"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RMSE and SMAPE for SARIMA is 5.839 and 24,49% respectively. This is an improvement over that of ARIMA,indicating that SARIMA is a better model when it comes to forecasting in this context","metadata":{}},{"cell_type":"code","source":"model2=SARIMAX(S1_I1.sales,order= (6,1,0),seasonal_order=(6,1,0,7), enforce_stationarity=False, enforce_invertibility=False).fit()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index_future_dates=pd.date_range(start='2018-01-01',end='2018-03-31')\n# print(index_future_dates)\npred=model2.predict(start=len(S1_I1),end=len(S1_I1)+90,typ='levels').rename('SARIMA Predictions')\n# pred.index=index_future_dates\nprint(pred)\n# print(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_S1_I1=test.loc[(test['store']==1) & (test['item']==1)]\n# test_S1_I1.head()\ntest_S1_I1['sales']=pred\n# test.loc['store'==1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_S1_I1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sarima_submission=pd.DataFrame(data=test_S1_I1,columns=['id','sales']).reset_index(drop=True)\nsarima_submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sarima_submission.to_csv('sarima_submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FINAL THOUGHTS ","metadata":{}},{"cell_type":"markdown","source":"This kernel has highlighted the use of both ARIMA and SARIMA methods for time-series forecasting. Both methods are powerful techniques that can be applied for time-series forecasting. It is important for analysts to conduct a thorough exploratory data analysis in the data to derive useful insights before moving on to forecasting.\n\nBesides the 2 techniques shown in this kernel, analysts can also consider utilising alternative methods, such as XGBoost and also Facebook Prophet which can achieve models that predicts more accurately and faster. However,one needs to be mindful of the business needs of the organisation/project before deciding on the how to approach the case.\n\nThank you for taking time to read through this kernel and I appreciate any feedback and advice as I seek to deepen my knowledge on data analytics techniques. \n\n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"References:\n\nhttps://www.kaggle.com/sumi25/understand-arima-and-tune-p-d-q/comments \n\nhttps://www.kaggle.com/alexdance/store-item-combination-part-3-month-and-arima \n\nhttps://www.kaggle.com/hmoritajp718/intro-to-time-series-forecast#ARIMA \n\nhttps://www.kaggle.com/thexyzt/keeping-it-simple-by-xyzt#Conclusion\n\nhttps://www.machinelearningplus.com/time-series/arima-model-time-series-forecasting-python/\n\nhttps://www.youtube.com/watch?v=z-uSBE8Pxwg\n\nhttps://realpython.com/train-test-split-python-data/#the-importance-of-data-splitting\n\nhttps://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/\n\nhttps://github.com/nachi-hebbar/ARIMA-Temperature_Forecasting/blob/master/Temperature_Forecast_ARIMA.ipynb\n\nhttps://www.youtube.com/watch?v=8FCDpFhd1zk&list=PLqYFiz7NM_SMC4ZgXplbreXlRY4Jf4zBP&index=6","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}