{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Objective:  Sales Forecasting Using Facebook Prophet\n\n#### Summary Of Steps Followed: \n1. Exploratory Data Analysis to undestand different data, its features and associated trend \n2. Extract Inference from Data Analysis \n3. Prepare Data for Training & Testing Model \n4. Create Forecast Model using Facebook Prophet \n5. Create Future Data Frame duration for which we need predection values \n6. Predect Sales for Future Data Frame\n7. Evaluate accuracy of your model \n8. Conclusion Report\n\nIf you like this notebook or learned anything from here. Share a token of appreciation by casting an upvote :)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport time\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom fbprophet import Prophet\nimport statsmodels.api as sm\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import graph_objs as go\ninit_notebook_mode(connected=True)\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 1: Exploratory Data Analysis to undestand different data, its features and associated trend","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_raw= pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/train.csv')\ndf_test=pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/test.csv')\ndf_raw.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nsns.barplot(data=df_raw,x='store',y='sales')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From above analysis we can see that Store 2 has the highest Sales, hence let's take this store for futher analysis to identify which Item is the most selling in recent year.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_store=df_raw[(df_raw['store']==2) & (df_raw['date']>='2017-01-01')]\ndf_sc=df_store.copy()\ndf_sc.loc[:,'month'] = pd.DatetimeIndex(df_sc['date']).month\n#df_sc['month'] = pd.DatetimeIndex(df_sc['date']).month\ndf_sc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_store1=pd.DataFrame(df_sc.groupby(['month','item']).sum()['sales'])\ndf_store1.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\nfig = px.line(df_store1, x='month', y='sales',color='item')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### With above visulisation its avident that the most selling item no is 15.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=pd.DataFrame(df_raw.groupby('date').sum()['sales'],columns=['sales'])\ndf2=df1.reset_index()\ndf2['date']=pd.to_datetime(df2['date'])\ndf2['sales']=df2['sales']*1.0\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nsns.lineplot(data=df2,x='date',y='sales')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3=df2.set_index(pd.to_datetime(df2['date']))\ndf3.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df3['sales'].resample('MS').mean() \ndecomposition = sm.tsa.seasonal_decompose(y)\nplt.figure(figsize=(16,12))\ndecomposition.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### With below decomposition we can see that the model is Additive, since the seasonal component is similar (not getting multiplied) over the period of time. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df4=df3.reset_index(drop=True)\ndf4.columns=['ds','y']\ndf4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df4['ds'].dt.strftime('%Y-%m')\ndf4['year'] = pd.DatetimeIndex(df4['ds']).year\ndf4['month'] = pd.DatetimeIndex(df4['ds']).month\ndf4['week'] = df4['ds'].dt.strftime('%A')\n#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Period.strftime.html\ndf4.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.lineplot(data=df4,x='year',y='y',ci=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nsns.lineplot(data=df4,x='month',y='y', hue='year',ci=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nsns.lineplot(data=df4,x='week',y='y',sort='True',hue='year',ci=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df4.head()\ndf_raw.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Inference from EDA: \n1. Store 2 has maxium sales in which the most popular Item is number 15\n2. With Trend analysis we can see that the Model is Additive (seasonal component is similar over period of time, not multiplying) \n3. Trend is positive with increase in sales over the year \n4. Sales increases in Q2, where in Maximum sales has been observed during month of July after which it again decreases \n5. With Weekly Trend we can infer that consumers prefer to shop during weekends. Sales trend increases from Friday with maximum sales on Sunday and then it again gets back to normal during weekdays.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Step 3: Prepare Data for Training & Testing Model  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_raw[(df_raw['item'] == 15) & (df_raw['store'] == 2) & (df_raw['date']<='2016-12-31')]\ndf_train.columns=['ds','store','item','y']\n#Renaming is required since Facebook Prohet requires date column with name as ds and metric column name as y\ndf_train.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 4: Create Forecast Model using Facebook Prophet ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"m = Prophet(yearly_seasonality=True, weekly_seasonality=True)\nm.fit(df_train[['ds','y']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 5: Create Future Data Frame duration for which we need predection values\nIn this case we are keeping it as 365 i.e. next complete year. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"future = m.make_future_dataframe(periods=365)\nfuture.tail(n=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 6: Predict Sales for Future Data Frame","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast = m.predict(future)\nforecast.head(n=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m.plot(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m.plot_components(forecast)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 7: Evaluate Accuracy Of Your Model ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_orig = df_raw[(df_raw['item'] == 15) & (df_raw['store'] == 2)]\ndf_orig.columns=['ds','store','item','y']\ndf_orig.loc[:,('ds')]=pd.to_datetime(df_orig['ds'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_forecast=forecast[['ds','yhat_lower','yhat_upper','yhat']]\ndf_result= pd.merge(df_orig,df_forecast,on='ds')\ndf_result.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet.diagnostics import cross_validation\ndf_cv = cross_validation(m, initial='730 days', period='90 days', horizon = '365 days')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet.diagnostics import performance_metrics\ndf_p = performance_metrics(df_cv)\ndf_p.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet.plot import plot_cross_validation_metric\nfig = plot_cross_validation_metric(df_cv, metric='mape')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> As you can see from above Cross Validation Report that the MAPE across all the date range is less then 0.1 i.e. 10% which is decent % Error. \n\n> ** Below is another traditional way of calculating MAPE, lets see what results it gives. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_absolute_percentage_error(df_result['y'],df_result['yhat'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_result_2017= df_result[df_result['ds']>='2017-01-01']\nmean_absolute_percentage_error(df_result_2017['y'],df_result_2017['yhat'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> With this we can see that the MAPE value is similar to what we have calcualted via prophet metric validation report.\nLets plot these error & see its quantification.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_result['y - yhat']=df_result['y'] - df_result['yhat']\nplt.figure(figsize=(16,6))\nsns.lineplot(data=df_result,x='ds',y='y - yhat')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Lets get the stats of Actuls, Forecasted & Error Values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_result.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion: \n\n1. FB Prophet works very well for this time series sales data set, its is givig us MAPE <10% which means accuracy of >90% \n2. With above measurement report & stats we can visualize that the variations (y-yhat) between actuals (y) & predicted (yhat)  value, which seems pretty good. \n\n\n> If you like this notebook or learned anything from here. Share a token of appreciation by casting an upvote. :) \n\nCheers For The Solution! \n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}