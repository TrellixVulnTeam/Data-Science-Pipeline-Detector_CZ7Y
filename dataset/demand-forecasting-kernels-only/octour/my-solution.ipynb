{"cells":[{"metadata":{"id":"83Sf3qpAQjOY","colab_type":"text"},"cell_type":"markdown","source":"# 0. Dependencies, load datasets"},{"metadata":{"id":"YFxL1ouE_uLT","colab_type":"text"},"cell_type":"markdown","source":"Определим все необходимые зависимости"},{"metadata":{"id":"6fk-KAY4kYzj","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D \nimport math\n\nfrom sklearn.decomposition import PCA\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn import preprocessing \nfrom sklearn import neighbors, tree, ensemble, linear_model\n\nimport xgboost as xgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.options.display.float_format = '{:.1f}'.format","execution_count":null,"outputs":[]},{"metadata":{"id":"O9cXz_gBk-kj","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\nsubm_df_raw = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"dgdhgui1_5yU","colab_type":"text"},"cell_type":"markdown","source":"Поскольку в данных присутствует дата, сразу перобразуем ее в месяц, год и день недели, которые в дальнейшем можно использовать как фичи."},{"metadata":{"id":"r7dX1e8vw1kU","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"train_df['date'] = pd.to_datetime(train_df.date)\ntrain_df['month'] = train_df.date.dt.month\ntrain_df['year'] = train_df.date.dt.year\ntrain_df['dayofweek'] = train_df.date.dt.dayofweek\n\ntest_df['date'] = pd.to_datetime(test_df.date)\ntest_df['month'] = test_df.date.dt.month\ntest_df['year'] = test_df.date.dt.year\ntest_df['dayofweek'] = test_df.date.dt.dayofweek\n\ntest_df = test_df.drop(['id'], 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"RmOJ60NEtZE1","colab_type":"text"},"cell_type":"markdown","source":"# 1. Data Analysis"},{"metadata":{"id":"DBmrwNw2vXBr","colab_type":"text"},"cell_type":"markdown","source":"### 1.1 First look"},{"metadata":{"id":"6MlQUQCIAFas","colab_type":"text"},"cell_type":"markdown","source":"При первом взгляде на данные при помощи функций info(), describe() можно определить какие фичи присутствуют в датасете, какого они типа, какое количество строк, есть ли пропущенные значения и аномалии (сравнение максимального значения, мат. ожидания и процентилей из функции describe())."},{"metadata":{"id":"ryxMC3U8tXqV","colab_type":"code","outputId":"1713b2bd-4a93-46a7-febb-1b7e2fff6f0c","colab":{"base_uri":"https://localhost:8080/","height":954},"trusted":false},"cell_type":"code","source":"print('train_df info')\nprint(train_df.info())\nprint('')\nprint('train_df describe')\nprint(train_df.describe())\nprint('')\nprint('test_df info')\nprint(test_df.info())\nprint('')\nprint('test_df describe')\nprint(test_df.describe())","execution_count":null,"outputs":[]},{"metadata":{"id":"zL7inSThA00U","colab_type":"text"},"cell_type":"markdown","source":"Как видно из функций, в массивах нет пропущеннх данных и аномалий. Все данные в численных значениях (кроме даты, из которой мы вытащили необходимые данные в числах)."},{"metadata":{"id":"bbT7zNKxJClp","colab_type":"text"},"cell_type":"markdown","source":"### 1.2 Graphs"},{"metadata":{"id":"UUWTpgnEAxUy","colab_type":"text"},"cell_type":"markdown","source":"Для анализа имеющихся данных можно вывести графики изменения параметров от разных значений. Поскольку мы имеем дело с продажами 50 единиц товаров в 10 магазинах, можно посмотреть следующие зависимости:\n* как изменяются продажи в зависимости от типа товара\n* как изменяются продажи в зависимости от магазина\n* как изменяются продажи в зависимости от года"},{"metadata":{"id":"195UmDT87wHw","colab_type":"code","outputId":"ff5ed257-65ec-47a4-9854-a23ace69523b","colab":{"base_uri":"https://localhost:8080/","height":607},"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(7, 10))\n\nplt.subplot(311)\nplt.xticks(range(0, 55, 5))\nplt.grid()\nplt.title('Sales by item')\n_ = plt.plot(train_df.groupby(['item'])['sales'].sum())\n\nplt.subplot(312)\nplt.xticks(range(1, 11))\nplt.grid()\nplt.title('Sales by store')\n_ = plt.plot(train_df.groupby(['store'])['sales'].sum())\n\nplt.subplot(313)\nplt.xticks(range(2013, 2018))\nplt.grid()\nplt.title('Sales by year')\n_ = plt.plot(train_df.groupby(train_df.date.dt.year)['sales'].sum())","execution_count":null,"outputs":[]},{"metadata":{"id":"j9zdUqi8Cnid","colab_type":"text"},"cell_type":"markdown","source":"Итого: \n* общие продажи в зависимости от типа товара имеют довольно случайный характер и нельзя выделить какой то полезной корреляции. В качестве обработки фич можно было бы выделить три корзины (binning) - товары которые плохо продаются, средние продажи и высокие продажи, но поскольку у нас нет проблемы большого разброса продажи отдельных позиций, нет необходимости в такой обработке. Тем более продажи зависят не только от типа товара, но и от магазина, поэтому в одном магазине один товар может продаваться лучше, в другом хуже. \n* количество товаров всех типов в каждом магазине одинаковое - то есть в магазинах одинаковый ассортимент (проверено итерацией `df.item[(df.store == j) & (df.item == i)].count()`, где i - итерация по товарам, j - итерация по магазинам), поэтому нельзя сейчас сделать вывод о том, что продажи какого то товара в одном магазине больше, потому что в этом магазине есть этот товар, а в других нет.\n* продажи растут с каждым годом.\n\nВ связи с последним выводом, можно посмотреть, растут ли продажи с измением года во всех магазинах, или для всех ли позиций. Для этого можно разделить данные по магазину и типам товаров:"},{"metadata":{"id":"jWClmzVNEOOa","colab_type":"code","outputId":"33f763f1-9a06-4476-a1e0-8abbd0481fc3","colab":{"base_uri":"https://localhost:8080/","height":879},"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(15, 15))\n\nplt.subplot(321)\nfor i in range(1, 11):\n    plt.plot(train_df.sales[(train_df.store == i) ].groupby(train_df.date.dt.year).sum(), label='Store ' + str(i))\nplt.title('Year, sum by store.')\nplt.xticks(range(2013, 2018))\nplt.grid()\nplt.legend(loc='upper left')\n\nplt.subplot(322) \nfor i in range(1, 51):\n    plt.plot(train_df.sales[(train_df.item == i) ].groupby(train_df.date.dt.year).sum(), label='Item' + str(i))\nplt.title('Year, sum by item.')\nplt.xticks(range(2013, 2018))\nplt.grid()\n\nplt.subplot(323)\nfor i in range(1, 11):\n    plt.plot(train_df.sales[(train_df.store == i)].groupby(train_df.date.dt.month).sum(), label='Store ' + str(i))\nplt.title('Month, sum by store.')\nplt.xticks(range(1, 13))\nplt.grid()\nplt.legend(loc='upper left')\n\nplt.subplot(324)\nfor i in range(1, 51):\n    plt.plot(train_df.sales[(train_df.item == i)].groupby(train_df.date.dt.month).sum(), label='Item ' + str(i))\nplt.title('Month, sum by item.')\nplt.xticks(range(1, 13))\nplt.grid()\n\nplt.subplot(325)\nfor i in range(1, 11):\n    plt.plot(train_df.sales[(train_df.store == i)].groupby(train_df.date.dt.dayofweek).sum(), label='Store ' + str(i))\nplt.title('Day of week, sum by store.')\nplt.grid()\nplt.legend(loc='upper left')\n\nplt.subplot(326)\nfor i in range(1, 51):\n    _ = plt.plot(train_df.sales[(train_df.item == i)].groupby(train_df.date.dt.dayofweek).sum(), label='Item ' + str(i))\nplt.title('Day of week, sum by item.')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"id":"wHxHtUAfHRXR","colab_type":"text"},"cell_type":"markdown","source":"Зависимости практически одинаковые для всех магазинов и типов товаров (форма кривой), изменяется только абсюлютные значения, что не дает возможности выделить какой то особый магазин или товар, для которых тренд был бы особым.\n\nАналогично, для разреза по магазинам наблюдается одинаковый тренд, как в зависимости от месяца, так и в зависимости от дня недели.\nБолее того, расположение линий по магазинам и товарам сохраняется при разных временных разрезах (то есть от мин к макс для магазинов: магазин 7, магазин 6, магазин 1, итд - одинаково как в разрезах для месяца, дня недели или года; аналогично и для типа товара)."},{"metadata":{"id":"UfaATkvGLsTH","colab_type":"text"},"cell_type":"markdown","source":"### 1.3 Scatter plot"},{"metadata":{"id":"RpAe-jaOLIlO","colab_type":"text"},"cell_type":"markdown","source":"По другому можно визуализировать данные используя три оси. Такая визуализация может быть полезной, так как позволит понять, можно ли выделить какие то кластеры в данных или нет. При наличии кластеров, возможно использование методов кластеризации, с последующим решением регрессионной задачи. Поскольку у нас имеется только 4 параметра: магазин, тип товара, продажи и время (месяц, год или день недели), наши данные можно визуализировать в трех плоскостях, используя разные цвета для точек в зависимости от магазина:"},{"metadata":{"id":"qlUQOwmKTs2n","colab_type":"code","outputId":"008425e8-05cd-4bda-e37b-0853df6f66c6","colab":{"base_uri":"https://localhost:8080/","height":575},"trusted":false},"cell_type":"code","source":"fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\n\nmark = ['', 'o', '^', '.', 'x', '^', 'o', '^', '.', 'x', 'o']\nfor i in range(1, 4):\n    ax.scatter(train_df.date.dt.month[train_df.store == i+2], \n               train_df.item[train_df.store == i+2], \n               train_df.sales[train_df.store == i+2], marker=mark[i])\n\nplt.xticks(range(1, 13))\nplt.yticks(range(1, 55, 5))\n\nax.set_xlabel('Month')\nax.set_ylabel('item')\nax.set_zlabel('Sales')  \nax.view_init(elev=10., azim=45)","execution_count":null,"outputs":[]},{"metadata":{"id":"Rh_b4muHNYe2","colab_type":"text"},"cell_type":"markdown","source":"Как видно, данные практически полностью перекрываются, поскольку находятся в одном диапазоне, и в них сложно выделить кластеры (и то используются только 3 магазина)."},{"metadata":{"id":"7nFGG_6yvc_I","colab_type":"text"},"cell_type":"markdown","source":"### 1.4 PCA"},{"metadata":{"id":"Rcbk7aLX4j-o","colab_type":"text"},"cell_type":"markdown","source":"Попробуем выделить главные компоненты, чтобы понять, возможно ли с их помощью разделить данные.\n\nПоскольку метод выделения главных компонент требует стандартизации данных, на финальном графике не будет видно полос, представляющих дискретные данные."},{"metadata":{"id":"B4O7NEBrvhsc","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"pca = PCA(n_components=3)\npca_df = train_df.drop(['date'], 1).copy()\npca_df = pd.DataFrame(preprocessing.StandardScaler().fit_transform(pca_df.values),\n                      columns=pca_df.columns, index=pca_df.index)\npca_result = pca.fit_transform(pca_df)\n\npca_df['pca-one'] = pca_result[:,0]\npca_df['pca-two'] = pca_result[:,1] \npca_df['pca-three'] = pca_result[:,2]\n\n# For reproducability of the results\nnp.random.seed(42)\nrndperm = np.random.permutation(pca_df.shape[0])","execution_count":0,"outputs":[]},{"metadata":{"id":"o5NYDeobP2UO","colab_type":"code","outputId":"cf8c68fa-cdf3-4f44-8e47-4bb7561ac363","colab":{"base_uri":"https://localhost:8080/","height":609},"trusted":false},"cell_type":"code","source":"# 2d scatter\nplt.figure(figsize=(16,10))\n_ = sns.scatterplot(x=\"pca-one\", y=\"pca-two\",\n                    hue=\"store\",\n                    palette=sns.color_palette(\"hls\", 10),\n                    data=pca_df.loc[rndperm, :],\n                    alpha=0.3)","execution_count":null,"outputs":[]},{"metadata":{"id":"2KqYwuj-z0o5","colab_type":"text"},"cell_type":"markdown","source":"В двух осях данные выглядят смешанными. Посмотрим в 3х осях."},{"metadata":{"id":"QmlDXdJ5QHoP","colab_type":"code","outputId":"8753558c-a009-4482-f8cf-d8e84037fc3a","colab":{"base_uri":"https://localhost:8080/","height":575},"trusted":false},"cell_type":"code","source":"# 3d scatter\nax = plt.figure(figsize=(16,10)).gca(projection='3d')\nax.scatter(xs=pca_df.loc[rndperm,:][\"pca-one\"], \n           ys=pca_df.loc[rndperm,:][\"pca-two\"], \n           zs=pca_df.loc[rndperm,:][\"pca-three\"], \n           c=pca_df.loc[rndperm,:][\"store\"], \n           cmap='tab10')\n\nax.set_xlabel('pca-one')\nax.set_ylabel('pca-two')\nax.set_zlabel('pca-three')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"MVbDPP9K5OqX","colab_type":"text"},"cell_type":"markdown","source":"При использовании трех компонент возможно разделить данные так, что явно видно определенные кластеры.\nПреобразуем данные используя PCA с пятью компонентами для тренировки моделей:"},{"metadata":{"id":"QaUCmyft021w","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"pca_targ = train_df['sales']\npca_df = train_df.drop(['date'], 1).copy()\npca_df = pd.DataFrame(preprocessing.StandardScaler().fit_transform(pca_df.values),\n                      columns=pca_df.columns, index=pca_df.index)\n\npca = PCA(n_components=5)\npca_result = pca.fit_transform(pca_df)\npca_features = pd.DataFrame(pca_result)\n\nX_test = pd.DataFrame(preprocessing.StandardScaler().fit_transform(test_df.drop(['date'], 1).values),\n                      index=test_df.drop(['date'], 1).index,\n                      columns=test_df.drop(['date'], 1).columns)\npca_test = pd.DataFrame(pca.fit_transform(X_test))","execution_count":0,"outputs":[]},{"metadata":{"id":"683bs9xWta9I","colab_type":"text"},"cell_type":"markdown","source":"# 2. Preparation"},{"metadata":{"id":"zH7kF-NATuZe","colab_type":"text"},"cell_type":"markdown","source":"### 2.1 Training and validation data"},{"metadata":{"id":"dpcUbcsZsOy7","colab_type":"text"},"cell_type":"markdown","source":"На стадии подготовки данных разделим тренировочный датасет на фичи и лейблы. Более того, для фич необходимо произвести стандартизацию (мат. ожидание = 0, среднеквадратичное отклонение = 1), поскольку многие модели из библиотеки sklearn предполагают, что данные стандартизированы.\n\nПеременная folds будет использоваться для свертках при кросс валидации. Выбираем 5 разделений, для сокращения времени тренировки моделей, но за счет увеличения числа разделений иногда можно улучшить производительность моделей."},{"metadata":{"id":"bkhXJuss3LE0","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"features = train_df.drop(['date', 'sales'], 1)\ntargets = train_df['sales']\n\nfeatures = preprocessing.StandardScaler().fit_transform(features.values)\n\nX_test = pd.DataFrame(preprocessing.StandardScaler().fit_transform(test_df.drop(['date'], 1).values),\n                      index=test_df.drop(['date'], 1).index,\n                      columns=test_df.drop(['date'], 1).columns)\n\nfolds = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)","execution_count":0,"outputs":[]},{"metadata":{"id":"lExcXhPcTt8x","colab_type":"text"},"cell_type":"markdown","source":"### 2.2 Class estimator (sklearn)"},{"metadata":{"id":"ywOmPLbOs0TS","colab_type":"text"},"cell_type":"markdown","source":"Создадим класс для использования с моделями sklearn."},{"metadata":{"id":"kns7LF0wT09e","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"class SKLEstimator:    \n\n    def __init__(self, estimator, param_grid, folds=folds, verbose=0):\n        self.grid = model_selection.GridSearchCV(estimator=estimator, param_grid=param_grid, scoring='neg_mean_squared_error', cv=folds, verbose=verbose, n_jobs=-1)\n        self.estimator = estimator\n    \n  \n    def fit_grid(self, X, y=targets, verbose=False):\n        self.grid.fit(X, y)\n        if verbose:\n            print('')\n            print('Best score: {:.2f}'.format(math.sqrt(abs(self.grid.best_score_))))\n            print('Best parameters: {}'.format(self.grid.best_params_))\n      \n    def get_best_estimator(self):\n      \n        return self.grid.best_estimator_\n \n\n    def best_params(self):\n      \n        return self.grid.best_params_\n      \n      \n    def train_estimator(self, X, y=targets, folds=folds, verbose=False):\n      \n        scores = []\n\n        for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n\n            X_train, X_valid = X[train_index], X[valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n\n            self.grid.best_estimator_.fit(X_train, y_train)\n            y_pred_valid = self.grid.best_estimator_.predict(X_valid).reshape(-1)\n\n            scores.append(metrics.mean_squared_error(y_valid, y_pred_valid))\n      \n        if verbose:\n            print('Mean of CV MSE scores (on train/valid set): {0:.2f}'.format(math.sqrt(abs(np.mean(scores)))))  \n      \n        return scores\n    \n    def predict_targets(self, X_test):\n      \n        return self.grid.best_estimator_.predict(X_test)    ","execution_count":0,"outputs":[]},{"metadata":{"id":"hFUy9BWzug5U","colab_type":"text"},"cell_type":"markdown","source":"Функция для сдачи результатов"},{"metadata":{"id":"NCrWhBWCugo2","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"submit_df = subm_df_raw.copy()\ndef submit_preds(estimator, name, X_test=X_test, submit_df=submit_df, to_csv=False):\n\n    submit_df['sales'] = estimator.predict(X_test).astype(int) \n    name_csv = name + \".csv\"\n    \n    if to_csv:\n        submit_df.to_csv(name_csv, index=False)\n    \n    return submit_df","execution_count":0,"outputs":[]},{"metadata":{"id":"EHxs6bXL6wRd","colab_type":"text"},"cell_type":"markdown","source":"# 3. Models"},{"metadata":{"id":"ywFPHGBhSLhB","colab_type":"text"},"cell_type":"markdown","source":"## 3.1 Sklearn models"},{"metadata":{"id":"j9XfB-rftHBN","colab_type":"text"},"cell_type":"markdown","source":"В данном разделе попробуем применить несколько регрессионных моделей. Поскольку данные довольно однородные и пока не удалось выделить какие то особенности магазинов или типов товаров (хотя не исключаю, что их можно найти), пока сложно сделать вывод о применимости какой либо модели, поэтому методом проб и ошибок, постараемся найти лучшую модель."},{"metadata":{"id":"rS4xX704o9rk","colab_type":"text"},"cell_type":"markdown","source":"### 3.1.1 Linear regression model"},{"metadata":{"id":"h3KlwENNpCik","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"30b4d7ce-1630-4576-ea8d-023d6ad29e50","trusted":false},"cell_type":"code","source":"grid_linreg = {'fit_intercept': [True, False]}\nlinreg_estim = SKLEstimator(linear_model.LinearRegression(), grid_linreg)\nlinreg_estim.fit_grid(features, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"g3f3FFLmu0Id","colab_type":"text"},"cell_type":"markdown","source":"Модель дает средний результат (не слишком ужасный, но и далеко от хорошего), на тестовом сете при LB score не ниже 25."},{"metadata":{"id":"YX79Q-ejvIuL","colab_type":"text"},"cell_type":"markdown","source":"Ниже для примера использования показаны модели линейной регресстии, использующие l1, l2 и l1+l2 регуляризацию. Итог на тестовом сете, не ниже 25. \n\n*В коде ниже тренировка моделей закомментирована, чтобы не тратить время на прогон скрипта*"},{"metadata":{"id":"yoOJhnX4vZBd","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"### Lasso estimator\ngrid_lasso = {'alpha': [1e-3, 1e-2, 1e-1, 1, 2],\n              'fit_intercept': [True, False],\n              'tol': [1e-4, 1e-3, 1e-1, 5e-1]}\nlasso_estim = SKLEstimator(linear_model.Lasso(), grid_lasso)\n#lasso_estim.fit_grid(features, verbose=True)","execution_count":0,"outputs":[]},{"metadata":{"id":"J11wvastvdq2","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"### Ridge estimator\ngrid_ridge = {'alpha': [1e-3, 1e-2, 1e-1],\n              'fit_intercept': [True, False],\n              #'tol': [1e-4, 1e-3, 1e-1, 5e-1],\n              'solver': ['svd', 'lsqr']}\nridge_estim = SKLEstimator(linear_model.Ridge(), grid_ridge, verbose=1)\n#ridge_estim.fit_grid(features, verbose=True)","execution_count":0,"outputs":[]},{"metadata":{"id":"1ZXZjjzZvtaq","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"### Elnet estimator\ngrid_elnet = {'alpha': [1e-3, 1e-1, 1, 2],\n              'l1_ratio': [1e-3, 1e-2, 5e-1],\n              'fit_intercept': [True, False]}\nelnet_estim = SKLEstimator(linear_model.ElasticNet(), grid_elnet, verbose=1)\n#elnet_estim.fit_grid(features, verbose=True)","execution_count":0,"outputs":[]},{"metadata":{"id":"R4tgYbTLtcvy","colab_type":"text"},"cell_type":"markdown","source":"### 3.1.2 KNN regressor model"},{"metadata":{"id":"uSN6H7Lvwhjz","colab_type":"text"},"cell_type":"markdown","source":"Модели на основе К ближайших соседей приводят к перетренированности модели. Результат на тренировочном сете хороший (MSE 11 и 8), но на тестовом сете показатель не меньше 40!\n\n*В коде ниже тренировка моделей закомментирована, чтобы не тратить время на прогон скрипта*\n\n*В комментариях - показатели моделей с разными параметрами*"},{"metadata":{"id":"HqzX4rTuqRGN","colab_type":"code","outputId":"c61cc9f9-709e-437c-ad66-8a467a56c928","colab":{"base_uri":"https://localhost:8080/"},"trusted":false},"cell_type":"code","source":"grid_knn = {'n_neighbors': [3, 5, 10],\n            'weights': ['uniform', 'distance'],\n            'algorithm': ['ball_tree', 'kd_tree']}\nknn_estim = SKLEstimator(neighbors.KNeighborsRegressor(), grid_knn, folds=folds, verbose=1)\n#knn_estim.fit_grid(features, verbose=True)\n\n# Best score: 11.60394831102819\n# Best parameters: {'n_neighbors': 3, 'weights': 'uniform'}\n  \n# Best score: 8.826924270536434\n# Best parameters: {'algorithm': 'kd_tree', 'n_neighbors': 10, 'weights': 'distance'}\n\n# Best score: 8.826924270536434\n# Best parameters: {'n_neighbors': 10, 'weights': 'distance'}","execution_count":0,"outputs":[]},{"metadata":{"id":"ip-dpb2nSbgs","colab_type":"text"},"cell_type":"markdown","source":"### 3.1.3 sklearn.tree.DecisionTreeRegressor"},{"metadata":{"id":"Lctkz1Iuw-6D","colab_type":"text"},"cell_type":"markdown","source":"Поскольку деревья решений используются в многих алгоритмах бустинга, можно попробовать использовать деревья без бустинга (разрешая им расти полностью - параметр `'max_depth': [None]`).\n\nТак же использовались фичи полученные выделением главных компонент (PCA), итоговый результат оказался хуже.\n\n*В коде ниже тренировка моделей закомментирована, чтобы не тратить время на прогон скрипта*\n\n*В комментариях - показатели моделей с разными параметрами*"},{"metadata":{"id":"SpN98f9DTYRX","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"grid_dt = {'splitter': ['best', 'random'],\n           'max_depth': [10, 20, 50],\n           'min_samples_split': [10, 30],\n           'min_samples_leaf': [5, 15],\n           'min_weight_fraction_leaf': [0, 0.2],          \n          }\ndt_estim = SKLEstimator(tree.DecisionTreeRegressor(), grid_dt, verbose=1)\n#dt_estim.fit_grid(features, verbose=True) # may take time to run!\n#submit_df = submit_preds(dt_estim.get_best_estimator(), \"DT_3\", to_csv=True)\n\n### PCA\n#dt_estim = SKLEstimator(tree.DecisionTreeRegressor(), grid_dt, verbose=1)\n#dt_estim.fit_grid(pca_features, pca_targ, verbose=True)\n#submit_df = submit_preds(dt_estim.get_best_estimator(), \"DT_pca1\", pca_test, to_csv=True)\n\n\n# Best score: 8.97\n# Best parameters: {'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0, 'splitter': 'best'}","execution_count":0,"outputs":[]},{"metadata":{"id":"6kkeRhJkyR4Q","colab_type":"text"},"cell_type":"markdown","source":"Дерево решений занимает время на тренировку (поскольку при возможности оно растет полностью), и при этом явно приводит к перетренированности модели: \n\nMSE на тренировочном сете = 9\n\nLB score на тестовом сете > 25.\n\nОднако, при лучшей настройки модели можно добиться лучших результатов."},{"metadata":{"id":"os9XAubPSc_C","colab_type":"text"},"cell_type":"markdown","source":"### 3.1.4 sklearn.ensemble.RandomForestRegressor"},{"metadata":{"id":"tXZXbqQ_zEgZ","colab_type":"text"},"cell_type":"markdown","source":"Лес деревьев так же приводит к перетренированности модели. LB score не ниже 24.\n\nТак же использовались фичи полученные выделением главных компонент (PCA), итоговый результат оказался хуже.\n\n*В коде ниже тренировка моделей закомментирована, чтобы не тратить время на прогон скрипта*\n\n*В комментариях - показатели моделей с разными параметрами*"},{"metadata":{"id":"K0Zgajy3mpmH","colab_type":"code","outputId":"1be98aeb-9fe3-4993-cb66-7e1ac12cacb1","colab":{"base_uri":"https://localhost:8080/","height":163},"trusted":false},"cell_type":"code","source":"grid_rf = {'n_estimators': [10, 100],\n           'max_depth': [2, 3, None],\n           'min_samples_split': [2, 30],\n           'min_samples_leaf': [1, 5],\n           'min_weight_fraction_leaf': [0, 0.5]}\n\ngrid_rf = {'n_estimators': [100],\n           'max_depth': [None],\n           'min_samples_split': [2],\n           'min_samples_leaf': [5],\n           'min_weight_fraction_leaf': [0]}\n\nrf_estim = SKLEstimator(ensemble.RandomForestRegressor(), grid_rf, folds=folds, verbose=1)\n#rf_estim.fit_grid(features, verbose=True) # may take time to run!\n\n\n### PCA:\n#rf_estim = SKLEstimator(ensemble.RandomForestRegressor(), grid_rf, folds=folds, verbose=1)\n#rf_estim.fit_grid(pca_features, pca_targ, verbose=True)\n#submit_df = submit_preds(rf_estim.get_best_estimator(), \"RF_pca1\", pca_test, to_csv=True)\n\n### Run 1:\n#Best score: 7.652671941592358\n#Best parameters: {'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0, 'n_estimators': 100}","execution_count":0,"outputs":[]},{"metadata":{"id":"itl7yOXcSRzv","colab_type":"text"},"cell_type":"markdown","source":"## 3.2 Boosting, ensemble models"},{"metadata":{"id":"wdqQYyCbfICt","colab_type":"text"},"cell_type":"markdown","source":"### 3.2.1 XGBoost"},{"metadata":{"id":"_fTbCkWzz6JS","colab_type":"text"},"cell_type":"markdown","source":"Сейчас популярен метод бустинга из библиотеки xgboost, из за его высокой производительности и сравнительно низкого времени тренировки. Попробуем несколько моделей с разными параметрами."},{"metadata":{"id":"upt8inm-fJtN","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"features_xgb = pd.DataFrame(preprocessing.StandardScaler().fit_transform(train_df.drop(['date', 'sales'], 1).values),\n                            index=train_df.drop(['date', 'sales'], 1).index, \n                            columns=train_df.drop(['date', 'sales'], 1).columns)\n\ndtrain = xgb.DMatrix(features_xgb, label=targets, feature_names=list(features_xgb.columns))\ndtest = xgb.DMatrix(X_test, feature_names=list(X_test.columns))","execution_count":0,"outputs":[]},{"metadata":{"id":"Hqjpt7p91rgX","colab_type":"text"},"cell_type":"markdown","source":"Всего было протестировано 10 моделей XGBoost с разными параметрами, на данный момент лучший результат LB private / public: 24.33419 / 14.78021. Заметно ухудшение результатов при переходе от public score к private score, что может говорить о перетренированности модели. Ниже приведено несколько моделей. Можно проводить дальнейшную настройку для улучшения результатов."},{"metadata":{"id":"g5fRMFpE1X0s","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"# LB private / public: 24.77016 / 15.06425\nxgb_grid = {'max_depth': 3, 'eta': 5e-1} # 'subsample': 1\n#xgb_estim = xgb.train(xgb_grid, dtrain, 1500)\n#submit_df = submit_preds(xgb_estim, \"XGB_2\", X_test=dtest, to_csv=True)","execution_count":0,"outputs":[]},{"metadata":{"id":"QFP3kV1y1hsg","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"# LB private / public: 24.21677 / 14.81929\nxgb_grid = {'max_depth': 5, 'eta': 5e-1}\n#xgb_estim = xgb.train(xgb_grid, dtrain, 1500)\n#submit_df = submit_preds(xgb_estim, \"XGB_4\", X_test=dtest, to_csv=True)","execution_count":0,"outputs":[]},{"metadata":{"id":"H1uKuP4V1jgx","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"# LB private / public: 24.20192 / 14.78730\nxgb_grid = {'max_depth': 5, 'eta': 2e-1}\nxgb_estim = xgb.train(xgb_grid, dtrain, 1500)\nsubmit_df = submit_preds(xgb_estim, \"XGB_1\", X_test=dtest, to_csv=True)","execution_count":0,"outputs":[]},{"metadata":{"id":"gkFfnC3d1pNx","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"# LB private / public: 24.33419 / 14.78021\nxgb_grid = {'max_depth': 5, 'eta': 1e-1}\n#xgb_estim = xgb.train(xgb_grid, dtrain, 1500)\n#submit_df = submit_preds(xgb_estim, \"XGB_9\", X_test=dtest, to_csv=True)","execution_count":0,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LB private / public: \nxgb_grid = {'booster': 'dart', \n            'max_depth': 5, \n            'eta': 5e-1}\n#xgb_estim = xgb.train(xgb_grid, dtrain, 1500)\n#submit_df = submit_preds(xgb_estim, \"XGB_11\", X_test=dtest, to_csv=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"QoqQVpod3g_-","colab_type":"text"},"cell_type":"markdown","source":"При использовании вместо фич гланых компонент результат получается хуже (не ниже 56! ((  ). Возможно произведен некорректный выбор гланых компонент, либо необходимо по другому настраивать модель бустинга."},{"metadata":{"id":"oLJVoOpK1M3r","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"dtrain_pca = xgb.DMatrix(pca_features, label=pca_targ)\ndtest_pca = xgb.DMatrix(pca_test)","execution_count":0,"outputs":[]},{"metadata":{"id":"rfbTNVrV1KJO","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"# LB private / public: 56.64127 / 56.36665\nxgb_grid = {'max_depth': 5, 'eta': 2e-1}\n#xgb_estim = xgb.train(xgb_grid, dtrain_pca, 1500)\n#submit_df = submit_preds(xgb_estim, \"XGB_pca_1\", X_test=dtest_pca, to_csv=True)","execution_count":0,"outputs":[]},{"metadata":{"id":"jjGV9NEefLGG","colab_type":"text"},"cell_type":"markdown","source":"### 3.2.2 sklearn.ensemble.ExtraTreesRegressor"},{"metadata":{"id":"PDo9er0f5TVK","colab_type":"text"},"cell_type":"markdown","source":"Данная модель показала пока лучшие результаты для LB private score = 23.41. Несмотря на то, что модель все еще сильно перетренирована, так как private score в два раза отличается от тренировочной метрики (хотя может там используется другая метрика, а не MSE), на тренировку не уходит много времени, так как деревья ограничены 3, 4 узлами. Данную модель можно улучшать далее путем настройки большего числа доступных параметров, наравне с XGBoost для улучшения результатов.\n\nДатасеты PCA снова не показали хороших результатов."},{"metadata":{"id":"VTdOvmh7fS2j","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"# LB private / public: 23.41066 / 14.79618\ngrid_extree = {'n_estimators': [100],\n               'max_depth': [None],\n               'min_samples_split': [2],\n               'min_samples_leaf': [1],\n               'bootstrap': [False],\n              }\nextree_estim = SKLEstimator(ensemble.ExtraTreesRegressor(n_jobs=-1), grid_extree, verbose=1)\nextree_estim.fit_grid(features, verbose=True) # may take time to run!\nsubmit_df = submit_preds(extree_estim.get_best_estimator(), \"extree_1\", to_csv=True)\n\n\n### PCA: \n#extree_estim = SKLEstimator(ensemble.ExtraTreesRegressor(n_jobs=-1), grid_extree, verbose=1)\n#extree_estim.fit_grid(pca_features, pca_targ, verbose=True)\n#submit_df = submit_preds(extree_estim.get_best_estimator(), \"extree_pca_2\", pca_test, to_csv=True)\n\n#Best score: 12.03\n#Best parameters: {'bootstrap': False, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}","execution_count":0,"outputs":[]},{"metadata":{"id":"4tY7sdEGfk_Y","colab_type":"text"},"cell_type":"markdown","source":"### 3.2.3 sklearn.ensemble.GradientBoostingRegressor"},{"metadata":{"id":"R70T8T4j7CPW","colab_type":"text"},"cell_type":"markdown","source":"Модель градиентного бустинга sklearn несколько более привычная чем xgboost и позволяет довольно тонко настраивать модель за счет использования множества инструментов библиотеки sklearn (например, изменение метода выбора сверток с KFold на StratifiedKFold), и в данной ситуации модель показывает хорошие результаты (хоть и далекие от идеала), которые можно улучшать:"},{"metadata":{"id":"tFyxkHtPfmF1","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"# LB private / public: 24.61912 / 15.31381\ngrid_gboost = {'learning_rate': [1e-2, 1e-1, 5e-1, 1],\n               'n_estimators': [100, 200],\n               'min_samples_split': [2, 3],\n               'min_samples_leaf': [1, 3],\n               'max_depth': [3, 5],\n              }\n\nbest_params = {'learning_rate': [5e-1],\n               'max_depth': [5],\n               'min_samples_leaf': [3],\n               'min_samples_split': [2],\n               'n_estimators': [200]}\n\n#gboost_estim = SKLEstimator(ensemble.GradientBoostingRegressor(), best_params, verbose=1)\n#gboost_estim.fit_grid(features, verbose=True) # may take time to run!\n\n#Best score: 16.00\n#Best parameters: {'learning_rate': 0.1, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 100}","execution_count":0,"outputs":[]},{"metadata":{"id":"SAz9Z1ZH83dL","colab_type":"text"},"cell_type":"markdown","source":"# 4. Conclusions"},{"metadata":{"id":"ProzIMby85xP","colab_type":"text"},"cell_type":"markdown","source":"* Определены следующие модели, которые можно настаивать далее для поиска лучших параметов: XGBoost, GradientBoostingRegressor, ExtraTreesRegressor.\n\n* Выделение главных компонент хоть и выглядело обещающе, за счет того, что позволяло разделить данные на кластеры (не стояла задача снижения размерности данных, поскольку мы не имеем много фич, а скорее задача трансформации фич с использованием главных компонент). Возможна настройка алгоритма поиска главных компонент, либо можно использовать некоторые компоненты как отдельные фичи вместе с другими основными. Для этого, после тренировки моделей, можно оценить важность фич за счет аргумента feature_importance, доступного в некоторых моделях (например .feature_importances_ для XGBoost, GradientBoostingRegressor)."}],"metadata":{"colab":{"name":"solution1.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["83Sf3qpAQjOY","DBmrwNw2vXBr","bbT7zNKxJClp","UfaATkvGLsTH","7nFGG_6yvc_I","lExcXhPcTt8x","rS4xX704o9rk","R4tgYbTLtcvy","TbqcvOTOg-pU","ip-dpb2nSbgs","os9XAubPSc_C","wdqQYyCbfICt","jjGV9NEefLGG","4tY7sdEGfk_Y","SAz9Z1ZH83dL","C8CeSqyL8mXs"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":1}