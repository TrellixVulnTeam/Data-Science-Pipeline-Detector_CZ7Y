{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Store Item Demand Forecasting Challenge :\n\nDataset of the \"Store Item Demand Forecasting Challenge\" (https://www.kaggle.com/c/demand-forecasting-kernels-only/) is a time series related case study for me during my \"Data Science and Machine Learning\" bootcamp journey.\n\nI develop a LigthGBM model including advanced feature engineering about different type approaches ie. smoothings, lag/shift injections on series, linear/nonlinear forecasting projections, encodings, model tuning etc. My notebook is able to reach public scores between 13.84000 - 13.87000. I would like to share it and any discussion/comment is welcome.\n\nI use couple of shared notebooks to improve my notebook, and a list of them here:\n* https://www.kaggle.com/ashishpatel26/keeping-it-simple-by-xyzt\n* https://www.kaggle.com/miladdoostan/handling-outliers-feature-engineering-lgbm\n* https://www.kaggle.com/ymatioun/simple-lightgbm\n* https://www.kaggle.com/elitcohen/store-sales-eda-and-linear-drift-prediction\n* https://www.kaggle.com/abhilashawasthi/feature-engineering-lgb-model\n* https://www.kaggle.com/ekrembayar/store-item-demand-forecasting-with-lgbm\n","metadata":{}},{"cell_type":"code","source":"# !pip install lightgbm==2.3.1\n# import lightgbm\n# lightgbm.__version__","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing Libraries and Loading Datasets :","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport warnings\nimport time\nstart_time = time.time()\nwarnings.filterwarnings('ignore')\n\n# load datasets\ntrain = pd.read_csv(r'../input/demand-forecasting-kernels-only/train.csv', parse_dates=['date'], index_col=['date'])\ntest = pd.read_csv(r'../input/demand-forecasting-kernels-only/test.csv', parse_dates=['date'], index_col=['date'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Non-Linear Growth Rate Projection for 2018 :\n#### *(data pre-processing)*\n\nThe next cell is adapted from a notebook of https://www.kaggle.com/ashishpatel26/keeping-it-simple-by-xyzt. Basically, with a nonlinear growth rate projection the sales prediction for 2018 is calculated with a high precision on reference tables of monthly, store and day of week sales by considering a full year. Quite smart solution! I attempt similar solutions mostly focus on first three months of year, but I could not achieve better than ~14.1 even though I fully calibrate day of week between each year. I use this sale prediction to expand volume of train dataset with also test dataset in the LightGBM model. ","metadata":{}},{"cell_type":"code","source":"def sales_prediction():\n\n    # Expand dataframe with more useful columns\n    def expand_df(dataframe):\n        dataframe['day'] = dataframe.index.day\n        dataframe['month'] = dataframe.index.month\n        dataframe['year'] = dataframe.index.year\n        dataframe['dayofweek'] = dataframe.index.dayofweek\n        return dataframe\n\n    data = expand_df(train)\n\n    # Only data 2015 and after is used\n    new_data = data.loc[data.year >= 2015]\n    grand_avg = new_data.sales.mean()\n\n    # Day of week - Item Look up table\n    dow_item_table = pd.pivot_table(new_data, index='dayofweek', columns='item', values='sales', aggfunc=np.mean)\n\n    # Month pattern\n    month_table = pd.pivot_table(new_data, index='month', values='sales', aggfunc=np.mean) / grand_avg\n\n    # Store pattern\n    store_table = pd.pivot_table(new_data, index='store', values='sales', aggfunc=np.mean) / grand_avg\n\n    # weighted growth rate\n    year_table = pd.pivot_table(data, index='year', values='sales', aggfunc=np.mean) / grand_avg\n    years = np.arange(2013, 2019)\n    annual_growth = np.poly1d(np.polyfit(years[:-1], year_table.values.squeeze(), 2, w=np.exp((years - 2018) / 10)[:-1]))\n\n    pred_sales = []\n    for _, row in test.iterrows():\n        dow, month, year = row.name.dayofweek, row.name.month, row.name.year\n        item, store = row['item'], row['store']\n        base_sales = dow_item_table.at[dow, item]\n        mul = month_table.at[month, 'sales'] * store_table.at[store, 'sales']\n        pred_sales.append(int(np.round(base_sales * mul * annual_growth(year), 0)))\n\n    return pred_sales\n\n\n# extending train dataset with test dataset by sale prediction for 2018\ntest['sales'] = sales_prediction()\ntrain = train.loc[train.index.year >= 2015, :] # use only data after 2015\ndf = pd.concat([train, test], sort=False)\ndf.reset_index(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generating Datetime Related Features :","metadata":{}},{"cell_type":"code","source":"# create feature from datetime columns\ndef create_date_features(dataframe):\n    dataframe['month'] = dataframe.date.dt.month\n    dataframe['day_of_month'] = dataframe.date.dt.day\n    dataframe['day_of_year'] = dataframe.date.dt.dayofyear\n    dataframe['week_of_year'] = dataframe.date.dt.weekofyear\n    dataframe['day_of_week'] = dataframe.date.dt.dayofweek + 1\n    dataframe['year'] = dataframe.date.dt.year\n    dataframe['is_wknd'] = dataframe.date.dt.weekday // 4\n    dataframe['is_month_start'] = dataframe.date.dt.is_month_start.astype(int)\n    dataframe['is_month_end'] = dataframe.date.dt.is_month_end.astype(int)\n    dataframe['quarter'] = dataframe.date.dt.quarter\n    dataframe['week_block_num'] = [int(x) for x in np.floor((dataframe.date - pd.to_datetime('2012-12-31')).dt.days / 7) + 1]\n    dataframe['quarter_block_num'] = (dataframe['year'] - 2013) * 4 + dataframe['quarter']\n    dataframe['week_of_month'] = dataframe['week_of_year'].values // 4.35\n    return dataframe\n                                                                                                                             \n                                                                                                                                              \ndf = create_date_features(df)                                                                                                                 \n                                                                                                                                              \n# day labeling features                                                                       \ndf['is_Mon'] = np.where(df['day_of_week'] == 1, 1, 0)                                                                                            \ndf['is_Tue'] = np.where(df['day_of_week'] == 2, 1, 0)                                                                                         \ndf['is_Wed'] = np.where(df['day_of_week'] == 3, 1, 0)                                                                                         \ndf['is_Thu'] = np.where(df['day_of_week'] == 4, 1, 0)                                                                                         \ndf['is_Fri'] = np.where(df['day_of_week'] == 5, 1, 0)                                                                                         \ndf['is_Sat'] = np.where(df['day_of_week'] == 6, 1, 0)                                                                                         \ndf['is_Sun'] = np.where(df['day_of_week'] == 7, 1, 0)      ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generating Sale Aggregation Based Feature :","metadata":{}},{"cell_type":"code","source":"# generating some new features from aggregation of sales within different time frames\nfeat_list = ['day_of_week', 'week_of_month', 'week_of_year', 'month', 'quarter', 'is_wknd'] + ['day_of_week', 'week_of_month']\nshift_values = [0, 0, 0, 0, 0, 0, 12, 12]\nfor time_item, shift_val in zip(feat_list, shift_values):\n    grouped_df = df.groupby(['store', 'item', time_item])['sales'].expanding().mean().shift(shift_val).bfill().reset_index()\n    grouped_df.columns = ['store', 'item', time_item, 'date', time_item + f'_ex_avg_sale{str(shift_val)}']\n    grouped_df = grouped_df.sort_values(by=['item', 'store', 'date'])\n    df[time_item + f'_ex_avg_sale{str(shift_val)}'] = grouped_df[time_item + f'_ex_avg_sale{str(shift_val)}'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generating Smoothing based Features with Lag/Shift, Rolling Mean and Exponentially Weighted Techniques :","metadata":{}},{"cell_type":"code","source":"# make sure dataset sorted with original order                                                  \ndf.sort_values(by=['item', 'store', 'date'], axis=0, inplace=True) \n\n\n#generating some noise                                                                   \ndef random_noise(dataframe):                                                                                                                  \n    return np.random.normal(scale=0.01, size=(len(dataframe),))    \n\n\n# Lag/Shifted Features                                                                                                                                                      \n# generating laggy features with different time windows                                                                                                                                 \ndef lag_features(dataframe, lags):                                                                                                            \n    dataframe = dataframe.copy()                                                                                                              \n    for lag in lags:                                                                                                                          \n        dataframe['sales_lag_' + str(lag)] = dataframe.groupby([\"item\", \"store\"])['sales'].transform(lambda x: x.shift(lag)) + random_noise(dataframe)                                                                                 \n    return dataframe                                                                                                                          \n                                                                                                                                              \n                                                                                                                                              \ndf = lag_features(df, [91, 98, 105, 112, 119, 126, 182, 364, 546, 728])                                                                       \n                                                                                                                                 \n\n    \n# Rolling Mean Features                                                                                                                       \ndef roll_mean_features(dataframe, windows):                                                                                                   \n    dataframe = dataframe.copy()                                                                                                              \n    for window in windows:                                                                                                                    \n        dataframe['sales_roll_mean_' + str(window)] = dataframe.groupby([\"item\", \"store\"])['sales'].\\\n        transform(lambda x: x.shift(1).rolling(window=window, min_periods=10, win_type=\"triang\").mean()) + random_noise(dataframe)            \n    return dataframe                                                                                                                          \n                                                                                                                                              \n                                                          \ndf = roll_mean_features(df, [91, 182, 365, 546, 730])                                                                                         \n                                                                                                                                              \n\n    \n# Exponentially Weighted Mean Features                                                                                                        \ndef ewm_features(dataframe, alphas, lags):                                                                                                    \n    dataframe = dataframe.copy()                                                                                                              \n    for alpha in alphas:                                                                                                                      \n        for lag in lags:                                                                                                                      \n            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n            dataframe.groupby([\"item\", \"store\"])['sales'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())                       \n    return dataframe                                                                                                                          \n                                                                                                                                              \n                                                                                                                                              \nalphas = [0.95, 0.9, 0.8, 0.7, 0.5]                                             \nlags = [91, 98, 105, 112, 180, 270, 365, 546, 728]\ndf = ewm_features(df, alphas, lags)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Final step for Data preparation :","metadata":{}},{"cell_type":"code","source":"# One-Hot Encoding                                                                                                                            \ndf_dum = pd.get_dummies(df[['store', 'item', 'day_of_week', 'month', ]], columns=['store', 'item', 'day_of_week', 'month', ], dummy_na=True)  \ndf = pd.concat([df, df_dum], axis=1)                                                                                                          \n\n# convert to logarithmic scale                                                                                                           \ndf['sales'] = np.log1p(df[\"sales\"].values)\n\nprint(f'End of feature engineering and data preparation.') \nprint(f'It takes {int(time.time()-start_time)} sec.')\nprint(f'---=> final dataframe has {df.shape[1]} features <=---') ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LightGM Model with Final Dataset :","metadata":{}},{"cell_type":"code","source":"# MODEL VALIDATION\nstart_time = time.time()\nprint(\"Final model calculation starts..\")                                                                \ncols = [col for col in df.columns if col not in ['date', 'id', \"sales\", \"year\"]]                                                           \n\ntrain = df.loc[~df.sales.isna()]                                                                                                              \nX_train, Y_train = train[cols], train['sales']                                                                                                                         \n                                                                                                                                              \ntest = df.loc[df.id.notnull()]                                                                                                                \nX_test = test[cols]                                                                                                                           \n                                                                                                                                              \niteration = 15000\n                                                                                                       \nlgb_params = {                                                                                                                            \n        'nthread': -1,\n        'metric': 'mae',\n        'boosting_type': 'gbdt',    \n        'max_depth': 7,\n        'num_leaves': 28,   \n        'task': 'train',                                                                                                                      \n        'objective': 'regression_l1',                                                                                                         \n        'learning_rate': 0.05,                                                                                                                \n        'feature_fraction': 0.9,                                                                                                              \n        'bagging_fraction': 0.8,                                                                                                              \n        'bagging_freq': 5,                                                                                                                    \n        'lambda_l1': 0.06,                                                                                                                    \n        'lambda_l2': 0.05,                                                                                                                    \n        'verbose': -1,     }                                                                                                                           \n                                                                                                                                              \n# LightGBM dataset                                                                                                                        \nlgbtrain_all = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)                                                                \nfinal_model = lgb.train(lgb_params, lgbtrain_all, num_boost_round=iteration)                                                              \ntest_preds = final_model.predict(X_test, num_iteration=iteration)\nprint(f'The model calculation is done in {int(time.time()-start_time)} sec.')   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generating the Submission File :","metadata":{}},{"cell_type":"code","source":"# create submission file\nsubmission = pd.DataFrame({ 'id': [*range(45000)], 'sales': np.round(np.expm1(test_preds),0) }) # turn back to normal scale\nsubmission['sales'] = submission.sales.astype(int)\nsubmission.to_csv('submission.csv', index=False)\nprint(f'OK, Submission file is created!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}