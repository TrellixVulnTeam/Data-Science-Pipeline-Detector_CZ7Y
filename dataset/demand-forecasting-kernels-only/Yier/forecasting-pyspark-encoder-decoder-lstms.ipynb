{"cells":[{"metadata":{},"cell_type":"markdown","source":"# multi-step time series forecasting\n\nA multi-step time series forecasting model is built in this notbook. In other word, past sequences are used to forecast the next 90 steps in the future. This fits in the the idea of seq2seq model, in which both inputs and outputs are sequences. For this task, the dataset needs to be prepared accordingly. \n\nSince we have 10 * 50 independent time series, I find it's easy to use spark window functions to handle feature and sequence generation. And then convert the Spark DataFrame to a PyTorch DataLoader using petastorm spark_dataset_converter. More information can be found in the following link:\n* https://databricks.com/notebooks/simple-aws/petastorm-spark-converter-pytorch.html","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip -q install pyspark\n!pip -q install petastorm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport time\nimport math\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, Window\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as f\n\nfrom pyspark.ml import Transformer, Pipeline\nfrom pyspark.ml.feature import VectorAssembler, OneHotEncoder\n\nfrom petastorm.spark import SparkDatasetConverter, make_spark_converter\nfrom petastorm import TransformSpec\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndef timeSince(since):\n    now = time.time()\n    s = now - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\ndef smape(forecast, actual):\n    f = np.asarray(forecast)\n    a = np.asarray(actual)\n    up = np.abs(f - a)\n    down = (np.abs(f) + np.abs(a))/2\n    np.mean(up/down)\n    return np.mean(up/down)*100\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nn_store = 10\nn_item = 50\nn_ts = 1826\nn_pred = 90","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spark = SparkSession.builder.master(\"local[*]\").appName(\"retail_demand_forecasting\").getOrCreate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"schema = StructType([StructField(\"date\", DateType()),StructField(\"store\", IntegerType()),\n                     StructField(\"item\", IntegerType()),StructField(\"sales\", FloatType())])\ntrain_df = spark.read.csv(path = '/kaggle/input/demand-forecasting-kernels-only/train.csv', schema=schema, header = True).cache()\ntrain_df.printSchema()\n\nschema = StructType([StructField(\"id\", IntegerType()),\n                     StructField(\"date\", DateType()),StructField(\"store\", IntegerType()),\n                     StructField(\"item\", IntegerType())])\ntest_df = spark.read.csv(path = '/kaggle/input/demand-forecasting-kernels-only/test.csv', schema=schema, header = True).cache()\ntest_df.printSchema()\n\n\ntrain_df = train_df.withColumn('type',f.lit(\"train\"))\ntrain_df = train_df.withColumn('id',f.lit(None))\n\ntest_df = test_df.withColumn('type',f.lit(\"test\"))\ntest_df = test_df.withColumn('sales',f.lit(None))\n\ndf = train_df.unionByName(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. make features use spark","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class logTransform(Transformer):\n    def __init__(self, inputCol, outputCol):\n        self.inputCol = inputCol\n        self.outputCol = outputCol\n\n    def _transform(self, df):\n        return df.withColumn(self.outputCol, f.log1p(f.col(self.inputCol)))\n    \nclass targetMaker(Transformer):\n    def __init__(self, inputCol, outputCol='target', dateCol='date', idCol=['store', 'item'], Range = 90):\n        self.inputCol = inputCol\n        self.outputCol = outputCol\n        self.dateCol = dateCol\n        self.idCol = idCol\n        self.Range = Range\n        \n    def _transform(self, df):\n        w = Window.partitionBy(self.idCol).orderBy(self.dateCol).rowsBetween(0, self.Range - 1)\n        df = df.withColumn(self.outputCol, f.collect_list(self.inputCol).over(w))\n        return df\n    \nclass seriesMaker(Transformer):\n    def __init__(self, inputCol, outputCol='input', dateCol='date', idCol=['store', 'item'], Range = 120):\n        self.inputCol = inputCol\n        self.outputCol = outputCol\n        self.dateCol = dateCol\n        self.idCol = idCol\n        self.Range = Range\n        \n    def _transform(self, df):\n        w = Window.partitionBy(self.idCol).orderBy(self.dateCol).rowsBetween(-self.Range, -1)\n        df = df.withColumn(self.outputCol, f.collect_list(self.inputCol).over(w))\n        return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"WINDOW_SIZE = 180\n# Feature extraction\nlogt = logTransform(inputCol ='sales', outputCol='logSales')\ntgtm = targetMaker(inputCol = 'logSales', Range = n_pred)\nsrsm = seriesMaker(inputCol = 'logSales', Range = WINDOW_SIZE)\nencoder = OneHotEncoder(inputCols=[\"store\",\"item\",],outputCols=[\"storeVec\",\"itemVec\"])\nassembler = VectorAssembler(inputCols=[\"storeVec\",\"itemVec\"], outputCol=\"covariates\")\n\npipeline = Pipeline(stages=[logt, tgtm, srsm, encoder, assembler])\nprocessing = pipeline.fit(df)\ntransformed = processing.transform(df)\ntransformed.printSchema()\n\ntransformed_train = transformed.filter( (f.size('input')>=WINDOW_SIZE) & (f.size('target')>=n_pred) & (f.col('type') == 'train') )\ntransformed_test = transformed.filter( (f.size('input')>=WINDOW_SIZE) & (f.col('type') == 'test'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Cache the Spark DataFrame using Petastorm Spark converter","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, \"file:///dbfs/tmp/petastorm/cache\")\n\nconverter_train = make_spark_converter(transformed_train.select('input','covariates','target'))\nconverter_test = make_spark_converter(transformed_test.select('input','covariates', 'store', 'item'))\n\nprint(f\"train: {len(converter_train)}, val: {len(converter_test)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class multiStepModel(nn.Module):\n    def __init__(self,  input_size = 1, covar_size = 60, output_size = 1, output_seq_len = 90, h1_dim = 32, h2_dim = 32):\n        super().__init__()\n        \n        self.input_size = input_size\n        self.output_size = output_size\n        self.output_seq_len = output_seq_len\n        self.h1_dim = h1_dim\n        self.h2_dim = h2_dim\n        \n        self.lstm_layer1 = nn.LSTM(input_size + covar_size, h1_dim, num_layers = 1, batch_first = True)\n        self.lstm_layer2 = nn.LSTM(h1_dim + covar_size, h2_dim, num_layers = 1, batch_first = True)\n        self.fc_layer = nn.Linear(h2_dim, output_size)\n        self.dropout = nn.Dropout(0.2)\n        \n\n    def forward(self, input, covar):\n        input_seq_len = input.size(1)\n        x = covar.unsqueeze(1).repeat(1, input_seq_len, 1) # expand to input seq length\n        lstm1_input = torch.cat([input, x], dim = 2) # combine with input seq\n        output, (hn, cn) = self.lstm_layer1(lstm1_input)\n        lstm1_output = F.relu(hn[-1]) # get the last hidden state of the last LSTM layer\n        \n        x = torch.cat([lstm1_output, covar], dim = 1).unsqueeze(1) # combine with covariates\n        lstm2_input = x.repeat(1, self.output_seq_len, 1)\n        output, (hn, cn) = self.lstm_layer2(lstm2_input)\n        lstm2_output = F.relu(output)\n        \n        fc_input = self.dropout(lstm2_output)\n        out = self.fc_layer(lstm2_output).squeeze()\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_one_epoch(dataiter, steps_per_epoch):\n    model.train()  # Set model to training mode\n\n    curr_loss = []\n    for step in range(1, steps_per_epoch+1):\n        pd_batch = next(dataiter)\n        x1 = pd_batch['input'].unsqueeze(2).to(device)\n        x2 = pd_batch['covariates'].to(device)\n        y = pd_batch['target'].to(device)\n    \n        # Track history in training\n        with torch.set_grad_enabled(True):\n            optimizer.zero_grad()\n\n            out = model(x1, x2)\n            loss = loss_fn(out, y)\n\n            loss.backward()\n            optimizer.step()\n\n        curr_loss.append(loss.item())\n        print('\\rprogress {:6.2f} %\\tloss {:8.4f}'.format(round(100*step/steps_per_epoch, 2), np.mean(curr_loss)), end = \"\")\n  \n    epoch_loss = np.mean(curr_loss)\n    print('\\rprogress {:6.2f} %\\tloss {:8.4f}'.format(round(100*step/steps_per_epoch, 2), epoch_loss ))\n    return epoch_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = 1e-3\nBATCH_SIZE = 128\nNUM_EPOCHS = 50\n\nmodel = multiStepModel().to(device)\noptimizer = optim.Adam(model.parameters(), lr = LR)\nloss_fn = nn.MSELoss()\n\nstart = time.time()\nwith converter_train.make_torch_dataloader(batch_size = BATCH_SIZE) as trainloader:\n    trainiter = iter(trainloader)\n    steps_per_epoch = len(converter_train) // BATCH_SIZE\n    for epoch in range(1, NUM_EPOCHS+1):\n        print('-' * 10)\n        print('Epoch {}/{}\\t{} batches'.format(epoch, NUM_EPOCHS, steps_per_epoch))\n        epoch_loss = train_one_epoch(trainiter, steps_per_epoch)\n        print('{}'.format(timeSince(start)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. forecast","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"date_df = test_df.select('date').distinct().sort('date').toPandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def forecast(model, converter_test):\n    \n    model.eval()  # Set model to evaluate mode\n    \n    with converter_test.make_torch_dataloader(batch_size = 1, num_epochs = 1) as testloader:\n        testiter = iter(testloader)\n        steps_per_epoch = len(converter_test)\n\n        final_df = pd.DataFrame()\n\n        for step in range(1, steps_per_epoch+1):\n        \n            pd_batch = next(testiter)\n            x1 = pd_batch['input'].unsqueeze(2).to(device)\n            x2 = pd_batch['covariates'].to(device)\n    \n            with torch.set_grad_enabled(False):\n                out = model(x1, x2)\n        \n            curr_df = date_df\n            curr_df['sales'] = np.expm1(out.tolist()).round()\n            curr_df['store'] = pd_batch['store'].item()\n            curr_df['item'] = pd_batch['item'].item()\n            final_df = final_df.append(curr_df)\n        \n            print('\\rprogress {:6.2f} %'.format(round(100*step/steps_per_epoch, 2)), end = \"\")\n        print('\\rprogress {:6.2f} %'.format(round(100*step/steps_per_epoch, 2)))\n    \n    return final_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df = forecast(model, converter_test)\n\ntest_df = test_df.drop('sales')\nsub_df = test_df.toPandas().merge(final_df, how = 'left', on = ['date','store','item'])\n\nsub_df[['id','sales']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}