{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport pandas as pd\n# from  pyecharts import *\nimport os\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.models import Sequential, load_model\nfrom sklearn.model_selection import  train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport collections\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error\n\n\n# 数据探查\ntrain_data=pd.read_csv(r'/kaggle/input/demand-forecasting-kernels-only/train.csv',engine='python')\ntrain_columns=train_data.columns #[date,store,item,sales],data:1826,2013-01-01,2017-12-31;store:1-10;item:1-50;\n\n#判断数据中是否含有空置\nisnull_data=train_data[train_data.isnull().T.any()]\n\n#判断每个店铺不同商品的异常值\nfor i,item in enumerate(list(set(train_data['store']))):\n    for j,key in enumerate(list(set(train_data['item']))):\n        percentile=np.percentile(train_data[(train_data['store']==item)&(train_data['item']==key)]['sales'],[0,25,50,75,100])\n        #q1-1.5*iqr,q3+1.5*iqr 异常点\n        low=percentile[1]-1.5*(percentile[3]-percentile[1])\n        up=percentile[3]+1.5*(percentile[3]-percentile[1])\n        #num=train_data[(train_data['store']==item)&(train_data['item']==key)&((train_data['sales']>up)|(train_data['sales']<low))]['sales'].count()\n        train_data.loc[(train_data['store']==item)&(train_data['item']==key)&(train_data['sales']>up),'sales']=up\n        train_data.loc[(train_data['store']==item)&(train_data['item']==key)&(train_data['sales']<low),'sales']=low\n\n# train_data=train_data[~train_data.index.isin(diff_total_percentile_index)]\n# train_data=train_data.reset_index(drop=True)\n# train_data=pd.DataFrame(train_data,columns=['date', 'store', 'item', 'sales'])\nprint(train_data.dtypes)\n\n\n#生成训练数据集，data:DataFrame,timestep：记忆时间跨度\ndef creata_dataset(data,timestep):\n    x_data,y_data=[],[]\n    for index in range(len(data)-timestep-1):\n        x_data.append(data[index:index + timestep])\n        y_data.append(data[index + timestep])\n\n    return np.array(x_data),np.array(y_data)\n##########以总的门店为例,单变量测试分析################\ntrain_data_1=pd.DataFrame(train_data['sales'],columns=['sales'])\n\ntrainsize=int(len(train_data_1)*0.7)\ntrain_data_1=train_data_1.values\ntrain_data_1 = train_data_1.astype('float32')\n\n#归一化 在下一步会讲解\nscaler = MinMaxScaler(feature_range=(0, 1))\ntrain_data_1 = scaler.fit_transform(train_data_1)\n\nx_train,y_train=creata_dataset(train_data_1[:trainsize],5)\nx_test,y_test=creata_dataset(train_data_1[trainsize:],5)\n\nprint(x_train,y_train)\n\nx_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1],1))\nx_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1],1))\n\nif os.path.exists('/kaggle/input/kernel3f8d042026/Test_md1.h5'):\n    model = load_model(\"/kaggle/input/kernel3f8d042026/Test_md1.h5\")\nelse:\n    model = Sequential()\n    model.add(LSTM(50, input_shape=(None,1)))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', optimizer='adam', metrics=[\"accuracy\"])\n    model.fit(x_train, y_train, epochs=100,batch_size=100, verbose=2)\n    model.save(\"./Test_md1.h5\")   \nprint('***********训练数据训练已完成*******************')\ntestPredict = model.predict(x_test)\ntrainPredict = model.predict(x_train)\n\n#反归一化\ntrainPredict = scaler.inverse_transform(trainPredict)\ny_train = scaler.inverse_transform(y_train)\ntestPredict = scaler.inverse_transform(testPredict)\ny_test = scaler.inverse_transform(y_test)\n\n\n#以门店1为例测试值与预测值\n# line2=Line('门店1的预测值与真实值对比情况')\n# line2.add('真实值',[i for i in range(len(y_test))],[item[0] for item in y_test.tolist()],is_datazoom_show=True,tooltip_tragger='axis')\n# line2.add('测试值',[i for i in range(len(testPredict))],[item[0] for item in testPredict.tolist()],is_datazoom_show=True,tooltip_tragger='axis')\n# line2.render('门店1的预测值与真实值对比情况.html')\n\nplt.plot(y_train)\nplt.plot(trainPredict[1:])\nplt.show()\nplt.plot(y_test)\nplt.plot(testPredict[1:])\nplt.show()\n\n#测试数据预测\ntest_data=pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/sample_submission.csv',engine='python')\ntest_data=pd.DataFrame(test_data['sales'],columns=['sales'])\ntest_data=test_data.values\ntest_data=test_data.astype('float32')\n\nscaler=MinMaxScaler(feature_range=(0,1))\ntest_data=scaler.fit_transform(test_data)\ntest_data=test_data.reshape((test_data.shape[0],test_data.shape[1],1))\ntest_predict=model.predict(test_data)\n\n#test_predict=test_data.reshape((test_predict.shape[0],test_predict[2]))\ntest_predict=scaler.inverse_transform(test_predict)\n\ny_test=pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/sample_submission.csv',engine='python')\ny_test=pd.DataFrame(y_test['sales'],columns=['sales'])\ny_submission=y_test.values\nval_loss=sqrt(mean_squared_error(y_submission,test_predict))\npd.DataFrame(test_predict,columns=['sales']).to_csv('test_predict.csv',encoding='utf-8')\nprint('***********测试数据预测已完成,均方误差：%f*******************'%val_loss)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"##########以总的门店为例,多变量测试分析################\n\n#查看总的时间所有门店销售情况\n# Line=Line('散点图','所有门店总体销售数')\n# date=train_data[(train_data['store']==1) & (train_data['item']==1)]['date']\n# sales=train_data[(train_data['store']==1) & (train_data['item']==1)]['sales']\n# Line.add('2013-2017年总体销售数量',date,sales,xaxis_name='日期',yaxis_name='销售量',is_datazoom_show=True,tooltip_tragger='axis',legend_text_size=10)\n# Line.render('所有门店总体销售数走势图.html')\n\n#查看每个门店的销售情况\n# shop_sales=train_data.groupby(['date','store'],as_index=False)['sales'].sum() #生成data_groupl类型数据,需循环计算\nshops=set(train_data['store'])\n# line2=Line('不同门店的销售情况')\n# print(shop_sales[shop_sales['store']==1])\n# for shop in shops:\n#     line2.add('商家%s'%shop,shop_sales[shop_sales['store']==shop]['date'],shop_sales[shop_sales['store']==shop]['sales'],is_datazoom_show=True,tooltip_tragger='axis')\n# line2.render('不同门店的销售情况.html')\n\n#查看不同门店不同商品销售情况\n# items=set(train_data['item'])\n\n# for shop in shops:\n#     line3=Line('门店%s不同商品的销售情况'%shop)\n#     for item in items:\n#         line3.add('商品%s'%item,train_data[(train_data['item']==item)&(train_data['store']==shop)]['date'],train_data[(train_data['item']==item)&(train_data['store']==shop)]['sales'],is_datazoom_show=True,tooltip_tragger='axis')\n#     line3.render('门店%s不同商品的销售情况.html'%shop)\n\n#计算每个商品的每月平均销售量\ntrain_data['mouth']=train_data['date'].str.extract('\\d+-(\\d+)-\\d+')\ndiff_item_mouth_avg_sales=train_data.groupby(['mouth','item'],as_index=False)['sales'].mean().rename(columns={'sales':'diff_item_mouth_avg_sales'})\ntrain_data=pd.merge(train_data,diff_item_mouth_avg_sales,how='left',on=['mouth','item'])\nprint('*****训练数据：每个商品的每月平均销售量*****,计算已完成')\n\n#计算不同商店不同商品每月平均销售量\ndiff_shop_item_mouth_avg_sales=train_data.groupby(['mouth','item','store'],as_index=False)['sales'].mean().rename(columns={'sales':'diff_shop_item_mouth_avg_sales'})\ntrain_data=pd.merge(train_data,diff_shop_item_mouth_avg_sales,how='left',on=['mouth','item','store'])\nprint('****训练数据：不同商店不同商品每月平均销售量*****,计算已完成')\n\n#计算不同商店每月平均销售量\ndiff_shop_mouth_avg_sales=train_data.groupby(['mouth','store'],as_index=False)['sales'].mean().rename(columns={'sales':'diff_shop_mouth_avg_sales'})\ntrain_data=pd.merge(train_data,diff_shop_mouth_avg_sales,how='left',on=['mouth','store'])\nprint('****训练数据：不同商店每月平均销售量*****,计算已完成')\n\n#计算不同商店每天的销售量\ntrain_data['day']=train_data['date'].str.extract('\\d+-(\\d+-\\d+)')\ndiff_shop_day_avg_sales=train_data.groupby(['day','store'],as_index=False)['sales'].mean().rename(columns={'sales':'diff_shop_day_avg_sales'})\ntrain_data=pd.merge(train_data,diff_shop_day_avg_sales,how='left',on=['day','store'])\nprint('****训练数据：不同商店每天的销售量*****,计算已完成')\n\n#计算不同商店不同商品每天平均销售量\ndiff_shop_item_day_avg_sales=train_data.groupby(['day','item','store'],as_index=False)['sales'].mean().rename(columns={'sales':'diff_shop_item_day_avg_sales'})\ntrain_data=pd.merge(train_data,diff_shop_item_day_avg_sales,how='left',on=['day','item','store'])\nprint('****训练数据：不同商店不同商品每天平均销售量*****,计算已完成')\n\n#计算每个商品的每天平均销售量\ndiff_item_day_avg_sales=train_data.groupby(['day','item'],as_index=False)['sales'].mean().rename(columns={'sales':'diff_item_day_avg_sales'})\ntrain_data=pd.merge(train_data,diff_item_day_avg_sales,how='left',on=['day','item'])\nprint('****训练数据：每个商品的每天平均销售量*****,计算已完成')\n\n#计算每个商店的每个日期平均销量\ndiff_store_date_avg_sales=train_data.groupby(['date','store'],as_index=False)['sales'].mean().rename(columns={'sales':'diff_store_date_avg_sales'})\ntrain_data=pd.merge(train_data,diff_store_date_avg_sales,how='left',on=['date','store'])\nprint('****训练数据：每个商店的每个日期平均销量*****,计算已完成')\n\n#计算每个商店每个商品不同季度的销售量\ndef reg_quarter(mouth):\n    if int(mouth)<=3:\n        return 1\n    elif int(mouth)<=6:\n        return 2\n    elif int(mouth)<=9:\n        return 3\n    else:\n        return 4\n\ntrain_data['quarter']=train_data['mouth'].apply(reg_quarter)\ndiff_store_item_quarter_avg_sales=train_data.groupby(['quarter','item','store'],as_index=False)['sales'].mean().rename(columns={'sales':'diff_store_item_quarter_avg_sales'})\ntrain_data=pd.merge(train_data,diff_store_item_quarter_avg_sales,how='left',on=['quarter','item','store'])\nprint('****训练数据：每个商店每个商品不同季度的销售量*****,计算已完成')\n\n#计算每个商店不同季度的销售量\ndiff_store_quarter_avg_sales=train_data.groupby(['quarter','store'],as_index=False)['sales'].mean().rename(columns={'sales':'diff_store_quarter_avg_sales'})\ntrain_data=pd.merge(train_data,diff_store_quarter_avg_sales,how='left',on=['quarter','store'])\nprint('****训练数据：每个商店不同季度的销售量*****,计算已完成')\n\n#计算每个商品不同季度的销售量\ndiff_item_quarter_avg_sales=train_data.groupby(['quarter','item'],as_index=False)['sales'].mean().rename(columns={'sales':'diff_item_quarter_avg_sales'})\ntrain_data=pd.merge(train_data,diff_item_quarter_avg_sales,how='left',on=['quarter','item'])\nprint('****训练数据：每个商品不同季度的销售量*****,计算已完成')\n\n#将商店以及商品one-hot稀疏化\n\nfrom sklearn.preprocessing import OneHotEncoder\n#将store稀疏化(one-hot)\ndef scartter_type_jhcz(name:str,num:int):\n    store_sales=[]\n    for i in range(len(set(train_data[name]))):\n        sales=train_data[train_data[name]==(i+1)]['sales'].T\n        store_sales.append(list(sales))\n\n    scartter=MinMaxScaler(feature_range=(0,1))\n    store_sales=scartter.fit_transform(store_sales)\n    model=KMeans(n_clusters=num)\n    model.fit(store_sales)\n    y=model.predict(store_sales)\n    store_sales=scartter.inverse_transform(store_sales)\n    store_class=collections.defaultdict(lambda:[])\n    store_data=collections.defaultdict(lambda:[])\n    for i,item in enumerate(y):\n        store_class[item].append(i+1)\n        store_data[item].append(store_sales[i])\n\n    return store_class,store_data,y\nstore_class,store_data,y=scartter_type_jhcz('store',2)\nitem_class,item_data,y=scartter_type_jhcz('item',3)\n#item_class:{1: [3, 4, 5, 16, 17, 23, 27, 34, 37, 40, 41, 42, 44, 47, 49], 2: [6, 7, 9, 14, 19, 20, 21, 26, 30, 31, 32, 39, 43, 46, 48], 0: [10, 11, 12, 13, 15, 18, 22, 24, 25, 28, 29, 33, 35, 36, 38, 45, 50]}) \nplt.scatter(np.array(item_data[0])[:,0],np.array(item_data[0])[:,1])\nplt.scatter(np.array(item_data[1])[:,0],np.array(item_data[1])[:,1])\nplt.scatter(np.array(item_data[2])[:,0],np.array(item_data[2])[:,1])\nplt.show()\ntrain_data['type']=0\nfor i,item in enumerate(item_class.keys()):\n    train_data.loc[train_data['item'].isin(item_class[item]),'type']=item #这样赋值怎末不成功？\n\ntrain_data['class']=0\nfor i,item in enumerate(store_class.keys()):\n    train_data.loc[train_data['store'].isin(store_class[item]),'class']=item\n    \ndf_item_dummy=pd.get_dummies(train_data['type'],prefix='item')\ntrain_data=pd.concat([train_data,df_item_dummy],axis=1)\n\n# df_store_dummy=pd.get_dummies(train_data['class'],prefix='store')\n# train_data=pd.concat([train_data,df_store_dummy],axis=1)\n\nprint(set(df_item_dummy),set(train_data['type']),train_data.dtypes,train_data[train_data['item'].isin(item_class[1])]['type'])\ndataset=pd.DataFrame(train_data[['class','item_0','item_1','item_2','diff_item_mouth_avg_sales','diff_shop_item_mouth_avg_sales'\n,'diff_shop_mouth_avg_sales','diff_shop_day_avg_sales','diff_shop_item_day_avg_sales'\n,'diff_item_day_avg_sales','diff_store_item_quarter_avg_sales','diff_store_quarter_avg_sales','diff_item_quarter_avg_sales','sales']],columns=['class','item_0','item_1','item_2','diff_item_mouth_avg_sales','diff_shop_item_mouth_avg_sales'\n,'diff_shop_mouth_avg_sales','diff_shop_day_avg_sales','diff_shop_item_day_avg_sales'\n,'diff_item_day_avg_sales','diff_store_item_quarter_avg_sales','diff_store_quarter_avg_sales','diff_item_quarter_avg_sales','sales'])\n\n# dataset=pd.DataFrame(train_data[(train_data['store']==1)&(train_data['item']==1)][['store_1','store_2','store_3','store_4','store_5','store_6','store_7','store_8','store_9','store_10'\n# ,'item_0','item_1','item_2','diff_item_day_avg_sales','diff_store_date_avg_sales','sales']])\n# dataset.to_csv('dataset.csv')\n# print(dataset.to_csv('dataset.csv'))\ndataset=dataset.values\ndataset=dataset.astype('float32')\n\n#特征选择\n# from sklearn.feature_selection import SelectKBest\n# from sklearn.feature_selection import chi2\n# test = SelectKBest(score_func=chi2, k=8)\n# X=dataset[:,:-1]\n# Y=dataset[:,-1]\n# fit = test.fit(X, Y)\n# print(fit.scores_)\n\n#所有变量归一化:仅对特征变量进行归一化，减少特征之间的差异\nscaler=MinMaxScaler(feature_range=(0,1))\ndataset_feature=scaler.fit_transform(dataset[:,:-1])\n\n#切分训练集与测试集\ntrainsize=int(len(dataset_feature)*0.65)\ntrain_list=dataset_feature[:trainsize,:]\nx_train,y_train=train_list[:,:],dataset[:trainsize,-1].reshape(-1, 1)\n\ntest_list=dataset_feature[trainsize:,:]\nx_test,y_test=test_list[:,:],dataset[trainsize:,-1].reshape(-1, 1)\n\n#reshape==>[samples,timestep,features]\nx_train=x_train.reshape((x_train.shape[0],1,x_train.shape[1]))\nx_test=x_test.reshape((x_test.shape[0],1,x_test.shape[1]))\n\nprint(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n\nif os.path.exists('./all_store.h5'):\n    model=load_model('./all_store.h5')\nelse:\n    #运行报错：numpy.linalg.LinAlgError: SVD did not converge;1.存在异常数据：NAN INF; 2.数据量太大导致内存不足报错\n    #design network\n    model=Sequential()\n    model.add(LSTM(100,input_shape=(x_train.shape[1],x_train.shape[2])))\n    model.add(Dense(1))\n    model.compile(loss='mae', optimizer='adam')\n    history =model.fit(x_train, y_train, epochs=100, batch_size=100, validation_data=(x_test, y_test), verbose=2, shuffle=False)\n    model.save('all_store.h5')\n\n    # plot history\n    plt.plot(history.history['loss'], label='train')\n    plt.plot(history.history['val_loss'], label='test')\n    plt.legend()\n    plt.show()\n\n# make a prediction\ntest_predict = model.predict(x_test)\n# x_test = x_test.reshape((x_test.shape[0], x_test.shape[2]))\n# # invert scaling for forecast\n# # inv_yhat = np.concatenate((test_predict, x_test[:, 1:]), axis=1)\n# inv_yhat = np.concatenate((test_predict, x_test), axis=1)\n# inv_yhat = scaler.inverse_transform(inv_yhat)\n# inv_yhat = inv_yhat[:,0]\n# # invert scaling for actual\n# inv_y=np.concatenate((y_test,x_test),axis=1)\n# inv_y = scaler.inverse_transform(inv_y)\n# inv_y = inv_y[:,0]\nrmse = sqrt(mean_squared_error(test_predict, y_test))\nprint('Test RMSE: %.3f' % rmse)\n\n##########预测分析################\npredict_data=pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/test.csv',engine='python')\n#计算预测数据的衍生变量\n\n#计算每个商品的每月平均销售量\npredict_data['mouth']=predict_data['date'].str.extract('\\d+-(\\d+)-\\d+')\npredict_data=pd.merge(predict_data,diff_item_mouth_avg_sales,how='left',on=['mouth','item'])\nprint('*****预测数据：每个商品的每月平均销售量*****,计算已完成')\n\n#计算不同商店不同商品每月平均销售量\npredict_data=pd.merge(predict_data,diff_shop_item_mouth_avg_sales,how='left',on=['mouth','item','store'])\n\nprint('****预测数据：不同商店不同商品每月平均销售量*****,计算已完成')\n\n#计算不同商店每月平均销售量\npredict_data=pd.merge(predict_data,diff_shop_mouth_avg_sales,how='left',on=['mouth','store'])\nprint('****预测数据：不同商店每月平均销售量*****,计算已完成')\n\n#计算不同商店每天的销售量\npredict_data['day']=predict_data['date'].str.extract('\\d+-(\\d+-\\d+)')\npredict_data=pd.merge(predict_data,diff_shop_day_avg_sales,how='left',on=['day','store'])\nprint('****预测数据：不同商店每天的销售量*****,计算已完成')\n\n#计算不同商店不同商品每天平均销售量\npredict_data=pd.merge(predict_data,diff_shop_item_day_avg_sales,how='left',on=['day','item','store'])\nprint('****预测数据：不同商店不同商品每天平均销售量*****,计算已完成')\n\n#计算每个商品的每天平均销售量\npredict_data=pd.merge(predict_data,diff_item_day_avg_sales,how='left',on=['day','item'])\nprint('****预测数据：每个商品的每天平均销售量*****,计算已完成')\n\n#计算每个商店每个商品不同季度的销售量\npredict_data['quarter']=predict_data['mouth'].apply(reg_quarter)\npredict_data=pd.merge(predict_data,diff_store_item_quarter_avg_sales,how='left',on=['quarter','item','store'])\nprint('****训练数据：每个商店每个商品不同季度的销售量*****,计算已完成')\n\n#计算每个商店不同季度的销售量\npredict_data=pd.merge(predict_data,diff_store_quarter_avg_sales,how='left',on=['quarter','store'])\nprint('****训练数据：每个商店不同季度的销售量*****,计算已完成')\n\n#计算每个商品不同季度的销售量\npredict_data=pd.merge(predict_data,diff_item_quarter_avg_sales,how='left',on=['quarter','item'])\nprint('****训练数据：每个商品不同季度的销售量*****,计算已完成')\n\n#计算每个商店的每个日期平均销量\n# predict_data=pd.merge(predict_data,diff_store_date_avg_sales,how='left',on=['date','store'])\n# print('****预测数据：每个商店的每个日期平均销量*****,计算已完成')\n\n#商店one-hot\n# predict_store_onehot=pd.get_dummies(predict_data['store'],prefix='store')\n# predict_data=pd.concat([predict_store_onehot,predict_data],axis=1)\npredict_data['class']=0\nfor i,item in enumerate(store_class.keys()):\n    predict_data.loc[predict_data['store'].isin(store_class[item]),'class']=item\n\n#商品one-hot\npredict_data['type']=0\nfor i,item in enumerate(item_class.keys()):\n    predict_data.loc[predict_data['item'].isin(item_class[item]),'type']=item\n\n\npredict_item_onehot=pd.get_dummies(predict_data['type'],prefix='item')\npredict_data=pd.concat([predict_item_onehot,predict_data],axis=1)\n\npredict_data=pd.DataFrame(predict_data[['class','item_0','item_1','item_2','diff_item_mouth_avg_sales','diff_shop_item_mouth_avg_sales'\n,'diff_shop_mouth_avg_sales','diff_shop_day_avg_sales','diff_shop_item_day_avg_sales'\n,'diff_item_day_avg_sales','diff_store_item_quarter_avg_sales','diff_store_quarter_avg_sales','diff_item_quarter_avg_sales']],columns=['class','item_0','item_1','item_2','diff_item_mouth_avg_sales','diff_shop_item_mouth_avg_sales'\n,'diff_shop_mouth_avg_sales','diff_shop_day_avg_sales','diff_shop_item_day_avg_sales'\n,'diff_item_day_avg_sales','diff_store_item_quarter_avg_sales','diff_store_quarter_avg_sales','diff_item_quarter_avg_sales'])\n\npredict_data=predict_data.values\nscaler=MinMaxScaler(feature_range=(0,1))\npredict_data=scaler.fit_transform(predict_data)\n\npredict_data=predict_data.reshape((predict_data.shape[0],1,predict_data.shape[1]))\nprint(predict_data.shape)\npredict_y=model.predict(predict_data)\n# predict_data=predict_data.reshape((predict_data.shape[0],predict_data.shape[2]))\n# predict_y=np.concatenate((predict_y,predict_data[:,1:]),axis=1)\n# predict_y=scaler.inverse_transform(predict_y)\n# print(predict_y[:5,:])\n# predict_y=predict_y[:,0]\nval_loss=sqrt(mean_squared_error(y_submission,predict_y))\nprint('**************多变量损失：%f*******************'%val_loss)\npredict_y=pd.DataFrame(predict_y,columns=['sales'])\npredict_y['id']=predict_y.index\npd.DataFrame(predict_y[['id','sales']],columns=['id','sales']).to_csv('submission.csv',index=False,encoding='utf-8')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}