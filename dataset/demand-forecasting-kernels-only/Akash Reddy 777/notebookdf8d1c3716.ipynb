{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-24T17:48:02.310527Z","iopub.execute_input":"2021-07-24T17:48:02.310894Z","iopub.status.idle":"2021-07-24T17:48:02.326502Z","shell.execute_reply.started":"2021-07-24T17:48:02.310862Z","shell.execute_reply":"2021-07-24T17:48:02.325523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on July 24 15:31:02\n\n@author: Palagiri Akash Reddy\n\"\"\"\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import describe\n\nimport warnings\n\nwarnings.simplefilter('ignore')\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nsns.set()\ndf_train = pd.read_csv('../input/demand-forecasting-kernels-only/train.csv')\n\ndf_train.head()\n\ndf_train['date'] = pd.to_datetime(df_train['date'])\ndf_train.index = pd.DatetimeIndex(df_train['date'])\ndf_train.drop('date', axis=1, inplace=True)\n\ndf_train.info()\n\n\n\nfrom itertools import product, starmap\n\n\ndef storeitems():\n    return product(range(1,51), range(1,11))\n\n\ndef storeitems_column_names():\n    return list(starmap(lambda i,s: f'item_{i}_store_{s}_sales', storeitems()))\n\n\ndef sales_by_storeitem(df):\n    ret = pd.DataFrame(index=df.index.unique())\n    for i, s in storeitems():\n        ret[f'item_{i}_store_{s}_sales'] = df[(df['item'] == i) & (df['store'] == s)]['sales'].values\n    return ret\n\ndf_train = sales_by_storeitem(df_train)\n\ndf_train.info()\n\n\n# load data\ndf_test = pd.read_csv('../input/demand-forecasting-kernels-only/test.csv')\ndf_test.head()\n\n# strings to dates\ndf_test['date'] = pd.to_datetime(df_test['date'])\ndf_test.index = pd.DatetimeIndex(df_test['date'])\ndf_test.drop('date', axis=1, inplace=True)\ndf_test.info()\n\n# mock sales to use same transformations as in df_train\ndf_test['sales'] = np.zeros(df_test.shape[0])\ndf_test = sales_by_storeitem(df_test)\ndf_test.info()\n\n# make sure all column names are the same and in the same order\ncol_names = list(zip(df_test.columns, df_train.columns))\nfor cn in col_names:\n    assert cn[0] == cn[1]\n    \ndf_test['is_test'] = np.repeat(True, df_test.shape[0])\ndf_train['is_test'] = np.repeat(False, df_train.shape[0])\ndf_total = pd.concat([df_train, df_test])\ndf_total.info()\n\nweekday_df = pd.get_dummies(df_total.index.weekday, prefix='weekday')\nweekday_df.index = df_total.index\nweekday_df.head()\n\nmonth_df = pd.get_dummies(df_total.index.month, prefix='month')\nmonth_df.index =  df_total.index\nmonth_df.head()\n\ndf_total = pd.concat([weekday_df, month_df, df_total], axis=1)\ndf_total.info()\n\n## make sure stacked and standard sales columns appear in the same order:\n#sales_cols = [col for col in df_total.columns if '_sales' in col and '_sales_' not in col]\n##stacked_sales_cols = [col for col in df_total.columns if '_sales_' in col]\n#other_cols = [col for col in df_total.columns if col not in set(sales_cols)]\n#\n#sales_cols = sorted(sales_cols)\n#\n#\n#new_cols = other_cols + sales_cols\n#\n#\n#df_total = df_total.reindex(columns=new_cols)\n\ndf_total.head()\n\ndf_total.tail()\n\ndf_total.describe()\n\nfrom sklearn.preprocessing import MinMaxScaler\ncols_to_scale = [col for col in df_total.columns if 'weekday' not in col and 'month' not in col and 'is_' not in col]\n\nscaler = MinMaxScaler(feature_range=(0,1))\nscaled_cols = scaler.fit_transform(df_total[cols_to_scale])\ndf_total[cols_to_scale] = scaled_cols\ndf_total.head()\n\ndf_train = df_total[df_total['is_test'] == False].drop('is_test', axis=1)\ndf_test = df_total[df_total['is_test'] == True].drop('is_test', axis=1)\n\ndf_train.info()\n\ndf_test.info()\n\n#df_train_changed = df_train\n#cols_to_sort = storeitems_column_names()\n#\n#df_train_changed =  df_train_changed.reset_index()\n#df_train_changed = df_train_changed.drop(['date'], axis =1)\n#temp = df_train_changed[cols_to_sort]\n#df_train_changed = df_train_changed.drop(cols_to_sort, axis =1)\n#df_train_changed = pd.concat([df_train_changed,temp],axis =1)\n#\n##changing the original\n#temp = df_total[cols_to_sort]\n#df_total = df_total.drop(cols_to_sort, axis =1)\n#df_total = pd.concat([df_total,temp],axis =1)\n\n\n\ndef to_sequences(dataset, seq_size=1):\n    x = []\n    y = []\n\n    for i in range(len(dataset)-seq_size):\n        #print(i)\n        window = dataset.iloc[i:(i+seq_size), :]\n        x.append(window)\n        y.append(dataset.iloc[i+seq_size, 19:])\n        \n    return np.array(x),np.array(y)\n\nseq_size = 3\nX, y = to_sequences(df_train, seq_size)\n\nprint(\"Shape of training set: {}\".format(X.shape))\ninputshape = (X.shape[1], X.shape[2])\nprint(inputshape)\n\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Flatten,Dropout,Dense\n\n\nmodel = Sequential()\nmodel.add(LSTM(128, activation='relu',input_shape=inputshape, return_sequences=True))\nmodel.add(Dropout(0.1))\nmodel.add(LSTM(64, activation='tanh', return_sequences=True))\nmodel.add(Dropout(0.1))\nmodel.add(LSTM(32, activation='tanh', return_sequences=False))\nmodel.add(Dense(500))\n\n\nmodel.compile(optimizer='adam', loss = 'mse', metrics = ['accuracy'])\nmodel.summary()\n\n\n\n\nmodel.fit(X, y, verbose=1, epochs = 20,batch_size = 1,shuffle = False)\n\n\ntemp = df_train.iloc[-seq_size:,:]\ntemp = pd.concat([temp,df_test],axis =0)\n\nfor i in range(0,90):\n    temp_list = []\n    temp_var = temp.iloc[i:(i+seq_size),:]\n    temp_list.append(temp_var)\n    temp_array = np.array(temp_list)\n    temp_array = temp_array.reshape(1,seq_size,519)\n    prediction = model.predict(temp_array)\n    temp.iloc[(i+seq_size):(i+seq_size+1),19:519] = prediction.flatten().tolist()\n\ntemp = temp.iloc[seq_size:,:]\ntemp[cols_to_scale] = scaler.inverse_transform(temp[cols_to_scale])\n\n\nresult = np.zeros(0,dtype=np.int)\n\nfor i, s in storeitems():\n    col_name = f'item_{i}_store_{s}_sales'\n    result = np.concatenate((result,temp[col_name].values))\nresult = result.round()\nresult = pd.DataFrame(result, columns=['sales'])\nresult.index.name = 'id'\nresult.head()\nresult.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-24T17:52:48.763775Z","iopub.execute_input":"2021-07-24T17:52:48.764266Z"},"trusted":true},"execution_count":null,"outputs":[]}]}