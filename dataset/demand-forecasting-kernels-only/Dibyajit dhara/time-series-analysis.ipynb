{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defination and Components of a Time Series Data\n\nAny data that has a time component involved in it is termed as a time-series data. For example, the number of orders made on a product ordering app per day is an example of time-series data\n\n**Different components of time series data**\n\n**Trend**:- As the name suggests trend depicts the variation in the output as time increases.It is often non-linear. Sometimes we will refer to trend as “changing direction” when it might go from an increasing trend to a decreasing trend.\n\n**Level**:- It basically depicts baseline value for the time series.\n\n**Seasonal**:- As its name depicts it shows the repeated pattern over time. In layman terms, it shows the seasonal variation of data over time.\n\n**Noise**:- It is basically external noises that vary the data randomly."},{"metadata":{},"cell_type":"markdown","source":"* we want to forecast sales and will follow different approch for that."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Import Libraries And Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"## import all required libraries\nimport numpy as np ## for linear algebra\nimport pandas as pd ## data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt ## for visualisation\nimport seaborn as sns ## for visualisation\nimport warnings ## for filterout warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## load our sales data\n\nsales_train = pd.read_csv('../input/demand-forecasting-kernels-only/train.csv',parse_dates=['date'],index_col=['date']) ## train data\nsales_test = pd.read_csv('../input/demand-forecasting-kernels-only/test.csv',parse_dates=['date'],index_col=['date']) ## test data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## check train head\n\nsales_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## check train shape\n\nsales_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## check info of train data\n\nsales_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* check for null values in train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"## null check in train\n\nsales_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## null check for test\n\nsales_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* visualise the sales data and try find some pattern on it."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.plot(sales_train['sales'].resample('W').sum(),label=\"sales\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As we can see sales have  seasonal and upword trend."},{"metadata":{"trusted":true},"cell_type":"code","source":"## create dataset for every shop and visualize sales trend\n\nshop1 = sales_train[sales_train.store==1]['sales'].sort_index(ascending=True)\nshop2 = sales_train[sales_train.store==2]['sales'].sort_index(ascending=True)\nshop3 = sales_train[sales_train.store==3]['sales'].sort_index(ascending=True)\nshop4 = sales_train[sales_train.store==4]['sales'].sort_index(ascending=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2,ax3,ax4) = plt.subplots(4,figsize=(12,13))\nshop1.resample('W').sum().plot(ax=ax1)\nshop2.resample('W').sum().plot(ax=ax2)\nshop3.resample('W').sum().plot(ax=ax3)\nshop4.resample('W').sum().plot(ax=ax4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pylab\nfrom pylab import rcParams\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nrcParams['figure.figsize'] = 12, 4\ndecomposition = seasonal_decompose(shop1, model='additive' ,period=365) # additive seasonal index\nfig = decomposition.trend.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* trend of shop1 sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"rcParams['figure.figsize'] = 12, 4\ndecomposition2 = seasonal_decompose(shop2, model='additive' ,period=365) # additive seasonal index\nfig = decomposition2.trend.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* shop2 sales trend"},{"metadata":{"trusted":true},"cell_type":"code","source":"rcParams['figure.figsize'] = 12, 4\ndecompositio3 = seasonal_decompose(shop3, model='additive' ,period=365) # additive seasonal index\nfig = decompositio3.trend.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* shop3 sales trend"},{"metadata":{"trusted":true},"cell_type":"code","source":"rcParams['figure.figsize'] = 12, 4\ndecompositio4 = seasonal_decompose(shop4, model='additive' ,period=365) # additive seasonal index\nfig = decompositio4.trend.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* shop4 sales trend."},{"metadata":{},"cell_type":"markdown","source":"From the above plots we ca say that 4 shops's sales are showing same kind of seasonal pattern and trend.next we will go deeper and visualise in aspect of store item and more."},{"metadata":{"trusted":true},"cell_type":"code","source":"## create one copy of dataframe\n\ndf = sales_train.copy()\n\ndf['day'] = df.index.day\ndf['month'] = df.index.month\ndf['year'] = df.index.year\ndf['dayofweek'] = df.index.dayofweek\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* create seperate column containing exact date , month year and day of the week."},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"## create two pivot tables one for sales of every item per year  \n\nsales_item = pd.pivot_table(df,index=['year'],values='sales',columns=['item'],aggfunc=np.mean).values\n\n## create two pivot tables one for sales of every store per year  \n\nsales_store = pd.pivot_table(df,index=['year'],values='sales',columns=['store'],aggfunc=np.mean).values\n\n## calculate relative sales of items with change in year\n\n## calculate relative sales of shops with change in year\n\n## lets plot relative sales per year with respect to items and store \n\nplt.figure(figsize=(12,5))\nplt.subplot(121)\nplt.plot(sales_item/sales_item.mean(0)[np.newaxis])\nplt.xlabel('year')\nplt.ylabel('relative sales')\nplt.title(\"Relative sales per item for every year\")\nplt.subplot(122)\nplt.plot(sales_store/sales_store.mean(0)[np.newaxis])\nplt.xlabel('year')\nplt.ylabel('relative sales')\nplt.title(\"Relative sales per store for every year\")\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It is clearly visible that there is a clear pattern when we plot relative sales per year with respect to items and store also all items and store is following a kind of simillar patterns.sales are increasing with every year."},{"metadata":{"trusted":true},"cell_type":"code","source":"## create two pivot tables one for sales of every item per month  \n\nsales_item = pd.pivot_table(df,index=['month'],values='sales',columns=['item'],aggfunc=np.mean).values\n\n## create two pivot tables one for sales of every store per month  \n\nsales_store = pd.pivot_table(df,index=['month'],values='sales',columns=['store'],aggfunc=np.mean).values\n\n## calculate relative sales of items with change in month\n\n## calculate relative sales of shops with change in month\n\n## lets plot relative sales per month with respect to items and store \n\nplt.figure(figsize=(12,5))\nplt.subplot(121)\nplt.plot(sales_item/sales_item.mean(0)[np.newaxis])\nplt.xlabel('months')\nplt.ylabel('relative sales')\nplt.title(\"Relative sales per item for every month\")\nplt.subplot(122)\nplt.plot(sales_store/sales_store.mean(0)[np.newaxis])\nplt.xlabel('month')\nplt.ylabel('relative sales')\nplt.title(\"Relative sales per store for every month\")\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is a clear pattern is visible when we plot sales with respect to items and store for every month of the year there is clear upword and downword pattern.enerally sales are more higher on the month of jun and july."},{"metadata":{"trusted":true},"cell_type":"code","source":"## create two pivot tables one for sales of every item per day  \n\nsales_item = pd.pivot_table(df,index=['day'],values='sales',columns=['item'],aggfunc=np.mean).values\n\n## create two pivot tables one for sales of every store per month  \n\nsales_store = pd.pivot_table(df,index=['day'],values='sales',columns=['store'],aggfunc=np.mean).values\n\n## calculate relative sales of items with change in day\n\n## calculate relative sales of shops with change in day\n\n## lets plot relative sales per day with respect to items and store \n\nplt.figure(figsize=(12,5))\nplt.subplot(121)\nplt.plot(sales_item/sales_item.mean(0)[np.newaxis])\nplt.xlabel('day')\nplt.ylabel('relative sales')\nplt.title(\"Relative sales per item for every day\")\nplt.subplot(122)\nplt.plot(sales_store/sales_store.mean(0)[np.newaxis])\nplt.xlabel('day')\nplt.ylabel('relative sales')\nplt.title(\"Relative sales per store for every day\")\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is no clear pattern if we plot sales per store and per items with respect to every month of the year.some of the month's end sales are very low compared to others."},{"metadata":{"trusted":true},"cell_type":"code","source":"## create two pivot tables one for sales of every item per dayofweek  \n\nsales_item = pd.pivot_table(df,index=['dayofweek'],values='sales',columns=['item'],aggfunc=np.mean).values\n\n## create two pivot tables one for sales of every store per month  \n\nsales_store = pd.pivot_table(df,index=['dayofweek'],values='sales',columns=['store'],aggfunc=np.mean).values\n\n## calculate relative sales of items with change in dayofweek\n\n## calculate relative sales of shops with change in dayofweek\n\n## lets plot relative sales per dayofweek with respect to items and store \n\nplt.figure(figsize=(12,5))\nplt.subplot(121)\nplt.plot(sales_item/sales_item.mean(0)[np.newaxis])\nplt.xlabel('day of the week')\nplt.ylabel('relative sales')\nplt.title(\"Relative sales per item for every dayofweek\")\nplt.subplot(122)\nplt.plot(sales_store/sales_store.mean(0)[np.newaxis])\nplt.xlabel('day of the week')\nplt.ylabel('relative sales')\nplt.title(\"Relative sales per store for every dayofweek\")\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* with each day of the week sales are getting increased it shows a pick on the weekends."},{"metadata":{},"cell_type":"markdown","source":"* From the above plots it's clearly visible that every item's sales and every store's sales follows a same kind of pattern."},{"metadata":{},"cell_type":"markdown","source":"# Model Building and Evalution."},{"metadata":{},"cell_type":"markdown","source":"* we will discuss about different types of models and check their predicton our data.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## split our train data to test prediction of our model\n\ntrain = sales_train.iloc[0:730400] ## train \ntest = sales_train.iloc[730400:] ## test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Method\n\n* let's start with the **naive method** where prediction is the last month's sale."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_naive = test.copy()\ntest_naive['forecast'] = train['sales'][len(train)-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* create one column containing the forecast now let's calculate their rmse and mape check our prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.metrics import mean_squared_error\n\nscore = mean_squared_error(test['sales'],test_naive['forecast'])\n\nresults = pd.DataFrame({'model':['naive model'],'mean squared error':round(score,2)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## check result\n\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# simple average method\n\n* **simple average method** where forecast is the average of all the past sales."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_simpleavg = test.copy()\ntest_simpleavg['forecast'] = train['sales'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_simavg = mean_squared_error(test['sales'],test_simpleavg['forecast']) ## calculate score\n\nresults2 = pd.DataFrame({'model':['simple average model'],'mean squared error':round(score_simavg,2)})\nresults = pd.concat([results,results2])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results ##check results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Model is slightly improved"},{"metadata":{},"cell_type":"markdown","source":"# Simple Moving Average Method\n\n* **simple moving average method** In this method, the forecasts are generally calculated using the average of the time-series data in the moving window considered. The window of the past observations in the time series data keeps moving, and hence the average values keep changing. This helps in forecasting values at every step in the dataset. older sales values have less impact in case of predicting futures sales and recent sales have more impact on future sales.\n\nLet's considered moving window as 12 and check"},{"metadata":{"trusted":true},"cell_type":"code","source":"#simp_mov_avg = test.copy()\nwindow = 12\nsimp_mov_avg = train['sales'].rolling(window).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"simp_avgmov = test.copy()\nsimp_avgmov['forecast'] = simp_mov_avg[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_simp_mov_avg = mean_squared_error(test['sales'],simp_avgmov['forecast']) ## calculate score\n\nresults3 = pd.DataFrame({'model':['simple average model'],'mean squared error':round(score_simp_mov_avg,2)})\nresults = pd.concat([results,results3])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results ## check result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# simple exponential smoothning technique\n\n* **simple exponential smoothning technique** used to forecast the level in the time series data.we start weighting all available observations while exponentially decreasing the weights as we move further back in time to capture the level.\n\nformula = l(t) = a.y(t)+(1-a)l(t-1)\n\nhere , l(t) is the level of time period t and forecast of y(t+1) is a function of l(t)\n\nAbove formula uses recursively to predict the forecast."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n\nmodel = SimpleExpSmoothing(train['sales'])\nmodel_fit = model.fit(optimized=True)\nparams = model_fit.params\nses_test = test.copy()\nses_test['forecast'] = model_fit.predict( start='2013-01-01', end='2017-12-31')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_ses_avg = mean_squared_error(test['sales'],ses_test['forecast']) ## calculate score\n\nresults4 = pd.DataFrame({'model':['simple exponential smoothing  model'],'mean squared error':round(score_ses_avg,2)})\nresults = pd.concat([results,results4])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Holt's exponential smoothing technique\n\n* **Holt's exponential smoothing technique** forecasts the level and trend of the time series data. Now the forecast equation is a function of both level and trend.\n\nThis method also captures the trend with level to predict or forecast future values\n\nhere, y(t+1) = l(t) + b(t)\n\ny(t+1) is the forecast of t+1 time period which is a function of level of 't' time period and trend of 't' time period\n\nl(t) = a*y(t) + (1-a)*(l(t-1)+b(t-1)) we already know that a is the smoothning level parameter\n\nand b(t) = beta*(l(t)-l(t-1)) + (1-beta)*(b(t-1) 'beta' is a smoothning trend parameter also know as smoothning slope in statsmodels package\n\nwe can tune this parameter to make a better prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\n\nmodel_exp = ExponentialSmoothing(np.asarray(train['sales']) ,seasonal_periods=12 ,trend='additive', seasonal=None)\nmodel_fit_exp = model_exp.fit(smoothing_level=0.2, smoothing_slope=0.01, optimized=False)\nparams = model_fit_exp.params\nhes_test = test.copy()\nhes_test['forecast'] = model_fit_exp.forecast(182600)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* we can also use optimized set to true to optimize the model for other parameters info check the link : [https://www.statsmodels.org/dev/generated/statsmodels.tsa.holtwinters.ExponentialSmoothing.html](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"score_hes_avg = mean_squared_error(test['sales'],hes_test['forecast']) ## calculate score\n\nresults5 = pd.DataFrame({'model':['Holts exponential smoothing  model'],'mean squared error':round(score_hes_avg,2)})\nresults = pd.concat([results,results5])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results ## check results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Holt-Winters’ smoothing technique\n\n* The **Holt-Winters’ smoothing technique** forecasts the level, trend as well as the seasonality for a time series data.\n\nThis method also captures the seasonality trend and level to predict or forecast future values\n\nhere, y(t+1) = l(t) + b(t) + s(t+1-m)\n\ny(t+1) is the forecast of t+1 time period which is a function of level of 't' time period and trend of 't' time period and seasonality of time period 't'\n\nl(t) = a*(y(t)-s(t-m)) + (1-a)*(l(t-1)+b(t-1)) we already know that a is the smoothning level parameter\n\nand b(t) = beta*(l(t)-l(t-1)) + (1-beta)*(b(t-1) 'beta' is a smoothning trend parameter also know as smoothning slope in statsmodels package\n\nand s(t) = gamma(y(t)-b(t-1)-l(t-1)) + (1-gamma)*s(t-m) where 'm' is the number of times a season repeats\n\n'gamma' is smoothning season parameter.\n\nwe can tune this parameter to make a better prediction.\n\nThere are two types of Holt-winter smoothning technique additive and multiplicative hence we can add season as 'add' for additive model and 'mul' for multiplicative model.\n\nBased upon the forecasting of above models we will chose our final model and test it into our ultimate test data.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}