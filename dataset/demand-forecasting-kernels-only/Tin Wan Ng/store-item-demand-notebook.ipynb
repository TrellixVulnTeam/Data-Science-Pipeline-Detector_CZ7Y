{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-21T23:45:39.672852Z","iopub.execute_input":"2021-10-21T23:45:39.673158Z","iopub.status.idle":"2021-10-21T23:45:39.695416Z","shell.execute_reply.started":"2021-10-21T23:45:39.673078Z","shell.execute_reply":"2021-10-21T23:45:39.694628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/train.csv') # 2013 01 - 2017 12\ntest = pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/test.csv')  # 2018 01-03\n\n\ndef dmy_sep_dayofweek(df):\n    df['date'] = pd.to_datetime(df.date, format=\"%Y-%m-%d\")\n    df['day'] = df.date.dt.day\n    df['month'] = df.date.dt.month\n    df['year'] = df.date.dt.year\n    df['day_of_week'] = df.date.dt.dayofweek  # Mon:0, Sun: 6\n    return df\n    \ntrain = dmy_sep_dayofweek(train)\ntest = dmy_sep_dayofweek(test)\n\n#plt.figure(figsize=(25,25))\n#mask = np.triu(np.ones_like(train.corr()))\n#sns.heatmap(train.corr(), vmin=-1, vmax=1, mask=mask, annot=True, cmap='BrBG')\n\n# day of week>year>month>item\n\n\n#for var in ['month', 'year', 'day_of_week', 'item']:\n#    grouped_sales = train.groupby(var).sales.mean()\n#    sns.lineplot(x=train[var].value_counts().index.sort_values(), y=grouped_sales)\n#    plt.xlabel(var)\n#    plt.show()\n\n#for year in range(2013,2018):\n#    monthly_sales = train.loc[train.year == year].groupby('month').sales.mean()\n#    sns.lineplot(x=range(1,13), y=monthly_sales, label=year)\n#    plt.xlabel('month')\n    \n# item-store relationship\n#plt.figure(figsize=(20,15))\n#for store in range(1,11):\n#    sales_byItem = train.loc[train.store==store].groupby('item').sales.mean()\n#    sns.lineplot(x=range(1,51), y=sales_byItem, label=store\n    \n# exact same trend, seems fabricated\n\n\n# relative sales ('smoother' to predict)\n# Yearly growth pattern -> predict the next pt in curve\nsales_avg = train.sales.mean()\nrel_sales_byYear = pd.pivot_table(train, index='year', values='sales', aggfunc=np.mean)\nrel_sales_byYear /= sales_avg\n\nrel_sales_year = rel_sales_byYear.values.squeeze()\n# try 2 ways of fitting\n# fit return coefficients, 1d make them polynomial\nfitdeg2 = np.poly1d(np.polyfit(range(2013,2018), rel_sales_year, 2))  # deg 3 too high, 4 would completely goes off\n\n#plt.figure(figsize=(10,8))\n#plt.plot(range(2013,2018), rel_sales_year, 'bo')\n#plt.plot(range(2013,2019), fitdeg2(range(2013,2019)))\n#plt.ylabel(\"Relative Sales\")\n#plt.xlabel(\"Year\")\n\nrel_sales2018 = round(fitdeg2(2018), 5)\n\n# instead of prediction, multiply pass sales by factors\n# 1. base sales day of week-item\n# 2. day of week>year>month>item factors\nbase_sales_ref = pd.pivot_table(train, index='day_of_week', columns='item', values='sales',aggfunc=np.mean)\nmonthly_relSales = pd.pivot_table(train, index='month', values='sales',aggfunc=np.mean)\nmonthly_relSales.sales /= sales_avg\nstore_relSales = pd.pivot_table(train, index='store', values='sales',aggfunc=np.mean)\nstore_relSales.sales /= sales_avg\n\nsubmission = pd.DataFrame(test['id'])\nsubmission['sales'] = np.nan\n\nfor _, row in test.iterrows():\n    dayofweek, month, year, item, store = row.day_of_week, row.month, row.year, row['item'], row.store\n    base_sales = base_sales_ref.loc[dayofweek, item]\n    monthFactor = monthly_relSales.loc[month]\n    storeFactor = store_relSales.loc[store]\n    pred_sales = base_sales * monthFactor * storeFactor * rel_sales2018\n    submission.at[row['id'], 'sales'] = pred_sales\n    \n\nsubmission['sales'] = np.round(submission['sales']).astype(int)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T23:45:39.696861Z","iopub.execute_input":"2021-10-21T23:45:39.69752Z","iopub.status.idle":"2021-10-21T23:46:07.780046Z","shell.execute_reply.started":"2021-10-21T23:45:39.697481Z","shell.execute_reply":"2021-10-21T23:46:07.779161Z"},"trusted":true},"execution_count":null,"outputs":[]}]}