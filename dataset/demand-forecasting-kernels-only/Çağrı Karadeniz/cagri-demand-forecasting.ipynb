{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport warnings\n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 500)\nwarnings.filterwarnings('ignore')\n\ntrain=pd.read_csv(\"../input/demand-forecasting-kernels-only/train.csv\")\ntest=pd.read_csv(\"../input/demand-forecasting-kernels-only/test.csv\")\nsample=pd.read_csv(\"../input/demand-forecasting-kernels-only/sample_submission.csv\")\ndf = pd.concat([train, test], sort=False)\ndf.date=pd.to_datetime(df.date)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef degisken_tiplerine_ayirma(data, cat_th, car_th):\n    \"\"\"\n   Veri:data parametresi ili fonksiyona girilen verinin değişkenlerin sınıflandırılması.\n    Parameters\n    ----------\n    data: pandas.DataFrame\n    İşlem yapılacak veri seti\n ​\n    cat_th:int\n    categoric değişken threshold değeri\n ​\n    car_th:int\n    Cardinal değişkenler için threshold değeri\n ​\n    Returns\n    -------\n     cat_deg:list\n     categorik değişken listesi\n     num_deg:list\n     numeric değişken listesi\n     car_deg:list\n     categoric ama cardinal değişken listesi\n ​\n    Examples\n    -------\n     df = dataset_yukle(\"breast_cancer\")\n     cat,num,car=degisken_tiplerine_ayirma(df,10,20)\n    Notes\n    -------\n     cat_deg + num_deg + car_deg = toplam değişken sayısı\n ​\n    \"\"\"\n\n\n\n    num_but_cat = [i for i in data.columns if data[i].dtypes != \"O\" and data[i].nunique() < cat_th]\n\n    car_deg = [i for i in data.columns if data[i].dtypes == \"O\" and data[i].nunique() > car_th]\n\n    num_deg = [i for i in data.columns if data[i].dtypes != \"O\" and i not in num_but_cat]\n\n    cat_deg = [i for i in data.columns if data[i].dtypes == \"O\" and i not in car_deg]\n\n    cat_deg = cat_deg + num_but_cat\n\n    print(f\"Dataset kolon/değişken sayısı: {data.shape[1]}\")\n    print(f\"Dataset satır/veri sayısı: {data.shape[0]}\")\n    print(\"********************************************\")\n    print(f\"Datasetin numeric değişken sayısı: {len(num_deg)}\")\n    print(f\"Datasetin numeric değişkenler: {num_deg}\")\n    print(\"********************************************\")\n    print(f\"Datasetin categoric değişken sayısı: {len(cat_deg)}\")\n    print(f\"Datasetin categoric değişkenler: {cat_deg}\")\n    print(\"********************************************\")\n    print(f\"Datasetin cardinal değişken sayısı: {len(car_deg)}\")\n    print(f\"Datasetin cardinal değişkenler: {car_deg}\")\n    print(\"********************************************\")\n\n    return cat_deg, num_deg, car_deg\n\n\ndef dataset_ozet(data, head=5):\n    print(\"##################### Shape #####################\")\n    print(f\"Satır sayısı: {data.shape[0]}\")\n    print(f\"Kolon sayısı: {data.shape[1]}\")\n\n\n    print(\"##################### Types #####################\")\n    print(data.dtypes)\n\n    print(\"##################### Head #####################\")\n    print(data.head(head))\n\n    print(\"##################### Tail #####################\")\n    print(data.tail(head))\n\n    print(\"##################### NA Kontrolü #####################\")\n    print(data.isnull().sum())\n\n    print(\"##################### Quantiles #####################\")\n    print(data.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\n    print(\"##################### Describe Tablosu #####################\")\n    print(data.describe().T)\n\n\ndef outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\n\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\n\ndef categoric_ozet(data, degisken, plot=False, null_control=False):\n    \"\"\"\n   Task\n    ----------\n    Datasetinde bulunan categoric değişkenlerin değişken tiplerinin sayısını ve totale karşı oranını bulur.\n    Ayrıca isteğe bağlı olarak değişken dağılımının grafiğini ve değişken içinde bulunan null sayısını çıkartır.\n​\n    Parameters\n    ----------\n    data:pandas.DataFrame\n    categoric değişkenin bulunduğu dataset.\n    degisken:String\n    Categoric değişken ismi.\n    plot:bool\n    Fonksiyonda categoric değişken dağılımının grafiğini çizdirmek için opsiyonel özellik.\n    null_control:bool\n    Fonksiyonda değişken içinde null değer kontolü için opsiyonel özellik\n​\n    Returns\n    -------\n    tablo:pandas.DataFrame\n    Unique değişkenlerin ratio olarak oran tablosu\n    Examples\n    -------\n    df=dataset_yukle(\"titanic\")\n    cat_deg,num_deg,car_deg=degisken_tiplerine_ayirma(df,10,20)\n    for i in cat_deg:\n        tablo=categoric_ozet(df,i,True,True)\n    \"\"\"\n\n\n    print(pd.DataFrame({degisken: data[degisken].value_counts(),\n                    \"Ratio\": 100 * data[degisken].value_counts() / len(data)}))\n    tablo = pd.DataFrame({degisken: data[degisken].value_counts(),\n                      \"Ratio\": 100 * data[degisken].value_counts() / len(data)})\n    print(\"##########################################\")\n    if plot:\n        sns.countplot(x=data[degisken], data=data)\n        plt.show()\n    if null_control:\n        print(f\"Null veri sayısı: {data[degisken].isnull().sum()}\")\n\n    return tablo\n\n\ndef numeric_ozet(data, degisken, plot=False, null_control=False):\n    \"\"\"\n    Task\n    ----------\n    Datasetinde bulunan numeric değişkenlerin değişken tiplerinin sayısını ve totale karşı oranını bulur.\n    Ayrıca isteğe bağlı olarak değişken dağılımının grafiğini ve değişken içinde bulunan null sayısını çıkartır.\n​\n    Parameters\n    ----------\n    data:pandas.DataFrame\n    categoric değişkenin bulunduğu dataset.\n    degisken:String\n    Categoric değişken ismi.\n    plot:bool\n    Fonksiyonda categoric değişken dağılımının grafiğini çizdirmek için opsiyonel özellik.\n    null_control:bool\n    Fonksiyonda değişken içinde null değer kontolü için opsiyonel özellik\n​\n    Returns\n    -------\n    tablo:pandas.DataFrame\n    Unique değişkenlerin ratio olarak oran tablosu\n    Examples\n    -------\n    df=dataset_yukle(\"titanic\")\n    cat_deg,num_deg,car_deg=degisken_tiplerine_ayirma(df,10,20)\n    for i in cat_deg:\n        tablo=categoric_ozet(df,i,True,True)\n    \"\"\"\n    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n    print(data[degisken].describe(quantiles).T)\n\n    if plot:\n        data[degisken].hist(bins=20)\n        plt.xlabel(degisken)\n        plt.title(degisken)\n        plt.show(block=True)\n    print(\"##########################################\")\n\n    if null_control:\n        print(f\"Null veri sayısı: {data[degisken].isnull().sum()}\")\n\n\n\ndef target_analyser(dataframe, target, num_deg, cat_deg):\n    for degisken in dataframe.columns:\n        if degisken in cat_deg:\n            print(degisken, \":\", len(dataframe[degisken].value_counts()))\n            print(pd.DataFrame({\"COUNT\": dataframe[degisken].value_counts(),\n                                \"RATIO\": dataframe[degisken].value_counts() / len(dataframe),\n                                \"TARGET_MEAN\": dataframe.groupby(degisken)[target].mean()}), end=\"\\n\\n\\n\")\n        if degisken in num_deg:\n            print(pd.DataFrame({\n                \"TARGET_MEAN\": dataframe.groupby(target)[degisken].mean()}), end=\"\\n\\n\\n\")\n\n\ndef outlier_threshold(data, degisken):\n    Q1 = data[degisken].quantile(0.25)\n    Q3 = data[degisken].quantile(0.75)\n    Q_Inter_Range = Q3 - Q1\n    alt_limit = Q1 - 1.5 * Q_Inter_Range\n    ust_limit = Q3 + 1.5 * Q_Inter_Range\n    return alt_limit, ust_limit\n\n\ndef threshold_degisimi(data, degisken):\n    alt_limit, ust_limit = outlier_threshold(data, degisken)\n    data[data[degisken] < alt_limit] = alt_limit\n    data[data[degisken] > ust_limit] = ust_limit\n    return data\n\n\ndef missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n\n    if na_name:\n        return na_columns\n\n\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=True):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\n\n\n\ndef create_date_features(df):\n    df['month'] = df.date.dt.month\n    df['day_of_month'] = df.date.dt.day\n    df['day_of_year'] = df.date.dt.dayofyear\n    df['week_of_year'] = df.date.dt.weekofyear\n    df['day_of_week'] = df.date.dt.dayofweek\n    df['year'] = df.date.dt.year\n    df[\"is_wknd\"] = df.date.dt.weekday // 4\n    df['is_month_start'] = df.date.dt.is_month_start.astype(int)\n    df['is_month_end'] = df.date.dt.is_month_end.astype(int)\n    df['is_Mon'] = np.where(df['day_of_week'] == 1, 1, 0)\n    df['is_Tue'] = np.where(df['day_of_week'] == 2, 1, 0)\n    df['is_Wed'] = np.where(df['day_of_week'] == 3, 1, 0)\n    df['is_Thu'] = np.where(df['day_of_week'] == 4, 1, 0)\n    df['is_Fri'] = np.where(df['day_of_week'] == 5, 1, 0)\n    df['is_Sat'] = np.where(df['day_of_week'] == 6, 1, 0)\n    df['is_Sun'] = np.where(df['day_of_week'] == 7, 1, 0)\n    return df\n\n\ndef lag_features(dataframe, lags):\n    for lag in lags:\n        dataframe['sales_lag_' + str(lag)] = dataframe.groupby([\"store\", \"item\"])['sales'].transform(\n            lambda x: x.shift(lag)) + random_noise(dataframe)\n    return dataframe\n\n\ndef roll_mean_features(dataframe, windows):\n    for window in windows:\n        dataframe['sales_roll_mean_' + str(window)] = dataframe.groupby([\"store\", \"item\"])['sales']. \\\n                                                          transform(\n            lambda x: x.shift(1).rolling(window=window, min_periods=10, win_type=\"triang\").mean()) + random_noise(\n            dataframe)\n    return dataframe\n\n\ndef random_noise(dataframe):\n    return np.random.normal(scale=1.6, size=(len(dataframe),))\n\n\ndef ewm_features(dataframe, alphas, lags):\n    for alpha in alphas:\n        for lag in lags:\n            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n                dataframe.groupby([\"store\", \"item\"])['sales'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n    return dataframe\n\n\ndef smape(preds, target):\n    n = len(preds)\n    masked_arr = ~((preds == 0) & (target == 0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds - target)\n    denom = np.abs(preds) + np.abs(target)\n    smape_val = (200 * np.sum(num / denom)) / n\n    return smape_val\n\n\ndef lgbm_smape(preds, train_data):\n    labels = train_data.get_label()\n    smape_val = smape(np.expm1(preds), np.expm1(labels))\n    return 'SMAPE', smape_val, False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=create_date_features(df)\n\ndf = lag_features(df, [91,98,  182,273, 364, 546])\ndf = roll_mean_features(df, [ 182, 365])\nalphas = [0.95, 0.9,0.8, 0.7,0.5]\nlags = [91,98, 182,273, 365, 546]\ndf=ewm_features(df, alphas, lags)\ncat_deg,num_deg,car_deg=degisken_tiplerine_ayirma(df,10,20)\ncat_deg=[i for i in cat_deg if i!=\"year\"]\ndf = pd.get_dummies(df, columns=cat_deg)\ndf['sales'] = np.log1p(df[\"sales\"].values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = df.loc[(df[\"date\"] < \"2017-01-01\"), :]\nval = df.loc[(df[\"date\"] >= \"2017-01-01\") & (df[\"date\"] < \"2017-04-01\"), :]\n\ncols = [col for col in train.columns if col not in ['date', 'id', \"sales\", \"year\"]]\n\nY_train = train['sales']\nX_train = train[cols]\n\nY_val = val['sales']\nX_val = val[cols]\n\nY_train.shape, X_train.shape, Y_val.shape, X_val.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_params = {'metric': {'mae'},\n              'num_leaves': 8,\n\n              'learning_rate': 0.03,\n              'feature_fraction': 0.9,\n              'max_depth': 3,\n              'verbose': 0,\n              \n\n              'num_boost_round': 20000,\n              'early_stopping_rounds': 300,\n              'nthread': -1}\n\nlgbtrain = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\nlgbval = lgb.Dataset(data=X_val, label=Y_val, reference=lgbtrain, feature_name=cols)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nmodel = lgb.train(lgb_params, lgbtrain,\n                  valid_sets=[lgbtrain, lgbval],\n                  num_boost_round=lgb_params['num_boost_round'],\n                  early_stopping_rounds=lgb_params['early_stopping_rounds'],\n                  feval=lgbm_smape,\n                  verbose_eval=100)\n\ny_pred_val = model.predict(X_val, num_iteration=model.best_iteration)\n\nsmape(np.expm1(y_pred_val), np.expm1(Y_val))\n\ndef plot_lgb_importances(model, plot=False, num=10):\n\n    gain = model.feature_importance('gain')\n    feat_imp = pd.DataFrame({'feature': model.feature_name(),\n                             'split': model.feature_importance('split'),\n                             'gain': 100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n    if plot:\n        plt.figure(figsize=(10, 10))\n        sns.set(font_scale=1)\n        sns.barplot(x=\"gain\", y=\"feature\", data=feat_imp[0:25])\n        plt.title('feature')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(feat_imp.head(num))\n\n\nplot_lgb_importances(model, num=30)\nplot_lgb_importances(model, num=30, plot=True)\n\nlgb.plot_importance(model, max_num_features=20, figsize=(10, 10), importance_type=\"gain\")\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain = df.loc[~df.sales.isna()]\nY_train = train['sales']\nX_train = train[cols]\n\ntest = df.loc[df.sales.isna()]\nX_test = test[cols]\n\nlgb_params = {'metric': {'mae'},\n              'num_leaves': 8,\n              'learning_rate': 0.03,\n              'feature_fraction': 0.9,\n              'max_depth': 3,\n              'verbose': 0,\n              \n              'nthread': -1,\n              \"num_boost_round\": model.best_iteration}\n            \n\n# LightGBM dataset\nlgbtrain_all = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\n\nmodel = lgb.train(lgb_params, lgbtrain_all, num_boost_round=model.best_iteration)\n\ntest_preds = model.predict(X_test, num_iteration=model.best_iteration)\n\n\nsubmission_df = test.loc[:, ['id', 'sales']]\nsubmission_df['sales'] = np.expm1(test_preds)\nsubmission_df['id'] = submission_df.id.astype(int)\n\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df.head(20)","metadata":{},"execution_count":null,"outputs":[]}]}