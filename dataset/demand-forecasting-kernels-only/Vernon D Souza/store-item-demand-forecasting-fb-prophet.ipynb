{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport seaborn as sns\n\nimport scipy.cluster.hierarchy as hac\n\nimport math\n\nimport random\n\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n\nfrom fbprophet import Prophet\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# References","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://alpynepyano.github.io/healthyNumerics/posts/time_series_clustering_with_python.html\n# https://kourentzes.com/forecasting/2014/11/09/additive-and-multiplicative-seasonality/\n# https://dius.com.au/2018/09/04/time-series-forecasting-with-fbprophet/\n# https://www.kaggle.com/beebopjones/bleepblop-prophet-model\n# https://www.kaggle.com/viridisquotient/sarima","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gain an understanding of data features","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/demand-forecasting-kernels-only/train.csv\")\nprint(df.head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find unique columns\nprint(\"length of database\",len(df))\nprint(\"Unique Stores\",df['store'].unique())\nprint(\"Unique items\",df['item'].unique())\nprint(\"Unique dates\",df['date'].nunique())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Include holidays\nholidays = pd.read_csv(\"../input/federal-holidays-usa-19662020/usholidays.csv\")\n\n\nholidays = holidays.drop(columns=[\"Unnamed: 0\"])\nprint(holidays.head(10))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['date'] = pd.to_datetime(df['date']) \nholidays['Date'] = pd.to_datetime(holidays['Date']) \nneeded_holidays = holidays[(holidays['Date']>df.iloc[0]['date'])&(holidays['Date']<df.iloc[-1]['date'])]['Date'].to_list()\nprint(len(needed_holidays))\n\n# For later Analysis\nholidays = holidays[(holidays['Date']>df.iloc[0]['date'])&(holidays['Date']<df.iloc[-1]['date'])]\nholidays.rename(columns={\"Date\": \"ds\", \"Holiday\": \"holiday\"},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"select_values = needed_holidays\ndf_cut = df[df['date'].isin(select_values)]['date']\nprint(df_cut.nunique())\n\n# Holidays have been included","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for Missing  data\ntotal = df.isnull().sum()\nprint(total)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting individual distributions of sales for each item. Taking sum to summate sales for different stores\ndf['date'] = pd.to_datetime(df['date']) \n\ndf_sales_item = df.groupby(['date','item']).sum()  \ndf_sales_item.reset_index(level=0, inplace=True)\ndf_sales_item.reset_index(level=0, inplace=True)\n#print(df_sales_item)\n\ngrid = sns.FacetGrid(df_sales_item, col=\"item\", col_wrap=5)\ngrid = grid.map(plt.scatter, \"date\", \"sales\", marker=\"o\", s=1, alpha=.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting individual distributions of sales for each store. Taking sum to summate sales for different items\ndf['date'] = pd.to_datetime(df['date']) \n\ndf_sales_store = df.groupby(['date','store']).sum()  \ndf_sales_store.reset_index(level=0, inplace=True)\ndf_sales_store.reset_index(level=0, inplace=True)\n\n\ngrid = sns.FacetGrid(df_sales_store, col=\"store\", col_wrap=5)\ngris = grid.map(plt.scatter, \"date\", \"sales\", s=1, alpha=.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets look at sales for a specified store and a item\nplt.figure(figsize=(30,5))\ndf_store_item = df[(df.store==10) & (df.item==5)]\nplt.plot(df_store_item['date'],df_store_item['sales'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The graphs of each combination will be different. Let's look we can cluster graphs with similar patterns","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Perform Data Clustering\nThis will help us understand about the variation among different groups of data and also decide in choosing the machine learning approach","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stores = df['store'].unique()\nitems = df['item'].unique()\n\nSales_series = []\ncount = 0\nIndexSale_Series = []\nDates = []\n\n#df['date'] = pd.to_datetime(df['date']) \n\nfor store in stores:\n    for item in items:\n        Sales = df[(df.store==store) & (df.item==item)]['sales'].to_list()\n        dates = df[(df.store==store) & (df.item==item)]['date'].to_list()\n        \n        Dates.append(dates)\n        Sales_series.append(Sales)\n        \n        index = [count,item,store]\n        IndexSale_Series.append(index)\n        \n        count +=1\n\nprint(\"Total Elements: \",count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_dendogram(Z):\n    with plt.style.context('fivethirtyeight' ): \n         plt.figure(figsize=(100, 40))\n         plt.title('Dendrogram of time series clustering',fontsize=25, fontweight='bold')\n         plt.xlabel('sample index', fontsize=25, fontweight='bold')\n         plt.ylabel('distance', fontsize=25, fontweight='bold')\n         hac.dendrogram( Z, leaf_rotation=90.,    # rotates the x axis labels\n                            leaf_font_size=15., ) # font size for the x axis labels\n         plt.show()\n        \ndef plot_resultsAndReturnClusters(timeSeries, D, cut_off_level):\n    result = pd.Series(hac.fcluster(D, cut_off_level, criterion='maxclust'))\n    clusters = result.unique()       \n    figX = 100; figY = 20\n    fig = plt.subplots(figsize=(figX, figY))   \n    mimg = math.ceil(cut_off_level/2.0)\n    gs = gridspec.GridSpec(mimg,2, width_ratios=[1,1])\n    cluster = []\n    for ipic, c in enumerate(clusters):\n        cluster_index = result[result==c].index\n        cluster.append(cluster_index)\n        \n        print(ipic, \"Cluster number %d has %d elements\" % (c, len(cluster_index)))\n        ax1 = plt.subplot(gs[ipic])\n        timeSeries = np.array(timeSeries)\n        ax1.plot(timeSeries.T[:,cluster_index])\n        ax1.set_title(('Cluster number '+str(c)), fontsize=15, fontweight='bold')      \n    \n    plt.show()\n    return cluster","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"D = hac.linkage(Sales_series, method='ward', metric='euclidean')\nplot_dendogram(D)\n\n#---- evaluate the dendogram\ncut_off_level = 2   # level where to cut off the dendogram\nclusters = plot_resultsAndReturnClusters(Sales_series, D, cut_off_level)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- From the data clusters and the pentagram, we can conclude that the there is a variation of data in the time series among different stores and items","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- First Cluster","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random time series in cluster 1\nno = random.randint(0,101)\n\nindex = clusters[0][no]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for weekly seasonality using ACF\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(211)\nfig = plot_acf(Sales_series[index],lags=40,ax=ax)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As we can see, there is a pattern of weekly seasonality in the data. This will be helpfull while modelling in FbProphet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for monthly seasonality using ACF\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(211)\nfig = plot_acf(Sales_series[index],lags=400,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a dip in sales in the middle of year, followed by a rebound in the end of the year. So rather the trend seems to follow similar patterns year on year","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for yearly seasonality using ACF\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(211)\nfig = plot_acf(Sales_series[index],lags=1800,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- A pattern of yearly seasonality but a decreasing but still significant one. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,10))\nprint(\"Image Corresponding to Item:\",IndexSale_Series[index][1],\"And Store:\",IndexSale_Series[index][2])\nplt.plot(Sales_series[index][:100])\nplt.show()\nplt.figure(figsize=(30,10))\nplt.plot(Sales_series[index][1500:])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The variance seems to be constant throughout, so we can use an additive model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- From the above, data looks very seasonal after every passing year. Hence Data is non stationary. Let's see what the Dicky Fuller Test says","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"NonStationary = []\n\nfor saleID in clusters[0]:\n    #print(Sales_series[saleID])\n    result = adfuller(Sales_series[saleID])\n    \n    if result[1] > 0.05:\n        \n        NonStationary.append(saleID)\n    else:\n        pass      \n\nprint(NonStationary)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Dicky Fuller Test detecting detecting non stationary behaviour only in some of the graphs. However, we know that is not the case as there is a clear seasonality in the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Forecast with FbProphet for a single time series. \n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SalesSample = pd.DataFrame()\nSalesSample['y'] = Sales_series[index]\nSalesSample['ds'] = Dates[index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_size = 90 # 3 months of data\n\ntrain = SalesSample[:-test_size]\ntest = SalesSample[-test_size:]\n\nplt.subplots(figsize=(20, 5))\n\nplt.plot(train['ds'], train['y'],color='blue', label='Train')\nplt.plot(test['ds'], test['y'], color='red', label='Test')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Prophet(daily_seasonality=False,\nweekly_seasonality=True,\nyearly_seasonality=True,\nholidays = holidays, seasonality_mode='additive',holidays_prior_scale=0.5,)\n\n# holiday prior scale to bring down the effect of holidays in the model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train)\nforecast = model.predict(test)\nmodel.plot_components(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30, 5))\n\nplt.plot(test['ds'], test['y'], c='r', label='Test')\nplt.plot(forecast['ds'], forecast['yhat'], c='blue', marker='o',label='Forecast')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate SMAPE\ny_true = test['y'].to_list()\ny_true = np.array(y_true)\ny_forecast = forecast['yhat'].to_list()\ny_forecast = np.array(y_forecast)\n\nsmape = (np.absolute(y_true - y_forecast) / (np.absolute(y_true) + np.absolute(y_forecast))).mean() * 200\nprint('SMAPE is:', smape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Lets try with applying Log Transform on the data and check for SMAPE. It will stabilse variances in the time series","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['y'] = np.log1p(train['y'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Prophet(daily_seasonality=False,\nweekly_seasonality=True,\nyearly_seasonality=True,\nholidays = holidays, seasonality_mode='additive',holidays_prior_scale=0.5,)\n\nmodel.fit(train)\nforecast = model.predict(test)\nmodel.plot_components(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast['yhat'] = np.expm1(forecast['yhat'])\nprint(forecast['yhat'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30, 5))\nplt.plot(test['ds'], test['y'], c='r', label='Test')\nplt.plot(forecast['ds'], forecast['yhat'], c='blue', marker='o',label='Forecast')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = test['y'].to_list()\ny_true = np.array(y_true)\ny_forecast = forecast['yhat'].to_list()\ny_forecast = np.array(y_forecast)\n\nsmape = (np.absolute(y_true - y_forecast) / (np.absolute(y_true) + np.absolute(y_forecast))).mean() * 200\nprint('SMAPE  is:', smape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Minor improvement or sometimes none through Log transform. However will include it in final model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Lets look at cluster 2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random time series in cluster 2\nno = random.randint(0,101)\n\nindex = clusters[1][no]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for weekly seasonality using ACF\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(211)\nfig = plot_acf(Sales_series[index],lags=24,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for monthly seasonality using ACF\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(211)\nfig = plot_acf(Sales_series[index],lags=120,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for yearly seasonality using ACF\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(211)\nfig = plot_acf(Sales_series[index],lags=750,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,10))\nprint(\"Image Corresponding to Item:\",IndexSale_Series[index][1],\"And Store:\",IndexSale_Series[index][2])\nplt.plot(Sales_series[index][:100])\nplt.show()\nplt.figure(figsize=(30,10))\nplt.plot(Sales_series[index][1500:])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NonStationary = []\n\nfor saleID in clusters[1]:\n    #print(Sales_series[saleID])\n    result = adfuller(Sales_series[saleID])\n    \n    if result[1] > 0.05:\n        \n        NonStationary.append(saleID)\n    else:\n        pass      \n\nprint(NonStationary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The patterns in both the clusters seems to be similar. We can use the same parameters for fitting","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Implement the model and find 3 months of results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/demand-forecasting-kernels-only/train.csv\", parse_dates=['date'], index_col=['date'])\ntest = pd.read_csv(\"../input/demand-forecasting-kernels-only/test.csv\",parse_dates=['date'],index_col=['date'])\n\nresults = test.reset_index()\nresults['sales'] = 0\n\nstores = df['store'].unique()\nitems = df['item'].unique()\n\nfor store in stores :\n    for item in items:\n        \n        to_train = train.loc[(train['store'] == store) & (train['item'] == item)].reset_index()\n        to_train.rename(columns={'date': 'ds', 'sales': 'y'}, inplace=True)\n        \n        to_train['y'] = np.log1p(to_train['y'])\n        \n        model = Prophet(daily_seasonality=False,\n        weekly_seasonality=True,\n        yearly_seasonality=True,\n        holidays = holidays, seasonality_mode='additive',holidays_prior_scale=0.5,)\n        \n        model.fit(to_train[['ds', 'y']])\n        \n        future = model.make_future_dataframe(periods=len(test.index.unique()),include_history=False)\n        forecast = model.predict(future)\n        \n        results.loc[(results['store'] == store) & (results['item'] == item),'sales'] = np.expm1(forecast['yhat']).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.drop(['date', 'store', 'item'], axis=1, inplace=True)\nresults.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results['sales'] = np.round(results['sales']).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}