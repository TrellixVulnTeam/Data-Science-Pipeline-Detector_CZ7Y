{"cells":[{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!pip install spektral","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !ls -lh \nTRAIN_FILE=\"/kaggle/input/demand-forecasting-kernels-only/train.csv\"","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"\"\"\" spektral_utilities \"\"\"\nimport numpy as np\nfrom scipy import sparse as sp\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.python.ops.linalg.sparse import sparse as tfsp\nfrom tensorflow.keras import backend as K\n\nSINGLE  = 1   # Single         (rank(a)=2, rank(b)=2)\nMIXED   = 2   # Mixed          (rank(a)=2, rank(b)=3)\niMIXED  = 3   # Inverted mixed (rank(a)=3, rank(b)=2)\nBATCH   = 4   # Batch          (rank(a)=3, rank(b)=3)\nUNKNOWN = -1  # Unknown\n\n\ndef transpose(a, perm=None, name=None):\n    \"\"\"\n    Transposes a according to perm, dealing automatically with sparsity.\n    :param a: Tensor or SparseTensor with rank k.\n    :param perm: permutation indices of size k.\n    :param name: name for the operation.\n    :return: Tensor or SparseTensor with rank k.\n    \"\"\"\n    if K.is_sparse(a):\n        transpose_op = tf.sparse.transpose\n    else:\n        transpose_op = tf.transpose\n\n    if perm is None:\n        perm = (1, 0)  # Make explicit so that shape will always be preserved\n    return transpose_op(a, perm=perm, name=name)\n\n\ndef reshape(a, shape=None, name=None):\n    \"\"\"\n    Reshapes a according to shape, dealing automatically with sparsity.\n    :param a: Tensor or SparseTensor.\n    :param shape: new shape.\n    :param name: name for the operation.\n    :return: Tensor or SparseTensor.\n    \"\"\"\n    if K.is_sparse(a):\n        reshape_op = tf.sparse.reshape\n    else:\n        reshape_op = tf.reshape\n\n    return reshape_op(a, shape=shape, name=name)\n\n\ndef autodetect_mode(a, b):\n    \"\"\"\n    Return a code identifying the mode of operation (single, mixed, inverted mixed and\n    batch), given a and b. See `ops.modes` for meaning of codes.\n    :param a: Tensor or SparseTensor.\n    :param b: Tensor or SparseTensor.\n    :return: mode of operation as an integer code.\n    \"\"\"\n    a_dim = K.ndim(a)\n    b_dim = K.ndim(b)\n    if b_dim == 2:\n        if a_dim == 2:\n            return SINGLE\n        elif a_dim == 3:\n            return iMIXED\n    elif b_dim == 3:\n        if a_dim == 2:\n            return MIXED\n        elif a_dim == 3:\n            return BATCH\n    return UNKNOWN\n\n\ndef filter_dot(fltr, features):\n    \"\"\"\n    Wrapper for matmul_A_B, specifically used to compute the matrix multiplication\n    between a graph filter and node features.\n    :param fltr:\n    :param features: the node features (N x F in single mode, batch x N x F in\n    mixed and batch mode).\n    :return: the filtered features.\n    \"\"\"\n    mode = autodetect_mode(fltr, features)\n    if mode == SINGLE or mode == BATCH:\n        return dot(fltr, features)\n    else:\n        # Mixed mode\n        return mixed_mode_dot(fltr, features)\n\n\ndef dot(a, b, transpose_a=False, transpose_b=False):\n    \"\"\"\n    Dot product between a and b along innermost dimensions, for a and b with\n    same rank. Supports both dense and sparse multiplication (including\n    sparse-sparse).\n    :param a: Tensor or SparseTensor with rank 2 or 3.\n    :param b: Tensor or SparseTensor with same rank as a.\n    :param transpose_a: bool, transpose innermost two dimensions of a.\n    :param transpose_b: bool, transpose innermost two dimensions of b.\n    :return: Tensor or SparseTensor with rank 2 or 3.\n    \"\"\"\n    a_is_sparse_tensor = isinstance(a, tf.SparseTensor)\n    b_is_sparse_tensor = isinstance(b, tf.SparseTensor)\n    if a_is_sparse_tensor:\n        a = tfsp.CSRSparseMatrix(a)\n    if b_is_sparse_tensor:\n        b = tfsp.CSRSparseMatrix(b)\n    out = tfsp.matmul(a, b, transpose_a=transpose_a, transpose_b=transpose_b)\n    if hasattr(out, 'to_sparse_tensor'):\n        return out.to_sparse_tensor()\n\n    return out\n\n\ndef mixed_mode_dot(a, b):\n    \"\"\"\n    Computes the equivalent of `tf.einsum('ij,bjk->bik', a, b)`, but\n    works for both dense and sparse input filters.\n    :param a: rank 2 Tensor or SparseTensor.\n    :param b: rank 3 Tensor or SparseTensor.\n    :return: rank 3 Tensor or SparseTensor.\n    \"\"\"\n    s_0_, s_1_, s_2_ = K.int_shape(b)\n    B_T = transpose(b, (1, 2, 0))\n    B_T = reshape(B_T, (s_1_, -1))\n    output = dot(a, B_T)\n    output = reshape(output, (s_1_, s_2_, -1))\n    output = transpose(output, (2, 0, 1))\n\n    return output\n\n\ndef degree_power(A, k):\n    r\"\"\"\n    Computes \\(\\D^{k}\\) from the given adjacency matrix. Useful for computing\n    normalised Laplacian.\n    :param A: rank 2 array or sparse matrix.\n    :param k: exponent to which elevate the degree matrix.\n    :return: if A is a dense array, a dense array; if A is sparse, a sparse\n    matrix in DIA format.\n    \"\"\"\n    degrees = np.power(np.array(A.sum(1)), k).flatten()\n    degrees[np.isinf(degrees)] = 0.\n    if sp.issparse(A):\n        D = sp.diags(degrees)\n    else:\n        D = np.diag(degrees)\n    return D\n\n\ndef normalized_adjacency(A, symmetric=True):\n    r\"\"\"\n    Normalizes the given adjacency matrix using the degree matrix as either\n    \\(\\D^{-1}\\A\\) or \\(\\D^{-1/2}\\A\\D^{-1/2}\\) (symmetric normalization).\n    :param A: rank 2 array or sparse matrix;\n    :param symmetric: boolean, compute symmetric normalization;\n    :return: the normalized adjacency matrix.\n    \"\"\"\n    if symmetric:\n        normalized_D = degree_power(A, -0.5)\n        output = normalized_D.dot(A).dot(normalized_D)\n    else:\n        normalized_D = degree_power(A, -1.)\n        output = normalized_D.dot(A)\n    return output\n\n\ndef localpooling_filter(A, symmetric=True):\n    r\"\"\"\n    Computes the graph filter described in\n    [Kipf & Welling (2017)](https://arxiv.org/abs/1609.02907).\n    :param A: array or sparse matrix with rank 2 or 3;\n    :param symmetric: boolean, whether to normalize the matrix as\n    \\(\\D^{-\\frac{1}{2}}\\A\\D^{-\\frac{1}{2}}\\) or as \\(\\D^{-1}\\A\\);\n    :return: array or sparse matrix with rank 2 or 3, same as A;\n    \"\"\"\n    fltr = A.copy()\n    if sp.issparse(A):\n        I = sp.eye(A.shape[-1], dtype=A.dtype)\n    else:\n        I = np.eye(A.shape[-1], dtype=A.dtype)\n    if A.ndim == 3:\n        for i in range(A.shape[0]):\n            A_tilde = A[i] + I\n            fltr[i] = normalized_adjacency(A_tilde, symmetric=symmetric)\n    else:\n        A_tilde = A + I\n        fltr = normalized_adjacency(A_tilde, symmetric=symmetric)\n\n    if sp.issparse(fltr):\n        fltr.sort_indices()\n    return fltr\n\n\"\"\" spektral_gcn \"\"\"\nfrom tensorflow.keras import activations, initializers, regularizers, constraints\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Layer\n\n#from spektral_utilities import filter_dot, dot, localpooling_filter\n\n\nclass GraphConv(Layer):\n    r\"\"\"\n    A graph convolutional layer (GCN) as presented by\n    [Kipf & Welling (2016)](https://arxiv.org/abs/1609.02907).\n    **Mode**: single, mixed, batch.\n    This layer computes:\n    $$\n        \\Z = \\hat \\D^{-1/2} \\hat \\A \\hat \\D^{-1/2} \\X \\W + \\b\n    $$\n    where \\( \\hat \\A = \\A + \\I \\) is the adjacency matrix with added self-loops\n    and \\(\\hat\\D\\) is its degree matrix.\n    **Input**\n    - Node features of shape `([batch], N, F)`;\n    - Modified Laplacian of shape `([batch], N, N)`; can be computed with\n    `spektral.utils.convolution.localpooling_filter`.\n    **Output**\n    - Node features with the same shape as the input, but with the last\n    dimension changed to `channels`.\n    **Arguments**\n    - `channels`: number of output channels;\n    - `activation`: activation function to use;\n    - `use_bias`: whether to add a bias to the linear transformation;\n    - `kernel_initializer`: initializer for the kernel matrix;\n    - `bias_initializer`: initializer for the bias vector;\n    - `kernel_regularizer`: regularization applied to the kernel matrix;\n    - `bias_regularizer`: regularization applied to the bias vector;\n    - `activity_regularizer`: regularization applied to the output;\n    - `kernel_constraint`: constraint applied to the kernel matrix;\n    - `bias_constraint`: constraint applied to the bias vector.\n    \"\"\"\n\n    def __init__(self,\n                 channels,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n\n        super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n        self.channels = channels\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n        self.supports_masking = False\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        input_dim = input_shape[0][-1]\n        self.kernel = self.add_weight(shape=(input_dim, self.channels),\n                                      initializer=self.kernel_initializer,\n                                      name='kernel',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.channels,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def call(self, inputs):\n        features = inputs[0]\n        fltr = inputs[1]\n\n        # Convolution\n        output = dot(features, self.kernel)\n        output = filter_dot(fltr, output)\n\n        if self.use_bias:\n            output = K.bias_add(output, self.bias)\n        if self.activation is not None:\n            output = self.activation(output)\n        return output\n\n    def compute_output_shape(self, input_shape):\n        features_shape = input_shape[0]\n        output_shape = features_shape[:-1] + (self.channels,)\n        return output_shape\n\n    def get_config(self):\n        config = {\n            'channels': self.channels,\n            'activation': activations.serialize(self.activation),\n            'use_bias': self.use_bias,\n            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n            'bias_initializer': initializers.serialize(self.bias_initializer),\n            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n            'bias_constraint': constraints.serialize(self.bias_constraint)\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @staticmethod\n    def preprocess(A):\n        return localpooling_filter(A)\n\nfrom spektral.layers import GraphConv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom datetime import date, timedelta\nimport os\nimport random\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom scipy.stats import skew, kurtosis\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### IMPORT SPEKTRAL CLASSES ###\n\n# from spektral_utilities import *\n# from spektral_gcn import GraphConv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### READ DATA ###\n\ndf = pd.read_csv(TRAIN_FILE)\ndf['date'] = pd.to_datetime(df['date'])\n\nprint(df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### SWITCH DATA FROM VERTICAL TO HORIZONTAL FORMAT ###\n\nunstaked_df = df.copy()\nunstaked_df['id'] = df['item'].astype(str)+'_'+df['store'].astype(str)\nunstaked_df.set_index(['id','date'], inplace=True)\nunstaked_df.drop(['store','item'], axis=1, inplace=True)\nunstaked_df = unstaked_df.astype(float).unstack()\nunstaked_df.columns = unstaked_df.columns.get_level_values(1)\n\nprint(unstaked_df.shape)\nunstaked_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### UTILITY FUNCTIONS FOR FEATURE ENGINEERING ###\n\nsequence_length = 14\n\n\n\ndef get_timespan(df, today, days):    \n    df = df[pd.date_range(today - timedelta(days=days), \n            periods=days, freq='D')] # day - n_days <= dates < day    \n    return df\n\ndef create_features(df, today, seq_len):\n    \n    all_sequence = get_timespan(df, today, seq_len).values\n    \n    group_store = all_sequence.reshape((-1, 10, seq_len))\n    \n    store_corr = np.stack([np.corrcoef(i) for i in group_store], axis=0)\n    \n    store_features = np.stack([\n              group_store.mean(axis=2),\n              group_store[:,:,int(sequence_length/2):].mean(axis=2),\n              group_store.std(axis=2),\n              group_store[:,:,int(sequence_length/2):].std(axis=2),\n              skew(group_store, axis=2),\n              kurtosis(group_store, axis=2),\n              np.apply_along_axis(lambda x: np.polyfit(np.arange(0, sequence_length), x, 1)[0], 2, group_store)\n            ], axis=1)\n    \n    group_store = np.transpose(group_store, (0,2,1))\n    store_features = np.transpose(store_features, (0,2,1))\n    \n    return group_store, store_corr, store_features\n\ndef create_label(df, today):\n    \n    y = df[today].values\n    \n    return y.reshape((-1, 10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### PLOT A SEQUENCE OF SALES FOR ITEM 10 IN ALL STORES ###\n\nsequence = get_timespan(unstaked_df, date(2017,11,1), 30)\nsequence.head(10).T.plot(figsize=(14,5))\nplt.ylabel('sales')\nplt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### DEFINE TRAIN, VALID, TEST DATES ###\n\ntrain_date = date(2013, 1, 1)\nvalid_date = date(2015, 1, 1)\ntest_date = date(2016, 1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### CREATE TRAIN FEATURES ###\n\nX_seq, X_cor, X_feat, y = [], [], [], []\n\nfor d in tqdm(pd.date_range(train_date+timedelta(days=sequence_length), valid_date)):\n    seq_, corr_, feat_ = create_features(unstaked_df, d, sequence_length)\n    y_ = create_label(unstaked_df, d)\n    X_seq.append(seq_), X_cor.append(corr_), X_feat.append(feat_), y.append(y_)\n    \nX_train_seq = np.concatenate(X_seq, axis=0).astype('float16')\nX_train_cor = np.concatenate(X_cor, axis=0).astype('float16')\nX_train_feat = np.concatenate(X_feat, axis=0).astype('float16')\ny_train = np.concatenate(y, axis=0).astype('float16')\n\nprint(X_train_seq.shape, X_train_cor.shape, X_train_feat.shape, y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### CREATE VALID FEATURES ###\n\nX_seq, X_cor, X_feat, y = [], [], [], []\n\nfor d in tqdm(pd.date_range(valid_date+timedelta(days=sequence_length), test_date)):\n    seq_, corr_, feat_ = create_features(unstaked_df, d, sequence_length)\n    y_ = create_label(unstaked_df, d)\n    X_seq.append(seq_), X_cor.append(corr_), X_feat.append(feat_), y.append(y_)\n    \nX_valid_seq = np.concatenate(X_seq, axis=0).astype('float16')\nX_valid_cor = np.concatenate(X_cor, axis=0).astype('float16')\nX_valid_feat = np.concatenate(X_feat, axis=0).astype('float16')\ny_valid = np.concatenate(y, axis=0).astype('float16')\n\nprint(X_valid_seq.shape, X_valid_cor.shape, X_valid_feat.shape, y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"### CREATE TEST FEATURES ###\n\nX_seq, X_cor, X_feat, y = [], [], [], []\n\nfor d in tqdm(pd.date_range(test_date+timedelta(days=sequence_length), date(2016,12,31))):\n    seq_, corr_, feat_ = create_features(unstaked_df, d, sequence_length)\n    y_ = create_label(unstaked_df, d)\n    X_seq.append(seq_), X_cor.append(corr_), X_feat.append(feat_), y.append(y_)\n    \nX_test_seq = np.concatenate(X_seq, axis=0).astype('float16')\nX_test_cor = np.concatenate(X_cor, axis=0).astype('float16')\nX_test_feat = np.concatenate(X_feat, axis=0).astype('float16')\ny_test = np.concatenate(y, axis=0).astype('float16')\n\nprint(X_test_seq.shape, X_test_cor.shape, X_test_feat.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"### SCALE SEQUENCES ###\n\nscaler_seq = StandardScaler()\nscaler_feat = StandardScaler()\n\nX_train_seq = scaler_seq.fit_transform(X_train_seq.reshape(-1,10)).reshape(X_train_seq.shape)\nX_valid_seq = scaler_seq.transform(X_valid_seq.reshape(-1,10)).reshape(X_valid_seq.shape)\nX_test_seq = scaler_seq.transform(X_test_seq.reshape(-1,10)).reshape(X_test_seq.shape)\n\ny_train = scaler_seq.transform(y_train)\ny_valid = scaler_seq.transform(y_valid)\ny_test = scaler_seq.transform(y_test)\n\nX_train_feat = scaler_feat.fit_transform(X_train_feat.reshape(-1,10)).reshape(X_train_feat.shape)\nX_valid_feat = scaler_feat.transform(X_valid_feat.reshape(-1,10)).reshape(X_valid_feat.shape)\nX_test_feat = scaler_feat.transform(X_test_feat.reshape(-1,10)).reshape(X_test_feat.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"### OBTAIN LAPLACIANS FROM CORRELATIONS ###\n\nX_train_lap = localpooling_filter(1 - np.abs(X_train_cor))\nX_valid_lap = localpooling_filter(1 - np.abs(X_valid_cor))\nX_test_lap = localpooling_filter(1 - np.abs(X_test_cor))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_model():\n\n    opt = Adam(lr=0.001)\n\n    inp_seq = Input((sequence_length, 10))\n    inp_lap = Input((10, 10))\n    inp_feat = Input((10, X_train_feat.shape[-1]))\n\n    x = GraphConv(32, activation='relu')([inp_feat, inp_lap])\n    x = GraphConv(16, activation='relu')([x, inp_lap])\n    x = Flatten()(x)\n\n    xx = LSTM(128, activation='relu', return_sequences=True)(inp_seq)\n    xx = LSTM(32, activation='relu')(xx)\n\n    x = Concatenate()([x,xx])\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dense(32, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    out = Dense(1)(x)\n\n    model = Model([inp_seq, inp_lap, inp_feat], out)\n    model.compile(optimizer=opt, loss='mse', \n                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model = get_model()\nmodel.summary() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"### TRAIN A MODEL FOR EACH STORES USING ALL THE DATA AVAILALBE FROM OTHER STORES ###\n\n\ntf.random.set_seed(33)\nos.environ['PYTHONHASHSEED'] = str(33)\nnp.random.seed(33)\nrandom.seed(33)\n\nsession_conf = tf.compat.v1.ConfigProto(\n    intra_op_parallelism_threads=1, \n    inter_op_parallelism_threads=1\n)\nsess = tf.compat.v1.Session(\n    graph=tf.compat.v1.get_default_graph(), \n    config=session_conf\n)\ntf.compat.v1.keras.backend.set_session(sess)\n\n\n\npred_valid_all = np.zeros(y_valid.shape)\npred_test_all = np.zeros(y_test.shape)\n\nfor store in range(10):\n\n    print('-------', 'store', store, '-------')\n    \n    es = EarlyStopping(patience=5, verbose=1, min_delta=0.001, monitor='val_loss', mode='auto', restore_best_weights=True)\n\n    model = get_model()\n    model.fit([X_train_seq, X_train_lap, X_train_feat], y_train[:,store], epochs=100, batch_size=256, \n              validation_data=([X_valid_seq, X_valid_lap, X_valid_feat], y_test[:,store]), callbacks=[es], verbose=2)\n\n    pred_valid_all[:,store] = model.predict([X_valid_seq, X_valid_lap, X_valid_feat]).ravel()\n    pred_test_all[:,store] = model.predict([X_test_seq, X_test_lap, X_test_feat]).ravel()\n\n\npred_valid_all = scaler_seq.inverse_transform(pred_valid_all)\nreverse_valid = scaler_seq.inverse_transform(y_valid)\npred_test_all = scaler_seq.inverse_transform(pred_test_all)\nreverse_test = scaler_seq.inverse_transform(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"### RMSE ON TEST DATA ###\n\nerror = {}\n\nfor store in range(10):\n    \n    error[store] = np.sqrt(mean_squared_error(reverse_test[:,store], pred_test_all[:,store]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"### PLOT RMSE ###\n\nplt.figure(figsize=(14,5))\nplt.bar(range(10), error.values())\nplt.xticks(range(10), ['store_'+str(s) for s in range(10)])\nplt.ylabel('error')\nnp.set_printoptions(False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"### UTILITY FUNCTION TO PLOT PREDICTION ###\n\ndef plot_predictions(y_true, y_pred, store, item):\n    \n    y_true = y_true.reshape(50,-1,10)\n    y_pred = y_pred.reshape(50,-1,10)\n    \n    plt.plot(y_true[item,:,store], label='true')\n    plt.plot(y_pred[item,:,store], label='prediction')\n    plt.title(f\"store: {store} item: {item}\"); plt.legend()\n    plt.ylabel('sales'); plt.xlabel('date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(11,5))\nplot_predictions(reverse_test, pred_test_all, 7,0)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}