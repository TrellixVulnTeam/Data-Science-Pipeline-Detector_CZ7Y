{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Summary:\n- Deep learning is used where RNN (LSTM) layers are applied to capture/learn the time sequences \n- Multivariate and multi-step future prediction are used\n    - Multivariate to learn based on more than one feature per step\n    - multi-step future prediction to predict multi days in the future\n- Input shape is (num of samples, num of history steps = 120 days for example, input features = 7 'date details and sales')\n- Output shape is (num of samples, num of future steps = 90 days, output features = 1 'sales')\n- To construct more important features, dates are expanded to separate columns, day, month, year, day of the week, ...\n\nThis code is inspired by the tensorflow [time series tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series)   \nNice data analysis can be found [here](https://www.kaggle.com/thexyzt/keeping-it-simple-by-xyzt) \n\n## Notes:\n- We have 50 items and 10 stores\n- We should have 500 models!\n- However, we can train one model for all items and stores (One global model)\n- At test time, we give the model the most recent historical data for a given item and store, and then ask the trained model to predict the next future steps\n\n\n*This code is for demonstration purposes only*","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option('display.max_rows', 20)\n\nPATH = \"../input/demand-forecasting-kernels-only\"\ntrain = pd.read_csv(f\"{PATH}/train.csv\", low_memory=False, parse_dates=['date'], index_col=['date'])\ntest = pd.read_csv(f\"{PATH}/test.csv\", low_memory=False, parse_dates=['date'], index_col=['date'])\nsample_sub = pd.read_csv(f\"{PATH}/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(train)\ndisplay(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Expand with more date columns\ndef expand_df(df, istest = True):\n    data = df.copy()\n    data['day'] = data.index.day\n    data['month'] = data.index.month\n    data['year'] = data.index.year\n    data['dayofweek'] = data.index.dayofweek\n    if istest:\n        data = data[['store', 'item', 'day', 'month', 'year', 'dayofweek', 'sales']]\n    else:\n        data = data[['id', 'store', 'item', 'day', 'month', 'year', 'dayofweek']]\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = expand_df(train)\ndisplay(train_data)\n\ngrand_avg = train_data.sales.mean()\nprint(f\"The grand average of sales in this dataset is {grand_avg:.4f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = expand_df(test, istest=False)\ndisplay(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_year_item = pd.pivot_table(train_data, index='year', columns='item', values='sales', aggfunc=np.mean).values\nagg_year_store = pd.pivot_table(train_data, index='year', columns='store', values='sales', aggfunc=np.mean).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_year_item.shape, agg_year_store.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(pd.pivot_table(train_data, index='year', columns='store', values='sales', aggfunc=np.mean))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 7))\nplt.subplot(121)\nplt.plot(agg_year_item) #/ agg_year_item.mean(0)[np.newaxis])\nplt.title(\"Items\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Relative Sales\")\nplt.subplot(122)\nplt.plot(agg_year_store) #/ agg_year_store.mean(0)[np.newaxis])\nplt.title(\"Stores\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Relative Sales\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_SPLIT = 40000\ndataset = train_data.values\ndataset = dataset[-50000:]\ndata_mean = dataset[:TRAIN_SPLIT].mean(axis=0)\ndata_std = dataset[:TRAIN_SPLIT].std(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data normalization\ndataset = (dataset-data_mean)/data_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def multivariate_data(dataset, target, start_index, end_index, history_size,\n                      target_size, step, single_step=False):\n  data = []\n  labels = []\n\n  start_index = start_index + history_size\n  if end_index is None:\n    end_index = len(dataset) - target_size\n\n  for i in range(start_index, end_index):\n    indices = range(i-history_size, i, step)\n    data.append(dataset[indices])\n\n    if single_step:\n      labels.append(target[i+target_size])\n    else:\n      labels.append(target[i:i+target_size])\n\n  return np.array(data), np.array(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"past_history = 120 # history length\nfuture_target = 90  # three future months as required to predict\nSTEP = 1 # every day\n\nx_train_multi, y_train_multi = multivariate_data(dataset, dataset[:, 6], 0, TRAIN_SPLIT,\n                                                 past_history,\n                                                 future_target, STEP)\nx_val_multi, y_val_multi = multivariate_data(dataset, dataset[:, 6], TRAIN_SPLIT, None,\n                                             past_history,\n                                             future_target, STEP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Single window of past history : {}'.format(x_train_multi[0].shape))\nprint ('\\n Target temperature to predict : {}'.format(y_train_multi[0].shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\n\n\nmpl.rcParams['figure.figsize'] = (8, 6)\nmpl.rcParams['axes.grid'] = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 256\nBUFFER_SIZE = 10000\n\ntrain_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\ntrain_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n\nval_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\nval_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_time_steps(length):\n  return list(range(-length, 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_train_history(history, title):\n  loss = history.history['loss']\n  val_loss = history.history['val_loss']\n\n  epochs = range(len(loss))\n\n  plt.figure()\n\n  plt.plot(epochs, loss, 'b', label='Training loss')\n  plt.plot(epochs, val_loss, 'r', label='Validation loss')\n  plt.title(title)\n  plt.legend()\n\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def multi_step_plot(history, true_future, prediction):\n  plt.figure(figsize=(12, 6))\n  num_in = create_time_steps(len(history))\n  num_out = len(true_future)\n\n  plt.plot(num_in, np.array(history[:, 6]), label='History')\n  plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'b-',\n           label='True Future')\n  if prediction.any():\n    plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'r-',\n             label='Predicted Future')\n  plt.legend(loc='upper left')\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x, y in train_data_multi.take(3):\n  multi_step_plot(x[0], y[0], np.array([0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multi_step_model = tf.keras.models.Sequential()\nmulti_step_model.add(tf.keras.layers.LSTM(32,\n                                          return_sequences=True,\n                                          input_shape=x_train_multi.shape[-2:]))\nmulti_step_model.add(tf.keras.layers.LSTM(16, activation='relu'))\nmulti_step_model.add(tf.keras.layers.Dense(y_train_multi.shape[-1]))\n\n# multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')\nmulti_step_model.compile(optimizer=tf.keras.optimizers.Adam(clipvalue=1.0), loss='mse')\nmulti_step_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sanity\nfor x, y in val_data_multi.take(1):\n  print (x.shape, multi_step_model.predict(x).shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, cooldown=4, patience=3, min_lr=0.00001, verbose=1)\nearly_stop = tf.keras.callbacks.EarlyStopping( monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\nmulti_step_history = multi_step_model.fit(train_data_multi, epochs=25,\n                                          steps_per_epoch=400,\n                                          validation_data=val_data_multi,\n                                          validation_steps=50, callbacks=[reduce_lr, early_stop]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_train_history(multi_step_history, 'Multi-Step Training and validation loss')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do not worry about the overfitting, we used **EarlyStopping** and **restore_best_weights**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for x, y in val_data_multi.take(3):\n  multi_step_plot(x[0], y[0], multi_step_model.predict(x)[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What is Next:\n- Prepare the whole data where the sequences are separated at item/store boundaries (Where no sequences containing different item/store)\n- Use one-hot-encoding for items and stores  \n- Hyper-parameter optimization for the model\n- Train, evaluate and then test the model on the required test set\n- Submit the results :)   ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}