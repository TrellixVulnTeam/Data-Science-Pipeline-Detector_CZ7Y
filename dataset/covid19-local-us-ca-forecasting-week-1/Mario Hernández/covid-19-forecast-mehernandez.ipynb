{"cells":[{"metadata":{},"cell_type":"markdown","source":"# This Kaggle challenge aims to predict the future contagious and fatality cases for COVID-19 in California US, in a way to prepare all medical infrastructure to treat this terrible desease that is hitting so strongly our society."},{"metadata":{},"cell_type":"markdown","source":"To do that, Kaggle provided a dataset with the information related to confirmed cases and fatalities in California US since the dates of the outbreak."},{"metadata":{},"cell_type":"markdown","source":"First We check the files provided by Kaggle:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To check the information we load the files into Pandas dataframes:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load data into Pandas dataframes\ndf_train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')\ndf_test = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')\ndf_submission = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check a preview of the data\ndf_train.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the information we need is the date, confirmed cases and fatalities, nevertheless we check that the other columns doesn't change their values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the properties of the data\n\nprint(df_train['Province/State'].unique())\nprint(df_train['Country/Region'].unique())\nprint(df_train['Lat'].unique())\nprint(df_train['Long'].unique())\nprint(df_train.dtypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We describe the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the distribution of the confirmed cases\n\ndf_train.hist(column='ConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the distribution of the fatalities\n\ndf_train.hist(column='Fatalities')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, most of the registers are from dates when there was no confirmed cases. We update the dataframe to include just the needed columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take only what we need: date, confirmed cases and fatalities\n\ndf_train = df_train[['Date', 'ConfirmedCases', 'Fatalities']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We transform the date column to the Pandas date format and sort the dataframe:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert Date column to Pandas date and orther to get chronological data\n\ndf_train['Date'] = pd.to_datetime(df_train['Date'])\ndf_train = df_train.sort_values(by=['Date'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We check the trend of confirmed cases and fatalities in a bar plot:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the trend in a chart\n\ndf_train.plot.bar(x='Date', y=['ConfirmedCases','Fatalities'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The growth seems to be exponential, but too many registers contains 0 confirmed cases, just to see closer we plot without this data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# As the confirmed cases are far away from the start we will focus in that time\n\ndf_train2 = df_train.query('ConfirmedCases != 0.0')\n\ndf_train2.plot.bar(x='Date', y=['ConfirmedCases', 'Fatalities'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, to make the predictions we first need to expand the features, in this case, we expand the date column:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Week'] = df_train['Date'].dt.week\ndf_train['Day'] = df_train['Date'].dt.day\ndf_train['WeekDay'] = df_train['Date'].dt.dayofweek\ndf_train['YearDay'] = df_train['Date'].dt.dayofyear","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we import all models, create them, fit them with data and check the scores for the best result:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import Lasso, ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import r2_score\n\nfrom sklearn.model_selection import train_test_split\n\npredictors = df_train.drop(['Date', 'ConfirmedCases', 'Fatalities'], axis=1)\ntarget = df_train[['ConfirmedCases', 'Fatalities']]\nx_train, x_test, y_train, y_test = train_test_split(predictors, target, test_size=0.2, random_state=1)\n\ndef scores(alg):\n    lin = alg()\n    lin.fit(x_train, y_train['ConfirmedCases'])\n    y_pred = lin.predict(x_test)\n    lin_r = r2_score(y_test['ConfirmedCases'], y_pred)\n    s.append(lin_r)\n    \n    lin.fit(x_train, y_train['Fatalities'])\n    y_pred = lin.predict(x_test)\n    lin_r = r2_score(y_test['Fatalities'], y_pred)\n    s2.append(lin_r)\n    \nalgos = [KNeighborsRegressor, LinearRegression, RandomForestRegressor, GradientBoostingRegressor, Lasso, ElasticNet, DecisionTreeRegressor]\n\ns = []\ns2 = []\n\nfor algo in algos:\n    scores(algo)\n    \nmodels = pd.DataFrame({\n    'Method': ['KNeighborsRegressor', 'LinearRegression', 'RandomForestRegressor', 'GradientBoostingRegressor', 'Lasso', 'ElasticNet', 'DecisionTreeRegressor'],\n    'ScoreCC': s,\n    'ScoreF' : s2\n})\n\nmodels.sort_values(by=['ScoreCC', 'ScoreF'], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the best prediction score is for the random forest regressor model, and with no hyperparameter tuning, that's amazing. Nevertheless, it can be a good practice to check if the ARIMA model which is highly used nowadays, can perform better."},{"metadata":{},"cell_type":"markdown","source":"First we need to check the autocorrelation plot, to fill the ARIMA model parameters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's try for last an ARIMA model\n\n# First we see that data is not stationary, so we need to check the autocorrelation of the time series\n\nfrom pandas.plotting import autocorrelation_plot\n\nautocorrelation_plot(df_train['ConfirmedCases'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The highest autocorrelation significant value is in near the 5th lag, now let's create the model, train it and check coeficients and residuals:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\nfrom matplotlib import*\n\narima_model = ARIMA(df_train['ConfirmedCases'], order=(4,1,0)).fit(disp=0, transparams=True, trend='c')\nprint(arima_model.summary())\n\nresiduals = pd.DataFrame(arima_model.resid)\nresiduals.plot()\npyplot.show()\nresiduals.plot(kind='kde')\npyplot.show()\nprint(residuals.describe())\n\narima_model2 = ARIMA(df_train['Fatalities'], order=(4,1,0)).fit(disp=0, transparams=True, trend='c')\nprint(arima_model2.summary())\n\nresiduals2 = pd.DataFrame(arima_model2.resid)\nresiduals2.plot()\npyplot.show()\nresiduals2.plot(kind='kde')\npyplot.show()\nprint(residuals2.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The coefficients are bad for confirmed cases and fatalities, also the residuals show there is a lot of variation that needs to be taken into account. Let's check the predictions in a plot:"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_arima = list(arima_model.predict())\npredictions_arima.append(arima_model.forecast()[0][0])\npredictions_arima.append(arima_model.forecast()[0][0])\n\ndf_train['arima'] = predictions_arima\n\npredictions_arima2 = list(arima_model2.predict())\npredictions_arima2.append(arima_model2.forecast()[0][0])\npredictions_arima2.append(arima_model2.forecast()[0][0])\n\ndf_train['arima2'] = predictions_arima2\n\ndf_train.plot.bar(x='Date', y=['ConfirmedCases', 'arima'])\ndf_train.plot.bar(x='Date', y=['Fatalities', 'arima2'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In fact, no good results at all, the best model for this case is the random forest regressor."},{"metadata":{},"cell_type":"raw","source":"First let's check the submission format and dates:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_test['Date'].values)\nprint(len(df_test['Date']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to provide the predictions using the test dataset, so we process the date column just as we did with the training dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = df_test[['ForecastId', 'Date']]\n\ndf_test['Date'] = pd.to_datetime(df_test['Date'])\ndf_test['Week'] = df_test['Date'].dt.week\ndf_test['Day'] = df_test['Date'].dt.day\ndf_test['WeekDay'] = df_test['Date'].dt.dayofweek\ndf_test['YearDay'] = df_test['Date'].dt.dayofyear\n\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We create the random forest regresor, fit it, predict using the test dataset registers and add the results to the dataframe:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestRegressor()\nmodel.fit(x_train, y_train['ConfirmedCases'])\n\nmodel2 = RandomForestRegressor()\nmodel2.fit(x_train, y_train['Fatalities'])\n\n\ndf_test['ConfirmedCases'] = model.predict(df_test.drop(['Date', 'ForecastId'], axis=1))\ndf_test['Fatalities'] = model2.predict(df_test.drop(['Date', 'ForecastId', 'ConfirmedCases'], axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final = df_test[['ForecastId', 'ConfirmedCases', 'Fatalities']] \ndf_final['ConfirmedCases'] = df_final['ConfirmedCases'].astype(int)\ndf_final['Fatalities'] = df_final['Fatalities'].astype(int)\n\ndf_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.plot.bar(x='ForecastId', y=['ConfirmedCases', 'Fatalities'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Those are the predicted values, now let's submit and we are done."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}