{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# First attempt\nThe approach here is to log-transform the data, try a linear regression on the transformed data, and use that linear fit to predict future values.\n\nI need to clean this notebook up because it's got some things in it from previous versions that I don't acutally use (the load_training_csv() function, e.g.), but I get a score of ~.03 on the case data alone which is pretty decent."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def load_training_csv(path):\n    \"\"\"helper function to regularize the preprocessing of dataframes\"\"\"\n    df = pd.read_csv(path, header=0, parse_dates=['Date'])\n    #df.drop(df[((df['ConfirmedCases'] == 0) & (df['Fatalities'] == 0))].index, inplace=True)\n    df['ConfirmedCases_log1p'] = df['ConfirmedCases'].map(np.log1p)\n    df['Fatalities_log1p'] = df['Fatalities'].map(np.log1p)\n    df.drop(['Province/State', 'Country/Region','Lat','Long'], axis=1, inplace=True)\n    return df\n\ndef load_case_data_from_csv(path):\n    \"\"\"load case count data where it's > 0 only and drop unneeded columns\"\"\"\n    df = pd.read_csv(path, header=0, parse_dates=['Date'])\n    df.drop(df[(df['ConfirmedCases'] == 0)].index, inplace=True)\n    df['ConfirmedCases_log1p'] = df['ConfirmedCases'].map(np.log1p)\n    df.drop(['Fatalities', 'Province/State', 'Country/Region','Lat','Long'], axis=1, inplace=True)\n    return df\n\ndef load_fatality_data_from_csv(path):\n    \"\"\"load fatality count data where it's > 0 only and drop unneeded columns\"\"\"\n    df = pd.read_csv(path, header=0, parse_dates=['Date'])\n    df.drop(df[(df['Fatalities'] == 0)].index, inplace=True)\n    df['Fatalities_log1p'] = df['Fatalities'].map(np.log1p)\n    df.drop(['ConfirmedCases', 'Province/State', 'Country/Region','Lat','Long'], axis=1, inplace=True)\n    return df\n\ndef rmsle(y_true, y_pred):\n    \"\"\"return the root mean squared logarithmic error: square root of the mean squared error of the natural log \n    of (value plus 1): sqrt(mean(power(log1p(p)-log1p(a),2)))\n    \"\"\"\n    return np.sqrt(np.mean(np.power(np.log1p(y_true) - np.log1p(y_pred),2)))\n\n\n\nfrom scipy.stats import linregress\n\nfull_df = load_training_csv(\"../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv\") # for reference\ncdf = load_case_data_from_csv(\"../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv\") # just case counts\nfdf = load_fatality_data_from_csv(\"../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv\") # just fatality counts\n\ny_cases_train = cdf['ConfirmedCases_log1p'].values\ny_cases_true = cdf['ConfirmedCases'].values\ny_fatalities_train = fdf['Fatalities_log1p'].values\ny_fatalities_true = fdf['Fatalities'].values\n\n# make some x values for regression purposes\ncase_xs = range(0, len(y_cases_train))\nfat_xs = range(0, len(y_fatalities_train))\n\n#\n# linear regressionon against log1p transformed data\n#\n\ncslope, cintercept, cr, cp, csterr = linregress(case_xs, y_cases_train)\nfslope, fintercept, fr, fp, fsterr = linregress(fat_xs, y_fatalities_train)\nprint(\"case log1p(y) = mx + b: {0:.4f}x + {1:.4f} sterr {2:.4f} r={3:.4f}\".format(cslope, cintercept, csterr, cr))\nprint(\"fatality log1p(y) = mx + b:{0:.4f}x + {1:.4f} sterr {2:.4f} r={3:.4f}\".format(fslope, fintercept, fsterr, fr))\n\n#\n# polyfit the transformed data\n#\n\npoly_degrees = 5\nzcase = np.polyfit(case_xs, y_cases_train, deg=poly_degrees)\nzfat = np.polyfit(fat_xs, y_fatalities_train, deg=poly_degrees)\ncase_poly = np.poly1d(zcase)\nfat_poly = np.poly1d(zfat)\n\n#\n# check linear fit against existing data\n#\n\ny_cases_pred_linear = np.round(np.expm1(case_xs * cslope + cintercept), 0)\ny_fatalities_pred_linear = np.round(np.expm1(fat_xs * fslope + fintercept), 0)\ncase_rmsle = rmsle(y_cases_true, y_cases_pred_linear)\nfat_rmsle = rmsle(y_fatalities_true, y_fatalities_pred_linear)\nplt.plot(case_xs, y_cases_true,'o')\nplt.plot(case_xs, y_cases_pred_linear, '-', label=\"RMSLE={0:.4f}\".format(case_rmsle))\nplt.plot(fat_xs, y_fatalities_true, 'o')\nplt.plot(fat_xs, y_fatalities_pred_linear, '-', label=\"RMSLE={0:.4f}\".format(fat_rmsle))\nplt.legend()\nplt.show()\n\n#\n# check poly fit against existing data\n#\n\ny_cases_pred_poly = np.round(np.expm1(case_poly(case_xs)))\ny_fatalities_pred_poly = np.round(np.expm1(fat_poly(fat_xs)))\ncase_poly_rmsle = rmsle(y_cases_true, y_cases_pred_poly)\nfat_poly_rmsle = rmsle(y_fatalities_true, y_fatalities_pred_poly)\nplt.plot(case_xs, y_cases_true, 'o')\nplt.plot(case_xs, y_cases_pred_poly, '-', label=\"RMSLE={0:.4f}\".format(case_poly_rmsle))\nplt.plot(fat_xs, y_fatalities_true, 'o')\nplt.plot(fat_xs, y_fatalities_pred_poly, '-', label='RMSLE={0:.4f}'.format(fat_poly_rmsle))\nplt.legend()\nplt.show()\n                                      \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So this is what I get as a predictor for my approach: two estimations of the training data, one pretty good, one could definitely be better.  What other ways could I try to get this data better?  Can I come up with a better way to fit this curve than log-transforming it?  I read about Taylor series; can I do that?\n\nPolynolial fitting on the transformed data is better, so let's try it with that.\n\nI'm fairly certain this grows too much, so I would need an approach that slows down some from the initial growth, since that's how diseases spread."},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(full_df.tail())\n\n# load test data and predict based on above regression\ntest_df = pd.read_csv(\"../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv\")\nprint(test_df.head())\nprint(test_df.shape)\ntest_xs = range(2,test_df.shape[0]+2)\nprint(len(test_xs))\n#print(list(test_xs))\ntest_df['xs'] = test_xs\n#test_df['ConfirmedCases'] = test_df.apply(lambda x:np.round(np.expm1(x['xs']*cslope + cintercept)), axis=1)\n#test_df['Fatalities'] = test_df.apply(lambda x: np.round(np.expm1(x['xs']*fslope + fintercept)), axis=1)\ntest_df['ConfirmedCases'] = test_df.apply(lambda x: np.round(np.expm1(case_poly(case_xs))), axis=1)\ntest_df['Fatalities'] = test_df.apply(lambda x: np.round(np.expm1(fat_poly(fat_xs))), axis=1)\n\nprint(test_df[test_df['Date'] > '2020-03-18'])\n\nsub_df = test_df[['ForecastId','ConfirmedCases','Fatalities']]\nsub_df.to_csv('submission.csv', index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The end\nI'll call it here, and make a new notebook with a different approach.\n\nIdeas for this challenge:\n- Taylor series for estimation of future values.\n- Trying to fit the data with a logistic function.\n - Challenge: you need to know where the logistic function stops. Current modeling suggests a total of 60-70% of the world will become infected in 1-2 years (per some German research groups); can I use CA population estimates for various percentages and lengths of time to define the logistic curve?\n \nIdeas for the global challenge, and the world in general:\n- So let's go with the log-transformed linear approach as an example. Can we then regress the slope of those lines based on things like population, country area, per capita health spending, number of MDs/hospital beds, etc. to forecast how many cases and deaths a country will have?\n - With the above data, how do we account for a country like Luxembourg, which is rich and small (land and pop'n), with a country like Norway, which is rich and bigger (land and pop'n), or the US, which is rich and huge (land and pop'n)? How can we model differences between socialized medicine and non?\n  - How can we model the effects of things like social distancing, whose adoption should have a big effect on the speed of transmission?\n  \nSo many questions, so little data.\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}