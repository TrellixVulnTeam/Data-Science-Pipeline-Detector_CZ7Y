{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.linear_model import LinearRegression,  Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean the data. only look at the data from the first confirmed case\ntrain = train[train.ConfirmedCases>0]\ntrain","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clearly, there is some missing data. We will look at other US states, based on the common habbits, wealth, and lack of health insurance, and fill the data from the one that is the most suitable for us"},{"metadata":{"trusted":true},"cell_type":"code","source":"whole_world_data = pd.read_csv('/kaggle//input/covid19-global-forecasting-week-1/train.csv')\nus_data = whole_world_data.loc[whole_world_data['Country/Region'] == 'US']\nus_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"possible_states = us_data['Province/State'].unique()[(us_data.groupby('Province/State').max().ConfirmedCases>144 ) &  (us_data.groupby('Province/State').min().ConfirmedCases<6)]\npossible_states","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"COLUMNS = ['ConfirmedCases', 'Fatalities', 'Province/State']\npossible_starts = pd.DataFrame(columns=COLUMNS)\nfor country in possible_states:\n    possible_starts = pd.concat([possible_starts, us_data[(us_data['Province/State'] == country) & (us_data['ConfirmedCases']<144) & (us_data['ConfirmedCases']>0)][COLUMNS]])\n\npossible_starts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is unclear whether the corona infections is following an exponential pattern or a polynomial one.\nSo, now, we are going to find which model (exponential or polynomial) and which possible start works best for the data we have got so far,\nbased on the time series cross validation method\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, find which start is the best for an exponential pattern, and what is the lowest cross validation we get with it\ndef test_exponential_accuraccy(y):\n    accuraccy = 0\n    X = np.asarray(list(range(len(y)))) +1\n    tscv = TimeSeriesSplit(len(X)-1)\n    for train_index, test_index in tscv.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        # Transform the data with a log function and after prediction, apply the exp() function.\n        regressor = TransformedTargetRegressor(regressor=LinearRegression(),\n                                                         func=np.log1p,\n                                                         inverse_func=np.expm1)\n        regressor.fit(np.array(X_train).reshape(-1,1), np.array(y_train).reshape(-1,1))\n        y_pred = regressor.predict(np.array(X_test).reshape(-1,1))\n        accuraccy += mean_squared_log_error(y_pred, y_test)\n        \n        \n    \n    return accuraccy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For every state, compute its sliding window error.\nbest_accuraccy = 100\nbest_start_state = ''\nfor state in possible_starts['Province/State'].unique():\n    possible_train_data = pd.concat([possible_starts[possible_starts['Province/State']==state], train])\n    \n    state_accuracy = test_exponential_accuraccy(possible_train_data['ConfirmedCases'].values) \n    if  state_accuracy < best_accuraccy:\n        best_start_state = state\n        best_accuraccy = state_accuracy\n                                \nprint(best_start_state, best_accuraccy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get the best time series cross validation accuracy for the exponential case when we use Wisconsin's start, and the accuracy is 0.343"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now, lets find which start is the best for a polynomial pattern, what is the degree of the polynom,\n# and what is the lowest cross validation we get with it\ndef test_polynomial_accuraccy(y, degree):\n    accuraccy = 0\n    X = np.asarray(list(range(len(y)))) +1\n    tscv = TimeSeriesSplit(len(X)-1)\n    for train_index, test_index in tscv.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        \n        # Fit the data with a polynom of the given degree.\n        model = make_pipeline(PolynomialFeatures(degree), Ridge())\n        model.fit(X_train.reshape(-1,1), y_train.reshape(-1,1))\n        y_pred = model.predict(X_test.reshape(-1,1))\n#         print(type(y_pred))\n        if y_pred < 0: \n            y_pred = np.array([0]) # this case can happen in the beggining of the sliding window, and a high degree polynom. y_pred=0 is a big enough error\n        accuraccy += mean_squared_log_error(y_pred, y_test)\n        \n        \n    \n    return accuraccy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For every state and polynom degree, compute its sliding window error\n# What we are going to do is look at a few options and decide where we get a low enough error, and not over generalize polynom.\n\ndegrees = [2,3,4,5,6]\nlowest_errors = pd.DataFrame(columns=['accuraccy', 'state', 'degree'])\nfor degree in degrees:\n    best_accuraccy = 100\n    best_start_state = ''\n    for state in possible_starts['Province/State'].unique():\n        possible_train_data = pd.concat([possible_starts[possible_starts['Province/State']==state], train])\n    \n        state_accuracy = test_polynomial_accuraccy(possible_train_data['ConfirmedCases'].values, degree)\n        if  state_accuracy < best_accuraccy:\n            best_start_state = state\n            best_accuraccy = state_accuracy\n    \n    lowest_errors = lowest_errors.append(pd.DataFrame({'accuraccy': [best_accuraccy], 'state': [best_start_state], 'degree': [degree]}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lowest_errors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Degree 3 gave us a good error, and it is quite generalized. we will use it. Its error is also much better then the exponential option\nAssuming that the death percentage out of the confirmed cases is pretty stable, the same model should work just fine for the fatalities as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"possible_starts[possible_starts['Province/State'] == 'Massachusetts']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([possible_starts[possible_starts['Province/State'] == 'Massachusetts'], train])[['ConfirmedCases', 'Fatalities']]\ntrain = train.loc[train.ConfirmedCases>0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"days_to_predict = 43 # Change to 29\npublic_leader_board_first_column=7 # Change to 26\nmodel = make_pipeline(PolynomialFeatures(3), Ridge())\n\n#ConfirmedCases predictions\nX_train = np.array(range(len(train))) + 1\nX_test = np.array(range(public_leader_board_first_column,public_leader_board_first_column+days_to_predict)) + 1\ny_train = train.ConfirmedCases.values\nmodel.fit(X_train.reshape(-1,1), y_train.reshape(-1,1))\nconfirmed_cases_predictions = model.predict(X_test.reshape(-1,1))\nconfirmed_cases_predictions = list(map(lambda x: x[0], confirmed_cases_predictions.tolist()))\n\n#Fatalities predictions\ny_train = train.Fatalities.values\nmodel.fit(X_train.reshape(-1,1), y_train.reshape(-1,1))\nfatalities_predictions = model.predict(X_test.reshape(-1,1))\nfatalities_predictions = list(map(lambda x: x[0], fatalities_predictions.tolist()))\n\nsubmissions = pd.DataFrame({'ConfirmedCases': confirmed_cases_predictions, 'Fatalities': fatalities_predictions})\nsubmissions.to_csv('submission.csv', index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}