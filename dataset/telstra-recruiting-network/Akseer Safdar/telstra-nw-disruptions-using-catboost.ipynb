{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Telstra Network Disruptions Challenge"},{"metadata":{},"cell_type":"markdown","source":"**In this challenge we are required to predict the fault severity i.e, if the fault is a normal glitch or it is critical and will result in total loss of service. As we know that Telecom Operatores strive to provide the best possible service to customers and network outages are taken very seriously as they effect user experience and also impact the revenue.**\n\n**We are given a training data where the fault_severity is already provided (labels). And we are required to predict the fault severity (ranging from 0 to 2) for the test data. Fault severity of 0 means no fault and 2 means critical (total loss of service)**"},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing/ Cleaning"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# making the initial imports\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#print the directory items\nprint(os.listdir('../input/telstra-recruiting-network'))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#reading the files\n\ntrain = pd.read_csv('../input/telstra-recruiting-network/train.csv')\ntest = pd.read_csv('../input/telstra-recruiting-network/test.csv')\nseverity_type = pd.read_csv('../input/telstra-recruiting-network/severity_type.csv', error_bad_lines= False, warn_bad_lines= False)\nresource_type = pd.read_csv('../input/telstra-recruiting-network/resource_type.csv', error_bad_lines= False, warn_bad_lines= False)\nlog_failure = pd.read_csv('../input/telstra-recruiting-network/log_feature.csv', error_bad_lines= False, warn_bad_lines= False)\nevent_type = pd.read_csv('../input/telstra-recruiting-network/event_type.csv', error_bad_lines=False, warn_bad_lines= False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Warn bad lines** has been used to avoid the un-necessary warnings generated by pandas. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#printing the shape of all given files\n\nprint('The shape of test set is: {}\\n'.format(test.shape))\nprint('The shape of train set is: {}\\n'.format(train.shape))\nprint('The shape of severity_type is: {}\\n'.format(severity_type.shape))\nprint('The shape of resource_type is: {}\\n'.format(resource_type.shape))\nprint('The shape of log_failure is: {}\\n'.format(log_failure.shape))\nprint('The shape of event_type is: {}'.format(event_type.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#id column in event_types is an object\n\nevent_type.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert the id column to numeric data type\n\nevent_type['id']=pd.to_numeric(event_type['id'],errors='coerce')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the shape of training set\n\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the head of training file before merging it with other files\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merging the data sets to have all the available info\n\ntrain_1 = train.merge(severity_type, how = 'left', left_on='id', right_on='id')\ntrain_2 = train_1.merge(resource_type, how = 'left', left_on='id', right_on='id')\ntrain_3 = train_2.merge(log_failure, how = 'left', left_on='id', right_on='id')\ntrain_4 = train_3.merge(event_type, how = 'left', left_on='id', right_on='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the head after merging\n\ntrain_4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the nulls in each column\n\ntrain_4.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#do the head method on training file.\ntrain_4.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the one id is being repeated multiple times resulting in high number of records in training file. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping the duplicate records\n\ntrain_4.drop_duplicates(subset= 'id', keep= 'first', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the shape of training file after dropping duplicate records\n\ntrain_4.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Duplicates have been removed. "},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#count plot for fault severity\n\nplt.figure(figsize = (8,6))\nsns.countplot(train_4['fault_severity'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not very balanced data set as the number of 0 values (indicating no fault) is high as compared with others."},{"metadata":{"trusted":true},"cell_type":"code","source":"#count plot for severity type\n\nplt.figure(figsize = (8,6))\nsns.countplot(train_4['severity_type'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"type 1 and type 2 and more frequent than others. These are just coded values we don't actually know what these types imply. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#count plot for resource type\n\nplt.figure(figsize = (14,6))\nsns.countplot(train_4['resource_type'])\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the records are with the type 2 and type 8. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting the correlation matrix\n\nplt.figure(figsize = (8,6))\nsns.heatmap(train_4.corr(), vmax = 0.8, linewidths= 0.01, square= True, \n           annot= True, cmap= 'viridis', linecolor= 'white')\n\nplt.title('Correlation Matrix', fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot only shows the correlation between the numeric columns. To see the complete correlation matrix we would have to convert the categorical columns to ints using label encoding etc and then visualize the correlation matrix again. "},{"metadata":{},"cell_type":"markdown","source":"# Catboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the catboost and train test split\n\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting into X and y (training data and training labels)\n\nX = train_4[['id', 'location', 'severity_type', 'resource_type',\n       'log_feature', 'volume', 'event_type']]\ny = train_4.fault_severity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#divide the training set into train/validation set with 20% set aside for validation. \n\nfrom sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.20, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#as we know that we can give categorical features to catboost to make best use of its performance. \n\n\ncategorical_features_indices = np.where(X_train.dtypes == object)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using pool to make the training and validation sets\n\ntrain_dataset = Pool(data=X_train,\n                     label=y_train,\n                     cat_features=categorical_features_indices)\n\neval_dataset = Pool(data=X_validation,\n                    label=y_validation,\n                    cat_features=categorical_features_indices)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize CatBoostClassifier\n\nmodel = CatBoostClassifier(iterations=1000,\n                           learning_rate=1,\n                           depth=2,\n                           loss_function='MultiClass',\n                           random_seed=1,\n                           bagging_temperature=22,\n                           od_type='Iter',\n                           metric_period=100,\n                           od_wait=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit model\n\nmodel.fit(train_dataset, eval_set= eval_dataset, plot= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the model was getting overfit after initial iterations so it was stopped by overfitting detector in catboost."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get predicted classes\n\npreds_class = model.predict(eval_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get predicted probabilities for each class\n\npreds_proba = model.predict_proba(eval_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we are getting the probabilities in this format.\n\npreds_proba","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting the Test Set Ready to feed into the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the head\n\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the shape of test set before merging with other files. \n\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merging the data sets to combine all the needed info\n\ntest_1 = test.merge(severity_type, how = 'left', left_on='id', right_on='id')\ntest_2 = test_1.merge(resource_type, how = 'left', left_on='id', right_on='id')\ntest_3 = test_2.merge(log_failure, how = 'left', left_on='id', right_on='id')\ntest_4 = test_3.merge(event_type, how = 'left', left_on='id', right_on='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checkingk the head 20 records\n\ntest_4.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that there are many duplicates in the test set as a result of merging with other files. Lets get rid of these duplicate records. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing the duplicates.\n\ntest_4.drop_duplicates(subset= 'id', keep= 'first', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checkingk the shape of test set again\n\ntest_4.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking for any null values. \n\ntest_4.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making Predictions on Test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#making predictions on test set\n\npredict_test=model.predict_proba(test_4)\npred_df=pd.DataFrame(predict_test,columns=['predict_0', 'predict_1', 'predict_2'])\nsubmission_cat=pd.concat([test[['id']],pred_df],axis=1)\nsubmission_cat.to_csv('sub_cat_1.csv',index=False,header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#having a look at the submission file\n\nsubmission_cat.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**These are the predicted probabilites. The column with the highest value is the predicted class of severity.**\n**We can use numpy.argmax to get the predicted classes**\n\n**But for this challenge the submission file is already in the desired format (as they will calculate the logloss against our given probabilities).**"},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{},"cell_type":"markdown","source":"Now we will try random forest as usually it is a good algorithm for classification tasks. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#making the imports\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the head of training file\n\ntrain_4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#initialize the label encoder\n\nlb = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#apply the label encoder to all the categorical columns\n\ntrain_4['location'] = lb.fit_transform(train_4['location'])\ntrain_4['severity_type'] = lb.fit_transform(train_4['severity_type'])\ntrain_4['resource_type'] = lb.fit_transform(train_4['resource_type'])\ntrain_4['log_feature'] = lb.fit_transform(train_4['log_feature'])\ntrain_4['event_type'] = lb.fit_transform(train_4['event_type'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the head of encoded training set\n\ntrain_4.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#divide the data into X and y\n\ny = train_4['fault_severity']\nX = train_4.drop('fault_severity', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making training and test sets with 75% and 25% ratio\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#instantiate and train the model\n\nrfc = RandomForestClassifier()\nrfc.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making predictions on test set\n\npred = rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#confusion matrix and classification report\n\nprint('Confusion matrix \\n')\nprint(metrics.confusion_matrix(y_test,pred))\nprint('*'*80)\nprint('\\n')\nprint('Classification report \\n')\nprint(metrics.classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mode performs better for class 0 and not so much for other classes. As the dataset was imbalanced (more training samples for class 0) so the model is biased towards predicting the class 0."},{"metadata":{},"cell_type":"markdown","source":"# Grid Search CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"#making the needed imports\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#divide the data into 5 folds\n\nfolds = KFold(n_splits= 5, shuffle= True, random_state= 101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#give the range for parameters to check for grid search\n\nparams = {'max_depth':[3,5,7,9],\n         'n_estimators':[500,800,1100,1400],\n         'min_samples_leaf': [150, 200, 250, 300], \n         'min_samples_split': [300, 350, 400, 450]}\n\nrf = RandomForestClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define the model with gridsearch parameters\n\nrf_fin = GridSearchCV(estimator= rf, cv = folds, param_grid= params, scoring= 'accuracy', return_train_score= True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#traint the model (will take some time)\n\nrf_fin.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting the results in a dataframe\n\nscores = rf_fin.cv_results_\n\nscores = pd.DataFrame(scores)\n\nscores.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting the best score\n\nprint('The best score was achieved using the parameters: {}'.format(rf_fin.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#so now we will use the above parameters to build the model\n\nrandom_final = RandomForestClassifier(max_depth= 3,\n                                      min_samples_leaf= 150,\n                                      min_samples_split= 300,\n                                      n_estimators= 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit the model and get the predictions\n\nrandom_final.fit(X_train,y_train)\n\npred_fin = random_final.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#printing the confusion matrix and classification reports\n\nprint('Confusion matrix \\n')\nprint(metrics.confusion_matrix(y_test,pred_fin))\nprint('*'*80)\nprint('\\n')\nprint('Classification report \\n')\nprint(metrics.classification_report(y_test,pred_fin))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The tuned model is only good at predicting calss '0'. On other classes it is performing terribly.\nSo we will just stick to our default random forest model. "},{"metadata":{},"cell_type":"markdown","source":"# Label Encoding the test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the orignal form of test_4 (without label encoding the data)\n\ntest_4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#applying the label encode to categorical columns\n\ntest_4['location'] = lb.fit_transform(test_4['location'])\ntest_4['severity_type'] = lb.fit_transform(test_4['severity_type'])\ntest_4['resource_type'] = lb.fit_transform(test_4['resource_type'])\ntest_4['log_feature'] = lb.fit_transform(test_4['log_feature'])\ntest_4['event_type'] = lb.fit_transform(test_4['event_type'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#label encoded test set\n\ntest_4.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making predictions using Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will use the predict_proba as this is needed format for kaggle submission. \n\npred_fin = rfc.predict_proba(test_4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making the submission file ready\n\npred_df=pd.DataFrame(pred_fin,columns=['predict_0', 'predict_1', 'predict_2'])\nsubmission_rf=pd.concat([test[['id']],pred_df],axis=1)\nsubmission_rf.to_csv('sub_random_forest.csv',index=False,header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the submission file. \n\nsubmission_rf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\n**As a conclusion we can say that it is a good dataset for practice and you can try different approaches.**\n\n**We did not try Support Vector Machines (SVM) classifier. Usually it can also perform good on classification tasks**.\n\n**Happy Machine Learning.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}