{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Solução do Desafio *Gendered Pronoun Resolution* do Kaggle, como avaliação parcial da disciplina Aprendizagem de Máquina**                                                                                                      "},{"metadata":{},"cell_type":"markdown","source":"Equipe: Delmiro Daladier Sampaio Neto, Delano Hélio Oliveira, Jéssica Feliciano Coutinho                             "},{"metadata":{},"cell_type":"markdown","source":"Link da descrição do problema no Kaggle: https://www.kaggle.com/c/gendered-pronoun-resolution"},{"metadata":{},"cell_type":"markdown","source":"**Bibliotecas utilizadas**"},{"metadata":{"id":"RS-qcHXgf96P","colab_type":"code","outputId":"052cb2e1-e176-4a22-b600-ddcbd3cad4c2","colab":{"base_uri":"https://localhost:8080/","height":17},"trusted":true},"cell_type":"code","source":"import os\nimport csv\nimport json\nimport string\nimport keras\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nfrom math import floor\nimport spacy\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.naive_bayes import MultinomialNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import accuracy_score, log_loss\n\nimport lightgbm as lgb\n\nimport time\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, Concatenate, Add, Flatten, CuDNNLSTM\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.engine.topology import Layer\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import defaultdict\n\n\n# keras libraries\nfrom keras.models import Model, load_model,Sequential\nfrom keras.layers import Dense, Input, Dropout,Bidirectional, GRU, Activation, concatenate, Embedding, SpatialDropout1D\nfrom keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D ,GlobalMaxPool1D, GlobalAvgPool1D, GlobalMaxPooling2D, GlobalAveragePooling2D\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence, text\nfrom keras import layers\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Inclusão das bases de dados**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/gapdevelopment/repository/google-research-datasets-gap-coreference-83135f2/gap-development.tsv',delimiter='\\t',encoding='utf-8')\ntest = pd.read_csv('../input/gapdevelopment/repository/google-research-datasets-gap-coreference-83135f2/gap-test.tsv',delimiter='\\t',encoding='utf-8');\nvalidation = pd.read_csv('../input/gapdevelopment/repository/google-research-datasets-gap-coreference-83135f2/gap-validation.tsv',delimiter='\\t',encoding='utf-8');\n\nprint(train.shape)\nprint(test.shape)\nprint(validation.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Exibição das bases de dados:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alguns ajustes dos atributos"},{"metadata":{},"cell_type":"markdown","source":"**GERANDO O DATAFRAME DE TREINAMENTO**"},{"metadata":{"trusted":true},"cell_type":"code","source":"true_B = train.loc[train['B-coref']== True ]\ntrue_B.drop('A',axis=1,inplace=True)\ntrue_B.drop('A-offset',axis=1,inplace=True)\ntrue_B.drop('A-coref',axis=1,inplace=True)\ntrue_B.rename(columns={'B-offset':'A-offset','B':'A','B-coref':'A-coref'},inplace=True)\ntrue_B.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)\n\ntrue_A = train.loc[train['A-coref']== True ]\ntrue_A.drop('B',axis=1,inplace=True)\ntrue_A.drop('B-offset',axis=1,inplace=True)\ntrue_A.drop('B-coref',axis=1,inplace=True)\ntrue_A.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)\n\nfalse_A = train.loc[train['A-coref']== False ]\nfalse_A.drop('B',axis=1,inplace=True)\nfalse_A.drop('B-offset',axis=1,inplace=True)\nfalse_A.drop('B-coref',axis=1,inplace=True)\nfalse_A.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)\n\nfalse_B = train.loc[train['B-coref']== False ]\nfalse_B.drop('A',axis=1,inplace=True)\nfalse_B.drop('A-offset',axis=1,inplace=True)\nfalse_B.drop('A-coref',axis=1,inplace=True)\nfalse_B.rename(columns={'B-offset':'A-offset','B':'A','B-coref':'A-coref'},inplace=True)\nfalse_B.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frames = [true_A,false_A,true_B,false_B]\nnew_train=pd.concat(frames)\nnew_train.loc[new_train['A-offset']== float('nan')]\nnew_train.dropna(how='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**GERANDO O DATAFRAME DE TESTE**"},{"metadata":{"trusted":true},"cell_type":"code","source":"true_test_B = test.loc[test['B-coref']== True ]\ntrue_test_B.drop('A',axis=1,inplace=True)\ntrue_test_B.drop('A-offset',axis=1,inplace=True)\ntrue_test_B.drop('A-coref',axis=1,inplace=True)\ntrue_test_B.rename(columns={'B-offset':'A-offset','B':'A','B-coref':'A-coref'},inplace=True)\ntrue_test_B.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)\n\ntrue_test_A = test.loc[test['A-coref']== True ]\ntrue_test_A.drop('B',axis=1,inplace=True)\ntrue_test_A.drop('B-offset',axis=1,inplace=True)\ntrue_test_A.drop('B-coref',axis=1,inplace=True)\ntrue_test_A.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)\n\nfalse_test_A = test.loc[test['A-coref']== False ]\nfalse_test_A.drop('B',axis=1,inplace=True)\nfalse_test_A.drop('B-offset',axis=1,inplace=True)\nfalse_test_A.drop('B-coref',axis=1,inplace=True)\nfalse_test_A.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)\n\nfalse_test_B = test.loc[test['B-coref']== False ]\nfalse_test_B.drop('A',axis=1,inplace=True)\nfalse_test_B.drop('A-offset',axis=1,inplace=True)\nfalse_test_B.drop('A-coref',axis=1,inplace=True)\nfalse_test_B.rename(columns={'B-offset':'A-offset','B':'A','B-coref':'A-coref'},inplace=True)\nfalse_test_B.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frames = [true_test_A,false_test_A,true_test_B,false_test_B]\nnew_test=pd.concat(frames)\nnew_test.loc[new_test['A-offset']== float('nan')]\nnew_test.dropna(how='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**GERANDO DATAFRAME DE VALIDAÇÃO**"},{"metadata":{"trusted":true},"cell_type":"code","source":"true_validation_B = validation.loc[validation['B-coref']== True ]\ntrue_validation_B.drop('A',axis=1,inplace=True)\ntrue_validation_B.drop('A-offset',axis=1,inplace=True)\ntrue_validation_B.drop('A-coref',axis=1,inplace=True)\ntrue_validation_B.rename(columns={'B-offset':'A-offset','B':'A','B-coref':'A-coref'},inplace=True)\ntrue_validation_B.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)\n\ntrue_validation_A = validation.loc[validation['A-coref']== True ]\ntrue_validation_A.drop('B',axis=1,inplace=True)\ntrue_validation_A.drop('B-offset',axis=1,inplace=True)\ntrue_validation_A.drop('B-coref',axis=1,inplace=True)\ntrue_validation_A.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)\n\nfalse_validation_A = validation.loc[validation['A-coref']== False ]\nfalse_validation_A.drop('B',axis=1,inplace=True)\nfalse_validation_A.drop('B-offset',axis=1,inplace=True)\nfalse_validation_A.drop('B-coref',axis=1,inplace=True)\nfalse_validation_A.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)\n\nfalse_validation_B = validation.loc[validation['B-coref']== False ]\nfalse_validation_B.drop('A',axis=1,inplace=True)\nfalse_validation_B.drop('A-offset',axis=1,inplace=True)\nfalse_validation_B.drop('A-coref',axis=1,inplace=True)\nfalse_validation_B.rename(columns={'B-offset':'A-offset','B':'A','B-coref':'A-coref'},inplace=True)\nfalse_validation_B.sort_values(by=['ID','Text','Pronoun','Pronoun-offset','A','URL'], axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frames = [true_validation_A,false_validation_A,true_validation_B,false_validation_B]\nnew_validation=pd.concat(frames)\nnew_validation.loc[new_validation['A-offset']== float('nan')]\nnew_validation.dropna(how='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PRIMEIRA PARTE - Tokenização e Vetorização dos textos**\n\nForam implementadas têrs métodos para criar as features:\n\n1 - Utilizando o texto, pronome e a palavra A \n\n2 - Utilizando o texto antes da palavra A e o texto antes do pronome \n\n3 - Apenas o texto\n\nTodas apresentaram resultados semelhantes."},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_data(data, label = None, test=False):    \n\n    \n    text = []\n    A = []\n    Pronoun = []\n    distances = []\n    for row in data[['Text','A','Pronoun','A-offset','Pronoun-offset']].values:\n        text.append(row[0])\n        A.append(row[1])\n        Pronoun.append(row[2])\n        distances.append(row[4]-row[3])\n        \n    tokenizer = Tokenizer(num_words=max_features, split=' ')\n    tokenizer.fit_on_texts(text)\n    texts = tokenizer.texts_to_sequences(text)  \n    texts = pad_sequences(texts, maxlen=maxlen)\n    word_index = tokenizer.word_index\n    prev_text = []\n    \n    tokenizer = Tokenizer(num_words=max_features, split=' ')\n    A_terms = tokenizer.texts_to_sequences(A)  \n    A_terms = pad_sequences(A_terms, maxlen=100)\n     \n    tokenizer = Tokenizer(num_words=max_features, split=' ')\n    \n    pronouns = tokenizer.texts_to_sequences(Pronoun)  \n    pronouns = pad_sequences(A_terms, maxlen=100)\n    \n    X = []\n    \n    i = 0 \n    data_examples = len(data)\n    for i in range(0,data_examples):\n        aux = np.append(texts[i],A_terms[i])\n        aux = np.append(aux,pronouns[i])\n        X.append(np.append(aux,distances[i]))\n\n    X = np.asarray(X)\n    print(X.shape)\n    #Y = pd.get_dummies(data[label]).values\n    \n    Y = pd.get_dummies(data[\"A-coref\"].values)\n    if test == True:\n        return X, word_index, tokenizer\n    else:\n        return X, Y, word_index, tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_previous_text(data,test=False):\n    \n    text_before_A = []\n    text_before_Pronoun = []\n    for row in data[['Text','A-offset','Pronoun-offset']].values:\n        text_before_A.append(row[0][:row[1]-1])\n        text_before_Pronoun.append(row[0][:row[2]-1])\n        \n    tokenizer = Tokenizer(num_words=max_features, split=' ')\n    tokenizer.fit_on_texts(text_before_A)\n    text_A = tokenizer.texts_to_sequences(text_before_A)  \n    texts_A = pad_sequences(text_A, maxlen=maxlen)\n    word_index = tokenizer.word_index\n    \n    tokenizer = Tokenizer(num_words=max_features, split=' ')\n    tokenizer.fit_on_texts(text_before_Pronoun)\n    text_pronoun = tokenizer.texts_to_sequences(text_before_Pronoun)  \n    text_pronoun = pad_sequences(text_pronoun, maxlen=maxlen)\n    word_index = tokenizer.word_index\n    \n    X = []\n    \n    i = 0 \n    data_examples = len(data)\n    for i in range(0,data_examples):\n        aux = np.append(text_pronoun[i],texts_A[i])\n        X.append(aux)\n\n    X = np.asarray(X)\n    #Y = pd.get_dummies(data[label]).values\n    \n    Y = pd.get_dummies(data[\"A-coref\"].values)\n    if test == True:\n        return X, word_index, tokenizer\n    else:\n        return X, Y, word_index, tokenizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tokenização e vetorização utilizada: utilizando todo texto e considerando como função de vetorização a função *pad_sequences* do Keras**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parâmetros de Vetorização\nmaxlen = 220\nembed_size = 500\nmax_features = 7000\n\n# Aplicação nos dados de treinamento\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer_list = list(train.Text.values)\ntokenizer.fit_on_texts(tokenizer_list)\n\ntrain_X = tokenizer.texts_to_sequences(train.Text.values)\ntrain_auX = tokenizer.texts_to_sequences(train.Text.values)\n\ntrain_X = pad_sequences(train_X, maxlen=maxlen)\ntrain_auX = pad_sequences(train_auX, maxlen=maxlen)\n\ny_train = pd.get_dummies(train[\"A-coref\"].values)\nword_index = tokenizer.word_index\nmax_features = len(word_index)\n\n# Aplicação nos dados de validação\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer_list = list(validation.Text.values)\ntokenizer.fit_on_texts(tokenizer_list)\n\nvalidation_X = tokenizer.texts_to_sequences(validation.Text.values)\nvalidation_auX = tokenizer.texts_to_sequences(validation.Text.values)\n\nvalidation_X = pad_sequences(validation_X, maxlen=maxlen)\nvalidation_auX = pad_sequences(validation_auX, maxlen=maxlen)\n\ny_validation = pd.get_dummies(validation[\"A-coref\"].values)\nword_index = tokenizer.word_index\n\n\n# Aplicação nos dados de teste\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer_list = list(test.Text.values)\ntokenizer.fit_on_texts(tokenizer_list)\n\ntest_X = tokenizer.texts_to_sequences(test.Text.values)\ntest_auX = tokenizer.texts_to_sequences(test.Text.values)\n\ntest_X = pad_sequences(test_X, maxlen=maxlen)\ntest_auX = pad_sequences(test_auX, maxlen=maxlen)\n\ny_test = pd.get_dummies(test[\"A-coref\"].values)\nword_index = tokenizer.word_index\n\n\nprint(train_X.shape)\nprint(y_train.shape)\nprint(test_X.shape)\nprint(y_test.shape)\n\n#y[1] --> True = 1 e False = 0 ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SEGUNDA PARTE - DEFINIÇÃO DOS ALGORITMOS DE CLASSIFICAÇÃO**\n\n**Inicialmente foram considerados 4 classificadores simples e usualmente aplicados para classificação de texto, definidos nas funções a seguir**\n\nOs classificadores considerados foram:\n\n* SVM \n* Naive Bayes\n* Randon Forest\n* Regressão Linear"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model_SVM(X_train, X_teste, Y_train):\n    model = SVC()\n    model.fit(X_train,Y_train)\n    return model.predict(X_teste)\n\ndef get_model_Bayes(X_train, X_teste, Y_train):\n    model = MultinomialNB()\n    model.fit(X_train,Y_train)\n    return model.predict(X_teste)\n\ndef get_model_forest(X_train,X_teste, Y_train):\n    model = RandomForestClassifier()\n    model.fit(X_train,Y_train)\n    return model.predict(X_teste)\n\ndef get_model_Regression(X_train,X_teste, Y_train):\n    model = LogisticRegression()\n    model.fit(X_train, Y_train)\n    return model.predict(X_teste)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Resultados obtidos para os classificadores simples:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Loss SVM:\", log_loss(y_test[1], get_model_SVM(train_X,test_X,y_train[1])))\nprint(\"Loss Bayes:\", log_loss(y_test[1],get_model_Bayes(train_X,test_X,y_train[1])))\nprint(\"Loss Random Forest:\", log_loss(y_test[1],get_model_forest(train_X,test_X,y_train[1])))\nprint(\"Loss Regressão Linear:\", log_loss(y_test[1],get_model_Regression(train_X,test_X,y_train[1])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Diante das baixas acurácias obtidas nos 4 modelos acima, decidiu-se então implementar uma rede neural artificial recursiva para obtenção de melhores resultados. \n\nUtilizou-se o modelo LSTM (*Long short-term memory*) do Keras junto com o mecanismo Attention para melhorar a compreensão para sequencias mais longas.\n\nUma descrição simplificada do mecanismo pode ser encontrada no post: https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f \n\nArtigo original: https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf \n\nExemplo de uso de attention com outra RNN: https://www.kaggle.com/keyit92/end2end-coref-resolution-by-attention-rnn"},{"metadata":{"id":"mKyYwJgxpfHj","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def dot_product(x, kernel):\n\n    if K.backend() == 'tensorflow':\n        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n    else:\n        return K.dot(x, kernel)\n\n      \nclass AttentionWithContext(Layer):\n\n    def __init__(self,\n                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n                 W_constraint=None, u_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n\n\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.u_regularizer = regularizers.get(u_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.u_constraint = constraints.get(u_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(AttentionWithContext, self).__init__(**kwargs)\n\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight((input_shape[-1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n\n        self.u = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_u'.format(self.name),\n                                 regularizer=self.u_regularizer,\n                                 constraint=self.u_constraint)\n\n        super(AttentionWithContext, self).build(input_shape)\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        uit = dot_product(x, self.W)\n\n        if self.bias:\n            uit += self.b\n\n        uit = K.tanh(uit)\n        ait = dot_product(uit, self.u)\n\n        a = K.exp(ait)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n\n        # in some cases especially in the early stages of training the sum may be almost zero\n        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], input_shape[-1]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A configuração da rede foi realizada conforme as definições da função *get_model_LSTM*, definida a seguir."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model_LSTM():\n  inp1 = Input(shape=(maxlen,))\n  inp2 = Input(shape=(maxlen,))\n\n  model1_out = Embedding(max_features, embed_size)(inp1)\n  model1_out = Bidirectional(LSTM(256, return_sequences=True))(model1_out)\n  model1_out = AttentionWithContext()(model1_out)\n  model1_out = Dropout(0.1)(model1_out)\n\n  model2_out = Embedding(max_features, embed_size)(inp2)\n  model2_out = Bidirectional(LSTM(256, return_sequences=True))(model2_out)\n  model2_out = AttentionWithContext()(model2_out)\n  model2_out = Dropout(0.1)(model2_out)\n\n  merged_out = keras.layers.Concatenate(axis=1)([model1_out, model2_out])\n\n  merged_out = Dense(32, activation=\"relu\")(merged_out)\n  merged_out = Dropout(0.1)(merged_out)\n  \n  merged_out = Dense(2, activation=\"sigmoid\")(merged_out)\n  model = Model(inputs=[inp1,inp2], outputs=merged_out)\n  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n  \n  return model","execution_count":null,"outputs":[]},{"metadata":{"id":"Xfz7MVMytVwE","colab_type":"code","outputId":"16c4e2b5-926e-465c-cc8b-fb99f58b665c","colab":{"base_uri":"https://localhost:8080/","height":683},"trusted":true},"cell_type":"code","source":"model = get_model_LSTM()\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Resultados obtidos com a aplicação da LSTM. **\n\nObserva-se acurácias superiores para os dados de treinamento do dataset *gap-development*"},{"metadata":{"id":"3vlc3awwp5vJ","colab_type":"code","outputId":"05b98362-aa58-4c2a-ef39-14513cd0b2ab","colab":{"base_uri":"https://localhost:8080/","height":1153},"trusted":true},"cell_type":"code","source":"model.fit([train_X, train_auX], y_train, batch_size=512, epochs=30, validation_data=([validation_X,validation_auX],y_validation))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Teste do modelo**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Perdas do modelo\nmodel.evaluate([test_X, test_auX],y_test,verbose=1,batch_size=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CONCLUSÕES:**\n\nDiante dos resultados obtidos nos classificadores acima, verifica-se que a aplicação da rede neural recursiva foi o melhor método adotado para o problema de classificação dos pronomes proposto para este desafio, atingindo valores de acurácia superiores a 70%.\n"}],"metadata":{"colab":{"name":"Gendered Pronoun Resolution: Version-10.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":1}