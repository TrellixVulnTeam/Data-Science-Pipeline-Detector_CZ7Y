{"cells":[{"metadata":{"_uuid":"eef73f480afe9a4b485f89361916b7469ed986ea"},"cell_type":"markdown","source":"### 下载数据集"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-development.tsv -q\n!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-test.tsv -q\n!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-validation.tsv -q","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 安装包"},{"metadata":{"_uuid":"98b461a0d1e2d27558502f9caefeaf7e47871efc"},"cell_type":"markdown","source":"“pytorch_helper_bot”是封装 pytorch 训练过程的一个包。"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install pytorch-pretrained-bert\n!pip install https://github.com/ceshine/pytorch_helper_bot/archive/0.0.4.zip","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 导入包"},{"metadata":{"_uuid":"4e128e4337fd5c906540c112bc1d4e0fd2f38ef3","trusted":true},"cell_type":"code","source":"import os\n\n# This variable is used by helperbot to make the training deterministic\nos.environ[\"SEED\"] = \"420\"\n\nimport logging\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel\n\nfrom helperbot import BaseBot, TriangularLR, WeightDecayOptimizerWrapper","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 数据处理"},{"metadata":{"_uuid":"6d3c64ff3a19456ee88ef77825b83690e5907475","trusted":true},"cell_type":"code","source":"# 根据代词和候选指代A、B的偏移量，插入相应的标记，便于在tokenization后定位\ndef insert_tag(row):\n    \"\"\"Insert custom tags to help us find the position of A, B, and the pronoun after tokenization.\"\"\"\n    # 指代A, B和代词的偏移量降序排序\n    to_be_inserted = sorted([\n        (row[\"A-offset\"], \" [A] \"),\n        (row[\"B-offset\"], \" [B] \"),\n        (row[\"Pronoun-offset\"], \" [P] \")\n    ], key=lambda x: x[0], reverse=True)\n    text = row[\"Text\"]\n    # 插入标记\n    for offset, tag in to_be_inserted:\n        text = text[:offset] + tag + text[offset:]\n    return text\n\n# 对文本进行tokenization，根据插入的标记，取得指代A、B和代词的位置\ndef tokenize(text, tokenizer):\n    \"\"\"Returns a list of tokens and the positions of A, B, and the pronoun.\"\"\"\n    entries = {}       # 根据标记定位token化后的A，B，pronoun的位置 \n    final_tokens = []  # token化后的词\n    for token in tokenizer.tokenize(text):\n        if token in (\"[A]\", \"[B]\", \"[P]\"):\n            entries[token] = len(final_tokens)     \n            continue\n        final_tokens.append(token)\n    return final_tokens, (entries[\"[A]\"], entries[\"[B]\"], entries[\"[P]\"])\n\n# 自定义pytorch dataset类，用于读取数据\nclass GAPDataset(Dataset):\n    \"\"\"Custom GAP Dataset class\"\"\"\n    def __init__(self, df, tokenizer, labeled=True):\n        self.labeled = labeled\n        # 设置label。\"A-coref\"：代词是否指代A。B同理。 \"Neither\"：既不指代A，也不指代B\n        if labeled:   \n            tmp = df[[\"A-coref\", \"B-coref\"]].copy()\n            tmp[\"Neither\"] = ~(df[\"A-coref\"] | df[\"B-coref\"])\n            self.y = tmp.values.astype(\"bool\")\n\n        # 提取tokens和A，B，P的偏移量\n        self.offsets, self.tokens = [], []\n        for _, row in df.iterrows():\n            text = insert_tag(row)       # 插入A、B、P标记，返回插入标记的文本\n            tokens, offsets = tokenize(text, tokenizer)\n            self.offsets.append(offsets)\n            self.tokens.append(tokenizer.convert_tokens_to_ids(\n                [\"[CLS]\"] + tokens + [\"[SEP]\"]))   # BERT输入格式，句首加入“[CLS]\",句尾加入”[SEP]\"\n    \n    # 获取数据集大小\n    def __len__(self):\n        return len(self.tokens)\n    \n    # 取数据函数\n    def __getitem__(self, idx):\n        if self.labeled:\n            return self.tokens[idx], self.offsets[idx], self.y[idx]\n        return self.tokens[idx], self.offsets[idx]\n    \n# 将一个batch的数据转换成tensor。将[(tokens,offsets,labels),...]转换成tokens tensor, offsets tensor, label tensor\ndef collate_examples(batch, truncate_len=500):\n    \"\"\"Batch preparation.\n    \n    1. Pad the sequences\n    2. Transform the target.\n    \"\"\"\n    # [(tokens1, offsets1), (tokens2, offsets2)] => [(tokens1, tokens2), (offsets1, offsets2)] \n    transposed = list(zip(*batch)) \n    # 输入序列的最大长度\n    max_len = min(\n        max((len(x) for x in transposed[0])),\n        truncate_len\n    )\n    \n    # tokens转成tensor\n    tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n    for i, row in enumerate(transposed[0]):\n        row = np.array(row[:truncate_len])    # tokens超过长度，截断\n        tokens[i, :len(row)] = row            \n    token_tensor = torch.from_numpy(tokens)\n    \n    # Offsets转换成tensor\n    offsets = torch.stack([\n        torch.LongTensor(x) for x in transposed[1]\n    ], dim=0) + 1 # Account for the [CLS] token\n    \n    # 将长度为3的one-hot label，转换为数字label\n    if len(transposed) == 2:\n        return token_tensor, offsets, None\n    one_hot_labels = torch.stack([\n        torch.from_numpy(x.astype(\"uint8\")) for x in transposed[2]\n    ], dim=0)\n    _, labels = one_hot_labels.max(dim=1)\n    \n    return token_tensor, offsets, labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 网络模型"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 多层感知机网络\nclass Head(nn.Module):\n    \"\"\"The MLP submodule\"\"\"\n    def __init__(self, bert_hidden_size: int):\n        super().__init__()\n        self.head_hidden_size = 1024  # MLP隐层大小\n        self.bert_hidden_size = bert_hidden_size   # Bert的隐层大小\n        self.fc = nn.Sequential(\n            nn.BatchNorm1d(bert_hidden_size * 3),  # 批标准化，*3是因为将A，B，P的bert_output展开成1维了\n            nn.Dropout(0.5),                       # 随机失活\n            nn.Linear(bert_hidden_size * 3, self.head_hidden_size), # 线性层\n            nn.ReLU(),                                              # 激活函数\n            nn.BatchNorm1d(self.head_hidden_size),\n            nn.Dropout(0.5),\n            nn.Linear(self.head_hidden_size, self.head_hidden_size),\n            nn.ReLU(),\n            nn.BatchNorm1d(self.head_hidden_size),\n            nn.Dropout(0.5),\n            nn.Linear(self.head_hidden_size, self.head_hidden_size),\n            nn.ReLU(),\n            nn.BatchNorm1d(self.head_hidden_size),\n            nn.Dropout(0.5),\n            nn.Linear(self.head_hidden_size, 3)\n        )\n        \n        # 参数初始化，不同网络块初始化方法不一样\n        for i, module in enumerate(self.fc):\n            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                nn.init.constant_(module.weight, 1)\n                nn.init.constant_(module.bias, 0)\n                print(\"Initing batchnorm\")\n            elif isinstance(module, nn.Linear):\n                if getattr(module, \"weight_v\", None) is not None:\n                    nn.init.uniform_(module.weight_g, 0, 1)\n                    nn.init.kaiming_normal_(module.weight_v)\n                    print(\"Initing linear with weight normalization\")\n                    assert model[i].weight_g is not None\n                else:\n                    nn.init.kaiming_normal_(module.weight)\n                    print(\"Initing linear\")\n                nn.init.constant_(module.bias, 0)\n    \n    # 前向传播函数\n    def forward(self, bert_outputs, offsets):\n        # bert_outputs:[batch_size, seq_length, hidden_szie]\n        assert bert_outputs.size(2) == self.bert_hidden_size   \n        \n        # 取出A，B，P的offsets处的embedding\n        # unsqueeze(2):将2维offsets拓展为3维\n        # 扩展某个size为1的维度。如(2,2,1)扩展为(2,2,3)\n        # input.gather(dim,index), 对指定维进行索引。比如4*3的张量，对dim=1进行索引，那么index的取值范围就是0~2.\n        extracted_outputs = bert_outputs.gather(\n            1, offsets.unsqueeze(2).expand(-1, -1, bert_outputs.size(2)) \n        ).view(bert_outputs.size(0), -1)      \n        return self.fc(extracted_outputs)\n\n# 指代消解模型\nclass GAPModel(nn.Module):\n    \"\"\"The main model.\"\"\"\n    def __init__(self, bert_model: str, device: torch.device):\n        super().__init__()\n        self.device = device  # 设备：cpu 或 gpu\n        if bert_model in (\"bert-base-uncased\", \"bert-base-cased\"):\n            self.bert_hidden_size = 768\n        elif bert_model in (\"bert-large-uncased\", \"bert-large-cased\"):\n            self.bert_hidden_size = 1024\n        else:\n            raise ValueError(\"Unsupported BERT model.\")\n        self.bert = BertModel.from_pretrained(bert_model).to(device)\n        self.head = Head(self.bert_hidden_size).to(device)\n    \n    def forward(self, token_tensor, offsets):\n        token_tensor = token_tensor.to(self.device)\n        bert_outputs, _ =  self.bert(\n            token_tensor, attention_mask=(token_tensor > 0).long(), \n            token_type_ids=None, output_all_encoded_layers=False)\n        head_outputs = self.head(bert_outputs, offsets.to(self.device))\n        return head_outputs            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"理解Head类中forward函数中 取A，B,P的offsets的embedding 示例"},{"metadata":{"trusted":true},"cell_type":"code","source":"offsets = torch.tensor([[0,1,2],[1,2,3]])  # batch_size=2, len(A,B,P)= 3\nprint(offsets.shape)\noffsets = offsets.unsqueeze(2)\nprint(offsets)\nprint(offsets.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"offsets=offsets.expand(-1,-1,5)   # 假设bert_hidden_size=5\nprint(offsets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_outputs = torch.tensor([[[ 1,  2,  3,  4,  5],\n                  [ 6,  7,  8,  9, 10],\n                  [11, 12, 13, 14, 15],\n                  [16, 17, 18, 19, 20]],\n                 [[21, 22, 23, 24, 25],\n                  [26, 27, 28, 29, 30],\n                  [31, 32, 33, 34, 35],\n                  [36, 37, 38, 39, 40]]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(offsets.shape)\nprint(bert_outputs.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_outputs.gather(1,offsets) # 按offsets到bert_outputs的第1维取数，两者除了第一维外，其他维度大小一致","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 控制是否训练模型参数"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 嵌套的网络结构，module包括很多children子网络模块\ndef children(m):\n    return m if isinstance(m, (list, tuple)) else list(m.children())\n\ndef set_trainable_attr(m, b):\n    m.trainable = b\n    for p in m.parameters():\n        p.requires_grad = b\n\ndef apply_leaf(m, f):\n    c = children(m)\n    if isinstance(m, nn.Module):\n        f(m)\n    if len(c) > 0:\n        for l in c:\n            apply_leaf(l, f)\n        \ndef set_trainable(l, b):\n    apply_leaf(l, lambda m: set_trainable_attr(m, b))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 训练过程对象"},{"metadata":{},"cell_type":"markdown","source":"将训练过程封装成对象，不用手动写训练过程的代码"},{"metadata":{"trusted":true},"cell_type":"code","source":"class GAPBot(BaseBot):\n    def __init__(self, model, train_loader, val_loader, optimizer, clip_grad=0,\n        avg_window=100, log_dir=\"./cache/logs/\", log_level=logging.INFO,\n        checkpoint_dir=\"./cache/model_cache/\", batch_idx=0, echo=False,\n        device=\"cuda:0\", use_tensorboard=False):\n        super().__init__(\n            model, train_loader, val_loader, \n            optimizer=optimizer, clip_grad=clip_grad,\n            log_dir=log_dir, checkpoint_dir=checkpoint_dir, \n            batch_idx=batch_idx, echo=echo,\n            device=device, use_tensorboard=use_tensorboard\n        )\n        self.criterion = torch.nn.CrossEntropyLoss()\n        self.loss_format = \"%.6f\"\n        \n    def extract_prediction(self, tensor):\n        return tensor\n    \n    # 打印日志\n    def snapshot(self):\n        loss = self.eval(self.val_loader)\n        loss_str = self.loss_format % loss\n        self.logger.info(\"Snapshot loss %s\", loss_str)\n        self.logger.tb_scalars(\n            \"losses\", {\"val\": loss},  self.step)\n        target_path = (\n            self.checkpoint_dir / \"best.pth\")        \n        if not self.best_performers or (self.best_performers[0][0] > loss):\n            torch.save(self.model.state_dict(), target_path)\n            self.best_performers = [(loss, target_path, self.step)]\n            self.logger.info(\"Saving checkpoint %s...\", target_path)\n        else:\n            new_loss_str = self.loss_format % self.best_performers[0][0]\n            self.logger.info(\"This performance:%s is not as a good as our previously saved:%s\", loss_str,new_loss_str )\n        assert Path(target_path).exists()\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d534c85ff69192b4dd1ec670fc9c2b9392cc7a62","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"gap-test.tsv\", delimiter=\"\\t\")\ndf_val = pd.read_csv(\"gap-validation.tsv\", delimiter=\"\\t\")\ndf_test = pd.read_csv(\"../input/test_stage_2.tsv\", delimiter=\"\\t\")\nsample_sub = pd.read_csv(\"../input/sample_submission_stage_2.csv\")\nassert sample_sub.shape[0] == df_test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(df_train))\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(df_test))\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d58d18c34f5df9ec8f8d8fb048ae6c10fbf9914a","trusted":true},"cell_type":"code","source":"BERT_MODEL = 'bert-large-uncased'\nCASED = False\n\ntokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL,\n    do_lower_case=CASED,\n    never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\", \"[A]\", \"[B]\", \"[P]\")\n)\n# These tokens are not actually used, so we can assign arbitrary values.\ntokenizer.vocab[\"[A]\"] = -1\ntokenizer.vocab[\"[B]\"] = -1\ntokenizer.vocab[\"[P]\"] = -1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69689365738454b33649a14d83eea49cc1b18687","trusted":true},"cell_type":"code","source":"train_ds = GAPDataset(df_train, tokenizer)\nval_ds = GAPDataset(df_val, tokenizer)\ntest_ds = GAPDataset(df_test, tokenizer, labeled=False)\n# dataset 转换成dataloader\ntrain_loader = DataLoader(\n    train_ds,\n    collate_fn = collate_examples,     # 构成batch函数\n    batch_size=20,\n    num_workers=2,\n    pin_memory=True,   # 使用锁页内存，这样tensor转传入cuda会快些\n    shuffle=True,\n    drop_last=True     # 丢弃不完整的batch\n)\nval_loader = DataLoader(\n    val_ds,\n    collate_fn = collate_examples,\n    batch_size=128,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)\ntest_loader = DataLoader(\n    test_ds,\n    collate_fn = collate_examples,\n    batch_size=128,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_loader), len(test_loader), len(val_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"next(iter(test_loader))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a85f87ed8d73b520e39a5dc07da1838867ac2653","trusted":true},"cell_type":"code","source":"model = GAPModel(BERT_MODEL, torch.device(\"cuda:0\"))\n# You can unfreeze the last layer of bert by calling set_trainable(model.bert.encoder.layer[23], True)\nset_trainable(model.bert, False)\nset_trainable(model.head, True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"493b0ed0887339dfe818df1a0be17c05a4c97d17","trusted":true},"cell_type":"code","source":"lr=1e-3\nweight_decay=5e-3\noptimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n\nbot = GAPBot(\n    model, train_loader, val_loader,\n    optimizer=optimizer, echo=True,\n    avg_window=25\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07ea447ea766df3d997779e6c9a8300b7532a049","trusted":true},"cell_type":"code","source":"steps_per_epoch = len(train_loader) \nn_steps = steps_per_epoch * 27\nbot.train(\n    n_steps,\n    log_interval=steps_per_epoch // 4,\n    snapshot_interval=steps_per_epoch,\n    scheduler=TriangularLR(\n        optimizer, max_mul=20, ratio=2, steps_per_cycle=n_steps)\n)     ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d92655729f678dcbe93a5ee824562b0d23fe275f","trusted":true},"cell_type":"code","source":"# Load the best checkpoint\nbot.load_model(bot.best_performers[0][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(), './model.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 预测函数\ndef predict(loader, *, return_y=False):\n    model.eval()\n    outputs, y_global = [], []\n    with torch.set_grad_enabled(False):\n        for input_tensors in loader:\n            input_tensors = [x.to(model.device) for x in input_tensors if x is not None]\n            outputs.append(bot.predict_batch(input_tensors).cpu())\n        outputs = torch.cat(outputs, dim=0)\n    return outputs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3b1bc4a4264ebda30d4eca35879f1df6f0a11c3","trusted":true},"cell_type":"code","source":"preds = predict(test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(preds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51b7e2335c8f9c1b821b6aeaba7c5122fe74530a","trusted":true},"cell_type":"code","source":"# Create submission file\ndf_sub = pd.DataFrame(torch.softmax(preds, -1).cpu().numpy().clip(1e-3, 1-1e-3), columns=[\"A\", \"B\", \"NEITHER\"])\ndf_sub[\"ID\"] = df_test.ID\ndf_sub.to_csv(\"submission.csv\", index=False)\ndf_sub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}