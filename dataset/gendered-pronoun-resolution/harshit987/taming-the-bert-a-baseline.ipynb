{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport zipfile\nimport sys\nimport time","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Downloading config and weights","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\nwith zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n    zip_ref.extractall()\n!ls 'uncased_L-12_H-768_A-12'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use some of the important scripts from the bert repo which you can find here(https://github.com/google-research/bert)","execution_count":null},{"metadata":{"_uuid":"3e0ac6bb63d1487866640ebe8e73f78b3a96c25a","trusted":true},"cell_type":"code","source":"!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/extract_features.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfccbec6c87185a0db428e3ce8ecb93aa9c4547e","trusted":true},"cell_type":"code","source":"import modeling\nimport extract_features\nimport tokenization\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally downloading the data from the git repo","execution_count":null},{"metadata":{"_uuid":"64c3f40f620c50434a4645a76fe5ad1c9d34ec74","trusted":true},"cell_type":"code","source":"!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\n!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv\n!ls","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f525be88f8fa80c8a8b680c0946e111cd7f653fa"},"cell_type":"markdown","source":"Next, we feed BERT the data from these three files. For each line, we want to obtain contextual embeddings for the 3 target words (A, B, Pronoun). Here are some helper functions to keep track of the offsets of the target words.","execution_count":null},{"metadata":{"_uuid":"1a655a90d41802605da6f27c605313eac4af4cc2","trusted":true},"cell_type":"code","source":"def compute_offset_no_spaces(text, offset):\n\tcount = 0\n\tfor pos in range(offset):\n\t\tif text[pos] != \" \": count +=1\n\treturn count\n\ndef count_chars_no_special(text):\n\tcount = 0\n\tspecial_char_list = [\"#\"]\n\tfor pos in range(len(text)):\n\t\tif text[pos] not in special_char_list: count +=1\n\treturn count\n\ndef count_length_no_special(text):\n\tcount = 0\n\tspecial_char_list = [\"#\", \" \"]\n\tfor pos in range(len(text)):\n\t\tif text[pos] not in special_char_list: count +=1\n\treturn count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def run_bert(data):\n# \t'''\n# \tRuns a forward propagation of BERT on input text, extracting contextual word embeddings\n# \tInput: data, a pandas DataFrame containing the information in one of the GAP files\n\n# \tOutput: emb, a pandas DataFrame containing contextual embeddings for the words A, B and Pronoun. Each embedding is a numpy array of shape (768)\n# \tcolumns: \"emb_A\": the embedding for word A\n# \t         \"emb_B\": the embedding for word B\n# \t         \"emb_P\": the embedding for the pronoun\n# \t         \"label\": the answer to the coreference problem: \"A\", \"B\" or \"NEITHER\"\n# \t'''\n#     # From the current file, take the text only, and write it in a file which will be passed to BERT\n# \ttext = data[\"Text\"]\n# \ttext.to_csv(\"input.txt\", index = False, header = False)\n\n#     # The script extract_features.py runs forward propagation through BERT, and writes the output in the file output.jsonl\n#     # I'm lazy, so I'm only saving the output of the last layer. Feel free to change --layers = -1 to save the output of other layers.\n# \tos.system(\"python3 extract_features.py \\\n# \t  --input_file=input.txt \\\n# \t  --output_file=output.jsonl \\\n# \t  --vocab_file=uncased_L-12_H-768_A-12/vocab.txt \\\n# \t  --bert_config_file=uncased_L-12_H-768_A-12/bert_config.json \\\n# \t  --init_checkpoint=uncased_L-12_H-768_A-12/bert_model.ckpt \\\n# \t  --layers=-1 \\\n# \t  --max_seq_length=256 \\\n# \t  --batch_size=8\")\n\n# \tbert_output = pd.read_json(\"output.jsonl\", lines = True)\n\n# \tos.system(\"rm output.jsonl\")\n# \tos.system(\"rm input.txt\")\n\n# \tindex = data.index\n# \tcolumns = [\"emb_A\", \"emb_B\", \"emb_P\", \"label\"]\n# \temb = pd.DataFrame(index = index, columns = columns)\n# \temb.index.name = \"ID\"\n\n# \tfor i in range(len(data)): # For each line in the data file\n# \t\t# get the words A, B, Pronoun. Convert them to lower case, since we're using the uncased version of BERT\n# \t\tP = data.loc[i,\"Pronoun\"].lower()\n# \t\tA = data.loc[i,\"A\"].lower()\n# \t\tB = data.loc[i,\"B\"].lower()\n\n# \t\t# For each word, find the offset not counting spaces. This is necessary for comparison with the output of BERT\n# \t\tP_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"Pronoun-offset\"])\n# \t\tA_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"A-offset\"])\n# \t\tB_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"B-offset\"])\n# \t\t# Figure out the length of A, B, not counting spaces or special characters\n# \t\tA_length = count_length_no_special(A)\n# \t\tB_length = count_length_no_special(B)\n\n# \t\t# Initialize embeddings with zeros\n# \t\temb_A = np.zeros(768)\n# \t\temb_B = np.zeros(768)\n# \t\temb_P = np.zeros(768)\n\n# \t\t# Initialize counts\n# \t\tcount_chars = 0\n# \t\tcnt_A, cnt_B, cnt_P = 0, 0, 0\n\n# \t\tfeatures = pd.DataFrame(bert_output.loc[i,\"features\"]) # Get the BERT embeddings for the current line in the data file\n# \t\tfor j in range(2,len(features)):  # Iterate over the BERT tokens for the current line; we skip over the first 2 tokens, which don't correspond to words\n# \t\t\ttoken = features.loc[j,\"token\"]\n\n# \t\t\t# See if the character count until the current token matches the offset of any of the 3 target words\n# \t\t\tif count_chars  == P_offset: \n# \t\t\t\t# print(token)\n# \t\t\t\temb_P += np.array(features.loc[j,\"layers\"][0]['values'])\n# \t\t\t\tcnt_P += 1\n# \t\t\tif count_chars in range(A_offset, A_offset + A_length): \n# \t\t\t\t# print(token)\n# \t\t\t\temb_A += np.array(features.loc[j,\"layers\"][0]['values'])\n# \t\t\t\tcnt_A +=1\n# \t\t\tif count_chars in range(B_offset, B_offset + B_length): \n# \t\t\t\t# print(token)\n# \t\t\t\temb_B += np.array(features.loc[j,\"layers\"][0]['values'])\n# \t\t\t\tcnt_B +=1\t\t\t\t\t\t\t\t\n# \t\t\t# Update the character count\n# \t\t\tcount_chars += count_length_no_special(token)\n# \t\t# Taking the average between tokens in the span of A or B, so divide the current value by the count\t\n# \t\temb_A /= cnt_A\n# \t\temb_B /= cnt_B\n\n# \t\t# Work out the label of the current piece of text\n# \t\tlabel = \"Neither\"\n# \t\tif (data.loc[i,\"A-coref\"] == True):\n# \t\t\tlabel = \"A\"\n# \t\tif (data.loc[i,\"B-coref\"] == True):\n# \t\t\tlabel = \"B\"\n\n# \t\t# Put everything together in emb\n# \t\temb.iloc[i] = [emb_A, emb_B, emb_P, label]\n\n# \treturn emb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6552a7a9786187610be695fcf25766594ca41685"},"cell_type":"markdown","source":"The following method takes the data from a file, passes it through BERT to obtain contextual embeddings for the target words, then returns these embeddings in the emb DataFrame. Below, we will use it 3 times, once for each of the files gap-test, gap-development, gap-validation.","execution_count":null},{"metadata":{"_uuid":"9d8437cb4f7c1737e0c915d7d16cc3d004612416","trusted":true},"cell_type":"code","source":"def run_bert(data):\n\t'''\n\tRuns a forward propagation of BERT on input text, extracting contextual word embeddings\n\tInput: data, a pandas DataFrame containing the information in one of the GAP files\n\n\tOutput: emb, a pandas DataFrame containing contextual embeddings for the words A, B and Pronoun. Each embedding is a numpy array of shape (768)\n\tcolumns: \"emb_A\": the embedding for word A\n\t         \"emb_B\": the embedding for word B\n\t         \"emb_P\": the embedding for the pronoun\n\t         \"label\": the answer to the coreference problem: \"A\", \"B\" or \"NEITHER\"\n\t'''\n    # From the current file, take the text only, and write it in a file which will be passed to BERT\n\ttext = data[\"Text\"]\n\ttext.to_csv(\"input.txt\", index = False, header = False)\n\n    # The script extract_features.py runs forward propagation through BERT, and writes the output in the file output.jsonl\n    # I'm lazy, so I'm only saving the output of the last layer. Feel free to change --layers = -1 to save the output of other layers.\n\tos.system(\"python3 extract_features.py \\\n\t  --input_file=input.txt \\\n\t  --output_file=output.jsonl \\\n\t  --vocab_file=uncased_L-12_H-768_A-12/vocab.txt \\\n\t  --bert_config_file=uncased_L-12_H-768_A-12/bert_config.json \\\n\t  --init_checkpoint=uncased_L-12_H-768_A-12/bert_model.ckpt \\\n\t  --layers=-1 \\\n\t  --max_seq_length=256 \\\n\t  --batch_size=8\")\n\n\tbert_output = pd.read_json(\"output.jsonl\", lines = True)\n\n\tos.system(\"rm output.jsonl\")\n\tos.system(\"rm input.txt\")\n\n\tindex = data.index\n\tcolumns = [\"emb_A\", \"emb_B\", \"emb_P\"]\n\temb = pd.DataFrame(index = index, columns = columns)\n\temb.index.name = \"ID\"\n\n\tfor i in range(len(data)): # For each line in the data file\n\t\t# get the words A, B, Pronoun. Convert them to lower case, since we're using the uncased version of BERT\n\t\tP = data.loc[i,\"Pronoun\"].lower()\n\t\tA = data.loc[i,\"A\"].lower()\n\t\tB = data.loc[i,\"B\"].lower()\n\n\t\t# For each word, find the offset not counting spaces. This is necessary for comparison with the output of BERT\n\t\tP_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"Pronoun-offset\"])\n\t\tA_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"A-offset\"])\n\t\tB_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"B-offset\"])\n\t\t# Figure out the length of A, B, not counting spaces or special characters\n\t\tA_length = count_length_no_special(A)\n\t\tB_length = count_length_no_special(B)\n\n\t\t# Initialize embeddings with zeros\n\t\temb_A = np.zeros(768)\n\t\temb_B = np.zeros(768)\n\t\temb_P = np.zeros(768)\n\n\t\t# Initialize counts\n\t\tcount_chars = 0\n\t\tcnt_A, cnt_B, cnt_P = 0, 0, 0\n\n\t\tfeatures = pd.DataFrame(bert_output.loc[i,\"features\"]) # Get the BERT embeddings for the current line in the data file\n\t\tfor j in range(2,len(features)):  # Iterate over the BERT tokens for the current line; we skip over the first 2 tokens, which don't correspond to words\n\t\t\ttoken = features.loc[j,\"token\"]\n\n\t\t\t# See if the character count until the current token matches the offset of any of the 3 target words\n\t\t\tif count_chars  == P_offset: \n\t\t\t\t# print(token)\n\t\t\t\temb_P += np.array(features.loc[j,\"layers\"][0]['values'])\n\t\t\t\tcnt_P += 1\n\t\t\tif count_chars in range(A_offset, A_offset + A_length): \n\t\t\t\t# print(token)\n\t\t\t\temb_A += np.array(features.loc[j,\"layers\"][0]['values'])\n\t\t\t\tcnt_A +=1\n\t\t\tif count_chars in range(B_offset, B_offset + B_length): \n\t\t\t\t# print(token)\n\t\t\t\temb_B += np.array(features.loc[j,\"layers\"][0]['values'])\n\t\t\t\tcnt_B +=1\t\t\t\t\t\t\t\t\n\t\t\t# Update the character count\n\t\t\tcount_chars += count_length_no_special(token)\n\t\t# Taking the average between tokens in the span of A or B, so divide the current value by the count\t\n\t\temb_A /= cnt_A\n\t\temb_B /= cnt_B\n\n# \t\t# Work out the label of the current piece of text\n# \t\tlabel = \"Neither\"\n# \t\tif (data.loc[i,\"A-coref\"] == True):\n# \t\t\tlabel = \"A\"\n# \t\tif (data.loc[i,\"B-coref\"] == True):\n# \t\t\tlabel = \"B\"\n\n\t\t# Put everything together in emb\n\t\temb.iloc[i] = [emb_A, emb_B, emb_P]\n\n\treturn emb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98ca59d6fc859477aff649ea2ca8a35f65201e9d"},"cell_type":"markdown","source":"Read the three GAP files, pass them through BERT, and write the contextual embeddings in json files. Unfortunately, I wasn't able to silence TensorFlow, so it's giving a lot of information and warnings when I run this cell.","execution_count":null},{"metadata":{"_uuid":"8298947fa33285e722bdaae2df41bca5dd795732","trusted":true},"cell_type":"code","source":"print(\"Started at \", time.ctime())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_data = pd.read_csv(\"gap-validation.tsv\", sep = '\\t')\nvalidation_emb = run_bert(validation_data)\n\ndevelopment_data = pd.read_csv(\"gap-development.tsv\", sep = '\\t')\ndevelopment_emb = run_bert(development_data)\n\nprint(\"Finished at \", time.ctime())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def featurize(embedding_df):\n    \n    pronoun_embs, a_embs, b_embs = [], [], []\n    \n    for i in tqdm(range(len(embedding_df))):\n        \n        pronoun_embs.append(embedding_df.loc[i, \"emb_P\"])\n        a_embs.append(embedding_df.loc[i, \"emb_A\"])\n        b_embs.append(embedding_df.loc[i, \"emb_B\"])\n\n#         label_map = {'A': 0, 'B': 1, 'Neither': 2}\n#         labels.append(label_map[embedding_df.loc[i, \"label\"]])\n\n    \n    a_embs = np.asarray(a_embs).astype('float')\n    b_embs = np.asarray(b_embs).astype('float') \n    pronoun_embs = np.asarray(pronoun_embs).astype('float')\n    \n    return np.concatenate([a_embs, b_embs, pronoun_embs], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def featurize(embedding_df):\n    \n#     pronoun_embs, a_embs, b_embs, labels = [], [], [], []\n    \n#     for i in tqdm(range(len(embedding_df))):\n        \n#         pronoun_embs.append(embedding_df.loc[i, \"emb_P\"])\n#         a_embs.append(embedding_df.loc[i, \"emb_A\"])\n#         b_embs.append(embedding_df.loc[i, \"emb_B\"])\n\n#         label_map = {'A': 0, 'B': 1, 'Neither': 2}\n#         labels.append(label_map[embedding_df.loc[i, \"label\"]])\n\n    \n#     a_embs = np.asarray(a_embs).astype('float')\n#     b_embs = np.asarray(b_embs).astype('float') \n#     pronoun_embs = np.asarray(pronoun_embs).astype('float')\n    \n#     return np.concatenate([a_embs, b_embs, pronoun_embs], axis=1), np.asarray(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, y_train = featurize(pd.concat([validation_emb, development_emb]).sort_index().reset_index())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = my_imputer.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit = LogisticRegression(C=0.0075, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n          n_jobs=4, penalty='l2', random_state=42, solver='lbfgs',\n          tol=0.0001, verbose=0, warm_start=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nfilename = 'finalized_model.sav'\npickle.dump(logit, open(filename, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv(\"../input/gendered-pronoun-resolution/test_stage_2.tsv\",sep = \"\\t\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loaded_model = pickle.load(open('./finalized_model.sav', 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/gendered-pronoun-resolution/sample_submission_stage_2.csv\", index_col = \"ID\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 0\nfor i in range(0,12360, 200):\n    test_emb = run_bert(test_data[i:i+200].reset_index())\n    X_test = featurize(test_emb.sort_index().reset_index())\n    X_test = my_imputer.fit_transform(X_test)\n    logit_test_pred = loaded_model.predict_proba(X_test)\n    for j in range(0, 200):\n        submission.iloc[count+j][\"A\"] = logit_test_pred[j, 0]\n        submission.iloc[count+j][\"B\"] = logit_test_pred[j, 1]\n        submission.iloc[count+j][\"NEITHER\"]= logit_test_pred[j, 2]\n    count +=200\n    print(count)\n        \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.iloc[12358]['A']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"./submissionf.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dm = pd.read_csv(\"./submissionf.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dm.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}