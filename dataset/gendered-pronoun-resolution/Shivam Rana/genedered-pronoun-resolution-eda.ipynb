{"cells":[{"metadata":{"_uuid":"27ccbadc82d2e37408379edb7ed82a5efcd70ba4"},"cell_type":"markdown","source":"Gendered Pronoun Resolution\n=====\n\n# Objective\n\nIn this two-stage competition, Kagglers are challenged to build pronoun resolution systems that perform equally well regardless of pronoun gender. Stage two's final evaluation will use a new dataset following the same format. To encourage gender-fair modeling, the ratio of masculine to feminine examples in the official test data will not be known ahead of time. \n\nIn this competition, you must identify the target of a pronoun within a text passage. The source text is taken from Wikipedia articles. You are provided with the pronoun and two candidate names to which the pronoun could refer. You must create an algorithm capable of deciding whether the pronoun refers to name ```A```, name ```B```, or neither.\n\n# Evaluation\n\nSubmissions are evaluated using the multi-class logarithmic loss. Each pronoun has been labeled with whether it refers to A, B, or NEITHER. For each pronoun, you must submit a set of predicted probabilities (one for each class). The formula is then\n\n$log loss = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^My_{ij}\\log(p_{ij})$\n\nwhere N is the number of samples in the test set, M is 3, log is the natural logarithm, $y_{ij}$ is 1 if observation $i$ belongs to class $j$ and 0 otherwise, and $p_{ij}$ is the predicted probability that observation $i$ belongs to class $j$.\n\nThe submitted probabilities are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with $max(min(p,1−10^{−15}),10^{−15})$.\n\n# Data\n\nUnlike many Kaggle challenges, this competition does not provide an explicit labeled training set. Files are also available on the [GAP Dataset Github Repo](https://github.com/google-research-datasets/gap-coreference). **Note that the labels for the test set are available on this page. However, your final score and ranking will be determined in stage 2, against a withheld private test set.**\n\n- ```test_stage_1.tsv``` - the test set data for stage 1\n\n## Columns\n\n- ```ID``` - Unique identifier for an example (Matches to Id in output file format)\n- ```Text``` - Text containing the ambiguous pronoun and two candidate names (about a paragraph in length)\n- ```Pronoun``` - The target pronoun (text)\n- ```Pronoun```- Offset The character offset of Pronoun in Text\n- ```A``` - The first name candidate (text)\n- ```A-offset``` - The character offset of name ```A``` in Text\n- ```A-coref``` - Whether ```A``` corefers with the pronoun, TRUE or FALSE\n- ```B``` - The second name candidate\n- ```B-offset``` - The character offset of name ```B``` in Text\n- ```B-coref``` - Whether ```B``` corefers with the pronoun, TRUE or FALSE\n- ```URL``` - The URL of the source Wikipedia page for the example\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom pathlib import Path\n\nprint(os.listdir(\"../input\"))\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"DATA_ROOT = Path(\"../input\")\n\ntest_path = DATA_ROOT / \"test_stage_1.tsv\"\ntest_path = \"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv\"\ndev_path = \"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\"\nval_path = \"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84dce7257dcc0326684963115c942dfcc2a13d6b"},"cell_type":"code","source":"devdf = pd.read_csv(dev_path, delimiter=\"\\t\")\ndevdf.shape\ndevdf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3303ede7d84e6ac193751fb71b4e5d2b94a80b00"},"cell_type":"code","source":"valdf = pd.read_table(val_path, delimiter=\"\\t\")\nvaldf.shape\nvaldf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e71eae90c6aaa73d4c15a8809f6bda51c2d31c90"},"cell_type":"code","source":"testdf = pd.read_table(test_path, delimiter=\"\\t\")\ntestdf.shape\ntestdf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76be2efbe2b616ec9e6c512f0e193f9280a25e6a"},"cell_type":"markdown","source":"Observations:\n\n- In the original Google AI Language's GAP dataset there are 8,908 coreference-labeled pairs - 4000 pairs in test and development each and 908 pairs in validation set\n- Through Github we are shared the half of that - 2000 pairs in test and development each and 454 pairs in validation set\n- All the 3 tables have all the 11 columns which includes the correct labels as well\n\nWe'll combine all of them together to do the exploratory analysis.\n"},{"metadata":{"trusted":true,"_uuid":"861eae2e45f3114f2fe019ca6ada76abb69861e5"},"cell_type":"code","source":"df = pd.concat([devdf, valdf, testdf])\ndf.shape\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0257f2b65609ff4a1f82d5333b153e2fb0e35be"},"cell_type":"markdown","source":"## Labels"},{"metadata":{"trusted":true,"_uuid":"e67d293f14b8471bb19ceff60cbe759d52414fcd"},"cell_type":"code","source":"filt1 = (df[\"A-coref\"] == True) & (df[\"B-coref\"] == True)\nfilt2 = (df[\"A-coref\"] == True) | (df[\"B-coref\"] == True)\n\ndf[\"label\"] = \"NEITHER\"\ndf.loc[df[\"A-coref\"] == True, \"label\"] = \"A\"\ndf.loc[df[\"B-coref\"] == True, \"label\"] = \"B\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"11a187f8b3664963995bd0f3f6ccc8150a86ea81"},"cell_type":"code","source":"print(\"Cases where both A and B are correct:\", filt1.sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0d9b1309dd65a20d63f8cb39ad1ef9e71bdc0e3"},"cell_type":"code","source":"print(\"Cases where either A or B is correct:\", filt2.sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1cd6e35bd5d53b198459ca6721beb13bccc26d4"},"cell_type":"code","source":"print(\"Cases where neither A nor B is correct:\", df[~filt2].shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c60342b8bb2278249132d06ee2f9074001d2df91"},"cell_type":"code","source":"df.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b13d4630173dc7a10c46fe68de2dd04c1134d6b"},"cell_type":"markdown","source":"Observations: \n- There are no cases where both A and B are correct (sanity check)\n- There are 490 cases where neither A nor B are the true labels\n- There are 3964 cases (1985 cases of B and 1979 cases of A) where either A or B are true labels"},{"metadata":{"trusted":true,"_uuid":"ee5001527699d645d5c6c26b48e3bb0ccaa1a20a"},"cell_type":"markdown","source":"## Text"},{"metadata":{"trusted":true,"_uuid":"879257b05d59b1d4459c3e6dc3bc90a82c61623b"},"cell_type":"code","source":"df[\"textlen\"] = df.Text.str.len()\ndf[\"textwords\"] = df.Text.str.split().str.len()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f56cbecb6cc8a67c38d40b3093ee13be9d7c41d3"},"cell_type":"code","source":"df.textlen.describe()\n\nax = df.textlen.plot.hist(bins=15, figsize=(10, 5))\n_ = ax.set_title(\"Text length distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81dc08901b1ec547d65ced46294dde4d0aa5cd9a"},"cell_type":"markdown","source":"Observations:\n- Text length ranges from 69 to 1347 characters\n- Most of the texts lie in 300 to 500 character\n\nLet's check the both extremes of the text based on text lengths"},{"metadata":{"trusted":true,"_uuid":"41ab3f64f71bcc0bd48f9fcb1b58f96284d4feca"},"cell_type":"code","source":"df[df.textlen < 70].T.to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4866bad2c7747a20266afef89f82f80a0912f3f"},"cell_type":"code","source":"df[df.textlen > 1340].T.to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c128ffef38d4d719ae48e73cd621cd2e70e282d"},"cell_type":"code","source":"df.textwords.describe()\n\nax = df.textwords.plot.hist(bins=15, figsize=(10, 5))\n_ = ax.set_title(\"Text words histogram\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa1747e5e820e7a9ec43dceebf331d3454341f4c"},"cell_type":"markdown","source":"Observations:\n\n- text words range from 12 to 223\n- Most words lie between 50 and 100\n\nLet's see one example of the both extremes"},{"metadata":{"trusted":true,"_uuid":"e27f0f6a03f4a0f523a815a6392d93c06b551766"},"cell_type":"code","source":"df[df.textwords < 13].T.to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18acc531b2ac47774a8ff6d34c7b77bfc5c5eea3"},"cell_type":"code","source":"df[df.textwords > 220].T.to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e742616feff9559cc25012895a86dbaef68632da"},"cell_type":"code","source":"ax = df[[\"textlen\", \"textwords\"]].plot.scatter(x=\"textlen\", y=\"textwords\", figsize=(10, 5))\n_ = ax.set_title(\"text words vs text length\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c27827edc8e1564c1f5bf3adc68c2cde775f4459"},"cell_type":"markdown","source":"Observations:\n- it follows a roughly linear relationship between text words and text lengths.\n- databset seems to be carefully created as I've never seen this plot this neat\n\n(by first looking at it, I was surprisingly reminded of the milky way galaxy)"},{"metadata":{"_uuid":"67b596a065c0a89245e5ab61f485677d14777c07"},"cell_type":"markdown","source":"## Pronoun and Gender"},{"metadata":{"trusted":true,"_uuid":"e68f4b712df6436dd153ae201e37c626c4cd3794"},"cell_type":"code","source":"male_pro = [\"his\", \"he\", \"He\", \"him\", \"His\"]\nfemale_pro = [\"her\", \"she\", \"She\", \"her\", \"hers\"]\n\ndf[\"gender\"] = df.Pronoun.apply(lambda x: \"male\" if x.lower() in male_pro else \"female\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f61bda005e237ceca50862fd6a3029877809093b"},"cell_type":"code","source":"df.gender.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2aebee694310aa09beaa5ba9b531966f30a05eba"},"cell_type":"code","source":"df.Pronoun.str.lower().value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02707e256b7f95170b2a6a855334e1d3d662090b"},"cell_type":"markdown","source":"Observations:\n- we have a 50-50 split in the genders as the competition details said, but this wont be true with the 2nd stage test data\n- in male pronouns, we have - he, his, him - with his being the highest in the frequency (1193)\n- in female pronouns we have - she, her, hers - with her being the highest in frequency (1315)"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4870dba8716a9f4c8f068f8f0de626b75fec4bad"},"cell_type":"code","source":"df[df.Pronoun == \"hers\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d41fd3145bcd58ddf1b69b5139d484614a49ad32"},"cell_type":"markdown","source":"Observations:\n\n- \"hers\" is not present in the train or validation set"},{"metadata":{"_uuid":"8bbb922b319b35e7db05e373f1508227399fc5f2"},"cell_type":"markdown","source":"## Pronoun position"},{"metadata":{"trusted":true,"_uuid":"9a43f23b925fb8c978fb68443660039778a34a12"},"cell_type":"code","source":"ax = df[[\"textlen\", \"Pronoun-offset\"]].plot.scatter(x=\"Pronoun-offset\", y=\"textlen\", figsize=(10, 5))\n_ = ax.set_title(\"text len vs pronoun offset\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a5a0ba683b3a4906b5c50aea93e39e0c88f2243"},"cell_type":"markdown","source":"Observations:\n- half of the area where there are no points is because pronoun offset can't get higher then the text length itself; this is also a sanity check of our data (```textlen=Pronoun-offset```)\n- there are many texts where the selected pronoun is towards the end; this relationship is explored next"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"f82ebdd6b723fc749c7864f4749b087b293b5f24"},"cell_type":"code","source":"(df.textlen / df[\"Pronoun-offset\"]).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5629e68e33b5cc95c9fbcbd2ab3cdc16b3dae42f"},"cell_type":"code","source":"df[(df.textlen / df[\"Pronoun-offset\"])>3].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2097d4abc3043ce4b3f0f14fc45874acca90ce1b"},"cell_type":"code","source":"((df.textlen / df[\"Pronoun-offset\"])<=2).sum() / df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4eda9577b05a1f69ab3068c795d9cebf5aaee7a6"},"cell_type":"code","source":"filt = (df.textlen / df[\"Pronoun-offset\"])<=3\ntemp = df.loc[filt, [\"textlen\", \"Pronoun-offset\"]]\nax = (temp.textlen / temp[\"Pronoun-offset\"]).plot.hist(bins=15, figsize=(10, 5))\n_ = ax.set_title(\"pronoun position to text length ratio distribution\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69b4913a226609a10c017588e0a31932b88b95a7"},"cell_type":"markdown","source":"Observations:\n\n- this ratio will never go below 1 as the pronoun offset cannot be greater than text length\n- from the plot it's apparent that majority of the pronouns are in the 2nd half of the text; (this majority is 96% of the data)\n- only 43 cases (0.9%) cases are there where the pronoun is in the initial 1/3rd of the data\n- this plot shows that in most of the cases, first the entity/person is introduced in a sentence and then the pronoun is used\n\nNext we'll explore the gap between the person and the pronoun"},{"metadata":{"trusted":true,"_uuid":"3e943cce43fa8926f3bcf074cc1b898b88a93df9"},"cell_type":"markdown","source":"## Entity Pronoun gap"},{"metadata":{"trusted":true,"_uuid":"e67d293f14b8471bb19ceff60cbe759d52414fcd"},"cell_type":"code","source":"filt = (df[\"A-coref\"] == True) | (df[\"B-coref\"] == True)\n\ndf[\"label_offset\"] = pd.np.nan\ndf.loc[df[\"A-coref\"] == True, \"label_offset\"] = df.loc[df[\"A-coref\"] == True, \"A-offset\"]\ndf.loc[df[\"B-coref\"] == True, \"label_offset\"] = df.loc[df[\"B-coref\"] == True, \"B-offset\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3ad89a0a7bd60dc42140a773506862d2ca9c7e3"},"cell_type":"code","source":"df[filt].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c54a868d3a669bf9af98e6ec2cd45d9bae778236"},"cell_type":"code","source":"temp = df.loc[filt, [\"Pronoun-offset\", \"label_offset\"]]\ntemp[\"label_pronoun_gap\"] = temp[\"label_offset\"] - temp[\"Pronoun-offset\"]\n\ntemp[\"label_pronoun_gap\"].describe()\n\nax = temp[\"label_pronoun_gap\"].plot.hist(bins=15, figsize=(10, 5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"183071e2d5a1a611829718bd382cc3771b253642"},"cell_type":"markdown","source":"Observations:\n- The gap between the entity and pronoun ranges from -502 to 291\n- mean is at -57 shows that entity appears before pronoun which we also previously\n- positive gap tells us about the cases where the pronoun is used before the entity is mentioned"},{"metadata":{"trusted":true,"_uuid":"83b3ba22fbe7795f04751f7553907e9e58d31f6d"},"cell_type":"code","source":"(temp.label_pronoun_gap > 0).value_counts()\n(temp.label_pronoun_gap > 0).value_counts()*100/temp.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcc0f2ff352fe5276409685658f64b81d4a1df9a"},"cell_type":"markdown","source":"Observations:\n\n- False means the cases where pronoun comes after entity (81%)\n- True means the cases where the entity comes after pronoun (19%)"},{"metadata":{"trusted":true,"_uuid":"bcb78b1009fd36afbbc506d8e8aa809f3b95d081"},"cell_type":"code","source":"ax = temp.plot.scatter(x=\"Pronoun-offset\", y=\"label_pronoun_gap\", figsize=(10, 5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bb56432f70f2f9fa670ebc8b60fdcaab5cb9641"},"cell_type":"markdown","source":"Observations:\n- the horizontal like at ```pronoun_label_gap=0```, tells us that the entity and pronoun offsets can never be same (which makes sense)\n- the vertical points at ```Pronoun-offset=0``` tells us about the points where pronoun comes at the start of the sentence and entities comes later in the sentense\n- anything above hirozontal line of 0 are cases where pronoun comes before entities (19% data)\n- anything below hirozontal line of 0 are cases where entities comes before pronouns (81% data)\n- the slanted line shows that the entities are at the start of the sentence since max gap possible (for cases where entity comes before pronoun) can be pronoun offset; entites can't come before the sentence even starts (```label_pronoun_gap = -Pronoun-offset```)"},{"metadata":{"trusted":true,"_uuid":"7fc566ed242e987b7c5d9f42bbe92e6a0a613702"},"cell_type":"code","source":"ax = temp.plot.scatter(x=\"label_offset\", y=\"label_pronoun_gap\", figsize=(10, 5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d53edfb7815429e545f055aa5e21471fb34d4e7b"},"cell_type":"markdown","source":"Observations:\n- this plot is very similar to the previous plot where we plotted label_pronoun_gap vs Pronoun-offset\n- horizontal line at label_pronoun_gap=0 means entity offset and pronoun offset can never be the same\n- anything above hirozontal line of 0 are cases where pronoun comes before entities (19% data)\n- anything below hirozontal line of 0 are cases where entities comes before pronouns (81% data)\n- the vertical line at ```label_offset=0``` shows that the entities are at the start of sentence and the pronouns later in the sentence. Since here, the entities are at the start, there can never be any case where the gap will be positive. this like is same as the slanted line from the previous plot.\n- the slight (and small) group of points in a slanted line are the points where pronoun is at the start of the sentence and entities come later. This line is similar to the vertical points at ```Pronoun-offset=0``` in the previous plot"},{"metadata":{"trusted":true,"_uuid":"4aafa85f379c020bc92dfcd0e400bd398e926814"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e429b40ad98210992bed6475309d5fc2c0c4976"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7764c1447761c4e657a8bc0932fe788d223de827"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f828c88c2dd2fa418a4e7d9a011df64d200535c2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09cef185ff8cfc74134f28e7949ad75c7723cbd4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}