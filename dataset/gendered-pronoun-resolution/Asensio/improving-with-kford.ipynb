{"cells":[{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport spacy\nfrom sklearn.model_selection import KFold\nimport numpy as np\nnlp = spacy.load('en_core_web_sm')\nfrom sklearn import *\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ntest = pd.read_csv('../input/test_stage_2.tsv', delimiter='\\t').rename(columns={'A': 'A_Noun', 'B': 'B_Noun'})\nsub = pd.read_csv('../input/sample_submission_stage_2.csv')\ntest.shape, sub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ngh_test = pd.read_csv(\"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv\", delimiter='\\t')\ngh_valid = pd.read_csv(\"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv\", delimiter='\\t')\ntrain = pd.concat((gh_test, gh_valid)).rename(columns={'A': 'A_Noun', 'B': 'B_Noun'}).reset_index(drop=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def name_replace(s, r1, r2):\n    s = str(s).replace(r1, r2)\n    for r3 in r1.split(' '):\n        s = str(s).replace(r3, r2)\n    return s\n\n\ndef get_features(df):\n    df['section_min'] = df[['Pronoun-offset', 'A-offset', 'B-offset']].min(axis=1)\n    df['Pronoun-offset2'] = df['Pronoun-offset'] + df['Pronoun'].map(len)\n    df['A-offset2'] = df['A-offset'] + df['A_Noun'].map(len)\n    df['B-offset2'] = df['B-offset'] + df['B_Noun'].map(len)\n    df['section_max'] = df[['Pronoun-offset2', 'A-offset2', 'B-offset2']].max(axis=1)\n    df['Text'] = df.apply(lambda r: name_replace(r['Text'], r['A_Noun'], 'subjectone'), axis=1)\n    df['Text'] = df.apply(lambda r: name_replace(r['Text'], r['B_Noun'], 'subjecttwo'), axis=1)\n\n    df['A-dist'] = (df['Pronoun-offset'] - df['A-offset']).abs()\n    df['B-dist'] = (df['Pronoun-offset'] - df['B-offset']).abs()\n    return (df)\n\ntrain = get_features(train)\ntest = get_features(test)\n\ndef get_nlp_features(s, w):\n    doc = nlp(str(s))\n    # print(doc)\n    tokens = pd.DataFrame([[token.text, token.dep_] for token in doc], columns=['text', 'dep'])\n    # print(tokens)\n    # token.text is the word, token.dep is the characteristic of a word\n    # print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n    # print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n    # tokens == 'poss' means possessive.\n    return len(tokens[((tokens['text']==w) & (tokens['dep']=='poss'))])\ntrain['A-poss'] = train['Text'].map(lambda x: get_nlp_features(x, 'subjectone'))\n# print(train['A-poss'])\n# 2453    2\n# Name: A-poss, Length: 2454, dtype: int64\n\ntrain['B-poss'] = train['Text'].map(lambda x: get_nlp_features(x, 'subjecttwo'))\ntest['A-poss'] = test['Text'].map(lambda x: get_nlp_features(x, 'subjectone'))\ntest['B-poss'] = test['Text'].map(lambda x: get_nlp_features(x, 'subjecttwo'))\n\ntrain = train.rename(columns={'A-coref':'A', 'B-coref':'B'})\ntrain['A'] = train['A'].astype(int)\ntrain['B'] = train['B'].astype(int)\ntrain['NEITHER'] = 1.0 - (train['A'] + train['B'])\n\ncol = ['Pronoun-offset', 'A-offset', 'B-offset', 'section_min', 'Pronoun-offset2', 'A-offset2', 'B-offset2', 'section_max', 'A-poss', 'B-poss', 'A-dist', 'B-dist']\nx1, x2, y1, y2 = model_selection.train_test_split(train[col].fillna(-1), train[['A', 'B', 'NEITHER']], test_size=0.2, random_state=1)\nprint(x1.head())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = multiclass.OneVsRestClassifier(ensemble.RandomForestClassifier(max_depth = 7, n_estimators=1000, random_state=33))\n\nfolds = 5\nxchange = 0.94\nkf = KFold(n_splits=folds, shuffle=False, random_state=11)\ntrn = train[col].fillna(-1)\nval = train[[\"A\", \"B\", \"NEITHER\"]]\nscores = []\n\nfor train_index, test_index in kf.split(train):\n    x1, x2 = trn.iloc[train_index], trn.iloc[test_index]\n    y1, y2 = val.iloc[train_index], val.iloc[test_index]\n\n    model.fit(x1, y1)\n    score = metrics.log_loss(y2, model.predict_proba(x2))\n    if score < xchange:\n        print(\"log-loss:\", score)\n    scores.append(score)\n\n# print(\"CV Score(log-loss):\", np.mean(scores))\nmodel.fit(x1, y1)\n\n# print('log_loss', metrics.log_loss(y2, model.predict_proba(x2)))\nmodel.fit(train[col].fillna(-1), train[['A', 'B', 'NEITHER']])\nresults = model.predict_proba(test[col])\ntest['A'] = results[:,0]\ntest['B'] = results[:,1]\ntest['NEITHER'] = results[:,2]\ntest[['ID', 'A', 'B', 'NEITHER']].to_csv('submission.csv', index=False)\n\n\n# 做可视化.\n# Feature\n# 调参Randomforest建议\n# Xgbootst的参数\n# https://www.cnblogs.com/zhizhan/p/5826089.html\n# Grid Search 的过程来确定一组最佳的参数。其实这个过程说白了就是根据给定的参数候选对所有的组合进行暴力搜索。\n# 用 20 个不同的随机种子来生成 Ensemble，最后取 Weighted Average。这个其实算是一种变相的 Bagging。\n# 其意义在于按我实现 Stacking 的方式，我在训练 Base Model 时只用了 80% 的训练数据，而训练第二层的 Model 时用了 100% 的数据，\n# 这在一定程度上增大了 Overfitting 的风险。而每次更改随机种子可以确保每次用的是不同的 80%，这样在多次训练取平均以后就相当于逼近了使用 100% 数据的效果","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}