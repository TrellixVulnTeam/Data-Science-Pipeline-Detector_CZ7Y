{"cells":[{"metadata":{"_uuid":"8cf32ed0a9bf164c275e9c7bd3fce38f0f59a3ff"},"cell_type":"markdown","source":"**For this Notebook:**\n* This notebook is forked from https://www.kaggle.com/chanhu/bert-score-layer-lb-0-475 . It's based on pure BERT, using mention-pair concept, and add some more distance, URL and other manual features, to improve the performance.\n* I delete some mislabeled training data, posted here: https://www.kaggle.com/c/gendered-pronoun-resolution/discussion/81331#503094\n* I add a gender feature, to see if gender is a great feature for this dataset.\n\n**Next step:**\n* I add some method to present F1 score, precision, and recall metrics, so that I can compare it with some other models, and the benchmark.\n* fine-tuning BERT: if have time, I will do this later.","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport zipfile\nimport gc\nfrom tqdm import tqdm_notebook as tqdm\nimport re\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\nwith zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n    zip_ref.extractall()\n!ls 'uncased_L-12_H-768_A-12'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e265e7becd34d793d13009f851a4fc7c6f7f95fa","trusted":true},"cell_type":"code","source":"!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/extract_features.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"440d7107d50a014003a2c53485554dc693f81982","trusted":true},"cell_type":"code","source":"import modeling\nimport extract_features\nimport tokenization\nimport tensorflow as tf\nimport spacy\nnlp = spacy.load('en_core_web_lg')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a15e5f4c32659d3408dee927035725ae0135a39","trusted":true},"cell_type":"code","source":"test_df  = pd.read_table('../input/gap-coreference/gap-development.tsv')\ntrain_df = pd.read_table('../input/gap-coreference/gap-test.tsv')\nval_df   = pd.read_table('../input/gap-coreference/gap-validation.tsv')\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"add functions to calculate metrics: F1 score, precision, recall\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TO_DROP data are mislabeled","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the mislabeled data\nTO_DROP = pd.DataFrame(\n    ['development-1030',\n    'development-1051',\n    'development-1086',\n    'development-1089',\n    'development-1161',\n    'development-1192',\n    'development-1216',\n    'development-1292',\n    'development-1369',\n    'development-1422',\n    'development-1569',\n    'development-1722',\n    'development-1745',\n    'development-1874',\n    'development-1934',\n    'development-291',\n    'development-328',\n    'development-346',\n    'development-401',\n    'development-41',\n    'development-444',\n    'development-541',\n    'development-566',\n    'development-578',\n    'development-697',\n    'development-837',\n    'development-857',\n    'development-871',\n    'development-921',\n    'development-955',\n    'development-1204',\n    'development-1565',\n    'development-1680',\n    'development-171',\n    'development-1710',\n    'development-1908',\n    'development-1994',\n    'development-219',\n    'development-260',\n    'development-302',\n    'development-325',\n    'development-337',\n    'development-36',\n    'development-528',\n    'development-779',\n    'development-822',\n    'development-95',\n    'development-967'],columns=['ID'])\n\ntest_df = test_df[~test_df.ID.isin(TO_DROP.ID.values)].copy()\ntest_df.reset_index(drop=True, inplace=True)\n# print(test_df.loc[:50,'Text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the code below is used to get the gender distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import csv \n# load the name gender frequency\ngender_frequency = {}\n# Gender by Name, DATASET BY DEREK HOWARD\nwith open('../input/name-gender/name_gender.csv', 'r', encoding='UTF-8') as gender_file:\n    gender_data = csv.reader(gender_file)\n    # gender_data = csv.DictReader(gender_file)\n    for row in gender_data:\n        gender_frequency[row[0].lower()] = row[1]\ngender_file.close()\nfor male in ['he','him','his']:\n    gender_frequency[male] = 'M'\nfor female in ['she','her','hers']:\n    gender_frequency[female] = 'F'\n    \n    \n# extract gender feature\ndef _extract_gender(ne_string):\n    try:\n        gender = gender_frequency[ne_string.lower()]\n        \n        # index 0 is masculine\n        if gender == 'M':\n            return 1\n        # index 1 is feminine\n        elif gender == 'F':\n            return 2\n        # index 2 is neuter\n        else:\n            return 0\n        \n    except KeyError:\n        return 0    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3d1e4d3037745835e8f34b44a6abea86a85ba7f","trusted":true},"cell_type":"code","source":"def bs(lens, target):\n    low, high = 0, len(lens) - 1\n\n    while low < high:\n        mid = low + int((high - low) / 2)\n\n        if target > lens[mid]:\n            low = mid + 1\n        elif target < lens[mid]:\n            high = mid\n        else:\n            return mid + 1\n\n    return low\n\ndef bin_distance(dist):\n    \n    buckets = [1, 2, 3, 4, 5, 8, 16, 32, 64]  \n    low, high = 0, len(buckets)\n    while low < high:\n        mid = low + int((high-low) / 2)\n        if dist > buckets[mid]:\n            low = mid + 1\n        elif dist < buckets[mid]:\n            high = mid\n        else:\n            return mid\n\n    return low\n\ndef distance_features(P, A, B, char_offsetP, char_offsetA, char_offsetB, text, URL):\n    \n    doc = nlp(text)\n    \n    gender_P=-1\n    gender_A=-1\n    gender_B=-1\n    \n    for token in doc:\n        if token.idx == char_offsetP:\n            gender_P = _extract_gender(token.text)\n        elif token.idx == char_offsetA:\n            gender_A = _extract_gender(token.text)\n        elif token.idx == char_offsetB:\n            gender_B = _extract_gender(token.text)   \n            \n    \n    lens = [token.idx for token in doc]\n    mention_offsetP = bs(lens, char_offsetP) - 1\n    mention_offsetA = bs(lens, char_offsetA) - 1\n    mention_offsetB = bs(lens, char_offsetB) - 1\n    \n    mention_distA = mention_offsetP - mention_offsetA \n    mention_distB = mention_offsetP - mention_offsetB\n    \n    splited_A = A.split()[0].replace(\"*\", \"\")\n    splited_B = B.split()[0].replace(\"*\", \"\")\n    \n    if re.search(splited_A[0], str(URL)):\n        contains = 0\n    elif re.search(splited_B[0], str(URL)):\n        contains = 1\n    else:\n        contains = 2             \n    \n        \n    # dist: distance between mentions, contains: simiarity of url and the mention\n    dist_binA = bin_distance(mention_distA)\n    dist_binB = bin_distance(mention_distB)\n    \n    if gender_P == gender_A:\n        gender_fit = 0\n    elif gender_P == gender_B:\n        gender_fit = 1\n    else:\n        gender_fit = 2\n        \n#     output =  [dist_binA, dist_binB, contains]\n    output =  [dist_binA, dist_binB, contains, gender_fit]\n    \n    return output\n\ndef extract_dist_features(df):\n    \n    index = df.index\n#     columns = [\"D_PA\", \"D_PB\", \"IN_URL\"]\n    columns = [\"D_PA\", \"D_PB\", \"IN_URL\", \"G_FIT\"]\n    dist_df = pd.DataFrame(index = index, columns = columns)\n\n    for i in tqdm(range(len(df))):\n        \n        text = df.loc[i, 'Text']\n        P_offset = df.loc[i,'Pronoun-offset']\n        A_offset = df.loc[i, 'A-offset']\n        B_offset = df.loc[i, 'B-offset']\n        P, A, B  = df.loc[i,'Pronoun'], df.loc[i, 'A'], df.loc[i, 'B']\n        URL = df.loc[i, 'URL']\n        \n        dist_df.iloc[i] = distance_features(P, A, B, P_offset, A_offset, B_offset, text, URL)\n        \n    return dist_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dist_df = extract_dist_features(test_df)\ntest_dist_df.to_csv('test_dist_df.csv', index=False)\nval_dist_df = extract_dist_features(val_df)\nval_dist_df.to_csv('val_dist_df.csv', index=False)\ntrain_dist_df = extract_dist_features(train_df)\ntrain_dist_df.to_csv('train_dist_df.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"128bf9ecc2123276f55c58cc7f49ed3e2c9eb30b","trusted":true},"cell_type":"code","source":"def count_char(text, offset):   \n    count = 0\n    for pos in range(offset):\n        if text[pos] != \" \": count +=1\n    return count\n\ndef candidate_length(candidate):\n    count = 0\n    for i in range(len(candidate)):\n        if candidate[i] !=  \" \": count += 1\n    return count\n\ndef count_token_length_special(token):\n    count = 0\n    special_token = [\"#\", \" \"]\n    for i in range(len(token)):\n        if token[i] not in special_token: count+=1\n    return count\n\ndef embed_by_bert(df):\n    \n    text = df['Text']\n    text.to_csv('input.txt', index=False, header=False)\n    os.system(\"python3 extract_features.py \\\n               --input_file=input.txt \\\n               --output_file=output.jsonl \\\n               --vocab_file=uncased_L-12_H-768_A-12/vocab.txt \\\n               --bert_config_file=uncased_L-12_H-768_A-12/bert_config.json \\\n               --init_checkpoint=uncased_L-12_H-768_A-12/bert_model.ckpt \\\n               --layers=-1 \\\n               --max_seq_length=256 \\\n               --batch_size=8\")\n    \n    bert_output = pd.read_json(\"output.jsonl\", lines = True)\n    bert_output.head()\n    \n    os.system(\"rm input.txt\")\n    os.system(\"rm output.jsonl\")\n    \n    index = df.index\n    columns = [\"emb_A\", \"emb_B\", \"emb_P\", \"label\"]\n    emb = pd.DataFrame(index = index, columns = columns)\n    emb.index.name = \"ID\"\n    \n    for i in tqdm(range(len(text))):\n        \n        features = bert_output.loc[i, \"features\"]\n        P_char_start = count_char(df.loc[i, 'Text'], df.loc[i, 'Pronoun-offset'])\n        A_char_start = count_char(df.loc[i, 'Text'], df.loc[i, 'A-offset'])\n        B_char_start = count_char(df.loc[i, 'Text'], df.loc[i, 'B-offset'])\n        A_length = candidate_length(df.loc[i, 'A'])\n        B_length = candidate_length(df.loc[i, 'B'])\n        \n        emb_A = np.zeros(768)\n        emb_B = np.zeros(768)\n        emb_P = np.zeros(768)\n        \n        char_count = 0\n        cnt_A, cnt_B = 0, 0\n        \n        for j in range(2, len(features)):\n            token = features[j][\"token\"]\n            token_length = count_token_length_special(token)\n            if char_count == P_char_start:\n                emb_P += np.asarray(features[j][\"layers\"][0]['values']) \n            if char_count in range(A_char_start, A_char_start+A_length):\n                emb_A += np.asarray(features[j][\"layers\"][0]['values'])\n                cnt_A += 1\n            if char_count in range(B_char_start, B_char_start+B_length):\n                emb_B += np.asarray(features[j][\"layers\"][0]['values'])\n                cnt_B += 1                \n            char_count += token_length\n        \n        emb_A /= cnt_A\n        emb_B /= cnt_B\n        \n        label = \"Neither\"\n        if (df.loc[i,\"A-coref\"] == True):\n            label = \"A\"\n        if (df.loc[i,\"B-coref\"] == True):\n            label = \"B\"\n\n        emb.iloc[i] = [emb_A, emb_B, emb_P, label]\n        \n    return emb     ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d2ca43cace4ee9eeea274df5c57ae5f5db04604","trusted":true},"cell_type":"code","source":"test_emb = embed_by_bert(test_df)\ntest_emb.to_json(\"contextual_embeddings_gap_test.json\", orient = 'columns')\nvalidation_emb = embed_by_bert(val_df)\nvalidation_emb.to_json(\"contextual_embeddings_gap_validation.json\", orient = 'columns')\ntrain_emb = embed_by_bert(train_df)\ntrain_emb.to_json(\"contextual_embeddings_gap_train.json\", orient = 'columns')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65d917c5ca0c0e308cf766f700488fcf183f748c","trusted":true},"cell_type":"code","source":"from keras.layers import *\nimport keras.backend as K\nfrom keras.models import *\nimport keras\nfrom keras import optimizers\nfrom keras import callbacks\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nclass End2End_NCR():\n    \n    def __init__(self, word_input_shape, dist_shape, embed_dim=20): \n        \n        self.word_input_shape = word_input_shape\n        self.dist_shape   = dist_shape\n        self.embed_dim    = embed_dim\n        self.buckets      = [1, 2, 3, 4, 5, 8, 16, 32, 64] \n        self.hidden_dim   = 150\n        \n    def build(self):\n        \n        A, B, P = Input((self.word_input_shape,)), Input((self.word_input_shape,)), Input((self.word_input_shape,))\n        dist1, dist2 = Input((self.dist_shape,)), Input((self.dist_shape,))\n        inputs = [A, B, P]\n        dist_inputs = [dist1, dist2]\n        \n        self.dist_embed = Embedding(len(self.buckets)+1, self.embed_dim, trainable=False)\n        self.ffnn       = Sequential([Dense(self.hidden_dim, use_bias=True),\n                                     Activation('relu'),\n                                     Dropout(rate=0.2, seed = 7),\n                                     Dense(1, activation='linear')])\n        \n        dist_embeds = [self.dist_embed(dist) for dist in dist_inputs]\n        dist_embeds = [Flatten()(dist_embed) for dist_embed in dist_embeds]\n        \n        #Scoring layer\n        #In https://www.aclweb.org/anthology/D17-1018, \n        #used feed forward network which measures if it is an entity mention using a score\n        #because we already know the word is mention.\n        #Here, I just focus on the pairwise score\n        PA = Multiply()([inputs[0], inputs[2]])\n        PB = Multiply()([inputs[1], inputs[2]])\n        # PairScore: sa(i,j) =wa·FFNNa([gi,gj,gi◦gj,φ(i,j)])\n        # gi is embedding of Pronoun\n        # gj is embedding of A or B\n        # gi◦gj is element-wise multiplication\n        # φ(i,j) is the distance embedding\n        PA = Concatenate(axis=-1)([P, A, PA, dist_embeds[0]])\n        PB = Concatenate(axis=-1)([P, B, PB, dist_embeds[1]])\n        PA_score = self.ffnn(PA)\n        PB_score = self.ffnn(PB)\n        # Fix the Neither to score 0.\n        score_e  = Lambda(lambda x: K.zeros_like(x))(PB_score)\n        \n        #Final Output\n        output = Concatenate(axis=-1)([PA_score, PB_score, score_e]) # [Pronoun and A score, Pronoun and B score, Neither Score]\n        output = Activation('softmax')(output)        \n        model = Model(inputs+dist_inputs, output)\n        \n        return model\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78ad73eabc65583a927c7f2ac721aa4551ae0992","trusted":true},"cell_type":"code","source":"def create_input(embed_df, dist_df):\n    \n    assert len(embed_df) == len(dist_df)\n    all_P, all_A, all_B = [] ,[] ,[]  # emb_P, emb_A, emb_B\n    all_label = [] # label\n    all_dist_PA, all_dist_PB = [], [] # D_PA,D_PB\n    url_A, url_B = [], []  #IN_URL\n    gender_A, gender_B = [], []  #G_FIT\n    \n    for i in tqdm(range(len(embed_df))):\n        \n        all_P.append(embed_df.loc[i, \"emb_P\"])\n        all_A.append(embed_df.loc[i, \"emb_A\"])\n        all_B.append(embed_df.loc[i, \"emb_B\"])\n        all_dist_PA.append(dist_df.loc[i, \"D_PA\"])\n        all_dist_PB.append(dist_df.loc[i, \"D_PB\"])\n        \n        if dist_df.loc[i, \"IN_URL\"] == 0:\n            url_A.append(1)\n            url_B.append(0)\n        elif dist_df.loc[i, \"IN_URL\"] == 1:\n            url_A.append(0)\n            url_B.append(1)\n        else:\n            url_A.append(0)\n            url_B.append(0)\n            \n        if dist_df.loc[i, \"G_FIT\"] == 0:\n            gender_A.append(1)\n            gender_B.append(0)\n        elif dist_df.loc[i, \"G_FIT\"] == 1:\n            gender_A.append(0)\n            gender_B.append(1)\n        else:\n            gender_A.append(0)\n            gender_B.append(0)\n        \n        label = embed_df.loc[i, \"label\"]\n        if label == \"A\": \n            all_label.append(0)\n        elif label == \"B\": \n            all_label.append(1)\n        else: \n            all_label.append(2)\n    \n    return [np.asarray(all_A), np.asarray(all_B), np.asarray(all_P),\n            np.expand_dims(np.asarray(all_dist_PA),axis=1),\n            np.expand_dims(np.asarray(all_dist_PB),axis=1)],all_label","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78cb5a82884acc0592330f65ac2df5e40408a747","trusted":true},"cell_type":"code","source":"new_emb_df = pd.concat([train_emb, validation_emb])\nnew_emb_df = new_emb_df.reset_index(drop=True)\nnew_dist_df = pd.concat([train_dist_df, val_dist_df])\nnew_dist_df = new_dist_df.reset_index(drop=True)\n\nnew_emb_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9eecf8cbabf3ebd4e98ba5c80c00b6681a0b4943","trusted":true},"cell_type":"code","source":"X_train, y_train = create_input(new_emb_df, new_dist_df)\nX_test, y_test = create_input(test_emb, test_dist_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bb874eb34b26635fbe65bc271a0acda9f9af237"},"cell_type":"code","source":"model = End2End_NCR(word_input_shape=X_train[0].shape[1], dist_shape=X_train[3].shape[1]).build()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73812eb91330412324d1da9eb641f672e43e0a59","trusted":true},"cell_type":"code","source":"SVG(model_to_dot(model).create(prog='dot', format='svg'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"727130aaa44085dec2282398e86762c818d1ae57","trusted":true},"cell_type":"code","source":"min_loss = 1.0\nbest_model = 0\n# Use Kfold to get best model\n\nfrom sklearn.model_selection import KFold\nn_fold = 5\nkfold = KFold(n_splits=n_fold, shuffle=True, random_state=3)\nfor fold_n, (train_index, valid_index) in enumerate(kfold.split(X_train[0])):\n    \n    X_tr  = [inputs[train_index] for inputs in X_train]\n    X_val = [inputs[valid_index] for inputs in X_train]\n    y_tr  = np.asarray(y_train)[train_index]\n    y_val = np.asarray(y_train)[valid_index]\n    \n    model = End2End_NCR(word_input_shape=X_train[0].shape[1], dist_shape=X_train[3].shape[1]).build()\n    \n    # compile the model\n    model.compile(optimizer=optimizers.Adam(lr=0.001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\",f1_m,precision_m, recall_m])\n#     model.compile(optimizer=optimizers.Adam(lr=0.001), loss=\"categorical_crossentropy\")\n    file_path = \"best_model_{}.hdf5\".format(fold_n+1)\n    check_point = callbacks.ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 0, save_best_only = True, mode = \"min\")\n    early_stop = callbacks.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience=100)\n    hist = model.fit(X_tr, y_tr, batch_size=128, epochs=1000, validation_data=(X_val, y_val), verbose=0,\n              shuffle=True, callbacks = [check_point, early_stop])\n    \n    if min(hist.history['val_loss']) < min_loss:\n        min_loss = min(hist.history['val_loss'])\n        best_model = fold_n + 1       \n    \n    # evaluate the model\n    loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=0)\n    print('---------------------------fold',fold_n,':----------------------------------')\n    print('loss:',loss)\n    print('accuracy:',accuracy)\n    print('f1_score:',f1_score)\n    print('precision:',precision)\n    print('recall:',recall)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fac0215cd009f4b1a9af60b0f795606a9924ab8"},"cell_type":"code","source":"del model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f16dbf78d029c6840415e69021a242bddfd3695","scrolled":true,"trusted":true},"cell_type":"code","source":"#Use best model to predict\nmodel = End2End_NCR(word_input_shape=X_train[0].shape[1], dist_shape=X_train[3].shape[1]).build()\nmodel.load_weights(\"./best_model_{}.hdf5\".format(best_model))\npred = model.predict(x = X_test, verbose = 0)\n\nsub_df = pd.DataFrame(columns = [\"ID\", \"A\", \"B\", \"NEITHER\"]) #generate a blank dataframe\nsub_df.loc[:, 'ID'] = test_df.loc[:, 'ID']\nsub_df.loc[:, 'A'] = pd.Series(pred[:, 0])\nsub_df.loc[:, 'B'] = pd.Series(pred[:, 1])\nsub_df.loc[:, 'NEITHER'] = pd.Series(pred[:, 2])\nsub_df.head","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b95c352c3847f2adeaf8153b4eaa46fab958096","trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\ny_one_hot = np.zeros((len(y_test), 3))\nfor i in range(len(y_test)):\n    y_one_hot[i, y_test[i]] = 1\nlog_loss(y_one_hot, pred) # Calculate the log loss ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"use the file below to do manual calculation of the evaluation metrics ","execution_count":null},{"metadata":{"_uuid":"d17c4e194ecfd465f20b465effe3b7a33038a6dc","trusted":true},"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}