{"cells":[{"metadata":{},"cell_type":"markdown","source":"Here we simply run logistic regression with BERT embeddings. Code for building BERT embeddings for A, B, and pronoun is taken from this great [Matei's Kernel](https://www.kaggle.com/mateiionita/taming-the-bert-a-baseline)."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport numpy as np\nimport pandas as pd \nfrom tqdm import tqdm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"!wget -q https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\n!wget -q https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv\n!wget -q https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv\n!wget -q https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n!wget -q https://raw.githubusercontent.com/google-research/bert/master/extract_features.py \n!wget -q https://raw.githubusercontent.com/google-research/bert/master/tokenization.py","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import modeling\nimport extract_features\nimport tokenization","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"6a15e5f4c32659d3408dee927035725ae0135a39","trusted":true},"cell_type":"code","source":"val_df = pd.read_table('gap-validation.tsv', index_col='ID').reset_index(drop=True)\ntest_df  = pd.read_table('gap-validation.tsv', index_col='ID').reset_index(drop=True)\ndev_df  = pd.read_table('gap-development.tsv', index_col='ID').reset_index(drop=True)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip \n!unzip uncased_L-12_H-768_A-12.zip","execution_count":5,"outputs":[{"output_type":"stream","text":"Archive:  uncased_L-12_H-768_A-12.zip\n   creating: uncased_L-12_H-768_A-12/\n  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n  inflating: uncased_L-12_H-768_A-12/bert_config.json  \nbert_config.json\t\t     bert_model.ckpt.index  vocab.txt\nbert_model.ckpt.data-00000-of-00001  bert_model.ckpt.meta\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_char(text, offset):   \n    count = 0\n    for pos in range(offset):\n        if text[pos] != \" \": count +=1\n    return count\n\ndef candidate_length(candidate):\n    count = 0\n    for i in range(len(candidate)):\n        if candidate[i] !=  \" \": count += 1\n    return count\n\ndef count_token_length_special(token):\n    count = 0\n    special_token = [\"#\", \" \"]\n    for i in range(len(token)):\n        if token[i] not in special_token: count+=1\n    return count\n\ndef embed_by_bert(df, path_to_bert='uncased_L-12_H-768_A-12', embed_size=768, batch_size=8,\n                 layers='-1', max_seq_length=256):\n    \n    text = df['Text']\n    text.to_csv('input.txt', index=False, header=False)\n    os.system(f\"python3 extract_features.py \\\n               --input_file=input.txt \\\n               --output_file=output.jsonl \\\n               --vocab_file={path_to_bert}/vocab.txt \\\n               --bert_config_file={path_to_bert}/bert_config.json \\\n               --init_checkpoint={path_to_bert}/bert_model.ckpt \\\n               --layers={layers} \\\n               --max_seq_length={max_seq_length} \\\n               --batch_size={batch_size}\")\n    \n    bert_output = pd.read_json(\"output.jsonl\", lines=True)\n    bert_output.head()\n    \n    os.system(\"rm input.txt\")\n    os.system(\"rm output.jsonl\")\n    \n    index = df.index\n    columns = [\"emb_A\", \"emb_B\", \"emb_P\", \"label\"]\n    emb = pd.DataFrame(index = index, columns = columns)\n    emb.index.name = \"ID\"\n    \n    for i in tqdm(range(len(text))):\n        \n        features = bert_output.loc[i, \"features\"]\n        P_char_start = count_char(df.loc[i, 'Text'], df.loc[i, 'Pronoun-offset'])\n        A_char_start = count_char(df.loc[i, 'Text'], df.loc[i, 'A-offset'])\n        B_char_start = count_char(df.loc[i, 'Text'], df.loc[i, 'B-offset'])\n        A_length = candidate_length(df.loc[i, 'A'])\n        B_length = candidate_length(df.loc[i, 'B'])\n        \n        emb_A, emb_B, emb_P = np.zeros(embed_size), np.zeros(embed_size), np.zeros(embed_size)\n        char_count, cnt_A, cnt_B = 0, 0, 0\n        \n        for j in range(2, len(features)):\n            token = features[j][\"token\"]\n            token_length = count_token_length_special(token)\n            if char_count == P_char_start:\n                emb_P += np.asarray(features[j][\"layers\"][0]['values']) \n            if char_count in range(A_char_start, A_char_start + A_length):\n                emb_A += np.asarray(features[j][\"layers\"][0]['values'])\n                cnt_A += 1\n            if char_count in range(B_char_start, B_char_start + B_length):\n                emb_B += np.asarray(features[j][\"layers\"][0]['values'])\n                cnt_B += 1                \n            char_count += token_length\n        \n        if cnt_A > 0:\n            emb_A /= cnt_A\n        if cnt_B > 0:\n            emb_B /= cnt_B\n        \n        label = \"Neither\"\n        if (df.loc[i,\"A-coref\"] == True):\n            label = \"A\"\n        if (df.loc[i,\"B-coref\"] == True):\n            label = \"B\"\n\n        emb.iloc[i] = [emb_A, emb_B, emb_P, label]\n        \n    return emb    ","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nval_bert_emb = embed_by_bert(val_df)\ntest_bert_emb = embed_by_bert(test_df)\ndev_bert_emb = embed_by_bert(dev_df)","execution_count":7,"outputs":[{"output_type":"stream","text":"100%|██████████| 454/454 [00:00<00:00, 937.31it/s]\n100%|██████████| 454/454 [00:00<00:00, 694.60it/s]\n100%|██████████| 2000/2000 [00:02<00:00, 862.42it/s]\n","name":"stderr"},{"output_type":"stream","text":"CPU times: user 32.3 s, sys: 15 s, total: 47.3 s\nWall time: 7min 4s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_bert_emb[\"emb_A\"].head().map(np.asarray).values[0].astype('float').shape","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"(768,)"},"metadata":{}}]},{"metadata":{"_uuid":"78ad73eabc65583a927c7f2ac721aa4551ae0992","trusted":true},"cell_type":"code","source":"def featurize(embedding_df):\n    \n    pronoun_embs, a_embs, b_embs, labels = [], [], [], []\n    \n    for i in tqdm(range(len(embedding_df))):\n        \n        pronoun_embs.append(embedding_df.loc[i, \"emb_P\"])\n        a_embs.append(embedding_df.loc[i, \"emb_A\"])\n        b_embs.append(embedding_df.loc[i, \"emb_B\"])\n\n        label_map = {'A': 0, 'B': 1, 'Neither': 2}\n        labels.append(label_map[embedding_df.loc[i, \"label\"]])\n\n    \n    a_embs = np.asarray(a_embs).astype('float')\n    b_embs = np.asarray(b_embs).astype('float') \n    pronoun_embs = np.asarray(pronoun_embs).astype('float')\n    \n    return np.concatenate([a_embs, b_embs, pronoun_embs], axis=1), np.asarray(labels)","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"9eecf8cbabf3ebd4e98ba5c80c00b6681a0b4943","trusted":true},"cell_type":"code","source":"X_train, y_train = featurize(pd.concat([val_bert_emb, dev_bert_emb]).sort_index().reset_index())","execution_count":29,"outputs":[{"output_type":"stream","text":"100%|██████████| 2454/2454 [00:00<00:00, 27859.32it/s]\n","name":"stderr"}]},{"metadata":{"_uuid":"4bb874eb34b26635fbe65bc271a0acda9f9af237","trusted":true},"cell_type":"code","source":"X_train.shape, y_train.shape","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"((2454, 2304), (2454,))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit = LogisticRegression(C=1e-2, random_state=17, solver='lbfgs', \n                           multi_class='multinomial', max_iter=100,\n                          n_jobs=4)","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlogit.fit(X_train, y_train)","execution_count":32,"outputs":[{"output_type":"stream","text":"CPU times: user 64 ms, sys: 340 ms, total: 404 ms\nWall time: 23.1 s\n","name":"stdout"},{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n          n_jobs=4, penalty='l2', random_state=17, solver='lbfgs',\n          tol=0.0001, verbose=0, warm_start=False)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Prediction for stage 1 test set "},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp gap-development.tsv stage1_test.tsv","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stage1_test_df  = pd.read_table('stage1_test.tsv', index_col='ID').reset_index(drop=True)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nstage1_test_bert_emb = embed_by_bert(stage1_test_df)","execution_count":35,"outputs":[{"output_type":"stream","text":"100%|██████████| 2000/2000 [00:02<00:00, 850.74it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test, y_test = featurize(stage1_test_bert_emb)","execution_count":36,"outputs":[{"output_type":"stream","text":"100%|██████████| 2000/2000 [00:00<00:00, 27172.13it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit_test_pred = logit.predict_proba(X_test)\nlog_loss(y_test, logit_test_pred)","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"0.2775730991751326"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Write the prediction to file for submission\nsubmission = pd.read_csv(\"../input/sample_submission_stage_1.csv\", index_col = \"ID\")\nsubmission[\"A\"] = logit_test_pred[:, 0]\nsubmission[\"B\"] = logit_test_pred[:, 1]\nsubmission[\"NEITHER\"] = logit_test_pred[:, 2]\nsubmission.to_csv(\"submission.csv\")","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}