{"cells":[{"metadata":{"trusted":true,"_uuid":"eef73f480afe9a4b485f89361916b7469ed986ea"},"cell_type":"markdown","source":"This is not my work.  It was forked from the pytorch bert baseline with .54 score.\n\nI have messed around with epochs, weight decay, and adding another fully connected layer in the head."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-development.tsv -q\n!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-test.tsv -q\n!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-validation.tsv -q","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"98b461a0d1e2d27558502f9caefeaf7e47871efc"},"cell_type":"markdown","source":"\"pytorch_helper_bot\" is a thin abstraction of some common PyTorch training routines. It can easily be replaced, so you can mostly ignore it and focus on the preprocessing and model definition instead."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"!pip install pytorch-pretrained-bert\n!pip install https://github.com/ceshine/pytorch_helper_bot/archive/0.0.4.zip","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting pytorch-pretrained-bert\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\n\u001b[K    100% |████████████████████████████████| 122kB 3.9MB/s ta 0:00:01\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2.21.0)\nRequirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.0.1.post2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.16.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (4.31.1)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.9.118)\nRequirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2018.1.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2.6)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (1.22)\nRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\nRequirement already satisfied: botocore<1.13.0,>=1.12.118 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (1.12.118)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\nRequirement already satisfied: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.118->boto3->pytorch-pretrained-bert) (0.14)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.118->boto3->pytorch-pretrained-bert) (2.6.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.118->boto3->pytorch-pretrained-bert) (1.12.0)\nInstalling collected packages: pytorch-pretrained-bert\nSuccessfully installed pytorch-pretrained-bert-0.6.1\nCollecting https://github.com/ceshine/pytorch_helper_bot/archive/0.0.4.zip\n  Downloading https://github.com/ceshine/pytorch_helper_bot/archive/0.0.4.zip\n\u001b[K     | 112kB 7.5MB/s\nRequirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from PyTorchHelperBot==0.0.4) (1.0.1.post2)\nBuilding wheels for collected packages: PyTorchHelperBot\n  Building wheel for PyTorchHelperBot (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-xbh8e6yi/wheels/1f/01/01/da39a14e8e30666f3eec7106664e59059789c330a11b5fa357\nSuccessfully built PyTorchHelperBot\nInstalling collected packages: PyTorchHelperBot\nSuccessfully installed PyTorchHelperBot-0.0.4\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"4e128e4337fd5c906540c112bc1d4e0fd2f38ef3"},"cell_type":"code","source":"import os\n\n# This variable is used by helperbot to make the training deterministic\nos.environ[\"SEED\"] = \"420\"\n\nimport logging\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel\n\nfrom helperbot import BaseBot, TriangularLR, WeightDecayOptimizerWrapper\n","execution_count":3,"outputs":[{"output_type":"stream","text":"Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"4209e502d9d0c58575d71a7580cabc66bbf7ff70"},"cell_type":"code","source":"BERT_MODEL = 'bert-large-uncased'\nCASED = False","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d3c64ff3a19456ee88ef77825b83690e5907475"},"cell_type":"code","source":"def insert_tag(row):\n    \"\"\"Insert custom tags to help us find the position of A, B, and the pronoun after tokenization.\"\"\"\n    to_be_inserted = sorted([\n        (row[\"A-offset\"], \" [A] \"),\n        (row[\"B-offset\"], \" [B] \"),\n        (row[\"Pronoun-offset\"], \" [P] \")\n    ], key=lambda x: x[0], reverse=True)\n    text = row[\"Text\"]\n    for offset, tag in to_be_inserted:\n        text = text[:offset] + tag + text[offset:]\n    return text\n\ndef tokenize(text, tokenizer):\n    \"\"\"Returns a list of tokens and the positions of A, B, and the pronoun.\"\"\"\n    entries = {}\n    final_tokens = []\n    for token in tokenizer.tokenize(text):\n        if token in (\"[A]\", \"[B]\", \"[P]\"):\n            entries[token] = len(final_tokens)\n            continue\n        final_tokens.append(token)\n    return final_tokens, (entries[\"[A]\"], entries[\"[B]\"], entries[\"[P]\"])\n\nclass GAPDataset(Dataset):\n    \"\"\"Custom GAP Dataset class\"\"\"\n    def __init__(self, df, tokenizer, labeled=True):\n        self.labeled = labeled\n        if labeled:\n            tmp = df[[\"A-coref\", \"B-coref\"]].copy()\n            tmp[\"Neither\"] = ~(df[\"A-coref\"] | df[\"B-coref\"])\n            self.y = tmp.values.astype(\"bool\")\n        # Extracts the tokens and offsets(positions of A, B, and P)\n        self.offsets, self.tokens = [], []\n        for _, row in df.iterrows():\n            text = insert_tag(row)\n            tokens, offsets = tokenize(text, tokenizer)\n            self.offsets.append(offsets)\n            self.tokens.append(tokenizer.convert_tokens_to_ids(\n                [\"[CLS]\"] + tokens + [\"[SEP]\"]))\n        \n    def __len__(self):\n        return len(self.tokens)\n\n    def __getitem__(self, idx):\n        if self.labeled:\n            return self.tokens[idx], self.offsets[idx], self.y[idx]\n        return self.tokens[idx], self.offsets[idx], None\n    \ndef collate_examples(batch, truncate_len=500):\n    \"\"\"Batch preparation.\n    \n    1. Pad the sequences\n    2. Transform the target.\n    \"\"\"\n    transposed = list(zip(*batch))\n    max_len = min(\n        max((len(x) for x in transposed[0])),\n        truncate_len\n    )\n    tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n    for i, row in enumerate(transposed[0]):\n        row = np.array(row[:truncate_len])\n        tokens[i, :len(row)] = row\n    token_tensor = torch.from_numpy(tokens)\n    # Offsets\n    offsets = torch.stack([\n        torch.LongTensor(x) for x in transposed[1]\n    ], dim=0) + 1 # Account for the [CLS] token\n    # Labels\n    if len(transposed) == 2:\n        return token_tensor, offsets, None\n    one_hot_labels = torch.stack([\n        torch.from_numpy(x.astype(\"uint8\")) for x in transposed[2]\n    ], dim=0)\n    _, labels = one_hot_labels.max(dim=1)\n    return token_tensor, offsets, labels\n\nclass Head(nn.Module):\n    \"\"\"The MLP submodule\"\"\"\n    def __init__(self, bert_hidden_size: int, head_hidden_size:int = 3^6, num_blocks:int = 5):\n        super().__init__()\n        self.bert_hidden_size = bert_hidden_size\n        fca = ([\n            nn.Linear(self.bert_hidden_size * 3, head_hidden_size),\n            nn.BatchNorm1d(head_hidden_size),\n            nn.ReLU(),\n        ])\n        for block in range(num_blocks):\n            fca.append(nn.Linear(head_hidden_size, head_hidden_size))\n            fca.append(nn.BatchNorm1d(head_hidden_size))\n            fca.append(nn.ReLU())\n        fca.append(nn.Linear(head_hidden_size, 3))\n        self.fc = nn.Sequential(*fca)\n        for i, module in enumerate(self.fc):\n            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                nn.init.constant_(module.weight, 1)\n                nn.init.constant_(module.bias, 0)\n                print(\"Initing batchnorm\")\n            elif isinstance(module, nn.Linear):\n                if getattr(module, \"weight_v\", None) is not None:\n                    nn.init.uniform_(module.weight_g, 0, 1)\n                    nn.init.kaiming_normal_(module.weight_v)\n                    print(\"Initing linear with weight normalization\")\n                    assert model[i].weight_g is not None\n                else:\n                    nn.init.kaiming_normal_(module.weight)\n                    print(\"Initing linear\")\n                nn.init.constant_(module.bias, 0)\n                \n    def forward(self, bert_outputs, offsets):\n        assert bert_outputs.size(2) == self.bert_hidden_size\n        extracted_outputs = bert_outputs.gather(\n            1, offsets.unsqueeze(2).expand(-1, -1, bert_outputs.size(2))\n        ).view(bert_outputs.size(0), -1)\n        return self.fc(extracted_outputs)\n\n    \nclass GAPModel(nn.Module):\n    \"\"\"The main model.\"\"\"\n    def __init__(self, bert_model: str, device: torch.device):\n        super().__init__()\n        self.device = device\n        if bert_model in (\"bert-base-uncased\", \"bert-base-cased\"):\n            self.bert_hidden_size = 768\n        elif bert_model in (\"bert-large-uncased\", \"bert-large-cased\"):\n            self.bert_hidden_size = 1024\n        else:\n            raise ValueError(\"Unsupported BERT model.\")\n        self.bert = BertModel.from_pretrained(bert_model).to(device)\n        self.head = Head(self.bert_hidden_size).to(device)\n    \n    def forward(self, token_tensor, offsets):\n        token_tensor = token_tensor.to(self.device)\n        bert_outputs, _ =  self.bert(\n            token_tensor, attention_mask=(token_tensor > 0).long(), \n            token_type_ids=None, output_all_encoded_layers=False)\n        head_outputs = self.head(bert_outputs, offsets.to(self.device))\n        return head_outputs            \n\n    \ndef children(m):\n    return m if isinstance(m, (list, tuple)) else list(m.children())\n\n\ndef set_trainable_attr(m, b):\n    m.trainable = b\n    for p in m.parameters():\n        p.requires_grad = b\n\n\ndef apply_leaf(m, f):\n    c = children(m)\n    if isinstance(m, nn.Module):\n        f(m)\n    if len(c) > 0:\n        for l in c:\n            apply_leaf(l, f)\n\n            \ndef set_trainable(l, b):\n    apply_leaf(l, lambda m: set_trainable_attr(m, b))\n    \n    \nclass GAPBot(BaseBot):\n    def __init__(self, model, train_loader, val_loader, *, optimizer, clip_grad=0,\n        avg_window=100, log_dir=\"./cache/logs/\", log_level=logging.INFO,\n        checkpoint_dir=\"./cache/model_cache/\", batch_idx=0, echo=False,\n        device=\"cuda:0\", use_tensorboard=False):\n        super().__init__(\n            model, train_loader, val_loader, \n            optimizer=optimizer, clip_grad=clip_grad,\n            log_dir=log_dir, checkpoint_dir=checkpoint_dir, \n            batch_idx=batch_idx, echo=echo,\n            device=device, use_tensorboard=use_tensorboard\n        )\n        self.criterion = torch.nn.CrossEntropyLoss()\n        self.loss_format = \"%.6f\"\n        \n    def extract_prediction(self, tensor):\n        return tensor\n    \n    def snapshot(self):\n        \"\"\"Override the snapshot method because Kaggle kernel has limited local disk space.\"\"\"\n        loss = self.eval(self.val_loader)\n        loss_str = self.loss_format % loss\n        self.logger.info(\"Snapshot loss %s\", loss_str)\n        self.logger.tb_scalars(\n            \"losses\", {\"val\": loss},  self.step)\n        target_path = (\n            self.checkpoint_dir / \"best.pth\")        \n        if not self.best_performers or (self.best_performers[0][0] > loss):\n            torch.save(self.model.state_dict(), target_path)\n            self.best_performers = [(loss, target_path, self.step)]\n            self.logger.info(\"Saving checkpoint %s...\", target_path)\n        else:\n            new_loss_str = self.loss_format % self.best_performers[0][0]\n            self.logger.info(\"This performance:%s is not as a good as our previously saved:%s\", loss_str,new_loss_str )\n        assert Path(target_path).exists()\n        return loss","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d534c85ff69192b4dd1ec670fc9c2b9392cc7a62"},"cell_type":"code","source":"df_train = pd.read_csv(\"gap-test.tsv\", delimiter=\"\\t\")\ndf_val = pd.read_csv(\"gap-validation.tsv\", delimiter=\"\\t\")\ndf_test = pd.read_csv(\"gap-development.tsv\", delimiter=\"\\t\")\nsample_sub = pd.read_csv(\"../input/sample_submission_stage_1.csv\")\nassert sample_sub.shape[0] == df_test.shape[0]","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d58d18c34f5df9ec8f8d8fb048ae6c10fbf9914a"},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL,\n    do_lower_case=CASED,\n    never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\", \"[A]\", \"[B]\", \"[P]\")\n)\n# These tokens are not actually used, so we can assign arbitrary values.\ntokenizer.vocab[\"[A]\"] = -1\ntokenizer.vocab[\"[B]\"] = -1\ntokenizer.vocab[\"[P]\"] = -1","execution_count":7,"outputs":[{"output_type":"stream","text":"100%|██████████| 231508/231508 [00:00<00:00, 6190762.65B/s]\n","name":"stderr"}]},{"metadata":{"trusted":true,"_uuid":"69689365738454b33649a14d83eea49cc1b18687"},"cell_type":"code","source":"train_ds = GAPDataset(df_train, tokenizer)\nval_ds = GAPDataset(df_val, tokenizer)\ntest_ds = GAPDataset(df_test, tokenizer)\ntrain_loader = DataLoader(\n    train_ds,\n    collate_fn = collate_examples,\n    batch_size=20,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=True,\n    drop_last=True\n)\nval_loader = DataLoader(\n    val_ds,\n    collate_fn = collate_examples,\n    batch_size=128,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)\ntest_loader = DataLoader(\n    test_ds,\n    collate_fn = collate_examples,\n    batch_size=128,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a85f87ed8d73b520e39a5dc07da1838867ac2653"},"cell_type":"code","source":"model = GAPModel(BERT_MODEL, torch.device(\"cuda:0\"))\n# You can unfreeze the last layer of bert by calling set_trainable(model.bert.encoder.layer[23], True)\nset_trainable(model.bert, False)\nset_trainable(model.head, True)","execution_count":9,"outputs":[{"output_type":"stream","text":"100%|██████████| 1248501532/1248501532 [00:25<00:00, 48194149.42B/s]\n","name":"stderr"},{"output_type":"stream","text":"Initing linear\nIniting batchnorm\nIniting linear\nIniting batchnorm\nIniting linear\nIniting batchnorm\nIniting linear\nIniting batchnorm\nIniting linear\nIniting batchnorm\nIniting linear\nIniting batchnorm\nIniting linear\nIniting batchnorm\nIniting linear\nIniting batchnorm\nIniting linear\nIniting batchnorm\nIniting linear\nIniting batchnorm\nIniting linear\nIniting batchnorm\nIniting linear\nIniting batchnorm\nIniting linear\nIniting batchnorm\nIniting linear\nIniting batchnorm\nIniting linear\nIniting batchnorm\nIniting linear\nIniting batchnorm\nIniting linear\nIniting batchnorm\nIniting linear\nIniting batchnorm\nIniting linear\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"493b0ed0887339dfe818df1a0be17c05a4c97d17"},"cell_type":"code","source":"lr=1e-4\nweight_decay=5e-5\noptimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n\nbot = GAPBot(\n    model, train_loader, val_loader,\n    optimizer=optimizer, echo=True,\n    avg_window=25\n)","execution_count":10,"outputs":[{"output_type":"stream","text":"[[04/01/2019 09:08:02 PM]] SEED: 420\n[[04/01/2019 09:08:02 PM]] # of paramters: 336,906,071\n[[04/01/2019 09:08:02 PM]] # of trainable paramters: 1,764,183\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07ea447ea766df3d997779e6c9a8300b7532a049"},"cell_type":"code","source":"steps_per_epoch = len(train_loader) \nn_steps = steps_per_epoch * 27\nbot.train(\n    n_steps,\n    log_interval=steps_per_epoch // 4,\n    snapshot_interval=steps_per_epoch,\n    scheduler=TriangularLR(\n        optimizer, 20, ratio=3, steps_per_cycle=n_steps)\n)","execution_count":11,"outputs":[{"output_type":"stream","text":"[[04/01/2019 09:08:02 PM]] Optimizer Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    initial_lr: 0.0001\n    lr: 5e-06\n    weight_decay: 5e-05\n)\n[[04/01/2019 09:08:02 PM]] Batches per epoch: 100\n[[04/01/2019 09:08:02 PM]] ====================Epoch 1====================\n[[04/01/2019 09:08:12 PM]] Step 25: train 1.138716 lr: 1.471e-05\n[[04/01/2019 09:08:22 PM]] Step 50: train 1.127204 lr: 2.527e-05\n[[04/01/2019 09:08:31 PM]] Step 75: train 1.136880 lr: 3.582e-05\n[[04/01/2019 09:08:39 PM]] Step 100: train 1.134579 lr: 4.638e-05\n100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n[[04/01/2019 09:08:48 PM]] Snapshot loss 1.112209\n[[04/01/2019 09:08:50 PM]] Saving checkpoint cache/model_cache/best.pth...\n[[04/01/2019 09:08:50 PM]] New low\n\n[[04/01/2019 09:08:50 PM]] ====================Epoch 2====================\n[[04/01/2019 09:09:01 PM]] Step 125: train 1.130063 lr: 5.693e-05\n[[04/01/2019 09:09:09 PM]] Step 150: train 1.123101 lr: 6.749e-05\n[[04/01/2019 09:09:19 PM]] Step 175: train 1.121119 lr: 7.804e-05\n[[04/01/2019 09:09:28 PM]] Step 200: train 1.114836 lr: 8.860e-05\n100%|██████████| 4/4 [00:08<00:00,  2.09s/it]\n[[04/01/2019 09:09:37 PM]] Snapshot loss 1.068508\n[[04/01/2019 09:09:39 PM]] Saving checkpoint cache/model_cache/best.pth...\n[[04/01/2019 09:09:39 PM]] New low\n\n[[04/01/2019 09:09:39 PM]] ====================Epoch 3====================\n[[04/01/2019 09:09:48 PM]] Step 225: train 1.110955 lr: 9.916e-05\n[[04/01/2019 09:09:57 PM]] Step 250: train 1.105146 lr: 9.676e-05\n[[04/01/2019 09:10:06 PM]] Step 275: train 1.100499 lr: 9.324e-05\n[[04/01/2019 09:10:16 PM]] Step 300: train 1.094251 lr: 8.973e-05\n100%|██████████| 4/4 [00:08<00:00,  2.09s/it]\n[[04/01/2019 09:10:25 PM]] Snapshot loss 1.061945\n[[04/01/2019 09:10:27 PM]] Saving checkpoint cache/model_cache/best.pth...\n[[04/01/2019 09:10:27 PM]] New low\n\n[[04/01/2019 09:10:27 PM]] ====================Epoch 4====================\n[[04/01/2019 09:10:36 PM]] Step 325: train 1.084254 lr: 8.621e-05\n[[04/01/2019 09:10:46 PM]] Step 350: train 1.081818 lr: 8.269e-05\n[[04/01/2019 09:10:55 PM]] Step 375: train 1.070902 lr: 7.917e-05\n[[04/01/2019 09:11:05 PM]] Step 400: train 1.065651 lr: 7.565e-05\n100%|██████████| 4/4 [00:08<00:00,  2.09s/it]\n[[04/01/2019 09:11:14 PM]] Snapshot loss 1.011921\n[[04/01/2019 09:11:16 PM]] Saving checkpoint cache/model_cache/best.pth...\n[[04/01/2019 09:11:16 PM]] New low\n\n[[04/01/2019 09:11:16 PM]] ====================Epoch 5====================\n[[04/01/2019 09:11:26 PM]] Step 425: train 1.059356 lr: 7.213e-05\n[[04/01/2019 09:11:35 PM]] Step 450: train 1.055791 lr: 6.861e-05\n[[04/01/2019 09:11:44 PM]] Step 475: train 1.051292 lr: 6.510e-05\n[[04/01/2019 09:11:53 PM]] Step 500: train 1.049854 lr: 6.158e-05\n100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n[[04/01/2019 09:12:02 PM]] Snapshot loss 1.083111\n[[04/01/2019 09:12:02 PM]] This performance:1.083111 is not as a good as our previously saved:1.011921\n[[04/01/2019 09:12:02 PM]] ====================Epoch 6====================\n[[04/01/2019 09:12:12 PM]] Step 525: train 1.044622 lr: 5.806e-05\n[[04/01/2019 09:12:21 PM]] Step 550: train 1.048463 lr: 5.454e-05\n[[04/01/2019 09:12:29 PM]] Step 575: train 1.047266 lr: 5.102e-05\n[[04/01/2019 09:12:39 PM]] Step 600: train 1.045621 lr: 4.750e-05\n100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n[[04/01/2019 09:12:48 PM]] Snapshot loss 1.057476\n[[04/01/2019 09:12:48 PM]] This performance:1.057476 is not as a good as our previously saved:1.011921\n[[04/01/2019 09:12:48 PM]] ====================Epoch 7====================\n[[04/01/2019 09:12:57 PM]] Step 625: train 1.048600 lr: 4.399e-05\n[[04/01/2019 09:13:07 PM]] Step 650: train 1.045020 lr: 4.047e-05\n[[04/01/2019 09:13:16 PM]] Step 675: train 1.044955 lr: 3.695e-05\n[[04/01/2019 09:13:25 PM]] Step 700: train 1.039647 lr: 3.343e-05\n100%|██████████| 4/4 [00:08<00:00,  2.09s/it]\n[[04/01/2019 09:13:34 PM]] Snapshot loss 1.067111\n[[04/01/2019 09:13:34 PM]] This performance:1.067111 is not as a good as our previously saved:1.011921\n[[04/01/2019 09:13:34 PM]] ====================Epoch 8====================\n[[04/01/2019 09:13:43 PM]] Step 725: train 1.040916 lr: 2.991e-05\n[[04/01/2019 09:13:53 PM]] Step 750: train 1.035095 lr: 2.639e-05\n[[04/01/2019 09:14:02 PM]] Step 775: train 1.032515 lr: 2.287e-05\n[[04/01/2019 09:14:11 PM]] Step 800: train 1.026480 lr: 1.936e-05\n100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n[[04/01/2019 09:14:20 PM]] Snapshot loss 1.076127\n[[04/01/2019 09:14:20 PM]] This performance:1.076127 is not as a good as our previously saved:1.011921\n[[04/01/2019 09:14:20 PM]] ====================Epoch 9====================\n[[04/01/2019 09:14:29 PM]] Step 825: train 1.027641 lr: 1.584e-05\n[[04/01/2019 09:14:38 PM]] Step 850: train 1.019137 lr: 1.232e-05\n[[04/01/2019 09:14:48 PM]] Step 875: train 1.022423 lr: 8.800e-06\n[[04/01/2019 09:14:57 PM]] Step 900: train 1.023400 lr: 5.281e-06\n100%|██████████| 4/4 [00:08<00:00,  2.09s/it]\n[[04/01/2019 09:15:06 PM]] Snapshot loss 1.070170\n[[04/01/2019 09:15:06 PM]] This performance:1.070170 is not as a good as our previously saved:1.011921\n","name":"stderr"}]},{"metadata":{"trusted":true,"_uuid":"d92655729f678dcbe93a5ee824562b0d23fe275f"},"cell_type":"code","source":"# Load the best checkpoint\nbot.load_model(bot.best_performers[0][1])","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efad23ae0f411fe486e837be0903dd24cab6cba5"},"cell_type":"code","source":"# Evaluate on the test dataset\nbot.eval(test_loader)","execution_count":13,"outputs":[{"output_type":"stream","text":"100%|██████████| 16/16 [00:40<00:00,  2.03s/it]\n","name":"stderr"},{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"1.011976761817932"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"e3b1bc4a4264ebda30d4eca35879f1df6f0a11c3"},"cell_type":"code","source":"# Extract predictions to the test dataset\npreds = bot.predict(test_loader)","execution_count":null,"outputs":[{"output_type":"stream","text":" 44%|████▍     | 7/16 [00:19<00:23,  2.63s/it]","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom torch.autograd import Variable\n#make_dot(model(test_ds), params=dict(model.named_parameters()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def exx(row):\n    if(row.A > row.B and row.A > row.NEITHER):\n          row.A = 1\n          row.B = 0\n          row.NEITHER = 0    \n    elif(row.B > row.A and row.B > row.NEITHER):\n          row.A = 0\n          row.B = 1\n          row.NEITHER = 0 \n    else:\n          row.A = 0\n          row.B = 0\n          row.NEITHER = 1\n    return row","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51b7e2335c8f9c1b821b6aeaba7c5122fe74530a"},"cell_type":"code","source":"# Create submission file\ndf_sub = pd.DataFrame(torch.softmax(preds, -1).cpu().numpy().clip(1e-3, 1-1e-3), columns=[\"A\", \"B\", \"NEITHER\"])\ndf_sub[\"ID\"] = df_test.ID\ndf_sub.to_csv(\"submission.csv\", index=False)\ndf_sub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}