{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Preparing GAP for BERT: Input Layer"},{"metadata":{},"cell_type":"markdown","source":"This kernel only shows how character offsets are converted to BERT token offsets for explanatory purposes. A full repository of the software to fine tune BERT and do predictions for this dataset is available [here.](https://github.com/kenkrige/BERT-Fine-tune-for-GAP)"},{"metadata":{},"cell_type":"markdown","source":"For a short working example, I've chosen something I might hear from one of my school students:\n> \"I saw Oratile slap Rorisang on her left shoulder.\""},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"example = \"example-1\tI saw Oratile slap Rorisang on her left shoulder.\ther\t31\tOratile\t6\tFalse\tRorisang\t19\tTrue\"\nline = example.split('\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_seq_length = 12 #For the cometition, I used 64 and 128, but keeping it very short for clarity.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = line[1]\nP_offset = int(line[3])\nA_offset = int(line[5])\nB_offset = int(line[8])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make an array of the char offsets, including an index column which will be used to remember positions of P, A, B after sorting by offset. Then sort by the offset column."},{"metadata":{"trusted":true},"cell_type":"code","source":"char_off = sorted([\n  [P_offset, 0],\n  [A_offset, 1],\n  [B_offset, 2]\n], key=lambda x: x[0])\nchar_off","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the offsets in the first column to split the text into 4 segments. This technique assists the accurate conversion of character offsets to token offsets without the need for any whitespace tokenization."},{"metadata":{"trusted":true},"cell_type":"code","source":"text_segments = [text[:char_off[0][0]], \ntext[char_off[0][0]:char_off[1][0]], \ntext[char_off[1][0]:char_off[2][0]], \ntext[char_off[2][0]:]]\ntext_segments","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use BERT Wordpiece to tokenize each segment."},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp ../input/bertfiles/tokenization.py .\nimport tokenization\ntokenizer = tokenization.FullTokenizer(vocab_file='../input/bertfiles/vocab.txt', do_lower_case=True)\ntoken_segments = []\nnum_tokens = []\nfor segment in text_segments:\n    token_segment = tokenizer.tokenize(segment)\n    token_segments.append(token_segment)\n    num_tokens.append(len(token_segment))\ntoken_segments","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Truncate by removing one token at a time until the number of tokens is two less than the maximum sequence length. The extra two allow for BERT's start and end tokens. Each time remove the furthest token from any offset point."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nwhile np.sum(num_tokens) > (max_seq_length - 2):\n    index = np.argmax([num_tokens[0] * 2, num_tokens[1], num_tokens[2], num_tokens[3] * 2])\n    if index == 0:\n        token_segments[index] = token_segments[index][1:]\n    elif index == 3:\n        token_segments[index] = token_segments[index][:-1]\n    else: #middle segments\n        middle = num_tokens[index] // 2\n        token_segments[index] = token_segments[index][:middle] + token_segments[index][middle + 1:]\n    num_tokens[index] -= 1\ntoken_segments","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Concatenate the segments back together."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = []\ntokens.append(\"[CLS]\")\nfor segment in token_segments:\n    temp = ''\n    for token in segment:\n        tokens.append(token)\ntokens.append(\"[SEP]\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Replace the char offsets with token offsets, using the lengths of token segments cumulatively. Then sort offsets on column 2 back to the original order of P, A, B."},{"metadata":{"trusted":true},"cell_type":"code","source":"offset = 1 #to account for \"[CLS]\"\nfor i, row in enumerate(char_off):\n    offset += num_tokens[i]\n    row[0] = offset\n\ntoken_off = sorted(char_off, key=lambda x: x[1])\ntoken_off","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Makle the position masks for P, A, B"},{"metadata":{"trusted":true},"cell_type":"code","source":"P_mask = [0] * max_seq_length\nA_mask = [0] * max_seq_length\nB_mask = [0] * max_seq_length\n\nP_mask[token_off[0][0]] = 1\nA_mask[token_off[1][0]] = 1\nB_mask[token_off[2][0]] = 1\n\nprint(P_mask)\nprint(A_mask)\nprint(B_mask)\nprint(tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Those are the inputs to the model. The masks are forwarded to the output layer and the sentence of tokens is processed by the BERT hidden layers."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}