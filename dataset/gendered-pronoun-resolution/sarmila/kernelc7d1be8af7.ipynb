{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport re\nprint(os.listdir(\"../input\"))\n\nimport spacy\nimport networkx as nx\n\nimport zipfile\n\nsample_submission = pd.read_csv(\"../input/gendered-pronoun-resolution/sample_submission_stage_1.csv\")\nfinal_test = pd.read_csv(\"../input/gendered-pronoun-resolution/test_stage_2.tsv\", sep = \"\\t\")\nnlp = spacy.load('en_core_web_sm')\ndep = [\"ACL\", \"ACOMP\", \"ADVCL\", \"ADVMOD\", \"AGENT\", \"AMOD\", \"APPOS\", \"ATTR\", \"AUX\", \"AUXPASS\",\n       \"CASE\", \"CC\", \"CCOMP\", \"COMPOUND\", \"CONJ\", \"CSUBJ\", \"CSUBJPASS\", \"DATIVE\", \"DEP\", \"DET\", \"DOBJ\"\n     , \"EXPL\", \"INTJ\", \"MARK\", \"META\", \"NEG\", \"NOUNMOD\", \"NPMOD\", \"NSUBJ\", \"NSUBJPASS\", \"NUMMOD\"\n     , \"OPRD\", \"PARATAXIS\", \"PCOMP\", \"POBJ\", \"POSS\", \"PRECONJ\", \"PREDET\", \"PREP\", \"PRT\", \"PUNCT\", \"QUANTMOD\",\n       \"RELCL\", \"ROOT\", \"XCOMP\", \"COMPLM\",\"INFMOD\",\"PARTMOD\",\"HMOD\",\"HYPH\",\"IOBJ\",\"NUM\",\n       \"NUMBER\",\"NMOD\",\"NN\",\"NPADVMOD\",\"POSSESSIVE\",\"RCMOD\",\"SUBTOK\"]\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# downloading test, train and validation data from github\n!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\n!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv\n!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv\n!ls\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"gap-development.tsv\", sep = \"\\t\")\nvalidation_data = pd.read_csv(\"gap-validation.tsv\", sep = \"\\t\")\ntest_data = pd.read_csv(\"gap-test.tsv\", sep = \"\\t\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmerge_data = pd.concat([train_data,validation_data]).reset_index(drop = True)\nmerge_data = pd.concat([merge_data,train_data]).reset_index(drop = True)\ncount = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef name_replace(s, r1, r2):\n    s = str(s).replace(r1,r2)\n    for r3 in r1.split(' '):\n        s = str(s).replace(r3,r2)\n    return s\ndef shortest_dependency_path(doc, e1=None, e2=None):\n    \n    edges = []\n    for token in doc:\n        for child in token.children:\n            edges.append(('{0}'.format(token),\n                          '{0}'.format(child)))\n    graph = nx.Graph(edges)\n    try:\n        shortest_path = nx.shortest_path(graph, source=e1, target=e2)\n    except Exception as e:\n        shortest_path = [e1, e2]\n        print(e)\n        print(doc, e1, e2)\n\n    return shortest_path\n\ndef dependency_vector(doc, pronoun, word):\n    \n    vector = [0] * 59\n#     for token in doc:\n#         if token.text == pronoun:\n#             pi = token.i\n#         elif token.text == word:\n#             wi = token.i\n#     if pi>wi:\n#         for token in doc[wi:pi+1]:\n#             index = dep.index(token.dep_.upper())\n#             vector[index] = 1\n\n#     else:\n#         for token in doc[pi:wi+1]:\n#             index = dep.index(token.dep_.upper())\n#             vector[index] = 1\n                \n#     return vector\n         \n    x = shortest_dependency_path(doc, pronoun, word)\n    for token in doc:\n        if token.text in x:\n            val = (x.index(str(token)) + 1) / len(x)\n            try:\n                index = dep.index(token.dep_.upper())\n                vector[index] = val\n            except:\n                pass\n    return vector\ndef get_features(df):\n    \n    df['A-offset2'] = df['A-offset'] + df['A'].map(len)\n    df['B-offset2'] = df['B-offset'] + df['B'].map(len)\n    df[\"Text\"] =  df.apply(lambda row: name_replace(row[\"Text\"], row[\"A\"], \"Noun_1\"), axis = 1)\n    df[\"Text\"] =  df.apply(lambda row: name_replace(row[\"Text\"], row[\"B\"], \"Noun_2\"), axis = 1)\n    new_df = pd.DataFrame([])\n    new_df[\"Pronoun-offset\"] = df[\"Pronoun-offset\"]\n    new_df['A-offset'] = df[\"A-offset\"]\n    new_df[\"B-offset\"] = df[\"B-offset\"]\n    new_df['A-offset2'] = df['A-offset2']\n    new_df['B-offset2'] = df['B-offset2']\n    new_df['A_dist'] = (df['Pronoun-offset'] - df['A-offset']).abs()\n    new_df['B_dist'] = (df['Pronoun-offset'] - df['B-offset']).abs()\n    df[\"Text\"] = df.Text.apply(lambda row: \" and \".join(row.split(\". \")))\n    vectors_A = df.apply(lambda row: dependency_vector(nlp(row[\"Text\"]), row[\"Pronoun\"],\"Noun_1\") + dependency_vector(nlp(row[\"Text\"]), row[\"Pronoun\"],\"Noun_2\"), axis = 1)\n    print(count)\n    new_df_2 = pd.DataFrame(vectors_A.tolist())\n    new_df = pd.concat([new_df, new_df_2], axis = 1)    \n    return new_df\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"feature = get_features(merge_data)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nY = merge_data[[\"A-coref\", \"B-coref\"]]\nY.columns = [\"A\",\"B\"]\nY[\"A\"] = Y[\"A\"].astype(int)\nY[\"B\"] = Y[\"B\"].astype(int)\nY[\"NEITHER\"] = 1- (Y[\"A\"] + Y[\"B\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import *\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import StandardScaler\nx1, x2, y1, y2 = model_selection.train_test_split(feature.fillna(-1), Y, test_size=0.2, random_state=1)\nx1.head()\nx2.head()\ny2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nx1 = scaler.fit_transform(x1)\nx2 = scaler.transform(x2)\nmodel = multiclass.OneVsRestClassifier(ensemble.RandomForestClassifier(max_depth = 7, n_estimators=1000, random_state=33))\n# model = multiclass.OneVsRestClassifier(ensemble.ExtraTreesClassifier(n_jobs=-1, n_estimators=100, random_state=33))\n\n# param_dist = {'objective': 'binary:logistic', 'max_depth': 1, 'n_estimators':1000, 'num_round':1000, 'eval_metric': 'logloss'}\n# model = multiclass.OneVsRestClassifier(xgb.XGBClassifier(**param_dist))\n\nmodel.fit(x1, y1)\nprint('log_loss', metrics.log_loss(y2, model.predict_proba(x2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test = pd.read_csv(\"../input/gendered-pronoun-resolution/test_stage_2.tsv\", sep = \"\\t\")\nfeature = get_features(final_test)\nprint(feature)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfeature = feature.fillna(-1)\n# feature = scaler.transform(feature)\nprint(feature)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nY = pd.DataFrame(model.predict_proba(feature).tolist(), columns=[\"A\",\"B\", \"NEITHER\"])\nr = final_test[[\"ID\"]]\nsubmission = pd.concat([r,Y], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(submission)\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}