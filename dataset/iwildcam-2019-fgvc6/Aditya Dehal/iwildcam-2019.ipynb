{"cells":[{"metadata":{},"cell_type":"markdown","source":"# iWildCam-2019\n### Categorize animals in wild"},{"metadata":{},"cell_type":"markdown","source":"In this kernel we will create a CNN using keras to categorize the images of wild animals. These images are captured using WildCams. These WildCams collect images in large quantities which then are used by biologists to monitor the biodiversity and population density of animals."},{"metadata":{},"cell_type":"markdown","source":"We will follow the steps below in this kernel:\n1. Import libraries\n2. Import dataset\n3. Create and train model\n4. Analyse the results\n5. Make predictions and submission\n\nLet's get started"},{"metadata":{},"cell_type":"markdown","source":"### 1. Import libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# For file manipulation\nimport os\n\n# For data manipulation\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# For our CNN model\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Import dataset\n\nSince, the data is too large and will take a lot of time I used data from another awesome [kernel](https://www.kaggle.com/xhlulu/reducing-image-sizes-to-32x32) by [xhlulu](https://www.kaggle.com/xhlulu). You can simply click on the \"Add Dataset\" on the top to add the data to your kernel."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Loading the train and test data\nx_train = np.load('../input/reducing-image-sizes-to-32x32/X_train.npy')\nx_test = np.load('../input/reducing-image-sizes-to-32x32/X_test.npy')\ny_train = np.load('../input/reducing-image-sizes-to-32x32/y_train.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing the image data\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255.\nx_test /= 255.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the required variables\nbatch_size = 64\nnum_classes = 14\nepochs = 30\nval_split = 0.1\ninput_shape=x_train.shape[1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Create and train model"},{"metadata":{},"cell_type":"markdown","source":"We will use Keras to create a CNN and then we will train it on our training data. There are many good architectures out there that we can use. These architectures can give you much better accuracy on both train and validation set. Here, we will use a simple architecture."},{"metadata":{"trusted":true},"cell_type":"code","source":"def baseline_model():\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n    model.add(Activation('relu'))\n    model.add(Conv2D(32, (3, 3)))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    #model.add(Dropout(0.25))\n\n    model.add(Conv2D(64, (3, 3), padding='same'))\n    model.add(Activation('relu'))\n    model.add(Conv2D(64, (3, 3)))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    #model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    \n    model.add(Dense(1024))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(1024))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(num_classes))\n    model.add(Activation('softmax'))\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = baseline_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compiling the model\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the model\nhist = model.fit(\n    x_train, \n    y_train,\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_split=val_split,\n    shuffle=True\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Analyse the results"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = hist.history\n\nfig, ax = plt.subplots(2)\n\nax[0].plot(history['acc'])\nax[0].plot(history['val_acc'])\nax[0].legend(['training accuracy', 'validation accuracy'])\n\nax[1].plot(history['loss'])\nax[1].plot(history['val_loss'])\nax[1].legend(['training loss', 'validation loss'])\n\nfor axs in ax.flat:\n    axs.label_outer()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Make predictions and submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = model.predict(x_test)\n\nsubmission_df = pd.read_csv('../input/iwildcam-2019-fgvc6/sample_submission.csv')\nsubmission_df['Predicted'] = y_test.argmax(axis=1)\nprint(submission_df.shape)\nsubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv('submission.csv',index=False)\n# history_df.to_csv('history.csv', index=False)\n\n# with open('history.json', 'w') as f:\n#     json.dump(hist.history, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}