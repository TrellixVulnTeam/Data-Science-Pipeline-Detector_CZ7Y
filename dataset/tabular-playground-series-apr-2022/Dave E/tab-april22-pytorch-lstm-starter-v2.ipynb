{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nimport random\nfrom sklearn.metrics import accuracy_score, log_loss, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgbm\nfrom scipy.stats import skew\nfrom tqdm import tqdm\nfrom torch.nn.modules.loss import _WeightedLoss\nimport pickle\nimport seaborn as sns\nplt.rcParams[\"figure.figsize\"] = (12, 6)\nsns.set(font_scale=1.6)\n\n\ndef show_close_plt():\n    plt.tight_layout()\n    plt.show()\n    plt.clf()\n\n\ndef seed_everything(seed=42):\n    print('Setting Random Seed')\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-09T06:57:29.153197Z","iopub.execute_input":"2022-04-09T06:57:29.153483Z","iopub.status.idle":"2022-04-09T06:57:29.168738Z","shell.execute_reply.started":"2022-04-09T06:57:29.153451Z","shell.execute_reply":"2022-04-09T06:57:29.167338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"# version 3 - added spatial dropout, dropped some of the train sequences with worst CV errors from TRAIN dataset, changed folds to groupfold style, lstm dim = 1024\n# and scale features after generating 'shift' features","metadata":{"execution":{"iopub.status.busy":"2022-04-09T06:57:29.189782Z","iopub.execute_input":"2022-04-09T06:57:29.190788Z","iopub.status.idle":"2022-04-09T06:57:29.196075Z","shell.execute_reply.started":"2022-04-09T06:57:29.190739Z","shell.execute_reply":"2022-04-09T06:57:29.194689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CONFIG:\n    PATH = '..//input//tabular-playground-series-apr-2022//'\n    if os.path.exists(PATH):\n        KAGGLE = True\n    else:\n        KAGGLE = False\n        PATH = 'NA'\n\n    N_FOLDS = 5\n    N_SENSORS = 13\n\n    N_EPOCHS = 12\n\n    LR = 1e-04\n    GAMMA = 0.9\n    BATCH_SIZE = 32\n    INFERENCE_BATCH_SIZE = 128\n\n    SMOOTHING = 0.0\n\n    FOLD_LIMIT = 5\n\n    LSTM_DIM = 1024\n    C1D_CHANNELS = 128\n    DENSE_DROPOUT = 0.5","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-09T06:57:29.264194Z","iopub.execute_input":"2022-04-09T06:57:29.26449Z","iopub.status.idle":"2022-04-09T06:57:29.275139Z","shell.execute_reply.started":"2022-04-09T06:57:29.264451Z","shell.execute_reply":"2022-04-09T06:57:29.274103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SENSOR_LIST = [f'sensor_{S:02d}' for S in range(0, CONFIG.N_SENSORS)]\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-09T06:57:29.318392Z","iopub.execute_input":"2022-04-09T06:57:29.318702Z","iopub.status.idle":"2022-04-09T06:57:29.325695Z","shell.execute_reply.started":"2022-04-09T06:57:29.318672Z","shell.execute_reply":"2022-04-09T06:57:29.324052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and process data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(f'{CONFIG.PATH}train.csv')\ntest = pd.read_csv(f'{CONFIG.PATH}test.csv')\ntrain_labels = pd.read_csv(f'{CONFIG.PATH}train_labels.csv')\nsample_submission = pd.read_csv(f'{CONFIG.PATH}sample_submission.csv')\nprint(train.shape, test.shape, train_labels.shape)\ndisplay(train.head(10))","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-09T06:57:29.363762Z","iopub.execute_input":"2022-04-09T06:57:29.364238Z","iopub.status.idle":"2022-04-09T06:57:36.427729Z","shell.execute_reply.started":"2022-04-09T06:57:29.364205Z","shell.execute_reply":"2022-04-09T06:57:36.426683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train_labels.head(10))","metadata":{"execution":{"iopub.status.busy":"2022-04-09T06:57:36.430456Z","iopub.execute_input":"2022-04-09T06:57:36.431132Z","iopub.status.idle":"2022-04-09T06:57:36.443476Z","shell.execute_reply.started":"2022-04-09T06:57:36.431084Z","shell.execute_reply":"2022-04-09T06:57:36.4423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shifts = [1, 2, ]\nfor S in SENSOR_LIST:\n    for shift in shifts:\n        train[f'delta_{shift}_{S}'] = train.groupby(['sequence'])[S].shift(shift).values - train[S]    \n        train[f'delta_{shift}_{S}'] = train[f'delta_{shift}_{S}'].fillna(value=0)\n\n        test[f'delta_{shift}_{S}'] = test.groupby(['sequence'])[S].shift(shift).values - test[S]\n        test[f'delta_{shift}_{S}'] = test[f'delta_{shift}_{S}'].fillna(value=0)\n\nfor S in SENSOR_LIST:    \n    for shift in shifts:\n        SENSOR_LIST = SENSOR_LIST + [f'delta_{shift}_{S}']\n\ntrain[SENSOR_LIST].head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T06:57:36.445165Z","iopub.execute_input":"2022-04-09T06:57:36.445899Z","iopub.status.idle":"2022-04-09T06:57:39.469911Z","shell.execute_reply.started":"2022-04-09T06:57:36.445846Z","shell.execute_reply":"2022-04-09T06:57:39.468684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SENSOR_LIST","metadata":{"execution":{"iopub.status.busy":"2022-04-09T06:57:39.472991Z","iopub.execute_input":"2022-04-09T06:57:39.473338Z","iopub.status.idle":"2022-04-09T06:57:39.481118Z","shell.execute_reply.started":"2022-04-09T06:57:39.473294Z","shell.execute_reply":"2022-04-09T06:57:39.47994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stsc = StandardScaler()\n\ntrain[SENSOR_LIST] = stsc.fit_transform(train[SENSOR_LIST])\ntest[SENSOR_LIST] = stsc.transform(test[SENSOR_LIST])","metadata":{"execution":{"iopub.status.busy":"2022-04-09T06:57:39.483102Z","iopub.execute_input":"2022-04-09T06:57:39.483471Z","iopub.status.idle":"2022-04-09T06:57:42.180252Z","shell.execute_reply.started":"2022-04-09T06:57:39.483426Z","shell.execute_reply":"2022-04-09T06:57:42.179282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_agg = {\n    S: [list] for S in SENSOR_LIST\n}\ntrain = train.groupby(['sequence']).agg(features_agg)\ntrain.columns = [a for a, b in train.columns]\nprint(train.shape)\ntrain.head(10)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-09T06:57:42.181747Z","iopub.execute_input":"2022-04-09T06:57:42.182084Z","iopub.status.idle":"2022-04-09T06:58:08.555108Z","shell.execute_reply.started":"2022-04-09T06:57:42.182038Z","shell.execute_reply":"2022-04-09T06:58:08.55313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test.groupby(['sequence']).agg(features_agg)\ntest.columns = [a for a, b in test.columns]\nprint(test.shape)\ntest.head(10)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-09T06:58:08.556946Z","iopub.execute_input":"2022-04-09T06:58:08.566982Z","iopub.status.idle":"2022-04-09T06:58:23.065406Z","shell.execute_reply.started":"2022-04-09T06:58:08.566923Z","shell.execute_reply":"2022-04-09T06:58:23.064443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels.head(10)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-09T06:58:23.068303Z","iopub.execute_input":"2022-04-09T06:58:23.068935Z","iopub.status.idle":"2022-04-09T06:58:23.081067Z","shell.execute_reply.started":"2022-04-09T06:58:23.068888Z","shell.execute_reply":"2022-04-09T06:58:23.079602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['target'] = train.index.map(dict(zip(train_labels['sequence'], train_labels['state'])))\ntrain = train.reset_index(drop=False)  # move sequence into the columns\ntrain['target'].value_counts()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-09T06:58:23.082782Z","iopub.execute_input":"2022-04-09T06:58:23.083213Z","iopub.status.idle":"2022-04-09T06:58:23.203625Z","shell.execute_reply.started":"2022-04-09T06:58:23.083168Z","shell.execute_reply":"2022-04-09T06:58:23.202523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = 'target'\nfold_df = pd.read_csv('../input/tab-april22-seq-data-prep-v4/train_labels.csv')\ntrain['fold'] = train.index.map(dict(zip(fold_df['sequence'], fold_df[f'fold_subject_{CONFIG.N_FOLDS}'])))\nprint(train[f'fold'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-04-09T06:58:23.208329Z","iopub.execute_input":"2022-04-09T06:58:23.208599Z","iopub.status.idle":"2022-04-09T06:58:23.275022Z","shell.execute_reply.started":"2022-04-09T06:58:23.208569Z","shell.execute_reply":"2022-04-09T06:58:23.273747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load CV predictions","metadata":{}},{"cell_type":"code","source":"# an existing set of CV predictions\ncv_preds = np.load('../input/tps-april-tensorflow-bi-lstm-10f-spatialdoexpts/cv_preds.npy')","metadata":{"execution":{"iopub.status.busy":"2022-04-09T06:58:23.276873Z","iopub.execute_input":"2022-04-09T06:58:23.277353Z","iopub.status.idle":"2022-04-09T06:58:23.285668Z","shell.execute_reply.started":"2022-04-09T06:58:23.277304Z","shell.execute_reply":"2022-04-09T06:58:23.284701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['state'] = train['sequence'].map(dict(zip(train_labels['sequence'], train_labels['state'])))\ntrain['cv_preds'] = cv_preds\nroc_auc_score(train['state'],train['cv_preds'])","metadata":{"execution":{"iopub.status.busy":"2022-04-09T06:58:23.286963Z","iopub.execute_input":"2022-04-09T06:58:23.287829Z","iopub.status.idle":"2022-04-09T06:58:23.34247Z","shell.execute_reply.started":"2022-04-09T06:58:23.287782Z","shell.execute_reply":"2022-04-09T06:58:23.341058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['error'] = np.abs(train['cv_preds'] - train['state'])\nsns.kdeplot(train['error'])","metadata":{"execution":{"iopub.status.busy":"2022-04-09T06:58:23.344777Z","iopub.execute_input":"2022-04-09T06:58:23.345194Z","iopub.status.idle":"2022-04-09T06:58:23.823939Z","shell.execute_reply.started":"2022-04-09T06:58:23.345146Z","shell.execute_reply":"2022-04-09T06:58:23.822965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#filter out sequences with a CV absolute error of > limit\nLIMIT = 0.99\nsum(train['error']>LIMIT)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T06:58:23.825817Z","iopub.execute_input":"2022-04-09T06:58:23.826132Z","iopub.status.idle":"2022-04-09T06:58:23.838054Z","shell.execute_reply.started":"2022-04-09T06:58:23.826087Z","shell.execute_reply":"2022-04-09T06:58:23.836797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"allowed_sequences = train['error']<=LIMIT\nsum(allowed_sequences) / len(train)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T06:58:23.83997Z","iopub.execute_input":"2022-04-09T06:58:23.840334Z","iopub.status.idle":"2022-04-09T06:58:23.857666Z","shell.execute_reply.started":"2022-04-09T06:58:23.840291Z","shell.execute_reply":"2022-04-09T06:58:23.856641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Dataset, Model","metadata":{}},{"cell_type":"code","source":"class SeqDataset(Dataset):\n    def __init__(self, csv, training=False):\n        self.csv = csv\n        self.training = training\n\n    def __len__(self):\n        return len(self.csv)\n\n    def __getitem__(self, item):\n\n        features = np.concatenate(\n            [x for x in [np.array(x).reshape(-1, 1) for x in self.csv.loc[item, SENSOR_LIST].values]], axis=1)\n\n        if self.training:\n            return {'features': torch.tensor(features, dtype=torch.float),\n                    'labels': torch.tensor(self.csv.loc[item, 'target'], dtype=torch.long)}\n\n        else:\n            return {'features': torch.tensor(features, dtype=torch.float), }\n\n\nexample_data = SeqDataset(train, training=True)\nexample_data[0]","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-09T06:58:23.860429Z","iopub.execute_input":"2022-04-09T06:58:23.860716Z","iopub.status.idle":"2022-04-09T06:58:23.898729Z","shell.execute_reply.started":"2022-04-09T06:58:23.860667Z","shell.execute_reply":"2022-04-09T06:58:23.897751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_data[0]['features'].shape","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-09T06:58:23.900329Z","iopub.execute_input":"2022-04-09T06:58:23.900899Z","iopub.status.idle":"2022-04-09T06:58:23.912744Z","shell.execute_reply.started":"2022-04-09T06:58:23.900851Z","shell.execute_reply":"2022-04-09T06:58:23.911418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SeqModel(torch.nn.Module):\n    def __init__(self, input_dim=13*2, lstm_dim=512,\n                 logit_dim=128, num_classes=1,c1d_channels=128,\n                dense_dropout=0.5):\n        super().__init__()\n\n        self.c1d_module = nn.Sequential(\n            nn.Conv1d(in_channels=input_dim, out_channels=c1d_channels,\n                      kernel_size=2, padding='same', stride=1),\n            nn.Conv1d(in_channels=c1d_channels, out_channels=c1d_channels*2,\n                      kernel_size=2, padding='same', stride=1)\n\n        )\n        \n        self.spatial_dropout = torch.nn.Dropout2d(p=0.25)\n\n        self.lstm1 = nn.LSTM(c1d_channels*2, lstm_dim, batch_first=True, bidirectional=True, dropout=0.0)\n        self.lstm2 = nn.LSTM(lstm_dim * 2, lstm_dim // 2, batch_first=True, bidirectional=True, dropout=0.0)\n        self.lstm3 = nn.LSTM(lstm_dim , lstm_dim // 4, batch_first=True, bidirectional=True, dropout=0.0)\n\n        self.logits = nn.Sequential(\n            nn.LazyLinear(out_features=logit_dim),  \n            nn.ReLU(),\n            nn.Dropout(p=dense_dropout),\n            nn.Linear(logit_dim, num_classes),\n        )\n\n    def forward(self, x):\n        \n        x = x.permute(0, 2, 1)\n        x = self.spatial_dropout(x)\n        \n        x = self.c1d_module(x)\n        \n        features1, _ = self.lstm1(x.permute(0, 2, 1))\n        features2, _ = self.lstm2(features1)\n        features3, _ = self.lstm3(features2)        \n        \n        features = features3.reshape(features3.shape[0], -1)\n        \n        pred = self.logits(features)\n        \n        return pred.squeeze(1)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-09T06:58:23.91515Z","iopub.execute_input":"2022-04-09T06:58:23.915495Z","iopub.status.idle":"2022-04-09T06:58:23.93265Z","shell.execute_reply.started":"2022-04-09T06:58:23.915451Z","shell.execute_reply":"2022-04-09T06:58:23.931579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets: torch.Tensor, n_labels: int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n                                           self.smoothing)\n\n        loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, targets, self.weight)\n\n        if self.reduction == 'sum':\n            loss = loss.sum()\n        elif self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-09T06:58:23.935579Z","iopub.execute_input":"2022-04-09T06:58:23.936385Z","iopub.status.idle":"2022-04-09T06:58:23.947975Z","shell.execute_reply.started":"2022-04-09T06:58:23.93634Z","shell.execute_reply":"2022-04-09T06:58:23.946725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run Training","metadata":{}},{"cell_type":"code","source":"FOLDS = sorted(train['fold'].unique())\n\ntest_dataset = SeqDataset(test.reset_index(drop=True), training=False)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=CONFIG.INFERENCE_BATCH_SIZE,\n    shuffle=False,\n    drop_last=False\n)\n\ntrain['oof'] = 0.0\n\nfor f in FOLDS[:CONFIG.FOLD_LIMIT]:\n    seed_everything(seed=42 + f)\n\n    trn_idx = (train['fold'] != f) & (allowed_sequences)\n    val_idx = train['fold'] == f\n\n    baseline_log_loss = log_loss(train.loc[val_idx, target], np.full(train.loc[val_idx, target].shape,\n                                                                     fill_value=train.loc[val_idx, target].mean()))\n    print(f'training fold {f}')\n    print(f'baseline log loss {baseline_log_loss}')\n\n    train_dataset = SeqDataset(train.loc[trn_idx].reset_index(drop=True), training=True)\n    val_dataset = SeqDataset(train.loc[val_idx].reset_index(drop=True), training=True)\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=CONFIG.BATCH_SIZE,\n        shuffle=True,\n        drop_last=True\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=CONFIG.INFERENCE_BATCH_SIZE,\n        shuffle=False,\n        drop_last=False\n    )\n\n    model = SeqModel(input_dim = len(SENSOR_LIST),\n                     lstm_dim=CONFIG.LSTM_DIM,\n                     c1d_channels=CONFIG.C1D_CHANNELS,\n                     dense_dropout=CONFIG.DENSE_DROPOUT)\n\n    model = model.to(DEVICE)\n\n    optimizer = torch.optim.AdamW(params=model.parameters(),\n                                  lr=CONFIG.LR, )\n\n    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer,\n                                                       gamma=CONFIG.GAMMA)\n\n    # criterion = nn.BCEWithLogitsLoss(reduction='mean')\n    criterion = SmoothBCEwLogits(smoothing=CONFIG.SMOOTHING)\n\n    best_fold_score = -999\n    train_aucs = []\n    val_aucs = []\n\n    for epoch in range(CONFIG.N_EPOCHS):\n        tbar = tqdm(train_loader)\n\n        model.train()\n        epoch_loss = 0\n\n        tr_labels = []\n        tr_predictions = []\n        for count, batch in enumerate(tbar):\n            optimizer.zero_grad()\n\n            features = batch['features'].to(DEVICE)\n            labels = batch['labels'].to(DEVICE)\n\n            preds = model(features)\n\n            loss = criterion(preds, labels.float())\n\n            loss.backward()\n\n            epoch_loss += loss.item()\n\n            optimizer.step()\n\n            tr_labels += [labels.detach().cpu().numpy()]\n            tr_predictions += [torch.sigmoid(preds).detach().cpu().numpy()]\n\n        scheduler.step()\n\n        tr_labels = np.concatenate([x for x in tr_labels], axis=0)\n        tr_predictions = np.concatenate([x for x in tr_predictions], axis=0)\n        tr_predictions = np.clip(tr_predictions, 0.00001, 1 - 0.00001)\n\n        tr_loss = log_loss(tr_labels, tr_predictions)\n        tr_auc = roc_auc_score(tr_labels, tr_predictions)\n\n        print(f'epoch {epoch} training log loss {tr_loss} training AUC {tr_auc}')\n        train_aucs += [tr_auc]\n\n        # Validation fold for epoch\n        tbar = tqdm(val_loader)\n\n        model.eval()\n        val_labels = []\n        val_predictions = []\n        for count, batch in enumerate(tbar):\n            features = batch['features'].to(DEVICE)\n            labels = batch['labels'].to(DEVICE)\n\n            with torch.no_grad():\n                preds = model(features)\n\n            val_labels += [labels.detach().cpu().numpy()]\n            val_predictions += [torch.sigmoid(preds).detach().cpu().numpy()]\n\n        val_labels = np.concatenate([x for x in val_labels], axis=0)\n        val_predictions = np.concatenate([x for x in val_predictions], axis=0)\n        val_predictions = np.clip(val_predictions, 0.00001, 1 - 0.00001)\n\n        train.loc[val_idx, 'oof'] = val_predictions\n\n        val_loss = log_loss(val_labels, val_predictions)\n        val_auc = roc_auc_score(val_labels, val_predictions)\n        print(f'epoch {epoch} validation log loss {val_loss},val auc {val_auc}')\n\n        print('learning rate ', optimizer.param_groups[0]['lr'])\n\n        torch.save(model.state_dict(), f'model_fold_{f}_epoch_{epoch}')\n\n        val_aucs += [val_auc]\n\n        if val_auc > best_fold_score:\n            print(f'best ROC AUC improved to {val_auc}')\n            best_fold_score = val_auc\n            torch.save(model.state_dict(), f'model_fold_{f}_best')\n        else:\n            print('validation score not improved')\n\n    # complete fold\n    print(f'finished fold with best AUC score {best_fold_score} final AUC score {val_aucs[-1]}')\n\n    plt.plot(range(CONFIG.N_EPOCHS),\n             train_aucs, color='Blue')\n    plt.plot(range(CONFIG.N_EPOCHS),\n             val_aucs, color='Red')\n\n    print('reloading weights from best epoch for inference')\n    model.load_state_dict(torch.load(f'model_fold_{f}_best'))\n\n    # final out of fold predictions with best weights\n    model.eval()\n    val_labels = []\n    val_predictions = []\n    for count, batch in enumerate(val_loader):\n        features = batch['features'].to(DEVICE)\n        labels = batch['labels'].to(DEVICE)\n\n        with torch.no_grad():\n            preds = model(features)\n\n        val_predictions += [torch.sigmoid(preds).detach().cpu().numpy()]\n\n    val_predictions = np.concatenate([x for x in val_predictions], axis=0)\n    train.loc[val_idx, 'oof'] = val_predictions\n\n    # final test inference with best weights\n    test_predictions = []\n    for count, batch in enumerate(test_loader):\n        features = batch['features'].to(DEVICE)\n\n        with torch.no_grad():\n            preds = model(features)\n\n        test_predictions += [torch.sigmoid(preds).detach().cpu().numpy()]\n\n    test_predictions = np.concatenate([x for x in test_predictions], axis=0)\n\n    sample_submission[f'fold_{f}'] = test_predictions\n\nplt.title('Train (Blue) and Validation (Red) AUC Scores')\nshow_close_plt()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-09T06:58:23.950027Z","iopub.execute_input":"2022-04-09T06:58:23.950418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Review Results & Export Submission","metadata":{}},{"cell_type":"code","source":"total_auc = roc_auc_score(train[target][train['fold'].isin(FOLDS[:CONFIG.FOLD_LIMIT])],\n                          train['oof'][train['fold'].isin(FOLDS[:CONFIG.FOLD_LIMIT])])\nprint(f'total OOF AUC score {total_auc} from CV on {len(FOLDS[:CONFIG.FOLD_LIMIT])} folds')\ntrain.to_csv('train_with_oof.csv')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for f in FOLDS[:CONFIG.FOLD_LIMIT]:\n    fold_score = roc_auc_score(train[target][train['fold']==f],\n                          train['oof'][train['fold']==f])\n    print(f'fold {f} AUC CV score is {fold_score}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x = train[target][train['fold'].isin(FOLDS[:CONFIG.FOLD_LIMIT])],\n                        y = train['oof'][train['fold'].isin(FOLDS[:CONFIG.FOLD_LIMIT])])\nplt.title('Distribution of OOF by target value')\nshow_close_plt()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for f in FOLDS[:CONFIG.FOLD_LIMIT]:\n    sns.kdeplot(sample_submission[f'fold_{f}'])\n    \nplt.title('Test predictions by CV fold')\nplt.legend(FOLDS[:CONFIG.FOLD_LIMIT])\nshow_close_plt()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['mean_prediction'] = sample_submission[[f'fold_{f}' for f in FOLDS[:CONFIG.FOLD_LIMIT]]].mean(axis=1)\nsns.kdeplot(sample_submission['mean_prediction'],color='Red')\nsns.kdeplot(train['oof'][train['fold'].isin(FOLDS[:CONFIG.FOLD_LIMIT])], color='Blue', shade=True)\nplt.legend(['Test mean prediction distribution', 'CV prediction distribution'])\nshow_close_plt()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['state'] = sample_submission['mean_prediction']\nsample_submission[['sequence', 'state']].head(10)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission[['sequence', 'state']].to_csv('submission.csv', index=False)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}