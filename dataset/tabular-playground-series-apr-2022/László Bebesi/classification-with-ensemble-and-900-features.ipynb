{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.options.display.float_format = '{:,.2f}'.format\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import kurtosis, skew\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib.ticker import PercentFormatter\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.inspection import permutation_importance\nfrom sklearn import metrics\nfrom scipy.fft import rfft\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-21T15:50:54.383667Z","iopub.execute_input":"2022-04-21T15:50:54.384262Z","iopub.status.idle":"2022-04-21T15:50:55.810845Z","shell.execute_reply.started":"2022-04-21T15:50:54.384132Z","shell.execute_reply":"2022-04-21T15:50:55.810028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 0. Helper functions <a name=\"help\"></a>","metadata":{}},{"cell_type":"code","source":"def fit_model_using_classifier(alg,\n                               dtrain,\n                               predictors,\n                               target=\"state\",\n                               performCV=True, \n                               printFeatureImportance=True, \n                               cv_folds=3,\n                               repeat=5,\n                               scoring='roc_auc',\n                               only_top_x_feature=60\n                              ):\n    \"\"\"\n    I used the function found in this source\n    https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n    I modified the code slightly\n    \"\"\"\n\n    # Perform cross-validation:\n    cv_score=list()\n    if performCV:\n        for i in range(0,repeat):\n            cv_score_temp = cross_val_score(\n                            alg, \n                            dtrain[predictors], \n                            dtrain[target], \n                            cv=cv_folds, \n                            scoring=scoring)\n            cv_score=cv_score+list(cv_score_temp)\n    \n    # Fit the algorithm on the data\n    alg.fit(dtrain[predictors], dtrain[target])\n        \n    # Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]        \n    \n    # Print model report:\n    print(\"\\nModel Report\")\n    print(\"Accuracy : \" + str(round(metrics.accuracy_score(\n        dtrain[target].values, dtrain_predictions),4)))\n    print(\"AUC Score (Train): \" + str(round(\n        metrics.roc_auc_score(dtrain[target], dtrain_predprob),4)))\n    \n    if performCV:\n        print(\"\\n Cross validation summary (\"+scoring+\")\")\n        print(\"Average: \"+str(round(np.mean(cv_score),4)))\n        print(\"Std    : \"+str(round(np.std(cv_score),4)))\n        print(\"Min    : \"+str(round(np.min(cv_score),4)))\n        print(\"Max    : \"+str(round(np.max(cv_score),4)))\n                \n    # Print Feature Importance:\n    if printFeatureImportance and \"feature_importances_\" in dir(alg):\n        plt.figure(figsize=(20,6))\n        feat_imp = pd.Series(alg.feature_importances_, predictors).sort_values(ascending=False)\n        feat_imp.head(only_top_x_feature).plot(kind='bar', title='Feature Importances',fontsize=12, color=\"#CBC3E3\")\n        plt.ylabel('Feature Importance Score')\n        return alg, feat_imp\n    else:\n        return alg, list()\n\ndef generate_features(df, metric_data, group_variables, sensor_identifiers, suffix=\"\"):\n    \"\"\"\n    Generates the features based on the provided metric_data map\n    \n    \"\"\"\n    all_metrics=pd.DataFrame(columns=group_variables)\n    for sensor_number in sensor_numbs:\n        sensor_v=\"sensor_\"+sensor_number\n        # I had to use this words solution because of list formatting\n        metrics = [ listv[0] \n                        for key, listv in metric_data.items()]\n        metric_cols=[key+sensor_number+suffix for key in metric_data.keys()]\n\n        temp_metrics=df.groupby(group_variables).agg({\n            sensor_v: metrics\n        }).reset_index()\n        temp_metrics.columns=group_variables+metric_cols\n        all_metrics=all_metrics.merge(temp_metrics,how=\"outer\",on=group_variables)\n\n    # finally we save down the variable names as well\n    generated_columns=list(set(all_metrics.columns)-set(group_variables))\n    generated_columns.sort()\n    return all_metrics, generated_columns\n\ndef create_frequencies(groups):\n    \"\"\"\n    Create frequencies up to frequency 30.\n    source https://www.kaggle.com/code/matanivanov/lgbm-with-fourier-transform\n    \"\"\"\n    return pd.concat(\n        [pd.Series(np.abs(rfft(groups[col].values)), \n                   index=[f'{col}_freq_{i}' for i in range(31)]) \n         for col in groups.columns if col not in ['sequence', 'subject', 'step']\n        ])\n\ndef fit_ensemble(models, X_train, X_val, y_train, y_val, soft_vote=True):\n    \"\"\"\n    Fit all models on the training set and predict on hold out set\n    \"\"\"\n    meta_X = list()\n    if X_val is None:\n        X_val=X_train\n    if y_val is None:\n        y_val=y_train\n    \n    for name, model in models:\n        model.fit(X_train, y_train)\n        if soft_vote:\n            yhat = model.predict_proba(X_val)[:,1]\n        else:\n            yhat = model.predict(X_val)\n        yhat2 = yhat.reshape(len(yhat), 1)\n        meta_X.append(yhat2)\n        del yhat\n    meta_X = np.hstack(meta_X)\n    blender = LogisticRegression()\n    blender.fit(meta_X, y_val)\n    return blender, meta_X\n\ndef predict_ensemble(models, blender, X_test, soft_vote=True):\n    \"\"\"\n    Predict outcome using the set of models\n    \"\"\"\n    meta_X = list()\n    for name, model in models:\n        if soft_vote:\n            yhat = model.predict_proba(X_test)[:,1]\n        else:\n            yhat = model.predict(X_test)\n        yhat2 = yhat.reshape(len(yhat), 1)\n        del yhat\n        meta_X.append(yhat2)\n    meta_X = np.hstack(meta_X)\n    return blender.predict_proba(meta_X)[:,1]","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:50:55.812159Z","iopub.execute_input":"2022-04-21T15:50:55.812366Z","iopub.status.idle":"2022-04-21T15:50:55.835122Z","shell.execute_reply.started":"2022-04-21T15:50:55.812341Z","shell.execute_reply":"2022-04-21T15:50:55.834457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Table of contents\n0. [Helper functions](#help)\n1. [Load and explore data](#introduction)\n2. [Variable construction](#vars)\n3. [EDA for the created features & selection](#vars2)\n4. [Estimate models](#modest)\n5. [Ensemble approach](#ens)\n6. [Acknowledgement](#ack)","metadata":{}},{"cell_type":"markdown","source":"# 1. Load and Explore data <a name=\"introduction\"></a>","metadata":{}},{"cell_type":"markdown","source":"Let us keep the data and columns descriptions in mind\n* train.csv - the training set, comprising ~26,000 60-second recordings of thirteen biological sensors for almost one thousand experimental participants\n> 1. sequence - a unique id for each sequence\n> 2. subject - a unique id for the subject in the experiment\n> 3. step - time step of the recording, in one second intervals\n> 4. sensor_00 - sensor_12 - the value for each of the thirteen sensors at that time step\n* train_labels.csv - the class label for each sequence.\n> 1. sequence - the unique id for each sequence.\n> 2. state - the state associated to each sequence. This is the target which you are trying to predict.\n* test.csv - the test set. For each of the ~12,000 sequences, you should predict a value for that sequence's state.\n* sample_submission.csv - a sample submission file in the correct format.","metadata":{}},{"cell_type":"code","source":"train_labels=pd.read_csv(\"/kaggle/input/tabular-playground-series-apr-2022/train_labels.csv\")\ndisplay(train_labels.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:50:55.836249Z","iopub.execute_input":"2022-04-21T15:50:55.836493Z","iopub.status.idle":"2022-04-21T15:50:55.879747Z","shell.execute_reply.started":"2022-04-21T15:50:55.836466Z","shell.execute_reply":"2022-04-21T15:50:55.878856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/tabular-playground-series-apr-2022/train.csv\")\ndisplay(train.head())\ndisplay(train.describe())","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:50:55.881675Z","iopub.execute_input":"2022-04-21T15:50:55.881884Z","iopub.status.idle":"2022-04-21T15:51:04.346864Z","shell.execute_reply.started":"2022-04-21T15:50:55.881859Z","shell.execute_reply":"2022-04-21T15:51:04.345711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# At first glance no problem with missing variables\ndisplay(train.info())","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:51:04.348283Z","iopub.execute_input":"2022-04-21T15:51:04.348495Z","iopub.status.idle":"2022-04-21T15:51:04.423091Z","shell.execute_reply.started":"2022-04-21T15:51:04.34847Z","shell.execute_reply":"2022-04-21T15:51:04.422258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=pd.read_csv(\"/kaggle/input/tabular-playground-series-apr-2022/test.csv\")\ndisplay(test.head())\ndisplay(test.describe())","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:51:04.424118Z","iopub.execute_input":"2022-04-21T15:51:04.424339Z","iopub.status.idle":"2022-04-21T15:51:08.298401Z","shell.execute_reply.started":"2022-04-21T15:51:04.424313Z","shell.execute_reply":"2022-04-21T15:51:08.297327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# At first glance no problem with missing variables\ndisplay(test.info())","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:51:08.300167Z","iopub.execute_input":"2022-04-21T15:51:08.300421Z","iopub.status.idle":"2022-04-21T15:51:08.336142Z","shell.execute_reply.started":"2022-04-21T15:51:08.300395Z","shell.execute_reply":"2022-04-21T15:51:08.335366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let us define a list of sensors for convinience\nsensors=list(test.columns[3:16])\nsensor_numbs=[sensor[7:9] for sensor in sensors]","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:51:08.337131Z","iopub.execute_input":"2022-04-21T15:51:08.337348Z","iopub.status.idle":"2022-04-21T15:51:08.342392Z","shell.execute_reply.started":"2022-04-21T15:51:08.337323Z","shell.execute_reply":"2022-04-21T15:51:08.341569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number if subjects \" + str(len(train[\"subject\"].unique())))\nprint(\"Number if sequences \" +str(len(train[\"sequence\"].unique())))","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:51:08.343603Z","iopub.execute_input":"2022-04-21T15:51:08.343813Z","iopub.status.idle":"2022-04-21T15:51:08.377106Z","shell.execute_reply.started":"2022-04-21T15:51:08.343788Z","shell.execute_reply":"2022-04-21T15:51:08.376115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_subject_check=train_labels.merge(train[[\"subject\",\"sequence\"]].copy().drop_duplicates(),how=\"left\",on=\"sequence\")\nstate_by_subject=train_subject_check.groupby(\"subject\")[[\"state\"]].mean().reset_index()\nstate_by_subject.sort_values(by=[\"state\"],inplace=True)\ndisplay(state_by_subject.head(10))\ndisplay(state_by_subject.tail(10))\n# There is correlation in subjects results, we might want to extract this later...","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:51:08.3783Z","iopub.execute_input":"2022-04-21T15:51:08.378886Z","iopub.status.idle":"2022-04-21T15:51:08.489583Z","shell.execute_reply.started":"2022-04-21T15:51:08.378843Z","shell.execute_reply":"2022-04-21T15:51:08.488764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# temporary...\n# train.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:51:08.490867Z","iopub.execute_input":"2022-04-21T15:51:08.491401Z","iopub.status.idle":"2022-04-21T15:51:08.494605Z","shell.execute_reply.started":"2022-04-21T15:51:08.491366Z","shell.execute_reply":"2022-04-21T15:51:08.493578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import random\n# subjects_list=sorted(set(train.subject))\n# selected_subject=random.choices(subjects_list, k=50)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:51:08.496011Z","iopub.execute_input":"2022-04-21T15:51:08.496474Z","iopub.status.idle":"2022-04-21T15:51:08.508033Z","shell.execute_reply.started":"2022-04-21T15:51:08.496435Z","shell.execute_reply":"2022-04-21T15:51:08.507025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train=train[train[\"subject\"].isin(selected_subject)].copy()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:51:08.510717Z","iopub.execute_input":"2022-04-21T15:51:08.511233Z","iopub.status.idle":"2022-04-21T15:51:08.521353Z","shell.execute_reply.started":"2022-04-21T15:51:08.511196Z","shell.execute_reply":"2022-04-21T15:51:08.520502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# subjects_list_test=sorted(set(test.subject))\n# selected_subject_test=random.choices(subjects_list_test, k=30)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:51:08.522466Z","iopub.execute_input":"2022-04-21T15:51:08.522825Z","iopub.status.idle":"2022-04-21T15:51:08.533768Z","shell.execute_reply.started":"2022-04-21T15:51:08.522782Z","shell.execute_reply":"2022-04-21T15:51:08.533057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test=test[test[\"subject\"].isin(selected_subject_test)].copy()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:51:08.534721Z","iopub.execute_input":"2022-04-21T15:51:08.534917Z","iopub.status.idle":"2022-04-21T15:51:08.547135Z","shell.execute_reply.started":"2022-04-21T15:51:08.534895Z","shell.execute_reply":"2022-04-21T15:51:08.546368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Variable construction <a name=\"vars\"></a>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:17px\"> \nSequence is described by multiple measurements using 13 sensors</p>\n<p style=\"font-size:17px\"> There is also a subject column, which indicates that subjects were involved in multiple measurements </p>\n\n<p style=\"font-size:17px\"> We need a methodology that is capable to generate features both on subject and on subject&sequence level</p>","metadata":{}},{"cell_type":"code","source":"%%time\ntrain_freq = train.sort_values(['subject', 'sequence', 'step']).groupby(['sequence', 'subject']).apply(create_frequencies)\ntrain_freq.reset_index(inplace=True)\n\ntest_freq = test.sort_values(['subject', 'sequence', 'step']).groupby(['sequence', 'subject']).apply(create_frequencies)\ntest_freq.reset_index(inplace=True)\nfreq_columns=list(train_freq.columns[2:])","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:51:08.548105Z","iopub.execute_input":"2022-04-21T15:51:08.548835Z","iopub.status.idle":"2022-04-21T15:53:10.185659Z","shell.execute_reply.started":"2022-04-21T15:51:08.548781Z","shell.execute_reply":"2022-04-21T15:53:10.184652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_freq.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:53:10.186839Z","iopub.execute_input":"2022-04-21T15:53:10.187333Z","iopub.status.idle":"2022-04-21T15:53:10.205838Z","shell.execute_reply.started":"2022-04-21T15:53:10.187297Z","shell.execute_reply":"2022-04-21T15:53:10.205045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metric_data_short={\n    \"mean_\": [np.nanmean],\n    \"std_\": [np.nanstd]\n}\n# mean and std achieves around 80-82% f1 score\n\n# lets extend these with new metrics\ndef auto_corr_1(x):\n    return np.corrcoef(x[1:],x[:-1])[0,1]\ndef auto_corr_2(x):\n    return np.corrcoef(x[2:],x[:-2])[0,1]\n\ndef p5(x):\n    return np.percentile(x,5)\ndef p10(x):\n    return np.percentile(x,10)\ndef p25(x):\n    return np.percentile(x,25)\ndef p75(x):\n    return np.percentile(x,75)\ndef p90(x):\n    return np.percentile(x,90)\ndef p95(x):\n    return np.percentile(x,95)\n\n\ndef mean_diff(x):\n    dd=x.diff().dropna()\n    return np.nanmean(dd)\n\ndef std_diff(x):\n    dd=x.diff().dropna()\n    return np.nanstd(dd)\n\ndef auto_corr_1_diff(x):\n    dd=x.diff().dropna()\n    return np.corrcoef(dd[1:],dd[:-1])[0,1]\n\ndef auto_corr_2_diff(x):\n    dd=x.diff().dropna()\n    return np.corrcoef(dd[2:],dd[:-2])[0,1]\n\ndef skew_diff(x):\n    dd=x.diff().dropna()\n    return skew(dd)\n\ndef kurtosis_diff(x):\n    dd=x.diff().dropna()\n    return kurtosis(dd)\n\ndef dp5(x):\n    dd=x.diff().dropna()\n    return np.percentile(dd,5)\ndef dp10(x):\n    dd=x.diff().dropna()\n    return np.percentile(dd,10)\ndef dp25(x):\n    dd=x.diff().dropna()\n    return np.percentile(dd,25)\ndef dp75(x):\n    dd=x.diff().dropna()\n    return np.percentile(dd,75)\ndef dp90(x):\n    dd=x.diff().dropna()\n    return np.percentile(dd,90)\ndef dp95(x):\n    dd=x.diff().dropna()\n    return np.percentile(dd,95)\n\n\nmetric_data={\n    \"mean_\": [np.nanmean],\n    \"std_\": [np.nanstd],\n    \"median_\": [np.median],\n    \"p05_\": [p5],\n    \"p10_\": [p10],\n    \"p25_\": [p25],\n    \"p75_\": [p75],\n    \"p90_\": [p90],\n    \"p95_\": [p95],\n    \"min_\": [np.nanmin],\n    \"max_\": [np.nanmax],\n    \"skew_\": [skew],\n    \"kurtosis_\": [kurtosis],\n    \"corr1_\": [auto_corr_1],\n    \"corr2_\": [auto_corr_2],\n    \"d_mean_\": [mean_diff],\n    \"d_std_\": [std_diff],\n    \"d_corr1_\": [auto_corr_1_diff],\n    \"d_corr2_\": [auto_corr_2_diff],\n    \"d_skew_\": [skew_diff],\n    \"d_kurtosis_\": [kurtosis_diff],\n    \"d_p05_\": [dp5],\n    \"d_p10_\": [dp10],\n    \"d_p25_\": [dp25],\n    \"d_p75_\": [dp75],\n    \"d_p90_\": [dp90],\n    \"d_p95_\": [dp95]\n}\n\nmetric_data_subj={\n    \"mean_\": [np.nanmean],\n    \"std_\": [np.nanstd],\n    \"median_\": [np.median],\n    \"p05_\": [p5],\n    \"p10_\": [p10],\n    \"p25_\": [p25],\n    \"p75_\": [p75],\n    \"p90_\": [p90],\n    \"p95_\": [p95],\n    \"min_\": [np.nanmin],\n    \"max_\": [np.nanmax],\n    \"skew_\": [skew],\n    \"kurtosis_\": [kurtosis]\n}\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:53:10.206916Z","iopub.execute_input":"2022-04-21T15:53:10.207127Z","iopub.status.idle":"2022-04-21T15:53:10.225863Z","shell.execute_reply.started":"2022-04-21T15:53:10.2071Z","shell.execute_reply":"2022-04-21T15:53:10.225157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n    train_features, gen_col_train = generate_features(df=train, \n                                                      metric_data=metric_data, \n                                                      group_variables=[\"sequence\"], \n                                                      sensor_identifiers=sensor_numbs, \n                                                      suffix=\"\")\n    test_features, gen_col_test = generate_features(df=test, \n                                                    metric_data=metric_data, \n                                                    group_variables=[\"sequence\"], \n                                                    sensor_identifiers=sensor_numbs, \n                                                    suffix=\"\")\n\n    train_features_s, gen_col_train_s = generate_features(df=train, \n                                                      metric_data=metric_data_subj, \n                                                      group_variables=[\"subject\"], \n                                                      sensor_identifiers=sensor_numbs, \n                                                      suffix=\"_subj\")\n    test_features_s, gen_col_test_s = generate_features(df=test, \n                                                    metric_data=metric_data_subj, \n                                                    group_variables=[\"subject\"], \n                                                    sensor_identifiers=sensor_numbs, \n                                                    suffix=\"_subj\")","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:53:10.227406Z","iopub.execute_input":"2022-04-21T15:53:10.227645Z","iopub.status.idle":"2022-04-21T17:19:10.869729Z","shell.execute_reply.started":"2022-04-21T15:53:10.227616Z","shell.execute_reply":"2022-04-21T17:19:10.867924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of sequences per subject\ntrain_subj_len=train[[\"sequence\",\"subject\"]].groupby(\"subject\").count().reset_index()\ntrain_subj_len.columns=[\"subject\",\"sequence_len\"]\ntest_subj_len=test[[\"sequence\",\"subject\"]].groupby(\"subject\").count().reset_index()\ntest_subj_len.columns=[\"subject\",\"sequence_len\"]","metadata":{"execution":{"iopub.status.busy":"2022-04-21T17:19:10.873916Z","iopub.execute_input":"2022-04-21T17:19:10.87432Z","iopub.status.idle":"2022-04-21T17:19:10.938645Z","shell.execute_reply.started":"2022-04-21T17:19:10.874242Z","shell.execute_reply":"2022-04-21T17:19:10.937758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merge the tables together for train and test\ntrain_feature_final=train_labels.merge(train[[\"subject\",\"sequence\"]].copy().drop_duplicates(),how=\"left\",on=\"sequence\")\ntrain_feature_final=train_feature_final.merge(train_features,how=\"left\",on=\"sequence\")\ntrain_feature_final=train_feature_final.merge(train_features_s,how=\"left\",on=\"subject\")\ntrain_feature_final=train_feature_final.merge(train_subj_len,how=\"left\",on=\"subject\")\n\ntrain_feature_final=train_feature_final.merge(train_freq,how=\"left\",on=['sequence', 'subject'])\n\nexplanatory_variables=gen_col_train+gen_col_train_s+[\"sequence_len\"]+freq_columns\n\ntest_feature_final=test[[\"subject\",\"sequence\"]].copy().drop_duplicates()\ntest_feature_final=test_feature_final.merge(test_features,how=\"left\",on=\"sequence\")\ntest_feature_final=test_feature_final.merge(test_features_s,how=\"left\",on=\"subject\")\ntest_feature_final=test_feature_final.merge(test_subj_len,how=\"left\",on=\"subject\")\ntest_feature_final=test_feature_final.merge(test_freq,how=\"left\",on=['sequence', 'subject'])","metadata":{"execution":{"iopub.status.busy":"2022-04-21T17:19:10.93967Z","iopub.execute_input":"2022-04-21T17:19:10.93987Z","iopub.status.idle":"2022-04-21T17:19:11.719192Z","shell.execute_reply.started":"2022-04-21T17:19:10.939845Z","shell.execute_reply":"2022-04-21T17:19:11.718354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_feature_final.fillna(0,inplace=True)\ntrain_feature_final.fillna(0,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T17:19:11.720438Z","iopub.execute_input":"2022-04-21T17:19:11.72067Z","iopub.status.idle":"2022-04-21T17:19:11.991177Z","shell.execute_reply.started":"2022-04-21T17:19:11.720635Z","shell.execute_reply":"2022-04-21T17:19:11.990308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Transform train and test to 0-1 scale\nscaler = MinMaxScaler()\ntrain_feature_final.loc[:,explanatory_variables]=scaler.fit_transform(train_feature_final.loc[:,explanatory_variables])\ntest_feature_final.loc[:,explanatory_variables]=scaler.transform(test_feature_final.loc[:,explanatory_variables])","metadata":{"execution":{"iopub.status.busy":"2022-04-21T17:19:11.992356Z","iopub.execute_input":"2022-04-21T17:19:11.992562Z","iopub.status.idle":"2022-04-21T17:19:18.899964Z","shell.execute_reply.started":"2022-04-21T17:19:11.992537Z","shell.execute_reply":"2022-04-21T17:19:18.899315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we have skewed distributions, we apply a sqrt functional form which translates the distribution to a least skewed one.\nfor var in explanatory_variables:\n    if (train_feature_final[var].skew()) > 3 and not train_feature_final[var].min()<0.0:\n        train_feature_final[var]=np.sqrt(train_feature_final[var])\n        test_feature_final[var]=np.sqrt(test_feature_final[var])\n        \ntest_feature_final.fillna(0,inplace=True)\ntrain_feature_final.fillna(0,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T17:19:18.900932Z","iopub.execute_input":"2022-04-21T17:19:18.901683Z","iopub.status.idle":"2022-04-21T17:19:19.802388Z","shell.execute_reply.started":"2022-04-21T17:19:18.90165Z","shell.execute_reply":"2022-04-21T17:19:19.801415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. EDA for the created features  <a name=\"vars2\"></a>","metadata":{}},{"cell_type":"code","source":"selected_features=[\"kurtosis_04\", \n                   \"sequence_len\",\n                   \"std_02\",\n                   \"kurtosis_10\",\n                   \"sensor_09_freq_0\",\n                   \"sensor_09_freq_1\",\n                   \"sensor_01_freq_0\",\n                   \"sensor_02_freq_2\",\n                   \"p05_09\",\n                   \"max_05\",\n                   \"p25_10\",\n                   \"p10_04\"]","metadata":{"execution":{"iopub.status.busy":"2022-04-21T17:19:19.803934Z","iopub.execute_input":"2022-04-21T17:19:19.804238Z","iopub.status.idle":"2022-04-21T17:19:19.809136Z","shell.execute_reply.started":"2022-04-21T17:19:19.804198Z","shell.execute_reply":"2022-04-21T17:19:19.808287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for feature in selected_features:\n    plt.figure(figsize=(20,6))\n    plt.hist(train_feature_final[train_feature_final[\"state\"]<1][feature],bins=200, density=True, label='State : 0',color='#CBC3E3')\n    plt.hist(train_feature_final[train_feature_final[\"state\"]>0][feature],bins=200, density=True, label='State : 1',color='#F4B123', alpha = 0.5)\n    plt.ylabel('Frequency')\n    plt.title('Distribution of values for feature: '+feature, fontsize=15)\n    #plt.gca().yaxis.set_major_formatter(PercentFormatter(xmax=1, decimals=1))\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T17:19:19.810066Z","iopub.execute_input":"2022-04-21T17:19:19.810687Z","iopub.status.idle":"2022-04-21T17:19:33.134119Z","shell.execute_reply.started":"2022-04-21T17:19:19.810654Z","shell.execute_reply":"2022-04-21T17:19:33.133233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# I deploy a simple selection procedure using a logit regression as a basis.","metadata":{}},{"cell_type":"code","source":"%%time\nlog00 =LogisticRegression(random_state=42,max_iter=12000, C=1.6)\n# here I increased iteration number from the low default value to avoid warnings\n# regularization param, arbitrarily decreased to respect large number of variables (default C = 1.0)\nlog00, feat_imp=fit_model_using_classifier(log00, \n                                          dtrain=train_feature_final, \n                                          predictors=explanatory_variables,\n                                          repeat=5,\n                                          scoring=\"roc_auc\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nperm_result = permutation_importance(log00, \n                                     X=train_feature_final[explanatory_variables],\n                                     y=train_feature_final[\"state\"], \n                                     n_repeats=20,\n                                     scoring=\"roc_auc\",\n                                     random_state=42)\n\nres_select=pd.DataFrame({\n    \"variable\": explanatory_variables,\n    \"importances_mean\": perm_result.importances_mean*100,\n    \"importances_std\": perm_result.importances_std\n})\nres_select.sort_values(by=[\"importances_mean\"],inplace=True,ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"super_short_list=list(res_select[res_select[\"importances_mean\"]>1][\"variable\"])\nshort_list=list(res_select[res_select[\"importances_mean\"]>0.1][\"variable\"])\nlonger_list=list(res_select[res_select[\"importances_mean\"]>0.05][\"variable\"])\n\nprint(\"Most important features\")\nprint(super_short_list)\n\nprint(\"Original number of features: \"+str(len(explanatory_variables)))\nprint(\"Short list number of features: \"+str(len(longer_list)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I set the variable list to the identified longer list\n# We have way to many variables by default more than 900, so we need to scale down.\n# On the other hand I think some lesser important vars still have an impact so I keep those, try to keep those with the longer list.\nexplanatory_variables=longer_list.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Estimate models  <a name=\"modest\"></a>","metadata":{}},{"cell_type":"code","source":"scoref=\"roc_auc\"\nrepeat_numb=3","metadata":{"execution":{"iopub.status.busy":"2022-04-21T17:19:33.135566Z","iopub.execute_input":"2022-04-21T17:19:33.136254Z","iopub.status.idle":"2022-04-21T17:19:33.140826Z","shell.execute_reply.started":"2022-04-21T17:19:33.136214Z","shell.execute_reply":"2022-04-21T17:19:33.140037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ngbm0 =GradientBoostingClassifier(random_state=42)\ngbm0, feat_imp=fit_model_using_classifier(gbm0, \n                                          dtrain=train_feature_final, \n                                          predictors=explanatory_variables,\n                                          repeat=repeat_numb,\n                                          scoring=scoref)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T17:19:33.141861Z","iopub.execute_input":"2022-04-21T17:19:33.142051Z","iopub.status.idle":"2022-04-21T18:28:45.54126Z","shell.execute_reply.started":"2022-04-21T17:19:33.142028Z","shell.execute_reply":"2022-04-21T18:28:45.54023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_model_submission=pd.DataFrame({\n    \"sequence\": test_feature_final[\"sequence\"],\n    \"state\": gbm0.predict_proba(test_feature_final[explanatory_variables])[:,1]})\nxgb_model_submission.to_csv(\"xgb_model_submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T18:28:45.546339Z","iopub.execute_input":"2022-04-21T18:28:45.54655Z","iopub.status.idle":"2022-04-21T18:28:45.713006Z","shell.execute_reply.started":"2022-04-21T18:28:45.546525Z","shell.execute_reply":"2022-04-21T18:28:45.712315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_feature_final.tail()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T18:28:45.714165Z","iopub.execute_input":"2022-04-21T18:28:45.714496Z","iopub.status.idle":"2022-04-21T18:28:45.732006Z","shell.execute_reply.started":"2022-04-21T18:28:45.714465Z","shell.execute_reply":"2022-04-21T18:28:45.731203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\next0 =ExtraTreesClassifier(random_state=42,min_samples_split=100,min_samples_leaf=50, n_jobs=-1)\n# added arbitrary number for min_sample_split, min_samples_leaf to avoid (super) overfitting... (I got f1=1 insample with default settings)\next0, feat_imp=fit_model_using_classifier(ext0, \n                                          dtrain=train_feature_final, \n                                          predictors=explanatory_variables,\n                                          repeat=repeat_numb,\n                                          scoring=scoref)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T18:28:45.733063Z","iopub.execute_input":"2022-04-21T18:28:45.733461Z","iopub.status.idle":"2022-04-21T18:29:16.192785Z","shell.execute_reply.started":"2022-04-21T18:28:45.73343Z","shell.execute_reply":"2022-04-21T18:29:16.191851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extra_model_submission=pd.DataFrame({\n    \"sequence\": test_feature_final[\"sequence\"],\n    \"state\": ext0.predict_proba(test_feature_final[explanatory_variables])[:,1]})\nextra_model_submission.to_csv(\"extra_model_submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T18:29:16.194289Z","iopub.execute_input":"2022-04-21T18:29:16.194498Z","iopub.status.idle":"2022-04-21T18:29:16.473646Z","shell.execute_reply.started":"2022-04-21T18:29:16.194472Z","shell.execute_reply":"2022-04-21T18:29:16.472938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nlog0 =LogisticRegression(random_state=42,max_iter=12000, C=1.6)\n# here I increased iteration number from the low default value to avoid warnings\n# regularization param, arbitrarily decreased to respect large number of variables (default C = 1.0)\nlog0, feat_imp=fit_model_using_classifier(log0, \n                                          dtrain=train_feature_final, \n                                          predictors=explanatory_variables,\n                                          repeat=repeat_numb*3,\n                                          scoring=scoref)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T18:29:16.474904Z","iopub.execute_input":"2022-04-21T18:29:16.475102Z","iopub.status.idle":"2022-04-21T18:38:36.004138Z","shell.execute_reply.started":"2022-04-21T18:29:16.475079Z","shell.execute_reply":"2022-04-21T18:38:36.00321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_coef=pd.DataFrame({\n                    \"variable\":explanatory_variables,\n                    \"coeff_abs\":list(abs(log0.coef_)[0])})\nlog_coef.sort_values(by=[\"coeff_abs\"],ascending=[False],inplace=True)\nlog_coef.set_index(\"variable\",inplace=True)\nplt.figure(figsize=(20,6))\nlog_coef.head(60)[\"coeff_abs\"].plot(kind='bar', title='Coefficient estimates',fontsize=12, color=\"#CBC3E3\")\nplt.ylabel('Coefficient estimates (absolute value)');","metadata":{"execution":{"iopub.status.busy":"2022-04-21T18:38:36.006019Z","iopub.execute_input":"2022-04-21T18:38:36.006638Z","iopub.status.idle":"2022-04-21T18:38:37.147074Z","shell.execute_reply.started":"2022-04-21T18:38:36.006589Z","shell.execute_reply":"2022-04-21T18:38:37.146226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_model_submission=pd.DataFrame({\n    \"sequence\": test_feature_final[\"sequence\"],\n    \"state\": log0.predict_proba(test_feature_final[explanatory_variables])[:,1]})\nlog_model_submission.to_csv(\"log_model_submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T18:38:37.148338Z","iopub.execute_input":"2022-04-21T18:38:37.14857Z","iopub.status.idle":"2022-04-21T18:38:37.234876Z","shell.execute_reply.started":"2022-04-21T18:38:37.148542Z","shell.execute_reply":"2022-04-21T18:38:37.233738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nrfs0 =RandomForestClassifier(random_state=42,n_estimators=300, min_samples_split=200, min_samples_leaf=100, n_jobs=-1)\n# added arbitrary number for min_sample_split to avoid overfitting... (I got f1=1 insample with default settings)\nrfs0, feat_imp=fit_model_using_classifier(rfs0, \n                                          dtrain=train_feature_final, \n                                          predictors=explanatory_variables,\n                                          repeat=repeat_numb,\n                                          scoring=scoref)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T18:38:37.236529Z","iopub.execute_input":"2022-04-21T18:38:37.236844Z","iopub.status.idle":"2022-04-21T18:43:21.060006Z","shell.execute_reply.started":"2022-04-21T18:38:37.236806Z","shell.execute_reply":"2022-04-21T18:43:21.058999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfs_model_submission=pd.DataFrame({\n    \"sequence\": test_feature_final[\"sequence\"],\n    \"state\": rfs0.predict_proba(test_feature_final[explanatory_variables])[:,1]})\nrfs_model_submission.to_csv(\"rfs_model_submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T18:43:21.061522Z","iopub.execute_input":"2022-04-21T18:43:21.061768Z","iopub.status.idle":"2022-04-21T18:43:21.445177Z","shell.execute_reply.started":"2022-04-21T18:43:21.061739Z","shell.execute_reply":"2022-04-21T18:43:21.444336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# svc0 = SVC(random_state=42, max_iter=12000,kernel=\"linear\",probability=True)\n# svc0, feat_imp=fit_model_using_classifier(svc0, \n#                                           dtrain=train_feature_final, \n#                                           predictors=explanatory_variables,\n#                                           repeat=repeat_numb,\n#                                           scoring=scoref)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T18:43:21.446292Z","iopub.execute_input":"2022-04-21T18:43:21.446526Z","iopub.status.idle":"2022-04-21T18:43:21.450172Z","shell.execute_reply.started":"2022-04-21T18:43:21.446499Z","shell.execute_reply":"2022-04-21T18:43:21.44955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# svc_model_submission=pd.DataFrame({\n#     \"sequence\": test_feature_final[\"sequence\"],\n#     \"state\": svc0.predict(test_feature_final[explanatory_variables])})\n# svc_model_submission.to_csv(\"svc_model_submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T18:43:21.451753Z","iopub.execute_input":"2022-04-21T18:43:21.452067Z","iopub.status.idle":"2022-04-21T18:43:21.463767Z","shell.execute_reply.started":"2022-04-21T18:43:21.452029Z","shell.execute_reply":"2022-04-21T18:43:21.462923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nlda0 = LinearDiscriminantAnalysis()\nlda0, feat_imp=fit_model_using_classifier(lda0, \n                                          dtrain=train_feature_final, \n                                          predictors=explanatory_variables,\n                                          repeat=repeat_numb,\n                                          scoring=scoref)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T18:43:21.465446Z","iopub.execute_input":"2022-04-21T18:43:21.465763Z","iopub.status.idle":"2022-04-21T18:44:09.724233Z","shell.execute_reply.started":"2022-04-21T18:43:21.465733Z","shell.execute_reply":"2022-04-21T18:44:09.723344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda_model_submission=pd.DataFrame({\n    \"sequence\": test_feature_final[\"sequence\"],\n    \"state\": lda0.predict_proba(test_feature_final[explanatory_variables])[:,1]})\nlda_model_submission.to_csv(\"lda_model_submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T18:44:09.725645Z","iopub.execute_input":"2022-04-21T18:44:09.726204Z","iopub.status.idle":"2022-04-21T18:44:09.836153Z","shell.execute_reply.started":"2022-04-21T18:44:09.726161Z","shell.execute_reply":"2022-04-21T18:44:09.835177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Ensemble approach <a name=\"ens\"></a>","metadata":{}},{"cell_type":"code","source":"def get_models():\n    models = list()\n    models.append(('extra', ext0))\n    models.append(('xgb', gbm0))\n    models.append(('log', log0))\n    models.append(('rfs0', rfs0))\n    models.append(('lda0', lda0))\n    return models","metadata":{"execution":{"iopub.status.busy":"2022-04-21T18:44:09.841901Z","iopub.execute_input":"2022-04-21T18:44:09.844621Z","iopub.status.idle":"2022-04-21T18:44:09.853889Z","shell.execute_reply.started":"2022-04-21T18:44:09.844565Z","shell.execute_reply":"2022-04-21T18:44:09.852984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(42,50):\n    train_in, train_out=train_test_split(train_feature_final,test_size=0.33, random_state=i)\n    blender, meta_X=fit_ensemble(models=get_models(),\n                     X_train=train_in[explanatory_variables],\n                     X_val=train_out[explanatory_variables],\n                     y_train=train_in[\"state\"],\n                     y_val=train_out[\"state\"], soft_vote=True)\n\n    pred_state=predict_ensemble(models=get_models(), blender=blender, X_test=train_out[explanatory_variables])\n    print(metrics.roc_auc_score(train_out[\"state\"],pred_state))","metadata":{"execution":{"iopub.status.busy":"2022-04-21T18:44:09.859112Z","iopub.execute_input":"2022-04-21T18:44:09.86165Z","iopub.status.idle":"2022-04-21T19:44:03.247551Z","shell.execute_reply.started":"2022-04-21T18:44:09.861593Z","shell.execute_reply":"2022-04-21T19:44:03.246228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"blender_final, _ =fit_ensemble(models=get_models(),\n             X_train=train_feature_final[explanatory_variables],\n             X_val=None,\n             y_train=train_feature_final[\"state\"],\n             y_val=None)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T19:44:03.253581Z","iopub.execute_input":"2022-04-21T19:44:03.253939Z","iopub.status.idle":"2022-04-21T19:55:36.157391Z","shell.execute_reply.started":"2022-04-21T19:44:03.253894Z","shell.execute_reply":"2022-04-21T19:55:36.156072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ens_model_submission=pd.DataFrame({\n    \"sequence\": test_feature_final[\"sequence\"],\n    \"state\": predict_ensemble(models=get_models(), blender=blender_final, X_test=test_feature_final[explanatory_variables])\n})\nens_model_submission.to_csv(\"ens_model_submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T19:55:36.164596Z","iopub.execute_input":"2022-04-21T19:55:36.168448Z","iopub.status.idle":"2022-04-21T19:55:37.02441Z","shell.execute_reply.started":"2022-04-21T19:55:36.168387Z","shell.execute_reply":"2022-04-21T19:55:37.023335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Acknowledgement  <a name=\"ack\"></a>","metadata":{}},{"cell_type":"code","source":"\"\"\"\n    I got the idea to use freuencies and Fourier transform by looking at Pavel Salikov's notebook\n    https://www.kaggle.com/code/matanivanov/lgbm-with-fourier-transform\n    \n    The fit_model_using_classifier function is based on this article\n    //www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n    \n    To create my ensemble solution I used this source:\n    https://machinelearningmastery.com/blending-ensemble-machine-learning-with-python/\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-04-21T19:55:37.02621Z","iopub.execute_input":"2022-04-21T19:55:37.02654Z","iopub.status.idle":"2022-04-21T19:55:37.038739Z","shell.execute_reply.started":"2022-04-21T19:55:37.026502Z","shell.execute_reply":"2022-04-21T19:55:37.034633Z"},"trusted":true},"execution_count":null,"outputs":[]}]}