{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Analysis using Neural Networks\n## Introduction\nThanks for coming to see this.\nThis notebook will show you how to build Neural Networks for Pytorch beginners!  ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom tqdm import tqdm\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('Active device：', device)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:31:47.865709Z","iopub.execute_input":"2022-04-05T10:31:47.866029Z","iopub.status.idle":"2022-04-05T10:31:47.874732Z","shell.execute_reply.started":"2022-04-05T10:31:47.866001Z","shell.execute_reply":"2022-04-05T10:31:47.873749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing\n### Data loading\nLoad data by using pandas.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-apr-2022/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')\ntrain_label_df = pd.read_csv('../input/tabular-playground-series-apr-2022/train_labels.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:03:06.092469Z","iopub.execute_input":"2022-04-05T10:03:06.092968Z","iopub.status.idle":"2022-04-05T10:03:18.240351Z","shell.execute_reply.started":"2022-04-05T10:03:06.092918Z","shell.execute_reply":"2022-04-05T10:03:18.239655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check the form and content of the data","metadata":{}},{"cell_type":"code","source":"print(train_df.shape) \ntrain_df.head(70)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:03:18.243876Z","iopub.execute_input":"2022-04-05T10:03:18.244245Z","iopub.status.idle":"2022-04-05T10:03:18.279809Z","shell.execute_reply.started":"2022-04-05T10:03:18.244212Z","shell.execute_reply":"2022-04-05T10:03:18.278965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_label_df.shape) \ntrain_label_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:03:18.282169Z","iopub.execute_input":"2022-04-05T10:03:18.282491Z","iopub.status.idle":"2022-04-05T10:03:18.292187Z","shell.execute_reply.started":"2022-04-05T10:03:18.282434Z","shell.execute_reply":"2022-04-05T10:03:18.291388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_df.shape)\ntest_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:03:18.293249Z","iopub.execute_input":"2022-04-05T10:03:18.293462Z","iopub.status.idle":"2022-04-05T10:03:18.320517Z","shell.execute_reply.started":"2022-04-05T10:03:18.293436Z","shell.execute_reply":"2022-04-05T10:03:18.319721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploratory Data Analysis\n","metadata":{}},{"cell_type":"markdown","source":"Use pandas' describe function to check information on training data  \n*   sensor_02, 03, 08, 12 have a median value of 0\n*   Most sensors have first and third quartiles and min, max values far apart","metadata":{}},{"cell_type":"code","source":"train_df.loc[:, 'sensor_00': 'sensor_12'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:03:18.321755Z","iopub.execute_input":"2022-04-05T10:03:18.322249Z","iopub.status.idle":"2022-04-05T10:03:19.219713Z","shell.execute_reply.started":"2022-04-05T10:03:18.322219Z","shell.execute_reply":"2022-04-05T10:03:19.218878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, combine data to process training and test data in batches","metadata":{}},{"cell_type":"code","source":"ntrain = train_df.shape[0]\nall_data = pd.concat((train_df, test_df))#.reset_index(drop=True)\nprint(all_data.shape)\nall_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:09:21.159469Z","iopub.execute_input":"2022-04-05T10:09:21.15982Z","iopub.status.idle":"2022-04-05T10:09:21.282258Z","shell.execute_reply.started":"2022-04-05T10:09:21.159786Z","shell.execute_reply":"2022-04-05T10:09:21.28128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check data type","metadata":{}},{"cell_type":"code","source":"all_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:09:21.795451Z","iopub.execute_input":"2022-04-05T10:09:21.795954Z","iopub.status.idle":"2022-04-05T10:09:21.80841Z","shell.execute_reply.started":"2022-04-05T10:09:21.795905Z","shell.execute_reply":"2022-04-05T10:09:21.807396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculate the first-order difference in step","metadata":{}},{"cell_type":"code","source":"features = all_data.columns.tolist()[3:]\nfor feature in features:\n    all_data[feature + '_lag1'] = all_data.groupby('sequence')[feature].shift(1)\n    all_data.fillna(0, inplace=True)\n    all_data[feature + '_diff1'] = all_data[feature] - all_data[feature + '_lag1']\n    all_data.drop(feature+'_lag1', axis=1, inplace=True)\nall_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:09:25.217581Z","iopub.execute_input":"2022-04-05T10:09:25.218078Z","iopub.status.idle":"2022-04-05T10:09:35.709124Z","shell.execute_reply.started":"2022-04-05T10:09:25.21803Z","shell.execute_reply":"2022-04-05T10:09:35.708269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Aggregate data by sequence.\nObtain average, standard deviation, maximum, minimum, sum, median, first quantile and third quantile data for each sequence.","metadata":{}},{"cell_type":"code","source":"features = all_data.columns[3:]\nprint(features)\n# mean\nmean_seq = all_data.groupby('sequence').mean()\nall_data_summ = mean_seq.rename(columns={s: s+'_mean' for s in features})\n# std\nstd_seq = all_data.groupby('sequence').std().drop(['step', 'subject'], axis=1)\nall_data_summ = pd.concat([all_data_summ, std_seq.rename(columns={s: s+'_std' for s in features})], axis=1)\n# max\nmax_seq = all_data.groupby('sequence').max().drop(['step', 'subject'], axis=1)\nall_data_summ = pd.concat([all_data_summ, max_seq.rename(columns={s: s+'_max' for s in features})], axis=1)\n# min\nmin_seq = all_data.groupby('sequence').min().drop(['step', 'subject'], axis=1)\nall_data_summ = pd.concat([all_data_summ, min_seq.rename(columns={s: s+'_min' for s in features})], axis=1)\n# sum\nsum_seq = all_data.groupby('sequence').sum().drop(['step', 'subject'], axis=1)\nall_data_summ = pd.concat([all_data_summ, sum_seq.rename(columns={s: s+'_sum' for s in features})], axis=1)\n# median\nmedi_seq = all_data.groupby('sequence').median().drop(['step', 'subject'], axis=1)\nall_data_summ = pd.concat([all_data_summ, medi_seq.rename(columns={s: s+'_medi' for s in features})], axis=1)\n# first quantile\nquan1_seq = all_data.groupby('sequence').quantile(0.25).drop(['step', 'subject'], axis=1)\nall_data_summ = pd.concat([all_data_summ, quan1_seq.rename(columns={s: s+'_quantile1' for s in features})], axis=1)\n# third quartile\nquan3_seq = all_data.groupby('sequence').quantile(0.75).drop(['step', 'subject'], axis=1)\nall_data_summ = pd.concat([all_data_summ, quan3_seq.rename(columns={s: s+'_quantile3' for s in features})], axis=1)\nprint(all_data_summ.columns.tolist())\nall_data_summ.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:09:42.697949Z","iopub.execute_input":"2022-04-05T10:09:42.698565Z","iopub.status.idle":"2022-04-05T10:10:23.114104Z","shell.execute_reply.started":"2022-04-05T10:09:42.698512Z","shell.execute_reply":"2022-04-05T10:10:23.113316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data_summ.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:10:23.115694Z","iopub.execute_input":"2022-04-05T10:10:23.115925Z","iopub.status.idle":"2022-04-05T10:10:23.135929Z","shell.execute_reply.started":"2022-04-05T10:10:23.115896Z","shell.execute_reply":"2022-04-05T10:10:23.135351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check missing values","metadata":{}},{"cell_type":"code","source":"all_data_na = (all_data_summ.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(22)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:10:23.137042Z","iopub.execute_input":"2022-04-05T10:10:23.137722Z","iopub.status.idle":"2022-04-05T10:10:23.173291Z","shell.execute_reply.started":"2022-04-05T10:10:23.137686Z","shell.execute_reply":"2022-04-05T10:10:23.172698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Restore the data","metadata":{}},{"cell_type":"code","source":"train_df = all_data_summ[:ntrain//60]\ntest_df = all_data_summ[ntrain//60:]\nprint(train_df.shape, test_df.shape)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:10:23.174657Z","iopub.execute_input":"2022-04-05T10:10:23.175352Z","iopub.status.idle":"2022-04-05T10:10:23.205311Z","shell.execute_reply.started":"2022-04-05T10:10:23.175315Z","shell.execute_reply":"2022-04-05T10:10:23.204498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trainig\n### Split data\nSplit training data into training data and test data using train_test_split in sklern","metadata":{}},{"cell_type":"code","source":"test_size = 0.20\nfeatures = train_df.columns[2:]\nX, y = train_df[features].values, train_label_df['state'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y)\nprint(X_train.shape, X_test.shape)\nprint(y_train.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:10:35.272014Z","iopub.execute_input":"2022-04-05T10:10:35.27263Z","iopub.status.idle":"2022-04-05T10:10:35.400075Z","shell.execute_reply.started":"2022-04-05T10:10:35.27258Z","shell.execute_reply":"2022-04-05T10:10:35.398991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Model\nThis section defines the model.\nThe three hidden layers of the network are 128, 64, and 32 nodes, respectively.  \nAlso, train and predict were added to this Class.","metadata":{}},{"cell_type":"code","source":"# the parameter of hidden layer\nlayer_2 = 128\nlayer_3 = 64\nlayer_4 = 32\n# define the class\nclass Net(nn.Module):\n    def __init__(self, input_size):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(input_size, layer_2)\n        self.bn1 = nn.BatchNorm1d(layer_2)\n        self.fc2 = nn.Linear(layer_2, layer_3)\n        self.bn2 = nn.BatchNorm1d(layer_3)\n        self.fc3 = nn.Linear(layer_3, layer_4)\n        self.bn3 = nn.BatchNorm1d(layer_4)\n        self.fc4 = nn.Linear(layer_4, 1)\n        self.dropout = nn.Dropout(0.4)\n        self.stdsc = StandardScaler()\n        self.logs_train = [[], [np.inf]]\n        self.logs_valid = [[], [np.inf]]\n\n    def forward(self, x):\n        x = self.bn1(F.relu(self.fc1(x)))\n        x = self.dropout(x)\n        x = self.bn2(F.relu(self.fc2(x)))\n        x = self.dropout(x)\n        x = self.bn3(F.relu(self.fc3(x)))\n        x = self.dropout(x)\n        x = torch.sigmoid(self.fc4(x))\n        return x\n\n    def fit(self, X, y, num_epochs=100, batch_size=1080):\n        # check whether GPU is available\n        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n        print('Active dvice：', device)\n        ####   Data processing↓\n        # standerd scaler\n        X = self.stdsc.fit_transform(X)\n        # Index acquisition to randomly rearrange the data set\n        num_train = len(X)\n        indices_train = list(range(num_train))\n        # create dataloder for read pytorch model\n        train_sampler = SubsetRandomSampler(indices_train)\n        train = torch.utils.data.TensorDataset(torch.Tensor(X), torch.tensor(y))\n        dataloader = torch.utils.data.DataLoader(train,sampler=train_sampler, batch_size=batch_size)\n        ####    training setup \n        # Setting up optimization methods\n        lr = 0.01\n        beta1, beta2 = 0.0, 0.9\n        optimizer = torch.optim.Adagrad(self.parameters(), lr)#, [beta1, beta2])\n        # Define the error function\n        criterion = nn.MSELoss()\n        #　Network to GPU\n        self.to(device)\n        #　Model in learning mode\n        self.train()\n        #　If the network is somewhat fixed, make it faster\n        torch.backends.cudnn.benchark = True\n        #　Save batch size\n        batch_size = dataloader.batch_size\n        #　Set iteration counter\n        iteration = 1\n        #epoch loop\n        for epoch in tqdm(range(num_epochs)):\n            # Save start time\n            t_epoch_strat = time.time()\n            epoch_loss = 0.0\n#             print('-------------')\n#             print('Epoch {}/{}'.format(epoch, num_epochs))\n#             print('-------------')\n#             print(' (train) ')\n            ### training\n            # Loop to retrieve minibatch by minibatch from the data loader\n            for data, targets in dataloader:\n                # Converted to be handled by GPU\n                data = data.to(device)\n                targets = targets.to(device)\n                # gradient initialization\n                optimizer.zero_grad()\n                # Get output\n                output = self.forward(data)\n                output = output.view(1,-1)[0]\n                targets = targets.to(torch.float32)\n                # Calculate error\n                loss = criterion(output, targets)\n                # backpropergation\n                loss.backward()\n                # step\n                optimizer.step()\n                \n                # memory error\n                epoch_loss += loss.item()\n                iteration += 1\n            # loss per epoch\n            t_epoch_finish = time.time()\n#             print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n#                 epoch+1, \n#                 epoch_loss/num_train,\n#                 ))\n#             print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_strat))\n            # save parameter\n            if epoch_loss/num_train < min(self.logs_train[1]):\n#                 print('--save model--')\n                torch.save(self.state_dict(), './models')\n            self.logs_train[0].append(epoch+1)\n            self.logs_train[1].append(epoch_loss/num_train)\n            \n    def predict(self, X):\n        valid_loss = 0.0\n        num_valid = len(X)\n        indices_valid = list(range(num_valid))\n        # standerd scaler\n        X = self.stdsc.transform(X)\n        # create dataloader\n        valid = torch.utils.data.TensorDataset(torch.Tensor(X))\n        valid_loader = torch.utils.data.DataLoader(valid,sampler=indices_valid, batch_size=1000)\n        model.eval()\n        y_pred = np.array([])\n        for data in valid_loader:\n            data = data[0]\n            output = self.forward(data)\n            output = output.view(1, -1)\n            output = output.to('cpu').detach().numpy().copy()\n            y_pred = np.append(y_pred, output[0])\n        y_pred = np.array(y_pred)\n        y_pred = y_pred.reshape(-1, 1)\n        return y_pred\n    \n    def loss_prot(self):\n        plt.plot(self.logs_train[0], self.logs_train[1][1:], '-b')\n        plt.xlabel('epoch')\n        plt.ylabel('loss')\n        plt.show()\n        \n###########  check operation　###############\nX = np.random.rand(1000, 13)\ny = np.random.rand(1000, 1)\n\ndataset = torch.utils.data.TensorDataset(torch.Tensor(X), torch.tensor(y))\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=32)\nmodel = Net(input_size=13)\nfor b, tup in enumerate(dataloader):\n    print('---------')\n    print(tup[0].shape, tup[1].shape)\n    data = tup[0].to(torch.float32)\n    print(model(data).shape)\n    break\n######################################","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:33:51.159248Z","iopub.execute_input":"2022-04-05T10:33:51.159848Z","iopub.status.idle":"2022-04-05T10:33:51.198514Z","shell.execute_reply.started":"2022-04-05T10:33:51.159799Z","shell.execute_reply":"2022-04-05T10:33:51.197588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### prameter\nk_split = 10\nnum_epochs = 200\nbatch_size = 512\n# k-fold cross-validation\nkfold = StratifiedKFold(n_splits=k_split,\n                        random_state=1, shuffle=True).split(X_train, y_train)     #(分割数、シード)を指定\nscores = []   # list to save score \nmodels = []   # list to save model\nfor k, (train, test) in enumerate(kfold):\n    # Instantiate Model\n    model = Net(len(X_train[0]))\n    # model to GPU\n    model.to(device)\n    # training\n    model.fit(X_train[train], y_train[train], num_epochs=num_epochs, batch_size=batch_size)\n    # load model \n    model.load_state_dict(torch.load('models'))\n    # predict valid data\n    pred_y_k = model.to('cpu').predict(X_train[test])\n    # calcrate score\n    score = roc_auc_score(y_train[test], pred_y_k)\n    print('Fold: %2d, AUC: %.3f' % (k+1, score))\n    scores.append(score)\n    models.append(model)\n    model.loss_prot()\nprint('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:33:52.322642Z","iopub.execute_input":"2022-04-05T10:33:52.322957Z","iopub.status.idle":"2022-04-05T10:47:33.221977Z","shell.execute_reply.started":"2022-04-05T10:33:52.322923Z","shell.execute_reply":"2022-04-05T10:47:33.221413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluated with test data\nEvaluate the model you created using the test data you just created.","metadata":{}},{"cell_type":"markdown","source":"Now that we have created k models, we need to create a predict function for them.","metadata":{}},{"cell_type":"code","source":"# predict by k-fold\ndef predict(models, X_test):\n#　Create array for storing test data\n    y_pred = np.zeros((len(X_test), len(models)))\n    for fold_, model in enumerate(models):\n        pred_ = model.predict(X_test)\n        # save predict\n        pred_ = pred_.reshape(1, -1)\n        y_pred[:, fold_] = pred_[0]\n    y_pred = y_pred.mean(axis=1)\n    return y_pred\ny_pred = predict(models, X_test)\nprint(y_pred[:10])","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:47:42.173599Z","iopub.execute_input":"2022-04-05T10:47:42.173922Z","iopub.status.idle":"2022-04-05T10:47:42.567208Z","shell.execute_reply.started":"2022-04-05T10:47:42.173892Z","shell.execute_reply":"2022-04-05T10:47:42.566601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluated by checking AUC and ROC curves against predicted values","metadata":{}},{"cell_type":"code","source":"y_pred = y_pred\nroc = roc_curve(y_test, y_pred)\nprint(\"roc\", roc_auc_score(y_test, y_pred))\nfpr, tpr, thresholds = roc\nplt.plot(fpr, tpr, marker='o')\nplt.xlabel('FPR: False positive rate')\nplt.ylabel('TPR: True positive rate')\nplt.grid()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:47:42.906545Z","iopub.execute_input":"2022-04-05T10:47:42.90716Z","iopub.status.idle":"2022-04-05T10:47:43.123745Z","shell.execute_reply.started":"2022-04-05T10:47:42.907111Z","shell.execute_reply":"2022-04-05T10:47:43.123124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_train = predict(models, X_train)\ny_pred_train = y_pred_train\nroc = roc_curve(y_train, y_pred_train)\nprint(\"roc\", roc_auc_score(y_train, y_pred_train))\nfpr, tpr, thresholds = roc\nplt.plot(fpr, tpr, marker='o')\nplt.xlabel('FPR: False positive rate')\nplt.ylabel('TPR: True positive rate')\nplt.grid()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:47:43.377173Z","iopub.execute_input":"2022-04-05T10:47:43.378099Z","iopub.status.idle":"2022-04-05T10:47:45.334305Z","shell.execute_reply.started":"2022-04-05T10:47:43.378055Z","shell.execute_reply":"2022-04-05T10:47:45.333484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Creation of files for submission\nX_submit = test_df[features].values\ny_submit = predict(models, X_submit)\nprint(y_submit[:10])\nplt.hist(y_submit, bins=40, density=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:47:45.33575Z","iopub.execute_input":"2022-04-05T10:47:45.335954Z","iopub.status.idle":"2022-04-05T10:47:46.641097Z","shell.execute_reply.started":"2022-04-05T10:47:45.33593Z","shell.execute_reply":"2022-04-05T10:47:46.640382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"load sample_submission.csv","metadata":{}},{"cell_type":"code","source":"submission_df = pd.read_csv('../input/tabular-playground-series-apr-2022/sample_submission.csv')\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:47:46.642354Z","iopub.execute_input":"2022-04-05T10:47:46.642554Z","iopub.status.idle":"2022-04-05T10:47:46.665561Z","shell.execute_reply.started":"2022-04-05T10:47:46.64253Z","shell.execute_reply":"2022-04-05T10:47:46.664756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df['state'] = pd.DataFrame(y_submit)\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:47:46.667346Z","iopub.execute_input":"2022-04-05T10:47:46.667571Z","iopub.status.idle":"2022-04-05T10:47:46.677939Z","shell.execute_reply.started":"2022-04-05T10:47:46.667543Z","shell.execute_reply":"2022-04-05T10:47:46.677058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False, header=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T10:47:46.679298Z","iopub.execute_input":"2022-04-05T10:47:46.680059Z","iopub.status.idle":"2022-04-05T10:47:46.734832Z","shell.execute_reply.started":"2022-04-05T10:47:46.680015Z","shell.execute_reply":"2022-04-05T10:47:46.734132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}