{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\nThanks for looking.\nIn this notebook, we will show a simple implementation of the lightgbm method for beginners in machine learning.","metadata":{}},{"cell_type":"markdown","source":"# Module Load","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport warnings\nimport time\nwarnings.simplefilter('ignore')\nimport math\nfrom statistics import mean\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import timedelta\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:25:12.024098Z","iopub.execute_input":"2022-04-18T15:25:12.024471Z","iopub.status.idle":"2022-04-18T15:25:13.45788Z","shell.execute_reply.started":"2022-04-18T15:25:12.024368Z","shell.execute_reply":"2022-04-18T15:25:13.457143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Data\nLoading data by using read_csv in pandas.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-apr-2022/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')\ntrain_label_df = pd.read_csv('../input/tabular-playground-series-apr-2022/train_labels.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:26:09.694096Z","iopub.execute_input":"2022-04-18T15:26:09.694586Z","iopub.status.idle":"2022-04-18T15:26:22.588697Z","shell.execute_reply.started":"2022-04-18T15:26:09.694534Z","shell.execute_reply":"2022-04-18T15:26:22.587746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# comfirm data\nprint(train_df.shape)  \ntrain_df.head(70)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:26:26.172769Z","iopub.execute_input":"2022-04-18T15:26:26.173259Z","iopub.status.idle":"2022-04-18T15:26:26.210645Z","shell.execute_reply.started":"2022-04-18T15:26:26.173205Z","shell.execute_reply":"2022-04-18T15:26:26.209989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_label_df.shape) \ntrain_label_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:26:40.114131Z","iopub.execute_input":"2022-04-18T15:26:40.114692Z","iopub.status.idle":"2022-04-18T15:26:40.127004Z","shell.execute_reply.started":"2022-04-18T15:26:40.114636Z","shell.execute_reply":"2022-04-18T15:26:40.126029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_df.shape)\ntest_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:26:48.514413Z","iopub.execute_input":"2022-04-18T15:26:48.514947Z","iopub.status.idle":"2022-04-18T15:26:48.540874Z","shell.execute_reply.started":"2022-04-18T15:26:48.514908Z","shell.execute_reply":"2022-04-18T15:26:48.540001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Summary\n***\n**train.csv**... Training set. Consists of 60 seconds of recordings from 13 biometric sensors of about 26,000 experimental participants.  \n*  sequence - ID for each sequence\n*  subject - ID of the subject who participated in the experiment\n*  step - Time step of recording, 1 second interval\n*  sensor_00 - sensor_12 - Value of each of the 13 sensors at that time step  \n\n**train_label.csv**... Class label for each sequence\n*  sequence - ID of each sequence\n*  state - The state of the subject at each sequence. This is the label we are predicting  \n\n**test.csv**... Data to predict the state of each of the 12218 (733080 divided by 60) people.\n***\nThere are 60 seconds of data for each person, and the state (0 or 1) of the person is predicted based on the value of the data.","metadata":{}},{"cell_type":"markdown","source":"## Explanatory Data Analysis\nFirst, check the statistics of each data using describe","metadata":{}},{"cell_type":"code","source":"train_df.loc[:, 'sensor_00': 'sensor_12'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:29:50.862131Z","iopub.execute_input":"2022-04-18T15:29:50.862424Z","iopub.status.idle":"2022-04-18T15:29:51.983397Z","shell.execute_reply.started":"2022-04-18T15:29:50.862395Z","shell.execute_reply":"2022-04-18T15:29:51.982469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprcessing\nCombine data to process test and training data in batches","metadata":{}},{"cell_type":"code","source":"ntrain = train_df.shape[0]\nall_data = pd.concat((train_df, test_df))#.reset_index(drop=True)\nprint(all_data.shape)\nall_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:30:48.923588Z","iopub.execute_input":"2022-04-18T15:30:48.923912Z","iopub.status.idle":"2022-04-18T15:30:49.047706Z","shell.execute_reply.started":"2022-04-18T15:30:48.923878Z","shell.execute_reply":"2022-04-18T15:30:49.046751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:30:55.183643Z","iopub.execute_input":"2022-04-18T15:30:55.183966Z","iopub.status.idle":"2022-04-18T15:30:55.200841Z","shell.execute_reply.started":"2022-04-18T15:30:55.183935Z","shell.execute_reply":"2022-04-18T15:30:55.199926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Add features","metadata":{}},{"cell_type":"code","source":"features = all_data.columns.tolist()[3:]\nfor feature in features:\n    all_data[feature + '_lag1'] = all_data.groupby('sequence')[feature].shift(1)\n    all_data[feature + '_back_lag1'] = all_data.groupby('sequence')[feature].shift(-1)\n    all_data.fillna(0, inplace=True)\n    all_data[feature + '_diff1'] = all_data[feature] - all_data[feature + '_lag1']\n    # New features\n    for window in [3,6,12]:\n        all_data[feature+'_roll_'+str(window)+'_mean'] = all_data.groupby('sequence')[feature]\\\n        .rolling(window=window, min_periods=1).mean().reset_index(level=0,drop=True)\n        \n        all_data[feature+'_roll_'+str(window)+'_std'] = all_data.groupby('sequence')[feature]\\\n        .rolling(window=window, min_periods=1).std().reset_index(level=0,drop=True)\n        \n        all_data[feature+'_roll_'+str(window)+'_sum'] = all_data.groupby('sequence')[feature]\\\n        .rolling(window=window, min_periods=1).sum().reset_index(level=0,drop=True)\n# Experemental features\nall_data['sens_00_06'] = all_data['sensor_00'] * all_data['sensor_06']\nall_data['sens_03_07'] = all_data['sensor_03'] * all_data['sensor_07']\nall_data['sens_03_11'] = all_data['sensor_03'] * all_data['sensor_11']\nfor feature in ['sens_00_06', 'sens_03_07', 'sens_03_11']:\n    all_data[feature + '_lag1'] = all_data.groupby('sequence')[feature].shift(1)\nall_data.fillna(0, inplace=True)\n\n\nall_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:32:11.219053Z","iopub.execute_input":"2022-04-18T15:32:11.219928Z","iopub.status.idle":"2022-04-18T15:32:47.466045Z","shell.execute_reply.started":"2022-04-18T15:32:11.219866Z","shell.execute_reply":"2022-04-18T15:32:47.465422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Consolidate information","metadata":{}},{"cell_type":"code","source":"features = all_data.columns[3:]\nprint(features)\n# mean\nmean_seq = all_data.groupby('sequence').mean()\nall_data_summ = mean_seq.rename(columns={s: s+'_mean' for s in features})\n# std\nstd_seq = all_data.groupby('sequence').std().drop(['step', 'subject'], axis=1)\nall_data_summ = pd.concat([all_data_summ, std_seq.rename(columns={s: s+'_std' for s in features})], axis=1)\n# max\nmax_seq = all_data.groupby('sequence').max().drop(['step', 'subject'], axis=1)\nall_data_summ = pd.concat([all_data_summ, max_seq.rename(columns={s: s+'_max' for s in features})], axis=1)\n# min\nmin_seq = all_data.groupby('sequence').min().drop(['step', 'subject'], axis=1)\nall_data_summ = pd.concat([all_data_summ, min_seq.rename(columns={s: s+'_min' for s in features})], axis=1)\n# sum\nsum_seq = all_data.groupby('sequence').sum().drop(['step', 'subject'], axis=1)\nall_data_summ = pd.concat([all_data_summ, sum_seq.rename(columns={s: s+'_sum' for s in features})], axis=1)\n# median\nmedi_seq = all_data.groupby('sequence').median().drop(['step', 'subject'], axis=1)\nall_data_summ = pd.concat([all_data_summ, medi_seq.rename(columns={s: s+'_medi' for s in features})], axis=1)\n# first quantile\nquan1_seq = all_data.groupby('sequence').quantile(0.25).drop(['step', 'subject'], axis=1)\nall_data_summ = pd.concat([all_data_summ, quan1_seq.rename(columns={s: s+'_quantile1' for s in features})], axis=1)\n# third quartile\nquan3_seq = all_data.groupby('sequence').quantile(0.75).drop(['step', 'subject'], axis=1)\nall_data_summ = pd.concat([all_data_summ, quan3_seq.rename(columns={s: s+'_quantile3' for s in features})], axis=1)\n\n# mean\nmean_seq = all_data.groupby('subject').mean().drop(['step', 'sequence'], axis=1)\nall_data_summ = pd.concat([all_data_summ, mean_seq.rename(columns={s: 'subject_'+s+'_mean' for s in features})], axis=1)\n# std\nstd_seq = all_data.groupby('subject').std().drop(['step', 'sequence'], axis=1)\nall_data_summ = pd.concat([all_data_summ, std_seq.rename(columns={s: 'subject_'+s+'_std' for s in features})], axis=1)\n# max\nmax_seq = all_data.groupby('subject').max().drop(['step', 'sequence'], axis=1)\nall_data_summ = pd.concat([all_data_summ, max_seq.rename(columns={s: 'subject_'+s+'_max' for s in features})], axis=1)\n# min\nmin_seq = all_data.groupby('subject').min().drop(['step', 'sequence'], axis=1)\nall_data_summ = pd.concat([all_data_summ, min_seq.rename(columns={s: 'subject_'+s+'_min' for s in features})], axis=1)\n# sum\nsum_seq = all_data.groupby('subject').sum().drop(['step', 'sequence'], axis=1)\nall_data_summ = pd.concat([all_data_summ, sum_seq.rename(columns={s: 'subject_'+s+'_sum' for s in features})], axis=1)\n# median\nmedi_seq = all_data.groupby('subject').median().drop(['step', 'sequence'], axis=1)\nall_data_summ = pd.concat([all_data_summ, medi_seq.rename(columns={s: 'subject_'+s+'_medi' for s in features})], axis=1)\n# first quantile\nquan1_seq = all_data.groupby('subject').quantile(0.25).drop(['step', 'sequence'], axis=1)\nall_data_summ = pd.concat([all_data_summ, quan1_seq.rename(columns={s: 'subject_'+s+'_quantile1' for s in features})], axis=1)\n# third quartile\nquan3_seq = all_data.groupby('subject').quantile(0.75).drop(['step', 'sequence'], axis=1)\nall_data_summ = pd.concat([all_data_summ, quan3_seq.rename(columns={s: 'subject_'+s+'_quantile3' for s in features})], axis=1)\n# print(all_data_summ.columns.tolist())\nall_data_summ.fillna(0, inplace=True)\nall_data_summ.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T16:27:22.730425Z","iopub.status.idle":"2022-04-18T16:27:22.730786Z","shell.execute_reply.started":"2022-04-18T16:27:22.73061Z","shell.execute_reply":"2022-04-18T16:27:22.730629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data_summ.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:39:09.330544Z","iopub.execute_input":"2022-04-18T15:39:09.331053Z","iopub.status.idle":"2022-04-18T15:39:09.434749Z","shell.execute_reply.started":"2022-04-18T15:39:09.331006Z","shell.execute_reply":"2022-04-18T15:39:09.433766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Undo test_df and train_df","metadata":{}},{"cell_type":"code","source":"train_df = all_data_summ[:ntrain//60]\ntest_df = all_data_summ[ntrain//60:]\nprint(train_df.shape, test_df.shape)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:39:09.436278Z","iopub.execute_input":"2022-04-18T15:39:09.436538Z","iopub.status.idle":"2022-04-18T15:39:09.467301Z","shell.execute_reply.started":"2022-04-18T15:39:09.436508Z","shell.execute_reply":"2022-04-18T15:39:09.466731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trainig\nNow that feature engineering is done, create and train the model","metadata":{}},{"cell_type":"markdown","source":"## Model Partitioning\nSplit `train_df` into training data (used to train the model) and test data (used to verify generalization performance of the model)","metadata":{}},{"cell_type":"code","source":"test_size = 0.20\nfeatures = train_df.columns[2:]\nX, y = train_df[features].values, train_label_df['state'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y)\nprint(X_train.shape, X_test.shape)\nprint(y_train.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:39:09.468707Z","iopub.execute_input":"2022-04-18T15:39:09.468917Z","iopub.status.idle":"2022-04-18T15:39:10.211032Z","shell.execute_reply.started":"2022-04-18T15:39:09.468891Z","shell.execute_reply":"2022-04-18T15:39:10.209842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter tuning\nFurthermore, the training data is split into training data and validation data, and hyperparameter tuning is performed using the validation data.\nThis process is time consuming, so skip this cell when implementing without tuning.","metadata":{}},{"cell_type":"code","source":"# import optuna.integration.lightgbm as lgb   # tuning model\n# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.30, random_state=1998, stratify=y_train)\n# # Create dataset for LightGBM\n# lgb_train = lgb.Dataset(X_train, y_train)\n# lgb_valid = lgb.Dataset(X_valid, y_valid)\n# # setting parameters\n# params = {\n#     # binary classification problem\n#     'objective': 'binary',\n#     # Aim to maximize AUC\n#     'metric': 'auc',\n#     # Fatal case output\n#     'verbosity': -1,\n#     # number of leaf\n#     'num_leaves': 50,\n#     # learning rate\n#     'learning_rate': 0.05,\n#     # feature fraction\n#     'feature_fraction': 0.9,\n#     # bagging fraction\n#     'bagging_fraction': 0.408242911006906,\n#     # bagging frequency\n#     'bagging_freq': 5,\n# }\n\n# # Model creation from training data\n# gbm = lgb.train(params, lgb_train, valid_sets=lgb_valid,\n#                 verbose_eval=50, # Learning result output every 50 iter\n#                 num_boost_round=10000, # max iteration\n#                 early_stopping_rounds=100\n#                )\n# best_params = gbm.params\n# print(best_params)\n# # Check forecast accuracy\n# y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n# print(y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:39:10.212227Z","iopub.execute_input":"2022-04-18T15:39:10.213195Z","iopub.status.idle":"2022-04-18T15:39:10.225632Z","shell.execute_reply.started":"2022-04-18T15:39:10.213151Z","shell.execute_reply":"2022-04-18T15:39:10.221502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Learning without Tuning.  \nHere, k-fold cross validation is performed to create a model with better generalization performance","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb   # without tuning\n# parameter\nparams = {'objective': 'binary', \n          'metric': 'auc', \n          'verbosity': -1, \n          'num_leaves': 45, \n          'learning_rate': 0.05, \n          'feature_fraction': 1.0, \n          'bagging_fraction': 0.7464532049351305, \n          'bagging_freq': 2, \n          'feature_pre_filter': False, \n          'lambda_l1': 0.004190355150239527, \n          'lambda_l2': 1.5273917248709095e-08, \n          'min_child_samples': 25}\n\nkfold = StratifiedKFold(n_splits=10,\n                        random_state=1, shuffle=True).split(X_train, y_train)     #(分割数、シード)を指定\nscores = []   # score list\nmodels = []   # model list\nfor k, (train, test) in enumerate(kfold):\n    X_trainset_lgb = lgb.Dataset(X_train[train], y_train[train])\n    X_validset_lgb = lgb.Dataset(X_train[test], y_train[test])\n\n    gbm = lgb.train(params, X_trainset_lgb, valid_sets=X_validset_lgb,\n                verbose_eval=50,\n                num_boost_round=10000, \n                early_stopping_rounds=100)\n    pred_t = gbm.predict(X_train[test], num_iteration=gbm.best_iteration)\n    score = roc_auc_score(y_train[test], pred_t) \n    scores.append(score)  \n    print('Fold: %2d, AUC: %.3f' % (k+1, score))\n    models.append(gbm)\nprint('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:39:27.843918Z","iopub.execute_input":"2022-04-18T15:39:27.84458Z","iopub.status.idle":"2022-04-18T16:27:22.701104Z","shell.execute_reply.started":"2022-04-18T15:39:27.844525Z","shell.execute_reply":"2022-04-18T16:27:22.700315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a function for predict, make predictions with k models, and average the predictions.","metadata":{}},{"cell_type":"code","source":"def predict(models, X_test):\n#　Create array for storing test data\n    y_pred = np.zeros((len(X_test), len(models)))\n\n    for fold_, model in enumerate(models):\n        # predict each model\n        pred_ = model.predict(X_test, num_iteration=model.best_iteration)\n        # save predict\n        y_pred[:, fold_] = pred_ \n    y_pred = y_pred.mean(axis=1)\n    return y_pred\ny_pred = predict(models, X_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T16:28:09.35397Z","iopub.execute_input":"2022-04-18T16:28:09.354344Z","iopub.status.idle":"2022-04-18T16:28:12.111561Z","shell.execute_reply.started":"2022-04-18T16:28:09.354302Z","shell.execute_reply":"2022-04-18T16:28:12.110831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"evaluate models by ROC curve and AUC","metadata":{}},{"cell_type":"code","source":"roc = roc_curve(y_test, y_pred)\nprint(\"roc\", roc_auc_score(y_test, y_pred))\nfpr, tpr, thresholds = roc\nplt.plot(fpr, tpr, marker='o')\nplt.xlabel('FPR: False positive rate')\nplt.ylabel('TPR: True positive rate')\nplt.grid()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T16:28:16.162671Z","iopub.execute_input":"2022-04-18T16:28:16.162977Z","iopub.status.idle":"2022-04-18T16:28:16.394545Z","shell.execute_reply.started":"2022-04-18T16:28:16.162947Z","shell.execute_reply":"2022-04-18T16:28:16.393529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_train = predict(models, X_train)\ny_pred_train = y_pred_train\nroc = roc_curve(y_train, y_pred_train)\nprint(\"roc\", roc_auc_score(y_train, y_pred_train))\nfpr, tpr, thresholds = roc\nplt.plot(fpr, tpr, marker='o')\nplt.xlabel('FPR: False positive rate')\nplt.ylabel('TPR: True positive rate')\nplt.grid()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T16:28:18.392257Z","iopub.execute_input":"2022-04-18T16:28:18.392552Z","iopub.status.idle":"2022-04-18T16:28:29.589326Z","shell.execute_reply.started":"2022-04-18T16:28:18.39252Z","shell.execute_reply":"2022-04-18T16:28:29.588646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit Prediction\nCreate data for submission","metadata":{}},{"cell_type":"code","source":"X_submit = test_df[features].values\ny_submit = predict(models, X_submit)\nprint(y_submit.shape)\nplt.hist(y_submit, bins=30, density=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T16:28:29.591034Z","iopub.execute_input":"2022-04-18T16:28:29.592069Z","iopub.status.idle":"2022-04-18T16:28:39.24414Z","shell.execute_reply.started":"2022-04-18T16:28:29.592004Z","shell.execute_reply":"2022-04-18T16:28:39.243011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.read_csv('../input/tabular-playground-series-apr-2022/sample_submission.csv')\nprint(submission_df.shape)\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T16:28:39.245797Z","iopub.execute_input":"2022-04-18T16:28:39.246135Z","iopub.status.idle":"2022-04-18T16:28:39.274306Z","shell.execute_reply.started":"2022-04-18T16:28:39.24608Z","shell.execute_reply":"2022-04-18T16:28:39.273687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df['state'] = pd.DataFrame(y_submit)\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T16:28:39.276116Z","iopub.execute_input":"2022-04-18T16:28:39.27638Z","iopub.status.idle":"2022-04-18T16:28:39.287279Z","shell.execute_reply.started":"2022-04-18T16:28:39.276347Z","shell.execute_reply":"2022-04-18T16:28:39.286433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False, header=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T16:28:39.288895Z","iopub.execute_input":"2022-04-18T16:28:39.289748Z","iopub.status.idle":"2022-04-18T16:28:39.34847Z","shell.execute_reply.started":"2022-04-18T16:28:39.289695Z","shell.execute_reply":"2022-04-18T16:28:39.347693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}