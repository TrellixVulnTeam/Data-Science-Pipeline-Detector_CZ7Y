{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Analysis by LSTM - Using Pytorch\nThis notebook implements the LSTM method by using Pytorch","metadata":{}},{"cell_type":"code","source":"# Import the required modules\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport time\nwarnings.simplefilter('ignore')\nimport math\nfrom statistics import mean\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import timedelta\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('Devices to be used：：', device)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T15:33:52.935654Z","iopub.execute_input":"2022-04-11T15:33:52.936208Z","iopub.status.idle":"2022-04-11T15:33:55.317208Z","shell.execute_reply.started":"2022-04-11T15:33:52.936121Z","shell.execute_reply":"2022-04-11T15:33:55.316474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\n## Loading Data\nFirst, download the data using pandas and check the contents","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-apr-2022/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')\ntrain_label_df = pd.read_csv('../input/tabular-playground-series-apr-2022/train_labels.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-11T15:34:02.856551Z","iopub.execute_input":"2022-04-11T15:34:02.857245Z","iopub.status.idle":"2022-04-11T15:34:14.195414Z","shell.execute_reply.started":"2022-04-11T15:34:02.857207Z","shell.execute_reply":"2022-04-11T15:34:14.194663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check data\nprint(train_df.shape)  # confirm shape\ntrain_df.head(70)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T15:34:14.197175Z","iopub.execute_input":"2022-04-11T15:34:14.197442Z","iopub.status.idle":"2022-04-11T15:34:14.23035Z","shell.execute_reply.started":"2022-04-11T15:34:14.197405Z","shell.execute_reply":"2022-04-11T15:34:14.229563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_label_df.shape) \ntrain_label_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T15:34:14.231836Z","iopub.execute_input":"2022-04-11T15:34:14.232078Z","iopub.status.idle":"2022-04-11T15:34:14.240681Z","shell.execute_reply.started":"2022-04-11T15:34:14.232047Z","shell.execute_reply":"2022-04-11T15:34:14.239979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_df.shape)\ntest_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T15:34:14.24279Z","iopub.execute_input":"2022-04-11T15:34:14.243259Z","iopub.status.idle":"2022-04-11T15:34:14.269197Z","shell.execute_reply.started":"2022-04-11T15:34:14.243225Z","shell.execute_reply":"2022-04-11T15:34:14.268346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Summary**\n***\n**train.csv**...the training set, comprising ~26,000 60-second recordings of thirteen biological sensors for almost one thousand experimental participants  \n*   sequence - a unique id for each sequence\n*   subject - a unique id for the subject in the experiment\n*   step - time step of the recording, in one second intervals\n*   sensor_00 - sensor_12 - the value for each of the thirteen sensors at that time step  \n\n**train_labels.csv**...the class label for each sequence.  \n*   sequence - the unique id for each sequence.\n*   state - the state associated to each sequence. This is the target which you are trying to predict.  \n\n**test.csv**...the test set. For each of the ~12,000 sequences, you should predict a value for that sequence's state.  \n***\nThere are 60 seconds of data per person, and the state (0 or 1) of the person is predicted from the values.","metadata":{}},{"cell_type":"markdown","source":"## Data Analysis\nLet's look at the information on the parameters of each sensor and see how they relate to STATE.  \nFirst, check the statistics of each data using DESCRIBE\n*   Four median zero (2, 3, 8, 12)\n*   Some of the quartile ranges and maximum-minimum ranges are very different (are there skipped values?)\n*   The average value is near zero overall.","metadata":{}},{"cell_type":"code","source":"train_df.loc[:, 'sensor_00': 'sensor_12'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T15:34:14.270457Z","iopub.execute_input":"2022-04-11T15:34:14.270813Z","iopub.status.idle":"2022-04-11T15:34:15.035161Z","shell.execute_reply.started":"2022-04-11T15:34:14.270763Z","shell.execute_reply":"2022-04-11T15:34:15.034471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using Seaborn to check the distribution of sensors for 1,000 people.\n*   There are quite a few outliers.\n*   Histograms are nicely distributed.","metadata":{}},{"cell_type":"code","source":"sns.set()\ncols = train_df.columns[3:]\nsns.pairplot(train_df[:60*100][cols], size = 3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T15:34:15.036329Z","iopub.execute_input":"2022-04-11T15:34:15.036568Z","iopub.status.idle":"2022-04-11T15:35:50.290985Z","shell.execute_reply.started":"2022-04-11T15:34:15.036533Z","shell.execute_reply":"2022-04-11T15:35:50.290283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing\nPre-processing.\nCombine the test and training data for batch processing","metadata":{}},{"cell_type":"code","source":"ntrain = train_df.shape[0]\nall_data = pd.concat((train_df, test_df))#.reset_index(drop=True)\nprint(all_data.shape)\nall_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T15:35:50.292021Z","iopub.execute_input":"2022-04-11T15:35:50.292768Z","iopub.status.idle":"2022-04-11T15:35:50.41826Z","shell.execute_reply.started":"2022-04-11T15:35:50.292725Z","shell.execute_reply":"2022-04-11T15:35:50.417428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check for missing values.\n*   No missing values","metadata":{}},{"cell_type":"code","source":"# Check missing values\nall_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(22)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T15:35:50.419681Z","iopub.execute_input":"2022-04-11T15:35:50.419953Z","iopub.status.idle":"2022-04-11T15:35:50.506581Z","shell.execute_reply.started":"2022-04-11T15:35:50.419919Z","shell.execute_reply":"2022-04-11T15:35:50.505735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Return to training and test data","metadata":{}},{"cell_type":"code","source":"train_df = all_data[:ntrain]\ntest_df = all_data[ntrain:]\nprint(train_df.shape, test_df.shape)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T15:35:50.508054Z","iopub.execute_input":"2022-04-11T15:35:50.508311Z","iopub.status.idle":"2022-04-11T15:35:50.526568Z","shell.execute_reply.started":"2022-04-11T15:35:50.508278Z","shell.execute_reply":"2022-04-11T15:35:50.525877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trainig\n## Splitting data\nSplit data for training models and for validation of model accuracy  \nIn this case, training data for model: validation data for accuracy = 8 : 2","metadata":{}},{"cell_type":"code","source":"length = len(train_df)\ntrain_size = int(length * 0.8) - int(length * 0.8 % 60)\ntest_size = length - train_size\nlength_y = len(train_label_df)\ntrain_size_y = int(length_y * 0.8)\ntest_size_y = length_y - train_size_y\nX_train, X_test = train_df[0:train_size], train_df[train_size:length]\ny_train, y_test = train_label_df[0:train_size_y], train_label_df[train_size_y:length_y]\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T15:35:50.529289Z","iopub.execute_input":"2022-04-11T15:35:50.52994Z","iopub.status.idle":"2022-04-11T15:35:50.556629Z","shell.execute_reply.started":"2022-04-11T15:35:50.529902Z","shell.execute_reply":"2022-04-11T15:35:50.555982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Creation\nI will use something called an LSTM model.\nFirst, create a class that creates a dataset that can be read by pytorch","metadata":{}},{"cell_type":"code","source":"from torch.utils.data.sampler import SubsetRandomSampler\nclass MyDataset(torch.utils.data.Dataset):\n    def __init__(self, X, sequence_num, y=None, mode='train'):\n        self.data = X\n        self.teacher = y\n        self.sequence_num = sequence_num\n        self.mode = mode\n    def __len__(self):\n        return len(self.teacher)\n\n    def __getitem__(self, idx):\n        out_data = self.data[idx]\n        if self.mode == 'train':\n            out_label =  self.teacher[idx[0]//self.sequence_num]\n            return out_data, out_label\n        else:\n            return out_data\ndef create_dataset(dataset, dataset_num, sequence_num, input_size, batch_size, shuffle=False):\n    sampler = np.array([list(range(i*sequence_num, (i+1)*sequence_num)) for i in range(dataset_num//sequence_num)])\n    if shuffle == True:\n        np.random.shuffle(sampler)\n    dataloader = DataLoader(dataset, batch_size, sampler=sampler)\n    return dataloader\n\n###########  operation check　###############\nsequence_num = 60\nX = np.random.rand(60*1000, 13)\ny = np.random.rand(60*1000, 1)\n\ndataset = MyDataset(X, y=y, sequence_num=sequence_num)\ndataloader = create_dataset(dataset, X.shape[0], sequence_num, X.shape[1], 32)\n# dataloader = DataLoader(dataset, batch_size=32)#, sampler=sampler)\nfor b, tup in enumerate(dataloader):\n    print('---------')\n    print(tup[0].shape, tup[1].shape)\n    break\nprint(X[-2], y[-1])\n############################################","metadata":{"execution":{"iopub.status.busy":"2022-04-11T15:35:50.557848Z","iopub.execute_input":"2022-04-11T15:35:50.558368Z","iopub.status.idle":"2022-04-11T15:35:50.623419Z","shell.execute_reply.started":"2022-04-11T15:35:50.558331Z","shell.execute_reply":"2022-04-11T15:35:50.622626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, input_size=5, sequence_num=60, lstm_dim1=48, lstm_dim2=128, lstm_dim3=256,\n                 num_layers=2, output_size=1, batch_size = 32):\n        super().__init__()\n        self.batch_size = batch_size\n        \n        self.lstm1 = nn.LSTM(input_size, lstm_dim1, num_layers, batch_first=True)\n        \n        self.linear1 = nn.Linear(lstm_dim1*sequence_num, 96)\n        self.bn1 = nn.BatchNorm1d(96)\n        self.linear2 = nn.Linear(96, 16)\n        self.bn2 = nn.BatchNorm1d(16)\n        self.linear3 = nn.Linear(16, 1)\n        self.dropout = nn.Dropout(0.3899990603626676)\n        self.logs_train = [[], [np.inf]]\n        self.logs_valid = [[], [np.inf]]\n        self.stdsc = StandardScaler()\n        self.stdsc_y = StandardScaler()\n\n    def forward(self, x):\n        lstm_out, _ = self.lstm1(x)\n        x = lstm_out.reshape(lstm_out.shape[0], -1)\n        x = self.bn1(F.leaky_relu(self.linear1(x)))\n        x = self.dropout(x)\n        x = self.bn2(F.leaky_relu(self.linear2(x)))\n        x = self.dropout(x)\n        x = torch.sigmoid(self.linear3(x))\n        return x\n\n    def fit(self, X, y, num_epochs=50, sequence_num=60, batch_size=32):\n        # Check whether GPU is available\n        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n        ####   data processing↓\n        # standardization\n        X = self.stdsc.fit_transform(X)\n        # Data set and data loader creation\n        num_train = len(X)\n        dataset_size = X.shape[0]# data size\n        input_size = X.shape[1]  # input size\n        # create dataset\n        dataset = MyDataset(X, y=y, sequence_num=sequence_num, mode='train')\n        # create dataloader\n        dataloader = create_dataset(dataset, dataset_size, sequence_num, input_size, batch_size)\n        ####    trainig settings\n        # Setting up optimization methods\n        lr = 0.028333303850396258\n        weight_decay = 1.0078357791694276e-05\n        optimizer = torch.optim.Adagrad(self.parameters(), lr=lr, weight_decay=weight_decay)\n        # Define the error function\n        criterion = nn.MSELoss()\n        # Network to GPU\n        self.to(device)\n        # Model in learning mode\n        self.train()\n        # If the network is somewhat fixed, make it faster\n        torch.backends.cudnn.benchark = True\n        # Save batch size\n        batch_size = dataloader.batch_size\n        # Set iteration counter\n        iteration = 1\n        # Loop of epoch\n        for epoch in tqdm(range(num_epochs)):\n            # Save start time\n            t_epoch_strat = time.time()\n            epoch_loss = 0.0\n            ### Training\n            # Loop to retrieve minibatch by minibatch from the data loader\n            for data, targets in dataloader:\n                # Converted to be handled by GPU\n                data = data.to(device)\n                targets = targets.to(device)\n                # gradient initialization\n                optimizer.zero_grad()\n\n                # Get output\n                data = data.to(torch.float32)\n                output = self.forward(data)\n                output = output.view(1,-1)[0]\n                targets = targets.to(torch.float32)\n                # Calculate error\n                loss = criterion(output, targets)\n                # backpropergation\n                loss.backward()\n                # step\n                optimizer.step()\n                # memory erorr\n                epoch_loss += loss.item()\n                iteration += 1\n            # loss per epoch\n            t_epoch_finish = time.time()\n            # save model\n            if epoch_loss/num_train < min(self.logs_train[1]):\n                torch.save(self.state_dict(), './models')\n            self.logs_train[0].append(epoch+1)\n            self.logs_train[1].append(epoch_loss/num_train)\n            \n\n    def predict(self, X, sequence_num=60):\n        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n        valid_loss = 0.0\n        num_valid = len(X)\n        indices_valid = list(range(num_valid))\n        # standerdization\n        X = self.stdsc.transform(X)\n        # create dataset\n        dataset = MyDataset(X, sequence_num=sequence_num, mode='valid')\n        # create dataloader\n        valid_loader = create_dataset(dataset, X.shape[0], sequence_num, X.shape[1], batch_size)\n        y_pred = np.array([])\n        for data in valid_loader:\n            data = data.to(torch.float32)\n            data = data.to(device)\n            output = self.forward(data)\n            output = output.view(1, -1)\n            output = output.to('cpu').detach().numpy().copy()\n            y_pred = np.append(y_pred, output[0])\n        return y_pred\n\n###########  operation check　###############\nmodel = LSTM(input_size=7)\ndata = np.random.rand(32, 60, 7)\ndata = torch.from_numpy(data.astype(np.float32)).clone()\nmodel(data)\n#############################################","metadata":{"execution":{"iopub.status.busy":"2022-04-11T16:01:43.390774Z","iopub.execute_input":"2022-04-11T16:01:43.391189Z","iopub.status.idle":"2022-04-11T16:01:43.444087Z","shell.execute_reply.started":"2022-04-11T16:01:43.391155Z","shell.execute_reply":"2022-04-11T16:01:43.443275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_fold = 10\nvalid_per = 1/k_fold\nepoch = 200\nbatch_size = 256\ncategorical_columns = X_train.columns[3:]\n\nscores = []\nmodels = []\nfor k in range(k_fold):\n    length = len(X_train)  \n    valid_size = int(length * valid_per) - int(length * valid_per % 60) \n    train_size = length - valid_size  \n    length_y = len(y_train)    \n    valid_size_y = int(length_y * valid_per)   \n    train_size_y = length_y - valid_size_y   \n    ### \n    X_train_k, X_test_k = X_train.drop(X_train.index[range(k*valid_size, (k+1)*valid_size)]), X_train[k*valid_size:(k+1)*valid_size] \n    y_train_k, y_test_k = y_train.drop(y_train.index[range(k*valid_size_y, (k+1)*valid_size_y)]), y_train[k*valid_size_y:(k+1)*valid_size_y]\n    #### \n    X_train_np = X_train_k[categorical_columns].values  \n    y_train_np = y_train_k['state'].values\n    X_test_np = X_test_k[categorical_columns].values\n    y_test_np = y_test_k['state'].values\n    print(X_train_np.shape, y_train_np.shape)\n    print(X_test_np.shape, y_test_np.shape)\n    ### \n    model_k = LSTM(input_size=len(categorical_columns))\n    model_k.to(device)\n    ### \n    model_k.fit(X_train_np, y_train_np, num_epochs=epoch, sequence_num=sequence_num, batch_size=batch_size)\n    ## \n    model_k.load_state_dict(torch.load('models'))\n    pred_k = model_k.predict(X_test_np, sequence_num=sequence_num)\n    score_k = roc_auc_score(y_test_np, pred_k)   \n    print('Fold: %2d, AUC: %.3f' % (k+1, score_k))\n    roc = roc_curve(y_test_np, pred_k)  \n    fpr, tpr, thresholds = roc\n    plt.plot(model_k.logs_train[0], model_k.logs_train[1][1:])\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.show()\n    \n    plt.plot(fpr, tpr, marker='o')\n    plt.xlabel('FPR: False positive rate')\n    plt.ylabel('TPR: True positive rate')\n    plt.grid()\n    plt.show()\n    scores.append(score_k)\n    models.append(model_k)\nprint('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))","metadata":{"execution":{"iopub.status.busy":"2022-04-11T16:01:48.817815Z","iopub.execute_input":"2022-04-11T16:01:48.818274Z","iopub.status.idle":"2022-04-11T16:26:08.174434Z","shell.execute_reply.started":"2022-04-11T16:01:48.818236Z","shell.execute_reply":"2022-04-11T16:26:08.173739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(models, X_test):\n    y_pred = np.zeros((len(X_test)//60, len(models)))\n    for fold_, model in enumerate(models):\n        pred_ = model.predict(X_test)\n        pred_ = pred_.reshape(1, -1)\n        y_pred[:, fold_] = pred_[0]\n    y_pred = y_pred.mean(axis=1)\n    return y_pred\nX_test_np = X_test[categorical_columns].values\ny_test_np = y_test['state'].values\ny_pred = predict(models, X_test_np)\nprint(y_pred[:10])","metadata":{"execution":{"iopub.status.busy":"2022-04-11T16:26:08.176223Z","iopub.execute_input":"2022-04-11T16:26:08.176682Z","iopub.status.idle":"2022-04-11T16:26:10.021075Z","shell.execute_reply.started":"2022-04-11T16:26:08.176645Z","shell.execute_reply":"2022-04-11T16:26:10.019371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred[y_pred < 0] = 0\ny_pred[y_pred > 1] = 1\nroc = roc_curve(y_test_np, y_pred)\nprint(\"roc\", roc_auc_score(y_test_np, y_pred))\nfpr, tpr, thresholds = roc\nplt.plot(fpr, tpr, marker='o')\nplt.xlabel('FPR: False positive rate')\nplt.ylabel('TPR: True positive rate')\nplt.grid()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T16:26:10.022276Z","iopub.execute_input":"2022-04-11T16:26:10.022525Z","iopub.status.idle":"2022-04-11T16:26:10.258162Z","shell.execute_reply.started":"2022-04-11T16:26:10.022489Z","shell.execute_reply":"2022-04-11T16:26:10.257494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_train = predict(models, X_train_np)\ny_pred_train = y_pred_train\nroc = roc_curve(y_train_np, y_pred_train)\nprint(\"roc\", roc_auc_score(y_train_np, y_pred_train))\nfpr, tpr, thresholds = roc\nplt.plot(fpr, tpr, marker='o')\nplt.xlabel('FPR: False positive rate')\nplt.ylabel('TPR: True positive rate')\nplt.grid()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T16:26:10.260157Z","iopub.execute_input":"2022-04-11T16:26:10.260414Z","iopub.status.idle":"2022-04-11T16:26:18.145833Z","shell.execute_reply.started":"2022-04-11T16:26:10.260378Z","shell.execute_reply":"2022-04-11T16:26:18.145157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission Data Preparation","metadata":{}},{"cell_type":"code","source":"X_submit = test_df[categorical_columns].values\ny_submit = predict(models, X_submit)\nprint(y_submit.shape)\nplt.hist(y_submit, bins=30, density=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T16:26:18.147132Z","iopub.execute_input":"2022-04-11T16:26:18.147372Z","iopub.status.idle":"2022-04-11T16:26:23.179314Z","shell.execute_reply.started":"2022-04-11T16:26:18.147338Z","shell.execute_reply":"2022-04-11T16:26:23.17858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.read_csv('../input/tabular-playground-series-apr-2022/sample_submission.csv')\nprint(submission_df.shape)\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T16:26:23.180464Z","iopub.execute_input":"2022-04-11T16:26:23.181249Z","iopub.status.idle":"2022-04-11T16:26:23.20883Z","shell.execute_reply.started":"2022-04-11T16:26:23.18121Z","shell.execute_reply":"2022-04-11T16:26:23.208169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df['state'] = pd.DataFrame(y_submit)\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T16:26:23.212335Z","iopub.execute_input":"2022-04-11T16:26:23.214296Z","iopub.status.idle":"2022-04-11T16:26:23.227194Z","shell.execute_reply.started":"2022-04-11T16:26:23.214259Z","shell.execute_reply":"2022-04-11T16:26:23.226537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False, header=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T16:26:23.230541Z","iopub.execute_input":"2022-04-11T16:26:23.232365Z","iopub.status.idle":"2022-04-11T16:26:23.295212Z","shell.execute_reply.started":"2022-04-11T16:26:23.232325Z","shell.execute_reply":"2022-04-11T16:26:23.29441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}