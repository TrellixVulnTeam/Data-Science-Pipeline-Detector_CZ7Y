{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Let's start by importing the necessary modules","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport statsmodels.formula.api as smf","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:19:45.046651Z","iopub.execute_input":"2022-04-05T06:19:45.046989Z","iopub.status.idle":"2022-04-05T06:19:45.053146Z","shell.execute_reply.started":"2022-04-05T06:19:45.04695Z","shell.execute_reply":"2022-04-05T06:19:45.052218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x = pd.read_csv(\"../input/tabular-playground-series-apr-2022/train.csv\")\ntrain_y = pd.read_csv(\"../input/tabular-playground-series-apr-2022/train_labels.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-apr-2022/test.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2022-04-05T04:39:41.669174Z","iopub.execute_input":"2022-04-05T04:39:41.669896Z","iopub.status.idle":"2022-04-05T04:39:53.652662Z","shell.execute_reply.started":"2022-04-05T04:39:41.66985Z","shell.execute_reply":"2022-04-05T04:39:53.651941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Unique no of sequences {train_x.sequence.nunique()}\\nUnique no of subject {train_x.subject.nunique()}')","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:37:23.856162Z","iopub.execute_input":"2022-04-05T05:37:23.856453Z","iopub.status.idle":"2022-04-05T05:37:23.912773Z","shell.execute_reply.started":"2022-04-05T05:37:23.85642Z","shell.execute_reply":"2022-04-05T05:37:23.911912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There arent any sequences that are repeated. But the subjects are only 672. It means subjects are being repeated. Same subject might have undergone repeated examination. ","metadata":{}},{"cell_type":"markdown","source":"Lets create a function to apply the Fast Fourier Transform on the 60 seconds time series data from the sensors. Using the FFT we will try to get the frequencies of the time series and we use those frequencies has the features representing the sensors.","metadata":{}},{"cell_type":"code","source":"def fft_sensor_freq(series):\n    n =  len(series)\n    g = np.fft.fft(series,n)\n    psd = g*np.conj(g)/n\n    L = np.arange(1, np.floor(n/2), dtype = \"int\")\n    freq = (1/(1*n)) * np.arange(n)\n    freq1 = freq[list(psd[L].real).index(psd[L].real.max())]\n    \n    return freq1\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-05T04:39:53.699597Z","iopub.execute_input":"2022-04-05T04:39:53.700474Z","iopub.status.idle":"2022-04-05T04:39:53.707857Z","shell.execute_reply.started":"2022-04-05T04:39:53.700429Z","shell.execute_reply":"2022-04-05T04:39:53.707114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = pd.DataFrame(columns =  ['sequence','sensor_00', 'sensor_01', 'sensor_02',\n       'sensor_03', 'sensor_04', 'sensor_05', 'sensor_06', 'sensor_07',\n       'sensor_08', 'sensor_09', 'sensor_10', 'sensor_11', 'sensor_12'])\nfor i in train_x.sequence.unique():\n    df = train_x.drop(columns = [\"subject\",\"step\"]).query(f'sequence == {i}')\n    df2 = dict()\n    df2[\"sequence\"] = [i]\n    cols = ['sensor_00', 'sensor_01', 'sensor_02',\n       'sensor_03', 'sensor_04', 'sensor_05', 'sensor_06', 'sensor_07',\n       'sensor_08', 'sensor_09', 'sensor_10', 'sensor_11', 'sensor_12']\n    for j in cols:\n        df2[j] = [fft_sensor_freq(df[j])]\n    df1 = df1.append(pd.DataFrame(df2))\n    #if (i % 200 == 0):\n        #print(i)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_train = df1.merge(train_y, on = \"sequence\", how = \"inner\")","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:12:52.967918Z","iopub.execute_input":"2022-04-05T05:12:52.968176Z","iopub.status.idle":"2022-04-05T05:12:52.989188Z","shell.execute_reply.started":"2022-04-05T05:12:52.968139Z","shell.execute_reply":"2022-04-05T05:12:52.988312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking the Correlation matrix in order to see the correlations between the any of the sensor frequencies.","metadata":{}},{"cell_type":"code","source":"sns.heatmap(final_train.corr())","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:28:21.919263Z","iopub.execute_input":"2022-04-05T05:28:21.92023Z","iopub.status.idle":"2022-04-05T05:28:22.34298Z","shell.execute_reply.started":"2022-04-05T05:28:21.920189Z","shell.execute_reply":"2022-04-05T05:28:22.342079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(data = final_train, x = \"sensor_12\", hue = \"state\", kind = \"kde\")","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:21:39.711295Z","iopub.execute_input":"2022-04-05T05:21:39.711578Z","iopub.status.idle":"2022-04-05T05:21:40.256375Z","shell.execute_reply.started":"2022-04-05T05:21:39.711541Z","shell.execute_reply":"2022-04-05T05:21:40.255729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = linear_model.LogisticRegression()\nxtrain, xtest, ytrain, ytest = train_test_split(final_train.drop(columns = [\"sequence\",\"state\"]), final_train.state, test_size = 0.35)\nmodel.fit(xtrain, ytrain)\nproba = model.predict_proba(xtest)\nproba_1 = [a[1] for a in proba]\nfor i in range(0,10,1):\n    some1 = np.array(proba_1) > i/10\n    print(i, sum(some1 == ytest)/len(ytest))","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:21:10.035479Z","iopub.execute_input":"2022-04-05T06:21:10.035792Z","iopub.status.idle":"2022-04-05T06:21:10.146525Z","shell.execute_reply.started":"2022-04-05T06:21:10.035745Z","shell.execute_reply":"2022-04-05T06:21:10.145619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"formula = 'ytrain~'+\"+\".join(list(xtrain.columns))\nmodel = smf.glm(formula, data = xtrain).fit()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:47:07.965081Z","iopub.execute_input":"2022-04-05T05:47:07.965359Z","iopub.status.idle":"2022-04-05T05:47:08.0892Z","shell.execute_reply.started":"2022-04-05T05:47:07.965329Z","shell.execute_reply":"2022-04-05T05:47:08.088299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see p-values of the sensors 3,6,7,are more than 0.05 hence lets try the model by removing these variables","metadata":{}},{"cell_type":"code","source":"formula = 'ytrain~'+\"+\".join(list(xtrain.drop(columns = [\"sensor_03\", \"sensor_06\", \"sensor_07\"]).columns))\nmodel = smf.glm(formula, data = xtrain).fit()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:58:59.958001Z","iopub.execute_input":"2022-04-05T05:58:59.959151Z","iopub.status.idle":"2022-04-05T05:59:00.078327Z","shell.execute_reply.started":"2022-04-05T05:58:59.959095Z","shell.execute_reply":"2022-04-05T05:59:00.077499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There isnt much change in the model statistics even ofter omitting the few of the sensors.Lets once try predicting the reuslts and see how the accuracy is.","metadata":{}},{"cell_type":"code","source":"proba = model.predict(xtest) \n#remember we will be getting the probabilities from glm model\n\nfor i in range(40,60,1):\n    print(i, sum(ytest == (proba > i/100))/len(ytest))","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:04:04.868703Z","iopub.execute_input":"2022-04-05T06:04:04.869013Z","iopub.status.idle":"2022-04-05T06:04:04.975204Z","shell.execute_reply.started":"2022-04-05T06:04:04.86898Z","shell.execute_reply":"2022-04-05T06:04:04.97438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A probability cut off of 0.48 is giving an accuracy of 56.85 but still this isnt that significant. As we have seen in the distibution plots above the frequences are almost following in same distribution for both the states. Lets apply LDA for dimensionality reduction and see how the results comes.","metadata":{}},{"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nmodel = LDA(n_components = 1)\nxtrain_lda = model.fit_transform(xtrain, ytrain)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:10:24.276275Z","iopub.execute_input":"2022-04-05T06:10:24.276557Z","iopub.status.idle":"2022-04-05T06:10:24.328139Z","shell.execute_reply.started":"2022-04-05T06:10:24.276525Z","shell.execute_reply":"2022-04-05T06:10:24.326308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(xtrain_lda)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:22:47.503735Z","iopub.execute_input":"2022-04-05T06:22:47.504027Z","iopub.status.idle":"2022-04-05T06:22:47.981725Z","shell.execute_reply.started":"2022-04-05T06:22:47.503997Z","shell.execute_reply":"2022-04-05T06:22:47.980986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LDA as dimensionality reduction isnt helpful since the transformed data as single peak and isnt able to separate the two classes.\n\nLets use the logistic regression with a cut off of 0.48 for predictions as of now","metadata":{}},{"cell_type":"code","source":"df1 = pd.DataFrame(columns =  ['sequence','sensor_00', 'sensor_01', 'sensor_02',\n       'sensor_03', 'sensor_04', 'sensor_05', 'sensor_06', 'sensor_07',\n       'sensor_08', 'sensor_09', 'sensor_10', 'sensor_11', 'sensor_12'])\nfor i in test.sequence.unique():\n    df = test.drop(columns = [\"subject\",\"step\"]).query(f'sequence == {i}')\n    df2 = dict()\n    df2[\"sequence\"] = [i]\n    cols = ['sensor_00', 'sensor_01', 'sensor_02',\n       'sensor_03', 'sensor_04', 'sensor_05', 'sensor_06', 'sensor_07',\n       'sensor_08', 'sensor_09', 'sensor_10', 'sensor_11', 'sensor_12']\n    for j in cols:\n        df2[j] = [fft_sensor_freq(df[j])]\n    df1 = df1.append(pd.DataFrame(df2))\n    #if (i % 200 == 0):\n        #print(i)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:28:09.371682Z","iopub.execute_input":"2022-04-05T06:28:09.371998Z","iopub.status.idle":"2022-04-05T06:36:05.574294Z","shell.execute_reply.started":"2022-04-05T06:28:09.371965Z","shell.execute_reply":"2022-04-05T06:36:05.573545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = df1.copy()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:39:19.71324Z","iopub.execute_input":"2022-04-05T06:39:19.713926Z","iopub.status.idle":"2022-04-05T06:39:19.718707Z","shell.execute_reply.started":"2022-04-05T06:39:19.713852Z","shell.execute_reply":"2022-04-05T06:39:19.718191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = linear_model.LogisticRegression()\nmodel1.fit(final_train.drop(columns = [\"sequence\",\"state\"]), final_train.state)\nproba = model.predict_proba(test.drop(columns = \"sequence\"))\nproba_test1 = [a[1] for a in proba ]","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:41:40.614711Z","iopub.execute_input":"2022-04-05T06:41:40.615012Z","iopub.status.idle":"2022-04-05T06:41:40.702339Z","shell.execute_reply.started":"2022-04-05T06:41:40.614979Z","shell.execute_reply":"2022-04-05T06:41:40.701465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = np.array(proba_test1) > 0.48\nsub = pd.DataFrame(zip(test.sequence, preds), columns = [\"sequence\", \"state\"])\nsub.state = sub.state.astype(\"category\").cat.codes\nsub.to_csv(\"Sub.csv\") \n##Accuracy of 0.51","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:42:06.573327Z","iopub.execute_input":"2022-04-05T06:42:06.573804Z","iopub.status.idle":"2022-04-05T06:42:06.578485Z","shell.execute_reply.started":"2022-04-05T06:42:06.573759Z","shell.execute_reply":"2022-04-05T06:42:06.577822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:43:46.917735Z","iopub.execute_input":"2022-04-05T06:43:46.918155Z","iopub.status.idle":"2022-04-05T06:43:46.940291Z","shell.execute_reply.started":"2022-04-05T06:43:46.918125Z","shell.execute_reply":"2022-04-05T06:43:46.939625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:43:56.403191Z","iopub.execute_input":"2022-04-05T06:43:56.403478Z","iopub.status.idle":"2022-04-05T06:43:56.43831Z","shell.execute_reply.started":"2022-04-05T06:43:56.403444Z","shell.execute_reply":"2022-04-05T06:43:56.437648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}