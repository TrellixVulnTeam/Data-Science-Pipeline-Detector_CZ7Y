{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport plotly as py\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected = True)\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n\nimport optuna\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import GlobalMaxPooling1D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.layers import Concatenate, LSTM, GRU\nfrom tensorflow.keras.layers import Bidirectional, Multiply\n\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\n\nnp.random.seed(2022)\ntf.random.set_seed(2022)\n\npd.set_option('display.max_columns', None)\n#########################################################\ntrain = pd.read_csv('../input/tabular-playground-series-apr-2022/train.csv')\nt_lbls = pd.read_csv('../input/tabular-playground-series-apr-2022/train_labels.csv')\ntest = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')\nss = pd.read_csv('../input/tabular-playground-series-apr-2022/sample_submission.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-13T22:40:03.983259Z","iopub.execute_input":"2022-04-13T22:40:03.984082Z","iopub.status.idle":"2022-04-13T22:40:28.849727Z","shell.execute_reply.started":"2022-04-13T22:40:03.983886Z","shell.execute_reply":"2022-04-13T22:40:28.848599Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize = (15, 10))\nfor i, sensor in enumerate(train.columns.tolist()[3:]):\n    plt.subplot(3,6,i+1)\n    plt.title(sensor, size = 13)\n    a = sns.kdeplot(train[sensor], color='#c21b1b', linewidth = 0.6)\n    sns.kdeplot(test[sensor], color='#21a5de', linewidth = 0.6)\n    for j in ['right', 'left', 'top', 'bottom']:\n        a.spines[j].set_visible(False)\n    plt.xlabel('')\n    plt.ylabel('')\n    plt.xticks(size=8)\n    plt.yticks([])\n    \nplt.figtext(0.5, 1.05, 'Ditribution of sensors', size = 23, ha = 'center')\nplt.figtext(0.48, 1.01, 'Train', size = 15, ha = 'center', color='#c21b1b')\nplt.figtext(0.52, 1.01, 'Test', size = 15, ha = 'center', color='#21a5de')\nfig.tight_layout(pad = 3)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-13T22:40:36.744862Z","iopub.execute_input":"2022-04-13T22:40:36.745223Z","iopub.status.idle":"2022-04-13T22:42:32.123448Z","shell.execute_reply.started":"2022-04-13T22:40:36.74519Z","shell.execute_reply":"2022-04-13T22:42:32.12233Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix = np.triu(train[train.columns.tolist()[3:]].corr())\nplt.figure(figsize = (14, 14))\nplt.title('Correlation between sensors', size = 23)\na = sns.heatmap(train[train.columns.tolist()[3:]].corr(), annot = True, cmap = 'Blues', \n            mask = matrix, vmin = -0.2, vmax = 0.6, linewidths = 0.2, linecolor = 'white', cbar = False)\na.set_xticklabels(list('s_'+str(i) for i in range(13)))\na.set_yticklabels(list('s_'+str(i) for i in range(13)))\nplt.xticks(size = 12)\nplt.yticks(size = 12)\nplt.show()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-04-13T22:42:32.125415Z","iopub.execute_input":"2022-04-13T22:42:32.12575Z","iopub.status.idle":"2022-04-13T22:42:34.729098Z","shell.execute_reply.started":"2022-04-13T22:42:32.125709Z","shell.execute_reply":"2022-04-13T22:42:34.728155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seqs, i = list(t_lbls[t_lbls['state']==0]['sequence'][:3]) + list(t_lbls[t_lbls['state']==1]['sequence'][:3]), 0\ncolors = ['#c21b1b', '#c21b1b', '#c21b1b', '#21a5de', '#21a5de', '#21a5de']\nfig = plt.figure(figsize = (15, 20))\nfor sensor in train.columns.tolist()[3:]:\n    for color, seq in zip(colors, seqs):\n        i += 1\n        plt.subplot(13,6,i)\n        sns.set_style(\"white\")\n        if i < 7: \n            plt.title(f\"Sequence {seq}\", size = 12, fontname = 'monospace')\n        a = sns.lineplot(data=train[train['sequence']==seq][sensor], color = color, linewidth = 1)\n        plt.xlabel('')\n        plt.ylabel('')\n        if (i-1) % 6 == 0: \n            plt.ylabel(sensor, size = 12, fontname = 'monospace')\n        plt.xticks([])\n        plt.yticks([])\n        \nfig.tight_layout(h_pad = 3)\n\nplt.figtext(0.5, 1.04, 'Sequences examples', fontsize = 23, fontname = 'monospace', ha='center')\nplt.figtext(0.22, 1.02, 'Target 0', fontsize = 20, fontname = 'monospace', color = '#c21b1b')\nplt.figtext(0.71, 1.02, 'Target 1', fontsize = 20, fontname = 'monospace', color = '#21a5de')\n\nplt.show()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-04-13T22:42:34.730487Z","iopub.execute_input":"2022-04-13T22:42:34.730865Z","iopub.status.idle":"2022-04-13T22:42:41.878417Z","shell.execute_reply.started":"2022-04-13T22:42:34.730818Z","shell.execute_reply":"2022-04-13T22:42:41.877676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def color(x):\n    if x <= 0.25:\n        return 0\n    elif x >= 0.75:\n        return 0.5\n    else:\n        return 1\n\nsub_stat = t_lbls.merge(train[['sequence', 'subject']], on='sequence', how='left')\\\n.drop_duplicates().groupby('subject').agg({'state':['mean', 'count']}).reset_index()\nsub_stat.columns = sub_stat.columns.map('_'.join)\nsub_stat['text'] = 'Subject - <b>' + sub_stat[\"subject_\"].astype('str') + \\\n'</b> <br>State - <b>' + round(sub_stat[\"state_mean\"], 2).astype('str') + \\\n'</b> <br>Count - <b>' + sub_stat[\"state_count\"].astype('str') + '</b> <extra></extra>'\n\n\nfig = go.Figure()\nfig.add_trace(go.Scatter( \n    x = sub_stat['subject_'], \n    y = sub_stat['state_mean'],\n    mode = 'markers',\n    marker=dict(\n        size=sub_stat['state_count']*0.3,\n        color=((sub_stat['state_mean'].apply(lambda x: color(x)))),\n        colorscale=[[0, '#c21b1b'], [0.5, '#21a5de'], [1, '#ffdc2b']],\n        line=dict(width=0.1, color='black')\n    ),\n    hovertemplate = sub_stat['text']\n))\n\nfig.update_layout(width = 1150, height=600, plot_bgcolor = 'white', title = 'Subject states', \n                  title_font_size = 27, title_x = 0.5, title_y = 0.9,\n                  font_family=\"Calibri\", font_color=\"black\")\n\nfig.update_yaxes(title_text='Mean state', showline = True, linecolor = '#f5f2f2', \n                 showgrid = True, gridwidth = 1, gridcolor = '#f5f2f2',\n                 linewidth = 2, tickfont_size = 12, tickvals=[0.0, 0.25, 0.50, 0.75, 1.0])\n\nfig.update_xaxes(title_text='Subject', showline = True, linecolor = '#f5f2f2')\n\nfig.show()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-04-13T22:42:41.880096Z","iopub.execute_input":"2022-04-13T22:42:41.880622Z","iopub.status.idle":"2022-04-13T22:42:42.362682Z","shell.execute_reply.started":"2022-04-13T22:42:41.880587Z","shell.execute_reply":"2022-04-13T22:42:42.361756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=t_lbls.groupby(['state'])['state'].count()\ny=len(t_lbls)\nr=((x/y)).round(4)\n\nmf_ratio = pd.DataFrame(r).T\n\nfig, ax = plt.subplots(1,1,figsize=(19, 2))\nplt.title('Target distribution', size=23, y=1.05)\n\nax.barh(mf_ratio.index, mf_ratio[0], \n        color='#c21b1b', alpha=0.9, label='Zero')\nax.barh(mf_ratio.index, mf_ratio[1], left=mf_ratio[0], \n        color='#21a5de', alpha=0.9, label='One')\n\nax.set_xlim(0, 1)\nax.set_xticks([])\nax.set_yticks([])\n\nax.annotate(f\"{mf_ratio[0]['state']*100}%\", \n                   xy=(mf_ratio[0]['state']/2, 'state'),\n                   va = 'center', ha='center',fontsize=40,\n                   color='white')\n\nax.annotate('Target \"0\"', xy=(mf_ratio[0]['state']/2, -0.25),\n                   va = 'center', ha='center',fontsize=15,\n                   color='white')\n    \nax.annotate(f\"{mf_ratio[1]['state']*100}%\", \n                   xy=(mf_ratio[0]['state']+mf_ratio[1]['state']/2, 'state'),\n                   va = 'center', ha='center',fontsize=40,\n                   color='white')\n    \nax.annotate('Target \"1\"', xy=(mf_ratio[0]['state']+mf_ratio[1]['state']/2, -0.25),\n                   va = 'center', ha='center',fontsize=15,\n                   color='white')\n\nfor i in ['top', 'left', 'right', 'bottom']:\n    ax.spines[i].set_visible(False)\n    \nplt.show()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-04-13T22:42:42.364062Z","iopub.execute_input":"2022-04-13T22:42:42.364421Z","iopub.status.idle":"2022-04-13T22:42:42.479739Z","shell.execute_reply.started":"2022-04-13T22:42:42.364385Z","shell.execute_reply":"2022-04-13T22:42:42.478811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"The first and most obvious features that came to mind in my first [notebook](https://www.kaggle.com/code/dmitryuarov/tps-sensors-2xlstm-xgb-auc-0-976) at the beginning of the competition were lags and differences between steps. After experimenting, first, I found new features that allowed me to get better mean val AUC on 12 folds: *rolling* + *mean/std/sum*. I also tried another functions in rolling, like *var*, *min*, *max* and *expanding* window function, but they only made the result worse. Then I added 3 experemental features because of ~0.5+ correlation between sensors.\n\nAnd the main thing, that turned out to be very important - correlation between state and count of sequences that the subject had. As you can see on my plot, subjects, who had more than ~95 sequences, were more likely to get target \"1\" and subjects, who had less than ~25 sequences, were more likely to get target \"0\". Therefore, dividing the sequences into three groups turned out to be a great idea. Although this improved the data, but in real life this would not have worked, because most likely we would have predicted the result of one sequence for one subject, therefore the feature about the number of sequences would have been useless.\n\nIn more details, how the new features affected, I wrote below.","metadata":{}},{"cell_type":"code","source":"features = train.columns.tolist()[3:]\n\ndef sub_imp(x):\n    if x < 25:\n        return 0\n    elif x > 95:\n        return 2\n    else:\n        return 1\n\ndef prep(df):\n    for feature in features:\n        df[feature+'_lag1'] = df.groupby('sequence')[feature].shift(1)\n        df[feature+'_back_lag1'] = df.groupby('sequence')[feature].shift(-1)\n        \n        df.fillna(0, inplace=True)\n        df[feature+'_diff1'] = df[feature] - df[feature+'_lag1']\n        \n        # New features\n        for window in [3,6,12]:\n            df[feature+'_roll_'+str(window)+'_mean'] = df.groupby('sequence')[feature]\\\n            .rolling(window=window, min_periods=1).mean().reset_index(level=0,drop=True)\n            \n            df[feature+'_roll_'+str(window)+'_std'] = df.groupby('sequence')[feature]\\\n            .rolling(window=window, min_periods=1).std().reset_index(level=0,drop=True)\n            \n            df[feature+'_roll_'+str(window)+'_sum'] = df.groupby('sequence')[feature]\\\n            .rolling(window=window, min_periods=1).sum().reset_index(level=0,drop=True)\n        \n    # Experemental features\n    df['sens_00_06'] = df['sensor_00'] * df['sensor_06']\n    df['sens_03_07'] = df['sensor_03'] * df['sensor_07']\n    df['sens_03_11'] = df['sensor_03'] * df['sensor_11']\n\n    for feature in ['sens_00_06', 'sens_03_07', 'sens_03_11']:\n        df[feature + '_lag1'] = df.groupby('sequence')[feature].shift(1)\n    df.fillna(0, inplace=True)\n    \n    # Subject feature\n    sub_stat = df[['sequence', 'subject']].drop_duplicates().groupby('subject').agg({'sequence': 'count'})\\\n    .rename(columns={'sequence': 'count'}).reset_index()\n    df = df.merge(sub_stat, on='subject', how='left')\n    df['sub_imp'] = df['count'].apply(lambda x: sub_imp(x))\n    df.drop('count', axis=1, inplace=True)\n     \nprep(train)\nprep(test)\n\nfeatures = train.columns.tolist()[3:]\nsc = StandardScaler()\ntrain[features] = sc.fit_transform(train[features])\ntest[features] = sc.transform(test[features])\n\ngroups = train[\"sequence\"]\nlabels = t_lbls[\"state\"]\n\ntrain = train.drop([\"sequence\", \"subject\", \"step\"], axis=1).values\ntrain = train.reshape(-1, 60, train.shape[-1])\n\ntest = test.drop([\"sequence\", \"subject\", \"step\"], axis=1).values\ntest = test.reshape(-1, 60, test.shape[-1])","metadata":{"execution":{"iopub.status.busy":"2022-04-13T22:42:42.480963Z","iopub.execute_input":"2022-04-13T22:42:42.481201Z","iopub.status.idle":"2022-04-13T22:50:25.096207Z","shell.execute_reply.started":"2022-04-13T22:42:42.481174Z","shell.execute_reply":"2022-04-13T22:50:25.094858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM ","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    BATCH_SIZE = tpu_strategy.num_replicas_in_sync * 64\n    print(\"Running on TPU:\", tpu.master())\n    print(f\"Batch Size: {BATCH_SIZE}\")\n    \nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\n    BATCH_SIZE = 256\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    print(f\"Batch Size: {BATCH_SIZE}\")\n    \ndef hist_plot(history):\n    fig = plt.figure(figsize = (5, 3))\n    plt.plot(history.history['auc'])\n    plt.plot(history.history['val_auc'])\n    plt.grid(color = 'gray', linestyle = '-', axis = 'both', linewidth=0.5, visible=0.5)\n    plt.plot(history.history['val_auc'].index(max(history.history['val_auc'])), \n         max(history.history['val_auc']), 'ko', markersize = 5,\n             fillstyle = 'full', color = 'r')\n    plt.title('model training')\n    plt.ylabel('AUC')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='lower right')\n    plt.show()","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-13T22:50:25.098985Z","iopub.execute_input":"2022-04-13T22:50:25.099336Z","iopub.status.idle":"2022-04-13T22:50:31.216162Z","shell.execute_reply.started":"2022-04-13T22:50:25.099293Z","shell.execute_reply":"2022-04-13T22:50:31.215124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dnn_model():\n\n    x_input = Input(shape=(train.shape[-2:]))\n    x1 = Bidirectional(LSTM(768, return_sequences=True))(x_input)\n        \n    x21 = Bidirectional(LSTM(512, return_sequences=True))(x1)\n    x22 = Bidirectional(LSTM(512, return_sequences=True))(x_input)\n    l2 = Concatenate(axis=2)([x21, x22])\n        \n    x31 = Bidirectional(LSTM(384, return_sequences=True))(l2)\n    x32 = Bidirectional(LSTM(384, return_sequences=True))(x21)\n    l3 = Concatenate(axis=2)([x31, x32])\n        \n    x41 = Bidirectional(LSTM(256, return_sequences=True))(l3)\n    x42 = Bidirectional(LSTM(128, return_sequences=True))(x32)\n    l4 = Concatenate(axis=2)([x41, x42])\n        \n    l5 = Concatenate(axis=2)([x1, l2, l3, l4])\n    g = GlobalMaxPooling1D()(l5)\n    x7 = Dense(128, activation='selu')(g)\n    d = Dropout(0.05)(x7)\n    x_output = Dense(units=1, activation=\"sigmoid\")(d)\n    \n    model = Model(inputs=x_input, outputs=x_output, name='lstm_model')\n    \n    return model\n\nmodel = dnn_model()\n\nplot_model(\n    model, \n    show_shapes=True,\n    show_layer_names=True\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T22:50:31.217784Z","iopub.execute_input":"2022-04-13T22:50:31.218657Z","iopub.status.idle":"2022-04-13T22:50:39.172293Z","shell.execute_reply.started":"2022-04-13T22:50:31.218612Z","shell.execute_reply":"2022-04-13T22:50:39.170906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I use strict learning conditions within which, if the val score has not increased in the epoch, then I reduce the learning rate by half. As the practice of learning from the available data has shown, this is permissible and significantly speeds up the learning process.","metadata":{}},{"cell_type":"code","source":"with tpu_strategy.scope():\n    VERBOSE = False\n    predictions, scores = [], []\n    k = GroupKFold(n_splits = 12)\n\n    for fold, (train_idx, val_idx) in enumerate(k.split(train, labels, groups.unique())):\n        print('-'*17, '>', f'Fold {fold+1}', '<', '-'*17)\n    \n        X_train, X_val = train[train_idx], train[val_idx]\n        y_train, y_val = labels.iloc[train_idx].values, labels.iloc[val_idx].values\n        \n        model = dnn_model()\n        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics='AUC')\n\n        lr = ReduceLROnPlateau(monitor=\"val_auc\", factor=0.5, \n                               patience=1, verbose=VERBOSE, mode=\"max\")\n\n        es = EarlyStopping(monitor=\"val_auc\", patience=3, \n                           verbose=VERBOSE, mode=\"max\", \n                           restore_best_weights=True)\n        \n        save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n        chk_point = ModelCheckpoint(f'./TPS_model_2022_{fold+1}C.h5', options=save_locally, \n                                    monitor='val_auc', verbose=VERBOSE, \n                                    save_best_only=True, mode='max')\n        \n        training = model.fit(X_train, y_train, \n                  validation_data=(X_val, y_val), \n                  epochs=20,\n                  verbose=VERBOSE,\n                  batch_size=BATCH_SIZE, \n                  callbacks=[lr, chk_point, es])\n        \n        hist_plot(training)\n        \n        load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n        model = load_model(f'./TPS_model_2022_{fold+1}C.h5', options=load_locally)\n        \n        y_pred = model.predict(X_val, batch_size=BATCH_SIZE).squeeze()\n        score = roc_auc_score(y_val, y_pred)\n        scores.append(score)\n        predictions.append(model.predict(test, batch_size=BATCH_SIZE).squeeze())\n        print(f\"Fold-{fold+1} | OOF Score: {score}\")\n    \n    print('-'*40)\n    print(f'Mean AUC on {k.n_splits} folds - {np.mean(scores)}')\n\ndel X_train, X_val, y_train, y_val\nss[\"state\"] = sum(predictions)/k.n_splits ","metadata":{"execution":{"iopub.status.busy":"2022-04-13T22:50:39.176297Z","iopub.execute_input":"2022-04-13T22:50:39.176923Z","iopub.status.idle":"2022-04-13T23:33:54.026595Z","shell.execute_reply.started":"2022-04-13T22:50:39.176858Z","shell.execute_reply":"2022-04-13T23:33:54.0253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, how new features improved the result of the model:\n\n1. Rolling features: *0.968 - 0.970*\n2. Experemental features: *0.970 - 0.971*\n3. Subject feature: *0.971 -* **0.977**","metadata":{}},{"cell_type":"markdown","source":"# Blending and postprocessing","metadata":{}},{"cell_type":"markdown","source":"Looking ahead, I will do what many people like to do.","metadata":{}},{"cell_type":"code","source":"s2 = pd.read_csv('../input/tps-apr/en_blend_0977.csv') # In this submission blending results from my first work (XGB + LSTM with old features)\nss['state'] = ss['state']*0.5 + s2['state']*0.5","metadata":{"execution":{"iopub.status.busy":"2022-04-13T23:49:23.546173Z","iopub.execute_input":"2022-04-13T23:49:23.546587Z","iopub.status.idle":"2022-04-13T23:49:23.575705Z","shell.execute_reply.started":"2022-04-13T23:49:23.54655Z","shell.execute_reply":"2022-04-13T23:49:23.574713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The idea of postprocessing is based on the number of sequences that the subjects had. There is a very dangerous point here, so in order not to take too much risk, I have set conditions suitable only for those sequences in which we can be very confident. This is a minor improvement that can improve the result on LB by about 0.0005-0.001.","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')\npost = ss.merge(test[['sequence', 'subject']], on='sequence', how='left').drop_duplicates()\npost = post.merge(post.groupby('subject').agg({'state':'count'})\\\n                  .reset_index().rename(columns={'state': 'count'}), on='subject', how='left')\n\nplt.title('Basic predictions')\nsns.histplot(post['state'])\nplt.show()\n\ndef repredict(row):\n    if row['count'] < 20 and row['state'] < 0.3:\n        return 0.0\n    elif row['count'] > 100 and row['state'] > 0.7:\n        return 1.0\n    else:\n        return row['state']\n    \npost['repredict'] = post[['state', 'subject', 'count']].apply(lambda row: repredict(row), axis=1)\n\nprint('-'*35)\nprint(f\"{len(ss) - sum(post['repredict'] == post['state'])} predictions have been rounded\")\nprint('-'*35)\nprint()\n\nplt.title('Postprocessing predictions')\nsns.histplot(post['repredict'])\nplt.show()\n\nss = post.drop(['state', 'subject', 'count'], axis=1).rename(columns={'repredict': 'state'})\nss.to_csv('blend_sub31_exp.csv', index=False)\nss","metadata":{"execution":{"iopub.status.busy":"2022-04-13T23:50:46.291477Z","iopub.execute_input":"2022-04-13T23:50:46.292367Z","iopub.status.idle":"2022-04-13T23:50:51.498418Z","shell.execute_reply.started":"2022-04-13T23:50:46.292279Z","shell.execute_reply":"2022-04-13T23:50:51.49699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**I think I have added new ideas for new experiments and hope you enjoyed my work, so I will be glad for the upvote :)**","metadata":{}}]}