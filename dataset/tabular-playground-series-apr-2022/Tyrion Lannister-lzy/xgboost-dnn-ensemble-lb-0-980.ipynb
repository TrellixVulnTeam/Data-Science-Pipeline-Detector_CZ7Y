{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hello Kagglers, in this Notebook I will try to sync with the workflow of a typical ML competition.\n\nand besides this is my first notebook, hope to gain experience in writing this..\n\n> If this notebook is useful to you, please DO UPVOTE ðŸ—³","metadata":{}},{"cell_type":"markdown","source":"<h2>Competition Goal</h2>\n\nWith  the provided thousands of sixty-second sequences of sensor data recorded from several hundred participants who could have been in either of two possible activity states. \n\ndetermine what state a participant was in from the sensor data.","metadata":{}},{"cell_type":"markdown","source":"<h1>Importing Libraries</h1>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected = True)\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:09:40.723616Z","iopub.execute_input":"2022-04-23T08:09:40.724055Z","iopub.status.idle":"2022-04-23T08:09:43.082399Z","shell.execute_reply.started":"2022-04-23T08:09:40.723941Z","shell.execute_reply":"2022-04-23T08:09:43.081579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Loading Data</h2>","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-apr-2022/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')\nsubmission = pd.read_csv(\"../input/tabular-playground-series-apr-2022/sample_submission.csv\")\nlabels = pd.read_csv(\"../input/tabular-playground-series-apr-2022/train_labels.csv\")\n\ntrain","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:09:43.083867Z","iopub.execute_input":"2022-04-23T08:09:43.08427Z","iopub.status.idle":"2022-04-23T08:09:55.226426Z","shell.execute_reply.started":"2022-04-23T08:09:43.084238Z","shell.execute_reply":"2022-04-23T08:09:55.225836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data Description from kaggle \n\ntrain.csv \n\n*  sequence - a unique id for each sequence\n\n* subject - a unique id for the subject in the experiment\n\n* step - time step of the recording, in one second intervals\n\n* sensor_00 - sensor_12 - the value for each of the thirteen sensors at that time step\n\n* state - the value for each of the thirteen sensors at that time step ##\n\n\ntrain_labels.csv - the class label for each sequence.\n\n* sequence - the unique id for each sequence.\n\n* state - the state associated to each sequence. This is the target which you are trying to predict.\n\n\ntest.csv - the test set. For each of the ~12,000 sequences, you should predict a value for that sequence's state.","metadata":{}},{"cell_type":"code","source":"# give you an quike insght into the train data \n# including count,mean,std,min,25%,50%,75% and max value\ntrain.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:09:55.227431Z","iopub.execute_input":"2022-04-23T08:09:55.227768Z","iopub.status.idle":"2022-04-23T08:09:56.147853Z","shell.execute_reply.started":"2022-04-23T08:09:55.227739Z","shell.execute_reply":"2022-04-23T08:09:56.147293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run the code below to check if missing data exits\ntrain.isnull().sum(axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:09:56.149431Z","iopub.execute_input":"2022-04-23T08:09:56.150181Z","iopub.status.idle":"2022-04-23T08:09:56.210858Z","shell.execute_reply.started":"2022-04-23T08:09:56.150146Z","shell.execute_reply":"2022-04-23T08:09:56.210043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>adding labels to train data</h3>","metadata":{}},{"cell_type":"code","source":"labels.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:09:56.212074Z","iopub.execute_input":"2022-04-23T08:09:56.212346Z","iopub.status.idle":"2022-04-23T08:09:56.221502Z","shell.execute_reply.started":"2022-04-23T08:09:56.212309Z","shell.execute_reply":"2022-04-23T08:09:56.220433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train =train.merge(labels,how='left', on=[\"sequence\"])\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:09:56.223113Z","iopub.execute_input":"2022-04-23T08:09:56.223448Z","iopub.status.idle":"2022-04-23T08:09:56.47963Z","shell.execute_reply.started":"2022-04-23T08:09:56.223405Z","shell.execute_reply":"2022-04-23T08:09:56.478788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Heatmap && Correlation</h2>","metadata":{}},{"cell_type":"markdown","source":"easy to fing that there is no missing data. \n\nin the next block we will use the **heatmap** hoping to find the \n\nrelationship between sensors through calcaluting **correlation**\n\n**here we use the whole train data to find the correlation**\n\nyou can change the train.corr() to ,like,**train[60:120].corr()** to find the second sequence's correlation\n\n","metadata":{}},{"cell_type":"markdown","source":"> If this notebook is useful to you, please DO UPVOTE ðŸ—³\n","metadata":{}},{"cell_type":"code","source":"# set the size of the map\nfeatures  = [col for col in test.columns if col not in (\"sequence\",\"step\",\"subject\")]\nplt.figure(figsize = (15,7))\n\nhm = sns.heatmap(train[features].corr(),    # data\n                cmap = 'coolwarm',# style\n                annot = True,     # True to show the specific values\n                fmt = '.2f',      # set the precision\n                linewidths = 0.05)\nplt.title('Correlation Heatmap for Train dataset', \n              fontsize=14, \n              fontweight='bold')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:09:56.480858Z","iopub.execute_input":"2022-04-23T08:09:56.481103Z","iopub.status.idle":"2022-04-23T08:09:58.844921Z","shell.execute_reply.started":"2022-04-23T08:09:56.481074Z","shell.execute_reply":"2022-04-23T08:09:58.843964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"through the heatmap we may have find that\nsensor **00, 01, 03, 06, 07, 09, 11** have something to dig\n\nalso the sensor 04 and 10 but we just ignore it temporarily\n\nso we will focus on them.","metadata":{}},{"cell_type":"code","source":"col_t=[\"sensor_00\",\"sensor_01\",\"sensor_03\",\"sensor_06\",\"sensor_07\",\"sensor_09\",\"sensor_11\"]\n\n# set the size of the map\nplt.figure(figsize = (9,5))\n\nhm = sns.heatmap(train[col_t].corr(),    # data\n                cmap = 'coolwarm',      \n                annot = True,     \n                fmt = '.2f', \n                linewidths = 0.05)\nplt.title('Correlation Heatmap for Selected columns from Train dataset', \n              fontsize=14, \n              fontweight='bold')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:09:58.846427Z","iopub.execute_input":"2022-04-23T08:09:58.84705Z","iopub.status.idle":"2022-04-23T08:09:59.727475Z","shell.execute_reply.started":"2022-04-23T08:09:58.847005Z","shell.execute_reply":"2022-04-23T08:09:59.726569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a quike glimpse into the data of the sensors.","metadata":{}},{"cell_type":"code","source":"sequences = [0, 1, 2, 3, 4, 5]\nfigure, axes = plt.subplots(13, len(sequences), sharex=True, figsize=(16, 16))\nfor i, sequence in enumerate(sequences):\n    for sensor in range(13):\n        sensor_name = f\"sensor_{sensor:02d}\"\n        plt.subplot(13, len(sequences), sensor * len(sequences) + i + 1)\n        plt.plot(range(60), train[train.sequence == sequence][sensor_name],\n                color=plt.rcParams['axes.prop_cycle'].by_key()['color'][i % 10])\n        if sensor == 0: plt.title(f\"Sequence {sequence}\")\n        if sequence == sequences[0]: plt.ylabel(sensor_name)\nfigure.tight_layout(w_pad=0.1)\nplt.suptitle('Selected Time Series', y=1.02)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:09:59.728956Z","iopub.execute_input":"2022-04-23T08:09:59.729475Z","iopub.status.idle":"2022-04-23T08:10:08.896287Z","shell.execute_reply.started":"2022-04-23T08:09:59.72943Z","shell.execute_reply":"2022-04-23T08:10:08.895073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Feature Seeking bewteen Target and Sensors Data</h1>","metadata":{}},{"cell_type":"markdown","source":"here we first introduce the concept of Mutual Information(MI).\n\nMutual information describes relationships in terms of uncertainty. The MI between two quantities is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other. ","metadata":{}},{"cell_type":"code","source":"# from sklearn.feature_selection import mutual_info_regression\n\n# def make_mi_scores(X, y, discrete_features):\n#     mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n#     mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n#     mi_scores = mi_scores.sort_values(ascending=False)\n#     return mi_scores","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:10:08.9003Z","iopub.execute_input":"2022-04-23T08:10:08.900907Z","iopub.status.idle":"2022-04-23T08:10:08.905125Z","shell.execute_reply.started":"2022-04-23T08:10:08.900859Z","shell.execute_reply":"2022-04-23T08:10:08.904234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_mi = train.copy()\n# y_mi = X_mi.pop(\"state\")\n\n# # Label encoding for categoricals\n# for colname in X_mi.select_dtypes(\"object\"):\n#     X_mi[colname], _ = X_mi[colname].factorize()\n\n# # All discrete features should now have integer dtypes (double-check this before using MI!)\n# discrete_features = X_mi.dtypes == int","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:10:08.906378Z","iopub.execute_input":"2022-04-23T08:10:08.906608Z","iopub.status.idle":"2022-04-23T08:10:08.916456Z","shell.execute_reply.started":"2022-04-23T08:10:08.90658Z","shell.execute_reply":"2022-04-23T08:10:08.915455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The least possible mutual information between quantities is 0.0. When MI is zero, the quantities are independent: neither can tell you anything about the other. Conversely, in theory there's no upper bound to what MI can be. In practice though values above 2.0 or so are uncommon. (Mutual information is a **logarithmic quantity**, so it increases very slowly.)\n","metadata":{}},{"cell_type":"code","source":"# %%time\n# mi_scores = make_mi_scores(X_mi, y_mi, discrete_features)\n# mi_scores[::3]  # show a few features with their MI scores","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:10:08.917627Z","iopub.execute_input":"2022-04-23T08:10:08.917951Z","iopub.status.idle":"2022-04-23T08:10:08.92856Z","shell.execute_reply.started":"2022-04-23T08:10:08.917923Z","shell.execute_reply":"2022-04-23T08:10:08.927508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mi_scores","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:10:08.929762Z","iopub.execute_input":"2022-04-23T08:10:08.930005Z","iopub.status.idle":"2022-04-23T08:10:08.938978Z","shell.execute_reply.started":"2022-04-23T08:10:08.929977Z","shell.execute_reply":"2022-04-23T08:10:08.938193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nbelow I will test the 'mean', 'max', 'min', 'var', 'mad', 'sum', 'median' value of the data hoping to dig anything valuable\n\n> the codes below are inspired by C4rl05/V with her work [https://www.kaggle.com/code/cv13j0/tps-apr-2022-xgboost-model](http://)","metadata":{}},{"cell_type":"code","source":"def aggregated_features(df, aggregation_cols = ['sequence'], prefix = ''):\n    agg_strategy = {'sensor_00': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_01': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_02': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_03': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_04': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_05': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_06': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_07': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_08': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_09': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_10': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_11': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_12': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                   }\n    group = df.groupby(aggregation_cols).aggregate(agg_strategy)\n    group.columns = ['_'.join(col).strip() for col in group.columns]\n    group.columns = [str(prefix) + str(col) for col in group.columns]\n    group.reset_index(inplace = True)\n    \n    temp = (df.groupby(aggregation_cols).size().reset_index(name = str(prefix) + 'size'))\n    group = pd.merge(temp, group, how = 'left', on = aggregation_cols,)\n    return group","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:10:08.940144Z","iopub.execute_input":"2022-04-23T08:10:08.940505Z","iopub.status.idle":"2022-04-23T08:10:08.953849Z","shell.execute_reply.started":"2022-04-23T08:10:08.940477Z","shell.execute_reply":"2022-04-23T08:10:08.953133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_merge_data = aggregated_features(train, aggregation_cols = ['sequence', 'subject'])\ntest_merge_data = aggregated_features(test, aggregation_cols = ['sequence', 'subject'])","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:10:08.955143Z","iopub.execute_input":"2022-04-23T08:10:08.955573Z","iopub.status.idle":"2022-04-23T08:13:37.240353Z","shell.execute_reply.started":"2022-04-23T08:10:08.955529Z","shell.execute_reply":"2022-04-23T08:13:37.239546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_subjects_merge_data = aggregated_features(train, aggregation_cols = ['subject'], prefix = 'subject_')\ntest_subjects_merge_data = aggregated_features(test, aggregation_cols = ['subject'], prefix = 'subject_')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:13:37.241487Z","iopub.execute_input":"2022-04-23T08:13:37.242177Z","iopub.status.idle":"2022-04-23T08:13:45.951075Z","shell.execute_reply.started":"2022-04-23T08:13:37.242137Z","shell.execute_reply":"2022-04-23T08:13:45.950012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"up to now we have a clear view of the values of sensors ","metadata":{}},{"cell_type":"code","source":"train_subjects_merge_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:13:45.95268Z","iopub.execute_input":"2022-04-23T08:13:45.953044Z","iopub.status.idle":"2022-04-23T08:13:45.98238Z","shell.execute_reply.started":"2022-04-23T08:13:45.953006Z","shell.execute_reply":"2022-04-23T08:13:45.981317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Experimenting with Lags</h3>\n\n>lagging is a commom techinic used in time series datasets\n\nLagging a time series means to shift its values forward one or more time steps, or equivalently, to shift the times in its index backward one or more steps. In either case, the effect is that the observations in the lagged series will appear to have happened later in time.","metadata":{}},{"cell_type":"code","source":"train['sensor_00_lag_01'] = train['sensor_00'].shift(1)\ntrain['sensor_00_lag_10'] = train['sensor_00'].shift(10)\ntrain.head(15)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:13:45.983668Z","iopub.execute_input":"2022-04-23T08:13:45.983934Z","iopub.status.idle":"2022-04-23T08:13:46.030418Z","shell.execute_reply.started":"2022-04-23T08:13:45.983903Z","shell.execute_reply":"2022-04-23T08:13:46.029351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Merging the Datasets before Training</h3>","metadata":{}},{"cell_type":"code","source":"train_merge_data = train_merge_data.merge(labels, how = 'left', on = 'sequence')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:13:46.031664Z","iopub.execute_input":"2022-04-23T08:13:46.031908Z","iopub.status.idle":"2022-04-23T08:13:46.053654Z","shell.execute_reply.started":"2022-04-23T08:13:46.031878Z","shell.execute_reply":"2022-04-23T08:13:46.052593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_merge_data = train_merge_data.merge(train_subjects_merge_data, how = 'left', on = 'subject')\ntest_merge_data = test_merge_data.merge(test_subjects_merge_data, how = 'left', on = 'subject')\ntrain_merge_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:13:46.05544Z","iopub.execute_input":"2022-04-23T08:13:46.056156Z","iopub.status.idle":"2022-04-23T08:13:46.106946Z","shell.execute_reply.started":"2022-04-23T08:13:46.056112Z","shell.execute_reply":"2022-04-23T08:13:46.106337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_merge_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:13:46.107973Z","iopub.execute_input":"2022-04-23T08:13:46.108713Z","iopub.status.idle":"2022-04-23T08:13:46.13496Z","shell.execute_reply.started":"2022-04-23T08:13:46.10868Z","shell.execute_reply":"2022-04-23T08:13:46.133799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Post Processing the Information for the Model</h3>","metadata":{}},{"cell_type":"code","source":"ignore = ['sequence', 'state', 'subject']\nfeatures = [feat for feat in train_merge_data.columns if feat not in ignore]\ntarget_feature = 'state'","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:13:46.136383Z","iopub.execute_input":"2022-04-23T08:13:46.13674Z","iopub.status.idle":"2022-04-23T08:13:46.145315Z","shell.execute_reply.started":"2022-04-23T08:13:46.136704Z","shell.execute_reply":"2022-04-23T08:13:46.144578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Train - Test Split </h3>\n\nyou may do cross-validation too.","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import train_test_split\ntest_size_pct = 0.3\nX_train, X_valid, y_train, y_valid = train_test_split(\n                                train_merge_data[features], \n                                train_merge_data[target_feature], \n                                test_size = test_size_pct, \n                                random_state = 2022)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:13:46.146657Z","iopub.execute_input":"2022-04-23T08:13:46.147609Z","iopub.status.idle":"2022-04-23T08:13:46.391137Z","shell.execute_reply.started":"2022-04-23T08:13:46.147572Z","shell.execute_reply":"2022-04-23T08:13:46.39008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Building a XGBoost Model</h3>","metadata":{}},{"cell_type":"code","source":"from xgboost  import XGBClassifier\n\nparams = {'n_estimators': 8192,\n          'max_depth': 7,\n          'learning_rate': 0.1,\n          'subsample': 0.96,\n          'colsample_bytree': 0.80,\n          'reg_lambda': 1.50,\n          'reg_alpha': 6.10,\n          'gamma': 1.40,\n          'random_state': 16,\n          'objective': 'binary:logistic',\n          #'tree_method': 'gpu_hist',\n         }\n\nxgb = XGBClassifier(**params)\nxgb.fit(X_train, y_train, \n        eval_set = [(X_valid, y_valid)], \n        eval_metric = ['auc','logloss'], \n        early_stopping_rounds = 64, \n        verbose = 32)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:13:46.392975Z","iopub.execute_input":"2022-04-23T08:13:46.393454Z","iopub.status.idle":"2022-04-23T08:15:51.246831Z","shell.execute_reply.started":"2022-04-23T08:13:46.393415Z","shell.execute_reply":"2022-04-23T08:15:51.245965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Building a LGBM Model</h3>","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\nparams = {'n_estimators': 8192,\n          'max_depth': 7,\n          'learning_rate': 0.1,\n          'subsample': 0.96,\n          'colsample_bytree': 0.80,\n          'reg_lambda': 1.50,\n          'reg_alpha': 6.10,\n          'gamma': 1.40,\n          'random_state': 16\n         }\n\nlgb=LGBMClassifier(**params)\nlgb.fit(X_train, y_train, \n        eval_set = [(X_valid, y_valid)], \n        eval_metric = ['auc','logloss'], \n        early_stopping_rounds = 64, \n        verbose = 32)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:15:51.248379Z","iopub.execute_input":"2022-04-23T08:15:51.24934Z","iopub.status.idle":"2022-04-23T08:16:00.316787Z","shell.execute_reply.started":"2022-04-23T08:15:51.249293Z","shell.execute_reply":"2022-04-23T08:16:00.316069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\net=ExtraTreesClassifier()\net.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:16:00.318279Z","iopub.execute_input":"2022-04-23T08:16:00.318764Z","iopub.status.idle":"2022-04-23T08:16:06.135399Z","shell.execute_reply.started":"2022-04-23T08:16:00.318726Z","shell.execute_reply":"2022-04-23T08:16:06.134625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\nada=AdaBoostClassifier(n_estimators=1000)\nada.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:16:06.13657Z","iopub.execute_input":"2022-04-23T08:16:06.136803Z","iopub.status.idle":"2022-04-23T08:16:39.918959Z","shell.execute_reply.started":"2022-04-23T08:16:06.136776Z","shell.execute_reply":"2022-04-23T08:16:39.916404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb=GradientBoostingClassifier()\ngb.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:16:39.920251Z","iopub.status.idle":"2022-04-23T08:16:39.920589Z","shell.execute_reply.started":"2022-04-23T08:16:39.920413Z","shell.execute_reply":"2022-04-23T08:16:39.920429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Score the built Models</h3>","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\npreds = xgb.predict_proba(X_valid)[:, 1]\nscore = roc_auc_score(y_valid, preds)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:16:39.921634Z","iopub.status.idle":"2022-04-23T08:16:39.921949Z","shell.execute_reply.started":"2022-04-23T08:16:39.921788Z","shell.execute_reply":"2022-04-23T08:16:39.921803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_lgb=lgb.predict_proba(X_valid)[:,1]\nscore = roc_auc_score(y_valid, preds_lgb)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:16:39.922974Z","iopub.status.idle":"2022-04-23T08:16:39.92329Z","shell.execute_reply.started":"2022-04-23T08:16:39.923112Z","shell.execute_reply":"2022-04-23T08:16:39.923126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_et=et.predict_proba(X_valid)[:,1]\nscore = roc_auc_score(y_valid, preds_et)\nprint(score)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:16:39.924296Z","iopub.status.idle":"2022-04-23T08:16:39.924595Z","shell.execute_reply.started":"2022-04-23T08:16:39.924441Z","shell.execute_reply":"2022-04-23T08:16:39.924455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_ada=ada.predict_proba(X_valid)[:,1]\nscore = roc_auc_score(y_valid, preds_ada)\nprint(score)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:16:39.926397Z","iopub.status.idle":"2022-04-23T08:16:39.92726Z","shell.execute_reply.started":"2022-04-23T08:16:39.927014Z","shell.execute_reply":"2022-04-23T08:16:39.927034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_gb=gb.predict_proba(X_valid)[:,1]\nscore = roc_auc_score(y_valid, preds_gb)\nprint(score)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:16:39.92798Z","iopub.status.idle":"2022-04-23T08:16:39.928742Z","shell.execute_reply.started":"2022-04-23T08:16:39.928544Z","shell.execute_reply":"2022-04-23T08:16:39.928563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Check the Feature Importance through plots</h3>","metadata":{}},{"cell_type":"code","source":"def plot_feature_importance(importance, names, model_type, max_features = 10):\n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n    fi_df = fi_df.head(max_features)\n\n    #Define size of bar plot\n    plt.figure(figsize=(8,6))\n    \n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + 'FEATURE IMPORTANCE')\n    plt.xlabel('IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:17:32.983217Z","iopub.execute_input":"2022-04-23T08:17:32.983636Z","iopub.status.idle":"2022-04-23T08:17:32.992545Z","shell.execute_reply.started":"2022-04-23T08:17:32.983598Z","shell.execute_reply":"2022-04-23T08:17:32.991325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplot_feature_importance(xgb.feature_importances_,X_train.columns,'XGBOOST ', max_features = 15)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:17:33.183374Z","iopub.execute_input":"2022-04-23T08:17:33.183715Z","iopub.status.idle":"2022-04-23T08:17:33.878751Z","shell.execute_reply.started":"2022-04-23T08:17:33.183682Z","shell.execute_reply":"2022-04-23T08:17:33.878081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Make Submission File</h3>","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nxgb_preds = xgb.predict_proba(test_merge_data[features])[:, 1]\nxgb_preds","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:17:33.880265Z","iopub.execute_input":"2022-04-23T08:17:33.880976Z","iopub.status.idle":"2022-04-23T08:17:34.031416Z","shell.execute_reply.started":"2022-04-23T08:17:33.880939Z","shell.execute_reply":"2022-04-23T08:17:34.030636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_preds=lgb.predict_proba(test_merge_data[features])[:, 1]\nlgb_preds","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:17:34.035568Z","iopub.execute_input":"2022-04-23T08:17:34.037495Z","iopub.status.idle":"2022-04-23T08:17:34.223115Z","shell.execute_reply.started":"2022-04-23T08:17:34.037442Z","shell.execute_reply":"2022-04-23T08:17:34.222425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ENsemble from DNN model\nlater will be updated..\n\nrough version.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import KFold, GroupKFold\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import GlobalMaxPooling1D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.layers import Concatenate, LSTM, GRU\nfrom tensorflow.keras.layers import Bidirectional, Multiply\nnp.random.seed(2022)\ntf.random.set_seed(2022)\ntrain = pd.read_csv('../input/tabular-playground-series-apr-2022/train.csv')\nt_lbls = pd.read_csv('../input/tabular-playground-series-apr-2022/train_labels.csv')\ntest = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')\ns6 = pd.read_csv('../input/tabular-playground-series-apr-2022/sample_submission.csv')\ns7=pd.read_csv('../input/sub7-dnn/submission7.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:17:34.224462Z","iopub.execute_input":"2022-04-23T08:17:34.224849Z","iopub.status.idle":"2022-04-23T08:17:47.123812Z","shell.execute_reply.started":"2022-04-23T08:17:34.224817Z","shell.execute_reply":"2022-04-23T08:17:47.122924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = train.columns.tolist()[3:]\ndef prep(df):\n    for feature in features:\n        df[feature + '_lag1'] = df.groupby('sequence')[feature].shift(1)\n        df.fillna(0, inplace=True)\n        df[feature + '_diff1'] = df[feature] - df[feature + '_lag1']    \n\nprep(train)\nprep(test)\n\nfeatures = train.columns.tolist()[3:]\nsc = StandardScaler()\ntrain[features] = sc.fit_transform(train[features])\ntest[features] = sc.transform(test[features])\n\ngroups = train[\"sequence\"]\nlabels = t_lbls[\"state\"]\n\ntrain = train.drop([\"sequence\", \"subject\", \"step\"], axis=1).values\ntrain = train.reshape(-1, 60, train.shape[-1])\n\ntest = test.drop([\"sequence\", \"subject\", \"step\"], axis=1).values\ntest = test.reshape(-1, 60, test.shape[-1])","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:17:47.126499Z","iopub.execute_input":"2022-04-23T08:17:47.127034Z","iopub.status.idle":"2022-04-23T08:18:00.99978Z","shell.execute_reply.started":"2022-04-23T08:17:47.126986Z","shell.execute_reply":"2022-04-23T08:18:00.998009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    BATCH_SIZE = tpu_strategy.num_replicas_in_sync * 64\n    print(\"Running on TPU:\", tpu.master())\n    print(f\"Batch Size: {BATCH_SIZE}\")\n    \nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\n    BATCH_SIZE = 256\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    print(f\"Batch Size: {BATCH_SIZE}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:18:01.002439Z","iopub.execute_input":"2022-04-23T08:18:01.002767Z","iopub.status.idle":"2022-04-23T08:18:07.260931Z","shell.execute_reply.started":"2022-04-23T08:18:01.002723Z","shell.execute_reply":"2022-04-23T08:18:07.260011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>DNN model1","metadata":{}},{"cell_type":"code","source":"def dnn_model():\n    \n    x_input = Input(shape=(train.shape[-2:]))\n    \n    x0 = Bidirectional(LSTM(units=256, return_sequences=True))(x_input)\n    x1 = Bidirectional(LSTM(units=512, return_sequences=True))(x0)\n    x2 = Bidirectional(LSTM(units=256, return_sequences=True))(x1)\n    z1 = Bidirectional(GRU(units=256, return_sequences=True))(x1)\n    \n    c = Concatenate(axis=2)([x2, z1])\n    \n    x3 = Bidirectional(LSTM(units=256, return_sequences=True))(c)\n    \n    x4 = GlobalMaxPooling1D()(x3)\n    x5 = Dense(units=128, activation='selu')(x4)\n    x_output = Dense(1, activation='sigmoid')(x5)\n\n    model = Model(inputs=x_input, outputs=x_output, name='lstm_model')\n    \n    return model\n\nmodel = dnn_model()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:18:07.262194Z","iopub.execute_input":"2022-04-23T08:18:07.263217Z","iopub.status.idle":"2022-04-23T08:18:10.312886Z","shell.execute_reply.started":"2022-04-23T08:18:07.263169Z","shell.execute_reply":"2022-04-23T08:18:10.312029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>DNN model2","metadata":{}},{"cell_type":"code","source":"def dnn_model2():\n    \n    x_input = Input(shape=(train.shape[-2:]))\n    xi = Bidirectional(LSTM(units=128, return_sequences=True))(x_input)\n    x0 = Bidirectional(LSTM(units=256, return_sequences=True))(xi)\n    x00 = Bidirectional(LSTM(units=512, return_sequences=True))(x0)\n    x1 = Bidirectional(LSTM(units=256, return_sequences=True))(x00)\n    \n    x2 = Bidirectional(LSTM(units=256, return_sequences=True))(x1)\n    z1 = Bidirectional(GRU(units=256, return_sequences=True))(x1)\n    \n    c = Concatenate(axis=2)([x2, z1])\n    \n    x3 = Bidirectional(LSTM(units=256, return_sequences=True))(c)\n    \n    x4 = GlobalMaxPooling1D()(x3)\n    x5 = Dense(units=128, activation='selu')(x4)\n    x6 = Dense(units=64, activation='selu')(x5)\n    x_output = Dense(1, activation='sigmoid')(x6)\n\n    model = Model(inputs=x_input, outputs=x_output, name='lstm_model2')\n    \n    return model\n\nmodel2 = dnn_model2()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:18:10.314539Z","iopub.execute_input":"2022-04-23T08:18:10.314893Z","iopub.status.idle":"2022-04-23T08:18:14.347247Z","shell.execute_reply.started":"2022-04-23T08:18:10.314847Z","shell.execute_reply":"2022-04-23T08:18:14.34638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tpu_strategy.scope():\n    VERBOSE = 64\n#     BATCH_SIZE = 256\n    predictions, scores = [], []\n    k = GroupKFold(n_splits = 6)\n\n    for fold, (train_idx, val_idx) in enumerate(k.split(train, labels, groups.unique())):\n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n\n        X_train, X_val = train[train_idx], train[val_idx]\n        y_train, y_val = labels.iloc[train_idx].values, labels.iloc[val_idx].values\n\n        model = dnn_model()\n        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics='AUC')\n\n        lr = ReduceLROnPlateau(monitor=\"val_auc\", factor=0.6, \n                               patience=5, verbose=VERBOSE)\n\n        es = EarlyStopping(monitor=\"val_auc\", patience=10, \n                           verbose=VERBOSE, mode=\"max\", \n                           restore_best_weights=True)\n\n        save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n        chk_point = ModelCheckpoint(f'./TPS_model_1_2022_{fold+1}C.h5', options=save_locally, \n                                    monitor='val_auc', verbose=VERBOSE, \n                                    save_best_only=True, mode='max')\n\n        model.fit(X_train, y_train, \n                  validation_data=(X_val, y_val), \n                  epochs=25,\n                  verbose=VERBOSE,\n                  batch_size=BATCH_SIZE, \n                  callbacks=[lr, chk_point, es])\n\n        load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n        model = load_model(f'./TPS_model_1_2022_{fold+1}C.h5', options=load_locally)\n\n        y_pred = model.predict(X_val, batch_size=BATCH_SIZE).squeeze()\n        score = roc_auc_score(y_val, y_pred)\n        scores.append(score)\n        predictions.append(model.predict(test, batch_size=BATCH_SIZE).squeeze())\n        print(f\"model1,Fold-{fold+1} | OOF Score: {score}\")\n\nprint(f'Mean accuracy on {k.n_splits} folds - {np.mean(scores)}')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:19:29.776634Z","iopub.execute_input":"2022-04-23T08:19:29.777028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s6[\"state\"] = sum(predictions)/k.n_splits\ns6[\"state\"]","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:19:12.762522Z","iopub.status.idle":"2022-04-23T08:19:12.762885Z","shell.execute_reply.started":"2022-04-23T08:19:12.762698Z","shell.execute_reply":"2022-04-23T08:19:12.762716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tpu_strategy.scope():\n    VERBOSE = 64\n    # BATCH_SIZE = 256\n    predictions, scores = [], []\n    k = GroupKFold(n_splits = 6)\n\n    for fold, (train_idx, val_idx) in enumerate(k.split(train, labels, groups.unique())):\n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n\n        X_train, X_val = train[train_idx], train[val_idx]\n        y_train, y_val = labels.iloc[train_idx].values, labels.iloc[val_idx].values\n\n        model = dnn_model2()\n        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics='AUC')\n\n        lr = ReduceLROnPlateau(monitor=\"val_auc\", factor=0.6, \n                               patience=5, verbose=VERBOSE)\n\n        es = EarlyStopping(monitor=\"val_auc\", patience=10, \n                           verbose=VERBOSE, mode=\"max\", \n                           restore_best_weights=True)\n\n        save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n        chk_point = ModelCheckpoint(f'./TPS_model_2_2022{fold+1}C.h5', options=save_locally, \n                                    monitor='val_auc', verbose=VERBOSE, \n                                    save_best_only=True, mode='max')\n\n        model.fit(X_train, y_train, \n                  validation_data=(X_val, y_val), \n                  epochs=25,\n                  verbose=VERBOSE,\n                  batch_size=BATCH_SIZE, \n                  callbacks=[lr, chk_point, es])\n\n        load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n        model = load_model(f'./TPS_model_2_2022_{fold+1}C.h5', options=load_locally)\n\n        y_pred = model.predict(X_val, batch_size=BATCH_SIZE).squeeze()\n        score = roc_auc_score(y_val, y_pred)\n        scores.append(score)\n        predictions.append(model.predict(test, batch_size=BATCH_SIZE).squeeze())\n        print(f\"model2,Fold-{fold+1} | OOF Score: {score}\")\n\nprint(f'Mean accuracy on {k.n_splits} folds - {np.mean(scores)}')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:19:12.764047Z","iopub.status.idle":"2022-04-23T08:19:12.764385Z","shell.execute_reply.started":"2022-04-23T08:19:12.764195Z","shell.execute_reply":"2022-04-23T08:19:12.764209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s7[\"state\"] = sum(predictions)/k.n_splits\ns7[\"state\"]","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:19:12.766146Z","iopub.status.idle":"2022-04-23T08:19:12.766513Z","shell.execute_reply.started":"2022-04-23T08:19:12.766347Z","shell.execute_reply":"2022-04-23T08:19:12.766364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"blendsub=pd.read_csv(\"../input/sensors-deep-analysis-0-98/blend_sub31_exp.csv\")\npreds=(s7.state+s6.state)*0.35/2+xgb_preds*0.15+lgb_preds*0.15+blendsub.state*0.35\npreds","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:19:12.767404Z","iopub.status.idle":"2022-04-23T08:19:12.767708Z","shell.execute_reply.started":"2022-04-23T08:19:12.76755Z","shell.execute_reply":"2022-04-23T08:19:12.767565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"just replace the state columns with your predicts","metadata":{}},{"cell_type":"code","source":"\nsubmission['state'] = preds\nsubmission.to_csv('my_submission_ty.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T08:19:12.768654Z","iopub.status.idle":"2022-04-23T08:19:12.768956Z","shell.execute_reply.started":"2022-04-23T08:19:12.7688Z","shell.execute_reply":"2022-04-23T08:19:12.768814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n\n**still working on find more features ....\nwill be updated soon!**","metadata":{}},{"cell_type":"markdown","source":"  ","metadata":{}}]}