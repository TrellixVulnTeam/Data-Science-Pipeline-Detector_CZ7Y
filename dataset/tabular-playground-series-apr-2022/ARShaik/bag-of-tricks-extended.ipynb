{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<div style=\"background-color:rgba(0, 120, 255, 0.6);border-radius:5px;display:fill\">\n    <h1><center>Tabular Playground Series - April 2022</center></h1>\n</div>\nWe are given 12 sensor data and based on the sensor data we need to do binary classification. Time series classification is very well known concept in  industry applications. For example, in a typical stress test patient is asked to run on the treadmill with various sensors attached to him as shown below. This sensor information is used in Medical diagonasis. Various efforts are made by several other kagglers to solve this issue. \n\n<img src=\"https://interpretersontherun.files.wordpress.com/2018/04/homer-simpson-running-gif-downsized.gif?w=500&h=351&crop=1&zoom=2\" width=\"750\" align=\"center\">>\n\n\n\n<div class=\"alert alert-block alert-info\"> ðŸ“Œ \n    <b>In this public notebook, I will showcase how some ofthe tricks which might be helpful for this and future competitions </div>\n\n\n\n\n\n## <span style=\"color:crimson;\"> DONT FORGET TO UPVOTE IF YOU FIND IT USEFUL.......!!!!!! </span>","metadata":{}},{"cell_type":"markdown","source":"## <font color=\"#blue\">The structure of notebook.</font>\n<a id=\"1\"></a><h2></h2>\n### <a href='#1'>1. Introduction </a><br>\n### <a href='#2'>2. Importing Libraries and preprocessing</a><br>\n### <a href='#3'>3. Trick-1: Converting tabular data to image data</a><br>\n### <a href='#4'>4. Trick-2: Train and tune your model with one line code, Grand Master way</a><br>\n### <a href='#5'>5. Trick-3: QC your model with one line code</a><br>\n### <a href='#6'>6. Trick-4: Speedup your model with Intex</a><br>\n### <a href='#7'>7. Trick-5: Faster dataframe processing using moodle</a><br>\n### <a href='#8'>8. Trick-6: Fast aggregation of large data using Python datatable</a><br>\n### <a href='#9'>9. Trick-7: Get quick intuition with Lazypredict</a><br>\n### <a href='#10'>10. Trick-8: Automatic EDA with Dataprep </a><br>\n### <a href='#11'>11. References</a><br>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a><h2></h2>\n<div style=\"background-color:rgba(255, 69, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>Importing Libraries and preprocessing</center></h1>\n</div>","metadata":{}},{"cell_type":"code","source":"!pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\nfrom fastbook import *\nfrom fastai.vision.all import *","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-26T17:17:11.180853Z","iopub.execute_input":"2022-04-26T17:17:11.181223Z","iopub.status.idle":"2022-04-26T17:17:22.210141Z","shell.execute_reply.started":"2022-04-26T17:17:11.181113Z","shell.execute_reply":"2022-04-26T17:17:22.20901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Basic packages\nimport torch\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport re\nsns.set_style('whitegrid')\nfrom sklearn.preprocessing import MinMaxScaler\nplt.rc('image', cmap='Greys')\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option(\"display.precision\", 8)","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-26T17:17:31.67937Z","iopub.execute_input":"2022-04-26T17:17:31.679693Z","iopub.status.idle":"2022-04-26T17:17:31.743959Z","shell.execute_reply.started":"2022-04-26T17:17:31.679661Z","shell.execute_reply":"2022-04-26T17:17:31.742893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read sensor information with more than 600 features\n# ref: https://www.kaggle.com/code/lucasmorin/feature-engineering-aggregation-functions\ntrain = pd.read_parquet('../input/fedata-tpsapril/train.parquet')\ntrain.head(2)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-26T17:17:34.456101Z","iopub.execute_input":"2022-04-26T17:17:34.456411Z","iopub.status.idle":"2022-04-26T17:17:37.300568Z","shell.execute_reply.started":"2022-04-26T17:17:34.456378Z","shell.execute_reply":"2022-04-26T17:17:37.299418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets investigate the train data\ntrain.reset_index(inplace=True)\ntrain.describe()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-26T17:17:37.302173Z","iopub.execute_input":"2022-04-26T17:17:37.302464Z","iopub.status.idle":"2022-04-26T17:17:39.882223Z","shell.execute_reply.started":"2022-04-26T17:17:37.302432Z","shell.execute_reply":"2022-04-26T17:17:39.881065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# is there any missing data\ntrain.isnull().sum()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-26T17:17:39.883553Z","iopub.execute_input":"2022-04-26T17:17:39.884228Z","iopub.status.idle":"2022-04-26T17:17:39.92832Z","shell.execute_reply.started":"2022-04-26T17:17:39.884189Z","shell.execute_reply":"2022-04-26T17:17:39.927456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merging the train data with the target information\nlabels = pd.read_csv(\"../input/tabular-playground-series-apr-2022/train_labels.csv\")\ntrain_df_lable = pd.merge(train, labels, on=\"sequence\")\ntrain_df_lable.tail(2)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-26T17:17:39.929925Z","iopub.execute_input":"2022-04-26T17:17:39.930235Z","iopub.status.idle":"2022-04-26T17:17:45.062502Z","shell.execute_reply.started":"2022-04-26T17:17:39.930201Z","shell.execute_reply":"2022-04-26T17:17:45.061588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# other kagglers suggested, groupkfold based on the subject is good idea. Lets include that\n#including subject column to the dataframe\ntrain_df = pd.read_csv('../input/tabular-playground-series-apr-2022/train.csv')\ntrain_df_subject=train_df[['sequence','subject']].copy()\ntrain_df_subject.drop_duplicates(inplace=True)\ntrain_df_lable=pd.merge(train_df_lable, train_df_subject, on=\"sequence\")\ntrain_df_lable.tail(2)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-26T17:17:45.063883Z","iopub.execute_input":"2022-04-26T17:17:45.064118Z","iopub.status.idle":"2022-04-26T17:17:53.129686Z","shell.execute_reply.started":"2022-04-26T17:17:45.064089Z","shell.execute_reply":"2022-04-26T17:17:53.128765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loading test data\n# ref: https://www.kaggle.com/code/lucasmorin/feature-engineering-aggregation-functions\ntest = pd.read_parquet('../input/fedata-tpsapril/test.parquet')\ntest.reset_index(inplace=True)\ntest.head(2)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-26T17:17:53.131399Z","iopub.execute_input":"2022-04-26T17:17:53.131641Z","iopub.status.idle":"2022-04-26T17:17:54.323864Z","shell.execute_reply.started":"2022-04-26T17:17:53.131613Z","shell.execute_reply":"2022-04-26T17:17:54.322858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#adding subject column to the test dataframe\ntest_df = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')\ntest_df_subject=test_df[['sequence','subject']].copy()\ntest_df_subject.drop_duplicates(inplace=True)\ntest_df2=pd.merge(test, test_df_subject, on=\"sequence\")\n# test_with_lable=pd.merge(test_with_lable, test_df_subject, on=\"sequence\")\ntest_df2.shape","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-26T17:17:54.325302Z","iopub.execute_input":"2022-04-26T17:17:54.325775Z","iopub.status.idle":"2022-04-26T17:17:59.78446Z","shell.execute_reply.started":"2022-04-26T17:17:54.325728Z","shell.execute_reply":"2022-04-26T17:17:59.783212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a><h2></h2>\n<div style=\"background-color:rgba(255, 69, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>Converting tabular data to image data</center></h1>\n</div>\n<br>\n@ambrosm was able to minimize the number of feature to less than fifty and able to reach 94% accuracy. Some other kagglers used around 600 features to reach 97.7% accuracy. I also want to take advange of well established image classfication techniques. Hence, I was motivated to find a way to handle handle large number of features and convert them to images.\nThe stragegy here is to convert the each row into different image hence typical image classfication techniques can be used. There are total 692 columns in train dataset. We will drop one column so we can convert each row into an image of size 23x30 with sequence as ID of that image.\n","metadata":{}},{"cell_type":"code","source":"train_df_lable.set_index(\"sequence\", inplace = True)\ntrain_df_lable.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:18:41.206231Z","iopub.execute_input":"2022-04-26T17:18:41.206579Z","iopub.status.idle":"2022-04-26T17:18:41.217772Z","shell.execute_reply.started":"2022-04-26T17:18:41.206545Z","shell.execute_reply":"2022-04-26T17:18:41.216826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets drop one column to make it 690 column+ target column\ntrain_df2=train_df_lable.drop(columns=['sensor_00_mean'])\n# fill in the Nan values\ntrain_df2=train_df2.interpolate()\ntrain_df2.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:18:41.395252Z","iopub.execute_input":"2022-04-26T17:18:41.395695Z","iopub.status.idle":"2022-04-26T17:18:41.856845Z","shell.execute_reply.started":"2022-04-26T17:18:41.395663Z","shell.execute_reply":"2022-04-26T17:18:41.855811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The 690 columns are converted into image of size 23 x 30\nimg_rows = 23\nimg_cols = 30\ntrain_df_nolable=train_df2.drop(columns=['state'])","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:18:41.858548Z","iopub.execute_input":"2022-04-26T17:18:41.858763Z","iopub.status.idle":"2022-04-26T17:18:41.906765Z","shell.execute_reply.started":"2022-04-26T17:18:41.858736Z","shell.execute_reply":"2022-04-26T17:18:41.905694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### There are total 692 columns. Similar to training we will drop one column so we can convert each row into an image of size 23x30 with sequence as ID of that image.","metadata":{}},{"cell_type":"code","source":"test_with_nolable=test_df2.drop(columns=['sensor_00_mean'])\ntest_with_nolable.set_index(\"sequence\", inplace = True)\ntest_with_nolable.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:18:41.909685Z","iopub.execute_input":"2022-04-26T17:18:41.910694Z","iopub.status.idle":"2022-04-26T17:18:42.319595Z","shell.execute_reply.started":"2022-04-26T17:18:41.910644Z","shell.execute_reply":"2022-04-26T17:18:42.318732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_with_nolable.shape, train_df_nolable.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:18:42.321173Z","iopub.execute_input":"2022-04-26T17:18:42.321981Z","iopub.status.idle":"2022-04-26T17:18:42.328783Z","shell.execute_reply.started":"2022-04-26T17:18:42.32193Z","shell.execute_reply":"2022-04-26T17:18:42.327807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Some preprocessing before converting to image","metadata":{}},{"cell_type":"code","source":"#scaling the train data\ncols = train_df_nolable.columns \ntrain_scaled=pd.DataFrame(columns=cols)\n\nfor i in range(len(cols)):\n    df = train_df_nolable[cols[i]]\n    df_scaled = (df-df.min())/(df.max()-df.min()+1E-8)*(256**3)\n    train_scaled[cols[i]] = df_scaled","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:18:42.381081Z","iopub.execute_input":"2022-04-26T17:18:42.381957Z","iopub.status.idle":"2022-04-26T17:20:12.083127Z","shell.execute_reply.started":"2022-04-26T17:18:42.381908Z","shell.execute_reply":"2022-04-26T17:20:12.082092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#scaling the test data also\ncols = test_with_nolable.columns \ntest_scaled=pd.DataFrame(columns=cols)\n\nfor i in range(len(cols)):\n    df = test_with_nolable[cols[i]]\n    df_scaled = (df-df.min())/(df.max()-df.min()+1E-8)\n    test_scaled[cols[i]] = df_scaled","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:20:12.084831Z","iopub.execute_input":"2022-04-26T17:20:12.085056Z","iopub.status.idle":"2022-04-26T17:20:49.645854Z","shell.execute_reply.started":"2022-04-26T17:20:12.085028Z","shell.execute_reply":"2022-04-26T17:20:49.644989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nThe RGB color model is an additive color model in which the red, green, and blue primary colors of light are added together in various ways to reproduce a broad array of colors. The name of the model comes from the initials of the three additive primary colors, red, green, and blue.\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/89/RGB_colors.gif\" width=\"750\" align=\"center\">>","metadata":{}},{"cell_type":"code","source":"# prepare a function to visualize the images\ndef plot_sensor(images, labels, indexes):\n    num_row = 5\n    num_col = 5\n    fig, axes = plt.subplots(num_row, num_col, constrained_layout=True,  sharex=True, sharey=True, figsize=(3*num_col,2*num_row))\n    for i in range(len(images)):\n        ax = axes[i//num_col, i%num_col]\n        image = images[i].reshape(img_rows, img_cols, 1)\n        ax.imshow(image, cmap='Spectral')\n        ax.set_title(f'{labels[i]}\\n{indexes[i]}')\n        plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.1, hspace=0.5)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:20:49.646952Z","iopub.execute_input":"2022-04-26T17:20:49.647564Z","iopub.status.idle":"2022-04-26T17:20:49.654327Z","shell.execute_reply.started":"2022-04-26T17:20:49.647527Z","shell.execute_reply":"2022-04-26T17:20:49.653244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = train_df2.state\nnum_classes = target.nunique()\nFEATURES = [col for col in train_df2.columns if col not in ['state']]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:20:49.656829Z","iopub.execute_input":"2022-04-26T17:20:49.657422Z","iopub.status.idle":"2022-04-26T17:20:49.676439Z","shell.execute_reply.started":"2022-04-26T17:20:49.657371Z","shell.execute_reply":"2022-04-26T17:20:49.675403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting first 25 sequences\nimages = train_df2[:25]  \nlabels = images.state.values\nindexes = images.index.values\nimages = images[FEATURES].values.reshape(images.shape[0], img_rows, img_cols, 1).astype('float32')\nplot_sensor(images, labels, indexes)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:20:49.678231Z","iopub.execute_input":"2022-04-26T17:20:49.679507Z","iopub.status.idle":"2022-04-26T17:20:52.052779Z","shell.execute_reply.started":"2022-04-26T17:20:49.679447Z","shell.execute_reply":"2022-04-26T17:20:52.051855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting the 690 columns to 23x30 columns and visualize one of the sample \ntrain_df2['state'] = train_df2['state'].astype('bool') \narr = tensor((train_scaled.iloc[1,0:690]).astype(np.float))\nLong1 = torch.reshape(arr,(23,30))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:20:52.054467Z","iopub.execute_input":"2022-04-26T17:20:52.05503Z","iopub.status.idle":"2022-04-26T17:20:52.102282Z","shell.execute_reply.started":"2022-04-26T17:20:52.054981Z","shell.execute_reply":"2022-04-26T17:20:52.1016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### identifying the blue colors in RGB color scheme","metadata":{}},{"cell_type":"code","source":"Blue_clr = (Long1/65536).int()\ndef bluepic(img):\n  df = pd.DataFrame(img)\n  return df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Blues')\nbluepic(Blue_clr)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:20:52.104491Z","iopub.execute_input":"2022-04-26T17:20:52.105465Z","iopub.status.idle":"2022-04-26T17:20:52.301708Z","shell.execute_reply.started":"2022-04-26T17:20:52.105411Z","shell.execute_reply":"2022-04-26T17:20:52.301009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Green_clr = ((Long1 - Blue_clr * 65536)/256).int()\ndef greenpic(img):\n  df = pd.DataFrame(img)\n  return df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greens')\ngreenpic(Green_clr)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:20:52.302907Z","iopub.execute_input":"2022-04-26T17:20:52.30328Z","iopub.status.idle":"2022-04-26T17:20:52.422655Z","shell.execute_reply.started":"2022-04-26T17:20:52.303237Z","shell.execute_reply":"2022-04-26T17:20:52.421807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#R = LONG - B * 65536 - G * 256\nRed_clr = (Long1 - Blue_clr *65536 - Green_clr * 256).int()\ndef redpic(img):\n  df = pd.DataFrame(img)\n  return df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Reds')\nredpic(Red_clr)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:20:52.424167Z","iopub.execute_input":"2022-04-26T17:20:52.425033Z","iopub.status.idle":"2022-04-26T17:20:52.552916Z","shell.execute_reply.started":"2022-04-26T17:20:52.424987Z","shell.execute_reply":"2022-04-26T17:20:52.551847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lets visualize the whole RGB image","metadata":{}},{"cell_type":"code","source":"def bigpic(img):\n  df = pd.DataFrame(img)\n  return df.style.set_properties(**{'font-size':'6pt'}).background_gradient('gist_rainbow')\nbigpic(Long1.int())","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:20:52.556133Z","iopub.execute_input":"2022-04-26T17:20:52.556768Z","iopub.status.idle":"2022-04-26T17:20:52.677184Z","shell.execute_reply.started":"2022-04-26T17:20:52.556708Z","shell.execute_reply":"2022-04-26T17:20:52.67627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### bit of post processing","metadata":{}},{"cell_type":"code","source":"pic = torch.stack((Red_clr.flatten(),Green_clr.flatten(),Blue_clr.flatten()))\npic.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:20:52.678691Z","iopub.execute_input":"2022-04-26T17:20:52.679647Z","iopub.status.idle":"2022-04-26T17:20:52.69396Z","shell.execute_reply.started":"2022-04-26T17:20:52.679598Z","shell.execute_reply":"2022-04-26T17:20:52.692982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pic = torch.reshape(pic,(3,23,30))\npic = torch.permute(pic, (1, 2, 0))\nimg = Image.fromarray(pic.numpy(), 'RGB')\nshow_image(img)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:20:52.69563Z","iopub.execute_input":"2022-04-26T17:20:52.695938Z","iopub.status.idle":"2022-04-26T17:20:52.83578Z","shell.execute_reply.started":"2022-04-26T17:20:52.6959Z","shell.execute_reply":"2022-04-26T17:20:52.834722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finally we managed to convert a row into an image with RGB color format. Lets visualize some more samples","metadata":{}},{"cell_type":"code","source":"target_values = train_df2['state'].unique()\nsamples_df = pd.DataFrame(columns=train_df2.columns)\n\nfor i in range(len(target_values)):\n    df_filter = train_df2[train_df2['state']==target_values[i]][0:10]\n    samples_df = samples_df.append(df_filter)    \nsamples_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:20:52.841636Z","iopub.execute_input":"2022-04-26T17:20:52.844659Z","iopub.status.idle":"2022-04-26T17:20:53.020161Z","shell.execute_reply.started":"2022-04-26T17:20:52.844585Z","shell.execute_reply":"2022-04-26T17:20:53.019206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_pics(train_scaled):\n    \n  pic_rgb=[]\n  for i in range(len(train_scaled)):\n    #cast into float tensor\n    arr = tensor((train_scaled.iloc[i,0:690]).astype(np.float))\n\n    #reshape into a picture 23x30\n    Long1 = torch.reshape(arr,(23,30))\n\n    #compute RGB channells\n    Blue1 = (Long1/65536).int()\n    Green1 = ((Long1 - Blue1 * 65536)/256).int()\n    Red1 = (Long1 - Blue1 *65536 - Green1 * 256).int()\n\n    #create yeansor image\n    pic = torch.stack((Red1.flatten(),Green1.flatten(),Blue1.flatten()))\n    pic = torch.reshape(pic,(3,23,30))\n    pic = torch.permute(pic, (1, 2, 0))\n\n    pic_rgb.append(pic)\n\n  pic_rgb_t = torch.stack(pic_rgb)\n\n  return pic_rgb_t","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:20:53.021857Z","iopub.execute_input":"2022-04-26T17:20:53.022173Z","iopub.status.idle":"2022-04-26T17:20:53.03132Z","shell.execute_reply.started":"2022-04-26T17:20:53.022133Z","shell.execute_reply":"2022-04-26T17:20:53.030348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_pics = create_pics(samples_df)\nfig, ax = plt.subplots(2,2, figsize=(20, 20))\n\nfor c in range(2):\n    for r in range(2):\n        image = sample_pics[c+r]\n        ax[r, c].imshow(image)\n        ax[r, c].set_title(target_values[c])\n        ax[r, c].axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:20:53.033121Z","iopub.execute_input":"2022-04-26T17:20:53.033749Z","iopub.status.idle":"2022-04-26T17:20:53.5932Z","shell.execute_reply.started":"2022-04-26T17:20:53.033697Z","shell.execute_reply":"2022-04-26T17:20:53.592454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A new datset can be creted which can act as an input to the any image classfication algorithums. Uncomment this section if you want to save. I have commented to save the memory","metadata":{}},{"cell_type":"code","source":"# new_pics_train = create_pics(train_scaled)\n# #new_pics_train.shape\n# train_y_labels = train_df2['state']\n# new_pics_test = create_pics(test_scaled)\n# torch.save(new_pics_train,'./train_Images')\n# torch.save(new_pics_test,'./test_Images')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:20:53.595349Z","iopub.execute_input":"2022-04-26T17:20:53.59567Z","iopub.status.idle":"2022-04-26T17:20:53.599931Z","shell.execute_reply.started":"2022-04-26T17:20:53.595627Z","shell.execute_reply":"2022-04-26T17:20:53.599089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython import get_ipython\nget_ipython().magic('reset -sf') ","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:20:53.601277Z","iopub.execute_input":"2022-04-26T17:20:53.601603Z","iopub.status.idle":"2022-04-26T17:20:53.81046Z","shell.execute_reply.started":"2022-04-26T17:20:53.601564Z","shell.execute_reply":"2022-04-26T17:20:53.809523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a href='#1'>Back to top </a><br>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a><h2></h2>\n<div style=\"background-color:rgba(255, 69, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>Trick-2: Train and tune your model with one line code</center></h1>\n</div>\nThis library is developed by <b> Grand Master Absheik Thakur </b>, which he used in several competitions. See the reference list for full details <b>","metadata":{}},{"cell_type":"code","source":"# preprocessing\n!pip install autoxgb","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-26T17:20:53.812145Z","iopub.execute_input":"2022-04-26T17:20:53.812549Z","iopub.status.idle":"2022-04-26T17:22:02.113944Z","shell.execute_reply.started":"2022-04-26T17:20:53.812502Z","shell.execute_reply":"2022-04-26T17:22:02.112412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!autoxgb train \\\n--train_filename ../input/tabular-playground-series-apr-2022/train.csv \\\n--test_filename ../input/tabular-playground-series-apr-2022/test.csv \\\n--idx Id \\\n--task regression \\\n--targets state \\\n--time_limit 3600 \\\n--output petf \\\n--use_gpu","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:22:02.116671Z","iopub.execute_input":"2022-04-26T17:22:02.117032Z","iopub.status.idle":"2022-04-26T17:22:17.78367Z","shell.execute_reply.started":"2022-04-26T17:22:02.11699Z","shell.execute_reply":"2022-04-26T17:22:17.782344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Points to note while using this trick: <br>\n1) make sure all the feature engineering is completed for train and test dataset and store them in respective csv files. For illustration purpose I have ignored this. If you are planning to use this, please include all the features<br>\n2) The predictions will be saved in the working directory as test_predictions.csv. <br>","metadata":{}},{"cell_type":"markdown","source":"### <a href='#1'>Back to top </a><br>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a><h2></h2>\n<div style=\"background-color:rgba(255, 69, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>Trick-3: QC your model with one line code </center></h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"Here we will utilize DeepChecks to validate your machine learning models and data, such as verifying your dataâ€™s integrity, inspecting its distributions, validating data splits, evaluating the model. Interestingly it will take only seven lines of code to do that.<br>\n<br>\n<br>\n<img src=\"https://docs.deepchecks.com/stable/_images/pipeline_when_to_validate.svg\" width=\"750\" align=\"center\">>\n<br>\nHere I will use one of the public notebook and QC the model as an example. All the credits for the model goes to the original author. Consider upvoting them.\nhttps://www.kaggle.com/code/cv13j0/tps-apr-2022-xgboost-model","metadata":{}},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport re\nsns.set_style('whitegrid')\nfrom sklearn.preprocessing import MinMaxScaler\nplt.rc('image', cmap='Greys')\nimport warnings\n\n# Notebook Configuration...\n\n# Amount of data we want to load into the Model...\nDATA_ROWS = None\n# Dataframe, the amount of rows and cols to visualize...\nNROWS = 100\nNCOLS = 15\n# Main data location path...\nBASE_PATH = '...'\npd.options.display.float_format = '{:,.2f}'.format\npd.set_option('display.max_columns', NCOLS) \npd.set_option('display.max_rows', NROWS)\n# Load the CSVs into a pandas dataframe for future data manipulation.\ntrn_data = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2022/train.csv')\ntrn_label_data = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2022/train_labels.csv')\ntst_data = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2022/test.csv')\n\nsub = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2022/sample_submission.csv')\n\ntrn_summary = trn_data[['sequence', 'subject', 'step']].groupby(['sequence', 'subject']).count().reset_index()\ntrn_summary[trn_summary['subject'] == 66].shape\nsummary_by_subject = trn_summary[['sequence', 'subject']].groupby(['subject']).count().reset_index()\ntrn_unique_subjects = set(list(trn_data['subject'].unique()))\ntst_unique_subjects = set(list(tst_data['subject'].unique()))\noverlap_subjets = trn_unique_subjects.intersection(tst_unique_subjects)\nprint('Repeated Subjects in Test Dataset:', len(overlap_subjets))\nfrom scipy.stats import kurtosis\ndef kurtosis_func(series):\n    '''\n    Describe something...\n    '''\n    return kurtosis(series)\n\ndef q01(series):\n    return np.quantile(series, 0.01)\n\ndef q05(series):\n    return np.quantile(series, 0.05)\n\ndef q95(series):\n    return np.quantile(series, 0.95)\n\ndef q99(series):\n    return np.quantile(series, 0.99)\n\ndef aggregated_features(df, aggregation_cols = ['sequence'], prefix = ''):\n    agg_strategy = {'sensor_00': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_01': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_02': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_03': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_04': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_05': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_06': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_07': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_08': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_09': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_10': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_11': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_12': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                   }\n    group = df.groupby(aggregation_cols).aggregate(agg_strategy)\n    group.columns = ['_'.join(col).strip() for col in group.columns]\n    group.columns = [str(prefix) + str(col) for col in group.columns]\n    group.reset_index(inplace = True)\n    \n    temp = (df.groupby(aggregation_cols).size().reset_index(name = str(prefix) + 'size'))\n    group = pd.merge(temp, group, how = 'left', on = aggregation_cols,)\n    return group\ntrn_merge_data = aggregated_features(trn_data, aggregation_cols = ['sequence', 'subject'])\ntst_merge_data = aggregated_features(tst_data, aggregation_cols = ['sequence', 'subject'])\ntrn_subjects_merge_data = aggregated_features(trn_data, aggregation_cols = ['subject'], prefix = 'subject_')\ntst_subjects_merge_data = aggregated_features(tst_data, aggregation_cols = ['subject'], prefix = 'subject_')\ntrn_data['sensor_00_lag_01'] = trn_data['sensor_00'].shift(1)\ntrn_data['sensor_00_lag_10'] = trn_data['sensor_00'].shift(10)\ntrn_merge_data = trn_merge_data.merge(trn_label_data, how = 'left', on = 'sequence')\ntrn_merge_data = trn_merge_data.merge(trn_subjects_merge_data, how = 'left', on = 'subject')\ntst_merge_data = tst_merge_data.merge(tst_subjects_merge_data, how = 'left', on = 'subject')\nignore = ['sequence', 'state', 'subject']\nfeatures = [feat for feat in trn_merge_data.columns if feat not in ignore]\ntarget_feature = 'state'\nfrom sklearn.model_selection import train_test_split\ntest_size_pct = 0.10\nX_train, X_valid, y_train, y_valid = train_test_split(trn_merge_data[features], trn_merge_data[target_feature], test_size = test_size_pct, random_state = 42)\n","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-26T17:22:17.786367Z","iopub.execute_input":"2022-04-26T17:22:17.78666Z","iopub.status.idle":"2022-04-26T17:31:57.32581Z","shell.execute_reply.started":"2022-04-26T17:22:17.786622Z","shell.execute_reply":"2022-04-26T17:31:57.324801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost  import XGBClassifier\nparams = {'n_estimators': 40,  # I changed it from 4096 to 40 \n          'max_depth': 7,\n          'learning_rate': 0.15,\n          'subsample': 0.95,\n          'colsample_bytree': 0.60,\n          'reg_lambda': 1.50,\n          'reg_alpha': 6.10,\n          'gamma': 1.40,\n          'random_state': 69,\n          'objective': 'binary:logistic',\n         }\nxgb = XGBClassifier(**params)\nxgb.fit(X_train, y_train, eval_set = [(X_valid, y_valid)], eval_metric = ['auc'], early_stopping_rounds = 128, verbose = 50)\nfrom sklearn.metrics import roc_auc_score\npreds = xgb.predict_proba(X_valid)[:, 1]\nscore = roc_auc_score(y_valid, preds)\nfrom sklearn.metrics import roc_auc_score\npreds = xgb.predict_proba(tst_merge_data[features])[:, 1]\n# end of public notebook","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-26T17:31:57.327131Z","iopub.execute_input":"2022-04-26T17:31:57.327743Z","iopub.status.idle":"2022-04-26T17:32:13.373885Z","shell.execute_reply.started":"2022-04-26T17:31:57.327704Z","shell.execute_reply":"2022-04-26T17:32:13.373121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing to prepare train and test datasets\n!pip install deepchecks --user\nfrom deepchecks.tabular.suites import full_suite\nfrom deepchecks.tabular.datasets.classification import iris\nfrom deepchecks.tabular import Dataset\n\nfeatures.append(\"sequence\")\ntrain=trn_merge_data[features]\nt_lbls=trn_merge_data[['sequence','state']]\ntrain_with_lable=pd.merge(train, t_lbls, on=\"sequence\")\nlabel_col='state'\ntrain_dataset = Dataset(train_with_lable, label=label_col, cat_features=[])\ntest_with_lable = tst_merge_data[features]\nsub['state'] = preds\ntest_with_lable=pd.merge(test_with_lable, sub, on=\"sequence\")\ntest_dataset = Dataset(test_with_lable, label=label_col, cat_features=[])","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-26T17:32:13.375113Z","iopub.execute_input":"2022-04-26T17:32:13.376656Z","iopub.status.idle":"2022-04-26T17:32:45.027185Z","shell.execute_reply.started":"2022-04-26T17:32:13.376607Z","shell.execute_reply":"2022-04-26T17:32:45.026084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now the QC the model with two lines","metadata":{}},{"cell_type":"code","source":"suite = full_suite()\nsuite.run(train_dataset=train_dataset, test_dataset=test_dataset, model=xgb)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:32:45.029987Z","iopub.execute_input":"2022-04-26T17:32:45.030254Z","iopub.status.idle":"2022-04-26T17:36:10.96124Z","shell.execute_reply.started":"2022-04-26T17:32:45.030213Z","shell.execute_reply":"2022-04-26T17:36:10.960571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython import get_ipython\nget_ipython().magic('reset -sf') ","metadata":{"execution":{"iopub.status.busy":"2022-04-26T17:36:10.962524Z","iopub.execute_input":"2022-04-26T17:36:10.963082Z","iopub.status.idle":"2022-04-26T17:36:11.169058Z","shell.execute_reply.started":"2022-04-26T17:36:10.96304Z","shell.execute_reply":"2022-04-26T17:36:11.167714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a href='#1'>Back to top </a><br>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"6\"></a><h2></h2>\n<div style=\"background-color:rgba(255, 69, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>Trick-4: Speedup your model with scikit-learn-intelex </center></h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"This is an nice trick to spped-up your model without changing your Sklearn based model. All we need to do is add two lines codes to import the library and patch it. I would like to thank **Devlikamov Vlad** for introducing this library. See an example <br>\n\n<img src=\"https://miro.medium.com/max/1400/1*C-PtCPom21g8zDIiDZsnNQ.png\" width=\"750\" align=\"center\">>\n\n<br> \n<b> See the references for full informaiton on this. Dont forget to include the following two lines in your code. <br>","metadata":{}},{"cell_type":"code","source":"from sklearnex import patch_sklearn\npatch_sklearn()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a href='#1'>Back to top </a><br>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"7\"></a><h2></h2>\n<div style=\"background-color:rgba(255, 69, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>Trick-5: Faster dataframe processing using modin </center></h1>\n</div>\n<br>\nI would like to thank Grand Master @cpmpml for introducing this package to me. Modin accelerates Pandas queries by 4x on an 8-core machine, only requiring users to change a single line of code in their notebooks. The system has been designed for existing Pandas users who would like their programs to run faster and scale better without significant code changes. <br>\n\n<img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2750279%2Ff7ff51190490b2f662f160da6ac8b499%2F0.png?generation=1568037893769516&alt=media\" width=\"750\" align=\"center\">>\n\n<br>\n<b>\nJust include the following three lines to imporve your speed.","metadata":{}},{"cell_type":"code","source":"!pip install modin\nimport modin.pandas as pd","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a href='#1'>Back to top </a><br>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"8\"></a><h2></h2>\n<div style=\"background-color:rgba(255, 69, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>Trick-6: Fast aggregation of large data with Python datatable </center></h1>\n</div>\n<br>\nAs dataset sizes are getting bigger, people are paying more attention to out-of-memory, multi-threaded data preprocessing tools to escape the performance limitations of Pandas. I would like to thank Grand Master @sudalairajkumar for introducing this package to me. This package is developed by H2O.AI team. This package might not show great improvement in this competitation but its perform extremly well on large datasets. Its the best package for handling any large tabular datasets. It is can be used for fast aggregation of large datasets, low latency add/update/remove of columns, quicker ordered joins, and a fast file reader.\n<br>\n<img src=\"https://miro.medium.com/max/446/0*w7dsjAY9CKNY7owL.png\" width=\"325\" align=\"center\">>\n\n\nGrand Master @sudalairajkumar has done extensive analysis for this package. It can be found below link <br>\nhttps://www.kaggle.com/code/sudalairajkumar/getting-started-with-python-datatable\n<br>\nThis package is not native to Kaggle and can be accessed using following code\n","metadata":{}},{"cell_type":"code","source":"!pip install https://s3.amazonaws.com/h2o-release/datatable/stable/datatable-0.8.0/datatable-0.8.0-cp36-cp36m-linux_x86_64.whl","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a href='#1'>Back to top </a><br>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"9\"></a><h2></h2>\n<div style=\"background-color:rgba(255, 69, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>Trick-7: Get quick intuition with Lazypredict </center></h1>\n</div>\n<br>\nI would like to thank @BEXGBoost for introducing this library to me. Lazypredict can be used to train almost all Sklearn models plus XGBoost and LightGBM in a single line of code. It only has two estimatorsâ€Š-â€Šone for regression and one for classification. Fitting either one on a dataset with a given target will evaluate more than 30 base models and generate a report with their rankings on several popular metrics.\n<br>\n<br>\n<img src=\"https://media-exp1.licdn.com/dms/image/C5612AQHZTuPnWQT6KQ/article-cover_image-shrink_600_2000/0/1520128748970?e=1654128000&v=beta&t=AIT6ChKW_1vVK4ueFhQn8q5T3Um6huaN0wZD6ouZDDI\" width=\"325\" align=\"center\">>\n\n\n<br>\nSee an example below. There are some other libraries such as Pycaret, Teapot which can do the similar tasks. Its worth checking them out.","metadata":{}},{"cell_type":"code","source":"# Load data and split\nX, y = load_boston(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Fit LazyRegressor\nreg = LazyRegressor(ignore_warnings=True, random_state=1121218, verbose=False)\nmodels, predictions = reg.fit(X_train, X_test, y_train, y_test)  # pass all sets","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a href='#1'>Back to top </a><br>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"10\"></a><h2></h2>\n<div style=\"background-color:rgba(255, 69, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>Trick-8: Automatic EDA with Dataprep  </center></h1>\n</div>\n<br>\nDataprep is one of the Automatic EDA library to get indication of the dataframe. Its one of the easy to use libary to get deep insight of the data.\n<br>\n<br>\n<img src=\"https://editor.analyticsvidhya.com/uploads/8465968747470733a2f2f6769746875622e636f6d2f7366752d64622f64617461707265702f7261772f646576656c6f702f6173736574732f6c6f676f2e706e67.png\" width=\"325\" align=\"center\">>\n\n\nOnce the dataframe is loaded with just one line code, we can get indepth analysis of the data. Its worth checking out.","metadata":{}},{"cell_type":"code","source":"!pip install dataprep\n# load the data\nplot(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color:crimson;\"> DONT FORGET TO UPVOTE IF YOU FIND IT USEFUL.......!!!!!! </span>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"11\"></a><h2></h2>\n<div style=\"background-color:rgba(255, 69, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>References</center></h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"1. https://www.kaggle.com/code/remekkinas/bacteria-image-conv2d-cv-grad-cam <br>\n2. https://www.kaggle.com/code/austinpowers/turn-it-into-a-toon-tps-feb-22 <br>\n3. https://www.kaggle.com/code/lucasmorin/feature-engineering-aggregation-functions <br>\n4. https://www.kaggle.com/code/abhishek/autoxgb-petfinder <br>\n5. https://www.kaggle.com/code/abhishek/autoxgb-nov-2021-tps <br>\n6. https://www.kaggle.com/code/lordozvlad/let-s-speed-up-your-kernels-using-sklearnex/notebook <br>\n7. https://www.kaggle.com/discussions/product-feedback/108155 <br>\n8. https://medium.com/intel-analytics-software/save-time-and-money-with-intel-extension-for-scikit-learn-33627425ae4\n9. https://www.kaggle.com/code/bextuychiev/7-coolest-packages-top-kagglers-are-using/notebook#3.-Lazypredict\n10. https://www.analyticsvidhya.com/blog/2021/05/dataprep-library-perform-eda-faster/","metadata":{}}]}