{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Gated Transformer Network for Sequence Classification**\nThomas Hopkins","metadata":{"papermill":{"duration":0.022337,"end_time":"2022-04-18T19:54:49.783683","exception":false,"start_time":"2022-04-18T19:54:49.761346","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## **Reference**\n\n[1] Liu, M., Ren, S., Ma, S., Jiao, J., Chen, Y., Wang, Z., & Song, W. (2021). Gated Transformer Networks for Multivariate Time Series Classification. arXiv preprint arXiv:2103.14438. ([link](https://arxiv.org/pdf/2103.14438.pdf))","metadata":{}},{"cell_type":"markdown","source":"## **Imports and Data**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.metrics import roc_auc_score\n\ntorch.manual_seed(32)\nnp.random.seed(0)\n\nBASE_DIR = '/kaggle/input/tabular-playground-series-apr-2022/'","metadata":{"papermill":{"duration":1.162928,"end_time":"2022-04-18T19:54:50.969052","exception":false,"start_time":"2022-04-18T19:54:49.806124","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-21T03:21:57.944653Z","iopub.execute_input":"2022-04-21T03:21:57.945182Z","iopub.status.idle":"2022-04-21T03:21:57.95584Z","shell.execute_reply.started":"2022-04-21T03:21:57.945125Z","shell.execute_reply":"2022-04-21T03:21:57.955044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sequences = pd.read_csv(BASE_DIR + \"train.csv\")\nval_sequences = train_sequences.iloc[:23340]\ntrain_sequences = train_sequences.iloc[23340:]\ntrain_labels = pd.read_csv(BASE_DIR + \"train_labels.csv\")\ntest_sequences = pd.read_csv(BASE_DIR + \"test.csv\")\n\nprint('training size: ', len(train_sequences) / 60)\nprint('validation size: ', len(val_sequences) / 60)\nprint('testing size: ', len(test_sequences) / 60)","metadata":{"papermill":{"duration":9.801279,"end_time":"2022-04-18T19:55:00.791175","exception":false,"start_time":"2022-04-18T19:54:50.989896","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-21T03:21:58.425546Z","iopub.execute_input":"2022-04-21T03:21:58.426162Z","iopub.status.idle":"2022-04-21T03:22:04.583098Z","shell.execute_reply.started":"2022-04-21T03:21:58.426123Z","shell.execute_reply":"2022-04-21T03:22:04.582243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this makes using DataLoader easier later on by allowing us to index based on\n# the local sequence number\ntrain_min_seq_num = train_sequences.sequence.min()\ntrain_sequences['sequence_local'] = train_sequences.sequence - train_min_seq_num\nval_min_seq_num = val_sequences.sequence.min()\nval_sequences['sequence_local'] = val_sequences.sequence - val_min_seq_num\ntest_min_seq_num = test_sequences.sequence.min()\ntest_sequences['sequence_local'] = test_sequences.sequence - test_min_seq_num","metadata":{"papermill":{"duration":0.046772,"end_time":"2022-04-18T19:55:00.858744","exception":false,"start_time":"2022-04-18T19:55:00.811972","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-21T03:22:04.584977Z","iopub.execute_input":"2022-04-21T03:22:04.58526Z","iopub.status.idle":"2022-04-21T03:22:04.604155Z","shell.execute_reply.started":"2022-04-21T03:22:04.585222Z","shell.execute_reply":"2022-04-21T03:22:04.603503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sequences.head()","metadata":{"papermill":{"duration":0.044613,"end_time":"2022-04-18T19:55:00.924264","exception":false,"start_time":"2022-04-18T19:55:00.879651","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-21T03:22:04.605503Z","iopub.execute_input":"2022-04-21T03:22:04.605989Z","iopub.status.idle":"2022-04-21T03:22:04.622976Z","shell.execute_reply.started":"2022-04-21T03:22:04.605941Z","shell.execute_reply":"2022-04-21T03:22:04.621927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_sequences.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T03:22:04.624791Z","iopub.execute_input":"2022-04-21T03:22:04.625458Z","iopub.status.idle":"2022-04-21T03:22:04.6438Z","shell.execute_reply.started":"2022-04-21T03:22:04.625417Z","shell.execute_reply":"2022-04-21T03:22:04.64314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sequences.head()","metadata":{"papermill":{"duration":0.042164,"end_time":"2022-04-18T19:55:00.987432","exception":false,"start_time":"2022-04-18T19:55:00.945268","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-21T03:22:04.644984Z","iopub.execute_input":"2022-04-21T03:22:04.645744Z","iopub.status.idle":"2022-04-21T03:22:04.664725Z","shell.execute_reply.started":"2022-04-21T03:22:04.645708Z","shell.execute_reply":"2022-04-21T03:22:04.663957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels.head()","metadata":{"papermill":{"duration":0.033035,"end_time":"2022-04-18T19:55:01.042868","exception":false,"start_time":"2022-04-18T19:55:01.009833","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-21T03:22:04.666861Z","iopub.execute_input":"2022-04-21T03:22:04.667431Z","iopub.status.idle":"2022-04-21T03:22:04.676655Z","shell.execute_reply.started":"2022-04-21T03:22:04.667396Z","shell.execute_reply":"2022-04-21T03:22:04.675948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Gated Transformer Network - Overview of the Architecture [1]**\nThe Gated Transformer Network (GTN) for time-series classification is an architecture with two encodings (also known as towers) that are merged by a gating mechanism. The two encodings are for the channel-wise and the step-wise correlations.\n\nIn this case, we have 13 different sensors (or channels) and 60 time-steps per observation. Since the channels do not have any defined ordering, we only use positional encoding for the step-wise correlations. Furthermore, since our data is continuous, the embedding layer is simply a fully connected layer with *tanh* activation. \n\nHere is the full architecture from the paper:\n\n![GTN architecure image](https://media.arxiv-vanity.com/render-output/5404421/Two_Tower_V5.png)\n\nNow we will implement this in PyTorch along with a data loader.","metadata":{"papermill":{"duration":0.06444,"end_time":"2022-04-18T19:55:06.061111","exception":false,"start_time":"2022-04-18T19:55:05.996671","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class SequenceDataset(Dataset):\n    def __init__(self, sequences, labels=None):\n        super().__init__()\n        self.sequences = sequences\n        self.labels = labels\n        self.sensor_cols = [c for c in self.sequences.columns if 'sensor' in c]\n        \n    def __getitem__(self, seq_key):\n        ''' Returns a single sequence (shape (60, num_features) and its label (0 or 1) '''\n        actual_seq_key = self.sequences[self.sequences.sequence_local == seq_key].sequence.iloc[0]\n        if self.labels is None:\n            label_tensor = np.nan\n        else:\n            label = self.labels[self.labels.sequence == actual_seq_key].state.iloc[0]\n            label_tensor = torch.tensor(label, dtype=torch.long)\n        seq_df = self.sequences[self.sequences.sequence == actual_seq_key]\n        sensors_arr = seq_df[self.sensor_cols].to_numpy()\n        sensor_tensor = torch.tensor(sensors_arr, dtype=torch.float32)\n        return sensor_tensor, label_tensor\n    \n    def __len__(self):\n        return len(self.sequences) // 60\n    \n\nclass PositionalEncoding(nn.Module):\n    ''' Adapted from https://pytorch.org/tutorials/beginner/transformer_tutorial.html '''\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n        \"\"\"\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)\n    \n    \nclass Gate(nn.Module):\n    ''' Gating mechanism for the two towers as described in [1] '''\n    def __init__(self, c_dim, s_dim):\n        super().__init__()\n        # project into 2 dimensions for the gating weights\n        self.linear_project = nn.Linear(c_dim + s_dim, 2)\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, c, s):\n        c = c.mean(dim=0)\n        s = s.mean(dim=0)\n        x = torch.cat([c, s], dim=1)\n        x = self.linear_project(x)\n        g = self.softmax(x)\n        g1 = g[:, 0].unsqueeze(-1)\n        g2 = g[:, 1].unsqueeze(-1)\n        return torch.cat([c * g1, s * g2], dim=1)\n\n\nclass GTN(nn.Module):\n    def __init__(self, num_channels=13, num_steps=60, step_embed_dim=128,\n                 channel_embed_dim=128, step_nheads=5, channel_nheads=5,\n                 step_ff_dim=2048, channel_ff_dim=2048,\n                 num_layers=4, dropout=0.1):\n        super().__init__()\n        # step-wise embedding: each step gets its own embedding\n        self.step_embed = nn.Linear(num_channels, step_embed_dim)\n        # channel-wise embedding: each channel gets its own embedding\n        self.channel_embed = nn.Linear(num_steps, channel_embed_dim)\n        self.tanh = nn.Tanh() \n        # only step-wise embeddings are ordered\n        self.pe = PositionalEncoding(step_embed_dim, dropout=dropout, max_len=num_steps)  \n        # step-wise transformer encoder\n        step_encoder_layer = nn.TransformerEncoderLayer(step_embed_dim, step_nheads,\n                                                        dim_feedforward=step_ff_dim)\n        self.step_transf = nn.TransformerEncoder(step_encoder_layer, num_layers)\n        \n        # channel-wise tranformer encoder\n        channel_encoder_layer = nn.TransformerEncoderLayer(channel_embed_dim, channel_nheads,\n                                                           dim_feedforward=channel_ff_dim)\n        self.channel_transf = nn.TransformerEncoder(channel_encoder_layer, num_layers)\n        # gating\n        self.gate = Gate(channel_embed_dim, step_embed_dim)\n        # linear output\n        self.out = nn.Linear(channel_embed_dim + step_embed_dim, 2)\n        \n        \n    def forward(self, x):\n        # input is (60, N, 13)\n        # x_channel should be (13, N, 60)\n        # x_step should be (60, N, 13)\n        x_step = x\n        x_channel = x.transpose(0, 2).contiguous()\n        # embedding (linear) layers\n        x_step = self.tanh(self.step_embed(x_step))\n        x_channel = self.tanh(self.channel_embed(x_channel))\n        # positional encoding (only for step embeds)\n        x_step = self.pe(x_step)\n        # transformer encodings\n        x_step = self.step_transf(x_step)\n        x_channel = self.channel_transf(x_channel)\n        # gating\n        x = self.gate(x_channel, x_step)\n        return self.out(x)\n    \n\ndef train_one_epoch(train_loader, optimizer, model, loss_func, avg_losses, device, disable=True):\n    model.train()\n    for x, y in tqdm(train_loader, disable=disable):\n        # transformer takes batch as second dim\n        x = x.transpose(0, 1).contiguous()\n        optimizer.zero_grad()\n        x = x.to(device)\n        y = y.to(device)\n        preds = model(x)\n        loss = loss_func(preds, y)\n        avg_losses.append(loss.item())\n        loss.backward()\n        optimizer.step()\n\n\ndef validate(val_loader, model, criterion, device, disable=True):\n    model.eval()\n    for x, y in tqdm(val_loader, disable=disable):\n        with torch.no_grad():\n            x = x.transpose(0, 1).contiguous()\n            x = x.to(device)\n            preds = model(x)\n            probs = F.softmax(preds.cpu(), dim=1)[:, 1]\n            c = criterion(y, probs)\n    return c","metadata":{"papermill":{"duration":0.051381,"end_time":"2022-04-18T19:55:06.155774","exception":false,"start_time":"2022-04-18T19:55:06.104393","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-21T03:22:04.678077Z","iopub.execute_input":"2022-04-21T03:22:04.678338Z","iopub.status.idle":"2022-04-21T03:22:04.708884Z","shell.execute_reply.started":"2022-04-21T03:22:04.678305Z","shell.execute_reply":"2022-04-21T03:22:04.708136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we set up our training hyperparameters and get the data ready. I'm not sure what works well here but I will most likely test a few different parameters on a subset of the training data.","metadata":{"papermill":{"duration":0.037366,"end_time":"2022-04-18T19:55:06.230179","exception":false,"start_time":"2022-04-18T19:55:06.192813","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\nprint(f\"Using device: {device}\")\n\nnum_channels = 13          # number of sensors\nnum_steps = 60             # number of time-steps\nstep_embed_dim = 256       # embedding size for steps\nchannel_embed_dim = 256    # embedding size for channels\nstep_nheads = 16           # number of heads in step-wise attention module\nchannel_nheads = 16        # number of heads in channel-wise attention module\nstep_ff_dim = 2048         # linear layer size within step-wise transformer\nchannel_ff_dim = 2048      # linear layer size within channel-wise transformer\nnum_layers = 4             # number of transformer layers for each\ndropout = 0.1              # dropout regularization rate\nepochs = 50                # number of passes over training data\nbatch_size = 256           # number of observations in model input\nlr = 0.001                 # optimizer learning rate\navg_losses = []\n\ntrain_dataset = SequenceDataset(train_sequences, labels=train_labels)\nval_dataset = SequenceDataset(val_sequences, labels=train_labels)\ntest_dataset = SequenceDataset(test_sequences)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=400, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\nmodel = GTN(num_channels=num_channels, num_steps=num_steps, step_embed_dim=step_embed_dim,\n            channel_embed_dim=channel_embed_dim, step_nheads=step_nheads, channel_nheads=channel_nheads,\n            step_ff_dim=step_ff_dim, channel_ff_dim=channel_ff_dim,\n            num_layers=num_layers, dropout=dropout)\nmodel = model.to(device)\n\noptimizer = optim.Adagrad(model.parameters(), lr=lr)\ncriterion = roc_auc_score\nlr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=10)\nloss_func = nn.CrossEntropyLoss()","metadata":{"papermill":{"duration":3.40982,"end_time":"2022-04-18T19:55:09.677763","exception":false,"start_time":"2022-04-18T19:55:06.267943","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-21T03:22:28.932079Z","iopub.execute_input":"2022-04-21T03:22:28.932547Z","iopub.status.idle":"2022-04-21T03:22:28.997198Z","shell.execute_reply.started":"2022-04-21T03:22:28.932512Z","shell.execute_reply":"2022-04-21T03:22:28.996443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for e in tqdm(range(epochs), total=epochs, disable=False):\n    train_one_epoch(train_loader, optimizer, model, loss_func, avg_losses, device, disable=True)\n    val_auc = validate(val_loader, model, criterion, device, disable=True)\n    print(f'ROC score: {val_auc}')\n    lr_scheduler.step(val_auc)","metadata":{"papermill":{"duration":4247.747921,"end_time":"2022-04-18T21:05:57.464235","exception":false,"start_time":"2022-04-18T19:55:09.716314","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-21T03:22:29.375185Z","iopub.execute_input":"2022-04-21T03:22:29.375694Z","iopub.status.idle":"2022-04-21T03:22:34.536857Z","shell.execute_reply.started":"2022-04-21T03:22:29.37566Z","shell.execute_reply":"2022-04-21T03:22:34.535414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(len(avg_losses)), avg_losses)\nplt.title('Average Loss Per Batch During Training')\nplt.ylabel('Avg Loss')\nplt.xlabel('Batch #')\nplt.show()","metadata":{"papermill":{"duration":0.252636,"end_time":"2022-04-18T21:05:57.768303","exception":false,"start_time":"2022-04-18T21:05:57.515667","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-21T03:22:34.537779Z","iopub.status.idle":"2022-04-21T03:22:34.538248Z","shell.execute_reply.started":"2022-04-21T03:22:34.537988Z","shell.execute_reply":"2022-04-21T03:22:34.538012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nstate_probabilities = []\nfor x, _ in tqdm(test_loader):\n    with torch.no_grad():\n        x = x.reshape(num_steps, -1, num_channels)\n        x = x.to(device)\n        preds = model(x)\n        probs = F.softmax(preds, dim=1)[0]\n        state_probabilities.append(probs[1].item())","metadata":{"papermill":{"duration":177.935602,"end_time":"2022-04-18T21:08:55.756572","exception":false,"start_time":"2022-04-18T21:05:57.82097","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-21T03:22:10.13975Z","iopub.status.idle":"2022-04-21T03:22:10.14036Z","shell.execute_reply.started":"2022-04-21T03:22:10.140134Z","shell.execute_reply":"2022-04-21T03:22:10.140158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(BASE_DIR + 'sample_submission.csv')\nsubmission.head()","metadata":{"papermill":{"duration":0.569308,"end_time":"2022-04-18T21:08:57.224258","exception":false,"start_time":"2022-04-18T21:08:56.65495","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-21T03:21:35.431158Z","iopub.execute_input":"2022-04-21T03:21:35.432391Z","iopub.status.idle":"2022-04-21T03:21:35.456446Z","shell.execute_reply.started":"2022-04-21T03:21:35.432348Z","shell.execute_reply":"2022-04-21T03:21:35.455704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.state = state_probabilities\nsubmission.head()","metadata":{"papermill":{"duration":0.547522,"end_time":"2022-04-18T21:08:58.305667","exception":false,"start_time":"2022-04-18T21:08:57.758145","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-21T03:21:35.45767Z","iopub.execute_input":"2022-04-21T03:21:35.457905Z","iopub.status.idle":"2022-04-21T03:21:35.467767Z","shell.execute_reply.started":"2022-04-21T03:21:35.457873Z","shell.execute_reply":"2022-04-21T03:21:35.467123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"papermill":{"duration":0.581957,"end_time":"2022-04-18T21:08:59.768237","exception":false,"start_time":"2022-04-18T21:08:59.18628","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-21T03:21:35.469788Z","iopub.execute_input":"2022-04-21T03:21:35.470363Z","iopub.status.idle":"2022-04-21T03:21:35.512452Z","shell.execute_reply.started":"2022-04-21T03:21:35.470327Z","shell.execute_reply":"2022-04-21T03:21:35.511826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}