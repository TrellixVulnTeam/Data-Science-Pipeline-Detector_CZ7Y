{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Bi-Directional LSTM for Sequence Classification**\nThomas Hopkins","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\ntorch.manual_seed(32)\nnp.random.seed(0)\n\nBASE_DIR = '/kaggle/input/tabular-playground-series-apr-2022/'","metadata":{"execution":{"iopub.status.busy":"2022-04-18T19:33:16.111759Z","iopub.execute_input":"2022-04-18T19:33:16.112005Z","iopub.status.idle":"2022-04-18T19:33:16.119103Z","shell.execute_reply.started":"2022-04-18T19:33:16.111975Z","shell.execute_reply":"2022-04-18T19:33:16.11828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sequences = pd.read_csv(BASE_DIR + \"train.csv\")\ntrain_labels = pd.read_csv(BASE_DIR + \"train_labels.csv\")\ntest_sequences = pd.read_csv(BASE_DIR + \"test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-18T19:33:16.252143Z","iopub.execute_input":"2022-04-18T19:33:16.252495Z","iopub.status.idle":"2022-04-18T19:33:22.059626Z","shell.execute_reply.started":"2022-04-18T19:33:16.252456Z","shell.execute_reply":"2022-04-18T19:33:22.058821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this makes using DataLoader easier later on by allowing us to index based on\n# the local sequence number\ntrain_min_seq_num = train_sequences.sequence.min()\ntrain_sequences['sequence_local'] = train_sequences.sequence - train_min_seq_num\ntest_min_seq_num = test_sequences.sequence.min()\ntest_sequences['sequence_local'] = test_sequences.sequence - test_min_seq_num","metadata":{"execution":{"iopub.status.busy":"2022-04-18T19:33:22.062653Z","iopub.execute_input":"2022-04-18T19:33:22.06315Z","iopub.status.idle":"2022-04-18T19:33:22.083532Z","shell.execute_reply.started":"2022-04-18T19:33:22.063112Z","shell.execute_reply":"2022-04-18T19:33:22.082713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sequences.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T19:33:22.087162Z","iopub.execute_input":"2022-04-18T19:33:22.087664Z","iopub.status.idle":"2022-04-18T19:33:22.110934Z","shell.execute_reply.started":"2022-04-18T19:33:22.087625Z","shell.execute_reply":"2022-04-18T19:33:22.110327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sequences.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T19:33:22.112692Z","iopub.execute_input":"2022-04-18T19:33:22.113417Z","iopub.status.idle":"2022-04-18T19:33:22.134904Z","shell.execute_reply.started":"2022-04-18T19:33:22.113381Z","shell.execute_reply":"2022-04-18T19:33:22.134249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T19:33:22.136116Z","iopub.execute_input":"2022-04-18T19:33:22.136969Z","iopub.status.idle":"2022-04-18T19:33:22.145665Z","shell.execute_reply.started":"2022-04-18T19:33:22.136934Z","shell.execute_reply":"2022-04-18T19:33:22.14494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Viewing a full sequence**\nHere we will look at how a single sequence varies over time for all of the different sensors. The first plot will be for a sequence with `state = 0` while the other will be for a sequence with `state = 1`.","metadata":{}},{"cell_type":"code","source":"def plot_sensor(data, sensor_num, ax=None):\n    if ax is None:\n        ax = plt.gca()\n    sensor_series = data[\"sensor_\" + sensor_num]\n    ax.plot(range(60), sensor_series, label=sensor_num)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T19:33:22.146976Z","iopub.execute_input":"2022-04-18T19:33:22.147636Z","iopub.status.idle":"2022-04-18T19:33:22.154923Z","shell.execute_reply.started":"2022-04-18T19:33:22.147602Z","shell.execute_reply":"2022-04-18T19:33:22.154072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq0 = 7 # has state = 0\nseq1 = 4 # has state = 1\nif train_labels[train_labels.sequence == seq0].state.iloc[0] == 1:\n    print(f'Warning: sequence {seq0} has state = 1')\nif train_labels[train_labels.sequence == seq1].state.iloc[0] == 0:\n    print(f'Warning: sequence {seq1} has state = 0')\nsequence_0 = train_sequences[train_sequences.sequence == seq0] \nsequence_1 = train_sequences[train_sequences.sequence == seq1] \nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 20))\n\n# comment out any sensors you don't want to see in the output\nplot_sensor(sequence_0, '00', ax=ax1)\nplot_sensor(sequence_0, '01', ax=ax1)\nplot_sensor(sequence_0, '02', ax=ax1)\nplot_sensor(sequence_0, '03', ax=ax1)\nplot_sensor(sequence_0, '04', ax=ax1)\nplot_sensor(sequence_0, '05', ax=ax1)\nplot_sensor(sequence_0, '06', ax=ax1)\nplot_sensor(sequence_0, '07', ax=ax1)\nplot_sensor(sequence_0, '08', ax=ax1)\nplot_sensor(sequence_0, '09', ax=ax1)\nplot_sensor(sequence_0, '10', ax=ax1)\nplot_sensor(sequence_0, '11', ax=ax1)\nplot_sensor(sequence_0, '12', ax=ax1)\nax1.set_title(f\"Sensor data for Sequence {seq0}: state = 0\")\nax1.set_xlabel(\"Step\")\nax1.set_ylabel(\"Reading\")\nax1.legend()\n\nplot_sensor(sequence_1, '00', ax=ax2)\nplot_sensor(sequence_1, '01', ax=ax2)\nplot_sensor(sequence_1, '02', ax=ax2)\nplot_sensor(sequence_1, '03', ax=ax2)\nplot_sensor(sequence_1, '04', ax=ax2)\nplot_sensor(sequence_1, '05', ax=ax2)\nplot_sensor(sequence_1, '06', ax=ax2)\nplot_sensor(sequence_1, '07', ax=ax2)\nplot_sensor(sequence_1, '08', ax=ax2)\nplot_sensor(sequence_1, '09', ax=ax2)\nplot_sensor(sequence_1, '10', ax=ax2)\nplot_sensor(sequence_1, '11', ax=ax2)\nplot_sensor(sequence_1, '12', ax=ax2)\nax2.set_title(f\"Sensor data for Sequence {seq1}: state = 1\")\nax2.set_xlabel(\"Step\")\nax2.set_ylabel(\"Reading\")\nax2.legend();","metadata":{"execution":{"iopub.status.busy":"2022-04-18T19:33:22.156269Z","iopub.execute_input":"2022-04-18T19:33:22.157155Z","iopub.status.idle":"2022-04-18T19:33:23.015419Z","shell.execute_reply.started":"2022-04-18T19:33:22.157119Z","shell.execute_reply":"2022-04-18T19:33:23.014641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cycling through a couple different examples (by changing `seq0` and `seq1`), we can see some differences in pattern between sequences with `state = 0` and `state = 1`. Don't let this influence your feature engineering because it might bias your decisions. This is purely to see if there are *any* differences we can immediately notice. If you have any questions about this point, please leave a comment so we can discuss.\n\nLet's see if a deep learning model can find these differences.\n\n## **Forward Lag and Difference Feature Engineering**\nHere we will add some basic feautures to make learning a bit easier. Lag takes a value from the previous time-step and makes it available for the current time-step. Difference is simply the difference between the current time-step value and the previous. This is taken in the forward direction which means in the order that the sequence actually occurred. Since this is classification and not forecasting, we can also use the backward lag and difference features.","metadata":{}},{"cell_type":"code","source":"def create_new_features(dataset):\n    ''' Create forward and backward lag and difference features for each sensor '''\n    sensor_cols = [c for c in dataset.columns if 'sensor' in c]\n    for sensor in sensor_cols:\n        dataset[f'{sensor}_lag1_forw'] = dataset.groupby('sequence')[sensor].shift(1)\n        dataset[f'{sensor}_lag1_forw'].fillna(dataset[sensor].median(), inplace=True)\n        dataset[f'{sensor}_diff_forw'] = dataset[sensor] - dataset[sensor + '_lag1_forw']\n        \n        dataset[f'{sensor}_lag1_back'] = dataset.groupby('sequence')[sensor].shift(-1)\n        dataset[f'{sensor}_lag1_back'].fillna(dataset[sensor].median(), inplace=True)\n        dataset[f'{sensor}_diff_back'] = dataset[sensor] - dataset[sensor + '_lag1_back']\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-04-18T19:33:23.016821Z","iopub.execute_input":"2022-04-18T19:33:23.01725Z","iopub.status.idle":"2022-04-18T19:33:23.026535Z","shell.execute_reply.started":"2022-04-18T19:33:23.017216Z","shell.execute_reply":"2022-04-18T19:33:23.025679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_seq = create_new_features(train_sequences)\ntest_seq = create_new_features(test_sequences)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T19:33:23.027693Z","iopub.execute_input":"2022-04-18T19:33:23.028062Z","iopub.status.idle":"2022-04-18T19:33:26.543139Z","shell.execute_reply.started":"2022-04-18T19:33:23.028029Z","shell.execute_reply":"2022-04-18T19:33:26.542419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_seq.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T19:33:26.545711Z","iopub.execute_input":"2022-04-18T19:33:26.545987Z","iopub.status.idle":"2022-04-18T19:33:26.571528Z","shell.execute_reply.started":"2022-04-18T19:33:26.54595Z","shell.execute_reply":"2022-04-18T19:33:26.570756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sanity check\ntrain_seq[['sequence', 'sensor_00', 'sensor_00_lag1_forw', 'sensor_00_diff_forw', 'sensor_00_lag1_back', 'sensor_00_diff_back']].head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T19:33:26.572987Z","iopub.execute_input":"2022-04-18T19:33:26.573238Z","iopub.status.idle":"2022-04-18T19:33:27.090174Z","shell.execute_reply.started":"2022-04-18T19:33:26.573206Z","shell.execute_reply":"2022-04-18T19:33:27.089369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Bidirectional LSTM**\nHere we will use a bidirectional LSTM for sequence classification. See https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM for details.\n\nFirst we will define a dataset so the data is easier to work with for our training loop.","metadata":{}},{"cell_type":"code","source":"class SequenceDataset(Dataset):\n    def __init__(self, sequences, labels=None):\n        super().__init__()\n        self.sequences = sequences\n        self.labels = labels\n        self.sensor_cols = [c for c in self.sequences.columns if 'sensor' in c]\n        \n    def __getitem__(self, seq_key):\n        ''' Returns a single sequence (shape (60, num_features) and its label (0 or 1) '''\n        if self.labels is None:\n            label_tensor = np.nan\n        else:\n            label = self.labels[self.labels.sequence == seq_key].state.iloc[0]\n            label_tensor = torch.tensor(label, dtype=torch.long)\n        seq_df = self.sequences[self.sequences.sequence_local == seq_key]\n        sensors_arr = seq_df[self.sensor_cols].to_numpy()\n        sensor_tensor = torch.tensor(sensors_arr, dtype=torch.float32)\n        return sensor_tensor, label_tensor\n    \n    def __len__(self):\n        if self.labels is None:\n            return len(self.sequences) // 60\n        return len(self.labels)\n    \n    \nclass LSTMClassifier(nn.Module):\n    def __init__(self, input_dim, hidden_size, num_layers, dropout_rate):\n        super().__init__()\n        self.LSTM = nn.LSTM(input_dim, hidden_size, num_layers, batch_first=True,\n                         dropout=dropout_rate, bidirectional=True)\n        self.l1 = nn.Linear(2 * hidden_size * 60,  128)\n        self.l2 = nn.Linear(128, 2)\n        \n    def forward(self, x):\n        # should be (N, L, 13)\n        x = self.LSTM(x)[0].flatten(start_dim=1)\n        # should be (N, 2 * hidden_size * 60)\n        x = F.relu(self.l1(x))\n        return self.l2(x)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T19:33:27.091677Z","iopub.execute_input":"2022-04-18T19:33:27.091991Z","iopub.status.idle":"2022-04-18T19:33:27.10718Z","shell.execute_reply.started":"2022-04-18T19:33:27.091955Z","shell.execute_reply":"2022-04-18T19:33:27.10631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we set up our training hyperparameters and get the data ready. I'm not sure what works well here but I will most likely test a few different parameters on a subset of the training data.","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\nprint(f\"Using device: {device}\")\n    \nEPOCHS = 40\nBATCH_SIZE = 128\nHIDDEN_SIZE = 256\nNUM_LAYERS = 8\nDROPOUT_RATE = 0.4\nLEARNING_RATE = 0.0005\nAVG_LOSSES = []\n\ntrain_dataset = SequenceDataset(train_seq, labels=train_labels)\ntest_dataset = SequenceDataset(test_seq)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\nmodel = LSTMClassifier(len(train_dataset.sensor_cols), HIDDEN_SIZE, NUM_LAYERS, DROPOUT_RATE)\nmodel = model.to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\nloss_func = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T19:52:25.512689Z","iopub.execute_input":"2022-04-18T19:52:25.512954Z","iopub.status.idle":"2022-04-18T19:52:25.653535Z","shell.execute_reply.started":"2022-04-18T19:52:25.512925Z","shell.execute_reply":"2022-04-18T19:52:25.652791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for e in tqdm(range(EPOCHS), total=EPOCHS):\n    for x, y in train_loader:\n        optimizer.zero_grad()\n        x = x.to(device)\n        y = y.to(device)\n        preds = model(x)\n        loss = loss_func(preds, y)\n        AVG_LOSSES.append(loss.item())\n        loss.backward()\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T19:52:25.921258Z","iopub.execute_input":"2022-04-18T19:52:25.921736Z","iopub.status.idle":"2022-04-18T19:54:21.086174Z","shell.execute_reply.started":"2022-04-18T19:52:25.921704Z","shell.execute_reply":"2022-04-18T19:54:21.084962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.close()\nplt.plot(range(len(AVG_LOSSES)), AVG_LOSSES)\nplt.title('Average Loss Per Batch During Training')\nplt.ylabel('Avg Loss')\nplt.xlabel('Batch #')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T19:48:05.244058Z","iopub.execute_input":"2022-04-18T19:48:05.244416Z","iopub.status.idle":"2022-04-18T19:48:05.440812Z","shell.execute_reply.started":"2022-04-18T19:48:05.244381Z","shell.execute_reply":"2022-04-18T19:48:05.44013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nstate_probabilities = []\nfor x, _ in tqdm(test_loader):\n    with torch.no_grad():\n        x = x.to(device)\n        preds = model(x)\n        probs = F.softmax(preds, dim=1)[0]\n        state_probabilities.append(probs[1].item())","metadata":{"execution":{"iopub.status.busy":"2022-04-18T19:48:05.442212Z","iopub.execute_input":"2022-04-18T19:48:05.442801Z","iopub.status.idle":"2022-04-18T19:50:20.831847Z","shell.execute_reply.started":"2022-04-18T19:48:05.442764Z","shell.execute_reply":"2022-04-18T19:50:20.830967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(BASE_DIR + 'sample_submission.csv')\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T19:50:20.83322Z","iopub.execute_input":"2022-04-18T19:50:20.83379Z","iopub.status.idle":"2022-04-18T19:50:20.857388Z","shell.execute_reply.started":"2022-04-18T19:50:20.833748Z","shell.execute_reply":"2022-04-18T19:50:20.856667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.state = state_probabilities\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T19:50:20.858644Z","iopub.execute_input":"2022-04-18T19:50:20.858952Z","iopub.status.idle":"2022-04-18T19:50:20.873584Z","shell.execute_reply.started":"2022-04-18T19:50:20.858915Z","shell.execute_reply":"2022-04-18T19:50:20.872884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T19:50:20.874787Z","iopub.execute_input":"2022-04-18T19:50:20.875314Z","iopub.status.idle":"2022-04-18T19:50:20.919487Z","shell.execute_reply.started":"2022-04-18T19:50:20.875262Z","shell.execute_reply":"2022-04-18T19:50:20.918742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}