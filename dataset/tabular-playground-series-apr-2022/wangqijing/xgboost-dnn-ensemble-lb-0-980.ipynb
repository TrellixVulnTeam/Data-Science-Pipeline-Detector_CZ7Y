{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is a simple ensemble method for submission.\n\nHere we use the 3 best solutions to get the best accuracy of the model, by selecting the best weights:\n\nhttps://www.kaggle.com/code/dmitryuarov/sensors-deep-analysis-0-98\nhttps://www.kaggle.com/code/tyrionlannisterlzy/xgboost-dnn-ensemble-lb-0-980\nhttps://www.kaggle.com/code/hasanbasriakcay/tpsapr22-fe-pseudo-labels-bi-lstm","metadata":{}},{"cell_type":"markdown","source":"<h1>Importing Libraries</h1>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected = True)\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:11:48.166428Z","iopub.execute_input":"2022-04-19T01:11:48.166771Z","iopub.status.idle":"2022-04-19T01:11:51.641227Z","shell.execute_reply.started":"2022-04-19T01:11:48.166665Z","shell.execute_reply":"2022-04-19T01:11:51.640535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Loading Data</h2>","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-apr-2022/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')\nsubmission = pd.read_csv(\"../input/tabular-playground-series-apr-2022/sample_submission.csv\")\nlabels = pd.read_csv(\"../input/tabular-playground-series-apr-2022/train_labels.csv\")\n\ntrain","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:11:51.642883Z","iopub.execute_input":"2022-04-19T01:11:51.643269Z","iopub.status.idle":"2022-04-19T01:12:02.962811Z","shell.execute_reply.started":"2022-04-19T01:11:51.643231Z","shell.execute_reply":"2022-04-19T01:12:02.962138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# give you an quike insght into the train data \n# including count,mean,std,min,25%,50%,75% and max value\ntrain.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:12:02.964038Z","iopub.execute_input":"2022-04-19T01:12:02.964262Z","iopub.status.idle":"2022-04-19T01:12:03.740768Z","shell.execute_reply.started":"2022-04-19T01:12:02.964229Z","shell.execute_reply":"2022-04-19T01:12:03.739905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run the code below to check if missing data exits\ntrain.isnull().sum(axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:12:03.742919Z","iopub.execute_input":"2022-04-19T01:12:03.743268Z","iopub.status.idle":"2022-04-19T01:12:03.797742Z","shell.execute_reply.started":"2022-04-19T01:12:03.743227Z","shell.execute_reply":"2022-04-19T01:12:03.796902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>adding labels to train data</h3>","metadata":{}},{"cell_type":"code","source":"labels.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:12:03.799035Z","iopub.execute_input":"2022-04-19T01:12:03.799357Z","iopub.status.idle":"2022-04-19T01:12:03.808334Z","shell.execute_reply.started":"2022-04-19T01:12:03.79932Z","shell.execute_reply":"2022-04-19T01:12:03.8074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train =train.merge(labels,how='left', on=[\"sequence\"])\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:12:03.810158Z","iopub.execute_input":"2022-04-19T01:12:03.810435Z","iopub.status.idle":"2022-04-19T01:12:04.022318Z","shell.execute_reply.started":"2022-04-19T01:12:03.8104Z","shell.execute_reply":"2022-04-19T01:12:04.021591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set the size of the map\nfeatures  = [col for col in test.columns if col not in (\"sequence\",\"step\",\"subject\")]\nplt.figure(figsize = (15,7))\n\nhm = sns.heatmap(train[features].corr(),    # data\n                cmap = 'coolwarm',# style\n                annot = True,     # True to show the specific values\n                fmt = '.2f',      # set the precision\n                linewidths = 0.05)\nplt.title('Correlation Heatmap for Train dataset', \n              fontsize=14, \n              fontweight='bold')","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:12:04.023804Z","iopub.execute_input":"2022-04-19T01:12:04.024062Z","iopub.status.idle":"2022-04-19T01:12:05.938294Z","shell.execute_reply.started":"2022-04-19T01:12:04.024027Z","shell.execute_reply":"2022-04-19T01:12:05.937405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"through the heatmap we may have find that\nsensor **00, 01, 03, 06, 07, 09, 10, 11** have something to dig\n\nalso the sensor 04 but we just ignore it temporarily\n\nso we will focus on them.","metadata":{}},{"cell_type":"code","source":"col_t=[\"sensor_00\",\"sensor_01\",\"sensor_03\",\"sensor_04\",\"sensor_06\",\"sensor_07\",\"sensor_09\",\"sensor_10\",\"sensor_11\"]\n\n# set the size of the map\nplt.figure(figsize = (9,5))\n\nhm = sns.heatmap(train[col_t].corr(),    # data\n                cmap = 'coolwarm',      \n                annot = True,     \n                fmt = '.2f', \n                linewidths = 0.05)\nplt.title('Correlation Heatmap for Selected columns from Train dataset', \n              fontsize=14, \n              fontweight='bold')","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:12:05.939627Z","iopub.execute_input":"2022-04-19T01:12:05.939909Z","iopub.status.idle":"2022-04-19T01:12:07.164214Z","shell.execute_reply.started":"2022-04-19T01:12:05.939871Z","shell.execute_reply":"2022-04-19T01:12:07.163519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a quike glimpse into the data of the sensors.","metadata":{}},{"cell_type":"code","source":"sequences = [0, 1, 2, 3, 4, 5]\nfigure, axes = plt.subplots(13, len(sequences), sharex=True, figsize=(16, 16))\nfor i, sequence in enumerate(sequences):\n    for sensor in range(13):\n        sensor_name = f\"sensor_{sensor:02d}\"\n        plt.subplot(13, len(sequences), sensor * len(sequences) + i + 1)\n        plt.plot(range(60), train[train.sequence == sequence][sensor_name],\n                color=plt.rcParams['axes.prop_cycle'].by_key()['color'][i % 10])\n        if sensor == 0: plt.title(f\"Sequence {sequence}\")\n        if sequence == sequences[0]: plt.ylabel(sensor_name)\nfigure.tight_layout(w_pad=0.1)\nplt.suptitle('Selected Time Series', y=1.02)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:12:07.165432Z","iopub.execute_input":"2022-04-19T01:12:07.165846Z","iopub.status.idle":"2022-04-19T01:12:15.017221Z","shell.execute_reply.started":"2022-04-19T01:12:07.165804Z","shell.execute_reply":"2022-04-19T01:12:15.016604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Feature Seeking bewteen Target and Sensors Data</h1>","metadata":{}},{"cell_type":"markdown","source":"here we first introduce the concept of Mutual Information(MI).\n\nMutual information describes relationships in terms of uncertainty. The MI between two quantities is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other. ","metadata":{}},{"cell_type":"code","source":"# from sklearn.feature_selection import mutual_info_regression\n\n# def make_mi_scores(X, y, discrete_features):\n#     mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n#     mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n#     mi_scores = mi_scores.sort_values(ascending=False)\n#     return mi_scores","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:12:15.019854Z","iopub.execute_input":"2022-04-19T01:12:15.020738Z","iopub.status.idle":"2022-04-19T01:12:15.024717Z","shell.execute_reply.started":"2022-04-19T01:12:15.020674Z","shell.execute_reply":"2022-04-19T01:12:15.023935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_mi = train.copy()\n# y_mi = X_mi.pop(\"state\")\n\n# # Label encoding for categoricals\n# for colname in X_mi.select_dtypes(\"object\"):\n#     X_mi[colname], _ = X_mi[colname].factorize()\n\n# # All discrete features should now have integer dtypes (double-check this before using MI!)\n# discrete_features = X_mi.dtypes == int","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:12:15.026071Z","iopub.execute_input":"2022-04-19T01:12:15.026667Z","iopub.status.idle":"2022-04-19T01:12:15.036271Z","shell.execute_reply.started":"2022-04-19T01:12:15.026629Z","shell.execute_reply":"2022-04-19T01:12:15.034047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The least possible mutual information between quantities is 0.0. When MI is zero, the quantities are independent: neither can tell you anything about the other. Conversely, in theory there's no upper bound to what MI can be. In practice though values above 2.0 or so are uncommon. (Mutual information is a **logarithmic quantity**, so it increases very slowly.)\n","metadata":{}},{"cell_type":"code","source":"# %%time\n# mi_scores = make_mi_scores(X_mi, y_mi, discrete_features)\n# mi_scores[::3]  # show a few features with their MI scores","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:12:15.037179Z","iopub.execute_input":"2022-04-19T01:12:15.037397Z","iopub.status.idle":"2022-04-19T01:12:15.044522Z","shell.execute_reply.started":"2022-04-19T01:12:15.037374Z","shell.execute_reply":"2022-04-19T01:12:15.043766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mi_scores","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:12:15.04614Z","iopub.execute_input":"2022-04-19T01:12:15.046394Z","iopub.status.idle":"2022-04-19T01:12:15.05423Z","shell.execute_reply.started":"2022-04-19T01:12:15.046363Z","shell.execute_reply":"2022-04-19T01:12:15.053285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nbelow I will test the 'mean', 'max', 'min', 'var', 'mad', 'sum', 'median' value of the data hoping to dig anything valuable\n\n> the codes below are inspired by C4rl05/V with her work [https://www.kaggle.com/code/cv13j0/tps-apr-2022-xgboost-model](http://)","metadata":{}},{"cell_type":"code","source":"def aggregated_features(df, aggregation_cols = ['sequence'], prefix = ''):\n    agg_strategy = {'sensor_00': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_01': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_02': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_03': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_04': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_05': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_06': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_07': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_08': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_09': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_10': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_11': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                    'sensor_12': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median'],\n                   }\n    group = df.groupby(aggregation_cols).aggregate(agg_strategy)\n    group.columns = ['_'.join(col).strip() for col in group.columns]\n    group.columns = [str(prefix) + str(col) for col in group.columns]\n    group.reset_index(inplace = True)\n    \n    temp = (df.groupby(aggregation_cols).size().reset_index(name = str(prefix) + 'size'))\n    group = pd.merge(temp, group, how = 'left', on = aggregation_cols,)\n    return group","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:12:15.056033Z","iopub.execute_input":"2022-04-19T01:12:15.056277Z","iopub.status.idle":"2022-04-19T01:12:15.070263Z","shell.execute_reply.started":"2022-04-19T01:12:15.056245Z","shell.execute_reply":"2022-04-19T01:12:15.069583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_merge_data = aggregated_features(train, aggregation_cols = ['sequence', 'subject'])\ntest_merge_data = aggregated_features(test, aggregation_cols = ['sequence', 'subject'])","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:12:15.072323Z","iopub.execute_input":"2022-04-19T01:12:15.07308Z","iopub.status.idle":"2022-04-19T01:15:23.23217Z","shell.execute_reply.started":"2022-04-19T01:12:15.073044Z","shell.execute_reply":"2022-04-19T01:15:23.23138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_subjects_merge_data = aggregated_features(train, aggregation_cols = ['subject'], prefix = 'subject_')\ntest_subjects_merge_data = aggregated_features(test, aggregation_cols = ['subject'], prefix = 'subject_')","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:15:23.233378Z","iopub.execute_input":"2022-04-19T01:15:23.234164Z","iopub.status.idle":"2022-04-19T01:15:31.332073Z","shell.execute_reply.started":"2022-04-19T01:15:23.234125Z","shell.execute_reply":"2022-04-19T01:15:31.331343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"up to now we have a clear view of the values of sensors ","metadata":{}},{"cell_type":"code","source":"train_subjects_merge_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:15:31.333487Z","iopub.execute_input":"2022-04-19T01:15:31.333707Z","iopub.status.idle":"2022-04-19T01:15:31.355555Z","shell.execute_reply.started":"2022-04-19T01:15:31.333675Z","shell.execute_reply":"2022-04-19T01:15:31.354775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Experimenting with Lags</h3>\n\n>lagging is a commom techinic used in time series datasets\n\nLagging a time series means to shift its values forward one or more time steps, or equivalently, to shift the times in its index backward one or more steps. In either case, the effect is that the observations in the lagged series will appear to have happened later in time.","metadata":{}},{"cell_type":"code","source":"train['sensor_00_lag_01'] = train['sensor_00'].shift(1)\ntrain['sensor_00_lag_10'] = train['sensor_00'].shift(10)\ntrain.head(15)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:15:31.3569Z","iopub.execute_input":"2022-04-19T01:15:31.35715Z","iopub.status.idle":"2022-04-19T01:15:31.394888Z","shell.execute_reply.started":"2022-04-19T01:15:31.357117Z","shell.execute_reply":"2022-04-19T01:15:31.394238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Merging the Datasets before Training</h3>","metadata":{}},{"cell_type":"code","source":"train_merge_data = train_merge_data.merge(labels, how = 'left', on = 'sequence')","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:15:31.396063Z","iopub.execute_input":"2022-04-19T01:15:31.39646Z","iopub.status.idle":"2022-04-19T01:15:31.415377Z","shell.execute_reply.started":"2022-04-19T01:15:31.396425Z","shell.execute_reply":"2022-04-19T01:15:31.414762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_merge_data = train_merge_data.merge(train_subjects_merge_data, how = 'left', on = 'subject')\ntest_merge_data = test_merge_data.merge(test_subjects_merge_data, how = 'left', on = 'subject')\ntrain_merge_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:15:31.416629Z","iopub.execute_input":"2022-04-19T01:15:31.416886Z","iopub.status.idle":"2022-04-19T01:15:31.461571Z","shell.execute_reply.started":"2022-04-19T01:15:31.416853Z","shell.execute_reply":"2022-04-19T01:15:31.460787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_merge_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:15:31.462847Z","iopub.execute_input":"2022-04-19T01:15:31.463152Z","iopub.status.idle":"2022-04-19T01:15:31.488226Z","shell.execute_reply.started":"2022-04-19T01:15:31.463115Z","shell.execute_reply":"2022-04-19T01:15:31.487542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Post Processing the Information for the Model</h3>","metadata":{}},{"cell_type":"code","source":"ignore = ['sequence', 'state', 'subject']\nfeatures = [feat for feat in train_merge_data.columns if feat not in ignore]\ntarget_feature = 'state'","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:15:31.489224Z","iopub.execute_input":"2022-04-19T01:15:31.48946Z","iopub.status.idle":"2022-04-19T01:15:31.493838Z","shell.execute_reply.started":"2022-04-19T01:15:31.48943Z","shell.execute_reply":"2022-04-19T01:15:31.492976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Train - Test Split </h3>\n\nyou may do cross-validation too.","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import train_test_split\ntest_size_pct = 0.20\nX_train, X_valid, y_train, y_valid = train_test_split(\n                                train_merge_data[features], \n                                train_merge_data[target_feature], \n                                test_size = test_size_pct, \n                                random_state = 16)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:15:31.494894Z","iopub.execute_input":"2022-04-19T01:15:31.495642Z","iopub.status.idle":"2022-04-19T01:15:31.570985Z","shell.execute_reply.started":"2022-04-19T01:15:31.495605Z","shell.execute_reply":"2022-04-19T01:15:31.570037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Building a XGBoost Model</h3>","metadata":{}},{"cell_type":"code","source":"from xgboost  import XGBClassifier\n\nparams = {'n_estimators': 8192,\n          'max_depth': 7,\n          'learning_rate': 0.1,\n          'subsample': 0.96,\n          'colsample_bytree': 0.80,\n          'reg_lambda': 1.50,\n          'reg_alpha': 6.10,\n          'gamma': 1.40,\n          'random_state': 16,\n          'objective': 'binary:logistic',\n          #'tree_method': 'gpu_hist',\n         }\n\nxgb = XGBClassifier(**params)\nxgb.fit(X_train, y_train, \n        eval_set = [(X_valid, y_valid)], \n        eval_metric = ['auc','logloss'], \n        early_stopping_rounds = 64, \n        verbose = 32)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:15:31.572526Z","iopub.execute_input":"2022-04-19T01:15:31.573111Z","iopub.status.idle":"2022-04-19T01:18:43.28465Z","shell.execute_reply.started":"2022-04-19T01:15:31.573067Z","shell.execute_reply":"2022-04-19T01:18:43.28394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\npreds = xgb.predict_proba(X_valid)[:, 1]\nscore = roc_auc_score(y_valid, preds)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:18:43.286005Z","iopub.execute_input":"2022-04-19T01:18:43.286392Z","iopub.status.idle":"2022-04-19T01:18:43.374155Z","shell.execute_reply.started":"2022-04-19T01:18:43.286355Z","shell.execute_reply":"2022-04-19T01:18:43.373554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Check the Feature Importance through plots</h3>","metadata":{}},{"cell_type":"code","source":"def plot_feature_importance(importance, names, model_type, max_features = 10):\n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n    fi_df = fi_df.head(max_features)\n\n    #Define size of bar plot\n    plt.figure(figsize=(8,6))\n    \n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + 'FEATURE IMPORTANCE')\n    plt.xlabel('IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:18:43.377141Z","iopub.execute_input":"2022-04-19T01:18:43.378581Z","iopub.status.idle":"2022-04-19T01:18:43.385802Z","shell.execute_reply.started":"2022-04-19T01:18:43.378551Z","shell.execute_reply":"2022-04-19T01:18:43.385032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplot_feature_importance(xgb.feature_importances_,X_train.columns,'XGBOOST ', max_features = 15)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:18:43.38722Z","iopub.execute_input":"2022-04-19T01:18:43.387577Z","iopub.status.idle":"2022-04-19T01:18:43.689264Z","shell.execute_reply.started":"2022-04-19T01:18:43.387544Z","shell.execute_reply":"2022-04-19T01:18:43.688552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Make Submission File</h3>","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nxgb_preds = xgb.predict_proba(test_merge_data[features])[:, 1]\nxgb_preds","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:18:43.692962Z","iopub.execute_input":"2022-04-19T01:18:43.693259Z","iopub.status.idle":"2022-04-19T01:18:43.892508Z","shell.execute_reply.started":"2022-04-19T01:18:43.69323Z","shell.execute_reply":"2022-04-19T01:18:43.891967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ENsemble from DNN model\nlater will be updated..\n\nrough version.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import KFold, GroupKFold\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import GlobalMaxPooling1D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.layers import Concatenate, LSTM, GRU\nfrom tensorflow.keras.layers import Bidirectional, Multiply\nnp.random.seed(2022)\ntf.random.set_seed(2022)\ntrain = pd.read_csv('../input/tabular-playground-series-apr-2022/train.csv')\nt_lbls = pd.read_csv('../input/tabular-playground-series-apr-2022/train_labels.csv')\ntest = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')\ns6 = pd.read_csv('../input/tabular-playground-series-apr-2022/sample_submission.csv')\ns7=pd.read_csv('../input/blend-sub/blend_sub12.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:18:43.895239Z","iopub.execute_input":"2022-04-19T01:18:43.895676Z","iopub.status.idle":"2022-04-19T01:18:54.823072Z","shell.execute_reply.started":"2022-04-19T01:18:43.895639Z","shell.execute_reply":"2022-04-19T01:18:54.822317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = train.columns.tolist()[3:]\ndef prep(df):\n    for feature in features:\n        df[feature + '_lag1'] = df.groupby('sequence')[feature].shift(1)\n        df.fillna(0, inplace=True)\n        df[feature + '_diff1'] = df[feature] - df[feature + '_lag1']    \n\nprep(train)\nprep(test)\n\nfeatures = train.columns.tolist()[3:]\nsc = StandardScaler()\ntrain[features] = sc.fit_transform(train[features])\ntest[features] = sc.transform(test[features])\n\ngroups = train[\"sequence\"]\nlabels = t_lbls[\"state\"]\n\ntrain = train.drop([\"sequence\", \"subject\", \"step\"], axis=1).values\ntrain = train.reshape(-1, 60, train.shape[-1])\n\ntest = test.drop([\"sequence\", \"subject\", \"step\"], axis=1).values\ntest = test.reshape(-1, 60, test.shape[-1])","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:18:54.824555Z","iopub.execute_input":"2022-04-19T01:18:54.824821Z","iopub.status.idle":"2022-04-19T01:19:03.222552Z","shell.execute_reply.started":"2022-04-19T01:18:54.824787Z","shell.execute_reply":"2022-04-19T01:19:03.221711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try:\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n#     BATCH_SIZE = tpu_strategy.num_replicas_in_sync * 64\n#     print(\"Running on TPU:\", tpu.master())\n#     print(f\"Batch Size: {BATCH_SIZE}\")\n    \n# except ValueError:\n#     strategy = tf.distribute.get_strategy()\n#     BATCH_SIZE = 256\n#     print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n#     print(f\"Batch Size: {BATCH_SIZE}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:19:03.223673Z","iopub.execute_input":"2022-04-19T01:19:03.22395Z","iopub.status.idle":"2022-04-19T01:19:03.227405Z","shell.execute_reply.started":"2022-04-19T01:19:03.223915Z","shell.execute_reply":"2022-04-19T01:19:03.226792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dnn_model():\n    \n    x_input = Input(shape=(train.shape[-2:]))\n    \n    x1 = Bidirectional(LSTM(units=512, return_sequences=True))(x_input)\n    x2 = Bidirectional(LSTM(units=256, return_sequences=True))(x1)\n    z1 = Bidirectional(GRU(units=256, return_sequences=True))(x1)\n    \n    c = Concatenate(axis=2)([x2, z1])\n    \n    x3 = Bidirectional(LSTM(units=128, return_sequences=True))(c)\n    \n    x4 = GlobalMaxPooling1D()(x3)\n    x5 = Dense(units=128, activation='selu')(x4)\n    x_output = Dense(1, activation='sigmoid')(x5)\n\n    model = Model(inputs=x_input, outputs=x_output, name='lstm_model')\n    \n    return model\n\nmodel = dnn_model()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:19:03.228766Z","iopub.execute_input":"2022-04-19T01:19:03.229171Z","iopub.status.idle":"2022-04-19T01:19:09.396672Z","shell.execute_reply.started":"2022-04-19T01:19:03.229134Z","shell.execute_reply":"2022-04-19T01:19:09.395928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with tpu_strategy.scope():\nVERBOSE = True\nBATCH_SIZE = 256\npredictions, scores = [], []\nk = GroupKFold(n_splits = 10)\n\nfor fold, (train_idx, val_idx) in enumerate(k.split(train, labels, groups.unique())):\n    print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n\n    X_train, X_val = train[train_idx], train[val_idx]\n    y_train, y_val = labels.iloc[train_idx].values, labels.iloc[val_idx].values\n\n    model = dnn_model()\n    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics='AUC')\n\n    lr = ReduceLROnPlateau(monitor=\"val_auc\", factor=0.6, \n                           patience=4, verbose=VERBOSE)\n\n    es = EarlyStopping(monitor=\"val_auc\", patience=7, \n                       verbose=VERBOSE, mode=\"max\", \n                       restore_best_weights=True)\n\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n    chk_point = ModelCheckpoint(f'./TPS_model_2022_{fold+1}C.h5', options=save_locally, \n                                monitor='val_auc', verbose=VERBOSE, \n                                save_best_only=True, mode='max')\n\n    model.fit(X_train, y_train, \n              validation_data=(X_val, y_val), \n              epochs=16,\n              verbose=VERBOSE,\n              batch_size=BATCH_SIZE, \n              callbacks=[lr, chk_point, es])\n\n    load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n    model = load_model(f'./TPS_model_2022_{fold+1}C.h5', options=load_locally)\n\n    y_pred = model.predict(X_val, batch_size=BATCH_SIZE).squeeze()\n    score = roc_auc_score(y_val, y_pred)\n    scores.append(score)\n    predictions.append(model.predict(test, batch_size=BATCH_SIZE).squeeze())\n    print(f\"Fold-{fold+1} | OOF Score: {score}\")\n\nprint(f'Mean accuracy on {k.n_splits} folds - {np.mean(scores)}')","metadata":{"execution":{"iopub.status.busy":"2022-04-19T01:19:09.397812Z","iopub.execute_input":"2022-04-19T01:19:09.398054Z","iopub.status.idle":"2022-04-19T02:03:58.250386Z","shell.execute_reply.started":"2022-04-19T01:19:09.398022Z","shell.execute_reply":"2022-04-19T02:03:58.249673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s6[\"state\"] = sum(predictions)/k.n_splits\ns6[\"state\"]","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:03:58.252027Z","iopub.execute_input":"2022-04-19T02:03:58.252283Z","iopub.status.idle":"2022-04-19T02:03:58.262467Z","shell.execute_reply.started":"2022-04-19T02:03:58.25225Z","shell.execute_reply":"2022-04-19T02:03:58.261739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"blendsub=pd.read_csv(\"../input/blens-sub31/blend_sub31_exp.csv\")\npreds=(s7.state+s6.state)*0.25+xgb_preds*0.24+blendsub.state*0.5\npreds","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:03:58.263842Z","iopub.execute_input":"2022-04-19T02:03:58.2643Z","iopub.status.idle":"2022-04-19T02:03:58.292272Z","shell.execute_reply.started":"2022-04-19T02:03:58.264266Z","shell.execute_reply":"2022-04-19T02:03:58.291613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"just replace the state columns with your predicts","metadata":{}},{"cell_type":"code","source":"\nsubmission['state'] = preds\nsubmission.to_csv('my_submission_ty.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T02:03:58.293446Z","iopub.execute_input":"2022-04-19T02:03:58.293686Z","iopub.status.idle":"2022-04-19T02:03:58.33757Z","shell.execute_reply.started":"2022-04-19T02:03:58.293653Z","shell.execute_reply":"2022-04-19T02:03:58.336923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n\n**still working on find more features ....\nwill be updated soon!**","metadata":{}},{"cell_type":"markdown","source":"  ","metadata":{}}]}