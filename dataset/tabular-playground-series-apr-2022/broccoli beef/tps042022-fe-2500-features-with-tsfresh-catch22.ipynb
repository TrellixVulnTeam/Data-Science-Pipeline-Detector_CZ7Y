{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-17T01:33:20.602563Z","iopub.execute_input":"2022-04-17T01:33:20.602937Z","iopub.status.idle":"2022-04-17T01:33:20.634367Z","shell.execute_reply.started":"2022-04-17T01:33:20.602826Z","shell.execute_reply":"2022-04-17T01:33:20.63348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('max_columns',None)\npd.set_option('display.max_rows', 1000)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-17T01:33:26.914523Z","iopub.execute_input":"2022-04-17T01:33:26.914945Z","iopub.status.idle":"2022-04-17T01:33:26.92037Z","shell.execute_reply.started":"2022-04-17T01:33:26.914896Z","shell.execute_reply":"2022-04-17T01:33:26.919356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Synopsis\n\n1. We use the `tsfresh` and `catch22` Python packages to extract 2500+ features.\n2. We use the `tsfresh` feature filtering module to identify \"relevant\" features from the 2500+ features, resulting in a smaller set of 1200+ features.\n3. We use the statistical measure of mutual information (MI) to rank the 2500+ features. In particular, out of the top 250 features according to MI, over 75% survive the `tsfresh` filtering.\n4. We save both feature sets to datasets for use in subsequent classification task.","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n\nThe TPS April 2022 competition is about binary classification of sequences each of which is associated with 13 sensor time series. Visual inspection of most of these time series does not reveal obvious features that distinguish between the two classes. Subtle statistical features are non-intuitive to the human mind and are best discovered by generating as many features as possible mechanistically and then resorting to feature selection and/or machine learning algorithms downstream.\n\nIn this notebook, we use the tsfresh library (already used in the kaggle [benchmark notebook](https://www.kaggle.com/code/ryanholbrook/tps-april-2022-benchmark)) and the catch22 library to generate features mechanistically. The only non-time-series feature we include is the \"repeated subject count\" which has been found to correlate with the target variable (e.g., see [this discussion](https://www.kaggle.com/competitions/tabular-playground-series-apr-2022/discussion/318527)).\n\nThe main goal of this notebook is to generate and save datasets with large number of features for the subsequent classification task. Since feature extraction takes a long time, it makes sense to separate the feature generation step from the classification step so that different classifcation algorithms can be experimented using the same dataset. No classifier will be built in this notebook, although there will be an indication of feature importance using the statistical measure of mutual information (MI).","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('../input/tabular-playground-series-apr-2022/train.csv')\nlabels=pd.read_csv('../input/tabular-playground-series-apr-2022/train_labels.csv')\ntest_data = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-17T01:36:43.275787Z","iopub.execute_input":"2022-04-17T01:36:43.276199Z","iopub.status.idle":"2022-04-17T01:36:56.586477Z","shell.execute_reply.started":"2022-04-17T01:36:43.276152Z","shell.execute_reply":"2022-04-17T01:36:56.585588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2022-04-17T01:37:01.146783Z","iopub.execute_input":"2022-04-17T01:37:01.147134Z","iopub.status.idle":"2022-04-17T01:37:01.191031Z","shell.execute_reply.started":"2022-04-17T01:37:01.147101Z","shell.execute_reply":"2022-04-17T01:37:01.190036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"execution":{"iopub.status.busy":"2022-04-17T01:37:02.515554Z","iopub.execute_input":"2022-04-17T01:37:02.51655Z","iopub.status.idle":"2022-04-17T01:37:02.546131Z","shell.execute_reply.started":"2022-04-17T01:37:02.516492Z","shell.execute_reply":"2022-04-17T01:37:02.545164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# catch22 features\n\nThese are 22 features with acronym for [CAnonical Time-series CHaracteristics](https://github.com/chlubba/catch22).","metadata":{}},{"cell_type":"code","source":"pip install catch22","metadata":{"execution":{"iopub.status.busy":"2022-04-17T01:37:27.408014Z","iopub.execute_input":"2022-04-17T01:37:27.409075Z","iopub.status.idle":"2022-04-17T01:37:44.987084Z","shell.execute_reply.started":"2022-04-17T01:37:27.409024Z","shell.execute_reply":"2022-04-17T01:37:44.985851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import catch22\n\ndef catch22_seq(x):\n    features = []\n    for i in range(13):\n        sensor = 'sensor_{:02d}'.format(i)\n        ts = x[['step',sensor]].sort_values(by='step')[sensor].to_numpy()\n        features.append(catch22.catch22_all(ts)['values'])\n    return np.concatenate(features)        ","metadata":{"execution":{"iopub.status.busy":"2022-04-17T01:37:55.545598Z","iopub.execute_input":"2022-04-17T01:37:55.545901Z","iopub.status.idle":"2022-04-17T01:37:55.558693Z","shell.execute_reply.started":"2022-04-17T01:37:55.545868Z","shell.execute_reply":"2022-04-17T01:37:55.557381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run on a dummy time series to get the names of the features\ncatch22_names = catch22.catch22_all([0]*60)['names']\ncatch22_names","metadata":{"execution":{"iopub.status.busy":"2022-04-17T01:38:03.329293Z","iopub.execute_input":"2022-04-17T01:38:03.329668Z","iopub.status.idle":"2022-04-17T01:38:03.338746Z","shell.execute_reply.started":"2022-04-17T01:38:03.329629Z","shell.execute_reply":"2022-04-17T01:38:03.337579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# tsfresh features\n\nMost of our features will come from the [tsfresh](https://tsfresh.readthedocs.io/en/latest/) package. The main challenge is to avoid running out of memory. Certain features alone (e.g., cwt) would generate hundreds of features *per sensor*. Our solution is to call the extraction module separately for certain voluminous features and optimize memory usage of the output right away.","metadata":{}},{"cell_type":"code","source":"# settings for the bulk of the tsfresh features (sans cwt and ar)\ntsfresh_default_settings = {'abs_energy': None,\n 'absolute_maximum': None,\n 'absolute_sum_of_changes': None,\n 'agg_autocorrelation': [{'f_agg': 'mean', 'maxlag': 10}, {'f_agg': 'median', 'maxlag': 10}, {'f_agg': 'var', 'maxlag': 10}, {'f_agg': 'ptp', 'maxlag': 10}],\n 'augmented_dickey_fuller': [{'attr': 'teststat'}, {'attr': 'pvalue'}, {'attr': 'usedlag'}],\n 'autocorrelation': [{'lag': 0}, {'lag': 1}, {'lag': 2}, {'lag': 3}, {'lag': 4}, {'lag': 5}, {'lag': 6}, {'lag': 7}, {'lag': 8}, {'lag': 9}],\n 'benford_correlation': None,\n 'c3': [{'lag': 1}, {'lag': 2}, {'lag': 3}],\n 'binned_entropy': [{'max_bins': 10}],\n 'cid_ce': [{'normalize': True}, {'normalize': False}], \n 'fft_aggregated': [{'aggtype': 'centroid'}, {'aggtype': 'variance'}, {'aggtype': 'skew'}, {'aggtype': 'kurtosis'}],\n 'fourier_entropy': [{'bins': 2}, {'bins': 3}, {'bins': 5}, {'bins': 10}, {'bins': 100}],  \n 'index_mass_quantile': [{'q': 0.1}, {'q': 0.2}, {'q': 0.3}, {'q': 0.4}, {'q': 0.6}, {'q': 0.7}, {'q': 0.8}, {'q': 0.9}],\n 'kurtosis': None,\n 'maximum': None,\n 'mean': None,\n 'mean_abs_change':None,\n 'mean_second_derivative_central': None,\n 'minimum':None,\n 'number_crossing_m': [{'m': 0}],\n 'number_cwt_peaks': [{'n': 1}, {'n': 5}],\n 'permutation_entropy': [{'tau': 1, 'dimension': 3}, {'tau': 1, 'dimension': 4}, {'tau': 1, 'dimension': 5}, {'tau': 1, 'dimension': 6}, {'tau': 1, 'dimension': 7}],\n 'partial_autocorrelation': [{'lag': 0}, {'lag': 1}, {'lag': 2}, {'lag': 3}, {'lag': 4}, {'lag': 5}, {'lag': 6}, {'lag': 7}, {'lag': 8}, {'lag': 9}],\n 'quantile': [{'q': 0.1}, {'q': 0.3}, {'q': 0.5}, {'q': 0.7}, {'q': 0.9}],\n 'sample_entropy':None,\n 'skewness':None,\n 'spkt_welch_density': [{'coeff': 2}, {'coeff': 5}, {'coeff': 8}],\n 'time_reversal_asymmetry_statistic': [{'lag': 1}, {'lag': 2}, {'lag': 3}], 'c3': [{'lag': 1}, {'lag': 2}, {'lag': 3}],\n 'variance':None,\n 'variation_coefficient': None}","metadata":{"execution":{"iopub.status.busy":"2022-04-17T01:39:32.264708Z","iopub.execute_input":"2022-04-17T01:39:32.26507Z","iopub.status.idle":"2022-04-17T01:39:32.283919Z","shell.execute_reply.started":"2022-04-17T01:39:32.265035Z","shell.execute_reply":"2022-04-17T01:39:32.282825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tsfresh.feature_extraction.extraction import extract_features\nfrom tsfresh.utilities.dataframe_functions import impute\n\ndef optimize_memory(df):\n    floats = df.select_dtypes(include=['float']).columns.tolist()\n    df[floats] = df[floats].apply(pd.to_numeric, downcast='float')\n    ints = df.select_dtypes(include=['int']).columns.tolist()\n    df[ints] = df[ints].apply(pd.to_numeric, downcast='integer')\n    return df\n\ndef make_features(data):\n    X=data[['sequence','subject']].groupby('sequence').mean()\n    X['subject']=X['subject'].astype('int')\n    X=X.reset_index()    \n    # tsfresh features\n    settings = {'cwt_coefficients': [{'widths': (2, 5, 10, 20), 'coeff': i, 'w': j} for i in range(0,60,3) for j in [2,5,10, 20]]}\n    X = X.join(optimize_memory(extract_features(data.drop(['subject'],axis=1),column_id='sequence',column_sort='step',default_fc_parameters=settings,\n                                n_jobs=1,impute_function=impute)), on=['sequence'])\n    settings = {'ar_coefficient': [{'coeff': i, 'k': 10} for i in range(11)]}\n    X = X.join(optimize_memory(extract_features(data.drop(['subject'],axis=1),column_id='sequence',column_sort='step',default_fc_parameters=settings,\n                                n_jobs=1,impute_function=impute)), on=['sequence'])\n    features_df = optimize_memory(extract_features(data.drop(['subject'],axis=1),column_id='sequence',column_sort='step',default_fc_parameters=tsfresh_default_settings,\n                                n_jobs=4,impute_function=impute))\n    features_df = features_df.reindex(columns=sorted(features_df.columns)) # multi-threaded extraction may result in random ordering of columns\n    X = X.join(features_df, on=['sequence'])\n    del features_df\n    # subject frequency\n    subjects, counts = np.unique(data.subject,return_counts=True)\n    X=X.join(pd.DataFrame(counts.reshape((-1,1)),columns=['subject_frequency'],index=subjects),on=['subject'])\n    # catch22 features\n    from tqdm import tqdm\n    sequences = data.sequence.unique()\n    rows = []\n    for seq in tqdm(sequences,desc='Catch22 Extraction'):\n        rows.append(catch22_seq(data[data.sequence==seq]))\n    X = X.join(optimize_memory(\n        pd.DataFrame(np.array(rows),columns=['sensor_{0:02d}__c22_{1}'.format(i,catch22_names[j]) for i in range(13) for j in range(22)],index=sequences)),\n               on=['sequence'])\n    \n    return X","metadata":{"execution":{"iopub.status.busy":"2022-04-17T01:40:35.723539Z","iopub.execute_input":"2022-04-17T01:40:35.7239Z","iopub.status.idle":"2022-04-17T01:40:38.755333Z","shell.execute_reply.started":"2022-04-17T01:40:35.723865Z","shell.execute_reply":"2022-04-17T01:40:38.75409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's run it on the training data. It would take several hours.","metadata":{}},{"cell_type":"code","source":"%%time\n\nX = make_features(train_data)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T01:40:43.674148Z","iopub.execute_input":"2022-04-17T01:40:43.675016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"execution":{"iopub.status.busy":"2022-04-13T16:30:58.250717Z","iopub.execute_input":"2022-04-13T16:30:58.251062Z","iopub.status.idle":"2022-04-13T16:30:59.218297Z","shell.execute_reply.started":"2022-04-13T16:30:58.251018Z","shell.execute_reply":"2022-04-13T16:30:59.217311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature relevance\n\nSome classification algorithms work better when unimportant features are removed. There are different algorithms for feature selection. `tsfresh` provides its own feature selection module that calculates \"relevance\" based on statistical hypothesis testing. Let's give it a try.\n\nOne caveat is that some catch22 features contain `NaN`. For the purpose of calculating relevance (and later on, mutual information), we fill these missing values with a dummy value `1e9`. We leave the `NaN`s alone when we export the data to files.","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom tsfresh.feature_selection.relevance import calculate_relevance_table\n\nrtable = calculate_relevance_table(X.fillna(1e9).drop(['subject'],axis=1).sort_values(by='sequence').set_index('sequence'),\n                                   labels.sort_values(by='sequence')['state'],\n                                  ml_task='classification',n_jobs=4)\nrtable.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the relevance table, about 50% of the features are relevant.","metadata":{}},{"cell_type":"code","source":"rtable.relevant.mean()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"relevant_cols = [c for c in X.drop(['sequence','subject'],axis=1).columns if rtable.set_index('feature').loc[c].relevant]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature importance by mutual information\n\nWe briefly evaluate feature importance using the statistical notion of mutual information (MI). It captures relationship between a single feature and the target variable. It does not address if the features are mutually dependent or not. \n\nHere are the top 250 features according to MI.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif\n\nN = 250\nfeatures = X.drop(['sequence','subject'],axis=1).columns\ntopN = pd.Series(mutual_info_classif(X.drop(['sequence','subject'],axis=1).fillna(1e9),\n                                 labels.state,discrete_features=False,random_state=42),\n             index=features).sort_values(ascending=False).head(N)\ntopN","metadata":{"execution":{"iopub.status.busy":"2022-04-13T16:35:39.314146Z","iopub.execute_input":"2022-04-13T16:35:39.314471Z","iopub.status.idle":"2022-04-13T16:37:33.773245Z","shell.execute_reply.started":"2022-04-13T16:35:39.314436Z","shell.execute_reply":"2022-04-13T16:37:33.772564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"More than 75% of the top 250 survives the `tsfresh` relevance test.","metadata":{}},{"cell_type":"code","source":"len([f for f in topN.index if f in relevant_cols])/N","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving to files","metadata":{}},{"cell_type":"markdown","source":"OK time to save the full feature set and the reduced set to files for later use.","metadata":{}},{"cell_type":"code","source":"X.to_csv('tps042022_train.csv',index=False)\nX[['sequence','subject']+relevant_cols].to_csv('tps042022_train_r.csv',index=False)\ndel X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, generate features for the test data.","metadata":{}},{"cell_type":"code","source":"%%time\n\nX_test = make_features(test_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.to_csv('tps042022_test.csv',index=False)\nX_test[['sequence','subject']+relevant_cols].to_csv('tps042022_test_r.csv',index=False)","metadata":{},"execution_count":null,"outputs":[]}]}