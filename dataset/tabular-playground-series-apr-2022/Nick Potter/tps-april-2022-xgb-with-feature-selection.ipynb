{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# XGBoost ðŸš€ with feature engineering and feature selection.\n\nMy approach to this dataset was:\n\n- Forget about the time-series structure (i.e. the `step` variable) and let the algorithm hopefully find some structure in the data\n- Use the subject as a group for cross-validation model selection (hyperparameter tuning)\n- Use XGBoost (I originally considered a neural network but XGBoost came out better)\n- Pivot the `step` data into a column variable as we are interested in prediction the `state` for each `sequence`, rather than `sequence`-`step` combinations\n- Aggregation of sensor data across \n\nI take advantage of the GPU accelerator resources available to us using the option `tree_method='gpu_hist'` in the XGBoost constructor.\n\nUseful notebooks:\n\nhttps://www.kaggle.com/code/cv13j0/tps-apr-2022-xgboost-model\n\nhttps://www.kaggle.com/code/hasanbasriakcay/tpsapr22-fe-pseudo-labels-bi-lstm\n\nhttps://www.kaggle.com/code/tyrionlannisterlzy/xgboost-dnn-ensemble-lb-0-980\n\nhttps://www.kaggle.com/code/ambrosm/tpsapr22-eda-which-makes-sense\n\nhttps://www.kaggle.com/competitions/tabular-playground-series-apr-2022/discussion/318527\n\nThanks everyone who entered this competition and shared notebooks and ideas - I continue to learn so much here at kaggle.","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# PARAMETERS\n\n\nN_FEATURES = 200\nN_ESTIMATORS = 500\n\n        ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-29T14:22:15.542275Z","iopub.execute_input":"2022-04-29T14:22:15.542806Z","iopub.status.idle":"2022-04-29T14:22:15.556777Z","shell.execute_reply.started":"2022-04-29T14:22:15.542719Z","shell.execute_reply":"2022-04-29T14:22:15.555973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data wrangling\n\n## Load data and pivot (put step variable as a column combined with the sensor)","metadata":{}},{"cell_type":"code","source":"%%time \n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom scipy.stats import kurtosis as kurt\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nif 'train' not in locals():\n    print('loading data', end='...')\n    train = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2022/train.csv')\n    test = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2022/test.csv')\nprint('')\n\nntrain = train.shape[0]\ntrain_sequences = train['sequence']\ntest_sequences = test['sequence']\nboth = pd.concat([train, test])\n\nprint('pivoting data', end='...')\nboth_long = both.melt(id_vars = ['sequence','subject','step'])\nboth_long['step_sensor'] = both_long['step'].map(lambda x: 'step_%02d' % x) + '_' + both_long['variable']\nboth_wide = both_long.pivot(index   = ['sequence','subject'], \n                            columns = 'step_sensor',\n                            values  = 'value')\nboth_wide = both_wide.reset_index()\nprint('')\n\nmetrix = ['mean','max','min','var','median','skew',kurt]\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature engineering: \n\nAggregate by sequence/subject and subject, also introduce [subject count variable](https://www.kaggle.com/code/ambrosm/tpsapr22-eda-which-makes-sense).","metadata":{}},{"cell_type":"code","source":"\nprint('Aggregating sensor by subject and sequence', end='...')\nt1 = both.filter(regex='sensor_|subject|sequence', axis=1).\\\n    groupby(['sequence','subject']).\\\n    aggregate(metrix)\n# Flatten multiindex column names\nt1.columns = [\"subject_\"+\"_\".join(x) for x in t1.columns]\nprint('')\n\nprint('Aggregating sensor by subject only', end='...')\nt2 = both.filter(regex='sensor_|subject', axis=1).\\\n    groupby(['subject']).\\\n    aggregate(metrix)\nt2.columns = [\"_\".join(x) for x in t2.columns]\nprint('')\n\nprint('merging', end='...')\n#Merge\n\nboth_all = both_wide.merge(t1, right_index=True, left_on = ['sequence', 'subject'])\nboth_all = both_all.merge(t2, right_index = True, left_on = 'subject')\n\n# Now add subject count variable (# times subject appears in data)\ncount = both_all['subject'].value_counts().to_frame()\ncount = count.rename(columns={\"subject\": \"subject_count\"})\n\nboth_all = both_all.merge(count, left_on='subject', right_index=True)\n\nprint('')\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:22:15.560947Z","iopub.execute_input":"2022-04-29T14:22:15.561281Z","iopub.status.idle":"2022-04-29T14:24:56.56781Z","shell.execute_reply.started":"2022-04-29T14:22:15.561241Z","shell.execute_reply":"2022-04-29T14:24:56.567211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Separate the data making it stays in the correct order\n\nI'm pretty sure pivoting the data reorders the subjects and sequences so I merge the data back with the retained sequences from test and training data. This ensures that I properly separate the sequences from the training and test datasets.","metadata":{}},{"cell_type":"code","source":"\nnewtrain = pd.DataFrame(np.unique(train_sequences)).merge(both_all, left_on=0, right_on='sequence')\nnewtrain = newtrain.drop(0, axis=1)\n\nnewtest = pd.DataFrame(np.unique(test_sequences)).merge(both_all, left_on=0, right_on='sequence')\nnewtest = newtest.drop(0, axis=1)\n\nlabels = pd.read_csv(\"/kaggle/input/tabular-playground-series-apr-2022/train_labels.csv\")\n\nnewtrain_with_labels = newtrain.merge(labels, how = 'left', on = 'sequence')\ntrain_sub = newtrain_with_labels['subject']\ntrain_seq = newtrain_with_labels['sequence']\nytrain = newtrain_with_labels['state']\nXtrain = newtrain_with_labels.drop(['subject','sequence','state'], axis=1)\n\ntest_seq = newtest['sequence']\nXtest = newtest.drop(['subject', 'sequence'], axis=1)\n\nXtrain.head()\nXtrain.shape\nXtest.head()\nXtest.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature selection\n\nReducing the number of variables (columns) means the fitting and hyperparameter tuning is quicker. Also, [AmbrosM](https://www.kaggle.com/competitions/tabular-playground-series-apr-2022/discussion/318527) claims too many variables passed into the model reduces model performance. I use `SelectKBest` with `f_classif` which just chooses variables using an ANOVA test (i.e. the largest between-state variable differences). There's probably (definitely) more sophisticated ways to select variables but I ran out of time to explore the differences between methods.","metadata":{}},{"cell_type":"code","source":"%%time\nprint(f'Reduce {Xtrain.shape[1]:d} columns down to {N_FEATURES:d}')\n# Select (filter) important columns\n\nfrom sklearn.metrics import roc_auc_score, make_scorer\nfrom sklearn.feature_selection import SequentialFeatureSelector, SelectKBest,f_classif\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.model_selection import GroupKFold\n\nfilter_columns = True\nif filter_columns:\n    import time\n    tt= time.time()\n    skb = SelectKBest(score_func = f_classif,\n                      k = N_FEATURES)\n    skb.fit(Xtrain, ytrain)\n    sum(skb.get_support())\n    #print(num_cols[sfs.support_])\n    print(time.time()-tt)\n    print(skb)\n    dir(skb)\n    print(len(skb.get_support()))\n    print(Xtrain.shape)\n    support = skb.get_support()\nelse:\n    support = [True for _ in range(Xtrain.shape[1])]\n\n\nprint('Selected variables:')\nprint(Xtrain.columns[support])\n    \n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fit an XGB model\n\nI use GridSearchCV to tune a few hyperparameters of the XGBoost model. Cross-validation proceeds with [GroupKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html) using `subject` as the group. This makes sense as sensor and state information should be similar for the same subject so we want [subjects split evenly across cross-validation folds](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#visualize-cross-validation-indices-for-many-cv-objects).","metadata":{}},{"cell_type":"code","source":"%%time \n# Fit an XGB model\n\nfrom sklearn.model_selection import GridSearchCV, GroupKFold\n\n\ncv = GroupKFold(n_splits = 3) # Subject is used as group, passed through the fit method below...\n\nfrom xgboost import XGBClassifier\n\nxgbc = XGBClassifier(n_estimators = N_ESTIMATORS, eval_metric='rmse', tree_method='gpu_hist',\n                    use_label_encoder=False)\ngscv = GridSearchCV(estimator = xgbc,\n                    param_grid = {'eta': [0,0.1,0.2,.3,.5],\n                                 'max_depth': [3,6,9],\n                                 'gamma': [0,.5,1,1.5,2]},\n                    scoring = make_scorer(roc_auc_score),\n                    cv = cv,\n                    n_jobs = -1, verbose = 1, refit=True)\n\n\ngscv.fit(Xtrain.loc[:,support], ytrain,\n         groups=train_sub)\nprint(gscv.best_estimator_)\nprint(gscv.best_score_)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:24:57.630232Z","iopub.execute_input":"2022-04-29T14:24:57.630486Z","iopub.status.idle":"2022-04-29T14:25:00.179003Z","shell.execute_reply.started":"2022-04-29T14:24:57.630455Z","shell.execute_reply":"2022-04-29T14:25:00.178152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make predictions","metadata":{}},{"cell_type":"code","source":"\nypred = gscv.best_estimator_.predict(Xtest.loc[:,support])","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:25:00.180278Z","iopub.execute_input":"2022-04-29T14:25:00.18051Z","iopub.status.idle":"2022-04-29T14:25:00.209913Z","shell.execute_reply.started":"2022-04-29T14:25:00.18048Z","shell.execute_reply":"2022-04-29T14:25:00.209267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission file:","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({'sequence':newtest['sequence'], 'state': [int(x) for x in ypred]})\n\nsubmission\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:30:19.825359Z","iopub.execute_input":"2022-04-29T14:30:19.825658Z","iopub.status.idle":"2022-04-29T14:30:19.843079Z","shell.execute_reply.started":"2022-04-29T14:30:19.825629Z","shell.execute_reply":"2022-04-29T14:30:19.842402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check feature importance\n\nFitted XGBoost models provide the `feature_importances` attribute. [Tyrion Lannister-lzy](https://www.kaggle.com/code/tyrionlannisterlzy/xgboost-dnn-ensemble-lb-0-980) provided a nice function that plots these easily so I borrowed/stole this (thanks Tyrion!).","metadata":{}},{"cell_type":"code","source":"# Function courtesy of Tyrion Lannister-lzy:\n# https://www.kaggle.com/code/tyrionlannisterlzy/xgboost-dnn-ensemble-lb-0-980\n\ndef plot_feature_importance(importance, names, model_type, max_features = 10):\n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n    fi_df = fi_df.head(max_features)\n\n    #Define size of bar plot\n    plt.figure(figsize=(8,6))\n\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + 'FEATURE IMPORTANCE')\n    plt.xlabel('IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplot_feature_importance(gscv.best_estimator_.feature_importances_,Xtrain.loc[:,support].columns,\n                        'XG BOOST ', max_features = 25)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:25:00.302206Z","iopub.execute_input":"2022-04-29T14:25:00.302502Z","iopub.status.idle":"2022-04-29T14:25:00.78492Z","shell.execute_reply.started":"2022-04-29T14:25:00.302467Z","shell.execute_reply":"2022-04-29T14:25:00.783963Z"},"trusted":true},"execution_count":null,"outputs":[]}]}