{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Feature Engineering: Aggregation Functions\n\nI have initially set-up this notebook for another competition (optiver volatility forecasting). It also was usefull for exploring The google ventilator challenge. Most of the functions initially come from the time series manipulation package tsfresh (https://tsfresh.readthedocs.io/en/latest/) and personnal market finance education / experience (drawdowns, max over min). \n","metadata":{"papermill":{"duration":0.016481,"end_time":"2021-09-05T16:32:48.105499","exception":false,"start_time":"2021-09-05T16:32:48.089018","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import random\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n\nimport glob\nimport os\nimport gc\n#from joblib import Parallel, delayed\n\nDEBUG = False","metadata":{"papermill":{"duration":1.11019,"end_time":"2021-09-05T16:32:49.233491","exception":false,"start_time":"2021-09-05T16:32:48.123301","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-06T19:13:29.077628Z","iopub.execute_input":"2022-04-06T19:13:29.077936Z","iopub.status.idle":"2022-04-06T19:13:29.084011Z","shell.execute_reply.started":"2022-04-06T19:13:29.077894Z","shell.execute_reply":"2022-04-06T19:13:29.083155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-apr-2022/train.csv')\ntrain_labels = pd.read_csv('../input/tabular-playground-series-apr-2022/train_labels.csv')\ntest = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')\n\nif DEBUG:\n    train = train[train.sequence.isin(train.sequence.unique()[:100])]\n    train_labels = train_labels.iloc[:100]","metadata":{"execution":{"iopub.status.busy":"2022-04-06T19:13:29.20353Z","iopub.execute_input":"2022-04-06T19:13:29.204497Z","iopub.status.idle":"2022-04-06T19:13:36.979371Z","shell.execute_reply.started":"2022-04-06T19:13:29.204431Z","shell.execute_reply":"2022-04-06T19:13:36.978544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef median(x):\n    return np.median(x)\n\ndef mean(x):\n    return np.mean(x)\n\ndef length(x):\n    return len(x)\n\ndef standard_deviation(x):\n    return np.std(x)\n\ndef large_standard_deviation(x):\n    if (np.max(x)-np.min(x)) == 0:\n        return np.nan\n    else:\n        return np.std(x)/(np.max(x)-np.min(x))\n\ndef variation_coefficient(x):\n    mean = np.mean(x)\n    if mean != 0:\n        return np.std(x) / mean\n    else:\n        return np.nan\n\ndef variance_std_ratio(x):\n    y = np.var(x)\n    if y != 0:\n        return y/np.sqrt(y)\n    else:\n        return np.nan\n\ndef ratio_beyond_r_sigma(x, r):\n    if x.size == 0:\n        return np.nan\n    else:\n        return np.sum(np.abs(x - np.mean(x)) > r * np.asarray(np.std(x))) / x.size\n\ndef range_ratio(x):\n    mean_median_difference = np.abs(np.mean(x) - np.median(x))\n    max_min_difference = np.max(x) - np.min(x)\n    if max_min_difference == 0:\n        return np.nan\n    else:\n        return mean_median_difference / max_min_difference\n    \ndef has_duplicate_max(x):\n    return np.sum(x == np.max(x)) >= 2\n\ndef has_duplicate_min(x):\n    return np.sum(x == np.min(x)) >= 2\n\ndef has_duplicate(x):\n    return x.size != np.unique(x).size\n\ndef count_duplicate_max(x):\n    return np.sum(x == np.max(x))\n\ndef count_duplicate_min(x):\n    return np.sum(x == np.min(x))\n\ndef count_duplicate(x):\n    return x.size - np.unique(x).size\n\ndef sum_values(x):\n    if len(x) == 0:\n        return 0\n    return np.sum(x)\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\ndef realized_abs_skew(series):\n    return np.power(np.abs(np.sum(series**3)),1/3)\n\ndef realized_skew(series):\n    return np.sign(np.sum(series**3))*np.power(np.abs(np.sum(series**3)),1/3)\n\ndef realized_vol_skew(series):\n    return np.power(np.abs(np.sum(series**6)),1/6)\n\ndef realized_quarticity(series):\n    return np.power(np.sum(series**4),1/4)\n\ndef count_unique(series):\n    return len(np.unique(series))\n\ndef count(series):\n    return series.size\n\n#drawdons functions are mine\ndef maximum_drawdown(series):\n    series = np.asarray(series)\n    if len(series)<2:\n        return 0\n    k = series[np.argmax(np.maximum.accumulate(series) - series)]\n    i = np.argmax(np.maximum.accumulate(series) - series)\n    if len(series[:i])<1:\n        return np.NaN\n    else:\n        j = np.max(series[:i])\n    return j-k\n\ndef maximum_drawup(series):\n    series = np.asarray(series)\n    if len(series)<2:\n        return 0\n    \n\n    series = - series\n    k = series[np.argmax(np.maximum.accumulate(series) - series)]\n    i = np.argmax(np.maximum.accumulate(series) - series)\n    if len(series[:i])<1:\n        return np.NaN\n    else:\n        j = np.max(series[:i])\n    return j-k\n\ndef drawdown_duration(series):\n    series = np.asarray(series)\n    if len(series)<2:\n        return 0\n\n    k = np.argmax(np.maximum.accumulate(series) - series)\n    i = np.argmax(np.maximum.accumulate(series) - series)\n    if len(series[:i]) == 0:\n        j=k\n    else:\n        j = np.argmax(series[:i])\n    return k-j\n\ndef drawup_duration(series):\n    series = np.asarray(series)\n    if len(series)<2:\n        return 0\n\n    series=-series\n    k = np.argmax(np.maximum.accumulate(series) - series)\n    i = np.argmax(np.maximum.accumulate(series) - series)\n    if len(series[:i]) == 0:\n        j=k\n    else:\n        j = np.argmax(series[:i])\n    return k-j\n\ndef max_over_min(series):\n    if len(series)<2:\n        return 0\n    if np.min(series) == 0:\n        return np.nan\n    return np.max(series)/np.min(series)\n\ndef max_over_min_sq(series):\n    if len(series)<2:\n        return 0\n    if np.min(series) == 0:\n        return np.nan\n    return np.square(np.max(series)/np.min(series))\n\ndef mean_n_absolute_max(x, number_of_maxima = 1):\n    \"\"\" Calculates the arithmetic mean of the n absolute maximum values of the time series.\"\"\"\n    assert (\n        number_of_maxima > 0\n    ), f\" number_of_maxima={number_of_maxima} which is not greater than 1\"\n\n    n_absolute_maximum_values = np.sort(np.absolute(x))[-number_of_maxima:]\n\n    return np.mean(n_absolute_maximum_values) if len(x) > number_of_maxima else np.NaN\n\n\ndef count_above(x, t):\n    if len(x)==0:\n        return np.nan\n    else:\n        return np.sum(x >= t) / len(x)\n\ndef count_below(x, t):\n    if len(x)==0:\n        return np.nan\n    else:\n        return np.sum(x <= t) / len(x)\n\n#number of valleys = number_peaks(-x, n)\ndef number_peaks(x, n):\n    \"\"\"\n    Calculates the number of peaks of at least support n in the time series x. A peak of support n is defined as a\n    subsequence of x where a value occurs, which is bigger than its n neighbours to the left and to the right.\n    \"\"\"\n    x_reduced = x[n:-n]\n\n    res = None\n    for i in range(1, n + 1):\n        result_first = x_reduced > _roll(x, i)[n:-n]\n\n        if res is None:\n            res = result_first\n        else:\n            res &= result_first\n\n        res &= x_reduced > _roll(x, -i)[n:-n]\n    return np.sum(res)\n\ndef mean_abs_change(x):\n    return np.mean(np.abs(np.diff(x)))\n\ndef mean_change(x):\n    x = np.asarray(x)\n    return (x[-1] - x[0]) / (len(x) - 1) if len(x) > 1 else np.NaN\n\ndef mean_second_derivative_central(x):\n    x = np.asarray(x)\n    return (x[-1] - x[-2] - x[1] + x[0]) / (2 * (len(x) - 2)) if len(x) > 2 else np.NaN\n\n\ndef median(x):\n    return np.median(x)\n\ndef mean(x):\n    return np.mean(x)\n\ndef length(x):\n    return len(x)\n\ndef standard_deviation(x):\n    return np.std(x)\n\ndef variation_coefficient(x):\n    mean = np.mean(x)\n    if mean != 0:\n        return np.std(x) / mean\n    else:\n        return np.nan\n\ndef variance(x):\n    return np.var(x)\n\ndef skewness(x):\n    if not isinstance(x, pd.Series):\n        x = pd.Series(x)\n    return pd.Series.skew(x)\n\ndef kurtosis(x):\n    if not isinstance(x, pd.Series):\n        x = pd.Series(x)\n    return pd.Series.kurtosis(x)\n\ndef root_mean_square(x):\n    return np.sqrt(np.mean(np.square(x))) if len(x) > 0 else np.NaN\n\ndef absolute_sum_of_changes(x):\n    return np.sum(np.abs(np.diff(x)))\n\ndef longest_strike_below_mean(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.max(_get_length_sequences_where(x < np.mean(x))) if x.size > 0 else 0\n\ndef longest_strike_above_mean(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.max(_get_length_sequences_where(x > np.mean(x))) if x.size > 0 else 0\n\ndef count_above_mean(x):\n    m = np.mean(x)\n    return np.where(x > m)[0].size\n\ndef count_below_mean(x):\n    m = np.mean(x)\n    return np.where(x < m)[0].size\n\ndef last_location_of_maximum(x):\n    x = np.asarray(x)\n    return 1.0 - np.argmax(x[::-1]) / len(x) if len(x) > 0 else np.NaN\n\ndef first_location_of_maximum(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.argmax(x) / len(x) if len(x) > 0 else np.NaN\n\ndef last_location_of_minimum(x):\n    x = np.asarray(x)\n    return 1.0 - np.argmin(x[::-1]) / len(x) if len(x) > 0 else np.NaN\n\ndef first_location_of_minimum(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.argmin(x) / len(x) if len(x) > 0 else np.NaN\n\n# Test non-consecutive non-reoccuring values ?\ndef percentage_of_reoccurring_values_to_all_values(x):\n    if len(x) == 0:\n        return np.nan\n    unique, counts = np.unique(x, return_counts=True)\n    if counts.shape[0] == 0:\n        return 0\n    return np.sum(counts > 1) / float(counts.shape[0])\n\ndef percentage_of_reoccurring_datapoints_to_all_datapoints(x):\n    if len(x) == 0:\n        return np.nan\n    if not isinstance(x, pd.Series):\n        x = pd.Series(x)\n    value_counts = x.value_counts()\n    reoccuring_values = value_counts[value_counts > 1].sum()\n    if np.isnan(reoccuring_values):\n        return 0\n\n    return reoccuring_values / x.size\n\n\ndef sum_of_reoccurring_values(x):\n    unique, counts = np.unique(x, return_counts=True)\n    counts[counts < 2] = 0\n    counts[counts > 1] = 1\n    return np.sum(counts * unique)\n\ndef sum_of_reoccurring_data_points(x):\n    unique, counts = np.unique(x, return_counts=True)\n    counts[counts < 2] = 0\n    return np.sum(counts * unique)\n\ndef ratio_value_number_to_time_series_length(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    if x.size == 0:\n        return np.nan\n\n    return np.unique(x).size / x.size\n\ndef abs_energy(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.dot(x, x)\n\ndef quantile(x, q):\n    if len(x) == 0:\n        return np.NaN\n    return np.quantile(x, q)\n\n# crossing the mean ? other levels ? \ndef number_crossing_m(x, m):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    # From https://stackoverflow.com/questions/3843017/efficiently-detect-sign-changes-in-python\n    positive = x > m\n    return np.where(np.diff(positive))[0].size\n\ndef maximum(x):\n    return np.max(x)\n\ndef absolute_maximum(x):\n    return np.max(np.absolute(x)) if len(x) > 0 else np.NaN\n\ndef minimum(x):\n    return np.min(x)\n\ndef value_count(x, value):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    if np.isnan(value):\n        return np.isnan(x).sum()\n    else:\n        return x[x == value].size\n\ndef range_count(x, min, max):\n    return np.sum((x >= min) & (x < max))\n\n\ndef _roll(a, shift):\n    \"\"\" Roll 1D array elements. Improves the performance of numpy.roll()\"\"\"\n\n\n    if not isinstance(a, np.ndarray):\n        a = np.asarray(a)\n    idx = shift % len(a)\n    return np.concatenate([a[-idx:], a[:-idx]])\n\n\ndef _get_length_sequences_where(x):\n    \"\"\" This method calculates the length of all sub-sequences where the array x is either True or 1. \"\"\"\n    if len(x) == 0:\n        return [0]\n    else:\n        res = [len(list(group)) for value, group in itertools.groupby(x) if value == 1]\n        return res if len(res) > 0 else [0]\n\ndef _aggregate_on_chunks(x, f_agg, chunk_len):\n    \"\"\"Takes the time series x and constructs a lower sampled version of it by applying the aggregation function f_agg on\n    consecutive chunks of length chunk_len\"\"\"\n    \n    return [\n        getattr(x[i * chunk_len : (i + 1) * chunk_len], f_agg)()\n        for i in range(int(np.ceil(len(x) / chunk_len)))\n    ]\n\ndef _into_subchunks(x, subchunk_length, every_n=1):\n    \"\"\"Split the time series x into subwindows of length \"subchunk_length\", starting every \"every_n\".\"\"\"\n    len_x = len(x)\n\n    assert subchunk_length > 1\n    assert every_n > 0\n\n    # how often can we shift a window of size subchunk_length over the input?\n    num_shifts = (len_x - subchunk_length) // every_n + 1\n    shift_starts = every_n * np.arange(num_shifts)\n    indices = np.arange(subchunk_length)\n\n    indexer = np.expand_dims(indices, axis=0) + np.expand_dims(shift_starts, axis=1)\n    return np.asarray(x)[indexer]\n\n\ndef set_property(key, value):\n    \"\"\"\n    This method returns a decorator that sets the property key of the function to value\n    \"\"\"\n\n    def decorate_func(func):\n        setattr(func, key, value)\n        if func.__doc__ and key == \"fctype\":\n            func.__doc__ = (\n                func.__doc__ + \"\\n\\n    *This function is of type: \" + value + \"*\\n\"\n            )\n        return func\n\n    return decorate_func","metadata":{"papermill":{"duration":0.093138,"end_time":"2021-09-05T16:32:52.546848","exception":false,"start_time":"2021-09-05T16:32:52.45371","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-06T19:13:36.981972Z","iopub.execute_input":"2022-04-06T19:13:36.982312Z","iopub.status.idle":"2022-04-06T19:13:37.069108Z","shell.execute_reply.started":"2022-04-06T19:13:36.982283Z","shell.execute_reply":"2022-04-06T19:13:37.068223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lambda functions to facilitate application","metadata":{"papermill":{"duration":0.015317,"end_time":"2021-09-05T16:32:52.577833","exception":false,"start_time":"2021-09-05T16:32:52.562516","status":"completed"},"tags":[]}},{"cell_type":"code","source":"count_above_0 = lambda x: count_above(x,0)\ncount_above_0.__name__ = 'count_above_0'\n\ncount_below_0 = lambda x: count_below(x,0)\ncount_below_0.__name__ = 'count_below_0'\n\nvalue_count_0 = lambda x: value_count(x,0)\nvalue_count_0.__name__ = 'value_count_0'\n\ncount_near_0 = lambda x: range_count(x,-0.00001,0.00001)\ncount_near_0.__name__ = 'count_near_0_0'\n\nratio_beyond_01_sigma = lambda x: ratio_beyond_r_sigma(x,0.1)\nratio_beyond_01_sigma.__name__ = 'ratio_beyond_01_sigma'\n\nratio_beyond_02_sigma = lambda x: ratio_beyond_r_sigma(x,0.2)\nratio_beyond_02_sigma.__name__ = 'ratio_beyond_02_sigma'\n\nratio_beyond_03_sigma = lambda x: ratio_beyond_r_sigma(x,0.3)\nratio_beyond_03_sigma.__name__ = 'ratio_beyond_03_sigma'\n\nnumber_crossing_0 = lambda x: number_crossing_m(x,0)\nnumber_crossing_0.__name__ = 'number_crossing_0'\n\nquantile_01 = lambda x: quantile(x,0.1)\nquantile_01.__name__ = 'quantile_01'\n\nquantile_025 = lambda x: quantile(x,0.25)\nquantile_025.__name__ = 'quantile_025'\n\nquantile_075 = lambda x: quantile(x,0.75)\nquantile_075.__name__ = 'quantile_075'\n\nquantile_09 = lambda x: quantile(x,0.9)\nquantile_09.__name__ = 'quantile_09'\n\nnumber_peaks_2 = lambda x: number_peaks(x,2)\nnumber_peaks_2.__name__ = 'number_peaks_2'\n\nmean_n_absolute_max_2 = lambda x: mean_n_absolute_max(x,2)\nmean_n_absolute_max_2.__name__ = 'mean_n_absolute_max_2'\n\nnumber_peaks_5 = lambda x: number_peaks(x,5)\nnumber_peaks_5.__name__ = 'number_peaks_5'\n\nmean_n_absolute_max_5 = lambda x: mean_n_absolute_max(x,5)\nmean_n_absolute_max_5.__name__ = 'mean_n_absolute_max_5'\n\nnumber_peaks_10 = lambda x: number_peaks(x,10)\nnumber_peaks_10.__name__ = 'number_peaks_10'\n\nmean_n_absolute_max_10 = lambda x: mean_n_absolute_max(x,10)\nmean_n_absolute_max_10.__name__ = 'mean_n_absolute_max_10'\n\nget_first = lambda x: x.iloc[0]\nget_first.__name__ = 'get_first'\n\nget_last = lambda x: x.iloc[-1]\nget_last.__name__ = 'get_last'","metadata":{"papermill":{"duration":0.032107,"end_time":"2021-09-05T16:32:52.625491","exception":false,"start_time":"2021-09-05T16:32:52.593384","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-06T19:13:37.070314Z","iopub.execute_input":"2022-04-06T19:13:37.07073Z","iopub.status.idle":"2022-04-06T19:13:37.084231Z","shell.execute_reply.started":"2022-04-06T19:13:37.070686Z","shell.execute_reply":"2022-04-06T19:13:37.083532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_stats = [mean,sum,length,standard_deviation,variation_coefficient,variance,skewness,kurtosis]\nhigher_order_stats = [abs_energy,root_mean_square,sum_values,realized_volatility,realized_abs_skew,realized_skew,realized_vol_skew,realized_quarticity]\nmin_median_max = [minimum,median,maximum]\nadditional_quantiles = [quantile_01,quantile_025,quantile_075,quantile_09]\nother_min_max = [absolute_maximum,max_over_min,max_over_min_sq]\nmin_max_positions = [last_location_of_maximum,first_location_of_maximum,last_location_of_minimum,first_location_of_minimum]\npeaks = [number_peaks_2, mean_n_absolute_max_2, number_peaks_5, mean_n_absolute_max_5, number_peaks_10, mean_n_absolute_max_10]\ncounts = [count_unique,count,count_above_0,count_below_0,value_count_0,count_near_0]\nreoccuring_values = [count_above_mean,count_below_mean,percentage_of_reoccurring_values_to_all_values,percentage_of_reoccurring_datapoints_to_all_datapoints,sum_of_reoccurring_values,sum_of_reoccurring_data_points,ratio_value_number_to_time_series_length]\ncount_duplicate = [count_duplicate,count_duplicate_min,count_duplicate_max]\nvariations = [mean_abs_change,mean_change,mean_second_derivative_central,absolute_sum_of_changes,number_crossing_0]\nranges = [variance_std_ratio,ratio_beyond_01_sigma,ratio_beyond_02_sigma,ratio_beyond_03_sigma,large_standard_deviation,range_ratio]\nget_first = [get_first, get_last]\n\nall_functions = base_stats + higher_order_stats + min_median_max + additional_quantiles + other_min_max + min_max_positions + peaks + counts + variations + ranges \n\n# too slow\n#+ reoccuring_values + count_duplicate \n","metadata":{"papermill":{"duration":0.026553,"end_time":"2021-09-05T16:32:52.667871","exception":false,"start_time":"2021-09-05T16:32:52.641318","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-06T19:13:37.086143Z","iopub.execute_input":"2022-04-06T19:13:37.086472Z","iopub.status.idle":"2022-04-06T19:13:37.102075Z","shell.execute_reply.started":"2022-04-06T19:13:37.08644Z","shell.execute_reply":"2022-04-06T19:13:37.101264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#feature names\nfeatures = ['sensor_'+ str(i).zfill(2) for i in range(13)]\n\n# build aggregation dict\n#feature_dict = {'subject': count}\n\nfeature_dict = {}\n\nfor f in features:\n    feature_dict[f] =  all_functions","metadata":{"execution":{"iopub.status.busy":"2022-04-06T19:13:37.103316Z","iopub.execute_input":"2022-04-06T19:13:37.103683Z","iopub.status.idle":"2022-04-06T19:13:37.117257Z","shell.execute_reply.started":"2022-04-06T19:13:37.103651Z","shell.execute_reply":"2022-04-06T19:13:37.116536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndef prepare_df(df):\n    df_feat = df.groupby('sequence').agg(feature_dict)\n    df_feat.columns = ['_'.join(col) for col in df_feat.columns]\n    map_sequence_subject = df.groupby(['sequence']).subject.min()\n    map_subject_count = df.groupby(['sequence']).subject.min().value_counts()\n    df_feat['count_sequence'] = df_feat.index.map(map_sequence_subject.map(map_subject_count))\n    return df_feat\n","metadata":{"papermill":{"duration":317.553246,"end_time":"2021-09-05T16:38:12.005173","exception":false,"start_time":"2021-09-05T16:32:54.451927","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-06T19:13:37.118604Z","iopub.execute_input":"2022-04-06T19:13:37.118949Z","iopub.status.idle":"2022-04-06T19:13:37.132194Z","shell.execute_reply.started":"2022-04-06T19:13:37.118907Z","shell.execute_reply":"2022-04-06T19:13:37.131284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_feat = prepare_df(train)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T19:13:37.133855Z","iopub.execute_input":"2022-04-06T19:13:37.134442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntest_feat = prepare_df(test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_feat.to_parquet('train.parquet')\ntest_feat.to_parquet('test.parquet')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = lgb.LGBMClassifier()\nclf.fit(train_feat, train_labels.state)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# feature importance","metadata":{}},{"cell_type":"code","source":"# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,train_feat.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = clf.predict(test_feat)\n\nsubmission = pd.read_csv('../input/tabular-playground-series-apr-2022/sample_submission.csv')\nsubmission['state'] = prediction\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# to better look at individual features","metadata":{}},{"cell_type":"code","source":"# def test_constant(df):\n#     filter = ~(df!= df.iloc[0]).any()\n#     return filter[filter]\n\n# constants_col = test_constant(train_feat.fillna(0)).index\n\n# error_cols = []\n\n# sns.set(rc={'figure.figsize':(12,8)})\n# sns.set_style(style='white')\n\n\n\n# for col in train_feat.columns:\n#     if col not in constants_col:\n#         try:\n#             data = train_feat[col]\n#             target = train_labels.state\n\n#             n_quantile = 10\n#             quantiles = data.quantile(np.arange(11)/10)\n#             q_groups = np.digitize(data,quantiles)\n#             agg_data = pd.DataFrame({'data':data,'target':target,'group_quantile':q_groups})\n#             to_plot = agg_data.groupby('group_quantile').mean()\n\n#             sns.regplot(x=to_plot.data,y=to_plot.target,color=(random.random(), random.random(), random.random()), order = 2, line_kws={\"color\": 'black'})\n#             plt.ylim(0, 1)\n#             plt.title(col+' v.s. target',size=20)\n#             plt.show()\n\n#         except:\n#             error_cols.append(col)\n#             pass\n\n# error_cols","metadata":{"papermill":{"duration":178.155277,"end_time":"2021-09-05T16:41:10.177131","exception":false,"start_time":"2021-09-05T16:38:12.021854","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}