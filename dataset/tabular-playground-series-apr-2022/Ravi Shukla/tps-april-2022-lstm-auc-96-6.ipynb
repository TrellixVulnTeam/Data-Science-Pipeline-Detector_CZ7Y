{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TPS April 2022 \n\n- Hello Kagglers, in this notebook I have used DNN/LSTM based architecture to get good performing model on public LeaderBoard.\n\n- This notebook is inspired from the work of DMITRY UAROV \nhttps://www.kaggle.com/code/dmitryuarov/tps-sensors-auc-0-964\n\n- The architecture proposed by DMITRY UAROV was overfitting so I tried to take measures to reduce overfitting in this notebook.\n\n- Please `upvote` this notebook if you find it useful","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"}},{"cell_type":"markdown","source":"### Data Descriptions\nIn this competition, you'll classify 60-second sequences of sensor data, indicating whether a subject was in either of two activity states for the duration of the sequence\n\n### Files and Field Descriptions\ntrain.csv - the training set, comprising ~26,000 60-second recordings of thirteen biological sensors for almost one thousand experimental participants\n* sequence - a unique id for each sequence\n* subject - a unique id for the subject in the experiment\n* step - time step of the recording, in one second intervals\n* sensor_00 - sensor_12 - the value for each of the thirteen sensors at that time step\n* train_labels.csv - the class label for each sequence.\n* sequence - the unique id for each sequence.\n* state - the state associated to each sequence. This is the target which you are trying to predict.\n\ntest.csv - the test set. For each of the ~12,000 sequences, you should predict a value for that sequence's state.\n\nsample_submission.csv - a sample submission file in the correct format.`","metadata":{}},{"cell_type":"markdown","source":"# Install Libraries","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:03:03.342546Z","iopub.execute_input":"2022-04-04T11:03:03.342889Z","iopub.status.idle":"2022-04-04T11:03:03.360852Z","shell.execute_reply.started":"2022-04-04T11:03:03.342811Z","shell.execute_reply":"2022-04-04T11:03:03.359869Z"}}},{"cell_type":"code","source":"! pip install pydot\n! pip install graphviz","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:04:12.261483Z","iopub.execute_input":"2022-04-04T11:04:12.262168Z","iopub.status.idle":"2022-04-04T11:04:28.224931Z","shell.execute_reply.started":"2022-04-04T11:04:12.262104Z","shell.execute_reply":"2022-04-04T11:04:28.224009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport plotly as py\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected = True)\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import KFold, GroupKFold\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import GlobalMaxPooling1D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.layers import Concatenate, LSTM, GRU\nfrom tensorflow.keras.layers import Bidirectional, Multiply\n\nnp.random.seed(2022)\ntf.random.set_seed(2022)\n\npd.set_option('display.max_columns', None)\n#########################################################\n\ntrain = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2022/train.csv')\nt_lbls = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2022/train_labels.csv')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2022/test.csv')\n\nss = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2022/sample_submission.csv')\n","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:04:28.226715Z","iopub.execute_input":"2022-04-04T11:04:28.226933Z","iopub.status.idle":"2022-04-04T11:04:47.633516Z","shell.execute_reply.started":"2022-04-04T11:04:28.226906Z","shell.execute_reply":"2022-04-04T11:04:47.632809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"train.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:04:47.635058Z","iopub.execute_input":"2022-04-04T11:04:47.635373Z","iopub.status.idle":"2022-04-04T11:04:47.663979Z","shell.execute_reply.started":"2022-04-04T11:04:47.635334Z","shell.execute_reply":"2022-04-04T11:04:47.663392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('DATA INFORMATION')\nprint()\nprint('Count of sequences:')\nprint(f'train - {int(len(train)/60)} | test - {int(len(test)/60)}')\nprint()\nprint('Missing values:')\nprint(f'train - {train.isna().sum().sum()} | test - {test.isna().sum().sum()}')\nprint()\nprint('Distribution of target:')\nprint(f'\"1\" - {round(t_lbls[\"state\"].value_counts()[1]/len(t_lbls)*100,2)}% | \"0\" - {round(t_lbls[\"state\"].value_counts()[0]/len(t_lbls)*100,2)}%')\nprint()\nprint('-'*39)\nprint()\nprint('Train features')\ndisplay(train[train.columns.tolist()[3:]].describe().transpose()[['mean', 'min', 'max']]\\\n.style.background_gradient(cmap='Blues'))\nprint()\nprint('-'*39)\nprint()\nprint('Test features')\ndisplay(test[test.columns.tolist()[3:]].describe().transpose()[['mean', 'min', 'max']]\\\n.style.background_gradient(cmap='Blues'))","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-04-04T11:04:47.665699Z","iopub.execute_input":"2022-04-04T11:04:47.665938Z","iopub.status.idle":"2022-04-04T11:04:48.983851Z","shell.execute_reply.started":"2022-04-04T11:04:47.665914Z","shell.execute_reply":"2022-04-04T11:04:48.983095Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"features = train.columns.tolist()[3:]\ndef prep(df):\n    for feature in features:\n        df[feature + '_lag1'] = df.groupby('sequence')[feature].shift(1)\n        df.fillna(0, inplace=True)\n        df[feature + '_diff1'] = df[feature] - df[feature + '_lag1']    \n\nprep(train)\nprep(test)\n\nfeatures = train.columns.tolist()[3:]\nsc = StandardScaler()\ntrain[features] = sc.fit_transform(train[features])\ntest[features] = sc.transform(test[features])\n\ngroups = train[\"sequence\"]\nlabels = t_lbls[\"state\"]\n\ntrain = train.drop([\"sequence\", \"subject\", \"step\"], axis=1).values\ntrain = train.reshape(-1, 60, train.shape[-1])\n\ntest = test.drop([\"sequence\", \"subject\", \"step\"], axis=1).values\ntest = test.reshape(-1, 60, test.shape[-1])","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:04:48.985032Z","iopub.execute_input":"2022-04-04T11:04:48.985339Z","iopub.status.idle":"2022-04-04T11:04:58.075416Z","shell.execute_reply.started":"2022-04-04T11:04:48.985312Z","shell.execute_reply":"2022-04-04T11:04:58.074669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DNN model","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    BATCH_SIZE = tpu_strategy.num_replicas_in_sync * 64\n    print(\"Running on TPU:\", tpu.master())\n    print(f\"Batch Size: {BATCH_SIZE}\")\n    \nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\n    BATCH_SIZE = 256\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    print(f\"Batch Size: {BATCH_SIZE}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:04:58.07655Z","iopub.execute_input":"2022-04-04T11:04:58.07676Z","iopub.status.idle":"2022-04-04T11:05:04.106704Z","shell.execute_reply.started":"2022-04-04T11:04:58.076736Z","shell.execute_reply":"2022-04-04T11:05:04.105823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dnn_model():\n    \n    x_input = Input(shape=(train.shape[-2:]))\n    \n    x1 = Bidirectional(LSTM(units=512, return_sequences=True))(x_input)\n    x1_dr = Dropout(rate = 0.2)(x1)\n    x2 = Bidirectional(LSTM(units=256, return_sequences=True))(x1_dr)\n    z1 = Bidirectional(GRU(units=256, return_sequences=True))(x1_dr)\n    \n    c = Concatenate(axis=2)([x2, z1])\n    c_dr = Dropout(rate = 0.2)(c)\n    b = BatchNormalization()(c_dr)\n    \n    x3 = Bidirectional(LSTM(units=128, return_sequences=True))(b)\n    x3_dr = Dropout(rate = 0.2)(x3)\n    \n    x4 = GlobalMaxPooling1D()(x3_dr)\n    x5 = Dense(units=128, activation='selu')(x4)\n    x6 = Dropout(rate = 0.2)(x5)\n    x_output = Dense(1, activation='sigmoid')(x6)\n\n    model = Model(inputs=x_input, outputs=x_output, name='lstm_model')\n    \n    return model\n\nmodel = dnn_model()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:05:04.107854Z","iopub.execute_input":"2022-04-04T11:05:04.108136Z","iopub.status.idle":"2022-04-04T11:05:06.303212Z","shell.execute_reply.started":"2022-04-04T11:05:04.108106Z","shell.execute_reply":"2022-04-04T11:05:06.302493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(\n    model, \n    to_file='Super_Model.png', \n    show_shapes=True,\n    show_layer_names=True\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-04T11:05:06.304197Z","iopub.execute_input":"2022-04-04T11:05:06.30441Z","iopub.status.idle":"2022-04-04T11:05:07.273966Z","shell.execute_reply.started":"2022-04-04T11:05:06.304386Z","shell.execute_reply":"2022-04-04T11:05:07.273104Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotHist(hist):\n    plt.plot(hist.history[\"auc\"])\n    plt.plot(hist.history[\"val_auc\"])\n    plt.title(\"model performance\")\n    plt.ylabel(\"area_under_curve\")\n    plt.xlabel(\"epoch\")\n    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:05:07.275539Z","iopub.execute_input":"2022-04-04T11:05:07.275757Z","iopub.status.idle":"2022-04-04T11:05:07.281064Z","shell.execute_reply.started":"2022-04-04T11:05:07.275731Z","shell.execute_reply":"2022-04-04T11:05:07.280333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    VERBOSE = True\n    predictions, scores = [], []\n    k = GroupKFold(n_splits = 10)\n\n    for fold, (train_idx, val_idx) in enumerate(k.split(train, labels, groups.unique())):\n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n    \n        X_train, X_val = train[train_idx], train[val_idx]\n        y_train, y_val = labels.iloc[train_idx].values, labels.iloc[val_idx].values\n        \n        model = dnn_model()\n        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics='AUC')\n\n        lr = ReduceLROnPlateau(monitor=\"val_auc\", factor=0.6, \n                               patience=4, verbose=VERBOSE)\n\n        es = EarlyStopping(monitor=\"val_auc\", patience=7, \n                           verbose=VERBOSE, mode=\"max\", \n                           restore_best_weights=True)\n        \n        save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n        chk_point = ModelCheckpoint(f'./TPS_model_2022_{fold+1}C.h5', options=save_locally, \n                                    monitor='val_auc', verbose=VERBOSE, \n                                    save_best_only=True, mode='max')\n        \n        history = model.fit(X_train, y_train, \n                  validation_data=(X_val, y_val), \n                  epochs=15,\n                  verbose=VERBOSE,\n                  batch_size=BATCH_SIZE, \n                  callbacks=[lr, chk_point, es])\n        \n        load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n        model = load_model(f'./TPS_model_2022_{fold+1}C.h5', options=load_locally)\n        \n        y_pred = model.predict(X_val, batch_size=BATCH_SIZE).squeeze()\n        score = roc_auc_score(y_val, y_pred)\n        scores.append(score)\n        predictions.append(model.predict(test, batch_size=BATCH_SIZE).squeeze())\n        \n        print(f\"Fold-{fold+1} | OOF Score: {score}\")\n        plotHist(history)\n    \n    print(f'Mean accuracy on {k.n_splits} folds - {np.mean(scores)}')","metadata":{"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2022-04-04T11:05:35.083191Z","iopub.execute_input":"2022-04-04T11:05:35.083693Z","iopub.status.idle":"2022-04-04T11:23:43.83751Z","shell.execute_reply.started":"2022-04-04T11:05:35.083663Z","shell.execute_reply":"2022-04-04T11:23:43.836866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss[\"state\"] = sum(predictions)/k.n_splits \nss.to_csv('submission.csv', index=False)\nss  ","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:23:43.838821Z","iopub.execute_input":"2022-04-04T11:23:43.839558Z","iopub.status.idle":"2022-04-04T11:23:43.894007Z","shell.execute_reply.started":"2022-04-04T11:23:43.839524Z","shell.execute_reply":"2022-04-04T11:23:43.893208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}