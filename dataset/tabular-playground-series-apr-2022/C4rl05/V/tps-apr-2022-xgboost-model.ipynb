{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# XGBoost is all you need, Maybe...\nHello Kaggle, in this Notebook I will try to push the limits of XGBoost, let see how I will do...\nI will take a lot of inspiration from this Book, basically as I read the book I will implement all the tips in this notebook...\n\n**Hands-On Gradient Boosting with XGBoost and scikit-learn: Perform accessible machine learning and extreme gradient boosting with Python**\n\nhttps://www.oreilly.com/library/view/hands-on-gradient-boosting/9781839218354/\n\n### Notebook Plan\n**Objective:**\nBuild a powerfull XGBoost Model that can provide a good estimation.\n\n**Strategy:**\nI think I will follow this strategy:\n* Contruct aggregated features, Min, Max. Mad, Var, Sum and Others. I will need to identify the best grouping strategy to create this features. -- Completed\n* Contruct lag features -- Working On\n* Contruct rolling features -- Working On\n* Contruct Expanding features -- Working On\n* Other ideas that I'm not sure at this point -- Researching\n\n\n---","metadata":{}},{"cell_type":"markdown","source":"### Data Descriptions\nIn this competition, you'll classify 60-second sequences of sensor data, indicating whether a subject was in either of two activity states for the duration of the sequence\n\n### Files and Field Descriptions\ntrain.csv - the training set, comprising ~26,000 60-second recordings of thirteen biological sensors for almost one thousand experimental participants\n* sequence - a unique id for each sequence\n* subject - a unique id for the subject in the experiment\n* step - time step of the recording, in one second intervals\n* sensor_00 - sensor_12 - the value for each of the thirteen sensors at that time step\n* train_labels.csv - the class label for each sequence.\n* sequence - the unique id for each sequence.\n* state - the state associated to each sequence. This is the target which you are trying to predict.\n\ntest.csv - the test set. For each of the ~12,000 sequences, you should predict a value for that sequence's state.\n\nsample_submission.csv - a sample submission file in the correct format.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{"execution":{"iopub.status.busy":"2022-04-02T20:36:44.124663Z","iopub.execute_input":"2022-04-02T20:36:44.125343Z","iopub.status.idle":"2022-04-02T20:36:44.130218Z","shell.execute_reply.started":"2022-04-02T20:36:44.125308Z","shell.execute_reply":"2022-04-02T20:36:44.129232Z"}}},{"cell_type":"markdown","source":"# 0. Installing Libraries\n","metadata":{}},{"cell_type":"code","source":"%%capture\n#Install optuna for hyper-param optimization\n!pip install optuna","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Loading the Requiered Libraries\nImporting the typical set of libraries to create a ML model, I tried to keep this to the minimun possible.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"# 2. Setting the Notebook\nIn this section I will configure some of the default parameters for my notebook execution. for example number of decimals, warning and numbers of rows I will like to load in case the dataset is to massive.","metadata":{}},{"cell_type":"code","source":"%%time\n# I like to disable my Notebook Warnings.\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Notebook Configuration...\n\n# Amount of data we want to load into the Model...\nDATA_ROWS = None\n# Dataframe, the amount of rows and cols to visualize...\nNROWS = 100\nNCOLS = 15\n# Main data location path...\nBASE_PATH = '...'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Configure notebook display settings to only use 2 decimal places, tables look nicer.\npd.options.display.float_format = '{:,.2f}'.format\npd.set_option('display.max_columns', NCOLS) \npd.set_option('display.max_rows', NROWS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"# 3. Loading the Information (CSV) Into A Dataframe\nIn this section I just import the datasets CSVs using Pandas, not much to see here this is a simple step","metadata":{}},{"cell_type":"code","source":"%%time\n# Load the CSVs into a pandas dataframe for future data manipulation.\ntrn_data = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2022/train.csv')\ntrn_label_data = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2022/train_labels.csv')\ntst_data = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2022/test.csv')\n\nsub = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2022/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Exploring the Information Available\nThe typical quick exploration to get an idea of the datsets loaded. I ussually like to load the information of the dataset, number of variables, type of variables.\nas also visualizing the first few rowns in the dataframe.\n","metadata":{}},{"cell_type":"markdown","source":"## 4.1. Analysing the Trian Dataset\nSimple analysis of the train dataset I ussually run\ninfo, head, descrive and number of nulls","metadata":{}},{"cell_type":"code","source":"%%time\ntrn_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_data.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 4.2. Analysing the Trian Labels Dataset","metadata":{}},{"cell_type":"code","source":"%%time\ntrn_label_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_label_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_label_data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_label_data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 4.3. Analysing the Trian Dataset, Using Groups","metadata":{}},{"cell_type":"code","source":"%%time\ntrn_summary = trn_data[['sequence', 'subject', 'step']].groupby(['sequence', 'subject']).count().reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_summary[trn_summary['subject'] == 66].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nsummary_by_subject = trn_summary[['sequence', 'subject']].groupby(['subject']).count().reset_index()\nsummary_by_subject.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_unique_subjects = set(list(trn_data['subject'].unique()))\ntst_unique_subjects = set(list(tst_data['subject'].unique()))\noverlap_subjets = trn_unique_subjects.intersection(tst_unique_subjects)\nprint('Repeated Subjects in Test Dataset:', len(overlap_subjets))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 5. Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"## 5.1. Visualization and Others (Under Construction!)","metadata":{}},{"cell_type":"code","source":"# ...","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Creating New Model Features","metadata":{}},{"cell_type":"markdown","source":"## 6.1. Creating Aggregated Features by Subject and Sequence","metadata":{}},{"cell_type":"code","source":"%%time\nfrom scipy.stats import kurtosis\n\ndef kurtosis_func(series: pd.Series) -> pd.Series:\n    '''\n    Calculates the Kurtosis of a Series.\n    '''\n    return kurtosis(series)\n\n\ndef q01(series: pd.Series) -> pd.Series:\n    '''\n    Calculates the 1% percentile quantile of a Series using numpy\n    '''\n    return np.quantile(series, 0.01)\n\n\ndef q05(series: pd.Series) -> pd.Series:\n    '''\n    Calculates the 5% percentile quantile of a Series using numpy\n    '''\nreturn np.quantile(series, 0.05)\n\n\ndef (series: pd.Series) -> pd.Series:\n    '''\n    Calculates the 95% percentile quantile of a Series using numpy\n    '''\n    return np.quantile(series, 0.95)\n\n\ndef (series: pd.Series) -> pd.Series:\n    '''\n    Calculates the 99% percentile quantile of a Series using numpy\n    '''\n    return np.quantile(series, 0.99)\n\n\ndef aggregated_features(df: pd.Dataframe, aggregation_cols = ['sequence']: list, prefix = '': str) -> pd.Dataframe :\n    '''\n    Creates an aggregated dataframe based on the aggregation colums requiered, renaming the field to the\n    indicated prefix\n    \n    Args:\n        df: A Dataframe containing the information that needs to be aggregated\n        aggregation_cols: A list of fields that will be used in the aggregation process\n        prefix: A name that will be used as a prefix on the new aggregated fields\n    \n    Returns:\n        df: A Dataframe with all the new aggregated fields\n    \n    \n    '''\n    # Aggretion dictionary, for each of variables in the Dataframe\n    \n    agg_strategy = {'sensor_00': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99, 'std'],\n                    'sensor_01': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99, 'std'],\n                    'sensor_02': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99, 'std'],\n                    'sensor_03': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99, 'std'],\n                    'sensor_04': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99, 'std'],\n                    'sensor_05': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99, 'std'],\n                    'sensor_06': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99, 'std'],\n                    'sensor_07': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99, 'std'],\n                    'sensor_08': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99, 'std'],\n                    'sensor_09': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99, 'std'],\n                    'sensor_10': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99, 'std'],\n                    'sensor_11': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99, 'std'],\n                    'sensor_12': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99, 'std'],\n                   }\n    \n    # Create a new group with all the aggregation fields\n    group = df.groupby(aggregation_cols).aggregate(agg_strategy)\n    # Rename the group based on the prefix\n    group.columns = ['_'.join(col).strip() for col in group.columns]\n    group.columns = [str(prefix) + str(col) for col in group.columns]\n    group.reset_index(inplace = True)\n    \n    # Create a new temp dataframe to merge the results of the aggregation back\n    temp = (df.groupby(aggregation_cols).size().reset_index(name = str(prefix) + 'size'))\n    group = pd.merge(temp, group, how = 'left', on = aggregation_cols,)\n    \n    # Return the aggregated fields\n    return group","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_merge_data = aggregated_features(trn_data, aggregation_cols = ['sequence', 'subject'])\ntst_merge_data = aggregated_features(tst_data, aggregation_cols = ['sequence', 'subject'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.2. Creating Aggregated Features by Subject","metadata":{}},{"cell_type":"code","source":"%%time\ntrn_subjects_merge_data = aggregated_features(trn_data, aggregation_cols = ['subject'], prefix = 'subject_')\ntst_subjects_merge_data = aggregated_features(tst_data, aggregation_cols = ['subject'], prefix = 'subject_')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_subjects_merge_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 6.3 Experimenting with Lags (Under Construction!)","metadata":{}},{"cell_type":"code","source":"%%time\n#trn_data['sensor_00_lag_01'] = trn_data['sensor_00'].shift(1)\n#trn_data['sensor_00_lag_10'] = trn_data['sensor_00'].shift(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Merging the Datasets for Training","metadata":{}},{"cell_type":"code","source":"%%time\ntrn_merge_data = trn_merge_data.merge(trn_label_data, how = 'left', on = 'sequence')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_merge_data = trn_merge_data.merge(trn_subjects_merge_data, how = 'left', on = 'subject')\ntst_merge_data = tst_merge_data.merge(tst_subjects_merge_data, how = 'left', on = 'subject')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_merge_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntst_merge_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 8. Post Processing the Information for the Model","metadata":{}},{"cell_type":"code","source":"%%time\nignore = ['sequence', 'state', 'subject', 'size']\nfeatures = [feat for feat in trn_merge_data.columns if feat not in ignore]\ntarget_feature = 'state'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 9. Creating a Simple Train / Test Split Strategy","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import train_test_split\ntest_size_pct = 0.10\nX_train, X_valid, y_train, y_valid = train_test_split(trn_merge_data[features], trn_merge_data[target_feature], test_size = test_size_pct, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 10. Building a Baseline GBT Model","metadata":{}},{"cell_type":"code","source":"%%time\nfrom xgboost  import XGBClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'n_estimators': 4096,\n          'max_depth': 7,\n          'learning_rate': 0.15,\n          'subsample': 0.95,\n          'colsample_bytree': 0.60,\n          'reg_lambda': 1.50,\n          'reg_alpha': 6.10,\n          'gamma': 1.40,\n          'random_state': 69,\n          'objective': 'binary:logistic',\n          'tree_method': 'gpu_hist',\n         }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nxgb = XGBClassifier(**params)\nxgb.fit(X_train, y_train, eval_set = [(X_valid, y_valid)], eval_metric = ['auc'], early_stopping_rounds = 128, verbose = 50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.metrics import roc_auc_score\npreds = xgb.predict_proba(X_valid)[:, 1]\nscore = roc_auc_score(y_valid, preds)\nprint(score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 11. Undertanding Model Behavior, Feature Importance","metadata":{}},{"cell_type":"code","source":"%%time\ndef plot_feature_importance(importance, names, model_type, max_features = 10):\n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n    fi_df = fi_df.head(max_features)\n\n    #Define size of bar plot\n    plt.figure(figsize=(8,6))\n    \n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + 'FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplot_feature_importance(xgb.feature_importances_,X_train.columns,'XG BOOST ', max_features = 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 12. Baseline Model Submission File Generation","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.metrics import roc_auc_score\npreds = xgb.predict_proba(tst_merge_data[features])[:, 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nsub['state'] = preds\nsub.to_csv('my_submission_041222.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 13. Hyper-Param Optimization","metadata":{}},{"cell_type":"code","source":"import optuna","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nX_train, X_valid, y_train, y_valid = train_test_split(trn_merge_data[features], trn_merge_data[target_feature], test_size = test_size_pct, random_state = 42)\n\ndef objective(trial):\n    n_estimators = trial.suggest_int(\"n_estimators\", 8, 2048)\n    max_depth = trial.suggest_int(\"max_depth\", 2, 16)\n    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.2)\n    subsample = trial.suggest_float(\"subsample\", 0.5, 1)\n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1)\n    reg_lambda = trial.suggest_float(\"reg_lambda\", 1, 20)\n    reg_alpha = trial.suggest_float(\"reg_alpha\", 0, 20)\n    gamma = trial.suggest_float(\"gamma\", 0, 20)\n    min_child_weight  = trial.suggest_int(\"min_child_weight\", 0, 128)\n    \n    clf = XGBClassifier(n_estimators  = n_estimators,\n                       learning_rate = learning_rate,\n                       max_depth = max_depth,\n                       subsample = subsample,\n                       colsample_bytree = colsample_bytree,\n                       reg_lambda = reg_lambda,\n                       reg_alpha = reg_alpha,\n                       gamma = gamma,\n                       min_child_weight = min_child_weight,\n                       random_state  = 69,\n                       objective = 'binary:logistic',\n                       tree_method = 'gpu_hist',\n                      )\n    \n    clf.fit(X_train, y_train)\n    \n    valid_pred = clf.predict_proba(X_valid)[:, 1]\n    score = roc_auc_score(y_valid, valid_pred)\n    \n    return score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nstudy = optuna.create_study(direction = \"maximize\")\nstudy.optimize(objective, n_trials = 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nparameters = study.best_params\nparameters","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 13.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}