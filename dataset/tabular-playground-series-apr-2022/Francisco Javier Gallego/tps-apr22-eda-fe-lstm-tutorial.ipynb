{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <b>1 <span style='color:lightseagreen'>|</span> Introduction</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>1.1 | Goal</b></p>\n</div>\n\nWelcome to the April edition of the 2022 Tabular Playground Series! This month's challenge is a time series classification problem. You've been provided with thousands of sixty-second sequences of biological sensor data recorded from several hundred participants who could have been in either of two possible activity states. Can you determine what state a participant was in from the sensor data?\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>1.2 | Competition Metric</b></p>\n</div>\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.But what are ROC and AUROC ?\n\n<div class=\"alert alert-block alert-info\"> 📌 The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, recall or probability of detection. The false-positive rate is also known as probability of false alarm and can be calculated as (1 − specificity). It can also be thought of as a plot of the power as a function of the Type I Error of the decision rule (when the performance is calculated from just a sample of the population, it can be thought of as estimators of these quantities). The ROC curve is thus the sensitivity or recall as a function of fall-out.</div>\n\n![](https://miro.medium.com/max/700/1*PU3_4LheadpGcpl6daO1mA.png)","metadata":{"_uuid":"3692a094-97eb-47af-9929-c9075881eec7","_cell_guid":"d982bcee-0b96-4f24-a506-0927b932cbde","trusted":true}},{"cell_type":"code","source":"# Basic Libraries\nfrom IPython.display import clear_output\nimport os\nimport warnings\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\nimport seaborn as sns\nfrom IPython.display import display\n\n# Encoders\nfrom pandas.api.types import CategoricalDtype\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler\nfrom category_encoders import MEstimateEncoder\n\n# Scikit Learn\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score, StratifiedKFold, learning_curve, train_test_split, GroupKFold\n\n# Algorithms\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\n# Optuna - Bayesian Optimization \nimport optuna\nfrom optuna.samplers import TPESampler\n\n# Plotly\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.offline as offline\nimport plotly.graph_objs as go\n\n# TPS April22 Metric\nfrom sklearn.metrics import roc_auc_score\n\n# Permutation Importance\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n# TensorFlow\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import GlobalMaxPooling1D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.layers import Concatenate, LSTM, GRU\nfrom tensorflow.keras.layers import Bidirectional, Multiply\n\nwarnings.filterwarnings('ignore')\n\ndef load_data():\n    data_dir = Path(\"../input/tabular-playground-series-apr-2022\")\n    df_train = pd.read_csv(data_dir / \"train.csv\")\n    df_test = pd.read_csv(data_dir / \"test.csv\")\n    train_labels = pd.read_csv(data_dir / 'train_labels.csv')\n    # Merge the splits so we can process them together\n    df_train = df_train.merge(train_labels, on='sequence', how='outer')\n    df = pd.concat([df_train, df_test])\n    return df\n\ndef plot_feature_importance(importance,names,model_type):\n    \n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n    \n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n    \n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n    fi_df = fi_df[fi_df.feature_importance > 0]\n    fig = px.bar(fi_df, x='feature_names', y='feature_importance', color=\"feature_importance\",\n             color_continuous_scale='Blugrn')\n    \n    # General Styling\n    fig.update_layout(height=750, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100,t=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Feature Importance Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)\n    fig.show()\n\ndf_data = load_data()\nclear_output()\n#pp.ProfileReport(df_data)","metadata":{"_uuid":"fe5c6cb5-b26b-4d4f-bea0-54b700d4e1e3","_cell_guid":"531c0a77-e760-41ba-8b16-e2f846917393","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T11:51:57.319489Z","iopub.execute_input":"2022-04-15T11:51:57.319976Z","iopub.status.idle":"2022-04-15T11:52:23.829307Z","shell.execute_reply.started":"2022-04-15T11:51:57.319832Z","shell.execute_reply":"2022-04-15T11:52:23.828279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>1.2 | Early Insights</b></p>\n</div>\n\n**<span style='color:lightseagreen'>Analysing the previous profiling report</span>** we have been able to observe the following about the dataset given: \n\n* 2.3M given examples \n* All variables are **<span style='color:lightseagreen'>numerical</span>**.\n* No missing values.\n* **<span style='color:lightseagreen'>High correlation</span>** between some pairs of sensor features. \n* None of sensor features are **<span style='color:lightseagreen'>uniformly distributed</span>**.\n\nWe are going to make a more exhaustive analysis in the following section. \n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>1.3 | Reducing Memory Usage</b></p>\n</div>\n\nAs we can observe in the previous report from the dataset, we have the amount of more than **<span style='color:lightseagreen'>2.3 million rows</span>**. Due to that, in order to not having **<span style='color:lightseagreen'>issues with memory</span>** in the kernel, we are going to reduce its memory usage with the following function. Below, we can appreciate that reduction was successful as we manage to make a **<span style='color:lightseagreen'>reduction of 50.7%</span>**.","metadata":{"_uuid":"00a1570d-8ee0-453a-9262-de74764b6c13","_cell_guid":"85b2c351-8cfa-4fef-b9fc-25ea4fc89d72","trusted":true}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n \n    return df\n\ndf_data = reduce_mem_usage(df_data)","metadata":{"_uuid":"c51231b5-0557-4500-a17c-9e202a2c405a","_cell_guid":"b957e627-b2e5-48ab-88cf-85f808179abb","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T11:52:23.831173Z","iopub.execute_input":"2022-04-15T11:52:23.83151Z","iopub.status.idle":"2022-04-15T11:52:24.53089Z","shell.execute_reply.started":"2022-04-15T11:52:23.831475Z","shell.execute_reply":"2022-04-15T11:52:24.529903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>2 <span style='color:lightseagreen'>|</span> Exploratory Data Analysis</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.1 | Heatmap</b></p>\n</div>\n\nAlthough we had one in the upper report, we are going to plot another in order to **<span style='color:lightseagreen'>analyse and study better</span>** correlations between our features. For anyone interested, my **<span style='color:lightseagreen'>visualizations</span>** are going to be made with **<span style='color:lightseagreen'>Plotly</span>** library. Hope you enjoy them ! **<span style='color:lightseagreen'>Do not hesitate to write me in the comments with any kind of tip to improve them</span>**.","metadata":{"_uuid":"5f56b0b3-1b1c-4951-ba75-0dd1090378ed","_cell_guid":"c850c848-0d45-4ef8-84a9-dce69f9caa02","trusted":true}},{"cell_type":"code","source":"fig = px.imshow(df_data[df_data.state.isnull() == False].corr(), color_continuous_scale='RdBu_r', color_continuous_midpoint=0, origin='lower', aspect='auto')\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False)\nfig.update_yaxes(showgrid = True, gridcolor='gray',gridwidth=0.5, linecolor='gray',linewidth=2, zeroline = False)\n\n# General Styling\nfig.update_layout(height=750, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100, t=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Heatmap</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)\nfig.show()","metadata":{"_uuid":"a6997dc8-ae20-4506-9fa5-9b50de16986e","_cell_guid":"d4e708fe-fea2-4941-b36b-6f6adc229714","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T11:52:24.53569Z","iopub.execute_input":"2022-04-15T11:52:24.535937Z","iopub.status.idle":"2022-04-15T11:52:27.03095Z","shell.execute_reply.started":"2022-04-15T11:52:24.535908Z","shell.execute_reply":"2022-04-15T11:52:27.028202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"📌 **Interpret:** we can make the following conclusions after analysing the heatmap plotted: \n\n* State feature is **<span style='color:lightseagreen'>significantly low correlated</span>** with almost every sensor feature. Sensor 2, is the one which is most correlated with target.\n* Correlations are **<span style='color:lightseagreen'>all positive</span>**, except few of them. \n* The following **<span style='color:lightseagreen'>sensors features are correlated</span>** between each other: Sensor 0, Sensor 6, Sensor 9, Sensor 3, Sensor 7 and Sensor 11\n        \n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.2 | Subject / Sequence Analysis</b></p>\n</div>\n\nWe have 38186 different sequences (taking into account test sequences as well). Each sequence has 60 time steps. As the train and test subjects are different, we cannot use the subject as a feature. On the contrary: we have to make sure that our classifier generalizes to previously unseen subjects. In the following notebook cell, we check how often each subject occurs in the training data. The most infrequent subject occurs only twice; the most frequent subject is a hundred times more frequent; it occurs 199 times. The picture for the test data is similar.","metadata":{"_uuid":"7b5a0d52-836b-41f0-bca6-5a07ad42bc23","_cell_guid":"0f2b08f4-b7c1-4927-81da-6e1d3d15cf1c","trusted":true}},{"cell_type":"code","source":"subject_count = pd.DataFrame(df_data[df_data.state.isnull() == False].subject.value_counts().sort_values() // 60).reset_index()\nsubject_count.columns = ['subject','count']\nsubject_count['subject'] = range(1,673)\n\n# chart\nfig = make_subplots(rows=1, cols=2, column_widths=[0.5, 0.5], vertical_spacing=0.1, horizontal_spacing=0.1,\n                    subplot_titles=('Sequence Count Distribution over Training Subjects','Sequence Count Distribution over Test Subjects'))\n\nfig.add_trace(go.Bar(x=subject_count['subject'], y=subject_count['count'], marker = dict(color=px.colors.sequential.Viridis[5])), row = 1, col = 1)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=1)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=1)\n\nsubject_count = pd.DataFrame(df_data[df_data.state.isnull() == True].subject.value_counts().sort_values() // 60).reset_index()\nsubject_count.columns = ['subject','count']\nsubject_count['subject'] = range(319)\n\nfig.add_trace(go.Bar(x=subject_count['subject'], y=subject_count['count'], marker = dict(color=px.colors.sequential.Viridis[5])), row = 1, col = 2)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=2)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=2)\n\n# General Styling\nfig.update_layout(height=400, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Sequence Count Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"_uuid":"d63b5688-1b34-4e0f-b160-6841b9d489ef","_cell_guid":"55597230-3bcf-48aa-9528-ccf20c668a2f","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T11:52:27.032006Z","iopub.execute_input":"2022-04-15T11:52:27.032658Z","iopub.status.idle":"2022-04-15T11:52:27.294798Z","shell.execute_reply.started":"2022-04-15T11:52:27.032622Z","shell.execute_reply":"2022-04-15T11:52:27.294021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_state = df_data[df_data.state.isnull() == False].groupby('subject').agg({'state':'mean'}).reset_index().sort_values(by='state',ascending=False)\nfig = px.scatter(mean_state, x='subject',y='state', color=\"state\", color_continuous_scale='Blugrn', size='state')\n\nfig.update_xaxes(showgrid = False, showline = True, gridwidth = 0.05, linecolor = 'gray', zeroline = False, linewidth = 2)\nfig.update_yaxes(showline = True, gridwidth = 0.05, linecolor = 'gray', linewidth = 2, zeroline = False)\n\n# General Styling\nfig.update_layout(height=500,\n              margin=dict(b=50,r=30,l=100,t=100),\n              title = \"<span style='font-size:36px; font-family:Times New Roman'>Subject Probability State Analysis</span>\",                  \n              plot_bgcolor='rgb(242,242,242)',\n              paper_bgcolor = 'rgb(242,242,242)',\n              font=dict(family=\"Times New Roman\", size= 14),\n              hoverlabel=dict(font_color=\"floralwhite\"),\n              showlegend=False)\nfig.show()","metadata":{"_uuid":"98849ef2-35dc-46a3-aa3e-5cc793412240","_cell_guid":"d3d109d1-51b6-4488-8a25-45aa9b063a40","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T11:52:27.296426Z","iopub.execute_input":"2022-04-15T11:52:27.296902Z","iopub.status.idle":"2022-04-15T11:52:27.570542Z","shell.execute_reply.started":"2022-04-15T11:52:27.296856Z","shell.execute_reply":"2022-04-15T11:52:27.569623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.3 | Sensor Features</b></p>\n</div>\n\n### 2.2.3 | Time Steps\n\nTime-step features let us model **<span style='color:lightseagreen'>time dependence</span>**. A series is time dependent if its values can be predicted from the time they occured. In the Hardcover Sales series, we can predict that sales later in the month are generally higher than sales earlier in the month.","metadata":{"_uuid":"b278aade-88b4-4d21-803e-acfdb6684b40","_cell_guid":"001313d9-6a1a-4b6c-9203-e027bcbc07c7","trusted":true}},{"cell_type":"code","source":"sensors = df_data[df_data.state.isnull() == False].columns[df_data[df_data.state.isnull() == False].columns.str.contains('sensor')]\n\nsteps_0 = df_data[df_data.state.isnull() == False][df_data[df_data.state.isnull() == False]['state']==0].pivot_table(\n    index=['step'],\n    values=sensors,\n    aggfunc='mean'\n)\n\nsteps_1 = df_data[df_data.state.isnull() == False][df_data[df_data.state.isnull() == False]['state']==1].pivot_table(\n    index=['step'],\n    values=sensors,\n    aggfunc='mean'\n)\n\ndef plot_steps0(fig, feature):\n    fig.add_trace(go.Scatter(x=steps_0.index, y=steps_0[feature], \n                  mode='lines+markers', marker = dict(color = px.colors.qualitative.G10[9]), name='State 0', visible=(feature==default_sensor)))\n\n    fig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False)\n    fig.update_yaxes(showgrid = False, linecolor='gray', linewidth=2, zeroline = False)\n    \ndef plot_steps1(fig, feature):\n    fig.add_trace(go.Scatter(x=steps_1.index, y=steps_1[feature], \n                  mode='lines+markers', marker = dict(color = px.colors.sequential.Viridis[6]), name='State 1', visible=(feature==default_sensor)))\n    \n    fig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False)\n    fig.update_yaxes(showgrid = False, linecolor='gray', linewidth=2, zeroline = False)\n\nfig = go.Figure()\n\nsensor_plot_names = []\nbuttons=[]\ndefault_sensor = \"sensor_00\"\n\nfor sensor in range(13):\n    sensor_name = f\"sensor_{sensor:02d}\"\n    plot_steps0(fig, sensor_name)\n    plot_steps1(fig, sensor_name)\n    \n    sensor_plot_names.extend([sensor_name]*2)\n    \nfor sensor in range(13):\n    sensor_name = f\"sensor_{sensor:02d}\"\n    buttons.append(dict(method='update',\n                        label=sensor_name,\n                        args = [{'visible': [sensor_name==r for r in sensor_plot_names]}]))\n\n# General Layout. Add dropdown menus to the figure\nfig.update_layout(height=500,\n              margin=dict(b=50,r=30,l=100,t=100),\n              title = \"<span style='font-size:36px; font-family:Times New Roman'>Time Steps Analysis</span>\",                  \n              plot_bgcolor='rgb(242,242,242)',\n              paper_bgcolor = 'rgb(242,242,242)',\n              font=dict(family=\"Times New Roman\", size= 14),\n              hoverlabel=dict(font_color=\"floralwhite\"),\n              showlegend=True, updatemenus=[{\"buttons\": buttons, \"direction\": \"down\", \"active\": 0, \"showactive\": True, \"x\": 0.5, \"y\": 1.22}])\n\nfig.update_layout(legend=dict(\n    orientation=\"h\",\n    yanchor=\"top\",\n    y=1.21,\n    xanchor=\"right\",\n    x=1,\n    #bgcolor=\"white\",\n    #bordercolor=\"Black\",\n    font=dict(\n        family=\"Times New Roman\",\n        size=12\n    #    color=\"black\"\n    ),\n))\n\nfig.show()","metadata":{"_uuid":"d155c68e-a925-48af-a460-f54ab38f6a8e","_cell_guid":"6521910f-d688-495c-9e72-04c3b49502f8","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T11:52:27.572099Z","iopub.execute_input":"2022-04-15T11:52:27.57241Z","iopub.status.idle":"2022-04-15T11:52:28.67126Z","shell.execute_reply.started":"2022-04-15T11:52:27.572379Z","shell.execute_reply":"2022-04-15T11:52:28.670417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"📌 **Interpret:** in this previous chart we have plotted every **<span style='color:lightseagreen'>sensor mean value per step, depending whether the state</span>** is 0 or 1. We can appreciate that for most sensors the graphs have peaks and troughs. However, let's take a look at the case of **<span style='color:lightseagreen'>sensor 2</span>**. Surprisingly, both scatter lines do not cut off at any point. Indeed, both lines seem to be almost constant functions with **<span style='color:lightseagreen'>very distinct values</span>**. \n### 2.3.2 | Distributions","metadata":{"_uuid":"8b7e1e92-5749-4301-bf98-d19c247c4ac5","_cell_guid":"23151855-b00a-4aab-8c89-1039d5e02b85","trusted":true}},{"cell_type":"code","source":"figure = plt.figure(figsize=(16, 8))\nfor sensor in range(13):\n    sensor_name = f\"sensor_{sensor:02d}\"\n    plt.subplot(4, 4, sensor+1)\n    plt.hist(df_data[df_data.state.isnull() == False][f\"{sensor_name}\"], bins=100)\n    plt.title(f\"{sensor_name}\")\nfigure.tight_layout(h_pad=1.0, w_pad=0.5)\nplt.suptitle('Sensor Normal Probability Plot', y=1.02)\nplt.show()","metadata":{"_uuid":"f85d50aa-8aa1-4f83-9aa7-8651fe5d1ec6","_cell_guid":"4bfe9422-8f76-410a-936f-2c7ce65b74f8","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T11:52:28.67262Z","iopub.execute_input":"2022-04-15T11:52:28.672827Z","iopub.status.idle":"2022-04-15T11:52:34.720528Z","shell.execute_reply.started":"2022-04-15T11:52:28.672802Z","shell.execute_reply":"2022-04-15T11:52:34.719612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"📌 **Interpret:** we can make the following conclusions after analysing the histograms plotted: \n\n* The histograms show that every sensor has outliers.\n* Most sensor features values have big amounts of zeros.\n* Sensor features are not normally distributed.\n\n### 2.3.2 | Skewness and Kurtosis\n\n[Skewness and Kurtosis Tutorial](https://www.universoformulas.com/estadistica/descriptiva/asimetria-curtosis/)\n\nLet's now check **<span style='color:lightseagreen'>skewness</span>**. Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative, or undefined.\n\nFor a unimodal distribution, negative skew commonly indicates that the tail is on the left side of the distribution, and positive skew indicates that the tail is on the right. In cases where one tail is long but the other tail is fat, skewness does not obey a simple rule. For example, a zero value means that the tails on both sides of the mean balance out overall; this is the case for a symmetric distribution, but can also be true for an asymmetric distribution where one tail is long and thin, and the other is short but fat. A skewness greater than 1 is generally judged to be skewed, so check mainly those greater than 1.","metadata":{"_uuid":"bd921dd2-70ab-464d-bc32-55a0c00636fc","_cell_guid":"72fb4bf4-95d9-48fe-8f9e-0630d8d533e0","trusted":true}},{"cell_type":"code","source":"from scipy.stats import skew, kurtosis\nsensors = quantitative = [f for f in df_data.columns if df_data.dtypes[f] == 'float32']\nsensors.remove('state')\nskew_features = pd.DataFrame(df_data[sensors].apply(lambda x : skew(x))).reset_index()\nskew_features.columns = ['sensor','skewness']\nline = pd.DataFrame(dict(\n    x = ['sensor_00','sensor_12'],\n    y = [1, 1]\n))\n\nfig = px.bar(skew_features, x='sensor',y='skewness', color = 'skewness', color_continuous_scale = 'Blugrn', range_y=[-14,10])\n\nfig.update_xaxes(showgrid = False, showline = True, gridwidth = 0.05, linecolor = 'gray', linewidth = 2)\nfig.update_yaxes(showline = True, gridwidth = 0.05, linecolor = 'gray', linewidth = 2, zerolinecolor = 'gray', zerolinewidth = 0.5)\n\n# General Styling\nfig.update_layout(height=500,\n              margin=dict(b=50,r=30,l=100,t=100),\n              title = \"<span style='font-size:36px; font-family:Times New Roman'>Sensors' Skewness</span>\",                  \n              plot_bgcolor='rgb(242,242,242)',\n              paper_bgcolor = 'rgb(242,242,242)',\n              font=dict(family=\"Times New Roman\", size= 14),\n              hoverlabel=dict(font_color=\"floralwhite\"),\n              showlegend=False)\nfig.show()","metadata":{"_uuid":"3f6ebe77-1594-4fc6-9ce2-f3bc86056710","_cell_guid":"2de904aa-1305-4915-93bb-9605e002ef1b","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T11:52:34.721822Z","iopub.execute_input":"2022-04-15T11:52:34.722096Z","iopub.status.idle":"2022-04-15T11:52:35.089886Z","shell.execute_reply.started":"2022-04-15T11:52:34.722067Z","shell.execute_reply":"2022-04-15T11:52:35.089082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now move into **<span style='color:lightseagreen'>kurtosis</span>**. In probability theory and statistics, kurtosis is a measure of the \"tailedness\" of the probability distribution of a real-valued random variable. Like skewness, kurtosis describes the shape of a probability distribution and there are different ways of quantifying it for a theoretical distribution and corresponding ways of estimating it from a sample from a population. Different measures of kurtosis may have different interpretations.\n\nThe standard measure of a distribution's kurtosis, originating with Karl Pearson, is a scaled version of the fourth moment of the distribution. This number is related to the tails of the distribution, not its peak; hence, the sometimes-seen characterization of kurtosis as \"peakedness\" is incorrect. For this measure, higher kurtosis corresponds to greater extremity of deviations (or outliers), and not the configuration of data near the mean.","metadata":{"_uuid":"37b9a85f-87bb-46f9-a35a-a2d067241dbf","_cell_guid":"5f51c479-44f5-4794-b97c-2ed84ee85e35","trusted":true}},{"cell_type":"code","source":"skew_features = pd.DataFrame(df_data[sensors].apply(lambda x : kurtosis(x))).reset_index()\nskew_features.columns = ['sensor','kurtosis']\nfig = px.bar(skew_features, x='sensor',y='kurtosis', color = 'kurtosis', color_continuous_scale = 'Blugrn')\n\nfig.update_xaxes(showgrid = False, showline = True, gridwidth = 0.05, linecolor = 'gray', linewidth = 2)\nfig.update_yaxes(showline = True, gridwidth = 0.05, linecolor = 'gray', linewidth = 2, zerolinecolor = 'gray', zerolinewidth = 0.5)\n\n# General Styling\nfig.update_layout(height=500,\n              margin=dict(b=50,r=30,l=100,t=100),\n              title = \"<span style='font-size:36px; font-family:Times New Roman'>Sensors' Kurtosis</span>\",                  \n              plot_bgcolor='rgb(242,242,242)',\n              paper_bgcolor = 'rgb(242,242,242)',\n              font=dict(family=\"Times New Roman\", size= 14),\n              hoverlabel=dict(font_color=\"floralwhite\"),\n              showlegend=False)\nfig.show()","metadata":{"_uuid":"410a66d5-3ad6-49ed-b0ee-29d833421107","_cell_guid":"f65ed57f-78c6-4755-8260-3f96ae7ded6e","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T11:52:35.093041Z","iopub.execute_input":"2022-04-15T11:52:35.09356Z","iopub.status.idle":"2022-04-15T11:52:35.406401Z","shell.execute_reply.started":"2022-04-15T11:52:35.093507Z","shell.execute_reply":"2022-04-15T11:52:35.405444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's plot the \"inner\" part of the histogram after removing 2 % outliers on either side. We see that the sensors differ in their characteristics:\n\n* Most measurements of sensor_02 return multiples of 0.33 (but the values in between occur with a low probability).\n* Sensor_08 has discrete values (multiples of 0.1).\n* Some sensors look like normal distributions, others (in particular sensor_12) have a long tail.\n\nThe distributions look symmetric with the center at 0. This next cell of the EDA has been taken from [Ambros EDA Section](https://www.kaggle.com/code/ambrosm/tpsapr22-eda-which-makes-sense/notebook).","metadata":{"_uuid":"bc285b6e-fd1f-42ad-8fee-0bcbf8e940d0","_cell_guid":"daab5b0e-3218-4eb2-9a7f-1ccadc897c14","trusted":true}},{"cell_type":"code","source":"figure = plt.figure(figsize=(16, 8))\nfor sensor in range(13):\n    sensor_name = f\"sensor_{sensor:02d}\"\n    plt.subplot(4, 4, sensor+1)\n    plt.hist(df_data[df_data.state.isnull() == False][sensor_name], bins=100,\n             range=(df_data[df_data.state.isnull() == False][sensor_name].quantile(0.02),\n                    df_data[df_data.state.isnull() == False][sensor_name].quantile(0.98)))\n    plt.title(f\"{sensor_name} histogram\")\nfigure.tight_layout(h_pad=1.0, w_pad=0.5)\nplt.suptitle('Sensor Histograms After Outlier Removal', y=1.02)\nplt.show()","metadata":{"_uuid":"a78e3ca8-f648-4c7f-80ed-c324509effe8","_cell_guid":"8f2901d5-dfce-40bc-a198-047572658ebe","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T11:52:35.407912Z","iopub.execute_input":"2022-04-15T11:52:35.408571Z","iopub.status.idle":"2022-04-15T11:52:44.519506Z","shell.execute_reply.started":"2022-04-15T11:52:35.40854Z","shell.execute_reply":"2022-04-15T11:52:44.51864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3.3 Normal Probability Plot\nThe **<span style='color:lightseagreen'>normal probability plot</span>** is a graphical technique for assessing whether or not a data set is approximately normally distributed. The data are plotted against a theoretical normal distribution in such a way that the points **<span style='color:lightseagreen'>should form an approximate straight line</span>**.","metadata":{"_uuid":"a2389de3-76a3-4dc7-8219-edf3cc2221ac","_cell_guid":"c8e4dc83-a08c-4440-8a3d-94b47c5f620f","trusted":true}},{"cell_type":"code","source":"from scipy import stats\nfigure = plt.figure(figsize=(16, 8))\nfor sensor in range(13):\n    sensor_name = f\"sensor_{sensor:02d}\"\n    plt.subplot(4, 4, sensor+1)\n    stats.probplot(df_data[df_data.state.isnull() == False][f\"{sensor_name}\"], plot=plt)\n    plt.title(f\"{sensor_name}\")\nfigure.tight_layout(h_pad=1.0, w_pad=0.5)\nplt.suptitle('Sensor Normal Probability Plot', y=1.02)\nplt.show()","metadata":{"_uuid":"63ba140a-6efd-450d-aeb3-8021cd6cff63","_cell_guid":"5f295dd3-875b-46df-8870-a00f742c3e3d","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T11:52:44.520694Z","iopub.execute_input":"2022-04-15T11:52:44.520938Z","iopub.status.idle":"2022-04-15T11:53:28.017733Z","shell.execute_reply.started":"2022-04-15T11:52:44.520909Z","shell.execute_reply":"2022-04-15T11:53:28.016827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from scipy import stats\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfigure = plt.figure(figsize=(16, 8))\nfor sensor in range(13):\n    sensor_name = f\"sensor_{sensor:02d}\"\n    plt.subplot(4, 4, sensor+1)\n    plot_acf(df_data[sensor_name],lags=10)\n    plt.title(f\"{sensor_name}\")\nfigure.tight_layout(h_pad=1.0, w_pad=0.5)\nplt.suptitle('Autocorrelation Analysis', y=1.02)\nplt.show()\nplot_pacf(df_data['sensor_03'],lags=2,title=\"Sensor 2\")","metadata":{"_uuid":"318a7606-033b-4293-8ae1-835e59e3ddb6","_cell_guid":"5740f9c4-5bd5-4ac8-87d4-7669473a67c3","execution":{"iopub.status.busy":"2022-04-13T14:51:12.262473Z","iopub.execute_input":"2022-04-13T14:51:12.262819Z"},"trusted":true}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.4 | Outliers</b></p>\n</div>\n\n### 2.4.1 | Outliers Definition\nOutlier is an observation that is numerically distant from the rest of the data or in a simple word it is the value which is out of the range.let’s take an example to check what happens to a data set with and data set without outliers.\n\n### 2.4.2 | Outliers Detection\n\nOutlier can be of two types: Univariate and Multivariate. These outliers can be found when we look at distribution of a single variable. Multi-variate outliers are outliers in an n-dimensional space. We'll start by detecting whether there are outliers in our dataset or not. \n\n#### 2.4.2.1 | Grubbs Test\n\n$$\n\\begin{array}{l}{\\text { Grubbs' test is defined for the hypothesis: }} \\\\ {\\begin{array}{ll}{\\text { Ho: }}  {\\text { There are no outliers in the data set }} \\\\ {\\mathrm{H}_{\\mathrm{1}} :}  {\\text { There is exactly one outlier in the data set }}\\end{array}}\\end{array}\n$$\n$$\n\\begin{array}{l}{\\text {The Grubbs' test statistic is defined as: }} \\\\ {\\qquad G_{calculated}=\\frac{\\max \\left|X_{i}-\\overline{X}\\right|}{SD}} \\\\ {\\text { with } \\overline{X} \\text { and } SD \\text { denoting the sample mean and standard deviation, respectively. }} \\end{array}\n$$\n$$\nG_{critical}=\\frac{(N-1)}{\\sqrt{N}} \\sqrt{\\frac{\\left(t_{\\alpha /(2 N), N-2}\\right)^{2}}{N-2+\\left(t_{\\alpha /(2 N), N-2}\\right)^{2}}}\n$$\n\n\\begin{array}{l}{\\text { If the calculated value is greater than critical, you can reject the null hypothesis and conclude that one of the values is an outlier }}\\end{array}","metadata":{"_uuid":"8cb928f1-9475-41a9-8d4f-afa377c48aa7","_cell_guid":"941c5588-be3a-45bc-bc67-1a5768b28618","trusted":true}},{"cell_type":"code","source":"import scipy.stats as stats\ndef grubbs_test(x):\n    n = len(x)\n    mean_x = np.mean(x)\n    sd_x = np.std(x)\n    numerator = max(abs(x-mean_x))\n    g_calculated = numerator/sd_x\n    print(\"Grubbs Calculated Value:\",g_calculated)\n    t_value = stats.t.ppf(1 - 0.05 / (2 * n), n - 2)\n    g_critical = ((n - 1) * np.sqrt(np.square(t_value))) / (np.sqrt(n) * np.sqrt(n - 2 + np.square(t_value)))\n    print(\"Grubbs Critical Value:\",g_critical)\n    if g_critical > g_calculated:\n        print(\"From grubbs_test we observe that calculated value is lesser than critical value, Accept null hypothesis and conclude that there is no outliers\\n\")\n    else:\n        print(\"From grubbs_test we observe that calculated value is greater than critical value, Reject null hypothesis and conclude that there is an outliers\\n\")","metadata":{"_uuid":"756c0e07-605a-461b-b904-d93ed17e39aa","_cell_guid":"905918df-b599-4229-8601-f85173361db8","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T11:53:28.019354Z","iopub.execute_input":"2022-04-15T11:53:28.019807Z","iopub.status.idle":"2022-04-15T11:53:28.028161Z","shell.execute_reply.started":"2022-04-15T11:53:28.019772Z","shell.execute_reply":"2022-04-15T11:53:28.027254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.4.2.2 | Z-score method\n\nUsing Z score method,we can find out how many standard deviations value away from the mean. \n\n![minipic](https://i.pinimg.com/originals/cd/14/73/cd1473c4c82980c6596ea9f535a7f41c.jpg)\n\n Figure in the left shows area under normal curve and how much area that standard deviation covers.\n* 68% of the data points lie between + or - 1 standard deviation.\n* 95% of the data points lie between + or - 2 standard deviation\n* 99.7% of the data points lie between + or - 3 standard deviation\n\n$\\begin{array}{l} {R.Z.score=\\frac{0.6745*( X_{i} - Median)}{MAD}}  \\end{array}$\n\nIf the z score of a data point is more than 3 (because it cover 99.7% of area), it indicates that the data value is quite different from the other values. It is taken as outliers.","metadata":{"_uuid":"afdac213-64fa-4d8a-8619-d8820fca1c97","_cell_guid":"e1765f4c-25e1-41ba-b489-eae4c848fff7","trusted":true}},{"cell_type":"code","source":"out=[]\ndef Zscore_outlier(df):\n    m = np.mean(df)\n    sd = np.std(df)\n    row = 0\n    for i in df: \n        z = (i-m)/sd\n        if np.abs(z) > 3: \n            out.append(row)\n        row += 1\n    return out","metadata":{"_uuid":"69a2fcd0-d3f4-444a-a688-b699f0cd8649","_cell_guid":"78e47a6f-953c-4dea-8145-4cc20fcbc5b4","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T11:53:28.029542Z","iopub.execute_input":"2022-04-15T11:53:28.030211Z","iopub.status.idle":"2022-04-15T11:53:28.067492Z","shell.execute_reply.started":"2022-04-15T11:53:28.030177Z","shell.execute_reply":"2022-04-15T11:53:28.06668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4.3 | Sensor Outliers Detection","metadata":{"_uuid":"b65afa23-55bc-4b1c-a5d7-6cbc8178ed9a","_cell_guid":"79f0f88e-4f08-493d-8ad9-5292bc05e7fd","trusted":true}},{"cell_type":"code","source":"def plot_outliers(feature):\n    grubbs_test(df_data[df_data.state.isnull() == False][feature])\n    outliers_index = Zscore_outlier(df_data[df_data.state.isnull() == False][feature])\n    print('Outlier %: ', len(outliers_index)/df_data[df_data.state.isnull() == False].shape[0])\n    print('\\n')\n\n    df_outlier = df_data[df_data.state.isnull() == False]\n    df_outlier['outlier'] = 0\n    df_outlier.loc[outliers_index,'outlier'] = 1\n\n    fig = px.scatter(df_outlier.loc[outliers_index,:], x=outliers_index, y=feature, color = feature, color_continuous_scale='Blugrn', color_continuous_midpoint=0)\n    fig.update_xaxes(showgrid = False, showline = True, gridwidth = 0.05, linecolor = 'gray', zeroline = False, linewidth = 2)\n    fig.update_yaxes(showline = True, gridwidth = 0.05, linecolor = 'gray', linewidth = 2, zeroline = False)\n\n    #General Styling\n    fig.update_layout(height=400, bargap=0.2,\n                      margin=dict(b=50,r=30,l=100, t=80),\n                      title = \"<span style='font-size:36px; font-family:Times New Roman'>Sensor Features Outliers Analysis</span>\",                  \n                      plot_bgcolor='rgb(242,242,242)',\n                      paper_bgcolor = 'rgb(242,242,242)',\n                      font=dict(family=\"Times New Roman\", size= 14),\n                      hoverlabel=dict(font_color=\"floralwhite\"),\n                      showlegend=False)\n    fig.show()\n\nsensors = quantitative = [f for f in df_data.columns if df_data.dtypes[f] == 'float32']\nsensors.remove('state')\ni = 0\nfor sensor in sensors:\n    print('Sensor ', i)\n    print('\\n')\n    i+=1\n    plot_outliers(sensor)","metadata":{"_uuid":"0323f162-a363-4e17-9afe-65966a1c4c0e","_cell_guid":"de623885-4561-4846-a335-20a291137c36","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T11:53:28.0688Z","iopub.execute_input":"2022-04-15T11:53:28.06911Z","iopub.status.idle":"2022-04-15T11:54:34.73496Z","shell.execute_reply.started":"2022-04-15T11:53:28.06907Z","shell.execute_reply":"2022-04-15T11:54:34.733231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>3 <span style='color:lightseagreen'>|</span> Feature Engineering</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.1 | Mutual Information</b></p>\n</div>\n\nMutual information describes **<span style='color:lightseagreen'>relationships</span>** in terms of **<span style='color:lightseagreen'>uncertainty</span>**. The mutual information (MI) between two quantities is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other. If you knew the value of a feature, how much more confident would you be about the target? Scikit-learn has two mutual information **<span style='color:lightseagreen'>metrics</span>** in its feature_selection module: one for **<span style='color:lightseagreen'>real-valued targets</span>** (mutual_info_regression) and one for **<span style='color:lightseagreen'>categorical targets</span>** (mutual_info_classif). The next cell computes the MI scores for our features and wraps them up in a nice dataframe. Hereafter, we are going to drop uninformative features as they are useless.","metadata":{"_uuid":"2ee2882a-e3a8-45be-920a-f564d09e01b6","_cell_guid":"a3d8a3fb-a5b5-4072-b0af-6a4e23619c24","trusted":true}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif\n\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\",\"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    #discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_classif(X, y, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\ndef uninformative_cols(df, mi_scores):\n    return df.loc[:, mi_scores == 0.0].columns\n    \ny = df_data[df_data.state.isnull() == False]['state']\nx = df_data[df_data.state.isnull() == False].drop(['state','sequence','subject'],axis=1)\n\nmi_scores = make_mi_scores(x, y)\n#col = uninformative_cols(x, mi_scores)\n#x = x.drop(col,axis=1)\nprint('Uninformative features:\\n{}'.format(mi_scores[mi_scores == 0.0]))\nmi_scores = pd.DataFrame(mi_scores).reset_index().rename(columns={'index':'Feature'})","metadata":{"_uuid":"31397e64-ed2c-4cdf-8e71-b0eaff11d9e1","_cell_guid":"ccb57426-56a4-4637-8f49-1cb51faa04fe","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T11:54:34.736707Z","iopub.execute_input":"2022-04-15T11:54:34.737003Z","iopub.status.idle":"2022-04-15T12:04:10.017547Z","shell.execute_reply.started":"2022-04-15T11:54:34.736947Z","shell.execute_reply":"2022-04-15T12:04:10.016628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hereafter, we are gooin to plot results obtained previously in order to regard which features are the **<span style='color:lightseagreen'>most informative</span>**, and which ones requires some more analysis. You can see that we have a number of features that are highly informative and also some that don't seem to be informative at all (at least by themselves). Top scoring features will usually pay-off the **<span style='color:lightseagreen'>most during feature development</span>**, so it could be a good idea to focus your efforts on those. On the other hand, training on uninformative features can lead to overfitting.","metadata":{"_uuid":"a9305dc1-7102-4b85-a98c-f656424e0021","_cell_guid":"f18aa4cd-80bf-4a81-a5d0-89c2a7ed6622","trusted":true}},{"cell_type":"code","source":"fig = px.bar(mi_scores, x='MI Scores', y='Feature', color=\"MI Scores\", color_continuous_scale='darkmint')\n\nfig.update_xaxes(showgrid = False, showline = True, gridwidth = 0.05, linecolor = 'gray', zeroline = False, linewidth = 2)\nfig.update_yaxes(showline = True, gridwidth = 0.05, linecolor = 'gray', linewidth = 2, zeroline = False)\n\nfig.update_layout(height = 750, title_text=\"Mutual Information Scores\", plot_bgcolor='rgb(242, 242, 242)', paper_bgcolor = 'rgb(242, 242, 242)',\n                  title_font=dict(size=29, family=\"Lato, sans-serif\"), xaxis={'categoryorder':'category ascending'}, margin=dict(t=80))","metadata":{"_uuid":"fe0c4c1b-067c-4e3d-abed-7d67230a6781","_cell_guid":"c42e1134-da67-4b5e-b0b9-a50bc873f151","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T12:04:10.019206Z","iopub.execute_input":"2022-04-15T12:04:10.019546Z","iopub.status.idle":"2022-04-15T12:04:10.115062Z","shell.execute_reply.started":"2022-04-15T12:04:10.019502Z","shell.execute_reply":"2022-04-15T12:04:10.114116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"📌 **Interpret:** we can appreciate that every sensor feature has scored a very low value of Mutual Information. This means that they do not seem so informative, **<span style='color:lightseagreen'>at least by themselves</span>**. Therefore, in next sections we'll focus on creating features related to these sensors, in order to get features with better MI Scores. \n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.2 | Statistics Features</b></p>\n</div>\n\nIn this section we'll be creating some new features related to Sensor ones. We are going to start with some statistic features. The procedure will be grouping by each different sequence and then, adding features for: maximum, minimum, mean, median, std and kurtosis. Hereafter, we'll make an exhaustive analysis, in order to study which statistic features works better.","metadata":{"_uuid":"cef9090b-a2cb-49ef-b52f-b90d2d0f6d27","_cell_guid":"22d2567e-a212-48f9-a936-dae5a2d18a76","trusted":true}},{"cell_type":"code","source":"import scipy.stats\nsensors = [col for col in df_data.columns if 'sensor_' in col]\nfor sensor in range(13):\n    sensor_name = f\"sensor_{sensor:02d}\"    \n    df_data[f'{sensor_name}''_max'] = df_data.groupby('sequence')[f'{sensor_name}'].transform('max') \n    df_data[f'{sensor_name}''_min'] = df_data.groupby('sequence')[f'{sensor_name}'].transform('min') \n    df_data[f'{sensor_name}''_mean'] = df_data.groupby('sequence')[f'{sensor_name}'].transform('mean')  \n    df_data[f'{sensor_name}''_std'] = df_data.groupby('sequence')[f'{sensor_name}'].transform('std')  \n    df_data[f'{sensor_name}''_median'] = df_data.groupby('sequence')[f'{sensor_name}'].transform('median')\n    df_data[f'{sensor_name}''_flip'] = df_data[f'{sensor_name}'] * -1\n# Kurtosis\nsequence = df_data.sequence.unique()\nlength = range(60)\nfor sensor in range(13):\n    kurtosis = []\n    sensor_name = f\"sensor_{sensor:02d}\"\n    for i in sequence: \n        kurt = scipy.stats.kurtosis(df_data[df_data.sequence == i][sensor_name])\n        for j in length:\n            kurtosis.append(kurt)\n    df_data[sensor_name + '_kurtosis'] = kurtosis","metadata":{"_uuid":"360fb6e5-6058-4899-83b7-94d583a5447d","_cell_guid":"54271acd-8641-48a4-b584-2959af436eba","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-15T12:04:10.116229Z","iopub.execute_input":"2022-04-15T12:04:10.116474Z","iopub.status.idle":"2022-04-15T12:21:07.811234Z","shell.execute_reply.started":"2022-04-15T12:04:10.116445Z","shell.execute_reply":"2022-04-15T12:21:07.810053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.1 | Feature Selection\n\nIn this section we'll analyse the usefulness of the features that we've just created. We'll plot how the target depends on every feature, that is, a diagram of $P(y=1|x)$. To get a meaningful plot, we apply two transformations:\n\n* The x axis is not the value of the feature, but its index (when sorted by feature value).\n* The y axis is not the target value (which can be only 0 or 1), but a rolling mean over 1000 targets.\n\nFeatures with an horizontal line as diagram (the probability of the positive target is 0.5 independently of the feature value), are going to be considered bad ones, not useful. On the other hand, good features would have a curve with high $y_{max} - y_{min}$.\n\n#### 3.2.1.1 | Max Features\n\nWe can observe that almost each of them presents a 'normal' (not high enough to be directly selected) value of difference between maximum and miminum value on their respective diagram. Thus, they could be useful for us. We do not discard any of those yet.","metadata":{"_uuid":"7e385937-d323-4cdd-825e-b5ad561a50ac","_cell_guid":"1b6286c6-558b-49e0-8ab0-bf0bfa8a8076","trusted":true}},{"cell_type":"code","source":"state = []\nlength = range(int(df_data[df_data.state.isnull() == False].shape[0] / 60))\ndf_train = df_data[df_data.state.isnull() == False]\nfor j in length:\n    state.append(df_train.at[60*j,'state'])\n\nmax_df = pd.DataFrame({'state':state})\nmax_cols = [col for col in df_data.columns if '_max' in col]\nfor col in max_cols:\n    values=[]\n    for i in length:\n         values.append(df_train.at[60*i,col])\n    max_df[col] = values\n            \nplt.subplots(1, 4, sharey=True, sharex=True, figsize=(20, 25))\ncounter = 1\nfor col in max_df.drop('state',axis=1).columns:\n    temp = pd.DataFrame({col: max_df[col].values,\n                        'state': max_df['state']})\n    \n    temp = temp.sort_values(col)\n    temp.reset_index(inplace=True)\n    plt.subplot(15, 4, counter)\n    plt.scatter(temp.index, temp.state.rolling(1000).mean(), s=2)\n    plt.xlabel(col)\n    plt.xticks([])\n    counter = counter +1\nplt.show()","metadata":{"_uuid":"b0a141eb-5294-4b0b-af04-d08c978b357b","_cell_guid":"e62daa79-957f-4b64-b9a3-ae7d3e9500fe","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T12:21:07.813178Z","iopub.execute_input":"2022-04-15T12:21:07.813874Z","iopub.status.idle":"2022-04-15T12:21:19.996362Z","shell.execute_reply.started":"2022-04-15T12:21:07.813808Z","shell.execute_reply":"2022-04-15T12:21:19.995758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2.1.2 | Min Features\n\nIn this case, we can observe that there are some features (like sensor_06_min) that present almost a horizontal line diagram. Those features, as commented previously are not going to be quite useful.","metadata":{"_uuid":"ae0ef0ff-1b65-495f-8652-eeeffffba8f5","_cell_guid":"bc9c90bc-1965-4276-a1e7-b7dabdd76a1f","trusted":true}},{"cell_type":"code","source":"max_df = pd.DataFrame({'state':state})\nmax_cols = [col for col in df_data.columns if '_min' in col]\nfor col in max_cols:\n    values=[]\n    for i in length:\n         values.append(df_train.at[60*i,col])\n    max_df[col] = values\n            \nplt.subplots(1, 4, sharey=True, sharex=True, figsize=(20, 25))\ncounter = 1\nfor col in max_df.drop('state',axis=1).columns:\n    temp = pd.DataFrame({col: max_df[col].values,\n                        'state': max_df['state']})\n    \n    temp = temp.sort_values(col)\n    temp.reset_index(inplace=True)\n    plt.subplot(15, 4, counter)\n    plt.scatter(temp.index, temp.state.rolling(1000).mean(), s=2)\n    plt.xlabel(col)\n    plt.xticks([])\n    counter = counter +1\nplt.show()","metadata":{"_uuid":"c9d283d7-0d8b-4a9d-a9b8-5b80b163b3a4","_cell_guid":"ee07d38c-0c11-438c-9d5c-33b2163c8824","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T12:21:19.997501Z","iopub.execute_input":"2022-04-15T12:21:19.99782Z","iopub.status.idle":"2022-04-15T12:21:29.965792Z","shell.execute_reply.started":"2022-04-15T12:21:19.997793Z","shell.execute_reply":"2022-04-15T12:21:29.964868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2.1.3 | Mean Features\n\nNow, regarding mean features charts we can observe that a big amount of them are convex curves. But taking a view carefully, we can appreciate that some of them have a low value of difference between the maxim and minimum value. Some of them are: sensor_00, sensor_03, sensor_05, sensor_09, sensor_10.","metadata":{"_uuid":"12df8ce4-2bf3-49a2-8721-5eae17d6b299","_cell_guid":"b92d7160-4fef-4f9f-8ab2-591868a4cafd","trusted":true}},{"cell_type":"code","source":"max_df = pd.DataFrame({'state':state})\nmax_cols = [col for col in df_data.columns if '_mean' in col]\nfor col in max_cols:\n    values=[]\n    for i in length:\n         values.append(df_train.at[60*i,col])\n    max_df[col] = values\n            \nplt.subplots(1, 4, sharey=True, sharex=True, figsize=(20, 25))\ncounter = 1\nfor col in max_df.drop('state',axis=1).columns:\n    temp = pd.DataFrame({col: max_df[col].values,\n                        'state': max_df['state']})\n    \n    temp = temp.sort_values(col)\n    temp.reset_index(inplace=True)\n    plt.subplot(15, 4, counter)\n    plt.scatter(temp.index, temp.state.rolling(1000).mean(), s=2)\n    plt.xlabel(col)\n    plt.xticks([])\n    counter = counter +1\nplt.show()","metadata":{"_uuid":"47be2e7b-f081-4bf9-8f5c-9a3e1248344c","_cell_guid":"d5b0b9a1-f224-40e8-9e14-e6b1141736c2","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T12:21:29.966898Z","iopub.execute_input":"2022-04-15T12:21:29.967113Z","iopub.status.idle":"2022-04-15T12:21:39.910705Z","shell.execute_reply.started":"2022-04-15T12:21:29.967086Z","shell.execute_reply":"2022-04-15T12:21:39.909665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2.1.4 | Median Features\n\nMedian features are not going to be of very usefulness. They're the ones with lowest value on difference between maximum and minimum values.","metadata":{"_uuid":"ee5273b1-bce2-4d5a-87a6-e1344e93a52d","_cell_guid":"78265fcc-8175-4286-99d7-07ff5835e881","trusted":true}},{"cell_type":"code","source":"max_df = pd.DataFrame({'state':state})\nmax_cols = [col for col in df_data.columns if '_median' in col]\nfor col in max_cols:\n    values=[]\n    for i in length:\n         values.append(df_train.at[60*i,col])\n    max_df[col] = values\n            \nplt.subplots(1, 4, sharey=True, sharex=True, figsize=(20, 25))\ncounter = 1\nfor col in max_df.drop('state',axis=1).columns:\n    temp = pd.DataFrame({col: max_df[col].values,\n                        'state': max_df['state']})\n    \n    temp = temp.sort_values(col)\n    temp.reset_index(inplace=True)\n    plt.subplot(15, 4, counter)\n    plt.scatter(temp.index, temp.state.rolling(1000).mean(), s=2)\n    plt.xlabel(col)\n    plt.xticks([])\n    counter = counter +1\nplt.show()","metadata":{"_uuid":"c2a17dd0-29b6-4dfc-9f85-edabaa5ac040","_cell_guid":"201eb5fd-7bdd-4ae2-9ea6-deb8454e82c5","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T12:21:39.912069Z","iopub.execute_input":"2022-04-15T12:21:39.912322Z","iopub.status.idle":"2022-04-15T12:21:50.458819Z","shell.execute_reply.started":"2022-04-15T12:21:39.912294Z","shell.execute_reply":"2022-04-15T12:21:50.457919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2.1.5 | Std Features\n\nIn this case, we can observe one potential feature. That's the case of **<span style='color:lightseagreen'>sensor_02_std</span>**. Looking at the y-axes, we can appreciate that difference value is quite high.","metadata":{"_uuid":"d9ba53c4-a6f3-4429-914a-f94d6edb8223","_cell_guid":"20517e5c-1dd2-4ffc-b717-c270155a2c39","trusted":true}},{"cell_type":"code","source":"max_df = pd.DataFrame({'state':state})\nmax_cols = [col for col in df_data.columns if '_std' in col]\nfor col in max_cols:\n    values=[]\n    for i in length:\n         values.append(df_train.at[60*i,col])\n    max_df[col] = values\n            \nplt.subplots(1, 4, sharey=True, sharex=True, figsize=(20, 25))\ncounter = 1\nfor col in max_df.drop('state',axis=1).columns:\n    temp = pd.DataFrame({col: max_df[col].values,\n                        'state': max_df['state']})\n    \n    temp = temp.sort_values(col)\n    temp.reset_index(inplace=True)\n    plt.subplot(15, 4, counter)\n    plt.scatter(temp.index, temp.state.rolling(1000).mean(), s=2)\n    plt.xlabel(col)\n    plt.xticks([])\n    counter = counter +1\nplt.show()","metadata":{"_uuid":"0ac53dba-6174-4c62-bf5b-e1bde6bb1d80","_cell_guid":"636e2c76-d657-45bc-ae6d-aff6b057b5f9","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T12:21:50.460255Z","iopub.execute_input":"2022-04-15T12:21:50.4605Z","iopub.status.idle":"2022-04-15T12:22:00.373872Z","shell.execute_reply.started":"2022-04-15T12:21:50.460473Z","shell.execute_reply":"2022-04-15T12:22:00.372847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2.1.6 | Kurtosis Features\n\nBy far, kurtosis features are the best ones. We can observe plenty potential feature. That's the case of **<span style='color:lightseagreen'>sensor_04_kurtosis</span>**, for example. Looking at the y-axes, we can appreciate that difference value is incredibly high.","metadata":{"_uuid":"31b97f69-12ac-4273-be6e-e28f0cd61d04","_cell_guid":"2e354738-d418-42d0-998c-d3bf7621c817","trusted":true}},{"cell_type":"code","source":"max_df = pd.DataFrame({'state':state})\nmax_cols = [col for col in df_data.columns if '_kurtosis' in col]\nfor col in max_cols:\n    values=[]\n    for i in length:\n         values.append(df_train.at[60*i,col])\n    max_df[col] = values\n            \nplt.subplots(1, 4, sharey=True, sharex=True, figsize=(20, 25))\ncounter = 1\nfor col in max_df.drop('state',axis=1).columns:\n    temp = pd.DataFrame({col: max_df[col].values,\n                        'state': max_df['state']})\n    \n    temp = temp.sort_values(col)\n    temp.reset_index(inplace=True)\n    plt.subplot(15, 4, counter)\n    plt.scatter(temp.index, temp.state.rolling(1000).mean(), s=2)\n    plt.xlabel(col)\n    plt.xticks([])\n    counter = counter +1\nplt.show()","metadata":{"_uuid":"1456b452-0f01-4c73-94fe-29cb117468d1","_cell_guid":"0d072c6e-ca19-4301-b83e-d914ae8c0d1e","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T12:22:00.375333Z","iopub.execute_input":"2022-04-15T12:22:00.376013Z","iopub.status.idle":"2022-04-15T12:22:10.414701Z","shell.execute_reply.started":"2022-04-15T12:22:00.375976Z","shell.execute_reply":"2022-04-15T12:22:10.413995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2.1.7 | Flip Features\n\nWe can observe that flip features are completely useless for us. They obtain by far the lowest difference value between $y_{max} - y_{min}$.","metadata":{"_uuid":"fb042c95-b016-4eec-86ea-ff5c1b34f872","_cell_guid":"b2ef3702-9d9b-49f3-a4e4-d3cc75941292","trusted":true}},{"cell_type":"code","source":"max_df = pd.DataFrame({'state':state})\nmax_cols = [col for col in df_data.columns if '_flip' in col]\nfor col in max_cols:\n    values=[]\n    for i in length:\n         values.append(df_train.at[60*i,col])\n    max_df[col] = values\n            \nplt.subplots(1, 4, sharey=True, sharex=True, figsize=(20, 25))\ncounter = 1\nfor col in max_df.drop('state',axis=1).columns:\n    temp = pd.DataFrame({col: max_df[col].values,\n                        'state': max_df['state']})\n    \n    temp = temp.sort_values(col)\n    temp.reset_index(inplace=True)\n    plt.subplot(15, 4, counter)\n    plt.scatter(temp.index, temp.state.rolling(1000).mean(), s=2)\n    plt.xlabel(col)\n    plt.xticks([])\n    counter = counter +1\nplt.show()","metadata":{"_uuid":"9eeab7fe-14ab-4799-ba51-9cd2b6243b5a","_cell_guid":"61f7db3a-f000-4dd1-bf57-416b95b349c6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T12:22:10.415948Z","iopub.execute_input":"2022-04-15T12:22:10.416878Z","iopub.status.idle":"2022-04-15T12:22:20.400523Z","shell.execute_reply.started":"2022-04-15T12:22:10.416815Z","shell.execute_reply":"2022-04-15T12:22:20.399892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"📌 **Interpret:** we can make the following conclusions after analysing the previous chart plotted: \n\n* **<span style='color:lightseagreen'>Kurtosis Features</span>** seems to be the ones working better, as it has obtained the **<span style='color:lightseagreen'>highest difference values</span>** between maximum and minimum values.\n* **<span style='color:lightseagreen'>Standard desviation</span>** feature for **<span style='color:lightseagreen'>sensor 2</span>** is one the best ones. \n* **<span style='color:lightseagreen'>Mean and Median</span>** seem to be useless features. \n\nTaking everything mentioned above into account, we are going to drop the following features:","metadata":{"_uuid":"3dd876d9-0ebf-450c-9cb3-4a36a5dc358d","_cell_guid":"bb48f270-7aad-4c54-9651-ff8c18b16bc5","trusted":true}},{"cell_type":"code","source":"max_col = [col for col in df_data.columns if '_max' in col]\nmin_col = [col for col in df_data.columns if '_min' in col]\nmean_col = [col for col in df_data.columns if '_mean' in col]\nmean_col.remove('sensor_04_mean')\nmedian_col = [col for col in df_data.columns if '_median' in col]\nstd_col = [col for col in df_data.columns if '_std' in col]\nstd_col.remove('sensor_02_std')\nkurtosis_col = [col for col in df_data.columns if '_kurtosis' in col]\nkurtosis_col.remove('sensor_10_kurtosis')\nkurtosis_col.remove('sensor_04_kurtosis')\nflip_col = [col for col in df_data.columns if '_flip' in col]\n\n# Drop useless features\ndf_data = df_data.drop(max_col, axis=1)\ndf_data = df_data.drop(min_col, axis=1)\ndf_data = df_data.drop(mean_col, axis=1)\ndf_data = df_data.drop(std_col, axis=1)\ndf_data = df_data.drop(median_col, axis=1)\ndf_data = df_data.drop(kurtosis_col, axis=1)\ndf_data = df_data.drop(flip_col, axis=1)","metadata":{"_uuid":"3eab4f36-d6d1-4300-8ae5-9710adf33422","_cell_guid":"0ad9866b-2ba3-4bf9-a235-04fda383b61e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T12:22:20.401639Z","iopub.execute_input":"2022-04-15T12:22:20.402085Z","iopub.status.idle":"2022-04-15T12:22:22.917436Z","shell.execute_reply.started":"2022-04-15T12:22:20.402039Z","shell.execute_reply":"2022-04-15T12:22:22.916519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.3 | Time Series Features</b></p>\n</div>\n\n### 3.3.1 | Lag Features. Autocorrelation (ACF). Partial Autocorrelation (PACF)\n\n* **<span style='color:lightseagreen'>Autocorrelation</span>** - The autocorrelation function (ACF) measures how a series is correlated with itself at different lags.\n* **<span style='color:lightseagreen'>Partial Autocorrelation</span>** - The partial autocorrelation function can be interpreted as a regression of the series against its past lags. The terms can be interpreted the same way as a standard linear regression, that is the contribution of a change in that particular lag while holding others constant.\n\n\nLagging a time series means to **<span style='color:lightseagreen'>shift its values forward</span>** one or more time steps, or equivalently, to **<span style='color:lightseagreen'>shift the times</span>** in its index backward one or more steps. In either case, the effect is that the observations in the lagged series will appear to have happened later in time. Here we've created a **<span style='color:lightseagreen'>1-step lag feature</span>**, though shifting by multiple steps is possible too.","metadata":{"_uuid":"2bd43498-7e86-4ac0-a351-b149e825b1d4","_cell_guid":"c0e9f501-d843-4b51-aa3a-9b7eb89db795","trusted":true}},{"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n#plot_acf(df_data['sensor_02'],lags=2,title=\"Sensor 2\")\n#plot_pacf(df_data['sensor_03'],lags=2,title=\"Sensor 2\")","metadata":{"_uuid":"e4bdf3d5-f1f2-488c-a2c5-8febb5a75b4f","_cell_guid":"aca8647d-79d4-43a3-850d-65071b34428f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T12:22:22.91894Z","iopub.execute_input":"2022-04-15T12:22:22.919216Z","iopub.status.idle":"2022-04-15T12:22:22.98581Z","shell.execute_reply.started":"2022-04-15T12:22:22.919179Z","shell.execute_reply":"2022-04-15T12:22:22.985122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sensor in range(13):\n    sensor_name = f\"sensor_{sensor:02d}\"    \n    df_data[f\"{sensor_name}\" + '_lag1'] = df_data.groupby('sequence')[f\"{sensor_name}\"].shift(1)  \n    df_data[f\"{sensor_name}\" + '_lag1'].fillna(0, inplace=True)\n    df_data[f\"{sensor_name}\" + '_diff1'] = df_data[f\"{sensor_name}\"] - df_data[f\"{sensor_name}\" + '_lag1']\n    #df_data[f\"{sensor_name}\" + '_flip'] = df_data[f\"{sensor_name}\"] * -1","metadata":{"_uuid":"76a01922-bf8e-47bf-b509-f5c9ab27dff8","_cell_guid":"55211400-7172-4345-9f5d-f210dabccc48","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T12:22:22.989362Z","iopub.execute_input":"2022-04-15T12:22:22.989778Z","iopub.status.idle":"2022-04-15T12:22:24.499663Z","shell.execute_reply.started":"2022-04-15T12:22:22.989737Z","shell.execute_reply":"2022-04-15T12:22:24.498675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>4 <span style='color:lightseagreen'>|</span> Modeling</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.1 | Recurrent Neural Networks </b></p>\n</div>\n\nIn a recurrent neural network we store the output activations from one or more of the layers of the network. Often these are hidden later activations. Then, the next time we feed an input example to the network, we include the previously-stored outputs as additional inputs. You can think of the additional inputs as being concatenated to the end of the “normal” inputs to the previous layer. For example, if a hidden layer had 10 regular input nodes and 128 hidden nodes in the layer, then it would actually have 138 total inputs (assuming you are feeding the layer’s outputs into itself à la Elman) rather than into another layer). Of course, the very first time you try to compute the output of the network you’ll need to fill in those extra 128 inputs with 0s or something.\n\n![](https://cdn-images-1.medium.com/max/1600/1*NKhwsOYNUT5xU7Pyf6Znhg.png)\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.2 | Long Short Term Memory (LSTM) </b></p>\n</div>\n\nModeling section tends to appear usually on final sections of kernel. However, this time **<span style='color:lightseagreen'>I decided to include it earlier in order to use LSTM in Local CV Scoring Function (Feature Engineering next section)</span>**. Now an introduction: Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997), and were refined and popularized by many people in following work.1 They work tremendously well on a large variety of problems, and are now widely used. LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn! All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer. Before moving into the implementation, just mention that this following part has been taken from [Torch Me LSTM Section](https://www.kaggle.com/code/kartushovdanil/top-1-tps-apr-22-eda-lstm). \n\n![](https://cdn-images-1.medium.com/max/1600/0*LyfY3Mow9eCYlj7o.)\n\n<div class=\"alert alert-block alert-info\"> 📌 With neural networks, it's generally a good idea to put all of your data on a common scale, perhaps with something like scikit-learn's StandardScaler or MinMaxScaler.</div> The reason is that SGD (Stochastic Gradient Descent) will shift the network weights in proportion to how large an activation the data produces. Features that tend to produce activations of very different sizes can make for unstable training behavior.\n\n\n### 4.2.1 | Components of LSTMs\n\nSo the LSTM cell contains the following components\n\n* Forget Gate “f” ( a neural network with sigmoid)\n* Candidate layer “C\"(a NN with Tanh)\n* Input Gate “I” ( a NN with sigmoid )\n* Output Gate “O”( a NN with sigmoid)\n* Hidden state “H” ( a vector )\n* Memory state “C” ( a vector)\n* Inputs to the LSTM cell at any step are Xt (current input) , Ht-1 (previous hidden state ) and Ct-1 (previous memory state).\n* Outputs from the LSTM cell are Ht (current hidden state ) and Ct (current memory state)\n\n### 4.2.2 | Working of gates in LSTMs\n\nFirst, LSTM cell takes the previous memory state Ct-1 and does element wise multiplication with forget gate (f) to decide if present memory state Ct. If forget gate value is 0 then previous memory state is completely forgotten else f forget gate value is 1 then previous memory state is completely passed to the cell ( Remember f gate gives values between 0 and 1 ).\n\n* $C_t$ = $C_{t-1}$ * $f_t$\n\nCalculating the new memory state:\n\n* $C_t$ = $C_t$ + ($I_t$ * $C`_t$)\n* Now, we calculate the output:\n* $H_t$ = $tanh(C_t)$","metadata":{"_uuid":"6d4d848e-a855-4bb7-9777-1f6345167f89","_cell_guid":"fd1ca07a-0d63-4d0c-a376-8be6abd21e01","trusted":true}},{"cell_type":"code","source":"features = df_data.columns.tolist()[3:]\nsc = StandardScaler()\n\ntrain_df = df_data[df_data.state.isnull() == False]\ntest_df = df_data[df_data.state.isnull() == True]\n\ntrain_df[features] = sc.fit_transform(train_df[features])\ntest_df[features] = sc.transform(test_df[features])\n\ngroups = train_df[\"sequence\"]\ntrain_labels = pd.read_csv('../input/tabular-playground-series-apr-2022/train_labels.csv')\nlabels = train_labels[\"state\"]\n\ntrain_df = train_df.drop([\"sequence\", \"subject\", \"step\",'state'], axis=1).values\ntrain_df = train_df.reshape(-1, 60, train_df.shape[-1])\n\ntest_df = test_df.drop([\"sequence\", \"subject\", \"step\",'state'], axis=1).values\ntest_df = test_df.reshape(-1, 60, test_df.shape[-1])","metadata":{"_uuid":"942ce0e7-8aed-4cbe-8adc-c57cd8e24df5","_cell_guid":"99f2c784-0dee-4082-92fa-a6411df8810f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T12:22:24.501109Z","iopub.execute_input":"2022-04-15T12:22:24.501624Z","iopub.status.idle":"2022-04-15T12:22:32.637715Z","shell.execute_reply.started":"2022-04-15T12:22:24.501577Z","shell.execute_reply":"2022-04-15T12:22:32.636722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    BATCH_SIZE = tpu_strategy.num_replicas_in_sync * 64\n    print(\"Running on TPU:\", tpu.master())\n    print(f\"Batch Size: {BATCH_SIZE}\")\n    \nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\n    BATCH_SIZE = 256\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    print(f\"Batch Size: {BATCH_SIZE}\")\n    \ndef dnn_model():\n    \n    x_input = Input(shape=(train_df.shape[-2:]))\n    \n    x1 = Bidirectional(LSTM(units=512, return_sequences=True))(x_input)\n    x2 = Bidirectional(LSTM(units=256, return_sequences=True))(x1)\n    z1 = Bidirectional(GRU(units=256, return_sequences=True))(x1)\n    \n    c = Concatenate(axis=2)([x2, z1])\n    \n    x3 = Bidirectional(LSTM(units=128, return_sequences=True))(c)\n    \n    x4 = GlobalMaxPooling1D()(x3)\n    x5 = Dense(units=128, activation='selu')(x4)\n    x_output = Dense(1, activation='sigmoid')(x5)\n\n    model = Model(inputs=x_input, outputs=x_output, name='lstm_model')\n    \n    return model\n\nmodel = dnn_model()\nclear_output()\nplot_model(model, show_shapes=True)","metadata":{"_uuid":"f0111556-0154-4647-b435-44edb83cc207","_cell_guid":"def9bd6f-1513-4438-ae72-15525821a797","collapsed":false,"_kg_hide-output":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-15T12:22:32.638965Z","iopub.execute_input":"2022-04-15T12:22:32.639199Z","iopub.status.idle":"2022-04-15T12:22:42.908187Z","shell.execute_reply.started":"2022-04-15T12:22:32.639174Z","shell.execute_reply":"2022-04-15T12:22:42.907026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.3 | Making Predictions </b></p>\n</div>","metadata":{"_uuid":"3c93fd1e-1246-4e23-9046-b2701e177cc7","_cell_guid":"34bea606-6f30-4531-86a5-57166add0d51","trusted":true}},{"cell_type":"code","source":"def plotHist(hist):\n    plt.plot(hist.history[\"auc\"])\n    plt.plot(hist.history[\"val_auc\"])\n    plt.title(\"model performance\")\n    plt.ylabel(\"area_under_curve\")\n    plt.xlabel(\"epoch\")\n    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n    plt.show()\n    return","metadata":{"_uuid":"519027df-197d-4482-adc6-b04cd3df9928","_cell_guid":"187f6f8b-8381-4f6e-beca-30697d9d8011","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T12:22:42.910267Z","iopub.execute_input":"2022-04-15T12:22:42.911052Z","iopub.status.idle":"2022-04-15T12:22:42.918179Z","shell.execute_reply.started":"2022-04-15T12:22:42.911008Z","shell.execute_reply":"2022-04-15T12:22:42.916943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tpu_strategy.scope():\n    VERBOSE = True\n    predictions, scores = [], []\n    k = GroupKFold(n_splits = 10)\n\n    for fold, (train_idx, val_idx) in enumerate(k.split(train_df, labels, groups.unique())):\n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n    \n        X_train, X_val = train_df[train_idx], train_df[val_idx]\n        y_train, y_val = labels.iloc[train_idx].values, labels.iloc[val_idx].values\n        \n        model = dnn_model()\n        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics='AUC')\n\n        lr = ReduceLROnPlateau(monitor=\"val_auc\", factor=0.6, \n                               patience=4, verbose=VERBOSE)\n\n        es = EarlyStopping(monitor=\"val_auc\", patience=2, \n                           verbose=VERBOSE, mode=\"max\", \n                           restore_best_weights=True)\n        \n        save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n        chk_point = ModelCheckpoint(f'./TPS_model_2022_{fold+1}C.h5', options=save_locally, \n                                    monitor='val_auc', verbose=VERBOSE, \n                                    save_best_only=True, mode='max')\n        \n        history = model.fit(X_train, y_train, \n                  validation_data=(X_val, y_val), \n                  epochs=15,\n                  verbose=VERBOSE,\n                  batch_size=BATCH_SIZE, \n                  callbacks=[lr, chk_point, es])\n        \n        load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n        model = load_model(f'./TPS_model_2022_{fold+1}C.h5', options=load_locally)\n        \n        y_pred = model.predict(X_val, batch_size=BATCH_SIZE).squeeze()\n        score = roc_auc_score(y_val, y_pred)\n        scores.append(score)\n        predictions.append(model.predict(test_df, batch_size=BATCH_SIZE).squeeze())\n        print(f\"Fold-{fold+1} | OOF Score: {score}\")\n        \n        history_df = pd.DataFrame(history.history)\n        history_df.loc[:, ['loss', 'val_loss']].plot();\n        plt.title(f\"{fold+1}\")\n        print(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))\n    \n    print(f'Mean accuracy on {k.n_splits} folds - {np.mean(scores)}')","metadata":{"_uuid":"57cf7e48-b3a6-4424-8020-38240b070ccf","_cell_guid":"92344445-15de-4cdf-b543-dcc42aefc282","collapsed":false,"_kg_hide-input":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T12:22:42.919989Z","iopub.execute_input":"2022-04-15T12:22:42.920329Z","iopub.status.idle":"2022-04-15T12:38:07.984731Z","shell.execute_reply.started":"2022-04-15T12:22:42.920277Z","shell.execute_reply":"2022-04-15T12:38:07.983585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ssub = pd.read_csv('../input/tabular-playground-series-apr-2022/sample_submission.csv')\nssub[\"state\"] = sum(predictions)/k.n_splits \nssub.to_csv('submission.csv', index=False)\nssub.head(3)","metadata":{"_uuid":"e3c8e79c-2294-4d4f-9184-aa67b861d857","_cell_guid":"d7aa395a-0ea2-409a-84f9-c8d93caddf19","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T12:38:07.986086Z","iopub.execute_input":"2022-04-15T12:38:07.986508Z","iopub.status.idle":"2022-04-15T12:38:08.060879Z","shell.execute_reply.started":"2022-04-15T12:38:07.986475Z","shell.execute_reply":"2022-04-15T12:38:08.059946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"8d6efbcf-c3e4-4a92-a6e0-e9524b4304e1","_cell_guid":"e0569800-6173-404c-9a84-4355f47ac3fa","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}