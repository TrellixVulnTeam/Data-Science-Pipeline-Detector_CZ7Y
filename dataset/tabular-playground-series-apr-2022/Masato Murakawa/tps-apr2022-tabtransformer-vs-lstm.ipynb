{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n# [Tabular Playground Series - Apr 2022][1]\n\n- This challenge is a time series classification problem.\n\n- The goal of this competition is to predict the state of sequence from the sensor data.\n\n---\n\n#### **The aim of this notebook is to**\n- **1. Conduct Exploratory Data Analysis with TensorFlow Data Validation (TFDV)**\n- **2. Compare the customized TabTransformer model and RNN(LSTM) model.**\n\n#### **Conclusions**\n- **RNN(LSTM) model seems to be more suitable for this competition's task.**\n\n\n---\n**References:** Thanks to previous great codes and notebooks.\n- [Get started with Tensorflow Data Validation][2]\n- [Migrating feature_columns to TF2's Keras Preprocessing Layers][3]\n- [Classify structured data using Keras preprocessing layers][4]\n- [Sachin's Blog Tensorflow Learning Rate Finder][5]\n- [ðŸ”¥ðŸ”¥[TensorFlow]TabTransformerðŸ”¥ðŸ”¥][6]\n- [Top 1% | TPS APR 22 EDA | LSTM][7]\n- [TPS Apr22 - EDA / FE + LSTM Tutorial][8]\n\n---\n\n#### **If you find this notebook useful, please do give me an upvote. It helps to keep up my motivation.**\n\n---\n\n[1]: https://www.kaggle.com/competitions/tabular-playground-series-apr-2022/overview\n[2]: https://www.tensorflow.org/tfx/data_validation/get_started\n[3]: https://www.tensorflow.org/guide/migrate/migrating_feature_columns\n[4]: https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers\n[5]: https://sachinruk.github.io/blog/tensorflow/learning%20rate/2021/02/15/Tensorflow-Learning-Rate-Finder.html\n[6]: https://www.kaggle.com/code/usharengaraju/tensorflow-tabtransformer\n[7]: https://www.kaggle.com/code/kartushovdanil/top-1-tps-apr-22-eda-lstm\n[8]: https://www.kaggle.com/code/javigallego/tps-apr22-eda-fe-lstm-tutorial","metadata":{}},{"cell_type":"markdown","source":"# 0. Settings","metadata":{}},{"cell_type":"code","source":"# Import dependencies \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\n\nimport seaborn as sns\n\nimport os\nimport pathlib\nimport gc\nimport sys\nimport re\nimport math \nimport random\nimport time \nimport datetime as dt\nfrom tqdm import tqdm \nfrom pprint import pprint\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow_addons as tfa\n\nprint('import done!')","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:26:40.713804Z","iopub.execute_input":"2022-04-16T07:26:40.714415Z","iopub.status.idle":"2022-04-16T07:26:46.407701Z","shell.execute_reply.started":"2022-04-16T07:26:40.714323Z","shell.execute_reply":"2022-04-16T07:26:46.406151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For reproducible results    \ndef seed_all(s):\n    random.seed(s)\n    np.random.seed(s)\n    tf.random.set_seed(s)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    os.environ['PYTHONHASHSEED'] = str(s) \n    print('Seeds setted!')\n    \nglobal_seed = 42\nseed_all(global_seed)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:26:46.409285Z","iopub.execute_input":"2022-04-16T07:26:46.409656Z","iopub.status.idle":"2022-04-16T07:26:46.417719Z","shell.execute_reply.started":"2022-04-16T07:26:46.409602Z","shell.execute_reply":"2022-04-16T07:26:46.416465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data Loading & Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Data Loading","metadata":{}},{"cell_type":"markdown","source":"---\n### [Files Descriptions](https://www.kaggle.com/competitions/tabular-playground-series-apr-2022/data)\n\n- **train.csv** - the training set, comprising ~26,000 60-second recordings of thirteen biological sensors for almost one thousand experimental participants\n\n- **train_labels.csv** - the class label for each sequence.\n\n- **test.csv** - the test set. For each of the ~12,000 sequences, you should predict a value for that sequence's state.\n\n- **sample_submission.csv** - a sample submission file in the correct format.\n\n---","metadata":{}},{"cell_type":"code","source":"data_config = {'train_csv_path': '../input/tabular-playground-series-apr-2022/train.csv',\n               'train_labels_path': '../input/tabular-playground-series-apr-2022/train_labels.csv',\n               'test_csv_path': '../input/tabular-playground-series-apr-2022/test.csv',\n               'sample_submission_path': '../input/tabular-playground-series-apr-2022/sample_submission.csv',\n              }\n\ntrain_df = pd.read_csv(data_config['train_csv_path'])\ntrain_labels_df = pd.read_csv(data_config['train_labels_path'])\ntest_df = pd.read_csv(data_config['test_csv_path'])\nsubmission_df = pd.read_csv(data_config['sample_submission_path'])\n\nprint(f'train_length: {len(train_df)}')\nprint(f'train_labels_length: {len(train_labels_df)}')\nprint(f'test_length: {len(test_df)}')\nprint(f'submission_length: {len(submission_df)}')","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:26:46.418834Z","iopub.execute_input":"2022-04-16T07:26:46.419065Z","iopub.status.idle":"2022-04-16T07:26:56.959723Z","shell.execute_reply.started":"2022-04-16T07:26:46.419034Z","shell.execute_reply":"2022-04-16T07:26:56.958155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Data Check","metadata":{}},{"cell_type":"markdown","source":"---\n### [Field Descriptions](https://www.kaggle.com/competitions/tabular-playground-series-apr-2022/data)\n\n- **train.csv**\n - `sequence` - a unique id for each sequence\n - `subject` - a unique id for the subject in the experiment\n - `step` - time step of the recording, in one second intervals\n - `sensor_00` - `sensor_12` - the value for each of the thirteen sensors at that time step\n\n- **train_labels.csv** - the class label for each sequence.\n - `sequence` - the unique id for each sequence.\n - `state` - the state associated to each sequence. This is the target which you are trying to predict.\n \n---","metadata":{}},{"cell_type":"code","source":"# Null Value Check\nprint('train_df.info()'); print(train_df.info(), '\\n')\nprint('train_labels_df.info()'); print(train_labels_df.info(), '\\n')\nprint('test_df.info()'); print(test_df.info(), '\\n')\nprint('submission_df.info()');  print(submission_df.info(), '\\n')","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-04-16T07:26:56.961764Z","iopub.execute_input":"2022-04-16T07:26:56.962192Z","iopub.status.idle":"2022-04-16T07:26:57.076103Z","shell.execute_reply.started":"2022-04-16T07:26:56.96215Z","shell.execute_reply":"2022-04-16T07:26:57.075307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_only_subject = [s for s in test_df['subject'].unique() if s not in train_df['subject'].unique()]\n#print(len(test_only_subject))\n\nif len(test_only_subject) == len(test_df['subject'].unique()):\n    print('There is no overlap in \"subject\" between train and test data.')\nelse:\n    print('There are some overlaps in \"subject\" between train and test data.')","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:26:57.07718Z","iopub.execute_input":"2022-04-16T07:26:57.077409Z","iopub.status.idle":"2022-04-16T07:26:59.509677Z","shell.execute_reply.started":"2022-04-16T07:26:57.077376Z","shell.execute_reply":"2022-04-16T07:26:59.508915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df Check\ntrain_df.head()","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-16T07:26:59.510817Z","iopub.execute_input":"2022-04-16T07:26:59.511451Z","iopub.status.idle":"2022-04-16T07:26:59.536294Z","shell.execute_reply.started":"2022-04-16T07:26:59.511408Z","shell.execute_reply":"2022-04-16T07:26:59.535536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df.duplicated().value_counts())\ntrain_df = train_df.drop_duplicates()","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-16T07:26:59.537603Z","iopub.execute_input":"2022-04-16T07:26:59.537869Z","iopub.status.idle":"2022-04-16T07:27:02.504049Z","shell.execute_reply.started":"2022-04-16T07:26:59.537835Z","shell.execute_reply":"2022-04-16T07:27:02.50328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_unique_category(df, column):\n    print(f'feature_name: {column}')\n    print(f'unique_category_number: {df[column].nunique()}')\n    print(f'categories: {df[column].unique()}\\n')\n\n# Categories in train_df\nprint_unique_category(train_df, 'sequence')\nprint_unique_category(train_df, 'subject')\nprint_unique_category(train_df, 'step')","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-16T07:27:02.505299Z","iopub.execute_input":"2022-04-16T07:27:02.505546Z","iopub.status.idle":"2022-04-16T07:27:02.565841Z","shell.execute_reply.started":"2022-04-16T07:27:02.505511Z","shell.execute_reply":"2022-04-16T07:27:02.564989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_labels_df Check\ntrain_labels_df.head()","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-16T07:27:02.567154Z","iopub.execute_input":"2022-04-16T07:27:02.567474Z","iopub.status.idle":"2022-04-16T07:27:02.57605Z","shell.execute_reply.started":"2022-04-16T07:27:02.567435Z","shell.execute_reply":"2022-04-16T07:27:02.575233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Categories in test_df \nprint_unique_category(test_df, 'sequence')\nprint_unique_category(test_df, 'subject')\nprint_unique_category(test_df, 'step')\n\n# test_df Check\ntest_df.head()","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-16T07:27:02.580051Z","iopub.execute_input":"2022-04-16T07:27:02.580327Z","iopub.status.idle":"2022-04-16T07:27:02.626489Z","shell.execute_reply.started":"2022-04-16T07:27:02.58029Z","shell.execute_reply":"2022-04-16T07:27:02.625692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission_df check\nsubmission_df.head()","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-16T07:27:02.627927Z","iopub.execute_input":"2022-04-16T07:27:02.628176Z","iopub.status.idle":"2022-04-16T07:27:02.637515Z","shell.execute_reply.started":"2022-04-16T07:27:02.628143Z","shell.execute_reply":"2022-04-16T07:27:02.636684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Data Scaling","metadata":{}},{"cell_type":"markdown","source":"### 1.3.1 Train Data","metadata":{}},{"cell_type":"code","source":"sensors = np.array(['sensor_00', 'sensor_01', 'sensor_02', 'sensor_03', 'sensor_04', 'sensor_05', 'sensor_06',\n           'sensor_07', 'sensor_08', 'sensor_09', 'sensor_10', 'sensor_11', 'sensor_12',])\n\ntrain_mean = train_df.mean()\ntrain_var = train_df.var()\ntrain_std = train_df.std()\ntrain_min = train_df.min()\ntrain_max = train_df.max()\ntrain_minus_flg = pd.DataFrame(np.where(train_df.values < 0, -1, 1), columns=train_df.columns)\n\ntrain_sensors_mean = train_mean.drop(['sequence', 'subject', 'step'])\ntrain_sensors_std = train_std.drop(['sequence', 'subject', 'step'])\ntrain_sensors_var = train_var.drop(['sequence', 'subject', 'step'])\ntrain_sensors_min = train_min.drop(['sequence', 'subject', 'step'])\ntrain_sensors_max = train_max.drop(['sequence', 'subject', 'step'])\ntrain_sensors_minus_flg = train_minus_flg.drop(['sequence', 'subject', 'step'], axis=1)\n\ntrain_df.describe() # Before Cleaning","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:27:02.639104Z","iopub.execute_input":"2022-04-16T07:27:02.63935Z","iopub.status.idle":"2022-04-16T07:27:03.96818Z","shell.execute_reply.started":"2022-04-16T07:27:02.639317Z","shell.execute_reply":"2022-04-16T07:27:03.967488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_quantiles = train_df.quantile([0.05, 0.95])\ntrain_quantiles","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-16T07:27:03.969468Z","iopub.execute_input":"2022-04-16T07:27:03.970164Z","iopub.status.idle":"2022-04-16T07:27:04.48555Z","shell.execute_reply.started":"2022-04-16T07:27:03.970118Z","shell.execute_reply":"2022-04-16T07:27:04.48487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def outlier_check(dataframe, means, stds, factor):\n    outlier_counter = dataframe.copy()\n    outlier_counter['count'] = 1\n    for sensor in sensors:\n        sensor_values = outlier_counter[sensor].values\n        \n        mean = means[sensor]\n        std = stds[sensor]\n        threshold_1 = mean - std * factor\n        threshold_2 = mean + std * factor\n        \n        sensor_values = np.where((sensor_values < threshold_1) | (sensor_values > threshold_2), 1, 0)\n        outlier_counter[sensor] = sensor_values\n    \n    outlier_counter = outlier_counter.drop(['sequence', 'subject', 'step'], axis=1)\n    print(outlier_counter.sum(axis=0))\n    \noutlier_check(train_df, train_sensors_mean, train_sensors_std, 3)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-16T07:27:04.486777Z","iopub.execute_input":"2022-04-16T07:27:04.487195Z","iopub.status.idle":"2022-04-16T07:27:05.17746Z","shell.execute_reply.started":"2022-04-16T07:27:04.487156Z","shell.execute_reply":"2022-04-16T07:27:05.176576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_thresholds = []\n\nfor sensor in sensors:\n    sensor_values = train_df[sensor].values\n    \n    # Clipping on (mean Â± std * 3)\n    #mean = train_sensors_mean[sensor]\n    #std = train_sensors_std[sensor]\n    #threshold_1 = mean - std * 3\n    #threshold_2 = mean + std * 3\n    #train_thresholds.append((threshold_1, threshold_2))\n    #sensor_values = np.where(sensor_values < threshold_1, threshold_1, sensor_values)\n    #sensor_values = np.where(sensor_values > threshold_2, threshold_2, sensor_values)\n    \n    # Clipping on Quantile.\n    #threshold_1 = train_quantiles[sensor].values[0]\n    #threshold_2 = train_quantiles[sensor].values[1]\n    #sensor_values = np.where(sensor_values < threshold_1, threshold_1, sensor_values)\n    #sensor_values = np.where(sensor_values > threshold_2, threshold_2, sensor_values)\n    \n    # Min-Max Scaling\n    #sensor_min = train_sensors_min[sensor]\n    #sensor_max = train_sensors_max[sensor]\n    #sensor_values = (sensor_values - sensor_min) / (sensor_max - sensor_min)\n    \n    # Logarithmic transformation_1\n    #sensor_values = sensor_values - train_sensors_min[sensor]\n    #sensor_values = np.log(1 + sensor_values)\n    \n    # Logarithmic transformation_2   \n    sensor_values = np.log(1 + np.abs(sensor_values))\n    sensor_values *= train_sensors_minus_flg[sensor].values\n    train_df[sensor] = sensor_values\n    \ntrain_df.describe() # After Cleaning","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:27:05.178766Z","iopub.execute_input":"2022-04-16T07:27:05.179526Z","iopub.status.idle":"2022-04-16T07:27:06.202351Z","shell.execute_reply.started":"2022-04-16T07:27:05.179481Z","shell.execute_reply":"2022-04-16T07:27:06.201673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3.2 Test Data","metadata":{}},{"cell_type":"code","source":"test_df.describe() # Before Cleaning","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:27:06.203722Z","iopub.execute_input":"2022-04-16T07:27:06.203968Z","iopub.status.idle":"2022-04-16T07:27:06.593236Z","shell.execute_reply.started":"2022-04-16T07:27:06.203936Z","shell.execute_reply":"2022-04-16T07:27:06.592549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sensor in sensors:\n    sensor_values = test_df[sensor].values\n    \n    # Clipping on (mean Â± std * 3)\n    #mean = train_sensors_mean[sensor]\n    #std = train_sensors_std[sensor]\n    #threshold_1 = mean - std * 3\n    #threshold_2 = mean + std * 3\n    #sensor_values = np.where(sensor_values < threshold_1, threshold_1, sensor_values)\n    #sensor_values = np.where(sensor_values > threshold_2, threshold_2, sensor_values)\n    \n    # Clipping on Quantile.\n    #threshold_1 = train_quantiles[sensor].values[0]\n    #threshold_2 = train_quantiles[sensor].values[1]\n    #sensor_values = np.where(sensor_values < threshold_1, threshold_1, sensor_values)\n    #sensor_values = np.where(sensor_values > threshold_2, threshold_2, sensor_values)\n    \n    # Min-Max Scaling\n    #sensor_min = train_sensors_min[sensor]\n    #sensor_max = train_sensors_max[sensor]\n    #sensor_values = (sensor_values - sensor_min) / (sensor_max - sensor_min)\n    \n    # Logarithmic transformation_1\n    #sensor_values = sensor_values - train_sensors_min[sensor]\n    #sensor_values = np.log(1 + sensor_values)\n    \n    # Logarithmic transformation_2   \n    test_sensors_minus_flg = pd.DataFrame(np.where(test_df.values < 0, -1, 1), columns=test_df.columns)\n    test_sensors_minus_flg = test_sensors_minus_flg.drop(['sequence', 'subject', 'step'], axis=1)\n    sensor_values = np.log(1 + np.abs(sensor_values)) \n    sensor_values *= test_sensors_minus_flg[sensor]\n    \n    test_df[sensor] = sensor_values\n    \ntest_df.describe() # After Cleaning","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:27:06.594597Z","iopub.execute_input":"2022-04-16T07:27:06.594868Z","iopub.status.idle":"2022-04-16T07:27:08.738972Z","shell.execute_reply.started":"2022-04-16T07:27:06.594832Z","shell.execute_reply":"2022-04-16T07:27:08.738267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3.3 Statistics Update","metadata":{}},{"cell_type":"code","source":"# Statistics Update\ntrain_mean = train_df.mean()\ntrain_var = train_df.var()\ntrain_std = train_df.std()\ntrain_min = train_df.min()\ntrain_max = train_df.max()\n\ntrain_sensors_mean = train_mean.drop(['sequence', 'subject', 'step'])\ntrain_sensors_std = train_std.drop(['sequence', 'subject', 'step'])\ntrain_sensors_var = train_var.drop(['sequence', 'subject', 'step'])\ntrain_sensors_min = train_min.drop(['sequence', 'subject', 'step'])\ntrain_sensors_max = train_max.drop(['sequence', 'subject', 'step'])","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:27:08.740338Z","iopub.execute_input":"2022-04-16T07:27:08.740601Z","iopub.status.idle":"2022-04-16T07:27:08.99433Z","shell.execute_reply.started":"2022-04-16T07:27:08.740566Z","shell.execute_reply":"2022-04-16T07:27:08.993449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.4 Feature Engineering","metadata":{}},{"cell_type":"code","source":"!pip install -q --user tsfresh","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-16T07:27:08.995697Z","iopub.execute_input":"2022-04-16T07:27:08.995971Z","iopub.status.idle":"2022-04-16T07:27:17.769663Z","shell.execute_reply.started":"2022-04-16T07:27:08.995934Z","shell.execute_reply":"2022-04-16T07:27:17.768806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tsfresh import extract_features\n#extracted_features = extract_features(train_tmp, column_id=\"sequence\", column_sort=\"step\") # This creates too much features!\n\nfeatures_df = train_df.drop(['subject'], axis=1)\nfc_parameters = {\n    'abs_energy': None,\n    'count_above_mean': None,\n    'count_below_mean': None,\n    'mean_abs_change': None,\n    'mean_change': None,\n}\nextracted_features = extract_features(features_df, column_id=\"sequence\", column_sort=\"step\", default_fc_parameters=fc_parameters)\nprint(extracted_features.shape)\nprint(extracted_features.columns)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:27:17.773137Z","iopub.execute_input":"2022-04-16T07:27:17.773359Z","iopub.status.idle":"2022-04-16T07:28:35.191933Z","shell.execute_reply.started":"2022-04-16T07:27:17.77333Z","shell.execute_reply":"2022-04-16T07:28:35.191164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_features_df = test_df.drop(['subject'], axis=1)\ntest_extracted_features = extract_features(test_features_df,\n                                           column_id=\"sequence\",\n                                           column_sort=\"step\",\n                                           default_fc_parameters=fc_parameters)\nprint(test_extracted_features.shape)\nprint(test_extracted_features.columns)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-16T07:28:35.193275Z","iopub.execute_input":"2022-04-16T07:28:35.193557Z","iopub.status.idle":"2022-04-16T07:29:07.447702Z","shell.execute_reply.started":"2022-04-16T07:28:35.193518Z","shell.execute_reply":"2022-04-16T07:29:07.446964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.5 EDA","metadata":{}},{"cell_type":"code","source":"sequences = [0, 1]\nsensors = np.array(['sensor_00', 'sensor_01', 'sensor_02', 'sensor_03', 'sensor_04', 'sensor_05', 'sensor_06',\n           'sensor_07', 'sensor_08', 'sensor_09', 'sensor_10', 'sensor_11', 'sensor_12',])\ncolors = ['#7A5197', '#BB5098']\n\nfigure, axes = plt.subplots(13, 2, sharex=True, figsize=(20, 16))\nfor i, sequence in enumerate(sequences):\n    for j, sensor in enumerate(sensors):\n        ax = plt.subplot(13, len(sequences), j * len(sequences) + (i + 1))\n        plt.plot(range(60), train_df[train_df.sequence == sequence][sensor],\n                color=colors[i])\n        if j == 0: \n            if sequence==0:\n                plt.title(\"Sequence 0: state=0\");\n            else:\n                plt.title(\"Sequence 1: state=1\")\n        if sequence == sequences[0]: plt.ylabel(sensor)\nfigure.tight_layout(w_pad=0.1)\nplt.suptitle('Selected Time Series', y=1.02, fontweight='bold')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:29:07.449006Z","iopub.execute_input":"2022-04-16T07:29:07.449337Z","iopub.status.idle":"2022-04-16T07:29:10.223416Z","shell.execute_reply.started":"2022-04-16T07:29:07.449297Z","shell.execute_reply":"2022-04-16T07:29:10.222721Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggs = {}\naggs['sequence'] = ['nunique', 'size']\ngdf = train_df.groupby('subject').agg(aggs)\n#print(gdf)\n\nfig = plt.figure(figsize=(10, 5))\nax = fig.add_subplot(1, 1, 1)\nplt.hist(gdf['sequence']['nunique'], bins=50)\nax.set_xlabel('number of associated sequences')\n#ax.set_ylabel('number of subjects')\nplt.suptitle('Distribution of subjects associated with multiple sequences', y=1.02, fontweight='bold')\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-16T08:07:33.287572Z","iopub.execute_input":"2022-04-16T08:07:33.288275Z","iopub.status.idle":"2022-04-16T08:07:33.654379Z","shell.execute_reply.started":"2022-04-16T08:07:33.288237Z","shell.execute_reply":"2022-04-16T08:07:33.653697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure = plt.figure(figsize=(16, 8))\n\nfor sensor in range(13):\n    sensor_name = f\"sensor_{sensor:02d}\"\n    ax = figure.add_subplot(4, 4, sensor+1)\n    ax.hist(train_df[f\"{sensor_name}\"], bins=100)\n    ax.axes.yaxis.set_visible(False)\n    ax.set_title(f\"{sensor_name}\")\nplt.suptitle('Distribution of Sensor Signals', y=1.02, fontweight='bold')\nfigure.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-16T08:01:43.391863Z","iopub.execute_input":"2022-04-16T08:01:43.392139Z","iopub.status.idle":"2022-04-16T08:01:47.94571Z","shell.execute_reply.started":"2022-04-16T08:01:43.392111Z","shell.execute_reply":"2022-04-16T08:01:47.944989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.5.1 EDA with [TensorFlow Data Validation](https://www.tensorflow.org/tfx/guide/tfdv)\n\n\"TensorFlow Data Validation (TFDV) is a library for analyzing and validating machine learning data. It is designed to be highly scalable and to work well with TensorFlow and TFX. TFDV includes:\n\n- Scalable calculation of summary statistics of training and test data.\n- Integration with a viewer for data distributions and statistics, as well as faceted comparison of pairs of datasets (Facets).\n- Automated data-schema generation to describe expectations about data like required values, ranges, and vocabularies.\n- A schema viewer to help you inspect the schema.\n- Anomaly detection to identify anomalies, such as missing features, out-of- range values, or wrong feature types, to name a few.\n- An anomalies viewer so that you can see what features have anomalies and learn more in order to correct them. \"\n\n(from \"[The TFX User Guide](https://www.tensorflow.org/tfx/guide)\")","metadata":{}},{"cell_type":"code","source":"!pip install -q --user tensorflow_data_validation[visualization]","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-16T07:29:10.610214Z","iopub.execute_input":"2022-04-16T07:29:10.610792Z","iopub.status.idle":"2022-04-16T07:30:19.988512Z","shell.execute_reply.started":"2022-04-16T07:29:10.610751Z","shell.execute_reply":"2022-04-16T07:30:19.987595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_data_validation as tfdv \nprint(f'TFDV version: {tfdv.version.__version__}')","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:30:19.991146Z","iopub.execute_input":"2022-04-16T07:30:19.991917Z","iopub.status.idle":"2022-04-16T07:30:20.971998Z","shell.execute_reply.started":"2022-04-16T07:30:19.991867Z","shell.execute_reply":"2022-04-16T07:30:20.971194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.5.2 Statistics of Training data","metadata":{}},{"cell_type":"code","source":"train_stats = tfdv.generate_statistics_from_dataframe(train_df)\ntfdv.visualize_statistics(train_stats)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:30:20.973435Z","iopub.execute_input":"2022-04-16T07:30:20.974234Z","iopub.status.idle":"2022-04-16T07:30:23.546355Z","shell.execute_reply.started":"2022-04-16T07:30:20.974194Z","shell.execute_reply":"2022-04-16T07:30:23.545671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.5.3 Comparing Test Data with Training Data","metadata":{}},{"cell_type":"code","source":"test_stats = tfdv.generate_statistics_from_dataframe(test_df)\n\ntfdv.visualize_statistics(\n    lhs_statistics=test_stats, \n    rhs_statistics=train_stats, \n    lhs_name='TEST_DATASET', \n    rhs_name='TRAIN_DATASET')","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:30:23.547607Z","iopub.execute_input":"2022-04-16T07:30:23.547949Z","iopub.status.idle":"2022-04-16T07:30:24.73119Z","shell.execute_reply.started":"2022-04-16T07:30:23.547908Z","shell.execute_reply":"2022-04-16T07:30:24.730341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.5.4 Anomaly Detection","metadata":{}},{"cell_type":"code","source":"schema = tfdv.infer_schema(train_stats)\ntfdv.display_schema(schema)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-16T07:30:24.73584Z","iopub.execute_input":"2022-04-16T07:30:24.736041Z","iopub.status.idle":"2022-04-16T07:30:24.75403Z","shell.execute_reply.started":"2022-04-16T07:30:24.736016Z","shell.execute_reply":"2022-04-16T07:30:24.75326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anomalies = tfdv.validate_statistics(statistics=test_stats, schema=schema)\ntfdv.display_anomalies(anomalies)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:30:24.755485Z","iopub.execute_input":"2022-04-16T07:30:24.755952Z","iopub.status.idle":"2022-04-16T07:30:24.768816Z","shell.execute_reply.started":"2022-04-16T07:30:24.755909Z","shell.execute_reply":"2022-04-16T07:30:24.768034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.6 Train Validation Split","metadata":{}},{"cell_type":"code","source":"unique_subjects = train_df['subject'].unique()\ntrain_subjects = unique_subjects[:600]\nvalid_subjects = unique_subjects[600:]\nprint(len(train_subjects), len(valid_subjects))\n\ntrain = train_df[train_df['subject'].isin(train_subjects)].reset_index(drop=True)\nvalid = train_df[train_df['subject'].isin(valid_subjects)].reset_index(drop=True)\nprint(len(train), len(valid))","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:30:24.770264Z","iopub.execute_input":"2022-04-16T07:30:24.770765Z","iopub.status.idle":"2022-04-16T07:30:24.95492Z","shell.execute_reply.started":"2022-04-16T07:30:24.770725Z","shell.execute_reply":"2022-04-16T07:30:24.954081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_mean = train.mean()\ntrain_var = train.var()\ntrain_std = train.std()\n#print(train_mean, '\\n', train_var, '\\n', train_std)\n\ntrain_sensors_mean = train_mean.drop(['sequence', 'subject', 'step'])\ntrain_sensors_std = train_std.drop(['sequence', 'subject', 'step'])\ntrain_sensors_var = train_var.drop(['sequence', 'subject', 'step'])\nprint(train_sensors_mean.shape)\n\ntrain_labels = train_labels_df[train_labels_df['sequence'].isin(train['sequence'])].reset_index(drop=True)\nvalid_labels = train_labels_df[train_labels_df['sequence'].isin(valid['sequence'])].reset_index(drop=True)\nprint('train_labels: \\n', train_labels['state'].value_counts(), '\\n')\nprint('valid_labels: \\n', valid_labels['state'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:30:24.956249Z","iopub.execute_input":"2022-04-16T07:30:24.956763Z","iopub.status.idle":"2022-04-16T07:30:25.154981Z","shell.execute_reply.started":"2022-04-16T07:30:24.956718Z","shell.execute_reply":"2022-04-16T07:30:25.154171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.6.1 Extracted Features by tsfresh","metadata":{}},{"cell_type":"code","source":"tmp_df = train_df.query('step==0')\ntrain_tmp_df = tmp_df[tmp_df['subject'].isin(train_subjects)]\ntrain_seq_index = train_tmp_df['sequence'].values\n\nvalid_tmp_df = tmp_df[tmp_df['subject'].isin(valid_subjects)]\nvalid_seq_index = valid_tmp_df['sequence'].values\n\nprint(train_seq_index.shape, valid_seq_index.shape)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-16T07:30:25.156391Z","iopub.execute_input":"2022-04-16T07:30:25.157028Z","iopub.status.idle":"2022-04-16T07:30:25.186871Z","shell.execute_reply.started":"2022-04-16T07:30:25.156987Z","shell.execute_reply":"2022-04-16T07:30:25.186136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_extracted_features = extracted_features.iloc[train_seq_index]\nvalid_extracted_features = extracted_features.iloc[valid_seq_index]\nprint(train_extracted_features.shape, valid_extracted_features.shape)\n\nextracted_features_mean = train_extracted_features.mean()\nextracted_features_var = train_extracted_features.var()\nprint(extracted_features_mean.shape, extracted_features_var.shape)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-16T07:30:25.188279Z","iopub.execute_input":"2022-04-16T07:30:25.188542Z","iopub.status.idle":"2022-04-16T07:30:25.208654Z","shell.execute_reply.started":"2022-04-16T07:30:25.188507Z","shell.execute_reply":"2022-04-16T07:30:25.207983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Model Training, Prediction and Submission","metadata":{}},{"cell_type":"markdown","source":"## 2.1 TabTransformer Model [TensorFlow]","metadata":{}},{"cell_type":"code","source":"# Limit GPU Memory in TensorFlow\nphysical_devices = tf.config.list_physical_devices('GPU')\nif len(physical_devices) > 0:\n    for device in physical_devices:\n        tf.config.experimental.set_memory_growth(device, True)\n        print('{} memory growth: {}'.format(device, tf.config.experimental.get_memory_growth(device)))\nelse:\n    print(\"Not enough GPU hardware devices available\")","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-16T07:30:25.209961Z","iopub.execute_input":"2022-04-16T07:30:25.210394Z","iopub.status.idle":"2022-04-16T07:30:25.235454Z","shell.execute_reply.started":"2022-04-16T07:30:25.210359Z","shell.execute_reply":"2022-04-16T07:30:25.232508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1.1 Dataset for TabTransformer","metadata":{}},{"cell_type":"code","source":"sensors = np.array(['sensor_00', 'sensor_01', 'sensor_02', 'sensor_03', 'sensor_04', 'sensor_05', 'sensor_06',\n           'sensor_07', 'sensor_08', 'sensor_09', 'sensor_10', 'sensor_11', 'sensor_12',])\n\ndef dataframe_to_dataset(dataframe, extracted_features_df=None, sensors=sensors):\n    seq_data = {}\n    \n    for i in range(dataframe['step'].nunique()):\n        tmp_df = dataframe.query(f'step=={i}')\n        seq_data[f'x_{i}'] = tmp_df[sensors].values\n    \n    seq_data.update(**{column: np.expand_dims(extracted_features_df[column].values, -1) for column in extracted_features_df.columns})\n        \n    # Like the [CLS] token in BERT, I prepare an embedding for classification.\n    seq_data['cls'] = np.ones((len(tmp_df), 1))\n    seq_data['cls'] = seq_data['cls'].astype('int64')\n        \n    ds = tf.data.Dataset.from_tensor_slices(seq_data)     \n    return ds\n\n\ntrain_data_ds = dataframe_to_dataset(train, train_extracted_features)\nvalid_data_ds = dataframe_to_dataset(valid, valid_extracted_features)\n\ntrain_labels_ds = tf.data.Dataset.from_tensor_slices(train_labels['state'])\nvalid_labels_ds = tf.data.Dataset.from_tensor_slices(valid_labels['state'])\n\ntrain_ds = tf.data.Dataset.zip((train_data_ds, train_labels_ds))\nvalid_ds = tf.data.Dataset.zip((valid_data_ds, valid_labels_ds))\n\n# Display a sample in train_ds.\nprint(f'length: {len(train_ds)}')\nfor example in train_ds.take(1):\n    input_keys_list = list(example[0].keys())\n    print(input_keys_list)\n    print(example[1])","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-16T07:30:25.236812Z","iopub.execute_input":"2022-04-16T07:30:25.237578Z","iopub.status.idle":"2022-04-16T07:30:28.883339Z","shell.execute_reply.started":"2022-04-16T07:30:25.237531Z","shell.execute_reply":"2022-04-16T07:30:28.882432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size= 512\n\ntrain_ds = train_ds.shuffle(buffer_size=(len(train_ds)))\ntrain_ds = train_ds.batch(batch_size)\ntrain_ds = train_ds.prefetch(batch_size)\n\nvalid_ds = valid_ds.batch(batch_size)\nvalid_ds = valid_ds.prefetch(batch_size)\n\n# Display a sample in batched train_ds.\nexample = next(iter(train_ds))[0]\nfor key in example:\n    print(f'{key}, shape:{example[key].shape}, {example[key].dtype}')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-16T07:30:28.884691Z","iopub.execute_input":"2022-04-16T07:30:28.885194Z","iopub.status.idle":"2022-04-16T07:30:31.844066Z","shell.execute_reply.started":"2022-04-16T07:30:28.885154Z","shell.execute_reply":"2022-04-16T07:30:31.843303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1.2 Preprocessing Model","metadata":{}},{"cell_type":"code","source":"preprocess_inputs = {}\n\npreprocess_inputs['cls'] = tf.keras.Input(shape=(1), dtype='int64')\n\nsensor_names = [f'x_{i}' for i in range(60)]\nfor key in input_keys_list:\n    if key != 'cls':\n        if key in sensor_names:\n            preprocess_inputs[key] = tf.keras.Input(shape=(13), dtype='float64')\n        else:\n            preprocess_inputs[key] = tf.keras.Input(shape=(1), dtype='float64')\n\npreprocess_outputs = {}\ncls_output = tf.keras.layers.IntegerLookup(\n    vocabulary=tf.constant([1]), output_mode='int')(preprocess_inputs['cls'])\npreprocess_outputs['cls'] = cls_output\n\nfor key in next(iter(train_ds))[0]:\n    if key != 'cls':\n        if key in sensor_names:\n            preprocess_outputs[key] = tf.keras.layers.Normalization(\n                axis=1, mean=train_sensors_mean.values, variance=train_sensors_var.values,\n            )(preprocess_inputs[key]) \n        else:\n            preprocess_outputs[key] = tf.keras.layers.Normalization(\n                mean=extracted_features_mean[key],\n                variance=extracted_features_var[key],\n            )(preprocess_inputs[key]) \n    \npreprocessing_model = tf.keras.Model(preprocess_inputs, \n                                     preprocess_outputs)","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-04-16T07:30:31.845203Z","iopub.execute_input":"2022-04-16T07:30:31.845452Z","iopub.status.idle":"2022-04-16T07:30:35.752525Z","shell.execute_reply.started":"2022-04-16T07:30:31.845415Z","shell.execute_reply":"2022-04-16T07:30:35.7518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the preprocessing in tf.data.Dataset.map\ntrain_ds = train_ds.map(lambda x, y: (preprocessing_model(x), y), \n                        num_parallel_calls=tf.data.AUTOTUNE)\n\nvalid_ds = valid_ds.map(lambda x, y: (preprocessing_model(x), y), \n                        num_parallel_calls=tf.data.AUTOTUNE)\n\n# Display a preprocessed input sample\nexample = next(train_ds.take(1).as_numpy_iterator())\nfor key in example[0]:\n    print(f'{key}, shape:{example[0][key].shape}, {example[0][key].dtype}')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-16T07:30:35.753905Z","iopub.execute_input":"2022-04-16T07:30:35.754148Z","iopub.status.idle":"2022-04-16T07:30:40.350247Z","shell.execute_reply.started":"2022-04-16T07:30:35.754116Z","shell.execute_reply":"2022-04-16T07:30:40.334016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1.3 [Tab Transformer Model](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/structured_data/ipynb/tabtransformer.ipynb) (Customized)","metadata":{}},{"cell_type":"markdown","source":"The TabTransformer architecture works as follows:\n\n- All the categorical features are encoded as embeddings, using the same embedding_dims. This means that each value in each categorical feature will have its own embedding vector.\n\n- A column embedding, one embedding vector for each categorical feature, is added (point-wise) to the categorical feature embedding.\n\n- The embedded categorical features are fed into a stack of Transformer blocks. Each Transformer block consists of a multi-head self-attention layer followed by a feed-forward layer.\n\n- The outputs of the final Transformer layer, which are the contextual embeddings of the categorical features, are concatenated with the input numerical features, and fed into a final MLP block.\n\n<img src=\"https://raw.githubusercontent.com/keras-team/keras-io/master/examples/structured_data/img/tabtransformer/tabtransformer.png\" width=\"500\"/>\n\n---\n\n#### **In this notebook, I customized TabTransformer on following points.**\n\n- I considered the values of thirteen sensors at one time step as an embedding feature.\n- Thus, a series of sensor value embeddings at 60 time steps are comprise categorical features of a sequence.\n- I added a 'cls' embedding to the categorical features for the classification task after the transformer blocks (learned from [CLS] token in BERT).\n- I considered the features extracted by tsfresh from data in chronological order as the numerical (continuous) features.\n- I concatenated 'cls' embedding, the other 60 contextual features and normalized numerical features, and fed into a final MLP block.\n\n---","metadata":{}},{"cell_type":"code","source":"embedding_dim = 64\n\nmodel_inputs = {}\n\nmodel_inputs['cls'] = tf.keras.Input(shape=(), dtype='int64')  \n\nsensor_names = [f'x_{i}' for i in range(60)]\nfor key in input_keys_list:\n    if key != 'cls':\n        if key in sensor_names:\n            model_inputs[key] = tf.keras.Input(shape=(13), dtype='float64')\n        else:\n            model_inputs[key] = tf.keras.Input(shape=(1), dtype='float64')\n            \n\ncls_embedding = tf.keras.layers.Embedding(2, embedding_dim)\ncls_features = cls_embedding(model_inputs['cls'])\ncls_features = tf.expand_dims(cls_features, axis=1)\n\nfirst_dense = tf.keras.Sequential([\n    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n    tf.keras.layers.BatchNormalization()])\n\nsensor_signals = []\nfor key in model_inputs:\n    if key in sensor_names:\n        sensor_signals.append(model_inputs[key])\nsensor_signals = tf.stack(sensor_signals, axis=1)\nsensor_signals = first_dense(sensor_signals)\n\ninput_features = tf.concat([cls_features, sensor_signals], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:30:40.351414Z","iopub.execute_input":"2022-04-16T07:30:40.351681Z","iopub.status.idle":"2022-04-16T07:30:40.741485Z","shell.execute_reply.started":"2022-04-16T07:30:40.351647Z","shell.execute_reply":"2022-04-16T07:30:40.740751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add column embedding to categorical feature embeddings.\nnum_columns = input_features.shape[1]\ncolumn_embedding = tf.keras.layers.Embedding(\n    input_dim=num_columns, output_dim=embedding_dim)\ncolumn_indices = tf.range(start=0, limit=num_columns, delta=1)\nencoded_features = input_features + column_embedding(column_indices)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:30:40.742767Z","iopub.execute_input":"2022-04-16T07:30:40.74302Z","iopub.status.idle":"2022-04-16T07:30:40.758779Z","shell.execute_reply.started":"2022-04-16T07:30:40.742986Z","shell.execute_reply":"2022-04-16T07:30:40.758014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create TabTransformer Model.\nnum_transformer_blocks = 8\nnum_heads = 4\ndropout_rate = 0.2\nmlp_hidden_units_factors= [2, 1] \n\ndef create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n    mlp_layers = []\n    for units in hidden_units:\n        mlp_layers.append(normalization_layer)\n        mlp_layers.append(tf.keras.layers.Dense(\n            units, activation=activation))\n        mlp_layers.append(tf.keras.layers.Dropout(dropout_rate))\n    return tf.keras.Sequential(mlp_layers, name=name)\n\n# Create multiple layers of the Transformer block.\nfor block_idx in range(num_transformer_blocks):\n    # Create a multi-head attention layer.\n    attention_output = tf.keras.layers.MultiHeadAttention(\n        num_heads=num_heads,\n        key_dim=embedding_dim, \n        dropout=dropout_rate, \n        name=f'multi-head_attention_{block_idx}'\n    )(encoded_features, encoded_features)\n    # Skip connection 1.\n    x = tf.keras.layers.Add(\n        name=f'skip_connection1_{block_idx}'\n    )([attention_output, encoded_features])\n    # Layer normalization 1.\n    x = tf.keras.layers.LayerNormalization(\n        name=f'layer_norm1_{block_idx}', epsilon=1e-6\n    )(x)\n    # Feedforward.\n    feedforward_output =  tf.keras.Sequential([\n                        tf.keras.layers.Dense(embedding_dim, activation=keras.activations.gelu),\n                        tf.keras.layers.Dropout(dropout_rate)\n                        ], name=f'feedforward_{block_idx}'\n    )(x)\n    # Skip connection 2.\n    x = tf.keras.layers.Add(\n        name=f'skip_connection2_{block_idx}'\n    )([feedforward_output, x])\n    # Layer normalization 2.\n    encoded_features = tf.keras.layers.LayerNormalization(\n        name=f'layer_norm2_{block_idx}', epsilon=1e-6\n    )(x)\n\n# Numerical features\nnumerical_feature_list = []\nfor numerical_feature_name in train_extracted_features.columns:\n    numerical_feature_list.append(model_inputs[numerical_feature_name])\nnumerical_features = tf.keras.layers.concatenate(numerical_feature_list)\n# Apply layer normalization to the numerical features.\nnumerical_features = tf.keras.layers.LayerNormalization(epsilon=1e-6)(numerical_features)\n\n\nfeatures_1 = encoded_features[:, 0, :]\n\nfeatures_2 = encoded_features[:, 1:, :]\nfeatures_2 = tf.keras.layers.Dense(8, activation='relu')(features_2)\nfeatures_2 = tf.keras.layers.BatchNormalization()(features_2) \nfeatures_2 = tf.keras.layers.Flatten()(features_2)\nfeatures_2 = tf.keras.layers.Dense(64, activation='relu')(features_2)\nfeatures_2 = tf.keras.layers.BatchNormalization()(features_2) \n\n# Prepare the input for the final MLP block.\nfeatures = tf.keras.layers.concatenate([features_1,\n                                        features_2,\n                                        numerical_features], axis=-1)\n\n# Compute MLP hidden_units.\nmlp_hidden_units = [\n    factor * features.shape[-1] for factor in mlp_hidden_units_factors\n]\n# Create final MLP.\nfeatures = create_mlp(\n    hidden_units=mlp_hidden_units, \n    dropout_rate=dropout_rate,\n    activation=tf.keras.activations.selu,\n    normalization_layer=tf.keras.layers.BatchNormalization(),\n    name='MLP',\n)(features)\n\nmodel_outputs = tf.keras.layers.Dense(\n    units=1, activation='sigmoid', name='sigmoid'\n)(features)\n\ntraining_tt_model = keras.Model(inputs=model_inputs, outputs=model_outputs)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:30:40.760307Z","iopub.execute_input":"2022-04-16T07:30:40.760593Z","iopub.status.idle":"2022-04-16T07:30:41.686194Z","shell.execute_reply.started":"2022-04-16T07:30:40.760552Z","shell.execute_reply":"2022-04-16T07:30:41.685446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LEARNING_RATE = 1e-3\nWEIGHT_DECAY = 0.0001\n\noptimizer = tfa.optimizers.AdamW(\n        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\ntraining_tt_model.compile(optimizer=optimizer,\n                       loss=tf.keras.losses.BinaryCrossentropy(),\n                       metrics=[\"accuracy\"])\n\ntraining_tt_model.summary()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-16T07:30:41.687461Z","iopub.execute_input":"2022-04-16T07:30:41.687715Z","iopub.status.idle":"2022-04-16T07:30:41.811991Z","shell.execute_reply.started":"2022-04-16T07:30:41.687681Z","shell.execute_reply":"2022-04-16T07:30:41.811212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tf.keras.utils.plot_model(training_model, show_shapes=True, rankdir=\"LR\")\n#tf.keras.utils.plot_model(training_model, show_shapes=True)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-16T07:30:41.813247Z","iopub.execute_input":"2022-04-16T07:30:41.813523Z","iopub.status.idle":"2022-04-16T07:30:41.817487Z","shell.execute_reply.started":"2022-04-16T07:30:41.813484Z","shell.execute_reply":"2022-04-16T07:30:41.816549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1.4 [Learning Rate Finder](https://sachinruk.github.io/blog/tensorflow/learning%20rate/2021/02/15/Tensorflow-Learning-Rate-Finder.html)","metadata":{}},{"cell_type":"code","source":"class LRFind(tf.keras.callbacks.Callback):\n    def __init__(self, min_lr, max_lr, n_rounds):\n        self.min_lr = min_lr \n        self.max_lr = max_lr \n        self.step_up = (max_lr / min_lr) ** (1 / n_rounds)\n        self.lrs = []\n        self.losses = []\n\n    def on_train_begin(self, logs=None):\n        self.weights= self.model.get_weights()\n        self.model.optimizer.lr = self.min_lr \n\n    def on_train_batch_end(self, batch, logs=None):\n        self.lrs.append(self.model.optimizer.lr.numpy())\n        self.losses.append(logs['loss'])\n        self.model.optimizer.lr = self.model.optimizer.lr * self.step_up \n        if self.model.optimizer.lr > self.max_lr:\n            self.model.stop_training = True \n\n    def on_train_end(self, logs=None):\n        self.model.set_weights(self.weights)\n\nlr_find_epochs = 1\nlr_finder_steps = 100\nlr_find = LRFind(1e-7, 5e-2, lr_finder_steps)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:30:41.818987Z","iopub.execute_input":"2022-04-16T07:30:41.819372Z","iopub.status.idle":"2022-04-16T07:30:41.831512Z","shell.execute_reply.started":"2022-04-16T07:30:41.819333Z","shell.execute_reply":"2022-04-16T07:30:41.83053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_find_batch_size = 256\nlr_find_sequence_n = lr_find_batch_size * lr_finder_steps\nlr_find_sample_n = 60 * lr_find_sequence_n\n\nlr_find_data_ds = dataframe_to_dataset(train[:lr_find_sample_n], train_extracted_features[:lr_find_sequence_n])\nlr_find_labels_ds = tf.data.Dataset.from_tensor_slices(train_labels['state'][:lr_find_sample_n])\nlr_find_ds = tf.data.Dataset.zip((lr_find_data_ds, lr_find_labels_ds))\nlr_find_ds = lr_find_ds.batch(batch_size=lr_find_batch_size)\nlr_find_ds = lr_find_ds.prefetch(lr_find_batch_size)\n\nlr_find_ds = lr_find_ds.map(lambda x, y: (preprocessing_model(x), y), \n                            num_parallel_calls=tf.data.AUTOTUNE)\n\ntraining_tt_model.fit(lr_find_ds,\n                   steps_per_epoch=lr_finder_steps,\n                   epochs=lr_find_epochs,\n                   callbacks=[lr_find])\n\nplt.plot(lr_find.lrs, lr_find.losses)\nplt.xscale('log')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:30:41.832904Z","iopub.execute_input":"2022-04-16T07:30:41.83329Z","iopub.status.idle":"2022-04-16T07:31:04.670001Z","shell.execute_reply.started":"2022-04-16T07:30:41.833248Z","shell.execute_reply":"2022-04-16T07:31:04.669247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1.5 Model Training","metadata":{}},{"cell_type":"code","source":"# Re-construct the model\nmodel_config = training_tt_model.get_config()\ntraining_tt_model = tf.keras.Model.from_config(model_config)\n\nepochs = 15\nsteps_per_epoch = len(train)//batch_size\n\nlearning_schedule = tf.keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate=5e-4,\n    decay_steps=epochs*steps_per_epoch,\n    alpha=0.0)\nweight_decay = 0.0001\n\noptimizer = tfa.optimizers.AdamW(\n        learning_rate=learning_schedule,\n        weight_decay=weight_decay)\n\ntraining_tt_model.compile(optimizer=optimizer,\n                       loss=tf.keras.losses.BinaryCrossentropy(),\n                       metrics=[\"accuracy\"])\n\ncheckpoint_filepath = '/tmp/tt/checkpoint'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath, \n    save_weights_only=True,\n    monitor='val_accuracy', \n    mode='max', \n    save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:31:04.671514Z","iopub.execute_input":"2022-04-16T07:31:04.67195Z","iopub.status.idle":"2022-04-16T07:31:05.959116Z","shell.execute_reply.started":"2022-04-16T07:31:04.67191Z","shell.execute_reply":"2022-04-16T07:31:05.958311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_tt_model.fit(train_ds, epochs=epochs, shuffle=True,\n                   validation_data=valid_ds, \n                   callbacks=[model_checkpoint_callback])\n\ntraining_tt_model.load_weights(checkpoint_filepath)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:31:05.960433Z","iopub.execute_input":"2022-04-16T07:31:05.960716Z","iopub.status.idle":"2022-04-16T07:35:10.825749Z","shell.execute_reply.started":"2022-04-16T07:31:05.960679Z","shell.execute_reply":"2022-04-16T07:35:10.825021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1.6 Prediction and Submission","metadata":{}},{"cell_type":"code","source":"# At inference time, it can be useful to combine these separate stages into a single model that handles raw feature inputs.\ntt_inputs = preprocessing_model.input \ntt_outputs = training_tt_model(preprocessing_model(tt_inputs))\ninference_tt_model = tf.keras.Model(tt_inputs, tt_outputs)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:35:10.827259Z","iopub.execute_input":"2022-04-16T07:35:10.827699Z","iopub.status.idle":"2022-04-16T07:35:11.889871Z","shell.execute_reply.started":"2022-04-16T07:35:10.827658Z","shell.execute_reply":"2022-04-16T07:35:11.889117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_tt_ds = dataframe_to_dataset(test_df, test_extracted_features)\ntest_tt_ds = test_tt_ds.batch(batch_size=batch_size,\n                        drop_remainder=False)\ntest_tt_ds = test_tt_ds.prefetch(buffer_size=batch_size)\n\ntt_pred = inference_tt_model.predict(test_tt_ds)\ntt_pred = np.squeeze(tt_pred)\n#tt_pred = np.where(tt_pred <  0.5, 0, 1)\nsubmission_df['state'] = tt_pred\nsubmission_df.to_csv('tt_submission.csv', index=False)\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:35:11.891272Z","iopub.execute_input":"2022-04-16T07:35:11.891541Z","iopub.status.idle":"2022-04-16T07:35:16.971171Z","shell.execute_reply.started":"2022-04-16T07:35:11.891505Z","shell.execute_reply":"2022-04-16T07:35:16.970464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Freeing up the Memory\ntf.keras.backend.clear_session()\ngc.collect()\n\nfrom numba import cuda\ncuda.select_device(0)\ncuda.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:35:16.972604Z","iopub.execute_input":"2022-04-16T07:35:16.973103Z","iopub.status.idle":"2022-04-16T07:35:17.774055Z","shell.execute_reply.started":"2022-04-16T07:35:16.973064Z","shell.execute_reply":"2022-04-16T07:35:17.773194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 RNN Model [Pytorch Lightning]","metadata":{}},{"cell_type":"code","source":"import torch\nimport pytorch_lightning as pl\nimport torchmetrics\n\n!pip install torchinfo -q --user\nfrom torchinfo import summary\n\nprint(pl.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:35:17.775658Z","iopub.execute_input":"2022-04-16T07:35:17.775912Z","iopub.status.idle":"2022-04-16T07:35:29.271785Z","shell.execute_reply.started":"2022-04-16T07:35:17.775877Z","shell.execute_reply":"2022-04-16T07:35:29.270848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.1 Dataset for RNN","metadata":{}},{"cell_type":"code","source":"class TPSApr22RNNDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, dataframe, labels=None, \n                 mean_df=train_sensors_mean, std_df=train_sensors_std, \n                 eps=1e-7):\n        self.df = dataframe.drop(['sequence', 'subject', 'step'], axis=1)\n        self.data = self.df.values\n        \n        if labels is not None:\n            self.labels = labels['state'].values\n        else:\n            self.labels = None \n            \n        self.means = mean_df.values\n        self.stds = std_df.values\n        self.eps = np.array(eps)\n        \n    def __len__(self):\n        return len(self.data) // 60\n    \n    def __getitem__(self, idx):\n        tmp_data = self.data[idx*60: (idx+1)*60]\n        tmp_data = (tmp_data - self.means) / (self.stds + self.eps)\n        seq_data = {'x': torch.tensor(tmp_data, dtype=torch.float32)}\n        \n        if self.labels is not None:\n            label = self.labels[idx]\n            label = torch.tensor(label, dtype=torch.float32)\n            seq_data['labels'] = torch.unsqueeze(label, -1)\n            \n        return seq_data","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:35:29.27404Z","iopub.execute_input":"2022-04-16T07:35:29.274319Z","iopub.status.idle":"2022-04-16T07:35:29.289581Z","shell.execute_reply.started":"2022-04-16T07:35:29.274279Z","shell.execute_reply":"2022-04-16T07:35:29.288852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 512\n\ntrain_rnn_ds = TPSApr22RNNDataset(train, train_labels)\nvalid_rnn_ds = TPSApr22RNNDataset(valid, valid_labels)\ntest_rnn_ds = TPSApr22RNNDataset(test_df)\n\ntrain_rnn_dl = torch.utils.data.DataLoader(train_rnn_ds, \n                                           batch_size=batch_size, \n                                           shuffle=True)\nvalid_rnn_dl = torch.utils.data.DataLoader(valid_rnn_ds, \n                                           batch_size=batch_size, \n                                           shuffle=False)\ntest_rnn_dl = torch.utils.data.DataLoader(test_rnn_ds, \n                                          batch_size=batch_size, \n                                          shuffle=False, \n                                          drop_last=False)\n\nprint('------ train_rnn_dl ------')\ntmp = train_rnn_dl.__iter__()\nbatch_sample = tmp.next()\nprint(f\"x : {batch_sample['x'].shape}\")\nprint(f\"labels: {batch_sample['labels'].shape}\")\nprint(f\"n_samples: {len(train_rnn_ds)}\")\nprint(f\"n_batches: {len(tmp)}\")\nprint()\n\nprint('------ test_rnn_dl ------')\ntmp = test_rnn_dl.__iter__()\nbatch_sample = tmp.next()\nprint(f\"x : {batch_sample['x'].shape}\")\nprint(f\"n_samples: {len(test_rnn_ds)}\")\nprint(f\"n_batches: {len(tmp)}\")","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-16T07:35:29.290716Z","iopub.execute_input":"2022-04-16T07:35:29.291053Z","iopub.status.idle":"2022-04-16T07:35:29.45792Z","shell.execute_reply.started":"2022-04-16T07:35:29.291007Z","shell.execute_reply":"2022-04-16T07:35:29.457059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TPSApr22RNNDataModule(pl.LightningDataModule):\n    \n    def __init__(self, train, train_labels, \n                 valid, valid_labels, test, \n                 batch_size=512):\n        super().__init__()\n        self.train = train\n        self.train_labels = train_labels\n        self.valid = valid\n        self.valid_labels = valid_labels\n        self.test = test\n        self.batch_size = batch_size\n        \n    def prepare_data(self):\n        pass\n    \n    def setup(self, stage=None):\n        # It receives stage arguments from Trainer\n        if stage == 'fit' or stage is None:\n            self.train_set = TPSApr22RNNDataset(self.train, \n                                                self.train_labels)\n            self.valid_set = TPSApr22RNNDataset(self.valid, \n                                                self.valid_labels)\n            self.n_train_samples = len(self.train_set)\n            self.steps_per_epoch = self.n_train_samples // self.batch_size\n            \n        if stage == 'predict' or stage is None:\n            self.test_set = TPSApr22RNNDataset(self.test)\n            \n    def train_dataloader(self):\n        return torch.utils.data.DataLoader(self.train_set, \n                                           batch_size=self.batch_size, \n                                           shuffle=True)\n    \n    def val_dataloader(self):\n        return torch.utils.data.DataLoader(self.valid_set, \n                                           batch_size=self.batch_size, \n                                           shuffle=False)\n    \n    def predict_dataloader(self):\n        return torch.utils.data.DataLoader(self.test_set, \n                                           batch_size=self.batch_size, \n                                           shuffle=False, \n                                           drop_last=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:35:29.459372Z","iopub.execute_input":"2022-04-16T07:35:29.459657Z","iopub.status.idle":"2022-04-16T07:35:29.470437Z","shell.execute_reply.started":"2022-04-16T07:35:29.459606Z","shell.execute_reply":"2022-04-16T07:35:29.469371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.2 [RNN Model (LSTM)](https://www.kaggle.com/code/kartushovdanil/top-1-tps-apr-22-eda-lstm)","metadata":{}},{"cell_type":"code","source":"class TPSApr22RNN_pl(pl.LightningModule):\n    \n    def __init__(self, lr, steps_per_epoch):\n        super().__init__()\n        self.save_hyperparameters() # I need this.\n        self.lr = lr\n        self.steps_per_epoch = steps_per_epoch\n        \n        # Layers for the model\n        self.lstm1 = torch.nn.LSTM(13, 512, bidirectional=True, \n                                   batch_first=True)\n        self.lstm2 = torch.nn.LSTM(1024, 256, bidirectional=True, \n                                   batch_first=True)\n        self.gru = torch.nn.GRU(1024, 256, bidirectional=True, \n                                 batch_first=True)\n        self.lstm3 = torch.nn.LSTM(1024, 128, bidirectional=True, \n                                   batch_first=True)\n        self.dense1 = torch.nn.Linear(256, 128)\n        self.dense2 = torch.nn.Linear(128, 1)\n        \n        # Metrics\n        self.train_acc = torchmetrics.Accuracy()\n        self.valid_acc = torchmetrics.Accuracy()\n        \n    def forward(self, x):\n        x, _ = self.lstm1(x)\n        y, _ = self.lstm2(x)\n        z, _ = self.gru(x)\n        c = torch.cat((y, z), axis=2)\n        x1, _ = self.lstm3(c)\n        x2, _ = torch.max(x1, dim=1)\n        x3 = self.dense1(x2)\n        x4 = torch.nn.SELU()(x3)\n        output = self.dense2(x4)\n        return output\n    \n    def training_step(self, batch, batch_idx):\n        x = batch['x']\n        labels = batch['labels']\n        \n        logits = self(x)\n        loss = torch.nn.BCEWithLogitsLoss()(logits, labels)\n        preds = (logits > 0).int()\n        \n        #self.log('train_loss', loss)\n        results = {'loss': loss, 'logits': logits, \n                   'preds': preds, 'labels': labels}\n        return results\n    \n    def training_epoch_end(self, train_step_outputs):\n        # This function is called after 'validation_epoch_end'\n        # It receives the list of returns of all training_steps.\n        preds = torch.cat([items['preds'] for items in train_step_outputs], dim=0)\n        labels = torch.cat([items['labels'] for items in train_step_outputs], dim=0)\n        losses = torch.tensor([items['loss'] for items in train_step_outputs])\n        \n        #num_correct = (preds == labels).sum()\n        #acc = (num_correct / preds.size(0)).item()\n        acc = self.train_acc(preds, labels.int())\n        \n        epoch_loss = losses.sum() / len(train_step_outputs)\n        \n        self.log('train_loss', epoch_loss)\n        self.log('train_acc', acc)\n        \n        print(f\"train loss: {epoch_loss:.4f}, train acc: {acc:.4f}\")\n        \n    def validation_step(self, batch, batch_idx):\n        x = batch['x']\n        labels = batch['labels']\n        \n        logits = self(x)\n        loss = torch.nn.BCEWithLogitsLoss()(logits, labels)\n        preds = (logits > 0).int()\n        \n        #self.log('val_loss', loss)\n        results = {'loss': loss, 'logits': logits, \n                   'preds': preds, 'labels': labels}\n        return results\n    \n    def validation_epoch_end(self, val_step_outputs):\n        preds = torch.cat([items['preds'] for items in val_step_outputs], dim=0)\n        labels = torch.cat([items['labels'] for items in val_step_outputs], dim=0)\n        losses = torch.tensor([items['loss'] for items in val_step_outputs])\n        \n        #num_correct = (preds == labels).sum()\n        #acc = (num_correct / preds.size(0)).item()\n        acc = self.valid_acc(preds, labels.int())\n        \n        epoch_loss = losses.sum() / len(val_step_outputs)\n        \n        self.log('val_loss', epoch_loss)\n        self.log('val_acc', acc)\n        \n        print(f\"------ Epoch{self.current_epoch+1} ------\")\n        print(f\"valid loss: {epoch_loss:.4f}, valid acc: {acc:.4f}\")\n        \n    def predict_step(self, batch, batch_idx):\n        x = batch['x']\n        \n        logits = self(x)\n        probs = torch.nn.functional.sigmoid(logits)\n        preds = (logits > 0).int()\n        \n        results = {'logits': logits, 'probs': probs, 'preds': preds}\n        return results\n    \n    def configure_optimizers(self):\n        #return torch.optim.Adam(self.parameters(), lr=self.lr)\n        \n        optimizer = torch.optim.AdamW(params=self.parameters(), \n                                      lr=self.lr)\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n                                                        max_lr=self.lr, \n                                                        pct_start=0.1, \n                                                        steps_per_epoch=self.steps_per_epoch, \n                                                        epochs=self.trainer.max_epochs)\n        return [optimizer, ], [scheduler, ]\n        ","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:35:29.472275Z","iopub.execute_input":"2022-04-16T07:35:29.472562Z","iopub.status.idle":"2022-04-16T07:35:29.498529Z","shell.execute_reply.started":"2022-04-16T07:35:29.472524Z","shell.execute_reply":"2022-04-16T07:35:29.497676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 512\ndm = TPSApr22RNNDataModule(train, train_labels, valid, valid_labels, \n                           test_df, batch_size=batch_size)\ndm.prepare_data()\ndm.setup('fit')\n\nmodel = TPSApr22RNN_pl(lr=1e-3, steps_per_epoch=dm.steps_per_epoch)\n\ncheckpoint = pl.callbacks.ModelCheckpoint(\n    monitor='val_loss', \n    mode='min', \n    save_top_k=1, \n    save_weights_only=True, \n    dirpath='/tmp/rnn/checkpoint',)\n\ntrainer = pl.Trainer(\n    gpus=1, \n    max_epochs=15,  \n    callbacks=[checkpoint])\n\nsummary(\n    model, \n    input_size=(batch_size, 60, 13), \n    col_names=['output_size', 'num_params'],)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:35:29.500337Z","iopub.execute_input":"2022-04-16T07:35:29.500941Z","iopub.status.idle":"2022-04-16T07:35:33.846363Z","shell.execute_reply.started":"2022-04-16T07:35:29.500898Z","shell.execute_reply":"2022-04-16T07:35:33.845486Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.3 Model Training","metadata":{}},{"cell_type":"code","source":"trainer.fit(model, dm)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:39:27.397735Z","iopub.execute_input":"2022-04-16T07:39:27.398525Z","iopub.status.idle":"2022-04-16T07:39:39.668089Z","shell.execute_reply.started":"2022-04-16T07:39:27.398476Z","shell.execute_reply":"2022-04-16T07:39:39.667306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.4 Prediction and Submission","metadata":{}},{"cell_type":"code","source":"model = TPSApr22RNN_pl.load_from_checkpoint(checkpoint.best_model_path)\n\nrnn_preds = trainer.predict(model, dm)\nprint(len(rnn_preds))\n\nrnn_probs = torch.cat([items['probs'] for items in rnn_preds], dim=0).numpy()\nrnn_preds = torch.cat([items['preds'] for items in rnn_preds], dim=0).numpy()\n\nrnn_probs = np.squeeze(rnn_probs)\nsubmission_df['state'] = rnn_probs\nsubmission_df.to_csv('rnn_submission_1.csv', index=False)\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:38:32.243921Z","iopub.execute_input":"2022-04-16T07:38:32.244194Z","iopub.status.idle":"2022-04-16T07:38:34.777105Z","shell.execute_reply.started":"2022-04-16T07:38:32.244159Z","shell.execute_reply":"2022-04-16T07:38:34.77603Z"},"trusted":true},"execution_count":null,"outputs":[]}]}