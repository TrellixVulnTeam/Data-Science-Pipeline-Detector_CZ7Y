{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground Series - Apr 2022","metadata":{}},{"cell_type":"markdown","source":"This is a notebook for Kaggle competition of tabular playground in April 2022. We are given 12 sensor data and based on those values, we need to perform a binary classification. Let's start!","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:20.020963Z","iopub.execute_input":"2022-04-27T10:27:20.021254Z","iopub.status.idle":"2022-04-27T10:27:20.029166Z","shell.execute_reply.started":"2022-04-27T10:27:20.021222Z","shell.execute_reply":"2022-04-27T10:27:20.028353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import libraries: ","metadata":{}},{"cell_type":"code","source":"import scipy as sp\nimport pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:20.037542Z","iopub.execute_input":"2022-04-27T10:27:20.037874Z","iopub.status.idle":"2022-04-27T10:27:20.04231Z","shell.execute_reply.started":"2022-04-27T10:27:20.037796Z","shell.execute_reply":"2022-04-27T10:27:20.041588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA (Exploratory Data Analysis)","metadata":{}},{"cell_type":"markdown","source":"### Check variables and observations in test and train data","metadata":{}},{"cell_type":"markdown","source":"#### Train data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/tabular-playground-series-apr-2022/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:20.052611Z","iopub.execute_input":"2022-04-27T10:27:20.053211Z","iopub.status.idle":"2022-04-27T10:27:25.191982Z","shell.execute_reply.started":"2022-04-27T10:27:20.053169Z","shell.execute_reply":"2022-04-27T10:27:25.191215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:25.193337Z","iopub.execute_input":"2022-04-27T10:27:25.193719Z","iopub.status.idle":"2022-04-27T10:27:25.213912Z","shell.execute_reply.started":"2022-04-27T10:27:25.19369Z","shell.execute_reply":"2022-04-27T10:27:25.213219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:25.215072Z","iopub.execute_input":"2022-04-27T10:27:25.215492Z","iopub.status.idle":"2022-04-27T10:27:25.228681Z","shell.execute_reply.started":"2022-04-27T10:27:25.215458Z","shell.execute_reply":"2022-04-27T10:27:25.228037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Test data","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:25.23049Z","iopub.execute_input":"2022-04-27T10:27:25.230913Z","iopub.status.idle":"2022-04-27T10:27:27.628142Z","shell.execute_reply.started":"2022-04-27T10:27:25.230865Z","shell.execute_reply":"2022-04-27T10:27:27.627113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:27.629447Z","iopub.execute_input":"2022-04-27T10:27:27.62969Z","iopub.status.idle":"2022-04-27T10:27:27.651648Z","shell.execute_reply.started":"2022-04-27T10:27:27.629661Z","shell.execute_reply":"2022-04-27T10:27:27.650448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:27.653094Z","iopub.execute_input":"2022-04-27T10:27:27.653345Z","iopub.status.idle":"2022-04-27T10:27:27.663804Z","shell.execute_reply.started":"2022-04-27T10:27:27.653314Z","shell.execute_reply":"2022-04-27T10:27:27.663127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train_labels","metadata":{}},{"cell_type":"code","source":"df_train_labels = pd.read_csv('../input/tabular-playground-series-apr-2022/train_labels.csv')\ndf_train_labels.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:27.665231Z","iopub.execute_input":"2022-04-27T10:27:27.665532Z","iopub.status.idle":"2022-04-27T10:27:27.691504Z","shell.execute_reply.started":"2022-04-27T10:27:27.665499Z","shell.execute_reply":"2022-04-27T10:27:27.690666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In *train_labels* the variable *state* is associated with each sequence in train data","metadata":{}},{"cell_type":"markdown","source":"### Target distribution","metadata":{}},{"cell_type":"markdown","source":"It's important to have target normally distributed in regression model or to have all targets equally represented in classification problem. The reason for that is that the model will be more precise and accurate.\n\nSo when we have high skewness in data, we need to use log normal transformation to have skewness placed approximately to zero.","metadata":{}},{"cell_type":"code","source":"positive_state, negative_state = df_train_labels.state.value_counts()\nprint('There are {} positive and {} negative states'.format(positive_state, negative_state))","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:27.692686Z","iopub.execute_input":"2022-04-27T10:27:27.692986Z","iopub.status.idle":"2022-04-27T10:27:27.69936Z","shell.execute_reply.started":"2022-04-27T10:27:27.692953Z","shell.execute_reply":"2022-04-27T10:27:27.698689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.pie([positive_state, negative_state], labels=['True', 'False'], autopct='%1.1f%%', shadow=True, startangle=45, textprops=dict(color=\"white\", fontsize=15, weight=\"bold\"), colors=['orangered', 'steelblue'])\nax.set_title('Target distribution')\nax.legend(title='State')","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:27.700549Z","iopub.execute_input":"2022-04-27T10:27:27.701147Z","iopub.status.idle":"2022-04-27T10:27:27.968397Z","shell.execute_reply.started":"2022-04-27T10:27:27.701108Z","shell.execute_reply":"2022-04-27T10:27:27.967391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see, the values of targets are equally distributed","metadata":{}},{"cell_type":"markdown","source":"### Input data","metadata":{}},{"cell_type":"markdown","source":"We need to check the following:\n* The type of variables\n* Are the values within the range (not applicable here)\n* Explore missing values and use best technique for handling missing values","metadata":{}},{"cell_type":"code","source":"# Data types\ndf_train.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:27.971214Z","iopub.execute_input":"2022-04-27T10:27:27.972154Z","iopub.status.idle":"2022-04-27T10:27:27.983222Z","shell.execute_reply.started":"2022-04-27T10:27:27.972096Z","shell.execute_reply":"2022-04-27T10:27:27.982018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Categorical variables","metadata":{}},{"cell_type":"markdown","source":"Although we see that there are no categorical variables in this dataset, formally, by checking the `object` dtype, we can conclude whether a column has a text.","metadata":{}},{"cell_type":"code","source":"# return boolean array\ns = (df_train.dtypes == 'obect')\n\n# Get only indices as a list\nobject_cols = list(s[s].index)\n\nprint(\"Numer of categorical variables: {}\".format(len(object_cols)))","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:37.86628Z","iopub.execute_input":"2022-04-27T10:27:37.866582Z","iopub.status.idle":"2022-04-27T10:27:37.872548Z","shell.execute_reply.started":"2022-04-27T10:27:37.86655Z","shell.execute_reply":"2022-04-27T10:27:37.871804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Check for NULL values","metadata":{}},{"cell_type":"code","source":"# Check for NULL values\ndf_train.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:37.88029Z","iopub.execute_input":"2022-04-27T10:27:37.880915Z","iopub.status.idle":"2022-04-27T10:27:37.942053Z","shell.execute_reply.started":"2022-04-27T10:27:37.880878Z","shell.execute_reply":"2022-04-27T10:27:37.941354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, there are no NULL values in our DataFrame","metadata":{}},{"cell_type":"markdown","source":"### Descriptive statistics ","metadata":{}},{"cell_type":"code","source":"# Describe values from that data frame\ndf_train.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:37.943421Z","iopub.execute_input":"2022-04-27T10:27:37.943769Z","iopub.status.idle":"2022-04-27T10:27:39.069869Z","shell.execute_reply.started":"2022-04-27T10:27:37.94374Z","shell.execute_reply":"2022-04-27T10:27:39.069053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I want to know if every measurement has the same length (0 - 59 seconds)","metadata":{}},{"cell_type":"markdown","source":"I realize that I would actually need to group by sequence and subject to be able to measure steps","metadata":{}},{"cell_type":"code","source":"for observation in df_train.groupby(['sequence', 'subject']).size():\n    if observation != 60:\n        print('There is a sequence which is not 60 seconds long')","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:39.070934Z","iopub.execute_input":"2022-04-27T10:27:39.071166Z","iopub.status.idle":"2022-04-27T10:27:39.193148Z","shell.execute_reply.started":"2022-04-27T10:27:39.071138Z","shell.execute_reply":"2022-04-27T10:27:39.192177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that each sequence is 60 seconds long (which is great)","metadata":{}},{"cell_type":"code","source":"# I want to know how many participants are there?\ndf_train.reset_index().subject.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:39.194508Z","iopub.execute_input":"2022-04-27T10:27:39.194744Z","iopub.status.idle":"2022-04-27T10:27:39.28979Z","shell.execute_reply.started":"2022-04-27T10:27:39.194716Z","shell.execute_reply":"2022-04-27T10:27:39.288886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are **672** participants in this data frame","metadata":{}},{"cell_type":"markdown","source":"I want to know how many times each subject had measurement","metadata":{}},{"cell_type":"code","source":"duration_of_measurement = 60 # In seconds\ndf_train['subject'].value_counts().sort_index() / duration_of_measurement","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:39.291813Z","iopub.execute_input":"2022-04-27T10:27:39.292126Z","iopub.status.idle":"2022-04-27T10:27:39.315877Z","shell.execute_reply.started":"2022-04-27T10:27:39.292091Z","shell.execute_reply":"2022-04-27T10:27:39.314888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The sensors","metadata":{}},{"cell_type":"markdown","source":"Firstly, we need to plot 13 boxplots with outliers to see the situation.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20,10))\nfor index, sensor in enumerate(df_train.columns):\n    if sensor.startswith('sensor'):\n        plt.subplot(4,4, index - 2)\n        plt.boxplot(df_train[sensor])","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:39.317543Z","iopub.execute_input":"2022-04-27T10:27:39.317845Z","iopub.status.idle":"2022-04-27T10:27:45.418889Z","shell.execute_reply.started":"2022-04-27T10:27:39.3178Z","shell.execute_reply":"2022-04-27T10:27:45.417948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the most sensors are defined by outliers. We can see that using boxplot might be kind of unpractical, because we have 1.5 million observations. That's why, we are using histograms","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(20,10))\nfor index, sensor in enumerate(df_train.columns):\n    if sensor.startswith('sensor'):\n        plt.subplot(4,4, index - 2)\n        plt.hist(df_train[sensor], bins = 30)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:45.420598Z","iopub.execute_input":"2022-04-27T10:27:45.421131Z","iopub.status.idle":"2022-04-27T10:27:48.185914Z","shell.execute_reply.started":"2022-04-27T10:27:45.421088Z","shell.execute_reply":"2022-04-27T10:27:48.185088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see the value of upper and lower bound for every sensor and it's outliers using the IQR method","metadata":{}},{"cell_type":"code","source":"# Create an empty data frame\nindex_labels = [col for col in df_train.columns if col.startswith('sensor')]\ndf_sensor_iqr = pd.DataFrame(columns=['upper_bound', 'lower_bound', 'mean', 'NumberOfOutliers'], index = index_labels)\n\nfor index, sensor in enumerate(df_train.columns):\n    if sensor.startswith('sensor'):\n        q1 = df_train[sensor].quantile(q = 0.25)\n        q3 = df_train[sensor].quantile(q = 0.75)\n        mean = df_train[sensor].mean()\n        \n        # IQR region\n        IQR = q3 - q1\n        \n        # finding upper and lower whiskers\n        upper_bound = q3 + (1.5 * IQR)\n        lower_bound = q1 - (1.5 * IQR)\n        \n        # Number of outliers\n        count_outliers = df_train[(df_train[sensor] <= lower_bound) | (df_train[sensor] >= upper_bound)]\n        \n        df_sensor_iqr.loc[index_labels[index - 3]] = [upper_bound, lower_bound, mean, count_outliers.shape[0]]\n        \ndf_sensor_iqr","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:48.187247Z","iopub.execute_input":"2022-04-27T10:27:48.188104Z","iopub.status.idle":"2022-04-27T10:27:49.379199Z","shell.execute_reply.started":"2022-04-27T10:27:48.188054Z","shell.execute_reply":"2022-04-27T10:27:49.377929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, we cannot just 'delete' outliers. Some sensors (like sensor_12) have almost 30% of the values out of interquartile range.\nIQR (Interquartile range) is a measure of statistical dispersion. There are 50% of all values within IQR, while also 99.3% od data within upper and lower bound.\n\nLet's plot the number of outliers (use barplot):","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n\nfig, ax = plt.subplots(figsize=(14,5))\n\nsns.barplot(x = df_sensor_iqr.index, y=df_sensor_iqr.NumberOfOutliers.sort_values(ascending=False))","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:49.381019Z","iopub.execute_input":"2022-04-27T10:27:49.381287Z","iopub.status.idle":"2022-04-27T10:27:50.669144Z","shell.execute_reply.started":"2022-04-27T10:27:49.381244Z","shell.execute_reply":"2022-04-27T10:27:50.668145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image\n\nImage(url='https://upload.wikimedia.org/wikipedia/commons/1/1a/Boxplot_vs_PDF.svg')","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:50.67059Z","iopub.execute_input":"2022-04-27T10:27:50.670827Z","iopub.status.idle":"2022-04-27T10:27:50.679949Z","shell.execute_reply.started":"2022-04-27T10:27:50.670795Z","shell.execute_reply":"2022-04-27T10:27:50.678884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This notebook: https://www.kaggle.com/code/ambrosm/tpsapr22-eda-which-makes-sense suggest to use some kind of non-linear transformation to get it to the normal distribution","metadata":{}},{"cell_type":"markdown","source":"#### *NOTE* : Mahalanobis distance","metadata":{}},{"cell_type":"markdown","source":"The best practice for dealing with outliers in multivariante statistics is to use Mahalanobis distance. I had two approaches with dealing with the outliers.\n\nThe first one is to implement Mahalanobis distance by myself and to add a new variable for each observation and detect outliers. This approach didn't work because there weren't enough memory space as the implementation is quite memory consuming.\n\nThe second approach is to use already implemented `mahalanobis` function in **R** language. However, the version of the package *rpy2* which serves as an interface to **R** is deprecated in Anaconda and cannot be used.  ","metadata":{}},{"cell_type":"markdown","source":"### Correlation between sensors","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14, 8))\n\nsensors = [column for column in df_train.columns if column.startswith('sensor')]\n\n# define the mask to set the values in the upper triangle to True\nmask = np.triu(np.ones_like(df_train[sensors].corr(), dtype=np.bool_))\n\nsns.heatmap(df_train[sensors].corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='seismic')","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:50.681281Z","iopub.execute_input":"2022-04-27T10:27:50.683134Z","iopub.status.idle":"2022-04-27T10:27:53.684922Z","shell.execute_reply.started":"2022-04-27T10:27:50.682346Z","shell.execute_reply":"2022-04-27T10:27:53.684265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the strongest positive correlation is between (*sensor_00*, *sensor_06*), (*sensor_00, sensor_09*), (*sensor_03*, *sensor_07*), (*sensor_03, sensor_11*)","metadata":{}},{"cell_type":"markdown","source":"### Feature engineering","metadata":{}},{"cell_type":"markdown","source":"As we have more than 1.5 million observations, it would be memory efficicient to remove highly correlated variables which would not have such an impact on our training model. ","metadata":{}},{"cell_type":"code","source":"# Drop values of specific columns\ndf_train = df_train.drop(['sensor_06', 'sensor_07', 'sensor_09', 'sensor_11'], axis = 1)\ndf_test = df_test.drop(['sensor_06', 'sensor_07', 'sensor_09', 'sensor_11'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:53.68619Z","iopub.execute_input":"2022-04-27T10:27:53.686646Z","iopub.status.idle":"2022-04-27T10:27:53.775498Z","shell.execute_reply.started":"2022-04-27T10:27:53.686585Z","shell.execute_reply":"2022-04-27T10:27:53.774449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14, 8))\n\nsensors = [column for column in df_train.columns if column.startswith('sensor')]\n\n# define the mask to set the values in the upper triangle to True\nmask = np.triu(np.ones_like(df_train[sensors].corr(), dtype=np.bool_))\n\nsns.heatmap(df_train[sensors].corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='seismic')","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:53.776774Z","iopub.execute_input":"2022-04-27T10:27:53.777345Z","iopub.status.idle":"2022-04-27T10:27:55.360985Z","shell.execute_reply.started":"2022-04-27T10:27:53.777299Z","shell.execute_reply":"2022-04-27T10:27:55.359929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature extraction","metadata":{}},{"cell_type":"markdown","source":"*NOTE*: This part is based on https://www.kaggle.com/code/reymaster/apr-2022-tps-simple-time-series-analysis-xgboost notebook","metadata":{}},{"cell_type":"markdown","source":"We are using *tsfresh* package for feature extraction. *tsfresh* is very convenient in this situation because it enables us to summarize variable's values based on a sequence number (a.k.a. it enables us a summary statistics for each observation which we got by grouping it by some other variable). Also, when we will train the model, the shape of training set and training labels will be the same which is suitable for our case.","metadata":{}},{"cell_type":"code","source":"from tsfresh.feature_extraction import extract_features, MinimalFCParameters","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:55.364927Z","iopub.execute_input":"2022-04-27T10:27:55.365243Z","iopub.status.idle":"2022-04-27T10:27:57.154962Z","shell.execute_reply.started":"2022-04-27T10:27:55.365207Z","shell.execute_reply":"2022-04-27T10:27:57.153848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Return the train_X which contains summary statistics for each variable group by sequence variable\ntrain_X = extract_features(df_train, default_fc_parameters=MinimalFCParameters(), column_id=\"sequence\", column_sort=\"step\")\n\n# Same here\ntest_X = extract_features(df_test, default_fc_parameters=MinimalFCParameters(), column_id=\"sequence\", column_sort=\"step\")","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:27:57.156169Z","iopub.execute_input":"2022-04-27T10:27:57.156435Z","iopub.status.idle":"2022-04-27T10:30:00.848226Z","shell.execute_reply.started":"2022-04-27T10:27:57.1564Z","shell.execute_reply":"2022-04-27T10:30:00.845708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X;","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:30:00.858023Z","iopub.execute_input":"2022-04-27T10:30:00.859747Z","iopub.status.idle":"2022-04-27T10:30:00.886287Z","shell.execute_reply.started":"2022-04-27T10:30:00.859562Z","shell.execute_reply":"2022-04-27T10:30:00.884102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_X;","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:30:00.891019Z","iopub.execute_input":"2022-04-27T10:30:00.891732Z","iopub.status.idle":"2022-04-27T10:30:00.912706Z","shell.execute_reply.started":"2022-04-27T10:30:00.891658Z","shell.execute_reply":"2022-04-27T10:30:00.910536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Impuation methods","metadata":{}},{"cell_type":"markdown","source":"`extract_feature` also produces NaN values which were created by feature calculators that can not be used on the given data, e.g., because the statistics are too low.","metadata":{}},{"cell_type":"markdown","source":"Although `.info()` method stated that there are no NaN values in our data, we will perform imputation for educational purpose.","metadata":{}},{"cell_type":"code","source":"#train_X.info();","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:30:00.916762Z","iopub.execute_input":"2022-04-27T10:30:00.917455Z","iopub.status.idle":"2022-04-27T10:30:00.934787Z","shell.execute_reply.started":"2022-04-27T10:30:00.917343Z","shell.execute_reply":"2022-04-27T10:30:00.932786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best way to perform impuation in *R* is to use MICE package. In python, there are no such complicated impuation methods (or there are still in experimental status). That's why we are using `SimpleImputer` to to replace missing values with the median value along each column. ","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Use median for imputation\nsimple_imputer = SimpleImputer(strategy = 'median')\n\n# Save column names as imputation is removing them\ncolumn_names = train_X.columns\n\ntrain_X = pd.DataFrame(simple_imputer.fit_transform(train_X))\ntest_X = pd.DataFrame(simple_imputer.transform(test_X))\n\n# Imputation removed column names; put them back\ntrain_X.columns = column_names\ntest_X.columns = column_names","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:30:00.938508Z","iopub.execute_input":"2022-04-27T10:30:00.939752Z","iopub.status.idle":"2022-04-27T10:30:02.461062Z","shell.execute_reply.started":"2022-04-27T10:30:00.939623Z","shell.execute_reply":"2022-04-27T10:30:02.459493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Select relevant features","metadata":{}},{"cell_type":"code","source":"from tsfresh import select_features\n\n# Get the state values of train_labels (using for selecting the features)\ny = pd.Series(df_train_labels.state)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:30:02.462714Z","iopub.execute_input":"2022-04-27T10:30:02.463333Z","iopub.status.idle":"2022-04-27T10:30:02.46986Z","shell.execute_reply.started":"2022-04-27T10:30:02.463276Z","shell.execute_reply":"2022-04-27T10:30:02.468698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select features which might be interesting (select appropriate columns)\nX_train_selected = select_features(train_X, y)\nX_train_selected","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:30:02.471593Z","iopub.execute_input":"2022-04-27T10:30:02.471863Z","iopub.status.idle":"2022-04-27T10:30:06.844153Z","shell.execute_reply.started":"2022-04-27T10:30:02.471822Z","shell.execute_reply":"2022-04-27T10:30:06.843368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select the same columns for test data (on which we will make predictions)\nX_test_selected = test_X[X_train_selected.columns]","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:30:06.845804Z","iopub.execute_input":"2022-04-27T10:30:06.846242Z","iopub.status.idle":"2022-04-27T10:30:06.858698Z","shell.execute_reply.started":"2022-04-27T10:30:06.846189Z","shell.execute_reply":"2022-04-27T10:30:06.857737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have two DataFrames - *X_train_selected* and *X_test_selected*. Both frames have same variables which are relevant features extracted from basic statistics provided by *tsfresh* package. ","metadata":{}},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"markdown","source":"Gradient boosting is a ensemble method that combine predictions of several models. XGBoost is an *extreme* GB which provides additional features focused on speed and performance. ","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:30:06.860217Z","iopub.execute_input":"2022-04-27T10:30:06.860665Z","iopub.status.idle":"2022-04-27T10:30:06.965342Z","shell.execute_reply.started":"2022-04-27T10:30:06.860629Z","shell.execute_reply":"2022-04-27T10:30:06.964602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model creation","metadata":{}},{"cell_type":"markdown","source":"***NOTE***: in this scenario, we don't use cross-validation as our dataset is classified as *large* (1.5 million observations). We have sufficient data to fit the model on training set and evaluate it on test set. Here, we can use cross-validation only for educational purposes.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split matrices into random train and test subsets\nX_train, X_test, y_train, y_test = train_test_split(X_train_selected, y)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:30:06.966819Z","iopub.execute_input":"2022-04-27T10:30:06.967301Z","iopub.status.idle":"2022-04-27T10:30:06.990403Z","shell.execute_reply.started":"2022-04-27T10:30:06.967264Z","shell.execute_reply":"2022-04-27T10:30:06.989219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Parameter tuning","metadata":{}},{"cell_type":"markdown","source":"In `XGBClassifier` class, we used few parameters:\n- `n_estimators` - the number of models we include in the ensemble\n- `early_stopping_rounds` - stop iteration after validation score stops inproving\n- `learning_rate` - size of the step in every iteration\n- `n_jobs` - equal to number of cores on the machine","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\n# Create the model\nmodel = XGBClassifier(n_estimators = 1000, \n                      early_stopping_rounds = 5, \n                      learning_rate = 0.05, \n                      n_jobs = 2)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:30:06.991815Z","iopub.execute_input":"2022-04-27T10:30:06.992097Z","iopub.status.idle":"2022-04-27T10:30:06.996795Z","shell.execute_reply.started":"2022-04-27T10:30:06.992063Z","shell.execute_reply":"2022-04-27T10:30:06.996086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Fit the model with X_train and y_train data \nmodel.fit(X_train, \n          y_train,\n          eval_set=[(X_test, y_test)], \n          verbose=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:30:06.99854Z","iopub.execute_input":"2022-04-27T10:30:06.998995Z","iopub.status.idle":"2022-04-27T10:32:34.567294Z","shell.execute_reply.started":"2022-04-27T10:30:06.998893Z","shell.execute_reply":"2022-04-27T10:32:34.566127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model evaluation","metadata":{}},{"cell_type":"markdown","source":"We use confusion matrix and F1 score to evaluate the model","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score, plot_confusion_matrix\n\n# Create confusion matrix\nplot_confusion_matrix(model, X_test, y_test)\n\n# Make prediction based on test_X\nxgb_prediction = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:32:34.569176Z","iopub.execute_input":"2022-04-27T10:32:34.569639Z","iopub.status.idle":"2022-04-27T10:32:35.048869Z","shell.execute_reply.started":"2022-04-27T10:32:34.569589Z","shell.execute_reply":"2022-04-27T10:32:35.047636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating the F1 score of classifier\nprint(f\"F1 Score of the classifier is: {f1_score(y_test, xgb_prediction)}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:32:35.051856Z","iopub.execute_input":"2022-04-27T10:32:35.052443Z","iopub.status.idle":"2022-04-27T10:32:35.063009Z","shell.execute_reply.started":"2022-04-27T10:32:35.052388Z","shell.execute_reply":"2022-04-27T10:32:35.062294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction","metadata":{}},{"cell_type":"code","source":"prediction_values = model.predict(X_test_selected)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:32:35.064564Z","iopub.execute_input":"2022-04-27T10:32:35.065488Z","iopub.status.idle":"2022-04-27T10:32:35.265306Z","shell.execute_reply.started":"2022-04-27T10:32:35.065447Z","shell.execute_reply":"2022-04-27T10:32:35.264602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"sample_sub = pd.read_csv(\"../input/tabular-playground-series-apr-2022/sample_submission.csv\")\n\nsubmission = pd.DataFrame({\n    \"sequence\" : sample_sub.sequence,\n    \"state\" : prediction_values\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T10:33:12.719329Z","iopub.execute_input":"2022-04-27T10:33:12.719761Z","iopub.status.idle":"2022-04-27T10:33:12.753236Z","shell.execute_reply.started":"2022-04-27T10:33:12.719722Z","shell.execute_reply":"2022-04-27T10:33:12.752202Z"},"trusted":true},"execution_count":null,"outputs":[]}]}