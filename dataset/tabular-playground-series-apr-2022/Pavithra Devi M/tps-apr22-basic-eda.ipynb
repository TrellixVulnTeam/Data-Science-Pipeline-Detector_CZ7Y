{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# #import my kaggle_utiles file that has all the custom funcitons i want.\n# import sys\n# sys.path.append(\"/home/pavithra/Pictures/learning/ML/kaggle/\")\n# sys.path\nimport kaggle_utils_py as kaggle_utils\n\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\nimport warnings \n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\nfrom tensorflow.keras.layers import GlobalMaxPooling1D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.layers import Concatenate, LSTM, GRU\nfrom tensorflow.keras.layers import Bidirectional, Multiply\n\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import KFold, GroupKFold\n\nwarnings.simplefilter(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-04-14T17:38:27.652112Z","iopub.execute_input":"2022-04-14T17:38:27.653113Z","iopub.status.idle":"2022-04-14T17:38:33.647222Z","shell.execute_reply.started":"2022-04-14T17:38:27.652993Z","shell.execute_reply":"2022-04-14T17:38:33.646435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain = pd.read_csv(\"../input/tabular-playground-series-apr-2022/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-apr-2022/test.csv\")\ntrain_labels = pd.read_csv(\"../input/tabular-playground-series-apr-2022/train_labels.csv\")\nsub = pd.read_csv(\"../input/tabular-playground-series-apr-2022/sample_submission.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T17:38:33.648801Z","iopub.execute_input":"2022-04-14T17:38:33.649066Z","iopub.status.idle":"2022-04-14T17:38:44.314709Z","shell.execute_reply.started":"2022-04-14T17:38:33.649031Z","shell.execute_reply":"2022-04-14T17:38:44.313983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Shape of the data --->\",train.shape)\nprint(\"Shape of the test data --->\",test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T17:38:44.315841Z","iopub.execute_input":"2022-04-14T17:38:44.316125Z","iopub.status.idle":"2022-04-14T17:38:44.322636Z","shell.execute_reply.started":"2022-04-14T17:38:44.316089Z","shell.execute_reply":"2022-04-14T17:38:44.32197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T17:38:44.32458Z","iopub.execute_input":"2022-04-14T17:38:44.325184Z","iopub.status.idle":"2022-04-14T17:38:44.35898Z","shell.execute_reply.started":"2022-04-14T17:38:44.325147Z","shell.execute_reply":"2022-04-14T17:38:44.358334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train_labels.head())\nprint(\"Shape of the label --->\", train_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T17:38:44.360185Z","iopub.execute_input":"2022-04-14T17:38:44.360408Z","iopub.status.idle":"2022-04-14T17:38:44.370614Z","shell.execute_reply.started":"2022-04-14T17:38:44.360377Z","shell.execute_reply":"2022-04-14T17:38:44.369868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## features\n### train.csv - the training set, comprising ~26,000 60-second recordings of thirteen biological sensors for almost one thousand experimental participants\n- sequence - a unique id for each sequence\n- subject - a unique id for the subject in the experiment\n- step - time step of the recording, in one second intervals\n- sensor_00 - sensor_12 - the value for each of the thirteen sensors at that time step\n### train_labels.csv - the class label for each sequence.\n- sequence - the unique id for each sequence.\n- state - the state associated to each sequence. This is the target which you are trying to predict.","metadata":{}},{"cell_type":"code","source":"# merge the dataset \n# adding labels to the train data\ndata = pd.merge(train, train_labels,how='left', on=\"sequence\")","metadata":{"execution":{"iopub.status.busy":"2022-04-14T17:38:44.372065Z","iopub.execute_input":"2022-04-14T17:38:44.372458Z","iopub.status.idle":"2022-04-14T17:38:44.560173Z","shell.execute_reply.started":"2022-04-14T17:38:44.372424Z","shell.execute_reply":"2022-04-14T17:38:44.559386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### find the window size \nwe have 25968 labels and 1558080 (60 * 25968) --> train data samples (there is no null values). each sequence has 60 steps(1 min) marked as 0 -59. So that could be the window size\n","metadata":{}},{"cell_type":"code","source":"data[data[\"sequence\"] == 0]","metadata":{"execution":{"iopub.status.busy":"2022-04-14T17:38:44.561438Z","iopub.execute_input":"2022-04-14T17:38:44.561711Z","iopub.status.idle":"2022-04-14T17:38:44.646042Z","shell.execute_reply.started":"2022-04-14T17:38:44.561664Z","shell.execute_reply":"2022-04-14T17:38:44.645349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:100%;text-align:left;border-radius:10px 10px;\">1.1 ) Common data Analysis</p>","metadata":{}},{"cell_type":"code","source":"columns, categorical_col, numerical_col,missing_value_df = kaggle_utils.Common_data_analysis(data, missing_value_highlight_threshold=5.0, display_df = True,\n                                                                                         only_show_missing=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T17:38:44.647098Z","iopub.execute_input":"2022-04-14T17:38:44.650101Z","iopub.status.idle":"2022-04-14T17:38:44.92284Z","shell.execute_reply.started":"2022-04-14T17:38:44.650059Z","shell.execute_reply":"2022-04-14T17:38:44.922207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.2\"></a>\n# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:100%;text-align:left;border-radius:10px 10px;\">1.2 ) Numerical Data -- descriptive, distribution, Quantitative</p>","metadata":{}},{"cell_type":"code","source":"kaggle_utils.numerical_data_analysis(data[numerical_col], numerical_col)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T17:38:44.924078Z","iopub.execute_input":"2022-04-14T17:38:44.92447Z","iopub.status.idle":"2022-04-14T17:38:46.584203Z","shell.execute_reply.started":"2022-04-14T17:38:44.924433Z","shell.execute_reply":"2022-04-14T17:38:46.583518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.2\"></a>\n# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:100%;text-align:left;border-radius:10px 10px;\">1.3 ) Distribution Analysis</p>","metadata":{}},{"cell_type":"code","source":"def plot_kde(data, columns, nrow, ncol, figsize, hue_value=None):\n    # find the distubution of the data. ( visualization would be so good)\n    fig, ax = plt.subplots(nrow,ncol, figsize=figsize)\n    # we have 9 numerical values.\n    col, row = ncol,nrow\n    col_count = 0\n    if row<=1:\n        for c in range(col):\n            if hue_value:\n                sns.kdeplot(data=data, x=columns[col_count],hue=hue_value, ax=ax[c])\n            else:\n                sns.kdeplot(data=data, x=columns[col_count], ax=ax[c])\n    else:\n        for r in range(row):\n            for c in range(col):\n                if col_count >= len(columns):\n                    ax[r,c].text(0.5, 0.5, \"no data\")\n                else:\n                    if hue_value:\n                        sns.kdeplot(data=data, x=columns[col_count],hue=hue_value, ax=ax[r,c])\n                    else:\n                        sns.kdeplot(data=data, x=columns[col_count], ax=ax[r,c])\n                col_count +=1\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T17:38:46.58719Z","iopub.execute_input":"2022-04-14T17:38:46.587397Z","iopub.status.idle":"2022-04-14T17:38:46.59822Z","shell.execute_reply.started":"2022-04-14T17:38:46.587373Z","shell.execute_reply":"2022-04-14T17:38:46.597437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.3.2\"></a>\n# <p style=\"font-family:newtimeroman;color:#5811D3;font-size:100%;text-align:left;border-radius:10px 10px;\">2.1 | Sequence/subject distribution</p>","metadata":{}},{"cell_type":"code","source":"# some visualization\ncol_name = ['sequence', 'subject', 'step']\nplot_kde(data, col_name, 1, 3, figsize=(18,8), hue_value=\"state\")","metadata":{"execution":{"iopub.status.busy":"2022-04-14T17:38:46.599231Z","iopub.execute_input":"2022-04-14T17:38:46.599765Z","iopub.status.idle":"2022-04-14T17:39:02.935601Z","shell.execute_reply.started":"2022-04-14T17:38:46.599721Z","shell.execute_reply":"2022-04-14T17:39:02.93484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some visualization\ncol_name = ['sequence', 'subject', 'step']\nfig, ax = plt.subplots(1, 3, figsize=(18,8))\nfor col in range(3):    \n    sns.histplot(data=data, x=col_name[col], hue=\"state\", ax=ax[col])","metadata":{"execution":{"iopub.status.busy":"2022-04-14T17:39:02.93673Z","iopub.execute_input":"2022-04-14T17:39:02.938094Z","iopub.status.idle":"2022-04-14T17:39:06.549618Z","shell.execute_reply.started":"2022-04-14T17:39:02.938053Z","shell.execute_reply":"2022-04-14T17:39:06.54896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some visualization\ncol_name = ['sequence', 'subject', 'step']\nplot_kde(data, col_name, 1, 3, figsize=(18,8))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T17:39:06.550853Z","iopub.execute_input":"2022-04-14T17:39:06.551262Z","iopub.status.idle":"2022-04-14T17:39:21.718987Z","shell.execute_reply.started":"2022-04-14T17:39:06.551223Z","shell.execute_reply":"2022-04-14T17:39:21.718282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## observation\n- step - has a uniform distribution accross all the data as well as based on state\n- sequence - has  a uniform distribution accross all the data","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.3.2\"></a>\n# <p style=\"font-family:newtimeroman;color:#5811D3;font-size:100%;text-align:left;border-radius:10px 10px;\">2.2 | Sensor distribution</p>","metadata":{}},{"cell_type":"code","source":"sensor_cols = ['sensor_'+'%02d'%i for i in range(1, 13)]\nplot_kde(data, sensor_cols, 3, 4, figsize=(18,10))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T17:39:21.720282Z","iopub.execute_input":"2022-04-14T17:39:21.720679Z","iopub.status.idle":"2022-04-14T17:40:16.93042Z","shell.execute_reply.started":"2022-04-14T17:39:21.720642Z","shell.execute_reply":"2022-04-14T17:40:16.929765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_hist(data, columns, nrow, ncol, figsize, hue_value=None):\n    # find the distubution of the data. ( visualization would be so good)\n    fig, ax = plt.subplots(nrow,ncol, figsize=figsize)\n    # we have 9 numerical values.\n    col, row = ncol,nrow\n    col_count = 0\n\n    for r in range(row):\n        for c in range(col):\n            if col_count >= len(columns):\n                ax[r,c].text(0.5, 0.5, \"no data\")\n            else:\n                if hue_value:\n                    sns.boxplot(data=data, x=columns[col_count],hue=hue_value, ax=ax[r,c])\n                else:\n                    sns.boxplot(data=data, x=columns[col_count], ax=ax[r,c])\n            col_count +=1\n\n\nsensor_cols = ['sensor_'+'%02d'%i for i in range(1, 13)]\nplot_hist(data, sensor_cols, 3, 4, figsize=(18,10))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T17:40:16.931757Z","iopub.execute_input":"2022-04-14T17:40:16.932198Z","iopub.status.idle":"2022-04-14T17:40:25.429399Z","shell.execute_reply.started":"2022-04-14T17:40:16.93216Z","shell.execute_reply":"2022-04-14T17:40:25.428759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## observation\n- Most of sensor datas are  normally distributed with outliers.\n- All sensors have large set of zero values :(\n- all sensors except 'sensor_02' has outliers at both side -- 'sensor_02' has outliers left side","metadata":{}},{"cell_type":"code","source":"def plot_scatter(data, columns, nrow, ncol, figsize, hue_value=None):\n    # find the distubution of the data. ( visualization would be so good)\n    fig, ax = plt.subplots(nrow,ncol, figsize=figsize)\n    # we have 9 numerical values.\n    col, row = ncol,nrow\n    col_count = 0\n\n    for r in range(row):\n        for c in range(col):\n            if col_count >= len(columns):\n                ax[r,c].text(0.5, 0.5, \"no data\")\n            else:\n                if hue_value:\n                    sns.scatterplot(data=data, x=columns[col_count],y=data.index, hue=hue_value, ax=ax[r,c])\n                else:\n                    sns.scatterplot(data=data, x=columns[col_count],y = data.index, ax=ax[r,c])\n            col_count +=1\n\n\nsensor_cols = ['sensor_'+'%02d'%i for i in range(1, 13)]\nplot_scatter(data, sensor_cols, 3, 4, figsize=(18,10), hue_value='state')\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T17:40:25.430822Z","iopub.execute_input":"2022-04-14T17:40:25.431291Z","iopub.status.idle":"2022-04-14T18:02:37.560322Z","shell.execute_reply.started":"2022-04-14T17:40:25.431254Z","shell.execute_reply":"2022-04-14T18:02:37.558111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## time series ","metadata":{}},{"cell_type":"code","source":"sequences = [0, 1, 2, 8364, 15404]\nfigure, axes = plt.subplots(13, len(sequences), sharex=True, figsize=(16, 16))\nfor i, sequence in enumerate(sequences):\n    for sensor in range(13):\n        sensor_name = f\"sensor_{sensor:02d}\"\n        plt.subplot(13, len(sequences), sensor * len(sequences) + i + 1)\n        plt.plot(range(60), train[train.sequence == sequence][sensor_name],\n                color=plt.rcParams['axes.prop_cycle'].by_key()['color'][i % 10])\n        if sensor == 0: plt.title(f\"Sequence {sequence}\")\n        if sequence == sequences[0]: plt.ylabel(sensor_name)\nfigure.tight_layout(w_pad=0.1)\nplt.suptitle('Selected Time Series', y=1.02)\nplt.show()\n# This part of code take from Ambros EDA Section. :0","metadata":{"execution":{"iopub.status.busy":"2022-04-14T18:02:37.561544Z","iopub.execute_input":"2022-04-14T18:02:37.562149Z","iopub.status.idle":"2022-04-14T18:02:44.448167Z","shell.execute_reply.started":"2022-04-14T18:02:37.562113Z","shell.execute_reply":"2022-04-14T18:02:44.447553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.3\"></a>\n# <p style=\"font-family:newtimeroman;color:#5811D3;font-size:100%;text-align:left;border-radius:10px 10px;\"> 2.3 | Target Class balance check(only for classification)</p>","metadata":{}},{"cell_type":"code","source":"# mostly has equal number of samples in both classes\nsns.countplot(data=data, y=\"state\")","metadata":{"execution":{"iopub.status.busy":"2022-04-14T18:02:44.449297Z","iopub.execute_input":"2022-04-14T18:02:44.449628Z","iopub.status.idle":"2022-04-14T18:02:44.71652Z","shell.execute_reply.started":"2022-04-14T18:02:44.449596Z","shell.execute_reply":"2022-04-14T18:02:44.715837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = data[\"state\"].value_counts()\nprint(count)\nprint(\"percentage of first class --- >\",count[0]/data.shape[0])\nprint(\"percentage of second class --->\", count[1]/data.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-04-14T18:02:44.717594Z","iopub.execute_input":"2022-04-14T18:02:44.717856Z","iopub.status.idle":"2022-04-14T18:02:44.73348Z","shell.execute_reply.started":"2022-04-14T18:02:44.717819Z","shell.execute_reply":"2022-04-14T18:02:44.732593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data for the first 60 seconds\ndata[data['sequence']==0]","metadata":{"execution":{"iopub.status.busy":"2022-04-14T18:02:44.73487Z","iopub.execute_input":"2022-04-14T18:02:44.735224Z","iopub.status.idle":"2022-04-14T18:02:44.786424Z","shell.execute_reply.started":"2022-04-14T18:02:44.735189Z","shell.execute_reply":"2022-04-14T18:02:44.785811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.2\"></a>\n# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:100%;text-align:left;border-radius:10px 10px;\">1.3 ) Outlier Detection</p>","metadata":{}},{"cell_type":"markdown","source":"- There are lots of hypothesis tests to find the presents of outlier. Since our sensor data follows almost normal distribution we can go with **Grubbs Test**","metadata":{}},{"cell_type":"code","source":"def grubbs_test(feature_value, col_name):\n    print(\"{:=^40}\".format(f\" Test starts for {col_name}\"))\n    n = len(feature_value)\n    mean_feature = np.mean(feature_value)\n    st_dev_feature = np.std(feature_value)\n    g = (max(abs(feature_value-mean_feature))) / st_dev_feature\n    print(\"Grubbs test statistic value:\",g)\n\n    t_value = stats.t.ppf(1 - 0.05 / (2 * n), n - 2)\n    g_critical = ((n - 1) * np.sqrt(np.square(t_value))) / (np.sqrt(n) * np.sqrt(n - 2 + np.square(t_value)))\n    print(\"Grubbs Critical Value:\",g_critical)\n    if g > g_critical:\n        print(\"So our G value is greater than G critical value --> so reject the null hypothesis -- variable has atleast one outlier :(\")\n    else:\n        print(\"So our G value is lesser than G critical value --> so accept the null hypothesis -- variable has no outlier :) \")\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-14T18:02:44.787587Z","iopub.execute_input":"2022-04-14T18:02:44.787972Z","iopub.status.idle":"2022-04-14T18:02:44.79531Z","shell.execute_reply.started":"2022-04-14T18:02:44.787924Z","shell.execute_reply":"2022-04-14T18:02:44.794574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in sensor_cols:\n    grubbs_test(data[col], col)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T18:02:44.796715Z","iopub.execute_input":"2022-04-14T18:02:44.796977Z","iopub.status.idle":"2022-04-14T18:02:47.13238Z","shell.execute_reply.started":"2022-04-14T18:02:44.796929Z","shell.execute_reply":"2022-04-14T18:02:47.131627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## observation\n- All the variables has outliers :( -- will get the number of outliers to come to an solution","metadata":{}},{"cell_type":"code","source":"df, outlier_df, lower_limit_df, upper_limit_df = kaggle_utils.find_outlier_z_score_method(data,new_feature=True, return_limits=True)\noutlier_df[\"percentage of outlier\"] = outlier_df[\"Number of outliers\"] / data.shape[0]\noutlier_df","metadata":{"execution":{"iopub.status.busy":"2022-04-14T18:02:47.13353Z","iopub.execute_input":"2022-04-14T18:02:47.133994Z","iopub.status.idle":"2022-04-14T18:02:47.453654Z","shell.execute_reply.started":"2022-04-14T18:02:47.133956Z","shell.execute_reply":"2022-04-14T18:02:47.452969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## observation\n- We have some amount of outliers -- we can build a deep nueral network / we need some create feature engineering to deal with this outliers. I am going to use deep nueral network so now we can leave this outliers as it is.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.2\"></a>\n# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:100%;text-align:left;border-radius:10px 10px;\">1.4 | Correlation</p>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,8))\nsns.heatmap(data.corr(), annot=True, cbar=True, cmap=\"YlGnBu\")","metadata":{"execution":{"iopub.status.busy":"2022-04-14T18:02:47.454745Z","iopub.execute_input":"2022-04-14T18:02:47.45533Z","iopub.status.idle":"2022-04-14T18:02:50.349426Z","shell.execute_reply.started":"2022-04-14T18:02:47.455289Z","shell.execute_reply":"2022-04-14T18:02:50.348759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## observation\n- **No null** values :)\n- **No categorical** values :)\n- **Not so much correlated** features :)\n- All features are **almost normally** distributed :)\n- **Balanced target** :) :)\n- All features has **outliers** :( --> but we are going to use deep nueral networks , so no need to worry about this :)\n\n- [action] Only thing we have limited features -- need most powerful feature engineering \n- [action] **Need to scale** the data. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.2\"></a>\n# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:100%;text-align:left;border-radius:10px 10px;\">1.4 | Feature Engineering </p>","metadata":{}},{"cell_type":"code","source":"for sensor in sensor_cols: \n    data[f\"{sensor}\" + '_lag1'] = data.groupby('sequence')[f\"{sensor}\"].shift(1)  \n    data[f\"{sensor}\" + '_lag1'].fillna(0, inplace=True)\n    data[f\"{sensor}\" + '_diff1'] = data[f\"{sensor}\"] - data[f\"{sensor}\" + '_lag1']\n\n    # do the same for test data\n    test[f\"{sensor}\" + '_lag1'] = test.groupby('sequence')[f\"{sensor}\"].shift(1)  \n    test[f\"{sensor}\" + '_lag1'].fillna(0, inplace=True)\n    test[f\"{sensor}\" + '_diff1'] = test[f\"{sensor}\"] - test[f\"{sensor}\" + '_lag1']","metadata":{"execution":{"iopub.status.busy":"2022-04-14T18:02:50.350612Z","iopub.execute_input":"2022-04-14T18:02:50.351349Z","iopub.status.idle":"2022-04-14T18:02:51.236274Z","shell.execute_reply.started":"2022-04-14T18:02:50.351314Z","shell.execute_reply":"2022-04-14T18:02:51.233602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T18:02:51.237487Z","iopub.execute_input":"2022-04-14T18:02:51.237731Z","iopub.status.idle":"2022-04-14T18:02:51.284686Z","shell.execute_reply.started":"2022-04-14T18:02:51.237698Z","shell.execute_reply":"2022-04-14T18:02:51.283964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scale the data \nstand_scale = StandardScaler() # since our data almost look like normal distribution Standscalar is the best option here.\n\ntrain_X = data[data[\"state\"].isnull() == False]\ntest_X = test.copy()\n\ncol = data.columns.tolist()[3:]\ncol.remove(\"state\")\ntrain_X[col] = stand_scale.fit_transform(train_X[col])\ntest[col] = stand_scale.transform(test[col])\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T18:02:51.286045Z","iopub.execute_input":"2022-04-14T18:02:51.286451Z","iopub.status.idle":"2022-04-14T18:02:53.520455Z","shell.execute_reply.started":"2022-04-14T18:02:51.286414Z","shell.execute_reply":"2022-04-14T18:02:53.519648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gte the label \ntrain_labels = pd.read_csv('../input/tabular-playground-series-apr-2022/train_labels.csv')\nlabels = train_labels[\"state\"]\n\ntrain_X = train_X.drop([\"sequence\", \"subject\", \"step\",'state'], axis=1).values\ntrain_X = train_X.reshape(-1, 60, train_X.shape[-1])\n\ntest = test.drop([\"sequence\", \"subject\", \"step\"], axis=1).values\ntest = test.reshape(-1, 60, test.shape[-1])","metadata":{"execution":{"iopub.status.busy":"2022-04-14T18:02:53.524365Z","iopub.execute_input":"2022-04-14T18:02:53.524567Z","iopub.status.idle":"2022-04-14T18:02:53.73574Z","shell.execute_reply.started":"2022-04-14T18:02:53.524542Z","shell.execute_reply":"2022-04-14T18:02:53.734988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"groups = data[\"sequence\"]","metadata":{"execution":{"iopub.status.busy":"2022-04-14T18:02:53.73718Z","iopub.execute_input":"2022-04-14T18:02:53.737432Z","iopub.status.idle":"2022-04-14T18:02:53.742818Z","shell.execute_reply.started":"2022-04-14T18:02:53.737397Z","shell.execute_reply":"2022-04-14T18:02:53.741997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-14T18:02:53.744605Z","iopub.execute_input":"2022-04-14T18:02:53.745098Z","iopub.status.idle":"2022-04-14T18:02:53.752828Z","shell.execute_reply.started":"2022-04-14T18:02:53.745058Z","shell.execute_reply":"2022-04-14T18:02:53.751987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.2\"></a>\n# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:100%;text-align:left;border-radius:10px 10px;\">1.4 | Modelling </p>","metadata":{}},{"cell_type":"code","source":"# doing it ... new to LSTM and RNN learning it..................","metadata":{"execution":{"iopub.status.busy":"2022-04-14T18:02:53.754009Z","iopub.execute_input":"2022-04-14T18:02:53.754412Z","iopub.status.idle":"2022-04-14T18:02:53.761704Z","shell.execute_reply.started":"2022-04-14T18:02:53.754376Z","shell.execute_reply":"2022-04-14T18:02:53.760893Z"},"trusted":true},"execution_count":null,"outputs":[]}]}