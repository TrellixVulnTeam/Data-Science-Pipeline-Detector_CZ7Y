{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Exploration and Regression\nThis notebook explores the tabular dataset and contains a simple solution based on regression. The results are not great, however this notebook is intended to provide insight into the dataset and show a basic approach to a complete kaggle workflow. This Notebook is mostly unfiltered and simultaniously records my process. After some experimentation I might create a summary notebook! I hope you guys like it!\n\n## Initialization","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Helper variables\nsample_data_path = \"/kaggle/input/tabular-playground-series-apr-2022/sample_submission.csv\"\ntrain_data_path = \"/kaggle/input/tabular-playground-series-apr-2022/train.csv\"\ntrain_labels_path = \"/kaggle/input/tabular-playground-series-apr-2022/train_labels.csv\"\ntest_data_path = \"/kaggle/input/tabular-playground-series-apr-2022/test.csv\"\n\n# Import data to pandas df\ntrain_data = pd.read_csv(train_data_path)\ntrain_labels = pd.read_csv(train_labels_path)\ntest_data = pd.read_csv(test_data_path)\n\n# Useful variables\nsensor_cols = [col for col in train_data.columns if col not in [\"sequence\", \"subject\", \"step\"]]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-12T03:44:30.882708Z","iopub.execute_input":"2022-04-12T03:44:30.883496Z","iopub.status.idle":"2022-04-12T03:44:41.453237Z","shell.execute_reply.started":"2022-04-12T03:44:30.883452Z","shell.execute_reply":"2022-04-12T03:44:41.452559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploration\nFirst some data exploration. Shows the format of the data and some relevant statistics.","metadata":{}},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:41.456594Z","iopub.execute_input":"2022-04-12T03:44:41.456852Z","iopub.status.idle":"2022-04-12T03:44:41.481366Z","shell.execute_reply.started":"2022-04-12T03:44:41.456818Z","shell.execute_reply":"2022-04-12T03:44:41.480178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that multiple rows correspond to the same measurement, or sequence. The rows have distinct time steps.","metadata":{}},{"cell_type":"code","source":"train_labels.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:41.482763Z","iopub.execute_input":"2022-04-12T03:44:41.483126Z","iopub.status.idle":"2022-04-12T03:44:41.498352Z","shell.execute_reply.started":"2022-04-12T03:44:41.483093Z","shell.execute_reply":"2022-04-12T03:44:41.49734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels['state'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:41.50087Z","iopub.execute_input":"2022-04-12T03:44:41.501698Z","iopub.status.idle":"2022-04-12T03:44:41.511197Z","shell.execute_reply.started":"2022-04-12T03:44:41.501637Z","shell.execute_reply":"2022-04-12T03:44:41.510531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The labels are mapped directly to a sequence. Time to evaluate the data a bit more by looking at some summary staticstics.","metadata":{}},{"cell_type":"markdown","source":"### Sequences and Labels\nLet's compare the number of sequences and number of timesteps first. We should verify whether every sequence has an equal number of time steps available.","metadata":{"execution":{"iopub.status.busy":"2022-04-09T03:13:25.813035Z","iopub.execute_input":"2022-04-09T03:13:25.813427Z","iopub.status.idle":"2022-04-09T03:13:25.81902Z","shell.execute_reply.started":"2022-04-09T03:13:25.813388Z","shell.execute_reply":"2022-04-09T03:13:25.817825Z"}}},{"cell_type":"code","source":"per_sequence = train_data.groupby(\"sequence\")\nper_sequence['step'].count().unique()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:41.512395Z","iopub.execute_input":"2022-04-12T03:44:41.512755Z","iopub.status.idle":"2022-04-12T03:44:41.575372Z","shell.execute_reply.started":"2022-04-12T03:44:41.512725Z","shell.execute_reply":"2022-04-12T03:44:41.574745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That seems to be the case, cool! Now let's check if the training labels fully account for all the sequences in the training data.","metadata":{}},{"cell_type":"code","source":"if (train_labels[\"sequence\"] == train_data[\"sequence\"].unique()).all():\n    print(\"All sequences accounted for!\")\nelse:\n    print(\"Nope, some sequences differ\")","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:41.576467Z","iopub.execute_input":"2022-04-12T03:44:41.57682Z","iopub.status.idle":"2022-04-12T03:44:41.602613Z","shell.execute_reply.started":"2022-04-12T03:44:41.576792Z","shell.execute_reply":"2022-04-12T03:44:41.601587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How about the number of sequences?","metadata":{}},{"cell_type":"code","source":"train_data['sequence'].unique().size","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:41.604018Z","iopub.execute_input":"2022-04-12T03:44:41.604665Z","iopub.status.idle":"2022-04-12T03:44:41.626349Z","shell.execute_reply.started":"2022-04-12T03:44:41.604617Z","shell.execute_reply":"2022-04-12T03:44:41.625376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since most of the data is present, we can take a look at some sensor measurement properties.\n\n### Sensor Measurement Properties\n\nWe can get a sense by plotting some standard statistics such as mean, count and distribution parameters. Luckely pandas has us covered with the `describe` functionality.","metadata":{}},{"cell_type":"code","source":"train_data[\"sensor_00\"].describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:41.627655Z","iopub.execute_input":"2022-04-12T03:44:41.628209Z","iopub.status.idle":"2022-04-12T03:44:41.700202Z","shell.execute_reply.started":"2022-04-12T03:44:41.628175Z","shell.execute_reply":"2022-04-12T03:44:41.699317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These statistics describe the properties of the data summed over **both sequences and steps**. We can conclude the the data has not yet been normalized, or the data contains big outliers. Since the difference in quantiles is quite large it might help to create some plots. ","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(sensor_cols.__len__(), 1, figsize=(15,100), sharex=True)\nplt.xlabel('sequence')\nfor col, plt_ax in zip(sensor_cols, axs):\n    plt_ax.title.set_text(col)\n    train_data[col].plot(ax=plt_ax)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:41.701413Z","iopub.execute_input":"2022-04-12T03:44:41.70165Z","iopub.status.idle":"2022-04-12T03:44:50.296019Z","shell.execute_reply.started":"2022-04-12T03:44:41.701622Z","shell.execute_reply":"2022-04-12T03:44:50.29531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the plots we can conclude that the data has not yet been normalized. We'll get back to preprocessing later. \n\nIt would be interesting to see how many unique subjects are present in the dataset. Lastly it would be good to see if any of the sensors or the subject entries are invalid (NAN). Let's do that now.","metadata":{}},{"cell_type":"code","source":"train_data[\"subject\"].unique().size","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:50.298543Z","iopub.execute_input":"2022-04-12T03:44:50.298933Z","iopub.status.idle":"2022-04-12T03:44:50.315253Z","shell.execute_reply.started":"2022-04-12T03:44:50.29889Z","shell.execute_reply":"2022-04-12T03:44:50.314263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:50.316806Z","iopub.execute_input":"2022-04-12T03:44:50.317255Z","iopub.status.idle":"2022-04-12T03:44:50.371638Z","shell.execute_reply.started":"2022-04-12T03:44:50.317214Z","shell.execute_reply":"2022-04-12T03:44:50.370779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No invalid data! Such a luxury ;). That means we can move on to preprocessing.\n\n## Preprocessing\n\nSince the sensor data will be used to train a simple regression model, we will scale the data so every value fits between 0 and 1. I am ignoring the peculiar data distribution in sensor_02 for now. ","metadata":{}},{"cell_type":"code","source":"from pandas import DataFrame\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\ndef normalize_columns(bio_data: DataFrame, cols) -> DataFrame:\n    \"\"\"\n    Normalize data in the provided columns between 0 and 1.\n    \"\"\"\n    bio_data_norm = bio_data.copy()\n    normalized_cols = pd.DataFrame(scaler.fit_transform(bio_data[cols]), columns=cols)\n    bio_data_norm[cols] = normalized_cols\n    return bio_data_norm","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:50.372822Z","iopub.execute_input":"2022-04-12T03:44:50.373049Z","iopub.status.idle":"2022-04-12T03:44:50.379053Z","shell.execute_reply.started":"2022-04-12T03:44:50.373022Z","shell.execute_reply":"2022-04-12T03:44:50.378238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"norm = normalize_columns(train_data, sensor_cols)\nnorm.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:50.380387Z","iopub.execute_input":"2022-04-12T03:44:50.380623Z","iopub.status.idle":"2022-04-12T03:44:51.750333Z","shell.execute_reply.started":"2022-04-12T03:44:50.380594Z","shell.execute_reply":"2022-04-12T03:44:51.749156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For now we can choose a basic feature set for prediction. A straightforward choice is the mean over time, per sequence, of each normalized sensor value. This generates a set of 13 features per sequence that can be used for prediction. Obviously these feature are not sensitive to distribution at all, but consider this a nice starting point. We can analyze the correlation of sensor means with respect to the state in the dataset by plotting a correlation heatmap.","metadata":{}},{"cell_type":"code","source":"from pandas import DataFrame\n\ndef time_mean_per_sequence(data_frame: DataFrame, cols):\n    \"\"\"\n    Return a dataframe with the mean over time steps of chosen columns.\n    \"\"\"\n    return data_frame[cols].groupby('sequence').mean()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:51.751868Z","iopub.execute_input":"2022-04-12T03:44:51.752136Z","iopub.status.idle":"2022-04-12T03:44:51.757685Z","shell.execute_reply.started":"2022-04-12T03:44:51.752105Z","shell.execute_reply":"2022-04-12T03:44:51.756698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_sensor_readings = time_mean_per_sequence(norm, sensor_cols + ['sequence'])\nmean_sensor_readings","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:51.759114Z","iopub.execute_input":"2022-04-12T03:44:51.759366Z","iopub.status.idle":"2022-04-12T03:44:52.005406Z","shell.execute_reply.started":"2022-04-12T03:44:51.759336Z","shell.execute_reply":"2022-04-12T03:44:52.004377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\nmean_with_labels = mean_sensor_readings.copy() \nmean_with_labels['state'] = train_labels['state']\n\nsns.set(rc = {'figure.figsize':(20,8)})\nsns.heatmap(mean_with_labels.corr(), annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:52.006816Z","iopub.execute_input":"2022-04-12T03:44:52.007075Z","iopub.status.idle":"2022-04-12T03:44:53.757815Z","shell.execute_reply.started":"2022-04-12T03:44:52.007043Z","shell.execute_reply":"2022-04-12T03:44:53.75692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that none of the features correlate strongly with the state, which probably means we will have to intruduce other features later to improve the model.","metadata":{"execution":{"iopub.status.busy":"2022-04-10T15:10:57.982656Z","iopub.execute_input":"2022-04-10T15:10:57.983052Z","iopub.status.idle":"2022-04-10T15:10:57.99078Z","shell.execute_reply.started":"2022-04-10T15:10:57.983014Z","shell.execute_reply":"2022-04-10T15:10:57.989938Z"}}},{"cell_type":"markdown","source":"## Regression Model\n\nRegression might not be the best tool for the job here, but it is again great to start with because it is simple to implement and understand. I'll be using `sklearn` to fit a multivariate regression model first. The preprocessor first normalizes all sensor values and then takes the mean over time. ","metadata":{}},{"cell_type":"code","source":"def pre_processor(data_frame):\n    return time_mean_per_sequence(normalize_columns(data_frame, sensor_cols), sensor_cols + ['sequence'])","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:53.75895Z","iopub.execute_input":"2022-04-12T03:44:53.759189Z","iopub.status.idle":"2022-04-12T03:44:53.763917Z","shell.execute_reply.started":"2022-04-12T03:44:53.759159Z","shell.execute_reply":"2022-04-12T03:44:53.762877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlm = LinearRegression()\n\nfeatures = pre_processor(train_data)\nlm.fit(features.values, train_labels['state'].values)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:53.765394Z","iopub.execute_input":"2022-04-12T03:44:53.765616Z","iopub.status.idle":"2022-04-12T03:44:54.226836Z","shell.execute_reply.started":"2022-04-12T03:44:53.76559Z","shell.execute_reply":"2022-04-12T03:44:54.225644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can use the model to predict on the test set. Then it's just a matter of putting the predictions in the right format and handing it in!","metadata":{}},{"cell_type":"code","source":"test_features = pre_processor(test_data)\ntest_predictions = lm.predict(test_features.values)\ntest_predictions_frame = pd.DataFrame(test_predictions, columns=['state'])\ntest_predictions_frame.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:54.229023Z","iopub.execute_input":"2022-04-12T03:44:54.229397Z","iopub.status.idle":"2022-04-12T03:44:54.519106Z","shell.execute_reply.started":"2022-04-12T03:44:54.229352Z","shell.execute_reply":"2022-04-12T03:44:54.517907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions_frame['state'] = test_predictions_frame['state'].clip(0, 1)\ntest_predictions_frame.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:54.521354Z","iopub.execute_input":"2022-04-12T03:44:54.521716Z","iopub.status.idle":"2022-04-12T03:44:54.54818Z","shell.execute_reply.started":"2022-04-12T03:44:54.521674Z","shell.execute_reply":"2022-04-12T03:44:54.547183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions_frame['sequence'] = test_data['sequence'].unique()\ntest_predictions_frame.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:54.550206Z","iopub.execute_input":"2022-04-12T03:44:54.550908Z","iopub.status.idle":"2022-04-12T03:44:54.579781Z","shell.execute_reply.started":"2022-04-12T03:44:54.55086Z","shell.execute_reply":"2022-04-12T03:44:54.57882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions_frame.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:54.582011Z","iopub.execute_input":"2022-04-12T03:44:54.582823Z","iopub.status.idle":"2022-04-12T03:44:54.633251Z","shell.execute_reply.started":"2022-04-12T03:44:54.582768Z","shell.execute_reply":"2022-04-12T03:44:54.632421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly the linear model predicts negative state values for most of the sequences. This might have to do with the fact that the features lack predictive power. The most logical followup would thus be to look for features with better predictive power, and use them for a better prediction. ","metadata":{}},{"cell_type":"markdown","source":"## Improved Features\n\nSince the previous model does not appear to be better than a coinflip, we can try to improve the performance by introducing lag features. When evaluated by their correlation with the target state we can get a sense of their predictive power. It's also worth plotting the sequence based mean of each sensor to try and identify global patterns over time. In this case the mean over sequences does not seem to contain clearly identifyable trends.","metadata":{}},{"cell_type":"code","source":"mean_per_step = train_data.groupby('step').mean()\nmean_per_step[sensor_cols].plot()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:54.635146Z","iopub.execute_input":"2022-04-12T03:44:54.635887Z","iopub.status.idle":"2022-04-12T03:44:55.249218Z","shell.execute_reply.started":"2022-04-12T03:44:54.635834Z","shell.execute_reply":"2022-04-12T03:44:55.248083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lag_features(data_frame):\n    \"\"\"\n    Return a dataframe containing several lag features of current columns,\n    excluding the sequence and step columns.\n    \"\"\"\n    lag_features = pd.DataFrame()\n    columns = [col for col in data_frame.columns if col not in ['subject', 'sequence', 'step'] ]\n    for col in columns:\n        lag_features[col + '_0_20'] = data_frame[(data_frame['step'] >= 0) & (data_frame['step'] < 20)].groupby('sequence').mean()[col]\n        lag_features[col + '_20_40'] = data_frame[(data_frame['step'] >= 20) & (data_frame['step'] < 40)].groupby('sequence').mean()[col]\n        lag_features[col + '_40_60'] = data_frame[(data_frame['step'] >= 40) & (data_frame['step'] < 60)].groupby('sequence').mean()[col]\n    return lag_features","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:55.251082Z","iopub.execute_input":"2022-04-12T03:44:55.25142Z","iopub.status.idle":"2022-04-12T03:44:55.262642Z","shell.execute_reply.started":"2022-04-12T03:44:55.251376Z","shell.execute_reply":"2022-04-12T03:44:55.261398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lagged_sensors = lag_features(train_data)\nlagged_sensors['state'] = train_labels['state']\nsns.heatmap(lagged_sensors.corr(), annot=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:44:55.264067Z","iopub.execute_input":"2022-04-12T03:44:55.264573Z","iopub.status.idle":"2022-04-12T03:45:00.205149Z","shell.execute_reply.started":"2022-04-12T03:44:55.264537Z","shell.execute_reply":"2022-04-12T03:45:00.204052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Still the correlation with the target state looks weak. Let's try using these features to create a regression model.\n\n## Regression With Lag Features","metadata":{}},{"cell_type":"code","source":"def pre_processor_lag(data_frame):\n    return lag_features(normalize_columns(data_frame, sensor_cols))","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:45:00.206645Z","iopub.execute_input":"2022-04-12T03:45:00.207563Z","iopub.status.idle":"2022-04-12T03:45:00.212271Z","shell.execute_reply.started":"2022-04-12T03:45:00.207515Z","shell.execute_reply":"2022-04-12T03:45:00.211359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlm_lag = LinearRegression()\n\nfeatures_with_lag = pre_processor_lag(train_data)\nlm_lag.fit(features_with_lag.values, train_labels['state'].values)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:45:00.213613Z","iopub.execute_input":"2022-04-12T03:45:00.213971Z","iopub.status.idle":"2022-04-12T03:45:04.246956Z","shell.execute_reply.started":"2022-04-12T03:45:00.213928Z","shell.execute_reply":"2022-04-12T03:45:04.245836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_features_lag = pre_processor_lag(test_data)\ntest_predictions_lag = lm_lag.predict(test_features_lag.values)\ntest_predictions_frame_lag = pd.DataFrame(test_predictions_lag, columns=['state'])\ntest_predictions_frame_lag['sequence'] = test_data['sequence'].unique()\ntest_predictions_frame_lag.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:45:04.251545Z","iopub.execute_input":"2022-04-12T03:45:04.252128Z","iopub.status.idle":"2022-04-12T03:45:06.262891Z","shell.execute_reply.started":"2022-04-12T03:45:04.252074Z","shell.execute_reply":"2022-04-12T03:45:06.261798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions_frame_lag['state'] = test_predictions_frame_lag['state'].clip(0, 1).round()\ntest_predictions_frame_lag.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T04:08:03.069275Z","iopub.execute_input":"2022-04-12T04:08:03.070517Z","iopub.status.idle":"2022-04-12T04:08:03.092713Z","shell.execute_reply.started":"2022-04-12T04:08:03.070461Z","shell.execute_reply":"2022-04-12T04:08:03.091828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions_frame_lag.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T04:08:11.399385Z","iopub.execute_input":"2022-04-12T04:08:11.399724Z","iopub.status.idle":"2022-04-12T04:08:11.43316Z","shell.execute_reply.started":"2022-04-12T04:08:11.399688Z","shell.execute_reply":"2022-04-12T04:08:11.432368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the model is not improving much, it is time for a new notebook with a new approach!","metadata":{}}]}