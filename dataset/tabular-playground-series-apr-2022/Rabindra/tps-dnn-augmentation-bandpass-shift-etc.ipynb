{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Sensor Data: Time-Series Classification\n\n- Augment data with: scaling, shift, vertical flip and bandpass filter of each sensor time series\n- Train LSTM/GRU - DNN model\n- **LS: 0.965**","metadata":{}},{"cell_type":"code","source":"# packages\nimport os\nimport numpy as np \nimport pandas as pd \npd.options.mode.chained_assignment = None\npd.set_option('display.max_columns', None)\nfrom typing import Tuple, List, Dict\nimport itertools\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n\nfrom scipy import signal\n\nfrom sklearn import metrics\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedGroupKFold\n\n# tensorflow\nimport tensorflow as tf\nfrom tensorflow.data import Dataset\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import GlobalMaxPooling1D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.layers import Concatenate, LSTM, GRU, Bidirectional","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-04T19:58:44.070731Z","iopub.execute_input":"2022-04-04T19:58:44.070987Z","iopub.status.idle":"2022-04-04T19:58:44.079356Z","shell.execute_reply.started":"2022-04-04T19:58:44.070958Z","shell.execute_reply":"2022-04-04T19:58:44.078606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    train_path = \"/kaggle/input/tabular-playground-series-apr-2022/train.csv\"\n    test_path = \"/kaggle/input/tabular-playground-series-apr-2022/test.csv\"\n    train_labels_path = \"/kaggle/input/tabular-playground-series-apr-2022/train_labels.csv\"\n    sample_path = \"/kaggle/input/tabular-playground-series-apr-2022/sample_submission.csv\"\n    \n    seq_length = 60\n    num_sensor = 13\n    \n    NFOLDS = 3\n    EPOCHS = 2\n    BATCH_SIZE = 128\n    \n# GPUs\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n    \n# data loading\ntrain = pd.read_csv(Config.train_path)\ntest = pd.read_csv(Config.test_path)\ntrain_labels = pd.read_csv(Config.train_labels_path)\n\nsubmission = pd.read_csv(Config.sample_path)\nsubmission.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T19:58:51.594399Z","iopub.execute_input":"2022-04-04T19:58:51.594864Z","iopub.status.idle":"2022-04-04T19:58:57.569602Z","shell.execute_reply.started":"2022-04-04T19:58:51.594826Z","shell.execute_reply":"2022-04-04T19:58:57.568845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.signal import butter, sosfilt\n\ndef scaling(signal, scale=0.05):\n    '''Introduces some noise into the signal'''\n    scale_factor = np.random.normal(loc=1.0, scale=scale, size=(1, signal.shape[1]))\n    noise = np.matmul(np.ones((signal.shape[0], 1)), scale_factor)\n    return signal * noise\n\ndef vertical_flip(signal):\n    '''\n    Input: signal\n    Return: vertically flipped signal\n    '''\n    return signal[::-1, :]\n\ndef shift(signal, interval=10):\n    '''\n    Input: signal\n    Return: shited signal by interval\n    '''\n    for col in range(signal.shape[1]):\n        offset = np.random.choice(range(-interval, interval))/100\n        signal[:, col] += offset\n    return signal\n\ndef butter_bandpass(lowcut, highcut, fs, order=5):\n    nyq = 0.5 * fs\n    low = lowcut / nyq\n    high = highcut / nyq\n    sos = butter(order, [low, high], analog = False, btype=\"band\", output=\"sos\")\n    return sos\n\ndef butter_bandpass_filter(signal, lowcut, highcut, fs, order=5):\n    sos = butter_bandpass(lowcut, highcut, fs, order=order)\n    transformed_signal = np.zeros([signal.shape[0], signal.shape[1]])\n    for i in range(signal.shape[1]): \n        transformed_signal[:, i] = sosfilt(sos, signal[:, i])  \n    return transformed_signal\n\ndef transform(signal, train=True):\n    '''\n    Input: a signal to transform\n    Output: for a training signla: it is randomly transformed and returned \n    '''\n    if train:\n        rn = np.random.randn()\n        \n        # randomly doing - scaling/flipping/shift/bandpass filter\n        if rn < 0.25:\n            signal = scaling(signal, scale=0.10)\n        elif rn >= 0.25 and rn < 0.50:\n            signal = vertical_flip(signal)\n        elif rn >= 0.5 and rn < 0.75:\n            signal = shift(signal, interval=10)\n        else:\n            signal = butter_bandpass_filter(signal, 0.05, 48, 256)\n\n    return signal","metadata":{"execution":{"iopub.status.busy":"2022-04-04T20:21:31.208867Z","iopub.execute_input":"2022-04-04T20:21:31.209175Z","iopub.status.idle":"2022-04-04T20:21:31.223034Z","shell.execute_reply.started":"2022-04-04T20:21:31.209143Z","shell.execute_reply":"2022-04-04T20:21:31.221957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot Sensor_01 first series\nplt.plot(train[:60].sensor_01.values);\nplt.title(\"original\");","metadata":{"execution":{"iopub.status.busy":"2022-04-04T20:21:33.547469Z","iopub.execute_input":"2022-04-04T20:21:33.547874Z","iopub.status.idle":"2022-04-04T20:21:33.794547Z","shell.execute_reply.started":"2022-04-04T20:21:33.547837Z","shell.execute_reply":"2022-04-04T20:21:33.793538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot Sensor_01 first series: with augmentations:\n\nfig, axs = plt.subplots(nrows=1, ncols=4, figsize=(24, 4))\n\naxs[0].set_title(\"scaling\")\naxs[0].plot(scaling(train[:60].sensor_01.values.reshape(-1, 1), scale=0.50).ravel());\naxs[1].set_title(\"shift\") \naxs[1].plot(shift(train[:60].sensor_01.values.reshape(-1, 1), interval=50).ravel()); # verticle shift\naxs[2].set_title(\"verticle flip\")\naxs[2].plot(vertical_flip(train[:60].sensor_01.values.reshape(-1, 1)).ravel());\naxs[3].set_title(\"bandpass filter\")\naxs[3].plot(butter_bandpass_filter(train[:60].sensor_01.values.reshape(-1, 1), 0.05, 48, 256).ravel());","metadata":{"execution":{"iopub.status.busy":"2022-04-04T20:21:37.262855Z","iopub.execute_input":"2022-04-04T20:21:37.263134Z","iopub.status.idle":"2022-04-04T20:21:37.836032Z","shell.execute_reply.started":"2022-04-04T20:21:37.263102Z","shell.execute_reply":"2022-04-04T20:21:37.835225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = train.columns.tolist()[3:]\n\n# Feature Normalization\nsc = StandardScaler()\ntrain[features] = sc.fit_transform(train[features])\ntest[features] = sc.transform(test[features])\n\ndef transformed_data(train):\n    sequences = train.sequence.values\n    subjects = train.subject.values\n    steps = train.step.values\n    \n    train = train.drop([\"sequence\", \"subject\", \"step\"], axis=1).values\n    train = train.reshape(-1, Config.seq_length, Config.num_sensor)\n    \n    Xs = []\n    for X in tqdm(train):\n        tmp = []\n        X = X.T  # 60 x 13 => 13 x 60\n        for data in X:\n            data = data.reshape(-1, 1)\n            Xt = transform(data).ravel()\n            tmp.append(Xt)\n        Xs.append(np.array(tmp).T)\n        \n    Xs = np.array(Xs)\n    Xs = Xs.reshape(len(Xs)*Config.seq_length, Config.num_sensor)\n    \n    df = pd.DataFrame(data=Xs)\n    df.columns = features\n    df.insert(0, \"step\", steps)\n    df.insert(0, \"subject\", subjects)\n    df.insert(0, \"sequence\", sequences)\n    df = df.sort_values(by=[\"sequence\", \"subject\", \"step\"])\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-04-04T19:29:49.893284Z","iopub.execute_input":"2022-04-04T19:29:49.893897Z","iopub.status.idle":"2022-04-04T19:29:50.378351Z","shell.execute_reply.started":"2022-04-04T19:29:49.893861Z","shell.execute_reply":"2022-04-04T19:29:50.377598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_aug = transformed_data(train[:500*60])","metadata":{"execution":{"iopub.status.busy":"2022-04-04T19:30:07.303793Z","iopub.execute_input":"2022-04-04T19:30:07.304042Z","iopub.status.idle":"2022-04-04T19:30:10.461302Z","shell.execute_reply.started":"2022-04-04T19:30:07.304012Z","shell.execute_reply":"2022-04-04T19:30:10.460624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_aug.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T19:30:17.786274Z","iopub.execute_input":"2022-04-04T19:30:17.786817Z","iopub.status.idle":"2022-04-04T19:30:17.805956Z","shell.execute_reply.started":"2022-04-04T19:30:17.786778Z","shell.execute_reply":"2022-04-04T19:30:17.80512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T19:30:21.421565Z","iopub.execute_input":"2022-04-04T19:30:21.422239Z","iopub.status.idle":"2022-04-04T19:30:21.44251Z","shell.execute_reply.started":"2022-04-04T19:30:21.422202Z","shell.execute_reply":"2022-04-04T19:30:21.441861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Preparation\nfeatures = train.columns.tolist()[3:]\n\ndef process_data(df):\n    for feature in features:\n        df[feature + '_lag1'] = df.groupby('sequence')[feature].shift(1)\n        df.fillna(0, inplace=True)\n        df[feature + '_diff1'] = df[feature] - df[feature + '_lag1']    \n\n# add new features in train and test\nprocess_data(train)\nprocess_data(train_aug)\nprocess_data(test)\n\ngroups = list(train[\"sequence\"].unique())\nlabels = list(train_labels[\"state\"].values)\n\n# add augmented records\ntrain = pd.concat([train, train_aug], sort=False)\ntrain = train.drop([\"sequence\", \"subject\", \"step\"], axis=1).values\ntrain = train.reshape(-1, Config.seq_length, train.shape[-1])\n\ngroups.extend(groups[:train_aug.sequence.nunique()])\nlabels.extend(labels[:train_aug.sequence.nunique()])\n\n# new groups and lables: augmented records included\ngroups = np.array(groups)\nlabels = np.array(labels)\n\n# test data prep\ntest = test.drop([\"sequence\", \"subject\", \"step\"], axis=1).values\ntest = test.reshape(-1, Config.seq_length, test.shape[-1])","metadata":{"execution":{"iopub.status.busy":"2022-04-04T19:41:32.158012Z","iopub.execute_input":"2022-04-04T19:41:32.158295Z","iopub.status.idle":"2022-04-04T19:41:39.383911Z","shell.execute_reply.started":"2022-04-04T19:41:32.158263Z","shell.execute_reply":"2022-04-04T19:41:39.383168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def DeepResidualModel(seq_length=60, depth=13, n_class=2): \n    \"\"\"\n        Ref: https://arxiv.org/pdf/1805.00794.pdf\n    \"\"\"\n    inputs = tf.keras.layers.Input(shape=(seq_length, depth))\n    out1 = tf.keras.layers.Conv1D(filters=16, kernel_size=3, strides=1)(inputs)\n    \n    for _ in range(4):\n    \n        out = tf.keras.layers.Conv1D(filters=16, kernel_size=3, strides=1, padding='same')(out1)\n        out = tf.keras.layers.Activation(\"relu\")(out)\n        out = tf.keras.layers.Conv1D(filters=16, kernel_size=3, strides=1, padding='same')(out)\n        out = tf.keras.layers.Add()([out, out1])\n        out = tf.keras.layers.Activation(\"relu\")(out)\n        out1 = tf.keras.layers.MaxPooling1D(pool_size=3, strides=2)(out)\n        \n    out = tf.keras.layers.Flatten()(out1)\n    out = tf.keras.layers.Dense(16)(out)\n    out = tf.keras.layers.Activation(\"relu\")(out)\n    out = tf.keras.layers.Dense(16)(out)\n    out = tf.keras.layers.Dense(n_class)(out)\n    out = tf.keras.layers.Sigmoid()(out)\n    \n    return tf.keras.Model(inputs=inputs, outputs=out)\n\n\ndef DNNModel():\n    \"\"\"\n        Ref: https://www.kaggle.com/code/dmitryuarov/tps-sensors-auc-0-964\n    \"\"\"\n    input_ = Input(shape=(train.shape[-2:]))\n    out = Bidirectional(LSTM(units=512, return_sequences=True))(input_)\n    \n    z1 = Bidirectional(LSTM(units=256, return_sequences=True))(out)\n    z2 = Bidirectional(GRU(units=256, return_sequences=True))(out)\n    out = Concatenate(axis=2)([z1, z2])\n    \n    out = Bidirectional(LSTM(units=128, return_sequences=True))(out)\n    \n    out = GlobalMaxPooling1D()(out)\n    out = Dense(units=128, activation='selu')(out)\n    out = Dense(1, activation='sigmoid')(out)\n\n    return Model(inputs=input_, outputs=out, name='DNNModel')","metadata":{"execution":{"iopub.status.busy":"2022-04-04T19:31:57.312334Z","iopub.execute_input":"2022-04-04T19:31:57.312585Z","iopub.status.idle":"2022-04-04T19:31:57.325611Z","shell.execute_reply.started":"2022-04-04T19:31:57.312556Z","shell.execute_reply":"2022-04-04T19:31:57.324944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DNNModel()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-04T19:33:22.400392Z","iopub.execute_input":"2022-04-04T19:33:22.400987Z","iopub.status.idle":"2022-04-04T19:33:26.66729Z","shell.execute_reply.started":"2022-04-04T19:33:22.40095Z","shell.execute_reply":"2022-04-04T19:33:26.666564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T19:33:29.525385Z","iopub.execute_input":"2022-04-04T19:33:29.525932Z","iopub.status.idle":"2022-04-04T19:33:30.348467Z","shell.execute_reply.started":"2022-04-04T19:33:29.525894Z","shell.execute_reply":"2022-04-04T19:33:30.346896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# StratifiedGroupKFold Cross-Validation Training and OOF predictions\n\n# Example run params\nConfig.EPOCHS = 2 # run for 20 for best results\nConfig.NFOLDS = 3 # run for 10 for best results\n\npredictions, scores = [], []\nsgkfolds = StratifiedGroupKFold(n_splits = Config.NFOLDS, shuffle=True)\nfor fold, (train_idx, valid_idx) in enumerate(sgkfolds.split(train, labels, groups)):\n    print('*'*20, f'FOLD: {fold+1}', '*'*20)\n\n    X_train, X_valid = train[train_idx], train[valid_idx]\n    y_train, y_valid = labels[train_idx], labels[valid_idx]\n\n    model = DNNModel()\n    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics='AUC')\n\n    learning_rate = ReduceLROnPlateau(monitor=\"val_auc\", factor=0.5, patience=5, verbose=True)\n    early_stopping = EarlyStopping(monitor=\"val_auc\", patience=5, verbose=True, mode=\"max\", restore_best_weights=True)\n    \n    # Model fitting\n    model.fit(X_train, y_train, \n              validation_data=(X_valid, y_valid), \n              epochs=Config.EPOCHS,\n              verbose=True,\n              batch_size=Config.BATCH_SIZE, \n              callbacks=[learning_rate, early_stopping],\n             )\n\n    y_proba = model.predict(X_valid, batch_size=Config.BATCH_SIZE).squeeze()\n    score = metrics.roc_auc_score(y_valid, y_proba)  # AUC score\n    scores.append(score)\n    predictions.append(model.predict(test, batch_size=Config.BATCH_SIZE).squeeze())\n    print(f\"Fold={fold+1}: OOF Validation AUC Score: {score}\")\n    print(\"\")\n\nprint(\"=\"*60)\nprint(f'Mean AUC score on {sgkfolds.n_splits} folds = {np.mean(scores)}')\nprint(\"=\"*60)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-04T19:42:17.170335Z","iopub.execute_input":"2022-04-04T19:42:17.170717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Train for 10 stratified group k folds and 20 epochs for the best results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[\"state\"] = sum(predictions)/Config.NFOLDS\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T19:55:10.225506Z","iopub.execute_input":"2022-04-04T19:55:10.225763Z","iopub.status.idle":"2022-04-04T19:55:10.238128Z","shell.execute_reply.started":"2022-04-04T19:55:10.225733Z","shell.execute_reply":"2022-04-04T19:55:10.236895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Success!\")","metadata":{"execution":{"iopub.status.busy":"2022-04-03T16:41:49.897322Z","iopub.execute_input":"2022-04-03T16:41:49.897845Z","iopub.status.idle":"2022-04-03T16:41:49.902143Z","shell.execute_reply.started":"2022-04-03T16:41:49.897805Z","shell.execute_reply":"2022-04-03T16:41:49.901418Z"},"trusted":true},"execution_count":null,"outputs":[]}]}