{"cells":[{"cell_type":"markdown","metadata":{},"source":"I describe a way to calculate uncertainty estimates for the outcome types. The observed outcome counts are converted to observed probabilities (outcome counts / sum of outcome counts). The question is this: what is the confidence interval of the true outcome probabilities based on the observed probabilities? Intuitively it is clear that the more animals we have in a category (e.g., in a specific breed, age, or color), the narrower the confidence interval should be. This notebook provides tools to quantify this intuition.\n\nHere are the main findings:\n\n- If there are 0 animals observed in one of the outcome types and the sum of outcome counts in the category is 10, 100, 1000, we can be 95% confident that the true outcome probability is within 0 - 28%, 0 - 3.7%, and 0 - 0.38%, respectively. \n- I show on a few examples from the training set that if there are roughly 20, 200, 2000 animals in a category, the 95% confidence interval has a width of roughly 30%, 10%, 3%, respectively. \n\nThese findings are important to consider because confidence intervals relate to the classification error. If there are many categories where the sum of outcome counts are low (e.g., dog breed), it is more likely to make classification errors based on that category. \n\nIf you are not familiar with confidence intervals, check this page:\n\nhttps://en.wikipedia.org/wiki/Confidence_interval\n\nHere is what I did:\n\nThe observed outcome probabilities follow a multinomial distribution with k = 5 outcomes because their sum is 1.\n\nhttps://en.wikipedia.org/wiki/Multinomial_distribution\n\nI calculate confidence intervals based on the Wilson score slightly modified for multinomial distributions:\n\nhttps://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval\n\nNote that while there are several ways to calculate confidence intervals for bi and multinomial distributions and the different methods give different answers, the various confidence intervals do not differ much. I chose the Wilson score intervals because it has good properties if the observed probabilities are 0 or 1. This is important if the total number of animals is small.\n\nPlease also check out my other scripts:\n\nThe classifier solution of my team (Kaggle for the paws) is available here:\n\nhttps://www.kaggle.com/c/shelter-animal-outcomes/forums/t/22538/solution-of-team-kaggle-for-the-paws-no-outcome-datetime-features\n\nI performed some data exploration. I studied how the age, gender, and the breed of cats and dogs influences the outcome.\n\nhttps://www.kaggle.com/andraszsom/shelter-animal-outcomes/age-gender-and-breed-vs-outcome\n\nI group dog breeds into dog groups (e.g., herding, sporting, toy). This conversion reduces the number of categories and provides new insigths into the problem.\n\nhttps://www.kaggle.com/andraszsom/shelter-animal-outcomes/dog-breeds-dog-groups\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Calculate confidence intervals\n\nimport scipy.stats\nimport numpy as np\n    \ndef multinomial_CI(obs=[1,1],conf_level=0.95):\n    \"\"\"\n    Calculate confidence intervals for multinomial distributions using Wilson score intervals:\n    https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval\n    \n    Parameters\n    ----------\n    \n    obs : list or numpy array with length n\n        The observed occurences of each outcome.\n    \n    conf_level : float between 0 and 1\n        The desired confidence level of the confidence interval. 0.95 is a 95% confidence level meaning that \n        the true outcome probability for which the difference between the true probability and the observed \n        rate is not statistically significant at the 5% level.\n    \n    Returns\n    -------\n    \n    CI : numpy array with shape (n,2)\n        The confidence intervales for each outcome. CI[:,0] is the lower end and CI[:,1] is the higher end.\n    \n    \"\"\"\n    alpha = (1e0-conf_level)\n    z = scipy.stats.norm.ppf(1e0-alpha/2e0)\n    n_tot = np.sum(obs)\n    # observed probabilities\n    probs = 1e0*np.array(obs) / n_tot\n    \n    CI = np.zeros([len(obs),2])\n    \n    term_pm = z * np.sqrt(probs*(1e0-probs)/n_tot + z**2/(4e0*n_tot**2))\n    \n    CI[:,0] = (probs + z**2/(2e0*n_tot) - term_pm) / (1e0 + z**2/n_tot)\n    CI[:,1] = (probs + z**2/(2e0*n_tot) + term_pm) / (1e0 + z**2/n_tot)\n        \n    return CI\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Confidence intervals as a function of total counts if one outcome type is counted 0 times\n\nn_sum = [10,30,100,300,1000]\n\nfor i in range(len(n_sum)):\n    print('Total number of animals in the category:',n_sum[i])\n    print('   The corresponding confidence interval of an outcome type with 0 animals is 0 -',np.around(multinomial_CI([0,n_sum[i]])[0,1]*100,2),'%')\n     \n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import matplotlib.pyplot as plt\nimport matplotlib\nfrom matplotlib.patches import Wedge\nfrom matplotlib.collections import PatchCollection\n\n# some outcomes:\n# There are roughly 20, 200, and 2000 animals within these breeds (mixes are included!)\nvizsla = [ 15.,   0.,   0.,   5.,   0.]\nshih_tzu = [  22.,    0.,    5.,   57.,  106.]\npit_bull = [ 737.,    8.,  313.,  750.,  584.]\n\nunique_outcomes = ['Adoption', 'Died', 'Euthanasia', 'Return_to_owner', 'Transfer']\n\ndef generate_wedges(data,colors,n_wedges,CI):\n\n    angles = data / np.sum(data) * 360\n    cumsum = np.cumsum(angles)\n    cumsum = np.insert(cumsum,0,0)\n    # the main wedges based on the observed rates, no facecolor just edges\n    patches_border = []\n    for i in range(len(data)):\n        patches_border.append(Wedge((0,0),1,cumsum[i]+90,cumsum[i+1]+90,width=0.5,facecolor='none',edgecolor='k'))\n\n    # lower CIs, no alpha    \n    patches_low_CI = []\n    for i in range(len(data)):\n        wedge_center = (cumsum[i]+cumsum[i+1])/2e0+90\n        patches_low_CI.append(Wedge((0,0),1,wedge_center - CI[i,0]/2*360,wedge_center + CI[i,0]/2*360,width=0.5,facecolor=colors[i],edgecolor='none',alpha=1))\n\n\n    # add n_wedges small Wedges with succesively reduced alphas\n    patches_transition = []\n    for i in range(len(data)):\n        wedge_center = (cumsum[i]+cumsum[i+1])/2e0+90\n        step_angle_left = np.linspace(wedge_center - CI[i,0]/2*360,wedge_center - CI[i,1]/2*360,num=n_wedges+1,endpoint=True)\n        step_angle_right = np.linspace(wedge_center+CI[i,0]/2*360,wedge_center+CI[i,1]/2*360,num=n_wedges+1,endpoint=True)\n        step_alpha = np.linspace(1,0,num=n_wedges,endpoint=False)\n        for j in range(n_wedges):\n\n            patches_transition.append(Wedge((0,0),1,step_angle_left[j+1],step_angle_left[j],width=0.5,facecolor=colors[i],edgecolor='none',alpha=step_alpha[j]))\n            patches_transition.append(Wedge((0,0),1,step_angle_right[j],step_angle_right[j+1],width=0.5,facecolor=colors[i],edgecolor='none',alpha=step_alpha[j]))\n    \n    return patches_border,patches_low_CI,patches_transition\n\ncolors=['#5A8F29', 'k', '#FF8F00', '#FFF5EE', '#3C7DC4']\nn_wedges = 30\n\nfig = plt.figure(figsize=(12,5))\n\nax1 = fig.add_subplot(131, aspect='equal')\nplt.title('Vizsla, '+str(int(np.sum(vizsla)))+' animals')\n# calculate confidence intervals\nCI = multinomial_CI(vizsla)\n# generate wedges\npatches_border,patches_low_CI,patches_transition = generate_wedges(vizsla,colors,n_wedges,CI)\n# plot\nplt.xlim([-1.1,1.1])\nplt.ylim([-1.1,1.1])\nplt.axis('off')\nfor p in patches_transition:\n    ax1.add_patch(p)\nlegend = []\nfor p in patches_low_CI:\n    wedge = ax1.add_patch(p)\n    legend.append(wedge)\nfor p in patches_border:\n    ax1.add_patch(p)\n\nax2 = fig.add_subplot(132, aspect='equal')\nplt.title('Shih Tzu, '+str(int(np.sum(shih_tzu)))+' animals')\n# calculate confidence intervals\nCI = multinomial_CI(shih_tzu)\n# generate wedges\npatches_border,patches_low_CI,patches_transition = generate_wedges(shih_tzu,colors,n_wedges,CI)\n#plot\nplt.xlim([-1.1,1.1])\nplt.ylim([-1.1,1.1])\nplt.axis('off')\nfor p in patches_transition:\n    ax2.add_patch(p)\nlegend = []\nfor p in patches_low_CI:\n    wedge = ax2.add_patch(p)\n    legend.append(wedge)\nfor p in patches_border:\n    ax2.add_patch(p)\n\nax3 = fig.add_subplot(133, aspect='equal')\nplt.title('Pit Bull, '+str(int(np.sum(pit_bull)))+' animals')\n# calculate confidence intervals\nCI = multinomial_CI(pit_bull)\n# generate wedges\npatches_border,patches_low_CI,patches_transition = generate_wedges(pit_bull,colors,n_wedges,CI)\n#plot\nplt.xlim([-1.1,1.1])\nplt.ylim([-1.1,1.1])\nplt.axis('off')\n\nlegend = []\nfor p in patches_low_CI:\n    wedge = ax3.add_patch(p)\n    legend.append(wedge)\nfor p in patches_transition:\n    ax3.add_patch(p)\nfor p in patches_border:\n    ax3.add_patch(p)\n    \nax2.legend(legend,unique_outcomes,loc='center',fontsize=12,bbox_to_anchor=(0.5, 1.2),\n          ncol=3, fancybox=True, shadow=True)\n    \nplt.tight_layout(w_pad=4)\nplt.savefig('outcome_uncertainty.jpg',dpi=150)\nplt.show()\n"},{"cell_type":"markdown","metadata":{},"source":"This figure shows the observed outcome probabilities (wedges with black edges) and the confidence intervals (the areas that become gradually transparent at the borders of the wedges). The transparency gradient starts at the lower end of the confidence interval within the black egdes area, and the transition region becomes completely transparent at the upper end of the confidence interval. That is, the transparent areas of neighboring wedges overlap. The narrower the confidence intervals are (more animals in the category), the narrower the transparent regions become.\n\nThe titles of the subplots show the three dog breeds I chose and how many of them are found in the training set.\n\nThe typical confidence intervals have a width of ~30%, ~10%, ~3%, when the number of animals is ~20, ~200, ~2000, respectively. See the cell below for the exact numbers."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"CI_vizsla = np.around(multinomial_CI(vizsla),2)*100\nCI_shih_tzu = np.around(multinomial_CI(shih_tzu),2)*100\nCI_pit_bull = np.around(multinomial_CI(pit_bull),3)*100\n\nprint('Vizsla')\nprint('        Lower end of CI, observed probability, higher end of CI [%]')\nvizsla_probs = vizsla / np.sum(vizsla)*100\nfor i in range((len(unique_outcomes))):\n    print('  ',unique_outcomes[i],':',CI_vizsla[i,0],',',vizsla_probs[i],',',CI_vizsla[i,1])\n    \nprint('Shih Tzu')\nprint('        Lower end of CI, observed probability, higher end of CI [%]')\nshih_tzu_probs = np.around(shih_tzu / np.sum(shih_tzu),2)*100\nfor i in range((len(unique_outcomes))):\n    print('  ',unique_outcomes[i],':',CI_shih_tzu[i,0],',',shih_tzu_probs[i],',',CI_shih_tzu[i,1])\n    \nprint('Pit Bull')\nprint('        Lower end of CI, observed probability, higher end of CI [%]')\npit_bull_probs = np.around(pit_bull / np.sum(pit_bull),3)*100\nfor i in range((len(unique_outcomes))):\n    print('  ',unique_outcomes[i],':',CI_pit_bull[i,0],',',pit_bull_probs[i],',',CI_pit_bull[i,1])"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}