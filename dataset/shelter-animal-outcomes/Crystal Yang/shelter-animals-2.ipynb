{"cells":[{"metadata":{"_uuid":"25bea2c9909f2c5e1df3eaab8891de22fa5cd216"},"cell_type":"markdown","source":"### 1 - Shelter Animals Part 2\nThis post is a continuation of my first shelter animal [post](https://github.com/yscyang1/ExploringDataScience/blob/master/5-ShelterAnimals1.ipynb) that will include a few things, including speeding up calculations, examining trees,and tuning hyperparameters.\n\nAs usual, first import all the libraries and data.  Since in my first post, I saved the dataframes into feather format after some processing, I can easily read the processed data using panda's read_feather function.  As you can see, all the categorial data has been encoded.  \n\nNote:  This notebook was originally run on my personal computer, so %time might be a little different."},{"metadata":{"trusted":true,"_uuid":"3f8ebf70f79b5f7104b5139cb2b7860b0e75e613"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f142330a4331384b921d0407e1f5f13fdf1fed6"},"cell_type":"code","source":"train_df =pd.read_feather('../input/shelter/train_df')\ntest_df = pd.read_feather('../input/shelter/test_df')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8b2a825385c062e5ad397a056eccdba6120a7c8"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6053603a32913572ad6834a743d809550124a80b"},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"624000e5a3c38ca4bdcb772c5a3cd51cc4519848"},"cell_type":"markdown","source":"In the training dataset, I again drop the outcomes and save it as X, and 'Outcome1' becomes my y. "},{"metadata":{"trusted":true,"_uuid":"ad437fd9bf842b0e2502b86f3e59b31fb188417f"},"cell_type":"code","source":"X = train_df.drop(['Outcome1', 'Outcome2'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3e52fc2b260ec812d56afebd2bfe3d3276caef7"},"cell_type":"code","source":"y = train_df['Outcome1']","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"191d477e75f329d2d4993009732f51b695724efd"},"cell_type":"markdown","source":"### 2 - Creating a Validation Set\nPerhaps in my first post, I commited a machine learning faux pas and didn't create a validation set.  I only submitted my solution once in the first post, so a validation set wasn't critical.  However, if I wanted to try and speed up my model or play with hyperparameters for example, then creating a validation set is important just so that I don't overfit for Kaggle's public leaderboard.  \n\nWhen creating a validation set, it is important to note if dates are important.  For example, if your goal is to try and predict future price, you wouldn't want create your validation set by picking out random data points.  Instead, you'd want the first 60-80% of the training set to stay as your training set, and take the remaining datapoints as your validation set.  \n\nFor this shelter animal outcome, the training and test sets come from the same time frame, and I'm not trying to forecast something in the future, so taking random data points for a validation set is reasonable.  To create my validation set, I'll be using scikit-learn.  I've chosen to make the validation set 43% of the training set because that's about the same size as test_df.  \n\n"},{"metadata":{"trusted":true,"_uuid":"4f1b338a89c9149a89a02e006341e4744c1f2857"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83a6c1d8edbbd1929900379b58b82167635de336"},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.43, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"291f2b4f81bbdfafc92b38f3624df29f59bb5799"},"cell_type":"code","source":"print('training shape: {}'.format(X_train.shape))\nprint('validation shape: {}'.format(X_val.shape))\nprint('test shape: {}'.format(test_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd4576ed8f6154e1323fe21402a2aeff37b4e336"},"cell_type":"markdown","source":"#### 2.1 - Random Forest with Validation Set\nNow to try the random forest model again with separate training and validation sets. "},{"metadata":{"trusted":true,"_uuid":"2a0aec13617ede6a7fed37b28770fdb918698134"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f6e4f95718942f652762a670c693d386fd8d41e"},"cell_type":"code","source":"rf1 = RandomForestClassifier(n_estimators=100, n_jobs= -1)\n%time rf1.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0872f61a3d0a8979e1e7e5067c7b9e3daca2bdb9"},"cell_type":"code","source":"def print_score(model, X_t, y_t, X_v, y_v, oob = False):\n    print('Training Score: {}'.format(model.score(X_t, y_t)))\n    print('Validation Score: {}'.format(model.score(X_v, y_v)))\n    if oob:\n        if hasattr(model, 'oob_score_'):\n            print(\"OOB Score:{}\".format(model.oob_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2051243ceb120a71fc51dfce5a95407e95524b5e"},"cell_type":"code","source":"print_score(rf1, X_train, y_train, X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dce5f729baec674d53b43b1597bb57468ee708fc"},"cell_type":"markdown","source":"I've written a function that prints out the score of the training and validation sets.  This score is the accuracy, or the number of correct predictions over the number of total predictions.  With the minimal processing done, the validation set got a score of 67.7%, which is far lower than the training score.  Clearly, there is quite a bit of overfitting.  "},{"metadata":{"_uuid":"dcd4f1f9392117dee80597c86e843038b59d7c80"},"cell_type":"markdown","source":"### 3 - Speeding Things Up\nWhy would we want to decrease computational time?  One reason would be if you have a large dataset and want to fiddle with hyperparameters.  If each computation took a minute or more, that's a lot of time wasted!  Instead, you could take a subset of the data, tune the hyperparameters, and then apply the hyperparameters to the whole dataset when you are ready.  \n\nAbove, I've used the %time function to see how long it took to fit the data (~1 seconds). \n\nThe function get_subset (code borrowed from this [stackoverflow](https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test) thread) takes in the training set, training and validation percent, and outputs a randomized subset of the training set.  I've included a way to get randomized training, validation, and test sets as well.  \n\nNote:  If you don't need a test set, then scikit-learn's train_test_split function will work as well, and it has a random seed function as well.  "},{"metadata":{"trusted":true,"_uuid":"940bc61f5ba096d43f9114ea2cefc5e4248d797f"},"cell_type":"code","source":"def get_subset(df, train_percent=.6, validate_percent=.2, copy = True, seed=None):\n    if copy:\n        df_copy = df.copy()\n    perm = np.random.RandomState(seed).permutation(df_copy.index)\n    length = len(df_copy.index)\n    train_end = int(train_percent * length)\n    validate_end = int(validate_percent * length) + train_end\n    train = df_copy.iloc[perm[:train_end]]\n    validate = df_copy.iloc[perm[train_end:validate_end]]\n    test = df_copy.iloc[perm[validate_end:]]\n    \n    return train, validate, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afa8a55c7f0931d9a0d8b3daa76ca6d20726d764"},"cell_type":"code","source":"train_speed, val_speed, test_speed = get_subset(train_df, 0.35, 0.35, seed = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34cfa80459371e3a00ca776982ba44ae5cc6b6d3"},"cell_type":"code","source":"train_speed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ddb7e66a2184b3991e055d54b9bbd88c5835e51"},"cell_type":"code","source":"train_speed.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a080e4c6a5731609c1f58ba5b1bec2b01a1610b2"},"cell_type":"code","source":"rf_speed = RandomForestClassifier(n_estimators=100, n_jobs=-1)\nX_train_speed = train_speed.drop(['Outcome1', 'Outcome2'], axis = 1)\ny_train_speed = train_speed['Outcome1']\nX_val_speed = val_speed.drop(['Outcome1', 'Outcome2'], axis = 1)\ny_val_speed = val_speed['Outcome1']\n%time rf_speed.fit(X_train_speed, y_train_speed)\nprint_score(rf_speed, X_train_speed, y_train_speed, X_val_speed, y_val_speed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12802832f8cafb1bd4907ddd60f872e16180d97e"},"cell_type":"markdown","source":"After testing the model on a subset of the data, the computational time decreased by 1 second, and both the training and validation accuracy decreased by a tiny bit.  "},{"metadata":{"_uuid":"b8ea3d729a27d527180457191687dd6a7c7bac6d"},"cell_type":"markdown","source":"### 4 - Examining Trees\nSimilar to a real forest, a random forest classifier is made of trees.  One way to really understand what is going on behind the scenes is to study a tree.  Keep in mind that each tree is unique.  \n\n"},{"metadata":{"_uuid":"a42b395d6ecfae90a475a52da236aa903bb25855"},"cell_type":"markdown","source":"#### 4.1 - Creating a Tree\nThe first step is to create a model.  I've skipped over it before, but in RandomForestClassifier and RandomForestRegressor, an estimator is a tree.  By setting n_estimators as one, I am creating only one tree in this model.  Max depth defines how many nodes there are in each tree.  When examining a tree, its good to set a max depth, otherwise the tree will be near impossible to interpret, especially if there are a lot of features.  \n\nIn addition, random forests introduces randomization by something called bootstrapping.  More a more in depth explanation can be found [here](https://nititek.wordpress.com/2013/12/10/bootstrapping/), but basically bootstrapping takes a random subset of the data, and samples the subset numerous times with replacement.  It sounds fancy, but a simple example would be to have a bag of 3 red, 3 blue, and 3 green marbles.  Sample the subset by drawing the marbles 9 times, but each time, put the marble back in the bag.  The act of replacing the marble drawin is the replacement part, as opposed to removing the drawn marble from the subset.\n\nLastly, n_jobs describes how many jobs to run in parallel.  If I'm not mistaken, this is machine dependent and depends on if you're using CPU vs GPU.  If you want to make things simple, the int -1 will tell your computer to use all processors.  "},{"metadata":{"trusted":true,"_uuid":"1ad52875b5d4137f7adc8e85f4b7f00ca67ba2b5"},"cell_type":"code","source":"rf_1tree = RandomForestClassifier(n_estimators=1, max_depth=3, bootstrap=False, random_state=23,  n_jobs=-1)\nrf_1tree.fit(X_train_speed, y_train_speed)\nprint_score(rf_1tree, X_train_speed, y_train_speed, X_val_speed, y_val_speed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bb4fff2cc1b6937190b0a7e37c9bb622e45616b"},"cell_type":"markdown","source":"You can see the accuracy of the training and validation sets dropped to about 50%.  Definately not a model to submit to Kaggle or your boss, but small and simple enough to examine deeply.  "},{"metadata":{"_uuid":"69f6c55556b65b6ee24ec176ed3fa13708686c64"},"cell_type":"markdown","source":"The second step is to extract and export the tree.  Extracting a single tree uses the .estimators_ function.  Since I built a model with only one tree, there is no picking and choosing of trees.\n\nViewing the tree involves importing the export_graphviz function and exporting the tree as a .dot file.  ProTip: rotating the image made the tree much easier to read."},{"metadata":{"trusted":true,"_uuid":"558e6e182058f599c554ce269962a60230fa8ec4"},"cell_type":"code","source":"from sklearn.tree import export_graphviz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e594cadf2a3b88bf2ec3c0370eb5c5c4cf718b9f"},"cell_type":"code","source":"estimator = rf_1tree.estimators_[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b59a4de993741ec3d8b0d9c0cc74c34d7d133aa"},"cell_type":"code","source":"export_graphviz(estimator, out_file = 'tree.dot', \n                feature_names = X_train_speed.columns, \n                class_names = rf_1tree.classes_,\n                rounded = True,\n                filled = True,\n                precision = 2,\n                rotate = True,\n                node_ids = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba38a7a40735e0f2c41b7b1238cd2e851d318b6b"},"cell_type":"markdown","source":"To actually view the tree, the .dot file should be changed to a png file.  I saw two ways to do this, but the one I understood better was with the pydot library."},{"metadata":{"trusted":true,"_uuid":"92a9613b89300d2cfebebc47dce781d36ce6a429"},"cell_type":"code","source":"import pydot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e193147fc5d704cc9533e65713ae4e59f082f560"},"cell_type":"code","source":"(graph,) = pydot.graph_from_dot_file('tree.dot')\ngraph.write_png('tree.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a0d571d608467e2192b19eb650c8fbd830782b1"},"cell_type":"markdown","source":"Lastly, view the image with IPython.display."},{"metadata":{"trusted":true,"_uuid":"48795ff9005bb77eb68c2adb875c862ac63905d3"},"cell_type":"code","source":"from IPython.display import Image\nImage(filename = 'tree.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2583e7a84f48e35a8e596a5235f2af432a3b75a"},"cell_type":"markdown","source":"Okay, so what's going on here?  There is a lot of information to unpack, but luckily, trees make it easy to interpret data.  \n\nFirst, what is a node?  A node is each of these boxes, and you can see each node is split into 2, depending on if it is True or False.  The conditional it is splitting on is written under the node number.  So at node 0, it is being split on if the value of 'Sex' is <= 2.0.  If this conditional is False, we end up at node 8, where the conditional again, is to split at 'Age' is <= 547.5.  \n\nSamples is self-explanatory, the number of samples in that node.  The sum of the samples in each layer of the tree should add up to the sample size in the root node 0.  \n\nThe values correspond to the outcomes(adoption, died, euthanasia, return to owner, and transfer, in this order), and can be viewed as the distribution of the sample.  Class identifies the most common outcome.  For leaf nodes (the nodes furthest to the right and has no conditional), the class is the prediction for all samples in that node.  \n\nSaving the best for last, is something called Gini.  Calculating the Gini Impurity sounds complicated, but it isn't as hard as it sounds.  [This website](https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76) goes in depth how to calculate it, and I've also calculated it for the [first node](https://github.com/yscyang1/ExploringDataScience/blob/master/6-ShelterAnimals2_SupportingInfo.ipynb).  The Gini Impurity ranges from 0 to 1, and is the probability that a randomly chosen sample in a node will be incorrectly labeled according to the distribution of values in that node.  The higher the Gini, the more likely the sample will be labeled incorrectly.  \n\nSo how does the tree pick what feature and where to split at?  This is where the Gini comes in.  Take the root node for example. The tree goes through the value of each feature and splits the node to find the greatest reduction in the Gini Impurity.  If you peek at the calculations page, then you will see that this is calculated with a weighted average.  Thus, as the tree move towards the leaves, the Gini should decrease. "},{"metadata":{"_uuid":"cb0fec49eda21a19ef946d224912080a573c0e10"},"cell_type":"markdown","source":"### 5 - Bagging\n\n"},{"metadata":{"_uuid":"ab346b5be7f583c08ad8439fc508229d90a7bba9"},"cell_type":"markdown","source":"#### 5.1 - Intro to Bagging\nSo in the last section, I created one shallow tree (max depth of 3) that had pretty terrible accuracy on both training and validation sets.  What happens if I create a deep tree?  How would that affect accuracy?  "},{"metadata":{"trusted":true,"_uuid":"50c387ea2de30ce641161534d6e98fda6544a4ce"},"cell_type":"code","source":"train_speed, val_speed, test_speed = get_subset(train_df, 0.5, 0.35)\nX_train_speed = train_speed.drop(['Outcome1', 'Outcome2'], axis = 1)\ny_train_speed = train_speed['Outcome1']\nX_val_speed = val_speed.drop(['Outcome1', 'Outcome2'], axis = 1)\ny_val_speed = val_speed['Outcome1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1dbac910b7a4c6afc3872440fc79df0fc2880f2"},"cell_type":"code","source":"rf_deeptree = RandomForestClassifier(n_estimators=1, n_jobs=-1)\nrf_deeptree.fit(X_train_speed, y_train_speed)\nprint_score(rf_deeptree, X_train_speed, y_train_speed, X_val_speed, y_val_speed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"303c199e62e2d365d333784f1ccdb9650e97e089"},"cell_type":"markdown","source":"As you can see, the training score improved drastically, but the validation score is still pretty terrible.  Is there a way to profit from this overtraining?  The answer is yes, using a technique called bagging.  The thought behind this technique is to combine a bunch of different bad models to create one better model.  In terms of trees and random forests, you could create a lot of different deep trees that overfit a LOT, like the one above.  But beacuse each tree has a different subset of the population, they have different errors (random errors).  If you take the average of random errors, you get 0.  \n\nLets see if this works.  Lets try it with 10 trees."},{"metadata":{"trusted":true,"_uuid":"4c3475e11283a969b117101fc79182c0373bab34"},"cell_type":"code","source":"rf_deeptree = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n%time rf_deeptree.fit(X_train_speed, y_train_speed)\nprint_score(rf_deeptree, X_train_speed, y_train_speed, X_val_speed, y_val_speed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f4a9658144a3ee9e01e20fdb99e8a3f27cc957c"},"cell_type":"markdown","source":"Indeed, the validation set's accuracy increased by about 10%.  \n"},{"metadata":{"_uuid":"0c7277d5ea933ede842d309745e65bf817d3af4b"},"cell_type":"markdown","source":"#### 5.2 - Extra Tree Classifier\n\nAn easy way to try to improve on the tree model and bagging is to use trees that are not correlated with each other, as opposed to more accurate trees.  Scikit-learn has a model called ExtraTreeClassifier (or regressor) that randomly splits for randomly selected features and chooses the best split.  Additional pros to the extra tree model is that it has less computing time, so you can build more trees in that time saved.  "},{"metadata":{"trusted":true,"_uuid":"124d6898bcf4ec1812259e6fd548bc657065b220"},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bbfab7e6edb4bd2c184555a6e0d1fc5dd87ad41"},"cell_type":"code","source":"etc_deeptree =ExtraTreesClassifier(n_estimators=100, n_jobs=-1)\n%time etc_deeptree.fit(X_train_speed, y_train_speed)\nprint_score(etc_deeptree, X_train_speed, y_train_speed, X_val_speed, y_val_speed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"595121249af24faa0810814f6546a95c3bd7f198"},"cell_type":"markdown","source":"Well, the computation time decreased by ~50 ms, but the validation score didn't improve.  In fact, it actually got a little bit worse. Perhaps in this case, the trees developed by the random forest classifier is already generally uncorrelated to each other, so introducing more randomness from the extra trees classifier didn't improve the model?  "},{"metadata":{"_uuid":"fac9144d8c2879d67cee95fdc1875a72e925c8e6"},"cell_type":"markdown","source":"#### 5.3 - Picking the Number of Trees\nIf you've been paying attention, the number of trees I've used for each model varied wildly, ranging from 1 to 100.  Yet, for the models that used 10 and 100 trees, the accuracy of the validation set was ~65%.  Clearly, there is a plateau for number of trees vs accuracy.  How do we figure out what it is?  \n\nFirst, I need a list of predictions for each tree, called preds.  The shape tells us there are 100 lists, one for each tree, and 9355 predictions for each row of the data set.  I also printed out the predictions, where you can see each outcome is a number instead of a category.  "},{"metadata":{"trusted":true,"_uuid":"683971d22479c8664b30eb2ef3d4c929eae768b2"},"cell_type":"code","source":"preds = np.stack([i.predict(X_val_speed) for i in rf_deeptree.estimators_])\nprint(preds.shape)\nprint(preds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"110648e6fd4a7d65d513f9d1e3ce1c59ad8b920c"},"cell_type":"markdown","source":"Since the predictions are numbers, I had to write a function to encode the outcomes of my y_val data.  I feel like scikit-learn should have something to already do this, but I couldn't find it.  Scikit-learn's label encoding function probably would have done the trick, but I don't know for sure if it labels based on alphabetical order."},{"metadata":{"trusted":true,"_uuid":"bfc19a8e4e6c0a269bfa3a2a57307cac4da5bd4b"},"cell_type":"code","source":"def convert_outcome1(col):\n    if col == 'Adoption':\n        return 0\n    if col == 'Died':\n        return 1\n    if col == 'Euthanasia':\n        return 2\n    if col == 'Return_to_owner':\n        return 3\n    if col == 'Transfer':\n        return 4\n\ny_val_speed_convert = y_val_speed.apply(convert_outcome1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05a586f51f70f974895ca668074ac465d6583997"},"cell_type":"code","source":"import scipy.stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07052025eb4ad4a028b434a8a63f32253061cba5"},"cell_type":"code","source":"from sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94e99327ac2715c348f8b622ab1d42b13148047e"},"cell_type":"markdown","source":"And finally, the graph.  What I'm doing is graphing the accuracy based on number of trees (as found from estimators_).  The predictions from each tree is averaged using the mode.  Note: if you are using a regression model instead, average using the mean.  \n\nWhat you see is that the accuracy starts plateauing at around 50 or 60 trees.  "},{"metadata":{"trusted":true,"_uuid":"1f409f0e7f3e571da3bd491a0451ba6876328350"},"cell_type":"code","source":"plt.plot([metrics.accuracy_score(y_val_speed_convert, np.round(scipy.stats.mode(preds[0:i+1],axis = 0)[0][0])) for i in range(100)])\nplt.xlabel('Number of Trees')\nplt.ylabel('Accuracy')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f709ca81a4d8577d72169d00d1a92e5d5111045"},"cell_type":"markdown","source":"Just to confirm, I've printed out the accuracy for 10, 20, 50, and 100 trees."},{"metadata":{"trusted":true,"_uuid":"63105077985d239a8ba065112e1407e00752b823"},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=10, n_jobs=-1)\nmodel.fit(X_train_speed, y_train_speed)\nprint_score(model, X_train_speed, y_train_speed, X_val_speed, y_val_speed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9908b863d3cd81bea3ba601df6cac4c97763cdd0"},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=20, n_jobs=-1)\nmodel.fit(X_train_speed, y_train_speed)\nprint_score(model, X_train_speed, y_train_speed, X_val_speed, y_val_speed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e04da100d545a06a934d4c16a6b3bf90e1b01859"},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=50, n_jobs=-1)\nmodel.fit(X_train_speed, y_train_speed)\nprint_score(model, X_train_speed, y_train_speed, X_val_speed, y_val_speed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23d5ebc023530233feb33299f71ad018d6c485ab"},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=100, n_jobs=-1)\nmodel.fit(X_train_speed, y_train_speed)\nprint_score(model, X_train_speed, y_train_speed, X_val_speed, y_val_speed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d294061a4d11d6919806b837f14fc4cee58c507f"},"cell_type":"markdown","source":"#### 5.4 - Out of Bag Score\nSomething unique to random forests is something called an out of bag score.  Since bagging only uses about 2/3 of the data, you could technically use the other 1/3 of the data as a validation set. Just like before, these \"validation\" trees can be averaged to find a prediction, and we can calculate a separate accuracy score from these validation trees.  \n\nThe advantage of an out of bag score (OOB) is to determine if there is overfitting without the need to create a whole new validation set.  Here, we see that with 50 trees, the validation and OOB score are similar, which is an indication there isn't overfitting."},{"metadata":{"trusted":true,"_uuid":"2c2edfff7961f70ce86d9b7adc7545f89d90265a"},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=50, oob_score= True, n_jobs=-1)\nmodel.fit(X_train_speed, y_train_speed)\nprint_score(model, X_train_speed, y_train_speed, X_val_speed, y_val_speed, oob=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ea377dcba50e4ecae7b86d050d6d2ae05d0a6d3"},"cell_type":"markdown","source":"### 6 - Reducing Overfitting\nThe general consensus on how to avoid overfitting in random forests (according to Jeremy and a few stack overflow threads at least) is to 1) adjust the number of estimators, and 2) make the trees less deep.  I've already gone through finding a good number of estimators, so next is to explore some popular techniques to\"prune\" a tree, or make it less deep.\n\nTo start off, lets get a model that utilizes the full data set.  Validation and OOB scores are similar."},{"metadata":{"trusted":true,"_uuid":"32b6881da92ddb7aac6da6297fca318ffb496df8"},"cell_type":"code","source":"rf_all = RandomForestClassifier(n_estimators=50, n_jobs=-1, oob_score=True, random_state=42)\nrf_all.fit(X_train, y_train)\nprint_score(rf_all, X_train, y_train, X_val, y_val, oob=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"620321ed7ddca403f2ccccbac8e1953114d2af04"},"cell_type":"markdown","source":"#### 6.1 - min_sample_leaf\nInstead of letting the branches keep splitting until it has a single sample, assigning the tree to keep splitting until a node has the specified number of samples.  According to the documentation, it can induce a \"smoothing effect, especially in regression.\".  "},{"metadata":{"trusted":true,"_uuid":"89b4e32f49a4194e5db2297c5ddc5e2c77957483"},"cell_type":"code","source":"rf_minleaf = RandomForestClassifier(n_estimators=50, min_samples_leaf=2, n_jobs=-1, oob_score=True, random_state=42)\nrf_minleaf.fit(X_train, y_train)\nprint_score(rf_minleaf, X_train, y_train, X_val, y_val, oob=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06995d6bf4f37e7638ffea84edaf98c3a1e5ccb6"},"cell_type":"markdown","source":"With a min sample size of 2, the validation score increases a tiny bit from 0.668 to 0.672."},{"metadata":{"_uuid":"c892b037b5f5ed1c349b6a1db13cf206987a3187"},"cell_type":"markdown","source":"#### 6.2 - max_features\nAnother hyperparamter to play around with is the max_features parameter.  Instead of splitting at every feature (i.e columns) and finding the best split, the tree chooses a certain percentage of features.  There are also special paramters such as square roots or logs of features.  Popular numbers to try range between 0.3-0.5, sqrt, and log2.  "},{"metadata":{"trusted":true,"_uuid":"ef785b934c442d911665cb014e2d93fcf60b7c1c"},"cell_type":"code","source":"rf_maxfeat = RandomForestClassifier(n_estimators=50, min_samples_leaf=2, max_features=0.3, n_jobs=-1, oob_score=True, random_state=42)\nrf_maxfeat.fit(X_train, y_train)\nprint_score(rf_maxfeat, X_train, y_train, X_val, y_val, oob=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b406bcf6d40edc5363f5c012dceb0799223b4b62"},"cell_type":"code","source":"rf_maxfeat = RandomForestClassifier(n_estimators=50, min_samples_leaf=2, max_features='sqrt', n_jobs=-1, oob_score=True, random_state=42)\nrf_maxfeat.fit(X_train, y_train)\nprint_score(rf_maxfeat, X_train, y_train, X_val, y_val, oob=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55c245b61ab78cf3047dce856eec7e820c7aaa40"},"cell_type":"code","source":"rf_maxfeat = RandomForestClassifier(n_estimators=50, min_samples_leaf=2, max_features='log2', n_jobs=-1, oob_score=True, random_state=42)\nrf_maxfeat.fit(X_train, y_train)\nprint_score(rf_maxfeat, X_train, y_train, X_val, y_val, oob=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57dbecaead013cabb201ead89afe5e731a4455c9"},"cell_type":"markdown","source":"Keeping the min samples to 2, I can eek out a little bit more accuracy when we randomly sample 30% of the features.   "},{"metadata":{"_uuid":"c3c792d72c1bbb21da175e69d3ed12ddd03d7495"},"cell_type":"markdown","source":"### 7 - Hyperparameter Tuning\nThe number of hyperparameters to tune and optimize is getting a little bit much by hand.  Luckily, there is something called  RandomizedSearchCV and GridSearchCV to help us out. "},{"metadata":{"_uuid":"ac3b7b06e59d40e3980107deb95e815b5d1b3b23"},"cell_type":"markdown","source":"#### 7.1 - RandomizedSearchCV\nIf there are a lot of hyperparameters to test out, generally using scikit-learn's RandomizedSearchCV is a good place to start.  We create a grid of hyperparameters that includes a range of values for each hyperparameter to test out.  The RandomizedSearchCV then tests out some of the parameters specified.  This gives us an idea of what values to use and can narrow it down using GridSearchCV.\n\nFirst, define the values of all the hyperparameters to test."},{"metadata":{"trusted":true,"_uuid":"32ee15a5e019a22451557af69f649c557ba99bae"},"cell_type":"code","source":"n_estimators = [int(x) for x in range(1,100,5)]\nmax_features = [float(x) for x in np.linspace(0.1,1,9)]\nmax_features.append('log2')\nmax_features.append('sqrt')\nmin_samples_split = [2,5,8,10,20,25]\nmin_samples_leaf = [2,5,8,10,20,25]\nbootstrap = [True, False]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1dc4be8c841383add81f44f20d6dad329ea847df"},"cell_type":"markdown","source":"Next, combine the hyperparameters into a dictionary that RandomizedSearchCV will take."},{"metadata":{"trusted":true,"_uuid":"dcbc12b43484b43bd125728559d476006c89dce2"},"cell_type":"code","source":"randomCV_grid = {'n_estimators': n_estimators, 'max_features': max_features, 'min_samples_split': min_samples_split, \n               'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9eedff9d13827a925a5028657cb9dbc302f85e4d"},"cell_type":"markdown","source":"Implementing RandomizedSearchCV is pretty similar to what we've seen before.  Create a RandomForestClassifier, create a RandomizedSearchCV, and fit the X and y training sets to RandomizedSearchCV.  The most important parameters to pay attention to are n_iter and cv.  The variable n_iter is how many combinations to try. Obviously, the more combinations, the more time it takes.  The variable cv is how many cross validation folds to do.  The higher the number, the less overfitting there is, but again, will take more time.  \n\nOne thing I noticed is that when I specified n_jobs = -1, I had an error message pop up, '[WinError 5] Access is denied:' to be specific.  Setting n_jobs to default seemed to fix it though."},{"metadata":{"trusted":true,"_uuid":"bdeea7f8c0b167bea9a919d734c2b60359d027f7"},"cell_type":"code","source":"rf_random = RandomForestClassifier(n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1534fb84be04f5f849e06532432f5d92e3a5a296"},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90a2869d4963cf40c0aff7099f08246f52161eb8"},"cell_type":"code","source":"rf_randomGrid = RandomizedSearchCV(rf_random, param_distributions=randomCV_grid, n_iter = 100, cv = 3, random_state=42,  verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11a5c5e23417d33b51b4d11ae6f56928ec144b23"},"cell_type":"code","source":"rf_randomGrid.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f8ec6b126524376ab20a7129a3af2f219188012"},"cell_type":"markdown","source":"To view which hyperparameters tested had the best result, use the following function.  "},{"metadata":{"trusted":true,"_uuid":"709d551f9eaa027abbe9adf1e41d1130b85a3fe0"},"cell_type":"code","source":"rf_randomGrid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7bdcab0d6567571513767041c7a4b85df1123cc"},"cell_type":"markdown","source":"We see that the validation set's accuracy is 67.17%, which did better than the base model's score of 66.79%, but worse than a previous model, which got 67.30%."},{"metadata":{"trusted":true,"_uuid":"b8630e5b4b12f091275bf6c54ac0fdc0d082c746"},"cell_type":"code","source":"best_randomCV_tree = rf_randomGrid.best_estimator_\nprint_score(best_randomCV_tree, X_train, y_train, X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"346b01ad6b58fbb0abfe3a9f6263d5cf02ecb484"},"cell_type":"markdown","source":"#### 7.2 - GridSearchCV\nFrom the results of RandomizedSearchCV, I am able to focus on a smaller range for each hyperparameter.  The implementation of GridSearchCV is similar to the randomized version.\n"},{"metadata":{"trusted":true,"_uuid":"1f6587d4f0bae2c81041d6c077ca69883346e86c"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31e23bcfaff42d83923379d0b7f771478b44369d"},"cell_type":"code","source":"CV_grid = {'n_estimators': [60, 65, 70, 75], \n           'max_features': [0.2, 0.3, 0.4, 0.5], \n           'min_samples_split': [16, 18, 20, 22], \n           'min_samples_leaf': [6, 7, 8, 9], \n           'bootstrap': [False]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63755bb2c11322466e3ad8ac847e6ed81ecb1fba"},"cell_type":"code","source":"rf_grid = RandomForestClassifier(n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb1b21ec9cc48cac6b216c205a015210e698489f"},"cell_type":"code","source":"rf_randomGrid = GridSearchCV(rf_grid, param_grid=CV_grid, cv = 3, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ead7b92a4f6d61e014ddb2234773cde63e1b312d"},"cell_type":"code","source":"rf_randomGrid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a2f88de41ed5724db9f82d9ac55f1beb6b86130"},"cell_type":"code","source":"rf_randomGrid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"745484debbd525ab3467869891b1bdb818ec6cdf"},"cell_type":"code","source":"print_score(rf_randomGrid, X_train, y_train, X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f2d16a334dc090fb637b1935c3842641a68075e"},"cell_type":"markdown","source":"These results are a slight decrease in accuracy from the randomized CV, so I'm pretty sure I've hit the limits of random forest hyperparater tuning for this data set without further data processing."},{"metadata":{"_uuid":"0ee9a36a3de110cd9970e292c162c5e8351013d6"},"cell_type":"markdown","source":"### 8 - Submitting to Kaggle\nUsing the model that got the highest accuracy on the validation set (from section 6.2), I got a score of 0.84781, placing me at 809 out of 1604.  This is actually worse than my first submission.  However, when I submitted using the optimized hyperparameters, I got a score of 0.77676, placing me at 548.  Thank you Randomized and GridSearchCV.  "},{"metadata":{"trusted":true,"_uuid":"7cb61555ab12edb2abd1cb7a42b2a022aa678234"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}