{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Achintya Gupta**\n\n*5th Mar 2020*\n\nThe noteboook structure :- \n            1. Introduction\n            2. Imports\n            3. Path and Initialisaitons\n            4. Reading the Data\n            5. Helper Function Class. \n               Functions definitions  :\n                4.1 calcLogLossError\n                4.2 createProbabHeatmap\n            6. Data Processing Class. \n               Functions definitions  :\n                5.1 preprocess_dataset\n                5.2 feature_engineering\n                5.3 process_dataset\n                5.4 impute\n                5.5 train_cv_split\n            7. Preparing The Data\n            8. Exploratory Data Analysis\n            9. Modelling\n                9.1 Base Modelling\n                9.2 Feature Combination Selection\n                9.3 Hyper Parameter Tuning\n                9.4 Model Interpretation\n"},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nCan you predict wether an animal, going out of shelter home would had been **Adopted** by someone,or probably **Returned_to_owner** or maybe **Transferred** someplace else or to someone, or if sadly it might have **Died** or **Euthanised** while in the shelter.\n\n<img src=\"https://images.unsplash.com/photo-1450778869180-41d0601e046e?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=2650&q=80\" width=\"600px\">\n\n\n***The Objective***, as stated above is simple : \"Predict the **Outcome_Type** of an Animal going out of shelter home\".\n\n****\n\n- You are given a dataset of following features :- \n\n    - **Name** : Name of the animal, if it had one.\n    - **DateTime** : Timestamp at which it was brought in the shelter home\n    - **OutcomeType** : Outcome of the animal, 5 classes to ***predict***.\n    - **OutcomeSubtype** : Much granular detail of the type of the outcome for that specific animal. \n    - **AnimalType** : Type of Animal, Dog or Cat.\n    - **SexuponOutcome** : Sex of the animal, when they leave the shelter home. (Don’t know why its mentioned ‘SexuponOutcome’ and not just 'Gender'? It kinda implies there might be a difference between 'SexuponOutcome' and 'SexuponIntake' that they troubled themselves to name it this way.)\n    - **AgeuponOutcome** : Age of the animal when they came out of the shelter home.\n    - **Breed** : Breed of the animal .\n    - **Color** : Color of the animal"},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"# Imports"},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T12:21:26.998108Z","start_time":"2020-03-05T12:21:26.994372Z"},"hidden":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# General\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport random, re, itertools\nfrom IPython.display import Math\n\n#Visualisations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Algo\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"# Path and Initialisations"},{"metadata":{"ExecuteTime":{"end_time":"2020-03-04T16:04:58.292094Z","start_time":"2020-03-04T16:04:58.289087Z"},"hidden":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"TRAIN_PATH = '/kaggle/input/shelter-animal-outcomes/train.csv.gz'\nTEST_PATH = '/kaggle/input/shelter-animal-outcomes/test.csv.gz'\nSAMPLESUBMISSION_PATH = '/kaggle/input/shelter-animal-outcomes/sample_submission.csv.gz'\n\n\nTARGET_VARIABLE = 'OutcomeType'","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"# Read Data"},{"metadata":{"ExecuteTime":{"end_time":"2020-03-04T16:04:59.372146Z","start_time":"2020-03-04T16:04:59.227511Z"},"hidden":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_data = pd.read_csv(TRAIN_PATH, index_col=0)\ntest_data = pd.read_csv(TEST_PATH, index_col=0)\nsamplesubmission_data = pd.read_csv(SAMPLESUBMISSION_PATH, index_col=0)\n\nprint('Shape of Train Data : ', train_data.shape)\nprint('Shape of Test Data : ', test_data.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper Function Class\n\nFirst lets create a a Helper Fucntion class, which would have just two, but functions frequently used in this entire notebook :- \n\n1. ***calcLogLossError*** : To calculate the log loss of the all the predicted probabilities, the predicted probabilites are first scaled to \n\n    *max(min(x,1-1e-15),1e-15)* to prevent the logarithmic transformations from blowing up.\n\n$logloss = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{M}y_{ij}log(p_{ij})$\n\nwhere $N$ is total number of animals, and $M$ is total number of classes to be predicted and $p_{ij}$ is the predicted probability of a particular animal ID belonging to one of the predict classes.\n\n\n2. ***createProbabHeatmap*** : Creates a Confusion Matrix + Scatter Plot with the mean of the predicted probabilities. Inspired from [here](https://andraszsom.wordpress.com/2016/07/27/kaggle-for-the-paws/)\n\n"},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T08:55:06.877577Z","start_time":"2020-03-05T08:55:06.840842Z"},"code_folding":[4,10,19,80],"hidden":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class HelperFunctions:\n    def __init__(self):\n        print('Initialising the HelperFunctions class..')\n        \n    def calcLogLossError(self, metrics):\n        \n        #Scaling\n        metrics.iloc[:,:5] = metrics.iloc[:,:5].apply(lambda x: x/x.sum(),axis=1)\n        metrics.iloc[:,:5] = metrics.iloc[:,:5].applymap(lambda x: max(min(x,1-1e-15),1e-15))\n        metrics.iloc[:,:5] = np.log(metrics.iloc[:,:5])\n        loglossScore = (-(metrics.iloc[:,:5]*pd.get_dummies(metrics['Actuals'])).sum().sum())/metrics.shape[0]\n        return loglossScore\n               \n    def createProbabHeatmap(self, cv_metrics):\n        \n        cv_metrics.sort_values('Actuals', inplace=True)\n        plt.rcParams['figure.dpi'] = 180\n        plt.rcParams['figure.figsize'] = (18,18)\n        fig, axes =plt.subplots(5,5, sharex=False, sharey=True)\n        fig.text(0.5, -0.05, 'Index', ha='center', fontsize=25)\n        fig.text(-0.05, 0.5, 'Predicted Probabilities', va='center', rotation='vertical', fontsize=25)\n        plt.tight_layout()\n\n        for idx, eachClass in enumerate(cv_metrics.Actuals.unique()):\n\n            probMatrix = cv_metrics[cv_metrics.Actuals == eachClass].reset_index().drop('AnimalID', axis=1)\n            temp2 = {k:0 for k in cv_metrics.Actuals.unique()}\n            predictionCounts = probMatrix.Predictions.value_counts().to_dict()\n            for k in predictionCounts.keys():\n                temp2[k] = predictionCounts[k]\n            predictionCounts = temp2\n\n            for idx1, eachPlotAX in enumerate(axes[idx]):\n\n                sns.scatterplot(x = probMatrix.iloc[:,idx1].index, \n                                y = probMatrix.iloc[:,idx1].values, \n                                ax = eachPlotAX,\n                                hue = probMatrix.iloc[:,idx1].values,\n                                s = 20,\n                                legend=False,\n                                palette = 'plasma')\n                eachPlotAX.hlines(np.mean(probMatrix.iloc[:,idx1].values),\n                                  0, probMatrix.iloc[:,idx1].index[-1]*1.1, color='maroon', linewidth=3)\n                \n                if idx == idx1 : \n                    eachPlotAX.set_facecolor('darkgray')\n                    eachPlotAX.text(0.5,0.5, \"{0}/{1}\".format(predictionCounts[probMatrix.iloc[:,idx1].name],\n                                                           probMatrix.shape[0]), size=25,\n                                ha=\"center\", color='black', transform=eachPlotAX.transAxes)\n                else:\n                    eachPlotAX.set_facecolor('lightblue')\n                    eachPlotAX.text(0.5,0.5, \"{0}/{1}\".format(predictionCounts[probMatrix.iloc[:,idx1].name],\n                                                           probMatrix.shape[0]), size=15,\n                                ha=\"center\", color='black', transform=eachPlotAX.transAxes)\n                    \n                eachPlotAX.grid()\n                eachPlotAX.linewidth =10\n\n                if idx1 == 0 : eachPlotAX.set_ylabel(eachClass)\n                if idx == len(axes)-1 : eachPlotAX.set_xlabel(cv_metrics.columns[:-1][idx1])\n\n        plt.close()\n        return fig\n\nhelper_functions_handler = HelperFunctions()","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"# Data Processing Class\n\nNow, Lets get to **THE** buisness! Processing The data.\n\n\n\n1. ***preprocess_dataset*** : Preprocessing the dataset, \n    - 1.1) The column, ***'OutcomeSubtype'*** was dropped from the training dataset.\n    - 1.2) The column, ***'DateTime'*** is converted to datetime pandas object. And since we are dealing with a dataset which contains a datetime element, it might be good idea to sort the dataset on this column, to make the later data correspond to the latest data with respect to time, hence in a way replicating the data model might get in the real world.\n    - 1.3) The column, ***'Name'*** having the name of the pet, is only useful to the extent of *\"did the pet have a name??\"*. Assuming NaN in this column means that the pet did not have a name, this column was converted to \"HAS_NAME\" and \"NO_NAME\".\n    - 1.4) The column, ***'AgeuponOutcome'*** is the age of the pet, and the data is represented in [years, months and days] so that was converted to *'AgeuponOutcomeDays'*.\n    \n2. ***feature_engineering*** : Engineering New Features,\n    - 1.1) ***Calendar Variables*** : From the *DateTime* column, **Month**, **Day**, **WeekOfYear**, **DayOfWeek** and **DayType** which represents the hour of the day in which the pet was delivered to the Shelter Home.\n    These engineered variables try to answer the following questionsm respectively:\n    - 1.2) ***AnimalType_Breed*** : Combining two Categorical Columns, Which type of animal of what breed? Does that combination affect our target variable?\n    - 1.3) ***AnimalType_Color*** : Combining two Categorical Columns, Which type of animal of what color? Does that combination affect our target variable?\n    - 1.4) ***AnimalType_Breed_Color*** : Combining three Categorical Columns, Which type of animal of what color of what breed? Does that combination affect our target variable?\n    - 1.5) ***AnimalType_SexuponOutcome*** : Combining two Categorical Columns, Which type of animal of what sex/gender? Does that combination affect our target variable?\n    - 1.5) ***AnimalType_Name*** : Combining two Categorical Columns, Which type of animal and did it have a name? Does that combination affect our target variable?\n    - 1.5) ***AnimalType_DayType*** : Combining two Categorical Columns, Which type of animal brought in at what hour of the day? Does that combination affect our target variable?\n    \n    - 1.5) ***AgeGroup*** : Bucketing the age of the pet into 100 buckets, the idea or the *hope* is that the age group might have a relationship with the target variable\n    \n3. ***process_dataset*** : A function which accepts the engineered data and the respective transformatin mapping, one of these : \n    - **NORM_MAX** : Normalising a numerical column by the maximum of that column.\n    - **NORM_MAXMIN** : Normalising a numerical column by the minimum of that column.\n    - **NORM_SUM** : Normalising a numerical column by the sum of that column.\n    - **NORM_CUSTOM** : Normalising a numerical column by a custom value.\n    - **ORIGNAL** : Let the column be as it is\n    - **OHE** : One Hot Encoding.\n    - **OHE_DROP** : One Hot Encoding, and dropping the first level because that might be redundant.\n    - **LE_O** : Label Encoding of a categorical column, Ordinal in nature having ordered categories.\n    - **LE_N** : Label Encoding of a categorical column, Nominal in nature having no order within the categories.\n    - **LE_O_NORM** : Label Encoding of a categorical column, Ordinal in nature having ordered categories. And then normalising with the max(For Neural Nets).\n    - **LE_N_NORM** : Label Encoding of a categorical column, Nominal in nature having no order within the categories.And then normalising with the max(For Neural Nets).\n    - **DROP** : Dropping the column.\n4. ***impute*** : For now just filling the missing values of with -1, in the hope that the missing values might have a relationship with the target variable\n5. ***train_cv_split*** : Since the dataset has a time component, the train and cross validation is split by 80-20 ratio.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T11:12:43.892636Z","start_time":"2020-03-05T11:12:43.853166Z"},"code_folding":[1,4,75,160,169],"hidden":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class DataProcessing:\n    def __init__(self):\n        print('Initialising Data Processinig Class..')\n\n    def preprocess_dataset(self, df, mode='train', sort_bydate=True):\n        \n        # Drop OutcomeSubtype\n        if mode == 'train':\n            df.drop(['OutcomeSubtype'], axis=1, inplace=True)\n\n        # Generate Calendar Variables\n        \n        df['DateTime'] = pd.to_datetime(df['DateTime'])\n        \n        if sort_bydate:\n            df.sort_values('DateTime', inplace=True)\n        \n        _nonnan_idx = df.loc[~df.Name.isna()].index\n        df.loc[_nonnan_idx, 'Name'] = ['HAS_NAME']*df.Name[~df.Name.isna()].shape[0]\n        df['Name'].fillna('NO_NAME', inplace=True)\n\n\n        # Transform AgeuponOutcome TO AgeuponOutcomeDays\n        def transformAgeuponOutcome(x):\n            x = str(x)\n            if x == 'nan':\n                return np.nan\n            elif 'year' in x:\n                return int(x.split(' ')[0])*365\n            elif 'month' in x:\n                return int(x.split(' ')[0])*30\n            elif 'day' in x:\n                return int(x.split(' ')[0])*1\n\n        df['AgeuponOutcome'] = df.AgeuponOutcome.apply(lambda x : transformAgeuponOutcome(x))\n        df.rename(columns={'AgeuponOutcome':'AgeuponOutcomeDays'},inplace=True)\n\n        return df\n    \n    def feature_engineering(self, df):\n        \n        # Calendar Variables\n        def generateHourOfTheDay(x):\n            if x < 12:\n                return 'Morning'\n            elif x>=12 and x < 17:\n                return 'Noon'\n            elif x>=17 and x < 20:\n                return 'Eevening'\n            elif x >= 20:\n                return 'Night'\n            \n        # Simplify Breed..\n        df.Breed = df.Breed.apply(lambda x : re.sub(' Mix', '', x))\n        df.Breed = df.Breed.apply(lambda x : x.split('/')[0])\n        df.Color = df.Color.apply(lambda x : x.split('/')[0])\n        \n        df['Month'] = df['DateTime'].dt.month\n        df['Day'] = df['DateTime'].dt.day\n        df['WeekOfYear'] = df['DateTime'].dt.weekofyear\n        df['DayOfWeek'] = df['DateTime'].dt.dayofweek\n        df['DayType'] = pd.to_datetime(df['DateTime']).dt.hour.apply(lambda x : generateHourOfTheDay(x))\n        df.drop(['DateTime'], axis=1, inplace=True)\n        \n        df['AnimalType_Breed'] = df.apply(lambda x : x.AnimalType+'_'+x.Breed, axis=1)\n        df['AnimalType_Color'] = df.apply(lambda x : x.AnimalType+'_'+x.Color, axis=1)\n        df['AnimalType_Breed_Color'] = df.apply(lambda x : x.AnimalType_Breed+'_'+x.Color, axis=1)\n        df['AnimalType_SexuponOutcome'] = df.apply(lambda x : np.nan if pd.isna(x.SexuponOutcome) else x.AnimalType+'_'+x.SexuponOutcome, \n                                                   axis=1)\n        df['AnimalType_Name'] = df.apply(lambda x : x.AnimalType+'_'+x.Name, axis=1)\n        df['AnimalType_DayType'] = df.apply(lambda x : x.AnimalType+'_'+x.DayType, axis=1)\n        \n        df['AgeGroup'] = pd.cut(df.AgeuponOutcomeDays.fillna(0), bins=50)\n\n        return df\n\n    def process_dataset(self, df, transformDict, mode='train', verbose=True):\n        \"\"\"Available Types :\n          'NORM_MAX','NORM_MIN', 'NORM_SUM','NORM_CUSTOM', 'ORIGNAL', 'OHE',\n          'OHE_DROP', 'LE_O', 'LE_N', 'LE_O_NORM', 'LE_N_NORM', 'DROP'\"\"\"\n        \n        catMappings = {}\n        print('Transforming Varables..')\n        \n        for eachKey, eachValue in transformDict.items():\n            \n            \n            if eachValue['Type'] == 'NORM_MAX':\n                df[eachKey] = df[eachKey]/df[eachKey].max()\n\n            elif eachValue['Type'] == 'NORM_MAXMIN':\n                df[eachKey] = df[eachKey]/(df[eachKey].max()-df[eachKey].min())\n\n            elif eachValue['Type'] == 'NORM_SUM':\n                df[eachKey] = df[eachKey]/df[eachKey].sum()\n            \n            elif eachValue['Type'] == 'NORM_CUSTOM':\n                df[eachKey] = df[eachKey]/ast.literal_eval(ast.literal_eval(eachRow['Params']))\n\n            elif eachValue['Type'] == 'ORIGNAL':\n                df[eachKey] = df[eachKey]\n\n            elif eachValue['Type'] == 'OHE':\n                _dummies = pd.get_dummies(df[[eachKey]].copy(),\n                           prefix=[eachKey], \n                           prefix_sep = '_',\n                           columns = [eachKey], \n                           drop_first=True)\n\n                df[_dummies.columns] = _dummies\n\n            elif eachValue['Type'] == 'OHE_DROP':\n                _dummies = pd.get_dummies(df[[eachKey]].copy(),\n                           prefix=[eachKey], \n                           prefix_sep = '_',\n                           columns = [eachKey], \n                           drop_first=True)\n\n                df[_dummies.columns] = _dummies\n                df.drop(eachKey, axis=1, inplace=True)\n\n            elif eachValue['Type'] == 'LE_O':\n                _param = eachValue['Params']\n                _CM = pd.Categorical(df[eachKey], categories=_param)\n                df[eachKey] = _CM.codes\n                catMappings[eachKey] = _CM\n            \n            elif eachValue['Type'] == 'LE_N':\n                _CM = pd.Categorical(df[eachKey])\n                df[eachKey] = _CM.codes\n                catMappings[eachKey] = _CM\n\n            elif eachValue['Type'] == 'LE_O_NORM':\n                _param = eachValue['Params']\n                _CM = pd.Categorical(df[eachKey], categories=_param)\n                df[eachKey] = _CM.codes\n                df[eachKey] = df[eachKey]/df[eachKey].max()\n                catMappings[eachKey] = _CM\n            \n            elif eachValue['Type'] == 'LE_N_NORM':\n                _CM = pd.Categorical(df[eachKey])\n                df[eachKey] = _CM.codes\n                df[eachKey] = df[eachKey]/df[eachKey].max()\n                catMappings[eachIdx] = _CM\n\n            elif eachValue['Type'] == 'DROP':\n                df.drop(eachKey, axis=1, inplace=True)\n\n        if mode == 'train':\n            _CM = pd.Categorical(df[TARGET_VARIABLE])\n            df[TARGET_VARIABLE] = _CM.codes\n            catMappings[TARGET_VARIABLE] = _CM\n            self.trainCatMappings = catMappings\n            \n        elif mode == 'test':\n            self.testCatMappings = catMappings\n\n        if verbose : print('Shape of the {0} dataset After Processing : {1}\\n'.format(mode, df.shape))\n\n        return df    \n\n    def impute(self, df):\n        \n        df.fillna(-1, inplace=True)\n        \n        return df\n        \n    def train_cv_split(self, df, ratio=0.2):\n        \n        upto = int(df.shape[0]*ratio)\n        trainDF = df[:-upto]\n        cvDF = df[-upto:]\n        \n        return trainDF, cvDF\n    \ndataProcessingHandler = DataProcessing()","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"# Processing the data\n\nProcessing the data for Train and Test set.\n\n**Pre Processsing** > **Feature Engineering** > **Process Data** > **Impute** \n\nBut first, we need to prepare the transformation dictionary for all the columns and that include the feature engineered columns as well. Which will be used for processing the dataset. It might be tedious at first,  but when the feature size is massive and we need to try various version of transformation this comes quite handy."},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T11:13:04.045191Z","start_time":"2020-03-05T11:13:04.037892Z"},"hidden":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"transformDict = {'Name': {'Type': 'OHE_DROP',\n                          'Params': None},\n                 'AnimalType': {'Type': 'LE_N',\n                                'Params': None},\n                 'SexuponOutcome': {'Type': 'OHE_DROP',\n                                    'Params': None},\n                 'AgeuponOutcomeDays': {'Type': 'ORIGNAL',\n                                        'Params': None},\n                 'Breed': {'Type': 'LE_N',\n                           'Params': None},\n                 'Color': {'Type': 'LE_N',\n                           'Params': None},\n                 'Month': {'Type': 'ORIGNAL',\n                           'Params': None},\n                 'Day': {'Type': 'ORIGNAL',\n                         'Params': None},\n                 'WeekOfYear': {'Type': 'ORIGNAL',\n                         'Params': None},\n                 'DayOfWeek': {'Type': 'ORIGNAL',\n                         'Params': None},\n                 'DayType': {'Type': 'LE_O',\n                             'Params': ['Morning',  'Noon', 'Eevening', 'Night']},\n                 'AnimalType_Breed': {'Type': 'LE_N',\n                                      'Params': None},\n                 'AnimalType_Color': {'Type': 'LE_N',\n                                      'Params': None},\n                 'AnimalType_Breed_Color': {'Type': 'LE_N',\n                                             'Params': None},\n                 'AnimalType_SexuponOutcome': {'Type': 'LE_N',\n                                               'Params': None},\n                 'AnimalType_Name': {'Type': 'LE_N',\n                                     'Params': None},\n                 'AnimalType_DayType': {'Type': 'LE_N',\n                                        'Params': None},\n                 'AgeGroup': {'Type': 'LE_N',\n                                        'Params': None}}","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T11:13:11.177315Z","start_time":"2020-03-05T11:13:05.115097Z"},"hidden":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Train Data\n_mode = \"train\"\nCOMPLETE_TRAINING_DATA = dataProcessingHandler.preprocess_dataset(train_data.copy(), \n                                                                   mode=_mode, \n                                                                   sort_bydate= True)\nCOMPLETE_TRAINING_DATA = dataProcessingHandler.feature_engineering(COMPLETE_TRAINING_DATA)\nCOMPLETE_TRAINING_DATA = dataProcessingHandler.process_dataset(COMPLETE_TRAINING_DATA,\n                                                                transformDict=transformDict, mode=_mode)\nCOMPLETE_TRAINING_DATA = dataProcessingHandler.impute(COMPLETE_TRAINING_DATA)\n\n\n# Test Data\n_mode = \"test\"\nTESTING_DATA = dataProcessingHandler.preprocess_dataset(test_data.copy(), \n                                                         mode=_mode)\nTESTING_DATA = dataProcessingHandler.feature_engineering(TESTING_DATA)\nTESTING_DATA = dataProcessingHandler.process_dataset(TESTING_DATA, \n                                                      transformDict=transformDict, \n                                                      mode=_mode)\nTESTING_DATA = dataProcessingHandler.impute(TESTING_DATA)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T10:45:31.746074Z","start_time":"2020-03-05T10:45:31.739094Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"TRAINING_DATA, CV_DATA = dataProcessingHandler.train_cv_split(COMPLETE_TRAINING_DATA.copy())\n\nprint('Shape of the Training Set : ', TRAINING_DATA.shape)\nprint('Shape of the Cross Validation Set : ', CV_DATA.shape)\nprint('Shape of the Testing Set : ', TESTING_DATA.shape)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T10:45:32.326773Z","start_time":"2020-03-05T10:45:32.318319Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"TRAINING_DATA_X = TRAINING_DATA.drop(TARGET_VARIABLE, axis=1).copy()\nTRAINING_DATA_y = TRAINING_DATA[TARGET_VARIABLE]\n\nCV_DATA_X = CV_DATA.drop(TARGET_VARIABLE, axis=1).copy()\nCV_DATA_y = CV_DATA[TARGET_VARIABLE]\n\nTESTING_DATA_X = TESTING_DATA.copy()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T10:45:33.209569Z","start_time":"2020-03-05T10:45:33.206342Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"targetCatMapping = {idx:k for idx, k in enumerate(dataProcessingHandler.trainCatMappings[TARGET_VARIABLE].categories)}\n","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"# EDA\n\nJust exploring..."},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T11:32:48.080486Z","start_time":"2020-03-05T11:32:44.216748Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"trainDataEDA = train_data.copy()\ntrainDataEDA = dataProcessingHandler.preprocess_dataset(trainDataEDA, \n                                                        mode='train', \n                                                        sort_bydate= True)\ntrainDataEDA = dataProcessingHandler.feature_engineering(trainDataEDA)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T11:32:54.738138Z","start_time":"2020-03-05T11:32:54.148525Z"},"hidden":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Analysing features w.r.t target\nanalysis_Cols = {'AnimalType': None,\n                 'SexuponOutcome': None,\n                 'AgeGroup': None,\n                 'DayOfWeek': None,\n                 'AnimalType_DayType': None,\n                 'AnimalType_Name': None}\n\nfor eachCol in [k for k in analysis_Cols.keys()]:\n    \n    fig, axes = plt.subplots(1,2, figsize =(25,9))\n    \n    temp = trainDataEDA.groupby([TARGET_VARIABLE, eachCol]).size().unstack().fillna(0)\n    \n    \n    t1 = temp.T/temp.sum(axis=1)\n    t1.T.plot(kind='bar', stacked=True, ax = axes[0])\n    t2 = temp/temp.sum(axis=0)\n    t2.T.plot(kind='bar', stacked=True, ax = axes[1])\n    \n    \n    axes[0].legend(loc='center left', bbox_to_anchor=(-0.25, 0.5))\n    axes[1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n    \n    plt.suptitle('Analysing {0} v/s {1}'.format(eachCol, TARGET_VARIABLE), fontsize=20)\n    analysis_Cols[eachCol] = fig\n    plt.close()\n    ","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T11:33:40.688507Z","start_time":"2020-03-05T11:33:39.936166Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"analysis_Cols['AnimalType']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analysing the target with respect to the Target column, Its evident that \n\n- More percentage of dogs are Adopted than cats and the relationship for Transfer is opposite\n- Majority percentage of Return to owner is for Dogs.\n- Cat has a more probability of dying than the dogs"},{"metadata":{"trusted":true},"cell_type":"code","source":"analysis_Cols['SexuponOutcome']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For 'SexuponOutcome', there is not much to be derived from here, but :\n\n- Interestingly, **Adoption** is clearly having a much biggere percentage in the cases of  **Neutered Male** and **Spayed Female**, so a One Hot Encoding of this feature might not be the bad idea. As that would make it easier for the model to capture the effect.\n- Seldom do **Unknown** category of intake goes back to the owmner.\n- **Adoption** of **Intact Female** is rather low."},{"metadata":{"trusted":true},"cell_type":"code","source":"analysis_Cols['AgeGroup']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For Feature Engineered 'AgeGroup', :\n\n- A very explicit and increasing pattern emerges for the case of **Transfer** as we move towards the older age groups. And so does ***Euthenasia*\n- In a similar fashion, the rate of **Adoption** and **Transfer** of the of a per is clearly decreasing as we move up the age group.\n\nI am starting to have a feeling that **Adoption** and **Transfer** classes might not be that seprabale."},{"metadata":{"trusted":true},"cell_type":"code","source":"analysis_Cols['DayOfWeek']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much to derive from here.."},{"metadata":{"trusted":true},"cell_type":"code","source":"analysis_Cols['AnimalType_DayType']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, wierd a little bit...\n\n- Of all the target classes **Adoption** is a proportionately higher than the other classes for either **Cats** or **Dogs** when the hour is late, either in evening or night."},{"metadata":{"trusted":true},"cell_type":"code","source":"analysis_Cols['AnimalType_Name']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, my two cents...\n- **Cats** and **Dogs** who have name have a higher chance of **Adoption** and those who dont have a higher chance of **Transfer**\n- **Dogs** who had a name were mostly returned to their owners."},{"metadata":{},"cell_type":"markdown","source":"# Modelling\n\n5th March - Lets take Random Forest for now.. A universal machine learning algorithm, which does a decent job of predicting in either regression or classification, philosiphacally. It builds a lots of [Decision Trees](https://en.wikipedia.org/wiki/Decision_tree) on random subsets of features and generalises the relationships in the datasets decently.."},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"## Base Modelling"},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T10:45:41.581948Z","start_time":"2020-03-05T10:45:36.518171Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"rfClf = RandomForestClassifier(oob_score=True, n_estimators=500, max_depth = 6, min_samples_leaf = 2)\n_=rfClf.fit(TRAINING_DATA_X, TRAINING_DATA_y)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T10:45:44.021906Z","start_time":"2020-03-05T10:45:41.78852Z"},"hidden":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ypred = rfClf.predict(CV_DATA_X)\nypredProba = rfClf.predict_proba(CV_DATA_X)\n\nCV_METRICS = pd.DataFrame(ypredProba, index=CV_DATA_X.index)\nCV_METRICS.columns = CV_METRICS.columns.map(targetCatMapping)\nCV_METRICS.applymap(lambda x: max(min(x,1-1e-15),1e-15))\nCV_METRICS['Actuals'] = CV_DATA_y.map(targetCatMapping)\nCV_METRICS['Predictions'] = pd.Series(ypred,\n                                      index= CV_DATA_X.index).map(targetCatMapping)\n\n\nloglossscore = helper_functions_handler.calcLogLossError(CV_METRICS.copy())\n\nfiDF = pd.DataFrame(dict(zip(TRAINING_DATA_X.columns, rfClf.feature_importances_)).items(),columns=['Features', 'Feature_Importance'])\nfiDF.set_index('Features', inplace=True)\nfiDF.sort_values('Feature_Importance',ascending=False, inplace=True)\nplt.rcParams['figure.figsize'] = (25,7)\nfiDF.plot(kind='bar')\nplt.hlines(fiDF.mean().values[0]*0.8,-20,200,color='r')\n_=plt.title('OOB Score : {0}, LogLossError : {1}'.format(rfClf.oob_score_, loglossscore), fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T10:45:48.381405Z","start_time":"2020-03-05T10:45:44.307458Z"},"hidden":true,"scrolled":false,"trusted":true},"cell_type":"code","source":"helper_functions_handler.createProbabHeatmap(CV_METRICS.copy())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahaaa! As suspected, the **Adoption** and **Transfer** categories are not that separable within the dataset, even so much that the tree classifies other classes as them as well. And it is doing perfectly miserable job in classifying **Deaths**.."},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"## Feature Combination Selection\n\nThis code block below comes in rather handy, when we are dealing with humungous feature size(which is not this case), but i will use it anyway to figure out. Which combination of the feature set might be the best to work with.\n\nFor a layman to read this function, this is how it will go :- \n\n***“Within the feature space of, lets say 100 features, randomly pick *n {preditorInjection}* number of features - *m {combinations_no}* number of times and see which works better. But dont let same combination of feature through to the model, which had been tried earlier.”***\n\n*NOTE : I am running this block from only 6 to 10 as the kaggle kernel keeps crashing with only 20 combinations.. but you can increase if you want to!*"},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T10:46:00.318889Z","start_time":"2020-03-05T10:46:00.3154Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"combinations_no = 20\nbestFeatures = []\nforced_columns = []\nmodel = RandomForestClassifier(oob_score=True, n_estimators=50, max_depth = 6, min_samples_leaf = 2)\n\nalgoModel = 'RandomForestClassifier'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, lets initiate our Feature Combination Selection process.."},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T12:21:06.256504Z","start_time":"2020-03-05T12:21:06.251139Z"},"hidden":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"loglossscore = 99999\ntriedCombinations = []\nallFeatures = [k for k in TRAINING_DATA_X.columns]\n\nfor preditorInjection in range(6,10):\n\n    pbar = tqdm(range(combinations_no))\n    \n    for eachR_Pick in pbar:\n        \n        availableCols = [k for k in allFeatures if k not in forced_columns+[TARGET_VARIABLE]]\n        randomsubset_cols = []\n        \n        if availableCols != []:\n            randomsubset_cols = random.sample(availableCols, preditorInjection)\n            randomsubset_cols.sort()\n        \n        if tuple(randomsubset_cols) not in triedCombinations:\n            triedCombinations += [tuple(randomsubset_cols)]\n            \n            selectedCols = forced_columns+randomsubset_cols+[TARGET_VARIABLE]\n\n            _modellingData = COMPLETE_TRAINING_DATA[selectedCols]\n            _TRAIN_DATA, _CV_DATA = dataProcessingHandler.train_cv_split(_modellingData, ratio=0.2)\n            \n            _modellingData_XTrain = _TRAIN_DATA.drop(TARGET_VARIABLE, axis=1).copy()\n            _modellingData_yTrain = _TRAIN_DATA[TARGET_VARIABLE].copy()\n\n            _modellingData_XCV = _CV_DATA.drop(TARGET_VARIABLE, axis=1).copy()\n            _modellingData_YCV = _CV_DATA[TARGET_VARIABLE].copy()\n\n            _modellingData_XTest = TESTING_DATA_X.copy()\n\n            model.fit(_modellingData_XTrain, _modellingData_yTrain)\n            _yPred = model.predict(_modellingData_XCV)\n            _yPredProba = model.predict_proba(_modellingData_XCV)\n            \n            \n            CV_METRICS = pd.DataFrame(_yPredProba, index=CV_DATA_X.index)\n            CV_METRICS.columns = CV_METRICS.columns.map(targetCatMapping)\n            CV_METRICS.applymap(lambda x: max(min(x,1-1e-15),1e-15))\n            CV_METRICS['Actuals'] = CV_DATA_y.map(targetCatMapping)\n            CV_METRICS['Predictions'] = pd.Series(_yPred, index= CV_DATA_X.index).map(targetCatMapping)\n\n            _loglossscore = helper_functions_handler.calcLogLossError(CV_METRICS)\n            pbar.set_description('Injection Level : {0} @ {1}'.format(preditorInjection, round(loglossscore,4)))\n\n            if _loglossscore < loglossscore:\n                loglossscore = _loglossscore\n                \n                bestFeatures.append( (loglossscore, _modellingData_XTrain.columns.tolist()))\n\n        else:\n            pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyper Parameter Tuning\n\nOnce done selecting the features, \n\n- Choose a grid of parameters that you want tune for a particular model.\n- Create a pandas Dataframe out of it.\n- Shuffle the dataframe. And run!"},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T12:51:19.434074Z","start_time":"2020-03-05T12:51:19.43058Z"},"trusted":true},"cell_type":"code","source":"_features = bestFeatures[-1][1]\n_features = ['AgeGroup',\n             'AgeuponOutcomeDays',\n             'AnimalType_Color',\n             'AnimalType_Name',\n             'AnimalType',\n             'AnimalType_Breed',\n             'DayType',\n             'SexuponOutcome_Unknown',\n             'SexuponOutcome_Neutered Male',\n             'SexuponOutcome_Spayed Female']","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T14:04:37.640018Z","start_time":"2020-03-05T14:04:37.621212Z"},"trusted":true},"cell_type":"code","source":"# Change the parameters according to the model being used...\n_max_depth = np.arange(5, 8, 1).astype(int)\n_min_samples_leaf = np.arange(4, 6, 1).astype(int)\n_n_estimators = np.arange(500, 1000, 50).astype(int)\n\n\nprint('All Hyper Param Combinations : ',len(_max_depth)*len(_min_samples_leaf)*len(_n_estimators))\n\n_hyperp = [_max_depth, _min_samples_leaf, _n_estimators]\nhyperCombinations = pd.DataFrame(list(itertools.product(*_hyperp)))\nhyperCombinations.columns = ['max_depth', 'min_samples_leaf', 'n_estimators']\n\nhyperCombinations.max_depth = hyperCombinations.max_depth.astype(int)\nhyperCombinations.n_estimators = hyperCombinations.n_estimators.astype(int)\nhyperCombinations = hyperCombinations.sample(frac=1)\nhyperCombinations.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-03-05T14:05:03.857Z"},"trusted":true},"cell_type":"code","source":"loglossscore = 99999\nhyperidx = 0\nbestHParams= []\n\npbar = tqdm(hyperCombinations.iterrows())\nfor idx, eachHyper in pbar:\n    \n    model = RandomForestClassifier(oob_score=True, max_features='log2', n_jobs=-1)\n    hparams = eachHyper.to_dict()\n    hparams['max_depth'] = int(hparams['max_depth'])\n    hparams['n_estimators'] = int(hparams['n_estimators'])\n    model.set_params(**hparams)\n    \n    _=model.fit(TRAINING_DATA_X[_features], TRAINING_DATA_y)\n    ypred = model.predict(CV_DATA_X[_features])\n    ypredProba = model.predict_proba(CV_DATA_X[_features])\n    CV_METRICS = pd.DataFrame(ypredProba, index=CV_DATA_X.index)\n    CV_METRICS.columns = CV_METRICS.columns.map(targetCatMapping)\n    CV_METRICS.applymap(lambda x: max(min(x,1-1e-15),1e-15))\n    CV_METRICS['Actuals'] = CV_DATA_y.map(targetCatMapping)\n    CV_METRICS['Predictions'] = pd.Series(ypred,\n                                          index= CV_DATA_X.index).map(targetCatMapping)\n    _loglossscore = helper_functions_handler.calcLogLossError(CV_METRICS.copy())\n    pbar.set_description('HpIdx : {0} @ {1}'.format(hyperidx, round(loglossscore,4)))\n    if _loglossscore < loglossscore:\n        loglossscore = _loglossscore\n        bestHParams.append([loglossscore, hparams])\n        hyperidx = idx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final Model\n\nOn the Chosen model > Selected Features > Selected Hyper Parameters, make a model which would be the final one."},{"metadata":{"trusted":true},"cell_type":"code","source":"bestHParams","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T12:55:54.596847Z","start_time":"2020-03-05T12:55:54.59298Z"},"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(oob_score=True, max_features='log2', n_jobs=-1)\nmodel.set_params(**bestHParams[-1][1])\n\n_=model.fit(TRAINING_DATA_X[_features], TRAINING_DATA_y)\nypred = model.predict(CV_DATA_X[_features])\nypredProba = model.predict_proba(CV_DATA_X[_features])\nCV_METRICS = pd.DataFrame(ypredProba, index=CV_DATA_X.index)\nCV_METRICS.columns = CV_METRICS.columns.map(targetCatMapping)\nCV_METRICS.applymap(lambda x: max(min(x,1-1e-15),1e-15))\nCV_METRICS['Actuals'] = CV_DATA_y.map(targetCatMapping)\nCV_METRICS['Predictions'] = pd.Series(ypred, index= CV_DATA_X.index).map(targetCatMapping)\n_loglossscore = helper_functions_handler.calcLogLossError(CV_METRICS.copy())\nprint(_loglossscore)\nhelper_functions_handler.createProbabHeatmap(CV_METRICS.copy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(oob_score=True, max_features='log2', n_jobs=-1)\nmodel.set_params(**bestHParams[-1][1])\n\n_=model.fit(COMPLETE_TRAINING_DATA[_features], COMPLETE_TRAINING_DATA[TARGET_VARIABLE])\nypredProba = model.predict_proba(TESTING_DATA_X[_features])\n\nsubmissionDF = pd.DataFrame(ypredProba)\nsubmissionDF.columns = submissionDF.columns.map(targetCatMapping)\nsubmissionDF.index = TESTING_DATA_X.index\nsubmissionDF.index.name = TESTING_DATA_X.index.name\nsubmissionDF.to_csv('submissionDF.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Interpretability\n\nDisect open your model to understand and make a layman understand what this model makes out of a particular feature. Or how the model is working..\n\n<img src=\"https://c4.wallpaperflare.com/wallpaper/485/436/987/warhammer-40k-techpriest-vitruvian-man-wallpaper-preview.jpg\" width=\"400px\">\n\n\nFollowing :-\n\n1.) Partial Dependence Plots, more about it [here](https://medium.com/@ag.ds.bubble/model-interpretability-a4244d82ffb2?source=friends_link&sk=38aa71fb0d8faf36878ff274a8707ad7)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pdpbox import pdp\n\nXTrain = COMPLETE_TRAINING_DATA[_features].copy()\n\npdp_StoreType = pdp.pdp_isolate(model=model, \n                                dataset=XTrain.sample(100), \n                                num_grid_points = 100,\n                                model_features=XTrain.columns.tolist(), \n                                feature='AgeGroup')\n\nfig, axes = pdp.pdp_plot(pdp_StoreType, 'AgeGroup', plot_lines=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pdp_StoreType = pdp.pdp_isolate(model=model, \n                                dataset=XTrain.sample(100), \n                                num_grid_points = 100,\n                                model_features=XTrain.columns.tolist(), \n                                feature='AgeuponOutcomeDays')\n\nfig, axes = pdp.pdp_plot(pdp_StoreType, 'AgeuponOutcomeDays', plot_lines=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Testing Space"},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T14:04:21.828931Z","start_time":"2020-03-05T14:04:21.818548Z"},"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-03-05T11:31:07.226796Z","start_time":"2020-03-05T11:31:07.2133Z"},"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}