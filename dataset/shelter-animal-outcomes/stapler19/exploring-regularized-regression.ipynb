{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom sklearn.cross_validation import train_test_split\n#import polynomial_regression as plr\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nmatplotlib.rcParams.update({'font.size': 12})\n\ndtype_dict = {'AnimalID':str,'Name':str,'DateTime':str,'OutcomeType':str,'OutcomeSubtype':str,'AnimalType':str,'SexuponOutcome':str,'AgeuponOutcome':str,'Breed':str,'Color':str}\n\n# Load Testing and Training\ntesting = pd.read_csv('../input/test.csv', sep=',')\ntraining = pd.read_csv('../input/train.csv', sep=',')\n\n## Split into training and validation sets \ndf2 = training.ix[1:]\ntrain, validation = train_test_split(df2, test_size=0.50) \n#I think that Shuffling is built in to this\n# See: http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html\n\n\n############## Functions ####################\n\ndef get_regression_predictions(input_feature, intercept, slope):\n    # calculate the predicted values:\n    predicted_values = intercept + input_feature*slope\n    return predicted_values\n\n\ndef polynomial_dataframe(data_frame, feature, animal, degree): # feature is pandas.Series type\n    # assume that degree >= 1\n    # initialize the dataframe:\n    poly_dataframe = pd.DataFrame()\n    # and set poly_dataframe['power_1'] equal to the passed feature\n    feature_values = np.array(data_frame.loc[data_frame['AnimalType'] == animal,feature])\n    if(feature == 'AgeuponOutcome'):\n       ages_in_years = age_to_years(feature_values)\n       poly_dataframe['power_1'] = ages_in_years\n#    # first check if degree > 1\n       if degree > 1:\n#        # then loop over the remaining degrees:\n        for power in range(2, degree+1):\n            # first we'll give the column a name:\n            name = 'power_' + str(power)\n            # assign poly_dataframe[name] to be feature^power; use apply(*)\n            tmp_power = pd.Series(ages_in_years).apply(lambda x: x**power)\n            # then assign poly_sframe[name] to the appropriate power of feature\n            poly_dataframe[name] = tmp_power\n       return poly_dataframe\n\n\ndef outcome_counts_for_feature(data_frame,feature):\n    # Some parts of this code were borrowed from Andy's example on the Kaggle Scripts Board\n    feature_values = np.array(data_frame[feature])\n    outcome = np.array(data_frame['OutcomeType'])\n    unique_outcomes = np.array(['Adoption', 'Died', 'Euthanasia', 'Return_to_owner', 'Transfer'], dtype=object)\n    unique_features = np.unique(feature_values)\n    outcome_contours = np.zeros([len(unique_outcomes),len(unique_features)])\n    outcome_counts = np.zeros(len(unique_features))  \n    for i in range(len(unique_features)):\n        for j in range(len(unique_outcomes)):\n            list1 = outcome[feature_values == unique_features[i]] #list of outcomes for that particular feature\n            outcome_contours[j,i] = len(list1[list1 == unique_outcomes[j]]) #/len(list1) # fraction of total for each outcome based on that feature\n        outcome_counts[i] = len(list1)\n    return unique_features, outcome_contours\n        \ndef get_residual_sum_of_squares_poly(model, data, outcome):\n    # Get Residual Sum of Squares (RSS) to assess error of fit\n    example_predictions = model.predict(data)\n    residuals = outcome - example_predictions\n    prod_res = residuals*residuals\n    RSS = prod_res.sum()\n    return(RSS)  \n\ndef print_coefficients(model):    \n    w0=model.coef_.tolist()\n    w=w0[0]\n    deg = len(w)-1\n    print('Learned polynomial for degree ' + str(deg) + ':')\n    w.reverse()\n    print(np.poly1d(w))\n\ndef polynomial_features(data, deg):\n    data_copy=data.copy()\n    for i in range(1,deg):\n        data_copy['X'+str(i+1)]=data_copy['X'+str(i)]*data_copy['X1']\n    return data_copy\n\ndef k_fold_cross_validation(k, l2_penalty, data, output, model):\n    # Perform k-fold cross validation\n    data1 = data.ix[1:]\n    n = len(data1)\n    for i in range(k):\n        start = round((n*i)/k)\n        end = round((n*(i+1))/k-1)\n        validation0=data1[start:end+1]\n        output_val = output[start:end+1]\n        last = data1[end+1:n]\n        first = data1[0:start] \n        train0=first.append(last)\n        last_out = output[end+1:n]\n        first_out = output[0:start]    \n        output_train=append_ndarray(first_out, last_out)\n    model.fit(train0, output_train)\n    rss_validation = get_residual_sum_of_squares_poly(model, validation0, output_val) \n    return rss_validation\n  \ndef append_ndarray(arr1, arr2):\n    #Append two ndarrays\n    target = []\n    for i in arr1:\n        target.append(i)\n    for j in arr2:\n        target.append(j)\n#    print(target)\n    return np.array(target)\n\ndef age_to_years(item):\n    # Based on age_to_days function written by \"Andy\" on the Kaggle Scripts board\n    if type(item) is str:\n        item = [item]\n    ages_in_years = np.zeros(len(item))\n    for i in range(len(item)):\n        if type(item[i]) is str:\n            if 'day' in item[i]:\n                ages_in_years[i] = int(item[i].split(' ')[0])/365\n            if 'week' in item[i]:\n                ages_in_years[i] = int(item[i].split(' ')[0])/(52) #approx\n            if 'month' in item[i]:\n                ages_in_years[i] = int(item[i].split(' ')[0])/12\n            if 'year' in item[i]:\n                ages_in_years[i] = int(item[i].split(' ')[0])    \n        else:\n            ages_in_years[i] = 0\n    return ages_in_years\n\n#==============================================================================\n# For degree in range(1, 20+1)):\n#  Learn a polynomial regression model to Age of Shelter Cats vs Adoption Rate with that \n#  degree on TRAIN data.\n#  Compute the RSS on VALIDATION data for that degree.\n#==============================================================================\n#Report which degree had the lowest RSS on validation data \n\nNumber_of_Polys = 12\nRSS = np.zeros((Number_of_Polys))\nval_output = np.array(validation['OutcomeType'])\n  \nprint('Cross validation for different degrees of polynomial fit to data')\nfor degree in range(1, Number_of_Polys+1):\n    polydata_train = polynomial_dataframe(train,'AgeuponOutcome', 'Cat',degree) \n    my_features = polydata_train.columns.values.tolist()\n    polydata_train['OutcomeType'] = train['OutcomeType'] # add price to the data since it's the target\n    unique_f,outcomes_f=outcome_counts_for_feature(polydata_train,'power_1')\n    new_polydata_train = pd.DataFrame()   \n    for f in my_features:\n        unique_f,not_used=outcome_counts_for_feature(polydata_train,f)\n        new_polydata_train[f]=unique_f\n    polydata_valid = polynomial_dataframe(validation,'AgeuponOutcome', 'Cat',degree)\n    polydata_valid['OutcomeType'] = validation['OutcomeType'] # add price to the data since it's the target\n    unique_val_f,outcomes_val_f=outcome_counts_for_feature(polydata_valid,'power_1')\n    new_polydata_valid = pd.DataFrame()  \n    for f in my_features:\n        unique_val_f,not_used=outcome_counts_for_feature(polydata_valid,f)\n        new_polydata_valid[f]=unique_val_f\n    model = LinearRegression()\n    nr_adopted=outcomes_f[0].reshape((len(outcomes_f[0]),1))\n    fitted = model.fit(new_polydata_train,nr_adopted)\n    nr_adopted_val=outcomes_val_f[0].reshape((len(outcomes_val_f[0]),1))    \n    RSS[degree-1] = get_residual_sum_of_squares_poly(model, new_polydata_valid,nr_adopted_val)\n    print('Fit to ' + str(degree) + ' degree polynomial' + ' with Error, RSS: ' + str(RSS[degree-1]))\n\n\n#################################################################################\n#      Plot Results of 8th order polynomial fit\n###################################################################################\n\nunique_f,outcomes_f=outcome_counts_for_feature(polydata_train,'power_1')\np_deg = np.poly1d(np.polyfit(unique_f,outcomes_f[0], 6))\nxp = np.linspace(0, 20, 100)\nplt.plot(unique_f, outcomes_f[0], '.', xp, p_deg(xp), '--')\nplt.title('Overfitted? -- Cats Adopted According to Age')\nplt.xlabel('age [years]')\nplt.ylabel('Number of Cats Adopted')\nplt.yscale('log')\nplt.show()\nplt.close()\n\n#==============================================================================\n# Use Ridge Regression with L2-penalty to prevent overfitting the data\n#==============================================================================\n \nRSSlist=[0]\nk = 10\nfor l2_penalty in np.logspace(-1, 0, num=10):\n    model_ridge = Ridge(normalize=True, alpha=l2_penalty, solver='auto', fit_intercept=True, tol=0.001)\n    RSSdatum=k_fold_cross_validation(k, l2_penalty, new_polydata_train, nr_adopted, model_ridge)\n    RSSlist.append(RSSdatum)\n    print('L2-penalty: ' + str(l2_penalty) + '     Error, RSS: ' + str(RSSdatum))\n\n\n###################### Compare with L2-penalty = 1 using TEST data ####################################\n\nmodel_ridge_best = Ridge(normalize=True, alpha=1, solver='auto', fit_intercept=True, tol=0.001)\nc = new_polydata_train['power_1']\nnew_c=c.reshape(len(c),1) \nmodelo_l21=model_ridge_best.fit(new_c, nr_adopted)\nAge_Data = polynomial_dataframe(testing,'AgeuponOutcome', 'Cat',1)  \nfeature_values = np.array(Age_Data['power_1'])\nunique_features_test = np.unique(feature_values)\nnew_f=unique_features_test.reshape(len(unique_features_test),1) \nexample_predictions = modelo_l21.predict(new_f)\n\n\n############## Plot the Predicted Fit\np1, = plt.plot(unique_f,outcomes_f[0], '.' )\np2, = plt.plot(new_f,example_predictions,'.-')\nplt.title('Ridge Regression Fit, L2 = 1')\nplt.xlabel('age [years]')\nplt.ylabel('Number of Cats Adopted')\nplt.yscale('log')\nplt.show()\nplt.close()\n\n###################### Compare with L2-penalty = 0.10 using TEST data  ####################################\n\nmodel_ridge_best = Ridge(normalize=True, alpha=1e-1, solver='auto', fit_intercept=True, tol=0.001)\nc = new_polydata_train['power_1']\nnew_c=c.reshape(len(c),1) \nmodelol201=model_ridge_best.fit(new_c, nr_adopted)\nAge_Data = polynomial_dataframe(testing,'AgeuponOutcome', 'Cat',1)  \nfeature_values = np.array(Age_Data['power_1'])\nunique_features_test = np.unique(feature_values)\nnew_f=unique_features_test.reshape(len(unique_features_test),1) \nexample_predictions = modelol201.predict(new_f)\n\n#example_predictions_6years = modelo.predict(6.0)\n#example_predictions_18years = modelo.predict(18.0)\n\n############## Plot the Predicted Fit\np1, = plt.plot(unique_f,outcomes_f[0], '.' )\np2, = plt.plot(new_f,example_predictions,'.-')\nplt.title('Ridge Regression Fit, L2 = 0.10')\nplt.xlabel('age [years]')\nplt.ylabel('Number of Cats Adopted')\nplt.yscale('log')\nplt.show()\nplt.close()\n\n#############################################################################################\n##   Create a DataFrame of Results and Plot based on TEST data\n\nIDs = np.array(testing.loc[testing['AnimalType'] == 'Cat','ID'])\nCat_Ages = feature_values\nAdoption_Numbers_L2_01 = []\nAdoption_Numbers_L2_1 = []\nfor a in Cat_Ages:\n    predicted_adoption_nrl201 = modelol201.predict(a)\n    pa01=predicted_adoption_nrl201.tolist()\n    Adoption_Numbers_L2_01.append(pa01[0][0])\n    predicted_adoption_nrl21 = modelo_l21.predict(a)\n    pa1=predicted_adoption_nrl21.tolist()\n    Adoption_Numbers_L2_1.append(pa1[0][0])\n#Avg_Adopted = np.array(Adoption_Numbers)/sum(Adoption_Numbers)*100\n#Avg_Adopted.tolist()\nCat_Ages.tolist()\nIDs.tolist()\n\nCat_Adoptions_Age = pd.DataFrame()\nCat_Adoptions_Age['ID [Cat]'] = IDs\nCat_Adoptions_Age['Age [ years]'] = Cat_Ages\nCat_Adoptions_Age['No. Adoptions with L2=0.1'] = Adoption_Numbers_L2_01\nCat_Adoptions_Age['No. Adoptions with L2=1'] = Adoption_Numbers_L2_1\n\np1, = plt.plot(Cat_Ages,Adoption_Numbers_L2_01, '+' )\np2, = plt.plot(Cat_Ages,Adoption_Numbers_L2_1, '.' )\np3, = plt.plot(np.linspace(0,25),np.zeros(len(np.linspace(0,25))), '-' )\nplt.title('Adoptions According to Ridge Regression Fits')\nplt.xlabel('age [years]')\nplt.ylabel('Number of Cats Adopted')\nplt.legend([p1,p2],['L2-penalty=0.10','L2-penalty=1'],loc=1)\nplt.show()\nplt.close()\n\nCat_Adoptions_Age.head()\n#dtype_dict_new = {'ID [Cat]':str,'Age [ years]':str,'No. Adoptions with L2=0.1':str,'No. Adoptions with L2=1':str}\nheader = ['ID [Cat]','Age [ years]','No. Adoptions with L2=0.1','No. Adoptions with L2=1']\n\nCat_Adoptions_Age.to_csv('Cat_Adoptions_RegularizedRegressionFit.csv', columns = header)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}