{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os","metadata":{"execution":{"iopub.status.busy":"2022-04-08T12:37:20.04289Z","iopub.execute_input":"2022-04-08T12:37:20.04321Z","iopub.status.idle":"2022-04-08T12:37:20.07213Z","shell.execute_reply.started":"2022-04-08T12:37:20.043128Z","shell.execute_reply":"2022-04-08T12:37:20.071389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *Sorghum-FGVC9*","metadata":{}},{"cell_type":"markdown","source":"![](https://storage.googleapis.com/kaggle-competitions/kaggle/26245/logos/header.png?t=2021-04-07-18-10-13)\n\n","metadata":{}},{"cell_type":"markdown","source":"# *Import libraries for Data Preprocessing*","metadata":{}},{"cell_type":"code","source":"\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom termcolor import colored\nfrom PIL import Image\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-08T12:37:21.999932Z","iopub.execute_input":"2022-04-08T12:37:22.000419Z","iopub.status.idle":"2022-04-08T12:37:26.483026Z","shell.execute_reply.started":"2022-04-08T12:37:22.000383Z","shell.execute_reply":"2022-04-08T12:37:26.482192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *Define data path file*","metadata":{}},{"cell_type":"code","source":"\n\nTRAIN_IMAGE_DIR = '../input/sorghum-id-fgvc-9/train_images'\nTEST_IMAGE_DIR = '../input/sorghum-id-fgvc-9/test'\n\nTRAIN_IMAGE_SAVE_DIR = '/kaggle/working/resized_train_images'\nTEST_IMAGE_SAVE_DIR = '/kaggle/working/resized_test_images'\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-08T12:37:28.997573Z","iopub.execute_input":"2022-04-08T12:37:28.997974Z","iopub.status.idle":"2022-04-08T12:37:29.001865Z","shell.execute_reply.started":"2022-04-08T12:37:28.997941Z","shell.execute_reply":"2022-04-08T12:37:29.001096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *Count Images in folder train and test*","metadata":{}},{"cell_type":"code","source":"\n\ntrain_images = os.listdir(TRAIN_IMAGE_DIR)\ntest_images = os.listdir(TEST_IMAGE_DIR)\n\nn_train_images = len(train_images)\nn_test_images = len(test_images)\n\nprint('# of training images = {}'.format(n_train_images))\nprint('# of testing images = {}'.format(n_test_images))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-08T12:37:31.742152Z","iopub.execute_input":"2022-04-08T12:37:31.742413Z","iopub.status.idle":"2022-04-08T12:37:32.794905Z","shell.execute_reply.started":"2022-04-08T12:37:31.742385Z","shell.execute_reply":"2022-04-08T12:37:32.79406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *Create function for resize images with 3 channel(RGB) and image size*","metadata":{}},{"cell_type":"code","source":"def resize_images(path):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [224, 224])\n    return img\n","metadata":{"execution":{"iopub.status.busy":"2022-04-08T12:37:34.573593Z","iopub.execute_input":"2022-04-08T12:37:34.574313Z","iopub.status.idle":"2022-04-08T12:37:34.578685Z","shell.execute_reply.started":"2022-04-08T12:37:34.574277Z","shell.execute_reply":"2022-04-08T12:37:34.577688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *The loop for resizing images in folder and put it into the new folder*","metadata":{}},{"cell_type":"code","source":"\n\nprint(colored('Processing training images...', 'red'))\nif not os.path.exists(TRAIN_IMAGE_SAVE_DIR):\n    os.makedirs(TRAIN_IMAGE_SAVE_DIR)\n    print('Training images saving directory created')\nelse:\n    print('Training images saving directory exists')\n    \nfor i, train_image in enumerate(train_images):\n    if (i+1) % 1000 == 0:\n        print('{}/{} images processed'.format(i+1, n_train_images))\n    resized_img = resize_images(os.path.join(TRAIN_IMAGE_DIR, train_image))\n    tf.keras.utils.save_img(os.path.join(TRAIN_IMAGE_SAVE_DIR, train_image), resized_img)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-08T12:37:37.703561Z","iopub.execute_input":"2022-04-08T12:37:37.704308Z","iopub.status.idle":"2022-04-08T13:04:02.43212Z","shell.execute_reply.started":"2022-04-08T12:37:37.704269Z","shell.execute_reply":"2022-04-08T13:04:02.431389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *Zipfile if you want to download it and use like custom datasets*","metadata":{}},{"cell_type":"code","source":"!zip -r resize_train.zip ./resized_train_images","metadata":{"execution":{"iopub.status.busy":"2022-04-07T17:00:18.044778Z","iopub.execute_input":"2022-04-07T17:00:18.045425Z","iopub.status.idle":"2022-04-07T17:01:57.126925Z","shell.execute_reply.started":"2022-04-07T17:00:18.045387Z","shell.execute_reply":"2022-04-07T17:01:57.126037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *Create CSV file for get path and label from folder datasets*","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nimport pandas as pd\nDATA_DIR  = './'\nTRAIN_DIR = DATA_DIR + 'resized_train_images/'\n# TEST_DIR  = DATA_DIR + 'resized_test_images/'\n\n\n\nlabel_encoder = LabelEncoder()\n\n# Load Train Data\ntrain_df = pd.read_csv('../input/sorghum-id-fgvc-9/train_cultivar_mapping.csv')\ntrain_df['Id'] = train_df['image'].apply(lambda x: f'{TRAIN_DIR}{x}')\n# Summary\nprint(f'train_df: {train_df.shape}')\ntrain_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-08T13:04:46.563069Z","iopub.execute_input":"2022-04-08T13:04:46.563331Z","iopub.status.idle":"2022-04-08T13:04:47.334176Z","shell.execute_reply.started":"2022-04-08T13:04:46.563301Z","shell.execute_reply":"2022-04-08T13:04:47.333468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *Count classes from column cultivars in csv file*","metadata":{}},{"cell_type":"code","source":"unique_cultivars = list(train_df.cultivar.unique())\nnum_classes = len(unique_cultivars)\nnum_classes","metadata":{"execution":{"iopub.status.busy":"2022-04-08T11:45:22.665897Z","iopub.execute_input":"2022-04-08T11:45:22.666585Z","iopub.status.idle":"2022-04-08T11:45:22.674963Z","shell.execute_reply.started":"2022-04-08T11:45:22.666549Z","shell.execute_reply":"2022-04-08T11:45:22.67398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['Id']","metadata":{"execution":{"iopub.status.busy":"2022-04-08T13:04:53.331182Z","iopub.execute_input":"2022-04-08T13:04:53.331445Z","iopub.status.idle":"2022-04-08T13:04:53.339696Z","shell.execute_reply.started":"2022-04-08T13:04:53.331418Z","shell.execute_reply":"2022-04-08T13:04:53.338909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['cultivar']","metadata":{"execution":{"iopub.status.busy":"2022-04-08T13:04:55.573243Z","iopub.execute_input":"2022-04-08T13:04:55.574132Z","iopub.status.idle":"2022-04-08T13:04:55.583073Z","shell.execute_reply.started":"2022-04-08T13:04:55.574087Z","shell.execute_reply":"2022-04-08T13:04:55.582342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.to_csv('train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-08T13:04:58.475701Z","iopub.execute_input":"2022-04-08T13:04:58.476244Z","iopub.status.idle":"2022-04-08T13:04:58.595127Z","shell.execute_reply.started":"2022-04-08T13:04:58.476209Z","shell.execute_reply":"2022-04-08T13:04:58.594395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('./train.csv')\n# data['file_path'] = data['file_path'].astype('str')\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-08T13:05:00.312294Z","iopub.execute_input":"2022-04-08T13:05:00.312557Z","iopub.status.idle":"2022-04-08T13:05:00.354625Z","shell.execute_reply.started":"2022-04-08T13:05:00.312526Z","shell.execute_reply":"2022-04-08T13:05:00.353895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data\n","metadata":{"execution":{"iopub.status.busy":"2022-04-08T13:05:03.358061Z","iopub.execute_input":"2022-04-08T13:05:03.358316Z","iopub.status.idle":"2022-04-08T13:05:03.371442Z","shell.execute_reply.started":"2022-04-08T13:05:03.358288Z","shell.execute_reply":"2022-04-08T13:05:03.370776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *Import libraries prepare for data augmentation and training loop processing*","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import DenseNet201, EfficientNetB3, InceptionV3, ResNet50\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nimport numpy as np\nimport os\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2022-04-08T13:05:06.218918Z","iopub.execute_input":"2022-04-08T13:05:06.219468Z","iopub.status.idle":"2022-04-08T13:05:06.242897Z","shell.execute_reply.started":"2022-04-08T13:05:06.21943Z","shell.execute_reply":"2022-04-08T13:05:06.242151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *Default epochs, batch_size and img_size*","metadata":{}},{"cell_type":"code","source":"EPOCHS = 30\nBATCH_SIZE = 256\nIMG_SIZE = 224","metadata":{"execution":{"iopub.status.busy":"2022-04-08T13:05:10.486943Z","iopub.execute_input":"2022-04-08T13:05:10.487189Z","iopub.status.idle":"2022-04-08T13:05:10.492262Z","shell.execute_reply.started":"2022-04-08T13:05:10.48716Z","shell.execute_reply":"2022-04-08T13:05:10.491585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cultivars = data['cultivar'].to_numpy()\ncultivars = cultivars\ncultivars","metadata":{"execution":{"iopub.status.busy":"2022-04-08T08:18:14.404915Z","iopub.execute_input":"2022-04-08T08:18:14.405439Z","iopub.status.idle":"2022-04-08T08:18:14.412434Z","shell.execute_reply.started":"2022-04-08T08:18:14.405405Z","shell.execute_reply":"2022-04-08T08:18:14.411549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *If you want to use data aug from keras you have to change dtype of column cultivar from object to string type*","metadata":{}},{"cell_type":"code","source":"data['cultivar'] = data['cultivar'].astype('str')\n# train_df['Id'] = train_df['Id'].astype('string')","metadata":{"execution":{"iopub.status.busy":"2022-04-08T13:06:03.252075Z","iopub.execute_input":"2022-04-08T13:06:03.252351Z","iopub.status.idle":"2022-04-08T13:06:03.259113Z","shell.execute_reply.started":"2022-04-08T13:06:03.252321Z","shell.execute_reply":"2022-04-08T13:06:03.258059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['cultivar']","metadata":{"execution":{"iopub.status.busy":"2022-04-08T13:06:06.193472Z","iopub.execute_input":"2022-04-08T13:06:06.194015Z","iopub.status.idle":"2022-04-08T13:06:06.200723Z","shell.execute_reply.started":"2022-04-08T13:06:06.193978Z","shell.execute_reply":"2022-04-08T13:06:06.200038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *Data augmentation*","metadata":{}},{"cell_type":"markdown","source":"![](https://www.researchgate.net/publication/347221279/figure/fig1/AS:1023619987673089@1621061431785/Image-data-augmentation-technique-for-increasing-the-size-of-an-image-to-train-a-deep.png)","metadata":{}},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n    rotation_range=30,\n    zoom_range=0.15,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.15,\n    horizontal_flip=True,\n    fill_mode=\"nearest\",\n    preprocessing_function=preprocess_input,\n    validation_split=0.2)\n\ntrain_generator=datagen.flow_from_dataframe(\ndataframe=data,\n# directory=\"./train/\",\nx_col=\"Id\",\ny_col=\"cultivar\",\nsubset=\"training\",\nbatch_size=BATCH_SIZE,\nshuffle=True,\nclass_mode=\"categorical\",\ntarget_size=(IMG_SIZE,IMG_SIZE))\n\n\nvalid_generator=datagen.flow_from_dataframe(\ndataframe=data,\n# directory=\"./train/\",\nx_col=\"Id\",\ny_col=\"cultivar\",\nsubset=\"validation\",\nbatch_size=BATCH_SIZE,\nshuffle=True,\nclass_mode=\"categorical\",\ntarget_size=(IMG_SIZE,IMG_SIZE))\n\n\ntest_datagen = ImageDataGenerator()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T08:20:10.84465Z","iopub.execute_input":"2022-04-08T08:20:10.844919Z","iopub.status.idle":"2022-04-08T08:20:11.197731Z","shell.execute_reply.started":"2022-04-08T08:20:10.84489Z","shell.execute_reply":"2022-04-08T08:20:11.197009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *Here, this is transfer learning using pretrained model InceptionV3, so the question is asked here Why we use InceptionV3, so until now we have many pretrained model and we can't use them indiscriminately, we have to know how we can extract the features of image in the best way with pretrained model, so in here I use InceptionV3 because, our image has many detail and we need this pretrained to take features and of course It's very deep*","metadata":{}},{"cell_type":"markdown","source":"![](https://res.cloudinary.com/practicaldev/image/fetch/s--eKV0swu_--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/2oyycxxs02jmcghplwc4.png)","metadata":{}},{"cell_type":"markdown","source":"![](https://camo.githubusercontent.com/8b243e646673dd9234f39cf8bdd5da1c6f051fd9/68747470733a2f2f7777772e50657465724d6f7373416d6c416c6c52657365617263682e636f6d2f6d656469612f696d616765732f7265706f7369746f726965732f5472616e736665722d4c6561726e696e672e6a7067)","metadata":{}},{"cell_type":"code","source":"base_model = InceptionV3(weights=\"imagenet\", include_top=False, input_shape=(IMG_SIZE,IMG_SIZE,3))\n\nfor layer in base_model.layers:\n    layer.trainable = True\n\nhead_model = base_model.output\nhead_model = GlobalAveragePooling2D()(head_model)\nhead_model = Flatten()(head_model)\n# head_model = Dense(128, activation=\"relu\")(head_model)\n# head_model = Dropout(0.4)(head_model)\n# head_model = Dense(64, activation=\"relu\")(head_model)\nhead_model = Dense(100, activation=\"softmax\")(head_model)\nmodel = Model(inputs=base_model.input, outputs=head_model)\nmodel.summary()\n\n# opt = Adam(learning_rate=learning_rate)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-04-08T08:20:14.682419Z","iopub.execute_input":"2022-04-08T08:20:14.682934Z","iopub.status.idle":"2022-04-08T08:20:17.487364Z","shell.execute_reply.started":"2022-04-08T08:20:14.682897Z","shell.execute_reply":"2022-04-08T08:20:17.486614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *Define checkpoint and early stopping for have the best model*","metadata":{}},{"cell_type":"code","source":"\ncp_callback = ModelCheckpoint(filepath='fgvc9_model_4.h5',\n                              monitor='val_accuracy',\n                              save_freq='epoch', verbose=1, period=1,\n                              save_best_only=True, save_weights_only=True)\n\nearly_stopping = EarlyStopping(monitor='val_accuracy',\n                               verbose=1, patience=5)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-08T08:20:28.569751Z","iopub.execute_input":"2022-04-08T08:20:28.570005Z","iopub.status.idle":"2022-04-08T08:20:28.575237Z","shell.execute_reply.started":"2022-04-08T08:20:28.569977Z","shell.execute_reply":"2022-04-08T08:20:28.574557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *Reduce learning rate on each epoch*","metadata":{}},{"cell_type":"code","source":"from datetime import datetime, timedelta\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nstart_time = datetime.now()\nprint('Time now is', start_time)\nend_training_by_tdelta = timedelta(seconds=8400)\nthis_run_file_prefix = start_time.strftime('%Y%m%d_%H%M_')\n\n\n# EPOCHS = 25\n# STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n\n\n\n# Learning Rate Schedule for Fine Tuning #\n# Learning rate schedule for TPU, GPU and CPU.\n# Using an LR ramp up because fine-tuning a pre-trained model.\n# Starting with a high LR would break the pre-trained weights.\n\nLR_START = 0.00001\nLR_MAX = 0.00005 * 8\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 0\nLR_EXP_DECAY = .8\nnum_epochs = 50\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = LR_START + (epoch * (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS)\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\nrng = [i for i in range(num_epochs)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-08T08:20:31.188483Z","iopub.execute_input":"2022-04-08T08:20:31.189019Z","iopub.status.idle":"2022-04-08T08:20:31.390081Z","shell.execute_reply.started":"2022-04-08T08:20:31.188984Z","shell.execute_reply":"2022-04-08T08:20:31.389412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *Model Fitting*","metadata":{}},{"cell_type":"code","source":"history = model.fit(\n                train_generator, \n                validation_data=valid_generator,\n                epochs=num_epochs,\n                steps_per_epoch=train_generator.samples // train_generator.batch_size,\n#                 validation_steps= (3712 / 256)*2,\n                callbacks=[cp_callback, early_stopping, lr_callback])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-08T08:20:34.819491Z","iopub.execute_input":"2022-04-08T08:20:34.819953Z","iopub.status.idle":"2022-04-08T10:38:20.2257Z","shell.execute_reply.started":"2022-04-08T08:20:34.819917Z","shell.execute_reply":"2022-04-08T10:38:20.224913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('fgvc_9_best_model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-04-08T10:39:13.747044Z","iopub.execute_input":"2022-04-08T10:39:13.7473Z","iopub.status.idle":"2022-04-08T10:39:34.572657Z","shell.execute_reply.started":"2022-04-08T10:39:13.747266Z","shell.execute_reply":"2022-04-08T10:39:34.571913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_model('../input/model-trained/fgvc_9_best_model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-04-08T13:05:20.711712Z","iopub.execute_input":"2022-04-08T13:05:20.71218Z","iopub.status.idle":"2022-04-08T13:05:26.840506Z","shell.execute_reply.started":"2022-04-08T13:05:20.712145Z","shell.execute_reply":"2022-04-08T13:05:26.839761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-04-08T02:12:06.767731Z","iopub.execute_input":"2022-04-08T02:12:06.767995Z","iopub.status.idle":"2022-04-08T02:12:07.571162Z","shell.execute_reply.started":"2022-04-08T02:12:06.767966Z","shell.execute_reply":"2022-04-08T02:12:07.570331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cultivars = data['cultivar'].to_numpy()\ncultivars = cultivars\ncultivars","metadata":{"execution":{"iopub.status.busy":"2022-04-08T13:05:51.965392Z","iopub.execute_input":"2022-04-08T13:05:51.965689Z","iopub.status.idle":"2022-04-08T13:05:51.974065Z","shell.execute_reply.started":"2022-04-08T13:05:51.965661Z","shell.execute_reply":"2022-04-08T13:05:51.973276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = np.unique(cultivars)\nnum_classes = len(class_names)\nnum_classes","metadata":{"execution":{"iopub.status.busy":"2022-04-08T13:06:11.670348Z","iopub.execute_input":"2022-04-08T13:06:11.670821Z","iopub.status.idle":"2022-04-08T13:06:11.698175Z","shell.execute_reply.started":"2022-04-08T13:06:11.670778Z","shell.execute_reply":"2022-04-08T13:06:11.697278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *Create Test Dataframe prepare for Prediction*","metadata":{}},{"cell_type":"code","source":"test_datagen = ImageDataGenerator()\ntest_filenames = os.listdir(\"../input/sorghum-id-fgvc-9/test\")\ntest_df = pd.DataFrame({\n    'filename': test_filenames\n})\ntest_generator = test_datagen.flow_from_dataframe(\n    test_df,\n    \"../input/sorghum-id-fgvc-9/test\",\n    x_col='filename',\n    y_col=None,\n    class_mode=None,\n    target_size=(IMG_SIZE,IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-08T13:06:14.343807Z","iopub.execute_input":"2022-04-08T13:06:14.344348Z","iopub.status.idle":"2022-04-08T13:06:44.15264Z","shell.execute_reply.started":"2022-04-08T13:06:14.344312Z","shell.execute_reply":"2022-04-08T13:06:44.151871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *Prediction*","metadata":{}},{"cell_type":"code","source":"# test_ds = test_ds.batch(BATCH_SIZE)\npreds = model.predict(test_generator)\npreds = np.argmax(preds, axis=1)\npreds = [class_names[i] for i in preds]\npreds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *Submission*","metadata":{}},{"cell_type":"code","source":"test_df['cultivar'] = preds\ntest_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T13:35:48.764789Z","iopub.execute_input":"2022-04-08T13:35:48.765054Z","iopub.status.idle":"2022-04-08T13:35:48.827568Z","shell.execute_reply.started":"2022-04-08T13:35:48.765026Z","shell.execute_reply":"2022-04-08T13:35:48.826815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2022-04-08T13:35:51.953027Z","iopub.execute_input":"2022-04-08T13:35:51.953303Z","iopub.status.idle":"2022-04-08T13:35:51.970079Z","shell.execute_reply.started":"2022-04-08T13:35:51.953271Z","shell.execute_reply":"2022-04-08T13:35:51.969208Z"},"trusted":true},"execution_count":null,"outputs":[]}]}