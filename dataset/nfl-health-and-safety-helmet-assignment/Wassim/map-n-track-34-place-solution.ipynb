{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\n\nimport os\nfrom IPython.display import Video, display\nimport matplotlib.pyplot as plt\nimport scipy.optimize\nimport scipy.signal\nfrom tqdm.auto import tqdm\nimport math\nimport random\n\nimport matplotlib.animation as animation\n\nimport sys\nsys.path.append('../input/easydict-master/easydict-master/')\n# https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch\nsys.path.append('../input/yolov5-deepsort-pytorch/Yolov5_DeepSort_Pytorch-master/Yolov5_DeepSort_Pytorch-master/deep_sort_pytorch/')\n\nfrom deep_sort.deep_sort import DeepSort\nfrom utils.parser import get_config\n\nfrom scipy.interpolate import interp1d\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\n!pip install ../input/helmet-assignment-helpers/helmet-assignment-main/ > /dev/null 2>&1\nfrom helmet_assignment.score import NFLAssignmentScorer, check_submission\n","metadata":{"execution":{"iopub.status.busy":"2021-09-19T08:51:46.33774Z","iopub.execute_input":"2021-09-19T08:51:46.338282Z","iopub.status.idle":"2021-09-19T08:51:48.849744Z","shell.execute_reply.started":"2021-09-19T08:51:46.33816Z","shell.execute_reply":"2021-09-19T08:51:48.848787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = False\nif train:\n    data_baseline_helmets = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/train_baseline_helmets.csv')\n    data_player_tracking = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/train_player_tracking.csv')\n    data_labels = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/train_labels.csv')\n    video_dir = '../input/nfl-health-and-safety-helmet-assignment/train/'\nelse:\n    data_baseline_helmets = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/test_baseline_helmets.csv')\n    data_player_tracking = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/test_player_tracking.csv')\n    video_dir = '../input/nfl-health-and-safety-helmet-assignment/test/'\n","metadata":{"execution":{"iopub.status.busy":"2021-09-19T08:51:48.851387Z","iopub.execute_input":"2021-09-19T08:51:48.85175Z","iopub.status.idle":"2021-09-19T08:51:49.0097Z","shell.execute_reply.started":"2021-09-19T08:51:48.851711Z","shell.execute_reply":"2021-09-19T08:51:49.008939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing \n\ndef process_baseline(baseline_df):\n    baseline_df['gameKey'] = baseline_df['video_frame'].apply(lambda x: int(x.split('_')[0]))\n    baseline_df['playID'] = baseline_df['video_frame'].apply(lambda x: int(x.split('_')[1]))\n    baseline_df['view'] = baseline_df['video_frame'].apply(lambda x: x.split('_')[2])\n    baseline_df['frame'] = baseline_df['video_frame'].apply(lambda x: int(x.split('_')[3]))\n    baseline_df['video'] = baseline_df['video_frame'].str.split('_').str[:3].str.join('_')\n    baseline_df['x'] = baseline_df.apply(lambda x: x.left + x.width/2, axis = 1)\n    baseline_df['y'] = baseline_df.apply(lambda x: x.top + x.height/2, axis = 1)\n    baseline_df['label'] = 'UNK'\n    return baseline_df\n\ndata_baseline_helmets = process_baseline(data_baseline_helmets)\n\ndef process_labels(labels_df):\n    labels_df['x'] = labels_df.apply(lambda x: x.left + x.width/2, axis = 1)\n    labels_df['y'] = labels_df.apply(lambda x: x.top + x.height/2, axis = 1)\n    return labels_df\n\nif train:\n    data_labels = process_labels(data_labels)\n\n# Copied from https://www.kaggle.com/go5kuramubon/merge-label-and-tracking-data\n\ndef add_track_features(tracks, fps=59.94, snap_frame=10):\n    \"\"\"\n    Add column features helpful for syncing with video data.\n    \"\"\"\n    tracks = tracks.copy()\n    tracks[\"game_play\"] = (\n        tracks[\"gameKey\"].astype(\"str\")\n        + \"_\"\n        + tracks[\"playID\"].astype(\"str\").str.zfill(6)\n    )\n    tracks[\"time\"] = pd.to_datetime(tracks[\"time\"])\n    snap_dict = (\n        tracks.query('event == \"ball_snap\"')\n        .groupby(\"game_play\")[\"time\"]\n        .first()\n        .to_dict()\n    )\n    tracks[\"snap\"] = tracks[\"game_play\"].map(snap_dict)\n    tracks[\"isSnap\"] = tracks[\"snap\"] == tracks[\"time\"]\n    tracks[\"team\"] = tracks[\"player\"].str[0].replace(\"H\", \"Home\").replace(\"V\", \"Away\")\n    tracks[\"snap_offset\"] = (tracks[\"time\"] - tracks[\"snap\"]).astype(\n        \"timedelta64[ms]\"\n    ) / 1_000\n    # Estimated video frame\n    tracks[\"est_frame\"] = (\n        ((tracks[\"snap_offset\"] * fps) + snap_frame).round().astype(\"int\")\n    )\n    return tracks\n\ndata_player_tracking = add_track_features(data_player_tracking)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T08:51:57.604778Z","iopub.execute_input":"2021-09-19T08:51:57.605293Z","iopub.status.idle":"2021-09-19T08:52:02.09733Z","shell.execute_reply.started":"2021-09-19T08:51:57.605261Z","shell.execute_reply":"2021-09-19T08:52:02.096457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def distance(p1, p2):\n    x1, y1 = p1\n    x2, y2 = p2\n    return np.sqrt((x1-x2)**2 + (y1-y2)**2)\n\ndef distance2(p1, p2):\n    x1, y1 = p1\n    x2, y2 = p2\n    return (x1-x2)**2 + (y1-y2)**2","metadata":{"execution":{"iopub.status.busy":"2021-09-19T08:52:02.098977Z","iopub.execute_input":"2021-09-19T08:52:02.099424Z","iopub.status.idle":"2021-09-19T08:52:02.10619Z","shell.execute_reply.started":"2021-09-19T08:52:02.099376Z","shell.execute_reply":"2021-09-19T08:52:02.105105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def deepsort_helmets(video_data, video_dir, deepsort_config='deepsort.yaml'):\n    \n    # Setup Deepsort\n    cfg = get_config()\n    cfg.merge_from_file(deepsort_config)    \n    deepsort = DeepSort(cfg.DEEPSORT.REID_CKPT,\n                        max_dist=cfg.DEEPSORT.MAX_DIST,\n                        min_confidence=cfg.DEEPSORT.MIN_CONFIDENCE,\n                        nms_max_overlap=cfg.DEEPSORT.NMS_MAX_OVERLAP,\n                        max_iou_distance=cfg.DEEPSORT.MAX_IOU_DISTANCE,\n                        max_age=cfg.DEEPSORT.MAX_AGE,\n                        n_init=cfg.DEEPSORT.N_INIT,\n                        nn_budget=cfg.DEEPSORT.NN_BUDGET,\n                        use_cuda=True)\n    \n    # Run through frames.\n    video_data = video_data.sort_values('frame').reset_index(drop=True)\n    ds = []\n    \n    myvideo = video_data.video.unique()[0]\n    cap = cv2.VideoCapture(f'{video_dir}/{myvideo}.mp4')\n    \n    for frame, d in tqdm(video_data.groupby(['frame']), total=video_data['frame'].nunique()):\n        videoframe = d.video_frame.unique()[0]\n        xywhs = d[['x','y','width','height']].values\n\n        success, image = cap.read()\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        confs = np.ones([len(d),])\n        clss =  np.zeros([len(d),])\n        try:\n            outputs = deepsort.update(xywhs, confs, clss, image)\n        except Exception as e:\n            outputs = []\n            print(f'Error {e} : Skipped')\n\n        preds_df = pd.DataFrame(outputs, columns=['left','top','right','bottom','cluster','class'])\n        \n        d = deepsort_merge(preds_df, d)\n        ds.append(d)\n        \n    dout = pd.concat(ds)\n    return dout\n\n\ndef deepsort_merge(deepsort_out, video_data):\n    \n    if len(deepsort_out) == 0:\n        video_data.loc[:,'cluster'] = 'UNK'\n        return video_data\n    \n    forfit = 10\n    deepsort_pts   = list(zip(deepsort_out['left'], deepsort_out['top']))\n    video_pts      = list(zip(video_data['left'], video_data['top']))\n\n    # Compute matching with Hungarian algorithm in both sides\n    match_cost  = np.array([ [distance(pt1, pt2) for pt1 in deepsort_pts] for pt2 in video_pts ])\n    trash_cost  = np.array([ [forfit for _ in deepsort_pts] for _ in deepsort_pts ])\n    \n    cost_matrix = np.concatenate([match_cost, trash_cost], axis = 0) \n    idxs1, idxs2 = scipy.optimize.linear_sum_assignment(cost_matrix)\n    try: \n        idxs1, idxs2 = np.array([ [idx1, idx2] for idx1, idx2 in zip(idxs1, idxs2) if idx1 < len(video_pts) ]).transpose()\n    except:\n        idxs1, idxs2 = [], []\n    \n    labels = deepsort_out.iloc[idxs2]['cluster'].copy()\n    \n    video_data.loc[:,'cluster'] = 'UNK'\n\n    video_data.iloc[idxs1, video_data.columns.get_loc('cluster') ] = labels\n\n    return video_data","metadata":{"execution":{"iopub.status.busy":"2021-09-19T08:52:02.108176Z","iopub.execute_input":"2021-09-19T08:52:02.108577Z","iopub.status.idle":"2021-09-19T08:52:02.128165Z","shell.execute_reply.started":"2021-09-19T08:52:02.108536Z","shell.execute_reply":"2021-09-19T08:52:02.127306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile deepsort.yaml\n\nDEEPSORT:\n  REID_CKPT: \"../input/yolov5-deepsort-pytorch/ckpt.t7\"\n  MAX_DIST: 0.4                          # Maximum cosine distance thold for similarity purpose\n  MIN_CONFIDENCE: 0.4                    # Min confidence for entry bboxes \n  NMS_MAX_OVERLAP: 1                     # Remove boxes with overlap !! We don't want NMS, NMS already done and could destroy \"Collision players\"\n  MAX_IOU_DISTANCE: 0.7                  # Gating IOU threshold. Associations with cost larger than this value are disregarded.\n  MAX_AGE: 30                            # Maximum number of misses before a track is deleted. --> We prefer IDswitch rather that incorect re-ID\n  N_INIT: 0                              # Number of consecutive detections before the track is confirmed. (0.1 s) The track state is set to `Deleted` if a miss occurs within the first `n_init` frames.\n  NN_BUDGET: 100                         # If not None, fix samples per class to at most this number. Removes the oldest samples when the budget is reached.","metadata":{"execution":{"iopub.status.busy":"2021-09-06T12:55:17.287327Z","iopub.execute_input":"2021-09-06T12:55:17.287852Z","iopub.status.idle":"2021-09-06T12:55:17.30007Z","shell.execute_reply.started":"2021-09-06T12:55:17.287804Z","shell.execute_reply":"2021-09-06T12:55:17.298837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nHelper functions from yolov5 to plot deepsort labels.\n\"\"\"\n\ndef compute_color_for_id(label):\n    \"\"\"\n    Simple function that adds fixed color depending on the id\n    \"\"\"\n    palette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n\n    color = [int((p * (label ** 2 - label + 1)) % 255) for p in palette]\n    return tuple(color)\n\ndef plot_one_box(x, im, color=None, label=None, line_thickness=3):\n    # Plots one bounding box on image 'im' using OpenCV\n    assert im.data.contiguous, 'Image not contiguous. Apply np.ascontiguousarray(im) to plot_on_box() input image.'\n    tl = line_thickness or round(0.002 * (im.shape[0] + im.shape[1]) / 2) + 1  # line/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(im, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label: \n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(im, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(im, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n    return im","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-06T12:55:17.30328Z","iopub.execute_input":"2021-09-06T12:55:17.30366Z","iopub.status.idle":"2021-09-06T12:55:17.318699Z","shell.execute_reply.started":"2021-09-06T12:55:17.303622Z","shell.execute_reply":"2021-09-06T12:55:17.31764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dbh_high = data_baseline_helmets[data_baseline_helmets.conf > 0.4]\ndbh_low  = data_baseline_helmets[data_baseline_helmets.conf <= 0.4]\ndbh_low['cluster'] = 'UNK'","metadata":{"execution":{"iopub.status.busy":"2021-09-06T12:55:17.320727Z","iopub.execute_input":"2021-09-06T12:55:17.321287Z","iopub.status.idle":"2021-09-06T12:55:17.353075Z","shell.execute_reply.started":"2021-09-06T12:55:17.321248Z","shell.execute_reply":"2021-09-06T12:55:17.352225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Shortcut \ndata_baseline_helmets = pd.read_csv('../input/baselinehelmetswithdeepsort/dbh_test_deepsortclustered_conf40.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-19T09:58:10.646689Z","iopub.execute_input":"2021-09-19T09:58:10.647064Z","iopub.status.idle":"2021-09-19T09:58:10.793858Z","shell.execute_reply.started":"2021-09-19T09:58:10.64703Z","shell.execute_reply":"2021-09-19T09:58:10.792937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shortcut\n#data_baseline_helmets = pd.read_csv('../input/fmot-det-test/fmot_df.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-19T09:06:21.694646Z","iopub.execute_input":"2021-09-19T09:06:21.695057Z","iopub.status.idle":"2021-09-19T09:06:21.993852Z","shell.execute_reply.started":"2021-09-19T09:06:21.69502Z","shell.execute_reply":"2021-09-19T09:06:21.992905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_baseline_helmets","metadata":{"execution":{"iopub.status.busy":"2021-09-19T09:06:27.957593Z","iopub.execute_input":"2021-09-19T09:06:27.957949Z","iopub.status.idle":"2021-09-19T09:06:27.989549Z","shell.execute_reply.started":"2021-09-19T09:06:27.957918Z","shell.execute_reply":"2021-09-19T09:06:27.988381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_baseline_helmets","metadata":{"execution":{"iopub.status.busy":"2021-09-19T11:24:53.394302Z","iopub.execute_input":"2021-09-19T11:24:53.394683Z","iopub.status.idle":"2021-09-19T11:24:53.426774Z","shell.execute_reply.started":"2021-09-19T11:24:53.394648Z","shell.execute_reply":"2021-09-19T11:24:53.426113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train:\n    video_dir = '../input/nfl-health-and-safety-helmet-assignment/train/'\nelse:\n    video_dir = '../input/nfl-health-and-safety-helmet-assignment/test/'\n\nouts = []\nfor myvideo, video_data in tqdm(dbh_high.groupby('video'), total=dbh_high['video'].nunique()):\n    print(f'==== {myvideo} ====')\n    out = deepsort_helmets(video_data, video_dir)        \n    outs.append(out)\ndata_baseline_helmets = pd.concat(outs).copy()\ndata_baseline_helmets = pd.concat([data_baseline_helmets, dbh_low]).sort_values(['video','frame','conf']).reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T09:04:22.460321Z","iopub.execute_input":"2021-09-19T09:04:22.4607Z","iopub.status.idle":"2021-09-19T09:04:22.4902Z","shell.execute_reply.started":"2021-09-19T09:04:22.460659Z","shell.execute_reply":"2021-09-19T09:04:22.48878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cur_id  = 100000\n\ndef id_gen():\n    global cur_id\n    cur_id = cur_id + 1 \n    return cur_id\n\ndata_baseline_helmets.cluster = data_baseline_helmets.cluster.apply(lambda x: id_gen() if x=='UNK' else x)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T09:58:14.726819Z","iopub.execute_input":"2021-09-19T09:58:14.72721Z","iopub.status.idle":"2021-09-19T09:58:14.757834Z","shell.execute_reply.started":"2021-09-19T09:58:14.727173Z","shell.execute_reply":"2021-09-19T09:58:14.756927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_nearest_frame(frame, tracked_helmets: pd.DataFrame):\n    \"\"\" Return the nearest estimated frame in the tracking data \"\"\"\n    \n    available_frames = tracked_helmets.est_frame.unique()\n    shift = min(abs(available_frames - frame))\n    plus_frame = frame + shift\n    minus_frmae = frame - shift\n    nearest_frame = plus_frame if plus_frame in available_frames else minus_frmae\n    return nearest_frame\n\ndef hungarian_matching(baseline_helmets: pd.DataFrame, tracked_helmets : pd.DataFrame, forfit = 10000):\n    \n    track_pts   = list(zip(tracked_helmets['xc'], tracked_helmets['yc']))\n    img_pts     = list(zip(baseline_helmets['x'], baseline_helmets['y']))\n    confidence  = baseline_helmets['conf']\n\n    # Compute matching with Hungarian algorithm in both sides\n    match_cost  = [ [c*distance2(pt1, pt2) for pt1 in track_pts] for (pt2,c) in zip(img_pts, confidence) ]\n    trash_cost  = [ [c*forfit for _ in img_pts] for c in confidence ]\n    cost_matrix = np.concatenate([match_cost, trash_cost], axis = 1) \n    idxs1, idxs2 = scipy.optimize.linear_sum_assignment(cost_matrix)\n    cost  = cost_matrix[idxs1, idxs2].sum()/len(idxs1)\n    try: \n        idxs1, idxs2 = np.array([ [idx1, idx2] for idx1, idx2 in zip(idxs1, idxs2) if idx2 < len(track_pts) ]).transpose()\n    except:\n        idxs1, idxs2 = [], []\n    labels = tracked_helmets.iloc[idxs2].player.tolist().copy()\n    baseline_helmets.loc[:,'label'] = 'UNK'\n    baseline_helmets.iloc[idxs1, baseline_helmets.columns.get_loc('label')] = labels\n\n    return baseline_helmets, cost\n\ndef apply_matrix(M, tracked_helmets):\n    all_src = np.float32([[tracked_helmets['x'] , tracked_helmets['y']]] ).transpose().reshape(-1,1,2)\n    tr_src = cv2.perspectiveTransform(all_src, M).transpose()\n\n    tracked_helmets.loc[:,'xc'] = tr_src[0,0]\n    tracked_helmets.loc[:,'yc'] = tr_src[1,0]\n    return tracked_helmets\n\ndef find_CR(baseline_helmets: pd.DataFrame, tracked_helmets: pd.DataFrame, adapt_to_view = None, use_confidence = True, flip = False):\n    \"\"\" \n    Find Center Reduce matrix \n\n    \n    If adapt_to_view = Endzone  switch x and y coordinates \n    If use_confidence, weight normalisation with confidence score \n    \n    \"\"\"\n    bh = baseline_helmets.copy()\n    th = tracked_helmets.copy()\n    \n    # Center matrix : Align centroid\n    if use_confidence:\n        bh_centroid = np.array([np.average(bh.x, weights = bh.conf), np.average(bh.y, weights = bh.conf)])\n    else:\n        bh_centroid = np.array([np.average(bh.x), np.average(bh.y)])\n    \n    th_centroid = np.array([np.average(th.x), np.average(th.y)])\n    \n    C1 = np.float32([[1 , 0,  -th_centroid[0]],\n                     [0 , 1,  -th_centroid[1]],\n                     [0 , 0,         1      ]])\n    \n    C2 = np.float32([[1 , 0,  bh_centroid[0]],\n                     [0 , 1,  bh_centroid[1]],\n                     [0 , 0,         1      ]])\n    \n    # Reduce matrix: Align lengths \n    \n    bh.loc[:,'d'] = bh.apply(lambda x: math.sqrt( (x.x - bh_centroid[0])**2\n                                           +(x.y - bh_centroid[1])**2 ) , axis = 1)\n    \n    if use_confidence:\n        bh_std = np.average(bh.d, weights = bh.conf)\n    else:\n        bh_std = np.average(bh.d)\n                         \n    \n    th.loc[:,'d'] = th.apply(lambda x: math.sqrt( (x.x - th_centroid[0])**2\n                                           +(x.y - th_centroid[1])**2 ) , axis = 1)\n\n    th_std = np.average(th.d)\n    \n    ratio = bh_std/th_std\n    \n    R = np.float32([[ratio,   0  ,   0],\n                    [0    , ratio,   0],\n                    [0    ,   0  ,   1]])\n    \n    # Adaptation matrix\n    \n    if adapt_to_view == 'Endzone':\n        \"\"\" Switch x and y because for Endzone images, x coordinate almost correspond to y coordinate of the stadium (dillate on x, squeeze on y)\"\"\"\n\n        A = np.float32([[0    ,   1.4 ,  0],\n                        [0.7  ,   0   ,  0],\n                        [0    ,   0   ,  1]])\n        \n    if adapt_to_view == 'Sideline':\n        \n        A = np.float32([[1.4  ,   0    ,  0],\n                        [0    ,   -0.7 ,  0],\n                        [0    ,   0    ,  1]])\n    \n    F = np.float32([[-1  ,   0  ,  0],\n                    [0   ,  -1  ,  0],\n                    [0   ,   0  ,  1]])\n    \n    if not flip:\n        CR = C2  @ A @ R @ C1\n    else:\n        CR = C2 @ F @ A @ R @ C1\n    return CR\n\ndef find_M(baseline_helmets: pd.DataFrame, tracked_helmets: pd.DataFrame):\n    conf_th = 0.7\n    left   = baseline_helmets[['x' ,'y' , 'conf', 'label']].set_index('label')\n    right  = tracked_helmets[['x','y','player']].set_index('player')\n    merged = left.join(right, how = 'inner', rsuffix = '_r')\n    merged = merged[merged['conf'] > conf_th]\n    src = np.float32([ merged['x_r'] , merged['y_r'] ]).transpose().reshape(-1,1,2)\n    dst = np.float32([ merged['x'] , merged['y'] ]).transpose().reshape(-1,1,2)\n    if len(src) == 3:\n        M = cv2.getAffineTransform(src, dst)\n        M = np.vstack((M,[0, 0, 1]))\n    elif len(src) < 3:\n        raise Exception('not enough input pts for homography mapping')\n    else:\n        M, _ = cv2.findHomography(src, dst)\n    return M","metadata":{"execution":{"iopub.status.busy":"2021-09-19T11:25:18.72344Z","iopub.execute_input":"2021-09-19T11:25:18.723977Z","iopub.status.idle":"2021-09-19T11:25:18.757276Z","shell.execute_reply.started":"2021-09-19T11:25:18.723942Z","shell.execute_reply":"2021-09-19T11:25:18.756133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Frame(object):\n    def __init__(self, bh, th, view, M = 'identity', cost = np.inf):\n        self.bh = bh\n        \n        # Transformation matrix from x,y to xc, yc\n        if M == 'identity':\n            self.matrix = np.float32([[1  ,   0  ,  0],\n                                      [0  ,   1  ,  0],\n                                      [0  ,   0  ,  1]])\n        else:\n            self.matrix = M\n        \n        # Add the column xc, yc to tracked helmets\n        self.th          = apply_matrix(self.matrix, th)\n        \n        # Initialise best arguments\n        self.view        = view\n        self.cost        = cost\n        self.best_matrix = self.matrix\n        \n    def projection(self, flip = False):\n        \"\"\" Center reduce matrix \"\"\"\n        \n        CR = find_CR(baseline_helmets = self.bh.copy(), tracked_helmets = self.th.copy(), adapt_to_view = self.view, flip = flip)\n        self.matrix = CR\n        self.th = apply_matrix(CR, self.th.copy())\n        \n    def homography(self, M = None):\n        \"\"\" Apply homography \"\"\"\n        if M is None:\n            try:\n                M = find_M(self.bh.copy(), self.th.copy())\n            except Exception as e:\n                return\n        self.matrix = M\n        self.th = apply_matrix(M, self.th.copy())\n\n    def match(self):\n        bh, cost = hungarian_matching(self.bh.copy(), self.th.copy())\n        if cost < self.cost:\n            bh['map_cost'] = cost\n            self.cost = cost\n            self.bh = bh\n            self.best_matrix = self.matrix\n    \n    # For visualisation \n    def get_bh_xy(self, conf_th = 0):\n        bh = self.bh.query('conf > @conf_th')\n        return bh.x.to_list(), bh.y.to_list()\n    \n    def get_th_xy(self):\n        return self.th.xc.to_list(), self.th.yc.to_list()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-19T11:25:19.2235Z","iopub.execute_input":"2021-09-19T11:25:19.223899Z","iopub.status.idle":"2021-09-19T11:25:19.237577Z","shell.execute_reply.started":"2021-09-19T11:25:19.223864Z","shell.execute_reply":"2021-09-19T11:25:19.236518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\nimport operator\n\ndef most_common(L):\n  # get an iterable of (item, iterable) pairs\n    SL = sorted((x, i) for i, x in enumerate(L))\n  # print 'SL:', SL\n    groups = itertools.groupby(SL, key=operator.itemgetter(0))\n    # auxiliary function to get \"quality\" for an item\n    def _auxfun(g):\n        item, iterable = g\n        count = 0\n        min_index = len(L)\n        for _, where in iterable:\n            count += 1\n            min_index = min(min_index, where)\n        # print 'item %r, count %r, minind %r' % (item, count, min_index)\n        return count, -min_index\n    # pick the highest-count/earliest item\n    return max(groups, key=_auxfun)[0]\n\ndef convolution(v, r):\n    frames = v['frame'].to_list()\n    labels = v['label'].to_list()\n    mc_labels = []\n    scores = []\n    for frame in frames:\n        min_frame = frame - r\n        max_frame = frame + r\n        available_labels = [label for (frame, label) in zip(frames, labels) if min_frame <= frame <= max_frame ]\n        mc = most_common(available_labels)\n        score = available_labels.count(mc)/len(available_labels)\n        mc_labels.append(mc)\n        scores.append(score)\n    v['mc_label'] = mc_labels\n    v['mc_score'] = scores\n    return v\n\nr = 80","metadata":{"execution":{"iopub.status.busy":"2021-09-19T11:25:19.438304Z","iopub.execute_input":"2021-09-19T11:25:19.43868Z","iopub.status.idle":"2021-09-19T11:25:19.449818Z","shell.execute_reply.started":"2021-09-19T11:25:19.438647Z","shell.execute_reply":"2021-09-19T11:25:19.448612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dpt = data_player_tracking.groupby(['playID'])\ndbh = data_baseline_helmets.groupby(['playID', 'view'])\n\nclass MapNtrack(object):\n    def __init__(self, playID, view):\n        # Get a copy of each helmets for this video\n        tracked_helmets  = dpt.get_group(playID).reset_index(drop = True).copy()\n        baseline_helmets = dbh.get_group((playID, view)).reset_index(drop = True).copy()\n        self.baseline_helmets = baseline_helmets\n        self.tracked_helmets = tracked_helmets\n        self.view = view\n        \n        # Group on frame\n        thg = tracked_helmets.groupby('est_frame')\n        bhg = baseline_helmets.groupby('frame')\n        \n        # Create a frame class for each frame\n        thg = tracked_helmets.groupby('est_frame')\n        bhg = baseline_helmets.groupby('frame')\n        self.frames    = []\n        for frameID in self.baseline_helmets.frame.unique():\n            nearest_frameID = get_nearest_frame(frameID, tracked_helmets)\n            th = thg.get_group(nearest_frameID).copy()\n            bh = bhg.get_group(frameID).copy()\n            self.frames.append(Frame(bh, th, view))\n        self.frames_mx = []\n        \n        \n    def animate(self):\n        \"\"\" Animate points \"\"\"\n        # Création de la figure et de l'axe\n        fig, ax = plt.subplots(figsize=(15,15))\n\n        # Création de la ligne qui sera mise à jour au fur et à mesure\n        point_th, = ax.plot([], [], ls=\"none\", marker=\"o\", color = 'blue')\n        point_bh, = ax.plot([], [], ls=\"none\", marker=\"o\", color = 'orange')\n        links     = [ax.plot([], [], color = 'green') for _ in range(22)]\n        # Création de la function qui sera appelée à \"chaque nouvelle image\"\n        def anim(k):\n            i = min(k, len(self.frames))\n            frame = self.frames[k]\n            \n            x,y = frame.get_bh_xy(conf_th = 0.2)\n            point_bh.set_data(x, y)\n            \n            x,y = frame.get_th_xy()\n            point_th.set_data(x, y) \n            \n            if 'label' in frame.bh.columns:\n                left   = frame.bh[['x' ,'y' ,'label']].set_index('label')\n                right  = frame.th[['xc','yc','player']].set_index('player')\n                merged = left.join(right, how = 'inner')\n                pts    = [([row.x, row.xc], [row.y, row.yc]) for _, row in merged.iterrows()]\n                \n                for link in links:\n                    link[0].set_data([], [])\n                for link, pt in zip(links, pts):\n                    link[0].set_data(pt[0], pt[1])\n            \n            return point_bh, point_th\n        \n        #Gestion des limites de la fenêtre\n        ax.set_xlim([-10, 1400])\n        ax.set_ylim([-10, 800])\n        \n        # Génération de l'animation, frames précise les arguments numérique reçus par func (ici animate), \n        ani = animation.FuncAnimation(fig=fig, func=anim, frames=range(len(self.frames)), interval=50, blit=True)\n        return ani\n    \n    # Map procedures\n    def update_map(self):\n        self.baseline_helmets = pd.concat([frame.bh for frame in mnt.frames])\n        self.frames_mx = [frame.best_matrix for frame in mnt.frames]\n    \n    def projection(self, flip = False):\n        \"\"\" Compute transformation on tracking data for each frame \"\"\"\n        for frame in self.frames:\n            frame.projection(flip = flip)\n    \n    def homography(self):\n        \"\"\" Compute transformation on tracking data for each frame \"\"\"\n        for frame in self.frames:\n            frame.homography()\n            \n    def match(self):\n        for frame in self.frames:\n            frame.match()\n        \n    def foward_repair(self):\n        # Apply previous homography on current frame \n        cur_M    = self.frames[0].matrix\n        for frame in self.frames:\n            frame.homography(cur_M)\n            frame.match()\n            frame.homography()\n            frame.match()\n            cur_M = frame.best_matrix\n    \n    def backward_repair(self):\n        # Apply next homography on current frame\n        cur_M    = self.frames[-1].matrix\n        for frame in reversed(self.frames):\n            frame.homography(cur_M)\n            frame.match()\n            frame.homography()\n            frame.match()\n            cur_M = frame.best_matrix\n        \n    # Track procedures # Track on the top occuring label for each cluster\n    def cluster_count_track(self):\n        # Find the top occuring label for each cluster\n        sortlabel_map = self.baseline_helmets.groupby('cluster')['label'].value_counts() \\\n            .sort_values(ascending=False).to_frame() \\\n            .rename(columns={'label':'label_count'}) \\\n            .reset_index() \\\n            .groupby(['cluster']) \\\n            .first()['label'].to_dict()\n        \n        # Find the # of times that label appears for the deepsort_cluster.\n        sortlabelcount_map = self.baseline_helmets.groupby('cluster')['label'].value_counts() \\\n            .sort_values(ascending=False).to_frame() \\\n            .rename(columns={'label':'label_count'}) \\\n            .reset_index() \\\n            .groupby(['cluster']) \\\n            .first()['label_count'].to_dict()\n        \n        # Find the total # of label for each deepsort_cluster.\n        sortlabeltotal_map = self.baseline_helmets.groupby('cluster')['label'].value_counts() \\\n            .sort_values(ascending=False).to_frame() \\\n            .rename(columns={'label':'label_count'}) \\\n            .reset_index() \\\n            .groupby(['cluster']) \\\n            .sum()['label_count'].to_dict()\n        \n        sortlabelconf_map = {k:(sortlabelcount_map[k]/sortlabeltotal_map[k]) for k in sortlabeltotal_map}\n\n        self.baseline_helmets['label_cluster'] = self.baseline_helmets['cluster'].map(sortlabel_map)\n        self.baseline_helmets['cluster_count'] = self.baseline_helmets['cluster'].map(sortlabelcount_map)\n        self.baseline_helmets['cluster_conf'] = self.baseline_helmets['cluster'].map(sortlabelconf_map)\n        \n        # Merge baseline_helmets with the tracking clusters infos\n        for _, example in self.baseline_helmets.groupby('video_frame'):\n            example['cluster_score'] = example.apply(lambda x: x.cluster_count*x.cluster_conf**3, axis = 1)\n            example.sort_values('cluster_score', ascending = False, inplace = True)\n            assigned = set()\n            for idx, row in example.iterrows():\n                if row.label_cluster not in assigned or row.label_cluster == 'UNK':\n                    assigned.add(row.label_cluster)\n                    mnt.baseline_helmets.loc[idx, 'label'] = row.label_cluster\n                elif row.label not in assigned:\n                    assigned.add(row.label)\n                    mnt.baseline_helmets.loc[idx, 'label'] = row.label\n                else:\n                    mnt.baseline_helmets.loc[idx, 'label'] = 'UNK'\n                    \n    def smooth_cluster_track(self):\n        \n        self.baseline_helmets = self.baseline_helmets.groupby('cluster').apply(convolution, r)\n        # Merge baseline_helmets with the tracking clusters infos\n        for _, example in self.baseline_helmets.groupby('video_frame'):\n            example.sort_values('mc_score', ascending = False, inplace = True)\n            assigned = set()\n            for idx, row in example.iterrows():\n                if row.mc_label not in assigned or row.mc_label == 'UNK':\n                    assigned.add(row.mc_label)\n                    mnt.baseline_helmets.loc[idx, 'label'] = row.mc_label\n                elif row.label not in assigned:\n                    assigned.add(row.label)\n                    mnt.baseline_helmets.loc[idx, 'label'] = row.label\n                else:\n                    mnt.baseline_helmets.loc[idx, 'label'] = 'UNK'\n    \n    def update_track(self):\n        # Group on frame\n        thg = self.tracked_helmets.groupby('est_frame')\n        bhg = self.baseline_helmets.groupby('frame')\n        \n        self.frames    = []\n        for frameID in self.baseline_helmets.frame.unique():\n            nearest_frameID = get_nearest_frame(frameID, self.tracked_helmets)\n            th = thg.get_group(nearest_frameID).copy()\n            bh = bhg.get_group(frameID).copy()\n            self.frames.append(Frame(bh, th, self.view))\n            ","metadata":{"execution":{"iopub.status.busy":"2021-09-19T11:25:19.592967Z","iopub.execute_input":"2021-09-19T11:25:19.593402Z","iopub.status.idle":"2021-09-19T11:25:19.65351Z","shell.execute_reply.started":"2021-09-19T11:25:19.593361Z","shell.execute_reply":"2021-09-19T11:25:19.652306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnts = []\nfor playID, view in tqdm(dbh.groups.keys()):\n    mnt = MapNtrack(playID, view)\n    mnt.projection(flip = True)\n    mnt.match()\n    mnt.projection(flip = False)\n    mnt.match()\n    mnt.homography()\n    mnt.match()\n    mnt.foward_repair()\n    mnt.backward_repair()\n    mnt.update_map()\n    mnt.cluster_count_track()\n    mnt.update_track()\n    mnt.homography()\n    mnts.append(mnt)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T11:25:19.735037Z","iopub.execute_input":"2021-09-19T11:25:19.735704Z","iopub.status.idle":"2021-09-19T11:29:33.230747Z","shell.execute_reply.started":"2021-09-19T11:25:19.735666Z","shell.execute_reply":"2021-09-19T11:29:33.22996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnts[2].baseline_helmets","metadata":{"execution":{"iopub.status.busy":"2021-09-19T11:29:33.232057Z","iopub.execute_input":"2021-09-19T11:29:33.232658Z","iopub.status.idle":"2021-09-19T11:29:33.272825Z","shell.execute_reply.started":"2021-09-19T11:29:33.232608Z","shell.execute_reply":"2021-09-19T11:29:33.271574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import HTML\nani = mnts[2].animate()\nHTML(ani.to_html5_video())","metadata":{"execution":{"iopub.status.busy":"2021-09-19T11:29:33.274696Z","iopub.execute_input":"2021-09-19T11:29:33.27502Z","iopub.status.idle":"2021-09-19T11:30:06.339035Z","shell.execute_reply.started":"2021-09-19T11:29:33.27499Z","shell.execute_reply":"2021-09-19T11:30:06.338017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/sample_submission.csv')\nsubmission = pd.concat([mnt.baseline_helmets for mnt in mnts])[ss.columns]","metadata":{"execution":{"iopub.status.busy":"2021-09-19T11:31:03.094771Z","iopub.execute_input":"2021-09-19T11:31:03.09532Z","iopub.status.idle":"2021-09-19T11:31:03.144261Z","shell.execute_reply.started":"2021-09-19T11:31:03.095285Z","shell.execute_reply":"2021-09-19T11:31:03.143455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = submission[submission['label'] != 'UNK']\nsubmission.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T11:31:03.505914Z","iopub.execute_input":"2021-09-19T11:31:03.506412Z","iopub.status.idle":"2021-09-19T11:31:03.709084Z","shell.execute_reply.started":"2021-09-19T11:31:03.506381Z","shell.execute_reply":"2021-09-19T11:31:03.708258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not train:\n    data_labels = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/train_labels.csv')\n    data_labels = data_labels[data_labels['playID'].isin([718, 109, 2798])].copy()\n\nscorer = NFLAssignmentScorer(data_labels)\nscorer.score(submission)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T11:31:04.579412Z","iopub.execute_input":"2021-09-19T11:31:04.579959Z","iopub.status.idle":"2021-09-19T11:31:08.535975Z","shell.execute_reply.started":"2021-09-19T11:31:04.57991Z","shell.execute_reply":"2021-09-19T11:31:08.535143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from helmet_assignment.video import video_with_predictions\nfrom IPython.display import Video, display\n\nsubmission['video'] = submission['video_frame'].str.split('_').str[:3].str.join('_') + '.mp4'\ndebug_videos = submission['video'].unique()\n\n# Create video showing predictions for one of the videos.\nvideo_out = video_with_predictions(\n    f'../input/nfl-health-and-safety-helmet-assignment/train/{debug_videos[2]}',\n    scorer.sub_labels,  freeze_impacts=False)\n\nfrac = 1 # scaling factor for display\ndisplay(Video(data=video_out,\n              embed=True,\n              height=int(720*frac),\n              width=int(1280*frac))\n       )","metadata":{"execution":{"iopub.status.busy":"2021-09-19T09:52:38.583164Z","iopub.execute_input":"2021-09-19T09:52:38.583556Z","iopub.status.idle":"2021-09-19T09:53:02.655609Z","shell.execute_reply.started":"2021-09-19T09:52:38.583523Z","shell.execute_reply":"2021-09-19T09:53:02.653783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission2[[\"left\", \"top\", \"height\", \"width\"]] = submission2[[\"left\", \"top\", \"height\", \"width\"]].astype('int')\nsubmission2.drop_duplicates(subset = ['video_frame', 'left', 'height', 'width', 'top'], inplace=True) \nc1 = (submission2.left >= 0)\nc2 = (submission2.top >= 0)\nc3 = (submission2.left + submission2.width  <= 1280)\nc4 = (submission2.top + submission2.height  <= 720)\nc5 = (submission2.height >= 0)\nc6 = (submission2.width >= 0)\nsubmission2 = submission2[c1 & c2 & c3 & c4 & c5 & c6]\nscorer.score(submission2)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T13:03:01.712339Z","iopub.execute_input":"2021-09-06T13:03:01.712731Z","iopub.status.idle":"2021-09-06T13:03:01.733124Z","shell.execute_reply.started":"2021-09-06T13:03:01.712695Z","shell.execute_reply":"2021-09-06T13:03:01.732383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission2.to_csv(\"submission.csv\", index=False) ","metadata":{"execution":{"iopub.status.busy":"2021-09-06T13:03:01.73426Z","iopub.execute_input":"2021-09-06T13:03:01.734619Z","iopub.status.idle":"2021-09-06T13:03:01.905564Z","shell.execute_reply.started":"2021-09-06T13:03:01.734585Z","shell.execute_reply":"2021-09-06T13:03:01.904757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import subprocess\ndef video_with_predictions(\n    video_path: str, sub_labels: pd.DataFrame, max_frame=9999, freeze_impacts=True,\n    verbose=True\n) -> str:\n    \"\"\"\n    Annotates a video with both the baseline model boxes and ground truth boxes.\n    \"\"\"\n    VIDEO_CODEC = \"MP4V\"\n    HELMET_COLOR = (0, 0, 0)  # Black\n    \n    INCORRECT_IMPACT_COLOR = (0, 0, 255)  # Red\n    CORRECT_IMPACT_COLOR = (51, 255, 255)  # Yellow\n\n    CORRECT_COLOR = (0, 255, 0)  # Green\n    INCORRECT_COLOR = (255, 0, 128)  # Rose\n    WHITE = (255, 255, 255)  # White \n\n    video_name = os.path.basename(video_path).replace(\".mp4\", \"\")\n    if verbose:\n        print(f\"Running for {video_name}\")\n    sub_labels = sub_labels.copy()\n    # Add frame and video columns:\n    sub_labels['video'] = sub_labels['video_frame'].str.split('_').str[:3].str.join('_')\n    sub_labels['frame'] = sub_labels['video_frame'].str.split('_').str[-1].astype('int')\n\n    vidcap = cv2.VideoCapture(video_path)\n    fps = vidcap.get(cv2.CAP_PROP_FPS)/5\n    width = int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    output_path = \"pred_\" + video_name + \".mp4\"\n    tmp_output_path = \"tmp_\" + output_path\n    output_video = cv2.VideoWriter(\n        tmp_output_path, cv2.VideoWriter_fourcc(*VIDEO_CODEC), fps, (width, height)\n    )\n    frame = 0\n    while True:\n        it_worked, img = vidcap.read()\n        if not it_worked:\n            break\n        frame += 1\n\n        img_name = f\"{frame} : {video_name}\"\n        cv2.putText(\n            img,\n            img_name,\n            (5, 20),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.5,\n            WHITE,\n            thickness=1,\n        )\n        \n        cv2.putText(\n            img,\n            str(frame),\n            (1230, 710),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.8,\n            WHITE,\n            thickness=1,\n        )\n        # Get stats about current state in frame\n        stats = sub_labels.query('video == @video and frame <= @frame')\n        correct_nonimp = len(stats.query('weight == 1 and isCorrect'))\n        total_nonimp = len(stats.query('weight == 1'))\n        correct_imp = len(stats.query('weight > 1 and isCorrect'))\n        total_imp = len(stats.query('weight > 1'))\n        correct_weighted = correct_nonimp + (correct_imp * 1000)\n        total_weighted = total_nonimp + (total_imp * 1000)\n        acc_imp = correct_imp/np.max([1, total_imp])\n        acc_nonimp = correct_nonimp/np.max([1, total_nonimp])\n        acc_weighted = correct_weighted/np.max([1, total_weighted])\n        cv2.putText(\n            img,\n            f'{acc_imp:0.4f} Impact Boxes Accuracy :      ({correct_imp}/{total_imp})',\n            (5, 40),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.5,\n            WHITE,\n            thickness=1,\n        )\n\n        cv2.putText(\n            img,\n            f'{acc_nonimp:0.4f} Non-Impact Boxes Accuracy: ({correct_nonimp}/{total_nonimp})',\n            (5, 60),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.5,\n            WHITE,\n            thickness=1,\n        )\n        \n        cv2.putText(\n            img,\n            f'{acc_weighted:0.4f} Weighted Accuracy:     ({correct_weighted}/{total_weighted})',\n            (5, 80),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.5,\n            WHITE,\n            thickness=1,\n        )\n\n        \n        video_frame = f'{video_name}_{frame}' \n        boxes = sub_labels.query(\"video_frame == @video_frame\")\n        if len(boxes) == 0:\n            return\n        for box in boxes.itertuples(index=False):\n            if box.isCorrect and box.weight == 1:\n                # CORRECT\n                box_color = CORRECT_COLOR\n                gt_color = CORRECT_COLOR\n                pred_thickness = 1\n            elif box.isCorrect and box.weight > 1:\n                box_color = CORRECT_IMPACT_COLOR\n                gt_color = CORRECT_IMPACT_COLOR\n                pred_thickness = 3\n            elif (box.isCorrect == False) and (box.weight > 1):\n                box_color = INCORRECT_IMPACT_COLOR\n                gt_color = INCORRECT_IMPACT_COLOR\n                pred_thickness = 3\n            elif (box.isCorrect == False) and (box.weight == 1):                \n                box_color = INCORRECT_COLOR\n                gt_color = HELMET_COLOR\n                pred_thickness = 1\n\n            # Ground Truth Box\n            cv2.rectangle(\n                img,\n                (box.left_gt, box.top_gt),\n                (box.left_gt + box.width_gt, box.top_gt + box.height_gt),\n                gt_color,\n                thickness=1,\n            )\n            # Prediction Box\n            cv2.rectangle(\n                img,\n                (int(box.left_sub), int(box.top_sub)),\n                (int(box.left_sub + box.width_sub), int(box.top_sub + box.height_sub)),\n                box_color,\n                thickness=pred_thickness,\n            )\n\n            cv2.putText(\n                img,\n                f\"{box.label_gt}:{box.label_sub}\",\n                (max(0, box.left_gt - box.width_gt), max(0, box.top_gt - 5)),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.5,\n                WHITE,\n                thickness=1,\n            )\n\n        if boxes['weight'].sum() > 22 and freeze_impacts:\n            for _ in range(60):\n                # Freeze for 60 frames on impacts\n                output_video.write(img)\n        else:\n            output_video.write(img)\n        \n        if frame >= max_frame:\n            break\n        \n    output_video.release()\n    # Not all browsers support the codec, we will re-load the file at tmp_output_path\n    # and convert to a codec that is more broadly readable using ffmpeg\n    if os.path.exists(output_path):\n        os.remove(output_path)\n    subprocess.run(\n        [\n            \"ffmpeg\",\n            \"-i\",\n            tmp_output_path,\n            \"-crf\",\n            \"18\",\n            \"-preset\",\n            \"veryfast\",\n            \"-vcodec\",\n            \"libx264\",\n            output_path,\n        ]\n    )\n    os.remove(tmp_output_path)\n\n    return output_path","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:05:28.132132Z","iopub.execute_input":"2021-09-06T07:05:28.132482Z","iopub.status.idle":"2021-09-06T07:05:28.158736Z","shell.execute_reply.started":"2021-09-06T07:05:28.132442Z","shell.execute_reply":"2021-09-06T07:05:28.157821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def post_process(video_player):\n    video_player[\"frame\"] = video_player['video_frame'].apply(lambda x: int(x.split('_')[-1]))\n    known = video_player.frame.unique()\n    video = video_player.video.unique()[0]\n    label = video_player.label.unique()[0]\n    missing_frames = [frame for frame in range(video_player.frame.min(), video_player.frame.max()) if frame not in known]\n    video_frame = [video.split('.')[0] + \"_\" + str(frame) for frame in missing_frames]\n    missing_df = pd.DataFrame({'video_frame': video_frame, 'label':[label]*len(missing_frames), 'video':[video]*len(missing_frames), 'frame':missing_frames})\n    video_player = pd.concat([missing_df, video_player], ignore_index = True)\n    video_player.sort_values('frame', inplace= True)\n    video_player = video_player.reset_index(drop = True)\n    for feat in ['top','left', 'width', 'height']:\n        r = lowess(video_player[feat], np.arange(video_player.frame.min(), video_player.frame.max()+1), frac=0.15)\n        try:\n            video_player[feat+'_reg'] = scipy.interpolate.interp1d(x = r[:, 0], y = r[:, 1])(video_player.frame)\n        except Exception as e:\n            return video_player\n        video_player[feat] = video_player[feat].fillna(video_player[feat+'_reg'])\n    return video_player\n\nsubmission['video'] = submission['video_frame'].str.split('_').str[:3].str.join('_') + '.mp4'\nsub2 = submission.groupby(['video', 'label']).apply(post_process)\n\nsubmission2 = sub2[ss.columns].reset_index(drop = True)\n\nsubmission2.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T07:05:28.162636Z","iopub.execute_input":"2021-09-06T07:05:28.162991Z","iopub.status.idle":"2021-09-06T07:05:28.171823Z","shell.execute_reply.started":"2021-09-06T07:05:28.162956Z","shell.execute_reply":"2021-09-06T07:05:28.171136Z"},"trusted":true},"execution_count":null,"outputs":[]}]}