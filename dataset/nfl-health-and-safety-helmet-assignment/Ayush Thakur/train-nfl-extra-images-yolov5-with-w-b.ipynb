{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is a starter kernel to train a YOLOv5 model on the extra images based on this [kernel](https://www.kaggle.com/ayuraj/train-covid-19-detection-using-yolov5).\n\n### Why train a YOLOv5 with extra images?\n\nFrom this [discussion post](https://www.kaggle.com/c/nfl-health-and-safety-helmet-assignment/discussion/264154#1467422): \n\n> The helmet predictions we provide in `train_baseline_helmets.csv` and `test_baseline_helmets.csv` are the output of a helmet detection model that was trained on the additional `images/` and labels from `image_labels.csv` (neither part of train or test). The predictions are imperfect but we do provide prediction confidence in the conf column.\n\n* The provided labels in the `image_labels.csv` files are - Helmet, Helmet-Blurred, Helmet-Difficult, Helmet-Sideline, Helmet-Partial. The ground truth lables for bounding boxes in `train_labels.csv` consist only of `Helmet` label. \n* YOLOv5 trained on extra data can be used as a secondary object detector to refine the prediction of a primary object detector trained using frames of the videos.\n\n### What more can this kernel offer?\n\nThe training methodology of this kernel can be extended to improve the `train_baseline_helmets.csv` score.\n\n# üñºÔ∏è What is YOLOv5?\n\nYOLO an acronym for 'You only look once', is an object detection algorithm that divides images into a grid system. Each cell in the grid is responsible for detecting objects within itself.\n\n[Ultralytics' YOLOv5](https://github.com/ultralytics/yolov5) (\"You Only Look Once\") model family enables real-time object detection with convolutional neural networks.\n\n# ü¶Ñ What is Weights and Biases?\n\n<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n<!--- @wandbcode{nfl_extra_yolo, v=2} -->\n\n[Weights & Biases](https://wandb.ai/site) (W&B) is a set of machine learning tools that helps you build better models faster. Check out Experiment Tracking with Weights and Biases to learn more.\nWeights & Biases is directly integrated into YOLOv5, providing experiment metric tracking, model and dataset versioning, rich model prediction visualization, and more. You can learn more about W&B in this [kernel](https://www.kaggle.com/ayuraj/experiment-tracking-with-weights-and-biases).","metadata":{}},{"cell_type":"markdown","source":"# ‚òÄÔ∏è Imports and Setup\n\nAccording to the official Train Custom Data guide, YOLOv5 requires a certain directory structure.\n```\n/parent_folder\n    /dataset\n         /images\n         /labels\n    /yolov5\n```\n    \n* We thus will create a /tmp directory.\n* Download YOLOv5 repository and pip install the required dependencies.\n* Install the latest version of W&B and login with your wandb account. You can create your free W&B account here.\n","metadata":{}},{"cell_type":"code","source":"%cd ../\n!mkdir tmp\n%cd tmp","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-13T04:19:12.898494Z","iopub.execute_input":"2021-08-13T04:19:12.898923Z","iopub.status.idle":"2021-08-13T04:19:13.645881Z","shell.execute_reply.started":"2021-08-13T04:19:12.898835Z","shell.execute_reply":"2021-08-13T04:19:13.644813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download YOLOv5\n!git clone https://github.com/ultralytics/yolov5  # clone repo\n%cd yolov5\n# Install dependencies\n%pip install -qr requirements.txt  # install dependencies\n\n%cd ../\nimport torch\nprint(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")","metadata":{"execution":{"iopub.status.busy":"2021-08-13T04:19:15.614531Z","iopub.execute_input":"2021-08-13T04:19:15.61492Z","iopub.status.idle":"2021-08-13T04:19:28.823822Z","shell.execute_reply.started":"2021-08-13T04:19:15.614871Z","shell.execute_reply":"2021-08-13T04:19:28.822753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### How to login to W&B?\n    \nThere are two ways you can login to W&B in a Kaggle kernel setting:\n\n* Run a cell with `wandb.login()`. It will ask for the API key, which you can copy + paste in. **This is ideal if you Quick Save your kernel**. \n\n* You can also use Kaggle Secrets to store your API key and use the code snippet below to login. If you are not familiar with Kaggle Secrets check this [forum post](https://www.kaggle.com/product-feedback/114053). **This is ideal if you do Run and Save All**. \n\n```\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\n# I have saved my API token with \"wandb_api\" as Label. \n# If you use some other Label make sure to change the same below. \nwandb_api = user_secrets.get_secret(\"wandb_api\") \n\nwandb.login(key=wandb_api)\n```\n\nMore on W&B login [here](https://docs.wandb.ai/ref/cli/wandb-login).","metadata":{}},{"cell_type":"code","source":"# Install W&B \n!pip install -q --upgrade wandb\n\n\n# Login \nimport wandb\nprint(wandb.__version__)\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T04:33:54.599186Z","iopub.execute_input":"2021-08-13T04:33:54.599684Z","iopub.status.idle":"2021-08-13T04:34:24.18096Z","shell.execute_reply.started":"2021-08-13T04:33:54.599647Z","shell.execute_reply":"2021-08-13T04:34:24.180025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> üìç Note: W&B comes pre-installed with Kaggle kernel but to ensure the lastest version of W&B use pip install. ","metadata":{}},{"cell_type":"code","source":"# Necessary/extra dependencies. \nimport os\nimport gc\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom shutil import copyfile\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-08-13T04:34:27.208265Z","iopub.execute_input":"2021-08-13T04:34:27.208717Z","iopub.status.idle":"2021-08-13T04:34:28.34615Z","shell.execute_reply.started":"2021-08-13T04:34:27.208679Z","shell.execute_reply":"2021-08-13T04:34:28.345164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ü¶Ü Hyperparameters","metadata":{}},{"cell_type":"code","source":"%cd ../\nTRAIN_PATH = 'input/nfl-health-and-safety-helmet-assignment/images/'\nIMG_SIZE = 256\nBATCH_SIZE = 16\nEPOCHS = 10\n\nprint(f'Number of extra images: {len(os.listdir(TRAIN_PATH))}') ","metadata":{"execution":{"iopub.status.busy":"2021-08-13T04:34:37.245101Z","iopub.execute_input":"2021-08-13T04:34:37.245486Z","iopub.status.idle":"2021-08-13T04:34:37.587822Z","shell.execute_reply.started":"2021-08-13T04:34:37.245452Z","shell.execute_reply":"2021-08-13T04:34:37.586806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üî® Prepare Dataset\n\nThis is the most important section when it comes to training an object detector with YOLOv5. The directory structure, bounding box format, etc must be in the correct order. This section builds every piece needed to train a YOLOv5 model.\n\n\n* Create train-validation split.\n* Create required /dataset folder structure and more the images to that folder.\n* Create data.yaml file needed to train the model.\n* Create bounding box coordinates in the required YOLO format.","metadata":{}},{"cell_type":"code","source":"# Load image level csv file\nextra_df = pd.read_csv('input/nfl-health-and-safety-helmet-assignment/image_labels.csv')\nprint('Number of ground truth bounding boxes: ', len(extra_df))\n\n# Number of unique labels\nlabel_to_id = {label: i for i, label in enumerate(extra_df.label.unique())}\nprint('Unique labels: ', label_to_id)\n\n# Group together bbox coordinates belonging to the same image. \nimage_bbox_label = {} # key is the name of the image, value is a dataframe with label and bbox coordinates. \nfor image, df in extra_df.groupby('image'): \n    image_bbox_label[image] = df.reset_index(drop=True)\n\n# Visualize\nextra_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T04:34:47.563655Z","iopub.execute_input":"2021-08-13T04:34:47.564039Z","iopub.status.idle":"2021-08-13T04:34:49.556773Z","shell.execute_reply.started":"2021-08-13T04:34:47.564003Z","shell.execute_reply":"2021-08-13T04:34:49.555842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üçò Train-validation split\n\nI am using a naive train-validation split. You can device multiple ways to do so. Some ideas are:\n* Count the number of bounding boxes in each image and label encode it. Use this to create stratified split. \n* Split based on labels. \n* If you want to do a K-fold training this kernel can be easily modified to do the same. Here's a [kernel that showcase K-fold training using YOLOv5](https://www.kaggle.com/ayuraj/train-yolov5-cross-validation-ensemble-w-b). ","metadata":{}},{"cell_type":"code","source":"# Create train and validation split.\ntrain_names, valid_names = train_test_split(list(image_bbox_label), test_size=0.2, random_state=42)\nprint(f'Size of dataset: {len(image_bbox_label)},\\\n       training images: {len(train_names)},\\\n       validation images: {len(valid_names)}')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T04:35:07.927957Z","iopub.execute_input":"2021-08-13T04:35:07.92843Z","iopub.status.idle":"2021-08-13T04:35:07.941012Z","shell.execute_reply.started":"2021-08-13T04:35:07.928367Z","shell.execute_reply":"2021-08-13T04:35:07.937429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üçö Prepare Required Folder Structure\n\nThe required folder structure for the dataset directory is:\n\n```\n/parent_folder\n    /dataset\n         /images\n             /train\n             /val\n         /labels\n             /train\n             /val\n    /yolov5\n```\n\nNote that I have named the directory `nfl_extra`.","metadata":{}},{"cell_type":"code","source":"os.makedirs('tmp/nfl_extra/images/train', exist_ok=True)\nos.makedirs('tmp/nfl_extra/images/valid', exist_ok=True)\n\nos.makedirs('tmp/nfl_extra/labels/train', exist_ok=True)\nos.makedirs('tmp/nfl_extra/labels/valid', exist_ok=True)\n\n# Move the images to relevant split folder.\nfor img_name in tqdm(train_names):\n    copyfile(f'{TRAIN_PATH}/{img_name}', f'tmp/nfl_extra/images/train/{img_name}')\n\nfor img_name in tqdm(valid_names):\n    copyfile(f'{TRAIN_PATH}/{img_name}', f'tmp/nfl_extra/images/valid/{img_name}')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T04:35:10.952936Z","iopub.execute_input":"2021-08-13T04:35:10.953263Z","iopub.status.idle":"2021-08-13T04:36:36.190632Z","shell.execute_reply.started":"2021-08-13T04:35:10.953231Z","shell.execute_reply":"2021-08-13T04:36:36.189634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> üìç Note: We can also do this without copying the files to a different location. To do so use `*.txt` files with image paths. ","metadata":{"execution":{"iopub.status.busy":"2021-08-12T06:20:36.14064Z","iopub.execute_input":"2021-08-12T06:20:36.141017Z","iopub.status.idle":"2021-08-12T06:21:55.691168Z","shell.execute_reply.started":"2021-08-12T06:20:36.140985Z","shell.execute_reply":"2021-08-12T06:21:55.689335Z"}}},{"cell_type":"markdown","source":"\n# üçú Create .YAML file\n\nThe `data.yaml`, is the dataset configuration file that defines\n\n* an \"optional\" download command/URL for auto-downloading,\n* a path to a directory of training images (or path to a *.txt file with a list of training images),\n* a path to a directory of validation images (or path to a *.txt file with a list of validation images),\n* the number of classes,\n* a list of class names\n\n> üìç Important: For extra images dataset, the bounding box in each image can belong to 5 classes. That's why I have used the number of classes, nc to be 5. YOLOv5 automatically handles the images without any bounding box coordinates.\n\n> üìç Note: The `data.yaml` is created in the `yolov5/data` directory as required.","metadata":{}},{"cell_type":"code","source":"# Create .yaml file \nimport yaml\n\ndata_yaml = dict(\n    train = '../nfl_extra/images/train',\n    val = '../nfl_extra/images/valid',\n    nc = 5,\n    names = list(extra_df.label.unique())\n)\n\n# Note that I am creating the file in the yolov5/data/ directory.\nwith open('tmp/yolov5/data/data.yaml', 'w') as outfile:\n    yaml.dump(data_yaml, outfile, default_flow_style=True)\n    \n%cat tmp/yolov5/data/data.yaml","metadata":{"execution":{"iopub.status.busy":"2021-08-13T04:37:59.655821Z","iopub.execute_input":"2021-08-13T04:37:59.656246Z","iopub.status.idle":"2021-08-13T04:38:00.368139Z","shell.execute_reply.started":"2021-08-13T04:37:59.656207Z","shell.execute_reply":"2021-08-13T04:38:00.3671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üçÆ Prepare Bounding Box Coordinated for YOLOv5\n\nFor every image with bounding box(es) a .txt file with the same name as the image will be created in the format shown below:\n\n* One row per object.\n* Each row is `class x_center y_center width height` format.\n* Box coordinates must be in normalized `xywh` format (from 0 - 1). We can normalize by the boxes in pixels by dividing `x_center` and `width` by `image width`, and `y_center` and `height` by `image height`.\n* Class numbers are zero-indexed (start from 0).\n\n> üìç Note: We don't have to remove the images without bounding boxes from the training or validation sets.","metadata":{}},{"cell_type":"code","source":"def get_yolo_format_bbox(img_w, img_h, box):\n    \"\"\"\n    Convert the bounding boxes in YOLO format.\n    \n    Input:\n    img_w - Original/Scaled image width\n    img_h - Original/Scaled image height\n    box - Bounding box coordinates in the format, \"left, width, top, height\"\n    \n    Output:\n    Return YOLO formatted bounding box coordinates, \"x_center y_center width height\".\n    \"\"\"\n    w = box.width # width \n    h = box.height # height\n    xc = box.left + int(np.round(w/2)) # xmin + width/2\n    yc = box.top + int(np.round(h/2)) # ymin + height/2\n\n    return [xc/img_w, yc/img_h, w/img_w, h/img_h] # x_center y_center width height\n    \n# Iterate over each image and write the labels and bbox coordinates to a .txt file. \nfor img_name, df in tqdm(image_bbox_label.items()):\n    # open image file to get the height and width \n    img = cv2.imread(TRAIN_PATH+'/'+img_name)\n    height, width, _ = img.shape \n    \n    # iterate over bounding box df\n    bboxes = []\n    for i in range(len(df)):\n        # get a row\n        box = df.loc[i]\n        # get bbox in YOLO format\n        box = get_yolo_format_bbox(width, height, box)\n        bboxes.append(box)\n    \n    if img_name in train_names:\n        img_name = img_name[:-4]\n        file_name = f'tmp/nfl_extra/labels/train/{img_name}.txt'\n    elif img_name in valid_names:\n        img_name = img_name[:-4]\n        file_name = f'tmp/nfl_extra/labels/valid/{img_name}.txt'\n        \n    with open(file_name, 'w') as f:\n        for i, bbox in enumerate(bboxes):\n            label = label_to_id[df.loc[i].label]\n            bbox = [label]+bbox\n            bbox = [str(i) for i in bbox]\n            bbox = ' '.join(bbox)\n            f.write(bbox)\n            f.write('\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T04:38:03.278456Z","iopub.execute_input":"2021-08-13T04:38:03.279388Z","iopub.status.idle":"2021-08-13T04:41:56.218954Z","shell.execute_reply.started":"2021-08-13T04:38:03.279305Z","shell.execute_reply":"2021-08-13T04:41:56.217919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extra utility function that can be used for inference. \ndef convert_yolo_bbox(img_w, img_h, box):\n    \"\"\"\n    Input:\n    img_w - Original/Scaled image width\n    img_h - Original/Scaled image height\n    box - YOLO formatted bbox coordinates in the format, \"x_center, y_center, width, height\"\n    \n    Output:\n    Return bounding box coordinates in the format, \"left, width, top, height\"\n    \"\"\"\n    xc, yc = int(np.round(box[0]*img_w)), int(np.round(box[1]*img_h))\n    w, h = int(np.round(box[2]*img_w)), int(np.round(box[3]*img_h))\n\n    left = xc - int(np.round(w/2))\n    top = yc - int(np.round(h/2))\n\n    return [left, top, w, h]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-13T04:45:12.770866Z","iopub.execute_input":"2021-08-13T04:45:12.771194Z","iopub.status.idle":"2021-08-13T04:45:12.77763Z","shell.execute_reply.started":"2021-08-13T04:45:12.771163Z","shell.execute_reply":"2021-08-13T04:45:12.776537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üöÖ Train with W&B","metadata":{}},{"cell_type":"code","source":"%cd tmp/yolov5/","metadata":{"execution":{"iopub.status.busy":"2021-08-13T04:45:15.717899Z","iopub.execute_input":"2021-08-13T04:45:15.718235Z","iopub.status.idle":"2021-08-13T04:45:15.725768Z","shell.execute_reply.started":"2021-08-13T04:45:15.718202Z","shell.execute_reply":"2021-08-13T04:45:15.723376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```\n--img {IMG_SIZE} \\ # Input image size.\n--batch {BATCH_SIZE} \\ # Batch size\n--epochs {EPOCHS} \\ # Number of epochs\n--data data.yaml \\ # Configuration file\n--weights yolov5s.pt \\ # Model name\n--save_period 1\\ # Save model after interval\n--project kaggle-siim-covid # W&B project name\n```","metadata":{}},{"cell_type":"code","source":"!python train.py --img 720 \\\n                 --batch 16 \\\n                 --epochs 10 \\\n                 --data data.yaml \\\n                 --weights yolov5s.pt \\\n                 --save_period 1\\\n                 --project nfl-extra","metadata":{"execution":{"iopub.status.busy":"2021-08-13T04:45:28.764696Z","iopub.execute_input":"2021-08-13T04:45:28.765053Z","iopub.status.idle":"2021-08-13T06:56:03.224399Z","shell.execute_reply.started":"2021-08-13T04:45:28.765021Z","shell.execute_reply":"2021-08-13T06:56:03.223371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Interactively debug your model performance.\n\nThe bounding box debugger can help you visually debug your YOLOv5 model performance. \n\n![Animation550.gif](https://i.postimg.cc/KjFYSPsS/Animation550.gif)\n\n## Check out the W&B Dashboard with useful metrics.\n\n### [W&B Dashboard $\\rightarrow$](https://wandb.ai/ayush-thakur/nfl-extra/runs/3fmveagt)\n\n![Animation551.gif](https://i.postimg.cc/Hn49dtYQ/Animation551.gif)","metadata":{}},{"cell_type":"markdown","source":"# üìç Next Steps\n\n* I am using full image resolution but training YOLOv5s. To get the most out of the data train a larger YOLOv5 model.\n* Play with different image sizes. \n* Find the best, hyperparameters using a train-validation split and use that set of hyperparams to train on full dataset. \n* You can ensemble the predictions with the baseline predictions. ","metadata":{}}]}