{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Helmet Mapping + Deepsort\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# Install helmet-assignment helper code\n!pip install ../input/helmet-assignment-helpers/helmet-assignment-main/ > /dev/null 2>&1\nfrom helmet_assignment.score import NFLAssignmentScorer, check_submission\nfrom helmet_assignment.features import add_track_features","metadata":{"execution":{"iopub.status.busy":"2021-10-25T06:48:30.569152Z","iopub.execute_input":"2021-10-25T06:48:30.569455Z","iopub.status.idle":"2021-10-25T06:49:00.777996Z","shell.execute_reply.started":"2021-10-25T06:48:30.56938Z","shell.execute_reply":"2021-10-25T06:49:00.777039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")","metadata":{"execution":{"iopub.status.busy":"2021-10-25T06:49:00.781457Z","iopub.execute_input":"2021-10-25T06:49:00.781745Z","iopub.status.idle":"2021-10-25T06:49:02.20519Z","shell.execute_reply.started":"2021-10-25T06:49:00.781717Z","shell.execute_reply":"2021-10-25T06:49:02.204254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !cp -r ../input/bytetrack/ByteTrack /kaggle/working/\n# %cd /kaggle/working/ByteTrack\n# !pip install -e . --no-deps\n# %cd /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2021-10-25T06:49:02.206984Z","iopub.execute_input":"2021-10-25T06:49:02.207555Z","iopub.status.idle":"2021-10-25T06:49:02.211379Z","shell.execute_reply.started":"2021-10-25T06:49:02.207509Z","shell.execute_reply":"2021-10-25T06:49:02.210552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline helmet mapping\nThis section uses the simple helmet mapping approach from the awesome notebook:\n\nhttps://www.kaggle.com/its7171/nfl-baseline-simple-helmet-mapping","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport itertools\nimport glob\nimport os\nimport cv2\nfrom sklearn.metrics import accuracy_score\nfrom tqdm.auto import tqdm\nfrom multiprocessing import Pool\nfrom matplotlib import pyplot as plt\nfrom sklearn.cluster import KMeans\nimport random","metadata":{"execution":{"iopub.status.busy":"2021-10-25T06:49:06.086481Z","iopub.execute_input":"2021-10-25T06:49:06.086916Z","iopub.status.idle":"2021-10-25T06:49:06.093385Z","shell.execute_reply.started":"2021-10-25T06:49:06.08688Z","shell.execute_reply":"2021-10-25T06:49:06.09239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Settings and loading data\n\nNote I've extracted `max_iter`, `DIG_STEP` and `DIG_MAX` to the top for easy experimentation. I've also modified the code to run in debug mode if running on the public test set.","metadata":{}},{"cell_type":"code","source":"n_test_videos = len(os.listdir('../input/nfl-health-and-safety-helmet-assignment/test/'))\n# Run in debug mode unless during submission\nif n_test_videos == 6:\n    debug = True\nelse:\n    debug = False\n\n# Configurables\nn_debug_samples = 1\nrandom_state = 42\nCONF_THRE = 0.4\nmax_iter = 1000\nDIG_STEP = 3\nDIG_MAX = DIG_STEP*10\n\n# Read in the data.\n\nBASE_DIR = '../input/nfl-health-and-safety-helmet-assignment'\ndebug = False\nlabels = pd.read_csv(f'{BASE_DIR}/train_labels.csv')\nif debug:\n    tracking = pd.read_csv(f'{BASE_DIR}/train_player_tracking.csv')\n    helmets = pd.read_csv(f'{BASE_DIR}/train_baseline_helmets.csv')\nelse:\n    tracking = pd.read_csv(f'{BASE_DIR}/test_player_tracking.csv')\n    helmets_baseline = pd.read_csv(f'{BASE_DIR}/test_baseline_helmets.csv')\n    split_img_path = './test_video_to_jpg/'\n    video_path = '../input/nfl-health-and-safety-helmet-assignment/test'\n    project_name = 'my_yolov5x_test_labels'\n    yolov5_pt_path = '../input/yolo-dataset/yolov5x/best.pt'\n\n    \ntracking = add_track_features(tracking)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T06:49:10.975259Z","iopub.execute_input":"2021-10-25T06:49:10.975585Z","iopub.status.idle":"2021-10-25T06:49:12.477136Z","shell.execute_reply.started":"2021-10-25T06:49:10.975555Z","shell.execute_reply":"2021-10-25T06:49:12.476299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not debug:\n    !pip install ../input/bytetrack/lap-0.4.0/lap-0.4.0\n    !pip install ../input/bytetrack/loguru-0.5.3-py3-none-any.whl\n    !pip install ../input/bytetrack/ninja-1.10.2.2-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl\n    !pip install ../input/bytetrack/thop-0.0.31.post2005241907-py3-none-any.whl\n    !pip install ../input/bytetrack/pycocotools-2.0.2/dist/pycocotools-2.0.2.tar","metadata":{"execution":{"iopub.status.busy":"2021-10-25T06:49:23.92996Z","iopub.execute_input":"2021-10-25T06:49:23.930294Z","iopub.status.idle":"2021-10-25T06:51:47.360439Z","shell.execute_reply.started":"2021-10-25T06:49:23.930263Z","shell.execute_reply":"2021-10-25T06:51:47.359398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not debug:\n    !mkdir {split_img_path}\n    !python ../input/nfl-utils-1/video2images.py --video_list {video_path} --out_dir {split_img_path} > /dev/null 2>&1","metadata":{"execution":{"iopub.status.busy":"2021-10-25T06:52:31.418574Z","iopub.execute_input":"2021-10-25T06:52:31.418954Z","iopub.status.idle":"2021-10-25T06:53:35.407744Z","shell.execute_reply.started":"2021-10-25T06:52:31.418918Z","shell.execute_reply":"2021-10-25T06:53:35.406589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if not debug:\n#     IMG_S = 960\n#     !python ../input/yolo-v5/detect.py --weights {yolov5_pt_path} \\\n#                       --source {split_img_path} \\\n#                       --img {IMG_S} \\\n#                       --save-txt \\\n#                       --save-conf \\\n#                       --nosave \\\n#                       --max-det 22\\\n#                       --project {project_name}\\\n#                       --device 0 > /dev/null 2>&1","metadata":{"execution":{"iopub.status.busy":"2021-10-24T15:47:48.501335Z","iopub.execute_input":"2021-10-24T15:47:48.501678Z","iopub.status.idle":"2021-10-24T15:52:00.849574Z","shell.execute_reply.started":"2021-10-24T15:47:48.501637Z","shell.execute_reply":"2021-10-24T15:52:00.848489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(glob.glob(f'{split_img_path}/*'))","metadata":{"execution":{"iopub.status.busy":"2021-10-24T15:52:00.853171Z","iopub.execute_input":"2021-10-24T15:52:00.853462Z","iopub.status.idle":"2021-10-24T15:52:00.872746Z","shell.execute_reply.started":"2021-10-24T15:52:00.853433Z","shell.execute_reply":"2021-10-24T15:52:00.87184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if not debug:\n#     all_txt = glob.glob(f\"/kaggle/working/{project_name}/exp/labels/*\")\n#     for i in tqdm(all_txt):\n#         with open('train_labels.txt', 'a') as f1:\n#             with open(i, 'r') as f:\n#                 for txt in f.readlines():\n#                     file_name = i.split(\"/\")[6].split(\".\")[0] + txt[1:]\n#                     f1.write(file_name)\n#     df = {\n#     'video_frame': [],\n#     'left': [],\n#     'top': [],\n#     'width': [],\n#     'height': [],\n#     'conf': [], }\n#     with open('train_labels.txt', 'r') as f:\n#         for each_txt in tqdm(f.readlines()):\n#             video_frame, left, top, width, height, conf = each_txt.split()\n#             df['video_frame'].append(video_frame)\n#             w = eval(width) * 1280\n#             h = eval(height) * 720\n#             x = eval(left) * 1280 - w / 2 # \n#             y = eval(top) * 720 - h / 2 # +\n#             df['left'].append(int(x + 0.5))\n#             df['top'].append(int(y + 0.5))\n#             df['width'].append(int(w + 0.5))\n#             df['height'].append(int(h + 0.5))\n#             df['conf'].append(eval(conf))\n#     df_yolov5 = pd.DataFrame(df)\n#     df_yolov5.to_csv(\"train_labels_yolov5.csv\", index=None)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T15:52:00.874133Z","iopub.execute_input":"2021-10-24T15:52:00.874721Z","iopub.status.idle":"2021-10-24T15:52:02.719313Z","shell.execute_reply.started":"2021-10-24T15:52:00.874682Z","shell.execute_reply":"2021-10-24T15:52:02.718452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not debug:\n    !cp -r ../input/bytetrack/YOLOX /kaggle/working/\n    %cd /kaggle/working/YOLOX\n    !pip install -e . --no-deps\n    %cd /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2021-10-25T06:55:37.237229Z","iopub.execute_input":"2021-10-25T06:55:37.237582Z","iopub.status.idle":"2021-10-25T06:56:18.017783Z","shell.execute_reply.started":"2021-10-25T06:55:37.237546Z","shell.execute_reply":"2021-10-25T06:56:18.016799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not debug:\n    !python ../input/bytetrack/YOLOX/tools/demo_helmet.py image \\\n    --path ./test_video_to_jpg \\\n    --ckpt ../input/yolo-dataset/yoloxl/best_ckpt.pth \\\n    -f ../input/bytetrack/yolox_l_mix_det.py \\\n    --device gpu\\\n    --save_result  > /dev/null 2>&1","metadata":{"execution":{"iopub.status.busy":"2021-10-25T06:57:58.921022Z","iopub.execute_input":"2021-10-25T06:57:58.921494Z","iopub.status.idle":"2021-10-25T07:03:01.249987Z","shell.execute_reply.started":"2021-10-25T06:57:58.921446Z","shell.execute_reply":"2021-10-25T07:03:01.248906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if not debug:\n#     !pip install ../input/bytetrack/ensemble_boxes-1.0.7-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2021-10-24T15:57:51.205111Z","iopub.execute_input":"2021-10-24T15:57:51.205564Z","iopub.status.idle":"2021-10-24T15:58:17.377649Z","shell.execute_reply.started":"2021-10-24T15:57:51.20549Z","shell.execute_reply":"2021-10-24T15:58:17.376647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def force_sub_requirements_before_mapping(sub, verbose=True):\n    \"\"\"\n    Enforces the submission submission\n    *Warning* Using this code may remove prediction rows in a sub-optimal manner.\n    \"\"\"\n    len_before = len(sub)\n    sub = sub.drop_duplicates(['video_frame', 'left','width','top','height'])\n#     sub = sub.drop_duplicates(['video_frame', 'label'])\n    # sub = sub.groupby(\"video_frame\").head(22)\n    sub = sub.loc[sub['width'] >= 0]\n    sub = sub.loc[sub['height'] >= 0]\n    sub = sub.loc[(sub['top'] + sub['height']) <= 720]\n    sub = sub.loc[(sub['left'] + sub['width']) <= 1280]\n    sub = sub.reset_index(drop=True)\n    if verbose:\n        len_after = len(sub)\n        n_removed = len_before - len_after\n        print(f'Forcing submission requirements removed {n_removed} rows ({len_before} -> {len_after})')\n    return sub\n","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:08:50.874815Z","iopub.execute_input":"2021-10-25T07:08:50.875287Z","iopub.status.idle":"2021-10-25T07:08:50.883273Z","shell.execute_reply.started":"2021-10-25T07:08:50.875233Z","shell.execute_reply":"2021-10-25T07:08:50.882337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n\n# from tqdm import tqdm\n# if not debug:\n#     from ensemble_boxes import nms, weighted_boxes_fusion\n#     yolox = force_sub_requirements_before_mapping(pd.read_csv('yolox_test_result.csv'))\n#     yolov5 = force_sub_requirements_before_mapping(pd.read_csv('train_labels_yolov5.csv'))\n\n#     yolox ['xmin'] = yolox['left']\n#     yolox ['xmax'] = yolox['left'] + yolox['width']\n#     yolox ['ymin'] = yolox['top']\n#     yolox ['ymax'] = yolox['top'] + yolox['height']\n#     yolov5['xmin'] = yolov5['left']\n#     yolov5['xmax'] = yolov5['left'] + yolox['width']\n#     yolov5['ymin'] = yolov5['top']\n#     yolov5['ymax'] = yolov5['top'] + yolox['height']\n\n#     video_frames = yolox['video_frame'].unique()\n#     sample_video_frames = video_frames[:50]\n#     print(len(video_frames))\n#     print(len(yolov5['video_frame'].unique()))\n\n#     width = 1280\n#     height = 720\n#     iou_thr = 0.5\n\n#     json_list = []\n\n#     for i in tqdm(range(len(video_frames))):\n#         boxes_list = [[], []]\n#         scores_list = [[], []]\n#         labels_list = [[], []]\n#         weights = [1, 1]\n#         # json_item = []\n\n#         # json_item.append([])\n#         cls_yolox  = yolox [yolox ['video_frame'] == video_frames[i]]\n#         cls_yolov5 = yolov5[yolov5['video_frame'] == video_frames[i]]\n#         for k in range(len(cls_yolox)):\n#             boxes_list[0].append([cls_yolox ['xmin'].iloc[k] / (1.0 * width), cls_yolox ['ymin'].iloc[k] / (1.0 * height),\n#                                   cls_yolox ['xmax'].iloc[k] / (1.0 * width), cls_yolox ['ymax'].iloc[k] / (1.0 * height)])\n#             scores_list[0].append(cls_yolox ['conf'].iloc[k])\n#             labels_list[0].append(1)\n#         for k in range(len(cls_yolov5)):\n#             boxes_list[1].append([cls_yolov5['xmin'].iloc[k] / (1.0 * width), cls_yolov5['ymin'].iloc[k] / (1.0 * height),\n#                                   cls_yolov5['xmax'].iloc[k] / (1.0 * width), cls_yolov5['ymax'].iloc[k] / (1.0 * height)])\n#             scores_list[1].append(cls_yolov5['conf'].iloc[k])\n#             labels_list[1].append(1)\n#         if len(cls_yolov5) == 0 and len(cls_yolox) == 0:\n#             continue\n#         elif len(cls_yolov5) == 0 and len(cls_yolox)!= 0:\n#             for bbox, score, label in zip(boxes_list[0], scores_list[0], labels_list[0]):\n#                 json_list.append(\\\n#                     [video_frames[i], round(float(bbox[0] * width), 2), round(float(bbox[1] * height), 2), round(float(bbox[2] * width), 2),\n#                      round(float(bbox[3] * height), 2), float(score)])\n#         elif len(cls_yolov5) != 0 and len(cls_yolox) == 0:\n#             for bbox, score, label in zip(boxes_list[1], scores_list[1], labels_list[1]):\n#                 json_list.append(\n#                     [video_frames[i], round(float(bbox[0] * width), 2), round(float(bbox[1] * height), 2), round(float(bbox[2] * width), 2),\n#                      round(float(bbox[3] * height), 2), float(score)])\n#         else:\n# #             boxes, scores, labels = nms(boxes_list, scores_list, labels_list, weights=weights, iou_thr=0.5)\n#             boxes, scores, labels = weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=weights, iou_thr=iou_thr, skip_box_thr=0.0001,conf_type='avg')\n\n#             for bbox, score, label in zip(boxes, scores, labels):\n#                 # 1 class\n#                 json_list.append(\n#                     [video_frames[i], round(float(bbox[0] * width), 2), round(float(bbox[1] * height), 2), round(float(bbox[2] * width), 2),\n#                      round(float(bbox[3] * height), 2), float(score)])\n    \n\n#     df = pd.DataFrame(json_list, columns=['video_frame', 'xmin', 'ymin', 'xmax', 'ymax', 'conf'])\n\n#     df['left'] = df['xmin'].fillna(0).astype(np.int16)\n#     df['width'] = (df['xmax']-df['xmin']).fillna(0).astype(np.int16)\n#     df['top'] = df['ymin'].fillna(0).astype(np.int16)\n#     df['height'] = (df['ymax']-df['ymin']).fillna(0).astype(np.int16)\n#     df = df.drop(columns=['xmin', 'ymin', 'xmax', 'ymax'])\n#     df = force_sub_requirements_before_mapping(df)\n#     df.to_csv('submission.csv', index=False)\n#     print('\\tEnsemble is OK!')","metadata":{"execution":{"iopub.status.busy":"2021-10-24T16:42:34.040386Z","iopub.execute_input":"2021-10-24T16:42:34.041291Z","iopub.status.idle":"2021-10-24T16:42:50.431126Z","shell.execute_reply.started":"2021-10-24T16:42:34.04119Z","shell.execute_reply":"2021-10-24T16:42:50.421604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('./yolox_test_result.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:08:56.349936Z","iopub.execute_input":"2021-10-25T07:08:56.350249Z","iopub.status.idle":"2021-10-25T07:08:56.606221Z","shell.execute_reply.started":"2021-10-25T07:08:56.350219Z","shell.execute_reply":"2021-10-25T07:08:56.605427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not debug:\n    helmets = df\n    df_list = []\n    helmets = force_sub_requirements_before_mapping(helmets)\n    helmets = helmets[helmets['conf']>0.3].copy()\n    before = len(helmets)\n    helmets_baseline = helmets_baseline[helmets_baseline['conf']>0.3].copy()\n    for frame_name,df in helmets.groupby('video_frame'):\n        if len(df)<=2:\n            df = df.append(helmets_baseline[helmets_baseline['video_frame']==frame_name])\n        df_list.append(df)\n\n    helmets = pd.concat(df_list)\n    helmets = helmets.reset_index(drop=True)\n    print('before fix {}'.format(before),'after fix {}'.format(len(helmets)))\n","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:09:01.540897Z","iopub.execute_input":"2021-10-25T07:09:01.541227Z","iopub.status.idle":"2021-10-25T07:09:02.648587Z","shell.execute_reply.started":"2021-10-25T07:09:01.541196Z","shell.execute_reply":"2021-10-25T07:09:02.647023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_cols(df):\n    df['game_play'] = df['video_frame'].str.split('_').str[:2].str.join('_')\n    if 'video' not in df.columns:\n        df['video'] = df['video_frame'].str.split('_').str[:3].str.join('_') + '.mp4'\n    return df\n\nif debug:\n    helmets = add_cols(helmets)\n    labels = add_cols(labels)\n    # Select `n_debug_samples` worth of videos to debug with\n    sample_videos = labels['video'].drop_duplicates() \\\n        .sample(n_debug_samples, random_state=random_state).tolist()\n    sample_gameplays = ['_'.join(x.split('_')[:2]) for x in sample_videos]\n    tracking = tracking[tracking['game_play'].isin(sample_gameplays)]\n    helmets = helmets[helmets['video'].isin(sample_videos)]\n    labels = labels[labels['video'].isin(sample_videos)]\ntracking.shape, helmets.shape, labels.shape","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-25T07:09:20.62781Z","iopub.execute_input":"2021-10-25T07:09:20.628142Z","iopub.status.idle":"2021-10-25T07:09:20.639276Z","shell.execute_reply.started":"2021-10-25T07:09:20.628112Z","shell.execute_reply":"2021-10-25T07:09:20.638119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_nearest(array, value):\n    value = int(value)\n    array = np.asarray(array).astype(int)\n    idx = (np.abs(array - value)).argmin()\n    return array[idx]\n\ndef norm_arr(a):\n    a = a-a.min()\n    a = a/a.max()\n    return a\n    \ndef dist(a1, a2):\n    return np.linalg.norm(a1-a2)\n\ndef dist_for_different_len(a1, a2):\n    assert len(a1) >= len(a2), f'{len(a1)}, {len(a2)}'\n    len_diff = len(a1) - len(a2)\n    a2 = norm_arr(a2)\n    if len_diff == 0:\n        a1 = norm_arr(a1)\n        return dist(a1,a2), ()\n    else:\n        min_dist = 10000\n        min_detete_idx = None\n        cnt = 0\n        del_list = list(itertools.combinations(range(len(a1)),len_diff))\n        if len(del_list) > max_iter:\n            del_list = random.sample(del_list, max_iter)\n        for detete_idx in del_list:\n            this_a1 = np.delete(a1, detete_idx)\n            this_a1 = norm_arr(this_a1)\n            this_dist = dist(this_a1, a2)\n            #print(len(a1), len(a2), this_dist)\n            if min_dist > this_dist:\n                min_dist = this_dist\n                min_detete_idx = detete_idx\n                \n        return min_dist, min_detete_idx\n        \ndef rotate_arr(u, t, deg=True):\n    if deg == True:\n        t = np.deg2rad(t)\n    R = np.array([[np.cos(t), -np.sin(t)],\n                  [np.sin(t),  np.cos(t)]])\n    return  np.dot(R, u)\n\ndef dist_rot(tracking_df, a2):\n    tracking_df = tracking_df.sort_values('x')\n    x = tracking_df['x']\n    y = tracking_df['y']\n    min_dist = 10000\n    min_idx = None\n    min_x = None\n    for dig in range(-DIG_MAX,DIG_MAX+1,DIG_STEP):\n        arr = rotate_arr(np.array((x,y)), dig)\n        this_dist, this_idx = dist_for_different_len(np.sort(arr[0]), a2)\n        if min_dist > this_dist:\n            min_dist = this_dist\n            min_idx = this_idx\n            min_x = arr[0]\n    tracking_df['x_rot'] = min_x\n    player_arr = tracking_df.sort_values('x_rot')['player'].values\n    players = np.delete(player_arr,min_idx)\n    return min_dist, players\n\n\ndef mapping_df(args):\n    video_frame, df = args\n    gameKey,playID,view,frame = video_frame.split('_')\n    gameKey = int(gameKey)\n    playID = int(playID)\n    frame = int(frame.rstrip('.jpg'))\n    this_tracking = tracking[(tracking['gameKey']==gameKey) & (tracking['playID']==playID)]\n    est_frame = find_nearest(this_tracking.est_frame.values, frame)\n    this_tracking = this_tracking[this_tracking['est_frame']==est_frame]\n    len_this_tracking = len(this_tracking)\n    df['center_h_p'] = (df['left']+df['width']/2).astype(int)\n    df['center_h_m'] = (df['left']+df['width']/2).astype(int)*-1\n    df = df[df['conf']>CONF_THRE].copy()\n    if len(df) > len_this_tracking:\n        df = df.tail(len_this_tracking)\n    df_p = df.sort_values('center_h_p').copy()\n    df_m = df.sort_values('center_h_m').copy()\n    \n    if view == 'Endzone':\n        this_tracking['x'], this_tracking['y'] = this_tracking['y'].copy(), this_tracking['x'].copy()\n    a2_p = df_p['center_h_p'].values\n    a2_m = df_m['center_h_m'].values\n    \n    try:\n        min_dist_p, min_detete_idx_p = dist_rot(this_tracking ,a2_p)\n        min_dist_m, min_detete_idx_m = dist_rot(this_tracking ,a2_m)\n    except IndexError:\n        print('{} has prolem'.format(video_frame))\n        return None      \n    if min_dist_p < min_dist_m:\n        min_dist = min_dist_p\n        min_detete_idx = min_detete_idx_p\n        tgt_df = df_p\n    else:\n        min_dist = min_dist_m\n        min_detete_idx = min_detete_idx_m\n        tgt_df = df_m\n    #print(video_frame, len(this_tracking), len(df), len(df[df['conf']>CONF_THRE]), this_tracking['x'].mean(), min_dist_p, min_dist_m, min_dist)\n    tgt_df['label'] = min_detete_idx\n    return tgt_df[['video_frame','left','width','top','height','label']]\n\np = Pool(processes=4)\nsubmission_df_list = []\ndf_list = list(helmets.groupby('video_frame'))\nwith tqdm(total=len(df_list)) as pbar:\n    for this_df in p.imap(mapping_df, df_list):\n        submission_df_list.append(this_df)\n        pbar.update(1)\np.close()\nsubmission_df_list = [df for df in submission_df_list if  df is not None]\nsubmission_df = pd.concat(submission_df_list)\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-25T07:18:20.386256Z","iopub.execute_input":"2021-10-25T07:18:20.386705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf YOLOX\n!rm -rf my_yolo5x_test_labels\n!rm -rf test_video_to_jpg\n# !rm deepsort.yaml\n# !rm submission-baseline.csv\n# !rm train_labels.txt\n# !rm train_labels_yolov5.csv\n!rm yolox_test_result.csv","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Score the predictions before applying deepsort postprocessing\n\nThe scores are roughly ~0.3, which is similar to the public leaderboard.","metadata":{}},{"cell_type":"code","source":"if debug:\n    scorer = NFLAssignmentScorer(labels)\n    baseline_score = scorer.score(submission_df)\n    print(f\"validation score {baseline_score:0.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deepsort Postprocessing\n\nDeepsort is a popular framework for object tracking within video. \n- [This blog post](https://nanonets.com/blog/object-tracking-deepsort/\n) shows some examples of it being put to use.\n- This notebook shows how to apply deepsort to this helmet dataset: https://www.kaggle.com/s903124/nfl-helmet-with-yolov5-deepsort-starter\n- You can also read the paper for deepsort here: https://arxiv.org/pdf/1703.07402.pdf\n\nThe approach is fairly simple:\n1. Step through each frame in a video and apply the deepsort algorithm. This clusters helmets across frames when it is the same player/helmet.\n2. Group by each of these deepsort clusters - and pick the most common label for that cluster. Then override all of the predictions for that helmet to the same player.","metadata":{}},{"cell_type":"markdown","source":"## Importing Deepsort from dataset\nBecause your submission is not allowed to use internet access, you can reference the deepsort codebase from the attached dataset. Deepsort also has a dependency of `easydict` which I've also added as a dataset.","metadata":{}},{"cell_type":"code","source":"# import sys\n# sys.path.append('../input/easydict-master/easydict-master/')\n# # https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch\n# sys.path.append('../input/yolov5-deepsort-pytorch/Yolov5_DeepSort_Pytorch-master/Yolov5_DeepSort_Pytorch-master/deep_sort_pytorch/')\n# from deep_sort.deep_sort import DeepSort\n# from utils.parser import get_config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deepsort config\n\nDeepsort uses a config yaml file for some settings. These are just the default configs and could be improved.","metadata":{}},{"cell_type":"code","source":"# %%writefile deepsort.yaml\n\n# DEEPSORT:\n#   REID_CKPT: \"../input/yolov5-deepsort-pytorch/ckpt.t7\"\n#   MAX_DIST: 0.2\n#   MIN_CONFIDENCE: 0.3\n#   NMS_MAX_OVERLAP: 0.5\n#   MAX_IOU_DISTANCE: 0.9\n#   MAX_AGE: 15\n#   N_INIT: 1\n#   NN_BUDGET: 30\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \"\"\"\n# Helper functions from yolov5 to plot deepsort labels.\n# \"\"\"\n\n# def compute_color_for_id(label):\n#     \"\"\"\n#     Simple function that adds fixed color depending on the id\n#     \"\"\"\n#     palette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n\n#     color = [int((p * (label ** 2 - label + 1)) % 255) for p in palette]\n#     return tuple(color)\n\n# def plot_one_box(x, im, color=None, label=None, line_thickness=3):\n#     # Plots one bounding box on image 'im' using OpenCV\n#     assert im.data.contiguous, 'Image not contiguous. Apply np.ascontiguousarray(im) to plot_on_box() input image.'\n#     tl = line_thickness or round(0.002 * (im.shape[0] + im.shape[1]) / 2) + 1  # line/font thickness\n#     color = color or [random.randint(0, 255) for _ in range(3)]\n#     c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n#     cv2.rectangle(im, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n#     if label: \n#         tf = max(tl - 1, 1)  # font thickness\n#         t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n#         c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n#         cv2.rectangle(im, c1, c2, color, -1, cv2.LINE_AA)  # filled\n#         cv2.putText(im, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n#     return im","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions to apply deepsort to helmet boxes.\n\nBelow are two functions `deepsort_helmets` which runs deepsort across a video. There is a lot of room for improving this function. The merging of deepsort labels onto the original helmet boxes is currently done in a very crude manner.\n\n`add_deepsort_label_col` mapps the most common label to each deepsort cluster.","metadata":{}},{"cell_type":"code","source":"# def deepsort_helmets(video_data,\n#                      video_dir,\n#                      deepsort_config='deepsort.yaml',\n#                      plot=False,\n#                      plot_frames=[]):\n    \n#     # Setup Deepsort\n#     cfg = get_config()\n#     cfg.merge_from_file(deepsort_config)    \n#     deepsort = DeepSort(cfg.DEEPSORT.REID_CKPT,\n#                         max_dist=cfg.DEEPSORT.MAX_DIST,\n#                         min_confidence=cfg.DEEPSORT.MIN_CONFIDENCE,\n#                         nms_max_overlap=cfg.DEEPSORT.NMS_MAX_OVERLAP,\n#                         max_iou_distance=cfg.DEEPSORT.MAX_IOU_DISTANCE,\n#                         max_age=cfg.DEEPSORT.MAX_AGE,\n#                         n_init=cfg.DEEPSORT.N_INIT,\n#                         nn_budget=cfg.DEEPSORT.NN_BUDGET,\n#                         use_cuda=True)\n    \n#     # Run through frames.\n#     video_data = video_data.sort_values('frame').reset_index(drop=True)\n#     ds = []\n#     for frame, d in tqdm(video_data.groupby(['frame']), total=video_data['frame'].nunique()):\n#         d['x'] = (d['left'] + round(d['width'] / 2))\n#         d['y'] = (d['top'] + round(d['height'] / 2))\n\n#         xywhs = d[['x','y','width','height']].values\n\n#         cap = cv2.VideoCapture(f'{video_dir}/{myvideo}.mp4')\n#         cap.set(cv2.CAP_PROP_POS_FRAMES, frame-1) # optional\n#         success, image = cap.read()\n#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n#         confs = np.ones([len(d),])\n#         clss =  np.zeros([len(d),])\n#         outputs = deepsort.update(xywhs, confs, clss, image)\n\n#         if (plot and frame > cfg.DEEPSORT.N_INIT) or (frame in plot_frames):\n#             for j, (output, conf) in enumerate(zip(outputs, confs)): \n\n#                 bboxes = output[0:4]\n#                 id = output[4]\n#                 cls = output[5]\n\n#                 c = int(cls)  # integer class\n#                 label = f'{id}'\n#                 color = compute_color_for_id(id)\n#                 im = plot_one_box(bboxes, image, label=label, color=color, line_thickness=2)\n#             fig, ax = plt.subplots(figsize=(15, 10))\n#             video_frame = d['video_frame'].values[0]\n#             ax.set_title(f'Deepsort labels: {video_frame}')\n#             plt.imshow(im)\n#             plt.show()\n\n#         preds_df = pd.DataFrame(outputs, columns=['left','top','right','bottom','deepsort_cluster','class'])\n#         if len(preds_df) > 0:\n#             # TODO Fix this messy merge\n#             d = pd.merge_asof(d.sort_values(['left','top']),\n#                               preds_df[['left','top','deepsort_cluster']] \\\n#                               .sort_values(['left','top']), on='left', suffixes=('','_deepsort'),\n#                               direction='nearest')\n#         ds.append(d)\n#     dout = pd.concat(ds)\n#     return dout\n\n# def add_deepsort_label_col(out):\n#     # Find the top occuring label for each deepsort_cluster\n#     sortlabel_map = out.groupby('deepsort_cluster')['label'].value_counts() \\\n#         .sort_values(ascending=False).to_frame() \\\n#         .rename(columns={'label':'label_count'}) \\\n#         .reset_index() \\\n#         .groupby(['deepsort_cluster']) \\\n#         .first()['label'].to_dict()\n#     # Find the # of times that label appears for the deepsort_cluster.\n#     sortlabelcount_map = out.groupby('deepsort_cluster')['label'].value_counts() \\\n#         .sort_values(ascending=False).to_frame() \\\n#         .rename(columns={'label':'label_count'}) \\\n#         .reset_index() \\\n#         .groupby(['deepsort_cluster']) \\\n#         .first()['label_count'].to_dict()\n    \n#     out['label_deepsort'] = out['deepsort_cluster'].map(sortlabel_map)\n#     out['label_count_deepsort'] = out['deepsort_cluster'].map(sortlabelcount_map)\n\n#     return out\n\n# def score_vs_deepsort(myvideo, out, labels):\n#     # Score the base predictions compared to the deepsort postprocessed predictions.\n#     myvideo_mp4 = myvideo + '.mp4'\n#     labels_video = labels.query('video == @myvideo_mp4')\n#     scorer = NFLAssignmentScorer(labels_video)\n#     out_deduped = out.groupby(['video_frame','label']).first().reset_index()\n#     base_video_score = scorer.score(out_deduped)\n    \n#     out_preds = out.drop('label', axis=1).rename(columns={'label_deepsort':'label'})\n#     print(out_preds.shape)\n#     out_preds = out_preds.groupby(['video_frame','label']).first().reset_index()\n#     print(out_preds.shape)\n#     deepsort_video_score = scorer.score(out_preds)\n#     print(f'{base_video_score:0.5f} before --> {deepsort_video_score:0.5f} deepsort')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Apply Deepsort to Baseline Predictions","metadata":{}},{"cell_type":"code","source":"# # Add video and frame columns to submission.\n# submission_df['video'] = submission_df['video_frame'].str.split('_').str[:3].str.join('_')\n# submission_df['frame'] = submission_df['video_frame'].str.split('_').str[-1].astype('int')\n\n# if debug:\n#     video_dir = '../input/nfl-health-and-safety-helmet-assignment/train/'\n# else:\n#     video_dir = '../input/nfl-health-and-safety-helmet-assignment/test/'\n\n# # Loop through test videos and apply. If in debug mode show the score change.\n# out_ds = []\n# outs = []\n# for myvideo, video_data in tqdm(submission_df.groupby('video'), total=submission_df['video'].nunique()):\n#     print(f'==== {myvideo} ====')\n#     if debug:\n#         # Plot deepsort labels when in debug mode.\n#         out = deepsort_helmets(video_data, video_dir, plot_frames=[10, 150, 250])\n#     else:\n#         out = deepsort_helmets(video_data, video_dir)\n#     out_ds.append(out)\n#     out = add_deepsort_label_col(out)\n#     outs.append(out)\n#     if debug:\n#         # Score\n#         score_vs_deepsort(myvideo, out, labels)\n# submission_deepsort = pd.concat(outs).copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check Submission & Save\nFinally we will create a submission file and check that it passes the submission requirements.\nThe steps are:\n1. Drop the `label` and replace with `label_deepsort` predictions.\n2. Remove any duplicate labels within a single video/frame. This is required to meet the submission requirements.\n3. Save the results.","metadata":{}},{"cell_type":"code","source":"# ss = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/sample_submission.csv')\n# # Final Checks\n# submission_deepsort['label_deepsort'] = submission_deepsort['label_deepsort'] \\\n#     .fillna(submission_deepsort['label'])\n# submission_deepsort = submission_deepsort.drop('label', axis=1) \\\n#     .rename(columns={'label_deepsort':'label'})[ss.columns]\n# # Drop duplicate labels\n# submission_deepsort = submission_deepsort.loc[\n#     ~submission_deepsort[['video_frame','label']].duplicated()]\n# check_submission(submission_deepsort)\n\n# submission_deepsort.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Display video showing predictions\n\nLastly, if we want to review our predictions we can create a video to review the predictions using the `video_with_predictions` function from the `helmet_assignment` helper package.","metadata":{}},{"cell_type":"code","source":"# from helmet_assignment.video import video_with_predictions\n# from IPython.display import Video, display\n\n# if debug:\n#     submission_deepsort['video'] = submission_deepsort['video_frame'].str.split('_').str[:3].str.join('_') + '.mp4'\n#     debug_videos = submission_deepsort['video'].unique()\n#     debug_labels = labels.query('video in @debug_videos')\n#     scorer = NFLAssignmentScorer(debug_labels)\n#     scorer.score(submission_deepsort)\n    \n#     # Create video showing predictions for one of the videos.\n#     video_out = video_with_predictions(\n#         f'../input/nfl-health-and-safety-helmet-assignment/train/{debug_videos[0]}',\n#         scorer.sub_labels)\n    \n#     frac = 0.60 # scaling factor for display\n#     display(Video(data=video_out,\n#                   embed=True,\n#                   height=int(720*frac),\n                  width=int(1280*frac))\n           )","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}