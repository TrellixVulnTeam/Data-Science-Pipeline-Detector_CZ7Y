{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Camera-Tracking Matching with Gradient Descent\n\nIf you wanna understand how I came up with this notebook please check the detailed explanation on https://www.kaggle.com/coldfir3/camera-tracking-matching-with-gradient-descent/edit","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom PIL import Image, ImageDraw\nfrom pathlib import Path\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2 as cv\nfrom tqdm.notebook import tqdm\nfrom joblib import Parallel, delayed","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-21T17:21:04.320784Z","iopub.execute_input":"2021-08-21T17:21:04.321385Z","iopub.status.idle":"2021-08-21T17:21:04.382309Z","shell.execute_reply.started":"2021-08-21T17:21:04.321355Z","shell.execute_reply":"2021-08-21T17:21:04.38136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fast_sub = (len(pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/test_baseline_helmets.csv')) == 72386)\nfast_sub","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:23:24.710399Z","iopub.execute_input":"2021-08-21T16:23:24.710666Z","iopub.status.idle":"2021-08-21T16:23:24.835072Z","shell.execute_reply.started":"2021-08-21T16:23:24.710639Z","shell.execute_reply":"2021-08-21T16:23:24.83347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# code from: https://www.kaggle.com/robikscube/nfl-helmet-assignment-getting-started-guide\ndef add_track_features(tracks, fps=59.94, snap_frame=10):\n    \"\"\"\n    Add column features helpful for syncing with video data.\n    \"\"\"\n    tracks = tracks.copy()\n    tracks[\"game_play\"] = (\n        tracks[\"gameKey\"].astype(\"str\")\n        + \"_\"\n        + tracks[\"playID\"].astype(\"str\").str.zfill(6)\n    )\n    tracks[\"time\"] = pd.to_datetime(tracks[\"time\"])\n    snap_dict = (\n        tracks.query('event == \"ball_snap\"')\n        .groupby(\"game_play\")[\"time\"]\n        .first()\n        .to_dict()\n    )\n    tracks[\"snap\"] = tracks[\"game_play\"].map(snap_dict)\n    tracks[\"isSnap\"] = tracks[\"snap\"] == tracks[\"time\"]\n    tracks[\"team\"] = tracks[\"player\"].str[0].replace(\"H\", \"Home\").replace(\"V\", \"Away\")\n    tracks[\"snap_offset\"] = (tracks[\"time\"] - tracks[\"snap\"]).astype(\n        \"timedelta64[ms]\"\n    ) / 1_000\n    # Estimated video frame\n    tracks[\"est_frame\"] = (\n        ((tracks[\"snap_offset\"] * fps) + snap_frame).round().astype(\"int\")\n    )\n    return tracks\n\ndef add_video_features(videos):\n    videos['game_play'] = videos['video_frame'].apply(lambda x: '_'.join(x.split('_')[:2]))\n    videos['camera'] = videos['video_frame'].apply(lambda x: x.split('_')[2])\n    videos['frame'] = videos['video_frame'].apply(lambda x: x.split('_')[-1])\n    videos['xc'] = (videos['left'] + videos['width']/2).astype(int).values\n    videos['yc'] = (videos['top'] + videos['height']/2).astype(int).values\n    return videos\n\n# TODO, add interpolation of tracking_df and replace nearest\nclass get_keypoints():\n    \n    def __init__(self, video_df = None, track_df = None):\n        if video_df is None:\n            video_df = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/test_baseline_helmets.csv')\n            self.video_df = add_video_features(video_df)\n        if track_df is None:\n            tracking_df = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/test_player_tracking.csv')\n            tracking_df = add_track_features(tracking_df)\n            self.tracking_df = tracking_df.query(\"est_frame > 0\")\n            \n    def __call__(self, game_play, frame, min_conf = 0.6, topk = 22, normalized = True, debug = False):\n        \n        kpS = self.video_df.query(\n            f\"game_play == '{game_play}' and frame == '{frame}' and camera == 'Sideline' and conf > {min_conf}\").nlargest(topk, 'conf')\n        kpE = self.video_df.query(\n            f\"game_play == '{game_play}' and frame == '{frame}' and camera == 'Endzone'and conf > {min_conf}\").nlargest(topk, 'conf')\n        \n        keypoints = dict()\n        keypoints['Sideline'] = kpS[['xc', 'yc']].values\n        keypoints['Endzone'] = kpE[['xc', 'yc']].values\n        frames = self.tracking_df.query(\n            f\"game_play == '{game_play}'\")['est_frame'].unique()\n        if frame not in frames:\n            index = (np.absolute(frames-frame)).argmin()\n            frame = frames[index]\n        keypoints['Tracking'] = self.tracking_df.query(\n            f\"game_play == '{game_play}' and est_frame == {frame}\")[['x', 'y']].values\n        if debug: print(keypoints)\n        if normalized:\n            for k, v in keypoints.items():\n                if len(v)> 0:\n                    keypoints[k] = (v - v.min(axis = 0)) / (v.max(axis = 0) - v.min(axis = 0))\n                \n        keypoints['Sideline'][:,1] = 1-keypoints['Sideline'][:,1]\n        \n        keypoints['Players'] = self.tracking_df.query(\n            f\"game_play == '{game_play}' and est_frame == {frame}\")['player'].values\n        \n        keypoints['BBoxes'] = {'Sideline':kpS,'Endzone':kpE}\n                \n        self.keypoints = keypoints\n            \n        return keypoints\n    \n    def plot(self, add_no = False):\n        if not hasattr(self, 'keypoints'):\n            print('you must run the function first...')\n        else:\n            kp = self.keypoints\n            plt.figure(figsize=(12, 6))\n            plt.scatter(kp['Endzone'][:,0], kp['Endzone'][:,1], marker = 'x', color = 'red')\n            plt.scatter(kp['Sideline'][:,0], kp['Sideline'][:,1], marker = '^', color = 'red')\n            plt.scatter(kp['Tracking'][:,0], kp['Tracking'][:,1], marker = 'o', color = 'green')  \n            if add_no:\n                for i in range(len(kp['Tracking'][:,0])):\n                    plt.annotate(i, (kp['Tracking'][i,0], kp['Tracking'][i,1]))\n                for i in range(len(kp['Sideline'][:,0])):\n                    plt.annotate(i, (kp['Sideline'][i,0], kp['Sideline'][i,1]))\n    \nget_kp = get_keypoints()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T17:19:49.368929Z","iopub.execute_input":"2021-08-21T17:19:49.369188Z","iopub.status.idle":"2021-08-21T17:19:49.623143Z","shell.execute_reply.started":"2021-08-21T17:19:49.369166Z","shell.execute_reply":"2021-08-21T17:19:49.622135Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Keypoint matching using Pytorch","metadata":{}},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:23:25.333401Z","iopub.execute_input":"2021-08-21T16:23:25.333733Z","iopub.status.idle":"2021-08-21T16:23:26.360712Z","shell.execute_reply.started":"2021-08-21T16:23:25.333697Z","shell.execute_reply":"2021-08-21T16:23:26.359907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def min_mse(preds, targets):\n    d = torch.cdist(preds.squeeze(2), targets.squeeze(2))\n    loss = (d.min(dim = 1).values**2).mean().sqrt()\n    return loss","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:23:26.361855Z","iopub.execute_input":"2021-08-21T16:23:26.362195Z","iopub.status.idle":"2021-08-21T16:23:26.367515Z","shell.execute_reply.started":"2021-08-21T16:23:26.362159Z","shell.execute_reply":"2021-08-21T16:23:26.366523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def step(src, trg, m, lr = 3e-3, prt = True):\n    preds = torch.matmul(m, src) # Homography transform\n    loss = min_mse(preds, trg)   # mse between the closes pair of points\n    if prt: print(f'loss: {(loss.item()):.5f}')\n    loss.backward()\n    m.data -= lr * m.grad.data\n    m.grad = None","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:23:26.36902Z","iopub.execute_input":"2021-08-21T16:23:26.369538Z","iopub.status.idle":"2021-08-21T16:23:26.38347Z","shell.execute_reply.started":"2021-08-21T16:23:26.369502Z","shell.execute_reply":"2021-08-21T16:23:26.382417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_predict(src, trg, init_rot = 0, init_scale = [1,1,1], lr = 3e-3, n_steps = 1000, verbose = True):\n    t = np.pi * init_rot / 180\n    m_rot = torch.tensor([[np.cos(t),-np.sin(t), 0],\n                          [np.sin(t), np.cos(t), 0],\n                          [        0,         0, 1]], dtype = torch.double)\n    m_scale = torch.tensor([[init_scale[0], 0, 0],\n                            [0, init_scale[0], 0],\n                            [0, 0, init_scale[0]]], dtype = torch.double)\n    m = m_scale @ m_rot\n    m.requires_grad_()\n    for i in range(n_steps): \n        if not (i % (n_steps//10)) and verbose:\n            step(src, trg, m, lr=lr)\n        else:\n            step(src, trg, m, lr=lr, prt=False)\n            \n    with torch.no_grad():\n        tfm = torch.matmul(m, src)\n        \n    if verbose:\n        plt.scatter(src[:,0], src[:,1], marker = 'o', color = 'red', label = 'source')\n        plt.scatter(trg[:,0], trg[:,1], marker = '^', color = 'green', label = 'target')  \n        plt.scatter(tfm[:,0], tfm[:,1], marker = 'o', color = 'blue', label = 'result')\n        plt.legend();\n        \n    return tfm","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:23:26.3844Z","iopub.execute_input":"2021-08-21T16:23:26.384597Z","iopub.status.idle":"2021-08-21T16:23:26.398165Z","shell.execute_reply.started":"2021-08-21T16:23:26.384576Z","shell.execute_reply":"2021-08-21T16:23:26.396857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def matching(tfm, trg, players):\n\n    d = torch.cdist(tfm[:,:2,0], trg[:,:2,0])\n    \n    greedy_order = d.min(axis = 1).values.argsort()\n    players=players[greedy_order]\n    d = d[greedy_order]\n    players_matched = []\n    for ix, p in enumerate(players):\n        iy = d[ix].argmin().item()\n        players_matched.append(iy)\n        d[:,iy] = np.Inf\n        if (d == np.Inf).all():\n            break\n\n    return players[torch.tensor(players_matched).argsort()]","metadata":{"execution":{"iopub.status.busy":"2021-08-21T17:06:09.333437Z","iopub.execute_input":"2021-08-21T17:06:09.333685Z","iopub.status.idle":"2021-08-21T17:06:09.33992Z","shell.execute_reply.started":"2021-08-21T17:06:09.333663Z","shell.execute_reply":"2021-08-21T17:06:09.338718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def end2end_prediction(video_frame):\n    \n    game, play, camera, frame = video_frame.split('_')\n    game_play = '_'.join([game, play])\n    frame = int(frame)\n    k = get_kp(game_play, frame)\n    src = torch.cat([torch.tensor(k['Tracking']), torch.ones(len(k['Tracking'])).unsqueeze(1)], axis = -1).unsqueeze(2)\n    trg = torch.cat([torch.tensor(k[camera]), torch.ones(len(k[camera])).unsqueeze(1)], axis = -1).unsqueeze(2)\n    tfm = fit_predict(src, trg, verbose = False)\n    \n    lbls = matching(tfm, trg, k['Players'])\n\n    pred = k[\"BBoxes\"][camera][['video_frame','left','width','top','height']].copy()\n#     print(pred)\n#     print(lbls)\n    pred['label'] = lbls\n    return pred","metadata":{"execution":{"iopub.status.busy":"2021-08-21T17:06:09.484992Z","iopub.execute_input":"2021-08-21T17:06:09.485321Z","iopub.status.idle":"2021-08-21T17:06:09.494667Z","shell.execute_reply.started":"2021-08-21T17:06:09.485293Z","shell.execute_reply":"2021-08-21T17:06:09.494083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_submission(sub):\n    # Maximum of 22 boxes per frame.\n    max_box_per_frame = sub.groupby([\"video_frame\"])[\"label\"].count().max()\n    if max_box_per_frame > 22:\n        print(\"Has more than 22 boxes in a single frame\")\n        return False\n    # Only one label allowed per frame.\n    has_duplicate_labels = sub[[\"video_frame\", \"label\"]].duplicated().any()\n    if has_duplicate_labels:\n        print(\"Has duplicate labels\")\n        return False\n    # Check for unique boxes\n    has_duplicate_boxes = (\n        sub[[\"video_frame\", \"left\", \"width\", \"top\", \"height\"]].duplicated().any()\n    )\n    if has_duplicate_boxes:\n        print(\"Has duplicate boxes\")\n        return False\n    return True","metadata":{"execution":{"iopub.status.busy":"2021-08-21T17:06:15.002536Z","iopub.execute_input":"2021-08-21T17:06:15.002933Z","iopub.status.idle":"2021-08-21T17:06:15.008266Z","shell.execute_reply.started":"2021-08-21T17:06:15.002909Z","shell.execute_reply":"2021-08-21T17:06:15.007124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if fast_sub:\n    print(end2end_prediction('57906_000718_Sideline_1'))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T17:06:16.367762Z","iopub.execute_input":"2021-08-21T17:06:16.368163Z","iopub.status.idle":"2021-08-21T17:06:16.372421Z","shell.execute_reply.started":"2021-08-21T17:06:16.368133Z","shell.execute_reply":"2021-08-21T17:06:16.371502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get predictions","metadata":{}},{"cell_type":"code","source":"# sample_sub = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/sample_submission.csv')\nvideo_frames = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/test_baseline_helmets.csv')['video_frame'].unique()\nif fast_sub:\n    video_frames = video_frames[:16]","metadata":{"execution":{"iopub.status.busy":"2021-08-21T17:06:24.654495Z","iopub.execute_input":"2021-08-21T17:06:24.654838Z","iopub.status.idle":"2021-08-21T17:06:24.751838Z","shell.execute_reply.started":"2021-08-21T17:06:24.65479Z","shell.execute_reply":"2021-08-21T17:06:24.751002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds = []\n# for video_frame in tqdm(video_frames):\n#     preds.append(end2end_prediction(video_frame))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = Parallel(n_jobs=4)(delayed(end2end_prediction)(x) for x in tqdm(video_frames))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T17:22:14.899375Z","iopub.execute_input":"2021-08-21T17:22:14.899807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.concat(preds).drop_duplicates(subset=['video_frame', 'label']).reset_index(drop = True)\nsub_ok = check_submission(submission)\nif sub_ok:\n    print('Submission passed, saving it now to submission.csv file...')\n    submission.to_csv('submission.csv', index = False)\nelse:\n    print('Submission FAILED')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}