{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook I will share one idea to merging traking data with sideline helmet label information.\n\nMain approch is that if we can find specific 4 pair points(cx, cy) which is matching with `tracking images` & `side or endline images`, we can find homography `H` for Perspective Transformation.\n\nin this notebook I'll use field line numbers to find homography `H` between `tracking images` and `sideline images`\n\nReference: \n- https://www.kaggle.com/robikscube/nfl-helmet-assignment-getting-started-guide\n- https://www.kaggle.com/c/nfl-health-and-safety-helmet-assignment/discussion/264361#1467283\n- https://www.kaggle.com/coldfir3/camera-tracking-matching-with-gradient-descent\n- https://www.kaggle.com/go5kuramubon/merge-label-and-tracking-data","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install imageio-ffmpeg","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:03:11.267901Z","iopub.execute_input":"2021-08-24T14:03:11.26829Z","iopub.status.idle":"2021-08-24T14:03:21.919561Z","shell.execute_reply.started":"2021-08-24T14:03:11.26826Z","shell.execute_reply":"2021-08-24T14:03:21.918618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport imageio\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nfrom tqdm.auto import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:32:32.33964Z","iopub.execute_input":"2021-08-24T14:32:32.34019Z","iopub.status.idle":"2021-08-24T14:32:32.344764Z","shell.execute_reply.started":"2021-08-24T14:32:32.340156Z","shell.execute_reply":"2021-08-24T14:32:32.343995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"## https://www.kaggle.com/go5kuramubon/merge-label-and-tracking-data\n\n# Read in data files\nBASE_DIR = '../input/nfl-health-and-safety-helmet-assignment'\n\n# Labels and sample submission\nlabels = pd.read_csv(f'{BASE_DIR}/train_labels.csv')\nss = pd.read_csv(f'{BASE_DIR}/sample_submission.csv')\n\n# Player tracking data\ntr_tracking = pd.read_csv(f'{BASE_DIR}/train_player_tracking.csv')\nte_tracking = pd.read_csv(f'{BASE_DIR}/test_player_tracking.csv')\n\n# Baseline helmet detection labels\ntr_helmets = pd.read_csv(f'{BASE_DIR}/train_baseline_helmets.csv')\nte_helmets = pd.read_csv(f'{BASE_DIR}/test_baseline_helmets.csv')\n\n# Extra image labels\nimg_labels = pd.read_csv(f'{BASE_DIR}/image_labels.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T13:50:13.387029Z","iopub.execute_input":"2021-08-24T13:50:13.38767Z","iopub.status.idle":"2021-08-24T13:50:16.292279Z","shell.execute_reply.started":"2021-08-24T13:50:13.387638Z","shell.execute_reply":"2021-08-24T13:50:16.291493Z"},"_kg_hide-output":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##https://www.kaggle.com/robikscube/nfl-helmet-assignment-getting-started-guide\n\ndef add_track_features(tracks, fps=59.94, snap_frame=10):\n    \"\"\"\n    Add column features helpful for syncing with video data.\n    \"\"\"\n    tracks = tracks.copy()\n    tracks[\"game_play\"] = (\n        tracks[\"gameKey\"].astype(\"str\")\n        + \"_\"\n        + tracks[\"playID\"].astype(\"str\").str.zfill(6)\n    )\n    tracks[\"time\"] = pd.to_datetime(tracks[\"time\"])\n    snap_dict = (\n        tracks.query('event == \"ball_snap\"')\n        .groupby(\"game_play\")[\"time\"]\n        .first()\n        .to_dict()\n    )\n    tracks[\"snap\"] = tracks[\"game_play\"].map(snap_dict)\n    tracks[\"isSnap\"] = tracks[\"snap\"] == tracks[\"time\"]\n    tracks[\"team\"] = tracks[\"player\"].str[0].replace(\"H\", \"Home\").replace(\"V\", \"Away\")\n    tracks[\"snap_offset\"] = (tracks[\"time\"] - tracks[\"snap\"]).astype(\n        \"timedelta64[ms]\"\n    ) / 1_000\n    # Estimated video frame\n    tracks[\"est_frame\"] = (\n        ((tracks[\"snap_offset\"] * fps) + snap_frame).round().astype(\"int\")\n    )\n    return tracks\n\n\ntr_tracking = add_track_features(tr_tracking)\nte_tracking = add_track_features(te_tracking)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T13:50:29.8591Z","iopub.execute_input":"2021-08-24T13:50:29.859423Z","iopub.status.idle":"2021-08-24T13:50:31.641504Z","shell.execute_reply.started":"2021-08-24T13:50:29.859397Z","shell.execute_reply":"2021-08-24T13:50:31.640782Z"},"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def merge_label_and_tracking(tracking_df, label_df):\n\n    tracking_with_game_index = tracking_df.set_index([\"gameKey\", \"playID\", \"player\"])\n\n    df_list = []\n\n    for key, _label_df in tqdm(label_df.groupby([\"gameKey\", \"playID\", \"view\", \"label\"])):\n        # skip because there are sideline player\n        if key[3] == \"H00\" or key[3] == \"V00\":\n            continue\n\n        tracking_data = tracking_with_game_index.loc[(key[0], key[1], key[3])]\n        _label_df = _label_df.sort_values(\"frame\")\n\n        # merge with frame and est_frame\n        merged_df = pd.merge_asof(\n            _label_df,\n            tracking_data,\n            left_on=\"frame\",\n            right_on=\"est_frame\",\n            direction='nearest',\n        )\n        df_list.append(merged_df)\n\n    all_merged_df = pd.concat(df_list)\n    all_merged_df = all_merged_df.sort_values([\"video_frame\", \"label\"], ignore_index=True)\n    \n    return all_merged_df","metadata":{"execution":{"iopub.status.busy":"2021-08-24T13:50:51.173698Z","iopub.execute_input":"2021-08-24T13:50:51.174062Z","iopub.status.idle":"2021-08-24T13:50:51.182195Z","shell.execute_reply.started":"2021-08-24T13:50:51.174026Z","shell.execute_reply":"2021-08-24T13:50:51.181199Z"},"jupyter":{"source_hidden":true},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df = merge_label_and_tracking(tr_tracking, labels)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T13:50:55.314826Z","iopub.execute_input":"2021-08-24T13:50:55.315356Z","iopub.status.idle":"2021-08-24T13:51:25.673826Z","shell.execute_reply.started":"2021-08-24T13:50:55.315324Z","shell.execute_reply":"2021-08-24T13:51:25.672858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_gameKeys = merged_df.gameKey.unique()\ncheck_frame = 1\nhomography_df = merged_df[(merged_df.gameKey == unique_gameKeys[0]) & (merged_df.frame == check_frame) & (merged_df.view =='Sideline')].copy()\nhomography_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:03:46.940386Z","iopub.execute_input":"2021-08-24T14:03:46.940758Z","iopub.status.idle":"2021-08-24T14:03:47.124882Z","shell.execute_reply.started":"2021-08-24T14:03:46.940724Z","shell.execute_reply":"2021-08-24T14:03:47.123769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PerspectiveTransform\n\nIf we know matched Keypoints in the images, we can find homography `H` using `cv2.findHomography`. \n\nbelow code show how we can transform sideline helmet boxes to tracking data scale.","metadata":{}},{"cell_type":"code","source":"trakcing_coordinate = np.float32(list(zip(homography_df['x'],53.33-homography_df['y']))).reshape(-1,1,2)\nlabel_coordinate =  np.float32(list(zip(homography_df['left']+homography_df['width']/2,homography_df['top']-homography_df['height']/2))).reshape(-1,1,2)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:07:59.03303Z","iopub.execute_input":"2021-08-24T14:07:59.033605Z","iopub.status.idle":"2021-08-24T14:07:59.041631Z","shell.execute_reply.started":"2021-08-24T14:07:59.033572Z","shell.execute_reply":"2021-08-24T14:07:59.040857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"H, mask = cv2.findHomography(label_coordinate, trakcing_coordinate)\ntransformed_coordinate =  cv2.perspectiveTransform(label_coordinate, H)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:09:30.610637Z","iopub.execute_input":"2021-08-24T14:09:30.610963Z","iopub.status.idle":"2021-08-24T14:09:30.615169Z","shell.execute_reply.started":"2021-08-24T14:09:30.610935Z","shell.execute_reply":"2021-08-24T14:09:30.614438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(H)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:09:31.200414Z","iopub.execute_input":"2021-08-24T14:09:31.200943Z","iopub.status.idle":"2021-08-24T14:09:31.206593Z","shell.execute_reply.started":"2021-08-24T14:09:31.200896Z","shell.execute_reply":"2021-08-24T14:09:31.205533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\n\nplt.scatter(transformed_coordinate[:, :, 0],transformed_coordinate[:, :, 1])\n\nplt.scatter(homography_df['x'], 53.33-homography_df['y'])\n\nplt.legend(['Transformed coordinate from Sideline helmet box','Ground truth tracking data'])","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:09:31.661183Z","iopub.execute_input":"2021-08-24T14:09:31.661538Z","iopub.status.idle":"2021-08-24T14:09:31.898982Z","shell.execute_reply.started":"2021-08-24T14:09:31.661508Z","shell.execute_reply":"2021-08-24T14:09:31.897876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"But important thing is that we can't match each keypoints exactly becaues we don't have a enough information to find homography.\n\nIf we can match specific pair points with tracking images, side & endzone images, we might find good homography.\n\nI'll use filed line numbers to match both images","metadata":{}},{"cell_type":"markdown","source":"## Video to Frame","metadata":{}},{"cell_type":"code","source":"video_name = homography_df.video.unique()\nvideo_path = f\"{BASE_DIR}/train/{video_name[0]}\"\n\nvid = imageio.get_reader(video_path, 'ffmpeg')\nimg = vid.get_data(check_frame - 1)\nplt.figure(figsize=(12, 10))\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:46:25.695956Z","iopub.execute_input":"2021-08-24T14:46:25.696384Z","iopub.status.idle":"2021-08-24T14:46:26.324013Z","shell.execute_reply.started":"2021-08-24T14:46:25.696349Z","shell.execute_reply":"2021-08-24T14:46:26.323015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finding filed line number points in sideline","metadata":{}},{"cell_type":"code","source":"line_numbers = [[110, 600],  ## Home Sideline 20\n                [550, 630],  ## Home Sideline 30\n                [990, 680],  ## Home Sideline 40\n                [1150, 200], ## Victory Sideline 40\n                [770, 200]]  ## Victory Sideline 30\nfor line_number in line_numbers:\n    img = cv2.circle(img, (line_number[0],line_number[1]), radius=2, color=(0, 255, 255), thickness=10)\n\nplt.figure(figsize=(12, 10))\nplt.imshow(img)    ","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:46:27.424549Z","iopub.execute_input":"2021-08-24T14:46:27.424921Z","iopub.status.idle":"2021-08-24T14:46:27.92337Z","shell.execute_reply.started":"2021-08-24T14:46:27.424884Z","shell.execute_reply":"2021-08-24T14:46:27.922229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ","metadata":{}},{"cell_type":"markdown","source":"## Finding filed line number points in tracking data\n![images](https://drive.google.com/uc?export=view&id=1IdUQHo9G673ifp-mIrwiG_ep0q88H13N)\n\nIf we treat tracking `x`, `y`  as pair points in this images, we can guess that Finding filed line number points.","metadata":{}},{"cell_type":"code","source":"projection_numbers = [[30, 53.3-10], ## Home Sideline 20\n                      [40, 53.3-10], ## Home Sideline 30\n                      [50, 53.3-10], ## Home Sideline 40\n                      [50, 10],      ## Victory Sideline 40\n                      [40, 10]]      ## Victory Sideline 30","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:50:38.630485Z","iopub.execute_input":"2021-08-24T14:50:38.630898Z","iopub.status.idle":"2021-08-24T14:50:38.636521Z","shell.execute_reply.started":"2021-08-24T14:50:38.630858Z","shell.execute_reply":"2021-08-24T14:50:38.635365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"H, mask = cv2.findHomography(np.float32(line_numbers).reshape(5, 2), np.float32(projection_numbers).reshape(5, 2))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:50:39.820665Z","iopub.execute_input":"2021-08-24T14:50:39.821053Z","iopub.status.idle":"2021-08-24T14:50:39.826055Z","shell.execute_reply.started":"2021-08-24T14:50:39.820994Z","shell.execute_reply":"2021-08-24T14:50:39.825183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(H)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:50:40.044582Z","iopub.execute_input":"2021-08-24T14:50:40.04496Z","iopub.status.idle":"2021-08-24T14:50:40.050807Z","shell.execute_reply.started":"2021-08-24T14:50:40.044924Z","shell.execute_reply":"2021-08-24T14:50:40.049599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformed_coordinate =  cv2.perspectiveTransform(label_coordinate, H)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:50:40.282389Z","iopub.execute_input":"2021-08-24T14:50:40.282765Z","iopub.status.idle":"2021-08-24T14:50:40.287463Z","shell.execute_reply.started":"2021-08-24T14:50:40.282731Z","shell.execute_reply":"2021-08-24T14:50:40.286559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\n\nplt.scatter(transformed_coordinate[:, :, 0],transformed_coordinate[:, :, 1])\n\nplt.scatter(homography_df['x'], 53.33-homography_df['y'])\n\nplt.legend(['Transformed coordinate from Sideline helmet box','Ground truth tracking data'])","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:50:40.674985Z","iopub.execute_input":"2021-08-24T14:50:40.675448Z","iopub.status.idle":"2021-08-24T14:50:40.931699Z","shell.execute_reply.started":"2021-08-24T14:50:40.675419Z","shell.execute_reply":"2021-08-24T14:50:40.930639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Next to \n- Matching label using homography information\n- Build filed number detection model ? \n- Merging with MOT models like deepsort, FairMOT","metadata":{}}]}