{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Helmet Mapping + Deepsort\n\nThis notebook is basically an effort to \n\n- make a more rationally plausible mapping step based on projections and Hungarian Algorithm assignment, \n- perform optimal initialization of parameters using basin hopping algorithm, and\n- apply consecutive local parameter search using an appropriate Kalman Filter. \n\nThe basis for it is the previous work of [Fireflies](https://www.kaggle.com/firefliesqn/tuning-deepsort-helmet-mapping)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# Install helmet-assignment helper code\n!pip install ../input/helmet-assignment-helpers/helmet-assignment-main/ > /dev/null 2>&1\nfrom helmet_assignment.score import NFLAssignmentScorer, check_submission\nfrom helmet_assignment.features import add_track_features\nfrom helmet_assignment.video import video_with_predictions\nfrom IPython.display import Video, display","metadata":{"execution":{"iopub.status.busy":"2021-10-13T19:16:33.075082Z","iopub.execute_input":"2021-10-13T19:16:33.075393Z","iopub.status.idle":"2021-10-13T19:17:03.368Z","shell.execute_reply.started":"2021-10-13T19:16:33.075324Z","shell.execute_reply":"2021-10-13T19:17:03.36709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport itertools\nimport glob\nimport os\nimport sys\nimport torch\nimport cv2\nimport traceback\nimport time\nfrom sklearn.metrics import accuracy_score\nfrom tqdm.auto import tqdm\nfrom multiprocessing import Pool\nfrom matplotlib import pyplot as plt\nimport random\nimport torchvision\nimport shutil\nfrom joblib import Parallel, delayed\nfrom scipy.spatial.transform import Rotation\nfrom math import pi, ceil, sqrt\nfrom scipy.spatial import distance_matrix\nfrom scipy.optimize import linear_sum_assignment\nfrom statistics import mode\nfrom sklearn.cluster import k_means\nimport importlib.util","metadata":{"execution":{"iopub.status.busy":"2021-10-13T19:17:03.369538Z","iopub.execute_input":"2021-10-13T19:17:03.36985Z","iopub.status.idle":"2021-10-13T19:17:04.982018Z","shell.execute_reply.started":"2021-10-13T19:17:03.369816Z","shell.execute_reply":"2021-10-13T19:17:04.98118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Settings and loading data\n\nNote I've extracted `max_iter`, `DIG_STEP` and `DIG_MAX` to the top for easy experimentation. I've also modified the code to run in debug mode if running on the public test set.","metadata":{}},{"cell_type":"code","source":"n_test_videos = len(os.listdir('../input/nfl-health-and-safety-helmet-assignment/test/'))\n# Run in debug mode unless during submission\nif n_test_videos == 6:\n    debug = True\nelse:\n    debug = False\n# Configurables\nn_debug_samples = 1\nRANDOM_STATE = 42\nCONF_THRE = 0.4\nmax_iter = 1000\nDIG_STEP = 3\nDIG_MAX = DIG_STEP*10\n\n# Read in the data.\n\nBASE_DIR = '../input/nfl-health-and-safety-helmet-assignment'\n\nlabels = pd.read_csv(f'{BASE_DIR}/train_labels.csv')\nif debug:\n    tracking = pd.read_csv(f'{BASE_DIR}/train_player_tracking.csv')\n    helmets = pd.read_csv(f'{BASE_DIR}/train_baseline_helmets.csv')\nelse:\n    tracking = pd.read_csv(f'{BASE_DIR}/test_player_tracking.csv')\n    helmets = pd.read_csv(f'{BASE_DIR}/test_baseline_helmets.csv')\nhelmets['frame'] = helmets.video_frame.apply(lambda x: int(x.split('_')[-1]))    \ntracking = add_track_features(tracking)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T19:17:04.983741Z","iopub.execute_input":"2021-10-13T19:17:04.984032Z","iopub.status.idle":"2021-10-13T19:17:11.472584Z","shell.execute_reply.started":"2021-10-13T19:17:04.984004Z","shell.execute_reply":"2021-10-13T19:17:11.471757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_cols(df):\n    df['game_play'] = df['video_frame'].str.split('_').str[:2].str.join('_')\n    if 'video' not in df.columns:\n        df['video'] = df['video_frame'].str.split('_').str[:3].str.join('_') + '.mp4'\n    return df\nhelmets = add_cols(helmets)\nif debug:\n    labels = add_cols(labels)\n    # Select `n_debug_samples` worth of videos to debug with\n    sample_videos = labels['video'].drop_duplicates() \\\n        .sample(n_debug_samples, random_state=RANDOM_STATE).tolist()\n    sample_gameplays = ['_'.join(x.split('_')[:2]) for x in sample_videos]\n    tracking = tracking[tracking['game_play'].isin(sample_gameplays)]\n    helmets = helmets[helmets['video'].isin(sample_videos)]\n    labels = labels[labels['video'].isin(sample_videos)]\ntracking.shape, helmets.shape, labels.shape","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-13T19:17:11.475943Z","iopub.execute_input":"2021-10-13T19:17:11.476199Z","iopub.status.idle":"2021-10-13T19:17:23.824446Z","shell.execute_reply.started":"2021-10-13T19:17:11.476174Z","shell.execute_reply":"2021-10-13T19:17:23.823625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_nearest(tracking, value):\n    value = int(value)\n    array = np.asarray(tracking['est_frame']).astype(int)\n    unique_frames = np.unique(array)\n    idx = np.argmin(np.abs(unique_frames - value))\n    if value > unique_frames[idx]:\n        curr_frame = tracking[tracking['est_frame'] == unique_frames[idx]]\n        try:\n            next_frame = tracking[\n                    tracking['est_frame'] == unique_frames[idx + 1]]\n        except IndexError:\n            return curr_frame\n\n    elif value < unique_frames[idx]:\n        next_frame = tracking[tracking['est_frame'] == unique_frames[idx]]\n        try:\n            curr_frame = tracking[\n                tracking['est_frame'] == unique_frames[idx - 1]]\n        except IndexError:\n            return next_frame\n        \n    else:\n        return tracking[tracking['est_frame'] == unique_frames[idx]].reset_index(drop=True)\n    try:\n        next_frame = next_frame.set_index('player')\n        curr_frame = curr_frame.set_index('player')\n\n        diff = next_frame.est_frame.iloc[0] - curr_frame.est_frame.iloc[0]\n        cols = ['x','y', 'a', 'dir', 's', 'o', 'est_frame']\n        if diff != 0:\n            speed = (next_frame[cols] - curr_frame[cols]) / diff\n        else:\n            speed = 0\n        ret = next_frame.copy()\n        ret[cols] = curr_frame[cols] + (value - curr_frame.est_frame.iloc[0]) * speed\n        ret = ret.dropna(axis=0, subset=['est_frame'])\n        ret['est_frame'] = ret['est_frame'].astype(int)\n    except:\n        print(next_frame)\n        print(curr_frame)\n        print(ret['est_frame'])\n        raise\n    return ret.reset_index()\n\n\ndef norm_arr(a):\n    a = a-a.min()\n    a = a/a.max()\n    return a\n    \ndef dist(a1, a2):\n    return np.linalg.norm(a1-a2)\n\ndef dist_for_different_len(a1, a2):\n    assert len(a1) >= len(a2), f'{len(a1)}, {len(a2)}'\n    len_diff = len(a1) - len(a2)\n#     a2 = norm_arr(a2)\n    if len_diff == 0:\n#         a1 = norm_arr(a1)\n        return dist(a1,a2), ()\n    else:\n        min_dist = 10000\n        min_detete_idx = None\n        cnt = 0\n        del_list = list(itertools.combinations(range(len(a1)),len_diff))\n        if len(del_list) > max_iter:\n            del_list = random.sample(del_list, max_iter)\n        for detete_idx in del_list:\n            this_a1 = np.delete(a1, detete_idx)\n#             this_a1 = norm_arr(this_a1)\n            this_dist = dist(this_a1, a2)\n            #print(len(a1), len(a2), this_dist)\n            if min_dist > this_dist:\n                min_dist = this_dist\n                min_detete_idx = detete_idx\n                \n        return min_dist, min_detete_idx\n        \ndef rotate_arr(u, t, deg=True):\n    if deg == True:\n        t = np.deg2rad(t)\n    R = np.array([[np.cos(t), -np.sin(t)],\n                  [np.sin(t),  np.cos(t)]])\n    return  np.dot(R, u)\n\ndef dist_rot(tracking_df, a2):\n    tracking_df = tracking_df.sort_values('x')\n    x = tracking_df['x']\n    y = tracking_df['y']\n    min_dist = 10000\n    min_idx = None\n    min_x = None\n    for dig in range(-DIG_MAX,DIG_MAX+1,DIG_STEP):\n        arr = rotate_arr(np.array((x,y)), dig)\n        this_dist, this_idx = dist_for_different_len(np.sort(arr[0]), a2)\n        if min_dist > this_dist:\n            min_dist = this_dist\n            min_idx = this_idx\n            min_x = arr[0]\n    tracking_df['x_rot'] = min_x\n    player_arr = tracking_df.sort_values('x_rot')['player'].values\n    players = np.delete(player_arr,min_idx)\n    return min_dist, players\n\ndef dist_matrix(points, dense_view=True):\n    z = np.array([complex(c[0], c[1]) for c in points])\n    if dense_view:\n        return np.abs(z[..., np.newaxis] - z)[np.triu_indices(len(z),1)]\n    else:\n        return np.abs(z[..., np.newaxis] - z)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-13T19:17:23.825893Z","iopub.execute_input":"2021-10-13T19:17:23.826266Z","iopub.status.idle":"2021-10-13T19:17:23.848671Z","shell.execute_reply.started":"2021-10-13T19:17:23.826222Z","shell.execute_reply":"2021-10-13T19:17:23.847655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mapping_df_fallback(tracking, df, previous_mapped=None):\n    gameKey,playID,view,frame = df.video_frame.iloc[0].split('_')\n    gameKey = int(gameKey)\n    playID = int(playID)\n    frame = int(frame)\n    this_tracking = tracking[(tracking['gameKey']==gameKey) & (tracking['playID']==playID)]\n    this_tracking = find_nearest(this_tracking, frame)\n    len_this_tracking = len(this_tracking)\n    df['center_h_p'] = (df['left']+df['width']/2).astype(int)\n    df['center_h_m'] = (df['left']+df['width']/2).astype(int)*-1\n    if 'conf' in df.columns:\n        df = df[df['conf']>CONF_THRE].copy()\n    if len(df) > len_this_tracking:\n        df = df.tail(len_this_tracking)\n    df_p = df.sort_values('center_h_p').copy()\n    df_m = df.sort_values('center_h_m').copy()\n    \n    if view == 'Endzone':\n        this_tracking['x'], this_tracking['y'] = this_tracking['y'].copy(), this_tracking['x'].copy()\n    a2_p = df_p['center_h_p'].values\n    a2_m = df_m['center_h_m'].values\n\n    min_dist_p, min_detete_idx_p = dist_rot(this_tracking ,a2_p)\n    min_dist_m, min_detete_idx_m = dist_rot(this_tracking ,a2_m)\n    if min_dist_p < min_dist_m:\n        min_dist = min_dist_p\n        min_detete_idx = min_detete_idx_p\n        tgt_df = df_p\n    else:\n        min_dist = min_dist_m\n        min_detete_idx = min_detete_idx_m\n        tgt_df = df_m\n    #print(video_frame, len(this_tracking), len(df), len(df[df['conf']>CONF_THRE]), this_tracking['x'].mean(), min_dist_p, min_dist_m, min_dist)\n    tgt_df['label'] = min_detete_idx\n    unmatched = this_tracking[~this_tracking['player'].isin(tgt_df['label'])]\n    return tgt_df[df.columns.tolist() + ['label']], this_tracking, {'fallback_mapping_used':True}, unmatched","metadata":{"execution":{"iopub.status.busy":"2021-10-13T19:17:23.850303Z","iopub.execute_input":"2021-10-13T19:17:23.850675Z","iopub.status.idle":"2021-10-13T19:17:23.865128Z","shell.execute_reply.started":"2021-10-13T19:17:23.850638Z","shell.execute_reply":"2021-10-13T19:17:23.864217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pykalman import KalmanFilter\nfrom typing import Dict, Iterable, List\nfrom numpy import ma\ndef cartesian_product(arrays):\n    la = len(arrays)\n    dtype = np.find_common_type([np.array(a).dtype for a in arrays], [])\n    arr = np.empty([len(a) for a in arrays] + [la], dtype=dtype)\n    for i, a in enumerate(np.ix_(*arrays)):\n        arr[..., i] = a\n    return arr.reshape(-1, la)\n\n\ndef get_observation_matrix(params_len):\n    return np.pad(np.eye(params_len), ((0,0),(0,2*params_len)))\ndef get_transition_matrix(params_len):\n    return np.eye(3 * params_len) + np.diag(\n                    np.ones(2 * params_len), params_len) + np.diag(\n                    0.5 * np.ones(params_len), 2 * params_len)\n\nclass KalmanFilterRoutine:\n    def __init__(self, params: List[str], init_frames=5):\n        self.init_frames = init_frames\n        self.kf = None\n        self.params_buffer = []\n        self.frame = 0\n        self.means = None\n        self.covariances = None\n        self.observation_matrix = None\n        self.transition_matrix = None\n        self.params = params\n        self.params_len = len(params)\n\n    @property\n    def is_ready(self):\n        return self.frame >= self.init_frames\n        \n    def update(self, **params):\n        self.frame += 1\n        if self.frame < self.init_frames:\n            self.params_buffer.append(params)\n        else:\n            if self.frame == self.init_frames:\n                self.params_buffer.append(params)\n                self.transition_matrix = get_transition_matrix(self.params_len)\n                self.observation_matrix = get_observation_matrix(self.params_len)\n                params_df = pd.DataFrame(self.params_buffer)\n                missing = [p for p in self.params if p not in params_df.columns]\n                params_df[missing] = np.nan\n                params_df = params_df.fillna(0)\n                params_df = params_df[self.params]\n                initial_state_mean = np.pad(params_df.mean(axis=0), (\n                    (0, 2*len(self.params))))\n                self.kf = KalmanFilter(transition_matrices=self.transition_matrix, \n                                       observation_matrices=self.observation_matrix,\n                                      initial_state_mean=initial_state_mean, random_state=RANDOM_STATE)\n                if len(params_df) < 3:\n                    params_df = pd.concat(\n                        [params_df] + [params_df.iloc[[-1]] for _ in range(3 - len(params_df))])\n                self.means, self.covariances = self.kf.filter(params_df.values)\n                self.means = self.means.tolist()\n                self.covariances = self.covariances.tolist()\n            if self.frame > self.init_frames:\n                observation = ma.asarray(np.array([params[k] if k in params else np.nan for k  in self.params]))\n                observation[np.isnan(observation)] = ma.masked\n                state_means, state_covs = self.kf.filter_update(\n                    self.means[-1],\n                    self.covariances[-1],\n                    observation =observation)\n                self.means.append(state_means)\n                self.covariances.append(state_covs)\n            self.updated_params =  np.array(self.means[-1][:len(self.params)])\n            self.updated_params_der = np.array(self.means[-1][len(self.params): 2 * len(self.params)])\n            self.updated_params_sder = np.array(self.means[-1][2 * len(self.params): 3 * len(self.params)])\n        ","metadata":{"execution":{"iopub.status.busy":"2021-10-13T19:17:23.866602Z","iopub.execute_input":"2021-10-13T19:17:23.867052Z","iopub.status.idle":"2021-10-13T19:17:23.895586Z","shell.execute_reply.started":"2021-10-13T19:17:23.866996Z","shell.execute_reply":"2021-10-13T19:17:23.894834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import deque\n\nclass ParamsCombinationsGenerator:\n    # Uses Kalman Filter with Taylor expansion up to the 2nd derivative\n    def __init__(self, params_ranges : Dict[str, Iterable],\n                 strictly_positive_params:List[str]=None,\n                 min_perturbations: Dict[str, float]=None,\n                 max_perturbations: Dict[str, float]=None,\n                 kalman_init=5,\n                 allowed_change_ratio=1, n_steps=5, \n                 obey_original_ranges=True, use_kalman=True, previous_frames_to_keep=10):\n        self.use_kalman = use_kalman\n        self.kalman_init = kalman_init\n        self.allowed_change_ratio = allowed_change_ratio\n        self.params_ranges = params_ranges\n        self.strictly_positive_params = strictly_positive_params\n        self.n_steps = n_steps\n        self.previous_frames_to_keep = previous_frames_to_keep\n        self.buffer_starts = deque(maxlen=previous_frames_to_keep)\n        self.buffer_ends = deque(maxlen=previous_frames_to_keep)\n        self.params_buffer = []\n        self.ori_options = [0, 1]\n        self.frame = 0\n        self.params = list(params_ranges.keys())\n        \n        self.means = None\n        self.covariances = None\n        self.observation_matrix = None\n        self.transition_matrix = None\n        self.obey_original_ranges = obey_original_ranges\n        if obey_original_ranges:\n            self.ranges_limits = np.array([[np.min(params_ranges[p]),\n                                            np.max(params_ranges[p])] for p in self.params])\n        if min_perturbations is None:\n            self.min_perturbations = np.zeros(len(self.params))\n        else:\n            self.min_perturbations = np.array([min_perturbations[x]  if x in min_perturbations else 0 for x in self.params])\n        if max_perturbations is not None:\n            self.max_perturbations = np.array([max_perturbations[x]  if x in max_perturbations else\n                                               np.inf for x in self.params])\n        else:\n            self.max_perturbations = np.zeros(len(self.params)) + np.inf\n        self.kf = KalmanFilterRoutine(self.params, self.kalman_init)\n        \n    \n    def reset(self):\n        self.kf = KalmanFilterRoutine(self.params, self.kalman_init)\n    \n    def zero_buffer(self):\n        self.buffer_starts = deque(maxlen=self.previous_frames_to_keep)\n        self.buffer_ends = deque(maxlen=self.previous_frames_to_keep)\n        \n    @property\n    def is_ready(self):\n        return self.kf.is_ready\n    \n    def update(self, **params):\n        self.prev_params = np.array([params[k] for k in self.params])\n        if not self.use_kalman:\n            return\n        self.kf.update(**params)\n\n    \n    def get_bounds(self):    \n        if not self.use_kalman or not self.kf.is_ready:\n            if not self.buffer_starts:\n                ranges = [self.params_ranges[x] for x in self.params]\n            else:\n                ranges = [(start,end) for start, end in zip(0.8 * np.mean(self.buffer_starts, axis=0), \n                                                            1.2 * np.mean(self.buffer_ends, axis=0))]\n        else:\n            diff1 = self.kf.updated_params - self.prev_params\n            diff2 = self.kf.updated_params_der + 0.5 * self.kf.updated_params_sder\n            \n            \n            changes =  np.maximum(np.abs(diff1 + diff2), self.min_perturbations)\n            perturbations = changes\n            starts = self.prev_params - perturbations\n            ends = self.prev_params + perturbations\n            assert np.all(ends - starts > 0), (changes, self.min_perturbations)\n            ranges = [(start, end) for start, end in zip(starts, ends)]\n            self.buffer_starts.append(starts)\n            self.buffer_ends.append(ends)\n                          \n        return ranges    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-13T19:17:23.898666Z","iopub.execute_input":"2021-10-13T19:17:23.899272Z","iopub.status.idle":"2021-10-13T19:17:23.918296Z","shell.execute_reply.started":"2021-10-13T19:17:23.899234Z","shell.execute_reply":"2021-10-13T19:17:23.917591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SIDELINE_START_THRES = 50\nMAX_COORDS = (120, 53.33)\nMAX_COST_SIDELINE = 50\nMAX_COST_ENDZONE = 100\nfrom scipy.optimize import basinhopping, minimize\ndef cost_function(this_tracking, expanded, im_centers, \n                  camera_height, camera_length, max_p, \n                  xdig, zdig, scaling, max_cost_thres=None, previous_tracking=None,\n                  ret_cost_only=False):\n    assert ~np.any(np.isnan([xdig]))\n\n\n    z_rot = Rotation.from_rotvec([0, 0, zdig]).as_matrix()\n    #camera sits somewhere near the middle of the appropriate x side of the field\n    camera_pos = np.array([camera_length,0,camera_height])\n\n    expanded = expanded - camera_pos \n\n    z_rot = Rotation.from_rotvec([0, 0, zdig]).as_matrix()\n    z_rotated = (z_rot @ expanded.T).T\n    x_rot = Rotation.from_rotvec([xdig,0,0]).as_matrix()\n    x_rotated =  (x_rot @ z_rotated.T).T\n    if not ret_cost_only:\n        # sort the closer to the observer to be the latter in the list of players coordinates\n        eliminated_dim = x_rotated[:, 1]\n        sorting_order = np.argsort(eliminated_dim)[::-1]\n        x_rotated = x_rotated[sorting_order, :]\n        this_tracking = this_tracking.iloc[sorting_order]\n    x_rotated = x_rotated[:, [0, 2]]\n    \n    opt_params = None\n    opt_rl_remapped = None\n    scaled = scaling * x_rotated\n    # the origin is now assumed to be at the center of the image, so we need to move it to the bottom left first\n    scaled = scaled + np.array([1280,720]) / 2\n    # and then revert the y axis\n    scaled[:,1] = 720 - scaled[:,1]\n        \n    d = distance_matrix(im_centers,\n                        scaled)\n    match_to, match_from = linear_sum_assignment(d)\n    if not ret_cost_only:        \n        match_to = match_to[np.argsort(match_from)]\n        match_from = np.sort(match_from)\n        \n        \n    as_costs = d[match_to, match_from]\n    cost = np.mean(as_costs)\n    \n\n    if max_cost_thres is not None:\n        mask = as_costs < max_cost_thres\n        match_from = match_from[mask]\n        match_to = match_to[mask]\n        as_costs = as_costs[mask]\n    if not ret_cost_only:\n        reduced_tracking = this_tracking.iloc[match_from].copy()\n        reduced_tracking[['x2im', 'y2im']] = scaled[match_from, :]\n        reduced_tracking[['im2x', 'im2y']] = im_centers[match_to, :]\n        um_index = list(set(range(len(this_tracking))) - set(match_from)) \n        unmatched_tracking = this_tracking.iloc[um_index].copy()\n        unmatched_tracking[['x2im', 'y2im']] = scaled[um_index, :]\n    assert np.all(np.isfinite(scaled))\n    adraneia = None\n    p = None\n    if previous_tracking is not None and 'x2im' in previous_tracking.columns:\n        if match_from.size > 0:\n            previous_scaled = previous_tracking[['x2im', 'y2im']].values\n            # there are nans because of image points not matched to GPS points (shown as nan)\n            previous_scaled = previous_scaled[np.all(np.isfinite(previous_scaled), axis=1), :]\n            d = distance_matrix(previous_scaled,\n                        scaled)\n            adraneia = d[linear_sum_assignment(d)].mean()\n\n            cost = 0.7 * cost + 0.3 * adraneia\n     \n\n    if ret_cost_only:\n        return cost                    \n                        \n                        \n    params = dict(xdig=xdig, zdig=zdig, scaling=scaling,\n                  camera_height=camera_height,\n                  camera_length=camera_length,\n                  cost=cost,\n                  p=p,\n                  c=np.mean(scaled, axis=0),\n                  adraneia=adraneia,\n                  match_from=match_from,\n                  match_to=match_to,\n                  match_costs=as_costs,\n                  max_cost=(as_costs.max() if np.any(as_costs)\n                            else max_cost_thres))\n    return params, reduced_tracking, unmatched_tracking","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-13T19:17:23.920393Z","iopub.execute_input":"2021-10-13T19:17:23.920806Z","iopub.status.idle":"2021-10-13T19:17:23.940752Z","shell.execute_reply.started":"2021-10-13T19:17:23.920765Z","shell.execute_reply":"2021-10-13T19:17:23.93978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_duplicates(x, cols=['left','top']):\n    assert np.all(x.groupby(cols).size() == 1), (x, x.groupby(cols).size())\ndef mapping_df(combs_generator, tracking, df, previous_mapped=None, \n               available_oris=(0,1), ignore_starting_preproc=False,\n               force_local_minimize=True, check_mapping=False, ratio = 0.8):\n    gameKey,playID,view,frame = df.video_frame.iloc[0].split('_')\n    gameKey = int(gameKey)\n    playID = int(playID)\n    frame = int(frame)\n    this_tracking = tracking[(tracking['gameKey']==gameKey) & (tracking['playID']==playID)]\n    this_tracking = find_nearest(this_tracking, frame)\n    df = df.reset_index(drop=True)\n    \n    max_p = MAX_COORDS\n    if view == 'Endzone':\n        max_cost_thres = MAX_COST_ENDZONE\n    else:\n        max_cost_thres = MAX_COST_SIDELINE\n    same_sgns = 0 # the projected axes on the image need to be reflected as of x or as of y\n    if view == 'Endzone':\n        this_tracking['x'], this_tracking['y'] = this_tracking['y'].copy(), this_tracking['x'].copy()\n        max_p = max_p[::-1]\n        # the projected axes need to be reflected both as of x and as of y or stay as is\n        same_sgns = 1 \n    if 'conf' in df.columns:\n        df = df[df['conf']>CONF_THRE].copy()\n    df_num = len(df)\n    if not ignore_starting_preproc and (view == 'Sideline') and not combs_generator.is_ready:\n        inc_mask = (df['top'] >= SIDELINE_START_THRES) & (df['top'] < 720 - SIDELINE_START_THRES)\n        if not np.all(inc_mask):\n            print(f\"Removing {(~inc_mask).sum()} bounding boxes that reside in the top or bottom edge of the screen\")\n            df = df[inc_mask].copy()\n        \n    im_centers = df[['left', 'top']].values+ (df[['width', 'height']]/2).values\n    rl_centers = this_tracking[['x','y']].values\n    \n    im_centers = im_centers\n    \n    opt_params = None\n    costs = {0: [], 1: []}\n    min_cost = 1e7\n    opt_params = None\n    for change_ori in available_oris:\n        # assume 1 yard average height \n        expanded = np.hstack([rl_centers,\n                              1 + np.zeros((len(rl_centers),1))])\n        if same_sgns:\n            c_translation = np.zeros(2)\n            c_scaling = np.ones(2)\n            if change_ori:\n                c_translation = max_p\n                c_scaling = - np.ones(2)\n        else:\n            if change_ori:\n                c_translation = np.array([0, max_p[1]])\n                c_scaling = np.array([1, -1])\n            else:\n                c_translation = np.array([max_p[0], 0])\n                c_scaling = np.array([-1, 1])\n        expanded[:, :2] = c_scaling * expanded[:, :2] + c_translation\n        expanded[:, 1] = max_p[1] - expanded[:, 1]\n\n\n        to_opt = combs_generator.params\n        x0 = [np.mean(combs_generator.params_ranges[x]) for x in to_opt]\n        bounds = combs_generator.get_bounds()\n        min_func = lambda p: cost_function(\n            this_tracking=this_tracking,\n            expanded=expanded,\n            im_centers=im_centers,\n            camera_length=p[to_opt.index('camera_length')],\n            camera_height=p[to_opt.index('camera_height')],\n            max_p=max_p,\n            zdig=p[to_opt.index('zdig')],                                 \n            xdig=p[to_opt.index('xdig')],\n            scaling=p[to_opt.index('scaling')],\n            ret_cost_only=True,\n            previous_tracking=previous_mapped,\n            max_cost_thres=None)\n        \n        if force_local_minimize or combs_generator.is_ready:\n            ret = minimize(min_func, x0=x0,\n                bounds=bounds)\n#             if ret.fun > max_cost_thres * ratio:\n#                 print(f'Resetting due to high cost({ret.fun} > {max_cost_thres * ratio})')\n#                 combs_generator.reset()\n#                 bounds = combs_generator.get_bounds()\n        if not combs_generator.is_ready and not force_local_minimize:\n            ret = basinhopping(min_func,\n                               x0=x0,niter=100 if combs_generator.buffer_starts else 6000,\n                               niter_success=5 if combs_generator.buffer_starts else 150,\n                minimizer_kwargs=dict(bounds=bounds), seed=RANDOM_STATE)\n            if ret.fun > max_cost_thres * ratio:\n                print('Basin Hopping Unsuccessful! Resetting due to high cost('\n                      f'{ret.fun} > {max_cost_thres * ratio})')\n                combs_generator.reset()\n                    \n        cost = ret.fun\n        if cost < min_cost:\n            xdig = ret.x[to_opt.index('xdig')]\n            zdig = ret.x[to_opt.index('zdig')]\n            scaling = ret.x[to_opt.index('scaling')]\n            camera_height = ret.x[to_opt.index('camera_height')]\n            camera_length = ret.x[to_opt.index('camera_length')]\n            found_params, found_tracking, unmatched_tracking = cost_function(\n                this_tracking=this_tracking,\n                expanded=expanded, \n                im_centers=im_centers,\n                camera_height=camera_height,\n                camera_length=camera_length,\n                max_p=max_p,\n                zdig=zdig,\n                xdig=xdig,\n                scaling=scaling,\n                ret_cost_only=False, \n                previous_tracking=previous_mapped,\n                max_cost_thres=max_cost_thres)\n            min_cost = cost\n            opt_ret = ret\n            opt_params = found_params\n            opt_params['change_ori'] = change_ori\n            opt_tracking = found_tracking\n            opt_unmatched = unmatched_tracking\n    \n\n    if opt_params is None:\n        combs_generator.reset()\n        if debug:\n            print('Failure')\n            print(ret)\n        raise\n    match_to = opt_params['match_to']\n    match_from = opt_params['match_from']\n    match_costs = opt_params['match_costs']\n    df['view'] = view\n    df[['im_x_remapped', 'im_y_remapped']] = im_centers\n    df_cols = [col for col in df.columns if col not in ['x','y','x2im', 'y2im', 'player']]\n    to_double_match = opt_unmatched.copy()\n    to_double_match_flag = ((to_double_match['x2im'] < 1280) &\n                       (to_double_match['x2im'] >= 0) &\n                       (to_double_match['y2im'] < 720) &\n                       (to_double_match['y2im'] >= 0))\n    # deactivating it\n    to_double_match_flag = np.zeros_like(to_double_match_flag)\n    double_match_aug = None\n    unmatched_df_inds = list(set(range(len(df))) - set(match_to))\n    if to_double_match_flag.any():\n        to_double_match = to_double_match[to_double_match_flag].copy()\n        dd = distance_matrix(im_centers, to_double_match[['x2im', 'y2im']].values)\n        dmatch_to, dmatch_from  = linear_sum_assignment(dd)\n        dcosts = dd[dmatch_to, dmatch_from]\n        dflag = dcosts <= max_cost_thres\n        dmatch_to = dmatch_to[dflag]\n        dmatch_from = dmatch_from[dflag]\n        \n#         unmatched_df_inds = list(set(unmatched_df_inds) - set(dmatch_to))\n        double_match_aug = df[df_cols].iloc[dmatch_to].copy().reset_index(drop=True)\n        double_match_aug['left'] += 1\n        double_match_aug[['x','y','x2im', 'y2im', 'player']] = to_double_match.iloc[dmatch_from][\n            ['x','y','x2im', 'y2im', 'player']].values\n        double_match_aug['cost'] = dd[dmatch_to, dmatch_from]\n        opt_unmatched = pd.concat(\n            [opt_unmatched[~to_double_match_flag],\n             to_double_match.iloc[list(set(range(len(to_double_match))) - set(dmatch_from))]],axis=0)\n        \n        \n    combs_generator.update(zdig=opt_params['zdig'],\n                           xdig=opt_params['xdig'],\n                           scaling=opt_params['scaling'],\n                           camera_height=opt_params['camera_height'],\n                           camera_length=opt_params['camera_length'])\n    \n    \n    \n            \n    ret = pd.concat(\n        [\n            df[df_cols].iloc[match_to].reset_index(drop=True),\n            opt_tracking[['x','y','x2im', 'y2im', 'player']].reset_index(drop=True)\n        ],\n        axis=1).set_index(df.iloc[match_to].index)\n    ret['cost'] = match_costs\n    \n    if double_match_aug is not None:\n        \n        ret = pd.concat([ret, double_match_aug],axis=0).reset_index(drop=True)\n        \n        \n        opt_tracking = pd.concat(\n            [opt_tracking, to_double_match.iloc[dmatch_from]],axis=0).reset_index(drop=True)\n        \n    \n    ret = pd.concat([ret, df.iloc[unmatched_df_inds][df_cols]], axis=0).reset_index(drop=True) # null labels for unmatched\n    assert df.left.isin(ret.left).all(), (sorted(df.left), sorted(ret.left))\n    ret['cost'] = ret['cost'].fillna(np.inf)\n    if double_match_aug is not None:\n        x = set(unmatched_df_inds) | set(match_to) |set(dmatch_to)\n        assert  len(x) == df_num, (len(df), x, df_num) \n        df = pd.concat([df, double_match_aug[df_cols]], axis=0).reset_index(drop=True)\n        assert len(df) >= df_num, (len(df), df_num)\n        assert df.left.isin(ret.left).all()\n    \n    ret.rename(columns={'player':'label'},inplace=True)\n    \n    \n    if previous_mapped is not None:\n        previous_mapped = previous_mapped[~previous_mapped['label'].isnull()]\n    if previous_mapped is not None and (len(previous_mapped) > 0) and ('cost' in previous_mapped.columns):\n        ori_len = len(ret)\n        \n        compared_ret, to_reassign_labels = compare_and_assign(ret, previous_mapped)\n        if ~to_reassign_labels.empty:\n            max_permitted_costs = compared_ret['cost'].max()\n            tracking_to_check = pd.concat([opt_tracking, opt_unmatched],axis=0)\n            tracking_mask = ~tracking_to_check.player.isin(\n                compared_ret['label'].values)\n            to_reassign_flag = ret['label'].isin(to_reassign_labels)\n            reduced_df = df.merge(ret.loc[to_reassign_flag, ['left', 'top']],\n                                  on=['left', 'top'], how='inner')\n            reduced_dist = distance_matrix(reduced_df[['im_x_remapped',\n                                                       'im_y_remapped']],\n                                           tracking_to_check[['x2im', 'y2im']])[:, tracking_mask]\n            reduced_dist[np.isinf(reduced_dist)] = 1e7\n            try:\n                matched_to, matched_from = linear_sum_assignment(reduced_dist)\n            except:\n                return ret, opt_tracking, opt_params, opt_unmatched\n            matched_costs = reduced_dist[matched_to, matched_from]\n            reassigned_ret = pd.concat(\n                [reduced_df[df_cols].iloc[\n                    matched_to].reset_index(drop=True),\n                 tracking_to_check[tracking_mask].iloc[matched_from][\n                     ['x','y','x2im', 'y2im', 'player']].reset_index(drop=True)],\n                axis=1).set_index(reduced_df.iloc[matched_to].index)\n            reassigned_ret['cost'] = matched_costs\n            reassigned_ret.rename(columns={'player':'label'},inplace=True)\n            reassigned_ret.loc[reassigned_ret['cost'] > max_permitted_costs, 'label'] = np.nan\n            \n            ret = pd.concat([compared_ret, reassigned_ret],axis=0)\n            mask = ~ret['label'].isnull()\n        else:\n            ret = compared_ret\n    ret = ret.sort_values('left')\n#     assert df['left'].isin(ret['left']).all(), (df['left'], ret['left'])\n    return ret[df_cols + ['label', 'cost', 'x2im', 'y2im']], opt_tracking, opt_params, opt_unmatched\n    \ndef compare_and_assign(ret, previous_mapped):\n    hist_dist_mat = distance_matrix(\n    ret[['left', 'top']].values,\n    previous_mapped[['left', 'top']].values)\n\n    matched_to, matched_from = linear_sum_assignment(hist_dist_mat)\n    costs = np.array([ret.iloc[matched_to]['cost'].values,\n               previous_mapped.iloc[matched_from]['cost'].values])\n    to_select = np.argmin(costs,axis=0)\n    to_keep_previous = to_select == 1\n    \n    matched_to_flag = np.zeros(len(ret)).astype(bool)\n\n    matched_from = matched_from[to_keep_previous]\n    matched_to = matched_to[to_keep_previous]\n    matched_to_flag[matched_to] = True\n    labels_to_keep_previous = previous_mapped.iloc[matched_from].label\n    to_change_df = ret[matched_to_flag] .copy()\n    to_change_df['cost'] = (\n        to_change_df['cost'].values +\n        previous_mapped.iloc[matched_from]['cost'].values) / 2\n    to_change_df['label'] = previous_mapped.iloc[matched_from]['label'].values\n    to_keep_df = ret[~matched_to_flag].copy()\n    to_reassign_flag = to_keep_df['label'].isin(to_change_df['label'].values).values\n    ret = pd.concat([to_change_df, to_keep_df[~to_reassign_flag]],axis=0)\n    mask = ~ret['label'].isnull()\n    if len(ret[mask]) != len(ret[mask].drop_duplicates('label')):\n        display(to_change_df)\n        display(to_keep_df[~to_reassign_flag])\n        print(matched_to)\n        raise\n   \n    return ret, to_keep_df['label'][to_reassign_flag]","metadata":{"execution":{"iopub.status.busy":"2021-10-15T16:20:06.12936Z","iopub.execute_input":"2021-10-15T16:20:06.129646Z","iopub.status.idle":"2021-10-15T16:20:06.256351Z","shell.execute_reply.started":"2021-10-15T16:20:06.12958Z","shell.execute_reply":"2021-10-15T16:20:06.255627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Mapping:\n    def __init__(self, tracking, view, use_kalman=True, use_previous=True, available_oris=(0,1),\n                 init_frames=19, ignore_starting_preproc=False):\n        self.tracking = tracking\n        self.use_kalman = use_kalman\n        self.use_previous = use_previous\n        self.available_oris = available_oris\n        self.init_frames = init_frames\n        self.ignore_starting_preproc = ignore_starting_preproc\n        \n        self.buffer_max_cost = deque(maxlen=30)\n        self.buffer_ori = []\n        self.buffer_costs = []\n        dig_step = np.deg2rad(DIG_STEP)\n        dig_max = np.deg2rad(DIG_MAX)\n        step_size = int(2 * DIG_MAX / DIG_STEP)\n        length = MAX_COORDS[0] if view=='Sideline' else MAX_COORDS[1]\n        self.params_ranges = dict(zdig=[- pi / 3, pi / 3],\n                                  xdig=[0, pi/3],\n                                  camera_height=[15, 50],\n                                  camera_length=[0.3 * length, 0.7 * length],\n                                  scaling=[20, 80])\n        self.min_perturbations = dict(zdig=np.deg2rad(5), xdig=np.deg2rad(5), scaling=0.1, camera_length=0.1,\n                                      camera_height=0.1)\n        self.max_perturbations = dict(zdig=np.deg2rad(10), xdig=np.deg2rad(10), scaling=1, camera_length=0.2,\n                                      camera_height=0.2)\n        \n        self.combinations_generator = ParamsCombinationsGenerator(\n            self.params_ranges, strictly_positive_params='scaling', min_perturbations=self.min_perturbations,\n        max_perturbations=self.max_perturbations, use_kalman=use_kalman, kalman_init=1, obey_original_ranges=False)\n        self.previous_df = None\n        self.max_cost_thres=None\n        self.frame = 0\n    \n    def __call__(self, this_df):\n        try:\n            self.previous_df, this_tracking, opt_params, opt_unmatched = mapping_df(\n                self.combinations_generator,\n                self.tracking, this_df, \n                previous_mapped=(self.previous_df\n                                 if self.use_previous else None),\n                available_oris=self.available_oris,\n                force_local_minimize=len(self.available_oris)==2, # we dont need much of accuracy when detecting orientation\n                ignore_starting_preproc=self.ignore_starting_preproc,\n            )\n            self.previous_df = pd.concat([self.previous_df, opt_unmatched[\n                    [col for col in opt_unmatched if col in self.previous_df.columns]]],axis=0)\n            \n            if len(self.buffer_ori) < self.init_frames:\n                self.buffer_ori.append(opt_params['change_ori'])\n                self.buffer_max_cost.append(opt_params['max_cost'])\n                self.buffer_costs.append(opt_params['cost'])\n            if len(self.buffer_ori) == self.init_frames:\n                ori_df = pd.DataFrame({'ori': self.buffer_ori, 'cost': self.buffer_costs})\n                mean_costs = ori_df.groupby('ori').median()\n                if len(self.available_oris) == 2:\n                    self.available_oris = [mean_costs.iloc[np.argmax(mean_costs)]]\n                self.max_cost_thres = 1.1 * np.max(self.buffer_max_cost)\n        except KeyboardInterrupt:\n            raise\n        except:\n            if debug:\n                raise\n            traceback.print_exc()\n            self.previous_df, this_tracking, opt_params, opt_unmatched = mapping_df_fallback(self.tracking, this_df)\n            opt_params['error'] = traceback.format_exc()\n        return self.previous_df[~self.previous_df.label.isnull()].copy(), this_tracking, opt_params, opt_unmatched\n\n\ndef apply_on_video(tracking, video_df):\n    submission_df_list = []\n    df_list = list(video_df.groupby('frame'))\n    view = video_df.iloc[0]['video_frame'].split('_')[2]\n    ori_mapping = Mapping(tracking, view=view, use_kalman=False, use_previous=False, ignore_starting_preproc=True)\n    print('Detecting video view orientation...')\n    for frame in tqdm(np.linspace(1, len(df_list)-1, ori_mapping.init_frames).astype(int)):\n        _, this_df = df_list[frame]\n        ori_mapping(this_df)\n    detected_ori = [mode(ori_mapping.buffer_ori)] if ori_mapping.buffer_ori else [0,1]\n    print(ori_mapping.buffer_ori)\n    print('Detected orientation:',detected_ori, '. Mapping...')\n    \n    mapping = Mapping(tracking,view=view,\n                      available_oris=detected_ori)\n    opt_params_dict = {}\n    try:\n        for frame, this_df in tqdm(df_list):\n            df, _, opt_params, _ = mapping(this_df)\n            if debug:\n                opt_params_dict[frame] = opt_params\n            submission_df_list.append(df)\n        submission_df = pd.concat(submission_df_list)\n    except KeyboardInterrupt:\n        if debug:\n            with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n                display(pd.DataFrame(opt_params_dict).T)\n        raise\n    return submission_df","metadata":{"execution":{"iopub.status.busy":"2021-10-13T19:17:23.98856Z","iopub.execute_input":"2021-10-13T19:17:23.988834Z","iopub.status.idle":"2021-10-13T19:17:24.011939Z","shell.execute_reply.started":"2021-10-13T19:17:23.988811Z","shell.execute_reply":"2021-10-13T19:17:24.011098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"videos_dfs = list(helmets.groupby('video'))\nif len(videos_dfs) == 1:\n    submission_df_list = [apply_on_video(tracking, videos_dfs[0][1])]\nelse:\n    submission_df_list = Parallel(n_jobs=-1)(delayed(apply_on_video)(tracking, video_df) for _, video_df in tqdm(videos_dfs))\nsubmission_df = pd.concat(submission_df_list)\nsubmission_df.to_csv('submission-baseline.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T19:17:24.013307Z","iopub.execute_input":"2021-10-13T19:17:24.013658Z","iopub.status.idle":"2021-10-13T19:21:08.765849Z","shell.execute_reply.started":"2021-10-13T19:17:24.013623Z","shell.execute_reply":"2021-10-13T19:21:08.764751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df","metadata":{"execution":{"iopub.status.busy":"2021-10-13T19:21:08.767225Z","iopub.execute_input":"2021-10-13T19:21:08.767615Z","iopub.status.idle":"2021-10-13T19:21:08.809198Z","shell.execute_reply.started":"2021-10-13T19:21:08.767577Z","shell.execute_reply":"2021-10-13T19:21:08.808258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if debug:\n    scorer = NFLAssignmentScorer(labels)\n    baseline_score = scorer.score(submission_df[~submission_df['label'].isnull()])\n    print(f\"validation score {baseline_score:0.4f}\")","metadata":{"execution":{"iopub.status.busy":"2021-10-13T19:21:08.82648Z","iopub.execute_input":"2021-10-13T19:21:08.827018Z","iopub.status.idle":"2021-10-13T19:21:09.1145Z","shell.execute_reply.started":"2021-10-13T19:21:08.826981Z","shell.execute_reply":"2021-10-13T19:21:09.113501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Score the predictions before applying deepsort postprocessing\n\nThe scores are roughly ~0.3, which is similar to the public leaderboard.","metadata":{}},{"cell_type":"markdown","source":"# Deepsort Postprocessing\n\nDeepsort is a popular framework for object tracking within video. \n- [This blog post](https://nanonets.com/blog/object-tracking-deepsort/\n) shows some examples of it being put to use.\n- This notebook shows how to apply deepsort to this helmet dataset: https://www.kaggle.com/s903124/nfl-helmet-with-yolov5-deepsort-starter\n- You can also read the paper for deepsort here: https://arxiv.org/pdf/1703.07402.pdf\n\nThe approach is fairly simple:\n1. Step through each frame in a video and apply the deepsort algorithm. This clusters helmets across frames when it is the same player/helmet.\n2. Group by each of these deepsort clusters - and pick the most common label for that cluster. Then override all of the predictions for that helmet to the same player.","metadata":{}},{"cell_type":"markdown","source":"## Importing Deepsort from dataset\nBecause your submission is not allowed to use internet access, you can reference the deepsort codebase from the attached dataset. Deepsort also has a dependency of `easydict` which I've also added as a dataset.","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/easydict-master/easydict-master/')\n# https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch\nsys.path.append('../input/yolov5-deepsort-pytorch/Yolov5_DeepSort_Pytorch-master/Yolov5_DeepSort_Pytorch-master/deep_sort_pytorch/')\nfrom deep_sort.deep_sort import DeepSort\nfrom utils.parser import get_config","metadata":{"execution":{"iopub.status.busy":"2021-10-13T19:21:09.115906Z","iopub.execute_input":"2021-10-13T19:21:09.116269Z","iopub.status.idle":"2021-10-13T19:21:09.225668Z","shell.execute_reply.started":"2021-10-13T19:21:09.11623Z","shell.execute_reply":"2021-10-13T19:21:09.22482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deepsort config\n\nDeepsort uses a config yaml file for some settings. These are just the default configs and could be improved.","metadata":{}},{"cell_type":"code","source":"%%writefile deepsort.yaml\n\nDEEPSORT:\n  REID_CKPT: \"../input/yolov5-deepsort-pytorch/ckpt.t7\"\n  MAX_DIST: 0.1\n  MIN_CONFIDENCE: 0.4\n  NMS_MAX_OVERLAP: 0.5\n  MAX_IOU_DISTANCE: 0.9\n  MAX_AGE: 15\n  N_INIT: 1\n  NN_BUDGET: 100","metadata":{"execution":{"iopub.status.busy":"2021-10-13T19:21:09.226833Z","iopub.execute_input":"2021-10-13T19:21:09.227173Z","iopub.status.idle":"2021-10-13T19:21:09.236239Z","shell.execute_reply.started":"2021-10-13T19:21:09.227136Z","shell.execute_reply":"2021-10-13T19:21:09.233823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nHelper functions from yolov5 to plot deepsort labels.\n\"\"\"\n\ndef compute_color_for_id(label):\n    \"\"\"\n    Simple function that adds fixed color depending on the id\n    \"\"\"\n    palette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n\n    color = [int((p * (label ** 2 - label + 1)) % 255) for p in palette]\n    return tuple(color)\n\ndef plot_one_box(x, im, color=None, label=None, line_thickness=3):\n    # Plots one bounding box on image 'im' using OpenCV\n    assert im.data.contiguous, 'Image not contiguous. Apply np.ascontiguousarray(im) to plot_on_box() input image.'\n    tl = line_thickness or round(0.002 * (im.shape[0] + im.shape[1]) / 2) + 1  # line/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(im, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label: \n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(im, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(im, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n    return im","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-13T19:21:09.237772Z","iopub.execute_input":"2021-10-13T19:21:09.238204Z","iopub.status.idle":"2021-10-13T19:21:09.250675Z","shell.execute_reply.started":"2021-10-13T19:21:09.238167Z","shell.execute_reply":"2021-10-13T19:21:09.249514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions to apply deepsort to helmet boxes.\n\nBelow are two functions `deepsort_helmets` which runs deepsort across a video. There is a lot of room for improving this function. The merging of deepsort labels onto the original helmet boxes is currently done in a very crude manner.\n\n`add_deepsort_label_col` mapps the most common label to each deepsort cluster.","metadata":{}},{"cell_type":"code","source":"class ValidRegionTracker:\n    def __init__(self):\n        self.state_surface = None\n        self.boundary_flag = None\n        self.input_shape = (512,512)\n        self.mask5 = np.ones((5,5), np.uint8)\n        self.mask3 = np.ones((3,3), np.uint8)\n        self.mask15 = np.ones((15,15), np.uint8)\n        self.large_mask = np.ones((self.input_shape[1]//5,self.input_shape[0]//5), np.uint8)\n        \n    def detect(self, image_data):\n        og_shape = image_data.shape[:2][::-1]\n        mask = self.get_mask(cv2.resize(image_data, self.input_shape))\n        return cv2.resize(\n                    mask.astype(np.uint8), og_shape, 0, 0, cv2.INTER_NEAREST) > 0\n\n    def get_mask(self, image_data):\n\n        \n        hls_img = cv2.cvtColor(image_data,  cv2.COLOR_RGB2HLS)\n        white_obj_mask = cv2.threshold(hls_img[:,:,1],150,1, cv2.THRESH_BINARY)[1]\n        seeds = cv2.erode(\n            cv2.morphologyEx(white_obj_mask.astype(np.uint8), cv2.MORPH_OPEN, self.mask5),\n            self.mask3)\n        seeds[3:-3,3:-3] = 0\n        sure_fg = seeds\n        unknown = cv2.subtract(cv2.threshold(hls_img[:,:,1],100,1,cv2.THRESH_BINARY)[1],sure_fg)\n        _, markers = cv2.connectedComponents(sure_fg)\n        markers = markers+1\n        markers[unknown==1] = 0\n        img = cv2.cvtColor(white_obj_mask * 255, cv2.COLOR_GRAY2RGB)\n        cv2.watershed(img, markers)\n        white_obj_on_im_edges = (markers>1).astype(np.uint8)\n        white_obj_on_im_edges = cv2.morphologyEx(white_obj_on_im_edges, cv2.MORPH_CLOSE,self.mask15)\n        white_obj_on_im_edges = cv2.morphologyEx(white_obj_on_im_edges, cv2.MORPH_OPEN, self.mask15)\n        from math import pi, sqrt\n        boundary_flag = np.zeros(white_obj_on_im_edges.shape[:2])\n        if self.boundary_flag is not None:\n            boundary_flag = cv2.erode(self.boundary_flag, self.mask15)\n        to_detect_edges = white_obj_on_im_edges\n        edges = cv2.morphologyEx(\n                cv2.Canny(to_detect_edges * 255,0,1,apertureSize = 3),cv2.MORPH_CLOSE,\n                self.mask15)\n        lines = cv2.HoughLinesP(\n            edges,\n            1, pi/180,100,maxLineGap=3\n            )\n        if lines is not None:\n            # keep 4 largest\n            lines_lengths = [ sqrt((x2-x1)**2 + (y2-y1)**2) for (x1,y1,x2,y2) in [l[0] for l in lines]]\n            lines = lines[np.argsort(lines_lengths)[-4:],:,:]\n            for line in lines:\n                x1,y1,x2,y2 = line[0]\n                cv2.line(boundary_flag,(x1,y1),(x2,y2),1,2)\n        sure_fg = boundary_flag.astype(np.uint8)\n        unknown = cv2.subtract(white_obj_on_im_edges,sure_fg)\n        _, markers = cv2.connectedComponents(sure_fg)\n        markers = markers+1\n        markers[unknown==1] = 0\n        img = cv2.cvtColor(\n            white_obj_on_im_edges*255, cv2.COLOR_GRAY2RGB)\n        cv2.watershed(\n            img, markers)\n        markers = (markers > 1).astype(np.uint8)\n        self.boundary_flag = cv2.morphologyEx(\n            markers, cv2.MORPH_CLOSE, self.large_mask)\n\n        return self.boundary_flag == 0","metadata":{"execution":{"iopub.status.busy":"2021-10-13T19:21:09.252058Z","iopub.execute_input":"2021-10-13T19:21:09.252459Z","iopub.status.idle":"2021-10-13T19:21:09.272808Z","shell.execute_reply.started":"2021-10-13T19:21:09.252409Z","shell.execute_reply":"2021-10-13T19:21:09.271905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def deepsort_helmets(video_data,\n                     video_dir,\n                     deepsort_config='deepsort.yaml',\n                     plot=False,\n                     plot_frames=[]):\n    \n    # Setup Deepsort\n    cfg = get_config()\n    cfg.merge_from_file(deepsort_config)    \n    deepsort = DeepSort(cfg.DEEPSORT.REID_CKPT,\n                        max_dist=cfg.DEEPSORT.MAX_DIST,\n                        min_confidence=cfg.DEEPSORT.MIN_CONFIDENCE,\n                        nms_max_overlap=cfg.DEEPSORT.NMS_MAX_OVERLAP,\n                        max_iou_distance=cfg.DEEPSORT.MAX_IOU_DISTANCE,\n                        max_age=cfg.DEEPSORT.MAX_AGE,\n                        n_init=cfg.DEEPSORT.N_INIT,\n                        nn_budget=cfg.DEEPSORT.NN_BUDGET,\n                        use_cuda=True)\n    tracker = ValidRegionTracker()\n    # Run through frames.\n    video_data = video_data.sort_values('frame').reset_index(drop=True)\n    ds = []\n    for frame, d in tqdm(video_data.groupby(['frame']), total=video_data['frame'].nunique()):\n        d['x'] = (d['left'] + round(d['width'] / 2))\n        d['y'] = (d['top'] + round(d['height'] / 2))\n\n        xywhs = d[['x','y','width','height']].values\n\n        cap = cv2.VideoCapture(f'{video_dir}/{myvideo}.mp4')\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame-1) # optional\n        success, image = cap.read()\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = tracker.detect(image)\n        image = image * mask[:, :, np.newaxis]\n        confs = np.ones([len(d),])\n        clss =  np.zeros([len(d),])\n        outputs = deepsort.update(xywhs, confs, clss, image)\n\n        if (plot and frame > cfg.DEEPSORT.N_INIT) or (frame in plot_frames):\n            for j, (output, conf) in enumerate(zip(outputs, confs)): \n\n                bboxes = output[0:4]\n                id = output[4]\n                cls = output[5]\n\n                c = int(cls)  # integer class\n                label = f'{id}'\n                color = compute_color_for_id(id)\n                im = plot_one_box(bboxes, image, label=label, color=color, line_thickness=2)\n            fig, ax = plt.subplots(figsize=(15, 10))\n            video_frame = d['video_frame'].values[0]\n            ax.set_title(f'Deepsort labels: {video_frame}')\n            plt.imshow(im)\n            plt.show()\n\n        preds_df = pd.DataFrame(outputs, columns=['left','top','right','bottom','deepsort_cluster','class'])\n        if len(preds_df) > 0:\n            # TODO Fix this messy merge\n            d[['left','top']] = d[['left','top']].astype(int)\n            preds_df[['left','top']] = preds_df[['left','top']].astype(int)\n            d = pd.merge_asof(d.sort_values(['left','top']),\n                              preds_df[['left','top','deepsort_cluster']] \\\n                              .sort_values(['left','top']), on='left', suffixes=('','_deepsort'),\n                              direction='nearest')\n        ds.append(d)\n    dout = pd.concat(ds)\n    return dout\n\ndef add_deepsort_label_col(out):\n    # Find the top occuring label for each deepsort_cluster\n    cum = out[~out['label'].isnull()].groupby('deepsort_cluster')['label'].value_counts() \\\n        .sort_values(ascending=False).to_frame() \\\n        .rename(columns={'label':'label_count'}) \\\n        .reset_index() \\\n        .groupby(['deepsort_cluster']) \\\n        .first()\n    \n    sortlabel_map = cum['label'].to_dict()\n    # Find the # of times that label appears for the deepsort_cluster.\n    sortlabelcount_map = cum['label_count'].to_dict()\n    \n    out['label_deepsort'] = out['deepsort_cluster'].map(sortlabel_map)\n    out['label_count_deepsort'] = out['deepsort_cluster'].map(sortlabelcount_map)\n    return out\n\ndef score_vs_deepsort(myvideo, out, labels):\n    # Score the base predictions compared to the deepsort postprocessed predictions.\n    myvideo_mp4 = myvideo + '.mp4'\n    labels_video = labels.query('video == @myvideo_mp4')\n    scorer = NFLAssignmentScorer(labels_video)\n    out_deduped = out.groupby(['video_frame','label']).first().reset_index()\n    base_video_score = scorer.score(out_deduped)\n    \n    out_preds = out.drop('label', axis=1).rename(columns={'label_deepsort':'label'})\n    out_preds = out_preds.groupby(['video_frame','label']).first().reset_index()\n    deepsort_video_score = scorer.score(out_preds)\n    print(f'{base_video_score:0.5f} before --> {deepsort_video_score:0.5f} deepsort')","metadata":{"execution":{"iopub.status.busy":"2021-10-13T19:25:10.778681Z","iopub.execute_input":"2021-10-13T19:25:10.779024Z","iopub.status.idle":"2021-10-13T19:25:10.802022Z","shell.execute_reply.started":"2021-10-13T19:25:10.778993Z","shell.execute_reply":"2021-10-13T19:25:10.800888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Apply Deepsort to Baseline Predictions","metadata":{}},{"cell_type":"code","source":"# Add video and frame columns to submission.\nsubmission_df['video'] = submission_df['video_frame'].str.split('_').str[:3].str.join('_')\nsubmission_df['frame'] = submission_df['video_frame'].str.split('_').str[-1].astype('int')\n\nif debug:\n    video_dir = '../input/nfl-health-and-safety-helmet-assignment/train/'\nelse:\n    video_dir = '../input/nfl-health-and-safety-helmet-assignment/test/'\n\n# Loop through test videos and apply. If in debug mode show the score change.\nout_ds = []\nouts = []\nfor myvideo, video_data in tqdm(submission_df.groupby('video'), total=submission_df['video'].nunique()):\n    print(f'==== {myvideo} ====')\n    if debug:\n        # Plot deepsort labels when in debug mode.\n        out = deepsort_helmets(video_data, video_dir, plot_frames=[10, 150, 250])\n    else:\n        out = deepsort_helmets(video_data, video_dir)\n    out_ds.append(out)\n    out = add_deepsort_label_col(out)\n    outs.append(out)\n    if debug:\n        # Score\n        score_vs_deepsort(myvideo, out, labels)\nsubmission_deepsort = pd.concat(outs).copy()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T19:25:11.37713Z","iopub.execute_input":"2021-10-13T19:25:11.377469Z","iopub.status.idle":"2021-10-13T19:29:31.307773Z","shell.execute_reply.started":"2021-10-13T19:25:11.377434Z","shell.execute_reply":"2021-10-13T19:29:31.306023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check Submission & Save\nFinally we will create a submission file and check that it passes the submission requirements.\nThe steps are:\n1. Drop the `label` and replace with `label_deepsort` predictions.\n2. Remove any duplicate labels within a single video/frame. This is required to meet the submission requirements.\n3. Save the results.","metadata":{}},{"cell_type":"code","source":"ss = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/sample_submission.csv')\n# Final Checks\nsubmission_deepsort.reset_index(inplace=True, drop=True)\nsubmission_deepsort['label_deepsort'] = submission_deepsort['label_deepsort'].fillna(submission_deepsort['label'])\nsubmission_deepsort = submission_deepsort[~submission_deepsort['label_deepsort'].isnull()]\nsubmission_deepsort = submission_deepsort.drop('label', axis=1) \\\n    .rename(columns={'label_deepsort':'label'})[ss.columns]\n# Drop duplicate labels\nsubmission_deepsort = submission_deepsort.loc[\n    ~submission_deepsort[['video_frame','label']].duplicated()]\ncheck_submission(submission_deepsort)\nsubmission_deepsort[['left','width','top','height']] = submission_deepsort[['left','width','top','height']].astype(int)\nsubmission_deepsort = submission_deepsort.dropna(axis=0)\nsubmission_deepsort.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T19:21:16.580062Z","iopub.status.idle":"2021-10-13T19:21:16.580795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Display video showing predictions\n\nLastly, if we want to review our predictions we can create a video to review the predictions using the `video_with_predictions` function from the `helmet_assignment` helper package.","metadata":{}},{"cell_type":"code","source":"if debug:\n    submission_deepsort['video'] = submission_deepsort['video_frame'].str.split('_').str[:3].str.join('_') + '.mp4'\n    debug_videos = submission_deepsort['video'].unique()\n    debug_labels = labels.query('video in @debug_videos')\n    scorer = NFLAssignmentScorer(debug_labels)\n    scorer.score(submission_deepsort)\n    for video in debug_videos:\n        # Create video showing predictions for one of the videos.\n        video_out = video_with_predictions(\n            f'../input/nfl-health-and-safety-helmet-assignment/train/{video}',\n            scorer.sub_labels.fillna(0))\n\n        frac = 0.60 # scaling factor for display\n        display(Video(data=video_out,\n                      embed=True,\n                      height=int(720*frac),\n                      width=int(1280*frac))\n               )","metadata":{"execution":{"iopub.status.busy":"2021-10-13T19:21:16.58192Z","iopub.status.idle":"2021-10-13T19:21:16.582638Z"},"trusted":true},"execution_count":null,"outputs":[]}]}