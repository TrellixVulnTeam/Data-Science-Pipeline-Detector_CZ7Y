{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Helmet Mapping + Deepsort\n\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"}},{"cell_type":"code","source":"# Install helmet-assignment helper code\n!pip install ../input/helmet-assignment-helpers/helmet-assignment-main/ > /dev/null 2>&1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from helmet_assignment.score import NFLAssignmentScorer, check_submission\nfrom helmet_assignment.features import add_track_features\nfrom helmet_assignment.video import video_with_predictions\nfrom IPython.display import Video, display","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline helmet mapping\nThis section uses the simple helmet mapping approach from the awesome notebook:\n\nhttps://www.kaggle.com/its7171/nfl-baseline-simple-helmet-mapping","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport itertools\nimport glob\nimport os\nimport sys\nimport torch\nimport cv2\nimport traceback\nimport time\nfrom sklearn.metrics import accuracy_score\nfrom tqdm.auto import tqdm\nfrom multiprocessing import Pool\nfrom matplotlib import pyplot as plt\nimport random\nimport torchvision\nimport shutil\nfrom joblib import Parallel, delayed\nfrom scipy.spatial.transform import Rotation\nfrom math import pi, ceil, sqrt\nfrom scipy.spatial import distance_matrix\nfrom scipy.optimize import linear_sum_assignment\nfrom statistics import mode\nfrom sklearn.cluster import k_means\nimport importlib.util","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Settings and loading data\n\nNote I've extracted `max_iter`, `DIG_STEP` and `DIG_MAX` to the top for easy experimentation. I've also modified the code to run in debug mode if running on the public test set.","metadata":{}},{"cell_type":"code","source":"# RANDOM_STATE = 43","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RANDOM_STATE = 373","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configurables\nn_debug_samples = 1\ndebug_video = None\n# debug_video = '57911_002492_Sideline.mp4'\n# RANDOM_STATE = np.random.randint(1000)\nRANDOM_STATE = 42","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONF_THRE = 0.4\nmax_iter = 1000\nDIG_STEP = 3\nDIG_MAX = DIG_STEP*10","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_test_videos = len(os.listdir('../input/nfl-health-and-safety-helmet-assignment/test/'))\n# Run in debug mode unless during submission\n\nif n_test_videos == 6:\n    debug = True\nelse:\n    debug = False\nif debug:\n    video_dir = '../input/nfl-health-and-safety-helmet-assignment/train/'\nelse:\n    video_dir = '../input/nfl-health-and-safety-helmet-assignment/test/'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Read in the data.\n\nBASE_DIR = '../input/nfl-health-and-safety-helmet-assignment'\n\nlabels = pd.read_csv(f'{BASE_DIR}/train_labels.csv')\nif debug:\n    tracking = pd.read_csv(f'{BASE_DIR}/train_player_tracking.csv')\n    helmets = pd.read_csv(f'{BASE_DIR}/train_baseline_helmets.csv')\nelse:\n    tracking = pd.read_csv(f'{BASE_DIR}/test_player_tracking.csv')\n    helmets = pd.read_csv(f'{BASE_DIR}/test_baseline_helmets.csv')\nhelmets['frame'] = helmets.video_frame.apply(lambda x: int(x.split('_')[-1]))\ntracking = add_track_features(tracking)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tracking","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_cols(df):\n    df['game_play'] = df['video_frame'].str.split('_').str[:2].str.join('_')\n    if 'video' not in df.columns:\n        df['video'] = df['video_frame'].str.split('_').str[:3].str.join('_') + '.mp4'\n    return df\nhelmets = add_cols(helmets)\nlabels = add_cols(labels)\nif debug:\n    # Select `n_debug_samples` worth of videos to debug with\n    if debug_video is None:\n        sample_videos = labels['video'].drop_duplicates() \\\n            .sample(n_debug_samples, random_state=RANDOM_STATE).tolist()\n    else:\n        sample_videos = [debug_video]\n    sample_gameplays = ['_'.join(x.split('_')[:2]) for x in sample_videos]\n    tracking = tracking[tracking['game_play'].isin(sample_gameplays)]\n    helmets = helmets[helmets['video'].isin(sample_videos)]\n    labels = labels[labels['video'].isin(sample_videos)]\ntracking.shape, helmets.shape, labels.shape","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tracking.player.isnull().any()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_nearest(tracking, value):\n    value = int(value)\n    array = np.asarray(tracking['est_frame']).astype(int)\n    unique_frames = np.unique(array)\n    idx = np.argmin(np.abs(unique_frames - value))\n    if value > unique_frames[idx]:\n        curr_frame = tracking[tracking['est_frame'] == unique_frames[idx]]\n        try:\n            next_frame = tracking[\n                    tracking['est_frame'] == unique_frames[idx + 1]]\n        except IndexError:\n            return curr_frame\n\n    elif value < unique_frames[idx]:\n        next_frame = tracking[tracking['est_frame'] == unique_frames[idx]]\n        try:\n            curr_frame = tracking[\n                tracking['est_frame'] == unique_frames[idx - 1]]\n        except IndexError:\n            return next_frame\n        \n    else:\n        return tracking[tracking['est_frame'] == unique_frames[idx]].reset_index(drop=True)\n    try:\n        next_frame = next_frame.set_index('player')\n        curr_frame = curr_frame.set_index('player')\n\n        diff = next_frame.est_frame.iloc[0] - curr_frame.est_frame.iloc[0]\n        cols = ['x','y', 'a', 'dir', 's', 'o', 'est_frame']\n        if diff != 0:\n            speed = (next_frame[cols] - curr_frame[cols]) / diff\n        else:\n            speed = 0\n        ret = next_frame.copy()\n        ret[cols] = curr_frame[cols] + (value - curr_frame.est_frame.iloc[0]) * speed\n        ret = ret.dropna(axis=0, subset=['est_frame'])\n        ret['est_frame'] = ret['est_frame'].astype(int)\n    except:\n        print(next_frame)\n        print(curr_frame)\n        print(ret['est_frame'])\n        raise\n    return ret.reset_index()\n\n\ndef norm_arr(a):\n    a = a-a.min()\n    a = a/a.max()\n    return a\n    \ndef dist(a1, a2):\n    return np.linalg.norm(a1-a2)\n\ndef dist_for_different_len(a1, a2):\n    assert len(a1) >= len(a2), f'{len(a1)}, {len(a2)}'\n    len_diff = len(a1) - len(a2)\n#     a2 = norm_arr(a2)\n    if len_diff == 0:\n#         a1 = norm_arr(a1)\n        return dist(a1,a2), ()\n    else:\n        min_dist = 10000\n        min_detete_idx = None\n        cnt = 0\n        del_list = list(itertools.combinations(range(len(a1)),len_diff))\n        if len(del_list) > max_iter:\n            del_list = random.sample(del_list, max_iter)\n        for detete_idx in del_list:\n            this_a1 = np.delete(a1, detete_idx)\n#             this_a1 = norm_arr(this_a1)\n            this_dist = dist(this_a1, a2)\n            #print(len(a1), len(a2), this_dist)\n            if min_dist > this_dist:\n                min_dist = this_dist\n                min_detete_idx = detete_idx\n                \n        return min_dist, min_detete_idx\n        \ndef rotate_arr(u, t, deg=True):\n    if deg == True:\n        t = np.deg2rad(t)\n    R = np.array([[np.cos(t), -np.sin(t)],\n                  [np.sin(t),  np.cos(t)]])\n    return  np.dot(R, u)\n\ndef dist_rot(tracking_df, a2):\n    tracking_df = tracking_df.sort_values('x')\n    x = tracking_df['x']\n    y = tracking_df['y']\n    min_dist = 10000\n    min_idx = None\n    min_x = None\n    for dig in range(-DIG_MAX,DIG_MAX+1,DIG_STEP):\n        arr = rotate_arr(np.array((x,y)), dig)\n        this_dist, this_idx = dist_for_different_len(np.sort(arr[0]), a2)\n        if min_dist > this_dist:\n            min_dist = this_dist\n            min_idx = this_idx\n            min_x = arr[0]\n    tracking_df['x_rot'] = min_x\n    player_arr = tracking_df.sort_values('x_rot')['player'].values\n    players = np.delete(player_arr,min_idx)\n    return min_dist, players\n\ndef dist_matrix(points, dense_view=True):\n    z = np.array([complex(c[0], c[1]) for c in points])\n    if dense_view:\n        return np.abs(z[..., np.newaxis] - z)[np.triu_indices(len(z),1)]\n    else:\n        return np.abs(z[..., np.newaxis] - z)\n","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the conditional above, x always shows the longitudinal\ndistance in the image, while y the latitudinal","metadata":{}},{"cell_type":"code","source":"def mapping_df_fallback(tracking, df, previous_mapped=None):\n    gameKey,playID,view,frame = df.video_frame.iloc[0].split('_')\n    gameKey = int(gameKey)\n    playID = int(playID)\n    frame = int(frame)\n    this_tracking = tracking[(tracking['gameKey']==gameKey) & (tracking['playID']==playID)]\n    this_tracking = find_nearest(this_tracking, frame)\n    len_this_tracking = len(this_tracking)\n    df['center_h_p'] = (df['left']+df['width']/2).astype(int)\n    df['center_h_m'] = (df['left']+df['width']/2).astype(int)*-1\n    if 'conf' in df.columns:\n        df = df[df['conf']>CONF_THRE].copy()\n    if len(df) > len_this_tracking:\n        df = df.tail(len_this_tracking)\n    df_p = df.sort_values('center_h_p').copy()\n    df_m = df.sort_values('center_h_m').copy()\n    \n    if view == 'Endzone':\n        this_tracking['x'], this_tracking['y'] = this_tracking['y'].copy(), this_tracking['x'].copy()\n    a2_p = df_p['center_h_p'].values\n    a2_m = df_m['center_h_m'].values\n\n    min_dist_p, min_detete_idx_p = dist_rot(this_tracking ,a2_p)\n    min_dist_m, min_detete_idx_m = dist_rot(this_tracking ,a2_m)\n    if min_dist_p < min_dist_m:\n        min_dist = min_dist_p\n        min_detete_idx = min_detete_idx_p\n        tgt_df = df_p\n    else:\n        min_dist = min_dist_m\n        min_detete_idx = min_detete_idx_m\n        tgt_df = df_m\n    #print(video_frame, len(this_tracking), len(df), len(df[df['conf']>CONF_THRE]), this_tracking['x'].mean(), min_dist_p, min_dist_m, min_dist)\n    tgt_df['label'] = min_detete_idx\n    return tgt_df[df.columns.tolist() + ['label']], this_tracking, {'fallback_mapping_used':True}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pykalman import KalmanFilter\nfrom typing import Dict, Iterable, List\nfrom numpy import ma\ndef cartesian_product(arrays):\n    la = len(arrays)\n    dtype = np.find_common_type([np.array(a).dtype for a in arrays], [])\n    arr = np.empty([len(a) for a in arrays] + [la], dtype=dtype)\n    for i, a in enumerate(np.ix_(*arrays)):\n        arr[..., i] = a\n    return arr.reshape(-1, la)\n\n\ndef get_observation_matrix(params_len):\n    return np.pad(np.eye(params_len), ((0,0),(0,2*params_len)))\ndef get_transition_matrix(params_len):\n    return np.eye(3 * params_len) + np.diag(\n                    np.ones(2 * params_len), params_len) + np.diag(\n                    0.5 * np.ones(params_len), 2 * params_len)\n\nclass KalmanFilterRoutine:\n    def __init__(self, params: List[str], init_frames=5):\n        self.init_frames = init_frames\n        self.kf = None\n        self.params_buffer = []\n        self.frame = 0\n        self.means = None\n        self.covariances = None\n        self.observation_matrix = None\n        self.transition_matrix = None\n        self.params = params\n        self.params_len = len(params)\n\n    @property\n    def is_ready(self):\n        return self.frame >= self.init_frames\n        \n    def update(self, **params):\n        self.frame += 1\n        if self.frame < self.init_frames:\n            self.params_buffer.append(params)\n        else:\n            if self.frame == self.init_frames:\n                self.params_buffer.append(params)\n                self.transition_matrix = get_transition_matrix(self.params_len)\n                self.observation_matrix = get_observation_matrix(self.params_len)\n                params_df = pd.DataFrame(self.params_buffer)\n                missing = [p for p in self.params if p not in params_df.columns]\n                params_df[missing] = np.nan\n                params_df = params_df.fillna(0)\n                params_df = params_df[self.params]\n                initial_state_mean = np.pad(params_df.mean(axis=0), (\n                    (0, 2*len(self.params))))\n                self.kf = KalmanFilter(transition_matrices=self.transition_matrix, \n                                       observation_matrices=self.observation_matrix,\n                                      initial_state_mean=initial_state_mean, random_state=RANDOM_STATE)\n                if len(params_df) < 3:\n                    params_df = pd.concat(\n                        [params_df] + [params_df.iloc[[-1]] for _ in range(3 - len(params_df))])\n                self.means, self.covariances = self.kf.filter(params_df.values)\n                self.means = self.means.tolist()\n                self.covariances = self.covariances.tolist()\n            if self.frame > self.init_frames:\n                observation = ma.asarray(np.array([params[k] if k in params else np.nan for k  in self.params]))\n                observation[np.isnan(observation)] = ma.masked\n                state_means, state_covs = self.kf.filter_update(\n                    self.means[-1],\n                    self.covariances[-1],\n                    observation =observation)\n                self.means.append(state_means)\n                self.covariances.append(state_covs)\n            self.updated_params =  np.array(self.means[-1][:len(self.params)])\n            self.updated_params_der = np.array(self.means[-1][len(self.params): 2 * len(self.params)])\n            self.updated_params_sder = np.array(self.means[-1][2 * len(self.params): 3 * len(self.params)])\n        ","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import deque\n\nclass ParamsCombinationsGenerator:\n    # Uses Kalman Filter with Taylor expansion up to the 2nd derivative\n    def __init__(self, params_ranges : Dict[str, Iterable],\n                 strictly_positive_params:List[str]=None,\n                 min_perturbations: Dict[str, float]=None,\n                 max_perturbations: Dict[str, float]=None,\n                 kalman_init=5,\n                 allowed_change_ratio=1, n_steps=5, \n                 obey_original_ranges=True, use_kalman=True, previous_frames_to_keep=10):\n        self.use_kalman = use_kalman\n        self.kalman_init = kalman_init\n        self.allowed_change_ratio = allowed_change_ratio\n        self.params_ranges = params_ranges\n        self.strictly_positive_params = strictly_positive_params\n        self.n_steps = n_steps\n        self.previous_frames_to_keep = previous_frames_to_keep\n        self.buffer_starts = deque(maxlen=previous_frames_to_keep)\n        self.buffer_ends = deque(maxlen=previous_frames_to_keep)\n        self.params_buffer = []\n        self.ori_options = [0, 1]\n        self.frame = 0\n        self.params = list(params_ranges.keys())\n        \n        self.means = None\n        self.covariances = None\n        self.observation_matrix = None\n        self.transition_matrix = None\n        self.obey_original_ranges = obey_original_ranges\n        if obey_original_ranges:\n            self.ranges_limits = np.array([[np.min(params_ranges[p]),\n                                            np.max(params_ranges[p])] for p in self.params])\n        if min_perturbations is None:\n            self.min_perturbations = np.zeros(len(self.params))\n        else:\n            self.min_perturbations = np.array([min_perturbations[x]  if x in min_perturbations else 0 for x in self.params])\n        if max_perturbations is not None:\n            self.max_perturbations = np.array([max_perturbations[x]  if x in max_perturbations else\n                                               np.inf for x in self.params])\n        else:\n            self.max_perturbations = np.zeros(len(self.params)) + np.inf\n        self.kf = KalmanFilterRoutine(self.params, self.kalman_init)\n        \n    \n    def reset(self):\n        self.kf = KalmanFilterRoutine(self.params, self.kalman_init)\n    \n    def zero_buffer(self):\n        self.buffer_starts = deque(maxlen=self.previous_frames_to_keep)\n        self.buffer_ends = deque(maxlen=self.previous_frames_to_keep)\n        \n    @property\n    def is_ready(self):\n        return self.kf.is_ready\n    \n    def update(self, **params):\n        self.prev_params = np.array([params[k] for k in self.params])\n        if not self.use_kalman:\n            return\n        self.kf.update(**params)\n\n    \n    def get_bounds(self):    \n        if not self.use_kalman or not self.kf.is_ready:\n            if not self.buffer_starts:\n                ranges = [self.params_ranges[x] for x in self.params]\n            else:\n                ranges = [(start,end) for start, end in zip(0.8 * np.mean(self.buffer_starts, axis=0), \n                                                            1.2 * np.mean(self.buffer_ends, axis=0))]\n        else:\n            diff1 = self.kf.updated_params - self.prev_params\n            diff2 = self.kf.updated_params_der + 0.5 * self.kf.updated_params_sder\n            \n            \n            changes =  np.maximum(np.abs(diff1 + diff2), self.min_perturbations)\n            \n            \n#             perturbations = np.minimum(np.maximum(\n#                 changes,self.min_perturbations), self.max_perturbations)\n            perturbations = changes\n#             starts = self.kf.updated_params - perturbations\n#             ends = self.kf.updated_params + perturbations\n            starts = self.prev_params - perturbations\n            ends = self.prev_params + perturbations\n#             print(stats, ends)\n#             if self.strictly_positive_params is not None:\n#                 flag = np.array([x in self.strictly_positive_params for x in self.params])\n#                 add = np.maximum(0, - starts[flag])\n#                 starts[flag] += add\n#                 ends[flag] += add\n#             if self.obey_original_ranges:\n\n#                 flag = np.array([x is not None for x in self.ranges_limits[:, 0]])\n#                 starts[flag] = np.maximum(self.ranges_limits[flag, 0], starts[flag])\n#                 ends[flag] = np.minimum(self.ranges_limits[flag, 1], ends[flag])\n            ranges = [(start, end) for start, end in zip(starts, ends)]\n            self.buffer_starts.append(starts)\n            self.buffer_ends.append(ends)\n                          \n        return ranges    \n    ","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SIDELINE_START_THRES = 50\nMAX_COORDS = (120, 53.33)\nMAX_COST_SIDELINE = 50\nMAX_COST_ENDZONE = 100\nfrom scipy.optimize import basinhopping, minimize\ndef cost_function(this_tracking, expanded, im_centers, \n                  camera_height, camera_length, max_p, \n                  xdig, zdig, scaling, max_cost_thres=None,\n                  ret_cost_only=False):\n    assert ~np.any(np.isnan([xdig]))\n\n\n    z_rot = Rotation.from_rotvec([0, 0, zdig]).as_matrix()\n    #camera sits somewhere near the middle of the appropriate x side of the field\n    camera_pos = np.array([camera_length,0,camera_height])\n\n    expanded = expanded - camera_pos \n\n    z_rot = Rotation.from_rotvec([0,0,zdig]).as_matrix()\n    z_rotated = (z_rot @ expanded.T).T\n    x_rot = Rotation.from_rotvec([xdig,0,0]).as_matrix()\n    x_rotated =  (x_rot @ z_rotated.T).T\n    x_rotated = x_rotated[:,[0,2]]\n    opt_params = None\n    opt_rl_remapped = None\n    arr = x_rotated\n    scaled = scaling * arr\n    # the origin is now assumed to be at the center of the image, so we need to move it to the bottom left first\n    scaled = scaled + np.array([1280,720])/2\n    # and then revert the y axis\n    scaled[:,1] = 720 - scaled[:,1]\n    if not ret_cost_only:\n        reduced_tracking = this_tracking.copy()\n        \n    d = distance_matrix(im_centers,\n                        scaled)\n    match_to, match_from = linear_sum_assignment(d)\n    match_to = match_to[np.argsort(match_from)]\n    match_from = np.sort(match_from)\n    as_costs = d[match_to, match_from]\n    cost = np.mean(as_costs)\n\n\n    if max_cost_thres is not None:\n        mask = as_costs < max_cost_thres\n        match_from = match_from[mask]\n        match_to = match_to[mask]\n        as_costs = as_costs[mask]\n        \n    if ret_cost_only:\n        return cost     \n    \n    reduced_tracking = reduced_tracking.iloc[match_from]\n    reduced_tracking[['x2im', 'y2im']] = scaled[match_from, :]\n    params = dict(xdig=xdig, zdig=zdig, scaling=scaling,\n                  camera_height=camera_height,\n                  camera_length=camera_length,\n                  cost=cost, \n                  match_from=match_from,\n                  match_to=match_to,\n                  match_costs=as_costs,\n                  max_cost=(as_costs.max() if np.any(as_costs)\n                            else max_cost_thres))\n    return params, reduced_tracking\n\n    \n        ","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mapping_df(combs_generator, tracking, df, previous_mapped=None, \n               available_oris=(0,1), ignore_starting_preproc=False,\n               force_local_minimize=True, check_mapping=False, ratio = 0.8):\n    gameKey,playID,view,frame = df.video_frame.iloc[0].split('_')\n    gameKey = int(gameKey)\n    playID = int(playID)\n    frame = int(frame)\n    this_tracking = tracking[(tracking['gameKey']==gameKey) & (tracking['playID']==playID)]\n    this_tracking = find_nearest(this_tracking, frame)\n    \n    max_p = MAX_COORDS\n    if view == 'Endzone':\n        max_cost_thres = MAX_COST_ENDZONE\n    else:\n        max_cost_thres = MAX_COST_SIDELINE\n    same_sgns = 0 # the projected axes on the image need to be reflected as of x or as of y\n    if view == 'Endzone':\n        this_tracking['x'], this_tracking['y'] = this_tracking['y'].copy(), this_tracking['x'].copy()\n        max_p = max_p[::-1]\n        # the projected axes need to be reflected both as of x and as of y or stay as is\n        same_sgns = 1 \n        \n\n    if 'conf' in df.columns:\n        df = df[df['conf']>CONF_THRE].copy()\n    if not ignore_starting_preproc and (view == 'Sideline') and not combs_generator.is_ready:\n        inc_mask = (df['top'] >= SIDELINE_START_THRES) & (df['top'] < 720 - SIDELINE_START_THRES)\n        if not np.all(inc_mask):\n            print(f\"Removing {(~inc_mask).sum()} bounding boxes that reside in the top or bottom edge of the screen\")\n            df = df[inc_mask].copy()\n        \n    im_centers = df[['left', 'top']].values+ (df[['width', 'height']]/2).values\n    rl_centers = this_tracking[['x','y']].values\n    \n    im_centers = im_centers\n    \n    opt_params = None\n    costs = {0: [], 1: []}\n    min_cost = 1e7\n    opt_params = None\n    for change_ori in available_oris:\n        # assume 1 yard average height \n        expanded = np.hstack([rl_centers,\n                              1 + np.zeros((len(rl_centers),1))])\n        if same_sgns:\n            c_translation = np.zeros(2)\n            c_scaling = np.ones(2)\n            if change_ori:\n                c_translation = max_p\n                c_scaling = - np.ones(2)\n        else:\n            if change_ori:\n                c_translation = np.array([0, max_p[1]])\n                c_scaling = np.array([1, -1])\n            else:\n                c_translation = np.array([max_p[0], 0])\n                c_scaling = np.array([-1, 1])\n        expanded[:, :2] = c_scaling * expanded[:, :2] + c_translation\n        expanded[:, 1] = max_p[1] - expanded[:, 1]\n\n\n        to_opt = combs_generator.params\n        x0 = [np.mean(combs_generator.params_ranges[x]) for x in to_opt]\n        bounds = combs_generator.get_bounds()\n        min_func = lambda p: cost_function(\n            this_tracking=this_tracking,\n            expanded=expanded,\n            im_centers=im_centers,\n            camera_length=p[to_opt.index('camera_length')],\n            camera_height=p[to_opt.index('camera_height')],\n            max_p=max_p,\n            zdig=p[to_opt.index('zdig')],                                 \n            xdig=p[to_opt.index('xdig')],\n            scaling=p[to_opt.index('scaling')],\n            ret_cost_only=True,\n            max_cost_thres=None)\n        \n        if force_local_minimize or combs_generator.is_ready:\n            ret = minimize(min_func, x0=x0,\n                bounds=bounds)\n#             if ret.fun > max_cost_thres * ratio:\n#                 print(f'Resetting due to high cost({ret.fun} > {max_cost_thres * ratio})')\n#                 combs_generator.reset()\n#                 bounds = combs_generator.get_bounds()\n        if not combs_generator.is_ready and not force_local_minimize:\n            ret = basinhopping(min_func,\n                               x0=x0,niter=100 if combs_generator.buffer_starts else 5000,\n                               niter_success=5 if combs_generator.buffer_starts else 100,\n                minimizer_kwargs=dict(bounds=bounds))\n            if ret.fun > max_cost_thres * ratio:\n                print('Basin Hopping Unsuccessful! Resetting due to high cost('\n                      f'{ret.fun} > {max_cost_thres * ratio})')\n                combs_generator.reset()\n                    \n        cost = ret.fun\n        if cost < min_cost:\n            xdig = ret.x[to_opt.index('xdig')]\n            zdig = ret.x[to_opt.index('zdig')]\n            scaling = ret.x[to_opt.index('scaling')]\n            camera_height = ret.x[to_opt.index('camera_height')]\n            camera_length = ret.x[to_opt.index('camera_length')]\n            found_params, found_tracking = cost_function(\n                this_tracking=this_tracking,\n                expanded=expanded, \n                im_centers=im_centers,\n                camera_height=camera_height,\n                camera_length=camera_length,\n                max_p=max_p,\n                zdig=zdig,\n                xdig=xdig,\n                scaling=scaling,\n                ret_cost_only=False, \n                max_cost_thres=max_cost_thres)\n            min_cost = cost\n            opt_ret = ret\n            opt_params = found_params\n            opt_params['change_ori'] = change_ori\n            opt_tracking = found_tracking\n\n    if opt_params is None:\n        combs_generator.reset()\n        if debug:\n            print('Failure')\n            print(ret)\n        raise\n    combs_generator.update(zdig=opt_params['zdig'],\n                           xdig=opt_params['xdig'],\n                           scaling=opt_params['scaling'],\n                           camera_height=opt_params['camera_height'],\n                           camera_length=opt_params['camera_length'])\n    match_to = opt_params['match_to']\n    match_from = opt_params['match_from']\n    match_costs = opt_params['match_costs']\n    \n    df['view'] = view\n    df[['im_x_remapped', 'im_y_remapped']] = im_centers\n \n\n    df_cols = [col for col in df.columns if col not in ['x','y','x2im', 'y2im', 'player']]\n            \n    ret = pd.concat(\n        [\n            df[df_cols].iloc[match_to].reset_index(drop=True),\n            opt_tracking[['x','y','x2im', 'y2im', 'player']].reset_index(drop=True)\n        ],\n        axis=1).set_index(df.iloc[match_to].index)\n    ret['cost'] = match_costs\n    ret = pd.concat([ret, df.loc[[x for x in df.index if x not in ret.index], df_cols]]) # null labels for unmatched\n    ret['cost'] = ret['cost'].fillna(np.inf)\n    ret.rename(columns={'player':'label'},inplace=True)\n    ret = ret.sort_values('left')\n    \n    return ret[df.columns.tolist() + ['label', 'cost']], opt_tracking, opt_params\n    \ndef compare_and_assign(ret, previous_mapped):\n    hist_dist_mat = distance_matrix(\n    ret[['left', 'top']].values,\n    previous_mapped[['left', 'top']].values)\n\n    matched_to, matched_from = linear_sum_assignment(hist_dist_mat)\n    costs = np.array([ret.iloc[matched_to]['cost'].values,\n               previous_mapped.iloc[matched_from]['cost'].values])\n    to_select = np.argmin(costs,axis=0)\n    to_keep_previous = to_select == 1\n    \n    matched_to_flag = np.zeros(len(ret)).astype(bool)\n\n    matched_from = matched_from[to_keep_previous]\n    matched_to = matched_to[to_keep_previous]\n    matched_to_flag[matched_to] = True\n    labels_to_keep_previous = previous_mapped.iloc[matched_from].label\n    to_change_df = ret[matched_to_flag] .copy()\n    to_change_df['cost'] = (\n        to_change_df['cost'].values +\n        previous_mapped.iloc[matched_from]['cost'].values) / 2\n    to_change_df['label'] = previous_mapped.iloc[matched_from]['label'].values\n    to_keep_df = ret[~matched_to_flag].copy()\n    to_reassign_flag_small = to_keep_df['label'].isin(to_change_df['label'].values).values\n    to_reassign_flag = (~matched_to_flag & ret['label'].isin(to_change_df['label'].values)).values\n    ret = pd.concat([to_change_df, to_keep_df[~to_reassign_flag_small]],axis=0)\n    mask = ~ret['label'].isnull()\n    if len(ret[mask]) != len(ret[mask].drop_duplicates('label')):\n        display(to_change_df)\n        display(to_keep_df[~to_reassign_flag_small])\n        print(matched_to)\n        raise\n    return ret, to_reassign_flag","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.widgets import Slider\nclass MappingCheck:\n    def __init__(self, img, change_ori, df, tracking, max_cost_thres=None, init=None):\n        \n        gameKey,playID,view,frame = df.video_frame.iloc[0].split('_')\n        gameKey = int(gameKey)\n        playID = int(playID)\n        frame = int(frame)\n        m = Mapping(tracking, view)\n        combs_generator = m.combinations_generator\n        this_tracking = tracking[(tracking['gameKey']==gameKey) & (tracking['playID']==playID)]\n        this_tracking = find_nearest(this_tracking, frame)\n\n        max_p = MAX_COORDS\n        if max_cost_thres is None:\n            if view == 'Endzone':\n                max_cost_thres = MAX_COST_ENDZONE\n            else:\n                max_cost_thres = MAX_COST_SIDELINE\n        same_sgns = 0 # the projected axes on the image need to be reflected as of x or as of y\n        if view == 'Endzone':\n            this_tracking['x'], this_tracking['y'] = this_tracking['y'].copy(), this_tracking['x'].copy()\n            max_p = max_p[::-1]\n            # the projected axes need to be reflected both as of x and as of y or stay as is\n            same_sgns = 1 \n\n\n        if 'conf' in df.columns:\n            df = df[df['conf']>CONF_THRE].copy()\n\n        im_centers = df[['left', 'top']].values+ (df[['width', 'height']]/2).values\n        rl_centers = this_tracking[['x','y']].values\n\n        im_centers = im_centers\n\n        opt_params = None\n        costs = {0: [], 1: []}\n        min_cost = 1e7\n        opt_params = None\n        # assume 1 yard average height \n        expanded = np.hstack([rl_centers,\n                              1 + np.zeros((len(rl_centers),1))])\n        self.players = this_tracking['player'].values\n        if same_sgns:\n            c_translation = np.zeros(2)\n            c_scaling = np.ones(2)\n            if change_ori:\n                c_translation = max_p\n                c_scaling = - np.ones(2)\n        else:\n            if change_ori:\n                c_translation = np.array([0, max_p[1]])\n                c_scaling = np.array([1, -1])\n            else:\n                c_translation = np.array([max_p[0], 0])\n                c_scaling = np.array([-1, 1])\n        expanded[:, :2] = c_scaling * expanded[:, :2] + c_translation\n        expanded[:, 1] = max_p[1] - expanded[:, 1]\n\n        rot_dist_matrix = dist_matrix(\n            expanded[:,:2], dense_view=False)   \n\n        to_opt = combs_generator.params\n        x0 = [np.mean(combs_generator.params_ranges[x]) for x in to_opt]\n        bounds = combs_generator.get_bounds()\n        ranges = {k: bounds[c] for c,k in enumerate(to_opt)}\n        import matplotlib\n        cmap = matplotlib.cm.get_cmap('jet')\n        self.players_colors = [(np.array(cmap(x/len(self.players)))[:3] * 255).astype(int) for x in range(len(self.players))]\n        \n        from matplotlib.patches import Patch\n\n        self.legend_elements = [Patch(facecolor=color.astype(float)/255, edgecolor='r',\n                                 label=player) for player,color in zip(self.players, self.players_colors)]\n        \n        \n        self.fig, self.ax = plt.subplots()\n        plt.subplots_adjust(left=0.35, bottom=0.35)\n        self.img = img\n        self.img_handle = self.ax.imshow(img)\n        self.expanded = expanded\n        self.im_centers = im_centers\n        self.max_p = max_p\n        self.this_tracking = this_tracking\n        self.ax.margins(x=0)\n\n        axcolor = 'lightgoldenrodyellow'\n        self.axch = plt.axes([0.25, 0.1, 0.65, 0.03], facecolor=axcolor)\n        self.axcl = plt.axes([0.25, 0.15, 0.65, 0.03], facecolor=axcolor)\n        self.axsc = plt.axes([0.25, 0.2, 0.65, 0.03], facecolor=axcolor)\n        self.axxd = plt.axes([0.25, 0.25, 0.65, 0.03], facecolor=axcolor)\n        self.axzd = plt.axes([0.25, 0.3, 0.65, 0.03], facecolor=axcolor)\n        self.axct = plt.axes([0.025, 0.5, 0.15, 0.15], facecolor=axcolor)\n        self.text = self.axct.text(0, 0,  '')\n        self.init_sliders(ranges, init=init)\n        plt.show()\n        self.update(None)\n\n\n\n    def init_sliders(self, ranges, init=None):\n        if init is None:\n            init = {k: np.mean(ranges[k]) for k in ranges}\n        self.ch = Slider(self.axch, 'CamH', ranges['camera_height'][0],\n                              ranges['camera_height'][-1], valinit=init['camera_height'])\n        self.cl = Slider(self.axcl, 'CamL', ranges['camera_length'][0],\n                              ranges['camera_length'][-1], valinit=init['camera_length'])\n        self.sc = Slider(self.axsc, 'Scal', ranges['scaling'][0],\n                              ranges['scaling'][-1], valinit=init['scaling'])\n        self.xd = Slider(self.axxd, 'Xdig', ranges['xdig'][0],\n                              ranges['xdig'][-1], valinit=init['xdig'])\n        self.zd = Slider(self.axzd, 'Zdig', ranges['zdig'][0],\n                              ranges['zdig'][-1], valinit=init['zdig'])\n        self.ch.on_changed(self.update)\n        self.cl.on_changed(self.update)\n        self.sc.on_changed(self.update)\n        self.xd.on_changed(self.update)\n        self.zd.on_changed(self.update)\n        self.leg = None\n\n\n\n    def update(self, val):\n        camera_height = self.ch.val\n        camera_length = self.cl.val\n        scaling=self.sc.val\n        xdig=self.xd.val\n        zdig=self.zd.val\n        opt_params, opt_tracking = cost_function(self.this_tracking, self.expanded, \n                                                self.im_centers, camera_height,\n                                                camera_length,\n                                                self.max_p, xdig, zdig, scaling)\n        cp = self.img.copy()\n        if opt_params is not None:\n            inds = []\n            for m, (index, row) in zip(opt_params['match_from'], opt_tracking.iterrows()):\n                x = row[['x2im', 'y2im']].values.astype(float)\n                if np.all(np.isfinite(x)):\n                    if (x[0] > 1280) or (x[0]<0):\n                        continue\n                    if (x[1] > 720) or (x[1]<0):\n                        continue\n                    inds.append(np.where(self.this_tracking.index==index)[0][0])\n                    cp[int(x[1]) - 10: int(x[1]) + 10, int(x[0])-10:int(x[0]) + 10, :] = self.players_colors[\n                        inds[-1]]\n            l_subset = [self.legend_elements[x] for x in inds]\n        if self.leg is not None:\n            self.leg.remove()\n        if opt_params is not None:\n            self.leg = self.axct.legend(handles=l_subset, loc='center')     \n            self.text.set_text(str(opt_params['cost']) + '\\n' + str(len(l_subset)) )\n        self.img_handle.set_data(cp)\n        self.fig.canvas.draw_idle()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Mapping:\n    def __init__(self, tracking, view, use_kalman=True, use_previous=True, available_oris=(0,1),\n                 init_frames=3, ignore_starting_preproc=False):\n        self.tracking = tracking\n        self.use_kalman = use_kalman\n        self.use_previous = use_previous\n        self.available_oris = available_oris\n        self.init_frames = init_frames\n        self.ignore_starting_preproc = ignore_starting_preproc\n        \n        self.buffer_max_cost = deque(maxlen=30)\n        self.buffer_ori = []\n        self.buffer_costs = []\n        dig_step = np.deg2rad(DIG_STEP)\n        dig_max = np.deg2rad(DIG_MAX)\n        step_size = int(2 * DIG_MAX / DIG_STEP)\n        length = MAX_COORDS[0] if view=='Sideline' else MAX_COORDS[1]\n        self.params_ranges = dict(zdig=[- pi / 3, pi / 3],\n                                  xdig=[0, pi/3],\n                                  camera_height=[15, 50],\n                                  camera_length=[0.3 * length, 0.7 * length],\n                                  scaling=[20, 80])\n        self.min_perturbations = dict(zdig=np.deg2rad(5), xdig=np.deg2rad(5), scaling=0.1, camera_length=0.1,\n                                      camera_height=0.1)\n        self.max_perturbations = dict(zdig=np.deg2rad(10), xdig=np.deg2rad(10), scaling=1, camera_length=0.2,\n                                      camera_height=0.2)\n        \n        self.combinations_generator = ParamsCombinationsGenerator(\n            self.params_ranges, strictly_positive_params='scaling', min_perturbations=self.min_perturbations,\n        max_perturbations=self.max_perturbations, use_kalman=use_kalman, kalman_init=1, obey_original_ranges=False)\n        self.previous_df = None\n        self.max_cost_thres=None\n        self.frame = 0\n    \n    def __call__(self, this_df):\n        try:\n            self.previous_df, this_tracking, opt_params = mapping_df(\n                self.combinations_generator,\n                self.tracking, this_df, \n                (self.previous_df if self.use_previous else None),\n                available_oris=self.available_oris,\n                force_local_minimize=len(self.available_oris)==2, # we dont need much of accuracy when detecting orientation\n                ignore_starting_preproc=self.ignore_starting_preproc,\n            )\n            \n            \n            if len(self.buffer_ori) < self.init_frames:\n                self.buffer_ori.append(opt_params['change_ori'])\n                self.buffer_max_cost.append(opt_params['max_cost'])\n                self.buffer_costs.append(opt_params['cost'])\n            if len(self.buffer_ori) == self.init_frames:\n                ori_df = pd.DataFrame({'ori': self.buffer_ori, 'cost': self.buffer_costs})\n                mean_costs = ori_df.groupby('ori').median()\n                if len(self.available_oris) == 2:\n                    self.available_oris = [mean_costs.iloc[np.argmax(mean_costs)]]\n                self.max_cost_thres = 1.1 * np.max(self.buffer_max_cost)\n        except KeyboardInterrupt:\n            raise\n        except:\n            traceback.print_exc()\n            self.previous_df, this_tracking, opt_params = mapping_df_fallback(self.tracking, this_df)\n            opt_params['error'] = traceback.format_exc()\n        return self.previous_df, this_tracking, opt_params\n\n\ndef apply_on_video(tracking, video_df):\n    submission_df_list = []\n    df_list = list(video_df.groupby('frame'))\n    view = video_df.iloc[0]['video_frame'].split('_')[2]\n    ori_mapping = Mapping(tracking, view=view, use_kalman=False, use_previous=False, ignore_starting_preproc=True)\n    print('Detecting video view orientation...')\n    for frame in tqdm(np.linspace(1, len(df_list)-1, ori_mapping.init_frames).astype(int)):\n        _, this_df = df_list[frame]\n        ori_mapping(this_df)\n    detected_ori = [mode(ori_mapping.buffer_ori)] if ori_mapping.buffer_ori else [0,1]\n    \n    print('Detected orientation:',detected_ori, '. Mapping...')\n    \n    mapping = Mapping(tracking,view=view,\n                      available_oris=detected_ori)\n    opt_params_dict = {}\n    try:\n        for frame, this_df in tqdm(df_list):\n            df, _, opt_params = mapping(this_df)\n            if debug:\n                opt_params_dict[frame] = opt_params\n            submission_df_list.append(df)\n        submission_df = pd.concat(submission_df_list)\n    except KeyboardInterrupt:\n        if debug:\n            with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n                display(pd.DataFrame(opt_params_dict).T)\n        raise\n    return submission_df","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# videos_dfs = list(helmets.groupby('video'))\n# if len(videos_dfs) == 1:\n#     submission_df_list = [apply_on_video(tracking, videos_dfs[0][1])]\n# else:\n#     submission_df_list = Parallel(n_jobs=-1)(delayed(apply_on_video)(tracking, video_df) for _, video_df in tqdm(videos_dfs))\n# submission_df = pd.concat(submission_df_list)\n# submission_df.to_csv('submission-baseline.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if debug:\n#     scorer = NFLAssignmentScorer(labels)\n#     baseline_score = scorer.score(submission_df[~submission_df['label'].isnull()])\n#     print(f\"validation score {baseline_score:0.4f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Score the predictions before applying deepsort postprocessing\n\nThe scores are roughly ~0.5.","metadata":{}},{"cell_type":"markdown","source":"# Deepsort Postprocessing\n\nDeepsort is a popular framework for object tracking within video. \n- [This blog post](https://nanonets.com/blog/object-tracking-deepsort/\n) shows some examples of it being put to use.\n- This notebook shows how to apply deepsort to this helmet dataset: https://www.kaggle.com/s903124/nfl-helmet-with-yolov5-deepsort-starter\n- You can also read the paper for deepsort here: https://arxiv.org/pdf/1703.07402.pdf\n\nThe approach is fairly simple:\n1. Step through each frame in a video and apply the deepsort algorithm. This clusters helmets across frames when it is the same player/helmet.\n2. Group by each of these deepsort clusters - and pick the most common label for that cluster. Then override all of the predictions for that helmet to the same player.","metadata":{}},{"cell_type":"markdown","source":"## Importing Yolov5 pretrained from dataset","metadata":{}},{"cell_type":"code","source":"\nmodel = torch.hub.load('../input/yolov5-git/yolov5-master/yolov5-master', 'yolov5m', pretrained='false', classes=1,  source='local')\nmodel = torch.load('../input/helmet-yolov5m/helmet_yolov5m.pt', map_location='cuda')['model']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing Deepsort from dataset\nBecause your submission is not allowed to use internet access, you can reference the deepsort codebase from the attached dataset. Deepsort also has a dependency of `easydict` which I've also added as a dataset.","metadata":{}},{"cell_type":"code","source":"\nsys.path.append('../input/easydict-master/easydict-master/')\n# https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch\nmodel_import_path = '../input/yolov5-deepsort-pytorch/Yolov5_DeepSort_Pytorch-master/Yolov5_DeepSort_Pytorch-master/deep_sort_pytorch/'\nsys.path.append(model_import_path)\nfrom deep_sort.deep_sort import DeepSort\nspec = importlib.util.spec_from_file_location(\"parser\", os.path.join(model_import_path, 'utils', 'parser.py'))\nfoo = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(foo)\nget_config = foo.get_config","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deepsort config\n\nDeepsort uses a config yaml file for some settings. These are just the default configs and could be improved.","metadata":{}},{"cell_type":"code","source":"%%writefile deepsort.yaml\n\nDEEPSORT:\n  REID_CKPT: \"../input/yolov5-deepsort-pytorch/ckpt.t7\"\n  MAX_DIST: 0.1\n  MIN_CONFIDENCE: 0.4\n  NMS_MAX_OVERLAP: 0.5\n  MAX_IOU_DISTANCE: 0.9\n  MAX_AGE: 15\n  N_INIT: 1\n  NN_BUDGET: 100\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Deepsort Config","metadata":{}},{"cell_type":"code","source":"cfg = get_config()\ncfg.merge_from_file('deepsort.yaml')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nHelper functions from yolov5 to plot deepsort labels.\n\"\"\"\n\ndef compute_color_for_id(label):\n    \"\"\"\n    Simple function that adds fixed color depending on the id\n    \"\"\"\n    palette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n\n    color = [int((p * (label ** 2 - label + 1)) % 255) for p in palette]\n    return tuple(color)\n\ndef plot_one_box(x, im, color=None, label=None, line_thickness=3):\n    # Plots one bounding box on image 'im' using OpenCV\n    assert im.data.contiguous, 'Image not contiguous. Apply np.ascontiguousarray(im) to plot_on_box() input image.'\n    tl = line_thickness or round(0.002 * (im.shape[0] + im.shape[1]) / 2) + 1  # line/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(im, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label: \n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(im, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(im, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n    return im","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Class to detect the playing region\nApplies morphological operations onto image and try to remove edge cases based on the white lines in the field borders","metadata":{}},{"cell_type":"code","source":"def markers2img(markers):\n    markers = 255 * (markers - np.min(markers)) / np.maximum(0.1, (np.max(markers) - np.min(markers)))\n    return cv2.applyColorMap(markers.astype(np.uint8), cv2.COLORMAP_JET)\n\nclass ValidRegionTracker:\n    def __init__(self):\n        self.state_surface = None\n        self.boundary_flag = None\n        self.input_shape = (512,512)\n        self.mask5 = np.ones((5,5), np.uint8)\n        self.mask3 = np.ones((3,3), np.uint8)\n        self.mask15 = np.ones((15,15), np.uint8)\n        self.large_mask = np.ones((self.input_shape[1]//5,self.input_shape[0]//5), np.uint8)\n        \n    def detect(self, image_data):\n        og_shape = image_data.shape[:2][::-1]\n        mask = self.get_mask(cv2.resize(image_data, self.input_shape))\n        return cv2.resize(\n                    mask.astype(np.uint8), og_shape, 0, 0, cv2.INTER_NEAREST) > 0\n\n    def get_mask(self, image_data):\n\n        \n        hls_img = cv2.cvtColor(image_data,  cv2.COLOR_RGB2HLS)\n        white_obj_mask = cv2.threshold(hls_img[:,:,1],150,1, cv2.THRESH_BINARY)[1]\n        seeds = cv2.erode(\n            cv2.morphologyEx(white_obj_mask.astype(np.uint8), cv2.MORPH_OPEN, self.mask5),\n            self.mask3)\n        seeds[3:-3,3:-3] = 0\n        sure_fg = seeds\n        unknown = cv2.subtract(cv2.threshold(hls_img[:,:,1],100,1,cv2.THRESH_BINARY)[1],sure_fg)\n        _, markers = cv2.connectedComponents(sure_fg)\n        markers = markers+1\n        markers[unknown==1] = 0\n        img = cv2.cvtColor(white_obj_mask * 255, cv2.COLOR_GRAY2RGB)\n        cv2.watershed(img, markers)\n        white_obj_on_im_edges = (markers>1).astype(np.uint8)\n        white_obj_on_im_edges = cv2.morphologyEx(white_obj_on_im_edges, cv2.MORPH_CLOSE,self.mask15)\n        white_obj_on_im_edges = cv2.morphologyEx(white_obj_on_im_edges, cv2.MORPH_OPEN, self.mask15)\n        from math import pi, sqrt\n        boundary_flag = np.zeros(white_obj_on_im_edges.shape[:2])\n        if self.boundary_flag is not None:\n            boundary_flag = cv2.erode(self.boundary_flag, self.mask15)\n        to_detect_edges = white_obj_on_im_edges\n        edges = cv2.morphologyEx(\n                cv2.Canny(to_detect_edges * 255,0,1,apertureSize = 3),cv2.MORPH_CLOSE,\n                self.mask15)\n        lines = cv2.HoughLinesP(\n            edges,\n            1, pi/180,100,maxLineGap=3\n            )\n        if lines is not None:\n            # keep 4 largest\n            lines_lengths = [ sqrt((x2-x1)**2 + (y2-y1)**2) for (x1,y1,x2,y2) in [l[0] for l in lines]]\n            lines = lines[np.argsort(lines_lengths)[-4:],:,:]\n            for line in lines:\n                x1,y1,x2,y2 = line[0]\n                cv2.line(boundary_flag,(x1,y1),(x2,y2),1,2)\n        sure_fg = boundary_flag.astype(np.uint8)\n        unknown = cv2.subtract(white_obj_on_im_edges,sure_fg)\n        _, markers = cv2.connectedComponents(sure_fg)\n        markers = markers+1\n        markers[unknown==1] = 0\n        img = cv2.cvtColor(\n            white_obj_on_im_edges*255, cv2.COLOR_GRAY2RGB)\n        cv2.watershed(\n            img, markers)\n        markers = (markers > 1).astype(np.uint8)\n        self.boundary_flag = cv2.morphologyEx(\n            markers, cv2.MORPH_CLOSE, self.large_mask)\n\n        return self.boundary_flag == 0\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions to apply deepsort to helmet boxes.\n\nBelow are two functions `deepsort_helmets` which runs deepsort across a video. There is a lot of room for improving this function. The merging of deepsort labels onto the original helmet boxes is currently done in a very crude manner.\n\n`add_deepsort_label_col` mapps the most common label to each deepsort cluster.","metadata":{}},{"cell_type":"markdown","source":"## Yolo5v code copy pasted for NMS and Image Resizing","metadata":{}},{"cell_type":"code","source":"\ndef xywh2xyxy(x):\n    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n    return y\ndef non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False,\n                        labels=(), max_det=300):\n    \"\"\"Runs Non-Maximum Suppression (NMS) on inference results\n\n    Returns:\n         list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n    \"\"\"\n\n    nc = prediction.shape[2] - 5  # number of classes\n    xc = prediction[..., 4] > conf_thres  # candidates\n\n    # Checks\n    assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n    assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n\n    # Settings\n    min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n    time_limit = 10.0  # seconds to quit after\n    redundant = True  # require redundant detections\n    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n    merge = False  # use merge-NMS\n\n    t = time.time()\n    output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n    for xi, x in enumerate(prediction):  # image index, image inference\n        # Apply constraints\n        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n        x = x[xc[xi]]  # confidence\n\n        # Cat apriori labels if autolabelling\n        if labels and len(labels[xi]):\n            l = labels[xi]\n            v = torch.zeros((len(l), nc + 5), device=x.device)\n            v[:, :4] = l[:, 1:5]  # box\n            v[:, 4] = 1.0  # conf\n            v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n            x = torch.cat((x, v), 0)\n\n        # If none remain process next image\n        if not x.shape[0]:\n            continue\n\n        # Compute conf\n        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n\n        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n        box = xywh2xyxy(x[:, :4])\n\n        # Detections matrix nx6 (xyxy, conf, cls)\n        if multi_label:\n            i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n            x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n        else:  # best class only\n            conf, j = x[:, 5:].max(1, keepdim=True)\n            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n\n        # Filter by class\n        if classes is not None:\n            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n\n        # Apply finite constraint\n        # if not torch.isfinite(x).all():\n        #     x = x[torch.isfinite(x).all(1)]\n\n        # Check shape\n        n = x.shape[0]  # number of boxes\n        if not n:  # no boxes\n            continue\n        elif n > max_nms:  # excess boxes\n            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n\n        # Batched NMS\n        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n        if i.shape[0] > max_det:  # limit detections\n            i = i[:max_det]\n        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n            # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n            iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n            weights = iou * scores[None]  # box weights\n            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n            if redundant:\n                i = i[iou.sum(1) > 1]  # require redundancy\n\n        output[xi] = x[i]\n        if (time.time() - t) > time_limit:\n            print(f'WARNING: NMS time limit {time_limit}s exceeded')\n            break  # time limit exceeded\n\n    return output\n\ndef letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n    # Resize and pad image while meeting stride-multiple constraints\n    shape = im.shape[:2]  # current shape [height, width]\n    if isinstance(new_shape, int):\n        new_shape = (new_shape, new_shape)\n\n    # Scale ratio (new / old)\n    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n        r = min(r, 1.0)\n\n    # Compute padding\n    ratio = r, r  # width, height ratios\n    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n    if auto:  # minimum rectangle\n        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n    elif scaleFill:  # stretch\n        dw, dh = 0.0, 0.0\n        new_unpad = (new_shape[1], new_shape[0])\n        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n\n    dw /= 2  # divide padding into 2 sides\n    dh /= 2\n\n    if shape[::-1] != new_unpad:  # resize\n        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n    return im, ratio, (dw, dh)\ndef clip_coords(boxes, shape):\n    # Clip bounding xyxy bounding boxes to image shape (height, width)\n    if isinstance(boxes, torch.Tensor):  # faster individually\n        boxes[:, 0].clamp_(0, shape[1])  # x1\n        boxes[:, 1].clamp_(0, shape[0])  # y1\n        boxes[:, 2].clamp_(0, shape[1])  # x2\n        boxes[:, 3].clamp_(0, shape[0])  # y2\n    else:  # np.array (faster grouped)\n        boxes[:, [0, 2]] = boxes[:, [0, 2]].clip(0, shape[1])  # x1, x2\n        boxes[:, [1, 3]] = boxes[:, [1, 3]].clip(0, shape[0])  # y1, y2\n\ndef scale_coords(img1_shape, coords, img0_shape, ratio_pad=None):\n    # Rescale coords (xyxy) from img1_shape to img0_shape\n    if ratio_pad is None:  # calculate from img0_shape\n        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n    else:\n        gain = ratio_pad[0][0]\n        pad = ratio_pad[1]\n    coords[:, [0, 2]] -= pad[0]  # x padding\n    coords[:, [1, 3]] -= pad[1]  # y padding\n    coords[:, :4] /= gain\n    clip_coords(coords, img0_shape)\n    return coords","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Yolov5 Detector helper class","metadata":{}},{"cell_type":"code","source":"class Detector:\n    def __init__(self, model, shape=832, nms_conf_thres=0.5, nms_iou_thres=0.5):\n        self.model = model\n        self.shape = shape\n        self.cuda = next(self.model.parameters()).is_cuda\n        self.nms_conf_thres = nms_conf_thres\n        self.nms_iou_thres = nms_iou_thres\n        self.ori_shape = None\n    \n    def preprocess(self, img):\n        self.ori_shape = img.shape\n        img = letterbox(img, (self.shape, self.shape), stride=32, auto=True)[0]\n        self.new_shape = img.shape\n        img = np.moveaxis(img, 2, 0)\n        \n        img = torch.from_numpy(img)\n        if self.cuda:\n            img = img.to('cuda')\n        img = img.half()  # uint8 to fp16/32\n        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n        if img.ndimension() == 3:\n            img = img.unsqueeze(0)\n        return img\n\n    def convert_pred(self, preds):\n        bboxes_list = []\n        for pred in preds:\n            bboxes = non_max_suppression(pred.unsqueeze(0), conf_thres=0.25,max_det=22,\n                                         iou_thres=0.45, classes=0, agnostic=False)[0]\n            bboxes[:, :4] = scale_coords(self.new_shape, bboxes[:, :4], self.ori_shape[:2]).round()\n            bboxes_list.append(bboxes[:,:5].cpu().numpy())\n            bboxes_list[-1][:,:4] = bboxes_list[-1][:,:4].astype(int) \n        if len(bboxes_list) == 1:\n            return bboxes_list[0]\n        return bboxes_list\n    \n    \n    def detect(self, img):\n        if not isinstance(img, list):\n            img = [img]\n        with torch.no_grad():\n            data = torch.cat([self.preprocess(im) for im in img], 0)\n            return self.convert_pred(self.model(data)[0])\n    \n    def __call__(self, img):\n        return self.detect(img)\ndetector = Detector(model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"video_dir","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\ndef detect_helmets(detector, tracking_group, game_play,\n                   video_dir, batch_size, view=None, frames_step=None):\n    video_prefix = game_play\n    views = ['Endzone', 'Sideline']\n    if view is not None:\n        if isinstance(view, str):\n            views = [view]\n        else:\n            views = views\n    for view in views:\n        tracker = ValidRegionTracker()\n        video = video_prefix + '_' + view\n        cap = cv2.VideoCapture(f'{video_dir}{video}.mp4')\n        frame = 0\n        imgs = []\n        while True:\n            if frames_step is not None:\n                cap.set(1, frame)\n                frame += frames_step\n            success, image = cap.read()\n            if success:\n                if frames_step is None:\n                    frame += 1\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n#                 mask = tracker.detect(image)\n#                 image = image * mask[:, :, np.newaxis]\n                imgs.append(image)\n            if (len(imgs) == batch_size) or not success and (len(imgs) > 0):\n                with torch.no_grad():\n                    bboxes_list = detector(imgs)\n                if batch_size == 1:\n                    bboxes_list = [bboxes_list]\n                d_df = pd.concat(\n                    [pd.DataFrame({'left':bboxes[:,0], 'right': bboxes[:,2], 'width': bboxes[:,2] - bboxes[:,0],\n                                  'top':bboxes[:,1], 'bottom': bboxes[:,3], 'height': bboxes[:,3] - bboxes[:,1], 'conf':bboxes[:,4],\n                                   'video_frame': [f'{video}_{frame - batch_size + cnt + 1}' for _\n                                                   in range(len(bboxes))], \n                                   'view':[view for _ in range(len(bboxes))], \n                                   'frame': np.zeros(len(bboxes)) + frame - batch_size + cnt + 1 })\n                     for cnt, bboxes in enumerate(bboxes_list)],axis=0)\n                d_df = d_df[d_df['conf'] > CONF_THRE]\n                d_df['x'] = (d_df['left'] + round(d_df['width'] / 2))\n                d_df['y'] = (d_df['top'] + round(d_df['height'] / 2))\n                yield d_df, imgs\n                imgs = []\n            if not success:\n                break\n\n\ndef detect_orientation(detector, tracking_group, game_play, video_dir, view=None, frames_step=30):\n    print('Detecting video view orientation...')\n    d_dfs = {}\n    \n    for frames_d_df,imgs in tqdm(\n        detect_helmets(detector, tracking_group, game_play, video_dir, 1, view=view, frames_step=frames_step)):\n        for ([frame, view], d_df),image in zip(frames_d_df.groupby(['frame', 'view']), imgs):\n            if view not in d_dfs:\n                d_dfs[view] = []\n            d_dfs[view].append(d_df)\n    ret = {}    \n    for view in d_dfs:\n        df_list = [x.reset_index(drop=True) for x in d_dfs[view]]\n        d_dfs[view] = pd.concat(df_list)\n    \n        ori_mapping = Mapping(tracking_group, view, use_kalman=False, use_previous=False,\n                              init_frames=len(df_list), ignore_starting_preproc=True)\n        df_list = list(d_dfs[view].groupby('frame'))\n        for frame in tqdm(np.linspace(1, len(df_list)-1, ori_mapping.init_frames).astype(int)):\n            _, this_df = df_list[frame]\n            ori_mapping(this_df)\n        ret[view] = [mode(ori_mapping.buffer_ori)]\n\n    return ret\n\ndef deepsort_helmets(detector,\n                     tracking_group,\n                     game_play,\n                     video_dir,\n                     deepsort_config='deepsort.yaml',\n                     plot=False,\n                     batch_size=64,\n                     plot_frames=[],\n                     save_all_to_dir=False,use_cuda=True, view=None):\n    \n    video_prefix = tracking_group['gameKey'].iloc[0].astype(str) + '_' + \\\n        tracking_group[\"playID\"].iloc[0].astype(str).zfill(6)\n        \n    \n    # Run through frames.\n    ds = []\n    objpoints = []\n    imgpoints = []\n    \n    oris = detect_orientation(detector, tracking_group, game_play, video_dir, view=view)\n    print('Detected orientation:',oris, '. Mapping...')\n    old_view = None    \n    for frames_d_df,imgs in tqdm(detect_helmets(detector, tracking_group, game_play,\n                                                video_dir, batch_size, view=view)):\n        for ([frame, view], d_df),image in zip(frames_d_df.groupby(['frame', 'view']), imgs):\n            if view != old_view:\n                old_view = view\n                deepsort = DeepSort(cfg.DEEPSORT.REID_CKPT, \n                        max_dist=cfg.DEEPSORT.MAX_DIST,\n                        min_confidence=cfg.DEEPSORT.MIN_CONFIDENCE,\n                        nms_max_overlap=cfg.DEEPSORT.NMS_MAX_OVERLAP,\n                        max_iou_distance=cfg.DEEPSORT.MAX_IOU_DISTANCE,\n                        max_age=cfg.DEEPSORT.MAX_AGE,\n                        n_init=cfg.DEEPSORT.N_INIT,\n                        nn_budget=cfg.DEEPSORT.NN_BUDGET,\n                        use_cuda=use_cuda)\n                mapping = Mapping(tracking_group, view, available_oris=oris[view])\n                if save_all_to_dir:\n                    out_dir = f'tmp/out/{video_prefix}_{view}'\n                    try:\n                        shutil.rmtree(out_dir)\n                    except IOError:\n                        pass\n                    os.makedirs(out_dir)\n               \n            video_frame = d_df['video_frame'].iloc[0]\n            mapped_df, this_tracking, opt_params = mapping(d_df)\n            mapped_df['x'] = (mapped_df['left'] + round(mapped_df['width'] / 2))\n            mapped_df['y'] = (mapped_df['top'] + round(mapped_df['height'] / 2))\n            mapped_df['left'] = mapped_df['left'].astype(int)\n            mapped_df['top'] = mapped_df['top'].astype(int)\n            \n            xywhs = mapped_df[['x','y','width','height']].values\n            confs = np.ones([len(mapped_df),])\n            clss =  np.zeros([len(mapped_df),])\n            outputs = deepsort.update(xywhs, confs, clss, image)\n#             ds_df = None\n            \n#             if frame > cfg.DEEPSORT.N_INIT:\n#                 ds_df = pd.DataFrame(outputs, columns=['left', 'top', 'right', 'bottom',\n#                                                        'deepsort_cluster', 'class'])\n#                 ds_df['width'] = ds_df['right'] - ds_df['left']\n#                 ds_df['height'] = ds_df['bottom'] - ds_df['top']\n#                 ds_df['video_frame'] = video_frame\n#                 ds_df.drop(columns='class', inplace=True)\n# #                 mapped_df, this_tracking, opt_params = mapping(ds_df)\n#             else:\n# #                 mapped_df, this_tracking, opt_params = mapping(d_df)\n#                 mapped_df['deepsort_cluster'] = np.arange(len(mapped_df))\n            preds_df = pd.DataFrame(outputs, columns=['left','top','right','bottom','deepsort_cluster','class'])\n            if len(preds_df) > 0:\n                preds_df['left'] = preds_df['left'].astype(int)\n                preds_df['top'] = preds_df['top'].astype(int)\n                # TODO Fix this messy merge\n                mapped_df = pd.merge_asof(mapped_df.sort_values(['left','top']),\n                                  preds_df[['left','top','deepsort_cluster']] \\\n                                  .sort_values(['left','top']), on='left', suffixes=('','_deepsort'),\n                                  direction='nearest')\n            ds.append(mapped_df)\n            if (plot and frame > cfg.DEEPSORT.N_INIT) or (frame in plot_frames) or save_all_to_dir:\n                ori = image.copy()\n                for j, row in mapped_df.iterrows(): \n                    row['right'] = row['left'] + row['width']\n                    row['bottom'] = row['top'] + row['height']\n                    bboxes = row[['left','top', 'right','bottom']].values\n                    id = row['label']\n                    label = f'{id}'\n                    im = plot_one_box(bboxes, image, label=label, color=[255,0,0], line_thickness=2)\n                if 'x2im' in this_tracking.columns:\n                    for j,row in this_tracking.iterrows():\n                        x = row[['x2im', 'y2im']].values.astype(float)\n                        if np.all(np.isfinite(x)):\n                            im[int(x[1]) - 10: int(x[1]) + 10, int(x[0])-10:int(x[0]) + 10, :] = 0\n                if (frame in plot_frames or plot):\n                    display(mapped_df)\n                    fig, ax = plt.subplots(figsize=(15, 10))\n                    ax.set_title(f'Deepsort labels: {frame}')\n                    plt.imshow(im)\n                    plt.show()\n                if save_all_to_dir:\n                    fname = os.path.join(out_dir , str(int(frame)).zfill(6))\n                    writer = pd.ExcelWriter(fname + '.xlsx', engine='xlsxwriter')\n                    this_tracking.sort_values('player')[['player','x','y','dir','o','s','a']\n                                                        + (['x2im', 'y2im'] if 'x2im' \n                                                        in this_tracking.columns else [])\n                                                       ].to_excel(writer,sheet_name='Tracking')\n                    d_df.sort_values('left').to_excel(writer,sheet_name='YoloV5')\n#                     if ds_df is not None:\n#                         ds_df.sort_values('left').to_excel(writer,sheet_name='DeepSort')\n                    mapped_df.sort_values('left').to_excel(writer,sheet_name='Final')\n                    pd.Series(opt_params).to_excel(writer, sheet_name='MappingOptParams')\n                    \n                    writer.save()\n                    cv2.imwrite(fname +'.png', cv2.cvtColor(im, cv2.COLOR_RGB2BGR))\n\n            \n    dout = pd.concat(ds)\n    return dout","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %matplotlib notebook\n# cap = cv2.VideoCapture(f'{video_dir}57783_003374_Endzone.mp4')\n# frame = 0\n# imgs = []\n# success, image = cap.read()\n# a = MappingCheck(image, 0, helmets, tracking)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_deepsort_label_col(out):\n    # Find the top occuring label for each deepsort_cluster\n    cum = out[~out['label'].isnull()].groupby('deepsort_cluster')['label'].value_counts() \\\n        .sort_values(ascending=False).to_frame() \\\n        .rename(columns={'label':'label_count'}) \\\n        .reset_index() \\\n        .groupby(['deepsort_cluster']) \\\n        .first()\n    \n    sortlabel_map = cum['label'].to_dict()\n    # Find the # of times that label appears for the deepsort_cluster.\n    sortlabelcount_map = cum['label_count'].to_dict()\n    \n    out['label_deepsort'] = out['deepsort_cluster'].map(sortlabel_map)\n    out['label_count_deepsort'] = out['deepsort_cluster'].map(sortlabelcount_map)\n    return out\ndef randomize_duplicates(group):\n    if len(group) > 1:\n        print(f\"Duplicates were found for frame {group['video_frame']}, randomizing..\")\n        group[['right','bottom']] +=  (\n            np.random.randint(-3, 3, group[['left','top']].shape))\n        group['bottom'] = np.maximum(np.minimum(group['bottom'], 720), 0)\n        group['right'] = np.maximum(np.minimum(group['right'], 1280), 0)\n        group.loc[group['bottom']<group['top'],'bottom'] = group.loc[group['bottom']<group['top'],'top']\n        group.loc[group['right']<group['left'],'right'] = group.loc[group['right']<group['left'],'left']\n        group['height'] = group['top'] - group['bottom']\n        group['width'] = group['right'] - group['left']\n    return group\ndef score_vs_deepsort(myvideo, out, labels):\n    # Score the base predictions compared to the deepsort postprocessed predictions.\n    myvideo_mp4 = myvideo + '.mp4'\n    labels_video = labels.query('video == @myvideo_mp4')\n    scorer = NFLAssignmentScorer(labels_video)\n    out_deduped = out.groupby(['video_frame','label']).first().reset_index()\n    base_video_score = scorer.score(out_deduped)\n    \n    out_preds = out.drop('label', axis=1).rename(columns={'label_deepsort':'label'})\n    out_preds = out_preds.groupby(['video_frame','label']).first().reset_index()\n    deepsort_video_score = scorer.score(out_preds)\n    print(f'{base_video_score:0.5f} before --> {deepsort_video_score:0.5f} deepsort')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nviews = ['Sideline','Endzone']\nif debug:\n#     from matplotlib import rcParams\n#     rcParams['figure.figsize'] = [20,20]\n    save_all_to_dir = True\n    try:\n        import xlsxwriter\n    except:\n        save_all_to_dir = False\nouts = []\nfor video in tqdm(helmets['video'].unique()):\n    game_key, play_id, view = video.split('_')\n    game_play = game_key + '_' + play_id\n    view = view[:-len('.mp4')]\n    game_key = int(game_key)\n    play_id = int(play_id)\n    tracking_group = tracking[(tracking['gameKey']==game_key)&\n                              (tracking['playID']==play_id)]\n    if debug:\n        # Plot deepsort labels when in debug mode.\n        out = deepsort_helmets(detector, tracking_group, game_play, video_dir,\n                               save_all_to_dir=save_all_to_dir, plot_frames=[0, 5,10, 150, 250],\n                              view=view)\n    else:\n        out = deepsort_helmets(detector, tracking_group, game_play, \n                               video_dir, save_all_to_dir=False, view=view)\n    out = add_deepsort_label_col(out)\n#     cols_to_drop = []\n#     for col in out.columns:\n#         if col != 'label':\n#             if out[col].isnull().any():\n#                 cols_to_drop.append(col)\n#     out = out[[col for col in out_ds.columns if col not in cols_to_drop]].copy()\n    out['right'] = out['left'] + out['width']\n    out['bottom'] = out['top'] + out['height']\n    out = out.groupby(['left','top', 'right', 'bottom', 'video_frame']).apply(\n        randomize_duplicates)\n    outs.append(out)\n    if debug:\n        t = tracking_group.iloc[0]\n        if video in labels['video'].values:\n            # Score\n            score_vs_deepsort(video[:-len('.mp4')], out, labels)\nsubmission_deepsort = pd.concat(outs).copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check Submission & Save\nFinally we will create a submission file and check that it passes the submission requirements.\nThe steps are:\n1. Drop the `label` and replace with `label_deepsort` predictions.\n2. Remove any duplicate labels within a single video/frame. This is required to meet the submission requirements.\n3. Save the results.","metadata":{}},{"cell_type":"code","source":"ss = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/sample_submission.csv')\n# Final Checks\nsubmission_deepsort.reset_index(inplace=True, drop=True)\nsubmission_deepsort['label_deepsort'] = submission_deepsort['label_deepsort'].fillna(submission_deepsort['label'])\nsubmission_deepsort = submission_deepsort[~submission_deepsort['label_deepsort'].isnull()]\nsubmission_deepsort = submission_deepsort.drop('label', axis=1) \\\n    .rename(columns={'label_deepsort':'label'})[ss.columns]\n# Drop duplicate labels\nsubmission_deepsort = submission_deepsort.loc[\n    ~submission_deepsort[['video_frame','label']].duplicated()]\ncheck_submission(submission_deepsort)\nsubmission_deepsort[['left','width','top','height']] = submission_deepsort[['left','width','top','height']].astype(int)\nsubmission_deepsort = submission_deepsort.dropna(axis=0)\nsubmission_deepsort.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Display video showing predictions\n\nLastly, if we want to review our predictions we can create a video to review the predictions using the `video_with_predictions` function from the `helmet_assignment` helper package.","metadata":{}},{"cell_type":"code","source":"if debug:\n    submission_deepsort['video'] = submission_deepsort['video_frame'].str.split('_').str[:3].str.join('_') + '.mp4'\n    debug_videos = submission_deepsort['video'].unique()\n    debug_labels = labels.query('video in @debug_videos')\n    scorer = NFLAssignmentScorer(debug_labels)\n    scorer.score(submission_deepsort)\n    for video in debug_videos:\n        # Create video showing predictions for one of the videos.\n        video_out = video_with_predictions(\n            f'../input/nfl-health-and-safety-helmet-assignment/train/{video}',\n            scorer.sub_labels.fillna(0))\n\n        frac = 0.60 # scaling factor for display\n        display(Video(data=video_out,\n                      embed=True,\n                      height=int(720*frac),\n                      width=int(1280*frac))\n               )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}