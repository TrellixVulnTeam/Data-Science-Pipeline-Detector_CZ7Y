{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Objective\n\nThe main objective is to track the player helmet assigning them proper identity, for better understanding of collision during game. For a given play a sideline and endzone view are taken. Overall the main aim is to detect and track multiple helmets. Detection involves predicting the right bounding boxes. The label assigned should be same as the one present on the jersey.    \n\n#### Datasets\nDirectory Information:\n* Train Data: train/\n* Train Labels: train_labels.csv\n* Test Videoss: test/\n* Images of Helmets: images/\n* Bounding box info of helmets: image_labels.csv\n* Baseline helmet Detection Boxes: train_baseline_helmets.csv","metadata":{}},{"cell_type":"markdown","source":"### Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patch\n\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor","metadata":{"execution":{"iopub.status.busy":"2021-08-25T08:27:56.22726Z","iopub.execute_input":"2021-08-25T08:27:56.227805Z","iopub.status.idle":"2021-08-25T08:27:57.753186Z","shell.execute_reply.started":"2021-08-25T08:27:56.227705Z","shell.execute_reply":"2021-08-25T08:27:57.752228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Exploring the csv files","metadata":{}},{"cell_type":"code","source":"home_dir = \"../input/nfl-health-and-safety-helmet-assignment/\"\nfor files in os.listdir(home_dir):\n    if ('.csv' in files):\n        print(files,'\\n')\n        dataset = pd.read_csv(os.path.join(home_dir,files))\n        print(dataset.head(),'\\n')\n        print(\"---------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2021-08-25T08:27:57.755199Z","iopub.execute_input":"2021-08-25T08:27:57.755623Z","iopub.status.idle":"2021-08-25T08:28:02.747682Z","shell.execute_reply.started":"2021-08-25T08:27:57.755577Z","shell.execute_reply":"2021-08-25T08:28:02.746683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### About Custom Datasets\nAn efficient way of loading datasets in pytorch, which helps in making the code more readable.  \nOur custom dataset inherits from the Dataset library and overrides the __len__ and __getitem__ methods  \n* **len** - returns the size of the dataset\n* **getitem** - for a given index returns the element at the index  \n\nA general approach:\n* load the csv files in the init method\n* load the images in the getitem magic function.   \nThis prevents loading all the images at once hence is memory efficient","metadata":{}},{"cell_type":"code","source":"class NFLDataset(Dataset):\n    \"\"\"NFL Helmet Assignment Dataset\"\"\"\n    def __init__(self, path_to_labels, path_to_images):\n        self.bounding_boxes = pd.read_csv(path_to_labels['detection'])\n        self.tracking = pd.read_csv(path_to_labels['tracking'])\n        self.identity = pd.read_csv(path_to_labels['identity'])\n        self.image_info = pd.read_csv(path_to_labels['images'])\n        self.unique_img_info = path_to_labels['unqimgs']\n        self.image_dir = path_to_images\n    \n    def __len__(self):\n        return len(self.unique_img_info)\n       \n    def __getitem__(self, index):\n        image = self.unique_img_info['unqimgs'][index]\n        sample = {'image':image}\n        return sample","metadata":{"execution":{"iopub.status.busy":"2021-08-25T08:28:02.749204Z","iopub.execute_input":"2021-08-25T08:28:02.749484Z","iopub.status.idle":"2021-08-25T08:28:02.756928Z","shell.execute_reply.started":"2021-08-25T08:28:02.749457Z","shell.execute_reply":"2021-08-25T08:28:02.755831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color:red\">**NOTE**</span>   In ***image_labels.csv*** since information about a single image is shared in multiple rows, using this directly for custom dataset is not recommended. Because when the data is loaded using dataloader with a particular batch size, for a single image bounding box information will be distributed among different batches which is not what we want.  \n\nInstead for a given image we want all the bounding boxes combined as its label. In the below cell this is done by creating a seperate dataframe consisting of only unique image name, which then used to load the image and all its corresponding bounding boxes together.","metadata":{}},{"cell_type":"code","source":"image_root_dir = '../input/nfl-health-and-safety-helmet-assignment/images'\ntrain_identity_labels = f'{home_dir}/train_labels.csv'\ntrain_tracking_labels = f'{home_dir}/train_player_tracking.csv'\ntrain_helmet_detection_labels = f'{home_dir}/train_baseline_helmets.csv'\ntrain_image_labels = f'{home_dir}/image_labels.csv'\n\nimgs = pd.read_csv(train_image_labels)\ndf = {'unqimgs':imgs.image.unique()}\nunique_imgs = pd.DataFrame(df)\n\npath_to_train_labels = {'identity':train_identity_labels, 'tracking': train_tracking_labels, \n                        'detection':train_helmet_detection_labels, 'images': train_image_labels,\n                       'unqimgs': unique_imgs}\n\ntrain_dataset = NFLDataset(path_to_train_labels, image_root_dir)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T08:28:02.758257Z","iopub.execute_input":"2021-08-25T08:28:02.758568Z","iopub.status.idle":"2021-08-25T08:28:05.97804Z","shell.execute_reply.started":"2021-08-25T08:28:02.758541Z","shell.execute_reply":"2021-08-25T08:28:05.977079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using DataLoader to wrap over the dataset\nUsing Dataset and Dataloader helps simplify the overall pipeline. It helps in loading and iterating over the given dataset.  \nIt allows to load minibatch of data and shuffle it along the way.  \nThis helps in mutiprocessing while at the same time prevents us from loading the complete data in the memory.","metadata":{}},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T08:28:05.979206Z","iopub.execute_input":"2021-08-25T08:28:05.979745Z","iopub.status.idle":"2021-08-25T08:28:05.984507Z","shell.execute_reply.started":"2021-08-25T08:28:05.979701Z","shell.execute_reply":"2021-08-25T08:28:05.98345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize Data\nNow each iteration throgh the dataloader returns a batch of features and labels.  \nThe next() and iter() built in function works as its name.\n* **iter()** creates a stream of data to iterate over\n* **next()** selects the next data from the given stream  \nThis combination can be used on the train_dataset (instance of custom dataset) to iterate over our given dataset.","metadata":{}},{"cell_type":"code","source":"data_info = pd.read_csv(train_image_labels)\n\ndef _get_bbox_info(img_path):\n    indx = data_info[data_info['image']==img_path].index.tolist()\n    left = data_info['left'][indx].tolist()\n    top = data_info['top'][indx].tolist()\n    width = data_info['width'][indx].tolist()\n    height = data_info['height'][indx].tolist()\n    right = [x+y for x,y in zip(left,width)]\n    bottom = [x+y for x,y in zip(top,height)]\n    start_point = [(x,y) for x,y in zip(left,top)]\n    end_point = [(x,y) for x,y in zip(right,bottom)]\n    \n    return (start_point, end_point)\n    \ndef _draw_bbox(img, sp, ep):\n    return cv2.rectangle(img, sp, ep, (255,0,0), 2)    ","metadata":{"execution":{"iopub.status.busy":"2021-08-25T08:28:05.985817Z","iopub.execute_input":"2021-08-25T08:28:05.98613Z","iopub.status.idle":"2021-08-25T08:28:06.151436Z","shell.execute_reply.started":"2021-08-25T08:28:05.986103Z","shell.execute_reply":"2021-08-25T08:28:06.150531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20,12))\nfor batch in train_dataloader:\n    for (index, img_path) in enumerate(batch['image']):\n        img = cv2.imread(f'{image_root_dir}/{img_path}')\n        sp, ep = _get_bbox_info(img_path)\n        for (x,y) in zip(sp,ep):\n            img = _draw_bbox(img, x, y)         \n        ax[index//2][index%2].imshow(img)\n    break    ","metadata":{"execution":{"iopub.status.busy":"2021-08-25T08:28:06.152936Z","iopub.execute_input":"2021-08-25T08:28:06.153381Z","iopub.status.idle":"2021-08-25T08:28:08.06082Z","shell.execute_reply.started":"2021-08-25T08:28:06.153339Z","shell.execute_reply":"2021-08-25T08:28:08.05957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Further details related to training and inference will be added soon","metadata":{}}]}