{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hello Kagglers! So many interesting competitons but not so much of time. With the amazing results from last year, The National Football League (NFL) has asked Kagglers to help them on one more task. This time the NFL wants to assign specific players to each helmet, which would help accurately identify each player's “exposures” throughout a football play.\n\nTo aid with helmet detection, the NFL has also provided an ancillary dataset of images showing helmets with labeled bounding boxes. These files are located in `images` directory and the `bounding boxes` information is in `image_labels.csv`\n\nOur goal is to analyze this dataset, and se how can we make the best use of it for additional training. Let's jump in","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set()\n%config IPCompleter.use_jedi = False","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-18T17:33:29.344938Z","iopub.execute_input":"2021-08-18T17:33:29.345432Z","iopub.status.idle":"2021-08-18T17:33:30.549635Z","shell.execute_reply.started":"2021-08-18T17:33:29.345332Z","shell.execute_reply":"2021-08-18T17:33:30.548657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Path to the parent directory containing all the data \ndata_path = Path(\"../input/nfl-health-and-safety-helmet-assignment/\")","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:33:30.550826Z","iopub.execute_input":"2021-08-18T17:33:30.551284Z","iopub.status.idle":"2021-08-18T17:33:30.5551Z","shell.execute_reply.started":"2021-08-18T17:33:30.551238Z","shell.execute_reply":"2021-08-18T17:33:30.55378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Supplementary Image Data\n\n1. Additional images provided for making a good helmet detector. Comparable to train/test distribution\n2. Load these images first, check how the images look like and how the labels are provided in images_labels.csv","metadata":{}},{"cell_type":"code","source":"additional_images = sorted((data_path / \"images\").glob(\"*.jpg\"))\nadditional_images_df = pd.read_csv(data_path / \"image_labels.csv\")\n\nprint(\"Number of images: \", len(additional_images))\nprint(\"Number of annotations: \", len(additional_images_df))\nprint(additional_images_df.head())\nprint(\"=\"*50)\nprint(\"\\nAverage number of annotations per frame: \", len(additional_images_df) // len(additional_images))","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:33:32.166836Z","iopub.execute_input":"2021-08-18T17:33:32.167405Z","iopub.status.idle":"2021-08-18T17:33:32.846725Z","shell.execute_reply.started":"2021-08-18T17:33:32.167362Z","shell.execute_reply":"2021-08-18T17:33:32.845563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will make two changes in this dataframe:\n1. We will modify the image path to the actual image path\n2. We will add `right` and `bottom` coordinates of the box","metadata":{}},{"cell_type":"code","source":"additional_images_df[\"image\"] = str(data_path / \"images\") + \"/\" + additional_images_df[\"image\"]\nadditional_images_df[\"right\"] = additional_images_df[\"left\"] + additional_images_df[\"width\"]\nadditional_images_df[\"bottom\"] = additional_images_df[\"top\"] + additional_images_df[\"height\"]\nadditional_images_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:33:33.072885Z","iopub.execute_input":"2021-08-18T17:33:33.07324Z","iopub.status.idle":"2021-08-18T17:33:33.165658Z","shell.execute_reply.started":"2021-08-18T17:33:33.073212Z","shell.execute_reply":"2021-08-18T17:33:33.164639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing the data","metadata":{}},{"cell_type":"code","source":"def plot_bboxes_on_frame(images, bbox, color=(0, 255, 0)):\n    \"\"\"Plots all the boundng boxes on a given frame.\n    \n    Args:\n        images: List of images\n        bbox: Corresposnding list of bboxes. All the bboxes corresponding\n            to an image should be provided in a single list of list of\n            dictionaries. For example:\n            [\n                [{\"left\": 100, \"top\":50, \"right\":30, \"bottom\":500}, {...}],\n                [{..}..],\n                ...\n            ]\n    \"\"\"\n    \n    if len(images) != len(bbox):\n        raise ValueError(\"Number of images and corresponding bboxes should be same\")\n        \n    for i in range(len(bbox)):\n        boxes = bbox[i]\n        for box in boxes:\n            cv2.rectangle(images[i], (box[\"left\"], box[\"top\"]), (box[\"right\"], box[\"bottom\"]), color, 2)    \n    return images\n\n\ndef get_bbox(df, idx, cols=[\"left\", \"right\", \"top\", \"bottom\"]):\n    \"\"\"Given an index, return a dictionary of box cooredinates.\n    \n    Args:\n        df: Dataframe containing the information\n        idx: The index to pick the data from\n        cols: The columns to extract the data\n    Returns:\n        A dictionary containing cooredinates of the bbox\n    \"\"\"\n    \n    box = {}\n    for col in cols:\n        box[col] = df[col][idx]\n    return box\n\n\ndef get_all_boxes(df, indices, cols=[\"left\", \"right\", \"top\", \"bottom\"]):\n    \"\"\"Gathers all bboxes corresponding to an image.\n    \n    Args:\n        df: Dataframe containing the information\n        indices: The indices to pick the data from\n        cols: The columns to extract the data\n    Returns:\n        A list of bboxes\n    \"\"\"\n    bbox = [get_bbox(df, idx) for idx in indices]\n    return bbox\n\ndef get_all_indices_of_image(df, image_path):\n    \"\"\"Find all the indices corresponding to an image.\n    \n    Args:\n        df: Dataframe containing the information\n        image_path: The image_path for which indices are required\n    Returns:\n        A list of indices\n    \"\"\"\n    \n    indices = df.index[df[\"image\"]==image_path]\n    return indices.tolist()\n\n\n\ndef read_images_and_bbox(df, images_path):\n    \"\"\"Read images and bbox information\"\"\"\n    images = []\n    bbox = []\n    \n    for i, img_path in enumerate(images_path):\n        indices = get_all_indices_of_image(df, img_path)\n        boxes = get_all_boxes(df, indices)\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        images.append(img)\n        bbox.append(boxes)\n        \n    return images, bbox","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:33:33.932344Z","iopub.execute_input":"2021-08-18T17:33:33.932697Z","iopub.status.idle":"2021-08-18T17:33:33.947049Z","shell.execute_reply.started":"2021-08-18T17:33:33.932668Z","shell.execute_reply":"2021-08-18T17:33:33.945674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the list of all unique images\nall_images = list(set(additional_images_df[\"image\"]))\n\n# Select how many rows and columns you want in the subplots\nnum_rows = 5\nnum_cols = 2\n\n# Select the desired number of images randomly\nselected_images = np.random.choice(all_images, size=num_rows*num_cols)\n\n# Read the images and get box coordinates\nimages, boxes = read_images_and_bbox(df=additional_images_df, images_path=selected_images)\n\n# Draw the boxes on the images\nimages_with_boxes = plot_bboxes_on_frame(images, boxes)\n\n\n# Visualize the images\n_, ax = plt.subplots(num_rows, num_cols, figsize=(20, 22))\n\nfor i in range(num_rows*num_cols):\n    title = selected_images[i].split(\"/\")[-1]\n    ax[i // num_cols][i % num_cols].imshow(images_with_boxes[i])\n    ax[i // num_cols][i % num_cols].axis(\"off\")\n    ax[i // num_cols][i % num_cols].set_title(title)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:33:34.241651Z","iopub.execute_input":"2021-08-18T17:33:34.241993Z","iopub.status.idle":"2021-08-18T17:33:38.356388Z","shell.execute_reply.started":"2021-08-18T17:33:34.241964Z","shell.execute_reply":"2021-08-18T17:33:38.355563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Views in frames\n\nGiven that the images are from `endzone` and `sideline`, let's try to group and align these images and see if this data can be used in a much smarter way","metadata":{}},{"cell_type":"code","source":"def get_image_number(img_path):\n    img_num = img_path.split(\"/\")[-1]\n    img_num = img_num.split(\"_\")[0]\n    return img_num\n\ndef get_frame_view(img_path):\n    img_view = img_path.split(\"/\")[-1]\n    if \"Endzone\" in img_view:\n        return \"Endzone\"\n    elif \"Sideline\" in img_view:\n        return \"Sideline\"\n    else:\n        return \"Unknown\"","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:33:38.357576Z","iopub.execute_input":"2021-08-18T17:33:38.357968Z","iopub.status.idle":"2021-08-18T17:33:38.365017Z","shell.execute_reply.started":"2021-08-18T17:33:38.35794Z","shell.execute_reply":"2021-08-18T17:33:38.364229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"additional_images_df[\"image_no\"] = additional_images_df[\"image\"].apply(get_image_number)\nadditional_images_df[\"frame_view\"] = additional_images_df[\"image\"].apply(get_frame_view)\nadditional_images_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:33:38.366643Z","iopub.execute_input":"2021-08-18T17:33:38.367116Z","iopub.status.idle":"2021-08-18T17:33:38.720662Z","shell.execute_reply.started":"2021-08-18T17:33:38.367088Z","shell.execute_reply":"2021-08-18T17:33:38.719757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Distribution of `views` ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nsns.countplot(data=additional_images_df, x=\"frame_view\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:33:38.72195Z","iopub.execute_input":"2021-08-18T17:33:38.72223Z","iopub.status.idle":"2021-08-18T17:33:39.073135Z","shell.execute_reply.started":"2021-08-18T17:33:38.722204Z","shell.execute_reply":"2021-08-18T17:33:39.072457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# So, we have more sideline views compared to endzone views, but by how much\nadditional_images_df[\"frame_view\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:33:39.074124Z","iopub.execute_input":"2021-08-18T17:33:39.074544Z","iopub.status.idle":"2021-08-18T17:33:39.124687Z","shell.execute_reply.started":"2021-08-18T17:33:39.074505Z","shell.execute_reply":"2021-08-18T17:33:39.123599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How many labels are there?\n\nIt looks like we have more than one class in this data. Let's take a look","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nsns.countplot(data=additional_images_df, x=\"label\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:33:39.126046Z","iopub.execute_input":"2021-08-18T17:33:39.12633Z","iopub.status.idle":"2021-08-18T17:33:39.517959Z","shell.execute_reply.started":"2021-08-18T17:33:39.126304Z","shell.execute_reply":"2021-08-18T17:33:39.517066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Different kind of helmets\n\nAs we saw above that the helmets have been annotated in five different ways. We will plot a few samples for each of these type of helmets to get an idea why are they annotated in this fashion","metadata":{}},{"cell_type":"code","source":"def filter_df(df, filter_col, value):\n    df = df[df[filter_col]==value]\n    df = df.reset_index(drop=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:34:57.270899Z","iopub.execute_input":"2021-08-18T17:34:57.271236Z","iopub.status.idle":"2021-08-18T17:34:57.275658Z","shell.execute_reply.started":"2021-08-18T17:34:57.271206Z","shell.execute_reply":"2021-08-18T17:34:57.274856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_samples(df, num_samples_to_plot, figsize=(20, 20)):\n    # Get the list of all unique images\n    all_images = list(set(df[\"image\"]))\n\n    # Select how many rows and columns you want in the subplots\n    num_rows = num_samples_to_plot // 2\n    num_cols = 2\n\n    # Select the desired number of images randomly\n    selected_images = np.random.choice(all_images, size=num_rows*num_cols)\n\n    # Read the images and get box coordinates\n    images, boxes = read_images_and_bbox(df=df, images_path=selected_images)\n\n    # Draw the boxes on the images\n    images_with_boxes = plot_bboxes_on_frame(images, boxes)\n\n\n    # Visualize the images\n    _, ax = plt.subplots(num_rows, num_cols, figsize=figsize)\n\n    for i in range(num_rows*num_cols):\n        title = selected_images[i].split(\"/\")[-1]\n        ax[i // num_cols][i % num_cols].imshow(images_with_boxes[i])\n        ax[i // num_cols][i % num_cols].axis(\"off\")\n        ax[i // num_cols][i % num_cols].set_title(title)\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:12:58.530171Z","iopub.execute_input":"2021-08-18T18:12:58.530605Z","iopub.status.idle":"2021-08-18T18:12:58.541013Z","shell.execute_reply.started":"2021-08-18T18:12:58.530567Z","shell.execute_reply":"2021-08-18T18:12:58.539894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can use groupby here but I will do it manually because\n# I would be using these subsets for further analysis\n\nhelmet_df = filter_df(additional_images_df, \"label\", \"Helmet\")\nhelmet_blurred_df = filter_df(additional_images_df, \"label\",\"Helmet-Blurred\")\nhelmet_difficult_df = filter_df(additional_images_df, \"label\", \"Helmet-Difficult\")\nhelmet_sideline_df = filter_df(additional_images_df, \"label\", \"Helmet-Sideline\")\nhelmet_partial_df =  filter_df(additional_images_df, \"label\", \"Helmet-Partial\")","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:12:59.644479Z","iopub.execute_input":"2021-08-18T18:12:59.644858Z","iopub.status.idle":"2021-08-18T18:12:59.850932Z","shell.execute_reply.started":"2021-08-18T18:12:59.644829Z","shell.execute_reply":"2021-08-18T18:12:59.849677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Blurred Helmets","metadata":{}},{"cell_type":"code","source":"plot_samples(helmet_blurred_df, num_samples_to_plot=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:13:02.933909Z","iopub.execute_input":"2021-08-18T18:13:02.934246Z","iopub.status.idle":"2021-08-18T18:13:06.277195Z","shell.execute_reply.started":"2021-08-18T18:13:02.934218Z","shell.execute_reply":"2021-08-18T18:13:06.275654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Difficult Helmets","metadata":{}},{"cell_type":"code","source":"plot_samples(helmet_difficult_df, num_samples_to_plot=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:13:18.365103Z","iopub.execute_input":"2021-08-18T18:13:18.36577Z","iopub.status.idle":"2021-08-18T18:13:21.79766Z","shell.execute_reply.started":"2021-08-18T18:13:18.365716Z","shell.execute_reply":"2021-08-18T18:13:21.796612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `difficult` helmet are actually hard to spot. This will require a very good object detector that can detect object at such small scale","metadata":{}},{"cell_type":"markdown","source":"## Sideline Helmets","metadata":{}},{"cell_type":"code","source":"plot_samples(helmet_partial_df, num_samples_to_plot=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:13:21.799204Z","iopub.execute_input":"2021-08-18T18:13:21.79956Z","iopub.status.idle":"2021-08-18T18:13:25.043594Z","shell.execute_reply.started":"2021-08-18T18:13:21.799529Z","shell.execute_reply":"2021-08-18T18:13:25.042246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Partial Helmets","metadata":{}},{"cell_type":"code","source":"plot_samples(helmet_partial_df, num_samples_to_plot=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:47:57.500384Z","iopub.execute_input":"2021-08-18T17:47:57.500729Z","iopub.status.idle":"2021-08-18T17:48:01.002748Z","shell.execute_reply.started":"2021-08-18T17:47:57.500702Z","shell.execute_reply":"2021-08-18T17:48:01.00183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I would consider `partial-helmet` and the `difficult-helmet` as the same category because these are really hard to spot and differentiate\n\n*To be continued..*","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}