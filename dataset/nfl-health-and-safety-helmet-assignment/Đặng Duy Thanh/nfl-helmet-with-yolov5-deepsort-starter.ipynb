{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NFL helmet with Yolov5-deepsort starter\n\nIn this competition NFL ask Kaggler to develop a solution for tracking identity of player helmet in order to have better understanding of player collision during the game. For machine learning, it's a problem named as \"Multi-Object Tracking\" or MOT in short which ultilize object detection and tracking detection box across frames. Paper with Codes leaderboard for SOTA method: https://paperswithcode.com/task/multi-object-tracking\n\nThe below code use Deepsort: https://arxiv.org/abs/1703.07402 which is a old but simple algothrim to try extracting detection box and link player identity.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2021-10-18T17:18:13.577813Z","iopub.execute_input":"2021-10-18T17:18:13.578708Z","iopub.status.idle":"2021-10-18T17:18:13.602567Z","shell.execute_reply.started":"2021-10-18T17:18:13.578591Z","shell.execute_reply":"2021-10-18T17:18:13.601603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone --recurse-submodules https://github.com/etrain-xyz/Yolov5_DeepSort_Pytorch.git","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-18T17:18:19.657596Z","iopub.execute_input":"2021-10-18T17:18:19.657846Z","iopub.status.idle":"2021-10-18T17:18:24.335235Z","shell.execute_reply.started":"2021-10-18T17:18:19.657818Z","shell.execute_reply":"2021-10-18T17:18:24.334274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls Yolov5_DeepSort_Pytorch/yolov5","metadata":{"execution":{"iopub.status.busy":"2021-10-18T16:27:49.735629Z","iopub.execute_input":"2021-10-18T16:27:49.736093Z","iopub.status.idle":"2021-10-18T16:27:50.521805Z","shell.execute_reply.started":"2021-10-18T16:27:49.736048Z","shell.execute_reply":"2021-10-18T16:27:50.520676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cd Yolov5_DeepSort_Pytorch && pip install -r requirements.txt","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-18T16:27:59.679889Z","iopub.execute_input":"2021-10-18T16:27:59.680209Z","iopub.status.idle":"2021-10-18T16:28:10.074209Z","shell.execute_reply.started":"2021-10-18T16:27:59.680175Z","shell.execute_reply":"2021-10-18T16:28:10.072844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yolov5 helmet detection weight from https://www.kaggle.com/duythanhng/yolov5-v6-0-helmet-detection","metadata":{}},{"cell_type":"code","source":"!cp \"../input/yolov5-v6-0-helmet-detection/yolov5_weight/exp/weights/best.pt\" Yolov5_DeepSort_Pytorch","metadata":{"execution":{"iopub.status.busy":"2021-10-18T16:28:14.947365Z","iopub.execute_input":"2021-10-18T16:28:14.947796Z","iopub.status.idle":"2021-10-18T16:28:15.85633Z","shell.execute_reply.started":"2021-10-18T16:28:14.947752Z","shell.execute_reply":"2021-10-18T16:28:15.854967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!add-apt-repository --yes ppa:ubuntu-toolchain-r/test\n!apt-get update --yes\n!apt-get upgrade --yes\n!apt install --yes gcc-9 libstdc++6","metadata":{"execution":{"iopub.status.busy":"2021-10-18T16:09:46.958022Z","iopub.execute_input":"2021-10-18T16:09:46.958434Z","iopub.status.idle":"2021-10-18T16:14:11.386217Z","shell.execute_reply.started":"2021-10-18T16:09:46.958387Z","shell.execute_reply":"2021-10-18T16:14:11.385167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd \"/kaggle/working/Yolov5_DeepSort_Pytorch\"","metadata":{"execution":{"iopub.status.busy":"2021-10-18T16:28:24.896265Z","iopub.execute_input":"2021-10-18T16:28:24.897194Z","iopub.status.idle":"2021-10-18T16:28:24.904075Z","shell.execute_reply.started":"2021-10-18T16:28:24.89716Z","shell.execute_reply":"2021-10-18T16:28:24.903064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\n# sys.path.insert(0, './yolov5')\nsys.path.append('/kaggle/working/Yolov5_DeepSort_Pytorch/yolov5')\n\nfrom yolov5.models.experimental import attempt_load\nfrom yolov5.utils.downloads import attempt_download\nfrom yolov5.utils.datasets import LoadImages, LoadStreams\nfrom yolov5.utils.general import check_img_size, non_max_suppression, scale_coords, check_imshow, xyxy2xywh\nfrom yolov5.utils.torch_utils import select_device, time_sync\nfrom yolov5.utils.plots import Annotator, colors\nfrom deep_sort_pytorch.utils.parser import get_config\nfrom deep_sort_pytorch.deep_sort import DeepSort\nimport argparse\nimport os\nimport platform\nimport shutil\nimport time\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\ndef detect(source):\n    out = \"inference/output\"\n    imgsz = 1280\n    yolo_weights = \"best.pt\"\n    deep_sort_weights = \"deep_sort_pytorch/deep_sort/deep/checkpoint/ckpt.t7\"\n    config_deepsort = \"deep_sort_pytorch/configs/deep_sort.yaml\"\n    augment = False\n    conf_thres = 0.4\n    iou_thres = 0.5\n    classes = 0\n    agnostic_nms = False\n    \n    # initialize deepsort\n    cfg = get_config()\n    cfg.merge_from_file(config_deepsort)\n    \n    attempt_download(deep_sort_weights, repo='mikel-brostrom/Yolov5_DeepSort_Pytorch')\n    deepsort = DeepSort(cfg.DEEPSORT.REID_CKPT,\n                        max_dist=cfg.DEEPSORT.MAX_DIST, min_confidence=cfg.DEEPSORT.MIN_CONFIDENCE,\n                        max_iou_distance=cfg.DEEPSORT.MAX_IOU_DISTANCE,\n                        max_age=cfg.DEEPSORT.MAX_AGE, n_init=cfg.DEEPSORT.N_INIT, nn_budget=cfg.DEEPSORT.NN_BUDGET,\n                        use_cuda=True)\n\n    # Initialize\n    device = select_device('0')\n\n    # The MOT16 evaluation runs multiple inference streams in parallel, each one writing to\n    # its own .txt file. Hence, in that case, the output folder is not restored\n    if os.path.exists(out):\n        pass\n        shutil.rmtree(out)  # delete output folder\n    os.makedirs(out)  # make new output folder\n\n    half = device.type != 'cpu'  # half precision only supported on CUDA\n    # Load model\n    model = attempt_load(yolo_weights, map_location=device)  # load FP32 model\n    stride = int(model.stride.max())  # model stride\n    imgsz = check_img_size(imgsz, s=stride)  # check img_size\n    names = model.module.names if hasattr(model, 'module') else model.names  # get class names\n    if half:\n        model.half()  # to FP16\n\n    # Set Dataloader\n    vid_path, vid_writer = None, None\n    \n    dataset = LoadImages(source, img_size=imgsz, stride=stride)\n\n    # Get names and colors\n    names = model.module.names if hasattr(model, 'module') else model.names\n\n    # Run inference\n    if device.type != 'cpu':\n        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n    t0 = time.time()\n\n    save_path = str(Path(out))\n    # extract what is in between the last '/' and last '.'\n    txt_file_name = source.split('/')[-1].split('.')[0]\n    txt_path = str(Path(out)) + '/' + txt_file_name + '.txt'\n\n    for frame_idx, (path, img, im0s, vid_cap) in enumerate(dataset):\n        img = torch.from_numpy(img).to(device)\n        img = img.half() if half else img.float()  # uint8 to fp16/32\n        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n        if img.ndimension() == 3:\n            img = img.unsqueeze(0)\n\n        # Inference\n        t1 = time_sync()\n        pred = model(img, augment=augment)[0]\n\n        # Apply NMS\n        pred = non_max_suppression(pred, conf_thres, iou_thres, classes=classes, agnostic=agnostic_nms)\n        t2 = time_sync()\n\n        # Process detections\n        for i, det in enumerate(pred):  # detections per image\n            p, s, im0 = path, '', im0s\n\n            s += '%gx%g ' % img.shape[2:]  # print string\n            save_path = str(Path(out) / Path(p).name)\n\n            annotator = Annotator(im0, line_width=2, pil=not ascii)\n\n            if det is not None and len(det):\n                # Rescale boxes from img_size to im0 size\n                det[:, :4] = scale_coords(\n                    img.shape[2:], det[:, :4], im0.shape).round()\n\n                # Print results\n                for c in det[:, -1].unique():\n                    n = (det[:, -1] == c).sum()  # detections per class\n                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n\n                xywhs = xyxy2xywh(det[:, 0:4])\n                confs = det[:, 4]\n                clss = det[:, 5]\n\n                # pass detections to deepsort\n                outputs = deepsort.update(xywhs.cpu(), confs.cpu(), clss.cpu(), im0)\n                \n                # draw boxes for visualization\n                if len(outputs) > 0:\n                    for j, (output, conf) in enumerate(zip(outputs, confs)): \n                        \n                        bboxes = output[0:4]\n                        id = output[4]\n                        cls = output[5]\n\n                        c = int(cls)  # integer class\n                        label = f'{id} {names[c]} {conf:.2f}'\n                        annotator.box_label(bboxes, label, color=colors(id, True))\n            else:\n                deepsort.increment_ages()\n\n            # Print time (inference + NMS)\n            print('%sDone. (%.3fs)' % (s, t2 - t1))\n\n            # Stream results\n            im0 = annotator.result()\n\n            # Save results (image with detections)\n            if vid_path != save_path:  # new video\n                vid_path = save_path\n                if isinstance(vid_writer, cv2.VideoWriter):\n                    vid_writer.release()  # release previous video writer\n                if vid_cap:  # video\n                    fps = vid_cap.get(cv2.CAP_PROP_FPS)\n                    w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n                    h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n                else:  # stream\n                    fps, w, h = 30, im0.shape[1], im0.shape[0]\n                    save_path += '.mp4'\n\n                vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n            vid_writer.write(im0)\n\n    print('Results saved to %s' % os.getcwd() + os.sep + out)\n    print('Done. (%.3fs)' % (time.time() - t0))\n","metadata":{"execution":{"iopub.status.busy":"2021-10-18T16:36:08.406794Z","iopub.execute_input":"2021-10-18T16:36:08.407149Z","iopub.status.idle":"2021-10-18T16:36:08.443094Z","shell.execute_reply.started":"2021-10-18T16:36:08.407118Z","shell.execute_reply":"2021-10-18T16:36:08.442057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detect(\"../../input/nfl-health-and-safety-helmet-assignment/test/57906_000718_Endzone.mp4\")","metadata":{"execution":{"iopub.status.busy":"2021-10-18T16:50:00.871884Z","iopub.execute_input":"2021-10-18T16:50:00.8728Z","iopub.status.idle":"2021-10-18T16:50:50.574543Z","shell.execute_reply.started":"2021-10-18T16:50:00.872763Z","shell.execute_reply":"2021-10-18T16:50:50.573178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let see the output:","metadata":{}},{"cell_type":"code","source":"from IPython.display import YouTubeVideo\nYouTubeVideo('TofMADTFkjI', width=800, height=450)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T17:00:13.676154Z","iopub.execute_input":"2021-10-18T17:00:13.676511Z","iopub.status.idle":"2021-10-18T17:00:13.794324Z","shell.execute_reply.started":"2021-10-18T17:00:13.676438Z","shell.execute_reply":"2021-10-18T17:00:13.793059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What's makes multi-objec tracking a difficult problem? From the above video it can already see clearly that some helmets are not tracked consistently throughout the video. Besides from false positive/false negative problem from traditional object detection there are also other issues. \n\n![](https://i.imgur.com/7IBr47d.png)\n![](https://i.imgur.com/DyUtK4y.png)\n\nFor example in the above two image the identity of two players switch after they are close together. \n\n![](https://i.imgur.com/7QzHM6q.png)\n![](https://i.imgur.com/1RHItha.png)\n\nAlso, sometimes the model use duplicated id to indicates the same object.\n\nTherefore usually in multiple-object tracking paper there are multiple criteria to judge the model's performance\n\n![TransMOT: Spatial-Temporal Graph Transformer for Multiple Object Tracking](https://i.imgur.com/tPMqbpv.png)\n![Real-Time Multiple Object Tracking A Study on the Importance of Speed](https://i.imgur.com/jG5wRTE.png)\n\nHope this notebook would be useful!","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/Yolov5_DeepSort_Pytorch\n!rm -rf .git inference \n%cd /kaggle/working\n!zip -r yolov5-deepsort-pytorch.zip Yolov5_DeepSort_Pytorch\n!mkdir -p yolov5_deepsort_pytorch_source\n!mv yolov5-deepsort-pytorch.zip ./yolov5_deepsort_pytorch_source/\n!rm -rf Yolov5_DeepSort_Pytorch","metadata":{"execution":{"iopub.status.busy":"2021-10-18T17:18:36.523261Z","iopub.execute_input":"2021-10-18T17:18:36.523596Z","iopub.status.idle":"2021-10-18T17:18:40.565217Z","shell.execute_reply.started":"2021-10-18T17:18:36.523564Z","shell.execute_reply":"2021-10-18T17:18:40.56419Z"},"trusted":true},"execution_count":null,"outputs":[]}]}