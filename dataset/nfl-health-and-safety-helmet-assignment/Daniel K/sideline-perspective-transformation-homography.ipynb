{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook I will share one idea to merging traking data with sideline helmet label information.\n\nMain approch is that if we can find specific 4 pair points(cx, cy) which is matching with `tracking images` & `side or endline images`, we can find homography `H` for Perspective Transformation.\n\nin this notebook I'll use field line numbers to find homography `H` between `tracking images` and `sideline images`\n\nReference: \n- https://www.kaggle.com/robikscube/nfl-helmet-assignment-getting-started-guide\n- https://www.kaggle.com/c/nfl-health-and-safety-helmet-assignment/discussion/264361#1467283\n- https://www.kaggle.com/coldfir3/camera-tracking-matching-with-gradient-descent\n- https://www.kaggle.com/go5kuramubon/merge-label-and-tracking-data","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install imageio-ffmpeg\n# !pip install clone+https://github.com/luna983/stitch-aerial-photos.git","metadata":{"execution":{"iopub.status.busy":"2021-10-04T22:16:55.325932Z","iopub.execute_input":"2021-10-04T22:16:55.326403Z","iopub.status.idle":"2021-10-04T22:17:05.28169Z","shell.execute_reply.started":"2021-10-04T22:16:55.326357Z","shell.execute_reply":"2021-10-04T22:17:05.28033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport imageio\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nfrom tqdm.auto import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-10-04T22:20:43.740709Z","iopub.execute_input":"2021-10-04T22:20:43.741068Z","iopub.status.idle":"2021-10-04T22:20:44.113488Z","shell.execute_reply.started":"2021-10-04T22:20:43.74104Z","shell.execute_reply":"2021-10-04T22:20:44.112257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"## https://www.kaggle.com/go5kuramubon/merge-label-and-tracking-data\n\n# Read in data files\nBASE_DIR = '../input/nfl-health-and-safety-helmet-assignment'\n\n# Labels and sample submission\nlabels = pd.read_csv(f'{BASE_DIR}/train_labels.csv')\nss = pd.read_csv(f'{BASE_DIR}/sample_submission.csv')\n\n# Player tracking data\ntr_tracking = pd.read_csv(f'{BASE_DIR}/train_player_tracking.csv')\nte_tracking = pd.read_csv(f'{BASE_DIR}/test_player_tracking.csv')\n\n# Baseline helmet detection labels\ntr_helmets = pd.read_csv(f'{BASE_DIR}/train_baseline_helmets.csv')\nte_helmets = pd.read_csv(f'{BASE_DIR}/test_baseline_helmets.csv')\n\n# Extra image labels\nimg_labels = pd.read_csv(f'{BASE_DIR}/image_labels.csv')","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-04T22:20:44.93118Z","iopub.execute_input":"2021-10-04T22:20:44.93176Z","iopub.status.idle":"2021-10-04T22:20:50.014045Z","shell.execute_reply.started":"2021-10-04T22:20:44.931725Z","shell.execute_reply":"2021-10-04T22:20:50.012758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##https://www.kaggle.com/robikscube/nfl-helmet-assignment-getting-started-guide\n\ndef add_track_features(tracks, fps=59.94, snap_frame=10):\n    \"\"\"\n    Add column features helpful for syncing with video data.\n    \"\"\"\n    tracks = tracks.copy()\n    tracks[\"game_play\"] = (\n        tracks[\"gameKey\"].astype(\"str\")\n        + \"_\"\n        + tracks[\"playID\"].astype(\"str\").str.zfill(6)\n    )\n    tracks[\"time\"] = pd.to_datetime(tracks[\"time\"])\n    snap_dict = (\n        tracks.query('event == \"ball_snap\"')\n        .groupby(\"game_play\")[\"time\"]\n        .first()\n        .to_dict()\n    )\n    tracks[\"snap\"] = tracks[\"game_play\"].map(snap_dict)\n    tracks[\"isSnap\"] = tracks[\"snap\"] == tracks[\"time\"]\n    tracks[\"team\"] = tracks[\"player\"].str[0].replace(\"H\", \"Home\").replace(\"V\", \"Away\")\n    tracks[\"snap_offset\"] = (tracks[\"time\"] - tracks[\"snap\"]).astype(\n        \"timedelta64[ms]\"\n    ) / 1_000\n    # Estimated video frame\n    tracks[\"est_frame\"] = (\n        ((tracks[\"snap_offset\"] * fps) + snap_frame).round().astype(\"int\")\n    )\n    return tracks\n\n\ntr_tracking = add_track_features(tr_tracking)\nte_tracking = add_track_features(te_tracking)\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-04T22:20:50.015864Z","iopub.execute_input":"2021-10-04T22:20:50.016333Z","iopub.status.idle":"2021-10-04T22:20:51.160254Z","shell.execute_reply.started":"2021-10-04T22:20:50.016298Z","shell.execute_reply":"2021-10-04T22:20:51.15956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def merge_label_and_tracking(tracking_df, label_df):\n\n    tracking_with_game_index = tracking_df.set_index([\"gameKey\", \"playID\", \"player\"])\n\n    df_list = []\n\n    for key, _label_df in tqdm(label_df.groupby([\"gameKey\", \"playID\", \"view\", \"label\"])):\n        # skip because there are sideline player\n        if key[3] == \"H00\" or key[3] == \"V00\":\n            continue\n\n        tracking_data = tracking_with_game_index.loc[(key[0], key[1], key[3])]\n        _label_df = _label_df.sort_values(\"frame\")\n\n        # merge with frame and est_frame\n        merged_df = pd.merge_asof(\n            _label_df,\n            tracking_data,\n            left_on=\"frame\",\n            right_on=\"est_frame\",\n            direction='nearest',\n        )\n        df_list.append(merged_df)\n\n    all_merged_df = pd.concat(df_list)\n    all_merged_df = all_merged_df.sort_values([\"video_frame\", \"label\"], ignore_index=True)\n    \n    return all_merged_df","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-04T22:20:51.16144Z","iopub.execute_input":"2021-10-04T22:20:51.161742Z","iopub.status.idle":"2021-10-04T22:20:51.169488Z","shell.execute_reply.started":"2021-10-04T22:20:51.161718Z","shell.execute_reply":"2021-10-04T22:20:51.167903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df = merge_label_and_tracking(tr_tracking, labels)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T22:20:51.170664Z","iopub.execute_input":"2021-10-04T22:20:51.171038Z","iopub.status.idle":"2021-10-04T22:21:16.622149Z","shell.execute_reply.started":"2021-10-04T22:20:51.171004Z","shell.execute_reply":"2021-10-04T22:21:16.620044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df.frame","metadata":{"execution":{"iopub.status.busy":"2021-10-04T22:21:16.626947Z","iopub.execute_input":"2021-10-04T22:21:16.627278Z","iopub.status.idle":"2021-10-04T22:21:16.637543Z","shell.execute_reply.started":"2021-10-04T22:21:16.62725Z","shell.execute_reply":"2021-10-04T22:21:16.636457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_gameKeys = merged_df.gameKey.unique()\ncheck_frame = 1\nhomography_df = merged_df[(merged_df.gameKey == unique_gameKeys[0]) & (merged_df.frame == check_frame) & (merged_df.view =='Sideline')].copy()\nhomography_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-04T22:21:16.639345Z","iopub.execute_input":"2021-10-04T22:21:16.639585Z","iopub.status.idle":"2021-10-04T22:21:16.747548Z","shell.execute_reply.started":"2021-10-04T22:21:16.63956Z","shell.execute_reply":"2021-10-04T22:21:16.746756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PerspectiveTransform\n\nIf we know matched Keypoints in the images, we can find homography `H` using `cv2.findHomography`. \n\nbelow code show how we can transform sideline helmet boxes to tracking data scale.","metadata":{}},{"cell_type":"code","source":"trakcing_coordinate = np.float32(list(zip(homography_df['x'],53.33-homography_df['y']))).reshape(-1,1,2)\nlabel_coordinate =  np.float32(list(zip(homography_df['left']+homography_df['width']/2,homography_df['top']-homography_df['height']/2))).reshape(-1,1,2)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T22:16:04.139935Z","iopub.status.idle":"2021-10-04T22:16:04.140244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"H, mask = cv2.findHomography(label_coordinate, trakcing_coordinate)\ntransformed_coordinate =  cv2.perspectiveTransform(label_coordinate, H)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T12:54:43.551832Z","iopub.execute_input":"2021-09-29T12:54:43.552311Z","iopub.status.idle":"2021-09-29T12:54:43.593235Z","shell.execute_reply.started":"2021-09-29T12:54:43.552277Z","shell.execute_reply":"2021-09-29T12:54:43.591959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(H)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T12:54:43.59473Z","iopub.execute_input":"2021-09-29T12:54:43.595068Z","iopub.status.idle":"2021-09-29T12:54:43.601149Z","shell.execute_reply.started":"2021-09-29T12:54:43.59504Z","shell.execute_reply":"2021-09-29T12:54:43.600025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\n\nplt.scatter(transformed_coordinate[:, :, 0],transformed_coordinate[:, :, 1])\n\nplt.scatter(homography_df['x'], 53.33-homography_df['y'])\n\nplt.legend(['Transformed coordinate from Sideline helmet box','Ground truth tracking data'])","metadata":{"execution":{"iopub.status.busy":"2021-10-04T22:16:04.14148Z","iopub.status.idle":"2021-10-04T22:16:04.141833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"But important thing is that we can't match each keypoints exactly becaues we don't have a enough information to find homography.\n\nIf we can match specific pair points with tracking images, side & endzone images, we might find good homography.\n\nI'll use filed line numbers to match both images","metadata":{}},{"cell_type":"markdown","source":"## Video to Frame","metadata":{}},{"cell_type":"code","source":"video_name = homography_df.video.unique()\nvideo_path = f\"{BASE_DIR}/train/{video_name[0]}\"\n\nvid = imageio.get_reader(video_path, 'ffmpeg')\nimg = vid.get_data(check_frame - 1)\nplt.figure(figsize=(12, 10))\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T22:16:04.142546Z","iopub.status.idle":"2021-10-04T22:16:04.142919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finding filed line number points in sideline","metadata":{}},{"cell_type":"code","source":"line_numbers = [[110, 600],  ## Home Sideline 20\n                [550, 630],  ## Home Sideline 30\n                [990, 680],  ## Home Sideline 40\n                [1150, 200], ## Victory Sideline 40\n                [770, 200]]  ## Victory Sideline 30\nfor line_number in line_numbers:\n    img = cv2.circle(img, (line_number[0],line_number[1]), radius=2, color=(0, 255, 255), thickness=10)\n\nplt.figure(figsize=(12, 10))\nplt.imshow(img)    ","metadata":{"execution":{"iopub.status.busy":"2021-09-29T12:56:21.153261Z","iopub.execute_input":"2021-09-29T12:56:21.153794Z","iopub.status.idle":"2021-09-29T12:56:21.667626Z","shell.execute_reply.started":"2021-09-29T12:56:21.153763Z","shell.execute_reply":"2021-09-29T12:56:21.666864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ","metadata":{}},{"cell_type":"markdown","source":"## Finding filed line number points in tracking data\n![images](https://drive.google.com/uc?export=view&id=1IdUQHo9G673ifp-mIrwiG_ep0q88H13N)\n\nIf we treat tracking `x`, `y`  as pair points in this images, we can guess that Finding filed line number points.","metadata":{}},{"cell_type":"code","source":"projection_numbers = [[30, 53.3-10], ## Home Sideline 20\n                      [40, 53.3-10], ## Home Sideline 30\n                      [50, 53.3-10], ## Home Sideline 40\n                      [50, 10],      ## Victory Sideline 40\n                      [40, 10]]      ## Victory Sideline 30","metadata":{"execution":{"iopub.status.busy":"2021-09-29T12:54:44.600242Z","iopub.execute_input":"2021-09-29T12:54:44.600801Z","iopub.status.idle":"2021-09-29T12:54:44.606886Z","shell.execute_reply.started":"2021-09-29T12:54:44.600737Z","shell.execute_reply":"2021-09-29T12:54:44.605883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"H, mask = cv2.findHomography(np.float32(line_numbers).reshape(5, 2), np.float32(projection_numbers).reshape(5, 2))","metadata":{"execution":{"iopub.status.busy":"2021-09-29T12:56:29.912509Z","iopub.execute_input":"2021-09-29T12:56:29.912966Z","iopub.status.idle":"2021-09-29T12:56:29.918594Z","shell.execute_reply.started":"2021-09-29T12:56:29.912926Z","shell.execute_reply":"2021-09-29T12:56:29.917736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(H)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T12:56:33.021404Z","iopub.execute_input":"2021-09-29T12:56:33.02193Z","iopub.status.idle":"2021-09-29T12:56:33.028097Z","shell.execute_reply.started":"2021-09-29T12:56:33.021897Z","shell.execute_reply":"2021-09-29T12:56:33.02667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformed_coordinate =  cv2.perspectiveTransform(label_coordinate, H)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T12:56:35.324986Z","iopub.execute_input":"2021-09-29T12:56:35.325475Z","iopub.status.idle":"2021-09-29T12:56:35.32958Z","shell.execute_reply.started":"2021-09-29T12:56:35.325445Z","shell.execute_reply":"2021-09-29T12:56:35.32876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\n\nplt.scatter(transformed_coordinate[:, :, 0],transformed_coordinate[:, :, 1])\n\nplt.scatter(homography_df['x'], 53.33-homography_df['y'])\n\nplt.legend(['Transformed coordinate from Sideline helmet box','Ground truth tracking data'])","metadata":{"execution":{"iopub.status.busy":"2021-09-29T12:56:36.541475Z","iopub.execute_input":"2021-09-29T12:56:36.542064Z","iopub.status.idle":"2021-09-29T12:56:36.80929Z","shell.execute_reply.started":"2021-09-29T12:56:36.54203Z","shell.execute_reply":"2021-09-29T12:56:36.808249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Next to \n- Matching label using homography information\n- Build filed number detection model ? \n- Merging with MOT models like deepsort, FairMOT","metadata":{}},{"cell_type":"markdown","source":"matches**Feature Matching**","metadata":{}},{"cell_type":"code","source":"class SiftKpDesc():\n    def __init__(self, kp, desc):\n        # List of keypoints in (x,y) crd -> N x 2\n        self.kp = kp\n\n        # List of Descriptors at keypoints : N x 128\n        self.desc = desc\n\n\nclass SiftMatching:\n\n    _BLUE = [255, 0, 0]\n    _GREEN = [0, 255, 0]\n    _RED = [0, 0, 255]\n    _CYAN = [255, 255, 0]\n\n    _line_thickness = 2\n    _radius = 5\n    _circ_thickness = 2\n\n\n    def __init__(self, img_1, img_2, results_fldr='', nfeatures=2000, gamma=0.8):\n\n#         fname_1 = os.path.basename(img_1_path)\n#         fname_2 = os.path.basename(img_2_path)\n\n#         if not results_fldr:\n#             results_fldr = os.path.split(img_1_path)[0]\n\n#         self.result_fldr = os.path.join(results_fldr, 'results')\n\n#         self.prefix = fname_1.split('.')[0] + '_' + fname_2.split('.')[0]\n\n#         if not os.path.exists(self.result_fldr):\n#             os.makedirs(self.result_fldr)\n\n        self.img_1_bgr = img_1\n        self.img_2_bgr = img_2\n\n        self.nfeatures = nfeatures\n        self.gamma = gamma\n\n\n    def read_image(self, img_path):\n\n        img_bgr = cv2.imread(img_path, cv2.IMREAD_COLOR)\n\n        return img_bgr\n\n\n\n    def get_sift_features(self, img_bgr, nfeatures=2000):\n\n        img_gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n\n        sift_obj = cv2.xfeatures2d.SIFT_create(nfeatures)\n\n        # kp_list_obj is a list of \"KeyPoint\" objects with location stored as tuple in \"pt\" attribute\n        kp_list_obj, desc = sift_obj.detectAndCompute(image=img_gray, mask=None)\n\n        kp = [x.pt for x in kp_list_obj]\n\n        return SiftKpDesc(kp, desc)\n\n\n    def match_features(self, sift_kp_desc_obj1, sift_kp_desc_obj2, gamma=0.8):\n        correspondence = []  # list of lists of [x1, y1, x2, y2]\n\n        for i in range(len(sift_kp_desc_obj1.kp)):\n            sc = np.linalg.norm(sift_kp_desc_obj1.desc[i] - sift_kp_desc_obj2.desc, axis=1)\n            idx = np.argsort(sc)\n\n            val = sc[idx[0]] / sc[idx[1]]\n\n            if val <= gamma:\n                correspondence.append([*sift_kp_desc_obj1.kp[i], *sift_kp_desc_obj2.kp[idx[0]]])\n\n        return correspondence\n\n\n    def draw_correspondence(self, correspondence, img_1, img_2):\n\n        if len(img_1.shape) == 2:\n            img_1 = np.repeat(img_1[:, :, np.newaxis], 3, axis=2)\n\n        if len(img_2.shape) == 2:\n            img_2 = np.repeat(img_2[:, :, np.newaxis], 3, axis=2)\n\n        h, w, _ = img_1.shape\n\n        img_stack = np.hstack((img_1, img_2))\n\n        for x1, y1, x2, y2 in correspondence:\n            x1_d = int(round(x1))\n            y1_d = int(round(y1))\n\n            x2_d = int(round(x2) + w)\n            y2_d = int(round(y2))\n\n            cv2.circle(img_stack, (x1_d, y1_d), radius=self._radius, color=self._BLUE,\n                       thickness=self._circ_thickness, lineType=cv2.LINE_AA)\n\n            cv2.circle(img_stack, (x2_d, y2_d), radius=self._radius, color=self._BLUE,\n                       thickness=self._circ_thickness, lineType=cv2.LINE_AA)\n\n            cv2.line(img_stack, (x1_d, y1_d), (x2_d, y2_d), color=self._CYAN,\n                     thickness=self._line_thickness)\n\n        fname = os.path.join(self.result_fldr, self.prefix + '_sift_corr.jpg')\n        cv2.imwrite(fname, img_stack)\n\n\n    def run(self):\n\n        sift_kp_desc_obj1 = self.get_sift_features(self.img_1_bgr, nfeatures=self.nfeatures)\n        sift_kp_desc_obj2 = self.get_sift_features(self.img_2_bgr, nfeatures=self.nfeatures)\n\n        correspondence = self.match_features(sift_kp_desc_obj1, sift_kp_desc_obj2, gamma=self.gamma)\n\n        self.draw_correspondence(correspondence, self.img_1_bgr, self.img_2_bgr)\n\n        return correspondence","metadata":{"execution":{"iopub.status.busy":"2021-09-29T12:49:06.689689Z","iopub.execute_input":"2021-09-29T12:49:06.690018Z","iopub.status.idle":"2021-09-29T12:49:06.710785Z","shell.execute_reply.started":"2021-09-29T12:49:06.68999Z","shell.execute_reply":"2021-09-29T12:49:06.709596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-10-04T22:16:37.73936Z","iopub.execute_input":"2021-10-04T22:16:37.739707Z","iopub.status.idle":"2021-10-04T22:16:37.744545Z","shell.execute_reply.started":"2021-10-04T22:16:37.739677Z","shell.execute_reply":"2021-10-04T22:16:37.743244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimg = vid.get_data(check_frame - 1)\nimg2 = vid.get_data(check_frame)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-29T12:52:13.82671Z","iopub.status.idle":"2021-09-29T12:52:13.827129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\n   \nvideo_name = homography_df.video.unique()\n# video_path = f\"{BASE_DIR}/train/57905_002404_Sideline.mp4\"\nvideo_path = f\"{BASE_DIR}/train/57584_000336_Sideline.mp4\"\n\nvid = imageio.get_reader(video_path, 'ffmpeg')\n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-04T23:32:24.324745Z","iopub.execute_input":"2021-10-04T23:32:24.325366Z","iopub.status.idle":"2021-10-04T23:32:24.46074Z","shell.execute_reply.started":"2021-10-04T23:32:24.32528Z","shell.execute_reply":"2021-10-04T23:32:24.459953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the query image as query_img\n# and traing image This query image\n# is what you need to find in train image\n# Save it in the same directory\n# with the name image.jpg  \nquery_img = np.array(vid.get_data(0))\ntrain_img = np.array(vid.get_data(100))\n\n# query_img = cv2.resize(query_img,(360,640))\n# train_img = cv2.resize(train_img,(360,640))\n\n# alpha = 1 # Contrast control (1.0-3.0)\n# beta = 64.0 # Brightness control (0-100)\n\n# train_img = cv2.convertScaleAbs(train_img, alpha=alpha, beta=beta)\n# query_img = cv2.convertScaleAbs(query_img, alpha=alpha, beta=beta)\n\n\n\n# Convert it to grayscale\nquery_img_bw = cv2.cvtColor(query_img,cv2.COLOR_BGR2GRAY)\ntrain_img_bw = cv2.cvtColor(train_img, cv2.COLOR_BGR2GRAY)\n\nquery_img_bw = cv2.GaussianBlur(query_img_bw,(35,35),cv2.BORDER_DEFAULT)\ntrain_img_bw = cv2.GaussianBlur(train_img_bw,(35,35),cv2.BORDER_DEFAULT)\n\n# query_img_bw = cv2.adaptiveThreshold(query_img_bw,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,51,28)\n# train_img_bw = cv2.adaptiveThreshold(train_img_bw,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,51,28)\n\n# kernel = np.ones((2, 2), 'uint8')\n\n# query_img_bw = cv2.adaptiveThreshold(query_img_bw,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,11,2)\n# query_img_bw = cv2.dilate(query_img_bw, kernel, iterations=1)\n\n# train_img_bw = cv2.adaptiveThreshold(train_img_bw,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,11,2)\n# train_img_bw = cv2.dilate(train_img_bw, kernel, iterations=1)\n\n# se1 = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))\n# se2 = cv2.getStructuringElement(cv2.MORPH_RECT, (1,1))\n# query_img_bw = cv2.morphologyEx(query_img_bw, cv2.MORPH_CLOSE, se1)\n# query_img_bw = cv2.morphologyEx(query_img_bw, cv2.MORPH_OPEN, se2)\n# kernel = np.ones((3, 3), np.uint8)\n# query_img_bw = query_img_bw-cv2.erode(query_img_bw, kernel)\n# query_img_bw[query_img_bw < 0] = 255\n\n# se1 = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))\n# se2 = cv2.getStructuringElement(cv2.MORPH_RECT, (1,1))\n# train_img_bw = cv2.morphologyEx(train_img_bw, cv2.MORPH_CLOSE, se1)\n# train_img_bw = cv2.morphologyEx(train_img_bw, cv2.MORPH_OPEN, se2)\n# kernel = np.ones((3, 3), np.uint8)\n# train_img_bw = train_img_bw-cv2.erode(train_img_bw, kernel)\n# train_img_bw[train_img_bw < 0] = 255\n\n# Initialize the ORB detector algorithm\norb = cv2.ORB_create(150)\n   \n# Now detect the keypoints and compute\n# the descriptors for the query image\n# and train image\nqueryKeypoints, queryDescriptors = orb.detectAndCompute(query_img_bw,None)\ntrainKeypoints, trainDescriptors = orb.detectAndCompute(train_img_bw,None)\n\n# Initialize the Matcher for matching\n# the keypoints and then match the\n# keypoints\nmatcher = cv2.BFMatcher()\nmatches = matcher.match(queryDescriptors,trainDescriptors)\n   \n# draw the matches to the final image\n# containing both the images the drawMatches()\n# function takes both images and keypoints\n# and outputs the matched query image with\n# its train image\nfinal_img = cv2.drawMatches(query_img, queryKeypoints, \ntrain_img, trainKeypoints, matches[0:3],None)\n   \nfinal_img = cv2.resize(final_img, (1000,650))\n  \n# Show the final image\nplt.figure(figsize=(12,10))\nplt.imshow(final_img)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T23:41:28.727497Z","iopub.execute_input":"2021-10-04T23:41:28.728021Z","iopub.status.idle":"2021-10-04T23:41:29.82459Z","shell.execute_reply.started":"2021-10-04T23:41:28.727991Z","shell.execute_reply":"2021-10-04T23:41:29.823918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(queryKeypoints[0].pt)\nprint(trainKeypoints[0].pt)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T13:14:21.50135Z","iopub.execute_input":"2021-09-29T13:14:21.5019Z","iopub.status.idle":"2021-09-29T13:14:21.506835Z","shell.execute_reply.started":"2021-09-29T13:14:21.501857Z","shell.execute_reply":"2021-09-29T13:14:21.506068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_img.shape\nprint(720/2,1280/2)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T13:49:00.058145Z","iopub.execute_input":"2021-09-29T13:49:00.058798Z","iopub.status.idle":"2021-09-29T13:49:00.066936Z","shell.execute_reply.started":"2021-09-29T13:49:00.058724Z","shell.execute_reply":"2021-09-29T13:49:00.065603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a = query_img_bw.astype(np.float)\n# gray = np.float32(query_img)\n# dst = cv2.cornerHarris(query_img_bw,2,3,0.04)\n# dst = cv2.dilate(dst,None)\n# query_img[dst>0.01*dst.max()]=[0,0,255]\n# plt.figure(figsize=(12,10))\n# plt.imshow(query_img)\n\nkernel = np.ones((2, 2), 'uint8')\nimg = cv2.adaptiveThreshold(query_img_bw,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,51,28)\n# img = cv2.dilate(img, kernel, iterations=1)\n\nplt.figure(figsize=(12,10))\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T22:29:07.369778Z","iopub.execute_input":"2021-10-04T22:29:07.37025Z","iopub.status.idle":"2021-10-04T22:29:07.938593Z","shell.execute_reply.started":"2021-10-04T22:29:07.370213Z","shell.execute_reply":"2021-10-04T22:29:07.937216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gray = query_img_bw.copy()\nimg = query_img.copy()\n\nedges = cv2.Canny( gray,50,150,apertureSize = 3)\n\n# Show result\nplt.figure(figsize=(15,10))\nplt.imshow(edges)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T23:13:22.552674Z","iopub.execute_input":"2021-10-04T23:13:22.553047Z","iopub.status.idle":"2021-10-04T23:13:22.894499Z","shell.execute_reply.started":"2021-10-04T23:13:22.553021Z","shell.execute_reply":"2021-10-04T23:13:22.893757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(queryKeypoints[matches[2].queryIdx].pt)\nprint(trainKeypoints[matches[2].trainIdx].pt)\n# queryDescriptors,\n# trainDescriptors","metadata":{"execution":{"iopub.status.busy":"2021-10-04T23:42:19.865436Z","iopub.execute_input":"2021-10-04T23:42:19.865768Z","iopub.status.idle":"2021-10-04T23:42:19.872299Z","shell.execute_reply.started":"2021-10-04T23:42:19.865742Z","shell.execute_reply":"2021-10-04T23:42:19.871534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arr = []\nfor i in range(len(matches)):\n    if matches[i].trainIdx in arr:\n        print(\"arr!!!!\",matches[i].trainIdx)\n        arr.append(matches[i].trainIdx) \n    else:\n        arr.append(matches[i].trainIdx) ","metadata":{"execution":{"iopub.status.busy":"2021-10-04T23:44:42.014036Z","iopub.execute_input":"2021-10-04T23:44:42.014406Z","iopub.status.idle":"2021-10-04T23:44:42.032257Z","shell.execute_reply.started":"2021-10-04T23:44:42.014378Z","shell.execute_reply":"2021-10-04T23:44:42.030241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arr[92]","metadata":{"execution":{"iopub.status.busy":"2021-10-04T23:46:54.033056Z","iopub.execute_input":"2021-10-04T23:46:54.033455Z","iopub.status.idle":"2021-10-04T23:46:54.040084Z","shell.execute_reply.started":"2021-10-04T23:46:54.033429Z","shell.execute_reply":"2021-10-04T23:46:54.03876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.imshow(query_img_bw)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T23:30:30.771468Z","iopub.execute_input":"2021-10-04T23:30:30.771815Z","iopub.status.idle":"2021-10-04T23:30:31.201304Z","shell.execute_reply.started":"2021-10-04T23:30:30.771766Z","shell.execute_reply":"2021-10-04T23:30:31.200033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir(f\"{BASE_DIR}/train/\")","metadata":{"execution":{"iopub.status.busy":"2021-10-04T23:27:10.406671Z","iopub.execute_input":"2021-10-04T23:27:10.407023Z","iopub.status.idle":"2021-10-04T23:27:10.428563Z","shell.execute_reply.started":"2021-10-04T23:27:10.406995Z","shell.execute_reply":"2021-10-04T23:27:10.427286Z"},"trusted":true},"execution_count":null,"outputs":[]}]}