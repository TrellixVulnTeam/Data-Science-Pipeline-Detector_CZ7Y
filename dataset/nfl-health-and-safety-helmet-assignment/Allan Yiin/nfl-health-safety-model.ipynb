{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%matplotlib inline\n#這是jupyter notebook的magic word˙\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom IPython import display","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-15T03:20:00.319217Z","iopub.execute_input":"2021-12-15T03:20:00.31955Z","iopub.status.idle":"2021-12-15T03:20:00.346621Z","shell.execute_reply.started":"2021-12-15T03:20:00.319458Z","shell.execute_reply":"2021-12-15T03:20:00.34598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n#判斷是否在jupyter notebook上\ndef is_in_ipython():\n    \"Is the code running in the ipython environment (jupyter including)\"\n    program_name = os.path.basename(os.getenv('_', ''))\n\n    if ('jupyter-notebook' in program_name or # jupyter-notebook\n        'ipython'          in program_name or # ipython\n        'jupyter' in program_name or  # jupyter\n        'JPY_PARENT_PID'   in os.environ):    # ipython-notebook\n        return True\n    else:\n        return False\n\n\n#判斷是否在colab上\ndef is_in_colab():\n    if not is_in_ipython(): return False\n    try:\n        from google import colab\n        return True\n    except: return False\n\n#判斷是否在kaggke_kernal上\ndef is_in_kaggle_kernal():\n    if 'kaggle' in os.environ['PYTHONPATH']:\n        return True\n    else:\n        return False\n\nif is_in_colab():\n    from google.colab import drive\n    drive.mount('/content/gdrive')\n\nos.environ['TRIDENT_BACKEND'] = 'pytorch'\nkaggle_kernal=None\nif is_in_kaggle_kernal():\n    os.environ['TRIDENT_HOME'] = './trident'\n    \nelif is_in_colab():\n    os.environ['TRIDENT_HOME'] = '/content/gdrive/My Drive/trident'\n  \n","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:20:00.348085Z","iopub.execute_input":"2021-12-15T03:20:00.34842Z","iopub.status.idle":"2021-12-15T03:20:00.359055Z","shell.execute_reply.started":"2021-12-15T03:20:00.348384Z","shell.execute_reply":"2021-12-15T03:20:00.358172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#為確保安裝最新版 \n\n!pip uninstall tridentx -y\n!pip install ../input/trident/tridentx-0.7.4-py3-none-any.whl --upgrade\n\nimport re\nimport pandas\nimport json\nimport copy\n\nimport numpy as np\n#調用trident api\nimport subprocess\nimport random\nfrom tqdm import tqdm\nimport scipy\nimport time\nimport glob\nimport pandas as pd\nfrom shutil import copyfile\nfrom torch import nn","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:20:00.360221Z","iopub.execute_input":"2021-12-15T03:20:00.360673Z","iopub.status.idle":"2021-12-15T03:20:13.0159Z","shell.execute_reply.started":"2021-12-15T03:20:00.36063Z","shell.execute_reply":"2021-12-15T03:20:13.014895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import trident as T\nfrom trident import *\n\npalette = [(255, 192, 0), (0, 192, 255), (128, 0, 255), (192,255,64), (255, 0, 128), (64,255,128)]","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:20:13.021296Z","iopub.execute_input":"2021-12-15T03:20:13.021585Z","iopub.status.idle":"2021-12-15T03:20:18.090434Z","shell.execute_reply.started":"2021-12-15T03:20:13.021547Z","shell.execute_reply":"2021-12-15T03:20:18.088462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read in data files\nBASE_DIR = '../input/nfl-health-and-safety-helmet-assignment'\n\n# 標籤與提交範例\ntrain_labels = pd.read_csv(f'{BASE_DIR}/train_labels.csv')\nss = pd.read_csv(f'{BASE_DIR}/sample_submission.csv')\n\n# 球員追蹤數據\ntrain_player_tracking = pd.read_csv(f'{BASE_DIR}/train_player_tracking.csv')\ntest_player_tracking = pd.read_csv(f'{BASE_DIR}/test_player_tracking.csv')\n\n# Baseline helmet detection labels\ntrain_baseline_helmets = pd.read_csv(f'{BASE_DIR}/train_baseline_helmets.csv')\ntest_baseline_helmets = pd.read_csv(f'{BASE_DIR}/test_baseline_helmets.csv')\n\n# 額外的圖像標註\nimg_labels = pd.read_csv(f'{BASE_DIR}/image_labels.csv')\n\n\n\n\n\nimg_labels","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:20:18.092272Z","iopub.execute_input":"2021-12-15T03:20:18.092523Z","iopub.status.idle":"2021-12-15T03:20:22.425799Z","shell.execute_reply.started":"2021-12-15T03:20:18.092485Z","shell.execute_reply":"2021-12-15T03:20:22.42511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extra_images=glob.glob('../input/nfl-health-and-safety-helmet-assignment/images/*.jpg')\nprint(len(extra_images))\n\nimg_labels_frequency=img_labels['label'].value_counts()\nprint(img_labels_frequency)\n\nlabel2index = {\n    'Helmet': 0,\n    'Helmet-Blurred': 1,\n    'Helmet-Difficult': 2,\n    'Helmet-Sideline': 3,\n    'Helmet-Partial': 4\n}\n\nfolder,filename,ext=split_path(extra_images[0])\nsample_data=img_labels[img_labels['image']==filename+ext]\nsample_data\n","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:20:22.42715Z","iopub.execute_input":"2021-12-15T03:20:22.427417Z","iopub.status.idle":"2021-12-15T03:20:22.695839Z","shell.execute_reply.started":"2021-12-15T03:20:22.42738Z","shell.execute_reply":"2021-12-15T03:20:22.695156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img=image2array(extra_images[0])\npillow_img=array2image(img)\n\nfor index, item in sample_data.iterrows():\n    box=[int(item['left']),int(item['top']),int(item['left'])+int(item['width']),int(item['top'])+int(item['height'])]\n    pillow_img =plot_bbox(box, pillow_img, palette[label2index[item['label']]] ,item['label'], line_thickness=2)\n\ndisplay.display(pillow_img)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:20:22.697199Z","iopub.execute_input":"2021-12-15T03:20:22.69743Z","iopub.status.idle":"2021-12-15T03:20:23.231475Z","shell.execute_reply.started":"2021-12-15T03:20:22.697397Z","shell.execute_reply":"2021-12-15T03:20:23.230772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bboxes_dataset_dict=unpickle('../input/nfl-health-safety-baseline/bboxes_dataset_dict.pkl')\nvideo_bboxes_dataset_dict=unpickle('../input/nfl-health-safety-baseline/video_bboxes_dataset_dict.pkl')\nprint(len(bboxes_dataset_dict))\nprint(len(video_bboxes_dataset_dict))","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:20:23.232448Z","iopub.execute_input":"2021-12-15T03:20:23.232695Z","iopub.status.idle":"2021-12-15T03:20:23.341835Z","shell.execute_reply.started":"2021-12-15T03:20:23.232663Z","shell.execute_reply":"2021-12-15T03:20:23.34111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trident.models import ssd,rfbnet\n\n#pretrainedmodel=rfbnet.RfbNet(pretrianed=True)\n#pretrainedmodel.summary()\n\nrfbmodel=rfbnet.RfbNet(pretrianed=True, num_classes=2, num_regressors=4)\nprint(rfbmodel.model.priors)\nrfbmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:20:23.343302Z","iopub.execute_input":"2021-12-15T03:20:23.343548Z","iopub.status.idle":"2021-12-15T03:20:33.680657Z","shell.execute_reply.started":"2021-12-15T03:20:23.343514Z","shell.execute_reply":"2021-12-15T03:20:33.679925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef intersect(box_a, box_b):\n    \"\"\" We Resize both tensors to [A,B,2] without new malloc:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [A,4].\n      box_b: (tensor) bounding boxes, Shape: [B,4].\n    Return:\n      (tensor) intersection area, Shape: [A,B].\n    \"\"\"\n    A = box_a.size(0)\n    B = box_b.size(0)\n    max_xy = torch.min(box_a[:, 2:4].unsqueeze(1).expand(A, B, 2), box_b[:, 2:4].unsqueeze(0).expand(A, B, 2))\n    min_xy = torch.max(box_a[:, 0:2].unsqueeze(1).expand(A, B, 2), box_b[:, 0:2].unsqueeze(0).expand(A, B, 2))\n    inter = torch.clamp((max_xy - min_xy), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\n\ndef jaccard(box_a, box_b):\n    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes.\n    E.g.:\n        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n    \"\"\"\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n    union = area_a + area_b - inter\n    return inter,union  # [A,B]\n\n\n\ndef match(truths, priors, variances, labels, keypoints=None,threshold=0.3):\n    \"\"\"Match each prior box with the ground truth box of the highest jaccard\n    overlap, encode the bounding boxes, then return the matched indices\n    corresponding to both confidence and location preds.\n    Args:\n\n        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].\n        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n        variances: (tensor) Variances corresponding to each prior coord,\n            Shape: [num_priors, 4].\n        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\n        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\n        idx: (int) current batch index\n        threshold: (float) The overlap threshold used when mathing boxes.\n\n    Return:\n        The matched indices corresponding to 1)location and 2)confidence preds.\n    \"\"\"\n    # jaccard index\n    priors2 = priors.clone()\n    num_priors = len(priors)\n    overlaps = jaccard(truths, xywh2xyxy(priors.clone()))\n    # (Bipartite Matching)\n    # [1,num_objects] best prior for each ground truth\n    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n\n    # ignore hard gt\n    valid_gt_idx = best_prior_overlap[:, 0] >= 0.2\n    best_prior_idx_filter = best_prior_idx[valid_gt_idx, :]\n    if best_prior_idx_filter.shape[0] <= 0:\n        return np.zeros((num_priors, 4)).astype(np.float32), np.zeros((num_priors,1)).astype(np.int64),np.zeros((num_priors, 16)).astype(np.float32)\n\n    # [1,num_priors] best ground truth for each prior\n    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n    best_truth_idx.squeeze_(0)\n    best_truth_overlap.squeeze_(0)\n    best_prior_idx.squeeze_(1)\n    best_prior_idx_filter.squeeze_(1)\n    best_prior_overlap.squeeze_(1)\n    best_truth_overlap.index_fill_(0, best_prior_idx_filter, 2)  # ensure best prior\n    # TODO refactor: index  best_prior_idx with long tensor\n    # ensure every gt matches with its prior of max overlap\n    for j in range(best_prior_idx.size(0)):\n        best_truth_idx[best_prior_idx[j]] = j\n    matches = truths[best_truth_idx]  # Shape: [num_priors,14]\n    conf = labels[best_truth_idx]  # Shape: [num_priors]\n    keypoints=keypoints[best_truth_idx]\n    conf[best_truth_overlap < threshold] = 0  # label as background\n\n    loc = encode(matches, priors2, variances)\n    return loc, conf,keypoints\n\n\n\ndef encode(matched, priors, variances):\n    \"\"\"\n\n    Args:\n        matched (tensor): Coords of ground truth for each prior in xyxy Shape: [num_priors, 4].\n        priors (tensor): Prior boxes in center-offset form Shape: [num_priors,4].\n        variances (list[float]):  Variances of priorboxes\n\n    Returns:\n        encoded boxes and landmarks (tensor), Shape: [num_priors, 14]\n\n    \"\"\"\n\n    # dist b/t match center and prior's center\n    priors = priors.clone()\n    g_cxcy = (matched[:, 0:2] + matched[:, 2:4]) / 2 - priors[:, 0:2]\n    # encode variance\n    g_cxcy /= (variances[0] * priors[:, 2:4])\n    # match wh / prior wh\n    g_wh = (matched[:, 2:4] - matched[:, 0:2]) / priors[:, 2:4]\n    g_wh = torch.log(g_wh) / variances[1]\n\n    # # landmarks\n    # g_xy1 = (matched[:, 4:6] - priors[:, 0:2]) / (variances[0] * priors[:, 2:4])\n    # g_xy2 = (matched[:, 6:8] - priors[:, 0:2]) / (variances[0] * priors[:, 2:4])\n    # g_xy3 = (matched[:, 8:10] - priors[:, 0:2]) / (variances[0] * priors[:, 2:4])\n    # g_xy4 = (matched[:, 10:12] - priors[:, 0:2]) / (variances[0] * priors[:, 2:4])\n    # g_xy5 = (matched[:, 12:14] - priors[:, 0:2]) / (variances[0] * priors[:, 2:4])\n\n    # return target for loss\n    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,14]\n\n\ndef decode(loc, priors, variances):\n    \"\"\"Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Adapted from https://github.com/Hakuyume/chainer-ssd\n\n    Args:\n        loc (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n\n    Return:\n        decoded bounding box predictions\n\n    \"\"\"\n    boxes = torch.cat([priors.unsqueeze(0)[:, :, 0:2] + loc[:, :, 0:2] * variances[0] * priors.unsqueeze(0)[:, :, 2:4],\n                       priors.unsqueeze(0)[:, :, 2:4] * torch.exp(loc[:, :, 2:4] * variances[1])], -1)\n    boxes[:, :, 0:2] -= boxes[:, :, 2:4] / 2\n    boxes[:, :, 2:4] += boxes[:, :, 0:2]\n    return boxes\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:20:33.683655Z","iopub.execute_input":"2021-12-15T03:20:33.683851Z","iopub.status.idle":"2021-12-15T03:20:33.707459Z","shell.execute_reply.started":"2021-12-15T03:20:33.683826Z","shell.execute_reply":"2021-12-15T03:20:33.706814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.set_printoptions(precision=3)\n\ndef point_in_box(point,box):\n    if ndim(box)+1==ndim(point):\n        box=np.expand_dims(box,1)\n    if ndim(point)==ndim(box)==1:\n        x,y=point\n        x1,y1,x2,y2=box\n        if x1<x<x2 and y1<y<y2:\n            return True\n        else:\n            return False\n    elif  ndim(point)==ndim(box)==2:\n        x=point[:,0]\n        y=point[:,1]\n        x1=box[:,0]\n        y1=box[:,1]\n        x2=box[:,2]\n        y2=box[:,3]\n        return np.greater_equal(x,x1)*np.greater_equal(x2,x)*np.greater_equal(y,y1)*np.greater_equal(y2,y)\n    elif  ndim(point)==ndim(box)==3:\n        x=point[:,:,0]\n        y=point[:,:,1]\n        x1=box[:,:,0]\n        y1=box[:,:,1]\n        x2=box[:,:,2]\n        y2=box[:,:,3]\n        return np.greater_equal(x,x1)*np.greater_equal(x2,x)*np.greater_equal(y,y1)*np.greater_equal(y2,y)\n   ","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:20:33.708547Z","iopub.execute_input":"2021-12-15T03:20:33.708828Z","iopub.status.idle":"2021-12-15T03:20:33.721781Z","shell.execute_reply.started":"2021-12-15T03:20:33.708771Z","shell.execute_reply":"2021-12-15T03:20:33.720993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef xywh2xyxy(locations):\n    return np.concatenate([locations[..., :2] - locations[..., 2:] / 2.0, locations[..., :2] + locations[..., 2:] / 2.0],\n                     axis=1)\n\nclass SsdBboxDataset(BboxDataset):\n    def __init__(self, boxes=None, image_size=(480, 640), priors=None, center_variance=0.1, size_variance=0.2,\n                 gt_overlap_tolerance=0.5, object_type=ObjectType.absolute_bbox, class_names=None,\n                 symbol='bbox', name=''):\n        super().__init__(boxes=boxes, image_size=image_size, object_type=object_type, class_names=class_names,\n                         symbol=symbol, name=name)\n        self._element_spec = TensorSpec(shape=TensorShape([None, 21]), name=self.symbol, object_type=self.object_type, is_spatial=True)\n        self.priors = priors\n        self.image_size = image_size\n        self.center_variance = center_variance\n        self.size_variance = size_variance\n        self.label_transform_funcs = []\n        self.gt_overlap_tolerance = gt_overlap_tolerance\n        self.bbox_post_transform_funcs = []\n        print('priors',self.priors.shape,self.priors )\n\n    def __getitem__(self, index: int):\n\n        # 取出xyxy定義boxes或是[]\n        # 為了避免[]清單與np.ndarray計算差異，因此補上np.array([[-1,-1,-1,-1,-1]])替代\n        if self.items[index] is None or self.items[index] == []:\n            return np.array([-1] * 21)\n        else:\n            \n            return self.items[index].astype(np.float32)\n\n    def area_of(self, left_top, right_bottom):\n        \"\"\"Compute the areas of rectangles given two corners.\n\n        Args:\n            left_top (N, 2): left top corner.\n            right_bottom (N, 2): right bottom corner.\n\n        Returns:\n            area (N): return the area.\n        \"\"\"\n        hw = np.clip(right_bottom - left_top, 0.0, None)\n        return hw[..., 0] * hw[..., 1]\n\n    def iou_of(self, boxes0, boxes1, eps=1e-5):\n        \"\"\"Return intersection-over-union (Jaccard index) of boxes.\n\n        Args:\n            boxes0 (N, 4): ground truth boxes.\n            boxes1 (N or 1, 4): predicted boxes.\n            eps: a small number to avoid 0 as denominator.\n        Returns:\n            iou (N): IoU values.\n        \"\"\"\n        overlap_left_top = np.maximum(boxes0[..., :2], boxes1[..., :2])\n        overlap_right_bottom = np.minimum(boxes0[..., 2:], boxes1[..., 2:])\n\n        overlap_area = self.area_of(overlap_left_top, overlap_right_bottom)\n        area0 = self.area_of(boxes0[..., :2], boxes0[..., 2:])\n        area1 = self.area_of(boxes1[..., :2], boxes1[..., 2:])\n        return overlap_area / (area0 + area1 - overlap_area + eps)\n\n    def convert_boxes_to_locations(self, center_form_boxes, center_form_priors):\n        if len(center_form_priors.shape) + 1 == len(center_form_boxes.shape):\n            center_form_priors = np.expand_dims(center_form_priors, 0)\n        return np.concatenate([(center_form_boxes[..., :2] - center_form_priors[..., :2]) / center_form_priors[...,\n                                                                                            2:] / self.center_variance,\n                               np.log(np.clip(center_form_boxes[..., 2:] / center_form_priors[..., 2:], 1e-8, np.inf)) / self.size_variance],\n                              axis=len(center_form_boxes.shape) - 1)\n\n    def assign_priors(self, gt_boxes, gt_labels,center_form_priors, iou_threshold):\n\n        corner_form_priors = xywh2xyxy(center_form_priors)\n        #print('corner_form_priors',corner_form_priors.shape,corner_form_priors)\n        ious = self.iou_of(np.expand_dims(gt_boxes, 0), np.expand_dims(corner_form_priors, 1))\n    \n        # size: num_priors\n        best_target_per_prior, best_target_per_prior_index = np.max(ious, axis=1), np.argmax(ious, axis=1)\n    \n        best_prior_per_target, best_prior_per_target_index = np.max(ious, axis=0), np.argmax(ious, axis=0)\n    \n        \n        for target_index, prior_index in enumerate(best_prior_per_target_index):\n            best_target_per_prior_index[prior_index] = target_index\n        \n        #2.0 is used to make sure every target has a prior assigned\n        best_prior_per_target_index_list = best_prior_per_target_index.tolist()\n        for i in range(best_target_per_prior.shape[0]):\n            if i in best_prior_per_target_index_list:\n                best_target_per_prior[i] = 2\n\n\n       \n        labels= gt_labels[best_target_per_prior_index]\n        labels[best_target_per_prior<2]=0\n    \n        boxes = gt_boxes[best_target_per_prior_index]\n\n\n        return boxes, labels\n\n    def data_transform(self, data):\n        if data is None or len(data) == 0:\n            return np.zeros((self.priors.shape[0], 21)).astype(np.float32)\n        elif isinstance(data, np.ndarray):\n            height, width = self.image_size\n\n            data[:, 0] = np.clip(data[:, 0], 0, width)\n            data[:, 2] = np.clip(data[:, 2], 0, width)\n            data[:, 1] = np.clip(data[:, 1], 0, height)\n            data[:, 3] = np.clip(data[:, 3], 0, height)\n            \n            \n            small_box_mask1=np.round(data[:, 0])==np.round(data[:, 2],0)\n            data[:, 2:3][small_box_mask1,:]+=1\n            small_box_mask2=np.round(data[:, 1])==np.round(data[:, 3],0)\n            data[:, 3:4][small_box_mask2,:]+=1\n            \n            \n            box_w = np.clip(np.expand_dims(data[:, 2] - data[:, 0], -1),1,640)\n            box_h = np.clip(np.expand_dims(data[:, 3] - data[:, 1], -1),1,480)\n            box_left = np.expand_dims(data[:, 0].copy(), -1)\n            box_top = np.expand_dims(data[:, 1].copy(), -1)\n            # print('box_w',box_w.shape)\n            # print('box_left',box_left.shape)\n\n            gt_box = data[:, :4]\n            gt_label = data[:, 4:5]\n            data[:, 4:5]=1\n            gt_box[:, 0] = gt_box[:, 0] / float(width)\n            gt_box[:, 2] = gt_box[:, 2] / float(width)\n            gt_box[:, 1] = gt_box[:, 1] / float(height)\n            gt_box[:, 3] = gt_box[:, 3] / float(height)\n\n        \n\n            if gt_box is not None and len(gt_box) > 0:\n                truths = to_tensor(gt_box).float()\n                labels = to_tensor(gt_label).long()\n      \n                \n                boxes, confidences = self.assign_priors(gt_box, gt_label,  to_numpy(self.priors.copy()), 0.5)\n                boxes = xyxy2xywh(boxes)\n                locations = self.convert_boxes_to_locations(boxes, to_numpy(self.priors.copy()))\n\n                locations = np.concatenate([to_numpy(locations).astype(np.float32),to_numpy(confidences).astype(np.float32)], axis=-1)\n                return to_numpy(locations).astype(np.float32)\n\n            num_priors = self.priors.shape[0]\n            return np.zeros((num_priors, 5)).astype(np.float32)\n\n\n    def bbox_transform(self, bbox):\n        return self.data_transform(bbox)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:20:33.723281Z","iopub.execute_input":"2021-12-15T03:20:33.723679Z","iopub.status.idle":"2021-12-15T03:20:33.759019Z","shell.execute_reply.started":"2021-12-15T03:20:33.723643Z","shell.execute_reply":"2021-12-15T03:20:33.758369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imgs=[]\nboxes=[]\nfor k,v in bboxes_dataset_dict.items():\n    imgs.append('../input/nfl-health-and-safety-helmet-assignment/images/'+k)\n    new_shape=list(v.shape)\n    new_shape[-1]=5\n    new_v=np.ones(new_shape)\n    new_v[:,:4]=v\n    boxes.append(new_v)\nprint(boxes[0])\n    \n    \n\nds1=ImageDataset(imgs,object_type=ObjectType.rgb,symbol='images')\nds2=SsdBboxDataset(boxes,image_size=(480, 640),priors=rfbmodel.model.priors,symbol='boxes')\n\ndata_provider=DataProvider(traindata=Iterator(data=ds1,label=ds2))\n#data_provider.batch_transform_funcs=[OneOf([ImageMosaic((480,640),keep_prob=0.4),DetectionMixup((480,640),keep_prob=0.4)])]\n#data_provider.batch_transform_funcs=[ImageMosaic((480,640),keep_prob=0.6)]\ndata_provider.paired_transform_funcs=[\n    Resize((600,800)),\n    RandomTransform(rotation_range=25,zoom_range=0.1, shift_range=0.1,  shear_range= 0.4,random_flip= 0.2),\n    RandomCrop((480,640),scale_range=(0.2,1.5))]\n    #RandomGridMask(size_range=(480,640),max_d1=64,rotation_range=(10,30),keep_prob=0.6)]\n#data_provider.image_transform_funcs=[]\n#設定影像轉換\ndata_provider.image_transform_funcs=[\n    RandomAdjustGamma(gamma_range=(0.5,1.5)),#調整明暗\n    RandomAdjustHue(scale=(-0.3,0.3)),#調整色相\n    RandomAdjustContrast(scale=(0.6,1.4)),#調整對比\n    RandomAdjustSaturation(scale=(0.6,1.4)),#調整飽和度\n    RandomErasing(size_range=(0.05, 0.1), transparency_range=(0.4, 0.8), transparancy_ratio=1, keep_prob=0.5), #加入隨機擦去\n    RandomBlur(scale=(3,7)),#隨機模糊\n    GrayMixRGB(keep_prob=0.75),\n    SaltPepperNoise(prob=0.002),#椒鹽噪音\n    RandomErasing(size_range=(0.05, 0.1), transparency_range=(0.4, 0.8), transparancy_ratio=0.5, keep_prob=0.5), #加入隨機擦去\n    Normalize(127.5,127.5)] #標準化\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:20:33.760237Z","iopub.execute_input":"2021-12-15T03:20:33.760646Z","iopub.status.idle":"2021-12-15T03:20:33.898726Z","shell.execute_reply.started":"2021-12-15T03:20:33.760609Z","shell.execute_reply":"2021-12-15T03:20:33.897386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imgs,boxes=data_provider.next()\n\nprint(boxes.shape)\ntarget_confidences=boxes[:,:,4]\ntarget_boxes=boxes[:,:,:4]\nthis_target_confidences=target_confidences[target_confidences>0]\n\nthis_target_boxes=target_boxes[target_confidences>0]\n\n\nprint(len(this_target_confidences))\nprint(boxes[target_confidences>0])\n","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:20:33.899994Z","iopub.execute_input":"2021-12-15T03:20:33.900494Z","iopub.status.idle":"2021-12-15T03:20:34.886987Z","shell.execute_reply.started":"2021-12-15T03:20:33.900456Z","shell.execute_reply":"2021-12-15T03:20:34.885297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_provider.preview_images()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:20:34.888207Z","iopub.execute_input":"2021-12-15T03:20:34.888535Z","iopub.status.idle":"2021-12-15T03:20:36.905094Z","shell.execute_reply.started":"2021-12-15T03:20:34.888488Z","shell.execute_reply":"2021-12-15T03:20:36.900994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_actual_target(target_locations,target_confidences,priors,center_variance=0.1, size_variance=0.2):\n    relative_target_boxes = decode(target_locations[:,:,:4].copy(),priors, (center_variance,size_variance))\n    relative_target_boxes=to_numpy(relative_target_boxes)\n    target_boxes = decode(target_locations[:,:,:4].copy(),priors, (center_variance,size_variance))*to_tensor([[640,480,640,480]])\n    target_boxes=np.round(to_numpy(target_boxes),0)\n    target_confidences=to_numpy(target_confidences)\n    all_sorted_boxes=[]\n    all_target_confidences=[]\n    for i in range(len(target_boxes)):\n        this_target_boxes=target_boxes[i][target_confidences[i]>0,:]\n        this_relative_target_boxes=relative_target_boxes[i][target_confidences[i]>0,:]\n        arr_idx=this_target_boxes[:,0]*1000000+this_target_boxes[:,1]*10000+this_target_boxes[:,2]*100+this_target_boxes[:,3]\n        np.sort(arr_idx)\n        sorted_indexes=np.argsort(arr_idx)\n        \n        sorted_target_confidences=target_confidences[i][target_confidences[i]>0][sorted_indexes]\n        \n        sorted_target_boxes=this_target_boxes[sorted_indexes]\n        sorted_relative_target_boxes=this_relative_target_boxes[sorted_indexes]\n        \n        mask=np.ones(len(sorted_target_boxes))\n        for n in range(len(sorted_target_boxes)):\n            if n>0 and np.array_equal(sorted_target_boxes[n],sorted_target_boxes[n-1]):\n                mask[n]=0\n        mask=mask.astype(np.bool)\n        sorted_target_confidences=sorted_target_confidences[mask]\n        sorted_target_boxes=sorted_target_boxes[mask,:]\n        sorted_relative_target_boxes=sorted_relative_target_boxes[mask,:]\n        #print(sorted_target_confidences)\n        label=np.zeros((priors.shape[0]))\n        if len(sorted_target_boxes)>0:\n            corner_form_priors = xywh2xyxy(to_numpy(priors)).astype(np.float32)\n            \n            ious = ds2.iou_of(np.expand_dims(sorted_relative_target_boxes.astype(np.float32), 0), np.expand_dims(corner_form_priors, 1))\n            best_target_per_prior, best_target_per_prior_index = np.max(ious, axis=1), np.argmax(ious, axis=1)\n            best_prior_per_target, best_prior_per_target_index = np.max(ious, axis=0), np.argmax(ious, axis=0)\n            #print(best_prior_per_target_index)\n            #print(sorted_target_confidences)\n            \n            for n,p in enumerate(best_prior_per_target_index):\n                label[p]=sorted_target_confidences[n]\n            #print(label)\n            all_target_confidences.append(label )\n            all_sorted_boxes.append(sorted_target_boxes)\n        else:\n            all_target_confidences.append(label)\n            all_sorted_boxes.append(sorted_target_boxes)\n        \n        \n    return all_sorted_boxes,all_target_confidences","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:20:36.906752Z","iopub.execute_input":"2021-12-15T03:20:36.907243Z","iopub.status.idle":"2021-12-15T03:20:36.927635Z","shell.execute_reply.started":"2021-12-15T03:20:36.907194Z","shell.execute_reply":"2021-12-15T03:20:36.926723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\ndef preprocess_data(training_context):\n    traindata=training_context['train_data']\n    boxes=traindata['boxes']\n    traindata['target_confidences']=boxes[:,:, 4].long().detach()\n    traindata['target_locations']=boxes[:,:,:4].detach()\n\n    \n    \ndef hard_negative_mining(loss,labels, neg_pos_ratio):\n    \"\"\"\n    It used to suppress the presence of a large number of negative prediction.\n    It works on image level not batch level.\n    For any example/image, it keeps all the positive predictions and\n     cut the number of negative predictions to make sure the ratio\n     between the negative examples and positive examples is no more\n     the given ratio for an image.\n\n    Args:\n        loss (N, num_priors): the loss for each example.\n        labels (N, num_priors): the labels.\n        neg_pos_ratio:  the ratio between the negative examples and positive examples.\n    \"\"\"\n    pos_mask = labels > 0\n    #print('loss',loss.shape,loss)\n\n    num_pos = pos_mask.long().sum(dim=-1, keepdim=True)\n    #print('num_pos',num_pos.shape,num_pos)\n    #print('num_pos',num_pos)\n    #避免此圖片無正案例\n    num_neg = clip(num_pos,min=1) * neg_pos_ratio\n    #print('num_neg',num_neg)\n\n    \n    loss[pos_mask] = -math.inf\n    _, indexes = loss.sort(dim=-1,descending=True)\n    _, orders = indexes.sort(dim=-1)\n    #print('orders',orders.shape)\n    neg_mask = orders < num_neg\n\n    return pos_mask | neg_mask\n\n\ndef jaccard(box_a, box_b):\n    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes.\n    E.g.:\n        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n    \"\"\"\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n    union = area_a + area_b - inter\n    return inter,union \n\n\ndef area_of(left_top, right_bottom):\n    \"\"\"Compute the areas of rectangles given two corners.\n\n    Args:\n        left_top (N, 2): left top corner.\n        right_bottom (N, 2): right bottom corner.\n\n    Returns:\n        area (N): return the area.\n    \"\"\"\n    hw = clip(right_bottom - left_top, 0.0, None)\n    return hw[..., 0] * hw[..., 1]\n\ndef iou_of(boxes0, boxes1, eps=1e-5):\n    \"\"\"Return intersection-over-union (Jaccard index) of boxes.\n\n    Args:\n        boxes0 (N, 4): ground truth boxes.\n        boxes1 (N or 1, 4): predicted boxes.\n        eps: a small number to avoid 0 as denominator.\n    Returns:\n        iou (N): IoU values.\n    \"\"\"\n    overlap_left_top = maximum(boxes0[..., :2], boxes1[..., :2])\n    overlap_right_bottom = minimum(boxes0[..., 2:], boxes1[..., 2:])\n\n    overlap_area =area_of(overlap_left_top, overlap_right_bottom)\n    area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n    area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n    return overlap_area / (area0 + area1 - overlap_area + eps),overlap_area ,area0 + area1 - overlap_area\n\ndef hard_nms(box_scores, nms_threshold, top_k=-1, candidate_size=200):\n    \"\"\"\n\n    Args:\n        box_scores (N, 5): boxes in corner-form and probabilities.\n        nms_threshold: intersection over union threshold.\n        top_k: keep top_k results. If k <= 0, keep all the results.\n        candidate_size: only consider the candidates with the highest scores.\n    Returns:\n         picked: a list of indexes of the kept boxes\n    \"\"\"\n    if box_scores is None or len(box_scores) == 0:\n        return None, None\n    all_scores = box_scores[:, -1]\n    all_class = box_scores[:, 4]\n    all_boxes = box_scores[:, :4]\n    all_index =np.arange(len(box_scores))\n    all_picked = []\n    for clas in [1,2,3]:\n        scores=all_scores[all_class==clas]\n        boxes=all_boxes[all_class==clas,:]\n        this_index=all_index[all_class==clas]\n        \n        picked = []\n        #_, indexes = scores.sort(descending=True)\n        indexes = np.argsort(-scores)\n        indexes = indexes[:candidate_size]\n        #indexes = indexes[-candidate_size:]\n        while len(indexes) > 0:\n            # current = indexes[0]\n            current = indexes[-1]\n            picked.append(current)\n            if 0 < top_k == len(picked) or len(indexes) == 1:\n                break\n            current_box = boxes[current, :]\n            # indexes = indexes[1:]\n            indexes = indexes[:-1]\n            rest_boxes = boxes[indexes, :]\n            iou, inter, union = iou_of(rest_boxes, expand_dims(current_box, axis=0), )\n            iot = inter / area_of(current_box[..., :2], current_box[..., 2:])\n\n            #indexes = indexes[((iou <= nms_threshold) * (iot <= nms_threshold)).astype(np.bool)]\n            indexes = indexes[(iou <= nms_threshold).astype(np.bool)]\n            all_picked.extend(this_index[picked])\n\n    return box_scores[all_picked, :], all_picked\n\ndef soft_nms(box_scores, nms_threshold, top_k=-1, candidate_size=200, sigma=0.5):\n    \"\"\"\n\n    Args:\n        box_scores (N, 5): boxes in corner-form and probabilities.\n        nms_threshold: intersection over union threshold.\n        top_k: keep top_k results. If k <= 0, keep all the results.\n        candidate_size: only consider the candidates with the highest scores.\n    Returns:\n         picked: a list of indexes of the kept boxes\n    \"\"\"\n    picked_box_scores = []\n    while box_scores.size(0) > 0:\n        max_score_index = torch.argmax(box_scores[:, 4])\n        cur_box_prob = torch.tensor(box_scores[max_score_index, :])\n        picked_box_scores.append(cur_box_prob)\n        if len(picked_box_scores) == top_k > 0 or box_scores.size(0) == 1:\n            break\n        cur_box = cur_box_prob[:-1]\n        box_scores[max_score_index, :] = box_scores[-1, :]\n        box_scores = box_scores[:-1, :]\n        ious = iou_of(cur_box.unsqueeze(0), box_scores[:, :-1])\n        box_scores[:, -1] = box_scores[:, -1] * torch.exp(-(ious * ious) / sigma)\n        box_scores = box_scores[box_scores[:, -1] > score_threshold, :]\n    if len(picked_box_scores) > 0:\n        return torch.stack(picked_box_scores)\n    else:\n        return torch.tensor([])\n\n\n\n\nclass MultiBoxLoss(nn.Module):\n    def __init__(self, priors, neg_pos_ratio=3.5, center_variance=0.1, size_variance=0.2):\n        \"\"\"Implement SSD Multibox Loss.\n\n        Basically, Multibox loss combines classification loss\n         and Smooth L1 regression loss.\n        \"\"\"\n        super(MultiBoxLoss, self).__init__()\n        self.neg_pos_ratio = neg_pos_ratio\n        self.center_variance = center_variance\n        self.size_variance = size_variance\n        self.priors = to_tensor(priors)\n        self.ce=CrossEntropyLoss(axis=-1,reduction='mean')\n\n    def forward(self, confidences, locations, target_confidences, target_locations):\n        \"\"\"Compute classification loss and smooth l1 loss.\n\n        Args:\n            confidence (batch_size, num_priors, num_classes): class predictions.\n            locations (batch_size, num_priors, 4): predicted locations.\n            target_confidence (batch_size, num_priors): real labels of all the priors.\n            target_locations (batch_size, num_priors, 4): real boxes corresponding all the priors.\n        \"\"\"\n        \n        num_classes = 2\n        batch= confidences.size(0)\n        boxes =locations[...,:4]\n        \n        \n        target_boxes =target_locations[...,:4]\n        #target_boxes = decode(target_boxes,self.priors, (self.center_variance,self.size_variance))#*to_tensor([[640,480,640,480]])\n       \n        regression_loss=0\n        classification_loss=0\n        for i in range(batch):\n            this_target_confidences=target_confidences[i,:]\n            this_confidences=confidences[i,:,:]\n            this_boxes=boxes[i,:,:]\n            this_target_boxes=target_boxes[i,:,:]\n            \n            pos_mask = this_target_confidences > 0\n            masked_boxes =this_boxes[pos_mask, :]#.view((-1,4))\n            masked_target_boxes =this_target_boxes[pos_mask, :].detach()\n            \n            pos_masked_confidences =this_confidences[pos_mask, :].reshape(-1, num_classes)\n            pos_masked_target_confidences =this_target_confidences[pos_mask].reshape(-1).detach()\n            if len(masked_target_boxes):\n                #print('masked_boxes:',masked_boxes.shape,'masked_target_boxes:',masked_target_boxes.shape)\n                #print('pos_masked_confidences:',pos_masked_confidences.shape,'pos_masked_target_confidences:',pos_masked_target_confidences.shape)\n                regression_loss=regression_loss+8*F.smooth_l1_loss(masked_boxes, masked_target_boxes.detach())#+AdaptiveWingLoss()(masked_boxes, masked_target_boxes)\n                #classification_loss=classification_loss+0.2*self.ce(pos_masked_confidences,pos_masked_target_confidences.detach())\n                \n        with torch.no_grad():\n            #已經取log_softmax了不需要再做一次\n            #loss=-confidences[:,:,0]#.sum(-1)\n            loss = -this_confidences[:,0]\n            mask = hard_negative_mining(loss, this_target_confidences, self.neg_pos_ratio)\n        masked_confidences =this_confidences[mask, :]#.reshape(-1, num_classes)\n        masked_target_confidences = this_target_confidences[mask]#.reshape(-1).detach()\n        classification_loss = classification_loss+self.ce(masked_confidences,masked_target_confidences.detach())\n\n\n           \n        return (classification_loss+regression_loss)/batch\n    \n\n@torch.no_grad()\ndef classification_recall(confidences,target_locations,target_confidences):\n    all_sorted_boxes,all_target_confidences=get_actual_target(target_locations,target_confidences,rfbmodel.model.priors,center_variance=0.1, size_variance=0.2)\n    \n    \n    confidences=to_numpy(exp(confidences))\n    target_confidences=np.stack(all_target_confidences,0)#to_numpy(target_confidences)\n    mask=target_confidences>0\n    #mask=((np.argmax(confidences[:,:,:],-1)>0)*(confidences[:,:,1:].max(-1)>=0.5)).astype(np.bool)\n    #print('np.argmax(confidences[:,:,1:],-1)+1',(np.argmax(confidences[:,:,1:],-1)+1).shape)\n    #print('confidences[:,:,0]<0.6',(confidences[:,:,0]<0.6).shape)\n    \n\n    pred_confidences=confidences[:,:,1]\n    least_confidences=confidences.max(-1)\n    \n    \n    pred_confidences[least_confidences>=0.5]=1\n    \n    masked_confidences=pred_confidences[mask]\n    masked_target_confidences=target_confidences[mask]\n    return equal(masked_confidences,masked_target_confidences).mean()\n\n@torch.no_grad()\ndef classification_accuracy(confidences,target_locations,target_confidences):\n    all_sorted_boxes,all_target_confidences=get_actual_target(target_locations,target_confidences,rfbmodel.model.priors,center_variance=0.1, size_variance=0.2)\n    \n    \n    confidences=to_numpy(exp(confidences))\n    target_confidences=to_numpy(target_confidences)\n    #mask=np.argmax(confidences[:,:,:],-1)>0\n    mask=np.argmax(confidences,-1)>0\n\n    masked_confidences=np.argmax(confidences,-1)[mask]\n    masked_target_confidences=target_confidences[mask]\n    if len(masked_confidences)>0:\n        return equal(masked_confidences,masked_target_confidences).mean()\n    else:\n        return 0\n\n\n@torch.no_grad()\ndef bbox_rmse(locations,target_locations,target_confidences):\n    bboxes=locations[:,:,:4].copy().detach()\n    target_bboxes=target_locations[:,:,:4].copy().detach()\n    bboxes = clip(decode(bboxes, rfbmodel.model.priors, (0.1,0.2)),min=0,max=1)\n    target_bboxes = decode(target_bboxes, rfbmodel.model.priors, (0.1,0.2))\n    \n    \n    \n    mask=target_confidences>0\n    masked_bboxes=bboxes[mask,:]\n    masked_target_bboxes=target_bboxes[mask,:]\n   \n    if len(masked_target_bboxes)>0:\n        return ((masked_bboxes-masked_target_bboxes)**2).mean().sqrt()\n    else:\n        return 0\n\n    \n    \n    \ndef bbox_iou(bboxes1, bboxes2):\n    \"\"\"\n\n    Args:\n        bboxes1 (Tensor): shape (n, 4)\n        bboxes2 (Tensor): shape (k, 4)\n\n    Returns:\n         ious(Tensor): shape (n, k)\n\n    Examples;\n    >>> boxes1=to_tensor(np.array([[39, 63, 203, 112], [49, 75, 203, 125],[31, 69, 201, 125],[50, 72, 197, 121],[35, 51, 196, 110]]))\n    >>> boxes2=to_tensor(np.array([[54, 66, 198, 114], [42, 78, 186, 126], [18, 63, 235, 135],[54, 72, 198, 120],[36, 60, 180, 108]]))\n    >>> iou_loss=(1-bbox_iou(boxes1,boxes2)).sum()/(boxes1.shape[0]*boxes2.shape[0])\n    >>> print(iou_loss.cpu())\n    tensor(0.3802)\n\n    >>> boxes1=to_tensor(np.array([[39, 63, 203, 112], [49, 75, 203, 125],[31, 69, 201, 125],[50, 72, 197, 121],[35, 51, 196, 110]]))\n    >>> boxes2=to_tensor(np.array([[54, 66, 198, 114], [42, 78, 186, 126], [18, 63, 235, 135],[54, 72, 198, 120]]))\n    >>> iou_loss=(1-bbox_iou(boxes1,boxes2)).sum()/(boxes1.shape[0]*boxes2.shape[0])\n    >>> print(iou_loss.cpu())\n    tensor(0.3703)\n\n\n\n\n    \"\"\"\n    rows = bboxes1.shape[0]\n    cols = bboxes2.shape[0]\n    ious = torch.zeros((rows, cols))\n    if rows * cols == 0:\n        return ious\n    exchange = False\n    if bboxes1.shape[0] > bboxes2.shape[0]:\n        bboxes1, bboxes2 = bboxes2, bboxes1\n        ious = torch.zeros((cols, rows))\n        exchange = True\n    area1 = (bboxes1[:, 2] - bboxes1[:, 0]) * (bboxes1[:, 3] - bboxes1[:, 1])\n    area2 = (bboxes2[:, 2] - bboxes2[:, 0]) * (bboxes2[:, 3] - bboxes2[:, 1])\n\n    lt = maximum(bboxes1[:, None, :2], bboxes2[:, :2])  # [N,M,2]\n    rb = minimum(bboxes1[:, None, 2:], bboxes2[:, 2:])  # [N,M,2]\n    wh = torch.clamp(rb - lt, min=0)  # [N,M,2]\n    inter_area = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n    #print('inter_area',inter_area)\n\n\n    union =  area1[:, None] + area2 -inter_area\n    #print('union',union)\n    ious = inter_area / union\n    ious = torch.clamp(ious,min=0,max = 1.0)\n    if exchange:\n        ious = ious.T\n    #print('ious',ious)\n    return reduce_max(ious,0).mean()\n\n@torch.no_grad()\ndef iou(confidences, locations, target_confidences, target_locations):\n    iou=0\n    confidences=to_numpy(exp(confidences))\n    target_confidences=to_numpy(target_confidences)\n    boxes1 =locations[:,:,:4]\n    boxes1 = clip(to_numpy(decode(boxes1, rfbmodel.model.priors, (0.1,0.2))*to_tensor([[640,480,640,480]])),min=0)\n\n    \n    boxes2 =target_locations[:,:,:4]\n    boxes2 =  to_numpy(decode(boxes2, rfbmodel.model.priors, (0.1,0.2))*to_tensor([[640,480,640,480]]))\n    all_sorted_boxes,all_target_confidences=get_actual_target(target_locations[:,:,:4],target_confidences,rfbmodel.model.priors,center_variance=0.1, size_variance=0.2)\n    \n    num_batch=0\n    for i in range(len(locations)):\n        #mask1=((np.argmax(confidences[i][:,:],-1)>0)*(confidences[i][:,1:].max()>0.5)).astype(np.bool)\n        mask1=confidences[i][:,1]>0.35\n        mask2=target_confidences[i,:]>0\n        #預測框遮罩包括應有框位置(mask2)以及預測出框(mask1)兩者的聯集\n   \n        this_boxes1=boxes1[i,:,:][mask1,:]\n     \n        this_boxes1_conf=np.expand_dims(np.argmax(confidences[i],-1)[mask1],-1)\n        this_boxes1_score=np.expand_dims(np.max(confidences[i],-1)[mask1],-1)\n        box_probs = np.concatenate([this_boxes1.astype(np.float32),this_boxes1_conf.astype(np.float32),this_boxes1_score.astype(np.float32)], axis=1)\n        \n        if len(box_probs) > 1:\n            box_probs,keep = hard_nms(box_probs, 0.5, top_k=-1, )\n            this_boxes1=this_boxes1[keep,:]\n            this_boxes1_conf=this_boxes1_conf[keep,:]\n            #print(this_boxes1)\n    \n    \n        #this_boxes2=boxes2[i,:,:][mask2,:]\n        this_boxes2=all_sorted_boxes[i]\n        #print(this_boxes2)\n        \n        if this_boxes2 is not None and len(this_boxes1)>0 and len(this_boxes2)>0:\n            iou_results=to_numpy(bbox_iou(to_tensor(this_boxes1),to_tensor(this_boxes2)))\n            #print(iou_results.mean())\n            iou=iou+iou_results\n            num_batch+=1\n        elif  len(this_boxes2)>0:\n            num_batch+=1\n    return iou/num_batch","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:20:36.929485Z","iopub.execute_input":"2021-12-15T03:20:36.929872Z","iopub.status.idle":"2021-12-15T03:20:37.166609Z","shell.execute_reply.started":"2021-12-15T03:20:36.929819Z","shell.execute_reply":"2021-12-15T03:20:37.165891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image, ImageDraw\n\ndef draw_detection(training_context):\n\n    traindata=training_context['train_data']\n    #print(traindata.keys())\n    imagedata=traindata['images'].copy()\n    labeldata=traindata['locations'].copy()\n    #print(softmax(traindata['confidences'].float(),-1))\n    #基於訓練階段輸出繪圖，信心水準應該要取exp\n    confidences =to_numpy(exp(traindata['confidences'].copy().float()))\n\n    boxes =to_tensor(labeldata[:,:,:4])\n\n    gc.collect()\n    #基於訓練階段輸出繪圖，boxes應該要decode\n    locations = decode(boxes, rfbmodel.model.priors, (0.1,0.2))*to_tensor([[640,480,640,480]])\n    for i in range(6):\n        pillow_img=array2image(reverse_image_backend_adaption(imagedata[i]*127.5+127.5))\n        draw = ImageDraw.Draw(pillow_img)\n        mask=confidences[i][:,1]>0.35\n\n        this_locations = to_numpy(locations[i])[mask, :]\n        this_locations=np.stack([np.clip(this_locations[:,0],0,640),np.clip(this_locations[:,1],0,480),np.clip(this_locations[:,2],0,640),np.clip(this_locations[:,3],0,480)],axis=-1)\n        this_confidences=confidences[i][mask,1]\n   \n        \n        #print(this_locations.shape)\n        if len(this_locations)>0:\n            print('this_locations',this_locations.shape)\n            print('this_confidences',this_confidences.shape)\n            print(reduce_max(confidences[i][:,1]))\n            \n#             area = area_of(this_locations[..., :2], this_locations[..., 2:])\n#             area_mask=area>=1\n#             this_locations=this_locations[area_mask,:]\n#             this_confidences=this_confidences[area_mask,:]\n    \n\n            if len(this_confidences)>0:\n                \n                \n                print('this_locations',this_locations.shape,this_locations)\n                print('this_confidences',this_confidences.shape,this_confidences)\n                box_probs=np.concatenate([this_locations.astype(np.float32),np.expand_dims(this_confidences,-1).astype(np.float32)], axis=-1)\n                #print(box_probs.shape)\n                if len(boxes) > 1:\n                    box_probs, keep = hard_nms(box_probs, 0.6, top_k=-1, )\n                    this_locations=this_locations[keep,:]\n\n                    this_confidences=this_confidences[keep]\n         \n                if len(this_locations)<=200:\n    \n                    for box,conf in zip(this_locations,this_confidences):\n                        #print('box',[int(v) for v in box],'conf',['{0:.3f}'.format(v) for v in conf])\n                        #print(box)\n\n                        pillow_img =plot_bbox([int(v) for v in box], pillow_img, palette[2] ,'Helmet', line_thickness=2)\n                        \n                else:\n                    print(this_confidences.shape)\n                    print(this_confidences[:10])\n                    print(reduce_max(confidences[i][:,1]))\n                    print(len(this_locations))\n                display.display(pillow_img)\n       \n        else:\n            print(confidences[i].shape)\n            print(reduce_max(confidences[i][:,1]))\n            display.display(pillow_img)\n            #pillow_img.save('data_provider_next_{0}.jpg'.format(get_time_suffix()))","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:20:37.168041Z","iopub.execute_input":"2021-12-15T03:20:37.168424Z","iopub.status.idle":"2021-12-15T03:20:37.185463Z","shell.execute_reply.started":"2021-12-15T03:20:37.168385Z","shell.execute_reply":"2021-12-15T03:20:37.184744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"is_resume=True\nif is_resume and os.path.exists('./Models/nfl_ssd.pth'):\n    rfbmodel.load_model('./Models/nfl_ssd.pth')\n    print('./Models/nfl_ssd.pth loaded')\nelse:\n    if os.path.exists('../input/nfl-health-safety-model/Models/nfl_ssd.pth'):\n        rfbmodel.load_model('../input/nfl-health-safety-model/Models/nfl_ssd.pth')\n        print('../input/nfl-health-safety-model/Models/nfl_ssd.pthh loaded')\n        \n        \nrfbmodel.with_optimizer(optimizer=AdaBelief,lr=1e-3,betas=(0.9, 0.999))\\\n.with_loss(MultiBoxLoss(rfbmodel.model.priors,neg_pos_ratio=2.5))\\\n.with_metric(classification_accuracy, name='accuracy',print_only=True)\\\n.with_metric(classification_recall, name='recall',print_only=True)\\\n.with_metric(bbox_rmse,print_only=True)\\\n.with_metric(iou)\\\n.with_regularizer('l2',reg_weight=1e-6)\\\n.trigger_when(when='on_loss_calculation_start',frequency=1,unit='batch',action=preprocess_data)\\\n.trigger_when(when='on_batch_end',frequency=50,unit='batch',action=draw_detection)\\\n.with_model_save_path('./Models/nfl_ssd.pth')\\\n.with_learning_rate_scheduler(PolyLR(max_lr=1e-3,max_iter=3000))\\\n.with_automatic_mixed_precision_training()\\","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:20:37.188541Z","iopub.execute_input":"2021-12-15T03:20:37.188774Z","iopub.status.idle":"2021-12-15T03:20:37.457638Z","shell.execute_reply.started":"2021-12-15T03:20:37.188747Z","shell.execute_reply":"2021-12-15T03:20:37.456923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plan=TrainingPlan()\\\n    .add_training_item(rfbmodel,name='rfbnet')\\\n    .with_data_loader(data_provider)\\\n    .with_batch_size(32)\\\n    .repeat_epochs(10)\\\n    .print_progress_scheduling(10,unit='batch')\\\n    .display_loss_metric_curve_scheduling(200)\\\n    .print_gradients_scheduling(100) \\\n    .save_model_scheduling(20,unit='batch')\\\n    .start_now()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:20:37.458887Z","iopub.execute_input":"2021-12-15T03:20:37.459112Z"},"trusted":true},"execution_count":null,"outputs":[]}]}