{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%matplotlib inline\n#這是jupyter notebook的magic word˙\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom IPython import display","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-14T11:11:40.871235Z","iopub.execute_input":"2021-12-14T11:11:40.871632Z","iopub.status.idle":"2021-12-14T11:11:40.887504Z","shell.execute_reply.started":"2021-12-14T11:11:40.871563Z","shell.execute_reply":"2021-12-14T11:11:40.886255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n#判斷是否在jupyter notebook上\ndef is_in_ipython():\n    \"Is the code running in the ipython environment (jupyter including)\"\n    program_name = os.path.basename(os.getenv('_', ''))\n\n    if ('jupyter-notebook' in program_name or # jupyter-notebook\n        'ipython'          in program_name or # ipython\n        'jupyter' in program_name or  # jupyter\n        'JPY_PARENT_PID'   in os.environ):    # ipython-notebook\n        return True\n    else:\n        return False\n\n\n#判斷是否在colab上\ndef is_in_colab():\n    if not is_in_ipython(): return False\n    try:\n        from google import colab\n        return True\n    except: return False\n\n#判斷是否在kaggke_kernal上\ndef is_in_kaggle_kernal():\n    if 'kaggle' in os.environ['PYTHONPATH']:\n        return True\n    else:\n        return False\n\nif is_in_colab():\n    from google.colab import drive\n    drive.mount('/content/gdrive')\n\nos.environ['TRIDENT_BACKEND'] = 'pytorch'\nkaggle_kernal=None\nif is_in_kaggle_kernal():\n    os.environ['TRIDENT_HOME'] = './trident'\n    \nelif is_in_colab():\n    os.environ['TRIDENT_HOME'] = '/content/gdrive/My Drive/trident'\n  \n","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:11:40.893871Z","iopub.execute_input":"2021-12-14T11:11:40.894356Z","iopub.status.idle":"2021-12-14T11:11:40.907261Z","shell.execute_reply.started":"2021-12-14T11:11:40.894316Z","shell.execute_reply":"2021-12-14T11:11:40.905912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#為確保安裝最新版 \n\n!pip uninstall tridentx -y\n!pip install ../input/trident/tridentx-0.7.4-py3-none-any.whl --upgrade\n\nimport re\nimport pandas\nimport json\nimport copy\n\nimport numpy as np\n#調用trident api\nimport subprocess\nimport random\nfrom tqdm import tqdm\nimport scipy\nimport time\nimport glob\nimport pandas as pd\nfrom shutil import copyfile","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:11:40.909351Z","iopub.execute_input":"2021-12-14T11:11:40.909949Z","iopub.status.idle":"2021-12-14T11:11:53.315818Z","shell.execute_reply.started":"2021-12-14T11:11:40.909898Z","shell.execute_reply":"2021-12-14T11:11:53.314623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import trident as T\nfrom trident import *\n\npalette = [(255, 192, 0), (0, 192, 255), (128, 0, 255), (192,255,64), (255, 0, 128), (64,255,128)]","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:11:53.31966Z","iopub.execute_input":"2021-12-14T11:11:53.320291Z","iopub.status.idle":"2021-12-14T11:11:57.159798Z","shell.execute_reply.started":"2021-12-14T11:11:53.320131Z","shell.execute_reply":"2021-12-14T11:11:57.158584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read in data files\nBASE_DIR = '../input/nfl-health-and-safety-helmet-assignment'\n\n# 標籤與提交範例\ntrain_labels = pd.read_csv(f'{BASE_DIR}/train_labels.csv')\nss = pd.read_csv(f'{BASE_DIR}/sample_submission.csv')\n\n# 球員追蹤數據\ntrain_player_tracking = pd.read_csv(f'{BASE_DIR}/train_player_tracking.csv')\ntest_player_tracking = pd.read_csv(f'{BASE_DIR}/test_player_tracking.csv')\n\n# Baseline helmet detection labels\ntrain_baseline_helmets = pd.read_csv(f'{BASE_DIR}/train_baseline_helmets.csv')\ntest_baseline_helmets = pd.read_csv(f'{BASE_DIR}/test_baseline_helmets.csv')\n\n# 額外的圖像標註\nimg_labels = pd.read_csv(f'{BASE_DIR}/image_labels.csv')\n\n\n\n\n\nimg_labels","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:11:57.161864Z","iopub.execute_input":"2021-12-14T11:11:57.163166Z","iopub.status.idle":"2021-12-14T11:12:01.470705Z","shell.execute_reply.started":"2021-12-14T11:11:57.163098Z","shell.execute_reply":"2021-12-14T11:12:01.469414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extra_images=glob.glob('../input/nfl-health-and-safety-helmet-assignment/images/*.jpg')\nprint(len(extra_images))\n\nimg_labels_frequency=img_labels['label'].value_counts()\nprint(img_labels_frequency)\n\nlabel2index = {\n    'Helmet': 0,\n    'Helmet-Blurred': 1,\n    'Helmet-Difficult': 2,\n    'Helmet-Sideline': 3,\n    'Helmet-Partial': 4\n}\n\nfolder,filename,ext=split_path(extra_images[0])\nsample_data=img_labels[img_labels['image']==filename+ext]\nsample_data\n","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:12:01.4727Z","iopub.execute_input":"2021-12-14T11:12:01.472992Z","iopub.status.idle":"2021-12-14T11:12:01.592698Z","shell.execute_reply.started":"2021-12-14T11:12:01.472958Z","shell.execute_reply":"2021-12-14T11:12:01.591696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img=image2array(extra_images[0])\npillow_img=array2image(img)\n\nfor index, item in sample_data.iterrows():\n    box=[int(item['left']),int(item['top']),int(item['left'])+int(item['width']),int(item['top'])+int(item['height'])] \n    pillow_img =plot_bbox(box, pillow_img, palette[label2index[item['label']]],item['label'], line_thickness=2)\n\ndisplay.display(pillow_img)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:12:01.594127Z","iopub.execute_input":"2021-12-14T11:12:01.5944Z","iopub.status.idle":"2021-12-14T11:12:02.216979Z","shell.execute_reply.started":"2021-12-14T11:12:01.594365Z","shell.execute_reply":"2021-12-14T11:12:02.216179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#額外圖片的數據\nbboxes_dataset_dict=OrderedDict()\nfor img_path in tqdm(extra_images):\n    folder,filename,ext=split_path(img_path)\n    sub_data=img_labels[img_labels['image']==filename+ext]\n    bboxes_dataset_dict[filename+ext]=[]\n\n    for index, item in sub_data.iterrows():\n        box=[int(item['left']),int(item['top']),int(item['left'])+int(item['width']),int(item['top'])+int(item['height'])]\n        bboxes_dataset_dict[filename+ext].append(box)\n    bboxes_dataset_dict[filename+ext]=np.array(bboxes_dataset_dict[filename+ext])\n        \nprint('bboxes_dataset_dict:',len(bboxes_dataset_dict))\npickle_it('./bboxes_dataset_dict.pkl',bboxes_dataset_dict)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:12:02.218397Z","iopub.execute_input":"2021-12-14T11:12:02.219697Z","iopub.status.idle":"2021-12-14T11:17:26.684011Z","shell.execute_reply.started":"2021-12-14T11:12:02.219614Z","shell.execute_reply":"2021-12-14T11:17:26.682678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import animation, rc\nfrom IPython.display import Video, display\n\ndef display_video(video_path, ratio=0.5):\n    return Video(f\"{video_path}\",\n                  embed=True,\n                  height = int(720 * ratio),\n                  width = int(1280 * ratio))\ntrain_videos=glob.glob('../input/nfl-health-and-safety-helmet-assignment/train/*.mp4')\n\ndisplay_video( train_videos[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:17:26.686401Z","iopub.execute_input":"2021-12-14T11:17:26.686765Z","iopub.status.idle":"2021-12-14T11:17:26.945149Z","shell.execute_reply.started":"2021-12-14T11:17:26.68672Z","shell.execute_reply":"2021-12-14T11:17:26.944306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_baseline_helmets[\"video\"] = train_baseline_helmets[\"video_frame\"].str.split(\"_\").str[:3].str.join(\"_\")\ntrain_labels[\"video\"] = train_labels[\"video_frame\"].str.split(\"_\").str[:3].str.join(\"_\")\n\ntrain_baseline_helmets[\"frame\"] = train_baseline_helmets[\"video_frame\"].str.split(\"_\").str[-1].astype(\"int\")\ntrain_labels[\"frame\"] = train_labels[\"video_frame\"].str.split(\"_\").str[-1].astype(\"int\")","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:17:26.946487Z","iopub.execute_input":"2021-12-14T11:17:26.946753Z","iopub.status.idle":"2021-12-14T11:17:46.57554Z","shell.execute_reply.started":"2021-12-14T11:17:26.94671Z","shell.execute_reply":"2021-12-14T11:17:46.57475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import IPython\nVIDEO_CODEC = \"mp4v\"\n\nvideo_bboxes_dataset_dict=OrderedDict()\n\nmake_dir_if_need('./videos')\nfor video_path in tqdm(train_videos):\n    folder,filename,ext=split_path(video_path)\n    make_dir_if_need('./videos/{0}'.format(filename))\n    video_name = os.path.basename(video_path).replace(\".mp4\", \"\")\n    videocap = cv2.VideoCapture(video_path)\n    fps = videocap.get(cv2.CAP_PROP_FPS)\n    width = int(videocap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(videocap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    frame = 0\n    while True:\n        it_worked, img = videocap.read()\n        if not it_worked:\n            break\n        # We need to add 1 to the frame count to match the label frame index\n        # that starts at 1\n        frame += 1\n        if random.random()<=0.035:\n\n        # Let's add a frame index to the video so we can track where we are\n      \n\n#         # Now, add the boxes\n#         boxes = train_baseline_helmets.query(\"video == @video_name and frame == @frame\")\n#         if len(boxes) == 0:\n#             print(\"Boxes incorrect\")\n#             return \n        \n  \n#         bboxes_dataset_dict[filename+ext]=[]\n#         for box in boxes.itertuples(index=False):\n#             bboxes_dataset_dict[filename+ext].append([box.left, box.top,box.left+box.width, box.top+box.height])\n   \n    \n\n            boxes = train_labels.query(\"video == @video_name and frame == @frame\")\n            if len(boxes) == 0:\n                pass\n            else:\n                print('./videos/{0}/{1}_{2}.png'.format(filename,filename,frame),'boxes:',len(boxes))\n                array2image(img).save('./videos/{0}/{1}_{2}.png'.format(filename,filename,frame))\n                video_bboxes_dataset_dict['{0}_{1}.png'.format(filename,frame)]=[]\n                for box in boxes.itertuples(index=False):\n                    video_bboxes_dataset_dict['{0}_{1}.png'.format(filename,frame)].append([box.left, box.top,box.left+box.width, box.top+box.height])\n                video_bboxes_dataset_dict['{0}_{1}.png'.format(filename,frame)]=np.array(video_bboxes_dataset_dict['{0}_{1}.png'.format(filename,frame)])\nIPython.display.clear_output(wait=True)\nprint('video_bboxes_dataset_dict:',len(video_bboxes_dataset_dict))\npickle_it('./video_bboxes_dataset_dict.pkl',video_bboxes_dataset_dict)  \n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:17:46.57714Z","iopub.execute_input":"2021-12-14T11:17:46.578076Z","iopub.status.idle":"2021-12-14T11:36:29.849388Z","shell.execute_reply.started":"2021-12-14T11:17:46.57803Z","shell.execute_reply":"2021-12-14T11:36:29.848294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython import display\nk=video_bboxes_dataset_dict.key_list[0]\n\nprint(glob.glob('./videos/*/'+k)[0])\nimg=image2array(glob.glob('./videos/*/'+k)[0])\npillow_img=array2image(img)\n\nfor box in video_bboxes_dataset_dict[k]:\n    pillow_img =plot_bbox(box, pillow_img, palette[0] ,'Helmet', line_thickness=2)\n\ndisplay.display(pillow_img)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:36:29.851313Z","iopub.execute_input":"2021-12-14T11:36:29.851763Z","iopub.status.idle":"2021-12-14T11:36:30.541838Z","shell.execute_reply.started":"2021-12-14T11:36:29.851719Z","shell.execute_reply":"2021-12-14T11:36:30.540708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def video_with_baseline_boxes(video_path, baseline_boxes, gt_labels, verbose=True):\n    \"\"\"\n    Annotates a video with both the baseline model boxes and ground truth boxes.\n    Baseline model prediction confidence is also displayed.\n    \"\"\"\n    VIDEO_CODEC = \"mp4v\"\n    HELMET_COLOR = (255, 0, 0) # Blue \n    BASELINE_COLOR = (0, 255, 0) # Green\n    IMPACT_COLOR = (0, 0, 255) # Red\n    video_name = os.path.basename(video_path).replace(\".mp4\", \"\")\n    \n    if verbose:\n        print(f\"Running for {video_name}\")\n    baseline_boxes = baseline_boxes.copy()\n    gt_labels = gt_labels.copy()\n\n    baseline_boxes[\"video\"] = baseline_boxes[\"video_frame\"].str.split(\"_\").str[:3].str.join(\"_\")\n    gt_labels[\"video\"] = gt_labels[\"video_frame\"].str.split(\"_\").str[:3].str.join(\"_\")\n\n    baseline_boxes[\"frame\"] = baseline_boxes[\"video_frame\"].str.split(\"_\").str[-1].astype(\"int\")\n    gt_labels[\"frame\"] = gt_labels[\"video_frame\"].str.split(\"_\").str[-1].astype(\"int\")\n\n    videocap = cv2.VideoCapture(video_path)\n    fps = videocap.get(cv2.CAP_PROP_FPS)\n    width = int(videocap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(videocap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    output_path = f\"labelled_{video_name}.mp4\"\n    tmp_output_path = f\"tmp_\" + output_path\n    output_video = cv2.VideoWriter(\n                        tmp_output_path,\n                        cv2.VideoWriter_fourcc(*VIDEO_CODEC),\n                        fps,\n                        (width, height)\n                    )\n    frame = 0\n    while True:\n        it_worked, img = videocap.read()\n        if not it_worked:\n            break\n        # We need to add 1 to the frame count to match the label frame index\n        # that starts at 1\n        frame += 1\n\n        # Let's add a frame index to the video so we can track where we are\n        img_name = f\"{video_name}_frame{frame}\"\n        cv2.putText(img, img_name, (0, 50), cv2.FONT_HERSHEY_SIMPLEX,\n                    1.0, HELMET_COLOR, thickness=2)\n\n        # Now, add the boxes\n        boxes = baseline_boxes.query(\"video == @video_name and frame == @frame\")\n        if len(boxes) == 0:\n            print(\"Boxes incorrect\")\n            return \n        for box in boxes.itertuples(index=False):\n            cv2.rectangle(img, (box.left, box.top), (box.left+box.width, box.top+box.height),\n                          BASELINE_COLOR, thickness=1)\n            cv2.putText(img, f\"{box.conf:.2f}\", (box.left, max(0, box.top-5)),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, BASELINE_COLOR, 1)\n\n        boxes = gt_labels.query(\"video == @video_name and frame == @frame\")\n        if len(boxes) == 0:\n            print(\"Boxes incorrect\")\n            return \n        for box in boxes.itertuples(index=False):\n            # Filter for definitive head impacts and turn red\n            if box.isDefinitiveImpact == True:\n                color, thickness = IMPACT_COLOR, 3\n            else:\n                color, thickness = HELMET_COLOR, 1\n            cv2.rectangle(img, (box.left, box.top), (box.left+box.width, box.top+box.height),\n                          color, thickness=thickness)\n            cv2.putText(img, box.label, (box.left+1, max(0, box.top-20)),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, thickness=1)\n\n        output_video.write(img)\n    output_video.release()\n    # Not all browsers support the codec, we will re-load the file at tmp_output_path\n    # and convert to a codec that is more readable using ffmpeg\n    if os.path.exists(output_path):\n        os.remove(output_path)\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\",\n        tmp_output_path,\n        \"-crf\",\n        \"18\",\n        \"-preset\",\n        \"veryfast\",\n        \"-vcodec\",\n        \"libx264\",\n        output_path\n    ])\n    os.remove(tmp_output_path)\n\n    return output_path","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:36:30.546302Z","iopub.execute_input":"2021-12-14T11:36:30.546807Z","iopub.status.idle":"2021-12-14T11:36:30.580426Z","shell.execute_reply.started":"2021-12-14T11:36:30.546762Z","shell.execute_reply":"2021-12-14T11:36:30.579065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import animation, rc\nfrom IPython.display import Video, display\noutput_video = video_with_baseline_boxes(train_videos[0],train_baseline_helmets, train_labels)\n\nfrac = 1.0 # scaling factor for display\ndisplay(Video(data=output_video,\n              embed=True,\n              height=int(720*frac),\n              width=int(1280*frac)))","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:36:30.582095Z","iopub.execute_input":"2021-12-14T11:36:30.582352Z","iopub.status.idle":"2021-12-14T11:37:59.238057Z","shell.execute_reply.started":"2021-12-14T11:36:30.582321Z","shell.execute_reply":"2021-12-14T11:37:59.235304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def create_animation(num_frames=10, interval=1000):\n#     fig = plt.figure(figsize=(12, 6))\n#     plt.axis('off')\n    \n#     image_bboxed = get_random_bboxed_image()\n#     image = plt.imshow(image_bboxed)\n    \n#     def animate_func(i):\n#         image_bboxed = get_random_bboxed_image()\n#         image.set_array(image_bboxed)\n#         return [image]\n    \n#     return animation.FuncAnimation(fig,\n#                                    animate_func,\n#                                    frames=num_frames,\n#                                    interval=interval # in ms\n#                                   )\n# rc('animation', html='jshtml')","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:37:59.241985Z","iopub.execute_input":"2021-12-14T11:37:59.242691Z","iopub.status.idle":"2021-12-14T11:37:59.253066Z","shell.execute_reply.started":"2021-12-14T11:37:59.242574Z","shell.execute_reply":"2021-12-14T11:37:59.250852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trident.models import ssd,rfbnet\n\n#pretrainedmodel=rfbnet.RfbNet(pretrianed=True)\n#pretrainedmodel.summary()\n\nrfbmodel=rfbnet.RfbNet(pretrianed=True, num_classes=2, num_regressors=4)\nprint(rfbmodel.model.priors)\nrfbmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:37:59.254747Z","iopub.execute_input":"2021-12-14T11:37:59.255712Z","iopub.status.idle":"2021-12-14T11:38:03.041311Z","shell.execute_reply.started":"2021-12-14T11:37:59.255613Z","shell.execute_reply":"2021-12-14T11:38:03.040256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.set_printoptions(precision=3)\n\ndef point_in_box(point,box):\n    if ndim(box)+1==ndim(point):\n        box=np.expand_dims(box,1)\n    if ndim(point)==ndim(box)==1:\n        x,y=point\n        x1,y1,x2,y2=box\n        if x1<x<x2 and y1<y<y2:\n            return True\n        else:\n            return False\n    elif  ndim(point)==ndim(box)==2:\n        x=point[:,0]\n        y=point[:,1]\n        x1=box[:,0]\n        y1=box[:,1]\n        x2=box[:,2]\n        y2=box[:,3]\n        return np.greater_equal(x,x1)*np.greater_equal(x2,x)*np.greater_equal(y,y1)*np.greater_equal(y2,y)\n    elif  ndim(point)==ndim(box)==3:\n        x=point[:,:,0]\n        y=point[:,:,1]\n        x1=box[:,:,0]\n        y1=box[:,:,1]\n        x2=box[:,:,2]\n        y2=box[:,:,3]\n        return np.greater_equal(x,x1)*np.greater_equal(x2,x)*np.greater_equal(y,y1)*np.greater_equal(y2,y)\n   ","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:38:03.042768Z","iopub.execute_input":"2021-12-14T11:38:03.04358Z","iopub.status.idle":"2021-12-14T11:38:03.057546Z","shell.execute_reply.started":"2021-12-14T11:38:03.043541Z","shell.execute_reply":"2021-12-14T11:38:03.056441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef xywh2xyxy(locations):\n    return np.concatenate([locations[..., :2] - locations[..., 2:] / 2.0, locations[..., :2] + locations[..., 2:] / 2.0],\n                     axis=1)\n\nclass SsdBboxDataset(BboxDataset):\n    def __init__(self, boxes=None, image_size=(480, 640), priors=None, center_variance=0.1, size_variance=0.2,\n                 gt_overlap_tolerance=0.5, object_type=ObjectType.absolute_bbox, class_names=None,\n                 symbol='bbox', name=''):\n        super().__init__(boxes=boxes, image_size=image_size, object_type=object_type, class_names=class_names,\n                         symbol=symbol, name=name)\n        self._element_spec = TensorSpec(shape=TensorShape([None, 21]), name=self.symbol, object_type=self.object_type, is_spatial=True)\n        self.priors = priors\n        self.image_size = image_size\n        self.center_variance = center_variance\n        self.size_variance = size_variance\n        self.label_transform_funcs = []\n        self.gt_overlap_tolerance = gt_overlap_tolerance\n        self.bbox_post_transform_funcs = []\n        print('priors',self.priors.shape,self.priors )\n\n    def __getitem__(self, index: int):\n\n        # 取出xyxy定義boxes或是[]\n        # 為了避免[]清單與np.ndarray計算差異，因此補上np.array([[-1,-1,-1,-1,-1]])替代\n        if self.items[index] is None or self.items[index] == []:\n            return np.array([-1] * 21)\n        else:\n            \n            return self.items[index].astype(np.float32)\n\n    def area_of(self, left_top, right_bottom):\n        \"\"\"Compute the areas of rectangles given two corners.\n\n        Args:\n            left_top (N, 2): left top corner.\n            right_bottom (N, 2): right bottom corner.\n\n        Returns:\n            area (N): return the area.\n        \"\"\"\n        hw = np.clip(right_bottom - left_top, 0.0, None)\n        return hw[..., 0] * hw[..., 1]\n\n    def iou_of(self, boxes0, boxes1, eps=1e-5):\n        \"\"\"Return intersection-over-union (Jaccard index) of boxes.\n\n        Args:\n            boxes0 (N, 4): ground truth boxes.\n            boxes1 (N or 1, 4): predicted boxes.\n            eps: a small number to avoid 0 as denominator.\n        Returns:\n            iou (N): IoU values.\n        \"\"\"\n        overlap_left_top = np.maximum(boxes0[..., :2], boxes1[..., :2])\n        overlap_right_bottom = np.minimum(boxes0[..., 2:], boxes1[..., 2:])\n\n        overlap_area = self.area_of(overlap_left_top, overlap_right_bottom)\n        area0 = self.area_of(boxes0[..., :2], boxes0[..., 2:])\n        area1 = self.area_of(boxes1[..., :2], boxes1[..., 2:])\n        return overlap_area / (area0 + area1 - overlap_area + eps)\n\n    def convert_boxes_to_locations(self, center_form_boxes, center_form_priors):\n        if len(center_form_priors.shape) + 1 == len(center_form_boxes.shape):\n            center_form_priors = np.expand_dims(center_form_priors, 0)\n        return np.concatenate([(center_form_boxes[..., :2] - center_form_priors[..., :2]) / center_form_priors[...,\n                                                                                            2:] / self.center_variance,\n                               np.log(np.clip(center_form_boxes[..., 2:] / center_form_priors[..., 2:], 1e-8, np.inf)) / self.size_variance],\n                              axis=len(center_form_boxes.shape) - 1)\n\n    def assign_priors(self, gt_boxes, gt_labels,center_form_priors, iou_threshold):\n\n        corner_form_priors = xywh2xyxy(center_form_priors)\n        #print('corner_form_priors',corner_form_priors.shape,corner_form_priors)\n        ious = self.iou_of(np.expand_dims(gt_boxes, 0), np.expand_dims(corner_form_priors, 1))\n    \n        # size: num_priors\n        best_target_per_prior, best_target_per_prior_index = np.max(ious, axis=1), np.argmax(ious, axis=1)\n    \n        best_prior_per_target, best_prior_per_target_index = np.max(ious, axis=0), np.argmax(ious, axis=0)\n    \n        \n        for target_index, prior_index in enumerate(best_prior_per_target_index):\n            best_target_per_prior_index[prior_index] = target_index\n        \n        #2.0 is used to make sure every target has a prior assigned\n        best_prior_per_target_index_list = best_prior_per_target_index.tolist()\n        for i in range(best_target_per_prior.shape[0]):\n            if i in best_prior_per_target_index_list:\n                best_target_per_prior[i] = 2\n\n\n       \n        labels= gt_labels[best_target_per_prior_index]\n        labels[best_target_per_prior<2]=0\n    \n        boxes = gt_boxes[best_target_per_prior_index]\n\n\n        return boxes, labels\n\n    def data_transform(self, data):\n        if data is None or len(data) == 0:\n            return np.zeros((self.priors.shape[0], 21)).astype(np.float32)\n        elif isinstance(data, np.ndarray):\n            height, width = self.image_size\n\n            data[:, 0] = np.clip(data[:, 0], 0, width)\n            data[:, 2] = np.clip(data[:, 2], 0, width)\n            data[:, 1] = np.clip(data[:, 1], 0, height)\n            data[:, 3] = np.clip(data[:, 3], 0, height)\n            \n            \n            small_box_mask1=np.round(data[:, 0])==np.round(data[:, 2],0)\n            data[:, 2:3][small_box_mask1,:]+=1\n            small_box_mask2=np.round(data[:, 1])==np.round(data[:, 3],0)\n            data[:, 3:4][small_box_mask2,:]+=1\n            \n            \n            box_w = np.clip(np.expand_dims(data[:, 2] - data[:, 0], -1),1,640)\n            box_h = np.clip(np.expand_dims(data[:, 3] - data[:, 1], -1),1,480)\n            box_left = np.expand_dims(data[:, 0].copy(), -1)\n            box_top = np.expand_dims(data[:, 1].copy(), -1)\n            # print('box_w',box_w.shape)\n            # print('box_left',box_left.shape)\n\n            gt_box = data[:, :4]\n            gt_label = data[:, 4:5]\n        \n            gt_box[:, 0] = gt_box[:, 0] / float(width)\n            gt_box[:, 2] = gt_box[:, 2] / float(width)\n            gt_box[:, 1] = gt_box[:, 1] / float(height)\n            gt_box[:, 3] = gt_box[:, 3] / float(height)\n\n        \n\n            if gt_box is not None and len(gt_box) > 0:\n                truths = to_tensor(gt_box).float()\n                labels = to_tensor(gt_label).long()\n      \n                \n                boxes, confidences = self.assign_priors(gt_box, gt_label,  to_numpy(self.priors.copy()), 0.5)\n                boxes = xyxy2xywh(boxes)\n                locations = self.convert_boxes_to_locations(boxes, to_numpy(self.priors.copy()))\n\n                locations = np.concatenate([to_numpy(locations).astype(np.float32),to_numpy(confidences).astype(np.float32)], axis=-1)\n                return to_numpy(locations).astype(np.float32)\n\n            num_priors = self.priors.shape[0]\n            return np.zeros((num_priors, 5)).astype(np.float32)\n\n\n    def bbox_transform(self, bbox):\n        return self.data_transform(bbox)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:38:03.059312Z","iopub.execute_input":"2021-12-14T11:38:03.060518Z","iopub.status.idle":"2021-12-14T11:38:03.106019Z","shell.execute_reply.started":"2021-12-14T11:38:03.060456Z","shell.execute_reply":"2021-12-14T11:38:03.104643Z"},"trusted":true},"execution_count":null,"outputs":[]}]}