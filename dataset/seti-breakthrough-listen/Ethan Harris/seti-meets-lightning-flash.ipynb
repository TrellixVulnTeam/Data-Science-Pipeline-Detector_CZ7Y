{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qU 'git+https://github.com/PyTorchLightning/lightning-flash.git#egg=lightning-flash[audio,image]'","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":50.220171,"end_time":"2021-08-14T06:29:21.114875","exception":false,"start_time":"2021-08-14T06:28:30.894704","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-16T10:57:05.6702Z","iopub.execute_input":"2021-08-16T10:57:05.670581Z","iopub.status.idle":"2021-08-16T10:57:46.059111Z","shell.execute_reply.started":"2021-08-16T10:57:05.670531Z","shell.execute_reply":"2021-08-16T10:57:46.058006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training with Flash\n\n[Flash](https://lightning-flash.readthedocs.io/en/stable) is a framework of tasks for fast prototyping, baselining, finetuning and solving business and scientific problems with deep learning. It is focused on:\n\n- Predictions\n- Finetuning\n- Task-based training\n\nIt is built for data scientists, machine learning practitioners, and applied researchers.","metadata":{"papermill":{"duration":0.009783,"end_time":"2021-08-14T06:29:21.13593","exception":false,"start_time":"2021-08-14T06:29:21.126147","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport functools\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn\nimport numpy as np\nfrom torch.nn import functional as F\nfrom torchmetrics import Accuracy, F1, Recall, Precision, MeanAbsoluteError\nimport torchvision\n\nimport flash\nfrom flash.audio import AudioClassificationData\nfrom flash.core.finetuning import FreezeUnfreeze\nfrom flash.image import ImageClassifier\n\nfrom flash.core.data.transforms import ApplyToKeys, merge_transforms\nfrom flash.audio.classification.transforms import default_transforms\n\nimport albumentations\n\n\nPATH_DATASET = Path(\"/kaggle/input/seti-breakthrough-listen\")","metadata":{"papermill":{"duration":12.281828,"end_time":"2021-08-14T06:29:33.428132","exception":false,"start_time":"2021-08-14T06:29:21.146304","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-16T10:57:46.063108Z","iopub.execute_input":"2021-08-16T10:57:46.063565Z","iopub.status.idle":"2021-08-16T10:57:57.389021Z","shell.execute_reply.started":"2021-08-16T10:57:46.063509Z","shell.execute_reply":"2021-08-16T10:57:57.388138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Flash DataModule\n\nIn this section we need to define our datast used later for classification.","metadata":{"papermill":{"duration":0.011183,"end_time":"2021-08-14T06:29:33.45116","exception":false,"start_time":"2021-08-14T06:29:33.439977","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def resolver(root, id):\n    return os.path.join(root, id[0], f\"{id}.npy\")\n\ndef preprocess(array):\n    array = np.vstack(array).transpose((1, 0))\n    return array.astype(np.float32)\n\ndef mixup(batch, alpha=1.0):\n    images = batch[\"input\"]\n    targets = batch[\"target\"].float().unsqueeze(1)\n    \n    lam = np.random.beta(alpha, alpha)\n    perm = torch.randperm(images.size(0))\n    \n    new_images = images * lam + images[perm] * (1 - lam)\n    new_targets = targets * lam + targets[perm] * (1 - lam)\n    batch[\"input\"] = new_images\n    batch[\"target\"] = new_targets\n    return batch\n\ndef convert_val_targets(batch):\n    batch[\"target\"] = batch[\"target\"].float().unsqueeze(1)\n    return batch\n\n\nclass AlbumentationsAdapter(nn.Module):\n    def __init__(self, transform):\n        super().__init__()\n        self.transform = transform\n        \n    def forward(self, x):\n        return self.transform(image=x)[\"image\"]\n\ntrain_transform = {\n    \"pre_tensor_transform\": nn.Sequential(\n        ApplyToKeys(\"input\", preprocess),\n        ApplyToKeys(\n            \"input\",\n            AlbumentationsAdapter(albumentations.Compose([\n                albumentations.Resize(340, 340),\n                #albumentations.HorizontalFlip(p=0.5),\n                #albumentations.VerticalFlip(p=0.5),\n                albumentations.ShiftScaleRotate(\n                    shift_limit=0.1,\n                    scale_limit=0.15,\n                    rotate_limit=20,\n                    p=0.5,\n                ),\n                albumentations.Resize(224, 224),\n                albumentations.RandomBrightness(limit=0.6, p=0.5),\n            ]))\n        ),\n    ),\n    \"to_tensor_transform\": nn.Sequential(\n        ApplyToKeys(\"input\", torchvision.transforms.ToTensor()),\n        ApplyToKeys(\"target\", torch.as_tensor),\n    ),\n    \"per_batch_transform\": mixup,\n}\n\nval_transform = {\n    \"pre_tensor_transform\": nn.Sequential(\n        ApplyToKeys(\"input\", preprocess),\n        ApplyToKeys(\"input\", AlbumentationsAdapter(albumentations.Resize(224, 224))),\n    ),\n    \"to_tensor_transform\": nn.Sequential(\n        ApplyToKeys(\"input\", torchvision.transforms.ToTensor()),\n        ApplyToKeys(\"target\", torch.as_tensor),\n    ),\n    \"per_batch_transform\": convert_val_targets,\n}\n\ndatamodule = AudioClassificationData.from_csv(\n    input_field=\"id\",\n    target_fields=\"target\",\n    train_file=str(PATH_DATASET / \"train_labels.csv\"),\n    train_images_root=str(PATH_DATASET / \"train\"),\n    train_resolver=resolver,\n    train_transform=train_transform,\n    val_resolver=resolver,\n    val_transform=val_transform,\n    batch_size=64,\n    num_workers=os.cpu_count(),\n    val_split=0.1,\n)","metadata":{"papermill":{"duration":24.072519,"end_time":"2021-08-14T06:29:57.535333","exception":false,"start_time":"2021-08-14T06:29:33.462814","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-16T10:59:51.833953Z","iopub.execute_input":"2021-08-16T10:59:51.834374Z","iopub.status.idle":"2021-08-16T11:00:09.288113Z","shell.execute_reply.started":"2021-08-16T10:59:51.834338Z","shell.execute_reply":"2021-08-16T11:00:09.287141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Classif. model\n\nFlash offers rich collection od backbones for image classification and simple plug-in integration with [TorchMetrics](https://torchmetrics.readthedocs.io/en/stable/)","metadata":{"papermill":{"duration":0.049157,"end_time":"2021-08-14T06:29:57.64495","exception":false,"start_time":"2021-08-14T06:29:57.595793","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model = ImageClassifier(\n    backbone=\"efficientnet_b3\",\n    backbone_kwargs={\"in_chans\": 1},\n    metrics=[MeanAbsoluteError()],\n    pretrained=True,\n    num_classes=1,\n    learning_rate=1e-5,\n    loss_fn=nn.BCEWithLogitsLoss(),\n)","metadata":{"papermill":{"duration":2.772319,"end_time":"2021-08-14T06:30:00.484988","exception":false,"start_time":"2021-08-14T06:29:57.712669","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-16T11:00:09.289686Z","iopub.execute_input":"2021-08-16T11:00:09.290033Z","iopub.status.idle":"2021-08-16T11:00:09.637189Z","shell.execute_reply.started":"2021-08-16T11:00:09.289978Z","shell.execute_reply":"2021-08-16T11:00:09.636207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training\n\nit is based on standard [Lightning](https://pytorch-lightning.readthedocs.io/en/stable) trainer","metadata":{"papermill":{"duration":0.052239,"end_time":"2021-08-14T06:30:00.587934","exception":false,"start_time":"2021-08-14T06:30:00.535695","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pytorch_lightning as pl\nlogger = pl.loggers.CSVLogger(save_dir='logs/')\n\ntrainer = flash.Trainer(\n    max_epochs=10,\n    gpus=torch.cuda.device_count(),\n    logger=logger,\n    val_check_interval=0.5,\n    precision=16,\n)\ntrainer.fit(model, datamodule=datamodule)\n\ntrainer.save_checkpoint(\"audio_classification_model.pt\")","metadata":{"papermill":{"duration":13897.657327,"end_time":"2021-08-14T10:21:38.277576","exception":false,"start_time":"2021-08-14T06:30:00.620249","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-16T11:01:01.878129Z","iopub.execute_input":"2021-08-16T11:01:01.87856Z","iopub.status.idle":"2021-08-16T11:01:31.790835Z","shell.execute_reply.started":"2021-08-16T11:01:01.878521Z","shell.execute_reply":"2021-08-16T11:01:31.789458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logged stats\n\nLet's check some training statistic, how los and defined metrics evolved over time/epochs","metadata":{"papermill":{"duration":0.043481,"end_time":"2021-08-14T10:21:38.365963","exception":false,"start_time":"2021-08-14T10:21:38.322482","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\n\nmetrics = pd.read_csv(f'{trainer.logger.log_dir}/metrics.csv')\ndisplay(metrics.head())\n\naggreg_metrics = []\nagg_col = \"epoch\"\nfor i, dfg in metrics.groupby(agg_col):\n    agg = dict(dfg.mean())\n    agg[agg_col] = i\n    aggreg_metrics.append(agg)\n\ndf_metrics = pd.DataFrame(aggreg_metrics)\ndf_metrics[['train_bcewithlogitsloss_step']].plot(grid=True, legend=True, xlabel=agg_col)\ndf_metrics[['train_meanabsoluteerror_step', 'val_meanabsoluteerror']].plot(grid=True, legend=True, xlabel=agg_col)\n# df_metrics[['train_f1_step', 'train_recall_step', 'train_precision_step', 'val_f1', 'val_recall', 'val_precision']].plot(grid=True, legend=True, xlabel=agg_col)","metadata":{"papermill":{"duration":1.123716,"end_time":"2021-08-14T10:21:39.534976","exception":false,"start_time":"2021-08-14T10:21:38.41126","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-16T11:01:40.644756Z","iopub.execute_input":"2021-08-16T11:01:40.645196Z","iopub.status.idle":"2021-08-16T11:01:41.152829Z","shell.execute_reply.started":"2021-08-16T11:01:40.645148Z","shell.execute_reply":"2021-08-16T11:01:41.152011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predictions\n\nWith trained model we just need run inference to prepeare dibission file","metadata":{"papermill":{"duration":0.04306,"end_time":"2021-08-14T10:21:39.618838","exception":false,"start_time":"2021-08-14T10:21:39.575778","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\n\nfrom flash.core.data.process import Serializer\n\n\nclass IdPredictionSerializer(Serializer):\n\n    def serialize(self, sample):\n        preds = sample[\"preds\"]\n        preds = torch.tensor(preds).to(torch.float)\n        preds = preds.sigmoid().item()\n        filepath = sample[\"metadata\"][\"filepath\"]\n        file_id = os.path.basename(filepath).split(\".\")[0]\n        return {\"id\": file_id, \"target\": preds}","metadata":{"papermill":{"duration":0.053461,"end_time":"2021-08-14T10:21:39.714896","exception":false,"start_time":"2021-08-14T10:21:39.661435","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-16T11:02:04.740459Z","iopub.execute_input":"2021-08-16T11:02:04.740798Z","iopub.status.idle":"2021-08-16T11:02:04.749666Z","shell.execute_reply.started":"2021-08-16T11:02:04.740765Z","shell.execute_reply":"2021-08-16T11:02:04.748523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nfrom tqdm import tqdm\nfrom itertools import chain\n\nfound_npy = glob.glob(str(PATH_DATASET / \"test\" / \"*\" / \"*.npy\"))\n\npredict_transform = {\n    \"pre_tensor_transform\": nn.Sequential(\n        ApplyToKeys(\"input\", preprocess),\n        ApplyToKeys(\"input\", AlbumentationsAdapter(albumentations.Resize(224, 224))),\n    ),\n    \"to_tensor_transform\": ApplyToKeys(\"input\", torchvision.transforms.ToTensor()),\n}\n\ndatamodule = AudioClassificationData.from_files(\n    predict_files=found_npy,\n    predict_transform=predict_transform,\n    batch_size=1024,\n    num_workers=os.cpu_count(),\n)\n\nmodel.serializer = IdPredictionSerializer()\n\nsubmission = trainer.predict(model, datamodule=datamodule)\n\nsubmission = list(chain.from_iterable(submission))\npd.DataFrame(submission).set_index(\"id\").to_csv(\"submission.csv\")","metadata":{"papermill":{"duration":470.565951,"end_time":"2021-08-14T10:29:30.324243","exception":false,"start_time":"2021-08-14T10:21:39.758292","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-16T11:02:07.271773Z","iopub.execute_input":"2021-08-16T11:02:07.272157Z","iopub.status.idle":"2021-08-16T11:09:58.397304Z","shell.execute_reply.started":"2021-08-16T11:02:07.272119Z","shell.execute_reply":"2021-08-16T11:09:58.395328Z"},"trusted":true},"execution_count":null,"outputs":[]}]}