{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üìö<span style = 'font-family:cursive;'> Libraries </span> ","metadata":{}},{"cell_type":"code","source":"#!pip install efficientnet -q","metadata":{"execution":{"iopub.status.busy":"2021-07-03T05:06:55.388885Z","iopub.execute_input":"2021-07-03T05:06:55.389283Z","iopub.status.idle":"2021-07-03T05:06:55.393794Z","shell.execute_reply.started":"2021-07-03T05:06:55.3892Z","shell.execute_reply":"2021-07-03T05:06:55.392746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Libraries\nimport sys\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsys.path = ['../input/efficientnet-pytorch/EfficientNet-PyTorch/EfficientNet-PyTorch-master',] + sys.path\n\n\nimport glob\nimport numpy as np\nimport pandas as pd\nimport math\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport cv2\nimport random\n\n# tensorflow\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import models\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\nfrom kaggle_datasets import KaggleDatasets\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn import metrics\n\nimport torch\nimport torchvision.models as models\nimport torch.nn as nn\nimport torchvision.transforms as transforms\n\n\nfrom tqdm import tqdm\n\nfrom efficientnet_pytorch import model as enet\n\n#albumentations\nimport albumentations\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau","metadata":{"execution":{"iopub.status.busy":"2021-07-03T05:06:55.396014Z","iopub.execute_input":"2021-07-03T05:06:55.396408Z","iopub.status.idle":"2021-07-03T05:06:58.094362Z","shell.execute_reply.started":"2021-07-03T05:06:55.396371Z","shell.execute_reply":"2021-07-03T05:06:58.093512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìù<span style=\"font-family:cursive;\"> Data Preparation</span>\n","metadata":{}},{"cell_type":"code","source":"data_dir = '../input/seti-breakthrough-listen'\ntrain_merger = os.path.join(data_dir,'train_labels.csv')\ntrain_labels = pd.read_csv(train_merger)\nprint('train_label_csv : ' +str(train_labels.shape[0]))\n#adding the path for each id for easier processing\ntrain_labels['path'] = train_labels['id'].apply(lambda x: f'../input/seti-breakthrough-listen/train/{x[0]}/{x}.npy')\ntrain_labels.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-03T05:06:58.095857Z","iopub.execute_input":"2021-07-03T05:06:58.096183Z","iopub.status.idle":"2021-07-03T05:06:58.161286Z","shell.execute_reply.started":"2021-07-03T05:06:58.096128Z","shell.execute_reply":"2021-07-03T05:06:58.160014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_datasets import KaggleDatasets\n\ndata_dir ='../input/seti-breakthrough-listen'\npath_dir = KaggleDatasets().get_gcs_path('seti-breakthrough-listen')\n\ntrain = os.path.join(data_dir, 'train_labels.csv')\ntrain_df = pd.read_csv(train)  \nprint(\"data_train_csv : \" + str(train_df.shape[0]))\ndisplay(train_df.head(5))\n\n\nsub = os.path.join(data_dir,'sample_submission.csv')\nsub_df = pd.read_csv(sub)\nprint(\"data_submission_csv : \" + str(sub_df.shape[0]) )\ndisplay(sub_df.head(5))\n\nlabel_cols = sub_df.columns[1:]\n#label_cols.values\nlabels = train_df[label_cols].values","metadata":{"execution":{"iopub.status.busy":"2021-07-03T05:06:58.162469Z","iopub.execute_input":"2021-07-03T05:06:58.162791Z","iopub.status.idle":"2021-07-03T05:06:58.633291Z","shell.execute_reply.started":"2021-07-03T05:06:58.162764Z","shell.execute_reply":"2021-07-03T05:06:58.632466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìó<span style=\"font-family:cursive;\"> Albumentations</span>","metadata":{}},{"cell_type":"code","source":"from typing import *\n\nclass Transform:\n    def __init__(self, aug_kwargs: Dict):\n        albumentations_aug = [getattr(A, name)(**kwargs)\n                            for name, kwargs in aug_kwargs.items()]\n        albumentations_aug.append(ToTensorV2(p=1))\n        self.transform = A.Compose(albumentations_aug)\n    \n    def __call__(self, image):\n        image = self.transform(image = image)['image']\n        return image","metadata":{"execution":{"iopub.status.busy":"2021-07-03T05:06:58.635872Z","iopub.execute_input":"2021-07-03T05:06:58.63614Z","iopub.status.idle":"2021-07-03T05:06:58.641782Z","shell.execute_reply.started":"2021-07-03T05:06:58.636111Z","shell.execute_reply":"2021-07-03T05:06:58.640678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ModeTransform():\n    def __init__(self, df_frame, config,type_mode,mode,target,type_spatial,type_channel,transform):\n        self.df_frame = df_frame\n        self.type_mode = type_mode\n        self.config = config\n        self.target = target\n        self.file_names = df_frame['path'].values\n        self.labels = df_frame['target'].values\n        self.transform = transform\n        self.mode = mode\n        self.type_spatial = type_spatial\n        self.type_channel = type_channel\n        \n    def __len__(self):\n        return len(self.df_frame)\n\n    def __getitem__(self, idx):\n        image = np.load(self.file_names[idx])\n        # print(image.shape) -> (6, 273, 256)\n        \n        if self.type_spatial:\n            if self.type_mode == 'spatial_6ch':\n                image = image.astype(np.float32)\n                image = np.vstack(image) # no transpose here (1638, 256) \n                #image = np.vstack(image).transpose((1, 0))\n                # print(image.shape) -> (256, 1638)\n            elif self.type_mode == 'spatial_3ch':\n                image = image[::2].astype(np.float32)\n                image = np.vstack(image).transpose((1, 0))\n        \n        elif self.type_channel:\n            if self.type_mode == '6_channel':\n                image = image.astype(np.float32)\n                image = np.transpose(image, (1,2,0))\n            elif self.type_mode == '3_channel':\n                image = image[::2].astype(np.float32)\n                image = np.transpose(image, (1,2,0))\n        \n        if self.transform:\n            image = self.transform(image)\n        else:\n            image = torch.from_numpy(image).float()\n\n        if self.mode == 'test':\n            return image    \n        else:\n            label = torch.tensor(self.labels[idx]).float()\n            return image, label","metadata":{"execution":{"iopub.status.busy":"2021-07-03T05:06:58.6431Z","iopub.execute_input":"2021-07-03T05:06:58.643938Z","iopub.status.idle":"2021-07-03T05:06:58.657424Z","shell.execute_reply.started":"2021-07-03T05:06:58.643895Z","shell.execute_reply":"2021-07-03T05:06:58.656516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = { \n    \"TRAIN_TRANSFORMS\": {        \n        \"VerticalFlip\": {\"p\": 0.5},\n        \"HorizontalFlip\": {\"p\": 0.5},\n        \"Resize\": {\"height\": 640, \"width\": 640, \"p\": 1},\n    }}\nconfig = CONFIG\n\n# Parameters\nparams_train  = {'mode'            : 'train',\n                 'type_mode'       : 'spatial_6ch',\n                 'target'          : True,\n                 'type_spatial'    : True,\n                 'type_channel'    : True}\n\ntrain_dset = ModeTransform(train_labels,config,\n                           **params_train,\n                           transform=Transform(config[\"TRAIN_TRANSFORMS\"]))\n\nfor i in range(2):\n    image, label = train_dset[i]\n    plt.imshow(image[0])\n    plt.title(f'label: {label}')\n    plt.show()\nimage.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-03T05:06:58.65901Z","iopub.execute_input":"2021-07-03T05:06:58.659458Z","iopub.status.idle":"2021-07-03T05:06:59.047962Z","shell.execute_reply.started":"2021-07-03T05:06:58.659419Z","shell.execute_reply":"2021-07-03T05:06:59.047101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed = 0):\n    '''Sets the seed of the entire notebook so results \n     are the same every time we run.This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random_state = np.random.RandomState(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    return random_state\n\nseed = 42\nrandom_state = set_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T05:06:59.050751Z","iopub.execute_input":"2021-07-03T05:06:59.05103Z","iopub.status.idle":"2021-07-03T05:06:59.059368Z","shell.execute_reply.started":"2021-07-03T05:06:59.051004Z","shell.execute_reply":"2021-07-03T05:06:59.058174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device('cuda')\n    print('GPU is available')\nelse:\n    device = torch.device('cpu')\n    print('GPU not available, CPU used')","metadata":{"execution":{"iopub.status.busy":"2021-07-03T05:06:59.06137Z","iopub.execute_input":"2021-07-03T05:06:59.061832Z","iopub.status.idle":"2021-07-03T05:06:59.088819Z","shell.execute_reply.started":"2021-07-03T05:06:59.061794Z","shell.execute_reply":"2021-07-03T05:06:59.087714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üóÉ <span style=\"font-family:cursive;\">Custom Dataset</span>","metadata":{}},{"cell_type":"code","source":"class CustomDataset:\n    \n    def __init__(self, image_dir, targets, isTrain=True): \n        self.image_dir = image_dir\n        self.targets = targets\n        self.isTrain = isTrain\n\n    def __len__(self):\n        return len(self.image_dir)\n    \n    def __getitem__(self, idx):      \n        image = np.load(self.image_dir[idx])\n        image1 = np.vstack(image).transpose((1, 0)).astype(np.float32)[np.newaxis, ]\n                \n        return {\n            \"image_trnspose\": torch.tensor(image1, dtype=torch.float),\n            \"targets\": torch.tensor(self.targets[idx], dtype=torch.long),\n        }   ","metadata":{"execution":{"iopub.status.busy":"2021-07-03T05:06:59.090271Z","iopub.execute_input":"2021-07-03T05:06:59.090679Z","iopub.status.idle":"2021-07-03T05:06:59.101221Z","shell.execute_reply.started":"2021-07-03T05:06:59.090636Z","shell.execute_reply":"2021-07-03T05:06:59.100482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/seti-breakthrough-listen/train_labels.csv')\nprint (df.shape)\ndf['img_path'] = df['id'].apply(lambda x: f'../input/seti-breakthrough-listen/train/{x[0]}/{x}.npy')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-03T05:06:59.104265Z","iopub.execute_input":"2021-07-03T05:06:59.104519Z","iopub.status.idle":"2021-07-03T05:06:59.169894Z","shell.execute_reply.started":"2021-07-03T05:06:59.104494Z","shell.execute_reply":"2021-07-03T05:06:59.168936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üß™<span style=\"font-family:cursive;\">Define Model</span>","metadata":{}},{"cell_type":"code","source":"class enetv2(nn.Module):\n    def __init__(self, backbone, out_dim):\n        super(enetv2, self).__init__()\n        self.enet = enet.EfficientNet.from_name(backbone)\n        self.enet.load_state_dict(torch.load(pretrained_model[backbone]))\n        \n        \n        self.myfc = nn.Linear(self.enet._fc.in_features, out_dim)\n        self.enet._fc = nn.Identity()\n        self.conv1 = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=3, bias=False)\n\n    def extract(self, x):\n        return self.enet(x)\n      \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.extract(x)\n        x = self.myfc(x)\n        \n        return x                               ","metadata":{"execution":{"iopub.status.busy":"2021-07-03T05:06:59.17138Z","iopub.execute_input":"2021-07-03T05:06:59.171734Z","iopub.status.idle":"2021-07-03T05:06:59.179361Z","shell.execute_reply.started":"2021-07-03T05:06:59.171698Z","shell.execute_reply":"2021-07-03T05:06:59.178249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üîÑ<span style=\"font-family:cursive;\"> MixUp Augmentation</span>","metadata":{}},{"cell_type":"code","source":"class MixUpAugmentation():\n    '''Returns mixed inputs, pairs of targets, and lambda'''\n    def __init__(self, x, y, alpha, use_cuda):\n        self.x = x\n        self.y = y\n        self.alpha = alpha\n        self.use_cuda = use_cuda\n        \n    def __getitem__(self):\n        if self.alpha > 0:\n            lmbda = np.random.beta(self.alpha, self.alpha)\n        else:\n            lmbda = 1\n         \n        batch_size = self.x.size()[0]\n        if self.use_cuda:\n            index = torch.randperm(batch_size).cuda()\n        else:\n            index = torch.randperm(batch_size)\n\n        mixed_x = lmbda * self.x + (1 - lmbda) * self.x[index, :]\n        y_a, y_b = self.y, self.y[index]\n        return mixed_x, y_a, y_b, lmbda    ","metadata":{"execution":{"iopub.status.busy":"2021-07-03T05:13:53.52778Z","iopub.execute_input":"2021-07-03T05:13:53.528139Z","iopub.status.idle":"2021-07-03T05:13:53.53663Z","shell.execute_reply.started":"2021-07-03T05:13:53.528107Z","shell.execute_reply":"2021-07-03T05:13:53.535485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MixUpCriterion():\n     def __init__(self, criterion, pred, y_a, y_b, lmbda):\n        self.criterion = criterion\n        self.pred = pred\n        self.y_a = y_a\n        self.y_b = y_b\n        self.lmbda = lmbda\n      \n     def __getitem__(self):\n        return self.lmbda * self.criterion(self.pred, self.y_a) + (1 - self.lmbda) * self.criterion(self.pred, self.y_b)    ","metadata":{"execution":{"iopub.status.busy":"2021-07-03T05:13:54.560135Z","iopub.execute_input":"2021-07-03T05:13:54.5605Z","iopub.status.idle":"2021-07-03T05:13:54.565928Z","shell.execute_reply.started":"2021-07-03T05:13:54.560471Z","shell.execute_reply":"2021-07-03T05:13:54.56503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nParameters\n'''\nparams_MixUpAug  = {'alpha'            : 1.0,\n                   'use_cuda'         : True} ","metadata":{"execution":{"iopub.status.busy":"2021-07-03T05:13:55.571775Z","iopub.execute_input":"2021-07-03T05:13:55.572095Z","iopub.status.idle":"2021-07-03T05:13:55.57646Z","shell.execute_reply.started":"2021-07-03T05:13:55.572062Z","shell.execute_reply":"2021-07-03T05:13:55.575387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìë<span style=\"font-family:cursive;\"> Train & Evaluate Loader</span>","metadata":{}},{"cell_type":"code","source":"def train(data_loader, model, optimizer, device):\n    \n    model.train()\n    \n    for data in tqdm(data_loader, position=0, leave=True, desc='Training'):\n        \n        inputs1 = data[\"image_trnspose\"]\n        targets = data['targets']\n        \n        inputs1, targets_a, targets_b, lam = MixUpAugmentation(inputs1, \n                                                               targets.view(-1, 1), **params_MixUpAug)\n\n        inputs1 = inputs1.to(device, dtype=torch.float)\n        targets_a = targets_a.to(device, dtype=torch.float)\n        targets_b = targets_b.to(device, dtype=torch.float)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs1)\n        loss = MixUpCriterion(criterion, outputs, targets_a, targets_b, lam)\n        loss.backward()\n        optimizer.step()\n        \ndef evaluate(data_loader, model, device):\n    model.eval()\n    \n    final_targets = []\n    final_outputs = []\n    \n    with torch.no_grad():\n        \n        for data in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n            inputs = data[\"image_trnspose\"]\n            targets = data[\"targets\"]\n            inputs = inputs.to(device, dtype=torch.float)\n            targets = targets.to(device, dtype=torch.float)\n            \n            output = model(inputs)\n            \n            targets = targets.detach().cpu().numpy().tolist()\n            output = output.detach().cpu().numpy().tolist()\n            \n            final_targets.extend(targets)\n            final_outputs.extend(output)\n            \n    return final_outputs, final_targets","metadata":{"execution":{"iopub.status.busy":"2021-07-03T05:13:57.361522Z","iopub.execute_input":"2021-07-03T05:13:57.361863Z","iopub.status.idle":"2021-07-03T05:13:57.372068Z","shell.execute_reply.started":"2021-07-03T05:13:57.361831Z","shell.execute_reply":"2021-07-03T05:13:57.371228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths = [\n 'efficientnet-b0-08094119.pth',\n 'efficientnet-b1-dbc7070a.pth',\n 'efficientnet-b2-27687264.pth',\n 'efficientnet-b3-c8376fa2.pth',\n 'efficientnet-b4-e116e8b3.pth',\n 'efficientnet-b5-586e6cc6.pth',\n 'efficientnet-b6-c76e70fd.pth',\n 'efficientnet-b7-dcc49843.pth',\n]\npretrained_model = {\n    'efficientnet-b0': '../input/efficientnet-pytorch/' + paths[0],\n    'efficientnet-b1': '../input/efficientnet-pytorch/' + paths[1],\n    'efficientnet-b2': '../input/efficientnet-pytorch/' + paths[2],\n    'efficientnet-b3': '../input/efficientnet-pytorch/' + paths[3],\n    'efficientnet-b4': '../input/efficientnet-pytorch/' + paths[4],\n    'efficientnet-b5': '../input/efficientnet-pytorch/' + paths[5],\n    'efficientnet-b6': '../input/efficientnet-pytorch/' + paths[6],\n    'efficientnet-b7': '../input/efficientnet-pytorch/' + paths[7],\n}","metadata":{"execution":{"iopub.status.busy":"2021-07-03T05:13:58.415293Z","iopub.execute_input":"2021-07-03T05:13:58.415617Z","iopub.status.idle":"2021-07-03T05:14:00.350083Z","shell.execute_reply.started":"2021-07-03T05:13:58.415588Z","shell.execute_reply":"2021-07-03T05:14:00.348217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\"\nmodel_name = 'efficientnet-b4'\nmodel = enetv2(model_name,out_dim = 1)\nmodel.to(device)\nmodel.load_state_dict(torch.load('../input/eff-mixup-v98/aug_training_0_10.pt'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('../input/seti-breakthrough-listen/sample_submission.csv')\nsubmission['img_path'] = submission['id'].apply(lambda x: f'../input/seti-breakthrough-listen/test/{x[0]}//{x}.npy')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = CustomDataset(image_dir=submission.img_path.values, \n                                     targets=submission.target.values,\n                                      )\n\ntest_loader = torch.utils.data.DataLoader(test_dataset, \n                                          batch_size=16, \n                                          shuffle=False, \n                                          num_workers=4)\n\ntest_predictions, test_targets = evaluate(test_loader,model, device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = np.array(test_predictions)\nsubmission.target = test_predictions[:, 0]\nsubmission.drop(['img_path'], axis=1, inplace=True)\nsubmission.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"font-family:cursive;\"> if it's notebook useful for you come on upvote üòÄ‚¨Üüîù</span>","metadata":{}}]}