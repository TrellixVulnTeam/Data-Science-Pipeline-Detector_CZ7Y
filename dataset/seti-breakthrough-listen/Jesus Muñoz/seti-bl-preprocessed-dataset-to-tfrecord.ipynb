{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exporting SETI - BL competition data to TFRecord\n\nI will export the [seti-breakthrough-listen competition's dataset](https://www.kaggle.com/c/seti-breakthrough-listen) to TFRecord format for faster data handling and enabling [TPU](https://www.kaggle.com/docs/tpu) processing. I will perform some preprocessing prior to saving the data, in order to accelerate posterior model training. In order to try different data preprocessing routines, I will commit a new version of this notebook each time I update the preprocessing function, and the dataset will be updated too.\n\nI will use a simple tool that I have developed for this task called [TFRecord Dataset](https://github.com/ChusJM/tfrecord_dataset). This tool has been inspired by some very interesting work I have found here in Kaggle, please read the [acknowledgements section](https://github.com/ChusJM/tfrecord_dataset#acknowledgements) in the repository page for further information.\n\n_Disclaimer_: To avoid unwanted data distribution outside the competition scope that might be against the competition rules (I'm not sure), I've kept the generated dataset as private, but you can create your own copy by running this notebook. This way, you can include your custom preprocessing routine by editing the notebook before running it.\n\n## Issues\n\nThe dataset is saved and later loaded correctly, but I have not been able to use it to train models on TPUs, although with GPU all works fine. I do not know if there is a problem with the dataset or with the training code on TPU itself. ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Preparing Kaggle Dataset\n\nBefore doing anything with the data, it is necessary to initialize the a Kaggle Dataset where the data will be uploaded. Thanks to [xhlulu](https://www.kaggle.com/xhlulu) for these notebooks ([train](https://www.kaggle.com/xhlulu/seti-create-training-tf-records), [test](https://www.kaggle.com/xhlulu/seti-create-test-tf-records)) which serve as a great explanation of the process.\n\nFirst, we initialize the local environment to work with Kaggle's API and the local directory with the needed metadata. For this snippet to work, it is neccessary that you include your own Kaggle credentials as *Secrets* in the notebook (see this [post](https://www.kaggle.com/product-feedback/114053) for more information).","metadata":{}},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\nsecrets = UserSecretsClient()\nos.environ['KAGGLE_USERNAME'] = secrets.get_secret(\"KAGGLE_USERNAME\")\nos.environ['KAGGLE_KEY'] = secrets.get_secret(\"KAGGLE_KEY\")","metadata":{"execution":{"iopub.status.busy":"2021-06-17T19:20:06.017833Z","iopub.execute_input":"2021-06-17T19:20:06.018214Z","iopub.status.idle":"2021-06-17T19:20:06.62953Z","shell.execute_reply.started":"2021-06-17T19:20:06.018176Z","shell.execute_reply":"2021-06-17T19:20:06.628698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And we create the local directory where the files will be saved. The directory is initialized with a metadata `*.json` file,","metadata":{}},{"cell_type":"code","source":"import json\nfrom pathlib import Path\n\n\nOUTPUT_DATASET_PATH = Path('/kaggle/dataset')\nDATASET_NAME = 'seti-breaktrough-listen-preprocessed'\nSUBSET_NAME = 'train'\nDATASET_TITLE = 'SETI Breakthrough Listen - Preprocessed train set'\nif not OUTPUT_DATASET_PATH.exists():\n    OUTPUT_DATASET_PATH.mkdir()\n\nmeta = dict(\n    id=f\"chusjm/{DATASET_NAME}-{SUBSET_NAME}\",\n    title=DATASET_TITLE,\n    isPrivate=True,\n    licenses=[\n        {\"name\": \"copyright-authors\"}\n    ]\n)\n\nwith open(OUTPUT_DATASET_PATH / 'dataset-metadata.json', 'w') as f:\n    json.dump(meta, f)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T19:20:06.630771Z","iopub.execute_input":"2021-06-17T19:20:06.631133Z","iopub.status.idle":"2021-06-17T19:20:06.63774Z","shell.execute_reply.started":"2021-06-17T19:20:06.631095Z","shell.execute_reply":"2021-06-17T19:20:06.63685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only the **first time** the notebook is run, it is also necessary to create the Kaggle Dataset.","metadata":{}},{"cell_type":"code","source":"#!touch /kaggle/dataset/dummy.txt\n#!kaggle datasets create -p \"/kaggle/dataset\" --dir-mode zip\n#!rm /kaggle/dataset/dummy.txt","metadata":{"execution":{"iopub.status.busy":"2021-06-17T19:20:06.639121Z","iopub.execute_input":"2021-06-17T19:20:06.639644Z","iopub.status.idle":"2021-06-17T19:20:06.647073Z","shell.execute_reply.started":"2021-06-17T19:20:06.639608Z","shell.execute_reply":"2021-06-17T19:20:06.646278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting the tool\n\nThis tool will allow us to handle all dataset files, split the dataset in even partitions and export the chunks to `*.tfrecord` format.","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/ChusJM/tfrecord_dataset.git","metadata":{"execution":{"iopub.status.busy":"2021-06-17T19:20:06.956498Z","iopub.execute_input":"2021-06-17T19:20:06.956837Z","iopub.status.idle":"2021-06-17T19:20:08.643284Z","shell.execute_reply.started":"2021-06-17T19:20:06.956806Z","shell.execute_reply":"2021-06-17T19:20:08.642296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading dataset structure\n\nIn this dataset, each ``*.npy`` file contains one data sample (_cadence snippet_), and there is a `train_labels.csv` file that contains the correspondence between each file name (without extension) and its label (``0`` or ``1``).\n\nThe first part of the tool will allow me to list all dataset files and read the labels.","metadata":{}},{"cell_type":"code","source":"import tfrecord_dataset.tfrecord_dataset.dataset as dataset\nimport tfrecord_dataset.tfrecord_dataset.tfrecords as tfrecords\nfrom pathlib import Path\n\n# The dataset root directory contains several subdirectories with *.npy files, one per example (cadence snippet).\nDATASET_PATH = Path('../input/seti-breakthrough-listen/')\n# Labels are integers: 1 when the cadence is labeled as positive (E.T. signal) and 0 when the cadence is labeled as negative.\nlabel_type = int\n# Used to ignore the first row of the CSV, which is just the names of the columns.\nlabels_file_has_header = True\n# Load dataset structure.\nseti_dataset = dataset.Dataset(\n    train_set_dir=str(DATASET_PATH / 'train'),  # Train subdirectory\n    test_set_dir=str(DATASET_PATH / 'test'),    # Test subdirectory\n    file_format='*.npy',\n    train_labels_file=str(DATASET_PATH / 'train_labels.csv'),\n    label_type=label_type,\n    train_labels_file_has_header=labels_file_has_header\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T19:20:10.133967Z","iopub.execute_input":"2021-06-17T19:20:10.134302Z","iopub.status.idle":"2021-06-17T19:21:01.110837Z","shell.execute_reply.started":"2021-06-17T19:20:10.134265Z","shell.execute_reply":"2021-06-17T19:21:01.109732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exporting to TFRecord\n\nTo create the TFRecord dataset, it is necessary to distribute the original data in different chunks or splits of equal size, and each one will be saved in a different TFRecord file. Thus, the content of several ``*.npy`` files will go together in one TFRecord. The best way to do this, as it is done in [awsaf49's](https://www.kaggle.com/awsaf49) [notebook](https://www.kaggle.com/awsaf49/seti-bl-256x256-tfrec-data/), is to distribute the ``*.npy`` files at random, but **ensuring** the original label distribution is kept on each chunk. This way, it is possible to split the TFRecord dataset _a posteriori_ in further subsets (train and validation, K-folds, etc.) just by distributing the TFRecord files.\n\nOnce the dataset structure has been loaded, the tool will help me split the dataset and iterate through the list of partitions in order to save them to `*.tfrecord` files.\n\n### Custom loading and preprocessing function\n\nBefore proceeding with the exportation, it is necessary to define a function that will load and preprocess the data. The tool includes a basic one for `*.npy` files, but it will be necessary to add more preprocessing steps to try to improve models performance for the competition.","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\n# Define output shape for the data. This one is selected to match ImageNet, since most\n# models are pretrained using this dataset and thus they have this shape.\nOUTPUT_SHAPE = (224, 224)\n\n\ndef custom_npy_data_preprocessor(example_path):\n    \"\"\"\n    Loads and preprocesses an example, given its path. The example must be a `*.npy` file that contains an array\n    that can (and will) be casted to float32. It also returns the data type and the shape before flattening the array,\n    to allow serialization and later reconstruction, and a string to serve as example identifier (e.g. the file name).\n    :param example_path: Path to the `*.npy` file.\n    :type example_path: str\n    :return: Flattened numpy array of type np.float32 (it must be compatible with a list of float), data type (to allow\n        for serialization and later parsing), data shape before flattening (to allow for unflattening), and example ID.\n    :rtype: tuple[numpy.ndarray, type, tuple[int], str]\n    \"\"\"\n    # Load cadence snippet.\n    data = np.load(example_path).astype(np.float32)\n    # Normalize each observation of the cadence. Observations are placed along the first axis.\n    # The normalization maps the input range to [0.0, 255.0]. This is needed to compute the\n    # equalization correctly.\n    data = np.array([_normalize(obs) for obs in data])\n    # Equalize each observation of the cadence\n    data = np.array([_equalize(obs) for obs in data])\n    # Transpose from OBS, H, W to H, W, OBS)\n    data = data.transpose((1, 2, 0))\n    # Resize to OUTPUT_SHAPE\n    data = cv2.resize(data, OUTPUT_SHAPE, cv2.INTER_AREA)\n    \n    return data.ravel(), data.dtype, data.shape, Path(example_path).stem\n\ndef _normalize(input_array):\n    \"\"\"\n    Normalizes a 2D array linearly using its maximum and minimum values to map its range to [0.0, 255.0]\n    \n    :param input_array: Input 2D array of type float.\n    :type input_array: numpy.ndarray\n    :return: Normalized array of type float in the range [0.0, 255.0]\n    :rtype: numpy.ndarray\n    \"\"\"\n    # Normalize and map range to [0.0, 255.0]\n    min_value, max_value = input_array.min(), input_array.max()\n    input_array = (input_array - min_value) / (max_value - min_value) * 255.0\n    \n    return input_array\n\ndef _equalize(input_array):\n    \"\"\"\n    Equalizes a 2D array using Contrast Limited Adaptive Histogram Equalization (CLAHE).\n    This enhances the local contrast in regions of the image.\n    \n    :param input_array: Input 2D array to be equalized, of type float. It will be cast to byte values so its data must in the range (0, 255)\n        to minimize rounding loss and to avoid clipping.\n    :type input_array: numpy.ndarray\n    :return: Equalized version of the same 2D array, cast back to float values (single precision),\n    :rtype: numpy.ndarray\n    \"\"\"\n    # Cast to integer.\n    input_array = np.round(input_array).astype(np.uint8)\n    # Apply CLAHE\n    clahe = cv2.createCLAHE(4)\n    input_array = clahe.apply(input_array)\n    # Back to float\n    input_array = input_array.astype(np.float32)\n\n    return input_array","metadata":{"execution":{"iopub.status.busy":"2021-06-17T19:21:01.112355Z","iopub.execute_input":"2021-06-17T19:21:01.112683Z","iopub.status.idle":"2021-06-17T19:21:01.288775Z","shell.execute_reply.started":"2021-06-17T19:21:01.112642Z","shell.execute_reply":"2021-06-17T19:21:01.287995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset exportation\n\nOnce the preprocessing function has been defined, we are ready to iterate through the list of chunks and save each chunk to a different TFRecord file.","metadata":{}},{"cell_type":"code","source":"import tqdm\n\n\nN_WORKERS = 8  # It seems that Kaggle's TPU support allocates 8 replicas.\nN_SPLITS = 10*N_WORKERS # Following Tensorflow's recommendations, the number of splits should be ~10*N,\n                        # provided the file size is 100+ MB (ideally), being N the number of workers.\n                        # In this case, the train and test subsets have tens of GB of data, so the TFRecord file\n                        # size will be in the order of hundreds of MB or even a thousand MB for 8 workers.\nSHUFFLE = True  # To ensure the sample distribution is truly random\nRANDOM_STATE = 42  # To ensure reproducibility (same dataset will be generated within different executions)\n\ndata_type = None\ndata_shape = None\n\n# Create splits after shuffling the examples using the specified seed for the random number generator.\nfor idx, train_set_split in tqdm.tqdm(\n        enumerate(seti_dataset.train_set_in_splits_generator(n_splits=N_SPLITS, shuffle=SHUFFLE, seed=RANDOM_STATE)),\n        total=N_SPLITS):\n    # Get the data type and shape from the first example, using the specific data preprocessing function.\n    if idx == 0:\n        _, data_type, data_shape, _ = custom_npy_data_preprocessor(train_set_split[0, 0])\n    # Save the split into a tfrecord file.\n    tfrecords.write_dataset_to_file(\n        dataset=train_set_split,\n        file_path=str(OUTPUT_DATASET_PATH / f'{DATASET_NAME}_{SUBSET_NAME}_{idx}.tfrecord'),\n        data_preprocessing_function=custom_npy_data_preprocessor\n    )","metadata":{"execution":{"iopub.status.busy":"2021-06-17T19:21:01.291399Z","iopub.execute_input":"2021-06-17T19:21:01.291764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -las \"/kaggle/dataset\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading from TFRecord\n\nFor demonstration purposes, the code to load the dataset from the generated files is included here.","metadata":{}},{"cell_type":"code","source":"# Load the train set from the tfrecords.\ntf_train_set = tfrecords.load_dataset_from_files(\n    file_paths=list(map(str, OUTPUT_DATASET_PATH.glob(f'{DATASET_NAME}_{SUBSET_NAME}_*.tfrecord'))),\n    data_shape=data_shape, data_type=data_type, label_type=label_type\n)\n\n\n# Plot an example to check it looks correct.\nimport matplotlib.pyplot as plt\n\ndef plt_cadence(cadence, title):\n    cadence_labels = 'ABACAD'\n    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n    fig = plt.figure(figsize=(9,9))\n    gs = fig.add_gridspec(cadence.shape[0], hspace=0)\n    axs = gs.subplots(sharex=True, sharey=True)\n    fig.suptitle(title)\n    for ax, (label, obs) in zip(axs, zip(cadence_labels, cadence)):\n        im = ax.imshow(obs.astype(float), aspect='auto')\n        ax.label_outer()\n        # place a text box in upper left in axes coords\n        ax.text(0.05, 0.95, label, transform=ax.transAxes, fontsize=12,\n                verticalalignment='top', bbox=props)\n        fig.colorbar(im, ax=ax)\n    plt.tight_layout()\n    return fig\n\n\n# Read each recovered example.\nfor recovered in tf_train_set:\n    data, label, example_id = recovered['data'], recovered['label'], recovered['example_id']\n    plt_cadence(data.numpy().transpose((2, 0, 1)), f'Example cadence #{example_id} (label: {label})')\n    # Show only the first and break the loop. One is enough for demonstration.\n    break\n","metadata":{"execution":{"iopub.status.busy":"2021-06-17T19:13:47.666236Z","iopub.execute_input":"2021-06-17T19:13:47.66665Z","iopub.status.idle":"2021-06-17T19:13:49.331324Z","shell.execute_reply.started":"2021-06-17T19:13:47.666617Z","shell.execute_reply":"2021-06-17T19:13:49.330153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Uploading as a Kaggle Dataset\n\nFinally I will update the dataset with the newly generated files using Kaggle's API. It seems that, although I am using the `-d` option, which tells Kaggle Datasets to delete the previous data, it is necessary to do it manually first to avoid reaching the storage limit. Because of that, before running this notebook, it is necessary to go to the dataset's page and upload a new version with all the previous files removed and only a \"placeholder\" empty `*.txt` file, to make room for the new files that will be uploaded with the next command.","metadata":{}},{"cell_type":"code","source":"!kaggle datasets version -p \"/kaggle/dataset\" -m \"Updated via notebook\" --dir-mode zip -d","metadata":{},"execution_count":null,"outputs":[]}]}