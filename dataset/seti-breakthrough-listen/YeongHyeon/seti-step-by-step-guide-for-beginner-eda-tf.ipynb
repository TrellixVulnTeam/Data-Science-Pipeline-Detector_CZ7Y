{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0. Introduction\n\nWelcome to the competition <a href=\"https://www.kaggle.com/c/seti-breakthrough-listen/overview\">'SETI Breakthrough Listen - E.T. Signal Search'</a>!  \n\nAlso, welcome to this source code.  \nThis source code is constructed for the following goals.  \n* Exploratory Data Analysis (EDA)  \n* Make it possible for users to simply and easily submit results.  \n\nTry this source code and upvote if you like it!</br>\nHave a nice day and good luck to you.  \n","metadata":{}},{"cell_type":"markdown","source":"# 1. Preparation  \nIn this section, we will prepare some of the python packages and define some of the python custom functions.","metadata":{}},{"cell_type":"code","source":"!pip install whiteboxlayer==0.1.2","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-02T15:16:47.929315Z","iopub.execute_input":"2021-06-02T15:16:47.929659Z","iopub.status.idle":"2021-06-02T15:17:45.99487Z","shell.execute_reply.started":"2021-06-02T15:16:47.929586Z","shell.execute_reply":"2021-06-02T15:17:45.993959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, glob, random\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport whiteboxlayer.layers as lay\n\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:17:45.996612Z","iopub.execute_input":"2021-06-02T15:17:45.996945Z","iopub.status.idle":"2021-06-02T15:17:48.185706Z","shell.execute_reply.started":"2021-06-02T15:17:45.996913Z","shell.execute_reply":"2021-06-02T15:17:48.184584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sorted_list(path):\n    \n    tmplist = glob.glob(path)\n    tmplist.sort()\n    \n    return tmplist\n\ndef read_csv(path):\n    \n    return pd.read_csv(path)\n\ndef min_max_norm(data):\n    \n    return (data - data.min()) / (data.max() - data.min())\n\ndef plot_cadenece(cadence):\n    # 6 positions of the cadence\n    \n    plt.figure(figsize=(12, 6))\n    \n    for idx_c in range(cadence.shape[0]):\n        try: cadence_tot = np.append(cadence_tot, cadence[idx_c], axis=0)\n        except: cadence_tot = cadence[idx_c]\n    plt.imshow(min_max_norm(cadence_tot).astype(np.float32).T)\n    for idx_c in range(cadence.shape[0]):\n        plt.axvline(idx_c*cadence[idx_c].shape[0], color='white', linestyle='--', linewidth=0.5, alpha=0.5)\n    \n    plt.tight_layout()    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:17:48.187707Z","iopub.execute_input":"2021-06-02T15:17:48.188034Z","iopub.status.idle":"2021-06-02T15:17:48.210928Z","shell.execute_reply.started":"2021-06-02T15:17:48.187999Z","shell.execute_reply":"2021-06-02T15:17:48.208982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. EDA","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Confirm Given Dataset","metadata":{}},{"cell_type":"code","source":"\"\"\" Step 1\nFind out given dataset\"\"\"\n\nsorted_list(os.path.join('../input/seti-breakthrough-listen', '*'))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:17:48.214014Z","iopub.execute_input":"2021-06-02T15:17:48.214874Z","iopub.status.idle":"2021-06-02T15:17:48.23435Z","shell.execute_reply.started":"2021-06-02T15:17:48.214834Z","shell.execute_reply":"2021-06-02T15:17:48.233584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Step 2\nDefine the path to call the each file\"\"\"\n\npath_df = '../input/seti-breakthrough-listen/train_labels.csv'\npath_tr = '../input/seti-breakthrough-listen/train'\npath_te = '../input/seti-breakthrough-listen/test'\npath_sb = '../input/seti-breakthrough-listen/sample_submission.csv'","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:17:48.237332Z","iopub.execute_input":"2021-06-02T15:17:48.237926Z","iopub.status.idle":"2021-06-02T15:17:48.244869Z","shell.execute_reply.started":"2021-06-02T15:17:48.237886Z","shell.execute_reply":"2021-06-02T15:17:48.24389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Confirm Training Set","metadata":{}},{"cell_type":"code","source":"\"\"\" Step 1\nLoad training set as a dataframe.\nThen, confirm the shape of training set.\"\"\"\n\ndf_tr = read_csv(path=path_df)\nprint(\"Shape of Training Set:\", df_tr.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:17:48.246246Z","iopub.execute_input":"2021-06-02T15:17:48.246757Z","iopub.status.idle":"2021-06-02T15:17:48.311631Z","shell.execute_reply.started":"2021-06-02T15:17:48.246719Z","shell.execute_reply":"2021-06-02T15:17:48.310606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Step 2\nConfirm head of the training set.\"\"\"\n\ndf_tr.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:17:48.315477Z","iopub.execute_input":"2021-06-02T15:17:48.315844Z","iopub.status.idle":"2021-06-02T15:17:48.341774Z","shell.execute_reply.started":"2021-06-02T15:17:48.31581Z","shell.execute_reply":"2021-06-02T15:17:48.341028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Step 3\nConfirm tail of the training set.\"\"\"\n\ndf_tr.tail(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:17:48.347179Z","iopub.execute_input":"2021-06-02T15:17:48.349213Z","iopub.status.idle":"2021-06-02T15:17:48.364127Z","shell.execute_reply.started":"2021-06-02T15:17:48.349173Z","shell.execute_reply":"2021-06-02T15:17:48.363384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Step 4\nConfirm unique label (target column) of the training set.\"\"\"\n\nlist_class = list(set(list(df_tr['target'])))\nprint(\"Class:\", list_class)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:17:48.368513Z","iopub.execute_input":"2021-06-02T15:17:48.370599Z","iopub.status.idle":"2021-06-02T15:17:48.391723Z","shell.execute_reply.started":"2021-06-02T15:17:48.37056Z","shell.execute_reply":"2021-06-02T15:17:48.39076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Step 5\nConfirm number of sample for each unique label.\nWe can confirm that the training set has a highly imbalanced form.\"\"\"\n\ndf_tr0 = df_tr[df_tr['target'] == 0]\ndf_tr1 = df_tr[df_tr['target'] != 0]\nnum_cls0, num_cls1 = df_tr0.shape[0], df_tr1.shape[0]\nprint(\"Class 0: %d\\nClass 1: %d\" %(num_cls0, num_cls1))\nhist = df_tr.hist()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:17:48.396113Z","iopub.execute_input":"2021-06-02T15:17:48.398359Z","iopub.status.idle":"2021-06-02T15:17:48.652961Z","shell.execute_reply.started":"2021-06-02T15:17:48.39832Z","shell.execute_reply":"2021-06-02T15:17:48.652204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Step 6\nConfirm number sub-ID of given dataset.\nGiven dataset inclues 16 sub-IDs.\"\"\"\n\nsorted_list(os.path.join(path_tr, '*'))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:17:48.654061Z","iopub.execute_input":"2021-06-02T15:17:48.654381Z","iopub.status.idle":"2021-06-02T15:17:48.663765Z","shell.execute_reply.started":"2021-06-02T15:17:48.654354Z","shell.execute_reply":"2021-06-02T15:17:48.662878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Step 7\nConfirm number of sample for each sub-ID.\nThe training set includes 16 of unique sub-ID as the shown list 'list_subid'.\nMoreover, we confirm the number of sample for each label for each sub-ID.\"\"\"\n\nlist_subid = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f']\nfor subid in list_subid:\n    df_sub = df_tr[df_tr['id'].str.startswith(subid)]\n    num_cls0, num_cls1 = df_sub[df_sub['target'] == 0].shape[0], df_sub[df_sub['target'] != 0].shape[0]\n    num_tot = df_sub.shape[0]\n    print(\"Sub-ID: %s | Class 0:%d (%.3f%%)  Class 1: %d (%.3f%%)\" %(subid, num_cls0, num_cls0/num_tot*100, num_cls1, num_cls1/num_tot*100))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:17:48.665197Z","iopub.execute_input":"2021-06-02T15:17:48.665576Z","iopub.status.idle":"2021-06-02T15:17:48.978195Z","shell.execute_reply.started":"2021-06-02T15:17:48.665538Z","shell.execute_reply":"2021-06-02T15:17:48.977002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Step 8\nListing the numpy file of the training set.\"\"\"\n\nlist_npy = sorted_list(os.path.join(path_tr, '*', '*.npy'))\nprint(\"Number of npy file: %d\" %(len(list_npy)))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:17:48.979672Z","iopub.execute_input":"2021-06-02T15:17:48.980159Z","iopub.status.idle":"2021-06-02T15:17:50.339232Z","shell.execute_reply.started":"2021-06-02T15:17:48.980119Z","shell.execute_reply":"2021-06-02T15:17:50.338351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Step 9.1\nConfirm the numpy file (includes cadence information) as a image.\nFirstly, this code block presents random samples with label 0.\"\"\"\n\nfor subid in list_subid:\n    terminator = False\n    while(True):\n        idx = random.randint(0, df_tr.shape[0])\n        if(df_tr.iloc[idx]['target'] == 0 and df_tr.iloc[idx]['id'].startswith(subid)): terminator = True \n        else: continue\n        print(\"Index:%d | ID: %s, Target: %d\" %(idx, df_tr.iloc[idx]['id'], df_tr.iloc[idx]['target']))\n        tmp_npy = np.load(list_npy[idx])\n        plot_cadenece(cadence=tmp_npy)\n        if(terminator): break","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:17:50.340532Z","iopub.execute_input":"2021-06-02T15:17:50.341004Z","iopub.status.idle":"2021-06-02T15:17:56.711835Z","shell.execute_reply.started":"2021-06-02T15:17:50.340964Z","shell.execute_reply":"2021-06-02T15:17:56.711036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Step 9.2\nAs 'Step 9.1', this code block presents random samples with label 1.\"\"\"\n\nfor subid in list_subid:\n    terminator = False\n    while(True):\n        idx = random.randint(0, df_tr.shape[0])\n        if(df_tr.iloc[idx]['target'] == 1 and df_tr.iloc[idx]['id'].startswith(subid)): terminator = True \n        else: continue\n        print(\"Index:%d | ID: %s, Target: %d\" %(idx, df_tr.iloc[idx]['id'], df_tr.iloc[idx]['target']))\n        tmp_npy = np.load(list_npy[idx])\n        plot_cadenece(cadence=tmp_npy)\n        if(terminator): break","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:17:56.713081Z","iopub.execute_input":"2021-06-02T15:17:56.713432Z","iopub.status.idle":"2021-06-02T15:18:03.485084Z","shell.execute_reply.started":"2021-06-02T15:17:56.713395Z","shell.execute_reply":"2021-06-02T15:18:03.483555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 Confirm Submission File (for Test)","metadata":{}},{"cell_type":"code","source":"\"\"\" Step 1\nLoad and confirm the submission file.\"\"\"\n\ndf_sb = read_csv(path=path_sb)\nprint(\"Shape of Submission Set:\", df_sb.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:18:03.486482Z","iopub.execute_input":"2021-06-02T15:18:03.486833Z","iopub.status.idle":"2021-06-02T15:18:03.518361Z","shell.execute_reply.started":"2021-06-02T15:18:03.486794Z","shell.execute_reply":"2021-06-02T15:18:03.517312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Step 2\nConfirm head of the submission file.\nAll the 'target' values have assigned with a value of 0.5.\nWe will replace each 'target' value in the test procedure after training the model.\"\"\"\n\ndf_sb.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:18:03.519705Z","iopub.execute_input":"2021-06-02T15:18:03.520053Z","iopub.status.idle":"2021-06-02T15:18:03.530233Z","shell.execute_reply.started":"2021-06-02T15:18:03.520017Z","shell.execute_reply":"2021-06-02T15:18:03.529093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Step 3\nConfirm number of the sample for each sub-ID.\nAlso, the percentage of each sub-ID is shown.\"\"\"\n\nfor subid in list_subid:\n    df_sub = df_sb[df_sb['id'].str.startswith(subid)]\n    num_sub, num_tot = df_sub.shape[0], df_sb.shape[0]\n    print(\"Sub-ID: %s | %d (%.3f%%)\" %(subid, num_sub, num_sub/num_tot*100))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:18:03.531751Z","iopub.execute_input":"2021-06-02T15:18:03.53237Z","iopub.status.idle":"2021-06-02T15:18:03.7529Z","shell.execute_reply.started":"2021-06-02T15:18:03.53233Z","shell.execute_reply":"2021-06-02T15:18:03.751968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Step 4\nPresents random samples of test set.\"\"\"\n\nfor _ in range(5):\n    idx = random.randint(0, df_sb.shape[0])\n    print(\"Index:%d | ID: %s, Target: %d\" %(idx, df_sb.iloc[idx]['id'], df_sb.iloc[idx]['target']))\n    tmp_npy = np.load(list_npy[idx])\n    plot_cadenece(cadence=tmp_npy)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:18:03.754396Z","iopub.execute_input":"2021-06-02T15:18:03.75478Z","iopub.status.idle":"2021-06-02T15:18:05.840693Z","shell.execute_reply.started":"2021-06-02T15:18:03.754742Z","shell.execute_reply":"2021-06-02T15:18:05.839936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Training","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Preparing for Training","metadata":{}},{"cell_type":"code","source":"\"\"\" Step 1\nConvert list of npy files to list of id.\"\"\"\n\nlist_npy = sorted_list(os.path.join(path_tr, '*', '*.npy'))\nlist_npy.extend(sorted_list(os.path.join(path_te, '*', '*.npy')))\n\nlist_id = [] # list_npy\nfor path_npy in list_npy:\n    list_id.append(path_npy.split('/')[-1].replace('.npy', ''))\n    \npath_npy = list_npy[list_id.index('%s' %(df_tr.iloc[idx]['id']))]\nprint(df_tr.iloc[idx]['id'], path_npy)\n\nsample = np.load(path_npy)\nprint(sample.shape)\n[h, w, c] = np.transpose(sample, [1, 2, 0]).shape\nprint(h, w, c)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:18:05.84193Z","iopub.execute_input":"2021-06-02T15:18:05.842265Z","iopub.status.idle":"2021-06-02T15:18:06.68848Z","shell.execute_reply.started":"2021-06-02T15:18:05.842226Z","shell.execute_reply":"2021-06-02T15:18:06.687479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Preparing the neural network","metadata":{}},{"cell_type":"code","source":"class Agent(object):\n\n    def __init__(self, **kwargs):\n\n        print(\"\\nInitializing Neural Network...\")\n\n        self.dim_h = kwargs['dim_h']\n        self.dim_w = kwargs['dim_w']\n        self.dim_c = kwargs['dim_c']\n        self.num_class = kwargs['num_class']\n        self.learning_rate = kwargs['learning_rate']\n        self.path_ckpt = kwargs['path_ckpt']\n\n        self.variables = {}\n\n        dummy = tf.zeros((1, self.dim_h, self.dim_w, self.dim_c), dtype=tf.float32)\n        self.__model = Neuralnet(**kwargs)\n        self.__model.forward(x=dummy, verbose=True)\n        print(\"\\nNum Parameter: %d\" %(self.__model.layer.num_params))\n\n        self.__init_propagation(path=self.path_ckpt)\n\n    def __init_propagation(self, path):\n\n        self.summary_writer = tf.summary.create_file_writer(self.path_ckpt)\n\n        self.variables['trainable'] = []\n        ftxt = open(\"list_parameters.txt\", \"w\")\n        for key in list(self.__model.layer.parameters.keys()):\n            trainable = self.__model.layer.parameters[key].trainable\n            text = \"T: \" + str(key) + str(self.__model.layer.parameters[key].shape)\n            if(trainable):\n                self.variables['trainable'].append(self.__model.layer.parameters[key])\n            ftxt.write(\"%s\\n\" %(text))\n        ftxt.close()\n\n        self.optimizer = tf.optimizers.Adam(learning_rate=self.learning_rate)\n        self.save_params()\n\n        conc_func = self.__model.__call__.get_concrete_function(\\\n            tf.TensorSpec(shape=(1, self.dim_h, self.dim_w, self.dim_c), dtype=tf.float32))\n        self.__get_flops(conc_func)\n\n    def __loss(self, y, y_hat):\n\n        entropy_b = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_hat)\n        entropy = tf.math.reduce_mean(entropy_b)\n\n        return {'entropy_b': entropy_b, 'entropy': entropy}\n\n    @tf.autograph.experimental.do_not_convert\n    def step(self, minibatch, iteration=0, training=False):\n\n        x = minibatch['x']\n        y = minibatch['y']\n\n        with tf.GradientTape() as tape:\n            logit, y_hat = self.__model.forward(x=x, verbose=False)\n            losses = self.__loss(y=y, y_hat=logit)\n\n        if(training):\n            gradients = tape.gradient(losses['entropy'], self.variables['trainable'])\n            self.optimizer.apply_gradients(zip(gradients, self.variables['trainable']))\n\n            with self.summary_writer.as_default():\n                tf.summary.scalar('%s/entropy' %(self.__model.who_am_i), losses['entropy'], step=iteration)\n\n        return {'y_hat':y_hat, 'losses':losses}\n\n    def __get_flops(self, conc_func):\n\n        frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(conc_func)\n\n        with tf.Graph().as_default() as graph:\n            tf.compat.v1.graph_util.import_graph_def(graph_def, name='')\n\n            run_meta = tf.compat.v1.RunMetadata()\n            opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n            flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\n\n            flop_tot = flops.total_float_ops\n            ftxt = open(\"flops.txt\", \"w\")\n            for idx, name in enumerate(['', 'K', 'M', 'G', 'T']):\n                text = '%.3f [%sFLOPS]' %(flop_tot/10**(3*idx), name)\n                print(text)\n                ftxt.write(\"%s\\n\" %(text))\n            ftxt.close()\n\n    def save_params(self, model='base', tflite=False):\n\n        if(tflite):\n            # https://github.com/tensorflow/tensorflow/issues/42818\n            conc_func = self.__model.__call__.get_concrete_function(\\\n                tf.TensorSpec(shape=(1, self.dim_h, self.dim_w, self.dim_c), dtype=tf.float32))\n            converter = tf.lite.TFLiteConverter.from_concrete_functions([conc_func])\n\n            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n            converter.experimental_new_converter = True\n            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n\n            tflite_model = converter.convert()\n\n            with open('model.tflite', 'wb') as f:\n                f.write(tflite_model)\n        else:\n            vars_to_save = self.__model.layer.parameters.copy()\n            vars_to_save[\"optimizer\"] = self.optimizer\n\n            ckpt = tf.train.Checkpoint(**vars_to_save)\n            ckptman = tf.train.CheckpointManager(ckpt, directory=os.path.join(self.path_ckpt, model), max_to_keep=1)\n            ckptman.save()\n\n    def load_params(self, model):\n\n        vars_to_load = self.__model.layer.parameters.copy()\n        vars_to_load[\"optimizer\"] = self.optimizer\n\n        ckpt = tf.train.Checkpoint(**vars_to_load)\n        latest_ckpt = tf.train.latest_checkpoint(os.path.join(self.path_ckpt, model))\n        status = ckpt.restore(latest_ckpt)\n        status.expect_partial()\n\nclass Neuralnet(tf.Module):\n\n    def __init__(self, **kwargs):\n        super(Neuralnet, self).__init__()\n\n        self.who_am_i = \"NN\"\n        self.dim_h = kwargs['dim_h']\n        self.dim_w = kwargs['dim_w']\n        self.dim_c = kwargs['dim_c']\n        self.num_class = kwargs['num_class']\n        self.filters = [self.dim_c, 8, 16, 32, 64, 64]\n\n        self.layer = lay.Layers()\n\n        self.forward = tf.function(self.__call__)\n\n    @tf.function\n    def __call__(self, x, verbose=False):\n\n        # origin deco: @tf.function\n        # @tf.autograph.experimental.do_not_convert\n        logit = self.__nn(x=x, name='nn', verbose=verbose)\n        y_hat = tf.nn.softmax(logit, name=\"y_hat\") \n\n        return logit, y_hat\n\n    def __nn(self, x, name='nn', verbose=True):\n\n        for idx, _ in enumerate(self.filters):\n            if(idx == 0): continue\n            x = self.layer.conv2d(x=x, stride=1, \\\n                filter_size=[3, 3, self.filters[idx-1], self.filters[idx]], dilations=[1, 1, 1, 1], \\\n                padding='VALID', batch_norm=False, activation='relu', name='%s-%d_1' %(name, idx), verbose=verbose)\n            x = self.layer.conv2d(x=x, stride=2, \\\n                filter_size=[3, 3, self.filters[idx], self.filters[idx]], dilations=[1, 1, 1, 1], \\\n                padding='VALID', batch_norm=False, activation='relu', name='%s-%d_2' %(name, idx), verbose=verbose)\n        \n        x = tf.reshape(x, shape=[x.shape[0], -1], name=\"flat\")\n        x = self.layer.fully_connected(x=x, c_out=128, \\\n                batch_norm=False, activation='relu', name=\"%s-clf0\" %(name), verbose=verbose)\n        x = self.layer.fully_connected(x=x, c_out=self.num_class, \\\n                batch_norm=False, activation=None, name=\"%s-clf1\" %(name), verbose=verbose)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:18:06.689972Z","iopub.execute_input":"2021-06-02T15:18:06.690546Z","iopub.status.idle":"2021-06-02T15:18:06.729422Z","shell.execute_reply.started":"2021-06-02T15:18:06.690494Z","shell.execute_reply":"2021-06-02T15:18:06.728451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Step 1\nGPU setting.\"\"\"\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        print(e)\n\n\"\"\" Step 2\nInitializing Neural Network.\"\"\"\nagent = Agent(\\\n    dim_h = h, \\\n    dim_w = w, \\\n    dim_c = c, \\\n    num_class = 2, \\\n    learning_rate = 5e-4, \\\n    path_ckpt = 'Checkpoint')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:18:06.730792Z","iopub.execute_input":"2021-06-02T15:18:06.731191Z","iopub.status.idle":"2021-06-02T15:18:07.767945Z","shell.execute_reply.started":"2021-06-02T15:18:06.731152Z","shell.execute_reply":"2021-06-02T15:18:07.76687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Training Iteration","metadata":{}},{"cell_type":"code","source":"def next_batch(df, idx, batch_size):\n    \n    batch_x, batch_y, batch_id, terminate = [], [], [], False\n    while(True):\n        \n        try: path_npy = list_npy[list_id.index('%s' %(df.iloc[idx]['id']))]\n        except: \n            idx = 0\n            terminate = True\n            break\n        else:\n            tmp_x = np.load(path_npy)\n            batch_x.append(np.transpose(sample, [1, 2, 0]))\n\n            try:\n                if(df.iloc[idx]['target'] == 0): batch_y.append(np.diag(np.ones(2))[0])\n                else: batch_y.append(np.diag(np.ones(2))[1])\n            except: \n                batch_y.append(np.diag(np.ones(2))[0])\n            \n            batch_id.append(df.iloc[idx]['id'])\n            \n            idx += 1\n            if(len(batch_x) == batch_size): break\n    \n    batch_x = np.asarray(batch_x)\n    batch_y = np.asarray(batch_y)\n    \n    return {'x':batch_x.astype(np.float32), 'y':batch_y.astype(np.float32), 'id':batch_id, 'terminate':terminate, 'idx':idx}","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:18:07.771305Z","iopub.execute_input":"2021-06-02T15:18:07.771559Z","iopub.status.idle":"2021-06-02T15:18:07.780375Z","shell.execute_reply.started":"2021-06-02T15:18:07.771532Z","shell.execute_reply":"2021-06-02T15:18:07.779537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Note that, This example does not fully complete the learning. \nLearning is up to you. Good luck! \"\"\"\nepochs, batch_size = 10, 32\niteration = 0\nfor epoch in range(epochs):\n    \n    idx, list_loss = 0, []\n    while(True):\n        minibatch = next_batch(df=df_tr, idx=idx, batch_size=batch_size)\n        idx, terminate = minibatch['idx'], minibatch['terminate']\n        \n        step_dict = agent.step(minibatch=minibatch, iteration=iteration, training=True)\n        list_loss.append(step_dict['losses']['entropy'])\n        if(iteration % 100 == 0): print(\" Iteration %10d | Loss: %.5f\" %(iteration, step_dict['losses']['entropy']))\n        iteration += 1\n        \n        del minibatch\n        if(terminate): break\n        \n    loss = np.average(np.asarray(list_loss))\n    print(\"Epoch [%d / %d] | Loss: %.5f\" %(epoch, epochs, loss))\n    \n    agent.save_params(model='model_0')\nagent.save_params(tflite=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:18:07.783319Z","iopub.execute_input":"2021-06-02T15:18:07.783809Z","iopub.status.idle":"2021-06-02T15:18:11.481547Z","shell.execute_reply.started":"2021-06-02T15:18:07.783771Z","shell.execute_reply":"2021-06-02T15:18:11.480639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Test","metadata":{}},{"cell_type":"code","source":"idx = 0\nwhile(True):\n    minibatch = next_batch(df=df_sb, idx=idx, batch_size=batch_size)\n    idx = minibatch['idx']\n    if(idx % (batch_size*20) == 0): print(\"Progress [%d/%d] (%.2f%%)\" %(idx, df_sb.shape[0], idx/df_sb.shape[0]))\n\n    step_dict = agent.step(minibatch=minibatch, training=False)\n\n    for idx_id, tmp_id in enumerate(minibatch['id']):\n        df_sb.loc[df_sb['id'] == tmp_id, 'target'] = np.argmax(step_dict['y_hat'][idx_id])\n\n    if(minibatch['terminate']): break","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:18:11.482826Z","iopub.execute_input":"2021-06-02T15:18:11.483339Z","iopub.status.idle":"2021-06-02T15:44:33.719964Z","shell.execute_reply.started":"2021-06-02T15:18:11.483297Z","shell.execute_reply":"2021-06-02T15:44:33.718297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Confirm the histogram of answer after test process. \"\"\"\nhist = df_sb['target'].hist()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:44:33.727147Z","iopub.execute_input":"2021-06-02T15:44:33.727436Z","iopub.status.idle":"2021-06-02T15:44:33.925017Z","shell.execute_reply.started":"2021-06-02T15:44:33.727408Z","shell.execute_reply":"2021-06-02T15:44:33.924189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Make Submission","metadata":{}},{"cell_type":"code","source":"df_sb.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:44:33.926234Z","iopub.execute_input":"2021-06-02T15:44:33.926559Z","iopub.status.idle":"2021-06-02T15:44:35.026616Z","shell.execute_reply.started":"2021-06-02T15:44:33.926531Z","shell.execute_reply":"2021-06-02T15:44:35.025768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}