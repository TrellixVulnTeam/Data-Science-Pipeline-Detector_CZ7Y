{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport shutil\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport itertools\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch import nn, utils, optim\nfrom torchvision import transforms, models\nsys.path.append(\"../input/efficientnet-pytorch/EfficientNet-PyTorch/EfficientNet-PyTorch-master\")\nfrom efficientnet_pytorch import model as enet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initial settings","metadata":{}},{"cell_type":"code","source":"# Be deterministic \ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Since it is learned multiple times when tuning parameters,\n# make it a function so that it can be initialized with the same seed each time. \ndef init_seed():\n    np.random.seed(0)\n    torch.manual_seed(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initial settings\nUSE_DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nBATCH_SIZE = 16\nBATCH_SIZE_VALID = 4\nNUM_WORKERS = 2\nNUM_EPOCHS = 3 # epochs for test to find parameter\nLR_TESTS = [5e-3,1e-4,5e-4] # Find the best parameter while changing the learning rate\nWEIGHT_TESTS = [0.5,0.7,0.9] # Try weighted classification weights-unbalanced dataset with more than 1 than 0 \nUSE_TRAIN_SUBSET = True # for test run","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deta model with DA","metadata":{}},{"cell_type":"code","source":"df_origin = pd.read_csv(\"../input/seti-breakthrough-listen/train_labels.csv\")\ndf_test = pd.concat([df_origin[df_origin.target==1][:500], df_origin[df_origin.target==0][:500]])\ndf_train = pd.concat([df_origin[df_origin.target==1][500:], df_origin[df_origin.target==0][500:]])\ndf_valid = pd.read_csv(\"../input/seti-breakthrough-listen/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset:\n    def __init__(self, test=False, valid=False):\n        # Read Training file\n        df = df_valid if valid else (df_test if test else df_train)\n        if (not valid) and USE_TRAIN_SUBSET:\n            _, df = train_test_split(df, test_size=0.1, random_state=0)\n        self.df = df\n        self.valid = valid # is prediction?\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, pos):\n        loc = self.df.iloc[pos]\n        _id = loc[\"id\"]\n        # read signal data\n        if self.valid:\n            fn = f\"../input/seti-breakthrough-listen/test/{_id[0]}/{_id}.npy\"\n        else:\n            fn = f\"../input/seti-breakthrough-listen/train/{_id[0]}/{_id}.npy\"\n        # target value\n        lb = int(loc[\"target\"])\n        arr = np.load(fn) # read signal\n        if not self.valid:\n            if np.random.random() < 0.2: # DA\n                # Since the horizontal direction is the time axis, rotate the time and shift the position. \n                pos = np.random.randint(arr.shape[2]-50)+50\n                X = arr.copy()\n                p = np.array([[np.linspace(0,arr[i,j,-1]-arr[i,j,-pos],pos) for j in range(arr.shape[1])] for i in range(arr.shape[0])])\n                q = np.array([[np.linspace(0,arr[i,j,pos]-arr[i,j,0],arr.shape[2]-pos) for j in range(arr.shape[1])] for i in range(arr.shape[0])])\n                X[:,:,0:pos] = arr[:,:,-pos:] - p\n                X[:,:,pos:] = arr[:,:,:-pos] - q\n                arr = X\n        return torch.tensor(arr, dtype=torch.float32), torch.tensor(lb, dtype=torch.int64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pytorch model","metadata":{}},{"cell_type":"code","source":"def get_model():\n    # modify output classes=2\n    model = enet.EfficientNet.from_name('efficientnet-b0')\n    model.load_state_dict(torch.load('../input/efficientnet-pytorch/efficientnet-b0-08094119.pth'))\n    model._conv_stem = nn.Conv2d(6, 32, kernel_size=3, bias=False)\n    model._fc = nn.Linear(1280, 2)\n    model = model.to(USE_DEVICE)\n    return model\n\ndef get_optim(model, lr):\n    params = model.parameters()\n    optimizer = optim.Adam(params, lr=lr)\n    return optimizer\n\ndef get_loss(weight):\n    # Loss function for classification with weights for unbalanced datasets \n    weight = torch.tensor([1.0-weight,weight], dtype=torch.float)\n    weight = weight.to(USE_DEVICE)\n    loss = nn.CrossEntropyLoss(weight=weight)\n    return loss\n\ndef get_score(true_valid, pred_valid):\n    # RocAUC Score\n    return roc_auc_score(true_valid, pred_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make dataset","metadata":{}},{"cell_type":"code","source":"train_ds = MyDataset(test=False)\ntest_ds = MyDataset(test=True)\ndata_loader = utils.data.DataLoader(\n    train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\ndata_loader_v = utils.data.DataLoader(\n    test_ds, batch_size=BATCH_SIZE_VALID, shuffle=False, num_workers=NUM_WORKERS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training with find best parameter","metadata":{}},{"cell_type":"code","source":"best_scores = [] # score of best epoch in each parameter set\n\nif not os.path.isdir('tmp'):\n    os.mkdir('tmp')\n\nfor t, (lr, weight) in enumerate(itertools.product(LR_TESTS, WEIGHT_TESTS)):\n    init_seed() # initialized with the same seed each time. \n    model = get_model() # get model\n\n    optimizer = get_optim(model, lr)\n    loss = get_loss(weight) # get weighted loss function\n\n    print(f'test #{t} lr={lr} weight={weight}') # try train&test\n    scores = []\n\n    for epoch in tqdm(range(NUM_EPOCHS)):\n        total_loss = []\n        model.train() # make model for train\n        \n        # train\n        for X, y in data_loader:\n            X = X.to(USE_DEVICE)\n            y = y.to(USE_DEVICE)\n\n            losses = loss(model(X), y)\n\n            optimizer.zero_grad()\n            losses.backward()\n            optimizer.step()\n\n            total_loss.append(losses.detach().cpu().numpy())\n\n        # test\n        with torch.no_grad():\n            total_loss_v = []\n            true_valid = []\n            pred_valid = []\n\n            model.eval() # make model for test\n            for i, (X, y) in enumerate(data_loader_v):\n                X = X.to(USE_DEVICE)\n                y = y.to(USE_DEVICE)\n\n                res = model(X)\n                losses = loss(res, y)\n\n                y = y.detach().cpu().numpy()\n                true_valid.extend(y.tolist())\n\n                res = torch.softmax(res, axis=1)\n                res = res.detach().cpu().numpy()\n                pred_valid.extend(res[:,1].tolist())\n\n                total_loss_v.append(losses.detach().cpu().numpy())\n\n        # Run tests for every epoch and use the one with the best epoch \n        total_loss = np.mean(total_loss)\n        total_loss_v = np.mean(total_loss_v)\n        score = get_score(true_valid, pred_valid)\n        scores.append(score) # scores in this parameter set\n        print(f'epoch #{epoch}: train_loss:{total_loss} valid_loss:{total_loss_v} score:{score}')\n        torch.save(model.state_dict(), f'tmp/checkpoint{epoch}.pth') # save model\n\n    # The best epoch for this parameter \n    best_epoch = np.argmax(scores)\n    shutil.copyfile(f'tmp/checkpoint{best_epoch}.pth',f'tmp/test{t}_best.pth')\n    best_scores.append(scores[best_epoch])\n\n    del model, optimizer, loss, X, y, res, losses\n    torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_ds, test_ds, data_loader, data_loader_v","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load best parameter-trained model","metadata":{}},{"cell_type":"code","source":"best_of_best = np.argmax(best_scores) # best epoch in best parameter\nmodel_name = f'tmp/test{best_of_best}_best.pth'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validmodel = get_model()\nvalidmodel.load_state_dict(torch.load(model_name, map_location=torch.device(USE_DEVICE)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Submission","metadata":{}},{"cell_type":"code","source":"# Make test data\nvalid_ds = MyDataset(test=False, valid=True)\ndata_loader_v = utils.data.DataLoader(\n    valid_ds, batch_size=BATCH_SIZE_VALID, shuffle=False, num_workers=NUM_WORKERS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction\nwith torch.no_grad():\n    pred_valid = []\n\n    validmodel.eval()\n    for i, (X, y) in tqdm(enumerate(data_loader_v), total=len(data_loader_v)):\n        X = X.to(USE_DEVICE)\n        y = y.to(USE_DEVICE)\n\n        res = validmodel(X)\n        res = torch.softmax(res, axis=1)\n        res = res.detach().cpu().numpy()\n        pred_valid.extend(res[:,1].tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Submission file","metadata":{}},{"cell_type":"code","source":"csv = \"../input/seti-breakthrough-listen/sample_submission.csv\"\ndf = pd.read_csv(csv)\ndf[\"target\"] = pred_valid\ndf.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training All Data in find parameters","metadata":{}},{"cell_type":"code","source":"df_train = df_origin # train all data\nUSE_TRAIN_SUBSET = False # train all data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = MyDataset(test=False)\ndata_loader = utils.data.DataLoader(\n    train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 0.0005 # find parameter\nweight = 0.5 # find parameter\nNUM_EPOCHS = 3 # find parameter\n\ninit_seed() # initialized with the same seed each time. \nmodel = get_model() # get model\n\noptimizer = get_optim(model, lr)\nloss = get_loss(weight) # get weighted loss function\n\nprint(f'train again lr={lr} weight={weight}') # train in find parameters\nscores = []\n\nfor epoch in tqdm(range(NUM_EPOCHS)):\n    total_loss = []\n    model.train() # make model for train\n\n    # train\n    for X, y in data_loader:\n        X = X.to(USE_DEVICE)\n        y = y.to(USE_DEVICE)\n\n        losses = loss(model(X), y)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Submission file","metadata":{}},{"cell_type":"code","source":"validmodel = model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction\nwith torch.no_grad():\n    pred_valid = []\n\n    validmodel.eval()\n    for i, (X, y) in tqdm(enumerate(data_loader_v), total=len(data_loader_v)):\n        X = X.to(USE_DEVICE)\n        y = y.to(USE_DEVICE)\n\n        res = validmodel(X)\n        res = torch.softmax(res, axis=1)\n        res = res.detach().cpu().numpy()\n        pred_valid.extend(res[:,1].tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv = \"../input/seti-breakthrough-listen/sample_submission.csv\"\ndf = pd.read_csv(csv)\ndf[\"target\"] = pred_valid\ndf.to_csv(\"submission2.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}