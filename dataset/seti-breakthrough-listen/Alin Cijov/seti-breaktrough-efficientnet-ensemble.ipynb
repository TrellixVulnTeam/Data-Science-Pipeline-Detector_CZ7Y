{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div>\n    <img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/23652/logos/header.png\" />\n</div>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nimport sys\n\neffnet_base_path = '../input/efficientnet-pytorch/EfficientNet-PyTorch/EfficientNet-PyTorch-master'\n\nsys.path = [effnet_base_path,] + sys.path\n\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nfrom efficientnet_pytorch import model as enet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-05-22T00:08:28.485743Z","iopub.execute_input":"2021-05-22T00:08:28.486031Z","iopub.status.idle":"2021-05-22T00:08:29.670546Z","shell.execute_reply.started":"2021-05-22T00:08:28.485961Z","shell.execute_reply":"2021-05-22T00:08:29.669743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"dataset\" style=\"color:#fefd1c; background:#0e1b1f; border:1px dotted #fefd1c;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"code","source":"class ClassificationDataset:\n    \n    def __init__(self, image_paths, targets): \n        self.image_paths = image_paths\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, item):      \n        image = np.load(self.image_paths[item]).astype(float)\n\n        targets = self.targets[item]\n                \n        return {\n            \"image\": torch.tensor(image, dtype=torch.float),\n            \"targets\": torch.tensor(targets, dtype=torch.long),\n        }","metadata":{"execution":{"iopub.status.busy":"2021-05-22T00:08:29.673083Z","iopub.execute_input":"2021-05-22T00:08:29.673435Z","iopub.status.idle":"2021-05-22T00:08:29.680432Z","shell.execute_reply.started":"2021-05-22T00:08:29.673396Z","shell.execute_reply":"2021-05-22T00:08:29.678426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/seti-breakthrough-listen/train_labels.csv')\ndf['img_path'] = df['id'].apply(lambda x: f'../input/seti-breakthrough-listen/train/{x[0]}/{x}.npy')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T00:08:29.682278Z","iopub.execute_input":"2021-05-22T00:08:29.682815Z","iopub.status.idle":"2021-05-22T00:08:29.750221Z","shell.execute_reply.started":"2021-05-22T00:08:29.682772Z","shell.execute_reply":"2021-05-22T00:08:29.749426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"model\" style=\"color:#fefd1c; background:#0e1b1f; border:1px dotted #fefd1c;\"> \n    <center>Model\n        <a class=\"anchor-link\" href=\"#model\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"code","source":"class enetv2(nn.Module):\n    def __init__(self, backbone, out_dim):\n        super(enetv2, self).__init__()\n        self.enet = enet.EfficientNet.from_name(backbone)\n        self.enet.load_state_dict(torch.load(pretrained_models[backbone]))\n        self.myfc = nn.Linear(self.enet._fc.in_features, out_dim)\n        self.enet._fc = nn.Identity()\n        self.conv1 = nn.Conv2d(6, 3, kernel_size=3, stride=1, padding=3, bias=False)\n\n    def extract(self, x):\n        return self.enet(x)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.extract(x)\n        x = self.myfc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-05-22T00:08:40.522972Z","iopub.execute_input":"2021-05-22T00:08:40.523291Z","iopub.status.idle":"2021-05-22T00:08:40.530025Z","shell.execute_reply.started":"2021-05-22T00:08:40.523258Z","shell.execute_reply":"2021-05-22T00:08:40.528975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"load\" style=\"color:#fefd1c; background:#0e1b1f; border:1px dotted #fefd1c;\"> \n    <center>Load Models\n        <a class=\"anchor-link\" href=\"#load\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"markdown","source":"## Weights","metadata":{}},{"cell_type":"code","source":"pretrained_models = {\n    'efficientnet-b0': '../input/efficientnet-pytorch/efficientnet-b0-08094119.pth',\n    'efficientnet-b1': '../input/efficientnet-pytorch/efficientnet-b1-dbc7070a.pth',\n    'efficientnet-b2': '../input/efficientnet-pytorch/efficientnet-b2-27687264.pth',\n    'efficientnet-b3': '../input/efficientnet-pytorch/efficientnet-b3-c8376fa2.pth',\n    'efficientnet-b4': '../input/efficientnet-pytorch/efficientnet-b4-e116e8b3.pth',\n    'efficientnet-b5': '../input/efficientnet-pytorch/efficientnet-b5-586e6cc6.pth',\n    'efficientnet-b6': '../input/efficientnet-pytorch/efficientnet-b6-c76e70fd.pth',\n}\n\npretrained_weights = {\n    'efficientnet-b0': '../input/seti-effnet/efficientnet-b0-4.pt',\n    'efficientnet-b1': '../input/seti-effnet/efficientnet-b1-4.pt',\n    'efficientnet-b2': '../input/seti-effnet/efficientnet-b2-4.pt',\n    'efficientnet-b3': '../input/seti-effnet/efficientnet-b3-4.pt',\n    'efficientnet-b4': '../input/seti-effnet/efficientnet-b4-4.pt',\n    'efficientnet-b5': '../input/seti-effnet/efficientnet-b5-4.pt',\n    'efficientnet-b6': '../input/seti-effnet/efficientnet-b6-4.pt',\n}","metadata":{"execution":{"iopub.status.busy":"2021-05-22T00:08:43.751667Z","iopub.execute_input":"2021-05-22T00:08:43.752005Z","iopub.status.idle":"2021-05-22T00:08:43.756891Z","shell.execute_reply.started":"2021-05-22T00:08:43.751971Z","shell.execute_reply":"2021-05-22T00:08:43.755838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Models","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodels = []\nfor key in pretrained_models.keys():\n    model = enetv2(key, out_dim=1)\n    model.load_state_dict(torch.load(pretrained_weights[key], map_location=torch.device(device)))\n    models.append(model.to(device))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T00:10:18.40946Z","iopub.execute_input":"2021-05-22T00:10:18.409847Z","iopub.status.idle":"2021-05-22T00:10:22.953435Z","shell.execute_reply.started":"2021-05-22T00:10:18.409807Z","shell.execute_reply":"2021-05-22T00:10:22.95244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"predict\" style=\"color:#fefd1c; background:#0e1b1f; border:1px dotted #fefd1c;\"> \n    <center>Predictions\n        <a class=\"anchor-link\" href=\"#predict\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"code","source":"def predict(data_loader, model, device):\n    model.eval()\n    \n    final_targets = []\n    final_outputs = []\n    \n    with torch.no_grad():\n        \n        for data in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n            inputs = data[\"image\"]\n            targets = data[\"targets\"]\n            inputs = inputs.to(device, dtype=torch.float)\n            targets = targets.to(device, dtype=torch.float)\n            \n            output = model(inputs)\n            \n            targets = targets.detach().cpu().numpy().tolist()\n            output = output.detach().cpu().numpy().tolist()\n            \n            final_targets.extend(targets)\n            final_outputs.extend(output)\n            \n    return final_outputs, final_targets","metadata":{"execution":{"iopub.status.busy":"2021-05-22T00:10:25.782694Z","iopub.execute_input":"2021-05-22T00:10:25.783039Z","iopub.status.idle":"2021-05-22T00:10:25.78924Z","shell.execute_reply.started":"2021-05-22T00:10:25.783001Z","shell.execute_reply":"2021-05-22T00:10:25.788448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare submission dataset","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('../input/seti-breakthrough-listen/sample_submission.csv')\nsubmission['img_path'] = submission['id'].apply(lambda x: f'../input/seti-breakthrough-listen/test/{x[0]}/{x}.npy')\n\ntest_dataset = ClassificationDataset(image_paths=submission.img_path.values, targets=submission.target.values)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T00:10:26.663006Z","iopub.execute_input":"2021-05-22T00:10:26.663318Z","iopub.status.idle":"2021-05-22T00:10:26.707503Z","shell.execute_reply.started":"2021-05-22T00:10:26.663287Z","shell.execute_reply":"2021-05-22T00:10:26.706589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predictions for each model","metadata":{}},{"cell_type":"code","source":"sig = torch.nn.Sigmoid()\nouts = []\nfor model in models:\n    predictions, valid_targets = predict(test_loader, model, device=device)\n    predictions = np.array(predictions)[:, 0]\n    out = sig(torch.from_numpy(predictions))\n    out = out.detach().numpy()\n    outs.append(out)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T00:10:27.8309Z","iopub.execute_input":"2021-05-22T00:10:27.831229Z","iopub.status.idle":"2021-05-22T00:10:38.880658Z","shell.execute_reply.started":"2021-05-22T00:10:27.8312Z","shell.execute_reply":"2021-05-22T00:10:38.87798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take the mean of all predictions\npred = np.mean(np.array(outs), axis=0)\n\nsubmission.target = pred\nsubmission.drop(['img_path'], axis=1, inplace=True)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"references\" style=\"color:#fefd1c; background:#0e1b1f; border:1px dotted #fefd1c;\"> \n    <center>References\n        <a class=\"anchor-link\" href=\"#references\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"markdown","source":"[Byfone training notebook](https://www.kaggle.com/byfone/efficientnetb1-on-seti-breakthrough-listen)","metadata":{}}]}