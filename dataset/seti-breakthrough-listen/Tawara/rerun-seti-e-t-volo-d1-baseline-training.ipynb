{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About\n\nHere I'm trying to train _VOLO_: Vision Outlooker for Visual Recognition.  \nIt is a new vison transformer which achinved good results with fewer parameters than others.   \n\npaper: https://arxiv.org/abs/2106.13112v1  \ncode: https://github.com/sail-sg/volo  \n\nI uploaded official code and pretrained weights on Kaggle Datasets:  \nhttps://www.kaggle.com/ttahara/volo-package\n\nI make some changes for simply training binary classification task.\n\n<br>\nI have prepared the training and inference notebooks because the training time per fold is longer than resnet18d.\n  \n  \nThis is the training notebook for **fold4**.","metadata":{}},{"cell_type":"markdown","source":"### Experimental Settings","metadata":{}},{"cell_type":"markdown","source":"### model\n* backbone: volo_d1\n* head classifier: one linear layer\n* num of input channels: **1**\n\n### data augmentation\n* implemented by [albumentations](https://albumentations.ai/docs/) **except for Mixup**\n* Train\n  * Resize\n  * HorizontalFlip\n  * VerticalFlip\n  * ShiftScaleRotate\n  * RandomResizedCrop\n  * Mixup(alpha=1.0)\n* Val, Test\n  * Resize\n\n### learning settings\n* CV Strategy: Stratified KFold (K=5)\n* max epochs: 30\n* data:\n  * input image size: 1x256x256\n  * batch size: 32\n* loss: [BCEWithLogitsLoss](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss)\n* optimizer: [AdamW](https://pytorch.org/docs/stable/optim.html#torch.optim.AdamW)\n  * weight decay: 5.0e-02\n* learning rate scheduler: [OneCycleLR](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.OneCycleLR) \n  * epochs: 30\n  * max_lr: 5e-5\n  * pct_start: 0.1\n  * anneal_strategy: cos\n  * div_factor: 1.0e+2\n  * final_div_factor: 1\n  \n### NOTE: I use only on-target ('A') observations\n\n```python\nimg = np.load(path)[[0, 2, 4]]          # shape: (3, 273, 256)\nimg = np.vstack(img)                    # shape: (819, 256)\nimg = img.transpose(1, 0)               # shape: (256, 819)\n```","metadata":{}},{"cell_type":"markdown","source":"# Prapere","metadata":{}},{"cell_type":"markdown","source":"## Install","metadata":{}},{"cell_type":"code","source":"%%bash\npip install pytorch-pfn-extras\npip install timm","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-15T12:24:14.093841Z","iopub.execute_input":"2021-07-15T12:24:14.094215Z","iopub.status.idle":"2021-07-15T12:24:36.070094Z","shell.execute_reply.started":"2021-07-15T12:24:14.094138Z","shell.execute_reply":"2021-07-15T12:24:36.069224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport copy\nimport yaml\nimport random\nimport shutil\nimport typing as tp\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.optim import lr_scheduler\nfrom torch.cuda import amp\n\nimport timm\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport pytorch_pfn_extras as ppe\nfrom pytorch_pfn_extras.config import Config\nfrom pytorch_pfn_extras.training import extensions as ppe_exts, triggers as ppe_triggers\n\nsys.path.append(\"../input/volo-package\")\nfrom volo.models import volo_d1, volo_d2, volo_d3, volo_d4, volo_d5  # register models to timm\nfrom volo.utils import load_pretrained_weights as volo_load_weights","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-15T12:24:36.071903Z","iopub.execute_input":"2021-07-15T12:24:36.072283Z","iopub.status.idle":"2021-07-15T12:24:41.627656Z","shell.execute_reply.started":"2021-07-15T12:24:36.072244Z","shell.execute_reply":"2021-07-15T12:24:41.626762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT = Path.cwd().parent\nINPUT = ROOT / \"input\"\nOUTPUT = ROOT / \"output\"\nDATA = INPUT / \"seti-breakthrough-listen\"\nTRAIN = DATA / \"train\"\nTEST = DATA / \"test\"\n\nTMP = ROOT / \"tmp\"\nTMP.mkdir(exist_ok=True)\n\nRANDAM_SEED = 1086\nCLASSES = [\"target\",]\nN_CLASSES = len(CLASSES)\n# FOLDS = [0, 1, 2, 3, 4]\n# N_FOLDS = len(FOLDS)\nFOLDS = [4,]\nN_FOLDS = 5","metadata":{"execution":{"iopub.status.busy":"2021-07-15T12:24:41.630693Z","iopub.execute_input":"2021-07-15T12:24:41.631052Z","iopub.status.idle":"2021-07-15T12:24:41.637122Z","shell.execute_reply.started":"2021-07-15T12:24:41.631022Z","shell.execute_reply":"2021-07-15T12:24:41.635997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read Data, Split folds","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(DATA / \"train_labels.csv\")\nsmpl_sub = pd.read_csv(DATA / \"sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-15T12:24:41.639056Z","iopub.execute_input":"2021-07-15T12:24:41.639731Z","iopub.status.idle":"2021-07-15T12:24:41.747151Z","shell.execute_reply.started":"2021-07-15T12:24:41.639689Z","shell.execute_reply":"2021-07-15T12:24:41.746199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDAM_SEED)\ntrain[\"fold\"] = -1\nfor fold_id, (_, val_idx) in enumerate(skf.split(train[\"id\"], train[\"target\"])):\n    train.loc[val_idx, \"fold\"] = fold_id","metadata":{"execution":{"iopub.status.busy":"2021-07-15T12:24:41.748607Z","iopub.execute_input":"2021-07-15T12:24:41.748998Z","iopub.status.idle":"2021-07-15T12:24:41.785568Z","shell.execute_reply.started":"2021-07-15T12:24:41.748959Z","shell.execute_reply":"2021-07-15T12:24:41.784794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby(\"fold\").agg(total=(\"id\", len), pos=(\"target\", sum))","metadata":{"execution":{"iopub.status.busy":"2021-07-15T12:24:41.786743Z","iopub.execute_input":"2021-07-15T12:24:41.78709Z","iopub.status.idle":"2021-07-15T12:24:41.821992Z","shell.execute_reply.started":"2021-07-15T12:24:41.787057Z","shell.execute_reply":"2021-07-15T12:24:41.820957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ","metadata":{}},{"cell_type":"markdown","source":"## Definition of Model, Dataset, Metric","metadata":{}},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"VOLO_CHECHPOINTS = {\n    \"volo_d1\": \"../input/volo-package/d1_224_84.2.pth.tar\",\n    \"volo_d1-384\": \"../input/volo-package/d1_384_85.2.pth.tar\",\n    \"volo_d2\": \"../input/volo-package/d2_224_85.2.pth.tar\",\n    \"volo_d2-384\": \"../input/volo-package/d2_384_86.0.pth.tar\",\n    \"volo_d3\": \"../input/volo-package/d3_224_85.4.pth.tar\",\n    \"volo_d3-448\": \"../input/volo-package/d3_448_86.3.pth.tar\",\n    \"volo_d4\": \"../input/volo-package/d4_224_85.7.pth.tar\",\n    \"volo_d4-448\": \"../input/volo-package/d4_448_86.79.pth.tar\",\n    \"volo_d5\": \"../input/volo-package/d5_224_86.10.pth.tar\",\n    \"volo_d5-448\": \"../input/volo-package/d5_448_87.0.pth.tar\",\n    \"volo_d5-512\": \"../input/volo-package/d5_512_87.07.pth.tar\",\n}\n\nclass BasicImageModel(nn.Module):\n    \n    def __init__(\n        self, base_name: str, dims_head: tp.List[int],\n        pretrained=False, in_channels: int=3, image_size: int=224 \n    ):\n        \"\"\"Initialize\"\"\"\n        self.base_name = base_name\n        super().__init__()\n        model_name = base_name.split(\"-\")[0]\n        assert timm.is_model(model_name), \"you can use only models in timm.\"\n        \n        if model_name[:4] == \"volo\":\n            base_model = timm.create_model(\n                model_name, img_size=image_size,\n                mix_token=False, return_dense=False, drop_path_rate=0.1)\n            in_features = base_model.head.in_features\n            if pretrained:\n                volo_load_weights(base_model, VOLO_CHECHPOINTS[base_name], strict=False)\n            \n            if in_channels != 3:\n                # # change input channel\n                # # I follow the manner used in timm.\n                first_conv = base_model.patch_embed.conv[0]\n                w_t = first_conv.weight.data  # shape: (out_ch, 3, 7, 7)\n                if in_channels == 1:\n                    new_w_t = w_t.sum(axis=1, keepdims=True)  # shape: (out_ch, 1, 7, 7)\n                else:\n                    n_repeats = (in_channels + 3 - 1) // 3\n                    new_w_t = w_t.repeat((1, n_repeats, 1, 1))\n                    new_w_t = new_w_t[:, :in_channels]\n                    new_w_t = new_w_t * 3 / in_channels  # shape: (out_ch, in_channels, 7, 7)\n\n                first_conv.weight.data = new_w_t\n        else:\n            base_model = timm.create_model(\n                base_name, pretrained=pretrained, in_chans=in_channels)\n            in_features = base_model.num_features\n            print(\"load imagenet pretrained:\", pretrained)\n            \n        base_model.reset_classifier(num_classes=0)\n        self.backbone = base_model\n        print(f\"{base_name}: {in_features}\")\n        \n        # # prepare head clasifier\n        if dims_head[0] is None:\n            dims_head[0] = in_features\n\n        layers_list = []\n        for i in range(len(dims_head) - 2):\n            in_dim, out_dim = dims_head[i: i + 2]\n            layers_list.extend([\n                nn.Linear(in_dim, out_dim),\n                nn.ReLU(), nn.Dropout(0.5),])\n        layers_list.append(\n            nn.Linear(dims_head[-2], dims_head[-1]))\n        self.head = nn.Sequential(*layers_list)\n\n    def forward(self, x):\n        \"\"\"Forward\"\"\"\n        h = self.backbone(x)\n        h = self.head(h)\n        return h","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-15T12:24:41.823472Z","iopub.execute_input":"2021-07-15T12:24:41.82387Z","iopub.status.idle":"2021-07-15T12:24:41.84084Z","shell.execute_reply.started":"2021-07-15T12:24:41.823832Z","shell.execute_reply":"2021-07-15T12:24:41.83969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-03T00:39:25.885443Z","iopub.execute_input":"2021-06-03T00:39:25.885782Z","iopub.status.idle":"2021-06-03T00:39:25.904695Z","shell.execute_reply.started":"2021-06-03T00:39:25.885753Z","shell.execute_reply":"2021-06-03T00:39:25.903901Z"}}},{"cell_type":"code","source":"FilePath = tp.Union[str, Path]\nLabel = tp.Union[int, float, np.ndarray]\n\n\nclass SetiSimpleDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset using 6 channels by stacking them along time-axis\n\n    Attributes\n    ----------\n    paths : tp.Sequence[FilePath]\n        Sequence of path to cadence snippet file\n    labels : tp.Sequence[Label]\n        Sequence of label for cadence snippet file\n    transform: albumentations.Compose\n        composed data augmentations for data\n    \"\"\"\n\n    def __init__(\n        self,\n        paths: tp.Sequence[FilePath],\n        labels: tp.Sequence[Label],\n        transform: A.Compose,\n    ):\n        \"\"\"Initialize\"\"\"\n        self.paths = paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        \"\"\"Return num of cadence snippets\"\"\"\n        return len(self.paths)\n\n    def __getitem__(self, index: int):\n        \"\"\"Return transformed image and label for given index.\"\"\"\n        path, label = self.paths[index], self.labels[index]\n        img = self._read_cadence_array(path)\n        img = self.transform(image=img)[\"image\"]\n        return {\"image\": img, \"target\": label}\n\n    def _read_cadence_array(self, path: Path):\n        \"\"\"Read cadence file and reshape\"\"\"\n        img = np.load(path)  # shape: (6, 273, 256)\n        img = np.vstack(img)  # shape: (1638, 256)\n        img = img.transpose(1, 0)  # shape: (256, 1638)\n        img = img.astype(\"f\")[..., np.newaxis]  # shape: (256, 1638, 1)\n        return img\n\n    def lazy_init(self, paths=None, labels=None, transform=None):\n        \"\"\"Reset Members\"\"\"\n        if paths is not None:\n            self.paths = paths\n        if labels is not None:\n            self.labels = labels\n        if transform is not None:\n            self.transform = transform\n\n\nclass SetiAObsDataset(SetiSimpleDataset):\n    \"\"\"Use only on-target observation\"\"\"\n\n    def _read_cadence_array(self, path: Path):\n        \"\"\"Read cadence file and reshape\"\"\"\n        img = np.load(path)[[0, 2, 4]]  # shape: (3, 273, 256)\n        img = np.vstack(img)  # shape: (819, 256)\n        img = img.transpose(1, 0)  # shape: (256, 819)\n        img = img.astype(\"f\")[..., np.newaxis]  # shape: (256, 819, 1)\n        return img","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-15T12:24:41.843851Z","iopub.execute_input":"2021-07-15T12:24:41.844198Z","iopub.status.idle":"2021-07-15T12:24:41.860008Z","shell.execute_reply.started":"2021-07-15T12:24:41.84416Z","shell.execute_reply":"2021-07-15T12:24:41.859092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Metric","metadata":{}},{"cell_type":"code","source":"Batch = tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor]]\nModelOut = tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor], torch.Tensor]\n\n\nclass ROCAUC(nn.Module):\n    \"\"\"ROC AUC score\"\"\"\n\n    def __init__(self, average=\"macro\") -> None:\n        \"\"\"Initialize.\"\"\"\n        self.average = average\n        super(ROCAUC, self).__init__()\n\n    def forward(self, y, t) -> float:\n        \"\"\"Forward.\"\"\"\n        if isinstance(y, torch.Tensor):\n            y = y.detach().cpu().numpy()\n        if isinstance(t, torch.Tensor):\n            t = t.detach().cpu().numpy()\n\n        return roc_auc_score(t, y, average=self.average)\n\n\ndef micro_average(\n    metric_func: nn.Module,\n    report_name: str, prefix=\"val\",\n    pred_index: int=-1, label_index: int=-1,\n    pred_key: str=\"logit\", label_key: str=\"target\",\n) -> tp.Callable:\n    \"\"\"Return Metric Wrapper for Simple Mean Metric\"\"\"\n    metric_sum = [0.]\n    n_examples = [0]\n    \n    def wrapper(batch: Batch, model_output: ModelOut, is_last_batch: bool):\n        \"\"\"Wrapping metric function for evaluation\"\"\"\n        if isinstance(batch, tuple): \n            t = batch[label_index]\n        elif isinstance(batch, dict):\n            t = batch[label_key]\n        else:\n            raise NotImplementedError\n\n        if isinstance(model_output, tuple):\n            y = model_output[pred_index]\n        elif isinstance(model_output, dict):\n            y = model_output[pred_key]\n        else:\n            y = model_output\n\n        metric = metric_func(y, t).item()\n        metric_sum[0] += metric * y.shape[0]\n        n_examples[0] += y.shape[0]\n\n        if is_last_batch:\n            final_metric = metric_sum[0] / n_examples[0]\n            ppe.reporting.report({f\"{prefix}/{report_name}\": final_metric})\n            # # reset state\n            metric_sum[0] = 0.\n            n_examples[0] = 0\n\n    return wrapper\n\n\ndef calc_across_all_batchs(\n    metric_func: nn.Module,\n    report_name: str, prefix=\"val\",\n    pred_index: int=-1, label_index: int=-1,\n    pred_key: str=\"logit\", label_key: str=\"target\",\n) -> tp.Callable:\n    \"\"\"\n    Return Metric Wrapper for Metrics caluculated on all data\n    \n    storing predictions and labels of evry batch, finally calculating metric on them.\n    \"\"\"\n    pred_list = []\n    label_list = []\n    \n    def wrapper(batch: Batch, model_output: ModelOut, is_last_batch: bool):\n        \"\"\"Wrapping metric function for evaluation\"\"\"\n        if isinstance(batch, tuple):\n            t = batch[label_index]\n        elif isinstance(batch, dict):\n            t = batch[label_key]\n        else:\n            raise NotImplementedError\n\n        if isinstance(model_output, tuple):\n            y = model_output[pred_index]\n        elif isinstance(model_output, dict):\n            y = model_output[pred_key]\n        else:\n            y = model_output\n\n        pred_list.append(y.numpy())\n        label_list.append(t.numpy())\n\n        if is_last_batch:\n            pred = np.concatenate(pred_list, axis=0)\n            label = np.concatenate(label_list, axis=0)\n            final_metric = metric_func(pred, label)\n            ppe.reporting.report({f\"{prefix}/{report_name}\": final_metric})\n            # # reset state\n            pred_list[:] = []\n            label_list[:] = []\n\n    return wrapper","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-15T12:24:41.86249Z","iopub.execute_input":"2021-07-15T12:24:41.863023Z","iopub.status.idle":"2021-07-15T12:24:41.886409Z","shell.execute_reply.started":"2021-07-15T12:24:41.862981Z","shell.execute_reply":"2021-07-15T12:24:41.885273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"markdown","source":"## config_types for evaluating configuration\n\nI use [pytorch-pfn-extras](https://github.com/pfnet/pytorch-pfn-extras) for training NNs. This library has useful config systems but requires some preparation.\n\nFor more details, see [docs](https://github.com/pfnet/pytorch-pfn-extras/blob/master/docs/config.md).","metadata":{}},{"cell_type":"code","source":"CONFIG_TYPES = {\n    # # utils\n    \"__len__\": lambda obj: len(obj),\n    \"method_call\": lambda obj, method: getattr(obj, method)(),\n\n    # # Dataset, DataLoader\n    \"SetiSimpleDataset\": SetiSimpleDataset,\n    \"SetiAObsDataset\": SetiAObsDataset,\n    \"DataLoader\": torch.utils.data.DataLoader,\n\n    # # Data Augmentation\n    \"Compose\": A.Compose, \"OneOf\": A.OneOf,\n    \"Resize\": A.Resize,\n    \"HorizontalFlip\": A.HorizontalFlip, \"VerticalFlip\": A.VerticalFlip,\n    \"ShiftScaleRotate\": A.ShiftScaleRotate,\n    \"RandomResizedCrop\": A.RandomResizedCrop,\n    \"Cutout\": A.Cutout,\n    \"ToTensorV2\": ToTensorV2,\n\n    # # Model\n    \"BasicImageModel\": BasicImageModel,\n\n    # # Optimizer\n    \"AdamW\": optim.AdamW,\n\n    # # Scheduler\n    \"OneCycleLR\": lr_scheduler.OneCycleLR,\n\n    # # Loss,Metric\n    \"BCEWithLogitsLoss\": nn.BCEWithLogitsLoss,\n    \"ROCAUC\": ROCAUC,\n\n    # # Metric Wrapper\n    \"micro_average\": micro_average,\n    \"calc_across_all_batchs\": calc_across_all_batchs,\n\n    # # PPE Extensions\n    \"ExtensionsManager\": ppe.training.ExtensionsManager,\n\n    \"observe_lr\": ppe_exts.observe_lr,\n    \"LogReport\": ppe_exts.LogReport,\n    \"PlotReport\": ppe_exts.PlotReport,\n    \"PrintReport\": ppe_exts.PrintReport,\n    \"PrintReportNotebook\": ppe_exts.PrintReportNotebook,\n    \"ProgressBar\": ppe_exts.ProgressBar,\n    \"ProgressBarNotebook\": ppe_exts.ProgressBarNotebook,\n    \"snapshot\": ppe_exts.snapshot,\n    \"LRScheduler\": ppe_exts.LRScheduler, \n\n    \"MinValueTrigger\": ppe_triggers.MinValueTrigger,\n    \"MaxValueTrigger\": ppe_triggers.MaxValueTrigger,\n    \"EarlyStoppingTrigger\": ppe_triggers.EarlyStoppingTrigger,\n}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-15T12:24:41.888041Z","iopub.execute_input":"2021-07-15T12:24:41.888597Z","iopub.status.idle":"2021-07-15T12:24:41.900456Z","shell.execute_reply.started":"2021-07-15T12:24:41.888429Z","shell.execute_reply":"2021-07-15T12:24:41.899629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## configration","metadata":{}},{"cell_type":"code","source":"pre_eval_cfg = yaml.safe_load(\n\"\"\"\nglobals:\n  seed: 1086\n  val_fold: null  # indicate when training\n  output_path: null # indicate when training\n  device: cuda\n  enable_amp: False\n  max_epoch: 30\n\nmodel:\n  type: BasicImageModel\n  dims_head: [null, 1]\n  base_name: volo_d1-384\n  pretrained: True\n  in_channels: 1\n  image_size: 256\n\ndataset:\n  height: 256\n  width: 256\n  mixup: {enabled: True, alpha: 1.0}\n  train:\n    type: SetiAObsDataset\n    paths: null  # set by lazy_init\n    labels: null  # set by lazy_init\n    transform:\n      type: Compose\n      transforms:\n        - {type: Resize, p: 1.0, height: \"@/dataset/height\", width: \"@/dataset/width\"}\n        - {type: HorizontalFlip, p: 0.5}\n        - {type: VerticalFlip, p: 0.5}\n        - {type: ShiftScaleRotate, p: 0.5, shift_limit: 0.2, scale_limit: 0.2,\n            rotate_limit: 20, border_mode: 0, value: 0, mask_value: 0}\n        - {type: RandomResizedCrop, p: 1.0,\n            scale: [0.9, 1.0], height: \"@/dataset/height\", width: \"@/dataset/width\"}\n        - {type: ToTensorV2, always_apply: True}\n  val:\n    type: SetiAObsDataset\n    paths: null  # set by lazy_init\n    labels: null  # set by lazy_init\n    transform:\n      type: Compose\n      transforms:\n        - {type: Resize, p: 1.0, height: \"@/dataset/height\", width: \"@/dataset/width\"}\n        - {type: ToTensorV2, always_apply: True}  \n  test:\n    type: SetiAObsDataset\n    paths: null  # set by lazy_init\n    labels: null  # set by lazy_init\n    transform: \"@/dataset/val/transform\"\n\nloader:\n  train: {type: DataLoader, dataset: \"@/dataset/train\",\n    batch_size: 32, num_workers: 4, shuffle: True, pin_memory: True, drop_last: True}\n  val: {type: DataLoader, dataset: \"@/dataset/val\",\n    batch_size: 64, num_workers: 4, shuffle: False, pin_memory: True, drop_last: False}\n  test: {type: DataLoader, dataset: \"@/dataset/test\",\n    batch_size: 64, num_workers: 4, shuffle: False, pin_memory: True, drop_last: False}\n\noptimizer:\n  type: AdamW\n  params: {type: method_call, obj: \"@/model\", method: parameters}\n  lr: 1.0e-07\n  weight_decay: 5.0e-02\n\nscheduler:\n  type: OneCycleLR\n  optimizer: \"@/optimizer\"\n  epochs: \"@/globals/max_epoch\"\n  steps_per_epoch: {type: __len__, obj: \"@/loader/train\"}\n  max_lr: 5.0e-5\n  pct_start: 0.1\n  anneal_strategy: cos\n  div_factor: 1.0e+2\n  final_div_factor: 1\n\nloss: {type: BCEWithLogitsLoss}\n\neval:\n  - type: micro_average\n    metric_func: {type: BCEWithLogitsLoss}\n    report_name: loss\n  - type: calc_across_all_batchs\n    metric_func: {type: ROCAUC}\n    report_name: metric\n\nmanager:\n  type: ExtensionsManager\n  models: \"@/model\"\n  optimizers: \"@/optimizer\"\n  max_epochs: \"@/globals/max_epoch\"\n  iters_per_epoch: {type: __len__, obj: \"@/loader/train\"}\n  out_dir: \"@/globals/output_path\"\n  # stop_trgiger: {type: EarlyStoppingTrigger,\n  #   monitor: val/metric, mode: max, patience: 5, verbose: True,\n  #   check_trigger: [1, epoch], max_trigger: [\"@/globals/max_epoch\", epoch]}\n\nextensions:\n  # # log\n  - {type: observe_lr, optimizer: \"@/optimizer\"}\n  - {type: LogReport}\n  - {type: PlotReport, y_keys: lr, x_key: epoch, filename: lr.png}\n  - {type: PlotReport, y_keys: [train/loss, val/loss], x_key: epoch, filename: loss.png}\n  - {type: PlotReport, y_keys: val/metric, x_key: epoch, filename: metric.png}\n  - {type: PrintReport, entries: [\n      epoch, iteration, lr, train/loss, val/loss, val/metric, elapsed_time]}\n  - {type: ProgressBarNotebook, update_interval: 20}\n  # snapshot\n  - extension: {type: snapshot, target: \"@/model\", filename: \"snapshot_by_metric_epoch_{.epoch}.pth\"}\n    trigger: {type: MaxValueTrigger, key: \"val/metric\", trigger: [1, epoch]}\n  # # lr scheduler\n  - {type: LRScheduler, scheduler: \"@/scheduler\", trigger: [1,  iteration]}\n\"\"\"\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T12:24:41.901868Z","iopub.execute_input":"2021-07-15T12:24:41.90226Z","iopub.status.idle":"2021-07-15T12:24:41.93766Z","shell.execute_reply.started":"2021-07-15T12:24:41.902194Z","shell.execute_reply":"2021-07-15T12:24:41.936685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## functions for training","metadata":{}},{"cell_type":"code","source":"def set_random_seed(seed: int = 42, deterministic: bool = False):\n    \"\"\"Set seeds\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = deterministic  # type: ignore\n\n\ndef to_device(\n    tensors: tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor]],\n    device: torch.device, *args, **kwargs\n):\n    if isinstance(tensors, tuple):\n        return (t.to(device, *args, **kwargs) for t in tensors)\n    elif isinstance(tensors, dict):\n        return {\n            k: t.to(device, *args, **kwargs) for k, t in tensors.items()}\n    else:\n        return tensors.to(device, *args, **kwargs)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-15T12:24:41.938855Z","iopub.execute_input":"2021-07-15T12:24:41.939187Z","iopub.status.idle":"2021-07-15T12:24:41.950102Z","shell.execute_reply.started":"2021-07-15T12:24:41.939151Z","shell.execute_reply":"2021-07-15T12:24:41.949211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_path_label(cfg: Config, train_all: pd.DataFrame):\n    \"\"\"Get file path and target info.\"\"\"\n    use_fold = cfg[\"/globals/val_fold\"]\n\n    train_df = train_all[train_all[\"fold\"] != use_fold]\n    val_df = train_all[train_all[\"fold\"] == use_fold]\n    \n    train_path_label = {\n        \"paths\": [TRAIN / f\"{img_id[0]}/{img_id}.npy\" for img_id in train_df[\"id\"].values],\n        \"labels\": train_df[CLASSES].values.astype(\"f\")}\n    val_path_label = {\n        \"paths\": [TRAIN / f\"{img_id[0]}/{img_id}.npy\" for img_id in val_df[\"id\"].values],\n        \"labels\": val_df[CLASSES].values.astype(\"f\")\n    }\n    return train_path_label, val_path_label\n\n\ndef get_eval_func(cfg, model, device):\n    \n    def eval_func(**batch):\n        \"\"\"Run evaliation for val or test. This function is applied to each batch.\"\"\"\n        batch = to_device(batch, device)\n        x = batch[\"image\"]\n        model.eval()\n        with amp.autocast(cfg[\"/globals/enable_amp\"]): \n            y = model(x)\n        return y.detach().cpu().to(torch.float32)  # input of metrics\n\n    return eval_func\n\n\ndef mixup_data(use_mixup, x, t, alpha=1.0, use_cuda=True):\n    '''Returns mixed inputs, pairs of targets, and lambda'''\n    if not use_mixup:\n        return x, t, None, None\n    \n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size()[0]\n    if use_cuda:\n        index = torch.randperm(batch_size).cuda()\n    else:\n        index = torch.randperm(batch_size)\n\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    t_a, t_b = t, t[index]\n    return mixed_x, t_a, t_b, lam\n\n\ndef get_criterion(use_mixup, loss_func):\n\n    def mixup_criterion(pred, t_a, t_b, lam):\n        return lam * loss_func(pred, t_a) + (1 - lam) * loss_func(pred, t_b)\n\n    def single_criterion(pred, t_a, t_b, lam):\n        return loss_func(pred, t_a)\n    \n    if use_mixup:\n        return mixup_criterion\n    else:\n        return single_criterion","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-15T12:24:41.951726Z","iopub.execute_input":"2021-07-15T12:24:41.952309Z","iopub.status.idle":"2021-07-15T12:24:41.968303Z","shell.execute_reply.started":"2021-07-15T12:24:41.952268Z","shell.execute_reply":"2021-07-15T12:24:41.966996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_one_fold(cfg, train_all):\n    \"\"\"Main\"\"\"\n    torch.backends.cudnn.benchmark = True\n    set_random_seed(cfg[\"/globals/seed\"], deterministic=True)\n    device = torch.device(cfg[\"/globals/device\"])\n    \n    train_path_label, val_path_label = get_path_label(cfg, train_all)\n    print(\"train: {}, val: {}\".format(len(train_path_label[\"paths\"]), len(val_path_label[\"paths\"])))\n   \n    cfg[\"/dataset/train\"].lazy_init(**train_path_label)\n    cfg[\"/dataset/val\"].lazy_init(**val_path_label)\n    train_loader = cfg[\"/loader/train\"]\n    val_loader = cfg[\"/loader/val\"]\n\n    model = cfg[\"/model\"]\n    model.to(device)\n    optimizer = cfg[\"/optimizer\"]\n    loss_func = cfg[\"/loss\"]\n    loss_func.to(device)\n    \n    manager = cfg[\"/manager\"]\n    for ext in cfg[\"/extensions\"]:\n        if isinstance(ext, dict):\n            manager.extend(**ext)\n        else:\n            manager.extend(ext)\n\n    evaluator = ppe_exts.Evaluator(\n        val_loader, model, eval_func=get_eval_func(cfg, model, device),\n        metrics=cfg[\"/eval\"], progress_bar=False)\n    manager.extend(evaluator, trigger=(1, \"epoch\"))\n\n    use_amp = cfg[\"/globals/enable_amp\"]\n    scaler = amp.GradScaler(enabled=use_amp)\n    use_mixup = cfg[\"/dataset/mixup/enabled\"]\n    mixup_alpha = cfg[\"/dataset/mixup/alpha\"]\n    \n    while not manager.stop_trigger:\n        model.train()\n        for batch in train_loader:\n            with manager.run_iteration():\n                batch = to_device(batch, device)\n                x, t = batch[\"image\"], batch[\"target\"]\n                # # for mixup\n                mixed_x, t_a, t_b, lam = mixup_data(use_mixup, x, t, mixup_alpha)\n                criterion = get_criterion(use_mixup, loss_func)\n                \n                optimizer.zero_grad()\n                with amp.autocast(use_amp):\n                    y = model(mixed_x)\n                    loss = criterion(y, t_a, t_b, lam)\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n                \n                ppe.reporting.report({'train/loss': loss.item()})","metadata":{"execution":{"iopub.status.busy":"2021-07-15T12:24:41.970112Z","iopub.execute_input":"2021-07-15T12:24:41.970649Z","iopub.status.idle":"2021-07-15T12:24:41.987776Z","shell.execute_reply.started":"2021-07-15T12:24:41.970564Z","shell.execute_reply":"2021-07-15T12:24:41.98689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## run train","metadata":{}},{"cell_type":"code","source":"pre_eval_cfg_list = []\nfor fold_id in FOLDS:\n    tmp_cfg = copy.deepcopy(pre_eval_cfg)\n    tmp_cfg[\"globals\"][\"val_fold\"] = fold_id\n    tmp_cfg[\"globals\"][\"output_path\"] = str(TMP / f\"fold{fold_id}\")\n    pre_eval_cfg_list.append(tmp_cfg)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T12:24:41.989201Z","iopub.execute_input":"2021-07-15T12:24:41.989663Z","iopub.status.idle":"2021-07-15T12:24:41.999299Z","shell.execute_reply.started":"2021-07-15T12:24:41.989624Z","shell.execute_reply":"2021-07-15T12:24:41.998343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for pre_eval_cfg in pre_eval_cfg_list:\n    cfg = Config(pre_eval_cfg, types=CONFIG_TYPES)\n    print(f\"\\n[fold {cfg['/globals/val_fold']}]\")\n    train_one_fold(cfg, train)\n    del cfg\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-07-15T12:24:42.001346Z","iopub.execute_input":"2021-07-15T12:24:42.002085Z","iopub.status.idle":"2021-07-15T12:42:36.315585Z","shell.execute_reply.started":"2021-07-15T12:24:42.002048Z","shell.execute_reply":"2021-07-15T12:42:36.313331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"markdown","source":"## Copy best models","metadata":{}},{"cell_type":"code","source":"best_log_list = []\nfor pre_eval_cfg, fold_id in zip(pre_eval_cfg_list, FOLDS):\n    exp_dir_path = TMP / f\"fold{fold_id}\"\n    log = pd.read_json(exp_dir_path / \"log\")\n    best_log = log.iloc[[log[\"val/metric\"].idxmax()],]\n    best_epoch = best_log.epoch.values[0]\n    best_log_list.append(best_log)\n    \n    best_model_path = exp_dir_path / f\"snapshot_by_metric_epoch_{best_epoch}.pth\"\n    copy_to = f\"./best_metric_model_fold{fold_id}.pth\"\n    shutil.copy(best_model_path, copy_to)\n    \n    for p in exp_dir_path.glob(\"*.pth\"):\n        p.unlink()\n    \n    shutil.copytree(exp_dir_path, f\"./fold{fold_id}\")\n    \n    with open(f\"./fold{fold_id}/config.yml\", \"w\") as fw:\n        yaml.dump(pre_eval_cfg, fw)\n    \npd.concat(best_log_list, axis=0, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T12:42:36.321252Z","iopub.execute_input":"2021-07-15T12:42:36.321536Z","iopub.status.idle":"2021-07-15T12:42:37.355444Z","shell.execute_reply.started":"2021-07-15T12:42:36.321507Z","shell.execute_reply":"2021-07-15T12:42:37.354509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference OOF & Test","metadata":{}},{"cell_type":"code","source":"# def run_inference_loop(cfg, model, loader, device):\n#     model.to(device)\n#     model.eval()\n#     pred_list = []\n#     with torch.no_grad():\n#         for batch in tqdm(loader):\n#             x = to_device(batch[\"image\"], device)\n#             y = model(x)\n#             pred_list.append(y.sigmoid().detach().cpu().numpy())\n        \n#     pred_arr = np.concatenate(pred_list)\n#     del pred_list\n#     return pred_arr","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-15T12:42:37.356671Z","iopub.execute_input":"2021-07-15T12:42:37.357095Z","iopub.status.idle":"2021-07-15T12:42:37.361307Z","shell.execute_reply.started":"2021-07-15T12:42:37.357067Z","shell.execute_reply":"2021-07-15T12:42:37.359796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label_arr = train[CLASSES].values\n# oof_pred_arr = np.zeros((len(train), N_CLASSES))\n# score_list = []\n# test_pred_arr = np.zeros((N_FOLDS, len(smpl_sub), N_CLASSES))\n# test_path_label = {\n#     \"paths\": [DATA / f\"test/{img_id[0]}/{img_id}.npy\" for img_id in smpl_sub[\"id\"].values],\n#     \"labels\": smpl_sub[CLASSES].values.astype(\"f\")\n# }\n\n# for fold_id in range(N_FOLDS):\n#     print(f\"\\n[fold {fold_id}]\")\n#     tmp_dir = Path(f\"./fold{fold_id}\")\n#     with open(tmp_dir / \"config.yml\", \"r\") as fr:\n#         cfg = Config(yaml.safe_load(fr), types=CONFIG_TYPES)\n#     device = torch.device(cfg[\"/globals/device\"])\n#     val_idx = train.query(\"fold == @fold_id\").index.values\n\n#     # # get_dataloader\n#     _, val_path_label = get_path_label(cfg, train)\n#     cfg[\"/dataset/val\"].lazy_init(**val_path_label)\n#     cfg[\"/dataset/test\"].lazy_init(**test_path_label)\n#     val_loader = cfg[\"/loader/val\"]\n#     test_loader = cfg[\"/loader/test\"]\n    \n#     # # get model\n#     model_path = f\"./best_metric_model_fold{fold_id}.pth\"\n#     model = cfg[\"/model\"]\n#     model.load_state_dict(torch.load(model_path, map_location=device))\n    \n#     # # inference\n#     val_pred = run_inference_loop(cfg, model, val_loader, device)\n#     val_score = roc_auc_score(label_arr[val_idx], val_pred)\n#     oof_pred_arr[val_idx] = val_pred\n#     score_list.append([fold_id, val_score])\n    \n#     test_pred_arr[fold_id] = run_inference_loop(cfg, model, test_loader, device)\n    \n#     del cfg, val_idx, val_path_label\n#     del model, val_loader, test_loader\n#     torch.cuda.empty_cache()\n#     gc.collect()\n    \n#     print(f\"val score: {val_score:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-15T12:42:37.36292Z","iopub.execute_input":"2021-07-15T12:42:37.363315Z","iopub.status.idle":"2021-07-15T12:42:37.370988Z","shell.execute_reply.started":"2021-07-15T12:42:37.363277Z","shell.execute_reply":"2021-07-15T12:42:37.36973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# oof_score = roc_auc_score(label_arr, oof_pred_arr)\n# score_list.append([\"oof\", oof_score])\n# pd.DataFrame(score_list, columns=[\"fold\", \"metric\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-15T12:42:37.372368Z","iopub.execute_input":"2021-07-15T12:42:37.372878Z","iopub.status.idle":"2021-07-15T12:42:37.383065Z","shell.execute_reply.started":"2021-07-15T12:42:37.372839Z","shell.execute_reply":"2021-07-15T12:42:37.382107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# oof_df = train.copy()\n# oof_df[CLASSES] = oof_pred_arr\n# oof_df.to_csv(\"./oof_prediction.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T12:42:37.384447Z","iopub.execute_input":"2021-07-15T12:42:37.384996Z","iopub.status.idle":"2021-07-15T12:42:37.392078Z","shell.execute_reply.started":"2021-07-15T12:42:37.384958Z","shell.execute_reply":"2021-07-15T12:42:37.391246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make submission","metadata":{}},{"cell_type":"code","source":"# sub_df = smpl_sub.copy()\n# sub_df[CLASSES] = test_pred_arr.mean(axis=0)\n# sub_df.to_csv(\"./submission.csv\", index=False)\n# sub_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-15T12:42:37.393323Z","iopub.execute_input":"2021-07-15T12:42:37.393992Z","iopub.status.idle":"2021-07-15T12:42:37.400782Z","shell.execute_reply.started":"2021-07-15T12:42:37.393955Z","shell.execute_reply":"2021-07-15T12:42:37.399759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EOF","metadata":{}}]}