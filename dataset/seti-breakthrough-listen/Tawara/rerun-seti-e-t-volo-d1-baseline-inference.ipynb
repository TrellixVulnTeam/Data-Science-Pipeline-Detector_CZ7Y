{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About\n\nHere I'm trying to train _VOLO_: Vision Outlooker for Visual Recognition.  \nIt is a new vison transformer which achinved good results with fewer parameters than others.   \n\npaper: https://arxiv.org/abs/2106.13112v1  \ncode: https://github.com/sail-sg/volo  \n\nI uploaded official code and pretrained weights on Kaggle Datasets:  \nhttps://www.kaggle.com/ttahara/volo-package\n\nI make some changes for simply training binary classification task.\n\n<br>\nI have prepared the training and inference notebooks because the training time per fold is longer than resnet18d.\n  \n  \nThis is an **inference** notebook.\n\n* model:\n  * backbone: volo_d1\n  * head classifier: one linear layer\n  * num of input channels: **1**\n* input image:\n  * size: 1x256x256\n  * use only on-target ('A') observations\n  \n  \n  ```python\n  img = np.load(path)[[0, 2, 4]]          # shape: (3, 273, 256)\n  img = np.vstack(img)                    # shape: (819, 256)\n  img = img.transpose(1, 0)               # shape: (256, 819)\n  ```\n\n\n* CVStrategy: Stratified KFold(K=5)\n\nIf you want to know more details of experimental settings, see training notebooks:  \n[fold0](https://www.kaggle.com/ttahara/rerun-seti-e-t-volo-d1-baseline-training?scriptVersionId=68249988), [fold1](https://www.kaggle.com/ttahara/rerun-seti-e-t-volo-d1-baseline-training?scriptVersionId=68250008), [fold2](https://www.kaggle.com/ttahara/rerun-seti-e-t-volo-d1-baseline-training?scriptVersionId=68281727), [fold3](https://www.kaggle.com/ttahara/rerun-seti-e-t-volo-d1-baseline-training?scriptVersionId=68281737), [fold4](https://www.kaggle.com/ttahara/rerun-seti-e-t-volo-d1-baseline-training?scriptVersionId=68335611)","metadata":{}},{"cell_type":"markdown","source":"# Prapere","metadata":{}},{"cell_type":"markdown","source":"## Install","metadata":{}},{"cell_type":"code","source":"%%bash\npip install pytorch-pfn-extras\npip install timm","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-17T03:31:20.268301Z","iopub.execute_input":"2021-07-17T03:31:20.26867Z","iopub.status.idle":"2021-07-17T03:31:43.011859Z","shell.execute_reply.started":"2021-07-17T03:31:20.26859Z","shell.execute_reply":"2021-07-17T03:31:43.010973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport copy\nimport yaml\nimport random\nimport shutil\nimport typing as tp\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.optim import lr_scheduler\nfrom torch.cuda import amp\n\nimport timm\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport pytorch_pfn_extras as ppe\nfrom pytorch_pfn_extras.config import Config\nfrom pytorch_pfn_extras.training import extensions as ppe_exts, triggers as ppe_triggers\n\nsys.path.append(\"../input/volo-package\")\nfrom volo.models import volo_d1, volo_d2, volo_d3, volo_d4, volo_d5  # register models to timm\nfrom volo.utils import load_pretrained_weights as volo_load_weights","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-17T03:31:43.013679Z","iopub.execute_input":"2021-07-17T03:31:43.014029Z","iopub.status.idle":"2021-07-17T03:31:48.442919Z","shell.execute_reply.started":"2021-07-17T03:31:43.013987Z","shell.execute_reply":"2021-07-17T03:31:48.442017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT = Path.cwd().parent\nINPUT = ROOT / \"input\"\nOUTPUT = ROOT / \"output\"\n# DATA = INPUT / \"seti-breakthrough-listen\"\nDATA = INPUT / \"c\" / \"seti-breakthrough-listen\"\nTRAIN = DATA / \"train\"\nTEST = DATA / \"test\"\nTRAINING_OUTPUT_PATH = INPUT / \"seti-breakthrough-listen-weights-for-volo-d1\"\n\nRANDAM_SEED = 1086\nCLASSES = [\"target\",]\nN_CLASSES = len(CLASSES)\nFOLDS = [0, 1, 2, 3, 4]\nN_FOLDS = len(FOLDS)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T03:31:48.444719Z","iopub.execute_input":"2021-07-17T03:31:48.445016Z","iopub.status.idle":"2021-07-17T03:31:48.454219Z","shell.execute_reply.started":"2021-07-17T03:31:48.444987Z","shell.execute_reply":"2021-07-17T03:31:48.453382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read Data, Split folds","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(DATA / \"train_labels.csv\")\nsmpl_sub = pd.read_csv(DATA / \"sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-17T03:31:48.455765Z","iopub.execute_input":"2021-07-17T03:31:48.456148Z","iopub.status.idle":"2021-07-17T03:31:48.54975Z","shell.execute_reply.started":"2021-07-17T03:31:48.45612Z","shell.execute_reply":"2021-07-17T03:31:48.548869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDAM_SEED)\ntrain[\"fold\"] = -1\nfor fold_id, (_, val_idx) in enumerate(skf.split(train[\"id\"], train[\"target\"])):\n    train.loc[val_idx, \"fold\"] = fold_id","metadata":{"execution":{"iopub.status.busy":"2021-07-17T03:31:48.550996Z","iopub.execute_input":"2021-07-17T03:31:48.551353Z","iopub.status.idle":"2021-07-17T03:31:48.584474Z","shell.execute_reply.started":"2021-07-17T03:31:48.551317Z","shell.execute_reply":"2021-07-17T03:31:48.583741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby(\"fold\").agg(total=(\"id\", len), pos=(\"target\", sum))","metadata":{"execution":{"iopub.status.busy":"2021-07-17T03:31:48.587126Z","iopub.execute_input":"2021-07-17T03:31:48.58738Z","iopub.status.idle":"2021-07-17T03:31:48.623259Z","shell.execute_reply.started":"2021-07-17T03:31:48.587355Z","shell.execute_reply":"2021-07-17T03:31:48.622443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ","metadata":{}},{"cell_type":"markdown","source":"## Definition","metadata":{}},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"VOLO_CHECHPOINTS = {\n    \"volo_d1\": \"../input/volo-package/d1_224_84.2.pth.tar\",\n    \"volo_d1-384\": \"../input/volo-package/d1_384_85.2.pth.tar\",\n    \"volo_d2\": \"../input/volo-package/d2_224_85.2.pth.tar\",\n    \"volo_d2-384\": \"../input/volo-package/d2_384_86.0.pth.tar\",\n    \"volo_d3\": \"../input/volo-package/d3_224_85.4.pth.tar\",\n    \"volo_d3-448\": \"../input/volo-package/d3_448_86.3.pth.tar\",\n    \"volo_d4\": \"../input/volo-package/d4_224_85.7.pth.tar\",\n    \"volo_d4-448\": \"../input/volo-package/d4_448_86.79.pth.tar\",\n    \"volo_d5\": \"../input/volo-package/d5_224_86.10.pth.tar\",\n    \"volo_d5-448\": \"../input/volo-package/d5_448_87.0.pth.tar\",\n    \"volo_d5-512\": \"../input/volo-package/d5_512_87.07.pth.tar\",\n}\n\nclass BasicImageModel(nn.Module):\n    \n    def __init__(\n        self, base_name: str, dims_head: tp.List[int],\n        pretrained=False, in_channels: int=3, image_size: int=224 \n    ):\n        \"\"\"Initialize\"\"\"\n        self.base_name = base_name\n        super().__init__()\n        model_name = base_name.split(\"-\")[0]\n        assert timm.is_model(model_name), \"you can use only models in timm.\"\n        \n        if model_name[:4] == \"volo\":\n            base_model = timm.create_model(\n                model_name, img_size=image_size, mix_token=False, return_dense=False)\n            in_features = base_model.head.in_features\n            if pretrained:\n                volo_load_weights(base_model, VOLO_CHECHPOINTS[base_name], strict=False)\n            \n            if in_channels != 3:\n                # # change input channel\n                # # I follow the manner used in timm.\n                first_conv = base_model.patch_embed.conv[0]\n                w_t = first_conv.weight.data  # shape: (out_ch, 3, 7, 7)\n                if in_channels == 1:\n                    new_w_t = w_t.sum(axis=1, keepdims=True)  # shape: (out_ch, 1, 7, 7)\n                else:\n                    n_repeats = (in_channels + 3 - 1) // 3\n                    new_w_t = w_t.repeat((1, n_repeats, 1, 1))\n                    new_w_t = new_w_t[:, :in_channels]\n                    new_w_t = new_w_t * 3 / in_channels  # shape: (out_ch, in_channels, 7, 7)\n\n                first_conv.weight.data = new_w_t\n        else:\n            base_model = timm.create_model(\n                base_name, pretrained=pretrained, in_chans=in_channels)\n            in_features = base_model.num_features\n            print(\"load imagenet pretrained:\", pretrained)\n            \n        base_model.reset_classifier(num_classes=0)\n        self.backbone = base_model\n        print(f\"{base_name}: {in_features}\")\n        \n        # # prepare head clasifier\n        if dims_head[0] is None:\n            dims_head[0] = in_features\n\n        layers_list = []\n        for i in range(len(dims_head) - 2):\n            in_dim, out_dim = dims_head[i: i + 2]\n            layers_list.extend([\n                nn.Linear(in_dim, out_dim),\n                nn.ReLU(), nn.Dropout(0.5),])\n        layers_list.append(\n            nn.Linear(dims_head[-2], dims_head[-1]))\n        self.head = nn.Sequential(*layers_list)\n\n    def forward(self, x):\n        \"\"\"Forward\"\"\"\n        h = self.backbone(x)\n        h = self.head(h)\n        return h","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-17T03:31:48.625968Z","iopub.execute_input":"2021-07-17T03:31:48.626237Z","iopub.status.idle":"2021-07-17T03:31:48.643218Z","shell.execute_reply.started":"2021-07-17T03:31:48.626212Z","shell.execute_reply":"2021-07-17T03:31:48.642191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-03T00:39:25.885443Z","iopub.execute_input":"2021-06-03T00:39:25.885782Z","iopub.status.idle":"2021-06-03T00:39:25.904695Z","shell.execute_reply.started":"2021-06-03T00:39:25.885753Z","shell.execute_reply":"2021-06-03T00:39:25.903901Z"}}},{"cell_type":"code","source":"FilePath = tp.Union[str, Path]\nLabel = tp.Union[int, float, np.ndarray]\n\n\nclass SetiSimpleDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset using 6 channels by stacking them along time-axis\n\n    Attributes\n    ----------\n    paths : tp.Sequence[FilePath]\n        Sequence of path to cadence snippet file\n    labels : tp.Sequence[Label]\n        Sequence of label for cadence snippet file\n    transform: albumentations.Compose\n        composed data augmentations for data\n    \"\"\"\n\n    def __init__(\n        self,\n        paths: tp.Sequence[FilePath],\n        labels: tp.Sequence[Label],\n        transform: A.Compose,\n    ):\n        \"\"\"Initialize\"\"\"\n        self.paths = paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        \"\"\"Return num of cadence snippets\"\"\"\n        return len(self.paths)\n\n    def __getitem__(self, index: int):\n        \"\"\"Return transformed image and label for given index.\"\"\"\n        path, label = self.paths[index], self.labels[index]\n        img = self._read_cadence_array(path)\n        img = self.transform(image=img)[\"image\"]\n        return {\"image\": img, \"target\": label}\n\n    def _read_cadence_array(self, path: Path):\n        \"\"\"Read cadence file and reshape\"\"\"\n        img = np.load(path)  # shape: (6, 273, 256)\n        img = np.vstack(img)  # shape: (1638, 256)\n        img = img.transpose(1, 0)  # shape: (256, 1638)\n        img = img.astype(\"f\")[..., np.newaxis]  # shape: (256, 1638, 1)\n        return img\n\n    def lazy_init(self, paths=None, labels=None, transform=None):\n        \"\"\"Reset Members\"\"\"\n        if paths is not None:\n            self.paths = paths\n        if labels is not None:\n            self.labels = labels\n        if transform is not None:\n            self.transform = transform\n\n\nclass SetiAObsDataset(SetiSimpleDataset):\n    \"\"\"Use only on-target observation\"\"\"\n\n    def _read_cadence_array(self, path: Path):\n        \"\"\"Read cadence file and reshape\"\"\"\n        img = np.load(path)[[0, 2, 4]]  # shape: (3, 273, 256)\n        img = np.vstack(img)  # shape: (819, 256)\n        img = img.transpose(1, 0)  # shape: (256, 819)\n        img = img.astype(\"f\")[..., np.newaxis]  # shape: (256, 819, 1)\n        return img","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-17T03:31:48.64634Z","iopub.execute_input":"2021-07-17T03:31:48.646847Z","iopub.status.idle":"2021-07-17T03:31:48.660875Z","shell.execute_reply.started":"2021-07-17T03:31:48.646813Z","shell.execute_reply":"2021-07-17T03:31:48.660112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Metric","metadata":{}},{"cell_type":"code","source":"Batch = tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor]]\nModelOut = tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor], torch.Tensor]\n\n\nclass ROCAUC(nn.Module):\n    \"\"\"ROC AUC score\"\"\"\n\n    def __init__(self, average=\"macro\") -> None:\n        \"\"\"Initialize.\"\"\"\n        self.average = average\n        super(ROCAUC, self).__init__()\n\n    def forward(self, y, t) -> float:\n        \"\"\"Forward.\"\"\"\n        if isinstance(y, torch.Tensor):\n            y = y.detach().cpu().numpy()\n        if isinstance(t, torch.Tensor):\n            t = t.detach().cpu().numpy()\n\n        return roc_auc_score(t, y, average=self.average)\n\n\ndef micro_average(\n    metric_func: nn.Module,\n    report_name: str, prefix=\"val\",\n    pred_index: int=-1, label_index: int=-1,\n    pred_key: str=\"logit\", label_key: str=\"target\",\n) -> tp.Callable:\n    \"\"\"Return Metric Wrapper for Simple Mean Metric\"\"\"\n    metric_sum = [0.]\n    n_examples = [0]\n    \n    def wrapper(batch: Batch, model_output: ModelOut, is_last_batch: bool):\n        \"\"\"Wrapping metric function for evaluation\"\"\"\n        if isinstance(batch, tuple): \n            t = batch[label_index]\n        elif isinstance(batch, dict):\n            t = batch[label_key]\n        else:\n            raise NotImplementedError\n\n        if isinstance(model_output, tuple):\n            y = model_output[pred_index]\n        elif isinstance(model_output, dict):\n            y = model_output[pred_key]\n        else:\n            y = model_output\n\n        metric = metric_func(y, t).item()\n        metric_sum[0] += metric * y.shape[0]\n        n_examples[0] += y.shape[0]\n\n        if is_last_batch:\n            final_metric = metric_sum[0] / n_examples[0]\n            ppe.reporting.report({f\"{prefix}/{report_name}\": final_metric})\n            # # reset state\n            metric_sum[0] = 0.\n            n_examples[0] = 0\n\n    return wrapper\n\n\ndef calc_across_all_batchs(\n    metric_func: nn.Module,\n    report_name: str, prefix=\"val\",\n    pred_index: int=-1, label_index: int=-1,\n    pred_key: str=\"logit\", label_key: str=\"target\",\n) -> tp.Callable:\n    \"\"\"\n    Return Metric Wrapper for Metrics caluculated on all data\n    \n    storing predictions and labels of evry batch, finally calculating metric on them.\n    \"\"\"\n    pred_list = []\n    label_list = []\n    \n    def wrapper(batch: Batch, model_output: ModelOut, is_last_batch: bool):\n        \"\"\"Wrapping metric function for evaluation\"\"\"\n        if isinstance(batch, tuple):\n            t = batch[label_index]\n        elif isinstance(batch, dict):\n            t = batch[label_key]\n        else:\n            raise NotImplementedError\n\n        if isinstance(model_output, tuple):\n            y = model_output[pred_index]\n        elif isinstance(model_output, dict):\n            y = model_output[pred_key]\n        else:\n            y = model_output\n\n        pred_list.append(y.numpy())\n        label_list.append(t.numpy())\n\n        if is_last_batch:\n            pred = np.concatenate(pred_list, axis=0)\n            label = np.concatenate(label_list, axis=0)\n            final_metric = metric_func(pred, label)\n            ppe.reporting.report({f\"{prefix}/{report_name}\": final_metric})\n            # # reset state\n            pred_list[:] = []\n            label_list[:] = []\n\n    return wrapper","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-17T03:31:48.662582Z","iopub.execute_input":"2021-07-17T03:31:48.662941Z","iopub.status.idle":"2021-07-17T03:31:48.683266Z","shell.execute_reply.started":"2021-07-17T03:31:48.662904Z","shell.execute_reply":"2021-07-17T03:31:48.682548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Utils","metadata":{}},{"cell_type":"code","source":"def set_random_seed(seed: int = 42, deterministic: bool = False):\n    \"\"\"Set seeds\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = deterministic  # type: ignore\n\n\ndef to_device(\n    tensors: tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor]],\n    device: torch.device, *args, **kwargs\n):\n    if isinstance(tensors, tuple):\n        return (t.to(device, *args, **kwargs) for t in tensors)\n    elif isinstance(tensors, dict):\n        return {\n            k: t.to(device, *args, **kwargs) for k, t in tensors.items()}\n    else:\n        return tensors.to(device, *args, **kwargs)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-17T03:31:48.684478Z","iopub.execute_input":"2021-07-17T03:31:48.684831Z","iopub.status.idle":"2021-07-17T03:31:48.698375Z","shell.execute_reply.started":"2021-07-17T03:31:48.684792Z","shell.execute_reply":"2021-07-17T03:31:48.697489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## config_types for evaluating configuration\n\nI use [pytorch-pfn-extras](https://github.com/pfnet/pytorch-pfn-extras) for training NNs. This library has useful config systems but requires some preparation.\n\nFor more details, see [docs](https://github.com/pfnet/pytorch-pfn-extras/blob/master/docs/config.md).","metadata":{}},{"cell_type":"code","source":"CONFIG_TYPES = {\n    # # utils\n    \"__len__\": lambda obj: len(obj),\n    \"method_call\": lambda obj, method: getattr(obj, method)(),\n\n    # # Dataset, DataLoader\n    \"SetiSimpleDataset\": SetiSimpleDataset,\n    \"SetiAObsDataset\": SetiAObsDataset,\n    \"DataLoader\": torch.utils.data.DataLoader,\n\n    # # Data Augmentation\n    \"Compose\": A.Compose, \"OneOf\": A.OneOf,\n    \"Resize\": A.Resize,\n    \"HorizontalFlip\": A.HorizontalFlip, \"VerticalFlip\": A.VerticalFlip,\n    \"ShiftScaleRotate\": A.ShiftScaleRotate,\n    \"RandomResizedCrop\": A.RandomResizedCrop,\n    \"Cutout\": A.Cutout,\n    \"ToTensorV2\": ToTensorV2,\n\n    # # Model\n    \"BasicImageModel\": BasicImageModel,\n\n    # # Optimizer\n    \"AdamW\": optim.AdamW,\n\n    # # Scheduler\n    \"OneCycleLR\": lr_scheduler.OneCycleLR,\n\n    # # Loss,Metric\n    \"BCEWithLogitsLoss\": nn.BCEWithLogitsLoss,\n    \"ROCAUC\": ROCAUC,\n\n    # # Metric Wrapper\n    \"micro_average\": micro_average,\n    \"calc_across_all_batchs\": calc_across_all_batchs,\n\n    # # PPE Extensions\n    \"ExtensionsManager\": ppe.training.ExtensionsManager,\n\n    \"observe_lr\": ppe_exts.observe_lr,\n    \"LogReport\": ppe_exts.LogReport,\n    \"PlotReport\": ppe_exts.PlotReport,\n    \"PrintReport\": ppe_exts.PrintReport,\n    \"PrintReportNotebook\": ppe_exts.PrintReportNotebook,\n    \"ProgressBar\": ppe_exts.ProgressBar,\n    \"ProgressBarNotebook\": ppe_exts.ProgressBarNotebook,\n    \"snapshot\": ppe_exts.snapshot,\n    \"LRScheduler\": ppe_exts.LRScheduler, \n\n    \"MinValueTrigger\": ppe_triggers.MinValueTrigger,\n    \"MaxValueTrigger\": ppe_triggers.MaxValueTrigger,\n    \"EarlyStoppingTrigger\": ppe_triggers.EarlyStoppingTrigger,\n}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-17T03:31:48.701684Z","iopub.execute_input":"2021-07-17T03:31:48.701975Z","iopub.status.idle":"2021-07-17T03:31:48.711316Z","shell.execute_reply.started":"2021-07-17T03:31:48.701946Z","shell.execute_reply":"2021-07-17T03:31:48.710441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"def get_path_label(cfg: Config, train_all: pd.DataFrame):\n    \"\"\"Get file path and target info.\"\"\"\n    use_fold = cfg[\"/globals/val_fold\"]\n\n    train_df = train_all[train_all[\"fold\"] != use_fold]\n    val_df = train_all[train_all[\"fold\"] == use_fold]\n    \n    train_path_label = {\n        \"paths\": [TRAIN / f\"{img_id[0]}/{img_id}.npy\" for img_id in train_df[\"id\"].values],\n        \"labels\": train_df[CLASSES].values.astype(\"f\")}\n    val_path_label = {\n        \"paths\": [TRAIN / f\"{img_id[0]}/{img_id}.npy\" for img_id in val_df[\"id\"].values],\n        \"labels\": val_df[CLASSES].values.astype(\"f\")\n    }\n    return train_path_label, val_path_label\n\n\ndef run_inference_loop(cfg, model, loader, device):\n    model.to(device)\n    model.eval()\n    pred_list = []\n    with torch.no_grad():\n        for batch in tqdm(loader):\n            x = to_device(batch[\"image\"], device)\n            y = model(x)\n            pred_list.append(y.sigmoid().detach().cpu().numpy())\n        \n    pred_arr = np.concatenate(pred_list)\n    del pred_list\n    return pred_arr","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-17T03:31:48.712631Z","iopub.execute_input":"2021-07-17T03:31:48.713273Z","iopub.status.idle":"2021-07-17T03:31:48.727271Z","shell.execute_reply.started":"2021-07-17T03:31:48.713233Z","shell.execute_reply":"2021-07-17T03:31:48.726324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run inference for each fold","metadata":{}},{"cell_type":"code","source":"# # debug\n# smpl_sub = smpl_sub.iloc[:32 * 10, :].copy()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T03:31:48.728629Z","iopub.execute_input":"2021-07-17T03:31:48.729018Z","iopub.status.idle":"2021-07-17T03:31:48.739381Z","shell.execute_reply.started":"2021-07-17T03:31:48.72898Z","shell.execute_reply":"2021-07-17T03:31:48.738528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.backends.cudnn.benchmark = True\nset_random_seed(1086, deterministic=True)\ndevice = torch.device(\"cuda\")\n\nlabel_arr = train[CLASSES].values\noof_pred_arr = np.zeros((len(train), N_CLASSES))\ntest_pred_arr = np.zeros((N_FOLDS, len(smpl_sub), N_CLASSES))\nscore_list = []\n\ntest_path_label = {\n    \"paths\": [DATA / f\"test/{img_id[0]}/{img_id}.npy\" for img_id in smpl_sub[\"id\"].values],\n    \"labels\": smpl_sub[CLASSES].values.astype(\"f\")\n}","metadata":{"execution":{"iopub.status.busy":"2021-07-17T03:31:48.740806Z","iopub.execute_input":"2021-07-17T03:31:48.741198Z","iopub.status.idle":"2021-07-17T03:31:49.167704Z","shell.execute_reply.started":"2021-07-17T03:31:48.74116Z","shell.execute_reply":"2021-07-17T03:31:49.166882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold_id in FOLDS:\n    print(f\"\\n[fold {fold_id}]\")\n    tmp_dir = TRAINING_OUTPUT_PATH / f\"fold{fold_id}\"\n    with open(tmp_dir / \"config.yml\", \"r\") as fr:\n        cfg = Config(yaml.safe_load(fr), types=CONFIG_TYPES)\n    val_idx = train.query(\"fold == @fold_id\").index.values\n\n    # # get_dataloader\n    _, val_path_label = get_path_label(cfg, train)\n    cfg[\"/dataset/val\"].lazy_init(**val_path_label)\n    cfg[\"/dataset/test\"].lazy_init(**test_path_label)\n    val_loader = cfg[\"/loader/val\"]\n    test_loader = cfg[\"/loader/test\"]\n    \n    # # get model\n    model_path = TRAINING_OUTPUT_PATH / f\"best_metric_model_fold{fold_id}.pth\"\n    model = cfg[\"/model\"]\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    \n    # # inference\n    val_pred = run_inference_loop(cfg, model, val_loader, device)\n    val_score = roc_auc_score(label_arr[val_idx], val_pred)\n    oof_pred_arr[val_idx] = val_pred\n    score_list.append([fold_id, val_score])\n    \n    test_pred_arr[fold_id] = run_inference_loop(cfg, model, test_loader, device)\n    \n    del cfg, val_idx, val_path_label\n    del model, val_loader, test_loader\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    print(f\"[fold {fold_id}] val score: {val_score:.4f}\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-07-17T03:31:49.169886Z","iopub.execute_input":"2021-07-17T03:31:49.170395Z","iopub.status.idle":"2021-07-17T03:32:13.976094Z","shell.execute_reply.started":"2021-07-17T03:31:49.170356Z","shell.execute_reply":"2021-07-17T03:32:13.974336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check oof score","metadata":{}},{"cell_type":"code","source":"oof_score = roc_auc_score(label_arr, oof_pred_arr)\nscore_list.append([\"oof\", oof_score])\npd.DataFrame(score_list, columns=[\"fold\", \"metric\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-17T03:32:13.977407Z","iopub.status.idle":"2021-07-17T03:32:13.978123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_df = train.copy()\noof_df[CLASSES] = oof_pred_arr\noof_df.to_csv(\"./oof_prediction.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T03:32:13.979362Z","iopub.status.idle":"2021-07-17T03:32:13.979957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make submission","metadata":{}},{"cell_type":"code","source":"sub_df = smpl_sub.copy()\nsub_df[CLASSES] = test_pred_arr.mean(axis=0)\nsub_df.to_csv(\"./submission.csv\", index=False)\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T03:32:13.981285Z","iopub.status.idle":"2021-07-17T03:32:13.98184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EOF","metadata":{}}]}