{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install and import package\n!pip install keras-tuner\n!pip install kaggledatasets\n# !pip install tensorflow_datasets","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:05:18.633208Z","iopub.execute_input":"2021-08-03T08:05:18.634004Z","iopub.status.idle":"2021-08-03T08:05:36.792672Z","shell.execute_reply.started":"2021-08-03T08:05:18.633846Z","shell.execute_reply":"2021-08-03T08:05:36.791403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport seaborn as sns\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow_datasets as tfds\nimport numpy as np\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom kaggle_datasets import KaggleDatasets\nfrom kaggle_secrets import UserSecretsClient\nfrom tensorflow.keras import layers\n","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:05:36.794979Z","iopub.execute_input":"2021-08-03T08:05:36.795507Z","iopub.status.idle":"2021-08-03T08:05:45.489343Z","shell.execute_reply.started":"2021-08-03T08:05:36.795446Z","shell.execute_reply":"2021-08-03T08:05:45.488303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\nimport copy\nfrom keras_tuner.engine import tuner_utils\nimport numpy as np\nimport keras_tuner as kt\n\n# Reimplement bayesianOptimization to run with tpu\nclass TpuBayesianOptimizationOracle(kt.oracles.BayesianOptimizationOracle):\n    def _save_trial(self, trial):\n        # Write trial status to trial directory\n        trial_id = trial.trial_id\n        # trial.save(os.path.join(self._get_trial_dir(trial_id), \"trial.json\"))\n\nclass TpuBayesianOptimization(kt.engine.multi_execution_tuner.MultiExecutionTuner):\n    def __init__(\n        self,\n        hypermodel,\n        objective,\n        max_trials,\n        num_initial_points=2,\n        alpha=1e-4,\n        beta=2.6,\n        seed=None,\n        hyperparameters=None,\n        tune_new_entries=True,\n        allow_new_entries=True,\n        **kwargs\n    ):\n        oracle = TpuBayesianOptimizationOracle(\n            objective=objective,\n            max_trials=max_trials,\n            num_initial_points=num_initial_points,\n            alpha=alpha,\n            beta=beta,\n            seed=seed,\n            hyperparameters=hyperparameters,\n            tune_new_entries=tune_new_entries,\n            allow_new_entries=allow_new_entries,\n        )\n        super(\n            TpuBayesianOptimization,\n            self,\n        ).__init__(oracle=oracle, hypermodel=hypermodel, **kwargs)\n        \n    def run_trial(self, trial, *fit_args, **fit_kwargs):\n        original_callbacks = fit_kwargs.pop(\"callbacks\", [])\n        # Run the training process multiple times.\n        metrics = collections.defaultdict(list)\n        for execution in range(self.executions_per_trial):\n            copied_fit_kwargs = copy.copy(fit_kwargs)\n            callbacks = self._deepcopy_callbacks(original_callbacks)\n            self._configure_tensorboard_dir(callbacks, trial, execution)\n            callbacks.append(tuner_utils.TunerCallback(self, trial))\n            # Only checkpoint the best epoch across all executions.\n            copied_fit_kwargs[\"callbacks\"] = callbacks\n\n            history = self._build_and_fit_model(trial, fit_args, copied_fit_kwargs)\n            for metric, epoch_values in history.history.items():\n                if self.oracle.objective.direction == \"min\":\n                    best_value = np.min(epoch_values)\n                else:\n                    best_value = np.max(epoch_values)\n                metrics[metric].append(best_value)\n\n        # Average the results across executions and send to the Oracle.\n        averaged_metrics = {}\n        for metric, execution_values in metrics.items():\n            averaged_metrics[metric] = np.mean(execution_values)\n        self.oracle.update_trial(\n            trial.trial_id, metrics=averaged_metrics, step=self._reported_step\n        )","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:05:45.491313Z","iopub.execute_input":"2021-08-03T08:05:45.491997Z","iopub.status.idle":"2021-08-03T08:05:45.705347Z","shell.execute_reply.started":"2021-08-03T08:05:45.491951Z","shell.execute_reply":"2021-08-03T08:05:45.704153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Turn on tpu\n# Detect TPU, return appropriate distribution strategy\nstrategy = tf.distribute.get_strategy() \n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:05:45.707239Z","iopub.execute_input":"2021-08-03T08:05:45.707572Z","iopub.status.idle":"2021-08-03T08:05:51.275817Z","shell.execute_reply.started":"2021-08-03T08:05:45.70754Z","shell.execute_reply":"2021-08-03T08:05:51.27455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If you use private dataset, uncomment it\n#user_secrets = UserSecretsClient()\n#user_credential = user_secrets.get_gcloud_credential()\n#user_secrets.set_tensorflow_credential(user_credential)\n\n\nds_name = [\n    \"setitfrecdatasettest02\",\n    \"setitfrecdatasettest34\",\n    \"setitfrecdatasettrain0\",\n    \"setitfrecdatasettrain1\",\n    \"setitfrecdatasettrain2\"\n]\n\nds_path = list(map(\n    lambda name: KaggleDatasets().get_gcs_path(name),\n    ds_name\n))\n\ntrain_filenames = tf.io.gfile.glob(list(map(\n    lambda path: path + \"/train*.tfrecords\",\n    ds_path\n)))\n#val_filenames = tf.io.gfile.glob(GCS_PATH + '/val/*.tfrec')\ntest_filenames = tf.io.gfile.glob(list(map(\n    lambda path: path + \"/test*.tfrecords\",\n    ds_path\n)))\n\n\n# List dir with real regex\n# [x for x in os.listdir('.') if re.match('index_[0-9]*.csv', x)]","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:05:51.277441Z","iopub.execute_input":"2021-08-03T08:05:51.277781Z","iopub.status.idle":"2021-08-03T08:05:53.798148Z","shell.execute_reply.started":"2021-08-03T08:05:51.277749Z","shell.execute_reply":"2021-08-03T08:05:53.796567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read train data\ntrain_tfrec = tf.data.TFRecordDataset(train_filenames)\n\n# Read val data\n#val_tfrec = tf.data.TFRecordDataset(val_filenames)\n\n# Read test dataset\ntest_tfrec = tf.data.TFRecordDataset(test_filenames)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:05:53.799825Z","iopub.execute_input":"2021-08-03T08:05:53.800158Z","iopub.status.idle":"2021-08-03T08:05:53.831942Z","shell.execute_reply.started":"2021-08-03T08:05:53.800127Z","shell.execute_reply":"2021-08-03T08:05:53.830839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Parse an train example to get feature_description\nfor raw_record in train_tfrec.take(1):\n    example = tf.train.Example()\n    example.ParseFromString(raw_record.numpy())\n    # print(example)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-03T08:05:53.833733Z","iopub.execute_input":"2021-08-03T08:05:53.834088Z","iopub.status.idle":"2021-08-03T08:05:57.557723Z","shell.execute_reply.started":"2021-08-03T08:05:53.834034Z","shell.execute_reply":"2021-08-03T08:05:57.556712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_dataset(dataset: tf.data.Dataset, validation_data_percent: int):\n    \"\"\"\n    Splits a dataset of type tf.data.Dataset into a training and validation dataset using given ratio. Fractions are\n    rounded up to two decimal places.\n    @param dataset: the input dataset to split.\n    @param validation_data_fraction: the fraction of the validation data as a float between 0 and 1.\n    @return: a tuple of two tf.data.Datasets as (training, validation)\n    \"\"\"\n\n    if not (0 <= validation_data_percent <= 100):\n        raise ValueError(\"validation data percent must be âˆˆ [0,100]\")\n\n    dataset = dataset.enumerate()\n    train_dataset = dataset.filter(lambda f, data: f % 100 > validation_data_percent)\n    validation_dataset = dataset.filter(lambda f, data: f % 100 <= validation_data_percent)\n\n    # remove enumeration\n    train_dataset = train_dataset.map(lambda f, data: data)\n    validation_dataset = validation_dataset.map(lambda f, data: data)\n\n    return train_dataset, validation_dataset","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:05:57.562122Z","iopub.execute_input":"2021-08-03T08:05:57.562717Z","iopub.status.idle":"2021-08-03T08:05:57.572212Z","shell.execute_reply.started":"2021-08-03T08:05:57.562662Z","shell.execute_reply":"2021-08-03T08:05:57.570754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# parse tfrecord to get feature and label\nfeature_description = {\n    \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n    \"image_id\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n    \"target\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n}\n\ndef parse_labeled_data(example_proto):\n    # Parse the input `tf.train.Example` proto using the dictionary above.\n    parsed = tf.io.parse_single_example(example_proto, feature_description)\n    image = tf.io.decode_raw(parsed[\"image\"], tf.float16)\n    image = tf.reshape(image, [6, 273, 256])\n    # image = tf.transpose(image, [1, 0, 2])\n    image = tf.reshape(image, (273*6, 256))\n    image = tf.expand_dims(image, axis=2) # shape(273*6, 256, 1)\n    return image, parsed[\"target\"]\n\ndef parse_unlabeled_data(example_proto):\n    # Parse the input `tf.train.Example` proto using the dictionary above.\n    parsed = tf.io.parse_single_example(example_proto, feature_description)\n    image = tf.io.decode_raw(parsed[\"image\"], tf.float16)\n    image = tf.reshape(image, [6, 273, 256])\n    # image = tf.transpose(image, [1, 0, 2])\n    image = tf.reshape(image, (273*6, 256))\n    image = tf.expand_dims(image, axis=2) # shape(273*6, 256, 1)\n    return image, parsed[\"image_id\"]\n\ndataset = train_tfrec.map(parse_labeled_data, num_parallel_calls=10)\n\ntrain_dataset, val_dataset = split_dataset(dataset, 20)\n\ntrain_dataset = train_dataset.shuffle(60000).batch(128)\ntrain_dataset = train_dataset.prefetch(10)\n\nval_dataset = val_dataset.shuffle(60000).batch(128).cache()\nval_dataset = val_dataset.prefetch(10)\n\ntest_dataset = test_tfrec.map(parse_unlabeled_data).batch(32)\ntest_dataset = test_dataset.cache()\ntest_dataset = test_dataset.prefetch(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:05:57.574301Z","iopub.execute_input":"2021-08-03T08:05:57.575018Z","iopub.status.idle":"2021-08-03T08:05:57.919549Z","shell.execute_reply.started":"2021-08-03T08:05:57.57496Z","shell.execute_reply":"2021-08-03T08:05:57.918649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create model\ndef create_model():\n    with strategy.scope():\n        # dense = hp.Choice(\"dense\", values=[128, 256, 384, 512, 768, 1024])\n        # dropout = hp.Float(\"dropout\", 0.2, 0.7)\n        # lr = hp.Float(\"lr\", 1e-4, 1e-7)\n        # wd = hp.Float(\"wd\", 1e-5, 1e-9)\n        \n        dense = 1408\n        dropout = 0.30989813859771875\n        lr = 0.0001\n        wd = 1e-05\n        \n        pretrained_model = tf.keras.applications.efficientnet.EfficientNetB2(\n            include_top=False, weights=\"imagenet\"\n        )\n\n        model = tf.keras.Sequential([\n            layers.Conv2D(3, (1, 1), input_shape=(273*6, 256, 1)),\n            pretrained_model,\n            layers.GlobalAveragePooling2D(),\n            layers.Dropout(dropout),\n            layers.Dense(dense, activation='relu'),\n            layers.Dropout(dropout),\n            layers.Dense(dense, activation='relu'),\n            layers.Dropout(dropout),\n            layers.Dense(1, activation='sigmoid')\n        ])\n\n        model.compile(\n            optimizer=tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd),\n            loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n            metrics=[tf.keras.metrics.AUC()]\n        )\n\n    return model\n\n'''\ntuner = TpuBayesianOptimization(\n    create_model,\n    objective=kt.Objective(\"val_auc\", direction=\"max\"),\n    max_trials=15,\n    overwrite=True,\n    directory=\"tuner\",\n    distribution_strategy=strategy,\n    project_name=\"seti\",\n)\ntuner.search_space_summary()\ntuner.search(\n    train_dataset,\n    epochs=3, \n    validation_data=val_dataset\n)\nbest_hp = tuner.get_best_hyperparameters()[0]\nmodel = tuner.hypermodel.build(best_hp)\ntuner.results_summary(num_trials=15)\n'''\n\nmodel = create_model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:05:57.9209Z","iopub.execute_input":"2021-08-03T08:05:57.921741Z","iopub.status.idle":"2021-08-03T08:06:42.558893Z","shell.execute_reply.started":"2021-08-03T08:05:57.921603Z","shell.execute_reply":"2021-08-03T08:06:42.552903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model\ncheckpoint_filepath = 'best_checkpoint'\noptions = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_auc',\n    mode='max',\n    save_best_only=True,\n    options=options\n)\n\nmodel.fit(\n    train_dataset, \n    epochs=20, \n    validation_data=val_dataset,\n    callbacks=[model_checkpoint_callback]\n)\n# save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n# model.save('saved-model', options=save_locally)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:06:42.560507Z","iopub.status.idle":"2021-08-03T08:06:42.561424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load model, predict and write submission file\nmodel.load_weights(checkpoint_filepath, options=options)\n\ntest_images_ds = test_dataset.map(lambda image, idnum: image)\npredictions = model.predict(test_images_ds)\nprint(predictions)\n\nprint('Generating submission.csv file...')\ntest_ids_ds = test_dataset.map(lambda image, idnum: idnum).unbatch()\ntest_ids = next(iter(test_ids_ds.batch(np.size(predictions)))).numpy().astype('U') # all in one batch\ndata = {\n    \"id\": test_ids,\n}\nsubmission = pd.DataFrame(data)\nsubmission = submission.assign(target=predictions)\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:06:42.562945Z","iopub.status.idle":"2021-08-03T08:06:42.563427Z"},"trusted":true},"execution_count":null,"outputs":[]}]}