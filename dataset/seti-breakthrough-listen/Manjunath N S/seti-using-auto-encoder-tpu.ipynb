{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#!pip install tensorflow==2.6.0\nimport tensorflow as tf\n\n\nfrom tensorflow import keras\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import StratifiedKFold\nimport tensorflow_addons as tfa\nfrom sklearn.utils import class_weight\nimport os \nfrom tensorflow.keras import layers\n\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras import layers\nimport glob","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-18T19:53:34.946355Z","iopub.execute_input":"2021-08-18T19:53:34.946726Z","iopub.status.idle":"2021-08-18T19:53:34.9539Z","shell.execute_reply.started":"2021-08-18T19:53:34.94669Z","shell.execute_reply":"2021-08-18T19:53:34.952928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:34.95572Z","iopub.execute_input":"2021-08-18T19:53:34.956084Z","iopub.status.idle":"2021-08-18T19:53:34.975084Z","shell.execute_reply.started":"2021-08-18T19:53:34.956051Z","shell.execute_reply":"2021-08-18T19:53:34.973812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install tensorflow-addons==0.13.0\nimport tensorflow_addons  as tfa","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:34.977044Z","iopub.execute_input":"2021-08-18T19:53:34.977367Z","iopub.status.idle":"2021-08-18T19:53:34.98565Z","shell.execute_reply.started":"2021-08-18T19:53:34.977335Z","shell.execute_reply":"2021-08-18T19:53:34.984814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    import efficientnet.keras as efn\nexcept:\n    \n    !pip install -U efficientnet\n    import efficientnet.keras as efn\n","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:34.988152Z","iopub.execute_input":"2021-08-18T19:53:34.988444Z","iopub.status.idle":"2021-08-18T19:53:34.998022Z","shell.execute_reply.started":"2021-08-18T19:53:34.988405Z","shell.execute_reply":"2021-08-18T19:53:34.997297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:34.998958Z","iopub.execute_input":"2021-08-18T19:53:34.99935Z","iopub.status.idle":"2021-08-18T19:53:40.368311Z","shell.execute_reply.started":"2021-08-18T19:53:34.99932Z","shell.execute_reply":"2021-08-18T19:53:40.367143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CREATE_TF_RECORD = True","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:40.36954Z","iopub.execute_input":"2021-08-18T19:53:40.369859Z","iopub.status.idle":"2021-08-18T19:53:40.373891Z","shell.execute_reply.started":"2021-08-18T19:53:40.369828Z","shell.execute_reply":"2021-08-18T19:53:40.372778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv = \"../input/seti-breakthrough-listen/train_labels.csv\"\n\ntrain_df_master = pd.read_csv( train_csv )\ntrain_df_master[\"path\"] = train_df_master[\"id\"].apply( lambda x: \"../input/seti-breakthrough-listen/train/\"+ str(x[0]) +\"/\"+x +\".npy\" )\ntrain_df_master.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:40.375477Z","iopub.execute_input":"2021-08-18T19:53:40.376172Z","iopub.status.idle":"2021-08-18T19:53:40.480514Z","shell.execute_reply.started":"2021-08-18T19:53:40.376129Z","shell.execute_reply":"2021-08-18T19:53:40.479698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"COMPETITION_NAME = \"seti-breakthrough-listen\"\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\ntrain_df_master[\"tpu_path\"] = train_df_master[\"path\"].apply( lambda x : x.replace(\"../input/seti-breakthrough-listen\",GCS_DS_PATH))\ntrain_df_master.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T09:27:26.390106Z","iopub.execute_input":"2021-08-18T09:27:26.390503Z","iopub.status.idle":"2021-08-18T09:27:26.573416Z","shell.execute_reply.started":"2021-08-18T09:27:26.390466Z","shell.execute_reply":"2021-08-18T09:27:26.571493Z"}}},{"cell_type":"code","source":"CFG= {\n    \n    \"IMG_LENGTH\" :  256,\n    \"IMG_WIDTH\" : 256,\n    \"CHANNELS\" : 3,\n    \"RANDOM_STATE\" : 100,\n    \"BATCH_SIZE\"  :8*50 * strategy.num_replicas_in_sync,\n    \"FOLDS\" : 5,\n    \"LEARNING_RATE\" : 0.1\n}","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:40.482756Z","iopub.execute_input":"2021-08-18T19:53:40.483038Z","iopub.status.idle":"2021-08-18T19:53:40.487836Z","shell.execute_reply.started":"2021-08-18T19:53:40.48301Z","shell.execute_reply":"2021-08-18T19:53:40.486812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gcs_path= KaggleDatasets().get_gcs_path(\"seti-tfrecord-256x256\")\ntf_rec_file_list = glob.glob( gcs_path )","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:40.489717Z","iopub.execute_input":"2021-08-18T19:53:40.490043Z","iopub.status.idle":"2021-08-18T19:53:40.904832Z","shell.execute_reply.started":"2021-08-18T19:53:40.489981Z","shell.execute_reply":"2021-08-18T19:53:40.904089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ngcs_path= KaggleDatasets().get_gcs_path(\"seti-tfrecord-256x256\")\ntf_rec_file_list = glob.glob( gcs_path )\n\ntrain_df_master_2 = train_df_master\ntrain_df,test_df = train_test_split ( train_df_master_2, train_size = 0.8, random_state= CFG[\"RANDOM_STATE\"],shuffle = True,stratify = train_df_master_2[\"target\"])\n\nprint (\"number of samples for train data set  = {} \".format(len ( train_df) ) )\nprint (\"number of samples for test data set  = {} \".format(len ( test_df)))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:40.906712Z","iopub.execute_input":"2021-08-18T19:53:40.907067Z","iopub.status.idle":"2021-08-18T19:53:41.309701Z","shell.execute_reply.started":"2021-08-18T19:53:40.907037Z","shell.execute_reply":"2021-08-18T19:53:41.308934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Creating data generator which can work on Both TPU + GPU\n\ndef decode_numpy(  channel  ):\n    \n    def read_image(file_name, channel = None   ):\n        np_data =  tf.io.read_file ( file_name )\n        np_data = tf.io.decode_raw( np_data, tf.float16 )\n        np_data = tf.reshape( np_data[64:], (6, 273, 256 )) # (6, 273, 256 ) is data origional shape \n        np_data_1 = tf.stack( (np_data[0],np_data[2] ,np_data[4]), axis = 2 ) \n        #np_data_2 = tf.stack( (np_data[1],np_data[3] ,np_data[5]), axis = 2 ) \n        np_data_1 = tf.image.resize( np_data_1, (256,256))\n        #np_data_2 = tf.image.resize( np_data_2, (256,256))\n        \n        if channel == None: \n            random_int = np.random.randint(3)\n            np_data = np_data_1[:,:,random_int ] \n            return tf.stack( (np_data, np_data , np_data ), axis = 2) \n        else:\n            np_data = np_data_1[:,:,channel ] \n            return tf.stack( (np_data, np_data , np_data ), axis = 2) \n        \n        \n      \n    def decode( file_name,target ):\n        channel = None \n        return read_image ( file_name, channel   ),tf.cast(target, tf.float32)\n        \n    def decode_test_channel_0( file_name,target ):\n        channel = 0\n        return read_image ( file_name, channel   ),tf.cast(target, tf.float32)\n    \n    def decode_test_channel_1( file_name,target ):\n        channel = 1\n        return read_image ( file_name, channel   ),tf.cast(target, tf.float32) \n    \n    def decode_test_channel_2( file_name,target ):\n        channel = 2\n        return read_image ( file_name, channel   ),tf.cast(target, tf.float32) \n    \n    if channel == None :\n        \n        return decode\n    elif channel == 0 :\n        return  decode_test_channel_0\n\n    elif channel == 1 :\n        return  decode_test_channel_1\n\n    elif channel == 2 :\n        return  decode_test_channel_2\n\ndef data_augmentation( ):\n    \n    def add_augmentation( image, target ):\n        \n        image = tf.image.random_flip_left_right( image, seed=CFG[\"RANDOM_STATE\"] )\n        image = tf.image.random_flip_up_down( image, seed=CFG[\"RANDOM_STATE\"] )\n        image = tf.image.random_contrast( image,0.2,0.5, seed=CFG[\"RANDOM_STATE\"] )\n        \n        return image,target\n    \n    return  add_augmentation\n\ndef datagenerator_rev_02(df,test = False,channel = None ):\n    file_list = df[\"tpu_path\"].to_list() \n    target = df[\"target\"].to_list() \n    decode_tf = decode_numpy( channel )\n    augment_fn = data_augmentation()\n    \n    datagen = tf.data.Dataset.from_tensor_slices( (file_list,target ))\n    datagen = datagen.map( decode_tf ,num_parallel_calls= tf.data.AUTOTUNE )\n    datagen = datagen.map(augment_fn, num_parallel_calls= tf.data.AUTOTUNE ) if not test else datagen\n    datagen = datagen.repeat() if not test else datagen\n    datagen = datagen.shuffle(1024) if not test else datagen\n    datagen = datagen.batch(CFG[\"BATCH_SIZE\"])\n    datagen = datagen.prefetch(tf.data.AUTOTUNE )\n    return  datagen","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:41.310782Z","iopub.execute_input":"2021-08-18T19:53:41.311202Z","iopub.status.idle":"2021-08-18T19:53:41.328274Z","shell.execute_reply.started":"2021-08-18T19:53:41.31116Z","shell.execute_reply":"2021-08-18T19:53:41.327553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Creating data generator which can work on Both TPU + GPU\n\ndef decode_numpy(    ):\n    \n    def read_image(file_name, channel ):\n        np_data =  tf.io.read_file ( file_name )\n         \n        np_data = tf.reshape( tf.io.decode_raw( np_data, tf.float16 )[64:], (6, 273, 256 ))  # (6, 273, 256 ) is data origional shape \n        #np_data_1 = tf.stack( (np_data[0],np_data[2] ,np_data[4]), axis = 2 ) \n        #np_data_1 = tf.image.resize( np_data_1, (256,256))\n        #np_data = np_data_1[:,:,channel ] \n        if channel == 1 : channel = 2\n        if channel == 2 : channel = 4\n        return  tf.image.resize(tf.stack( (np_data[channel], np_data[channel] , np_data[channel] ), axis = 2) , (256,256) )\n        \n        \n      \n    def decode( file_name,target,channel ):\n        \n        return read_image ( file_name, channel   ),tf.cast(target, tf.float32)\n     \n    return decode\n    \n    \ndef data_augmentation( ):\n    \n    def add_augmentation( image, target ):\n        \n        return  tf.image.random_contrast( tf.image.random_flip_up_down( tf.image.random_flip_left_right( image, seed=CFG[\"RANDOM_STATE\"] \n                                                                                                       ), \n                                                                       seed=CFG[\"RANDOM_STATE\"] \n                                                                      ),\n                                         0.3,0.8, seed=CFG[\"RANDOM_STATE\"] \n                                        ), target\n       \n        #return image,target\n    \n    return  add_augmentation\n\ndef datagenerator_rev_02(df,test = False, channel =None  ):\n    file_list = df[\"tpu_path\"].to_list() \n    target = df[\"target\"].to_list() \n    df_channel = [channel]*df.shape[0] # df[\"channel\"].to_list() \n    \n    decode_tf = decode_numpy(  )\n    augment_fn = data_augmentation()\n    \n    datagen = tf.data.Dataset.from_tensor_slices( (file_list,target, df_channel ))\n    datagen = datagen.map( decode_tf ,num_parallel_calls= tf.data.AUTOTUNE )\n    datagen = datagen.map(augment_fn, num_parallel_calls= tf.data.AUTOTUNE ) if not test else datagen\n    datagen = datagen.repeat() if not test else datagen\n    datagen = datagen.shuffle(1024) if not test else datagen\n    datagen = datagen.batch(CFG[\"BATCH_SIZE\"])\n    datagen = datagen.prefetch(tf.data.AUTOTUNE )\n    return  datagen","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:41.329287Z","iopub.execute_input":"2021-08-18T19:53:41.329684Z","iopub.status.idle":"2021-08-18T19:53:41.342869Z","shell.execute_reply.started":"2021-08-18T19:53:41.329644Z","shell.execute_reply":"2021-08-18T19:53:41.342181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# File to TFRECORD conversion\n","metadata":{}},{"cell_type":"code","source":"def _bytes_feature(value):\n    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n    if isinstance(value, type(tf.constant(0))):\n        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _float_feature(value):\n    \"\"\"Returns a float_list from a float / double.\"\"\"\n    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\ndef _int64_feature(value):\n    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\ndef train_serialize_example(image, img_id, target ):\n    feature = {\n      'image'         : _bytes_feature(image),\n      'image_id'      : _bytes_feature(img_id),   \n      'target'        : _int64_feature(target),\n      }\n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n    return example_proto.SerializeToString()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:41.343914Z","iopub.execute_input":"2021-08-18T19:53:41.344311Z","iopub.status.idle":"2021-08-18T19:53:41.35808Z","shell.execute_reply.started":"2021-08-18T19:53:41.344272Z","shell.execute_reply":"2021-08-18T19:53:41.35718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nif CREATE_TF_RECORD and False :\n    \n    !mkdir \"256x256_channel_1_tf_record\"\n    each_file_contain = 100\n    \n    for i in range( 100, 200):\n        print ( i )\n        file_list = train_df_master[\"path\"][i*each_file_contain: (i+1)*each_file_contain] \n        target_val = train_df_master[\"target\"][i*each_file_contain: (i+1)*each_file_contain] \n\n        with tf.io.TFRecordWriter( \"./256x256_channel_1_tf_record/256x256_tfrecord_\" +str(i) + \".tfrec\" ) as writer:\n            for file_name, target in zip (file_list,target_val):\n                np_data = np.load ( file_name )\n                np_data= np.dstack(( np_data[0],np_data[2],np_data[4]))\n                np_data =  tf.image.resize( np_data,( 256,256) ).numpy()\n                np_data  = np_data.astype( np.float32 )\n                file_id  = file_name.split(\"/\")[-1].replace( \".npy\",\"_chan_\")\n                for channel in ( 0, 1, 2 ):\n                    example = train_serialize_example(np_data[:,:,channel].tobytes() , str.encode(file_id +\"str(channel)\"), target )\n                    writer.write(example)\n            writer.close()\n            \n    !zip -r  \"./256x256_channel_1_tf_record_part1.zip\" \"./256x256_channel_1_tf_record\"\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:41.359077Z","iopub.execute_input":"2021-08-18T19:53:41.359473Z","iopub.status.idle":"2021-08-18T19:53:41.377151Z","shell.execute_reply.started":"2021-08-18T19:53:41.359431Z","shell.execute_reply":"2021-08-18T19:53:41.376207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading TF Record","metadata":{}},{"cell_type":"code","source":"## code decode tfrecode \ndef decode_image(image_data):\n    image = tf.io.decode_raw( image_data,tf.float32 )\n    #image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n    #image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    return image\n\ndef prepare_target(target):    \n    target = tf.cast(target, tf.float32)            \n    target = tf.reshape(target, [1])         \n    return target\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\" : tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"image_id\":tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    image  = tf.reshape(image, [256, 256])\n    image = tf.stack( (image, image, image), axis = 2)\n    target = prepare_target(example['target'])\n    return image, target # returns a dataset of (image, label) pairs\n\ndef augmanet_data(image, target ):\n    \n    mask = random.randrange(2, 40, 2)\n   \n    #offset = random.randrange( 1, 200, 2 )\n    \n    image =  tf.image.random_contrast( tf.image.random_flip_up_down( tf.image.random_flip_left_right( image, seed=CFG[\"RANDOM_STATE\"]  ),  seed=CFG[\"RANDOM_STATE\"] \n                                                               ),0.3,0.8, seed=CFG[\"RANDOM_STATE\"] )\n    \n    #image= tf.squeeze(tfa.image.random_cutout( tf.expand_dims(image,0), (10, 10) ) )\n    #image = tfa.image.cutout( tf.expand_dims(image,0),(10,10), constant_values = 0.0,offset = (2,2,2) )\n    #image = tfa.image.cutout( images= image, mask_size = (mask,mask), constant_values = 0  )#, offset =(2,2 ), constant_values = 0)\n      \n    return image , target\n    \n                                        \n\ndef load_dataset(fileids, augment = False ,labeled=True, ordered=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(fileids, num_parallel_reads=tf.data.AUTOTUNE) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord,num_parallel_calls= tf.data.AUTOTUNE)\n   # dataset = dataset.map( augmanet_data ,num_parallel_calls= tf.data.AUTOTUNE) if augment else dataset\n    \n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset\n\n\n## Main function \ndef get_training_dataset(file_ist,repeat = True, augment= True   ):\n    dataset = load_dataset(file_ist,augment, labeled=True, ordered = False )\n    dataset = dataset.repeat()  if repeat else  dataset # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(20, seed=CFG[\"RANDOM_STATE\"])\n    dataset = dataset.batch(CFG[\"BATCH_SIZE\"])\n    dataset = dataset.prefetch(tf.data.AUTOTUNE) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\n#data_gen = get_training_dataset( train_files[:2],repeat = True, augment= True  )","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:41.378212Z","iopub.execute_input":"2021-08-18T19:53:41.378604Z","iopub.status.idle":"2021-08-18T19:53:41.3932Z","shell.execute_reply.started":"2021-08-18T19:53:41.378563Z","shell.execute_reply":"2021-08-18T19:53:41.392505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def short_effnet_model():\n    #with strategy.scope():\n        \n    model_input = layers.Input( shape=  ( CFG[\"IMG_LENGTH\"],CFG[\"IMG_WIDTH\"], 3 ) , name= \"encoder_input_layer\" )\n\n\n    efff_net =efn.EfficientNetB0(include_top = False, \n                                   weights =\"noisy-student\" , \n                                   input_shape = ( CFG[\"IMG_LENGTH\"], CFG[\"IMG_WIDTH\"], CFG[\"CHANNELS\"]) ,\n                                   input_tensor = model_input ,\n                                   classes=2,\n                                   pooling = True,\n                                   #classifier_activation='softmax',\n                                   drop_connect_rate= 0.7\n                                  ) \n\n    for layer in  efff_net.layers  : layer.trainable = True\n    \n    gaussian_noise = tf.keras.layers.GaussianNoise( stddev = 0.3 ) ( model_input )\n    random_crop = tf.keras.layers.experimental.preprocessing.RandomCrop( height = 30, width = 30 , seed=CFG[\"RANDOM_STATE\"]  ) (gaussian_noise)\n    random_flip =tf.keras.layers.experimental.preprocessing.RandomFlip( mode=\"horizontal_and_vertical\", seed=CFG[\"RANDOM_STATE\"] ) ( random_crop )\n    zoom_layer = tf.keras.layers.experimental.preprocessing.RandomZoom(  height_factor =(-0.3, -0.2)  , width_factor=(-0.3, -0.2), fill_mode='reflect', interpolation='bilinear', seed=CFG[\"RANDOM_STATE\"], fill_value=0.0 ) ( random_flip)\n    random_contrast = tf.keras.layers.experimental.preprocessing.RandomContrast( factor =[0.2, 0.8 ] , seed=CFG[\"RANDOM_STATE\"] ) ( zoom_layer )\n    \n    efff_net.layers[0] ( random_contrast )\n    layer_00 = efff_net.layers[-1].output\n    layer_01 = layers.Flatten()( layer_00 )\n    layer_02 = layers.Dense( 1, activation =\"sigmoid\") ( layer_01)\n    model_short = tf.keras.Model( inputs = model_input, outputs = layer_02 )\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate= 0.00126000004/2 ) \n    model_short.compile( optimizer= optimizer,loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n                 metrics=[tf.keras.metrics.AUC() ])#AUC(curve='ROC')\n    \n    return model_short","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:41.394225Z","iopub.execute_input":"2021-08-18T19:53:41.394619Z","iopub.status.idle":"2021-08-18T19:53:41.408969Z","shell.execute_reply.started":"2021-08-18T19:53:41.394579Z","shell.execute_reply":"2021-08-18T19:53:41.408072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_effnet = short_effnet_model()\nmodel_effnet.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:41.41001Z","iopub.execute_input":"2021-08-18T19:53:41.410516Z","iopub.status.idle":"2021-08-18T19:53:43.809837Z","shell.execute_reply.started":"2021-08-18T19:53:41.410465Z","shell.execute_reply":"2021-08-18T19:53:43.808828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gcs_path= KaggleDatasets().get_gcs_path(\"seti-tfrecord-256x256\") +\"/256x256_channel_tf_record\"\ntf_rec_file_list = glob.glob( gcs_path ) \ntf_rec_file_list","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:43.811039Z","iopub.execute_input":"2021-08-18T19:53:43.811308Z","iopub.status.idle":"2021-08-18T19:53:44.258018Z","shell.execute_reply.started":"2021-08-18T19:53:43.811281Z","shell.execute_reply":"2021-08-18T19:53:44.25713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG[\"RANDOM_STATE\"] =3000\ntrain_files , val_files = train_test_split ( glob.glob( \"../input/seti-tfrecord-256x256/256x256_channel_tf_record/*.tfrec\"), train_size = 0.8, random_state= CFG[\"RANDOM_STATE\"],shuffle = True)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:44.267272Z","iopub.execute_input":"2021-08-18T19:53:44.26771Z","iopub.status.idle":"2021-08-18T19:53:44.280612Z","shell.execute_reply.started":"2021-08-18T19:53:44.267678Z","shell.execute_reply":"2021-08-18T19:53:44.279591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if True :\n    \n    group_list =[]\n    for i in range( 0, 300 ):\n        group_list = group_list + [ i ]*200\n\n    train_df_master[\"group_tfrec\"] = group_list\n   \n    anamally_count_group = train_df_master[[\"group_tfrec\", \"target\"]].groupby( by = \"group_tfrec\").sum().reset_index()\n    \n    test_file_group  = [ int( x.split(\"/\")[-1].replace(\"256x256_tfrecord_\",\"\").replace(\".tfrec\",\"\")) for x in train_files ]\n    \n    anamally_sum = anamally_count_group[\"target\"].loc[ test_file_group ].sum()\n    \n    class_weight  = { 0:1, 1: ( ( ( len(train_files)* 200 ) - anamally_sum )  / anamally_sum ) }\n    class_weight","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:44.281573Z","iopub.execute_input":"2021-08-18T19:53:44.281836Z","iopub.status.idle":"2021-08-18T19:53:44.361728Z","shell.execute_reply.started":"2021-08-18T19:53:44.28181Z","shell.execute_reply":"2021-08-18T19:53:44.360753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weight","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:44.363146Z","iopub.execute_input":"2021-08-18T19:53:44.363421Z","iopub.status.idle":"2021-08-18T19:53:44.368788Z","shell.execute_reply.started":"2021-08-18T19:53:44.363395Z","shell.execute_reply":"2021-08-18T19:53:44.36788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n    \nwith strategy.scope():\n    model_effnet = short_effnet_model()\n\n\nlr_reducer = tf.keras.callbacks.ReduceLROnPlateau(  patience=2,\n                                                    min_lr= 0.000001,\n                                                    monitor='val_loss', \n                                                    factor=0.45, \n                                                    verbose=1,\n                                                    min_delta = 0.2,\n                                                    cooldown=2,\n                                                    mode='auto', \n                                                   )\n\n\n\n\ntrain_file_count = len( train_files )*200*3\nval_file_count  = len( val_files )*200 *3\n\nCFG[\"BATCH_SIZE\"]= 16*45 * strategy.num_replicas_in_sync\n\nCFG[\"TRAIN_STEPS\"] = int ( train_file_count /CFG[\"BATCH_SIZE\"] ) + (1 if train_file_count % CFG[\"BATCH_SIZE\"] != 0 else 0)\nCFG[\"VAL_STEPS\"] = int ( val_file_count/CFG[\"BATCH_SIZE\"] ) + (1 if val_file_count% CFG[\"BATCH_SIZE\"] != 0 else 0)\n\n\n\n#model_effnet.load_weights(\"../input/seti-gpu-rev-01-model/Efficient_Net_Model_Rev_01.h5\")\ncheckpoint = tf.keras.callbacks.ModelCheckpoint( f'model{1}.h5', save_best_only=True, monitor='val_loss', mode='min')\n\n\nmodel_history = model_effnet.fit( get_training_dataset([ gcs_path +\"/\"+ x.split(\"/\")[-1] for x in train_files],repeat = True, augment= True  ),\n                        class_weight= class_weight ,\n                         steps_per_epoch= CFG[\"TRAIN_STEPS\"], \n                         epochs =12, \n                         validation_data= get_training_dataset([ gcs_path +\"/\"+ x.split(\"/\")[-1] for x in val_files],repeat = True, augment= False  ),\n                         validation_steps = CFG[\"VAL_STEPS\"],\n                         callbacks=[ checkpoint,lr_reducer ]\n                       )\n","metadata":{"execution":{"iopub.status.busy":"2021-08-18T19:53:44.372124Z","iopub.execute_input":"2021-08-18T19:53:44.372394Z","iopub.status.idle":"2021-08-18T20:14:06.51039Z","shell.execute_reply.started":"2021-08-18T19:53:44.372368Z","shell.execute_reply":"2021-08-18T20:14:06.509165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST = False","metadata":{"execution":{"iopub.status.busy":"2021-08-18T20:14:06.513038Z","iopub.execute_input":"2021-08-18T20:14:06.513378Z","iopub.status.idle":"2021-08-18T20:14:06.523723Z","shell.execute_reply.started":"2021-08-18T20:14:06.513342Z","shell.execute_reply":"2021-08-18T20:14:06.522204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nif TEST:\n    ## Creating data generator which can work on Both TPU + GPU\n\n    def decode_numpy(    ):\n\n        def read_image(file_name, channel ):\n            np_data =  tf.io.read_file ( file_name )\n\n            np_data = tf.reshape( tf.io.decode_raw( np_data, tf.float16 )[64:], (6, 273, 256 ))  # (6, 273, 256 ) is data origional shape \n\n            if channel == 1 : channel = 2\n            if channel == 2 : channel = 4\n            return  tf.image.resize(tf.stack( (np_data[channel], np_data[channel] , np_data[channel] ), axis = 2) , (256,256) )\n\n\n\n        def decode( file_name,target,channel ):\n\n            return read_image ( file_name, channel   ),tf.cast(target, tf.float32)\n\n        return decode\n\n\n\n    def datagenerator_rev_03(df,test = False,channel = 0  ):\n        file_list = df[\"tpu_path\"].to_list() \n        target = df[\"target\"].to_list() \n        df_channel = [channel]*df.shape[0]\n\n        decode_tf = decode_numpy(  )\n        augment_fn = data_augmentation()\n\n        datagen = tf.data.Dataset.from_tensor_slices( (file_list,target, df_channel ))\n        datagen = datagen.map( decode_tf ,num_parallel_calls= tf.data.AUTOTUNE )\n        datagen = datagen.repeat() if not test else datagen\n        datagen = datagen.shuffle(1024) if not test else datagen\n        datagen = datagen.batch(CFG[\"BATCH_SIZE\"])\n        datagen = datagen.prefetch(tf.data.AUTOTUNE )\n        return  datagen","metadata":{"execution":{"iopub.status.busy":"2021-08-18T20:14:06.525859Z","iopub.execute_input":"2021-08-18T20:14:06.52641Z","iopub.status.idle":"2021-08-18T20:14:06.542403Z","shell.execute_reply.started":"2021-08-18T20:14:06.526363Z","shell.execute_reply":"2021-08-18T20:14:06.541391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TEST:\n    submission_gcs_path = KaggleDatasets().get_gcs_path(\"seti-breakthrough-listen\") +\"/test/\"\n    submission_df = pd.read_csv(\"../input/seti-breakthrough-listen/sample_submission.csv\")\n    submission_df[\"tpu_path\"] = submission_df[\"id\"].apply( lambda x: submission_gcs_path+ str(x[0]) +\"/\"+x +\".npy\" )\n    submission_df[\"target\"] = [1]*submission_df.shape[0]\n","metadata":{"execution":{"iopub.status.busy":"2021-08-18T20:14:06.543751Z","iopub.execute_input":"2021-08-18T20:14:06.544145Z","iopub.status.idle":"2021-08-18T20:14:06.561091Z","shell.execute_reply.started":"2021-08-18T20:14:06.544108Z","shell.execute_reply":"2021-08-18T20:14:06.560044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TEST:\n    CFG[\"BATCH_SIZE\"]= 20 * strategy.num_replicas_in_sync\n    submission_data_gen = datagenerator_rev_03( submission_df, True, 0   )\n    submission_df[\"prediction_Channel_0\"] = model_effnet.predict( submission_data_gen )\n    print (\"completed channel-0\")\n    submission_data_gen = datagenerator_rev_03( submission_df, True, 1   )\n    submission_df[\"prediction_Channel_1\"] = model_effnet.predict( submission_data_gen ) \n    print (\"completed channel -1\")\n    submission_data_gen = datagenerator_rev_03( submission_df, True, 2   )\n    submission_df[\"prediction_Channel_2\"] = model_effnet.predict( submission_data_gen ) \n    print (\"completed channel -2\")\n    submission_df[\"target\"] =submission_df[[\"prediction_Channel_0\",\"prediction_Channel_1\",\"prediction_Channel_2\"]].apply( lambda x :  np.max( x ),axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T20:14:06.562274Z","iopub.execute_input":"2021-08-18T20:14:06.56255Z","iopub.status.idle":"2021-08-18T20:14:06.574878Z","shell.execute_reply.started":"2021-08-18T20:14:06.562523Z","shell.execute_reply":"2021-08-18T20:14:06.573791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TEST: submission_df[\"target\"] =submission_df[[\"prediction_Channel_0\",\"prediction_Channel_1\",\"prediction_Channel_2\"]].apply( lambda x :  np.max( x ),axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T20:14:06.576669Z","iopub.execute_input":"2021-08-18T20:14:06.576975Z","iopub.status.idle":"2021-08-18T20:14:06.595584Z","shell.execute_reply.started":"2021-08-18T20:14:06.576946Z","shell.execute_reply":"2021-08-18T20:14:06.594373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TEST: submission_df[[\"id\",\"target\"]].to_csv(\"./sample_submission.csv\",index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T20:14:06.597505Z","iopub.execute_input":"2021-08-18T20:14:06.597854Z","iopub.status.idle":"2021-08-18T20:14:06.613572Z","shell.execute_reply.started":"2021-08-18T20:14:06.597825Z","shell.execute_reply":"2021-08-18T20:14:06.6122Z"},"trusted":true},"execution_count":null,"outputs":[]}]}