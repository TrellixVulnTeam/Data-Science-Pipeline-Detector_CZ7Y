{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"References and Credit :\n\nhttps://www.kaggle.com/awsaf49/seti-bl-tf-starter-tpu\n","metadata":{}},{"cell_type":"markdown","source":"![](https://drive.google.com/uc?id=1rkibSC447DCaBvjbwAOvD1gvg8mhOy3g)","metadata":{}},{"cell_type":"markdown","source":"<center><h1>Problem Statement üìù</h1></center>\n\n###  üéØGoal: \n>**The goal of this competition is to identify the anomalous signals in the scans of Breakthrough Listen targets**\n\n###  üéØDescription: \n<div class=\"alert alert-block alert-warning\">   \n\n<strong>Green Bank Telescope :</strong>\n\nüìå The Robert C. Byrd Green Bank Telescope, or GBT, is the world‚Äôs premiere single-dish radio telescope operating at meter to millimeter wavelengths. Its enormous 100-meter diameter collecting area, its unblocked aperture, and its excellent surface accuracy provide unprecedented sensitivity across the telescope‚Äôs full 0.1 ‚Äì 116 GHz (3.0m ‚Äì 2.6mm) operating range.\n    \n<br>\n\nüìå The single focal plane is ideal for rapid, wide-field imaging systems ‚Äì cameras. Because the GBT has access to 85% of the celestial sphere, it serves as the wide-field imaging complement to ALMA and the EVLA. Its operation is highly efficient, and it is used for observations about 6500 hours every year, with 2000-3000 hours per year available to high frequency science.\n    \n<br>\n\n![](https://drive.google.com/uc?id=1d3ZXLr5yKUseO_aEICWozdF3OT9HcW5P)\n    \n<br>\n    \n<strong> About Breakthrough Listen project: </strong>\n\nüìå It is an initiative to find signs of intelligent life in the universe. This 100 million dollar project was started by famous cosmologist Stephen Hawkins and Yuri Milner(an Internet investor) in 2015. Its teams are spread all over the world to find signs of intelligent life in universe. It is a ten years project where around 1,000,000 stars near to earth will be surveyed by scanning whole galactic plane of milky way.\n    \n<br>\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"**This problem is as interesting as debating about the existence of Super Power or GOD . Aamir Khan's Peekay movie which released in 2014 was one of my favourite movies in Bollywood . The film follows an alien who comes to Earth on a research mission, but loses his remote to a thief, who later sells it to a godman.**\n\n![](https://drive.google.com/uc?id=1jmm2VSVq7zkJuY1EJcKgDRVdAtql9XpG)\n\n","metadata":{}},{"cell_type":"markdown","source":"<center><h1>Data Description ü§ø </h1></center>\n\n> - ```train/``` - a training set of cadence snippet files stored in numpy float16 format (v1.20.1), one file per cadence snippet id, with corresponding labels found in the train_labels.csv file. Each file has dimension (6, 273, 256), with the 1st dimension representing the 6 positions of the cadence, and the 2nd and 3rd dimensions representing the 2D spectrogram.\n> - ```test/``` - the test set cadence snippet files; you must predict whether or not the cadence contains a \"needle\", which is the target for this competition\n> - ```sample_submission.csv``` - a sample submission file in the correct format\ntrain_labels - targets corresponding (by id) to the cadence snippet files found in the train/ folder \n> \n","metadata":{}},{"cell_type":"markdown","source":"<center><h1>Understanding the Data </h1></center>\n<div class=\"alert alert-block alert-info\">  \n\nBreakthrough Listen generates similar spectrograms which are stored either as filterbank format or HDF5 format files.\n<br>\n\nüìå <strong> FilterBank Format :</strong>\n<br>\n2D Arrays of intensity as a function of frequency and time, accompanied by headers containing metadata such as the direction the telescope was pointed in, the frequency scale.\n<br>\nüìå <strong> Snippets :</strong>\n<br>\nNumpy arrays consisting of small regions of the spectrograms .\n<br>\nüìå <strong> Technosignatures :</strong>\n<br>\nTechnosignature or technomarker is any measurable property or effect that provides scientific evidence of past or present technology. Technosignatures are analogous to the biosignatures that signal the presence of life, whether or not intelligent.\n<br>\nüìå <strong> Cadence Snippets :</strong>\n<br>\nConsider three nearby stars A , B and C to our primary target. Isolation of candidate technosignatures from RFI happens by alternating observations of primary target star with observations of three nearby stars .  5 minutes on star ‚ÄúA‚Äù, then 5 minutes on star ‚ÄúB‚Äù, then back to star ‚ÄúA‚Äù for 5 minutes, then ‚ÄúC‚Äù, then back to ‚ÄúA‚Äù, then finishing with 5 minutes on star ‚ÄúD‚Äù. One set of six observations (ABACAD) is referred to as a ‚Äúcadence‚Äù.\n<br>\nüìå <strong> Haystack :</strong>\n<br>\nThousands of cadence snippets put together is referred to as haystack\n<br>\nüìå <strong> Needle Signal /Target :</strong>\n<br>\nThe goal of this problem is to identify the hidden needle signals in this haystack .\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<center><h1>Import Libraries üìö</h1></center>","metadata":{}},{"cell_type":"code","source":"! pip install -q efficientnet","metadata":{"_kg_hide-output":true,"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom functools import partial\nimport efficientnet.tfkeras as efn\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA üìä\n\n<center><h1>Coming Soon </h1></center>","metadata":{}},{"cell_type":"code","source":"# TPU or GPU detection\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU {tpu.master()}')\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH = KaggleDatasets().get_gcs_path('setibl-256x256-tfrec-dataset')\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [256, 256]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"files_train = np.sort(np.array(tf.io.gfile.glob(GCS_PATH + '/train*.tfrec')))\nTEST_FILENAMES  = np.sort(np.array(tf.io.gfile.glob(GCS_PATH + '/test*.tfrec')))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAINING_FILENAMES, VALID_FILENAMES = train_test_split(\n    files_train,\n    test_size=0.2, random_state=42\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train TFRecord Files:', len(TRAINING_FILENAMES))\nprint('Validation TFRecord Files:', len(VALID_FILENAMES))\nprint('Test TFRecord Files:', len(TEST_FILENAMES))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_train_tfrecord(example):\n    tfrec_format = {\n        'image'                        : tf.io.FixedLenFeature([], tf.string),\n        'image_id'                     : tf.io.FixedLenFeature([], tf.string),\n        'target'                       : tf.io.FixedLenFeature([], tf.int64)\n    }           \n    example = tf.io.parse_single_example(example, tfrec_format)\n    return example['image'], example['target']\n\n\ndef read_test_tfrecord(example, return_image_id):\n    tfrec_format = {\n        'image'                        : tf.io.FixedLenFeature([], tf.string),\n        'image_id'                     : tf.io.FixedLenFeature([], tf.string),\n    }\n    example = tf.io.parse_single_example(example, tfrec_format)\n    return example['image'], example['image_id'] if return_image_id else 0\n\n \ndef prepare_image(img, augment=True, dim=IMAGE_SIZE[0]):    \n    img = tf.image.decode_png(img, channels=3)\n    img = tf.cast(img, tf.float32) / 255.0\n    \n    if augment:\n        \n        img = tf.image.random_flip_left_right(img)\n           \n                      \n    img = tf.reshape(img, [dim,dim, 3])\n            \n    return img\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"      \ndef load_dataset(filenames, ordered=False, augment = True,labeled=True ,return_image_ids=True):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n    dataset = dataset.with_options(ignore_order)\n    if labeled: \n        dataset = dataset.map(read_train_tfrecord, num_parallel_calls=AUTO)\n    else:\n        dataset = dataset.map(lambda example: read_test_tfrecord(example, return_image_ids), \n                    num_parallel_calls=AUTO) \n    dataset = dataset.map(lambda img, imgid_or_label: (prepare_image(img, augment=augment, dim=IMAGE_SIZE[0]), \n                                               imgid_or_label), \n                num_parallel_calls=AUTO)\n    return dataset\n        \n\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, ordered=False, augment = True,labeled=True)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(128, seed = 0)\n    dataset = dataset.batch(BATCH_SIZE,drop_remainder=True)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_validation_dataset(ordered=True):\n    dataset = load_dataset(VALID_FILENAMES, ordered=ordered, augment = False,labeled=True)\n    dataset = dataset.batch(BATCH_SIZE,drop_remainder=True)\n    #dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, ordered=ordered, augment = False,labeled=True  ,return_image_ids=True)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\nNUM_VALIDATION_IMAGES = count_data_items(VALID_FILENAMES)\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\nprint(\n    'Dataset: {} training images, {} validation images, {} unlabeled test images'.format(\n        NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES\n    )\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_lrfn(lr_start=0.00001, lr_max=0.000075, lr_min=0.000001, lr_rampup_epochs=20, lr_sustain_epochs=0, lr_exp_decay=.8):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay ** (epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    return lrfn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    \n    train_dataset = get_training_dataset()\n    valid_dataset = get_validation_dataset()\n    \n    model = tf.keras.Sequential([\n        efn.EfficientNetB6(\n            input_shape=(256,256,3),\n            weights='imagenet',\n            include_top=False\n        ),\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(512, activation= 'relu'), \n        tf.keras.layers.Dropout(0.25), \n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss = 'binary_crossentropy',\n        metrics=['accuracy']\n    )\n\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lrfn = build_lrfn()\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\nVALID_STEPS = NUM_VALIDATION_IMAGES // BATCH_SIZE\n\n\nhistory = model.fit(\n    train_dataset, epochs=1,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=valid_dataset,\n    validation_steps=VALID_STEPS,\n    callbacks=[\n        tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1),\n        tf.keras.callbacks.ModelCheckpoint(\n            os.path.join(\"./model.h5\"),\n            monitor='train_loss', verbose=0,\n            save_best_only=True, save_weights_only=False,\n            mode='auto', save_freq='epoch'\n        )\n    ]\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> Work in progress üöß </h1>","metadata":{}}]}