{"cells":[{"metadata":{"_uuid":"da5e89e841a7306acd3788a56701dd775dbaaf64"},"cell_type":"markdown","source":"Along the popular kernel notebook: https://www.kaggle.com/artgor/movie-review-sentiment-analysis-eda-and-models"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom nltk.tokenize import TweetTokenizer\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\npd.set_option('max_colwidth',400)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv', sep=\"\\t\")\ntest = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv', sep=\"\\t\")\nsub = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv', sep=\",\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"037a39698fc99385d67087e77681bb6edb380278"},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"983d9581c1c1ef97ba98e02719786881a772b427"},"cell_type":"markdown","source":"# Definition of features 変数の定義の確認\n\nfrom the [data](https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only/data) page:\n\n```\nThe dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset.\n\nThe train/test split has been preserved for the purposes of benchmarking, \nbut the sentences have been shuffled from their original order. \n\nEach Sentence has been parsed into many phrases by the Stanford parser. \nEach phrase has a PhraseId. \nEach sentence has a SentenceId. \nPhrases that are repeated (such as short/common words) are only included once in the data.\n\ntrain.tsv contains the phrases and their associated sentiment labels. \nWe have additionally provided a SentenceId so that you can track which phrases belong to a single sentence.\n   \ntest.tsv contains just phrases. \nYou must assign a sentiment label to each phrase.\n\nThe sentiment labels are:\n0 - negative\n1 - somewhat negative\n2 - neutral\n3 - somewhat positive\n4 - positive\n```\n\nthat is, \n```\nwhole document\n|\n└ sentence <--> SentenceId\n|   └ phrase <--> PhraseId, Sentence, Sentiment\n|   └ phrase <--> PhraseId, Sentence, Sentiment\n|   └ phrase <--> PhraseId, Sentence, Sentiment\n|\n└ sentence <--> SentenceId\n|   └ phrase <--> PhraseId, Sentence, Sentiment\n|   └ phrase <--> PhraseId, Sentence, Sentiment\n|   └ phrase <--> PhraseId, Sentence, Sentiment\n|\n└ sentence <--> SentenceId\n|   └ phrase <--> PhraseId, Sentence, Sentiment\n|   └ phrase <--> PhraseId, Sentence, Sentiment\n|   └ phrase <--> PhraseId, Sentence, Sentiment\n...\n```"},{"metadata":{"trusted":true,"_uuid":"27ea0dc62f5cc17916fd05fde82e3476b33bb9ef"},"cell_type":"code","source":"train.loc[train.SentenceId == 10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fce66e9f1a679e50df8d4d0642cca2970113728"},"cell_type":"code","source":"# Average count of phrases per sentence in train is:\ntrain.groupby('SentenceId')['Phrase'].count().mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b59d0cd936d3914c5c248b7566110382e6a65604"},"cell_type":"code","source":"# Average count of phrases per sentence in test is:\ntest.groupby('SentenceId')['Phrase'].count().mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb54512f0d544ed964874ad24b30f3c7c2e6bb9e"},"cell_type":"code","source":"# Number of phrases in train:\ntrain.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fde6b58005f33a75c9faf673ed0077063c092bd0"},"cell_type":"code","source":"# Number of sentences in train:\nlen(train.SentenceId.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc56699fff7c7c160bb60b9a90b8e27977b1a7be"},"cell_type":"code","source":"# Number of phrases in test:\ntest.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6e1d06360cca9e46f08c36f2d5773cdb2fea732"},"cell_type":"code","source":"# Number of sentences in test:\nlen(test.SentenceId.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00b8aa6186c1e3012a5eff450b40265091539b29"},"cell_type":"code","source":"# Average word length of phrases in train is:\ntrain.Phrase.apply(lambda x: x.count(\" \") + 1).mean()\n# or train.Phrase.apply(lambda x: len(x.split())).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4701ea3b20e23090cd8585f28f42d0ff1e84fcf2"},"cell_type":"code","source":"# Average word length of phrases in test is:\ntest.Phrase.apply(lambda x : len(x.split())).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bb4f3af728f8b2d3f70ca31f1aa1ac589ea257c"},"cell_type":"markdown","source":"# Most common trigrams for positive phrases"},{"metadata":{"_uuid":"b5e5e039624160223090841f6940f436b7ccbd14"},"cell_type":"markdown","source":"### Concatenate phrases"},{"metadata":{"trusted":true,"_uuid":"4f2c248c6f2fc61b1edcd29f97a829b88d4a45c9"},"cell_type":"code","source":"text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\ntext","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06f701ba90f86e9451f45b64107b4635f32e6273"},"cell_type":"code","source":"ngrams(text.split(), 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d06f51ee90198ab94eb1d4e3b67c98ffe44735a5"},"cell_type":"code","source":"list(ngrams(text.split(), 3))[:5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a5b1e8958b9a302ec1cb638cef45216d3f40654"},"cell_type":"markdown","source":"### Most Common Trigrams"},{"metadata":{"trusted":true,"_uuid":"5afcfc70b73b637a0936adb28cdae4dc96cf5781"},"cell_type":"code","source":"text_trigrams = list(ngrams(text.split(), 3))\ntext_trigrams","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"f41ccc2b899535e7faebbd9f13529a597abbccdb"},"cell_type":"code","source":"Counter(text_trigrams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75a2dce524130f5f382c8859805ba8f390cd2f37"},"cell_type":"code","source":"Counter(text_trigrams).most_common(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68380ee87bf24e244f24f2dafe434f24a96bd507"},"cell_type":"markdown","source":"### excluding the stop words"},{"metadata":{"trusted":true,"_uuid":"f713ff6401bce34813c3cb6149023f207ed1e2a4"},"cell_type":"code","source":"text_ = [i for i in text.split() if i not in stopwords.words('english')]\ntext_trigrams = list(ngrams(text_, 3))\nCounter(text_trigrams).most_common(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e815d881c8e63e0665f72c03334a443fd30608ae"},"cell_type":"code","source":"stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80d9a9d59fa09efbab649a0976e99470049cb474"},"cell_type":"markdown","source":"> The results show the main problem with this dataset: there are to many common words due to sentenced splitted in phrases. As a result stopwords shouldn't be removed from text."},{"metadata":{"_uuid":"43dedeeda2c521f73695633c83e9e9d430d96cbb"},"cell_type":"markdown","source":">>\nSo, we have only phrases as data. And a phrase can contain a single word. And one punctuation mark can cause phrase to receive a different sentiment. Also assigned sentiments can be strange. This means several things:\n> -    using stopwords can be a bad idea, especially when phrases contain one single stopword;\n> -    puntuation could be important, so it should be used;\n> -    ngrams are necessary to get the most info from data;\n> -    using features like word count or sentence length won't be useful;\n\nor,\n- **Good idea to focus on:**\n    - punctuations\n    - using ngrams\n- **Not good idea to focus on:**\n    - stopwords\n    - wordcounts, sentence lengths\n"},{"metadata":{"_uuid":"796adf259d99cc4554df33619e8d0cd9f7e9a0a0"},"cell_type":"markdown","source":"### Introducing Tokenizer"},{"metadata":{"trusted":true,"_uuid":"b529488972b772b61039e687b81cbf77963028ec"},"cell_type":"code","source":"tokenizer = TweetTokenizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"657c33ba8108ab228d948ac75accd076760d6612"},"cell_type":"code","source":"tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b77e00c58cab326c0bf0b05ea863b3ab7912d88"},"cell_type":"code","source":"tokenizer.tokenize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"148d536334aaa8c21b8695ce238bb904d1558412"},"cell_type":"code","source":"tokenizer.tokenize('Hello world, Mr. Smith! I am new to this field.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4fd57f59b9a2b09288e3d09a4bc5ef26fc71313"},"cell_type":"markdown","source":"### Introducing Vectorizer with TweetTokenizer"},{"metadata":{"trusted":true,"_uuid":"4a7f58a2a9311c69b00ffee41a56a2d3de037675"},"cell_type":"code","source":"vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\nvectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc253af0b45a36f089228081e6598acf0b00c27a"},"cell_type":"code","source":"full_text = list(train.Phrase.values) + list(test.Phrase.values)\nfull_text\n\n# QUESTION: Is it OK to tokenize phrases in test data?\n# Preprocessing is outside the training?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96d553cec325394e4b0580df795795ccac7f5072"},"cell_type":"code","source":"vectorizer.fit(full_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ba975e4822bbb0532d17b6e8816915ff15ff514"},"cell_type":"code","source":"train_vectorized = vectorizer.transform(train.Phrase)\ntest_vectorized = vectorizer.transform(test.Phrase)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b78bd8a65b7bc586e5295d72e2922724e9e53bdd"},"cell_type":"code","source":"y = train.Sentiment","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b3b7116b24e88d89c504d770707829c9e7de104"},"cell_type":"markdown","source":"### Approach 1. Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"38bbac18fb31888ec6cfde0246d1d954eca3a87f"},"cell_type":"code","source":"logreg = LogisticRegression()\novr = OneVsRestClassifier(logreg) # LEARN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d73e2e475a8734625eb69586bd6c60e801191584"},"cell_type":"code","source":"%%time\novr.fit(train_vectorized, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"452476e45919ee64116196db612683df5e3a9b75"},"cell_type":"markdown","source":"### Scoring"},{"metadata":{"trusted":true,"_uuid":"902ee1836131b062973e0bcada8a346c958cd1b3"},"cell_type":"code","source":"scores = cross_val_score(ovr, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebacce3650adfda91e86a971cac5eb1446b6a064"},"cell_type":"code","source":"# Cross-validation mean accuracy\nscores.mean(), scores.std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"72c14a3ce486fc448599d66698bfe23ebc7a7888"},"cell_type":"code","source":"print('accuracy: {0:.2f}%, std: {1:.2f}%pt'.format(scores.mean() * 100, scores.std() * 100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4994695b1e0efe01407714eaaf259951d1fc102"},"cell_type":"markdown","source":"### Approach 2. Linear SVC"},{"metadata":{"trusted":true,"_uuid":"19078600780f6b0e34862e3d4dc9acd60f8d5120"},"cell_type":"code","source":"%%time\nsvc = LinearSVC(dual=False)\nsvc.fit(train_vectorized, y)\nscores = cross_val_score(svc, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\nprint('CV mean accuracy: {0:.2f}%, std: {1:.2f}%pt'.format(scores.mean()*100, scores.std()*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e62ee4d9e769bbe9930ad7f9f7798479e479df20"},"cell_type":"markdown","source":"### Approach 3. Deep Learning"},{"metadata":{"_uuid":"855f93a02a1e998d740e8cd48f136faa98015ffe"},"cell_type":"markdown","source":">>\nAnd now let's try DL. DL should work better for text classification with multiple layers. I use an architecture similar to those which were used in toxic competition."},{"metadata":{"trusted":true,"_uuid":"8b80ab4778df35742d93a16bff918dd2ff9ff01a"},"cell_type":"code","source":"# LEARN: toxic competition","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a46679c725f6b85fab89324b7ca1ffcf52cd5ab"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\n\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da3b64f6a72d3f278b978959374e2ca5d2b9042d"},"cell_type":"markdown","source":"### Tokenize (keras way)"},{"metadata":{"trusted":true,"_uuid":"38ad70c184dc290d5eaa8fdaa8505642a1c60fa8"},"cell_type":"code","source":"tk = Tokenizer(lower=True, filters='')\ntk.fit_on_texts(full_text)\ntrain_tokenized = tk.texts_to_sequences(train.Phrase)\ntest_tokenized = tk.texts_to_sequences(test.Phrase)\n\n# NOTE: tokenize is to convert each word into integer code (frequent words only)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a63d7432bb7b89fc239b02ea6b21b130651011c5"},"cell_type":"code","source":"tk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"487cf22171e741166b31b1c0573a001de243c7f7"},"cell_type":"code","source":"full_text[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0c50d2bbe994dfb7b1b746dac5888494109e6c7"},"cell_type":"code","source":"len(train.Phrase)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"933efdf9ef6e57fdc152306ac8a901e5212775ce"},"cell_type":"code","source":"len(train_tokenized), train_tokenized\n# NOTE: number of train_tokenized's rows equals to that of train.Phase","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df5eea8687bf7f8ae9a7f3ed1d850ffe4198e84a"},"cell_type":"code","source":"max_len = 50\nX_train = pad_sequences(train_tokenized, maxlen=max_len) # NOTE: justified each row to right, padding zeros on the left\nX_test = pad_sequences(test_tokenized, maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d594e7fef6b0780ac3633e5a96b4b7b7840758de"},"cell_type":"code","source":"X_train[:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b99b80ce889fc79e9bb00a0205e85cfe919f4ce7"},"cell_type":"markdown","source":"### Create embeddings matrix for each word"},{"metadata":{"trusted":true,"_uuid":"9e9f8b8b0eb6349221d222f794c1dc6e6b81175d"},"cell_type":"code","source":"embedding_path = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\n\n# NOTE: serialized matrix of shape (2M rows, 300 dimention embeddings), \n# each row has 300-dimentional embeddings for the preceding word\n\n# LEARN: FastText, from where this embeddings data comes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"951cff60e083e8764a2137e0d358591bb46b9aab"},"cell_type":"code","source":"embed_size = 300\nmax_features = 3e4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"711fcbbdd7e58715d60153b541cca7ef28f92b57"},"cell_type":"code","source":"list(o for o in open(embedding_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"fb889da5806a9c9e50aecf68153382a82e41131a"},"cell_type":"code","source":"list(o.split()[0] for o in open(embedding_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cd498cede95b53af21a663ce51f25517759ca2f"},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n# NOTE: 'word, *arr' corresponds to each text row in the embedding file\n# this function converts only the array part into numpy array\n# in preparation for creating dictionary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"171b77da3da504ff5c12ed01f9e2e81bbfd17186"},"cell_type":"code","source":"%%time\nembedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"462f8db4a91d7b929f8f0f13e1607de25bc8daa2"},"cell_type":"code","source":"embedding_index # array of embeddings for words included in FastText file (not yet adjusted for the dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fceb6d844932d088b0d629c33eea05685ae60c75"},"cell_type":"code","source":"word_index = tk.word_index\nword_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32a4564554093aaf0aaae816fd07f714863ce88b"},"cell_type":"code","source":"nb_words = min(max_features, len(word_index)) # NOTE: 'nb' means 'number'\nembedding_matrix = np.zeros((nb_words + 1, embed_size)) \nembedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c82e0e7361d59ae7cdfefd5633b22d12f0eacb36"},"cell_type":"code","source":"for word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d66ff0515c9f26f8e0647ae8edf50d687f61751"},"cell_type":"code","source":"embedding_matrix # array of embeddings for most frequent words in full_text","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0d9c7b0fc6e29aed9e4bf0f3c0216757fc63f3b"},"cell_type":"markdown","source":"### One Hot Encoding"},{"metadata":{"trusted":true,"_uuid":"2b475e1f634da1f14fb0db6dbefbb23a9e875cad"},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\ny_ohe = ohe.fit_transform(y.values.reshape(-1, 1)) \n# NOTE: fit/transform done together for one-hot encoding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd4812a02f5715e8a066a6ad66195bcdb5a218c3"},"cell_type":"code","source":"np.reshape?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd51850ff9e7c86db498d92a08ac79755408701b"},"cell_type":"code","source":"y.values.reshape?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64a0612be1231c259c43d79fba6d3c5bc64062a7"},"cell_type":"code","source":"y.values.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8030050d82a644a9d1315859c268c140c67bed31"},"cell_type":"code","source":"y_ohe # one-hot encoded version of labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c18078305f9a0c11686d0ef2b86b0d696799ebc7"},"cell_type":"code","source":"def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, \n                 kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n    file_path = 'best_model.hdf5'\n    check_point = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, \n                                 save_best_only=True, mode='min')\n    early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=3)\n    \n    inp = Input(shape=(max_len, )) \n    \n    \n    x = Embedding(19479, embed_size, weights=[embedding_matrix], trainable=False)(inp) # QUESTION: what is 19479 ??\n    x1 = SpatialDropout1D(spatial_dr)(x)\n    \n    x_gru = Bidirectional(CuDNNGRU(units, return_sequences=True))(x1)\n    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)    \n    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n    max_pool1_gru = GlobalMaxPooling1D()(x1)\n    \n    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)    \n    avg_pool3_gru = GlobalAveragePooling1D()(x1)\n    max_pool3_gru = GlobalMaxPooling1D()(x1)\n    \n    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences=True))(x1)\n    \n    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)    \n    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n    \n    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)    \n    avg_pool3_lstm = GlobalAveragePooling1D()(x1)\n    max_pool3_lstm = GlobalMaxPooling1D()(x1)\n    \n    \n    x = concatenate([\n        avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n        avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm,\n    ])\n    x = BatchNormalization()(x)\n    x = Dense(dense_units, activation='relu')(x)\n    x = Dropout(dr)(x)\n    x = BatchNormalization()(x)\n    x = Dense(int(dense_units / 2), activation='relu')(x)\n    x = Dropout(dr)(x)\n    x = Dense(5, activation='sigmoid')(x)\n    \n    model = Model(inputs=inp, outputs=x)\n    model.compile(\n        loss='binary_crossentropy', \n        optimizer=Adam(lr=lr, decay=lr_d), \n        metrics=['accuracy']\n    )\n    history = model.fit(X_train, y_ohe, \n        batch_size=128, epochs=10, validation_split=0.1, verbose=1, \n        callbacks=[check_point, early_stop]\n    ) # execute fitting\n    model = load_model(file_path) # load best model obtained\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d086836d73adf9877627bec44ab07ac4458608a6"},"cell_type":"markdown","source":"### Build model 1"},{"metadata":{"trusted":true,"_uuid":"470da0b7768c935f2189fb9a7395929c01118642"},"cell_type":"code","source":"model1 = build_model1(lr=1e-3, lr_d=1e-10, units=64, spatial_dr=0.3, \n                      kernel_size1=3, kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"298a3f99b6ea697bafc9717b68d7a60a6a8c0c75"},"cell_type":"markdown","source":"```\nTrain on 140454 samples, validate on 15606 samples\nEpoch 1/20\n140454/140454 [==============================] - 78s 556us/step - loss: 0.3517 - acc: 0.8377 - val_loss: 0.3242 - val_acc: 0.8485\n\nEpoch 00001: val_loss improved from inf to 0.32423, saving model to best_model.hdf5\nEpoch 2/20\n140454/140454 [==============================] - 74s 524us/step - loss: 0.3103 - acc: 0.8585 - val_loss: 0.3098 - val_acc: 0.8547\n\nEpoch 00002: val_loss improved from 0.32423 to 0.30981, saving model to best_model.hdf5\nEpoch 3/20\n140454/140454 [==============================] - 71s 507us/step - loss: 0.3004 - acc: 0.8627 - val_loss: 0.3084 - val_acc: 0.8581\n\nEpoch 00003: val_loss improved from 0.30981 to 0.30841, saving model to best_model.hdf5\nEpoch 4/20\n140454/140454 [==============================] - 75s 531us/step - loss: 0.2924 - acc: 0.8669 - val_loss: 0.3162 - val_acc: 0.8562\n\nEpoch 00004: val_loss did not improve from 0.30841\nEpoch 5/20\n140454/140454 [==============================] - 76s 545us/step - loss: 0.2858 - acc: 0.8697 - val_loss: 0.3043 - val_acc: 0.8584\n\nEpoch 00005: val_loss improved from 0.30841 to 0.30430, saving model to best_model.hdf5\nEpoch 6/20\n140454/140454 [==============================] - 75s 537us/step - loss: 0.2800 - acc: 0.8725 - val_loss: 0.3053 - val_acc: 0.8574\n\nEpoch 00006: val_loss did not improve from 0.30430\nEpoch 7/20\n140454/140454 [==============================] - 74s 524us/step - loss: 0.2757 - acc: 0.8741 - val_loss: 0.3031 - val_acc: 0.8579\n\nEpoch 00007: val_loss improved from 0.30430 to 0.30311, saving model to best_model.hdf5\nEpoch 8/20\n140454/140454 [==============================] - 76s 541us/step - loss: 0.2719 - acc: 0.8764 - val_loss: 0.3013 - val_acc: 0.8610\n\nEpoch 00008: val_loss improved from 0.30311 to 0.30131, saving model to best_model.hdf5\nEpoch 9/20\n140454/140454 [==============================] - 75s 531us/step - loss: 0.2687 - acc: 0.8783 - val_loss: 0.3037 - val_acc: 0.8597\n\nEpoch 00009: val_loss did not improve from 0.30131\nEpoch 10/20\n140454/140454 [==============================] - 73s 523us/step - loss: 0.2658 - acc: 0.8795 - val_loss: 0.3026 - val_acc: 0.8611\n\nEpoch 00010: val_loss did not improve from 0.30131\nEpoch 11/20\n140454/140454 [==============================] - 76s 544us/step - loss: 0.2636 - acc: 0.8809 - val_loss: 0.3044 - val_acc: 0.8588\n\nEpoch 00011: val_loss did not improve from 0.30131\n```"},{"metadata":{"trusted":true,"_uuid":"7206ea44baa2adae89b991b74477d5f9719f63d2"},"cell_type":"code","source":"pred1 = model1.predict(X_test, batch_size = 1024, verbose = 1)\npred1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d5da286648e72e3c9559769d89e46eeb2b2ac73"},"cell_type":"code","source":"pred = pred1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d51ae8f30e933bd19b899748ff0f55ee65b7e292"},"cell_type":"code","source":"predictions = np.round(np.argmax(pred, axis=1)).astype(int)\npredictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57d92e96afbd9cadfcf8ae15edefd7007354526e"},"cell_type":"code","source":"sub['Sentiment'] = predictions\nsub.to_csv(\"blend.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}