{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['movie-review-sentiment-analysis-kernels-only', 'glove-twitter', 'fasttext-crawl-300d-2m']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom nltk.tokenize import TweetTokenizer\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\npd.set_option('max_colwidth',400)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv', sep=\"\\t\")\ntest = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv', sep=\"\\t\")\nsub = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv', sep=\",\")","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(15)","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"    PhraseId    ...      Sentiment\n0          1    ...              1\n1          2    ...              2\n2          3    ...              2\n3          4    ...              2\n4          5    ...              2\n5          6    ...              2\n6          7    ...              2\n7          8    ...              2\n8          9    ...              2\n9         10    ...              2\n10        11    ...              2\n11        12    ...              2\n12        13    ...              2\n13        14    ...              2\n14        15    ...              2\n\n[15 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PhraseId</th>\n      <th>SentenceId</th>\n      <th>Phrase</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>A series of escapades demonstrating the adage that what is good for the goose</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>A series</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>A</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>series</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>1</td>\n      <td>of escapades demonstrating the adage that what is good for the goose</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>1</td>\n      <td>of</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>1</td>\n      <td>escapades demonstrating the adage that what is good for the goose</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>1</td>\n      <td>escapades</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>1</td>\n      <td>demonstrating the adage that what is good for the goose</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>1</td>\n      <td>demonstrating the adage</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>1</td>\n      <td>demonstrating</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>1</td>\n      <td>the adage</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14</td>\n      <td>1</td>\n      <td>the</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>15</td>\n      <td>1</td>\n      <td>adage</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"   PhraseId                           ...                                                                            Phrase\n0    156061                           ...                            An intermittently pleasing but mostly routine effort .\n1    156062                           ...                              An intermittently pleasing but mostly routine effort\n2    156063                           ...                                                                                An\n3    156064                           ...                                 intermittently pleasing but mostly routine effort\n4    156065                           ...                                        intermittently pleasing but mostly routine\n\n[5 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PhraseId</th>\n      <th>SentenceId</th>\n      <th>Phrase</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>156061</td>\n      <td>8545</td>\n      <td>An intermittently pleasing but mostly routine effort .</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>156062</td>\n      <td>8545</td>\n      <td>An intermittently pleasing but mostly routine effort</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>156063</td>\n      <td>8545</td>\n      <td>An</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>156064</td>\n      <td>8545</td>\n      <td>intermittently pleasing but mostly routine effort</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>156065</td>\n      <td>8545</td>\n      <td>intermittently pleasing but mostly routine</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train.SentenceId == 2]","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"    PhraseId    ...      Sentiment\n63        64    ...              4\n64        65    ...              3\n65        66    ...              2\n66        67    ...              4\n67        68    ...              3\n68        69    ...              2\n69        70    ...              3\n70        71    ...              3\n71        72    ...              3\n72        73    ...              2\n73        74    ...              2\n74        75    ...              4\n75        76    ...              2\n76        77    ...              3\n77        78    ...              4\n78        79    ...              2\n79        80    ...              2\n80        81    ...              2\n\n[18 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PhraseId</th>\n      <th>SentenceId</th>\n      <th>Phrase</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>63</th>\n      <td>64</td>\n      <td>2</td>\n      <td>This quiet , introspective and entertaining independent is worth seeking .</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>65</td>\n      <td>2</td>\n      <td>This quiet , introspective and entertaining independent</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>66</td>\n      <td>2</td>\n      <td>This</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>67</td>\n      <td>2</td>\n      <td>quiet , introspective and entertaining independent</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>68</td>\n      <td>2</td>\n      <td>quiet , introspective and entertaining</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>69</td>\n      <td>2</td>\n      <td>quiet</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>70</td>\n      <td>2</td>\n      <td>, introspective and entertaining</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>71</td>\n      <td>2</td>\n      <td>introspective and entertaining</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>72</td>\n      <td>2</td>\n      <td>introspective and</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>73</td>\n      <td>2</td>\n      <td>introspective</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>74</td>\n      <td>2</td>\n      <td>and</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>75</td>\n      <td>2</td>\n      <td>entertaining</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>76</td>\n      <td>2</td>\n      <td>independent</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>77</td>\n      <td>2</td>\n      <td>is worth seeking .</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>78</td>\n      <td>2</td>\n      <td>is worth seeking</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>79</td>\n      <td>2</td>\n      <td>is worth</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>80</td>\n      <td>2</td>\n      <td>worth</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>81</td>\n      <td>2</td>\n      <td>seeking</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('SentenceId')['Phrase'].count().mean()))\nprint('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('SentenceId')['Phrase'].count().mean()))","execution_count":7,"outputs":[{"output_type":"stream","text":"Average count of phrases per sentence in train is 18.\nAverage count of phrases per sentence in test is 20.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of phrases in train: {}. Number of sentences in train: {}.'.format(train.shape[0], len(train.SentenceId.unique())))\nprint('Number of phrases in test: {}. Number of sentences in test: {}.'.format(test.shape[0], len(test.SentenceId.unique())))","execution_count":8,"outputs":[{"output_type":"stream","text":"Number of phrases in train: 156060. Number of sentences in train: 8529.\nNumber of phrases in test: 66292. Number of sentences in test: 3310.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Average word length of phrases in train is {0:.0f}.'.format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\nprint('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))","execution_count":9,"outputs":[{"output_type":"stream","text":"Average word length of phrases in train is 7.\nAverage word length of phrases in test is 7.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\ntext_trigrams = [i for i in ngrams(text.split(), 3)]","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(text_trigrams).most_common(30)","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"[(('one', 'of', 'the'), 199),\n (('of', 'the', 'year'), 103),\n (('.', 'is', 'a'), 87),\n (('of', 'the', 'best'), 80),\n (('of', 'the', 'most'), 70),\n (('is', 'one', 'of'), 50),\n (('One', 'of', 'the'), 43),\n ((',', 'and', 'the'), 40),\n (('the', 'year', \"'s\"), 38),\n (('It', \"'s\", 'a'), 38),\n (('it', \"'s\", 'a'), 37),\n (('.', \"'s\", 'a'), 37),\n (('a', 'movie', 'that'), 35),\n (('the', 'edge', 'of'), 34),\n (('the', 'kind', 'of'), 33),\n (('of', 'your', 'seat'), 33),\n (('the', 'film', 'is'), 31),\n ((',', 'this', 'is'), 31),\n (('the', 'film', \"'s\"), 31),\n ((',', 'the', 'film'), 30),\n (('film', 'that', 'is'), 30),\n (('as', 'one', 'of'), 30),\n (('edge', 'of', 'your'), 29),\n ((',', 'it', \"'s\"), 27),\n (('a', 'film', 'that'), 27),\n (('as', 'well', 'as'), 27),\n ((',', 'funny', ','), 25),\n ((',', 'but', 'it'), 23),\n (('films', 'of', 'the'), 23),\n (('some', 'of', 'the'), 23)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\ntext = [i for i in text.split() if i not in stopwords.words('english')]\ntext_trigrams = [i for i in ngrams(text, 3)]\nCounter(text_trigrams).most_common(30)","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"[((',', 'funny', ','), 33),\n (('one', 'year', \"'s\"), 28),\n (('year', \"'s\", 'best'), 26),\n (('movies', 'ever', 'made'), 19),\n ((',', 'solid', 'cast'), 19),\n (('solid', 'cast', ','), 18),\n ((\"'ve\", 'ever', 'seen'), 16),\n (('.', 'It', \"'s\"), 16),\n ((',', 'making', 'one'), 15),\n (('best', 'films', 'year'), 15),\n ((',', 'touching', ','), 15),\n (('exquisite', 'acting', ','), 15),\n (('acting', ',', 'inventive'), 14),\n ((',', 'inventive', 'screenplay'), 14),\n (('jaw-dropping', 'action', 'sequences'), 14),\n (('good', 'acting', ','), 14),\n ((\"'s\", 'best', 'films'), 14),\n (('I', \"'ve\", 'seen'), 14),\n (('funny', ',', 'even'), 14),\n (('best', 'war', 'movies'), 13),\n (('purely', 'enjoyable', 'satisfying'), 13),\n (('funny', ',', 'touching'), 13),\n ((',', 'smart', ','), 13),\n (('inventive', 'screenplay', ','), 13),\n (('funniest', 'jokes', 'movie'), 13),\n (('action', 'sequences', ','), 13),\n (('sequences', ',', 'striking'), 13),\n ((',', 'striking', 'villains'), 13),\n (('exquisite', 'motion', 'picture'), 13),\n (('war', 'movies', 'ever'), 12)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**Feature processing and engineering****"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = TweetTokenizer()","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\nfull_text = list(train['Phrase'].values) + list(test['Phrase'].values)\nvectorizer.fit(full_text)\ntrain_vectorized = vectorizer.transform(train['Phrase'])\ntest_vectorized = vectorizer.transform(test['Phrase'])","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['Sentiment']","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()\novr = OneVsRestClassifier(logreg)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\novr.fit(train_vectorized, y)","execution_count":17,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"CPU times: user 8.2 s, sys: 12 ms, total: 8.21 s\nWall time: 8.21 s\n","name":"stdout"},{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='warn',\n          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n          tol=0.0001, verbose=0, warm_start=False),\n          n_jobs=None)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(ovr, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\nprint('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))","execution_count":18,"outputs":[{"output_type":"stream","text":"Cross-validation mean accuracy 56.55%, std 0.07.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nsvc = LinearSVC(dual=False)\nscores = cross_val_score(svc, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\nprint('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))","execution_count":19,"outputs":[{"output_type":"stream","text":"Cross-validation mean accuracy 56.51%, std 0.68.\nCPU times: user 64 ms, sys: 20 ms, total: 84 ms\nWall time: 22.6 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"ovr.fit(train_vectorized, y)\nsvc.fit(train_vectorized, y)","execution_count":20,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n","name":"stderr"},{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n     verbose=0)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**Deep Learning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU \nfrom keras.layers import CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\n\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping","execution_count":21,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tk = Tokenizer(lower = True, filters = '')\ntk.fit_on_texts(full_text)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tokenized = tk.texts_to_sequences(train['Phrase'])\ntest_tokenized = tk.texts_to_sequences(test['Phrase'])","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 50\nX_train = pad_sequences(train_tokenized, maxlen = max_len)\nX_test = pad_sequences(test_tokenized, maxlen = max_len)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_path = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_size = 300\nmax_features = 30000","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n\nword_index = tk.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words + 1, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\ny_ohe = ohe.fit_transform(y.values.reshape(-1, 1))","execution_count":28,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\nIf you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\nIn case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n  warnings.warn(msg, FutureWarning)\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"**Model 1 Architecture**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, \n                 dense_units=128, dr=0.0,conv_size=32):\n    file_path = \"best_model.hdf5\"\n    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                                  save_best_only = True, mode = \"min\")\n    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n    \n    inp = Input(shape = (max_len,))\n    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n    x1 = SpatialDropout1D(spatial_dr)(x)\n\n    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n    max_pool1_gru = GlobalMaxPooling1D()(x1)\n    \n    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n    max_pool3_gru = GlobalMaxPooling1D()(x3)\n    \n    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n    \n    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n    \n    \n    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n    x = Dense(5, activation = \"sigmoid\")(x)\n    model = Model(inputs = inp, outputs = x)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n                        verbose = 1, callbacks = [check_point, early_stop])\n    model = load_model(file_path)\n    return model","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, \n                      kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)","execution_count":30,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nDeprecated in favor of operator or tf.math.divide.\nTrain on 140454 samples, validate on 15606 samples\nEpoch 1/20\n140454/140454 [==============================] - 28s 198us/step - loss: 0.3642 - acc: 0.8311 - val_loss: 0.3177 - val_acc: 0.8509\n\nEpoch 00001: val_loss improved from inf to 0.31765, saving model to best_model.hdf5\nEpoch 2/20\n140454/140454 [==============================] - 23s 162us/step - loss: 0.3123 - acc: 0.8577 - val_loss: 0.3094 - val_acc: 0.8554\n\nEpoch 00002: val_loss improved from 0.31765 to 0.30944, saving model to best_model.hdf5\nEpoch 3/20\n140454/140454 [==============================] - 23s 162us/step - loss: 0.3005 - acc: 0.8628 - val_loss: 0.3063 - val_acc: 0.8570\n\nEpoch 00003: val_loss improved from 0.30944 to 0.30633, saving model to best_model.hdf5\nEpoch 4/20\n140454/140454 [==============================] - 23s 163us/step - loss: 0.2923 - acc: 0.8665 - val_loss: 0.3044 - val_acc: 0.8583\n\nEpoch 00004: val_loss improved from 0.30633 to 0.30442, saving model to best_model.hdf5\nEpoch 5/20\n140454/140454 [==============================] - 23s 161us/step - loss: 0.2855 - acc: 0.8696 - val_loss: 0.2996 - val_acc: 0.8610\n\nEpoch 00005: val_loss improved from 0.30442 to 0.29962, saving model to best_model.hdf5\nEpoch 6/20\n140454/140454 [==============================] - 23s 166us/step - loss: 0.2797 - acc: 0.8723 - val_loss: 0.3042 - val_acc: 0.8592\n\nEpoch 00006: val_loss did not improve from 0.29962\nEpoch 7/20\n140454/140454 [==============================] - 23s 162us/step - loss: 0.2756 - acc: 0.8741 - val_loss: 0.3020 - val_acc: 0.8609\n\nEpoch 00007: val_loss did not improve from 0.29962\nEpoch 8/20\n140454/140454 [==============================] - 23s 162us/step - loss: 0.2720 - acc: 0.8757 - val_loss: 0.2982 - val_acc: 0.8625\n\nEpoch 00008: val_loss improved from 0.29962 to 0.29820, saving model to best_model.hdf5\nEpoch 9/20\n140454/140454 [==============================] - 23s 162us/step - loss: 0.2683 - acc: 0.8781 - val_loss: 0.3051 - val_acc: 0.8603\n\nEpoch 00009: val_loss did not improve from 0.29820\nEpoch 10/20\n140454/140454 [==============================] - 23s 162us/step - loss: 0.2657 - acc: 0.8794 - val_loss: 0.3029 - val_acc: 0.8610\n\nEpoch 00010: val_loss did not improve from 0.29820\nEpoch 11/20\n140454/140454 [==============================] - 23s 162us/step - loss: 0.2633 - acc: 0.8803 - val_loss: 0.3010 - val_acc: 0.8614\n\nEpoch 00011: val_loss did not improve from 0.29820\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 128, \n                      spatial_dr = 0.5, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.2, conv_size=32)","execution_count":31,"outputs":[{"output_type":"stream","text":"Train on 140454 samples, validate on 15606 samples\nEpoch 1/20\n140454/140454 [==============================] - 33s 236us/step - loss: 0.3676 - acc: 0.8330 - val_loss: 0.3188 - val_acc: 0.8491\n\nEpoch 00001: val_loss improved from inf to 0.31879, saving model to best_model.hdf5\nEpoch 2/20\n140454/140454 [==============================] - 29s 204us/step - loss: 0.3204 - acc: 0.8540 - val_loss: 0.3115 - val_acc: 0.8533\n\nEpoch 00002: val_loss improved from 0.31879 to 0.31146, saving model to best_model.hdf5\nEpoch 3/20\n140454/140454 [==============================] - 29s 204us/step - loss: 0.3117 - acc: 0.8579 - val_loss: 0.3076 - val_acc: 0.8570\n\nEpoch 00003: val_loss improved from 0.31146 to 0.30757, saving model to best_model.hdf5\nEpoch 4/20\n140454/140454 [==============================] - 29s 209us/step - loss: 0.3035 - acc: 0.8614 - val_loss: 0.3081 - val_acc: 0.8579\n\nEpoch 00004: val_loss did not improve from 0.30757\nEpoch 5/20\n140454/140454 [==============================] - 29s 205us/step - loss: 0.2964 - acc: 0.8649 - val_loss: 0.3013 - val_acc: 0.8607\n\nEpoch 00005: val_loss improved from 0.30757 to 0.30127, saving model to best_model.hdf5\nEpoch 6/20\n140454/140454 [==============================] - 29s 205us/step - loss: 0.2913 - acc: 0.8668 - val_loss: 0.3038 - val_acc: 0.8601\n\nEpoch 00006: val_loss did not improve from 0.30127\nEpoch 7/20\n140454/140454 [==============================] - 29s 209us/step - loss: 0.2865 - acc: 0.8696 - val_loss: 0.3016 - val_acc: 0.8610\n\nEpoch 00007: val_loss did not improve from 0.30127\nEpoch 8/20\n140454/140454 [==============================] - 29s 204us/step - loss: 0.2827 - acc: 0.8709 - val_loss: 0.3010 - val_acc: 0.8606\n\nEpoch 00008: val_loss improved from 0.30127 to 0.30099, saving model to best_model.hdf5\nEpoch 9/20\n140454/140454 [==============================] - 29s 207us/step - loss: 0.2789 - acc: 0.8731 - val_loss: 0.3008 - val_acc: 0.8612\n\nEpoch 00009: val_loss improved from 0.30099 to 0.30077, saving model to best_model.hdf5\nEpoch 10/20\n140454/140454 [==============================] - 29s 204us/step - loss: 0.2752 - acc: 0.8749 - val_loss: 0.3023 - val_acc: 0.8626\n\nEpoch 00010: val_loss did not improve from 0.30077\nEpoch 11/20\n140454/140454 [==============================] - 29s 205us/step - loss: 0.2728 - acc: 0.8765 - val_loss: 0.2990 - val_acc: 0.8625\n\nEpoch 00011: val_loss improved from 0.30077 to 0.29904, saving model to best_model.hdf5\nEpoch 12/20\n140454/140454 [==============================] - 29s 208us/step - loss: 0.2706 - acc: 0.8776 - val_loss: 0.2997 - val_acc: 0.8625\n\nEpoch 00012: val_loss did not improve from 0.29904\nEpoch 13/20\n140454/140454 [==============================] - 29s 204us/step - loss: 0.2679 - acc: 0.8790 - val_loss: 0.3006 - val_acc: 0.8625\n\nEpoch 00013: val_loss did not improve from 0.29904\nEpoch 14/20\n140454/140454 [==============================] - 29s 205us/step - loss: 0.2657 - acc: 0.8797 - val_loss: 0.2998 - val_acc: 0.8623\n\nEpoch 00014: val_loss did not improve from 0.29904\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model2(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n    file_path = \"best_model.hdf5\"\n    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                                  save_best_only = True, mode = \"min\")\n    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n\n    inp = Input(shape = (max_len,))\n    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n    x1 = SpatialDropout1D(spatial_dr)(x)\n\n    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n    \n    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n    \n    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n    \n    \n    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n    \n    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n    \n    \n    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n                    avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n    x = Dense(5, activation = \"sigmoid\")(x)\n    model = Model(inputs = inp, outputs = x)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n                        verbose = 1, callbacks = [check_point, early_stop])\n    model = load_model(file_path)\n    return model","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3 = build_model2(lr = 1e-4, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=4, \n                      kernel_size2=3, dense_units=32, dr=0.1, conv_size=32)","execution_count":33,"outputs":[{"output_type":"stream","text":"Train on 140454 samples, validate on 15606 samples\nEpoch 1/20\n140454/140454 [==============================] - 29s 204us/step - loss: 0.4568 - acc: 0.7994 - val_loss: 0.3648 - val_acc: 0.8438\n\nEpoch 00001: val_loss improved from inf to 0.36479, saving model to best_model.hdf5\nEpoch 2/20\n140454/140454 [==============================] - 23s 166us/step - loss: 0.3692 - acc: 0.8391 - val_loss: 0.3374 - val_acc: 0.8477\n\nEpoch 00002: val_loss improved from 0.36479 to 0.33737, saving model to best_model.hdf5\nEpoch 3/20\n140454/140454 [==============================] - 24s 173us/step - loss: 0.3508 - acc: 0.8440 - val_loss: 0.3282 - val_acc: 0.8492\n\nEpoch 00003: val_loss improved from 0.33737 to 0.32823, saving model to best_model.hdf5\nEpoch 4/20\n140454/140454 [==============================] - 23s 166us/step - loss: 0.3411 - acc: 0.8469 - val_loss: 0.3218 - val_acc: 0.8509\n\nEpoch 00004: val_loss improved from 0.32823 to 0.32183, saving model to best_model.hdf5\nEpoch 5/20\n140454/140454 [==============================] - 23s 166us/step - loss: 0.3346 - acc: 0.8490 - val_loss: 0.3197 - val_acc: 0.8521\n\nEpoch 00005: val_loss improved from 0.32183 to 0.31974, saving model to best_model.hdf5\nEpoch 6/20\n140454/140454 [==============================] - 23s 166us/step - loss: 0.3303 - acc: 0.8506 - val_loss: 0.3186 - val_acc: 0.8533\n\nEpoch 00006: val_loss improved from 0.31974 to 0.31859, saving model to best_model.hdf5\nEpoch 7/20\n140454/140454 [==============================] - 24s 169us/step - loss: 0.3257 - acc: 0.8529 - val_loss: 0.3162 - val_acc: 0.8542\n\nEpoch 00007: val_loss improved from 0.31859 to 0.31621, saving model to best_model.hdf5\nEpoch 8/20\n140454/140454 [==============================] - 23s 167us/step - loss: 0.3223 - acc: 0.8538 - val_loss: 0.3145 - val_acc: 0.8552\n\nEpoch 00008: val_loss improved from 0.31621 to 0.31449, saving model to best_model.hdf5\nEpoch 9/20\n140454/140454 [==============================] - 23s 165us/step - loss: 0.3194 - acc: 0.8549 - val_loss: 0.3158 - val_acc: 0.8553\n\nEpoch 00009: val_loss did not improve from 0.31449\nEpoch 10/20\n140454/140454 [==============================] - 24s 169us/step - loss: 0.3165 - acc: 0.8561 - val_loss: 0.3111 - val_acc: 0.8556\n\nEpoch 00010: val_loss improved from 0.31449 to 0.31110, saving model to best_model.hdf5\nEpoch 11/20\n140454/140454 [==============================] - 23s 166us/step - loss: 0.3144 - acc: 0.8569 - val_loss: 0.3121 - val_acc: 0.8563\n\nEpoch 00011: val_loss did not improve from 0.31110\nEpoch 12/20\n140454/140454 [==============================] - 23s 165us/step - loss: 0.3118 - acc: 0.8580 - val_loss: 0.3098 - val_acc: 0.8572\n\nEpoch 00012: val_loss improved from 0.31110 to 0.30983, saving model to best_model.hdf5\nEpoch 13/20\n140454/140454 [==============================] - 24s 169us/step - loss: 0.3093 - acc: 0.8592 - val_loss: 0.3088 - val_acc: 0.8569\n\nEpoch 00013: val_loss improved from 0.30983 to 0.30881, saving model to best_model.hdf5\nEpoch 14/20\n140454/140454 [==============================] - 24s 168us/step - loss: 0.3078 - acc: 0.8593 - val_loss: 0.3073 - val_acc: 0.8577\n\nEpoch 00014: val_loss improved from 0.30881 to 0.30734, saving model to best_model.hdf5\nEpoch 15/20\n140454/140454 [==============================] - 23s 165us/step - loss: 0.3057 - acc: 0.8607 - val_loss: 0.3075 - val_acc: 0.8574\n\nEpoch 00015: val_loss did not improve from 0.30734\nEpoch 16/20\n140454/140454 [==============================] - 24s 170us/step - loss: 0.3042 - acc: 0.8611 - val_loss: 0.3061 - val_acc: 0.8572\n\nEpoch 00016: val_loss improved from 0.30734 to 0.30612, saving model to best_model.hdf5\nEpoch 17/20\n140454/140454 [==============================] - 24s 169us/step - loss: 0.3026 - acc: 0.8617 - val_loss: 0.3096 - val_acc: 0.8566\n\nEpoch 00017: val_loss did not improve from 0.30612\nEpoch 18/20\n140454/140454 [==============================] - 23s 166us/step - loss: 0.3010 - acc: 0.8626 - val_loss: 0.3062 - val_acc: 0.8579\n\nEpoch 00018: val_loss did not improve from 0.30612\nEpoch 19/20\n140454/140454 [==============================] - 23s 166us/step - loss: 0.2994 - acc: 0.8632 - val_loss: 0.3071 - val_acc: 0.8581\n\nEpoch 00019: val_loss did not improve from 0.30612\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model4 = build_model2(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, \n                      kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.3, conv_size=32)","execution_count":34,"outputs":[{"output_type":"stream","text":"Train on 140454 samples, validate on 15606 samples\nEpoch 1/20\n140454/140454 [==============================] - 30s 216us/step - loss: 0.3780 - acc: 0.8288 - val_loss: 0.3188 - val_acc: 0.8530\n\nEpoch 00001: val_loss improved from inf to 0.31881, saving model to best_model.hdf5\nEpoch 2/20\n140454/140454 [==============================] - 23s 165us/step - loss: 0.3250 - acc: 0.8534 - val_loss: 0.3125 - val_acc: 0.8531\n\nEpoch 00002: val_loss improved from 0.31881 to 0.31251, saving model to best_model.hdf5\nEpoch 3/20\n140454/140454 [==============================] - 24s 168us/step - loss: 0.3148 - acc: 0.8570 - val_loss: 0.3112 - val_acc: 0.8549\n\nEpoch 00003: val_loss improved from 0.31251 to 0.31121, saving model to best_model.hdf5\nEpoch 4/20\n140454/140454 [==============================] - 23s 166us/step - loss: 0.3078 - acc: 0.8599 - val_loss: 0.3059 - val_acc: 0.8571\n\nEpoch 00004: val_loss improved from 0.31121 to 0.30588, saving model to best_model.hdf5\nEpoch 5/20\n140454/140454 [==============================] - 24s 168us/step - loss: 0.3014 - acc: 0.8627 - val_loss: 0.3024 - val_acc: 0.8594\n\nEpoch 00005: val_loss improved from 0.30588 to 0.30237, saving model to best_model.hdf5\nEpoch 6/20\n140454/140454 [==============================] - 23s 166us/step - loss: 0.2963 - acc: 0.8651 - val_loss: 0.3063 - val_acc: 0.8598\n\nEpoch 00006: val_loss did not improve from 0.30237\nEpoch 7/20\n140454/140454 [==============================] - 24s 168us/step - loss: 0.2913 - acc: 0.8672 - val_loss: 0.3022 - val_acc: 0.8611\n\nEpoch 00007: val_loss improved from 0.30237 to 0.30222, saving model to best_model.hdf5\nEpoch 8/20\n140454/140454 [==============================] - 23s 166us/step - loss: 0.2876 - acc: 0.8692 - val_loss: 0.3015 - val_acc: 0.8601\n\nEpoch 00008: val_loss improved from 0.30222 to 0.30154, saving model to best_model.hdf5\nEpoch 9/20\n140454/140454 [==============================] - 24s 170us/step - loss: 0.2847 - acc: 0.8703 - val_loss: 0.2976 - val_acc: 0.8628\n\nEpoch 00009: val_loss improved from 0.30154 to 0.29758, saving model to best_model.hdf5\nEpoch 10/20\n140454/140454 [==============================] - 24s 169us/step - loss: 0.2817 - acc: 0.8718 - val_loss: 0.3035 - val_acc: 0.8616\n\nEpoch 00010: val_loss did not improve from 0.29758\nEpoch 11/20\n140454/140454 [==============================] - 23s 166us/step - loss: 0.2794 - acc: 0.8730 - val_loss: 0.3000 - val_acc: 0.8619\n\nEpoch 00011: val_loss did not improve from 0.29758\nEpoch 12/20\n140454/140454 [==============================] - 23s 165us/step - loss: 0.2772 - acc: 0.8738 - val_loss: 0.3013 - val_acc: 0.8612\n\nEpoch 00012: val_loss did not improve from 0.29758\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model5 = build_model2(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.3, \n                      kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.4, conv_size=64)","execution_count":35,"outputs":[{"output_type":"stream","text":"Train on 140454 samples, validate on 15606 samples\nEpoch 1/20\n140454/140454 [==============================] - 33s 236us/step - loss: 0.3755 - acc: 0.8291 - val_loss: 0.3222 - val_acc: 0.8507\n\nEpoch 00001: val_loss improved from inf to 0.32217, saving model to best_model.hdf5\nEpoch 2/20\n140454/140454 [==============================] - 25s 176us/step - loss: 0.3212 - acc: 0.8548 - val_loss: 0.3149 - val_acc: 0.8521\n\nEpoch 00002: val_loss improved from 0.32217 to 0.31487, saving model to best_model.hdf5\nEpoch 3/20\n140454/140454 [==============================] - 25s 180us/step - loss: 0.3076 - acc: 0.8594 - val_loss: 0.3076 - val_acc: 0.8583\n\nEpoch 00003: val_loss improved from 0.31487 to 0.30757, saving model to best_model.hdf5\nEpoch 4/20\n140454/140454 [==============================] - 25s 178us/step - loss: 0.2979 - acc: 0.8635 - val_loss: 0.3040 - val_acc: 0.8595\n\nEpoch 00004: val_loss improved from 0.30757 to 0.30396, saving model to best_model.hdf5\nEpoch 5/20\n140454/140454 [==============================] - 25s 177us/step - loss: 0.2900 - acc: 0.8674 - val_loss: 0.3049 - val_acc: 0.8587\n\nEpoch 00005: val_loss did not improve from 0.30396\nEpoch 6/20\n140454/140454 [==============================] - 25s 179us/step - loss: 0.2826 - acc: 0.8704 - val_loss: 0.3058 - val_acc: 0.8608\n\nEpoch 00006: val_loss did not improve from 0.30396\nEpoch 7/20\n140454/140454 [==============================] - 25s 176us/step - loss: 0.2786 - acc: 0.8729 - val_loss: 0.3027 - val_acc: 0.8612\n\nEpoch 00007: val_loss improved from 0.30396 to 0.30268, saving model to best_model.hdf5\nEpoch 8/20\n140454/140454 [==============================] - 25s 181us/step - loss: 0.2734 - acc: 0.8753 - val_loss: 0.3028 - val_acc: 0.8608\n\nEpoch 00008: val_loss did not improve from 0.30268\nEpoch 9/20\n140454/140454 [==============================] - 25s 179us/step - loss: 0.2692 - acc: 0.8774 - val_loss: 0.3084 - val_acc: 0.8616\n\nEpoch 00009: val_loss did not improve from 0.30268\nEpoch 10/20\n140454/140454 [==============================] - 25s 177us/step - loss: 0.2666 - acc: 0.8793 - val_loss: 0.3053 - val_acc: 0.8613\n\nEpoch 00010: val_loss did not improve from 0.30268\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred1 = model1.predict(X_test, batch_size = 1024, verbose = 1)\npred = pred1\npred2 = model2.predict(X_test, batch_size = 1024, verbose = 1)\npred += pred2\npred3 = model3.predict(X_test, batch_size = 1024, verbose = 1)\npred += pred3\npred4 = model4.predict(X_test, batch_size = 1024, verbose = 1)\npred += pred4\npred5 = model5.predict(X_test, batch_size = 1024, verbose = 1)\npred += pred5\n","execution_count":36,"outputs":[{"output_type":"stream","text":"66292/66292 [==============================] - 4s 57us/step\n66292/66292 [==============================] - 4s 64us/step\n66292/66292 [==============================] - 4s 58us/step\n66292/66292 [==============================] - 4s 56us/step\n66292/66292 [==============================] - 4s 58us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n#sub['Sentiment'] = predictions\n#sub.to_csv(\"blend.csv\", index=False)","execution_count":37,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Attention + LSTM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention(Layer):\n    \"\"\"\n    Keras Layer that implements an Attention mechanism for temporal data.\n    Supports Masking.\n    Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n    # Input shape\n        3D tensor with shape: `(samples, steps, features)`.\n    # Output shape\n        2D tensor with shape: `(samples, features)`.\n    :param kwargs:\n    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n    The dimensions are inferred based on the output shape of the RNN.\n    Example:\n        model.add(LSTM(64, return_sequences=True))\n        model.add(Attention())\n    \"\"\"\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n        if self.bias:\n            eij += self.b\n        eij = K.tanh(eij)\n        a = K.exp(eij)\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(maxlen, max_features, embed_size, embedding_matrix):\n    input_words = Input((max_len, ))\n    x_words = Embedding(19479,\n                        embed_size,\n                        weights=[embedding_matrix],\n                        mask_zero=True,\n                        trainable=False)(input_words)\n    x_words = SpatialDropout1D(0.2)(x_words)\n    x_words = Bidirectional(LSTM(128, return_sequences=True))(x_words)\n    x_words = Bidirectional(LSTM(128, return_sequences=True))(x_words)\n    \n    x = Attention(maxlen)(x_words)\n    #x = GlobalMaxPooling1D()(x)\n    #x = GlobalAveragePooling1D()(x)\n    x = Dropout(0.2)(x)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    pred = Dense(5, activation='softmax')(x)\n\n    model = Model(inputs=input_words, outputs=pred)\n    return model\n\nmodel = build_model(max_len, max_features, embed_size, embedding_matrix)\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","execution_count":39,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_6 (InputLayer)         (None, 50)                0         \n_________________________________________________________________\nembedding_6 (Embedding)      (None, 50, 300)           5843700   \n_________________________________________________________________\nspatial_dropout1d_6 (Spatial (None, 50, 300)           0         \n_________________________________________________________________\nbidirectional_11 (Bidirectio (None, 50, 256)           439296    \n_________________________________________________________________\nbidirectional_12 (Bidirectio (None, 50, 256)           394240    \n_________________________________________________________________\nattention_1 (Attention)      (None, 256)               306       \n_________________________________________________________________\ndropout_11 (Dropout)         (None, 256)               0         \n_________________________________________________________________\ndense_16 (Dense)             (None, 64)                16448     \n_________________________________________________________________\ndropout_12 (Dropout)         (None, 64)                0         \n_________________________________________________________________\ndense_17 (Dense)             (None, 5)                 325       \n=================================================================\nTotal params: 6,694,315\nTrainable params: 850,615\nNon-trainable params: 5,843,700\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"save_file = 'model_by_tyk.h5'\n#history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.2, \n                        #verbose = 1, callbacks = [check_point, early_stop])\nhistory = model.fit(X_train, y_ohe,\n                    epochs=20, verbose=1,\n                    batch_size=512, shuffle=True)","execution_count":40,"outputs":[{"output_type":"stream","text":"Epoch 1/20\n156060/156060 [==============================] - 114s 732us/step - loss: 0.3315 - acc: 0.8501\nEpoch 2/20\n156060/156060 [==============================] - 105s 671us/step - loss: 0.3023 - acc: 0.8613\nEpoch 3/20\n156060/156060 [==============================] - 103s 663us/step - loss: 0.2926 - acc: 0.8659\nEpoch 4/20\n156060/156060 [==============================] - 104s 670us/step - loss: 0.2843 - acc: 0.8698\nEpoch 5/20\n156060/156060 [==============================] - 104s 669us/step - loss: 0.2771 - acc: 0.8734\nEpoch 6/20\n156060/156060 [==============================] - 103s 660us/step - loss: 0.2698 - acc: 0.8775\nEpoch 7/20\n156060/156060 [==============================] - 105s 674us/step - loss: 0.2640 - acc: 0.8801\nEpoch 8/20\n156060/156060 [==============================] - 104s 666us/step - loss: 0.2574 - acc: 0.8838\nEpoch 9/20\n156060/156060 [==============================] - 103s 661us/step - loss: 0.2533 - acc: 0.8863\nEpoch 10/20\n156060/156060 [==============================] - 105s 670us/step - loss: 0.2483 - acc: 0.8883\nEpoch 11/20\n156060/156060 [==============================] - 104s 667us/step - loss: 0.2442 - acc: 0.8907\nEpoch 12/20\n156060/156060 [==============================] - 103s 660us/step - loss: 0.2406 - acc: 0.8925\nEpoch 13/20\n156060/156060 [==============================] - 103s 662us/step - loss: 0.2365 - acc: 0.8948\nEpoch 14/20\n156060/156060 [==============================] - 106s 678us/step - loss: 0.2335 - acc: 0.8964\nEpoch 15/20\n156060/156060 [==============================] - 103s 659us/step - loss: 0.2302 - acc: 0.8983\nEpoch 16/20\n156060/156060 [==============================] - 103s 663us/step - loss: 0.2270 - acc: 0.8992\nEpoch 17/20\n156060/156060 [==============================] - 106s 676us/step - loss: 0.2241 - acc: 0.9010\nEpoch 18/20\n156060/156060 [==============================] - 103s 659us/step - loss: 0.2209 - acc: 0.9025\nEpoch 19/20\n156060/156060 [==============================] - 103s 661us/step - loss: 0.2177 - acc: 0.9043\nEpoch 20/20\n156060/156060 [==============================] - 105s 670us/step - loss: 0.2152 - acc: 0.9054\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(X_test, batch_size = 1024, verbose = 1)","execution_count":41,"outputs":[{"output_type":"stream","text":"66292/66292 [==============================] - 12s 179us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['Sentiment'] = pred\nsub.to_csv(\"blend.csv\", index=False)","execution_count":42,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}