{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n# add glove6b300dtxt dataset \n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv',delimiter='\\t')\ntest =pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv',delimiter='\\t')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The sentiment labels are:\n\n* 0 - negative\n* 1 - somewhat negative\n* 2 - neutral\n* 3 - somewhat positive\n* 4 - positive"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# -1 for len(train)  One shape dimension can be -1. In this case, the value is inferred from the length of the array and remaining dimensions.\ntrain.Sentiment.values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Sentiment.values.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" OneHotEncoder requires that all values are integers\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.iloc[192]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ohe=OneHotEncoder(sparse=False)\n# -1 for len(train)  One shape dimension can be -1. In this case, the value is inferred from the length of the array and remaining dimensions.\n# OneHotEncoder Expected 2D array\nohe=ohe.fit(train.Sentiment.values.reshape(-1,1))\nprint(train.Sentiment.values[192])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('sentimental distribution\\n{}'.format(train.Sentiment.value_counts()/len(train)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"phrase present in test "},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(train['Phrase']).intersection(set(test['Phrase'])))/len(test)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import  CountVectorizer\n\ncv1= CountVectorizer()\ncv2= CountVectorizer()\n\ncv1.fit(train.Phrase)\ncv2.fit(test.Phrase)\nprint('train total vocabulary size = {}'.format(len(cv1.vocabulary_)))\nprint('test total vocabulary size = {}'.format(len(cv2.vocabulary_)))\n\nprint('comman word in both ={}'.format(len(set(cv1.vocabulary_.keys()).intersection(set(cv2.vocabulary_.keys())))))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"groupby=train.groupby(\"SentenceId\")\ngroupby.count()[:3]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tranfer(data):\n    data['Phrase_count']=data.groupby(\"SentenceId\")['Phrase'].transform('count')\n    data['word_count']=data['Phrase'].apply(lambda x :len(x.split(' ')) )\n    data['upper_char']=data['Phrase'].apply(lambda x : x.lower()!=x)\n    data['start_comma']=data['Phrase'].apply(lambda x : x.startswith(','))\n    data['sentence_end']=data['Phrase'].apply(lambda x :x.endswith('.') )\n    data['sentence_start']=data['Phrase'].apply(lambda x :x[0].upper()==x[0] )\n    data[\"Phrase\"] = data[\"Phrase\"].apply(lambda x: x.lower())\n    return data\ntrain = tranfer(train)\ntest = tranfer(test)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_FOLDS = 5\n\ntrain[\"fold_id\"] = train[\"SentenceId\"].apply(lambda x: x%NUM_FOLDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby(\"Sentiment\")[train.columns[4:]].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_FOLDS = 5\n\ntrain[\"fold_id\"] = train[\"SentenceId\"].apply(lambda x: x%NUM_FOLDS)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tranfer knowlege from pre train databset with 300 dimention , with diff features like gender , royal etc \n# glove Embeddig\nEMBEDDING_FILE =  '../input/glove6b300dtxt/glove.6B.300d.txt'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f=open(EMBEDDING_FILE)\nfor line in f:\n    # split each iteam in first line \n    value=line.split(' ')\n    # first time is wrord \n    word = value[0]\n    print('1st word=',word)\n    print(value[:20])\n    print(line[:20])\n    break;\n    \nEMBEDDING_DIM=300","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f=open(EMBEDDING_FILE)\n# unin test and train unique\nall_word = set(cv1.vocabulary_.keys()).union(set(cv2.vocabulary_.keys()))\n# store \nembedding_index={}\nfor line in f:\n    value=line.split(' ')\n    word = value[0]\n    if word in all_word:\n        coef =value[1:]\n        embedding_index[word]=coef;\n    f.close\nprint('word not in Glove ={}'.format(len(set(all_word)-set(embedding_index))))\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max(max(train.Phrase.apply(lambda x : len(x.split(' ')))),max(test.Phrase.apply(lambda x : len(x.split(' ')))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH=56","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ADD POLARITY AND SUBJECTIVITY\n\n* This tells us that the English phrase “not a very great calculation” has a polarity of about -0.3, meaning it is slightly negative, and a subjectivity of about 0.6, meaning it is fairly subjective."},{"metadata":{},"cell_type":"markdown","source":"# input sequence for LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport textblob\nMAX_SEQUENCE_LENGTH = 60\n\ntokenizer = Tokenizer()\n# numbering each word in vocab\ntokenizer.fit_on_texts(np.append(train[\"Phrase\"].values, test[\"Phrase\"].values))\nword_index = tokenizer.word_index\n\nnb_words = len(word_index) + 1\nembedding_matrix = np.random.rand(nb_words, EMBEDDING_DIM + 2)\n\nfor word, i in word_index.items():\n    embedding_vector = embedding_index.get(word)\n    sent = textblob.TextBlob(word).sentiment\n    if embedding_vector is not None:\n        embedding_matrix[i] = np.append(embedding_vector, [sent.polarity, sent.subjectivity])\n    else:\n        embedding_matrix[i, -2:] = [sent.polarity, sent.subjectivity]\n        \nseq = pad_sequences(tokenizer.texts_to_sequences(train[\"Phrase\"]), maxlen=MAX_SEQUENCE_LENGTH)\ntest_seq = pad_sequences(tokenizer.texts_to_sequences(test[\"Phrase\"]), maxlen=MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Model****"},{"metadata":{"trusted":true},"cell_type":"code","source":"textblob.TextBlob('good').sentiment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import  *\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping\n\n\ndense_features = train.columns[4:10]\n\ndef build_model():\n    embading_layer =Embedding(input_dim=nb_words,output_dim=EMBEDDING_DIM+2,\n                              weights=[embedding_matrix],\n                              input_length=MAX_SEQUENCE_LENGTH,\n                              trainable=True)\n    dropout = Dropout(0.2)\n    mask =Masking()\n    lstm= LSTM(50)\n    \n    seq_input =Input(shape=(MAX_SEQUENCE_LENGTH,),dtype='int32')\n    dense_input =Input(shape=(len(dense_features),))\n    dense_vect =BatchNormalization()(dense_input)\n    phrase_vect=lstm(mask(dropout(embading_layer(seq_input))))\n    \n    f_vec= concatenate([dense_vect,phrase_vect])\n    f_vec= Dense(50,activation='relu')(f_vec)\n    f_vec=Dense(20,activation='relu')(f_vec)\n    output = Dense(5,activation='softmax')(f_vec)\n    \n    model = Model(inputs=[seq_input,dense_input],outputs=[output])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = np.zeros((test.shape[0], 5))\n\nfor i in range(NUM_FOLDS):\n    print(\"FOLD\", i+1)\n    \n    print(\"Splitting the data into train and validation...\")\n    train_seq, val_seq = seq[train[\"fold_id\"] != i], seq[train[\"fold_id\"] == i]\n    train_dense, val_dense = train[train[\"fold_id\"] != i][dense_features], train[train[\"fold_id\"] == i][dense_features]\n    y_train = ohe.transform(train[train[\"fold_id\"] != i][\"Sentiment\"].values.reshape(-1, 1))\n    y_val = ohe.transform(train[train[\"fold_id\"] == i][\"Sentiment\"].values.reshape(-1, 1))\n    \n    print(\"Building the model...\")\n    model = build_model()\n    model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"acc\"])\n    \n    early_stopping = EarlyStopping(monitor=\"val_acc\", patience=2, verbose=1)\n    \n    print(\"Training the model...\")\n    model.fit([train_seq, train_dense], y_train, validation_data=([val_seq, val_dense], y_val),\n              epochs=15, batch_size=1024, shuffle=True, callbacks=[early_stopping], verbose=1)\n    \n    print(\"Predicting...\")\n    test_preds += model.predict([test_seq, test[dense_features]], batch_size=1024, verbose=1)\n    print()\n    \ntest_preds /= NUM_FOLDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dense_features","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}