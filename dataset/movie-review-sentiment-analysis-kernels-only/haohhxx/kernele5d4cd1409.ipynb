{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import pickle\nimport platform\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import (Input, Dense, Embedding, Bidirectional,\n                          Conv1D, Dropout, BatchNormalization, Activation, CuDNNGRU, CuDNNLSTM, Multiply, Layer)\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D, SpatialDropout2D\n\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.optimizers import Adam, SGD, Nadam\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras import initializers\n\n__system__ = platform.system()\n\n# MAX_LEN = 100\nMAX_LEN = 50\nEMBEDDING_DIM = 300\nMAX_FEATURES = 100000\nRANDOM_STATE = 91\n\nif __system__ is 'Windows':\n    GLOVE_DIR = \"D:/data/word2vec/en/crawl-300d-2M.vec/crawl-300d-2M.vec\"\nelse:\n    GLOVE_DIR = \"/mnt/haohhxx/d/data/word2vec/en/crawl-300d-2M.vec/crawl-300d-2M.vec\"\n    \nGLOVE_DIR = r\"../input/fatsttext-common-crawl/crawl-300d-2M/crawl-300d-2M.vec\"\n\n\ndef preprocessing(train, test, max_len=MAX_LEN, max_features=MAX_FEATURES, train_size=0.75):\n    \"\"\"\n        https://www.kaggle.com/antmarakis/cnn-baseline-model\n    \"\"\"\n    X = train['Phrase'].values.tolist()\n    # X = [x.replace(\"n't\", \"not\") for x in X]\n    X_test = test['Phrase'].values.tolist()\n    # X_test = [x.replace(\"n't\", \"not\") for x in X_test]\n\n    X_tok = X + X_test\n    tokenizer = Tokenizer(num_words=max_features, filters='')\n    tokenizer.fit_on_texts(X_tok)\n\n    X = tokenizer.texts_to_sequences(X)\n    X = pad_sequences(X, maxlen=max_len)\n    X_test = tokenizer.texts_to_sequences(X_test)\n    X_test = pad_sequences(X_test, maxlen=max_len)\n\n    word_index = tokenizer.word_index\n\n    y = train['Sentiment'].values\n\n    Y = to_categorical(y)\n    X_train, X_valid, y_train, y_valid = train_test_split(X,\n                                                          Y,\n                                                          train_size=train_size,\n                                                          shuffle=True,\n                                                          random_state=RANDOM_STATE,\n                                                          stratify=y)\n\n    loss_weights = [1 / 5 for _ in range(5)]\n    return X_train, X_valid, y_train, y_valid, X_test, loss_weights, word_index\n\n\ndef get_model(embedding_matrix, max_len=MAX_LEN, units=128, dr=0.5, embed_size=EMBEDDING_DIM):\n    inp = Input(shape=(max_len,))\n    x = Embedding(19479, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x1 = SpatialDropout1D(dr)(x)\n\n    x_gru = Bidirectional(CuDNNGRU(units, return_sequences=True))(x1)\n    x1 = Conv1D(32, kernel_size=3, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n    max_pool1_gru = GlobalMaxPooling1D()(x1)\n\n    x3 = Conv1D(32, kernel_size=2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n    max_pool3_gru = GlobalMaxPooling1D()(x3)\n\n    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences=True))(x1)\n    x1 = Conv1D(32, kernel_size=3, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n\n    x3 = Conv1D(32, kernel_size=2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n\n    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n                     avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(Dense(128, activation='relu')(x))\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(Dense(100, activation='relu')(x))\n    out = Dense(5, activation=\"sigmoid\")(x)\n\n    model = Model(inputs=inp, outputs=out)\n\n    return model\n\n\ndef get_model_t(embedding_matrix, word_index, max_len=MAX_LEN, units=128, dr=0.3, embed_size=300):\n    inp = Input(shape=(max_len,))\n    x = Embedding(len(word_index) + 1, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n    x1 = SpatialDropout1D(dr)(x)\n    # x1 = Dropout(dr)(x)\n    x1 = Bidirectional(CuDNNLSTM(units, return_sequences=True))(x1)\n\n    conv2 = Conv1D(128, kernel_size=2, padding='valid', kernel_initializer='he_uniform', activation='relu')(x1)\n    conv3 = Conv1D(128, kernel_size=3, padding='valid', kernel_initializer='he_uniform', activation='relu')(x1)\n\n    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences=True))(x1)\n    x_lstm_c2 = Bidirectional(CuDNNLSTM(units, return_sequences=True))(conv2)\n    x_lstm_c3 = Bidirectional(CuDNNLSTM(units, return_sequences=True))(conv3)\n    # batch * seqlen * 2hidden\n\n    atl1 = Dense(24, activation='relu', input_shape=(50,))(x_lstm)\n    atl2 = Dense(1, activation='softmax', input_shape=(50,))(atl1)\n    x_lstm = Multiply()([x_lstm, atl2])\n    # atl1 = Dense(24, activation='relu', input_shape=(50,))(x_lstm_c2)\n    # atl2 = Dense(1, activation='softmax', input_shape=(50,))(atl1)\n    # x_lstm_c2_at = Multiply()([x_lstm_c2, atl2])\n    # atl1 = Dense(24, activation='relu', input_shape=(50,))(x_lstm_c3)\n    # atl2 = Dense(1, activation='softmax', input_shape=(50,))(atl1)\n    # x_lstm_c3_at = Multiply()([x_lstm_c3, atl2])\n\n    # avg_pool1_lstm = GlobalAveragePooling1D()(x_lstm)\n    max_pool1_lstm = GlobalMaxPooling1D()(x_lstm)\n    max_pool1_lstm_c2 = GlobalMaxPooling1D()(x_lstm_c2)\n    max_pool1_lstm_c3 = GlobalMaxPooling1D()(x_lstm_c3)\n\n    # max_pool1_conv2 = GlobalMaxPooling1D()(conv2)\n    # max_pool1_conv3 = GlobalMaxPooling1D()(conv3)\n    # max_pool1_conv4 = GlobalMaxPooling1D()(conv4)\n    # max_pool1_conv6 = GlobalMaxPooling1D()(conv6)\n    convs = concatenate([max_pool1_lstm_c2, max_pool1_lstm_c3, max_pool1_lstm])\n\n    x = BatchNormalization()(convs)\n    x = Dropout(0.1)(Dense(256, activation='relu')(x))\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(Dense(100, activation='relu')(x))\n    out = Dense(5, activation=\"softmax\")(x)\n\n    model = Model(inputs=inp, outputs=out)\n\n    return model\n\n#\n# def attention_layer(X, n_h, Ty):\n#     \"\"\"\n#     Creates an attention layer.\n#\n#     Input:\n#     X - Layer input (m, Tx, x_vocab_size)\n#     n_h - Size of LSTM hidden layer\n#     Ty - Timesteps in output sequence\n#\n#     Output:\n#     output - The output of the attention layer (m, Tx, n_h)\n#     \"\"\"\n#     # Define the default state for the LSTM layer\n#     h = Lambda(lambda X: K.zeros(shape=(K.shape(X)[0], n_h)))(X)\n#     c = Lambda(lambda X: K.zeros(shape=(K.shape(X)[0], n_h)))(X)\n#     # Messy, but the alternative is using more Input()\n#\n#     at_LSTM = LSTM(n_h, return_state=True)\n#\n#     output = []\n#\n#     # Run attention step and RNN for each output time step\n#     for _ in range(Ty):\n#         context = one_step_of_attention(h, X)\n#\n#         h, _, c = at_LSTM(context, initial_state=[h, c])\n#\n#         output.append(h)\n#\n#     return output\n#\n#\n# def model_lstm(embedding_matrix, word_index, max_len=MAX_LEN, units=128, dr=0.3, embed_size=300):\n#     inp = Input(shape=(max_len,))\n#     x = Embedding(len(word_index) + 1, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n#     x1 = SpatialDropout1D(dr)(x)\n#     x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences=True))(x1)\n#\n#     atl1 = Dense(24, activation='relu', input_shape=(50,))(x_lstm)\n#     atl2 = Dense(1, activation='softmax', input_shape=(50,))(atl1)\n#     x_lstm = Multiply()([x_lstm, atl2])\n#\n#     max_pool1_lstm = GlobalMaxPooling1D()(x_lstm)\n#\n#     x = BatchNormalization()(max_pool1_lstm)\n#     x = Dropout(0.1)(Dense(128, activation='relu')(x))\n#     x = BatchNormalization()(x)\n#     x = Dropout(0.2)(Dense(100, activation='relu')(x))\n#     out = Dense(5, activation=\"softmax\")(x)\n#\n#     model = Model(inputs=inp, outputs=out)\n#\n#     return model\n#\n\ndef get_glove(word_index, path=GLOVE_DIR):\n    \"\"\"\n        https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n    \"\"\"\n    embeddings_index = {}\n    with open(path, 'r', encoding='utf-8') as f:\n        f.readline()\n        for line in f:\n            values = line.strip().split(\" \")\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n    print('Found %s word vectors.' % len(embeddings_index))\n    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix\n\n\ndef prepare(train_size):\n    train = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv', sep=\"\\t\")\n    test = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv', sep=\"\\t\")\n\n    X_train, X_valid, y_train, y_valid, X_test, loss_weights, word_index = preprocessing(train, test,\n                                                                                         train_size=train_size)\n    data = {\n        \"X_train\": X_train,\n        \"X_valid\": X_valid,\n        \"y_train\": y_train,\n        \"y_valid\": y_valid,\n        \"X_test\": X_test,\n        \"loss_weights\": loss_weights,\n        \"word_index\": word_index,\n    }\n    pickle.dump(data, open(\"./keras.data\", 'wb'))\n\n\ndef main():\n    train_size = 0.9\n    prepare(train_size)\n\n    data = pickle.load(open(\"./keras.data\", 'rb'))\n    X_train = data[\"X_train\"]\n    X_valid = data[\"X_valid\"]\n    y_train = data[\"y_train\"]\n    y_valid = data[\"y_valid\"]\n    X_test = data[\"X_test\"]\n    loss_weights = data[\"loss_weights\"]\n    word_index = data[\"word_index\"]\n    glove_emb = get_glove(word_index)\n    # model = get_model(glove_emb)\n    model = get_model_t(glove_emb, word_index)\n    model.summary()\n\n    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=1e-4, decay=0), metrics=[\"accuracy\"])\n    # opt = Nadam(lr=1e-4, schedule_decay=0.04, decay=0)\n    # model.compile(loss=weighted_categorical_crossentropy(loss_weights),\n    #               optimizer=opt,\n    #               metrics=['accuracy'])\n\n    check_point = ModelCheckpoint('best_weights.h5',\n                                  monitor='val_acc',\n                                  verbose=0,\n                                  save_best_only=True,\n                                  save_weights_only=False,\n                                  mode='max',\n                                  period=1)\n\n    early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n\n    # model.fit(X_train, y_ohe, batch_size=128, epochs=15, validation_split=0.1, verbose=1,\n    #           callbacks=[check_point, early_stop])\n    model.fit(X_train,\n              y_train,\n              batch_size=128,\n              epochs=20,\n              verbose=1,\n              validation_data=[X_valid, y_valid],\n              callbacks=[check_point, early_stop])\n    model.load_weights('best_weights.h5')\n    sub = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv')\n\n    sub['Sentiment'] = np.argmax(model.predict(X_test, verbose=1), axis=-1)\n    sub.to_csv('sub.csv', index=False)\n\n\nif __name__ == '__main__':\n    main()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}