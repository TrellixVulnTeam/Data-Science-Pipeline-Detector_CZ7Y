{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LAB 7: Ben and Saatvik ","metadata":{}},{"cell_type":"markdown","source":"## Setup and Installation","metadata":{}},{"cell_type":"markdown","source":"### In the following notebook we explore three possible solutions for this competition. These solutions included using the `AWD_LSTM`, a bag of words approach, and a custum embeddings approach.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nfrom fastai.text.all import * \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom torch import optim\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Here we unzip the datasets for the competition.","metadata":{}},{"cell_type":"code","source":"!unzip '/kaggle/input/movie-review-sentiment-analysis-kernels-only/test.tsv.zip'\n!unzip '/kaggle/input/movie-review-sentiment-analysis-kernels-only/train.tsv.zip'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('train.tsv', sep=\"\\t\")\ndf_test = pd.read_csv('test.tsv', sep=\"\\t\")\ndf_test = df_test.rename({'Phrase': 'text'}, axis='columns')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[:10000]\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create a dataloader for the training dataset.","metadata":{}},{"cell_type":"code","source":"# create train dataloader\ndls = TextDataLoaders.from_df(df, text_col='Phrase', label_col='Sentiment')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine Tune `AWD_LSTM` Model","metadata":{}},{"cell_type":"code","source":"awd_learner = text_classifier_learner(dls, AWD_LSTM, metrics=accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"awd_learner.fine_tune(100, cbs=[SaveModelCallback, EarlyStoppingCallback(patience=10, min_delta=0.01)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model for future use\nawd_learner.export('export_lab7.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bag Of Words Approach","metadata":{}},{"cell_type":"markdown","source":"\nHere we create a vector that represents the set of words that are present in the review and use it to predict the sentiment.\nOur approahc uses a standard linear neural network with 2 hidden layers. The layer at the beginning of our network converts rom the format the dataloader provides, a tensor of word numbers, to a tensor of length equal to the number of words in the dictionary, with a 1 at each location that a word is present.","metadata":{}},{"cell_type":"code","source":"vocab_size = len(dls.train.vocab[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_multi_hot(arr):\n  mh = [0]*vocab_size\n  for i in range(len(arr)):\n    mh[arr[i]] = 1\n  return mh\n\ndef batch_mh(big_arr):\n  big_arr = big_arr.tolist()\n  result=[]\n  for x in big_arr:\n    result += [to_multi_hot(x)]\n  result = torch.Tensor(result)\n  result = to_device(result)\n  return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = nn.Sequential(\n    Lambda(batch_mh),\n    nn.Linear(vocab_size, 30),\n    nn.ReLU(),\n    nn.Linear(30,5)  # note that output doesn't have a softmax layer.\n                      # That gets handled in CrossEntropyLoss function. \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bow_learner = Learner(dls=dls, model=model, \n                opt_func=SGD, \n                loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy)\nbow_learner.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bow_learner.fit(100, cbs=[SaveModelCallback(), EarlyStoppingCallback, ReduceLROnPlateau])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bow_learner.export('bag_of_words.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embeddings ","metadata":{}},{"cell_type":"markdown","source":"Here we create an embedding matrix that is applied to each word. Reviews are limited to a size of 100 since we cannot have a variable length of input.","metadata":{}},{"cell_type":"code","source":"# Defining some constants\nreview_size = 100\nembedding_size = 10\nhidden_layer_size = 20","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\nclass FirstHundred(Module):\n    # This function returns the indices of the first hundred words in the review.\n    def forward(self, tns):\n        padded_tns = F.pad(tns, pad=(0, review_size - tns.shape[1], 0, 0), value=1)\n        padded_tns = padded_tns[:, :review_size]\n        padded_tns = to_device(padded_tns)\n        return padded_tns\n\nclass PrintShape(Module):\n    # This function prints the size of the current input.\n    def forward(self, arr):\n        print(arr.size())\n        return arr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = nn.Sequential(\n    FirstHundred(),\n    nn.Embedding(vocab_size, embedding_size),\n    nn.Flatten(),\n    nn.Linear(embedding_size * review_size, hidden_layer_size),\n    nn.ReLU(),\n    nn.Linear(hidden_layer_size,5) \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emb_learner = Learner(dls=dls, model=model,\n                loss_func=CrossEntropyLossFlat(), \n                      opt_func=SGD,\n                metrics=accuracy)\nemb_learner.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emb_learner.fit(100, cbs=[SaveModelCallback(), EarlyStoppingCallback(patience=20)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emb_learner.export('embeddings.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make predictions","metadata":{}},{"cell_type":"code","source":"# load the saved models\n# awd_learner = load_learner('../input/export/export_lab7.pkl')\n# emb_learner = load_learner('../input/embeddings/embeddings.pkl')\nbow_learner = load_learner('../input/bagofwords/bag_of_words.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a test dataloader\ntest_dl = bow_learner.dls.test_dl(df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make preds with the given dataloader\n# awd_preds,awd_probs = awd_learner.get_preds(dl=test_dl)\nbow_preds,bow_probs = bow_learner.get_preds(dl=test_dl)\n# emb_preds,emb_probs = emb_learner.get_preds(dl=test_dl)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# awd_list_preds = []\n# for row in preds:\n#     awd_list_preds.append(torch.argmax(row).item())\n    \nbow_list_preds = []\nfor row in bow_preds:\n    bow_list_preds.append(torch.argmax(row).item())\n    \n# emb_list_preds = []\n# for row in preds:\n#     emb_list_preds.append(torch.argmax(row).item())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Submission File","metadata":{}},{"cell_type":"code","source":"# clean up test file for submission\ndel df_test['SentenceId']\ndel df_test['text']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test.insert(1, \"Sentiment\", awd_list_preds, True)\n# df_test.head()\n# df_test.to_csv('awd_submission.csv', index=False)\n\n# del df_test['Sentiment']\n\n# df_test.insert(1, \"Sentiment\", bow_list_preds, True)\n# df_test.head()\ndf_test.to_csv('submission.csv', index=False)\n\n# del df_test['Sentiment']\n\n# df_test.insert(1, \"Sentiment\", emb_list_preds, True)\n# df_test.head()\n# df_test.to_csv('emb_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}