{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import style\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### About Dataset\nThis dataset consists of Phrases that are obtained from the Rotten Tomatoes dataset. Each sentence has a phrase.Sentences are shuffled but having a unique Sentence Id. Each Phrase has its own Phrase Id also.Phrases that are repeated (such as short/common words) are only included once in the data.\n\n     There are two separate Train/Test csv files.\n     1.) train.tsv contains the Sentence Id,phrases Id and their associated sentiment labels. \n     2.) test.tsv contains just phrases. The purpose is to assign a sentiment label to each phrase.\n     The sentiment labels are:\n     0 - negative\n     1 - somewhat negative\n     2 - neutral\n     3 - somewhat positive\n     4 - positive"},{"metadata":{},"cell_type":"markdown","source":"Let us read dataset first and check its various columns for NaN Values and Datatypes of each column."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading data files first\ntrain=pd.read_csv('../input/movie-reviews-data/train.csv')\n#train\ntest=pd.read_csv('../input/movie-reviews-data/test.csv')\n#test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking datatypes,no. of elements and empty data columns\ntrain.dtypes\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info() #No null entry in training dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info() #No null entry in test dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Analysis and Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a new column having sentiment names against their Sentiment Label\ntrain['Sentiment_Name']= train['Sentiment'].replace({0:\"Negative\",1:\"Somewhat Negative\",2:\"Neutral\",3:'Somewhat Positive',4:'Positive'})\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### First of all let us analyse various Sentiment labels. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are five preset seaborn themes: darkgrid, whitegrid, dark, white, and ticks\n#sns.axes_style()\n#The four preset contexts, in order of relative size, are paper, notebook, talk, and poster.By default noetbook is used.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"ticks\")\nsns.set_context(\"talk\")\n#style.use('seaborn-bright')\nplt.figure(figsize=(10,7))\nsns.countplot(\"Sentiment_Name\",data= train)\nplt.title('Seniment Types')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the records belong to Neutral sentiments(Label-2). "},{"metadata":{},"cell_type":"markdown","source":"### >> Now we check the relationship between Sentiment labels w.r.t the Phrases  and Sentences used."},{"metadata":{},"cell_type":"markdown","source":"####   Phrases Used in movies vs Sentiments"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sentiments vs Pharses Count\nsenti_phrase=train.groupby(['Sentiment'])['PhraseId'].count().sort_values(ascending=False)\nprint(\"Phrases count wr.t various Sentiments:\\n \",senti_phrase)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting Sentiment labels vs  Pharses Count\nsns.set_style(\"ticks\")\nsns.set_context(\"talk\")\nx=senti_phrase.index\ny=senti_phrase.values\nplt.figure(figsize=(10,7))\nsns.barplot(x,y,color='r')\nplt.xlabel('Sentiment Labels')\nplt.ylabel('Phrases Count')\nplt.title('Sentiments vs Phrases Used')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This means  the movies with Neutral Sentiment types(i.e Sentiment label-2) had highest number pf phrases used  and Sentiment label-0 i.e Negative sentiments had minimum number of phrases used."},{"metadata":{},"cell_type":"markdown","source":"### Various Sentiment Labels vs Sentences used in movies"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sentiment label-vs-Sentences\nsenti_sentence=train.groupby(['Sentiment'])['SentenceId'].nunique().sort_values(ascending=False)\nsenti_sentence","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sentiment label-2(Neutral) are having higher number of phrases sentences."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting Sentiment-wise Sentences Count\nsns.set_style(\"ticks\")\nsns.set_context(\"talk\")\nx1=senti_sentence.index\ny1=senti_sentence.values\nplt.figure(figsize=(10,7))\nsns.barplot(x1,y1,color='c')\nplt.xlabel('Sentiment Labels')\nplt.ylabel('Sentences Count')\nplt.title('Sentiments vs Sentences Count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This means  the movies with Neutral Sentiment types(i.e Sentiment label-2) had highest number pf phrase sentences  and Sentiment label-0 i.e Negative sentiments had minimum number of sentences used."},{"metadata":{},"cell_type":"markdown","source":"### Plotting relationship between Sentiment labels vs Sentences and Pharses used"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting relationship between Sentiment labels vs Sentences and Pharses used\nsns.set_style(\"darkgrid\")\nsns.set_context(\"talk\")\nplt.figure(figsize=(15,10))\n\nx=senti_phrase.index\ny=senti_phrase.values\n\nplt.subplot(2,1,1)\nplt.bar(x,y,color='c')\nplt.title('Sentiment vs Phrases Used')\n\nx1=senti_sentence.index\ny1=senti_sentence.values\n\nplt.subplot(2,1,2)\nplt.bar(x1,y1,color='y')\nplt.title('Sentiment vs Sentences')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This can be seen clearly that the highest sentences and phrases used in movie having Sentiment label-2(i.e. Neutral)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sentences vs Phrases Count\nsentence_phrase=train.groupby(['SentenceId'])['Phrase'].count().sort_values(ascending=False).head(20)\nprint(\"No. of phrases used per sentence:\\n \",sentence_phrase)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The sentence with Id-1  and 5555 have highest number of phrases i.e 63. The top highest sentences have 50-60 phrases in a sentnence."},{"metadata":{},"cell_type":"markdown","source":"### >>Further we check the most frequent words used in Phrases and find out if there is any relation to their Sentiments."},{"metadata":{},"cell_type":"markdown","source":"### Top words used in whole phrases records"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most frequently used Words in Phrases","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = ' '.join(train['Phrase'])\ncorpus = corpus.replace('.', '. ')\nwordcloud= WordCloud(stopwords=STOPWORDS,background_color='white', width=2400,height=2000,).generate(corpus)\nplt.figure(figsize=(15,20))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The words with larger sizes are used most freuquently in phrases such as *MOVIE*,*FILM*. After that words **ONE**, *CAN'T* are used and so on."},{"metadata":{"trusted":true},"cell_type":"code","source":"# To create a image of WORDCLOUD\nwordcloud.to_file(\"Frequent Phrases Words.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let us now check the top phrases words used in each sentiment labels. "},{"metadata":{},"cell_type":"markdown","source":"#### Negative Sentiments(Sentiment label-0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most frequently used Words in Phrases with Negative Sentiment\nneg_sent_record= train[train['Sentiment']==0]\nneg_sent_record.head() # Record of negative sentiment phrases\n\n#Plotting Wordcloud for Negative Sentiment\ncorpus = ' '.join(neg_sent_record['Phrase'])\ncorpus = corpus.replace('.', '. ')\nwordcloud= WordCloud(stopwords=STOPWORDS,background_color='white', width=2400,height=2000,).generate(corpus)\nplt.figure(figsize=(15,20))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Somewhat Negative Sentiments(Sentiment label-1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Top phrases words used having Somewhat Negative Sentiments(Sentiment label-1)\nsmwtneg_sent_record= train[train['Sentiment']==1]\nsmwtneg_sent_record.head() # Record of negative sentiment phrases\n\n#Plotting Wordcloud for Somewhat positive Sentiment\ncorpus = ' '.join(smwtneg_sent_record['Phrase'])\ncorpus = corpus.replace('.', '. ')\nwordcloud= WordCloud(stopwords=STOPWORDS,background_color='white', width=2400,height=2000,).generate(corpus)\nplt.figure(figsize=(15,20))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Neutral Sentiments(Sentiment label-2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Top phrases words used having Neutral Sentiments(Sentiment label-2)\nneutral_sent_record= train[train['Sentiment']==2]\nneutral_sent_record.head() # Record of neutral sentiment phrases\n\n#Plotting Wordcloud for Somewhat positive Sentiment\ncorpus = ' '.join(neutral_sent_record['Phrase'])\ncorpus = corpus.replace('.', '. ')\nwordcloud= WordCloud(stopwords=STOPWORDS,background_color='white', width=2400,height=2000,).generate(corpus)\nplt.figure(figsize=(15,20))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Somewhat positive Sentiments(Sentiment label-3)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Top phrases words used having Somewhat positive Sentiments(Sentiment label-3)\nsmwtpos_sent_record= train[train['Sentiment']==3]\nsmwtpos_sent_record.head()\n\n#Plotting Wordcloud for Somewhat positive Sentiment\ncorpus = ' '.join(smwtpos_sent_record['Phrase'])\ncorpus = corpus.replace('.', '. ')\nwordcloud= WordCloud(stopwords=STOPWORDS,background_color='white', width=2400,height=2000,).generate(corpus)\nplt.figure(figsize=(15,20))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Positive Sentiments(Sentiment label-4)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Top phrases words used having Positive Sentiments(Sentiment label-4)\npos_sent_record= train[train['Sentiment']==4]\npos_sent_record.head()\n\n#Plotting Wordcloud for Somewhat positive Sentiment\ncorpus = ' '.join(pos_sent_record['Phrase'])\ncorpus = corpus.replace('.', '. ')\nwordcloud= WordCloud(stopwords=STOPWORDS,background_color='black', width=2400,height=2000,).generate(corpus)\nplt.figure(figsize=(15,20))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly the frequent used words in various phrases doesnot have the direct impact  with each Sentiment types."},{"metadata":{},"cell_type":"markdown","source":"## Applying Algorithms\nLet us now apply Naive Bayes': Multinomial and Bernaulli's NB algorithms  to predict the Sentiment labels of 'Test' Data sheet based on the train datase available.\n* We will train our system based on training dataset from 'Train' sheet .At first we take 'Phrase' Column as independent variable and 'Sentiment' as dependent variable.\n* Output of test data is predicted based on training data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Input dataset\n#for Training\nx_trn=train.loc[:,'Phrase'] # Independent Variable\n#Output Data\ny_trn= train.loc[:,'Sentiment'] # Dependent Variable\n\n#for Test\nx_tst=test.loc[:,'Phrase']# Independent Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing Punctuations and Stop words from input data\nimport nltk\nfrom nltk import word_tokenize  #NLTK is a leading platform for building Python programs to work with human language data.\nfrom nltk.corpus import stopwords\nstopwrds=stopwords.words('english')\n#print(\"Stopwords:\\n \",stopwrds)\nimport string\n\n#first we define a function to remove punctuation\ndef text_cleaning(x_trn):\n    textseparate = [char for char in trn if char not in string.punctuation]\n    print(textseparate)\n    textseparate=''.join(textseparate)\n    print(textseparate)\n    print(textseparate.split())\n  # now we need to remove stopwords from our column  \n    return [word for word in textseparate.split() if word.lower() not in stopwords.words('english')]\n # now we have cleaned title - no punctuation and stopword in it now\n    \n# Cleaning the test data in the same way\n#Removing punctuation\ndef text_cleaning(x_tst):\n    textseparate = [char for char in trn if char not in string.punctuation]\n    print(textseparate)\n    textseparate=''.join(textseparate)\n    textseparate\n    textseparate.split()\n  # now we need to remove stopwords from our column  \n    return [word for word in textseparate.split() if word.lower() not in stopwords.words('english')]\n # now we have cleaned title - no punctuation and stopword in it now","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying Algorithms using Tfidf"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using TfIfd to give weightage to each word in input dataset\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf= TfidfVectorizer()\ntrn_data1 = tfidf.fit_transform(x_trn)\n\n#tranforming x_test\ntst_data1=tfidf.transform(x_tst)\ntst_data1\n\n# Converting trn_data and tst_data to array\nnp.array(trn_data1)\nprint(trn_data1) #Final input training data\n\nnp.array(tst_data1)#Final input training data\nprint(tst_data1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Applying Multinomial Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nmultiNB= MultinomialNB()\nlearning= multiNB.fit(trn_data1,y_trn)\n\n# Predicting Output\ny_pred_mult=learning.predict(tst_data1)\nprint (\"Prediction of Sentiment Labels through Multinomial Algorithm:\",  y_pred_mult)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Applying Bernoulli NB Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\nbern= BernoulliNB()\n\nlearning=bern.fit(trn_data1,y_trn)\n\ny_pred_bern = bern.predict(tst_data1)\nprint (\"Prediction of Sentiment Labels through Bernoullis Algorithm:\",  y_pred_bern)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using CountVectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vector=CountVectorizer()\ntrn_data2 = count_vector.fit_transform(x_trn)\n\n#tranforming x_test\ntst_data2=count_vector.transform(x_tst)\ntst_data2\n\n# Converting trn_data and tst_data to array\nnp.array(trn_data2)\n#print(trn_data2) #Final input training data\n\nnp.array(tst_data2)#Final input training data\n#print(tst_data2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying Algorithms Using CountVectorizer"},{"metadata":{},"cell_type":"markdown","source":"#### Multinomial Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nmultiNB1= MultinomialNB()\nlearning_cv_m= multiNB1.fit(trn_data2,y_trn)\n\n# Predicting Output\ny_pred_mult2=learning_cv_m.predict(tst_data2)\nprint (\"Prediction of Sentiment Labels through Multinomial Algorithm :\",  y_pred_mult2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bernoullis Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\nbern1= BernoulliNB()\n\nlearning_cv_b=bern1.fit(trn_data2,y_trn)\n\ny_pred_bern2 = learning_cv_b.predict(tst_data2)\nprint (\"Prediction of Sentiment Labels through Bernoullis Algorithm:\",  y_pred_bern2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now let us extract our various algorithm results in  a csv file. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading Sample Submission File\noutput_file=pd.read_csv('../input/movie-reviews-data/SampleSubmission.csv')\noutput_file.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tfidf Algorithm Outputs\n#Adding our Algorithm outputs using Output File in new column\noutput_file['Sentiment']= y_pred_mult # Extracting output from Mutinomial algorithm\noutput_file\n\n# Converting this output sheet in a csv file\noutput_file.to_csv('Submission_Mutlinomial through_TfIdf.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_file['Sentiment']= y_pred_bern # Extracting output from Bernoullis algorithm\n\n# Converting this output sheet in a csv file\noutput_file.to_csv('Submission_Bernoulli through_TfIdf.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count Vectorizer Algorithm Outputs\n#Adding our Algorithm outputs using Output File in new column\noutput_file['Sentiment']= y_pred_mult2 # Extracting output from Mutinomial algorithm\noutput_file\n# Converting this output sheet in a csv file\noutput_file.to_csv('Submission_Mutlinomial through_CountVectorizer.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_file['Sentiment']= y_pred_bern2 # Extracting output from Bernaullis algorithm\noutput_file.head()\n\n# Converting this output sheet in a csv file\noutput_file.to_csv('Submission_Bernoulli through_CountVectorizer.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}