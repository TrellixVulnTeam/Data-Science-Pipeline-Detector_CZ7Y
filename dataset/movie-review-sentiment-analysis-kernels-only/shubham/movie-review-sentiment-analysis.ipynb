{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir('../input/movie-review-sentiment-analysis-kernels-only/'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_dir = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv.zip',sep='\\t')\ntest_dir = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv.zip',sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir['Sentiment'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn')\nplt.figure(figsize=(7,5))\nsns.countplot(data=train_dir,x='Sentiment')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_dir))\nprint(len(test_dir))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dir.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_dir.drop('Sentiment',axis=1)\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_dir['Sentiment']\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport re\nfrom nltk.stem import WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncorpus = []\n\nfor i in range(len(X)):\n    text = re.sub('[^a-zA-Z]',' ',X['Phrase'][i])\n    text = text.lower()\n    text = text.split()\n    \n    text = [lemmatizer.lemmatize(word) for word in text if not word in nltk.corpus.stopwords.words('english')]\n    text = ' '.join(text)\n    corpus.append(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras \nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = to_categorical(y)\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ttest_dir = test_dir.drop('PhraseId',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dir","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_corpus = []\n\nfor i in range(len(test_dir)):\n    text = re.sub('[^a-zA-Z]',' ',test_dir['Phrase'][i])\n    text = text.lower()\n    text = text.split()\n    \n    text = [lemmatizer.lemmatize(word) for word in text if not word in nltk.corpus.stopwords.words('english')]\n    text = ' '.join(text)\n    test_corpus.append(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2count = {}\n\nfor sentence in corpus:\n    words = nltk.word_tokenize(sentence)\n    \n    for word in words:\n        if word not in word2count.keys():\n            word2count[word] = 1\n        else:\n            word2count[word] += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(word2count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import heapq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_freq = heapq.nlargest(5000,word2count,key=word2count.get)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,Dense,LSTM,Dropout\nfrom keras.preprocessing.text import one_hot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(word_freq)\none_hot_train = []\nfor sentences in corpus:\n    Z = one_hot(sentences,vocab_size)\n    one_hot_train.append(Z)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_train[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_test = []\nfor sentences in test_corpus:\n    Z = one_hot(sentences,vocab_size)\n    one_hot_test.append(Z)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_test[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"length = 20\ntrain_embedded_sents = pad_sequences(one_hot_train,padding='pre',maxlen=length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_embedded_sents = pad_sequences(one_hot_test,padding='pre',maxlen=length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_embedded_sents[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_embedded_sents[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_feature_vectors = 40\nmodel = Sequential()\nmodel.add(Embedding(vocab_size,embedding_feature_vectors,input_length=length))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(5,activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_final = np.asarray(train_embedded_sents)\ny_final = np.asarray(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_valid,y_train,y_valid = train_test_split(X_final,y_final,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train,y_train,validation_data=(X_valid,y_valid),epochs=20,batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('dark_background')\nfig, ax = plt.subplots(2, 2, figsize=(10, 10))\nsns.lineplot(x=np.arange(1, 21), y=history.history.get('loss'), ax=ax[0, 0])\nsns.lineplot(x=np.arange(1, 21), y=history.history.get('accuracy'), ax=ax[0, 1])\nsns.lineplot(x=np.arange(1, 21), y=history.history.get('val_loss'), ax=ax[1, 0])\nsns.lineplot(x=np.arange(1, 21), y=history.history.get('val_accuracy'), ax=ax[1, 1])\nax[0, 0].set_title('Training Loss vs Epochs')\nax[0, 1].set_title('Training Accuracy vs Epochs')\nax[1, 0].set_title('Validation Loss vs Epochs')\nax[1, 1].set_title('Validation Accuracy vs Epochs')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = np.asarray(test_embedded_sents)\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['labels'] = model.predict_classes(test,batch_size=128)\nsub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score,confusion_matrix\nscore = accuracy_score(sub['Sentiment'],sub['labels'])\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}