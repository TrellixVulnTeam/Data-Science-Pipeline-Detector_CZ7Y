{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Movie Reviews Predeiction Sentiment Analysis**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**About Dataset**\n\n\nThe dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset. The train/test split has been preserved for the purposes of benchmarking, but the sentences have been shuffled from their original order. Each Sentence has been parsed into many phrases by the Stanford parser. Each phrase has a PhraseId. Each sentence has a SentenceId. Phrases that are repeated (such as short/common words) are only included once in the data.\n\ntrain.tsv contains the phrases and their associated sentiment labels. We have additionally provided a SentenceId so that you can track which phrases belong to a single sentence.\n\ntest.tsv contains just phrases. You must assign a sentiment label to each phrase.\n\nThe sentiment labels are:\n\n0 - negative\n\n1 - somewhat negative\n\n2 - neutral\n\n3 - somewhat positive\n\n4 - positive","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nimport sys\nimport os\nimport warnings\nwarnings.simplefilter(action = 'ignore' ,category = FutureWarning)\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom joblib import Parallel, delayed\nimport string \nimport time \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_data = pd.read_csv(r\"../input/movie-review-sentiment-analysis-kernels-only/train.tsv.zip\",header = 0, delimiter = '\\t',encoding='utf-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_test = pd.read_csv(r\"../input/movie-review-sentiment-analysis-kernels-only/test.tsv.zip\",header = 0, delimiter = '\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(movies_data['Phrase'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_data['Sentiment'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt1 = movies_data.groupby('Sentiment')['Phrase'].count()\nplt1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data = movies_data , x = 'Sentiment')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing and Cleaning the Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### lemmatize and removing stopwords","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlemma = WordNetLemmatizer() \nstopwords = stopwords.words('english')\nstopwords.extend(['cinema', 'film', 'series', 'movie', 'one', 'like', 'story', 'plot'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_review(review):\n    tokens = review.lower().split()\n    filtered_tokens = [lemma.lemmatize(w) for w in tokens if w not in stopwords]\n    return \" \".join(filtered_tokens)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\nclean_train_data = movies_data.copy()\nclean_train_data['Phrase'] = Parallel(n_jobs=4)(delayed(clean_review)(review) for review in movies_data['Phrase'])\nprint(clean_train_data['Phrase'] )\nend_time = time.time()\nprint(\"Cleaning Training Data Time - Processing Time = \", end_time - start_time)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove missing values\nprint(\"Cleaned entries: \", clean_train_data.shape[0], \" out of \", movies_data.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Forming Training and Cross-Validation Set**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntarget = clean_train_data.Sentiment\ntrain_X_, validation_X_, train_y, validation_y = train_test_split(clean_train_data['Phrase'], target, test_size=0.2, random_state=22)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hence, we succesfully split our data into training and validation sets. Now, we convert the data into integers using TFIDF Vectorizer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer as tfidf\n\ntfidf_vec = tfidf(min_df=3,  max_features=None, ngram_range=(1, 2), use_idf=1)\ntrain_X = tfidf_vec.fit_transform(train_X_)\n\nprint(\"Succesfully vectorized the data.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n\nfrom sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vectorizer = CountVectorizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_trans = TfidfTransformer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(sublinear_tf = True ,min_df = 5 ,norm = 'max',ngram_range = (1,2),stop_words = 'english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mx_movies = count_vectorizer.fit_transform(train_X_)\nx_train_tf = tfidf_trans.fit_transform(mx_movies)\nprint(x_train_tf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### model building Logistic ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log.fit(x_train_tf,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_tf =  count_vectorizer.transform(validation_X_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred1 = log.predict(x_test_tf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(validation_y,y_pred1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscore = cross_val_score(log , x_test_tf ,y_pred1 , scoring = 'accuracy' , cv =7)\nprint(\" mean accuracy of the model is \" , np.mean(score)*100 , np.std(score)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM linearSVC.....","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = LinearSVC()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm.fit(x_train_tf,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred2 = svm.predict(x_test_tf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(validation_y,y_pred2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscore = cross_val_score(svm , x_test_tf ,y_pred2 , scoring = 'accuracy' , cv =7)\nprint(\" mean accuracy of the model is \" , np.mean(score)*100 , np.std(score)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest classifier ...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RF = RandomForestClassifier(max_depth = 150 , random_state = 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RF.fit(x_train_tf,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred3 = RF.predict(x_test_tf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(validation_y,y_pred3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscore = cross_val_score(RF , x_test_tf ,y_pred3 , scoring = 'accuracy' , cv =7)\nprint(\" mean accuracy of the model is \" , np.mean(score)*100 , np.std(score)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfpr,tpr,threshold=roc_curve(validation_y,y_pred3,pos_label=2)\nplt.plot(fpr,tpr)\nplt.xlabel(\"false Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.plot([0,1],[0,1],'k--')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### KNN classification....","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KNN = KNeighborsClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KNN.fit(x_train_tf ,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred4 = KNN.predict(x_test_tf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred4\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(validation_y,y_pred4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscore = cross_val_score(KNN , x_test_tf , y_pred4, scoring = 'accuracy' , cv =7)\nprint(\" mean accuracy of the model is \" , np.mean(score)*100 , np.std(score)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_test_phrase = movies_test['Phrase'].size\nclean_test_phrase = []\nfor i in range(0, num_test_phrase):\n    if( (i+1)%10000 == 0 ):\n        print (\"Review %d of %d\\n\" % ( i+1, num_test_phrase ))\n    clean_test_phrase.append(clean_review(movies_test['Phrase'][i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get a bag of words for the test set, and convert to a numpy array\ntest_data_features = count_vectorizer.transform(clean_test_phrase)\ntest_data_features = test_data_features.toarray()\n\n#Use the random forest to make sentiment label predictions\nresult = RF.predict(test_data_features)\n\n# Copy the results to a pandas dataframe with an \"id\" column and\n# a \"sentiment\" column\noutput = pd.DataFrame( data={\"PhraseId\":movies_test[\"PhraseId\"], \"Sentiment\":result} )\n\n# Use pandas to write the comma-separated output file\noutput.to_csv( \"submission.csv\", index=False, quoting=3 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}