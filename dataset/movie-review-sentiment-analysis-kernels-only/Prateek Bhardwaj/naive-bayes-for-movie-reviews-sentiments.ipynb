{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Naive Bayes Classifier for Sentiment Analysis\n\nThis is my first NLP competiton submission. Here, I am going to use the Naive Bayes Classifier from sklearn with some tweaks.","metadata":{}},{"cell_type":"markdown","source":"## Import the libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport zipfile\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-07T08:31:10.123482Z","iopub.execute_input":"2021-09-07T08:31:10.124012Z","iopub.status.idle":"2021-09-07T08:31:10.131442Z","shell.execute_reply.started":"2021-09-07T08:31:10.123953Z","shell.execute_reply":"2021-09-07T08:31:10.130067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get the Data\n\nOur textual dataset is stored in zipfiles for this competition, so we are going to extract the tsv files from their zips and then work on it.","metadata":{}},{"cell_type":"code","source":"# Get the training data\nwith zipfile.ZipFile('../input/movie-review-sentiment-analysis-kernels-only/train.tsv.zip') as z:\n    with z.open(\"train.tsv\") as t:\n        \n        train = pd.read_csv(t, sep = \"\\t\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:26:49.07664Z","iopub.execute_input":"2021-09-07T08:26:49.077301Z","iopub.status.idle":"2021-09-07T08:26:49.350825Z","shell.execute_reply.started":"2021-09-07T08:26:49.07725Z","shell.execute_reply":"2021-09-07T08:26:49.349746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the test data\nwith zipfile.ZipFile('../input/movie-review-sentiment-analysis-kernels-only/test.tsv.zip') as z:\n    with z.open(\"test.tsv\") as t:\n        \n        test = pd.read_csv(t, sep = \"\\t\")\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:26:49.352759Z","iopub.execute_input":"2021-09-07T08:26:49.353091Z","iopub.status.idle":"2021-09-07T08:26:49.473169Z","shell.execute_reply.started":"2021-09-07T08:26:49.35306Z","shell.execute_reply":"2021-09-07T08:26:49.47197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\nprint(test.shape)\n\nprint(train['Sentiment'].unique())","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:26:49.475721Z","iopub.execute_input":"2021-09-07T08:26:49.476211Z","iopub.status.idle":"2021-09-07T08:26:49.486164Z","shell.execute_reply.started":"2021-09-07T08:26:49.476161Z","shell.execute_reply":"2021-09-07T08:26:49.48506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have extracted out both of our data files and stored in dataframes to work with later. Also, as we can see there are 5 different sentiment classes (target) from 0-4, as explained in the competition page.","metadata":{}},{"cell_type":"code","source":"X = train['Phrase']\ny = train['Sentiment']\nX_test = test['Phrase']","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:26:49.488298Z","iopub.execute_input":"2021-09-07T08:26:49.488658Z","iopub.status.idle":"2021-09-07T08:26:49.497277Z","shell.execute_reply.started":"2021-09-07T08:26:49.488624Z","shell.execute_reply":"2021-09-07T08:26:49.49589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The NLP Pipeline\n\nFor the textual data, we will have to first take it through an NLP pipeline to preprocess it to be able to work with it using our classifier. We are going to take the text through a series of tokenization, stemming, and removing all the stopwords.","metadata":{}},{"cell_type":"code","source":"# Initialize all the preprocessing objects\n\ntokenizer = RegexpTokenizer(r\"\\w+\") # only select alphanumeric characters\nen_stop = set(stopwords.words('english')) # get all the English language stopwords\nps = PorterStemmer() # to extract stem out of any given word","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:26:49.498908Z","iopub.execute_input":"2021-09-07T08:26:49.499376Z","iopub.status.idle":"2021-09-07T08:26:49.511984Z","shell.execute_reply.started":"2021-09-07T08:26:49.499335Z","shell.execute_reply":"2021-09-07T08:26:49.510385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getStemmedReview(review):\n    \"\"\"\n        This function takes a review string and then performs the preprocessing steps on it\n        to return the cleaned review which will be more effective in predictions later made by the \n        classifier.\n    \"\"\"\n    review = review.lower()\n    \n    tokens = tokenizer.tokenize(review)\n    new_tokens = [token for token in tokens if token not in en_stop]\n    stemmed_tokens = [ps.stem(token) for token in new_tokens]\n    \n    cleaned_review = ' '.join(stemmed_tokens)\n    \n    return cleaned_review","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:26:49.513935Z","iopub.execute_input":"2021-09-07T08:26:49.514496Z","iopub.status.idle":"2021-09-07T08:26:49.526871Z","shell.execute_reply.started":"2021-09-07T08:26:49.514311Z","shell.execute_reply":"2021-09-07T08:26:49.525502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check out the results of the function \nprint(\"Review ===> \", X[0])\nprint(\"Preprocessed Review ===>\", getStemmedReview(X[0]))","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:26:49.528736Z","iopub.execute_input":"2021-09-07T08:26:49.529235Z","iopub.status.idle":"2021-09-07T08:26:49.542533Z","shell.execute_reply.started":"2021-09-07T08:26:49.529179Z","shell.execute_reply":"2021-09-07T08:26:49.541664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the preprocessed review is much more shorter, and conveys the same meaning as the original review.","metadata":{}},{"cell_type":"code","source":"# Apply the function on the whole dataset\nX_cleaned = X.apply(getStemmedReview)\n\nXtest_cleaned = X_test.apply(getStemmedReview)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:33:06.923559Z","iopub.execute_input":"2021-09-07T08:33:06.924008Z","iopub.status.idle":"2021-09-07T08:33:35.504939Z","shell.execute_reply.started":"2021-09-07T08:33:06.923973Z","shell.execute_reply":"2021-09-07T08:33:35.503933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_cleaned","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:27:18.306046Z","iopub.execute_input":"2021-09-07T08:27:18.306951Z","iopub.status.idle":"2021-09-07T08:27:18.318934Z","shell.execute_reply.started":"2021-09-07T08:27:18.306889Z","shell.execute_reply":"2021-09-07T08:27:18.317935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtest_cleaned","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:32:32.623531Z","iopub.execute_input":"2021-09-07T08:32:32.624025Z","iopub.status.idle":"2021-09-07T08:32:32.67042Z","shell.execute_reply.started":"2021-09-07T08:32:32.623982Z","shell.execute_reply":"2021-09-07T08:32:32.669051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove the reviews with empty ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's get to our Classifier\n\nFor our dataset here, we will use the Multinomial Naive Bayes Classifier to predict the different sentiments for each review","metadata":{}},{"cell_type":"code","source":"## First of all though, we'll need to convert our data into a count vector to be able \n## to work with the Multinomial Naive Bayes model\n\ncv = CountVectorizer()\n\nX_vec = cv.fit_transform(X_cleaned).toarray()\n\nX_vec.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:33:53.079622Z","iopub.execute_input":"2021-09-07T08:33:53.080265Z","iopub.status.idle":"2021-09-07T08:33:57.686886Z","shell.execute_reply.started":"2021-09-07T08:33:53.080226Z","shell.execute_reply":"2021-09-07T08:33:57.686073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A total of 10619 featureshave been extracted from our dataset. It would have been exponentially large had we not preprocessed it earlier. Next, we'll use this vectorizer to transform the testing data","metadata":{}},{"cell_type":"code","source":"Xtest_vec = cv.transform(Xtest_cleaned).toarray()\n\nXtest_vec.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:34:03.010653Z","iopub.execute_input":"2021-09-07T08:34:03.011287Z","iopub.status.idle":"2021-09-07T08:34:04.008058Z","shell.execute_reply.started":"2021-09-07T08:34:03.011247Z","shell.execute_reply":"2021-09-07T08:34:04.006931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have got our feature vectors, we will feed it into the Multinomial Naive Bayes Classifier and then check our model's accuracy score.\n\n","metadata":{}},{"cell_type":"code","source":"# Train the classifier\n\nmnb = MultinomialNB()\nmnb.fit(X_vec, y)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:34:07.358797Z","iopub.execute_input":"2021-09-07T08:34:07.359293Z","iopub.status.idle":"2021-09-07T08:40:24.480998Z","shell.execute_reply.started":"2021-09-07T08:34:07.359252Z","shell.execute_reply":"2021-09-07T08:40:24.47967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Time to make some predictions and submit them\n\npredictions = pd.Series(mnb.predict(Xtest_vec))\n\npredictions","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:48:30.017761Z","iopub.execute_input":"2021-09-07T08:48:30.018301Z","iopub.status.idle":"2021-09-07T08:48:32.814346Z","shell.execute_reply.started":"2021-09-07T08:48:30.018259Z","shell.execute_reply":"2021-09-07T08:48:32.813226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.concat([test.PhraseId, predictions], \n                      keys = ['PhraseId', 'Sentiment'],\n                      axis = 1)\n\nsubmission.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T08:49:25.716761Z","iopub.execute_input":"2021-09-07T08:49:25.717214Z","iopub.status.idle":"2021-09-07T08:49:25.850088Z","shell.execute_reply.started":"2021-09-07T08:49:25.717177Z","shell.execute_reply":"2021-09-07T08:49:25.849034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}