{"cells":[{"metadata":{},"cell_type":"markdown","source":"# This notebook shows how to implement different Machine learning algorithms to do sentiment analyses."},{"metadata":{},"cell_type":"markdown","source":"# Please Upvote this notebook if you find it useful"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data loading"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv.zip', delimiter='\\t')\ntest_df = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv.zip', delimiter='\\t')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.Phrase[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.Phrase[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['SentenceId'].unique()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.Phrase[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.Phrase[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data visualization "},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nimport matplotlib.pyplot as plt \n\ntarget_cnt = Counter(train_df.Sentiment)\n\nplt.figure(figsize=(16,8))\nplt.bar(target_cnt.keys(), target_cnt.values())\nplt.title(\"Dataset labels distribuition\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_Sen = Counter(train_df.SentenceId)\n\nplt.figure(figsize=(16,8))\nplt.bar(target_Sen.keys(), target_Sen.values())\nplt.title(\"Dataset SentenceId distribuition\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_Sen = Counter(train_df.PhraseId)\n\nplt.figure(figsize=(16,8))\nplt.bar(target_Sen.keys(), target_Sen.values())\nplt.title(\"Dataset PhraseId distribuition\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_EDA1 = train_df[train_df['Sentiment']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_EDA1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_Sen = Counter(df_EDA1.SentenceId)\n\nplt.figure(figsize=(16,8))\nplt.bar(target_Sen.keys(), target_Sen.values())\nplt.title(\"Dataset SentenceId distribuition\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_EDA1.SentenceId.value_counts().shape)\nprint(train_df.SentenceId.value_counts().shape)\nprint(df_EDA1.SentenceId.value_counts().shape[0]/train_df.SentenceId.value_counts().shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_EDA1.SentenceId.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_EDA2 = train_df[train_df['SentenceId']==3189]\ndf_EDA2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cnt = Counter(df_EDA2.Sentiment)\n\nplt.figure(figsize=(16,8))\nplt.bar(target_cnt.keys(), target_cnt.values())\nplt.title(\"Dataset labels distribuition of SentenceId 3189\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The graph shows that most of the sentiments in SentenceId 3189 are negative but most of the sentiment in the full data set is natural.\nso the SentenceId can be used as a feature to improve the results."},{"metadata":{},"cell_type":"markdown","source":"# Let's combined SentenceId and Phrase data."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **wordcloud of Very negative review**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nwc = WordCloud(max_words= 500, width= 1600, height= 800, stopwords= stop_words).generate(\" \".join(train_df[train_df.Sentiment==0].Phrase))\nplt.imshow(wc, interpolation= 'bilinear')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# wordcloud of negative review"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nwc = WordCloud(max_words= 500, width= 1600, height= 800, stopwords= stop_words).generate(\" \".join(train_df[train_df.Sentiment==1].Phrase))\nplt.imshow(wc, interpolation= 'bilinear')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# wordcloud of Very natural review"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nwc = WordCloud(max_words= 500, width= 1600, height= 800, stopwords= stop_words).generate(\" \".join(train_df[train_df.Sentiment==2].Phrase))\nplt.imshow(wc, interpolation= 'bilinear')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wordcloud of Very postive review "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nwc = WordCloud(max_words= 500, width= 1600, height= 800, stopwords= stop_words).generate(\" \".join(train_df[train_df.Sentiment==3].Phrase))\nplt.imshow(wc, interpolation= 'bilinear')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# wordcloud of Very very postive review"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nwc = WordCloud(max_words= 500, width= 1600, height= 800, stopwords= stop_words).generate(\" \".join(train_df[train_df.Sentiment==4].Phrase))\nplt.imshow(wc, interpolation= 'bilinear')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Base line for a model"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_line = len(train_df[train_df['Sentiment']==2])/len(train_df.Sentiment)\nbase_line","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive bayes moldes without data preprocesing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n#split\nx_train1,x_test1,y_train1,y_test1=train_test_split(train_df['Phrase'],train_df['Sentiment'],test_size=0.2,random_state=42)\n#vectorizing\ncv=CountVectorizer()\nX_train1=cv.fit_transform(x_train1.values)\n#model \nmodel1=MultinomialNB()\nmodel1.fit(X_train1,y_train1)\ntest_count1=cv.transform(x_test1.values)\nModel1_score =model1.score(test_count1,y_test1)\npred1 = model1.predict(test_count1)\n\nprint('Model1 score:' ,Model1_score) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Naive Bayes is the simplest Machine learning model that can use to do classifications. Even without pre-processing the text, Naive Bayes got an accuracy of 61%"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score\nfrom matplotlib import pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\nconf = confusion_matrix(y_test1, pred1)\n\ncm = pd.DataFrame(\n    conf, index = [i for i in ['0', '1', '2', '3', '4']],\n    columns = [i for i in ['0', '1', '2', '3', '4']]\n)\n\nplt.figure(figsize = (12,7))\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boosting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngbrt = GradientBoostingClassifier(random_state =0)\ngbrt.fit(X_train1,y_train1)\nModel2_score =gbrt.score(test_count1,y_test1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model2_Accscore =gbrt.score(X_train1,y_train1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model2_Accscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model2_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model2_score\npred2 = gbrt.predict(test_count1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf = confusion_matrix(y_test1, pred2)\n\ncm = pd.DataFrame(\n    conf, index = [i for i in ['0', '1', '2', '3', '4']],\n    columns = [i for i in ['0', '1', '2', '3', '4']]\n)\n\nplt.figure(figsize = (12,7))\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\nlinear_svm = LinearSVC(C=100).fit(X_train1,y_train1)\nModel3_score =linear_svm.score(test_count1,y_test1)\npred3 = linear_svm.predict(test_count1)\n\nprint('Model13 score:' ,Model3_score)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model traing Acc"},{"metadata":{"trusted":true},"cell_type":"code","source":"Model3_Accscore =linear_svm.score(X_train1,y_train1)\nModel3_Accscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf = confusion_matrix(y_test1, pred3)\n\ncm = pd.DataFrame(\n    conf, index = [i for i in ['0', '1', '2', '3', '4']],\n    columns = [i for i in ['0', '1', '2', '3', '4']]\n)\n\nplt.figure(figsize = (12,7))\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm = SVC().fit(X_train1,y_train1)\nModel4_score =svm.score(test_count1,y_test1)\npred4 = linear_svm.predict(test_count1)\n\nprint('Model14 score:' ,Model4_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TensorFlow Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_datasets as tfds ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = y_train1\ndepth = 5\ny_train = tf.one_hot(indices, depth)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], output_shape=[512,16], \n  dtype=tf.string,trainable= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Sequential([\n  hub_layer,\n    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n    \n  tf.keras.layers.Dense(256, activation='relu'),\n  tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dense(5, activation='softmax')\n  ])\nmodel.summary()\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics= ['acc','MAE'])\n#model.fit(x_train1, y_train, batch_size= 64, validation_split = 0.2, epochs= 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tf.config.experimental_run_functions_eagerly(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x_train1, y_train, batch_size= 64, validation_split = 0.2, epochs= 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred4 = model.predict_classes(x_test1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf = confusion_matrix(y_test1, pred4)\n\ncm = pd.DataFrame(\n    conf, index = [i for i in ['0', '1', '2', '3', '4']],\n    columns = [i for i in ['0', '1', '2', '3', '4']]\n)\n\nplt.figure(figsize = (12,7))\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This notebook shows how to implement Machine learning and deep learning models to do sentiment analysis even without any data preprocessing and hyperparameter tuning the accuracies of the models are reasonably good.**\n"},{"metadata":{},"cell_type":"markdown","source":"# Let's see model performance after cleaning the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) >2 and token not in stop_words:\n            result.append(token)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Phrase_lower'] = train_df['Phrase'].str.lower()\ntrain_df['Phrase_text_new'] = train_df['Phrase_lower'].str.replace(r'[^A-Za-z0-9]+', ' ')\ntrain_df['cleen_Phrase'] =train_df['Phrase_text_new'].apply(preprocess)\ntrain_df['cleen_Phrase']= train_df['cleen_Phrase'].apply(lambda x: \" \".join(x))\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['cleen_Phrase'][2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train2,x_test2,y_train2,y_test2=train_test_split(train_df['cleen_Phrase'],train_df['Sentiment'],test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = tf.keras.models.Sequential([\n  hub_layer,\n    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n    \n  tf.keras.layers.Dense(256, activation = tf.keras.layers.LeakyReLU(alpha=0.3)),\n    tf.keras.layers.Dropout(0.3),\n    \n  tf.keras.layers.Dense(64, activation = tf.keras.layers.LeakyReLU(alpha=0.3)),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(8, activation='relu'),\n  tf.keras.layers.Dense(5, activation='softmax')\n  ])\nmodel1.summary()\nmodel1.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics= ['acc','MAE'])\n#model.fit(x_train1, y_train, batch_size= 64, validation_split = 0.2, epochs= 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = y_train2\ndepth = 5\nY_train = tf.one_hot(indices, depth)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.fit(x_train2, Y_train, batch_size= 64, validation_split = 0.2, epochs= 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred5 = model1.predict_classes(x_test2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf = confusion_matrix(y_test2, pred5)\n\ncm = pd.DataFrame(\n    conf, index = [i for i in ['0', '1', '2', '3', '4']],\n    columns = [i for i in ['0', '1', '2', '3', '4']]\n)\n\nplt.figure(figsize = (12,7))\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# I hope you find this notebook useful please upvote the notebook and give your comment."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}