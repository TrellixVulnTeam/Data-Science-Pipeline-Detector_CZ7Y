{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#import libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport tensorflow as tf\nimport random\nimport pandas as pd\nimport os\nprint(os.listdir(\"../input\"))","execution_count":17,"outputs":[{"output_type":"stream","text":"['train.tsv', 'test.tsv', 'sampleSubmission.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data_path = os.path.join(\"../input\", 'train.tsv')\ntest_data_path = os.path.join(\"../input\", 'test.tsv')\ndata = pd.read_csv(data_path, sep='\\t')\ntest_data = pd.read_csv(test_data_path, sep='\\t')\ndata.describe()","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"            PhraseId     SentenceId      Sentiment\ncount  156060.000000  156060.000000  156060.000000\nmean    78030.500000    4079.732744       2.063578\nstd     45050.785842    2502.764394       0.893832\nmin         1.000000       1.000000       0.000000\n25%     39015.750000    1861.750000       2.000000\n50%     78030.500000    4017.000000       2.000000\n75%    117045.250000    6244.000000       3.000000\nmax    156060.000000    8544.000000       4.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PhraseId</th>\n      <th>SentenceId</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>156060.000000</td>\n      <td>156060.000000</td>\n      <td>156060.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>78030.500000</td>\n      <td>4079.732744</td>\n      <td>2.063578</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>45050.785842</td>\n      <td>2502.764394</td>\n      <td>0.893832</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>39015.750000</td>\n      <td>1861.750000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>78030.500000</td>\n      <td>4017.000000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>117045.250000</td>\n      <td>6244.000000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>156060.000000</td>\n      <td>8544.000000</td>\n      <td>4.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"   PhraseId    ...      Sentiment\n0         1    ...              1\n1         2    ...              2\n2         3    ...              2\n3         4    ...              2\n4         5    ...              2\n\n[5 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PhraseId</th>\n      <th>SentenceId</th>\n      <th>Phrase</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>A series of escapades demonstrating the adage ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>A series of escapades demonstrating the adage ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>A series</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>A</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>series</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_texts = data['Phrase']\ntrain_labels = np.array(data['Sentiment'])\n\n#train_labels = pd.get_dummies(train_labels)\n\ntest_texts = test_data['Phrase']\nX_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.33, random_state=42)\ny_temp = y_train\ny_train = pd.get_dummies(y_train)\ny_test = pd.get_dummies(y_test)\n\ndata = ((X_train, np.array(y_train)),(X_test, np.array(y_test)))","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\nNGRAM_RANGE = (1,3)\nTOP_K = 9000\nTOKEN_MODE = 'word'\nMIN_DOCUMENT_FREQUENCY = 5","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ngram_vectorize(train_texts, train_labels, val_texts, test_texts):\n    \"\"\"Vectorizes texts as n-gram vectors.\n    # Arguments\n        train_texts: list, training text strings.\n        train_labels: np.ndarray, training labels.\n        val_texts: list, validation text strings.\n\n    # Returns\n        x_train, x_val: vectorized training and validation texts\n    \"\"\"\n    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n    \n        # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n    stop_words_lst = text.ENGLISH_STOP_WORDS.union([\"\\'s\"])\n    kwargs = {\n            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n            'dtype': 'int32',\n            'strip_accents': 'unicode',\n            'decode_error': 'replace',\n            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n            'min_df': MIN_DOCUMENT_FREQUENCY,\n            'stop_words' : stop_words_lst\n    }\n    vectorizer = TfidfVectorizer(**kwargs)\n\n    # Learn vocabulary from training texts and vectorize training texts.\n    x_train = vectorizer.fit_transform(train_texts)\n\n    # Vectorize validation texts.\n    x_val = vectorizer.transform(val_texts)\n    \n    # Vectorize test texts.\n    x_test = vectorizer.transform(test_texts)\n    \n    # Select top 'k' of the vectorized features.\n    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n    selector.fit(x_train, y_temp)\n    \n    \n    x_train = selector.transform(x_train).astype('float32')\n    x_val = selector.transform(x_val).astype('float32')\n    x_test = selector.transform(x_test).astype('float32')\n    return x_train, x_val, x_test","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.keras import models\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.python.keras.layers import Dropout","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n    \"\"\"Creates an instance of a multi-layer perceptron model.\n\n    # Arguments\n        layers: int, number of `Dense` layers in the model.\n        units: int, output dimension of the layers.\n        dropout_rate: float, percentage of input to drop at Dropout layers.\n        input_shape: tuple, shape of input to the model.\n        num_classes: int, number of output classes.\n\n    # Returns\n        An MLP model instance.\n    \"\"\"\n    print('input shape : ', input_shape)\n    model = models.Sequential()\n    model.add(Dropout(dropout_rate, input_shape=input_shape))\n    i=0\n    for _ in range(layers-1):\n        model.add(Dense(units=units, activation='relu'))\n        model.add(Dropout(rate=dropout_rate))\n        pass\n    \n    model.add(Dense(units=64, activation='relu'))\n    model.add(Dropout(rate=dropout_rate))\n        \n    model.add(Dense(units=num_classes, activation='softmax', name='d2'))\n    return model","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_ngram_model(X_train, \n                      X_label,\n                      X_validate,\n                      val_label,\n                      learning_rate=1e-3,\n                      epochs=50,\n                      batch_size=150,\n                      layers=3,\n                      units=128,\n                      dropout_rate=0.2):\n    \"\"\"Trains n-gram model on the given dataset.\n\n    # Arguments\n        data: tuples of training and test texts and labels.\n        learning_rate: float, learning rate for training model.\n        epochs: int, number of epochs.\n        batch_size: int, number of samples per batch.\n        layers: int, number of `Dense` layers in the model.\n        units: int, output dimension of Dense layers in the model.\n        dropout_rate: float: percentage of input to drop at Dropout layers.\n    \"\"\"\n    num_classes = 5\n    \n    x_train, x_val = X_train, X_validate\n    val_labels = val_label\n    # Create model instance.\n    model = mlp_model(layers=layers,\n                      units=units,\n                      dropout_rate=dropout_rate,\n                      input_shape=x_train.shape[1:],\n                      num_classes=num_classes)\n    loss = 'categorical_crossentropy'\n    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n   \n    callbacks = [tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss', patience=6)]\n    # Train and validate model.\n    history = model.fit(\n            x_train,\n            X_label,\n            epochs=epochs,\n            callbacks=callbacks,\n            validation_data=(x_val, val_labels),\n            verbose=2,  # Logs once per epoch.\n            batch_size=batch_size)\n    # Print results.\n    history = history.history\n    print('Validation accuracy: {acc}, loss: {loss}'.format(\n            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n    \n    return model","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(train_texts, train_labels), (val_texts, val_labels) = data\nx_train, x_val, x_test = ngram_vectorize(train_texts, train_labels, val_texts, test_texts)\nmodel = train_ngram_model(x_train, train_labels, x_val, val_labels)","execution_count":35,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1577: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n  UserWarning)\n","name":"stderr"},{"output_type":"stream","text":"input shape :  (9000,)\nTrain on 104560 samples, validate on 51500 samples\nEpoch 1/50\n - 7s - loss: 1.1304 - acc: 0.5661 - val_loss: 0.9996 - val_acc: 0.6028\nEpoch 2/50\n - 6s - loss: 0.9893 - acc: 0.6208 - val_loss: 0.9797 - val_acc: 0.6127\nEpoch 3/50\n - 6s - loss: 0.9623 - acc: 0.6335 - val_loss: 0.9780 - val_acc: 0.6162\nEpoch 4/50\n - 6s - loss: 0.9496 - acc: 0.6397 - val_loss: 0.9734 - val_acc: 0.6203\nEpoch 5/50\n - 6s - loss: 0.9413 - acc: 0.6431 - val_loss: 0.9707 - val_acc: 0.6221\nEpoch 6/50\n - 6s - loss: 0.9317 - acc: 0.6466 - val_loss: 0.9692 - val_acc: 0.6225\nEpoch 7/50\n - 6s - loss: 0.9248 - acc: 0.6504 - val_loss: 0.9696 - val_acc: 0.6255\nEpoch 8/50\n - 6s - loss: 0.9199 - acc: 0.6517 - val_loss: 0.9725 - val_acc: 0.6257\nEpoch 9/50\n - 6s - loss: 0.9165 - acc: 0.6529 - val_loss: 0.9741 - val_acc: 0.6269\nEpoch 10/50\n - 6s - loss: 0.9111 - acc: 0.6557 - val_loss: 0.9807 - val_acc: 0.6273\nEpoch 11/50\n - 6s - loss: 0.9067 - acc: 0.6567 - val_loss: 0.9762 - val_acc: 0.6272\nEpoch 12/50\n - 6s - loss: 0.9042 - acc: 0.6584 - val_loss: 0.9780 - val_acc: 0.6284\nValidation accuracy: 0.6283689141273499, loss: 0.9780016478982944\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make prediction\ny_test = model.predict(x_test)\ny_class = np.argmax(y_test, axis=1)\n\n#write output\nmy_submission = pd.DataFrame({'PhraseId': test_data.PhraseId, 'Sentiment': y_class})\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}