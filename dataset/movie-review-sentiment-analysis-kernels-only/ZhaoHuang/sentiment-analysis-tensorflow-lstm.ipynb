{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport tensorflow as tf \nimport numpy as np \nimport pandas as pd\nimport random\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n#load data from file\nword_embeddings = np.load(\"../input/preprocessed-data/word_embeddings.npy\")\nword_id_train_val = np.load(\"../input/preprocessed-data/word_id_train.npy\")\nsequence_len_train_val = np.load(\"../input/preprocessed-data/sequence_len_train.npy\")\nsentiment_vectors_train_val = np.load(\"../input/preprocessed-data/sentiment_vectors_train.npy\")\n\nword_id_test = np.load(\"../input/preprocessed-data/word_id_test.npy\")\nsequence_len_test = np.load(\"../input/preprocessed-data/sequence_len_test.npy\")\n\nseq_len = len(word_id_train_val[0])\nVOCAB_LENGTH,EMBEDDING_DIMENSION = word_embeddings.shape\n\nrnn_size = EMBEDDING_DIMENSION\nlearning_rate = 0.01\nlearning_rate_dacay = 0.9\nnum_layers = 2\nnum_labels = 5\n\nwith tf.name_scope(\"inputs\"):\n  inputs = tf.placeholder(tf.int32, [None,seq_len])\n  targets = tf.placeholder(tf.float32, [None,num_labels])\n  input_sequence_length = tf.placeholder(tf.int32, [None,])\n  BATCH_SIZE = tf.placeholder(tf.int32, [], name='BATCH_SIZE')\n\nwith tf.name_scope(\"embedding\"):\n  glove_weights_initializer = tf.constant_initializer(word_embeddings)\n  embedding_weights = tf.get_variable(name='embedding_weights', shape=(VOCAB_LENGTH, EMBEDDING_DIMENSION), initializer=glove_weights_initializer,trainable=True)\n  embedded_inputs = tf.nn.embedding_lookup(embedding_weights, inputs)\n\n#RNN cells\nwith tf.name_scope(\"rnn\"):\n  cell = tf.nn.rnn_cell.BasicLSTMCell(rnn_size)\n  cell =  tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob = 0.5)\n  cell = tf.nn.rnn_cell.MultiRNNCell([cell for _ in range(num_layers)])\n  \n  outputs, state = tf.nn.dynamic_rnn(cell= cell, inputs = embedded_inputs, sequence_length = input_sequence_length, dtype = tf.float32)\n  #state dimension: [num_layers, 2, batch_size, rnn_size]\n  last = state[-1][1]\n\n#output layer\nwith tf.name_scope(\"score\"):\n  weights = tf.Variable(tf.random_normal([rnn_size, num_labels]))\n  bias = tf.Variable(tf.random_normal([num_labels]))\n  logits = tf.nn.bias_add(tf.matmul(last, weights), bias)\n  #logits = tf.layers.dense(last, num_labels)\n\n# calculate loss and create optimizer\nwith tf.name_scope(\"optimize\"):\n  loss = tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=logits)\n  total_loss = tf.reduce_mean(loss)\n  trainable_vars = tf.trainable_variables()\n  grads,_ = tf.clip_by_global_norm(tf.gradients(total_loss, trainable_vars), 5.0)\n  lr = tf.Variable(0.0, trainable=False)\n  optimizer = tf.train.AdamOptimizer(lr)\n  train_op = optimizer.apply_gradients(zip(grads, trainable_vars))\n  #train_op = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n\nwith tf.name_scope(\"accuracy\"):\n  predictions = tf.argmax(logits, axis = 1) #按行查找\n  correct_pred = tf.equal(predictions, tf.argmax(targets,1))\n  accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n#split the data into training set and validation set 8:2\nlength_training = int (len(word_id_train_val)*0.8)\nword_id_train = word_id_train_val[:length_training]\nword_id_val = word_id_train_val[length_training:]\nsequence_len_train = sequence_len_train_val[:length_training]\nsequence_len_val = sequence_len_train_val[length_training:]\nsentiment_vectors_train = sentiment_vectors_train_val[:length_training]\nsentiment_vectors_val = sentiment_vectors_train_val[length_training:]\n\nprint (\"the number of samples in training set: \", len(word_id_train))\nprint (\"the number of samples in validation set: \", len(word_id_val) )\n\n\ndef shuffle_data(word_id, sentiment, sequence_len):\n  ids = list(range(len(word_id)))\n  random.shuffle(ids) #shuffle will change the list in-place\n  return [word_id[i] for i in ids], [sentiment[i] for i in ids], [sequence_len[i] for i in ids]\n\ndef get_batches(word_id, sentiment, sequence_len, batch_size):\n  data_batches, labels_batches, length_batches = [], [], []\n  for i in range(len(word_id)//batch_size):\n    data_batch = word_id[i*batch_size: (i+1)*batch_size]\n    labels_batch = sentiment[i*batch_size: (i+1)*batch_size]\n    length_batch = sequence_len[i*batch_size: (i+1)*batch_size]\n    data_batches.append(data_batch)\n    labels_batches.append(labels_batch)\n    length_batches.append(length_batch)\n  return data_batches, labels_batches, length_batches\n\n#feed data into NN and start training\nnum_epoches = 4\nbatch_size = 128\nnum_batches = len(word_id_train)//batch_size\ninit_op = tf.global_variables_initializer()\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n\n  sess.run(init_op)\n  for epoch in range(num_epoches):\n    sess.run(tf.assign(lr, learning_rate *\n                               (learning_rate_dacay ** (epoch))))\n    training_accuracy = []\n    training_loss = []\n    shuffled_word_id_train, shuffled_sentiment_train, shuffled_sequence_len_train = shuffle_data(word_id_train, sentiment_vectors_train, sequence_len_train)\n    data_batches, labels_batches, length_batches = get_batches(shuffled_word_id_train, shuffled_sentiment_train, shuffled_sequence_len_train, batch_size)\n    \n    for batch in range(num_batches):\n      feed = {inputs: data_batches[batch], targets: labels_batches[batch], input_sequence_length: length_batches[batch], BATCH_SIZE:batch_size}\n      loss,_, train_accuracy = sess.run([total_loss, train_op, accuracy], feed_dict=feed)\n      training_loss.append(loss)\n      training_accuracy.append(train_accuracy)\n      if (batch%100 == 0):\n        data_batches_val, labels_batches_val, length_batches_val = get_batches(word_id_val, sentiment_vectors_val,sequence_len_val, batch_size)\n        val_accuracy = []\n        for i in range(len(word_id_val)//batch_size):\n          feed_val = {inputs: data_batches_val[i], targets: labels_batches_val[i], input_sequence_length: length_batches_val[i], BATCH_SIZE:batch_size}\n          accuracy_i = sess.run(accuracy, feed_dict=feed_val)\n          val_accuracy.append(accuracy_i)\n        \n       \n        print('Epoch {:>3}/{} - Training Loss: {:>6.3f} - Training accuracy: {:>6.3f} - Validation accuracy: {:>6.3f}'\n             .format( epoch+1,\n                      num_epoches,  \n                      loss,\n                      train_accuracy,\n                      np.mean(val_accuracy)\n                      ))\n\n      \n    print('Epoch {:>3}/{} - Epoch Training Loss: {:>6.3f} - Epoch Training accuracy: {:>6.3f}'\n             .format( epoch+1,\n                      num_epoches,  \n                      np.mean(training_loss),\n                      np.mean(training_accuracy)\n                      ))\n\n  \n\n  #make predictions on test data\n  predictions = []\n\n  for i in range(len(word_id_test)):\n\n    feed_prediction = {inputs: [word_id_test[i]], \n                      input_sequence_length: [sequence_len_test[i]],\n                      BATCH_SIZE:1 }\n\n \n    logit = sess.run(logits, feed_dict=feed_prediction)\n    predictions.append(np.argmax(logit, axis = 1)[0])\n\n  data = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv', sep='\\t')\n  data['Sentiment'] = predictions\n  data.drop(['Phrase', 'SentenceId'], axis=1, inplace=True)\n  print(data.head(10))\n  data.to_csv('Submission.csv', header=True, index=None, sep=',')\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}