{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Movie Review Sentiment Analysis"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://www.rottentomatoes.com/assets/pizza-pie/head-assets/images/RT_TwitterCard_2018.jpg\" alt=\"drawing\" width=\"500\" height=\"600\"/>\n<br><br>\n\"There's a thin line between likably old-fashioned and fuddy-duddy, and The Count of Monte Cristo ... never quite settles on either side.\" \n<br><br>\nThe <b>Rotten Tomatoes<b> movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee.In their work on sentiment treebanks, Socher et al. used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. This competition presents a chance to benchmark your sentiment-analysis ideas on the Rotten Tomatoes dataset. You are asked to label phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging."},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer,WordNetLemmatizer\nfrom string import punctuation\nfrom wordcloud import WordCloud\nfrom nltk.tokenize import TweetTokenizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.decomposition import TruncatedSVD\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split,GridSearchCV,StratifiedKFold\nfrom sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score,confusion_matrix\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.callbacks import EarlyStopping,ReduceLROnPlateau\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense,Embedding,Bidirectional,Dropout,SpatialDropout1D,GlobalMaxPool1D,LSTM,BatchNormalization,Conv1D,MaxPool1D\nfrom keras.models import Sequential,load_model\nfrom keras.optimizers import Adam,RMSprop\nfrom keras import regularizers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/train.tsv.zip\",sep=\"\\t\")\ntest = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/test.tsv.zip\",sep=\"\\t\")\nsub = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Count Of Tweets\nsns.countplot(train['Sentiment'],palette='rocket_r')\nplt.title(\"No of Tweet Sentiments\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The sentiment labels are:\n\n0 - negative <br>\n1 - somewhat negative <br>\n2 - neutral <br>\n3 - somewhat positive <br>\n4 - positive"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Phrases Per Sentence\nfig,ax = plt.subplots(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(train.groupby('SentenceId')['PhraseId'].count())\nplt.title(\"Avg Phrases Per Sentence in Train\")\nplt.subplot(1,2,2)\nsns.distplot(test.groupby('SentenceId')['PhraseId'].count())\nplt.title(\"Avg Phrases Per Sentence in Test\")\n\nprint(\"Avg Phrases Per Sentence in Train: \",round(train.groupby('SentenceId')['PhraseId'].count().mean()))\nprint(\"Avg Phrases Per Sentence in Test: \",round(test.groupby('SentenceId')['PhraseId'].count().mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#No of Words in Phrases\nfig,ax = plt.subplots(figsize=(22,5))\nfig.suptitle(\"Avg Words In Phrases\",fontsize=16)\n\nplt.subplot(1,5,1)\nsns.distplot(train[train['Sentiment']==0]['Phrase'].str.split().apply(lambda x:len(x)))\nplt.title(\"Sentiment 0\")\nprint(\"Avg Words in Phrases with Sentiment 0: \",round(train[train['Sentiment']==0]['Phrase'].str.split().apply(lambda x:len(x)).mean()))\n\nplt.subplot(1,5,2)\nsns.distplot(train[train['Sentiment']==1]['Phrase'].str.split().apply(lambda x:len(x)))\nplt.title(\"Sentiment 1\")\nprint(\"Avg Words in Phrases with Sentiment 1: \",round(train[train['Sentiment']==1]['Phrase'].str.split().apply(lambda x:len(x)).mean()))\n\nplt.subplot(1,5,3)\nsns.distplot(train[train['Sentiment']==2]['Phrase'].str.split().apply(lambda x:len(x)))\nplt.title(\"Sentiment 2\")\nprint(\"Avg Words in Phrases with Sentiment 2: \",round(train[train['Sentiment']==2]['Phrase'].str.split().apply(lambda x:len(x)).mean()))\n\nplt.subplot(1,5,4)\nsns.distplot(train[train['Sentiment']==3]['Phrase'].str.split().apply(lambda x:len(x)))\nplt.title(\"Sentiment 3\")\nprint(\"Avg Words in Phrases with Sentiment 3: \",round(train[train['Sentiment']==3]['Phrase'].str.split().apply(lambda x:len(x)).mean()))\n\nplt.subplot(1,5,5)\nsns.distplot(train[train['Sentiment']==4]['Phrase'].str.split().apply(lambda x:len(x)))\nplt.title(\"Sentiment 4\")\nprint(\"Avg Words in Phrases with Sentiment 4: \",round(train[train['Sentiment']==4]['Phrase'].str.split().apply(lambda x:len(x)).mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Avg Characters in Phrases\nfig,ax = plt.subplots(figsize=(22,5))\nfig.suptitle(\"Avg Characters In Phrases\",fontsize=16)\n\nplt.subplot(1,5,1)\nsns.distplot(train[train['Sentiment']==0]['Phrase'].str.len())\nplt.title(\"Sentiment 0\")\nprint(\"Avg Characters in Phrases with Sentiment 0: \",round(train[train['Sentiment']==0]['Phrase'].str.len().mean()))\n\nplt.subplot(1,5,2)\nsns.distplot(train[train['Sentiment']==1]['Phrase'].str.len())\nplt.title(\"Sentiment 1\")\nprint(\"Avg Characters in Phrases with Sentiment 1: \",round(train[train['Sentiment']==1]['Phrase'].str.len().mean()))\n\nplt.subplot(1,5,3)\nsns.distplot(train[train['Sentiment']==2]['Phrase'].str.len())\nplt.title(\"Sentiment 2\")\nprint(\"Avg Characters in Phrases with Sentiment 2: \",round(train[train['Sentiment']==2]['Phrase'].str.len().mean()))\n\nplt.subplot(1,5,4)\nsns.distplot(train[train['Sentiment']==3]['Phrase'].str.len())\nplt.title(\"Sentiment 3\")\nprint(\"Avg Characters in Phrases with Sentiment 3: \",round(train[train['Sentiment']==3]['Phrase'].str.len().mean()))\n\nplt.subplot(1,5,5)\nsns.distplot(train[train['Sentiment']==4]['Phrase'].str.len())\nplt.title(\"Sentiment 4\")\nprint(\"Avg Characters in Phrases with Sentiment 4: \",round(train[train['Sentiment']==4]['Phrase'].str.len().mean()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#WordCloud\nfig,ax = plt.subplots(figsize=(20,40))\nplt.axis('off')\n\nplt.subplot(5,1,1)\ntext = \" \".join(train[train['Sentiment']==0]['Phrase'])\nwordcloud = WordCloud(max_font_size = None,background_color='white',width=1000,height=1000).generate(text)\nplt.title(\"WordCloud for Sentiment 0\")\nplt.axis('off')\nplt.imshow(wordcloud)\n\nplt.subplot(5,1,2)\ntext = \" \".join(train[train['Sentiment']==1]['Phrase'])\nwordcloud = WordCloud(max_font_size = None,background_color='white',width=1000,height=1000).generate(text)\nplt.title(\"WordCloud for Sentiment 1\")\nplt.axis('off')\nplt.imshow(wordcloud)\n\nplt.subplot(5,1,3)\ntext = \" \".join(train[train['Sentiment']==2]['Phrase'])\nwordcloud = WordCloud(max_font_size = None,background_color='white',width=1000,height=1000).generate(text)\nplt.title(\"WordCloud for Sentiment 2\")\nplt.axis('off')\nplt.imshow(wordcloud)\n\nplt.subplot(5,1,4)\ntext = \" \".join(train[train['Sentiment']==3]['Phrase'])\nwordcloud = WordCloud(max_font_size = None,background_color='white',width=1000,height=1000).generate(text)\nplt.title(\"WordCloud for Sentiment 3\")\nplt.axis('off')\nplt.imshow(wordcloud)\n\nplt.subplot(5,1,5)\ntext = \" \".join(train[train['Sentiment']==4]['Phrase'])\nwordcloud = WordCloud(max_font_size = None,background_color='white',width=1000,height=1000).generate(text)\nplt.title(\"WordCloud for Sentiment 4\")\nplt.axis('off')\nplt.imshow(wordcloud)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning"},{"metadata":{},"cell_type":"markdown","source":"We can't remove stopwords and punctuations as we have phrases as our data and phrases can contain a single word."},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = SnowballStemmer('english',ignore_stopwords=True)\nlemmatizer = WordNetLemmatizer()\ndef clean(text):\n    sentence=[]\n    for word in text.split():\n        word = re.sub('[^a-zA-Z]','',word)\n        word = word.lower()\n        word = lemmatizer.lemmatize(word)\n        word = word.strip()\n        sentence.append(word)\n    return \" \".join(sentence)\n\ntrain['Phrase'] = train['Phrase'].apply(lambda x:clean(x))\ntest['Phrase'] = test['Phrase'].apply(lambda x:clean(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_valid,y_train,y_valid = train_test_split(train['Phrase'],train['Sentiment'],test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Neural Network (Conv + LSTM)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tokenize\nvocab_size=20000\nembedding_dim=200\nmax_length=50\ntrunc_type=\"post\"\npad_type=\"post\"\noov_tok=\"<OOV>\"\nepochs=10\nbatch_size=128\n\ntokenizer = Tokenizer(num_words=vocab_size,oov_token=oov_tok)\ntokenizer.fit_on_texts(list(x_train)+list(x_valid))\nword_index = tokenizer.word_index\n\ntrain_seq = tokenizer.texts_to_sequences(x_train)\ntrain_pad = pad_sequences(train_seq,maxlen=max_length,truncating = trunc_type,padding=pad_type)\n\nval_seq = tokenizer.texts_to_sequences(x_valid)\nval_pad = pad_sequences(val_seq,maxlen=max_length,truncating = trunc_type,padding=pad_type)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Glove Embeddings\nembeddings_index={}\nwith open(\"../input/glove6b/glove.6B.200d.txt\",'r',encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n        \nembeddings_matrix = np.zeros((len(word_index)+1, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model\n\nmodel = Sequential()\nmodel.add(Embedding(len(word_index)+1,embedding_dim,input_length=max_length,weights=[embeddings_matrix]))\nmodel.add(SpatialDropout1D(0.4))\n\nmodel.add(Conv1D(128,3,activation='relu',padding='same'))\nmodel.add(MaxPool1D(2))\n\nmodel.add(Conv1D(64,3,activation='relu',padding='same'))\nmodel.add(MaxPool1D(2))\n\nmodel.add(Bidirectional(LSTM(64,recurrent_dropout=0.5,dropout=0.5,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(64,recurrent_dropout=0.5,dropout=0.5,return_sequences=True)))\n\nmodel.add(GlobalMaxPool1D())\n\nmodel.add(Dense(64,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(32,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(5,activation='softmax'))\n\nmodel.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Callbacks\nearlystop = EarlyStopping(monitor='val_loss',patience=2,verbose=1)\nlearning_reduce = ReduceLROnPlateau(patience=1,monitor=\"val_acc\",verbose=1,min_lr=0.00001,factor=0.5,cooldown=1)\ncallbacks = [earlystop,learning_reduce]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_pad,y_train,epochs=epochs,validation_data=(val_pad,y_valid),callbacks=callbacks,\n                    batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history[\"val_\"+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string,\"val_\"+string])\n    plt.show()\nplot_graphs(history,'acc')\nplot_graphs(history,'loss')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_tokenizer = TweetTokenizer()\ntfidf = TfidfVectorizer(strip_accents=None,lowercase=False,preprocessor=None,ngram_range=(1,3),\n                        tokenizer=tweet_tokenizer.tokenize,use_idf=True,norm='l2',smooth_idf=True)\ntfidf.fit(list(x_train.values) + list(x_valid.values))\nxtrain_tfv = tfidf.transform(x_train)\nxvalid_tfv = tfidf.transform(x_valid)\n\nscl = preprocessing.StandardScaler(with_mean=False)\nxtrain_tfv_std = scl.fit_transform(xtrain_tfv)\nxvalid_tfv_std = scl.transform(xvalid_tfv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic = LogisticRegressionCV(cv=3,scoring='accuracy',random_state=42,n_jobs=-1,verbose=3)\nlogistic.fit(xtrain_tfv_std,y_train)\nlogistic_accuracy = logistic.score(xvalid_tfv_std,y_valid)\nprint(\"Accuracy:\",logistic_accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM "},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(C=0.1,random_state=42,verbose=2)\nsvc.fit(xtrain_tfv_std,y_train)\nsvc_accuracy = svc.score(xvalid_tfv_std,y_valid)\nprint(\"Accuracy:\",svc_accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"naive = MultinomialNB()\nnaive.fit(xtrain_tfv_std,y_train)\nnaive_accuracy = naive.score(xvalid_tfv_std,y_valid)\nprint(\"Accuracy:\",naive_accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgboost = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                            subsample=0.8, nthread=10, learning_rate=0.1,verbose=2)\nxgboost.fit(xtrain_tfv_std,y_train)\nxg_accuracy = xgboost.score(xvalid_tfv_std,y_valid)\nprint(\"Accuracy:\",xg_accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sequences = tokenizer.texts_to_sequences(test['Phrase'])\ntest_pad = pad_sequences(test_sequences,maxlen=max_length,truncating=trunc_type,padding=pad_type)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CNN+LSTM\nypred = model.predict_classes(test_pad,verbose=1)\nsub['Sentiment'] = ypred\nsub.to_csv(\"submission.csv\", index=False, header=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}