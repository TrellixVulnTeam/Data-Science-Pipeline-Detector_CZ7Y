{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **SENTIMENT CLASSIFIER**\n\nSentiment Classification is a very important part of Natural Language Processing. This notebook uses a CNN + Bi-LSTM model on a dataset with phrases extracted from the rotten tomatoes dataset and classifies them into 5 different sentiments.","metadata":{}},{"cell_type":"markdown","source":"Importing the essential libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nfrom sklearn.model_selection import train_test_split\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom itertools import count\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense,Embedding,Bidirectional,Dropout,SpatialDropout1D,GlobalMaxPool1D,LSTM,BatchNormalization,Conv1D,MaxPool1D\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras import regularizers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Importing the dataset. The dataset can be downloaded at https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only/data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/train.tsv.zip\",sep=\"\\t\")\ntest = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/test.tsv.zip\",sep=\"\\t\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(train['Sentiment'])\nplt.title(\"No of Tweet Sentiments\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The **create_vocabulary(df)** function defined below creates an indexed vocabulary from the lemmatized tokens of words present in the dataframe passed to it.","metadata":{}},{"cell_type":"code","source":"def create_vocabulary(df):\n    counter = count(2)  # index 0 reserved for padding, index 1 for UNK token\n    vocabulary = dict()\n    lemmatizer = WordNetLemmatizer()\n    for k in df['Phrase']:\n        tokens = k.lower().split(\" \")\n        for token in tokens:\n            lemmatoken = lemmatizer.lemmatize(token)\n            if lemmatoken in vocabulary:\n                continue\n            vocabulary[lemmatoken] = next(counter)\n    print(\"Vocabulary length: {}\".format(max(vocabulary.values())))  \n    return vocabulary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The **function preprocess_df(df, vocabulary, max_sentence_length)** defined below is used to pre process the dataframe before sending into the model to be defined later on. The function converts the sentiment labels into inteher values. Also it converts the phrases into lemmatized tokens of their words represented by their respective indices in the vocabulary created earlier.","metadata":{}},{"cell_type":"code","source":"def preprocess_df(df, vocabulary, max_sentence_length):\n    vocabulary_length = max(vocabulary.values())\n    X = []\n    # Use the same function for test sets.\n    Y = label_binarize(df.Sentiment.to_xarray(), classes=[0, 1, 2, 3, 4]) if 'Sentiment' in df else None\n    lemmatizer = WordNetLemmatizer()\n    for sample in df.iterrows():\n        tokens = sample[1]['Phrase'].lower().split(\" \")\n        vocab_tokens = []\n        for i in range(max_sentence_length):\n            try:\n                vocab_tokens.append(vocabulary.get(lemmatizer.lemmatize(tokens[i]), 1))  # 1 : UNK token\n            except IndexError:\n                vocab_tokens.append(0)  # 0 : padding token\n        X.append(vocab_tokens)\n    return np.asarray(X), Y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calling the functions defined above","metadata":{}},{"cell_type":"code","source":"vocabulary = create_vocabulary(train)\nX, Y = preprocess_df(train, vocabulary, 52)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train-Test Split","metadata":{}},{"cell_type":"code","source":"train_X,x_valid,train_Y,y_valid = train_test_split(X,Y,test_size=0.2,random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Defining the **model**. It uses 1D CNN layers followed by few bi-LSTM layers and ending with dense layers.","metadata":{}},{"cell_type":"code","source":"model = keras.models.Sequential()\nmodel.add(keras.layers.Embedding(input_dim=15189, output_dim=10, mask_zero=True))\n\nmodel.add(Conv1D(128,3,activation='relu',padding='same'))\nmodel.add(MaxPool1D(2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv1D(128,3,activation='relu',padding='same'))\nmodel.add(MaxPool1D(2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv1D(128,3,activation='relu',padding='same'))\nmodel.add(MaxPool1D(2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.15))\n\nmodel.add(Conv1D(64,3,activation='relu',padding='same'))\nmodel.add(MaxPool1D(2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.15))\n\nmodel.add(Conv1D(64,3,activation='relu',padding='same'))\nmodel.add(MaxPool1D(2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.15))\n\nmodel.add(Bidirectional(LSTM(1280,recurrent_dropout=0.5,dropout=0.2,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(640,recurrent_dropout=0.5,dropout=0.2,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(640,recurrent_dropout=0.5,dropout=0.2,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(320,recurrent_dropout=0.5,dropout=0.2,return_sequences=True)))\n\nmodel.add(GlobalMaxPool1D())\n\nmodel.add(Dense(64,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.15))\n\nmodel.add(Dense(32,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.15))\n\nmodel.add(Dense(32,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.15))\n\nmodel.add(Dense(5,activation='softmax'))\n\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fitting the model","metadata":{}},{"cell_type":"code","source":"history=model.fit(x=train_X, y=train_Y, batch_size=256, epochs=15,validation_data=(x_valid,y_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting the loss and accuracy with respect to epochs","metadata":{}},{"cell_type":"code","source":"def plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history[\"val_\"+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string,\"val_\"+string])\n    plt.show()\nplot_graphs(history,'acc')\nplot_graphs(history,'loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preprocessing the test data","metadata":{}},{"cell_type":"code","source":"test_X,test_Y = preprocess_df(test, vocabulary, 52)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predicting the model on the test dataset and converting results into the desired format","metadata":{}},{"cell_type":"code","source":"predictions = model.predict(x=np.asarray(test_X))\n\nprediction_results = pd.concat([test,\n                                pd.DataFrame([np.argmax(k) for k in predictions], columns=['Sentiment'])],\n                               axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Submitting results","metadata":{}},{"cell_type":"code","source":"submission = prediction_results[['PhraseId', 'Sentiment']]\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}