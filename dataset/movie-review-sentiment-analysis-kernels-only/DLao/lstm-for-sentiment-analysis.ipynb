{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport keras\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, CuDNNLSTM, CuDNNGRU, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, SpatialDropout1D, concatenate\nfrom keras.models import Model, Sequential, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers.pooling import GlobalMaxPooling1D, GlobalAveragePooling1D\nimport nltk\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom textblob import TextBlob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73c37ba858900dd5939f752158abb789be339c74","collapsed":true},"cell_type":"code","source":"train_f = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/train.tsv\",sep=\"\\t\")\ntrain_f.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22c79d86bfcd094c8e83bfe455310e571fa0481a","collapsed":true},"cell_type":"code","source":"train_f.groupby('Sentiment').agg({'PhraseId': 'count'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a19e43e31954622d24ebd8fc42aa58a51bb943f4","collapsed":true},"cell_type":"code","source":"test_f = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/test.tsv\",sep=\"\\t\")\ntest_f.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4092abf97d3d053820225765cc0345f2ca40c41","collapsed":true},"cell_type":"code","source":"corpus_sentences = list(map(str,train_f[\"Phrase\"])) + list(map(str,test_f[\"Phrase\"]))\ncorpus_sentences[66292]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4925e6b926545f45021e18435373a65b73fcd631","collapsed":true},"cell_type":"code","source":"len(corpus_sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a87bd67a2afa4967206c7acd6e4de2de852b2ec","collapsed":true},"cell_type":"code","source":"#stemmer = PorterStemmer()\n#stemmed_words = [stemmer.stem(word.lower()) for word in corpus_sentences]\n#stops = set(stopwords.words(\"english\"))\n\ndef clean_text(c):\n    lemmatizer = WordNetLemmatizer()\n    lemmed_words = c.copy()\n    i = 0\n    for sentences in c:\n        temp = [lemmatizer.lemmatize(j) for j in sentences.lower().split()]\n        lemmed_words[i] = \" \".join(temp)\n        i+=1\n    text = lemmed_words.copy()\n    #text = [re.sub(r'[^\\w\\s]','',s) for s in lemmed_words]\n    return(text)\n\ntext = clean_text(corpus_sentences)\nlen(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d56a5d663e4dceb6f322b2636ccccc09f32eb40","collapsed":true},"cell_type":"code","source":"for i in range(2000):\n    if corpus_sentences[i].lower() != text[i] and len(corpus_sentences[i]) > 200:\n        print(i)\ni=563\n\nprint(corpus_sentences[i])\n#print(lemmed_words[i])\nprint(text[i])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3a250a0cf977b8b0563eecacd69b4e80504a9d0","collapsed":true},"cell_type":"code","source":"#X_train, X_val, y_train, y_val = train_test_split(train_f['Phrase'],train_f['Sentiment'],test_size=0.1)\n\n#print(len(X_train))\n#print(len(X_val))\n\n#print(len(y_train))\n#print(len(y_val))\n\nX_train = train_f['Phrase']\ny_train = train_f['Sentiment']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb92b173d435a781b07131adcfd00a0f69a904aa","collapsed":true},"cell_type":"code","source":"Xy_train = pd.concat([X_train, y_train], axis=1)\nXy_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71b8e3573406a37859af2162b1fcc34e5f93499b","collapsed":true},"cell_type":"code","source":"#Xy_val = pd.concat([X_val, y_val], axis=1)\n#Xy_val.groupby('Sentiment').agg({'Phrase': 'count'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a81346de0b54cb8a1324a4d9238e21cf4496ca20","collapsed":true},"cell_type":"code","source":"Xy_train.groupby('Sentiment').agg({'Phrase': 'count'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb0b75c10bdceabad3726f5fa526d9946420a64e","collapsed":true},"cell_type":"code","source":"Xy_train = Xy_train.reset_index(drop=True)\n\n\ni_class0 = np.where(Xy_train['Sentiment'] == 0)[0]\ni_class1 = np.where(Xy_train['Sentiment'] == 1)[0]\ni_class2 = np.where(Xy_train['Sentiment'] == 2)[0]\ni_class3 = np.where(Xy_train['Sentiment'] == 3)[0]\ni_class4 = np.where(Xy_train['Sentiment'] == 4)[0]\n\nm = max(len(i_class0), len(i_class1), len(i_class2), len(i_class3), len(i_class4))\n\n\nprint(m)\nprint(len(i_class0))\nprint(len(i_class1))\nprint(len(i_class2))\nprint(len(i_class3))\nprint(len(i_class4))\n\ni_class0_upsampled = np.random.choice(i_class0, size=m, replace=True)\ni_class1_upsampled = np.random.choice(i_class1, size=m, replace=True)\ni_class2_upsampled = i_class2 #max\ni_class3_upsampled = np.random.choice(i_class3, size=m, replace=True)\ni_class4_upsampled = np.random.choice(i_class4, size=m, replace=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4684e76b9067daeacfda6c0618cd89768f3a588","collapsed":true},"cell_type":"code","source":"t0 = Xy_train.loc[i_class0_upsampled, ]\nt1 = Xy_train.loc[i_class1_upsampled, ]\nt2 = Xy_train.loc[i_class2_upsampled, ]\nt3 = Xy_train.loc[i_class3_upsampled, ]\nt4 = Xy_train.loc[i_class4_upsampled, ]\n\ntrain_fu = t0.append(t1).append(t2).append(t3).append(t4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14d58b924700b9ffc59c51e78929aff551b9b1d2","collapsed":true},"cell_type":"code","source":"train_fu.groupby('Sentiment').agg({'Phrase': 'count'})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7321b91b3efc944faaa011f880924d8d6f706c08","collapsed":true},"cell_type":"code","source":"train_fu['Phrase'] = clean_text(list(map(str,train_fu[\"Phrase\"])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0feb3acb1b6d553e48be50dce3a7e01e40db8e30","collapsed":true},"cell_type":"code","source":"Xy_train['Phrase'] = clean_text(list(map(str,Xy_train[\"Phrase\"])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7764dda39b6b39a3c8ed49c04e6c9685ff334b68","collapsed":true},"cell_type":"code","source":"#Xy_val['Phrase'] = clean_text(list(map(str,Xy_val[\"Phrase\"])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c7a56b9bb4e84c670380d3815a5b1ddb1db42a2","collapsed":true},"cell_type":"code","source":"test_f['Phrase'] = clean_text(list(map(str,test_f[\"Phrase\"])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7280b9d6cc9ead2169da03697024f3968b6b01e","collapsed":true},"cell_type":"code","source":"max_words = 15000\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(list(text))\n\n#list_tokenized_train = tokenizer.texts_to_sequences(train_fu[\"Phrase\"])\nlist_tokenized_train = tokenizer.texts_to_sequences(Xy_train[\"Phrase\"])\n#list_tokenized_val = tokenizer.texts_to_sequences(Xy_val[\"Phrase\"])\nlist_tokenized_test = tokenizer.texts_to_sequences(test_f[\"Phrase\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"512927472284e4ff6ccd6593616a1e70c7183863","collapsed":true},"cell_type":"code","source":"len(list_tokenized_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32c94b1f1d94f3a7eb208939409745658f7579ea","collapsed":true},"cell_type":"code","source":"#len(list_tokenized_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ae262bb30a8ecb5502320c082bc94ea77ecbc3a","collapsed":true},"cell_type":"code","source":"len(list_tokenized_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fadea53bf1f042edc87db7e29d585809d19a1e47","scrolled":true,"collapsed":true},"cell_type":"code","source":"num_words = [len(i) for i in text]\nplt.hist(num_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c547de65ba9e61e18628bd6422fe90dc54233ca9","collapsed":true},"cell_type":"code","source":"np.mean(num_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cce5c30ff81150c89bb4255f936ea4d70483c96","collapsed":true},"cell_type":"code","source":"max_len = 80\nX_train_final = pad_sequences(list_tokenized_train,maxlen=max_len)\n#X_val_final = pad_sequences(list_tokenized_val,maxlen=max_len)\nX_test_final = pad_sequences(list_tokenized_test,maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2e6ec1484052fab9497a4a132359b791596fa63","collapsed":true},"cell_type":"code","source":"X_train_final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d2be46f131a54857751afa275e0d43b0311d74c","collapsed":true},"cell_type":"code","source":"#train_dummies = pd.get_dummies(train_fu['Sentiment'])\ntrain_dummies = pd.get_dummies(Xy_train['Sentiment'])\ny_train_final = train_dummies.values\ny_train_final[:10,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8988f75f607d9dfe71e8a7bf90fea3502d0d3ff","collapsed":true},"cell_type":"code","source":"#train_dummies = pd.get_dummies(Xy_val['Sentiment'])\n#y_val_final = train_dummies.values\n#y_val_final[:10,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00ed9609d58c1e17f961b65bbd1e085423fb38ee","collapsed":true},"cell_type":"code","source":"np.random.seed(226)\nshuffle_indices = np.random.permutation(np.arange(len(X_train_final)))\nX_trains = X_train_final[shuffle_indices]\ny_trains = y_train_final[shuffle_indices]\n\nprint(X_train_final[1])\nprint(X_trains[1])\n\nprint(y_train_final[1])\nprint(y_trains[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6a7f27fd9427189d39e575cee19733c5e4bc4a8","collapsed":true},"cell_type":"code","source":"phs = Xy_train['Phrase'][shuffle_indices]\nphs[0:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b8484b331ed512214efaf59e1ff7801373f8b1d","collapsed":true},"cell_type":"code","source":"td = 100\n\nvec = TfidfVectorizer(max_features=td, ngram_range=(1,2))\nx_tfidf = vec.fit_transform(phs).toarray()\nnp.count_nonzero(x_tfidf)/len(phs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cd18e87b396a0b063c7ea993657eab0bb9628c3","collapsed":true},"cell_type":"code","source":"test_tfidf = vec.transform(test_f['Phrase']).toarray()\ntest_tfidf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"832f8e1641e30b941a13a0fec5138cc21c4552f3","collapsed":true},"cell_type":"code","source":"def cal_score(c):\n    sid = SentimentIntensityAnalyzer()\n    i = 0\n    a = np.zeros(shape=(len(c),5))\n    for sentences in c:\n        temp1 = sum([sid.polarity_scores(j)['compound'] for j in sentences.split()])\n        temp2 = sum([sid.polarity_scores(j)['compound'] > 0.5 for j in sentences.split()])\n        temp3 = sum([sid.polarity_scores(j)['compound'] < -0.5 for j in sentences.split()])\n        temp4 = sid.polarity_scores(sentences)['compound']\n        temp5 = TextBlob(sentences).sentiment.polarity\n        a[i][0] = temp1\n        a[i][1] = temp2 / (1+len(sentences.split()))\n        a[i][2] = temp3 / (1+len(sentences.split()))\n        a[i][3] = temp4\n        a[i][4] = temp5\n        #a[i][5] = temp4 / (1+len(sentences.split()))\n        #a[i][6] = temp5 / (1+len(sentences.split()))\n        i+=1\n    return(a)\n\nphs2 = cal_score(phs)\nphs2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9acdda2e8e4132ab3a7326bc901a2e0e8fd46a07","collapsed":true},"cell_type":"code","source":"test_phs2 = cal_score(test_f['Phrase'])\ntest_phs2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39198275bb8448fbf270eef833a74eb61501a07e","collapsed":true},"cell_type":"code","source":"#X_train_t, X_train_dev, y_train_t, y_train_dev = train_test_split(np.array(X_trains),np.array(y_trains),test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffac1ce8f0be5736661ada30cea7add2447fa122","collapsed":true},"cell_type":"code","source":"#print(len(X_train_dev))\n\n#remove_i = []\n\n#for i in range(len(X_train_dev)):\n#    if (X_train_dev[i] in X_train_t):\n#        remove_i.append(i)\n        \n#print(len(remove_i))\n#print(remove_i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"600f4748a8c30d33eabb48b64b49a7737a13b3f6"},"cell_type":"code","source":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n\ndef create_eb(path, s):\n    embedding_path = path\n    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n    embed_size = s\n\n    word_index = tokenizer.word_index\n    nb_words = min(max_words, len(word_index))\n    embedding_matrix = np.zeros((nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_words: continue\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    return embedding_matrix\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15cbae504b8b5c69e7f96cc22b13c6a0dd48eb59","collapsed":true},"cell_type":"code","source":"fast_text_eb = create_eb(\"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\", 300)\nfast_text_eb.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4499d46014de437b07c738fe0c7b9322c2b9ddb","collapsed":true},"cell_type":"code","source":"glove_eb = create_eb(\"../input/glove840b300dtxt/glove.840B.300d.txt\", 300)\nglove_eb.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea8c6f093389c0d547252a124914de8c9fa20c10","collapsed":true},"cell_type":"code","source":"def keras_dl(model, eb, embed_size, batch_size, epochs):   \n    inp = Input(shape = (max_len,), name = 'lstm')\n    #x = Embedding(max_words,embed_size,input_length=max_len)(inp)\n    x = Embedding(max_words,embed_size,weights = [eb], trainable = False)(inp)\n    #model.add(Embedding(max_words,embed_size,weights = [embedding_matrix], trainable = False))\n    x1 = SpatialDropout1D(0.5)(x)\n    \n    x_lstm = CuDNNLSTM(128, return_sequences = True)(x1)\n    x_lstm_c1d = Conv1D(64,kernel_size=3,padding='valid',activation='relu')(x_lstm)\n    x_lstm_c1d_gp = GlobalMaxPooling1D()(x_lstm_c1d)\n    #x_lstm_c1d_gp = Flatten()(x_lstm_c1d)\n    \n    #x_c1d = Conv1D(128,kernel_size=3,padding='same',activation='tanh')(x1)\n    #x_c1d = MaxPooling1D()(x_c1d)\n    #x_c1d_lstm = CuDNNLSTM(64)(x_c1d)\n    #x_c1d_gru = CuDNNGRU(64)(x_c1d)\n    \n    x_gru = CuDNNGRU(128, return_sequences = True)(x1)\n    x_gru_c1d = Conv1D(64,kernel_size=2,padding='valid',activation='relu')(x_gru)\n    x_gru_c1d_gp = GlobalMaxPooling1D()(x_gru_c1d)\n    #x_gru_c1d_gp = Flatten()(x_gru_c1d)\n    \n    inp2 = Input(shape = (td,), name = 'tfidf')\n    x2 = BatchNormalization()(inp2)\n    x2 = Dense(16, activation='relu')(x2)\n    \n    inp3 = Input(shape = (5,), name = 'score')\n    x3 = BatchNormalization()(inp3)\n    x3 = Dense(3, activation='tanh')(x3)\n    \n    x_f = concatenate([x_lstm_c1d_gp, x_gru_c1d_gp])#, x_c1d_lstm, x_c1d_gru])\n    x_f = BatchNormalization()(x_f)\n    x_f = Dropout(0.5)(Dense(128, activation='tanh') (x_f))    \n    x_f = BatchNormalization()(x_f)\n    x_f = concatenate([x_f, x2, x3])\n    x_f = Dropout(0.5)(Dense(32, activation='tanh') (x_f))\n    x_f = BatchNormalization()(x_f)\n    x_f = Dropout(0.5)(Dense(16, activation='tanh') (x_f))\n    #x_f = Dense(5, activation = \"sigmoid\")(x_f)\n    x_f = Dense(5, activation = \"softmax\")(x_f)\n    model = Model(inputs = [inp, inp2, inp3], outputs = x_f)\n    \n    #model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    #model.compile(loss='hinge',optimizer='adadelta',metrics=['accuracy'])\n    print(model.summary())\n    return (model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df2f4d94ba9765fb123a05373ac1d58a1125d911","scrolled":true,"collapsed":true},"cell_type":"code","source":"embed_size = 300\nbatch_size = 256\nepochs = 30\nmodel = Sequential()\n\nfile_path1 = \"best_model1.hdf5\"\ncheck_point = ModelCheckpoint(file_path1, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 4)\n\nfirstmodel1 = keras_dl(model, fast_text_eb, embed_size, batch_size, epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ce5dd6d2f4647c0dcebc26dd5c58dccd14daa46","collapsed":true},"cell_type":"code","source":"text_model1 = firstmodel1.fit({'lstm': X_trains, 'tfidf': x_tfidf, 'score': phs2}, y_trains, batch_size=batch_size,epochs=epochs,verbose=0,\n                            validation_split = 0.1, #validation_data=(X_val_final,y_val_final), \n                            callbacks = [check_point, early_stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"71789246cf97df72230afe535731077c4378cd36"},"cell_type":"code","source":"firstmodel1 = load_model(file_path1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a214198e40854eb441176ff68a2609fafe0246bc"},"cell_type":"code","source":"embed_size = 300\nbatch_size = 256\nepochs = 30\nmodel = Sequential()\n\nfile_path2 = \"best_model2.hdf5\"\ncheck_point = ModelCheckpoint(file_path2, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 4)\n\nfirstmodel2 = keras_dl(model, glove_eb, embed_size, batch_size, epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4e772be44c7ba26b4b51269f77cca6ed020dab7"},"cell_type":"code","source":"text_model2 = firstmodel2.fit({'lstm': X_trains, 'tfidf': x_tfidf, 'score': phs2}, y_trains, batch_size=batch_size,epochs=epochs,verbose=0,\n                            validation_split = 0.1, #validation_data=(X_val_final,y_val_final), \n                            callbacks = [check_point, early_stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b2fd7986955f02f7ecfb96e7d107512adc8d2e6e"},"cell_type":"code","source":"firstmodel2 = load_model(file_path2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd3012e2ffad3461ecc7bdf6c31764f42d2a2619"},"cell_type":"code","source":"pred = firstmodel1.predict([np.array(X_test_final), test_tfidf, test_phs2], verbose = 1)\npred_new = firstmodel2.predict([np.array(X_test_final), test_tfidf, test_phs2], verbose = 1)\n\nprint(pred.shape)\nprint(pred_new.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a100185a76839b545c99067fd25936fbd676d2f2"},"cell_type":"code","source":"pred_combi = (np.array(pred) + np.array(pred_new)) / 2\npred2 = np.round(np.argmax(pred_combi, axis=1)).astype(int)\n\nprint(pred_combi.shape)\nprint(pred2.shape)\n\npred2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96935d70d7a86e8c135638ad5def4625527d4786"},"cell_type":"code","source":"# Pseudo-labeling\nPL_X = np.vstack((X_trains, X_test_final))\nPL_tfidf = np.vstack((x_tfidf, test_tfidf))\nPL_s = np.vstack((phs2, test_phs2))\n\ny_test = pd.get_dummies(pred2)\nPL_y = np.vstack((y_trains, y_test))\n\nprint(PL_X.shape)\nprint(PL_tfidf.shape)\nprint(PL_s.shape)\nprint(PL_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"38673581f207c42742f29d84f4f15aae8b18d8f5"},"cell_type":"code","source":"model = Sequential()\nfile_path1 = \"best_model1.hdf5\"\ncheck_point = ModelCheckpoint(file_path1, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 4)\n\nfinalmodel1 = keras_dl(model, fast_text_eb, embed_size, batch_size, epochs)\n\nfinal_text_model1 = finalmodel1.fit({'lstm': PL_X, 'tfidf': PL_tfidf, 'score': PL_s}, PL_y, batch_size=batch_size,epochs=epochs,verbose=0,\n                                validation_split = 0.1, #validation_data=(X_val_final,y_val_final), \n                                callbacks = [check_point, early_stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"540fac19620ec599c5b00732c230a4266cf16a48"},"cell_type":"code","source":"finalmodel1 = load_model(file_path1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"54f49fcb6fbdd19f9a72eb28299b7cbf45e3f35a"},"cell_type":"code","source":"model = Sequential()\n\nfile_path2 = \"best_model2.hdf5\"\ncheck_point = ModelCheckpoint(file_path2, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 4)\n\nfinalmodel2 = keras_dl(model, glove_eb, embed_size, batch_size, epochs)\n\nfinal_text_model2 = finalmodel2.fit({'lstm': PL_X, 'tfidf': PL_tfidf, 'score': PL_s}, PL_y, batch_size=batch_size,epochs=epochs,verbose=0,\n                                validation_split = 0.1, #validation_data=(X_val_final,y_val_final), \n                                callbacks = [check_point, early_stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"03a779e2686cebf7968c5365b885f9d74bf55db5"},"cell_type":"code","source":"finalmodel2 = load_model(file_path2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"02ce9ed417797cb372b670d33f4ccdec605a3096"},"cell_type":"code","source":"final_pred = finalmodel1.predict([np.array(X_test_final), test_tfidf, test_phs2], verbose = 1)\nfinal_pred_new = finalmodel2.predict([np.array(X_test_final), test_tfidf, test_phs2], verbose = 1)\n\nfinal_pred_combi = (final_pred + final_pred_new) / 2\nfinal_pred2 = np.round(np.argmax(final_pred_combi, axis=1)).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"917d3742e5efb26149fc38756fc50ba5446382fb","collapsed":true},"cell_type":"code","source":"sub = pd.DataFrame({'PhraseId': test_f['PhraseId'],\n                   'Sentiment': final_pred2})\n\nsub.to_csv(\"DL.csv\", index = False, header = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"489ca4d2dc7a6a23181b6a10e063babe6118ea43","collapsed":true},"cell_type":"code","source":"sub.groupby('Sentiment').agg({'PhraseId': 'count'})","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}