{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing import sequence\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense,LSTM,Dropout,Embedding\nfrom keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to print very long sentences in pandas df\npd.set_option('display.max_colwidth', -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!unzip  ../input/movie-review-sentiment-analysis-kernels-only/test.tsv.zip\n!unzip  ../input/movie-review-sentiment-analysis-kernels-only/train.tsv.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/working","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/working/train.tsv',sep = '\\t')\ntest = pd.read_csv('/kaggle/working/test.tsv',sep = '\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submsission =  pd.read_csv('/kaggle/input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total number of original reviews are:', len(pd.unique(train['SentenceId'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's check some random reviews\nindices = np.random.randint(0,train.shape[0],5)\nfor index in indices:\n    print(train['Phrase'][index])\n    print(train['Sentiment'][index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#These reviews are break down in small phrases ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's check some review by id\ntrain[train['SentenceId']==10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['SentenceId']==187]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Average phrases per sentence in Train Data are: {:.0f}' .format(train.groupby(['SentenceId'])['Phrase'].count().mean()))\nprint('Average phrases per sentence in Test Data are: {:.0f}' .format(test.groupby(['SentenceId'])['Phrase'].count().mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total phrases in Train Data is {} and total sentences are {}' .format(train.shape[0],len(pd.unique(train['SentenceId']))))\nprint('Total phrases in Test Data is {} and total sentences are {}' .format(test.shape[0],len(pd.unique(test['SentenceId']))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Average words in phrases in Train Data is {:f}' .format(train['Phrase'].apply(lambda x: len(x.split())).mean()))\nprint('Average words in phrases in Test Data is {:f}' .format(test['Phrase'].apply(lambda x: len(x.split())).mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Maximum words in phrases in Train Data is {:f}' .format(train['Phrase'].apply(lambda x: len(x.split())).max()))\nprint('Maximum words in phrases in Test Data is {:f}' .format(test['Phrase'].apply(lambda x: len(x.split())).max()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Minimum words in phrases in Train Data is {:f}' .format(train['Phrase'].apply(lambda x: len(x.split())).min()))\nprint('Minimum words in phrases in Test Data is {:f}' .format(test['Phrase'].apply(lambda x: len(x.split())).min()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is some empty data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing empty data from train data\nto_remove = []\nfor i,row in train.iterrows():\n    if(len(row['Phrase'].split())== 0):\n        to_remove.append(i)\nprint(len(to_remove))\ntrain.drop(to_remove,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking again minimun length of phrase\nprint('Minimum words in phrases in Train Data is {:f}' .format(train['Phrase'].apply(lambda x: len(x.split())).min()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's plot number of reviews\nplt.figure(figsize = (12,10))\ntrain['Sentiment'].value_counts().sort_index().plot(kind = 'bar')\nplt.xlabel('Review')\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"As expected most of the reviews are neutral as most of the sentences are small phrases."},{"metadata":{},"cell_type":"markdown","source":"## EDA on words and Preparing dataset for sequence models."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_text = list(test['Phrase'].values) + list(train['Phrase'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.fit_on_texts(full_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's print some word counts\ni = 0\nfor k,v in dict(tokenizer.word_counts).items():\n    print(k,v)\n    if i== 10:\n        break\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#total phrases\nprint(tokenizer.document_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#word index for one hot encoding\n#let's print some\ni = 0\nfor k,v in tokenizer.word_index.items():\n    print(k,v)\n    if i== 10:\n        break\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total number of unique words in all of data : {}'.format( max(tokenizer.word_index.values())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Most_used_words = dict(tokenizer.word_counts)\nprint('Most used words are:')\nsorted(Most_used_words.items() ,key =lambda x:x[1], reverse = True)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dividing train data in training and validation data\nX_train, X_valid, y_train, y_valid = train_test_split(train['Phrase'],train['Sentiment'],test_size = .1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape,y_train.shape)\nprint(X_valid.shape,y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = tokenizer.texts_to_sequences(X_train)\nX_valid = tokenizer.texts_to_sequences(X_valid)\nX_test = tokenizer.texts_to_sequences(test['Phrase'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#example\nprint(X_train[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to the input to rnn/lstm we need data of constant length. we can cut down some big sentences and pad down small sentences."},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's use maximum sequence length of 40\nmax_len = 40\n#using default pre padding. if phrase length is more than 40, it is truncated from starting.\nX_train = sequence.pad_sequences(X_train, maxlen=max_len)\nX_valid = sequence.pad_sequences(X_valid, maxlen=max_len)\nX_test = sequence.pad_sequences(X_test, maxlen=max_len)\nprint(X_train.shape,X_valid.shape,X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = to_categorical(y_train)\ny_valid = to_categorical(y_valid)\nprint(y_train.shape,y_valid.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building and Training Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = 17780 #using all unique words\nembedding_dim = 150\nnum_classes = 5\nbatch_size = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#callbacks\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\nreduce_lr =  ReduceLROnPlateau(monitor='val_loss',verbose=1, factor=.1,patience=5)\ncheckpointer = ModelCheckpoint('model.hdf5', monitor='val_loss', verbose=1, save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_features + 1, embedding_dim, input_length= max_len, mask_zero = True))        #input dim is max_features + 1 because 0 index is used in padding.\nmodel.add(LSTM(100,dropout=0.6, recurrent_dropout=0.5,return_sequences=True))                         #returning full sequence for next layer, also using recurrent output\nmodel.add(LSTM(64,dropout=0.6, recurrent_dropout=0.5,return_sequences=False))                         #returning only last output.  \nmodel.add(Dense(num_classes,activation='softmax'))                                                    #final output\n\nmodel.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.01),metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train, validation_data=(X_valid, y_valid),epochs=50, batch_size=batch_size, verbose=1,callbacks = [es,reduce_lr,checkpointer])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's plot losses\n\nhistory = model.history.history\n# list all data in history\n#print(history.keys())\n# summarize history for accuracy\nplt.plot(history['loss'])\nplt.plot(history['val_loss'])\n\nticks = list(range(len(history['loss'])+1)) # we need integers in x axis (epochs)\nplt.xticks(ticks)\n\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading the best weights\nmodel.load_weights('model.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = predictions.argmax(axis = 1) \ny_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submsission.Sentiment = y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submsission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}