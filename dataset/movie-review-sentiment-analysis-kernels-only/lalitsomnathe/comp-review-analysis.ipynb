{"cells":[{"metadata":{"_uuid":"ac297bacd3b04fee0f57094399891b1e06c790f4"},"cell_type":"markdown","source":"**I am using Glove vector representation in this notebook.**\n>If you find some things or approches wrong , your comments are welcomed."},{"metadata":{"_uuid":"4b9014a4cd6ab72ef83e5d61480c923709285e99"},"cell_type":"markdown","source":"> LOADING input files in pandas"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input/movie-review-sentiment-analysis-kernels-only\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/train.tsv\",delimiter='\\t')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16382d5e500881112e60386c82754bb59e5903dd"},"cell_type":"code","source":"test=pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/test.tsv\",delimiter='\\t')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98088d17f92a6855da0c733c45ee29c9b397c0b2"},"cell_type":"markdown","source":">Basic info about our dataset"},{"metadata":{"trusted":true,"_uuid":"5bba858e1c9d797c7f09672d037d7daeb3da57bb"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3930563124c4d46db40d6528cdcc5d3a31e41fa4"},"cell_type":"markdown","source":"Exploration of dataset :-\nThere are few unique Sentences and these sentences are divided to make multiple phrases."},{"metadata":{"trusted":true,"_uuid":"b7e3db8403c358661207c1e02fdb383a993a4911"},"cell_type":"code","source":"#Unique setneces in our dataset\ndf['SentenceId'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99fdd93826ce72fac6be23de4ef180a586bf286c"},"cell_type":"code","source":"tmp=df.head(50)\ntmp","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a70bb9929a82274e93181b9bb253991609dcfd1"},"cell_type":"markdown","source":"We can see above PhraseId=38 have only single character that too special one. Also PhraseId49 ,43 etc have only single words in corressponding Phrase.\n\nBeing a phrase, special characters like exclamation can sometimes make the phrase to a sarcastic statement, hence I wont remove special characters from our dataset."},{"metadata":{"trusted":true,"_uuid":"0d7514f97ed1b40725e0758732e8b66eb6c105c2","scrolled":true},"cell_type":"code","source":"#phrases having only single character\nimport re\ndef find_onechar(x):\n#     if x.startswith('A'):\n    if re.search(r'^\\W$',x):\n        return(x)\nprint(tmp['Phrase'].apply(find_onechar).count())\ndf[df['Phrase']==df['Phrase'].apply(find_onechar)] ## need to delete as no information","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc8ba5856ff2e18237623c58f8b7000e9f73d9ef"},"cell_type":"code","source":"test[test['Phrase']==test['Phrase'].apply(find_onechar)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8b86e817865a3d8719c3a0032a4636f197f8733"},"cell_type":"code","source":"#Phrases with only one word\ndef find_oneword(x):\n    if re.search(r'^\\w+$',x):\n        return(x)\n    \nprint('total distinct 1 words in tmp : {} '.format(tmp['Phrase'].apply(find_oneword).nunique()))\nprint('total distinct 1 words in df : {} '.format(df['Phrase'].apply(find_oneword).nunique()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d270ac1c09961c94ff3f0bc41f27c329bb52258c"},"cell_type":"code","source":"#No of words which are lonely present in Phrase\n# type(df['Phrase'].apply(find_oneword).value_counts().sort_values(ascending=False).to_frame())\ndf['Phrase'].apply(find_oneword).value_counts(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2059331ce8c4a29ac4fb53908a25bbb6b16537a"},"cell_type":"markdown","source":">Preparing inputs for model (LSTM)\n\nNeed to collect sentence highest no. of words(from train and test). \nWe will then give this number(of words) as arguments to LSTM so that LSTM will remember \ntill that many words before prediction."},{"metadata":{"trusted":true,"_uuid":"d4172075e8bd2672880e64ec037295edecd2e9d0"},"cell_type":"code","source":"tmp_idx=tmp.index[tmp['Phrase']==max(tmp['Phrase'], key=len)]\ntmp_idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a23df3a090d7b7122802e48c7d2f16565db2a22"},"cell_type":"code","source":"tmp.loc[tmp_idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83ec41e9e57ac9cef7b93c3ba8ff5134b956bece"},"cell_type":"code","source":"#Longest sentence in train df\nidx=df.index[df['Phrase']==max(df['Phrase'], key=len)]\nprint ( 'row index with maximum length of phrase :' ,idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bfaad8d0b91ea4803dec446dbef544bbe9dbb77"},"cell_type":"code","source":"# df.loc[idx]['Phrase']\nfrom IPython.display import display\nprint(max(df['Phrase'], key=len))\nval=df.loc[idx].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a574d165e9fe262a5e70a6a41f58aa59f21b38c"},"cell_type":"code","source":"val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f4bd3dcb352aead1b097cce1c38618355d79a36"},"cell_type":"code","source":"maxLen = len(max(df['Phrase'], key=len).split())\nprint('maximum length of Phrase : ', maxLen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5be9ee9826d93d7387bf4aac7f778b7af74ae907"},"cell_type":"code","source":"maxLen1 = len(max(test['Phrase'], key=len).split())\n\nidx1=test.index[test['Phrase']==max(test['Phrase'], key=len)]\nprint('maximum length of Phrase : ', maxLen1)\nprint ('====================================')\n\nprint(max(test['Phrase'], key=len))\nprint ('====================================')\nprint ('index of max length in test :', idx1)\nprint ( test.loc[idx1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62be17897336e40756f657293c2bbb19702ef833"},"cell_type":"markdown","source":">Creating a funtion to read Glove txt file."},{"metadata":{"trusted":true,"_uuid":"f6d0b8a37637ae303cdc60e2fbb23044b8117959"},"cell_type":"code","source":"# https://github.com/cryer/Emojify/blob/master/emo_utils.py\ndef read_glove_vecs(glove_file):\n    with open(glove_file, 'r',encoding='UTF-8') as f:\n        words = set()\n        word_to_vec_map = {}\n        for line in f:\n            line = line.strip().split()\n            curr_word = line[0]\n            words.add(curr_word)\n            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n        \n        i = 1\n        words_to_index = {}\n        index_to_words = {}\n        for w in sorted(words):\n            words_to_index[w] = i\n            index_to_words[i] = w\n            i = i + 1\n    return words_to_index, index_to_words, word_to_vec_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44f5f2cb5413048ae59f9abbe147ef236ac952e9"},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input/glove-global-vectors-for-word-representation\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fb39bc05d129d0938fd11c2afd019b3d89aaa84"},"cell_type":"code","source":"word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('../input/glove-global-vectors-for-word-representation/glove.6B.50d.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0dd6eb6fdf893239ac2c68b2426511b5a669f1a2"},"cell_type":"code","source":"len(word_to_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4898b675d87ea9a249c302a6bdd50443ed95005d"},"cell_type":"code","source":"word=\"film\"\nvec=word_to_vec_map[word]\nprint('shape of vector is :', vec.shape)\nprint(vec)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a12216b0d28f32acf3104ee7b361abe0172fbd6"},"cell_type":"markdown","source":"As we have not removed special characters, we need to make sure we have glove vector for special characters."},{"metadata":{"trusted":true,"_uuid":"2b1e8edc1908a347281dee77bc19e45a634af527"},"cell_type":"code","source":"index_to_word[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e6ea2f291bb37148a9314436e16928aef0fab59"},"cell_type":"code","source":"# x1=np.array([\"funny lol\", \"lets play baseball\", \"I self-glorification u\",\"food is ready for you\",])\n# # from textblob import TextBlob\n# # # x1[0].apply(lambda x: str(TextBlob(x).correct()))\n# # # TextBlob(\"self-glorification\").correct()\n# sent=[]\n# temp=[]\n# def list_flatten(l, a=None):\n#     #check a\n#     if a is None:\n#         #initialize with empty list\n#         a = []\n#     for i in l:\n#         if isinstance(i, list):\n#             list_flatten(i, a)\n#         else:\n#             a.append(i)\n#     return a\n\n# for i in x1:\n#     print(i)\n#     sent=i.lower().split()\n# #     print(sent)\n#     for j in sent :\n#         flag=0\n#         match=re.search(r'\\w-[\\w]',j)\n#         if match:\n#             flag=1\n# #             print(i, '====',  j)\n# #             sent.remove(j)\n#             idx=sent.index(j)\n# #             print (idx)\n#             temp=[]\n#             temp=j.split('-')\n# #             print(temp)\n#     if flag==1:\n#         sent.insert(idx,temp)\n#         sent=list_flatten(sent)\n    \n# #     print(sent)\n# #     list(np.array(sent).flat)\n#     print(sent)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e7cd0f6eaa5bb5cf3c18c978576b6e771e5d751"},"cell_type":"markdown","source":"I faced problems while converting words into vectors as the words were not clean. \n\ne.g. self-glorification :- we can seperate this into two words \"self\" and \"glorification\" by splitting it with \"-\" and this will also keep the meaning of original word."},{"metadata":{"trusted":true,"_uuid":"752624861bf82de396eabfbdd4f408ecfd9bf752"},"cell_type":"code","source":"import re \nx1=np.array([\"funny lol\", \"lets play baseball\", \"I self-glorification u\",\"food is ready for you\",])\nfor i in x1:\n    print(i)\n    match=re.search(r'\\w-[\\w]',i)\n    if match:\n        i=re.sub(r'(\\w)-([\\w])',r'\\1 \\2',i)\n        print(i)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68aa9530a69f21073f79842056660684fe2d8f9e"},"cell_type":"markdown","source":"Creating function which will include special handling for complex words like splitting into two words with the help of regualr expressions, autocorrecting the words using lemmatizer, TextBlobl etc. \n\nAnd also the words for which are not vectorized are collected into array \"fault\" . These can be looked over and over and our function can be modified accordingly."},{"metadata":{"trusted":true,"_uuid":"59822d5d5eba80b9a1ae5b73244b9683302ddd28"},"cell_type":"code","source":"x1=np.array([\"funny lol\", \"lets play baseball\", \"I self@glorification u \", \"food is ready for you\", \"It's there\"])\n\n# from nltk.stem import PorterStemmer\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nst = PorterStemmer()\nlem = WordNetLemmatizer()\n\ndef sentence_to_indices(X,word_to_index,max_len):\n    m=X.shape[0]\n    fault=[]\n    #vecs=word_to_vec_map[\"film\"].shape[0]\n    X_indices=np.zeros((m,max_len))\n    \n    for i in range(m):\n        #print(X[i])\n        match=re.search(r'\\w[!@#$%^&*()_+=-][\\w]',X[i])\n        if match:\n            X[i]=re.sub(r'(\\w)[!@#$%^&*()_+=-]([\\w])',r'\\1 \\2',X[i])\n#             print(i)\n#         print(X[i])\n        sentence=X[i].lower().split()\n#         print(sentence)\n        #print(sentence)\n        count=0\n        for j in sentence:\n            try:\n                if j in word_to_index:\n                    X_indices[i,count] = word_to_index[j]            \n#                 elif word_to_index[str(TextBlob(st.stem(j)).correct())]:\n                else:\n#                 str(TextBlob(st.stem(j)).correct()) not in word_to_index:\n#                 print (j ,'is not in word_to_index. It is in line ', i)            \n                    X_indices[i,count] = word_to_index[str(TextBlob(st.stem(j)).correct())]\n            except:\n                fault.append((j,i))\n            count=count+1\n    return X_indices,fault\n\nprint ('shape is ', x1.shape)\nsent,fault=sentence_to_indices(x1,word_to_index,6)\nprint(sent)\nprint('=====')\nprint(fault)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a4e30e1d6c6b592dbb7a0b9bea848b8bbed420e"},"cell_type":"code","source":"print(fault)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"354b6ac997d7d8ca10bce9b5fab65e608fe286f3"},"cell_type":"markdown","source":"Bit of testing out of curiosity what combination can give simple words out of a complex word."},{"metadata":{"trusted":true,"_uuid":"29b83caa0920296efbbe8b7545209fcbdfb4035d"},"cell_type":"code","source":"print(  \"it's\" in word_to_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7eb638feeaea64c5d49ade48c492532e6facacda"},"cell_type":"code","source":"from textblob import Word\nprint(Word(\"it's\").lemmatize())\n# word_to_index[str(TextBlob(st.stem(\"it's\")).correct())]\n# print(TextBlob(\"it's\").correct())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4d3642405df87c99bac48f53e8e03bda6bae072"},"cell_type":"code","source":"# print(word_to_index[str(TextBlob(st.stem(\"it's\")).correct())])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6961acf6b904f0894e6def43381c6a3845ba42c","scrolled":true},"cell_type":"code","source":"# from nltk.stem import PorterStemmer\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nst = PorterStemmer()\nlem = WordNetLemmatizer()\n\nprint( st.stem('substitutable'))\nprint(lem.lemmatize('substitutable'))\n# print(word_to_vec_map[st.stem('substitutable')])\nfrom textblob import Word\nprint(Word('substitutable').lemmatize())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7be86d7efec6b684b1058edd8d08d409ef73547"},"cell_type":"code","source":"from textblob import TextBlob\nprint(TextBlob(st.stem('substitutable')).correct())\nprint(TextBlob('substitut').correct())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11d75cc0311c71362bce45c9e64a95314a435273"},"cell_type":"markdown","source":">Creating Embedding layer. \n>Remember we do not want to learn embedding layer. We will use Glove vectors  instead.\n"},{"metadata":{"trusted":true,"_uuid":"1827e71e56dbeac0f38d6197def6abd01e29a5f3"},"cell_type":"code","source":"from keras.layers import Embedding\ndef pretrained_embedding_layer(word_to_vec_map, word_to_index):\n    vocab_len=len(word_to_index)+1\n    vec_length=word_to_vec_map[\"film\"].shape[0]\n    emb_matrix=np.zeros((vocab_len,vec_length))\n    \n    for word,index in word_to_index.items():\n        emb_matrix[index,:]=word_to_vec_map[word]\n        \n    embedding_layer = Embedding(vocab_len,vec_length,trainable=False)\n    embedding_layer.build((None,))\n    \n    embedding_layer.set_weights([emb_matrix])\n    \n    return embedding_layer\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"998cd3994cadc5ca669cf15e724b1ad401faa2be"},"cell_type":"code","source":"embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\nprint(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbfa4009aedb6f11843bef691b42cedc80bb289f"},"cell_type":"code","source":"t1=tmp['Sentiment'].head()\nt1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4dc19ab0a2eaf3ea8326f7a3a11bdea7cbb522c"},"cell_type":"markdown","source":"Converting output into a categorical/one-hot-encoded."},{"metadata":{"trusted":true,"_uuid":"de82df8cd1f51ef15a5e3f6733b1838cbe595887"},"cell_type":"code","source":"from keras.utils.np_utils import to_categorical \nto_categorical(t1,num_classes=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a87ee2c7c4054753f06437e695b360a14493d0d9"},"cell_type":"code","source":"\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c866f72d3c9c185906330b81c0a959cc5d83a90"},"cell_type":"code","source":"word_to_index['#']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7197a482d2a00e1a09b9e37300a75347ead926f0"},"cell_type":"markdown","source":"Framing inputs and outputs"},{"metadata":{"trusted":true,"_uuid":"f4cc0e1d63c9532c76571c2c47f60a32c210addc"},"cell_type":"code","source":"X= df['Phrase']\nY= df['Sentiment']\nprint(X.shape)\nprint(Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6548df7059aaec5869216cf82803b861e97cee6"},"cell_type":"markdown","source":">Below is exploration when I get some errors.\ne.g. PhraseId170 hasword comedy-drama , for this we dont have any glove vector so we have to make our regular expressions which could adapt to such words."},{"metadata":{"trusted":true,"_uuid":"0397bb47b5c08a1cf6fc07c1424efd7bf523c6a6"},"cell_type":"code","source":"tmp_train=df['Phrase'].head(170)\n# tmp_train\ndf[(df.index>=151) & (df.index<=170)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91740d7bdfc9247856261850315e5b616df3ceae"},"cell_type":"code","source":"display(df[(df['PhraseId']==105156 ) & (df['SentenceId']==5555)] )#105156, 5555,","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ccaf8d2bed42fd6117130a8579d616dffa8d099"},"cell_type":"code","source":"tmp_train.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"daae800965b5ae17d0518849afbd0d9effea9777"},"cell_type":"code","source":"tmp_index,fault=sentence_to_indices(tmp_train, word_to_index,max_len= 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2668efec05b5fa634bd0bd30ffdf43a025aef36e"},"cell_type":"code","source":"fault","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e11ac2d15a8174eeba1f1d86d5c548b8c990560e"},"cell_type":"markdown","source":">I don't get what \"-ERB\" and similar words are for in this dataset. I sepent almost a week searchi for such words on internet. Are these words for line break etc ?"},{"metadata":{"trusted":true,"_uuid":"75288dc218468411311684b9e94b77292b9dad30"},"cell_type":"code","source":"# str(TextBlob(st.stem('demonstr')).correct())\n# word_to_index['demonstr']\n# str(TextBlob('demonstr').correct())\nprint(st.stem('-ERB-'))\nprint(lem.lemmatize('-ERB-'))\nprint(word_to_index['demonstrating'] )\nprint(word_to_index['manipulative'] )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e908a177fd7f7562f88113e10e8f1f409e5522f"},"cell_type":"code","source":"# from nltk.stem import PorterStemmer\n\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nsts = SnowballStemmer('english')\nlem = WordNetLemmatizer()\n\nprint( st.stem('-erb-'))\n# print(lem.lemmatize('substitutable'))\n# # print(word_to_vec_map[st.stem('substitutable')])\n# from textblob import Word\n# print(Word('substitutable').lemmatize())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ec1b88db68440e93943d599935e513113d504e2"},"cell_type":"code","source":"# X_train_indices,fault = sentence_to_indices(X_train, word_to_index, 56)\n# # # sentence_to_indices(x1,word_to_index,6)\n# # # (t1,num_classes=4)\n# # Y_train_oh = to_categorical(Y_train, num_classes= 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d843894a9fa77ee45d8f440d5d7ccb518503410c"},"cell_type":"code","source":"# print(datetime.datetime.now())\n# tmp2=df['Phrase'].head(1000000\n# tmp2_ind,fault2 = sentence_to_indices(tmp2, word_to_index, 56)\n# print(datetime.datetime.now())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be8b5b8dd37d9dcbca497207c41916bc70843744"},"cell_type":"code","source":"# # 156059\n# tmp3=[]\n# print(datetime.datetime.now())\n# tmp3=df.iloc[(df['Phrase'].index.values> 145000)]\n# # tmp2_ind,fault2 = sentence_to_indices(tmp2, word_to_index, 56)\n\n# display(tmp3.head())\n# print(type(tmp3))\n# print('--------')\n# display(tmp1.head())\n# print(type(tmp1))\n\n# print(datetime.datetime.now())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5449c99def77ad2dfeab5ecc7696bf4751feaee8"},"cell_type":"code","source":"# tmp4=tmp3['Phrase'].reset_index()['Phrase']\n# tmp4.head()\n# print(datetime.datetime.now())\n\n# tmp4_ind,fault4 = sentence_to_indices(tmp4, word_to_index, 56)\n# print(datetime.datetime.now())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e78b31c4aff45df47bf3a6987914afce1cc5dcfb"},"cell_type":"code","source":"# print(datetime.datetime.now())\n# tmp3=df.head()\n# # print(tmp3['Phrase'].index.values)\n# # tmp3_ind,fault3 = sentence_to_indices(tmp3, word_to_index, 56)\n# # print(datetime.datetime.now())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43fd83afa21326df3ac0e71d7ba4b22c33ab4e5e"},"cell_type":"markdown","source":">I was getting some error while processing line by line in dataset. Finally resset_index fix it."},{"metadata":{"trusted":true,"_uuid":"620092d3f4cd597b889179d352fa01cd73c8e47b"},"cell_type":"code","source":"X_train= df.reset_index()['Phrase']\n# Y_train= df['Sentiment']\n# X_train=X_train.reset_index()['Phrase']\n# print(X_train.shape)\n# print(Y_train.shape)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a21cbfb5761328cdbb5ce6f05c6a7c84800f81b0"},"cell_type":"code","source":"from datetime import datetime\nprint(datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a2ef99bbf9ff999ea2a6688485f8fc270bc1900"},"cell_type":"code","source":"X_train_ind,fault = sentence_to_indices(X_train, word_to_index, 56)\n# X_test_ind,fault1 = sentence_to_indices(X_test, word_to_index, 56)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9a36944bdf016f9963ebcd50d28c898e326104e"},"cell_type":"code","source":"from datetime import datetime\nprint(datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bf9b836d14fbc0f3802dc4b1cec46329bde04f1"},"cell_type":"markdown","source":"we can analyse values in \"fault\" list and adjust our regex again."},{"metadata":{"trusted":true,"_uuid":"ef378d4eef56efae547a1876f27ff0cf705a39c1"},"cell_type":"code","source":"from keras.utils.np_utils import to_categorical \nY_train= df['Sentiment']\nY_train_ind=to_categorical(Y_train,num_classes=5)\nprint(Y_train_ind.shape) \nprint(X_train_ind.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"255a0342151ac516f7878fa439bbd46af001f08b"},"cell_type":"markdown","source":"Split dataset in train and test sets"},{"metadata":{"trusted":true,"_uuid":"e7947bf57c18a26a85dbe03dc3a18e2807387c99"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_to_train,X_to_test,Y_to_train,Y_to_test=train_test_split(X_train_ind,Y_train_ind, test_size=0.2, \n                                                           random_state=22)\nprint(X_to_train.shape)\nprint(X_to_test.shape)\nprint(Y_to_train.shape)\nprint(Y_to_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a070003bbcd8c59225eb809e2e7a5c1d0a118174"},"cell_type":"code","source":"type(fault)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fda08b685bf17ef6dfc65bbd65cc290717b9e3f5"},"cell_type":"code","source":"print(len(fault))\ntmp_fault=fault[:10]\nprint(tmp_fault)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fcd9d2e566796711a11ada59f45d2de89173971"},"cell_type":"code","source":"# def Myfun(x):\n#     return x[0]\n# print(sorted(tmp_fault))\n# print(sorted(tmp_fault,key=Myfun))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ec958911476712ec9a96573da380629df168191"},"cell_type":"code","source":"fault1=[x[0] for x in fault]\nfault_out=list(set(sorted(fault1)))\nprint(fault_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd7a33fa5d2179e429a8576b4f924c12c8268766"},"cell_type":"code","source":"import numpy as np\n# np.random.seed(0)\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, LSTM, Activation\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nfrom keras.initializers import glorot_uniform\n# np.random.seed(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26f20554628c00d2445a002214c963eb0b897e9d"},"cell_type":"code","source":"def Sentiment(input_shape, word_to_vec_map, word_to_index):\n    \"\"\"\n    \n    Arguments:\n    input_shape -- shape of the input, usually (max_len,)\n    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n\n    Returns:\n    model -- a model instance in Keras\n    \"\"\"\n    \n    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n    sentence_indices = Input(shape=input_shape, dtype='int32')\n    \n    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n    \n    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n    embeddings = embedding_layer(sentence_indices)   \n    \n    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n    # Be careful, the returned output should be a batch of sequences.\n    X = LSTM(128, return_sequences=True)(embeddings)\n    # Add dropout with a probability of 0.5\n    X = Dropout(0.5)(X)\n    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n    X = LSTM(128, return_sequences=False)(X)\n    # Add dropout with a probability of 0.5\n    X = Dropout(0.5)(X)\n    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n    X = Dense(5)(X)\n    # Add a softmax activation\n    X = Activation('softmax')(X)\n    \n    # Create Model instance which converts sentence_indices into X.\n    model = Model(inputs=[sentence_indices],outputs=X)\n    \n    ### END CODE HERE ###\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e805162c29a007ed11fabb3a1b0aea7eee6042ed"},"cell_type":"code","source":"maxLen=56\nmodel = Sentiment((maxLen,), word_to_vec_map, word_to_index)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a6da3fbedf8693a517aeb55f3bf773338a2bfb6"},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90b010db7b56d175e6a54cba9416724b85fcd624"},"cell_type":"code","source":"print(datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"092337f3997036e5b56ab5a9ca734b66e89320bb"},"cell_type":"code","source":"model.fit(X_to_train, Y_to_train, epochs = 10, batch_size = 32, shuffle=True)\nprint(datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"237645ed1b9f1e7ce7c3c8bac5704bdbef13281b"},"cell_type":"code","source":"print(X_to_test.shape)\nprint(Y_to_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b821c5b7a570d11ea4756b4d7ae83dd84379cc5a"},"cell_type":"markdown","source":"Predictions using the model"},{"metadata":{"trusted":true,"_uuid":"710d41bc7f0231740937901eba7159c66f014f9a"},"cell_type":"code","source":"print(datetime.now())\npred=model.predict(X_to_test) \nprint(datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"066b6916bdc88bb805f37ad5a65d5ffb448a1689"},"cell_type":"markdown","source":"Another way can be :-"},{"metadata":{"trusted":true,"_uuid":"60f4a36c3a0f57359110ae7ad8a24cff1d631425"},"cell_type":"code","source":"print(datetime.now())\nloss, acc = model.evaluate(X_to_test, Y_to_test)\nprint('LOSS is :- ' , loss)\nprint('ACCURACY is :- ', acc)\nprint(datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0facb0fbced3679fb170a9a78d681003ed51fe1"},"cell_type":"markdown","source":"**I am of opinion that we can run some more epochs while training making better fit , also try  to extract real world words from complex words using regex. Remember , when there is no vector presentation for a word in GLove , it is considered as zero vector.**"},{"metadata":{"_uuid":"dfd207c787606d42a3b232da0ef186cde776b8a9"},"cell_type":"markdown","source":"**THANK YOU . Kindly comment to improve the note book.**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}