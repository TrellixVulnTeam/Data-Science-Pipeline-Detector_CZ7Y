{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"os.listdir(\"../input/movie-review-sentiment-analysis-kernels-only\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\ntrain = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv',sep='\\t')\ntest = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv',sep='\\t')\nsub = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv', sep=\",\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bec3b538476a3421eff87dad89d296ef029bd2d","collapsed":true},"cell_type":"code","source":"full_text = list(train['Phrase'].values)+list(test['Phrase'].values)\ntk = Tokenizer(lower = True,filters = '')\ntk.fit_on_texts(full_text)\ntrain_tokenized = tk.texts_to_sequences(train['Phrase'])\ntest_tokenized = tk.texts_to_sequences(test['Phrase'])\ny = train['Sentiment']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"942ccf0a60f8cbf027d0c959a990c760ca50a1b1","collapsed":true},"cell_type":"code","source":"max_len = 50\nX_train = pad_sequences(train_tokenized,maxlen=max_len)\nX_test = pad_sequences(test_tokenized,maxlen=max_len) # 最大长度为50 不足50的前方补零\nembed_size = 300\nbatch_size = 1000\n\nlr = 0.01\ntraining_iter = X_train.shape[0]\nn_inputs = 300\nn_steps = 50\nn_hidden_unit = 128\nn_classes = len(set(y))\n\nembedding_path = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\ndef get_coefs(word,*arr): return word,np.asarray(arr,dtype = 'float32')\nembeding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\nword_index = tk.word_index\n\n\nembedding_matrix = np.zeros((len(word_index)+1,embed_size))\nfor word,i in word_index.items():\n    embedding_vector = embeding_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\nfrom sklearn.preprocessing import OneHotEncoder # 将y 值转换为ont_hot 编码的形式\nohe = OneHotEncoder(sparse=False)\ny_ohe = ohe.fit_transform(y.values.reshape(-1,1)) # 将其转换为列向量\n\ndef getTrainData(step ,batch_size):\n    train_index = X_train[((step-1)*batch_size):(step * batch_size)]# shape = [batch_size,n_step]\n    train_data = np.zeros(shape = [train_index.shape[0],train_index.shape[1],embed_size])\n\n    for i in range(train_index.shape[0]):\n        for j in range(train_index.shape[1]):\n            for k in range(embed_size):\n                train_data[i][j][k] = embedding_matrix[train_index[i][j]][k]\n\n    train_label = y_ohe[((step-1)*batch_size):(step * batch_size)]\n\n    # print(train_index.shape,train_label.shape)\n    # print(train_index[:5],train_label[:5])\n    return train_data,train_label\n\n# getTrainData(1,batch_size)\ndef getTestData(step, batch_size):\n    test_index = X_test[((step-1)*batch_size):(step * batch_size)]\n    test_data = np.zeros(shape = [test_index.shape[0],test_index.shape[1],embed_size])\n    for i in range(test_index.shape[0]):\n        for j in range(test_index.shape[1]):\n            for k in range(embed_size):\n                test_data[i][j][k] = embedding_matrix[test_index[i][j]][k]\n    test_y = y_ohe[0:test_index.shape[0]]\n    return test_data,test_y\n# tensorflow\n\nx = tf.placeholder(tf.float32,shape = [None,n_steps,n_inputs])\ny = tf.placeholder(tf.float32,shape = [None,n_classes])\n\nweights = {\n    \"in\":tf.Variable(tf.random_normal([n_inputs,n_hidden_unit])),\n    \"out\":tf.Variable(tf.random_normal([n_hidden_unit,n_classes]))\n}\nbiases ={\n    \"in\": tf.Variable(tf.constant(0.1,shape =[n_hidden_unit,])),\n    \"out\":tf.Variable(tf.constant(0.1,shape = [n_classes,]))\n}\n\ndef RNN(X, weights, biases):\n    # X.shape = n_batch,n_steps,n_inputs\n    X = tf.reshape(X, [-1,n_inputs])\n    # X.shape = [batch_size *28, 28]\n    X_in = tf.matmul(X, weights[\"in\"])+ biases[\"in\"]\n    # X,shape = [batch_size,n_steps,n_hidden_unit]\n    X_in = tf.reshape(X_in , shape = [-1, n_steps, n_hidden_unit])  #(n_batch, 28,128 hidden_size)\n\n    # cell\n    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden_unit,forget_bias = 1.0, state_is_tuple = True)\n\n    #_init_state =lstm_cell.zero_state(batch_size, dtype = tf.float32)\n\n    outputs, states = tf.nn.dynamic_rnn(lstm_cell , X_in, dtype = tf.float32, time_major =False)\n\n    # result = tf.matmul(states[-1], weights['out']) + biases['out']\n    outputs = tf.unstack(tf.transpose(outputs, [1, 0, 2]))\n\n    results = tf.matmul(outputs[-1], weights['out']) + biases['out']\n    return results;\n\npred = RNN(x, weights, biases)\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred,labels = y))\ntrain_op = tf.train.AdamOptimizer(lr).minimize(cost)\n\ncorrect_pred = tf.argmax(pred, 1)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for i in range(2):\n        if i == 1: # reverse data \n            reversed_train = reversed(X_train)\n            reversed_y = reversed(y_ohe)\n            X_train = np.array(list(reversed_train))\n            y_ohe = np.array(list(reversed_y))\n        step = 1\n        while step * batch_size < training_iter:\n            train_x,train_y = getTrainData(step ,batch_size)\n            sess.run([train_op],feed_dict = {\n                x:train_x,\n                y:train_y\n            })\n            if step % 100 ==0:\n                print(sess.run([cost],feed_dict = {\n                    x: train_x,\n                    y: train_y\n                }))\n            step+=1\n    ans = []\n    step = 1\n    while step * batch_size < X_test.shape[0]:\n        test_data,test_y = getTestData(step, batch_size)\n        tmp = sess.run([correct_pred],feed_dict ={\n         x:test_data,\n         y: test_y\n        })\n        ans.extend(tmp[0])\n        step +=1\n    test_data, test_y = getTestData(step, batch_size)\n    tmp = sess.run([correct_pred], feed_dict = {\n        x: test_data,\n        y: test_y\n    })\n    ans.extend(tmp[0])\n    sub['Sentiment'] = ans\n    sub.to_csv(\"blend1.csv\", index = False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a51a639b717497674de755fa643666e36c696b8d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}