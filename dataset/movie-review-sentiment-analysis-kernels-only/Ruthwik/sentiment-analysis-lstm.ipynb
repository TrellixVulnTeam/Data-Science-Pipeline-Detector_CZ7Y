{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, LSTM, Activation\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom keras.utils import to_categorical\nfrom keras.initializers import glorot_uniform\nfrom keras.preprocessing.text import Tokenizer\nnp.random.seed(1)\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b408a29a44cab83e1e25e77fd0b66995e5c016ca"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load Data\ndf = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv', delimiter='\\t')\ntest = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv', delimiter='\\t')\npd.set_option('display.max_colwidth', -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a92705363d52d05d6b697dd73792bed314db7acf"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"56c31ee3faec4996cf8dca6ae0c73ab0e2afe732"},"cell_type":"code","source":"seed = 101 \nnp.random.seed(seed)\n\nX = df['Phrase']\ntemp = test['Phrase']\ny = to_categorical(df['Sentiment'])\nnum_classes = df['Sentiment'].nunique()\nmaxLen = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5a08b7a2dd0a81928e81d1386e1e34b62f45681f"},"cell_type":"code","source":"def read_glove_vecs(glove_file):\n    with open(glove_file, 'r', encoding=\"utf8\") as f:\n        words = set()\n        word_to_vec_map = {}\n        for line in f:\n            line = line.strip().split()\n            curr_word = line[0]\n            words.add(curr_word)\n            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n        \n        i = 1\n        words_to_index = {}\n        index_to_words = {}\n        for w in sorted(words):\n            words_to_index[w] = i\n            index_to_words[i] = w\n            i = i + 1\n    return words_to_index, index_to_words, word_to_vec_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2109ddbeb8a39eadc65ea18bcd2f6081131ad5f"},"cell_type":"code","source":"word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('../input/nlpword2vecembeddingspretrained/glove.6B.100d.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c24003ddb7cbfeca961f8eb6b3d5c9d5d66af193"},"cell_type":"code","source":"def sentences_to_indices(X, word_to_index, max_len):\n    \"\"\"\n    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n    The output shape should be such that it can be given to `Embedding()`\n    \"\"\"\n    \n    m = X.shape[0]                                   # number of training examples\n\n    X_indices = np.zeros((m,max_len))\n    \n    for i in range(m):                               # loop over training examples\n        \n\n        sentence_words = text_to_word_sequence(X[i],lower=True)\n        \n        j = 0\n        \n        # Loop over the words of sentence_words\n        for w in sentence_words:\n            try:\n                word_index =  word_to_index[w]\n                if word_index is not None:\n                    X_indices[i, j] = word_index\n                    j = j+1\n            except Exception:            # pass any exception occured\n                pass\n\n    \n    return X_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d00764206940d6be99467879a3de7f8f94414047"},"cell_type":"code","source":"def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n    \"\"\"\n    Creates a Keras Embedding() layer and loads in pre-trained GloVe 100-dimensional vectors.\n    \n    \"\"\"\n    \n    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n    emb_dim = word_to_vec_map[\"happy\"].shape[0]      # define dimensionality of the GloVe word vectors (= 50)\n    \n    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n    emb_matrix = np.zeros((vocab_len,emb_dim))\n    \n    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n    for word, index in word_to_index.items():\n        emb_matrix[index, :] = word_to_vec_map[word]\n\n    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n    embedding_layer = Embedding(vocab_len,emb_dim,trainable=False)\n    ### END CODE HERE ###\n\n    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n    embedding_layer.build((None,))\n    \n    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n    embedding_layer.set_weights([emb_matrix])\n    \n    return embedding_layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dce8bd905802d0d752fdd75fa42a393197e53f22"},"cell_type":"code","source":"def get_model(input_shape, word_to_vec_map, word_to_index):\n    \n    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n    sentence_indices = Input(input_shape, dtype='int32')\n    \n    # Create the embedding layer pretrained with GloVe Vectors (â‰ˆ1 line)\n    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n    \n    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n    embeddings = embedding_layer(sentence_indices)     \n    \n    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n    X = LSTM(128, return_sequences=True)(embeddings)\n    # Add dropout with a probability of 0.5\n    X = Dropout(0.5)(X)\n    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n    X = LSTM(128, return_sequences=False)(X)\n    # Add dropout with a probability of 0.5\n    X = Dropout(0.5)(X)\n    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n    X = Dense(5)(X)\n    # Add a softmax activation\n    X = Activation('softmax')(X)\n    \n    # Create Model instance which converts sentence_indices into X.\n    model = Model(inputs=sentence_indices, outputs=X)\n    \n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"450076ad443ed31b51bcdfee6005fc56e5a7e65b"},"cell_type":"code","source":"model = get_model((maxLen,), word_to_vec_map, word_to_index)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"377e8ac459714edda32bf41fd7bd5b41631ac12d"},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bac88625879042e778c52ed1332ec0290fa1f653"},"cell_type":"code","source":"# Spilt Train Test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,stratify=y,random_state=seed)\n# print(X_train.shape,y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d7bed89e04a6d8f3bf4de8736feb97b6329acd5"},"cell_type":"code","source":"X_train = np.asarray(X_train,dtype=str)\nX_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\nX_test = np.asarray(X_test,dtype=str)\nX_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29129df599d5bb8b475648323d62bc21bf5fab40"},"cell_type":"code","source":"model.fit(X_train_indices, y_train,validation_data=(X_test_indices, y_test), epochs = 7, batch_size = 128, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"73b1f1c50cbd1cb99c19e74914ab2063c46b57fd"},"cell_type":"code","source":"temp = np.asarray(temp,dtype=str)\ntemp_indices = sentences_to_indices(temp, word_to_index, maxLen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4b606b19614e2fad721e6e11a889a5b6b4421db3"},"cell_type":"code","source":"predict_classes = model.predict(temp_indices)\nclasses_list = []\n\nfor x_test_predict in predict_classes:\n    classes_list.append(np.argmax(x_test_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f7f783d0afeb11165df24430d5ee78321d07e07"},"cell_type":"code","source":"sub = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv')\nsub['Sentiment'] = pd.Series(classes_list)\nsub.to_csv(\"lstm_glove.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"497ab52ed31997b0a93463163a7c0613e49fe3e4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}