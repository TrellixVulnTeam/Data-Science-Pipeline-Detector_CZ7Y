{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport nltk\nimport os\nimport gc\nfrom keras.preprocessing import sequence,text\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Embedding,LSTM,Conv1D,GlobalMaxPooling1D,Flatten,MaxPooling1D,GRU,SpatialDropout1D,Bidirectional\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report,f1_score\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#pd.set_option('display.max_colwidth',100)\npd.set_option('display.max_colwidth', -1)\nfrom nltk import FreqDist\nfrom nltk.stem import SnowballStemmer,WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n\ntrain = pd.read_table(\"../input/movie-review-sentiment-analysis-kernels-only/train.tsv\")\ntest = pd.read_table(\"../input/movie-review-sentiment-analysis-kernels-only/test.tsv\")\nsub = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv\")\n\n\n\n\ndef standardize_text(df, text_field):\n    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n    df[text_field] = df[text_field].str.lower()\n    return df\n\ntrain = standardize_text(train, \"Phrase\")\ntest = standardize_text(test, \"Phrase\")\n\ntrain_text=train.Phrase.values\ntest_text=test.Phrase.values\ntarget=train.Sentiment.values\ny=to_categorical(target)\nprint(train_text.shape,target.shape,y.shape)\n\nfrom sklearn.model_selection import train_test_split\nX_train_text,X_val_text,y_train,y_val=train_test_split(train_text,y,test_size=0.2,stratify=y,random_state=123)\nprint(X_train_text.shape,y_train.shape)\nprint(X_val_text.shape,y_val.shape)\n\ncorpus = train.Phrase.tolist() + test.Phrase.tolist()\nall_words=' '.join(corpus)\nall_words=word_tokenize(all_words)\ndist=FreqDist(all_words)\nnum_unique_word=len(dist)\n\n\nmax_features = 15000\nmax_words = 100\nbatch_size = 256\nepochs = 3\nnum_classes=5\n\n#corpus = list(X_train_text)+ list(X_train_text)\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(corpus))\nX_train = tokenizer.texts_to_sequences(X_train_text)\nX_val = tokenizer.texts_to_sequences(X_val_text)\nX_test = tokenizer.texts_to_sequences(test_text)\n\nX_train = sequence.pad_sequences(X_train, maxlen=max_words)\nX_val = sequence.pad_sequences(X_val, maxlen=max_words)\nX_test = sequence.pad_sequences(X_test, maxlen=max_words)\nembed_size = 100 # how big is each word vector\n#max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 \n\nEMBEDDING_FILE = \"../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\"\n\n\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n    \ndef get_embed_mat(EMBEDDING_FILE, max_features,embed_dim):\n    # word vectors\n    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding='utf8'))\n    print('Found %s word vectors.' % len(embeddings_index))\n\n    # embedding matrix\n    word_index = tokenizer.word_index\n    num_words = min(max_features, len(word_index) + 1)\n    all_embs = np.stack(embeddings_index.values()) #for random init\n    embedding_matrix = np.random.normal(all_embs.mean(), all_embs.std(), \n                                        (num_words, embed_dim))\n    for word, i in word_index.items():\n        if i >= max_features:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    max_features = embedding_matrix.shape[0]\n    \n    return embedding_matrix\n        \n\n\n    \nembed_dim = 100 #word vector dim\nembedding_matrix = get_embed_mat(EMBEDDING_FILE,max_features,embed_dim)\nprint(embedding_matrix.shape)    \n    \n#-------------------------------------------------------------------------------- GRU\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_dim,input_length=X_train.shape[1], weights=[embedding_matrix],trainable=True))\nmodel.add(SpatialDropout1D(0.25))\nmodel.add(Bidirectional(GRU(128,return_sequences=True)))\nmodel.add(Bidirectional(GRU(64,return_sequences=False)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(5, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n\nmodel.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=4, batch_size=batch_size, verbose=1)\ny_pred=model.predict_classes(X_test, verbose=1)\n\n\n# LSTM\nmodel1=Sequential()\nmodel1.add(Embedding(max_features,embed_size, weights=[embedding_matrix],))\n#model1.add(LSTM(128,dropout=0.4, recurrent_dropout=0.4,return_sequences=True))\nmodel1.add(LSTM(64,dropout=0.4, recurrent_dropout=0.4,return_sequences=True))\nmodel1.add(LSTM(32,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\nmodel1.add(Dense(5,activation='softmax'))\nmodel1.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])\nmodel1.summary()\n  \n\nmodel1.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=4, batch_size=batch_size, verbose=1)\ny_pred1=model1.predict_classes(X_test, verbose=1)\n\nsub_agg=pd.DataFrame({'model1':y_pred1,'model':y_pred})\npred=sub_agg.agg('mode',axis=1)[0].values\nsub_agg.head()\n\npred=[int(i) for i in pred]\nsub.Sentiment=pred\nsub.to_csv('desole.csv',index=False)\nsub.head()\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06ffdaf33088a8b7c9a6ef8ffd448cb8db8bd894"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}