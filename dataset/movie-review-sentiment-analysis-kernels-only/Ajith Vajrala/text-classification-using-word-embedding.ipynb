{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator#\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, Flatten\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom nltk.corpus import stopwords\nfrom keras.utils import to_categorical\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/train.tsv\", sep=\"\\t\")\ntest_df= pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/test.tsv\", sep=\"\\t\")\npos_df = pd.read_csv(\"../input/pos-neg-files/positive words.txt\", sep=\"\\n\", header=None)\nneg_df = pd.read_csv(\"../input/pos-neg-files/Negative words.txt\", sep=\"\\n\", header=None, encoding = \"ISO-8859-1\")\npos_df.columns = ['words']\nneg_df.columns = ['words']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52ca978ce42398f776c8aeb20f2dd4216848fc0c"},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport string\nfor df in [train_df, test_df]:\n    df['words_length'] = df['Phrase'].apply(lambda x: len(x))\n    df['sent_length'] = df['Phrase'].apply(lambda x: len(word_tokenize(x)))\n    df['no_stops'] = df['Phrase'].apply(lambda x: len([w for w in word_tokenize(x.lower()) if w in stop_words]))\n    df['no_non_stops'] = df['Phrase'].apply(lambda x: len([w for w in word_tokenize(x.lower()) if w not in stop_words]))\n    df['no_punctuations'] = df['Phrase'].apply(lambda x: \n                                               len([w for w in word_tokenize(x.lower()) if w in string.punctuation if w not in \".\" \n                                                   if w not in \",\"]))\n    \n    df['pos_words'] = df['Phrase'].apply(lambda x: len([w for w in word_tokenize(x.lower()) if w in pos_df.words.values]))\n    df['neg_words'] = df['Phrase'].apply(lambda x: len([w for w in word_tokenize(x.lower()) if w in neg_df.words.values]))\n    df['neutral_words'] = df['Phrase'].apply(lambda x: len([w for w in word_tokenize(x.lower()) if w not in neg_df.words.values\n                                                           if w not in pos_df.words.values]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acf8e7f605261d3791312fa892df58332f5a20ab"},"cell_type":"code","source":"train_df['Phrase'][(train_df['words_length']==1) & (train_df['Sentiment']==0) ] = \"bad\" \ntrain_df['Phrase'][(train_df['words_length']==1)& (train_df['Sentiment']==1 )] = \"bad\" \ntrain_df['Phrase'][(train_df['words_length']==1) & (train_df['Sentiment']==2) ] = \"seem\"\ntrain_df['Phrase'][(train_df['words_length']==2) & (train_df['Sentiment'] >=2) ] = \"seem\"\ntest_df['Phrase'][(test_df['words_length']==1)]  = \"seem\"\ntest_df['Phrase'][(test_df['words_length']==2) & ((test_df['Phrase'] != \"no\") | (test_df['Phrase'] != \"No\"))] = \"seem\"\n#new words length\nfor df in [train_df, test_df]:\n    df['words_length'] = df['Phrase'].apply(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a308286fc2ef3793c40e0033f0d3c65226ba7fa"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38b67ee1c278dabd890c2ca41004cfd63ad03000"},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79aa7fb1fc721937805f18b36e4a15c3bc7c44a7"},"cell_type":"code","source":"old_train = train_df[['words_length','sent_length', 'no_stops', 'no_non_stops', 'no_punctuations',\n                   'pos_words', 'neg_words', 'neutral_words']]\nold_test = test_df[['words_length','sent_length', 'no_stops', 'no_non_stops', 'no_punctuations',\n                   'pos_words', 'neg_words', 'neutral_words']]\ny_train = train_df['Sentiment']\nold_train =(old_train - old_train.min())/(old_train.max() - old_train.min())\nold_test =(old_test - old_test.min())/(old_test.max() - old_test.min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d5a3f80aed4b201ff1f7b9fb41436991fe5e1ae"},"cell_type":"code","source":"old_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f25ab7fc973f659493f1b53defc1ce869ae0dfb8"},"cell_type":"code","source":"load_train_df = train_df.copy()\nload_test_df = test_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1357ba711e17494799e79346479252e8e0d16386"},"cell_type":"code","source":"#split the data for training and cross validation\ntrain_df, val_df = train_test_split(train_df, test_size = 0.1, random_state= 144)\nprint(train_df.shape)\nprint(val_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fb09462a63213fcf5b20e77ae29b5e6d286f715"},"cell_type":"code","source":"## some config values \nembed_size = 100 # how big is each word vector\nmax_features = 100000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 50 # max number of words in a question to use\n\n## fill up the missing values\ntrain_X = train_df[\"Phrase\"].fillna(\"##\").values\nval_X = val_df[\"Phrase\"].fillna(\"##\").values\ntest_X = test_df['Phrase'].fillna(\"##\").values\nprint(\"before tokenization\")\nprint(train_X.shape)\nprint(val_X.shape)\nprint(test_X.shape)\n\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\n\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\nprint(\"after tokenization\")\nprint(len(train_X))\nprint(len(val_X))\nprint(len(test_X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1029b086201c8322aeea36e64a50d5967b392ec3"},"cell_type":"code","source":"## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\nprint(\"after padding\")\nprint(len(train_X))\nprint(len(val_X))\nprint(len(test_X))\n\n## Get the target values\ntrain_y = train_df['Sentiment'].values\nval_y = val_df['Sentiment'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3d87d257b2001f15a19af118eaf6c55da48cfad"},"cell_type":"code","source":"#shuffling the data\nnp.random.seed(2018)\ntrn_idx = np.random.permutation(len(train_X))\nval_idx = np.random.permutation(len(val_X))\n\ntrain_y = train_df['Sentiment'].values\nval_y = val_df['Sentiment'].values\n\ntrain_X = train_X[trn_idx]\nval_X = val_X[val_idx]\ntrain_y = train_y[trn_idx]\nval_y = val_y[val_idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff01041ebfae464a7b5e9e1ed6c8e274d9169b3e"},"cell_type":"code","source":"new_train_x = old_train.loc[trn_idx].values\nnew_val_X = old_train.loc[val_idx].values\n#new_train_x = pad_sequences(new_train_x, maxlen=maxlen)\n#new_val_X = pad_sequences(new_val_X, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32594807bd9700be2ac1eb31272b4f7624722681"},"cell_type":"code","source":"print(new_train_x.shape)\nprint(new_val_X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d12670e624bfebe459e9f0056786744e1904cd89"},"cell_type":"code","source":"final_train_x = pd.concat([pd.DataFrame(train_X), pd.DataFrame(new_train_x)], axis=1)\nfinal_val_x  =  pd.concat([pd.DataFrame(val_X), pd.DataFrame(new_val_X)], axis=1)\nfinal_test_x = pd.concat([pd.DataFrame(test_X), pd.DataFrame(old_test.values)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee10814f2a5c70c5f77eceb9325cecce412feb4d"},"cell_type":"code","source":"maxlen = 60\nfinal_train_x = pad_sequences(final_train_x.values, maxlen=maxlen)\nfinal_val_x = pad_sequences(final_val_x.values, maxlen=maxlen)\nfinal_test_x = pad_sequences(final_test_x.values, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d38ba8625c01f1b2dac542ce3706f85067751649"},"cell_type":"code","source":"print(final_train_x.shape)\nprint(final_val_x.shape)\nprint(final_test_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be4f63ac637fefe63c7d65bdcf355776aa2d6a06"},"cell_type":"code","source":"#other_inp = Input(shape=(8,))\n#other_inp = Embedding(8, 8)(other_inp)\n#other_inp = Bidirectional(CuDNNLSTM(128, return_sequences=True))(other_inp)\n#other_inp = Flatten()(other_inp)\n#other_inp = Dense(64, activation=\"relu\")(other_inp)\n#auxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(other_inp)\n#x = Dense(5, activation=\"softmax\")(x)\n#model = Model(inputs=other_inp, outputs=x)\n#model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3), metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23f1e512f17d68969cb2d39f3c2b103727e4c14e"},"cell_type":"code","source":"from tensorflow import keras\ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size)(inp)\nx = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\nx = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\nx = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\nx = Flatten()(x)\nx = Dense(64, activation=\"relu\")(x)\nx = Dense(64, activation=\"relu\")(x)\nx = Dense(5, activation=\"softmax\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3), metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b19de6c394bededc2ea2a61243c4f88f48e5b818"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69ad81cd78e11a401085812ae24543db11759521"},"cell_type":"code","source":"train_y = to_categorical(train_y, num_classes=5)\nval_y = to_categorical(val_y, num_classes=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78e8f12629de03a5f9d950cc859c165d5da82c22"},"cell_type":"code","source":"# ## Train the model \nmodel.fit(final_train_x, train_y, batch_size=512, epochs=4, validation_data=(final_val_x, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b8de4738fd51541e137c0e7afcba729ef38cf59"},"cell_type":"code","source":"pred_glove_val_y = model.predict([final_test_x], batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbfa8ff64d8b1bdb81209a8935bd1adbe2d8e5b8"},"cell_type":"code","source":"predictions = []\nfor i in range(len(pred_glove_val_y)):\n    predictions.append(np.argmax(pred_glove_val_y[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56ef103158a19170edb070d5183a7f823bb893f6"},"cell_type":"code","source":"len(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29f35fea24e024e5ef1aea0ca6c8525b9ae35106"},"cell_type":"code","source":"submission_df = pd.DataFrame()\nsubmission_df['PhraseId'] = test_df['PhraseId']\nsubmission_df['Sentiment'] = predictions \nsubmission_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5524d6045404db96cfcda1a989576127c7f3ac62"},"cell_type":"code","source":"submission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf8befff5edbdfb514069d14d41838a6b3140fdb"},"cell_type":"code","source":"submission_df['Sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d05b7ad6de00323e40775f5694bb936a79f3dc86"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}