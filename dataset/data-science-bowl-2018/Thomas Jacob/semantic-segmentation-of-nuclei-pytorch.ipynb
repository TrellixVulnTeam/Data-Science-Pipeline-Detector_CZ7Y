{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Defining Accelerator","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\ntorch.manual_seed(42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport sys\nimport random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport zipfile\nimport matplotlib.pyplot as plt\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\n\nfrom skimage.io import imread, imshow\nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, Dataset\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \nseed = 42\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extract Relevant Files","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"with zipfile.ZipFile('../input/data-science-bowl-2018/stage1_train.zip') as z:\n    z.extractall('stage1_train')\n\nwith zipfile.ZipFile('../input/data-science-bowl-2018/stage1_test.zip') as z:\n    z.extractall('stage1_test')\n    \nwith zipfile.ZipFile('../input/data-science-bowl-2018/stage1_test.zip') as z:\n    z.extractall('stage2_test_final')    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_PATH = '/kaggle/working/stage1_train/'\nTEST_PATH = '/kaggle/working/stage1_test/'\nTEST_PATH_2 = '/kaggle/working/stage2_test_final/'\n\ntrain_files = next(os.walk(TRAIN_PATH))[1]\ntest_files = next(os.walk(TEST_PATH))[1]\ntest_files_2 = next(os.walk(TEST_PATH_2))[1]\ntest_files_final = test_files + test_files_2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining Train & Test Images ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.zeros((len(train_files), 128, 128, 3), dtype = np.uint8)\nY_train = np.zeros((len(train_files), 128, 128, 1), dtype = np.bool)\nX_test = np.zeros((len(test_files), 128, 128, 3), dtype = np.uint8)\nX_test_2 = np.zeros((len(test_files), 128, 128, 3), dtype = np.uint8)\n\nprint('Getting training data...')\nsys.stdout.flush()\nfor n, id_ in tqdm(enumerate(train_files), total = len(train_files)):\n    img_path = TRAIN_PATH + id_ + '/images/' + id_ + '.png'\n    img = imread(img_path)[:,:,:3]\n    img = resize(img, (128, 128), mode='constant', preserve_range=True)\n    X_train[n] = img\n    \n    masks_path = TRAIN_PATH + id_ + '/masks/'\n    mask = np.zeros((128, 128, 1))\n    mask_images = next(os.walk(masks_path))[2]\n    for mask_id in mask_images:\n        mask_path = masks_path + mask_id\n        mask_ = imread(mask_path)\n        mask_ = np.expand_dims(resize(mask_, (128, 128), mode='constant', preserve_range=True), axis=-1)\n        mask = np.maximum(mask, mask_)\n    Y_train[n] = mask\n\nprint('Getting testing data for stage 1...')\nsys.stdout.flush()\n\nsizes_test = []\nfor n, id_ in tqdm(enumerate(test_files), total = len(test_files)):\n    img_path = TEST_PATH + id_ + '/images/' + id_ + '.png'\n    img = imread(img_path)[:,:,:3]\n    sizes_test.append([img.shape[0], img.shape[1]])\n    img = resize(img, (128, 128), mode='constant', preserve_range=True)\n    X_test[n] = img\n\nprint('Getting testing data for stage 2...')\nsys.stdout.flush()\n\nsizes_test_2 = []\nfor n, id_ in tqdm(enumerate(test_files_2), total = len(test_files_2)):\n    img_path = TEST_PATH + id_ + '/images/' + id_ + '.png'\n    img = imread(img_path)[:,:,:3]\n    sizes_test_2.append([img.shape[0], img.shape[1]])\n    img = resize(img, (128, 128), mode='constant', preserve_range=True)\n    X_test_2[n] = img\n\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_final = np.concatenate((X_test, X_test_2), axis = 0)\nsizes_test_final = sizes_test + sizes_test_2\nprint(X_test_final.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining Class for Data Augmentation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Nuc_Seg(Dataset):\n    def __init__(self, images_np, masks_np):\n        self.images_np = images_np\n        self.masks_np = masks_np\n    \n    def transform(self, image_np, mask_np):\n        ToPILImage = transforms.ToPILImage()\n        image = ToPILImage(image_np)\n        mask = ToPILImage(mask_np.astype(np.int32))\n        \n        image = TF.pad(image, padding = 20, padding_mode = 'reflect')\n        mask = TF.pad(mask, padding = 20, padding_mode = 'reflect')\n        \n        angle = random.uniform(-10, 10)\n        width, height = image.size\n        max_dx = 0.1 * width\n        max_dy = 0.1 * height\n        translations = (np.round(random.uniform(-max_dx, max_dx)), np.round(random.uniform(-max_dy, max_dy)))\n        scale = random.uniform(0.8, 1.2)\n        shear = random.uniform(-0.5, 0.5)\n        image = TF.affine(image, angle = angle, translate = translations, scale = scale, shear = shear)\n        mask = TF.affine(mask, angle = angle, translate = translations, scale = scale, shear = shear)\n        \n        image = TF.center_crop(image, (128, 128))\n        mask = TF.center_crop(mask, (128, 128))\n        \n        image = TF.to_tensor(image)\n        mask = TF.to_tensor(mask)\n        return image, mask\n        \n    def __len__(self):\n        return len(self.images_np)\n    \n    def __getitem__(self, idx):\n        image_np = self.images_np[idx]\n        mask_np = self.masks_np[idx]\n        image, mask = self.transform(image_np, mask_np)\n        \n        return image, mask    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining DataLoader","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state = seed)\n\ntrain_dataset = Nuc_Seg(X_train, Y_train)\ntrain_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True)\nvalid_dataset = Nuc_Seg(X_val, Y_val)\nvalid_loader = DataLoader(valid_dataset, batch_size = 16, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking One Random Training & Validation Image","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axis = plt.subplots(2, 2)\naxis[0][0].imshow(X_train[0].astype(np.uint8))\naxis[0][1].imshow(np.squeeze(Y_train[0]).astype(np.uint8))\naxis[1][0].imshow(X_val[0].astype(np.uint8))\naxis[1][1].imshow(np.squeeze(Y_val[0]).astype(np.uint8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking Random Augmented Image","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nfor ex_img, ex_mask in train_loader:\n    \n    img = np.array(TF.to_pil_image(ex_img[0]))\n    mask = np.array(TF.to_pil_image(ex_mask[0]))\n    \n    fig, (axis_1, axis_2) = plt.subplots(1, 2)\n    axis_1.imshow(img.astype(np.uint8))\n    axis_2.imshow(mask.astype(np.uint8))\n    \n    break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining IoU Metric","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def iou(pred, target, n_classes = 2):\n    \n    iou = []\n    pred = pred.view(-1)\n    target = target.view(-1)\n\n    # Ignore IoU for background class (\"0\")\n    for cls in range(1, n_classes):  # This goes from 1:n_classes-1 -> class \"0\" is ignored\n      pred_inds = pred == cls\n      target_inds = target == cls\n      intersection = (pred_inds[target_inds]).long().sum().data.cpu().item()  # Cast to long to prevent overflows\n      union = pred_inds.long().sum().data.cpu().item() + target_inds.long().sum().data.cpu().item() - intersection\n    \n      if union == 0:\n        iou.append(float('nan'))  # If there is no ground truth, do not include in evaluation\n      else:\n        iou.append(float(intersection) / float(max(union, 1)))\n     \n    return sum(iou)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def iou_metric(y_pred, y_true, n_classes = 2):\n    miou = []\n    for i in np.arange(0.5, 1.0, 0.05):\n        y_pred_ = (y_pred > i)\n        iou_init = iou(y_pred_, y_true, n_classes = n_classes)\n        miou.append(iou_init)\n    \n    return sum(miou)/len(miou)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining UNet Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class UNet(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        \n        self.conv1_1 = nn.Conv2d(3, 16, kernel_size = 3, padding = 1)\n        self.drop1_1 = nn.Dropout2d(0.1)\n        self.conv1_2 = nn.Conv2d(16, 16, kernel_size = 3, padding = 1)\n        \n        self.conv2_1 = nn.Conv2d(16, 32, kernel_size = 3, padding = 1)\n        self.drop2_1 = nn.Dropout2d(0.1)\n        self.conv2_2 = nn.Conv2d(32, 32, kernel_size = 3, padding = 1)\n        \n        self.conv3_1 = nn.Conv2d(32, 64, kernel_size = 3, padding = 1)\n        self.drop3_1 = nn.Dropout2d(0.2)\n        self.conv3_2 = nn.Conv2d(64, 64, kernel_size = 3, padding = 1)\n        \n        self.conv4_1 = nn.Conv2d(64, 128, kernel_size = 3, padding = 1)\n        self.drop4_1 = nn.Dropout2d(0.2)\n        self.conv4_2 = nn.Conv2d(128, 128, kernel_size = 3, padding = 1)\n        \n        self.conv5_1 = nn.Conv2d(128, 256, kernel_size = 3, padding = 1)\n        self.drop5_1 = nn.Dropout2d(0.3)\n        self.conv5_2 = nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n        \n        self.conv_trans6_1 = nn.ConvTranspose2d(256, 128, kernel_size = 2, stride = (2, 2))\n        self.conv6_1 = nn.Conv2d(256, 128, kernel_size = 3, padding = 1)\n        self.drop6_1 = nn.Dropout2d(0.2)\n        self.conv6_2 = nn.Conv2d(128, 128, kernel_size = 3, padding = 1)\n        \n        self.conv_trans7_1 = nn.ConvTranspose2d(128, 64, kernel_size = 2, stride = (2, 2))\n        self.conv7_1 = nn.Conv2d(128, 64, kernel_size = 3, padding = 1)\n        self.drop7_1 = nn.Dropout2d(0.2)\n        self.conv7_2 = nn.Conv2d(64, 64, kernel_size = 3, padding = 1)\n        \n        self.conv_trans8_1 = nn.ConvTranspose2d(64, 32, kernel_size = 2, stride = (2, 2))\n        self.conv8_1 = nn.Conv2d(64, 32, kernel_size = 3, padding = 1)\n        self.drop8_1 = nn.Dropout2d(0.1)\n        self.conv8_2 = nn.Conv2d(32, 32, kernel_size = 3, padding = 1)\n        \n        self.conv_trans9_1 = nn.ConvTranspose2d(32, 16, kernel_size = 2, stride = (2, 2))\n        self.conv9_1 = nn.Conv2d(32, 16, kernel_size = 3, padding = 1)\n        self.drop9_1 = nn.Dropout2d(0.1)\n        self.conv9_2 = nn.Conv2d(16, 16, kernel_size = 3, padding = 1)\n        \n        self.conv10 = nn.Conv2d(16, 1, kernel_size = 3, padding = 1)\n    \n    def forward(self, s):\n        \n        c1 = F.elu(self.conv1_1(s))\n        c1 = self.drop1_1(c1)\n        c1 = F.elu(self.conv1_2(c1))\n        p1 = F.max_pool2d(c1, kernel_size = (2, 2), stride = 2)\n        \n        c2 = F.elu(self.conv2_1(p1))\n        c2 = self.drop2_1(c2)\n        c2 = F.elu(self.conv2_2(c2))\n        p2 = F.max_pool2d(c2, kernel_size = (2, 2), stride = 2)\n        \n        c3 = F.elu(self.conv3_1(p2))\n        c3 = self.drop3_1(c3)\n        c3 = F.elu(self.conv3_2(c3))\n        p3 = F.max_pool2d(c3, kernel_size = (2, 2), stride = 2)\n        \n        c4 = F.elu(self.conv4_1(p3))\n        c4 = self.drop4_1(c4)\n        c4 = F.elu(self.conv4_2(c4))\n        p4 = F.max_pool2d(c4, kernel_size = (2, 2), stride = 2)\n        \n        c5 = F.elu(self.conv5_1(p4))\n        c5 = self.drop5_1(c5)\n        c5 = F.elu(self.conv5_2(c5))\n        \n        u6 = self.conv_trans6_1(c5)\n        u6 = torch.cat((u6, c4), axis = 1)\n        c6 = F.elu(self.conv6_1(u6))\n        c6 = self.drop6_1(c6)\n        c6 = F.elu(self.conv6_2(c6))\n        \n        u7 = self.conv_trans7_1(c6)\n        u7 = torch.cat((u7, c3), axis = 1)\n        c7 = F.elu(self.conv7_1(u7))\n        c7 = self.drop7_1(c7)\n        c7 = F.elu(self.conv7_2(c7))\n    \n        u8 = self.conv_trans8_1(c7)\n        u8 = torch.cat((u8, c2), axis = 1)\n        c8 = F.elu(self.conv8_1(u8))\n        c8 = self.drop8_1(c8)\n        c8 = F.elu(self.conv8_2(c8))\n        \n        u9 = self.conv_trans9_1(c8)\n        u9 = torch.cat((u9, c1), axis = 1)\n        c9 = F.elu(self.conv9_1(u9))\n        c9 = self.drop9_1(c9)\n        c9 = F.elu(self.conv9_2(c9))\n        \n        output = torch.sigmoid(self.conv10(c9))\n        \n        return output        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = UNet()\nmodel = model.float()\nmodel = model.to(device)\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(total_params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining Optimizer, Callback and Loss Function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"opt = optim.Adam(model.parameters(), lr = 0.001)\nloss_func = nn.BCELoss()\nlr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, patience = 3, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit(model, epochs, opt, loss_func, train_loader, valid_loader, alpha):\n\n    for epoch in range(epochs):\n        \n        #Going into training mode\n        model.train()\n        \n        train_loss = 0 \n        iou = 0\n        \n        for image, mask in train_loader:\n            image = image.to(device)   #Passing the input mini-batch to the GPU\n            mask = mask.to(device)   #Passing the label mini-batch to the GPU\n            opt.zero_grad()      #Setting the grads to zero to avoid accumulation of gradients\n            out = model(image.float())\n            loss = loss_func(out.float(), mask.float())    \n            train_loss += loss\n            \n            iou += iou_metric(out, mask)\n            iou_rev = 16 - iou_metric(out, mask)\n            loss += alpha * iou_rev\n            \n            loss.backward()\n            opt.step()\n        \n        lr_scheduler.step(train_loss/len(train_loader))   #Setting up lr decay  \n        \n        model.eval()            #Going into eval mode                            \n        with torch.no_grad():   #No backprop\n            valid_loss = 0\n            valid_iou = 0\n            \n            for image_val, mask_val in valid_loader:\n                image_val = image_val.to(device)  \n                mask_val = mask_val.to(device)\n                out_val = model(image_val.float())\n                valid_loss += loss_func(out_val.float(), mask_val.float())\n                \n                valid_iou += iou_metric(out_val, mask_val)\n        \n        print(\"Epoch \", epoch + 1, \" Training Loss: \", train_loss/len(train_loader), \"CV Loss: \", valid_loss/len(valid_loader))\n        print(\"Training IoU: \", iou/len(train_loader), \"CV IoU: \", valid_iou/len(valid_loader))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit(model, 30, opt, loss_func, train_loader, valid_loader, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing Random Predicted and True Masks","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nfor ex_img, ex_mask in train_loader:\n    \n    img = ex_img[1].to(device)\n    img.unsqueeze_(0)\n    mask_pred = model(img.float())\n    mask_pred = mask_pred.cpu()\n    mask_pred = (mask_pred > 0.75)\n    mask_true = ex_mask[1]\n    \n    img = TF.to_pil_image(mask_pred.float().squeeze(0))\n    mask = TF.to_pil_image(mask_true)\n    \n    img = np.array(img)\n    mask = np.array(mask)\n    \n    fig, (axis_1, axis_2) = plt.subplots(1, 2)\n    axis_1.imshow(img.astype(np.uint8))\n    axis_2.imshow(mask.astype(np.uint8))\n    \n    break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining RLE Encoding Functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run-length encoding stolen from https://www.kaggle.com/rakhlin/fast-run-length-encoding-python\ndef rle_encoding(x):\n    dots = np.where(x.T.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return run_lengths\n\ndef prob_to_rles(x, cutoff=0.5):\n    lab_img = label(x > cutoff)\n    for i in range(1, lab_img.max() + 1):\n        yield rle_encoding(lab_img == i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions for Stage 1 Testing Images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    \n    test = torch.from_numpy(X_test)\n    test = test/255.0\n    test = test.permute(0, 3, 1, 2)\n    test = test.to(device)\n    preds = model(test.float())\n    preds = preds.permute(0, 2, 3, 1) \n    print(preds.shape)\n\n    preds = preds*255.0\n    preds = preds.cpu().numpy()\n\nprint(preds.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_t = (preds > 0.5).astype(np.uint8)\n\npreds_upsampled = []\nfor i in range(len(preds)):\n    preds_upsampled.append(resize(np.squeeze(preds[i]), \n                                       (sizes_test[i][0], sizes_test[i][1]), \n                                       mode='constant', preserve_range=True))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_test_ids = []\nrles = []\nfor n, id_ in enumerate(test_files):\n    rle = list(prob_to_rles(preds_upsampled[n]))\n    rles.extend(rle)\n    new_test_ids.extend([id_] * len(rle))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stage 1 Submission File","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame()\nsub['ImageId'] = new_test_ids\nsub['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\nsub.to_csv('sub-dsbowl2018-1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions for Stage 2 Testing Images ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    \n    test = torch.from_numpy(X_test_2)\n    test = test/255.0\n    test = test.permute(0, 3, 1, 2)\n    test = test.to(device)\n    preds_2 = model(test.float())\n    preds_2 = preds_2.permute(0, 2, 3, 1) \n    print(preds_2.shape)\n\n    preds_2 = preds_2*255.0\n    preds_2 = preds_2.cpu().numpy()\n\nprint(preds_2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_t_2 = (preds_2 > 0.5).astype(np.uint8)\n\npreds_upsampled_2 = []\nfor i in range(len(preds_2)):\n    preds_upsampled_2.append(resize(np.squeeze(preds_2[i]), \n                                       (sizes_test_2[i][0], sizes_test_2[i][1]), \n                                       mode='constant', preserve_range=True))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_test_ids_2 = []\nrles_2 = []\nfor n, id_ in enumerate(test_files_2):\n    rle = list(prob_to_rles(preds_upsampled_2[n]))\n    rles_2.extend(rle)\n    new_test_ids_2.extend([id_] * len(rle))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stage 2 Submission File","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_2 = pd.DataFrame()\nsub_2['ImageId'] = new_test_ids_2\nsub_2['EncodedPixels'] = pd.Series(rles_2).apply(lambda x: ' '.join(str(y) for y in x))\nsub_2.to_csv('sub-dsbowl2018-2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combined Stage 1 & 2 Predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    \n    test = torch.from_numpy(X_test_final)\n    test = test/255.0\n    test = test.permute(0, 3, 1, 2)\n    test = test.to(device)\n    preds_final = model(test.float())\n    preds_final = preds_final.permute(0, 2, 3, 1) \n    print(preds_final.shape)\n\n    preds_final = preds_final*255.0\n    preds_final = preds_final.cpu().numpy()\n\nprint(preds_final.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_t_final = (preds_final > 0.5).astype(np.uint8)\n\npreds_upsampled_final = []\nfor i in range(len(preds_final)):\n    preds_upsampled_final.append(resize(np.squeeze(preds_final[i]), \n                                       (sizes_test_final[i][0], sizes_test_final[i][1]), \n                                       mode='constant', preserve_range=True))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_test_ids_final = []\nrles_final = []\nfor n, id_ in enumerate(test_files_final):\n    rle = list(prob_to_rles(preds_upsampled_final[n]))\n    rles_final.extend(rle)\n    new_test_ids_final.extend([id_] * len(rle))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combined Stage 1 & 2 File Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_final = pd.DataFrame()\nsub_final['ImageId'] = new_test_ids_2\nsub_final['EncodedPixels'] = pd.Series(rles_final).apply(lambda x: ' '.join(str(y) for y in x))\nsub_final.to_csv('sub-dsbowl2018-final.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}