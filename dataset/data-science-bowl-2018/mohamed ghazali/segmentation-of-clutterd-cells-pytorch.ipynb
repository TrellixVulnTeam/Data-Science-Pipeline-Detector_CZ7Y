{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /tmp/pip/cache/\n!cp ../input/segmentationmodelspytorch/segmentation_models/efficientnet_pytorch-0.6.3.xyz /tmp/pip/cache/efficientnet_pytorch-0.6.3.tar.gz\n!cp ../input/segmentationmodelspytorch/segmentation_models/pretrainedmodels-0.7.4.xyz /tmp/pip/cache/pretrainedmodels-0.7.4.tar.gz\n!cp ../input/segmentationmodelspytorch/segmentation_models/segmentation-models-pytorch-0.1.2.xyz /tmp/pip/cache/segmentation_models_pytorch-0.1.2.tar.gz\n!cp ../input/segmentationmodelspytorch/segmentation_models/timm-0.1.20-py3-none-any.whl /tmp/pip/cache/\n!cp ../input/segmentationmodelspytorch/segmentation_models/timm-0.2.1-py3-none-any.whl /tmp/pip/cache/\n!pip install --no-index --find-links /tmp/pip/cache/ efficientnet-pytorch\n!pip install --no-index --find-links /tmp/pip/cache/ segmentation-models-pytorch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport shutil\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nfrom torchvision import transforms, utils\nfrom torch import nn\nimport albumentations as A\nfrom albumentations.pytorch import ToTensor\n#import tqdm as tqdm\nfrom tqdm import tqdm as tqdm\n\nfrom torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\nfrom torch.optim import Adam, SGD\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom albumentations import (HorizontalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise)\nimport cv2\nfrom zipfile import ZipFile\n\nimport random\n\nfrom segmentation_models_pytorch.unet import Unet\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /root/.cache/torch/hub/checkpoints/\n!cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b0-355c32eb.pth /root/.cache/torch/hub/checkpoints/\n!cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b1-f1951068.pth /root/.cache/torch/hub/checkpoints/\n!cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b2-8bb594d6.pth /root/.cache/torch/hub/checkpoints/\n!cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b3-5fb5a3c3.pth /root/.cache/torch/hub/checkpoints/\n!cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b4-6ed6700e.pth /root/.cache/torch/hub/checkpoints/\n!cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b5-b6417697.pth /root/.cache/torch/hub/checkpoints/\n!cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b6-c76e70fd.pth /root/.cache/torch/hub/checkpoints/\n!cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b7-dcc49843.pth /root/.cache/torch/hub/checkpoints/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extracting Train zip file\nfile_name = \"../input/data-science-bowl-2018/stage1_train.zip\"\nwith ZipFile(file_name, 'r') as zip: \n    print('Extracting the files') \n    zip.extractall(\"stage1_train\") \n    print('ok!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extracting Test zip file\nfile_name = \"../input/data-science-bowl-2018/stage1_test.zip\"\nwith ZipFile(file_name, 'r') as zip: \n    print('Extracting the files') \n    zip.extractall(\"stage1_test\") \n    print('ok!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_PATH = 'stage1_train/'\nTEST_PATH = 'stage1_test/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load Datasets\n#--------------------------#\n#Albumentation\ndef get_train_transform():\n    return A.Compose(\n       [\n        A.Resize(256, 256),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        A.HorizontalFlip(p=0.25),\n        A.VerticalFlip(p=0.25),\n        ToTensor()\n        ])\n##Dataset Loader\nclass LoadDataSet(Dataset):\n        def __init__(self,path, transform=None):\n            self.path = path\n            self.folders = os.listdir(path)\n            self.transforms = get_train_transform()\n            \n        \n        def __len__(self):\n            return len(self.folders)\n              \n        \n        def __getitem__(self,idx):\n            ### link of image\n            image_folder = os.path.join(self.path,self.folders[idx],'images/')\n            img_path = os.path.join(image_folder,os.listdir(image_folder)[0])\n            \n            ### read image and resize via transform image 128x128\n            img = io.imread(img_path)[:,:,:3].astype('float32')\n            img = transform.resize(img,(128,128))\n            \n            ### link of masks\n            if self.path == 'stage1_train/' :\n                mask_folder = os.path.join(self.path,self.folders[idx],'masks/')\n                ### read mask and resize via transform image 128x128 and return for masks max\n                mask = np.zeros((128, 128, 1))\n                for mask_ in os.listdir(mask_folder):\n                    path_mask_ = os.path.join(mask_folder,mask_)\n                    mask_ = io.imread(path_mask_)\n                    mask_ = transform.resize(mask_,(128,128))\n                    mask_ = np.expand_dims(mask_,axis=-1)\n                    mask = np.maximum(mask,mask_)\n                mask = mask.astype('float32')\n\n                # agmunted img ans masks via transfomer :get_train_transform()\n\n                agmuented = self.transforms(image=img,mask=mask)\n                img = agmuented['image']\n                mask = agmuented['mask']\n                mask = mask[0].permute(2,0,1)\n\n                return img,mask\n             # agmunted img ans masks via transfomer :get_train_transform()\n\n            agmuented = self.transforms(image=img)\n            img = agmuented['image']\n             \n                \n            return img\n               \n            \n            \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = LoadDataSet(TRAIN_PATH, transform=get_train_transform())\nimage , mask = train_dataset.__getitem__(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('train',image.shape)\nprint('train',mask.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = LoadDataSet(TEST_PATH, transform=get_train_transform())\nimage_test = test_dataset.__getitem__(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('test',image_test.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.__len__()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset.__len__()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def imshow(img,mask):\n    img = img / 2 + 0.5     # unnormalize\n    mask = mask / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(npimg.T)\n    plt.show()\n    plt.imshow(mask.T, interpolation=\"nearest\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imshow(image,mask)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_image(img):\n    img = np.array(np.transpose(img, (1,2,0)))\n    mean=np.array((0.485, 0.456, 0.406))\n    std=np.array((0.229, 0.224, 0.225))\n    img  = std * img + mean\n    img = img*255\n    img = img.astype(np.uint8)\n    return img\ndef format_mask(mask):\n    mask = np.squeeze(np.transpose(mask, (1,2,0)))\n    return mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"format_image()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize images as well as masks\ndef visualize_dataset(n_images, predict=None):\n    images = random.sample(range(0, 670), n_images)\n    figure, ax = plt.subplots(nrows=len(images), ncols=2, figsize=(5, 8))\n\n    for i in range(0, len(images)):\n        img_no = images[i]\n        print(images[i])\n        image, mask = train_dataset.__getitem__(img_no)\n        image = format_image(image)\n        mask = format_mask(mask)\n        ax[i, 0].imshow(image)\n        ax[i, 1].imshow(mask, interpolation=\"nearest\")\n        ax[i, 0].set_title(\"Ground Truth Image\"+str(i))\n        ax[i, 1].set_title(\"Mask\")\n        ax[i, 0].set_axis_off()\n        ax[i, 1].set_axis_off()\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_dataset(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize images as well as masks\ndef visualize_dataset_test(n_images, predict=None):\n    images = random.sample(range(0, 65), n_images)\n    figure, ax = plt.subplots(nrows=len(images), ncols=2, figsize=(5, 8))\n\n    for i in range(0, len(images)):\n        img_no = images[i]\n        image = test_dataset.__getitem__(img_no)\n        image = format_image(image)\n#         mask = format_mask(mask)\n        ax[i, 0].imshow(image)\n#         ax[i, 1].imshow(mask, interpolation=\"nearest\")\n        ax[i, 0].set_title(\"Ground Truth Image\"+str(i))\n#         ax[i, 1].set_title(\"Mask\")\n        ax[i, 0].set_axis_off()\n#         ax[i, 1].set_axis_off()\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_dataset_test(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Subset\nfrom sklearn.model_selection import train_test_split\ndef train_val_dataset(dataset,val_split=0.25):\n    train_idx, val_idx = train_test_split(list(range(len(train_dataset))), test_size=val_split)\n    datasets = {}\n    datasets['train'] = Subset(train_dataset, train_idx)\n    datasets['val'] = Subset(train_dataset, val_idx)\n    return datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_dataset))\ndatasets = train_val_dataset(train_dataset)\ntrain_data = datasets['train']\nvalid_data = datasets['val']\nprint(\"Length of train and valid datas: {}, {}\".format(len(train_data), len(valid_data)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(dataset=train_data, batch_size=10, shuffle=True)\n\nval_loader = DataLoader(dataset=valid_data, batch_size=10)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=10,shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA , TARGET = next(iter(train_loader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA[4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TARGET[4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nimshow(torchvision.utils.make_grid(DATA[4]),torchvision.utils.make_grid(TARGET[4]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##########  Model #######","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLUTTERDCELLS(nn.Module):\n    def __init__(self):\n        super(CLUTTERDCELLS, self).__init__()\n        self.cnn_model = Unet('efficientnet-b6', encoder_weights=\"imagenet\", classes=1, activation=None)\n        #self.cnn_model.decoder.blocks.append(self.cnn_model.decoder.blocks[-1])\n        #self.cnn_model.decoder.blocks[-2] = self.cnn_model.decoder.blocks[-3]\n    \n    def forward(self, imgs):\n        img_segs = self.cnn_model(imgs)\n        \n        return img_segs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CLUTTERDCELLS().to(device)\noptimizer = torch.optim.Adam(model.parameters(),lr = 1e-3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model(DATA.to(device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(DATA.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####### LOsss #######\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DiceBCELoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceBCELoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()                            \n        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n        Dice_BCE = BCE + dice_loss\n        \n        return Dice_BCE\n\n\nclass DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        #inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()                            \n        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        \n        return 1 - dice\n\n\nclass IoU(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(IoU, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #intersection is equivalent to True Positive count\n        #union is the mutually inclusive area of all labels & predictions \n        intersection = (inputs * targets).sum()\n        total = (inputs + targets).sum()\n        union = total - intersection \n        \n        IoU = (intersection + smooth)/(union + smooth)\n                \n        return IoU","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_ckp(state, is_best, checkpoint_path, best_model_path):\n    \"\"\"\n    state: checkpoint we want to save\n    is_best: is this the best checkpoint; min validation loss\n    checkpoint_path: path to save checkpoint\n    best_model_path: path to save best model\n    \"\"\"\n    f_path = checkpoint_path\n    # save checkpoint data to the path given, checkpoint_path\n    torch.save(state, f_path)\n    # if it is a best model, min validation loss\n    if is_best:\n        best_fpath = best_model_path\n        # copy that checkpoint file to best path given, best_model_path\n        shutil.copyfile(f_path, best_fpath)\n\ndef load_ckp(checkpoint_fpath, model, optimizer):\n    \"\"\"\n    checkpoint_path: path to save checkpoint\n    model: model that we want to load checkpoint parameters into       \n    optimizer: optimizer we defined in previous training\n    \"\"\"\n    # load check point\n    checkpoint = torch.load(checkpoint_fpath)\n    # initialize state_dict from checkpoint to model\n    model.load_state_dict(checkpoint['state_dict'])\n    # initialize optimizer from checkpoint to optimizer\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    # initialize valid_loss_min from checkpoint to valid_loss_min\n    valid_loss_min = checkpoint['valid_loss_min']\n    # return model, optimizer, epoch value, min validation loss \n    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not os.path.exists(\"model\"):\n    os.makedirs(\"model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#####\" train ########\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_path = 'model/chkpoint_'\nbest_model_path = 'model/bestmodel.pt'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from engine import evaluate\ncriterion = DiceBCELoss()\naccuracy_metric = IoU()\nnum_epochs=20\nvalid_loss_min = np.Inf\n\ntotal_train_loss = []\ntotal_train_score = []\ntotal_valid_loss = []\ntotal_valid_score = []\nlosses_value = 0\nfor epoch in range(num_epochs):\n  \n    train_loss = []\n    train_score = []\n    valid_loss = []\n    valid_score = []\n #<-----------Training Loop---------------------------->\n    pbar = tqdm(train_loader, desc = 'description')\n    for x_train, y_train in pbar:\n        x_train = torch.autograd.Variable(x_train).to(device)\n        y_train = torch.autograd.Variable(y_train).to(device)\n        optimizer.zero_grad()\n        output = model(x_train).to(device)\n      #Loss\n        loss = criterion(output, y_train)\n        losses_value = loss.item()\n      #Score\n        score = accuracy_metric(output,y_train)\n        loss.backward()\n        optimizer.step()\n        train_loss.append(losses_value)\n        train_score.append(score.item())\n      #train_score.append(score)\n        pbar.set_description(f\"Epoch: {epoch+1}, loss: {losses_value:.4f}, IoU: {score:4f}\")\n#     total_train_loss.append(np.mean(train_loss))\n#     total_train_score.append(np.mean(train_score))\n#     print(f\"\\n###############Train Loss: {total_train_loss[-1]:.4f}, Train IOU: {total_train_score[-1]:.4f}###############\")\n\n    #<---------------Validation Loop---------------------->\n    with torch.no_grad():\n        for image,mask in val_loader:\n            image = torch.autograd.Variable(image).cuda()\n            mask = torch.autograd.Variable(mask).cuda()\n            output = model(image)\n            ## Compute Loss Value.\n            loss = criterion(output, mask)\n            losses_value = loss.item()\n            ## Compute Accuracy Score\n            score = accuracy_metric(output,mask)\n            valid_loss.append(losses_value)\n            valid_score.append(score.item())\n\n    total_train_loss.append(np.mean(train_loss))\n    total_train_score.append(np.mean(train_score))\n    total_valid_loss.append(np.mean(valid_loss))\n    total_valid_score.append(np.mean(valid_score))\n    print(f\"\\n###############Train Loss: {total_train_loss[-1]:.4f}, Train IOU: {total_train_score[-1]:.4f}###############\")\n    print(f\"###############Valid Loss: {total_valid_loss[-1]:.4f}, Valid IOU: {total_valid_score[-1]:.4f}###############\")\n    \n    #Save best model Checkpoint\n    # create checkpoint variable and add important data\n    checkpoint = {\n        'epoch': epoch + 1,\n        'valid_loss_min': total_valid_loss[-1],\n        'state_dict': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n    }\n    \n    # save checkpoint\n    save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n    \n    ## TODO: save the model if validation loss has decreased\n    if total_valid_loss[-1] <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,total_valid_loss[-1]))\n        # save checkpoint as best model\n        save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n        valid_loss_min = total_valid_loss[-1]\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\nplt.figure(1)\nplt.figure(figsize=(15,5))\nsns.set_style(style=\"darkgrid\")\nplt.subplot(1, 2, 1)\nsns.lineplot(x=range(1,num_epochs+1), y=total_train_loss, label=\"Train Loss\")\nsns.lineplot(x=range(1,num_epochs+1), y=total_valid_loss, label=\"Valid Loss\")\nplt.title(\"Loss\")\nplt.xlabel(\"epochs\")\nplt.ylabel(\"DiceBCELoss\")\n\nplt.subplot(1, 2, 2)\nsns.lineplot(x=range(1,num_epochs+1), y=total_train_score, label=\"Train Score\")\nsns.lineplot(x=range(1,num_epochs+1), y=total_valid_score, label=\"Valid Score\")\nplt.title(\"Score (IoU)\")\nplt.xlabel(\"epochs\")\nplt.ylabel(\"IoU\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loading the saved model\nmodel, optimizer, start_epoch, valid_loss_min = load_ckp(checkpoint_path, model, optimizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_predict(model, n_images):\n  #model = model.eval()\n  figure, ax = plt.subplots(nrows=n_images, ncols=3, figsize=(15, 18))\n  with torch.no_grad():\n    for data,mask in val_loader:\n        data = torch.autograd.Variable(data, volatile=True).cuda()\n        mask = torch.autograd.Variable(mask, volatile=True).cuda()\n        o = model(data)\n        o= F.sigmoid(o)\n        break\n  for img_no in range(0, n_images):\n    tm=o[img_no][0].data.cpu().numpy()\n    img = data[img_no].data.cpu()\n    msk = mask[img_no].data.cpu()\n    img = format_image(img)\n    msk = format_mask(msk)\n    ax[img_no, 0].imshow(img)\n    ax[img_no, 1].imshow(msk, interpolation=\"nearest\")\n    ax[img_no, 2].imshow(tm, interpolation=\"nearest\")\n    ax[img_no, 0].set_title(\" Image\")\n    ax[img_no, 1].set_title(\" Mask Image\")\n    ax[img_no, 2].set_title(\"Predicted Mask\")\n    ax[img_no, 0].set_axis_off()\n    ax[img_no, 1].set_axis_off()\n    ax[img_no, 2].set_axis_off()\n  plt.tight_layout()\n  plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_predict(model, 4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_predict_test(model, n_images):\n  #model = model.eval()\n  figure, ax = plt.subplots(nrows=n_images, ncols=2, figsize=(15, 18))\n  with torch.no_grad():\n    for data in test_loader:\n        data = torch.autograd.Variable(data, volatile=True).cuda()\n#         mask = torch.autograd.Variable(mask, volatile=True).cuda()\n        o = model(data)\n        o= F.sigmoid(o)\n        break\n  for img_no in range(0, n_images):\n    tm=o[img_no][0].data.cpu().numpy()\n    img = data[img_no].data.cpu()\n#     msk = mask[img_no].data.cpu()\n    img = format_image(img)\n#     msk = format_mask(msk)\n    ax[img_no, 0].imshow(img)\n#     ax[img_no, 1].imshow(msk, interpolation=\"nearest\")\n    ax[img_no, 1].imshow(tm, interpolation=\"nearest\")\n    ax[img_no, 0].set_title(\" Image\")\n#     ax[img_no, 1].set_title(\" Mask Image\")\n    ax[img_no, 1].set_title(\"Predicted Mask\")\n    ax[img_no, 0].set_axis_off()\n    ax[img_no, 1].set_axis_off()\n#     ax[img_no, 2].set_axis_off()\n  plt.tight_layout()\n  plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_predict_test(model,10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}