{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimport numpy as np\nimport os\nimport pandas as pd\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport seaborn as sns\n\nplt.rcParams['figure.figsize']=(20,10)\nprint(os.listdir(\"../input\"))\npy.init_notebook_mode(connected=False)\n\n%env JOBLIB_TEMP_FOLDER=/tmp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 1: Import the Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/labels.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['breed'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Explore the Data\n### Let's visualize the data, get an intuition of the distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = pd.DataFrame({'breed': df['breed'].value_counts().index, 'instances': df['breed'].value_counts().values})\ntemp = temp.sort_values(by=['breed'])\ntemp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace = go.Bar(x=temp['breed'], y=temp['instances'])\ndata = [trace]\nlayout = go.Layout(\n        title='Breed Counts',\n        autosize=False,\n        width=5000,\n        height=500,\n        margin=dict(\n            l=100,\n            r=100,\n            b=100,\n            t=100\n        )\n    )\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, almost all the breeds have sufficient enough training images. Hence, we won't have to generate additional data right now."},{"metadata":{},"cell_type":"markdown","source":"### Step 3: One-Hot Encode the Breed columns and prepare the data for a classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['breed'] = pd.Categorical(df['breed'])\ndf['breed'] = df['breed'].cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's create the train data by creating a torch dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.utils.data.dataset import Dataset\nimport torch.nn.functional as F\nfrom PIL import Image\n\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch.optim as optim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['image_path'] = '../input/train/' + df['id'].astype(str) + '.jpg'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = torch.tensor(df['breed'].tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, image_path, labels=[], train=True):\n        self.image_path = image_path\n        self.labels = labels\n        self.transform = transforms.Compose([\n                        transforms.Resize(255),\n                        transforms.CenterCrop(224),\n                        transforms.ToTensor(),\n                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                    ])\n\n    def __getitem__(self, index):\n        image = Image.open(self.image_path[index])\n        t_image = self.transform(image)\n        \n        if len(self.labels) > 0:\n            return t_image, self.labels[index]\n        return t_image\n\n    def __len__(self):  # return count of sample we have\n        return len(self.image_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = CustomDataset(df['image_path'], labels, train=True)\n# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)\nbatch_size = 32\nvalidation_split = .2\nshuffle_dataset = True\nrandom_seed= 42\n\ndataset_size = len(dataset)\nindices = list(range(dataset_size))\nsplit = int(np.floor(validation_split * dataset_size))\n\nif shuffle_dataset :\n    np.random.seed(random_seed)\n    np.random.shuffle(indices)\ntrain_indices, val_indices = indices[split:], indices[:split]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sampler = SubsetRandomSampler(train_indices)\nvalid_sampler = SubsetRandomSampler(val_indices)\n\ntrain_loader = DataLoader(dataset, batch_size=batch_size, \n                                           sampler=train_sampler)\nvalidation_loader = DataLoader(dataset, batch_size=batch_size,\n                                                sampler=valid_sampler)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=0)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=1, padding=0)\n        self.conv2 = nn.Conv2d(16, 64, kernel_size=5, stride=1, padding=0)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=1, padding=0)\n        self.fc1 = nn.Linear(216*216*64, 1024)\n        self.fc2 = nn.Linear(1024, 512)\n        self.fc3 = nn.Linear(512, 120)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = x.view(-1, 64*216*216)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def outputSize(in_size, kernel_size, stride, padding):\n    output = int((in_size - kernel_size + 2*(padding)) / stride) + 1\n    return(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ot1 = outputSize(224, 3, 1, 0)\not2 = outputSize(ot1, 2, 1, 0)\not3 = outputSize(ot2, 5, 1, 0)\not4 = outputSize(ot3, 2, 1, 0)\not4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Classifier()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Move model to the device specified above\nmodel.to(device)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.NLLLoss()\n# Set the optimizer function using torch.optim as optim library\noptimizer = optim.Adam(model.parameters())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 10\nfor epoch in range(epochs):\n    train_loss = 0\n    val_loss = 0\n    accuracy = 0\n    \n    # Training the model\n    model.train()\n    counter = 0\n    for inputs, labels in train_loader:\n        # Move to device\n        inputs, labels = inputs.to(device), labels.to(device)\n        if epoch == 0:\n                print('IMG1: ', inputs[0].shape)\n        # Clear optimizers\n        optimizer.zero_grad()\n        # Forward pass\n        output = model.forward(inputs)\n        # Loss\n        loss = criterion(output, labels)\n        # Calculate gradients (backpropogation)\n        loss.backward()\n        # Adjust parameters based on gradients\n        optimizer.step()\n        # Add the loss to the training set's rnning loss\n        train_loss += loss.item()*inputs.size(0)\n        \n        # Print the progress of our training\n        counter += 1\n        #print(counter, \"/\", len(train_loader))\n        \n        # Evaluating the model\n    model.eval()\n    counter = 0\n    # Tell torch not to calculate gradients\n    with torch.no_grad():\n        for inputs, labels in validation_loader:\n            # Move to device\n            inputs, labels = inputs.to(device), labels.to(device)\n            # Forward pass\n            output = model.forward(inputs)\n            # Calculate Loss\n            valloss = criterion(F.log_softmax(output), labels)\n            # Add loss to the validation set's running loss\n            val_loss += valloss.item()*inputs.size(0)\n            \n            # Since our model outputs a LogSoftmax, find the real \n            # percentages by reversing the log function\n            output = torch.exp(output)\n            # Get the top class of the output\n            top_p, top_class = output.topk(1, dim=1)\n            # See how many of the classes were correct?\n            equals = top_class == labels.view(*top_class.shape)\n            # Calculate the mean (get the accuracy for this batch)\n            # and add it to the running accuracy for this epoch\n            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n            \n            # Print the progress of our evaluation\n            counter += 1\n            #print(counter, \"/\", len(val_loader))\n    \n    # Get the average loss for the entire epoch\n    train_loss = train_loss/len(train_loader.dataset)\n    valid_loss = val_loss/len(validation_loader.dataset)\n    # Print out the information\n    print('Accuracy: ', accuracy/len(validation_loader))\n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Saving the model\n# checkpoint = {'model': model,\n#               'state_dict': model.state_dict(),\n#               'optimizer' : optimizer.state_dict()}\n\n# torch.save(checkpoint, 'model1_checkpoint.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_checkpoint(filepath):\n    checkpoint = torch.load(filepath)\n    model = checkpoint['model']\n    model.load_state_dict(checkpoint['state_dict'])\n    for parameter in model.parameters():\n        parameter.requires_grad = False\n    \n    model.eval()\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = load_checkpoint('model1_checkpoint.pth')\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_image_paths = '../input/test/' + df_submission['id'].astype(str) + '.jpg'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = CustomDataset(test_image_paths, train=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_labels = []\nwith torch.no_grad():\n    model.cuda()\n    model.eval()\n    for images in test_loader:\n        images.cuda()\n        output = model.forward(images)\n        output = torch.exp(output)\n        print('OUTPUT :', output)\n        pred_labels.append(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(pred_labels)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}