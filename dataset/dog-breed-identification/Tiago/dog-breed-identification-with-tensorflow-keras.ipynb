{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Identify Dog Breeds\n\nThis notebook will be use to test different pre-trained architectures, fine-tunning and mainly to study how convolution neural nets will handle the data to make a prediction.","metadata":{}},{"cell_type":"markdown","source":"### Create a folder to save models and other necessary files","metadata":{}},{"cell_type":"code","source":"%rm -r saved_models\n%mkdir saved_models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports\nHere I import all the stuff that I will use in this notebook","metadata":{}},{"cell_type":"code","source":"# Install gdown\n!pip install gdown\n\nimport os # Iterate through the directories/files\nimport gdown # Download my trained models from GDrive\nimport numpy as np\nimport pandas as pd # Work with the labels.csv\nimport tensorflow as tf # Preprocess data, train and inference models\nimport tensorflow_hub as hub # Get the pretrained models\nfrom tensorflow.keras.applications import MobileNet # MobileNetV1\nfrom timeit import default_timer as timer # Compute the train time\nfrom tqdm import tqdm # Fancy progress bars made easy \nfrom IPython.display import FileLink # Download files from kaggle","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load DataFrame\nLoad all the CSV with the name of the images for training","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/dog-breed-identification/labels.csv\")\n\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The id is the name of the images on `train` folder, perhaps they doesn't have the .jpg extension. Here I add it.","metadata":{}},{"cell_type":"code","source":"# Adicionar extensão às imagens\ndf[\"id\"] = df[\"id\"].apply(lambda x: str(x)+\".jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Available Models\n\nAn easy way to have the models and the correct input sizes","metadata":{}},{"cell_type":"code","source":"# Dict with all the models from Tf Hub those I used on my tests\nALL_HUB_MODELS = {\n    \"InceptionV3\": {\n        \"url\": \"https://tfhub.dev/google/imagenet/inception_v3/feature_vector/4\",\n        \"size\": 299\n    },\n    \"EfficientNetB2\":{\n        \"url\": \"https://tfhub.dev/google/efficientnet/b2/feature-vector/1\",\n        \"size\": 260\n    },\n    \"EfficientNetB2_Trainable_Tf2\":{\n        \"url\": \"https://tfhub.dev/tensorflow/efficientnet/b2/feature-vector/1\",\n        \"size\": 260\n    },\n    \"EfficientNetB5\":{\n        \"url\": \"https://tfhub.dev/google/efficientnet/b5/feature-vector/1\",\n        \"size\": 456\n    },\n    \"EfficientNetB5_Trainable_Tf2\":{\n        \"url\": \"https://tfhub.dev/tensorflow/efficientnet/b5/feature-vector/1\",\n        \"size\": 456\n    },\n    \"MobileNetV2\":{\n        \"url\": \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\",\n        \"size\": 224\n    },\n}\n\nALL_TF_MODELS = {\n\n    \"MobileNet\": {\n        \"model\": MobileNet(input_shape=(224,224,3), include_top=False),\n        \"size\": 224\n    }\n}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Constants\n\nHere I define all the constants used on the notebook","metadata":{}},{"cell_type":"code","source":"# Choose the model to use\nCUR_MODEL = ALL_HUB_MODELS[\"EfficientNetB2_Trainable_Tf2\"]\n\n# Base directory\nBASE_DIR = os.path.abspath(os.path.dirname(\".\"))\n\n# A small way to tell the tensorflow that I want to use AUTOTUNE\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n# Batch size for training and validation subset\nBATCH_SIZE = 64\n\n# Number of epochs\nEPOCHS = 20\n\n# Portion of train data used in validation\nVAL_SPLIT = .30\n\n# This will be used to know how many epochs remain\n# after a break in training\nEPOCHS_CHANGED = EPOCHS\n\n# URL for Tf Hub model\nMODULE_HANDLE = CUR_MODEL[\"url\"]\n\n# Tuple with the \"correct\" size of the input (images)\n# for the chosen model\nIMG_SIZE=(CUR_MODEL[\"size\"] , CUR_MODEL[\"size\"] )\n\n\n# Boolean to know if we already did the warmup step\nWARMUP_DONE = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classes and functions to help in my adventure\nHere will be all the function and classes that will helping me processing and handling data and models.","metadata":{}},{"cell_type":"code","source":"class SaveModel(tf.keras.callbacks.Callback):\n    \"\"\"\n        For each epoch tgis callback will called saving the model\n        and writing the current epoch on a file\n\n    \"\"\"\n\n    def __init__(self, warmup):\n        super(SaveModel, self).__init__()\n\n        self.warmup = warmup\n    \n    def on_epoch_end(self, epoch, logs=None):\n        \n        self.model.save(\"saved_models/warm_up.h5\")\n\n        with open(\"saved_models/epochs.txt\", \"a+\") as f:\n            \n            # If it is the first epoch lets verify it is\n            # a warmup or the real train\n            if epoch == 0:\n                if self.warmup:\n                    f.write(\"Warmup:\\n\")\n\n                elif not self.warmup:\n                    f.write(\"Train:\\n\")\n                    \n            f.write(str(epoch+1)+\"\\n\")\n\n\n            \ndef write_2_file(filename, content):\n    \"\"\"\n        Function that write text on a given file\n\n\n        Parameters\n        -----------\n        filename: str\n            Path to the file\n        \n        content: str\n            Text to write on file\n    \"\"\"\n    \n    with open(filename, \"a+\") as f:\n        f.write(content)\n    \n    return\n\n\ndef scheduler(epoch, lr):\n    \"\"\"\n        Função que a cada 2 epochs diminui o learning rate\n        para uma aprendizagem mais lenta\n        Function that decrease the learning rate by a factor of 1/2\n        with a 2 epochs step\n\n        Parameters\n        -----------\n        epoch: int\n            Current epoch\n        \n        lr: float\n            Current learning rate\n\n        Return\n        --------\n        return: float\n            New learning rate\n    \"\"\"\n    print(lr)\n\n    if epoch % 10 == 0 and epoch != 0:\n        lr *= 1./2\n    \n    return lr\n\n\ndef train(model, train_gen, steps, val_gen, val_steps, epochs, callback, warmup):\n\n    \"\"\"\n        Train the model\n\n        Parameters\n        -----------\n        model: Keras Model\n            Model created with Keras\n        \n        train_gen: Keras ImageDataGenerator\n            Object that will create new transformed images\n            based on the original, to train\n        \n        steps: int\n            Steps per epoch during train\n\n        val_gen: Keras ImageDataGenerator\n            Object that will create new transformed images\n            based on the original, to validate\n        \n        val_steps: int\n            Steps per epoch during validation\n\n        epochs: int\n            Number of epochs\n        \n        callback: Keras Callback\n            Object with the callback\n        \n        warmup: bool\n            Define if it is the warmup train or not\n        \n\n        Return\n        --------\n        return: dict\n            Dict with all the metrics computed during the train\n    \"\"\"\n\n    # List of callbacks, the SaveModel callback is always set\n    callbacks = [SaveModel(warmup)]\n\n    if callback is not None:\n        callbacks.append(callback)\n\n    # Train and save the computed metrics\n    hist = model.fit(\n        train_gen,\n        epochs=epochs,\n        steps_per_epoch=steps,\n        validation_data=val_gen,\n        validation_steps=val_steps,\n        callbacks=callbacks\n    )\n\n    return hist\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DataLoader\nLet's preprocess and prepare all the data for training","metadata":{}},{"cell_type":"code","source":"# Arguments for the data generator and data flow\n# Here we define the portion of split data and rescale the RGB values\n# from 0-255 to 0-1\ndatagen_kwargs = dict(rescale=1./255, validation_split=VAL_SPLIT)\ndataflow_kwargs = dict(target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n                       interpolation=\"bilinear\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Image Generator for validation (here we don't apply any transformation)\nval_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_kwargs)\n\n\n# Image flow for validation (here we load and prepare our images from the DataFrame)\nval_gen = val_datagen.flow_from_dataframe(\n    df,\n    directory=\"../input/dog-breed-identification/train\",\n    x_col=\"id\",\n    y_col=\"breed\",\n    subset=\"validation\",\n    save_format=\"jpg\",\n    shuffle=False,\n    **dataflow_kwargs\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Image Generator for training (here we have DataAugmentation)\ntrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rotation_range=45,\n    horizontal_flip=True,\n    width_shift_range=0.4,\n    height_shift_range=0.4,\n    shear_range=0.4,\n    zoom_range=0.4,\n    **datagen_kwargs\n)\n\n\n# Image Flow for training (again we prepare and load the images for training)\ntrain_gen = train_datagen.flow_from_dataframe(\n    df,\n    directory=\"../input/dog-breed-identification/train\",\n    x_col=\"id\",\n    y_col=\"breed\",\n    subset=\"training\",\n    shuffle=True,\n    **dataflow_kwargs\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Verify if we want to use my custom decay learning rate callback\nresp = \"\"\ncallback = None\n\n\"\"\"\nwhile resp != \"1\" and resp != \"2\" and resp != \"3\":\n    resp = input(\"No callback/scheduler [1/2]: \")\n\n    if resp == \"2\":\n        callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n\n        \n# Verify if we want to warm up before the real train\nresp = \"\"\n\nwhile resp != \"y\" and resp != \"n\":\n    resp = input(\"Warm up or not [y/n]: \")\"\"\"\n\nresp = \"n\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we calculate the number of steps per epoch using the formula: $$\\left \\lfloor{\\frac{number\\ images}{batch\\ size}}\\right \\rfloor$$","metadata":{}},{"cell_type":"code","source":"steps_per_epoch = train_gen.samples // train_gen.batch_size\nval_steps = val_gen.samples // train_gen.batch_size\n\nsteps_per_epoch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model creation\n\nLet's import our feature extractor model and add the new high level layers","metadata":{}},{"cell_type":"code","source":"# Main Model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.InputLayer(input_shape=IMG_SIZE + (3,)),\n    hub.KerasLayer(MODULE_HANDLE, trainable=False),\n    tf.keras.layers.Dense(512),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(len(train_gen.class_indices), activation=\"softmax\")\n])\n\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n    loss=tf.keras.losses.CategoricalCrossentropy(),\n    metrics=[\"accuracy\"]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"# Verify if we want to train\nisTrain = \"\"\n\n\"\"\"\nwhile isTrain != \"y\" and isTrain != \"n\":\n    isTrain = input(\"Do you want to train? [y/n]:\")\n\"\"\"\n\nisTrain = \"y\"\n\nif isTrain == \"y\":\n    \n    # Verify if we have selected to warmup\n    if resp == \"y\":\n\n        # VVerify if warmup was interrupted\n        if os.path.isfile(\"saved_models/epochs.txt\"):\n            with open(\"saved_models/epochs.txt\", \"r\") as f:\n\n                lines = f.read().splitlines()\n\n                if \"Train:\" in lines:\n                    # Warmup Done\n                    WARMUP_DONE = True\n\n                # Update number of epochs\n                last_epoch = lines[-1]\n                EPOCHS_CHANGED = EPOCHS - int(last_epoch)\n\n\n        print(\"WARMUP BEGUN!\")\n\n        # Start counter for compute execution time\n        start = timer()\n\n        # If the warmup are not done, let's continue the warmup\n        if WARMUP_DONE == False:\n\n            # If we have the model saved, load it\n            if os.path.isfile(\"saved_models/warm_up.h5\"):\n                model = tf.keras.models.load_model(\"saved_models/warm_up.h5\",\n                                           custom_objects={\"KerasLayer\": hub.KerasLayer})\n\n            # Train the model\n            train(model, train_gen, steps_per_epoch, val_gen, val_steps, EPOCHS_CHANGED, callback, True)\n\n            # Save the model\n            model.save(\"saved_models/warm_up.h5\")\n        \n        # End the timer\n        end = timer()\n\n        # Write the execution time \n        write_2_file(\"saved_models/timer.txt\", \"WarmUp: \"+str(end-start))\n\n        print(\"WARMUP ENDED\\n\")\n        \n        ## Start the Real Train\n        \n        # Load again the saved model\n        model = tf.keras.models.load_model(\"saved_models/warm_up.h5\",\n                                           custom_objects={\"KerasLayer\": hub.KerasLayer})\n\n        # All layers are trainable now\n        model.trainable = True\n\n\n        # Define an exponencial decay for decrease the learning rate for each\n        # step\n        \"\"\"\n        lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n            0.05,\n            decay_steps=10000000,\n            decay_rate=0.7,\n            staircase=True\n        )\"\"\"\n        \n        model.compile(\n            optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True),\n            loss=tf.keras.losses.CategoricalCrossentropy(),\n            metrics=[\"accuracy\"]\n        )\n\n        print(\"MAIN TRAIN\\n\")\n        \n        # Start counter for compute execution time\n        start = timer()\n\n        # If the main train was interrupt we will calculate\n        # the remain epochs to continue the train\n        if WARMUP_DONE:\n            EPOCHS = EPOCHS_CHANGED\n        \n        \n        # Early stop after 5 epochs without improvements, at least\n        # improvements of 0.001 decreases on validation loss\n        callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20, min_delta=0.001)\n        \n        # Train the model\n        hist = train(model, train_gen, steps_per_epoch, val_gen, val_steps, EPOCHS*5, callback, False)\n        \n        # End the timer\n        end = timer()\n\n        # Save the execution time on the file\n        write_2_file(\"saved_models/timer.txt\", \" Train: \"+str(end-start))\n\n        print(\"MAIN TRAIN ENDED\\n\")\n\n\n    # Main Train only\n    else:\n        print(\"NORMAL TRAIN\")\n        \n        # Start the timer for execution time\n        start = timer()\n\n        # Treinar the model\n        hist = train(model, train_gen, steps_per_epoch, val_gen, val_steps, EPOCHS, None, False)\n        \n        # End the timer\n        end = timer()\n\n        print(\"NORMAL TRAIN ENDED\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if isTrain == \"y\":\n    \n    # Create a dict with the mean of each metric per epoch\n    metrics ={\n\n        \"acc\": np.mean( hist.history[\"accuracy\"] ), \n        \"val_acc\": np.mean( hist.history[\"val_accuracy\"] ),\n        \"loss\": np.mean( hist.history[\"loss\"] ),\n        \"val_loss\": np.mean( hist.history[\"val_loss\"] )\n    }\n\n\n    # Show each metric mean\n    for k, v in metrics.items():\n        print(\"{}: {:.3f} -\".format(k, v), end=\" \")\n\n    # Show execution time\n    print(\"\\n\\nExecution time: \", end-start)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download of the model\n\nGenerate a link for direct download of the saved model in h5 format","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\nFileLink(r'saved_models/warm_up.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate\n\nLet's prepare the .csv for submission","metadata":{}},{"cell_type":"code","source":"## Optional (uncomment to use)\n## If you have a .h5 model save on your Google Drive, just make it public, copy\n## the id of it and past it here\n\n#url = \"https://drive.google.com/uc?id=your_id\"\n#gdown.download(url, \"saved_models/warm_up.h5\", quiet=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading only the model will not work when we try to predict. To fix the problem I load the model save the weights, create a new model based on the inputs and outputs layers (they are connected to other intermediate layers) and load the weights to use in the new model ","metadata":{}},{"cell_type":"code","source":"model = tf.keras.models.load_model(\"saved_models/warm_up.h5\",\n                                       custom_objects={\"KerasLayer\": hub.KerasLayer})\n\n\nmodel.save_weights(\"saved_models/cpkt\")\n\nmodel = tf.keras.Model(inputs=model.inputs, outputs=model.outputs)\n\nmodel.load_weights(\"saved_models/cpkt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Path for directory with the test images\nBASE_PATH = \"../input/dog-breed-identification/test/\"\n\n# Get all file names (with extension now)\ntest_imgs = os.walk(BASE_PATH).__next__()[2]\n\n# Create a list with all the possible classes\ncols = list(train_gen.class_indices.keys())\n\n# Append the column id to identify the image\ncols = [\"id\"] + cols\n\n# Now create a DataFrame with the cols\ndf_pred = pd.DataFrame(columns=cols)\n\n\n# Infer the model with more than 10k images (take a coffee or maybe ... ten)\nfor test_img in tqdm(test_imgs):\n    \n    # Get the id\n    name = test_img.split(\".\")[0]\n    \n    # Preprocess the image to be valid to pass into the model\n    img = tf.keras.preprocessing.image.load_img(BASE_PATH+test_img, target_size=IMG_SIZE)\n    img = tf.keras.preprocessing.image.img_to_array(img)\n    img = tf.expand_dims(img, 0)\n    rescale_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n    img = rescale_layer(img)\n    \n    # Inference\n    out = model.predict(img)[0]\n    \n    # Create a dict to fill with the computed probabilities\n    # for each breed\n    \n    out_dict = {k:0 for k in cols}\n    out_dict[\"id\"] = name\n    \n    for i in range(len(out)):\n        for k,v in train_gen.class_indices.items():\n            if v == i:\n                out_dict[k] = out[i]\n    \n    # Append the dict (row) to our DataFrame\n    df_pred = df_pred.append(out_dict, ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Write to a .csv file to make the submission \ndf_pred.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}