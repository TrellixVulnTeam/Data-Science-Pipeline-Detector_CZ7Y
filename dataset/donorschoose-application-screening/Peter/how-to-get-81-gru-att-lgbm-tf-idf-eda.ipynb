{"cells":[{"metadata":{"_cell_guid":"88a715d0-9520-41b9-85d1-f3ac75ea529f","_uuid":"dffb94b9e76a8dce42cb56728863f1a6e10bea20"},"cell_type":"markdown","source":"# Hi, Welcome to my kernel. We will show you how we achieve 80+% in this challenge. \n# If you find this helpful, <span style=\"color:#3\">please support us!!!</span>\n## We will focus on the rate of **<span style=\"color:red\">REJECTED</span>** according to different factors. This may help you to design your model on features that truly matter.\n![donor](https://i2.wp.com/speechisbeautiful.com/wp-content/uploads/2015/06/donorschoose_logo.png)\n### DonorsChoose is an US organisation that provide funding to school teachers who wish to improve their education environment. They received approximately thounsands of project proposals every year and the challange they are facing is dealing with enormous of proposal with limited volunteers. We're writing this kernel to help the organizer and participants to filter out insignificant features when designing pre-screening algorithm.\n### Let's see what we going to analyze in this kernel:\n1.  Introduction of dataset\n> Importing the libraries<br/>\n> Data preparation  <br/>\n2. Analysis\n> **Which state has the highest rate of rejected?**<br/>\n> **What is the relationship between funding amount and rate of rejected? ** <br/>\n> **Will grade categories affect the rate of rejected?** <br/>\n> **Which combination of category and sub-category has highest rate of rejected?** <br/>\n> **What is the relationship between essay sentiment and rate of rejected?** <br/>\n3. Suggestion\n4. LGBM + TFIDF\n5. GRU-ATT\n6. Results\n\n"},{"metadata":{"_cell_guid":"b4b47154-d8a3-40a0-b3f9-b8af8d056bd5","_uuid":"80e2d41de00bf26a92126775bc04f8b10333b964"},"cell_type":"markdown","source":"# 1.1 Importing the libraries"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","collapsed":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"46d655fa-7793-436a-b7da-dd5022691f91","_uuid":"34c8baca9ef0c0afd566b59032560037b0e7587f"},"cell_type":"markdown","source":"# 1.2 Data Preparation"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"scrolled":false,"_kg_hide-input":false,"trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/donorschoose-application-screening/train.csv')\ndf.sort_values('project_submitted_datetime').head(3)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"af6fb370-b804-4348-9182-6c66f24e28a7","_uuid":"0b82f96577a417cd6f2a4885e329d8b159cccd54"},"cell_type":"markdown","source":"## In the [train.csv](https://www.kaggle.com/c/donorschoose-application-screening/data), we got total of 16 unique features:\n* id (unique id, no repeated)\n* teacher_id (unique id, repeated)\n* teacher_prefix (Mr. Ms. Mrs. Teacher)\n* school_state \n* project_submitted_datetime\n* project_grade_categories (4 categories)\n* project_subject_categories (9 categories)\n* project_subject_subcategories (30 categories)\n* project_title\n* project_essay_1\n* project_essay_2\n* project_essay_3\n* project_essay_4\n* project_resource_summary\n* teacher_number_of_previously_posted_projects\n* project_is_approved (0 - Rejected , 1 - Approved)"},{"metadata":{"_cell_guid":"7796e5ef-f294-4ec6-af8c-fa16fe03623d","collapsed":true,"_uuid":"edd7e4e763f63f18f2851aedbdcb191179982225","_kg_hide-input":false,"trusted":false},"cell_type":"code","source":"resources_df = pd.read_csv('../input/donorschoose-application-screening/resources.csv')\nresources_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"db8e2edf-fd58-415d-8ed5-a7a83854a6e1","_uuid":"7039763f2e0875e9d029e76c10022d9ef399ffb7"},"cell_type":"markdown","source":"## In the [resources.csv](https://www.kaggle.com/c/donorschoose-application-screening/data), we got total of 4 unique features:\n* id (refer to id in train.csv, can be multiple entries)\n* description\n* quantity\n* price"},{"metadata":{"_cell_guid":"edb2ea39-c38f-4ebf-aa1f-ed92aee5e8ca","_uuid":"1e0fcaddc831aceaf9af8cf60a18f3e5f15a5f66"},"cell_type":"markdown","source":"## Migrate the features from resource.csv to train.csv\nWe found that the resources.csv is additional features for a project proposal, it includes the resources that request by teachers with details such as desriptions, quantity and price. What we can do is we calculate the total quantity multiply with the price to get the total sum that requested by a project proposal. This amount is then concatenate into the main dataset *df*."},{"metadata":{"_cell_guid":"c6988b2f-f6bc-4e4f-83dd-330a18e6f884","collapsed":true,"_uuid":"5b82e2c225d1747a1c918dd8bbc3ce3c2587277d","_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"resources_df['amount'] = resources_df['quantity']*resources_df['price']\namount_df = resources_df.groupby('id')['amount'].agg('sum').sort_values(ascending=False).reset_index()\n\nresource_amount_map = {}\nfor i, row in amount_df.iterrows():\n    resource_amount_map[row['id']] = row['amount']\n\ndf.insert(4,'total_amount',df['id'].map(resource_amount_map))\ndf.tail(3)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4594449e-07d8-4608-945b-8d09c39cb4c5","_uuid":"315dd53232d056739721e7663d6090a9a7fa11c3"},"cell_type":"markdown","source":"# 2 Analysis\nThis section we will focus on analyzing the data and find out the possible reason behind the rejected proposals. As people have uploaded comprehensive individual feature visualization, we will straight away move into deeper analysis. You can always check other's kernel, e.g [An Educated Guess - DonorsChoose EDA](https://www.kaggle.com/headsortails/an-educated-guess-donorschoose-eda). Also, we will focus on the rate of rejected rather than approved as the ratio of approved is 85:15. The small percentage of rejected proposal is exactly what we going to analyse.\n\n# 2.1 Which state has the highest rate of rejected?\n### Statistics of proposal submission from each state\nFirst of all, let's see what is the number of submission from each states, you can always **hover** to see more details. "},{"metadata":{"_cell_guid":"bf706733-fbae-4055-9103-85f9b7adcfd5","collapsed":true,"_uuid":"0d4f2842cf8598997dfc364ce7fc314525f7e40c","_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"scl = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n\nstate_df = df.groupby('school_state')['project_is_approved'].agg('sum').reset_index().sort_values('school_state').reset_index()\nsubmit_df = df['school_state'].value_counts().reset_index()\nsubmit_df.columns=['school_state','total_submit']\nsubmit_df = submit_df.sort_values('school_state').reset_index()\n\ndata = [ dict(\n        type='choropleth',\n        colorscale = scl,\n        autocolorscale = False,\n        locations = state_df['school_state'],\n        z = submit_df['total_submit'].astype(int),\n        locationmode = 'USA-states',\n        marker = dict(\n            line = dict (\n                color = 'rgb(255,255,255)',\n                width = 2\n            )\n        ),\n        colorbar = dict(\n            title = \"Number of Proposal\"\n        )\n    ) ]\n\nlayout = dict(\n        title = 'US DonorChoose Proposal Submission from States',\n        geo = dict(\n            scope='usa',\n            projection=dict( type='albers usa' ),\n            showlakes = True,\n            lakecolor = 'rgb(255, 255, 255)',\n        ),\n    )\n\nfig = dict(data=data, layout=layout)\n\nurl = py.iplot(fig, filename='donor-state-submit')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d8a5982b-6f31-4daa-a668-1cc44093d9a0","_uuid":"550f5c9e8b088da8ca44ace1acbe7c107433b2c5"},"cell_type":"markdown","source":"### Statistics of rate of rejected proposal from each state"},{"metadata":{"_cell_guid":"7e0cdadc-a0c5-4ddd-ad1e-efdc92190c93","collapsed":true,"_uuid":"98c12d009d1603aa8aeaac594398a0f12e11b38e","_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"scl = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n\nstate_df = df.groupby('school_state')['project_is_approved'].agg('sum').reset_index().sort_values('school_state').reset_index()\nsubmit_df = df['school_state'].value_counts().reset_index()\nsubmit_df.columns=['school_state','total_submit']\nsubmit_df = submit_df.sort_values('school_state').reset_index()\n\ndata = [ dict(\n        type='choropleth',\n        colorscale = scl,\n        autocolorscale = False,\n        locations = submit_df['school_state'],\n        z = (submit_df['total_submit'].astype(int)-state_df['project_is_approved'].astype(int))/submit_df['total_submit'].astype(int),\n        locationmode = 'USA-states',\n        marker = dict(\n            line = dict (\n                color = 'rgb(255,255,255)',\n                width = 2\n            )\n        ),\n        colorbar = dict(\n            title = \"Rate of Rejected\"\n        )\n    ) ]\n\nlayout = dict(\n        title = 'US DonorChoose Approval in States',\n        geo = dict(\n            scope='usa',\n            projection=dict( type='albers usa' ),\n            showlakes = True,\n            lakecolor = 'rgb(255, 255, 255)',\n        ),\n    )\n\nfig = dict(data=data, layout=layout)\n\nurl = py.iplot(fig, filename='donor-state-reject')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9aab4a3f-4450-4595-bd62-e65548cc2d4f","_uuid":"0a663d04ac77af0834e6d8b0b3d03e96c1fb3e60"},"cell_type":"markdown","source":"### Observation:\n* We found that the state that has most submission is **California**(25.7k), follow by **Texas**(12.3k) and **New York**(12.15k).\n* There is no visible relationship between neighbours of states and number of submissions. \n* Texas has the highest rate of rejected (18.43%) which above the average (~15%).\n* All the neighbours of Texas have relatively high rate of rejected."},{"metadata":{"_cell_guid":"866f3ebf-dbe9-4f28-bbdb-beecf5ab6f07","_uuid":"676c8feeff6eb38e397ad1bbc88685391c4f6776"},"cell_type":"markdown","source":"# 2.2 What is the relationship between funding amount and rate of rejected?\nWe usually assume that people will reject proposal that request for excessive amount of money especially from non-profit organization. Will it be true in this case? Let'see.\n### Average amount of a single funded proposal in states"},{"metadata":{"_cell_guid":"db59f190-4f04-4fe9-a689-c1fe3727662e","collapsed":true,"_uuid":"be6bca220a1c9754060cd84d53e90d4578db6223","_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"state_df = df[df['project_is_approved']==1]\nstate_df = state_df.groupby('school_state')['project_is_approved'].agg('sum').reset_index().sort_values('school_state').reset_index()\na_df = df.groupby('school_state')['total_amount'].agg('sum').reset_index().sort_values('school_state').reset_index()\nsubmit_df = df['school_state'].value_counts().reset_index()\nsubmit_df.columns=['school_state','total_submit']\nsubmit_df = submit_df.sort_values('school_state').reset_index()\n\ndata = [ dict(\n        type='choropleth',\n        colorscale = scl,\n        autocolorscale = False,\n        locations = state_df['school_state'],\n        z = a_df['total_amount'].astype(int)/state_df['project_is_approved'].astype(int),\n        #z = (submit_df['total_submit'].astype(int)-state_df['project_is_approved'].astype(int))/submit_df['total_submit'].astype(int),\n        locationmode = 'USA-states',\n        marker = dict(\n            line = dict (\n                color = 'rgb(255,255,255)',\n                width = 2\n            )\n        ),\n        colorbar = dict(\n            title = \"USD\"\n        )\n    ) ]\n\nlayout = dict(\n        title = 'US DonorChoose Average Funding in States',\n        geo = dict(\n            scope='usa',\n            projection=dict( type='albers usa' ),\n            showlakes = True,\n            lakecolor = 'rgb(255, 255, 255)',\n        ),\n    )\n\nfig = dict(data=data, layout=layout)\n\nurl = py.iplot(fig, filename='donor-state-funding')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d850ea03-e047-49aa-ba9a-b8899dc1adb6","_uuid":"de3b143a3dfb60fa41b5f7956bce2d06c93950aa"},"cell_type":"markdown","source":"### Statistic of amount of proposal's request fund"},{"metadata":{"_cell_guid":"b0ee226a-d69b-4e96-b944-a8f81c18340a","collapsed":true,"_uuid":"27bb10ebbcc7bdbfbbf9fe6e77280dca74917c51","_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"df['total_amount']=df['total_amount'].apply(lambda x: int(round(x,-2)))\nfund_total_df = df.groupby('total_amount').count()['project_is_approved'].head(50)\nfund_total_df = fund_total_df.reset_index()\ndata = [\n    go.Bar(\n        x=fund_total_df['total_amount'], # assign x as the dataframe column 'x'\n        y=fund_total_df['project_is_approved']\n    )\n]\n\nurl = py.iplot(data, filename='fund-total-chart')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a1845b1e-a1d4-4cd2-a658-1645e60a662f","_uuid":"4346307b3b4196eddde3434997b5559dbd14ca73"},"cell_type":"markdown","source":"### Funding Amount v/s Rate of Reject"},{"metadata":{"_cell_guid":"0eef64ef-a3ab-4353-bf9e-695b1a1f0a33","collapsed":true,"_uuid":"d80e8ce89a5038e4da033215faa934884742d921","_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"fund_df = ((df.groupby('total_amount').count()['project_is_approved']-df.groupby('total_amount')['project_is_approved'].agg('sum'))/df.groupby('total_amount').count()['project_is_approved']).head(50)\nfund_df = fund_df.reset_index()\ndata = [\n    go.Bar(\n        x=fund_df['total_amount'], # assign x as the dataframe column 'x'\n        y=fund_df['project_is_approved']\n    )\n]\n\nurl = py.iplot(data, filename='fund-reject-chart')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9b48c873-3e2d-4df6-adb6-cb2512c34223","_uuid":"9af6a003928e823e382f31c2ea169e73385d4b48"},"cell_type":"markdown","source":"### Observation:\n* Most proposal request for fund in between 100USD to 500USD and the number of proposal drop significantly after 2000USD.\n* Rate of reject increase gradually after 500USD from ~15% to ~20%.\n* The rate of reject after 2000USD should be consider carefully as the lesser the data, the greater the uncertainties."},{"metadata":{"_cell_guid":"3a6af9d2-cb94-407f-9500-24716d64842b","_uuid":"0ae9b433ab954d3382b712f2306938097e45e132"},"cell_type":"markdown","source":"# 2.3 Will grade categories affect the rate of rejected?\nSince we got 4 categories in grade (3-5, 6-8, 9-12 and PreK-2), let's figure out whether the grades will affect the rate of rejected? \n### Grades v/s Funding Amount"},{"metadata":{"_cell_guid":"af424272-92f0-40cf-87dc-286929d5835d","collapsed":true,"_uuid":"d52f957facf21c35247770e76231b96242cbcf4f","_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"df['total_amount'] = df['total_amount'].clip(upper=3000)\ngrade_amount_df = df.groupby(['project_grade_category','total_amount']).count()['id'].unstack().clip(upper=2000)\n\nplt.figure(figsize=(20,2))#You can Arrange The Size As Per Requirement\nax = sns.heatmap(grade_amount_df, cmap='viridis_r')\nplt.title(\"Correlation between Grades v/s Funding Amount\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1587e90e-49bd-4797-8cd0-1dbf839d3609","collapsed":true,"_uuid":"f6c41244e09b61ee08457406d0accd275ff7b757","_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"import cufflinks as cf\ncf.set_config_file(offline=True, world_readable=True, theme='ggplot')\n\ngrade_amount_df = df.groupby('project_grade_category').count()['total_amount']\ngrade_amount_df.iplot(kind='bar', yTitle='Number of Proposal', title='Submission from Grade Categories',\n             filename='Grade categorical-bar-chart')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7ec3c000-4519-43a3-a7d0-4054fb0e8e07","_uuid":"272e6932ec116ce60db47154143770be9f61f938","collapsed":true,"scrolled":true,"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"grade_approval_df = df.groupby('project_grade_category')['project_is_approved'].agg('sum')\ngrade_total_df = df.groupby('project_grade_category').count()['id']\n((grade_total_df-grade_approval_df)/grade_total_df).iplot(kind='bar', yTitle='Rate of Rejected', title='Rejected Rate of Grade Categories',\n             filename='Grade categorical-bar-chart')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fe79b52e-9b4f-4aa0-98b4-ccee94c88e77","_uuid":"ee458ed2d7d71d906239754d112289eeaa875554"},"cell_type":"markdown","source":"### Observation:\n* Grade 3-5 and PreK-2 have the considerably greater amount in proposal submission compare to grade 6-8 and grade 9-12.\n* However, they have very close rate of rejected which lies in between 14.6% to 16.7%.\n* This shows that the grade categories is a poor features to training which can be ignored."},{"metadata":{"_cell_guid":"b8b3d233-d702-4665-ad6e-7952bdcabd5f","_uuid":"6fe92b87f9de0f06f6518abdf9a0beaa0692175e"},"cell_type":"markdown","source":"# 2.4 Which combination of category and sub-category has highest rate of rejected?"},{"metadata":{"_cell_guid":"bcadeeff-14ca-41ef-aa2d-556342c35f60","collapsed":true,"_uuid":"b8555a05145cbac6772eb66afb7e1882df7f847a","_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"cat_df = df[['project_subject_categories','project_subject_subcategories','project_is_approved']]\nnew_cat,new_sub,new_approve = [],[],[]\n\nfor i, row in cat_df.iterrows():\n    cats = row['project_subject_categories'].split(', ')\n    subs = row['project_subject_subcategories'].split(', ')\n    for j in range(len(cats)):\n        for k in range(len(subs)):\n            new_cat.append(cats[j])\n            new_sub.append(subs[k])\n            new_approve.append(row['project_is_approved'])\n            \nnew_cat = pd.DataFrame({'project_subject_categories':new_cat})\nnew_sub = pd.DataFrame({'project_subject_subcategories':new_sub})\nnew_approve = pd.DataFrame({'project_is_approved':new_approve})\n\ncat_total_df = pd.concat([new_cat,new_sub,new_approve],axis=1).reset_index()\ncat_approval_df = cat_total_df.groupby(['project_subject_categories','project_subject_subcategories'])['project_is_approved'].agg('sum')\ncat_all_df = cat_total_df.groupby(['project_subject_categories','project_subject_subcategories']).count()['project_is_approved']\ncat_heat = (cat_approval_df/cat_all_df).unstack()\n\nplt.figure(figsize=(18,5))#You can Arrange The Size As Per Requirement\nax = sns.heatmap(cat_heat, cmap='viridis_r')\nplt.title(\"Aprroved rate for categories and sub-categories combination\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b5df18fa-73c5-4e2c-8fd4-2ad50fdb8470","collapsed":true,"_uuid":"04c73ad341817f23e5cdf1afc076a4cc32467a7c"},"cell_type":"markdown","source":"### Observation:\n* There are total 9x30 combinations of categories and sub-categories.\n* Top 3 categories that consistent the most in the rate of approved is Applied Learning, Literacy & Language and Math & Science.\n* Both Warmth and Care & Hunger have very extreme rate of approved and rejected on certain sub-categories.\n"},{"metadata":{"_cell_guid":"5ef8a773-c6e9-4216-bf41-ea8ad14d0ba8","_uuid":"d22ff9ac4a35fc6148712457a989bd74458c8d83"},"cell_type":"markdown","source":"# 2.5 What is the relationship between essay sentiment and rate of rejected?\nIn this section, we going to analyze:\n* The relationship between length of sentences in essays and rate of rejected\n* The relationship between essay sentiment (Polarity and Subjectivity) and rate of rejected\n\n### Length of Sentences in Essays v/s Rate of Rejected"},{"metadata":{"_cell_guid":"e054ad6a-13d4-476e-9ee9-a90ad60a6a88","collapsed":true,"_uuid":"bc89c7a91b6237e66d7cb619705670e598220705","_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"from nltk import sent_tokenize, word_tokenize\n\ndef count_sent(text):\n    if text == \"nan\" or not text:\n        return 0\n    sents = sent_tokenize(text)\n    return len(sents)\n\nsent_df = df[['project_essay_1','project_is_approved']]\nsent_df['sent_length'] = sent_df['project_essay_1'].apply(count_sent)\n\napr = sent_df.groupby('sent_length')['project_is_approved'].agg('sum').reset_index().sort_values('sent_length')\ntot = sent_df.groupby('sent_length').count()['project_is_approved'].reset_index().sort_values('sent_length')\nrat = (tot['project_is_approved']-apr['project_is_approved'])/tot['project_is_approved']\nrat.iplot(kind='bar', yTitle='Number of Sentences', title='Length of Sentences in Essay_1 v/s Rate of Rejected',\n             filename='Grade categorical-bar-chart')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9e7ffe70-707f-49ad-aaec-dfdd4a38b3f5","collapsed":true,"_uuid":"ee44780f74eff39fccd38551a1969121680ade57","_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"sent_df = df[['project_essay_2','project_is_approved']]\nsent_df['project_essay_2'] = sent_df['project_essay_2'].astype(str)\nsent_df['sent_length'] = sent_df['project_essay_2'].apply(count_sent)\n\napr = sent_df.groupby('sent_length')['project_is_approved'].agg('sum').reset_index().sort_values('sent_length')\ntot = sent_df.groupby('sent_length').count()['project_is_approved'].reset_index().sort_values('sent_length')\nrat = (tot['project_is_approved']-apr['project_is_approved'])/tot['project_is_approved']\nrat.iplot(kind='bar', yTitle='Number of Sentences', title='Length of Sentences in Essay_2 v/s Rate of Rejected',\n             filename='Grade categorical-bar-chart')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"09bc2f02-8e84-4a40-89ac-771a2f842d31","collapsed":true,"_uuid":"32ddc193efe823051ef5b892326fdd1ffcefb177","_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"sent_df = df[['project_essay_3','project_is_approved']]\nsent_df['project_essay_3'] = sent_df['project_essay_3'].astype(str)\nsent_df['sent_length'] = sent_df['project_essay_3'].apply(count_sent)\n\napr = sent_df.groupby('sent_length')['project_is_approved'].agg('sum').reset_index().sort_values('sent_length')\ntot = sent_df.groupby('sent_length').count()['project_is_approved'].reset_index().sort_values('sent_length')\nrat = (tot['project_is_approved']-apr['project_is_approved'])/tot['project_is_approved']\nrat.iplot(kind='bar', yTitle='Number of Sentences', title='Length of Sentences in Essay_3 v/s Rate of Rejected',\n             filename='Grade categorical-bar-chart')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"81c43e29-c9d1-4819-905d-30ea6095ac11","collapsed":true,"_uuid":"70f5a26316b6d69892a4063e1c7314102d91bf0b","_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"sent_df = df[['project_essay_4','project_is_approved']]\nsent_df['project_essay_4'] = sent_df['project_essay_4'].astype(str)\nsent_df['sent_length'] = sent_df['project_essay_4'].apply(count_sent)\n\napr = sent_df.groupby('sent_length')['project_is_approved'].agg('sum').reset_index().sort_values('sent_length')\ntot = sent_df.groupby('sent_length').count()['project_is_approved'].reset_index().sort_values('sent_length')\nrat = (tot['project_is_approved']-apr['project_is_approved'])/tot['project_is_approved']\nrat.iplot(kind='bar', yTitle='Number of Sentences', title='Length of Sentences in Essay_4 v/s Rate of Rejected',\n             filename='Grade categorical-bar-chart')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4cb18c9a-dddf-4b21-817b-ecf51bd3a3e6","_uuid":"4f01821b0a5ac819e07b43d916b8f997bcc05247"},"cell_type":"markdown","source":"### Observation :\n* There is no significant correlation in between length of sentences and rate of rejected.\n* Both essay_3 and essay_4 leave unfill by most of the project proposal.\n* High rate of rejected is observed in proposal with 0 sentences in essay_2 (~32%)"},{"metadata":{"_cell_guid":"fa97791f-44e3-47cd-a8b6-95c55ee070a9","_uuid":"27665b3569fc90b87cddfa7070a3be6b0b4eb771"},"cell_type":"markdown","source":"# 2.5 What is the relationship between essay sentiment and rate of rejected?\nIn this section we going to analyze how essay's sentiment (polarity and subjectivity) will affect rate of rejected. First of all, we concatenate all the essay_1 to essay_4. Then, textblob sentiment is used to get the level of polarity and subjectivity for each project proposal. Below is the plotted heatmap that shows what essay sentiment have highest chance to be rejected. "},{"metadata":{"_cell_guid":"017928ac-ccb6-4235-83c5-51e8acde9829","collapsed":true,"_uuid":"43365f154a3b596892e4c1fece26bdf9fa2335da","trusted":false},"cell_type":"code","source":"from textblob import TextBlob\ndf[\"essay\"] = df[\"project_essay_1\"].map(str) + df[\"project_essay_2\"].map(str) + df[\"project_essay_3\"].map(str) + df[\"project_essay_4\"].map(str)\ndef get_polarity(text):\n    textblob = TextBlob(text)\n    pol = textblob.sentiment.polarity\n    return round(pol,2)\n\ndef get_subjectivity(text):\n    textblob = TextBlob(text)\n    subj = textblob.sentiment.subjectivity\n    return round(subj,2)\n\npol_df = df[['essay','project_is_approved']]\npol_df['sent_polarity'] = pol_df['essay'].apply(get_polarity)\npol_df['sent_subjectivity'] = pol_df['essay'].apply(get_subjectivity)\n\npol_rj_df = pol_df[pol_df['project_is_approved']==0]\n#sub_df = df[['project_essay_1','project_is_approved']]\n#sub_df['project_essay_1'] = sub_df['project_essay_1'].astype(str)\n#sub_df['sent_subjectivity'] = sub_df['project_essay_1'].apply(subjectivity)\n\npol_total_df = pol_df.groupby(['sent_polarity','sent_subjectivity']).count()['project_is_approved'].unstack().clip(upper=5000)\npol_rj_df = pol_rj_df.groupby(['sent_polarity','sent_subjectivity']).count()['project_is_approved'].unstack().clip(upper=5000)\npol_rat_df = pol_rj_df/pol_total_df\n#pol_heat_df\nplt.figure(figsize=(20,20))#You can Arrange The Size As Per Requirement\nax = sns.heatmap(pol_rat_df, cmap='viridis_r')\nplt.title(\"Correlation between Polarity and Subjectivity for Rate of\")\n#sent_df\n#apr = sent_df.groupby('sent_polarity')['project_is_approved'].agg('sum').reset_index()\n#tot = sent_df.groupby('sent_polarity').count()['project_is_approved'].reset_index()\n#rat = apr['project_is_approved']/tot['project_is_approved']\n#rat","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"64d740c5-c43a-4aea-8a58-2f743688983a","_uuid":"544749db1f3eb7723372dfde228861cc9c9c2a0a"},"cell_type":"markdown","source":"### Observation :\n* The essay in centre of sentiment heatmap has the lowest rate of rejected\n* They rejected whatever proposal that subjectivity(greater than 0.78 , less than 0.18) and polarity(less than0.08 , greater than 0.58)\n* The rate of rejected increase gradually from centre of heatmap to periphery of heatmap.\n\n# 3 Suggestion\n* From this kernel, we shows that not all the features will fit your training model, some are considerably flat and might be redundant to your model.\n* Features that suggested is **Categories & Sub-categories** and ** Sentiment of Essay**.\n* DonorChoose.org can use these features as pre-screen to allocate volunteer effectively.\n"},{"metadata":{"_cell_guid":"2e626000-6a3a-4a3d-af07-1c8866d327b5","_uuid":"288d6d8064a993d997e65f47f2cfd21be8159fb6"},"cell_type":"markdown","source":"# 4 LightGBM + TFIDF\nFirst of all, we need to thank **Oleg Panichev ** for his [LightGBM and Tf-idf Starter](https://www.kaggle.com/opanichev/lightgbm-and-tf-idf-starter/code). We implement his model but we have modified it by adding and removing some features as mentioned in EDA above.\n1. We removed the teacher_prefix and grade_categories\n2. We included the sentiment analysis (Polarity and Subjectivity)\n"},{"metadata":{"_cell_guid":"bf5e0de8-b455-4d9b-9833-91279499cbe6","collapsed":true,"_uuid":"b601f493ead568ccb1ab79b5d66a843d4872723f","_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"'''\nimport gc\nimport numpy as np\nimport pandas as pd\nimport os\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold, RepeatedKFold\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom tqdm import tqdm\nimport lightgbm as lgb\n\n\n# Load Data\ndtype = {\n    'id': str,\n    'teacher_id': str,\n    'teacher_prefix': str,\n    'school_state': str,\n    'project_submitted_datetime': str,\n    'project_grade_category': str,\n    'project_subject_categories': str,\n    'project_subject_subcategories': str,\n    'project_title': str,\n    'project_essay_1': str,\n    'project_essay_2': str,\n    'project_essay_3': str,\n    'project_essay_4': str,\n    'project_resource_summary': str,\n    'teacher_number_of_previously_posted_projects': int,\n    'project_is_approved': np.uint8,\n}\ndata_path = os.path.join('..', 'input')\ntrain = pd.read_csv(os.path.join(data_path, 'train.csv'), dtype=dtype, low_memory=True)\ntest = pd.read_csv(os.path.join(data_path, 'test.csv'), dtype=dtype, low_memory=True)\nres = pd.read_csv(os.path.join(data_path, 'resources.csv'))\n\nprint(train.head())\n# print(test.head())\nprint(train.shape, test.shape)\n\n\n# Preprocess data\ntrain['project_essay'] = train.apply(lambda row: ' '.join([\n    str(row['teacher_prefix']), \n    str(row['school_state']), \n    str(row['project_grade_category']), \n    str(row['project_subject_categories']), \n    str(row['project_subject_subcategories']), \n    str(row['project_essay_1']), \n    str(row['project_essay_2']), \n    str(row['project_essay_3']), \n    str(row['project_essay_4']),\n    ]), axis=1)\ntest['project_essay'] = test.apply(lambda row: ' '.join([\n    str(row['teacher_prefix']), \n    str(row['school_state']), \n    str(row['project_grade_category']), \n    str(row['project_subject_categories']), \n    str(row['project_subject_subcategories']), \n    str(row['project_essay_1']), \n    str(row['project_essay_2']), \n    str(row['project_essay_3']), \n    str(row['project_essay_4']),\n    ]), axis=1)\n\ndef extract_features(df):\n    df['project_title_len'] = df['project_title'].apply(lambda x: len(str(x)))\n    df['project_essay_1_len'] = df['project_essay_1'].apply(lambda x: len(str(x)))\n    df['project_essay_2_len'] = df['project_essay_2'].apply(lambda x: len(str(x)))\n    df['project_essay_3_len'] = df['project_essay_3'].apply(lambda x: len(str(x)))\n    df['project_essay_4_len'] = df['project_essay_4'].apply(lambda x: len(str(x)))\n    df['project_resource_summary_len'] = df['project_resource_summary'].apply(lambda x: len(str(x)))\n  \nextract_features(train)\nextract_features(test)\n\nfrom textblob import TextBlob\ndef get_polarity(text):\n    textblob = TextBlob(unicode(text, 'utf-8'))\n    pol = textblob.sentiment.polarity\n    return round(pol,3)\n\ndef get_subjectivity(text):\n    textblob = TextBlob(unicode(text, 'utf-8'))\n    subj = textblob.sentiment.subjectivity\n    return round(subj,3)\n\ntrain['polarity'] = train['project_essay'].apply(get_polarity)\ntrain['subjectivity'] = train['project_essay'].apply(get_subjectivity)\ntest['polarity'] = test['project_essay'].apply(get_polarity)\ntest['subjectivity'] = test['project_essay'].apply(get_subjectivity)\n\ntrain = train.drop([\n    'project_essay_1', \n    'project_essay_2', \n    'project_essay_3', \n    'project_essay_4'], axis=1)\ntest = test.drop([\n    'project_essay_1', \n    'project_essay_2', \n    'project_essay_3', \n    'project_essay_4'], axis=1)\n\ndf_all = pd.concat([train, test], axis=0)\ngc.collect()\n\n# Merge with resources\nres = pd.DataFrame(res[['id', 'price']].groupby('id').price.agg(\\\n    [\n        'count', \n        'sum', \n        'min', \n        'max', \n        'mean', \n        'std', \n        # 'median',\n        lambda x: len(np.unique(x)),\n    ])).reset_index()\nprint(res.head())\ntrain = train.merge(res, on='id', how='left')\ntest = test.merge(res, on='id', how='left')\ndel res\ngc.collect()\n\n# Preprocess columns with label encoder\nprint('Label Encoder...')\ncols = [\n    'teacher_id', \n    'school_state', \n    'project_subject_categories', \n    'project_subject_subcategories'\n]\n\nfor c in tqdm(cols):\n    le = LabelEncoder()\n    le.fit(df_all[c].astype(str))\n    train[c] = le.transform(train[c].astype(str))\n    test[c] = le.transform(test[c].astype(str))\ndel le\ngc.collect()\nprint('Done.')\n\n\n# Preprocess timestamp\nprint('Preprocessing timestamp...')\n\ntrain['project_submitted_datetime'] = pd.to_datetime(train['project_submitted_datetime']).values.astype(np.int64)\ntest['project_submitted_datetime'] = pd.to_datetime(test['project_submitted_datetime']).values.astype(np.int64)\nprint('Done.')\n\n\n# Preprocess text\nprint('Preprocessing text...')\ncols = [\n    'project_title', \n    'project_essay', \n    'project_resource_summary'\n]\nn_features = [\n    400, \n    5000, \n    400\n]\n\nfor c_i, c in tqdm(enumerate(cols)):\n    tfidf = TfidfVectorizer(max_features=n_features[c_i], min_df=3)\n    tfidf.fit(df_all[c])\n    tfidf_train = np.array(tfidf.transform(train[c]).todense(), dtype=np.float16)\n    tfidf_test = np.array(tfidf.transform(test[c]).todense(), dtype=np.float16)\n\n    for i in range(n_features[c_i]):\n        train[c + '_tfidf_' + str(i)] = tfidf_train[:, i]\n        test[c + '_tfidf_' + str(i)] = tfidf_test[:, i]\n        \n    del tfidf, tfidf_train, tfidf_test\n    gc.collect()\n    \nprint('Done.')\ndel df_all\ngc.collect()\n\n# Prepare data\ncols_to_drop = [\n    'id',\n    'project_title', \n    'project_essay', \n    'project_resource_summary',\n    'project_is_approved',\n]\nX = train.drop(cols_to_drop, axis=1, errors='ignore')\ny = train['project_is_approved']\nX_test = test.drop(cols_to_drop, axis=1, errors='ignore')\nid_test = test['id'].values\nfeature_names = list(X.columns)\nprint(X.shape, X_test.shape)\n\ndel train, test\ngc.collect()\n\n# Build the model\ncnt = 0\np_buf = []\nn_splits = 5\nn_repeats = 1\nkf = RepeatedKFold(\n    n_splits=n_splits, \n    n_repeats=n_repeats, \n    random_state=0)\nauc_buf = []   \n\nfor train_index, valid_index in kf.split(X):\n    print('Fold {}/{}'.format(cnt + 1, n_splits))\n    params = {\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc',\n        'max_depth': 16,\n        'num_leaves': 31,\n        'learning_rate': 0.025,\n        'feature_fraction': 0.85,\n        'bagging_fraction': 0.85,\n        'bagging_freq': 5,\n        'verbose': 0,\n        'num_threads': 1,\n        'lambda_l2': 1,\n        'min_gain_to_split': 0,\n    }  \n\n    \n    model = lgb.train(\n        params,\n        lgb.Dataset(X.loc[train_index], y.loc[train_index], feature_name=feature_names),\n        num_boost_round=10000,\n        valid_sets=[lgb.Dataset(X.loc[valid_index], y.loc[valid_index])],\n        early_stopping_rounds=100,\n        verbose_eval=100,\n    )\n\n    if cnt == 0:\n        importance = model.feature_importance()\n        model_fnames = model.feature_name()\n        tuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]\n        tuples = [x for x in tuples if x[1] > 0]\n        print('Important features:')\n        print(tuples[:50])\n\n    p = model.predict(X.loc[valid_index], num_iteration=model.best_iteration)\n    auc = roc_auc_score(y.loc[valid_index], p)\n\n    print('{} AUC: {}'.format(cnt, auc))\n\n    p = model.predict(X_test, num_iteration=model.best_iteration)\n    if len(p_buf) == 0:\n        p_buf = np.array(p)\n    else:\n        p_buf += np.array(p)\n    auc_buf.append(auc)\n\n    cnt += 1\n    if cnt > 0: # Comment this to run several folds\n        break\n    \n    del model\n    gc.collect\n\nauc_mean = np.mean(auc_buf)\nauc_std = np.std(auc_buf)\nprint('AUC = {:.6f} +/- {:.6f}'.format(auc_mean, auc_std))\n\npreds = p_buf/cnt\n\nsubm = pd.DataFrame()\nsubm['id'] = id_test\nsubm['project_is_approved'] = preds\nsubm.to_csv('submission.csv', index=False)\n'''","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dbe0596e-531a-4129-bb0a-a4af0a4220ac","_uuid":"94940981025663f515ce86f7818877c043df0ac1"},"cell_type":"markdown","source":"# 5 GRU-ATT\nThen, we introduce another model called GRU-ATT network. It's come from our machine learning research and will get publish soon. This model is serve for text classification and it got state-of-art performance in some dataset. We comment all the code as it will exceed the running time allow by Kaggle notebook. Let's see what's the output if we apply this model to DonorChoose data."},{"metadata":{"_cell_guid":"5a5e9b86-f489-410a-ac50-2d20ba288291","collapsed":true,"_uuid":"b23f106c42b7ea14dcecabef7bd4331fc6400e2a","_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"#Author-Poon\n'''\nimport numpy as np\nimport pandas as pd\nimport cPickle\nfrom collections import defaultdict\nimport re\n\nfrom bs4 import BeautifulSoup\nimport sys\nimport os\n\nos.environ['KERAS_BACKEND']='theano'\nimport keras\nfrom keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\n\nfrom keras.layers import Embedding\nfrom keras.layers import Dense, Input, Flatten\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\nfrom keras.models import Model\nfrom keras.models import load_model\n\nfrom keras import backend as K\nfrom keras.engine.topology import Layer, InputSpec\nfrom keras import initializers\n\nMAX_SENT_LENGTH = 100\nMAX_SENTS = 50\nMAX_NB_WORDS = 50000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.2\n\ndef clean_str(string):\n    \"\"\"\n    Tokenization/string cleaning for dataset\n    Every dataset is lower cased except\n    \"\"\"\n    string = re.sub(r\"\\\\\", \"\", string)    \n    string = re.sub(r\"\\'\", \"\", string)    \n    string = re.sub(r\"\\\"\", \"\", string)   \n    return string.strip().lower()\n\ndtype = {\n    'id': str,\n    'teacher_id': str,\n    'teacher_prefix': str,\n    'school_state': str,\n    'project_submitted_datetime': str,\n    'project_grade_category': str,\n    'project_subject_categories': str,\n    'project_subject_subcategories': str,\n    'project_title': str,\n    'project_essay_1': str,\n    'project_essay_2': str,\n    'project_essay_3': str,\n    'project_essay_4': str,\n    'project_resource_summary': str,\n    'teacher_number_of_previously_posted_projects': int,\n    'project_is_approved': np.uint8,\n}\n#data_path = os.path.join('..', 'input')\ntrain = pd.read_csv('train.csv', dtype=dtype, low_memory=True)\ntest = pd.read_csv( 'test.csv', dtype=dtype, low_memory=True)\n\ntest['project_is_approved'] = 1\n\ntrain['text'] = train.apply(lambda row: ' '.join([\n    str(row['project_title']), \n    str(row['project_resource_summary']), \n    str(row['project_essay_1']), \n    str(row['project_essay_2']), \n    str(row['project_essay_3']), \n    str(row['project_essay_4'])]), axis=1)\ntest['text'] = test.apply(lambda row: ' '.join([\n    str(row['project_title']), \n    str(row['project_resource_summary']), \n    str(row['project_essay_1']), \n    str(row['project_essay_2']), \n    str(row['project_essay_3']), \n    str(row['project_essay_4'])]), axis=1)\n\ntrain = train.drop([\n    'teacher_id',\n    'teacher_prefix',\n    'school_state',\n    'project_submitted_datetime',\n    'project_grade_category',\n    'project_subject_categories',\n    'project_subject_subcategories',\n    'project_title',\n    'project_essay_1',\n    'project_essay_2',\n    'project_essay_3',\n    'project_essay_4',\n    'project_resource_summary',\n    'teacher_number_of_previously_posted_projects'], axis=1)\ntest = test.drop([\n    'teacher_id',\n    'teacher_prefix',\n    'school_state',\n    'project_submitted_datetime',\n    'project_grade_category',\n    'project_subject_categories',\n    'project_subject_subcategories',\n    'project_title',\n    'project_essay_1',\n    'project_essay_2',\n    'project_essay_3',\n    'project_essay_4',\n    'project_resource_summary',\n    'teacher_number_of_previously_posted_projects'], axis=1)\n\ndata_train = pd.concat([train,test],axis = 0).reset_index()\n\nimport nltk\nfrom nltk import tokenize\n\nreviews = []\nlabels = []\ntexts = []\ninstance_inputs = []\ncomment_id = []\n\n#Return dimension of data_train.review([0]=row)\nfor idx in range(data_train.text.shape[0]):\n    sys.stdout.write(\"\\rProcessing ---- %d\"%idx)\n    sys.stdout.flush()\n    comment_id.append(data_train.id[idx])\n    text = ''.join(data_train.text[idx])\n    #parse the sentences into beautifulsoup object\n    #print text\n    text = BeautifulSoup(text)\n    text = clean_str(text.get_text().encode('ascii','ignore'))\n    #insert clear text into texts array\n    texts.append(text)\n    #Return a sentence-tokenized copy of text( divide string into substring by punkt)\n    sentences = tokenize.sent_tokenize(text)\n    reviews.append(sentences)\n    labels.append(data_train.project_is_approved[idx])\n\n#Class for vectorizing texts (Tokenizer)\ntokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n#list of texts to train on\ntokenizer.fit_on_texts(texts)\n\n#New 3D array filled with zero with (length,15,100) length= num of char\ndata = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\nword_len = np.zeros(10000)\n#enumerate produce a tuple(index)\nfor i, sentences in enumerate(reviews):\n    for j, sent in enumerate(sentences):\n\tword_len[len(sentences)] +=1\n        if j< MAX_SENTS:\n\t    #Split sentence into a list of words\n            wordTokens = text_to_word_sequence(sent)\n            k=0\n            for _, word in enumerate(wordTokens):\n                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n\t\t    #dictionary mapping word to their rank/index (int)\n                    data[i,j,k] = tokenizer.word_index[word]\n                    k=k+1                    \n                    \nword_index = tokenizer.word_index\nprint('Total %s unique tokens.' % len(word_index))\n\n#Converts a class vector (integers) to binary class matrix\nlabels = to_categorical(np.asarray(labels))\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)\n\nnb_validation_samples = 78035\n#split training and validation set\nx_train = data[:-nb_validation_samples]\ny_train = labels[:-nb_validation_samples]\nx_val = data[-nb_validation_samples:]\ny_val = labels[-nb_validation_samples:]\ncomment_id = comment_id[-nb_validation_samples:]\n\nprint('Number of positive and negative reviews in traing and validation set')\nprint y_train.sum(axis=0)\nprint y_val.sum(axis=0)\n\nGLOVE_DIR = \"../../Glove\"\nembeddings_index = {}\nf = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\nfor line in f:\n    #split the vector of 100d\n    values = line.split()\n    #word at values[0]\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nembedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n        \nembedding_layer = Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=MAX_SENT_LENGTH,\n                            trainable=True)\n\nclass AttLayer(Layer):\n    def __init__(self, **kwargs):\n        self.init = initializers.get('normal')\n        super(AttLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape)==3\n        self.W = self.add_weight(name='kernel', \n                                  shape=(input_shape[-1],),\n                                  initializer='normal',\n                                  trainable=True)\n        super(AttLayer, self).build(input_shape) \n\n    def call(self, x, mask=None):\n        eij = K.tanh(K.dot(x, self.W))     \n        ai = K.exp(eij)\n        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n        weighted_input = x*weights.dimshuffle(0,1,'x')\n        return weighted_input.sum(axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])\n\n    def get_config(self):\n        config = {}\n        base_config = super(AttLayer, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\nprint('Shape of data tensor:', data.shape)\nsentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32', name='main_input')\nembedded_sequences = embedding_layer(sentence_input)\nl_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\nl_dense = TimeDistributed(Dense(200))(l_lstm)\nl_att = AttLayer()(l_dense)\nsentEncoder = Model(sentence_input, l_att)\nreview_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\nreview_encoder = TimeDistributed(sentEncoder)(review_input)\nl_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\nl_dense_sent = TimeDistributed(Dense(200))(l_lstm_sent)\nl_att_sent = AttLayer()(l_dense_sent)\npreds = Dense(2, activation='softmax')(l_att_sent)\nmodel = Model(review_input, preds)\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['acc'])\n\nprint(\"model fitting - Hierachical attention network\")\nprint model.summary()\nmodel.fit(x_train, y_train, validation_data=(x_val, y_val),\n          nb_epoch=3, batch_size=100, verbose=2)\n\nscore = model.evaluate(data, labels, batch_size = 100, verbose=1)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\nyFit = model.predict(data, batch_size = 100, verbose=2)\nyFit = yFit[-nb_validation_samples:,1]\nval_pre = pd.DataFrame({'project_is_approved':yFit})\nval_id = pd.DataFrame({'id':comment_id})\ndata_test_merged = pd.concat([val_id,val_pre], axis=1)\ndata_test_merged.to_csv('GRU.csv', encoding='utf-8', index = False)\n'''","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1e0d8fba-7c27-459a-9b4b-7c3e6d59635a","_uuid":"2d542da8eec403bb35bb4761886e9c7c185d5971"},"cell_type":"markdown","source":"# Result"},{"metadata":{"_cell_guid":"dc1c2722-3109-4574-9810-f56131a12ea2","collapsed":true,"_uuid":"0105f4e41db10bd58d459299d695376ce698e83d","trusted":false},"cell_type":"code","source":"LGBM = pd.read_csv('../input/submit/LGBMTFIDF.csv')\nGRU = pd.read_csv('../input/submit/GRU.csv')\ncombine = pd.read_csv('../input/submit/submission_combine7.csv')\n\nres_df = pd.concat([LGBM['project_is_approved'],GRU['project_is_approved'],combine['project_is_approved']],axis=1)\nres_df.columns=['LGBMTDIDF','GRU','Combined']\n\ndata = []\nfor col in res_df.columns:\n    data.append(  go.Box( y=res_df[col], name=col, showlegend=False ) )\n    \ndata.append( go.Scatter( x = res_df.columns, y = res_df.mean(), mode='lines', name='mean' ) )\n\n# IPython notebook\n# py.iplot(data, filename='pandas-box-plot')\n\nurl = py.iplot(data, filename='pandas-box-plot')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2dc2eced-4c9b-47d5-bb3c-434e9f4886d1","_uuid":"285fd5165a6117b3e03fb333bea656642c41ea1f"},"cell_type":"markdown","source":"From the box plot we can see the result from LGBM-TFIDF has the higher mean compare to result from GRU which shows that GRU is more distributed in comparison. By combining result from both model with weights, we can have advantages from both model and a more balanced result.\n## Result submission"},{"metadata":{"_cell_guid":"f16d24b5-8b52-45a9-bf11-3dca5c93633a","collapsed":true,"_uuid":"94ad46d9120b80d8af3c068a85a7812452ad7faa","trusted":false},"cell_type":"code","source":"output = pd.read_csv('../input/submit/submission_combine7.csv')\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"59758b3c-7965-4c55-899c-4e6e8e8d9d52","_uuid":"51bbc440e76528c2518ccbbdd9ce152516046057"},"cell_type":"markdown","source":"----\n# Thanks for watching. We are new to Kaggle, if you find this kernel is helpful, please **VOTE** for our kernel. Your support is Greatly appreciate!!! Also, please feel free to drop us any question or comment.\n\n# To be continued.."}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}