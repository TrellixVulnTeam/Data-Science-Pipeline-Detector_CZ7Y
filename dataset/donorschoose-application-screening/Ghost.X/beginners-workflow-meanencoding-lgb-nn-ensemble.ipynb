{"cells":[{"metadata":{"_uuid":"91644da541ac4fdc0eeb7865951adf76de2db2c6"},"cell_type":"markdown","source":"Last Updated: 04/03/18   \nMean encoding implement on version 19, such as teacher_prefix, school_state,  project_grade_category,   \nwhich performances much better than label encoding on single models."},{"metadata":{"_cell_guid":"c579b7f4-3b2b-4604-9ffc-5d231c93abce","_uuid":"bea7ceb46375be0126c48f7a315566a49a0cf677"},"cell_type":"markdown","source":"I have been in kaggle for half a year, I learned a lot from kaggle kernels, thanks to kagglers, I like these kernels!\n\nIn this kernel, I will show you different ways to do text classifier, including LogisticRegression, RandomForest, lightgbm and neural networks.\n\nThen,  I'll introduce you a powerful tool for model stacking."},{"metadata":{"_cell_guid":"6be2a621-a3fc-4878-9aca-19ddee791f36","_uuid":"226e571798d5cad939533352f4d49ab785dfdb88"},"cell_type":"markdown","source":""},{"metadata":{"_cell_guid":"18d49c54-5161-4be7-b450-b02c4d9072ac","_uuid":"7bfd9b12770d2e4f64129e079c59d064244d20e3","collapsed":true,"trusted":true},"cell_type":"code","source":"# kernel params config\n\n# I set quick_run to True to run a little part of training datasets because of space and time limit, \n# you can run the whole datasets on you local machine.\nquick_run = False\n\n# max features for word embedding\nmax_features = 80000\n# word vector length\nembed_size = 300\n\n# dpcnn config\npj_repeat = 3\nrs_repeat = 1\ndpcnn_folds = 5\nbatch_size = 64\nepochs = 5\nproject_maxlen = 210\nresouse_max_len = 30\nmaxlen = project_maxlen + resouse_max_len\n\n# To complete this kernel in a very short time\nif quick_run == True:\n    max_features = 1000\n    epochs = 2\n    \nEMBEDDING_FILE = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'","execution_count":58,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"import os; os.environ['OMP_NUM_THREADS'] = '4'\nimport gc\nimport numpy as np\nimport pandas as pd\n\nfrom functools import reduce\nfrom functools import partial\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":59,"outputs":[]},{"metadata":{"_cell_guid":"ba33803d-4eaf-4268-9b9c-8587c4fe9dce","_uuid":"d5caaa77c01fc6936f7080408d6c278859085a5a","collapsed":true,"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/donorschoose-application-screening/train.csv')\ntest_df = pd.read_csv('../input/donorschoose-application-screening/test.csv')\nresouse_df = pd.read_csv('../input/donorschoose-application-screening/resources.csv')","execution_count":60,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":false,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%%time\nresouse_df['description'].fillna('', inplace=True)\nres_nums = pd.DataFrame(resouse_df[['id', 'price']].groupby('id').price.agg(['count', \n                                                                             'sum', \n                                                                             'min', \n                                                                             'max', \n                                                                             'mean',  \n                                                                             'std', \n                                                                             lambda x: len(np.unique(x)),])).reset_index()\nres_nums = res_nums.rename(columns={'count': 'res_count', \n                                    'sum': 'res_sum',\n                                    'min':  'res_min', \n                                    'max':  'res_max',\n                                    'mean': 'res_mean', \n                                    'std':  'res_std',\n                                    '<lambda>': 'res_unique' })\nres_descp = resouse_df[['id', 'description']].groupby('id').description.agg([ lambda x: ' '.join(x) ]).reset_index().rename(columns={'<lambda>':'res_description'})\nresouse_df = res_nums.merge(res_descp, on='id', how='left')\ntrain_df = train_df.merge(resouse_df, on='id', how='left')\ntest_df = test_df.merge(resouse_df, on='id', how='left')\ndel res_nums\ndel res_descp\ndel resouse_df\ngc.collect()","execution_count":61,"outputs":[]},{"metadata":{"_cell_guid":"6f2c0a7e-72c5-4833-8f07-2bbb6c4997ce","_uuid":"dbd09d3ed687ad83c6447de1b97661edca3d95ff","collapsed":true,"trusted":true},"cell_type":"code","source":"if quick_run == True:\n    train_df = train_df[:10000]\n    test_df = test_df[:100]","execution_count":62,"outputs":[]},{"metadata":{"_cell_guid":"6bdb19c5-1a11-422b-9f5d-9f01a479773b","_uuid":"644988d39f268dfc80e9e2253e799969728494a3","trusted":true},"cell_type":"code","source":"train_target = train_df['project_is_approved'].values\n# train_df = train_df.drop('project_is_approved', axis=1)\ngc.collect()","execution_count":63,"outputs":[]},{"metadata":{"_cell_guid":"d97fe127-e590-4bfb-aafd-3fd71286f2af","_uuid":"dae3466a38682a7a11e911eae5d21653de149704","trusted":true},"cell_type":"code","source":"train_df.shape, test_df.shape","execution_count":64,"outputs":[]},{"metadata":{"_cell_guid":"12999e41-0bc3-4762-b9f7-b340eb69418a","_uuid":"71c06db13a94ed78eb5dd400a1b9883a559635d5","trusted":true},"cell_type":"code","source":"essay_cols = ['project_essay_1', 'project_essay_2','project_essay_3', 'project_essay_4']\nessay_length_cols = [item+'_len' for item in essay_cols]\n\ndef count_essay_length(df):\n    for col in essay_cols:\n        df[col] = df[col].fillna('unknown')\n        df[col+'_len'] = df[col].apply(len)\n    return df\ntrain_df = count_essay_length(train_df)\ntest_df = count_essay_length(test_df)\n\ntrain_df['project_essay'] = ''\ntest_df['project_essay'] = ''\nfor col in essay_cols:\n    train_df['project_essay'] += train_df[col] + ' '\n    test_df['project_essay'] += test_df[col] + ' '\ntrain_df[['project_essay']].head()","execution_count":65,"outputs":[]},{"metadata":{"_cell_guid":"6874ec1f-1377-48e0-aba8-7e7912d945ba","_uuid":"a6ed50988d7b3d36534ffe09593e949ed5604ece","collapsed":true,"trusted":true},"cell_type":"code","source":"time_cols = ['sub_year', 'sub_month', 'sub_day', 'sub_hour', 'sub_dayofweek', 'sub_dayofyear']\ndef time_stamp_features(df):\n    time_df = pd.to_datetime(df['project_submitted_datetime'])\n    df['sub_year'] = time_df.apply(lambda x: x.year)\n    df['sub_month'] = time_df.apply(lambda x: x.month)\n    df['sub_day'] = time_df.apply(lambda x: x.day)\n    df['sub_hour'] = time_df.apply(lambda x: x.hour)\n    df['sub_dayofweek'] = time_df.apply(lambda x: x.dayofweek)\n    df['sub_dayofyear'] = time_df.apply(lambda x: x.dayofyear)\n    return df\ntrain_df = time_stamp_features(train_df)\ntest_df = time_stamp_features(test_df)","execution_count":66,"outputs":[]},{"metadata":{"_cell_guid":"b3cbcaaf-9357-45c3-ae24-884a257e3be1","_uuid":"3d198e580f340b9ced11c91bba0b004856e85b4c","collapsed":true,"trusted":true},"cell_type":"code","source":"str_cols = ['teacher_id', 'teacher_prefix', 'school_state',\n       'project_submitted_datetime', 'project_grade_category',\n       'project_subject_categories', 'project_subject_subcategories',\n       'project_title', 'project_resource_summary','res_description', 'project_essay']\nnum_cols = ['teacher_number_of_previously_posted_projects', \n            'res_count', 'res_sum', 'res_min', 'res_max', 'res_mean', 'res_std', 'res_unique'] + essay_length_cols \ntrain_df[str_cols] =train_df[str_cols].fillna('unknown')\ntrain_df[num_cols] = train_df[num_cols].fillna(0)\ntest_df[str_cols] =test_df[str_cols].fillna('unknown')\ntest_df[num_cols] = test_df[num_cols].fillna(0)\nfor col in str_cols:\n    train_df[col] = train_df[col].str.lower()\n    test_df[col] = test_df[col].str.lower()","execution_count":67,"outputs":[]},{"metadata":{"_cell_guid":"d40ea164-a70a-4d6d-b3f7-82ee0f28d9f8","_uuid":"f014e44495ec06e16576a28e3a90ae8436ef3e01","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nstd_scaler = MinMaxScaler()\ntrain_none_text_features = std_scaler.fit_transform(train_df[num_cols].values)\ntest_none_text_features = std_scaler.transform(test_df[num_cols].values)\n\ntrain_df = train_df.drop(num_cols, axis=1)\ntest_df = test_df.drop(num_cols, axis=1)\ndel std_scaler\ngc.collect()","execution_count":68,"outputs":[]},{"metadata":{"_cell_guid":"2e1d5210-6614-446d-bc0e-39d33c1cef60","_uuid":"c0b7917d7b38aef50ae985de79a1c3a82eaf50ce","trusted":true},"cell_type":"code","source":"train_df['project_descp'] = train_df['project_subject_categories'] + ' ' + train_df['project_subject_subcategories'] + ' ' + train_df['project_title'] + ' ' + train_df['project_resource_summary'] + ' ' + train_df['project_essay']\ntest_df['project_descp'] = test_df['project_subject_categories'] + ' ' + test_df['project_subject_subcategories'] + ' ' + test_df['project_title'] + ' ' + test_df['project_resource_summary'] + ' ' + test_df['project_essay']\ntrain_df = train_df.drop([ 'project_essay'], axis=1)\ntest_df = test_df.drop(['project_essay'], axis=1)\ngc.collect()","execution_count":69,"outputs":[]},{"metadata":{"_cell_guid":"65f79999-3f92-4e47-8753-9b398ccdca89","_uuid":"64653cb8f5d731039fac2777bae0decc4760fa6d","scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_cols = [\n    # 'teacher_id',\n    'teacher_prefix', \n    'school_state', \n    'project_grade_category', \n    'project_subject_categories', \n    'project_subject_subcategories'\n] + time_cols\n\ndef mean_encoding(train_df, test_df, col):\n    gp = train_df.groupby(col)['project_is_approved'].mean().reset_index().rename(columns={'project_is_approved': col+'_mean'})\n    train_df = pd.merge(train_df, gp, how='left', on=[col])\n    test_df = pd.merge(test_df, gp, how='left', on=[col])\n    return train_df, test_df\nfor col in label_cols:\n    train_df, test_df = mean_encoding(train_df, test_df, col)\nlabel_mean_cols = [item+'_mean' for item in label_cols]\ntrain_label_features = train_df[label_mean_cols].values\ntest_df[label_mean_cols] = test_df[label_mean_cols].fillna(test_df[label_mean_cols].mean())\ntest_label_features = test_df[label_mean_cols].values\n\n'''for col in label_cols:\n    le = LabelEncoder()\n    le.fit(np.hstack([train_df[col].values, test_df[col].values]))\n    train_df[col] = le.transform(train_df[col])\n    test_df[col] = le.transform(test_df[col])\ntrain_label_features = train_df[label_cols].values\ntest_label_features = test_df[label_cols].values'''\n\ntrain_df = train_df.drop(label_cols, axis=1)\ntest_df = test_df.drop(label_cols, axis=1)\ntrain_df = train_df.drop(label_mean_cols, axis=1)\ntest_df = test_df.drop(label_mean_cols, axis=1)\n# del le\ngc.collect()","execution_count":70,"outputs":[]},{"metadata":{"_cell_guid":"d2642662-a1ad-45cc-a1b0-c2b89e050850","_uuid":"bccb3a91c63337abb774b7730bcf4a68d05ba456","trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":71,"outputs":[]},{"metadata":{"_cell_guid":"48d21f37-f24d-4736-8f24-229fc986a741","_uuid":"20c1f1d90ef68f2339c88ae0613de452401b4eb4","trusted":true},"cell_type":"code","source":"%%time\nimport re\n\ndef clean_descp(descp):\n    low_case = re.compile('([a-z]*)')\n    words = low_case.findall(descp)\n    # words = [item for item in filter(lambda x: x not in stopwords, words)]\n    return ' '.join(words)\n\nfor col in essay_cols:\n    train_df[col] = train_df[col].apply(clean_descp)\n    test_df[col]  = test_df[col].apply(clean_descp)\ntrain_df['project_descp']  = train_df['project_descp'].apply(clean_descp)\ntest_df['project_descp']  = test_df['project_descp'].apply(clean_descp)","execution_count":72,"outputs":[]},{"metadata":{"_cell_guid":"baf67c32-05ee-43c2-b769-684e33f4eefe","_uuid":"175971b026466bea10e5a022e2cb1e62c759ebdc"},"cell_type":"markdown","source":"The cell below is  from @Ehsan's [Ultimate Feature Engineering -> XGB+LGB [LB 0.813]](https://www.kaggle.com/safavieh/ultimate-feature-engineering-xgb-lgb-lb-0-813)  \nExtracting text features one by one instead of processing them together can improve predicting score.\n\n"},{"metadata":{"_cell_guid":"f23244ee-55eb-40de-b95b-df936bc8feef","_uuid":"b2120c989d1c4ce65b8932a656d4b3a8f462088e","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\ndef getTextFeatures(Col, max_features=10000):\n    print('processing: ', Col)\n    vectorizer = CountVectorizer(stop_words=None,\n                                 # preprocessor=wordPreProcess,\n                                 max_features=max_features,\n                                 binary=True,\n                                 ngram_range=(1,2))\n    train_features = vectorizer.fit_transform(train_df[Col])\n    test_features = vectorizer.transform(test_df[Col])\n    return train_features, test_features\ntrain_text_es1, test_text_es1 = getTextFeatures('project_essay_1', max_features=5000)\ntrain_text_es2, test_text_es2 = getTextFeatures('project_essay_2', max_features=7000)\ntrain_text_prs, test_text_prs = getTextFeatures('project_resource_summary', max_features=1000)\ntrain_text_rsd, test_text_rsd = getTextFeatures('res_description', max_features=1000)\ntrain_text_pjt, test_text_pjt = getTextFeatures('project_title', max_features=500)","execution_count":73,"outputs":[]},{"metadata":{"_cell_guid":"ae7319b5-8f7d-4867-955f-b9f7320d24a9","_uuid":"0b4422cd51bdb541985e141b133c7b455804a495","trusted":true},"cell_type":"code","source":"from scipy.sparse import csr_matrix, hstack\ntrain_text_features = hstack([train_text_es1, train_text_es2, train_text_prs, train_text_rsd, train_text_pjt]).tocsr()\ntest_text_features = hstack([test_text_es1, test_text_es2, test_text_prs, test_text_rsd, test_text_pjt]).tocsr()\ntrain_text_features.shape, test_text_features.shape","execution_count":74,"outputs":[]},{"metadata":{"_cell_guid":"b0a57dba-a0f3-4429-9fca-394f7769831a","_uuid":"988b194d6526b4910f532b49bc4124a473642df7","trusted":true},"cell_type":"code","source":"train_features = hstack([train_none_text_features, train_label_features, train_text_features]).tocsr()\ntest_features = hstack([test_none_text_features, test_label_features, test_text_features]).tocsr()\n\ndel train_text_es1\ndel train_text_es2\ndel train_text_prs\ndel train_text_rsd\ndel train_text_pjt\ndel test_text_es1\ndel test_text_es2\ndel test_text_prs\ndel test_text_rsd\ndel test_text_pjt\ndel train_text_features\ndel test_text_features\ngc.collect()","execution_count":75,"outputs":[]},{"metadata":{"_cell_guid":"23259f07-5a65-4cb7-a739-559d0011ee70","_uuid":"9c9a254a3a2457556644b3de3ad5480dd0ecd451"},"cell_type":"markdown","source":"It's convenient to do stacking on any model with the class below.\n\nI named it qiaofeng to pay tribute to my big bother, sun e phone."},{"metadata":{"_cell_guid":"8e142ad5-a877-463f-b8a3-5b3e1987a87f","_uuid":"9419f8d208258a986665e8959268271347e0fdd0","collapsed":true,"trusted":true},"cell_type":"code","source":"from functools import reduce\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nclass qiaofeng_kfold_stack:\n    def __init__(self, train, train_target, test, split_target, model, preprocess_func=None, score_func=None, kfolds=5, random_seed=9527, logger=None):\n        self.train = train\n        self.train_target = train_target\n        self.test = test\n        self.split_target = split_target\n        self.model = model\n        self.preprocess_func = preprocess_func\n        self.score_func = score_func\n        self.kfolds = kfolds\n        self.random_seed = random_seed\n        self.logger= logger\n        self.skf = KFold(n_splits=self.kfolds, random_state= self.random_seed)\n        self.predict_test_kfolds = []\n        self.predict_valid_kfolds = np.zeros((self.train.shape[0]))\n    def print_params(self):\n        print('kfolds : %s' % self.kfolds)\n        print('random seed : %s' % self.random_seed)\n    def preprocess(self):\n        if self.preprocess_func != None:\n            self.train, self.test = self.preprocess_func(self.train, self.test)\n    def score(self, target, predict):\n        return self.score_func(target, predict)\n    def model_fit(self, train, train_target):\n        self.model.fit(train, train_target)\n    def model_predict(self, dataset):\n        return self.model.predict(dataset)\n    def model_fit_predict(self, train, train_target, dataset):\n        self.model_fit(train, train_target)\n        predict_train = None#self.model_predict(train)\n        predict_valid = self.model_predict(dataset)\n        predict_test = self.model_predict(self.test)\n        return predict_train, predict_valid, predict_test\n    def clear_predicts(self):\n        self.predict_test_kfolds = []\n        self.predict_valid_kfolds = np.zeros((self.train.shape[0]))\n    def model_train_with_kfold(self):\n        self.clear_predicts()\n        for (folder_index, (train_index, valid_index)) in enumerate(self.skf.split(self.train)):\n            x_train, x_valid = self.train[train_index], self.train[valid_index]\n            y_train, y_valid = self.train_target[train_index], self.train_target[valid_index]\n            predict_train, predict_valid, predict_test = self.model_fit_predict(x_train, y_train, x_valid)\n            self.predict_test_kfolds.append(predict_test)\n            self.predict_valid_kfolds[valid_index] = predict_valid\n            if self.logger != None:\n                valid_score = self.score(y_valid, predict_valid)\n                # train_score = self.score(y_train, predict_train)\n                self.logger('Fold: %s, valid score: %s' % (folder_index, valid_score))\n    def predict_test_mean(self):\n        return reduce(lambda x,y:x+y, self.predict_test_kfolds)  / self.kfolds","execution_count":76,"outputs":[]},{"metadata":{"_cell_guid":"b962c1b7-4762-4280-887c-51c2e2fd15c7","_uuid":"4b964c4a53eb45c48e03ab1c4ac2613e89f24f89","collapsed":true,"trusted":true},"cell_type":"code","source":"class qiaofeng_predict_prob(qiaofeng_kfold_stack):\n    def model_predict(self, dataset):\n        return self.model.predict_proba(dataset)[:,1]","execution_count":77,"outputs":[]},{"metadata":{"_cell_guid":"81014f88-b89d-4f43-87d6-ebe24ebe7ef6","_uuid":"ce17905f2bf339e1efee89d5260e1bd8dc3ce0a3","scrolled":true,"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier( n_jobs=4, \n                                criterion=\"entropy\",\n                                max_depth=20, \n                                n_estimators=100, \n                                max_features='sqrt', \n                                random_state=233,\n                                # min_samples_leaf = 50\n                                )\nqiaofeng_rf = qiaofeng_predict_prob(train=train_features, train_target=train_target, test=test_features, kfolds=5,split_target=train_target,\n                                          score_func=roc_auc_score, logger=print, model=model)\nqiaofeng_rf.model_train_with_kfold()\npred_valid_rf = qiaofeng_rf.predict_valid_kfolds\npred_test_avg_rf = qiaofeng_rf.predict_test_mean()\ndel qiaofeng_rf\ngc.collect()","execution_count":78,"outputs":[]},{"metadata":{"_cell_guid":"b95eace8-858b-4ab4-ae58-b5b8d4dba45f","_uuid":"763c66bb2b19cfaaa2b482015a22b5f5756f567c","trusted":true},"cell_type":"code","source":"%%time\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\nclass qiaofeng_lgb_clf(qiaofeng_kfold_stack):\n    def model_fit(self, train, train_target):\n        X_tra, X_val, y_tra, y_val = train_test_split(train, train_target, train_size=0.95, random_state=233)\n        self.model = lgb.LGBMClassifier( n_jobs=4,\n                                         max_depth=4,\n                                         metric=\"auc\",\n                                         n_estimators=1000,\n                                         num_leaves=15,\n                                         boosting_type=\"gbdt\",\n                                         learning_rate=0.05,\n                                         feature_fraction=0.45,\n                                         colsample_bytree=0.45,\n                                         bagging_fraction=0.8,\n                                         bagging_freq=5,\n                                         reg_lambda=0.2)\n        self.model.fit(X=X_tra, y=y_tra,\n                      eval_set=[(X_val, y_val)],\n                      early_stopping_rounds=200,\n                      verbose=False)\n    def model_predict(self, dataset):\n        return self.model.predict_proba(dataset)[:,1]\n\nqiaofeng_lgb = qiaofeng_lgb_clf(train=train_features, train_target=train_target, test=test_features, kfolds=5,split_target=train_target,\n                                          score_func=roc_auc_score, logger=print, model=None)\nqiaofeng_lgb.model_train_with_kfold()\npred_valid_lgb = qiaofeng_lgb.predict_valid_kfolds\npred_test_avg_lgb = qiaofeng_lgb.predict_test_mean()\ndel qiaofeng_lgb\ngc.collect()","execution_count":79,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76d3bf782d885079f19c313f1bc6fa4539610529"},"cell_type":"code","source":"%%time\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\nclass qiaofeng_lgb_clf(qiaofeng_kfold_stack):\n    def model_fit(self, train, train_target):\n        X_tra, X_val, y_tra, y_val = train_test_split(train, train_target, train_size=0.95, random_state=233)\n        self.model = lgb.LGBMClassifier( n_jobs=4,\n                                         max_depth=4,\n                                         metric=\"auc\",\n                                         n_estimators=1000,\n                                         num_leaves=15,\n                                         boosting_type=\"gbdt\",\n                                         learning_rate=0.05,\n                                         feature_fraction=0.45,\n                                         colsample_bytree=0.45,\n                                         bagging_fraction=0.8,\n                                         bagging_freq=5,\n                                         reg_lambda=0.2)\n        self.model.fit(X=X_tra, y=y_tra,\n                      eval_set=[(X_val, y_val)],\n                      early_stopping_rounds=200,\n                      verbose=False)\n    def model_predict(self, dataset):\n        return self.model.predict_proba(dataset)[:,1]\n\nqiaofeng_lgb = qiaofeng_lgb_clf(train=train_features.astype(np.bool).astype(np.float32), \n                                train_target=train_target, \n                                test=test_features.astype(np.bool).astype(np.float32), \n                                kfolds=5,split_target=train_target,\n                                score_func=roc_auc_score, \n                                logger=print, model=None)\nqiaofeng_lgb.model_train_with_kfold()\npred_valid_lgb_bool = qiaofeng_lgb.predict_valid_kfolds\npred_test_avg_lgb_bool = qiaofeng_lgb.predict_test_mean()\ndel qiaofeng_lgb\ngc.collect()","execution_count":80,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1455172e40dd0964a0a7b37fdfebd2297a1b5ab"},"cell_type":"code","source":"pd.Series(pred_valid_lgb).corr(pd.Series(pred_valid_lgb_bool)), pd.Series(pred_test_avg_lgb).corr(pd.Series(pred_test_avg_lgb_bool))","execution_count":81,"outputs":[]},{"metadata":{"_cell_guid":"61de2bd8-7043-4b80-aea9-1312afff67ea","_uuid":"7c525cda904d7677a935473908474e321e54c133","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom matplotlib import pyplot as plt\nfrom wordbatch.models import FTRL, FM_FTRL\n\nclf = FTRL(alpha=0.01, beta=0.1, L1=0.1, L2=1000.0, D=train_features.shape[1], iters=1, \n                 inv_link=\"identity\", threads=4)\nX_tra, X_val, y_tra, y_val = train_test_split(train_features, train_target, train_size=0.95, random_state=7)\n\ntrain_scores = []\nvalid_scores = []\nfor i in range(10):\n    clf.fit(X_tra, y_tra)\n    train_predict = clf.predict(X_tra)\n    tp = train_predict - train_predict.min()\n    tp = tp / tp.max()\n    valid_predict = clf.predict(X_val)\n    vp = valid_predict - valid_predict.min()\n    vp = vp / vp.max()\n    train_auc = roc_auc_score(y_tra, tp)\n    valid_auc = roc_auc_score(y_val, vp)\n    train_scores.append(train_auc)\n    valid_scores.append(valid_auc)\n    print(i+1, train_auc, valid_auc)\nplt.clf()\nplt.figure(figsize=(10,10))\nplt.plot(np.arange(len(train_scores)), train_scores, label='train')\nplt.plot(np.arange(len(valid_scores)), valid_scores, label='valid')\nvalid_max = max(valid_scores)\nplt.title('%s - %s'%(valid_scores.index(valid_max)+1, valid_max))\nplt.legend()\nplt.show()","execution_count":82,"outputs":[]},{"metadata":{"_cell_guid":"1a7a90b3-1977-4bc3-bf71-f3aa8064d2d3","_uuid":"dd5ff5702a95cae55fbe85895a3b483fac82d499","trusted":true},"cell_type":"code","source":"%%time\nfrom wordbatch.models import FTRL, FM_FTRL\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass qiaofeng_ftrl(qiaofeng_kfold_stack):\n    def model_fit(self, train, train_target):\n        self.model = FTRL(alpha=0.01, beta=0.1, L1=0.1, L2=1000.0, D=train_features.shape[1], iters=2, \n                             inv_link=\"identity\", threads=4)\n        self.model.fit(train, train_target)\n    def model_predict(self, dataset):\n        predict = self.model.predict(dataset)\n        pred_nan = np.isnan(predict)\n        if pred_nan.shape[0] == predict.shape[0]:\n            predict[pred_nan] = 0\n        else:\n            predict[pred_nan] = np.nanmean(predict)\n        return sigmoid(predict)\n        \nqiaofeng_ftrl = qiaofeng_ftrl(train=train_features, train_target=train_target, test=test_features, kfolds=20, split_target=train_target,\n                                          score_func=roc_auc_score, logger=print, model=None)\nqiaofeng_ftrl.model_train_with_kfold()\npred_valid_ftrl = qiaofeng_ftrl.predict_valid_kfolds\npred_test_avg_ftrl = qiaofeng_ftrl.predict_test_mean()","execution_count":83,"outputs":[]},{"metadata":{"_cell_guid":"e31a0535-c2ed-4ec0-9d76-db2d1b6393a7","_uuid":"0e1bebcce1b322c9e3b63eed3d275be975c7a621","trusted":true},"cell_type":"code","source":"del train_features\ndel test_features\ngc.collect()","execution_count":84,"outputs":[]},{"metadata":{"_cell_guid":"1a87898a-dd6e-4b51-baca-3a15e4ccaf38","_uuid":"6fdf6cb11180fa5ce676651a07a8c209f9907229","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.utils import shuffle\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\nfrom keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, LSTM, Dropout, BatchNormalization\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_df['project_descp']) + list(test_df['project_descp']) + list(train_df['res_description']) + list(test_df['res_description']))\ntrain_pj = sequence.pad_sequences(tokenizer.texts_to_sequences(train_df['project_descp']), maxlen=project_maxlen)\ntest_pj = sequence.pad_sequences(tokenizer.texts_to_sequences(test_df['project_descp']), maxlen=project_maxlen)\n\ntrain_res = sequence.pad_sequences(tokenizer.texts_to_sequences(train_df['res_description']), maxlen=resouse_max_len)\ntest_res = sequence.pad_sequences(tokenizer.texts_to_sequences(test_df['res_description']), maxlen=resouse_max_len)\n\ntrain_seq = np.hstack([train_pj, train_res])\ntest_seq = np.hstack([test_pj, test_res])","execution_count":85,"outputs":[]},{"metadata":{"_cell_guid":"9d7c7fdf-7516-430e-81bd-0a0455c2d769","_uuid":"0231a8800c1769cafd6026c344696842b5e0df1c","collapsed":true,"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\n\nclass RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            logs['roc_auc_val'] = score\n            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))","execution_count":86,"outputs":[]},{"metadata":{"_cell_guid":"1342f5ce-51de-4854-9924-41af90220cd1","_uuid":"d4650f0b59f494331039635b72763031b6c1c427","trusted":true},"cell_type":"code","source":"train_num_features = np.hstack([train_none_text_features, train_label_features])\ntest_num_features = np.hstack([test_none_text_features, test_label_features])\n\ndel train_none_text_features\ndel train_label_features\ndel test_none_text_features\ndel test_label_features\ngc.collect()","execution_count":87,"outputs":[]},{"metadata":{"_cell_guid":"983d863c-61a1-497f-8670-0e222a14174d","_uuid":"7b58c94ed87bda8c6683ee77206490b0e76cc106","collapsed":true,"trusted":true},"cell_type":"code","source":"train_seq = np.hstack([train_seq, train_num_features])\ntest_seq = np.hstack([test_seq, test_num_features])","execution_count":88,"outputs":[]},{"metadata":{"_cell_guid":"59432f01-3e5e-4705-aca9-8f9adffa834f","_uuid":"1e4d994acf87282cf386163d99bf9145972fbb3d","collapsed":true,"trusted":true},"cell_type":"code","source":"gc.collect()\ngc.disable()","execution_count":89,"outputs":[]},{"metadata":{"_cell_guid":"1c777020-1445-4a91-8282-634b275dc4c4","_uuid":"2344c9876e03e66f16ca22f048d9bdbf7394b890","collapsed":true,"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\nfrom keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, LSTM, Dropout, BatchNormalization,Conv1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\nfrom keras.layers import Input, Dense, Embedding, MaxPooling1D, Conv1D, SpatialDropout1D\nfrom keras.layers import add, Dropout, PReLU, BatchNormalization, GlobalMaxPooling1D\n\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras import optimizers\nfrom keras import initializers, regularizers, constraints, callbacks\n\nif 1:\n    def get_model():\n        session_conf = tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4)\n        K.set_session(tf.Session(graph=tf.get_default_graph(), config=session_conf))\n\n        filter_nr = 32\n        filter_size = 3\n        max_pool_size = 3\n        max_pool_strides = 2\n        dense_nr = 64\n        spatial_dropout = 0.2\n        dense_dropout = 0.05\n        train_embed = False\n        \n        project = Input(shape=(project_maxlen,), name='project')\n        emb_project = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=train_embed)(project)\n        emb_project = SpatialDropout1D(spatial_dropout)(emb_project)\n        \n        pj_block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(emb_project)\n        pj_block1 = BatchNormalization()(pj_block1)\n        pj_block1 = PReLU()(pj_block1)\n        pj_block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(pj_block1)\n        pj_block1 = BatchNormalization()(pj_block1)\n        pj_block1 = PReLU()(pj_block1)\n        \n        #we pass embedded comment through conv1d with filter size 1 because it needs to have the same shape as block output\n        #if you choose filter_nr = embed_size (300 in this case) you don't have to do this part and can add emb_comment directly to block1_output\n        pj_resize_emb = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear')(emb_project)\n        pj_resize_emb = PReLU()(pj_resize_emb)\n            \n        pj_block1_output = add([pj_block1, pj_resize_emb])\n        # pj_block1_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(pj_block1_output)\n        for _ in range(pj_repeat):  \n            pj_block1_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(pj_block1_output)\n            pj_block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(pj_block1_output)\n            pj_block2 = BatchNormalization()(pj_block2)\n            pj_block2 = PReLU()(pj_block2)\n            pj_block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(pj_block2)\n            pj_block2 = BatchNormalization()(pj_block2)\n            pj_block2 = PReLU()(pj_block2)\n            pj_block1_output = add([pj_block2, pj_block1_output])\n        \n        resouse = Input(shape=(resouse_max_len,), name='resouse')\n        emb_resouse = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=train_embed)(resouse)\n        emb_resouse = SpatialDropout1D(spatial_dropout)(emb_resouse)\n        \n        rs_block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(emb_resouse)\n        rs_block1 = BatchNormalization()(rs_block1)\n        rs_block1 = PReLU()(rs_block1)\n        rs_block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(rs_block1)\n        rs_block1 = BatchNormalization()(rs_block1)\n        rs_block1 = PReLU()(rs_block1)\n\n        #we pass embedded comment through conv1d with filter size 1 because it needs to have the same shape as block output\n        #if you choose filter_nr = embed_size (300 in this case) you don't have to do this part and can add emb_comment directly to block1_output\n        rs_resize_emb = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear')(emb_resouse)\n        rs_resize_emb = PReLU()(rs_resize_emb)\n\n        rs_block1_output = add([rs_block1, rs_resize_emb])\n        # rs_block1_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(rs_block1_output)\n        for _ in range(rs_repeat):  \n            rs_block1_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(rs_block1_output)\n            rs_block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(rs_block1_output)\n            rs_block2 = BatchNormalization()(rs_block2)\n            rs_block2 = PReLU()(rs_block2)\n            rs_block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(rs_block2)\n            rs_block2 = BatchNormalization()(rs_block2)\n            rs_block2 = PReLU()(rs_block2)\n            rs_block1_output = add([rs_block2, rs_block1_output])\n            \n\n        pj_output = GlobalMaxPooling1D()(pj_block1_output)\n        pj_output = BatchNormalization()(pj_output)\n        rs_output = GlobalMaxPooling1D()(rs_block1_output)\n        rs_output = BatchNormalization()(rs_output)\n        inp_num = Input(shape=(train_seq.shape[1]-maxlen, ), name='num_input')\n        bn_inp_num = BatchNormalization()(inp_num)\n        conc = concatenate([pj_output, rs_output, bn_inp_num])\n        \n        output = Dense(dense_nr, activation='linear')(conc)\n        output = BatchNormalization()(output)\n        output = PReLU()(output)\n        output = Dropout(dense_dropout)(output)\n        output = Dense(1, activation='sigmoid')(output)\n        model = Model(inputs=[project, resouse, inp_num], outputs=output)\n        model.compile(loss='binary_crossentropy', \n                    optimizer='nadam',\n                    metrics=['accuracy'])\n\n        return model","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2b082624-a135-4253-a759-fc44d2d393ef","_uuid":"89b3589aff838e018bf57630ce9e2b95f29c42ea","scrolled":true,"trusted":true},"cell_type":"code","source":"class qiaofeng_dpcnn(qiaofeng_kfold_stack):\n    def model_fit_predict(self, train, train_target, valid):\n        self.model = get_model()\n        early_stopping = EarlyStopping(monitor='roc_auc_val', patience=1, mode='max',min_delta=0.0005)  \n        X_tra, X_val, y_tra, y_val = train_test_split(train, train_target, train_size=0.98, random_state=233)\n        X_tra = { 'project' : X_tra[:,:project_maxlen], 'resouse' : X_tra[:,project_maxlen:project_maxlen+resouse_max_len], 'num_input' : X_tra[:,maxlen:]  }\n        X_val = { 'project' : X_val[:,:project_maxlen], 'resouse' : X_val[:,project_maxlen:project_maxlen+resouse_max_len], 'num_input' : X_val[:,maxlen:]  }\n        x_test = { 'project' : self.test[:,:project_maxlen], 'resouse' : self.test[:,project_maxlen:project_maxlen+resouse_max_len], 'num_input' : self.test[:,maxlen:]  }\n        valid = { 'project' : valid[:,:project_maxlen], 'resouse' : valid[:,project_maxlen:project_maxlen+resouse_max_len], 'num_input' : valid[:,maxlen:]  }\n        \n        RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n        hist = self.model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n                             callbacks=[RocAuc, early_stopping], verbose=2)\n        predict_train = None#self.model.predict(X_tra, batch_size=1024)[:, 0]\n        predict_valid = self.model.predict(valid, batch_size=1024)[:, 0]\n        predict_test = self.model.predict(x_test, batch_size=1024)[:, 0]\n        return predict_train, predict_valid, predict_test            \n\ndpcnn_kfold_model = qiaofeng_dpcnn(train=train_seq, train_target=train_target, test=test_seq, kfolds=dpcnn_folds, split_target=train_target,\n                                          score_func=roc_auc_score, logger=print, model=None)\ndpcnn_kfold_model.model_train_with_kfold()\npred_valid_dpcnn = dpcnn_kfold_model.predict_valid_kfolds\npred_test_avg_dpcnn = dpcnn_kfold_model.predict_test_mean()\n    \ndel dpcnn_kfold_model\ngc.enable()\ngc.collect()\ngc.disable()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"48eabd09-354f-42e5-8569-53075c75e6b5","_uuid":"c871f0df9f9259cec4dfef88a08c14eb2547aec4","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\npredict_valid_list = [pred_valid_rf, pred_valid_lgb, pred_valid_dpcnn, pred_valid_ftrl]\npredict_test_list = [pred_test_avg_rf, pred_test_avg_lgb, pred_test_avg_dpcnn, pred_test_avg_ftrl]\n\nvalid_results = np.hstack([scaler.fit_transform(item.reshape((item.shape[0], 1))) for item in predict_valid_list])\ntest_results = np.hstack([scaler.fit_transform(item.reshape((item.shape[0], 1))) for item in predict_test_list])\ntrain_features = np.hstack([valid_results ])\ntest_features = np.hstack([test_results ])\n\nlgb_model = lgb.LGBMClassifier(  n_jobs=4,\n                                 max_depth=4,\n                                 metric=\"auc\",\n                                 n_estimators=400,\n                                 num_leaves=10,\n                                 boosting_type=\"gbdt\",\n                                 learning_rate=0.01,\n                                 feature_fraction=0.45,\n                                 colsample_bytree=0.45,\n                                 bagging_fraction=0.4,\n                                 bagging_freq=5,\n                                 reg_lambda=0.2)\nX_tra, X_val, y_tra, y_val = train_test_split(train_features, train_target, train_size=0.8, random_state=233)\nlgb_model.fit(X=X_tra, y=y_tra,\n              eval_set=[(X_val, y_val)],\n              verbose=False)\nprint('Valid Score is %.4f' % roc_auc_score(y_val, lgb_model.predict_proba(X_val)[:,1]))\nfinal_predict = lgb_model.predict_proba(test_features)[:,1]\n\nif quick_run == False:\n    sample_df = pd.read_csv('../input/donorschoose-application-screening/sample_submission.csv')\n    sample_df['project_is_approved'] = final_predict\n    sample_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"b1c89a2abab0c907b6192caf67599d1854138b00"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\npredict_valid_list = [pred_valid_rf, pred_valid_lgb, pred_valid_dpcnn, pred_valid_ftrl, pred_valid_lgb_bool]\npredict_test_list = [pred_test_avg_rf, pred_test_avg_lgb, pred_test_avg_dpcnn, pred_test_avg_ftrl, pred_test_avg_lgb_bool]\n\nvalid_results = np.hstack([scaler.fit_transform(item.reshape((item.shape[0], 1))) for item in predict_valid_list])\ntest_results = np.hstack([scaler.fit_transform(item.reshape((item.shape[0], 1))) for item in predict_test_list])\ntrain_features = np.hstack([valid_results ])\ntest_features = np.hstack([test_results ])\n\nlgb_model = lgb.LGBMClassifier(  n_jobs=4,\n                                 max_depth=4,\n                                 metric=\"auc\",\n                                 n_estimators=400,\n                                 num_leaves=10,\n                                 boosting_type=\"gbdt\",\n                                 learning_rate=0.01,\n                                 feature_fraction=0.45,\n                                 colsample_bytree=0.45,\n                                 bagging_fraction=0.4,\n                                 bagging_freq=5,\n                                 reg_lambda=0.2)\nX_tra, X_val, y_tra, y_val = train_test_split(train_features, train_target, train_size=0.8, random_state=233)\nlgb_model.fit(X=X_tra, y=y_tra,\n              eval_set=[(X_val, y_val)],\n              verbose=False)\nprint('Valid Score is %.4f' % roc_auc_score(y_val, lgb_model.predict_proba(X_val)[:,1]))\nfinal_predict = lgb_model.predict_proba(test_features)[:,1]\n\nif quick_run == False:\n    sample_df = pd.read_csv('../input/donorschoose-application-screening/sample_submission.csv')\n    sample_df['project_is_approved'] = final_predict\n    sample_df.to_csv('submission_with_lgb_bool.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eb233238-b4d5-4299-bcbb-7fe536fe4efc","_uuid":"949e32652b401ef695bd3b77000ed1e5ff372802"},"cell_type":"markdown","source":"# TODO LIST:\n\n1.  Text propressing, move stop words, word stem.\n2.  Meta features of texts,  text length, word length and so on.\n3. More nn networks, such as BiRNN, RCNN, which are widely used in Toxic Comment Classification Challenge.\n4. Try MLP and bool features, like [https://www.kaggle.com/lopuhin/mercari-golf-0-3875-cv-in-75-loc-1900-s](http://)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}