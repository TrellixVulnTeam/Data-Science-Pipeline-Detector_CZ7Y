{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"I'd like to first thank DonorsChoose.org and Kaggle for providing us with an interesting, real world dataset of text based applications to play with!\n\nA little about my path: last fall I enrolled in [Andrew Ng's Deep Learning course](https://www.coursera.org/specializations/deep-learning) on [Coursera](https://www.coursera.org) and fell under the spell of machine learning.  After Coursera, I read a number of Python machine learning and DNN books.  But my real machine learning education has been through Kaggle.  I participated in the [Porto Seguro](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction)  and [Recruit Restaurant](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting) competitions, doing pretty badly but gaining a lot of knowledge through the kernels and discussions shared by other participants.  I particularly enjoy reading about the innovative top solutions posted at the end of the competitions, even competitions I didn't participate in.  Over time I have built up a bag of tricks, and some reusable ML utility code.  DonorsChoose.org was my introduction to NLP and a great dataset for me to put these tools to use in.\n\n**Summary of  Models:**\n\nMy final ensemble of many models was blended together using the [HillClimb](https://www.kaggle.com/hhstrand/hillclimb-ensembling) algorithm (more about that later) and also stacked using a non-linear [XBG](https://github.com/dmlc/xgboost) stacker: 4 parts HillClimb, 1 part XGB.  So three levels in all.  Quick overview of model groups:\n\n* GRU-ATT and GRU-LSTM models as introduced by [Peter](https://www.kaggle.com/hoonkeng) in the [GRU-ATT](https://www.kaggle.com/hoonkeng/how-to-get-81-gru-att-lgbm-tf-idf-eda) kernel.  Each model used a different word embedding or had their sentences reversed (a kind of data augmentation).\n* Bi-LSTM models inspired by [huiqin](https://www.kaggle.com/qinhui1999)'s [Deep learning is all you need!](https://www.kaggle.com/qinhui1999/deep-learning-is-all-you-need-lb-0-80x), some with two different word embeddings in the same model.   I re-used the feature hashing huiqin introduced in my other DNN models that included categorical features.\n* Capsule Network models.\n* Combined Bi-GRU and Conv1D models based on [Zafar](https://www.kaggle.com/fizzbuzz)'s [The All-in-one Model ](https://www.kaggle.com/fizzbuzz/the-all-in-one-model) kernel.\n* [LGB](https://github.com/Microsoft/LightGBM) model of depth 17.\n* [XBG](https://github.com/dmlc/xgboost) model of depth 7 (shallow vs LGB's deep).\n\nEach DNN model had a different take on the data, increasing diversity.  DNN provided 54% of the blend vs 46% for gradient boosted trees."},{"metadata":{"collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"**Feature Engineering:**\n\n[Ehsun](https://www.kaggle.com/safavieh)’s great [Ultimate Feature Engineering](https://www.kaggle.com/safavieh/ultimate-feature-engineering-xgb-lgb-nn) kernel brought home for me just how important feature engineering (FE) is.  Using his features gave a big boost to my models.  The kitchen sink method works best with FE: create as many new features as you can without thinking too much about whether they will help or not.  Let your models automatically decide for you what works, and reject what doesn't!\n\nI added stemming and stop word removal to his common words counting to get a more meaningful, reduced overlap.  And I then went on a quest to add as many new features as I could think of.   Ehsun' kernel included what I would call a \"Polynomial Feature Creation Machine\" code block.  Running the combination of Ehsun's features plus my new features through it created many new interesting \"poly\" features.\n\nNew numerical features I added to Ehsun's:\n\n* Count of capital letters in each text feature\n* [Abishek](https://www.kaggle.com/abhishek)’s [is_that_a_duplicate_question](https://github.com/abhishekkrthakur/is_that_a_duplicate_quora_question/blob/master/feature_engineering.py) on from the [Quora Question Pairs](https://www.kaggle.com/c/quora-question-pairs) competition produced great features comparing essay_1 and essay_2.\n* Many lazy teachers used almost or exactly the same text for essay_1 and essay_2 (measured as fuzz_WRatio + quora.fuzz_qratio > 180) which became a feature.\n* Abishek’s skew and curtosis calculations for essay_1 and essay_2 and for \"corpus\" which is the concatenation of title + essay_1 + essay_2 + resource_summary.\n* Grouping essay_1 and essay_2 by their text revealed many identical essays, i.e. exactly the same essays have been posted over and over for essay_1 and essay_2.  I kept a count of how many times essay_1 and essay_2 appeared over all applications.  This might be create a small leak, in the that the past should not know about the future.\n* [Wrosinski](https://www.kaggle.com/wrosinski/)’s Quora competition Jupyter notebook, [Extraction – Textacy Features](https://github.com/Wrosinski/Kaggle-Quora/blob/master/features/Extraction - Textacy Features 17.05.ipynb), inspired me to add all of the [Textacy TextStats](http://textacy.readthedocs.io/en/latest/api_reference.html) calculated from \"corpus\".  Also, per his [notebook](https://github.com/Wrosinski/Kaggle-Quora/blob/master/features/Extraction - Textacy Features 17.05.ipynb) I added docfreq_mean, docfreq_max, termfreq_mean, termfreq_max, keyterms and sgrank.\n* As in my [Spellchecking feature improves AUC](https://www.kaggle.com/shadowwarrior/spellchecking-feature-improves-auc) kernel, I added number of spelling errors found in \"corpus\".\n\nFor diversity, I did not use all of these features in all models.  I had three variations of feature sets for my models:\n1. All numerical features + categorical features + text features\n2. Small subset of important numerical features + categorical features + text features\n3. Just text features."},{"metadata":{"_cell_guid":"f81f9fc0-393c-4601-9471-434572a2fa43","_uuid":"e86b854e59e86fdfc4fe9afdf6420c72187f76d5"},"cell_type":"markdown","source":"**FlexStacker and HillClimb:**\n\nI saved out-of-fold (OOF) predictions for all of my test runs, as discussed by [Tilli](https://www.kaggle.com/tilii7) in [You should use out-of-fold data](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52224). I've written my own ensembler / stacker, called FlexStacker, originating from [Simple Stacker](https://www.kaggle.com/yekenot/simple-stacker-lb-0-284) by [Vladimir Demidov](https://www.kaggle.com/yekenot). \n\nOOF predictions allow you to create a meta-model to optimally find how models should be ensembled / stacked together as opposed to random guessing which may lead to overfitting to the leaderboard.  With FlexStacker I can do linear or non-linear stacking.  Non-linear stacking using XGB worked well.  I found a non-[LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) based linear ensembling mechanism that worked great with my OOF predictions: [Hillclimb ensembling](https://www.kaggle.com/hhstrand/hillclimb-ensembling) by [Håkon Hapnes Strand](https://www.kaggle.com/hhstrand) from the recent [Toxic Comment](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) competition.\n\nI upgraded the Hillclimb class to generalize it, and make it capable of optimizing weights per fold since each fold in N-fold cross validation (CV) has its own model created for it.  I used 8-fold CV for all of my models.  Hillclimb is also great for testing your newly built model to see if it helps your ensemble.  Just add it to the list, rerun HillClimb, and see what its percent contribution, if any, is to your ensemble."},{"metadata":{"_cell_guid":"4902d17b-254b-4915-b89a-6d251d1bf369","_uuid":"94570f17f62aa873c6b934fd14e2dd7198d797b0"},"cell_type":"markdown","source":"**LGB and XGB**\n\nLGB and XGB models were my top individual scorers.  I made the LGB tree depth deep, and the XGB tree shallow, for diversity.  As with all of my models, I used hyperparameter optimization iterating over randomized grids.  First I used a coarse bounding grid, and then, as a second step, a finer grained one, once the coarse search hinted at a local optima.  \n\nThe input to my GBT models was Ehsun's plus my new features.  Here are the LGB and XGB parameters that I used:\n\n<pre><code>lgb_params = {\n          'objective': 'binary',\n          'metric': 'auc',\n          'boosting_type': 'dart',\n          'learning_rate': 0.01,\n          'max_bin': 15,\n          'max_depth': 17,\n          'num_leaves': 63,\n          'subsample': 0.8,\n          'subsample_freq': 5,\n          'colsample_bytree': 0.8,\n          'reg_lambda': 7}</code></pre>\n        \n<pre><code>xgb_params = {\n          'objective': 'binary:logistic',\n          'eval_metric': 'auc',\n          'eta': 0.01,\n          'max_depth': 7,\n          'subsample': 0.8, \n          'colsample_bytree': 0.4,\n          'min_child_weight': 10,\n          'gamma': 2}</code></pre>"},{"metadata":{"_cell_guid":"f7d681b9-9b32-47e6-a980-5d4232582bf3","_uuid":"29ddc1ab989d85b25253435c9f34985dff9f0386"},"cell_type":"markdown","source":"**Word Embeddings**\n\nAll of my neural network models used pre-trained word embeddings, a kind of transfer learning.  For diversity, I built neural network models with different word embeddings, all with trainable=False since that worked better for me.  Here are the three external ones I switched between:\n\n1. [Twitter GloVe](https://nlp.stanford.edu/projects/glove/) 200d\n2. [Common Crawl GloVe](https://nlp.stanford.edu/projects/glove/) 300d\n3. [Common Crawl FastText](https://nlp.stanford.edu/projects/glove/) 300d\n\nI also built a brand new 100d word embedding based on the DonorsChoose text \"corpus\" using [Facebook's fasttext](https://github.com/facebookresearch/fastText/tree/master/python):\n\n<pre><code>model100d = fasttext.skipgram('corpus.txt', '../output/ft_corpus.100d', dim=100, min_count=1)</code></pre>\n\n'corpus.txt' is a file containing the \"corpus\" of DonorsChoose text, previously referred to, written out sentence by sentence.  **fasttext.skipgram** produces two files, **ft_corpus.100d.bin**, the model, and **ft_corpus.100d.vec**, the word vectors.  I also used **ft_corpus.100d.vec** as a word embedding, and it worked surprisingly well, adding diversity.\n\nI found a great way to handle out-of-vocabulary (OOV) words using embedding imputation from the [Toxic Comment](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) competition in [Matt Motoki](https://www.kaggle.com/mmotoki)'s [33rd Place Solution Using Embedding Imputation](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52666) discussion post.  Replacing missing vectors with zeros or random numbers is suboptimal. Using FastText's built-in OOV prediction instead of naive replacement works much better.  I used the **ft_corpus.100d.bin** model to fill in OOV entries when Twitter, GloVe and FastText embeddings were used."},{"metadata":{"_cell_guid":"528f654c-8f94-41c3-afe1-fff5bb88ac70","_uuid":"01ad2bd44eb5bd18155501c333345b7452c61072"},"cell_type":"markdown","source":"**Neural Networks**\n\nBelow you'll find the Keras implementations of the DNN models I discussed, above.  I modified the architecture and hyperparameter optimized the DNN's provided by Kaggle contributors over various competitions (including this one).  Not shown: I sometimes used Dense layers as configured by [LittleBoat](https://www.kaggle.com/xiaozhouwang) in his [2nd place Porto Seguro DNN](https://github.com/xiaozhouwang/kaggle-porto-seguro/blob/master/code/nn_model290.py), with two replications of Dense followed by PReLU, BatchNormalization and Dropout.\n\nSome notes:\n\n* Attention was really useful as a layer and for pooling.\n* Use CuDNNGRU and CuDNNLSTM over GRU and LSTM whenever you can; they are much faster.\n* Use different word embeddings, even two different ones as the same time.\n\nThis is the end of the explanatory.  The rest of the kernel is just Python / Keras code. "},{"metadata":{"_cell_guid":"51aebf6c-f657-47a7-ba9d-4308848ac3fd","_uuid":"b8ee4b8a12b1cb6ec8d1fee07de8fa1f73b996fe","trusted":false,"collapsed":true},"cell_type":"code","source":"import os\nimport numpy as np\n\n# Keras imports\n\nfrom keras.layers import Embedding, Dense, Input\nfrom keras.layers import Bidirectional, TimeDistributed, CuDNNGRU, CuDNNLSTM, Convolution1D\nfrom keras.layers import Conv1D, merge # old\nfrom keras.layers import Flatten, concatenate, Dropout, PReLU, Activation, BatchNormalization\nfrom keras.layers import GlobalMaxPool1D, GlobalAveragePooling1D, SpatialDropout1D\nfrom keras.engine import InputSpec, Layer\nfrom keras.models import Model\nfrom keras import initializers\nfrom keras import constraints\nfrom keras import optimizers\nfrom keras import regularizers\nfrom keras import backend as K\n\n# suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport tensorflow as tf\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d37580c4-88ec-40ad-b5a7-307920f9140e","_uuid":"7b3e1a0b4170c0c596fe29b75d39a8c8fbdb8724","trusted":false,"collapsed":true},"cell_type":"code","source":"# GRU-ATT\n\nMAX_SENT_LENGTH = 50 \nMAX_SENTS = 35\n\nembedding_matrix = np.zeros((50000, 300)) # dummy\n\n# https://github.com/richliao/textClassifier/blob/master/textClassifierHATT.py AND\n# https://www.kaggle.com/hoonkeng/how-to-get-81-gru-att-lgbm-tf-idf-eda\n\nclass AttLayer(Layer):\n  def __init__(self, use_bias=True, activation ='tanh', **kwargs):\n    self.init = initializers.get('normal')\n    self.use_bias = use_bias\n    self.activation = activation\n    super(AttLayer, self).__init__(**kwargs)\n\n  def build(self, input_shape):\n    assert len(input_shape)==3\n    self.W = self.add_weight(name='kernel', \n                             shape=(input_shape[-1],1),\n                             initializer='normal',\n                             trainable=True)\n    if self.use_bias:\n      self.bias = self.add_weight(name='bias', \n                                  shape=(1,),\n                                  initializer='zeros',\n                                  trainable=True)\n    else:\n      self.bias = None\n    super(AttLayer, self).build(input_shape) \n\n  def call(self, x, mask=None):\n    eij = K.dot(x, self.W)\n    if self.use_bias:\n      eij = K.bias_add(eij, self.bias)\n    if self.activation == 'tanh':\n      eij = K.tanh(eij)\n    elif self.activation =='relu':\n      eij = K.relu(eij)\n    else:\n      eij = eij\n    ai = K.exp(eij)\n    weights = ai/K.sum(ai, axis=1, keepdims=True)\n    weighted_input = x*weights\n    return K.sum(weighted_input, axis=1)\n\n  def compute_output_shape(self, input_shape):\n    return (input_shape[0], input_shape[-1])\n\n  def get_config(self):\n    config = { 'activation': self.activation }\n    base_config = super(AttLayer, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))\n   \ndef get_attgru_model(gru_dense_dim = (64, 128), trainable=False, lr=0.0007, lr_decay=1e-16): \n  \n  # Encoder\n  \n  sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32', name='main_input') \n  \n  embedding_layer = Embedding(embedding_matrix.shape[0],\n                              embedding_matrix.shape[1],\n                              weights=[embedding_matrix],\n                              input_length=MAX_SENT_LENGTH,\n                              trainable=trainable)   \n\n  embedded_sequences = embedding_layer(sentence_input)\n\n  l_lstm = Bidirectional(CuDNNGRU(gru_dense_dim[0], return_sequences=True))(embedded_sequences)\n  l_dense = TimeDistributed(Dense(gru_dense_dim[1]))(l_lstm)\n  l_att = AttLayer()(l_dense)\n  sentEncoder = Model(sentence_input, l_att)\n  \n  # Decoder\n  \n  review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n  review_encoder = TimeDistributed(sentEncoder)(review_input)\n  l_lstm_sent = Bidirectional(CuDNNLSTM(gru_dense_dim[0], return_sequences=True))(review_encoder)\n  l_dense_sent = TimeDistributed(Dense(gru_dense_dim[1]))(l_lstm_sent)\n  l_att_sent = AttLayer()(l_dense_sent)\n  preds = Dense(2, activation='softmax')(l_att_sent)\n  sentDecoder = Model(review_input, preds)\n\n  sentDecoder.compile(loss='categorical_crossentropy',\n                      optimizer=optimizers.RMSprop(lr, lr_decay),\n                      metrics=['acc'])\n  \n  return sentEncoder, sentDecoder # fit on sentDecoder\n\nsentEncoder, sentDecoder = get_attgru_model()\n\nprint('encoder:')\nsentEncoder.summary()\n\nprint('decorder:')\nsentDecoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3d1381e4-1078-457f-ab0f-40577243746e","_uuid":"5ec930a951ce077a5ca94a5e9291e281776e4eae","trusted":false,"collapsed":true},"cell_type":"code","source":"# Bi-LSTM\n\ncat_features_hash = ['cat_%d' % n for n in range(1,11)] # dummy\nmax_cat_hash_size = 50000\nnum_features = ['num_%d' % n for n in range(1,401)] # dummy\nMAX_WORDS = 500\n\nembedding_matrix_2 = np.zeros((50000, 200)) # dummy\n\n# https://www.kaggle.com/qinhui1999/deep-learning-is-all-you-need-lb-0-80x\n# https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2\n\ndef dot_product(x, kernel):\n    \"\"\"\n    Wrapper for dot product operation, in order to be compatible with both\n    Theano and Tensorflow\n    Args:\n        x (): input\n        kernel (): weights\n    Returns:\n    \"\"\"\n    if K.backend() == 'tensorflow':\n        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n    else:\n        return K.dot(x, kernel)\n\nclass AttentionWithContext(Layer):\n    \"\"\"\n    Attention operation, with a context/query vector, for temporal data.\n    Supports Masking.\n    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n    \"Hierarchical Attention Networks for Document Classification\"\n    by using a context vector to assist the attention\n    # Input shape\n        3D tensor with shape: `(samples, steps, features)`.\n    # Output shape\n        2D tensor with shape: `(samples, features)`.\n    How to use:\n    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n    The dimensions are inferred based on the output shape of the RNN.\n    Note: The layer has been tested with Keras 2.0.6\n    Example:\n        model.add(LSTM(64, return_sequences=True))\n        model.add(AttentionWithContext())\n        # next add a Dense layer (for classification/regression) or whatever...\n    \"\"\"\n\n    def __init__(self,\n                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n                 W_constraint=None, u_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.u_regularizer = regularizers.get(u_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.u_constraint = constraints.get(u_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(AttentionWithContext, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight((input_shape[-1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n\n        self.u = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_u'.format(self.name),\n                                 regularizer=self.u_regularizer,\n                                 constraint=self.u_constraint)\n\n        super(AttentionWithContext, self).build(input_shape)\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        uit = dot_product(x, self.W)\n\n        if self.bias:\n            uit += self.b\n\n        uit = K.tanh(uit)\n        ait = dot_product(uit, self.u)\n\n        a = K.exp(ait)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n        # in some cases especially in the early stages of training the sum may be almost zero\n        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], input_shape[-1]\n\ndef get_model_bilstm(cat_embed_output_dim=10, trainable=False, gru_spec=(50, 100), gru_dropout=5e-5, lr=0.0006):\n  input_cat = Input((len(cat_features_hash), ))\n  input_num = Input((len(num_features), ))\n  input_words = Input((MAX_WORDS, ))\n  \n  x_cat = Embedding(max_cat_hash_size, cat_embed_output_dim)(input_cat)    \n  x_cat = SpatialDropout1D(0.3)(x_cat)\n  x_cat = Flatten()(x_cat)\n  \n  x_words = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                      weights=[embedding_matrix],\n                      trainable=trainable)(input_words)\n  x_words = SpatialDropout1D(0.25)(x_words) # 0.30\n  \n  x_words = Bidirectional(CuDNNLSTM(gru_spec[0], return_sequences=True,\n                          kernel_regularizer=regularizers.l2(gru_dropout),\n                          recurrent_regularizer=regularizers.l2(gru_dropout)))(x_words)\n  x_words = Convolution1D(gru_spec[1], 3, activation=\"relu\")(x_words)\n  \n  x_words1_1 = GlobalMaxPool1D()(x_words)\n  x_words1_2 = GlobalAveragePooling1D()(x_words)\n  x_words1_3 = AttentionWithContext()(x_words)\n  x_words = concatenate([x_words1_1, x_words1_2, x_words1_3])\n  x_words = Dropout(0.25)(x_words)       \n  x_words = Dense(100, activation=\"relu\")(x_words) # 100\n    \n  if embedding_matrix_2 is not None:\n    x_words_2 = Embedding(embedding_matrix_2.shape[0], embedding_matrix_2.shape[1],\n                        weights=[embedding_matrix_2],\n                        trainable=trainable)(input_words)\n    x_words_2 = SpatialDropout1D(0.25)(x_words_2) # 0.30\n\n    x_words_2 = Bidirectional(CuDNNLSTM(gru_spec[0], return_sequences=True,\n                 kernel_regularizer=regularizers.l2(gru_dropout),\n                 recurrent_regularizer=regularizers.l2(gru_dropout)))(x_words_2)\n    x_words_2 = Convolution1D(gru_spec[1], 3, activation=\"relu\")(x_words_2)\n    \n    x_words2_1 = GlobalMaxPool1D()(x_words_2)\n    x_words2_2 = GlobalAveragePooling1D()(x_words_2)\n    x_words2_3 = AttentionWithContext()(x_words_2)\n    x_words_2 = concatenate([x_words2_1, x_words2_2, x_words2_3])\n    x_words_2 = Dropout(0.25)(x_words_2)       \n    x_words_2 = Dense(100, activation=\"relu\")(x_words_2) # 100\n   \n    x_words = concatenate([x_words, x_words_2])\n\n  # extra dense later to handle >400 numerical features\n  x_num = Dense(200, activation=\"relu\")(input_num)\n  x_num = Dropout(0.25)(x_num)\n  x_num = Dense(100, activation=\"relu\")(x_num)\n\n  x = concatenate([x_cat, x_num, x_words])\n\n  x = Dense(50 + (0 if embedding_matrix_2 is None or gru_spec[1] == 0 else 14), activation=\"relu\")(x) # was 30\n  x = Dropout(0.25)(x)\n  predictions = Dense(1, activation=\"sigmoid\")(x)\n  model = Model(inputs=[input_cat, input_num, input_words], outputs=predictions)\n  model.compile(optimizer=optimizers.Adam(lr, decay=1e-6),\n                loss='binary_crossentropy',\n                metrics=['accuracy'])\n\n  return model\n\nmodel = get_model_bilstm()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"_cell_guid":"d13f34a7-3904-4df4-9ed1-20d15a84fa78","_uuid":"32f7b80e56c5b4227ca4e6318dc917e74c27622e","trusted":false,"collapsed":true},"cell_type":"code","source":"# Capsule\n# https://www.kaggle.com/chongjiujjin/capsule-net-with-gru\n\ndef squash(x, axis=-1):\n    # s_squared_norm is really small\n    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n    # return scale * x\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n    scale = K.sqrt(s_squared_norm + K.epsilon())\n    return x / scale\n\n# A Capsule Implement with Pure Keras\nclass Capsule(Layer):\n    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n                 activation='default', **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.kernel_size = kernel_size\n        self.share_weights = share_weights\n        if activation == 'default':\n            self.activation = squash\n        else:\n            self.activation = Activation(activation)\n\n    def build(self, input_shape):\n        super(Capsule, self).build(input_shape)\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(1, input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     # shape=self.kernel_size,\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(input_num_capsule,\n                                            input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n\n    def call(self, u_vecs):\n        if self.share_weights:\n            u_hat_vecs = K.conv1d(u_vecs, self.W)\n        else:\n            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n\n        batch_size = K.shape(u_vecs)[0]\n        input_num_capsule = K.shape(u_vecs)[1]\n        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n                                            self.num_capsule, self.dim_capsule))\n        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n\n        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n        for i in range(self.routings):\n            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n            c = K.softmax(b)\n            c = K.permute_dimensions(c, (0, 2, 1))\n            b = K.permute_dimensions(b, (0, 2, 1))\n            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n            if i < self.routings - 1:\n                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)\n      \ndef get_model_capsule(cat_embed_output_dim=10, trainable=False, gru_spec=(128, 0), gru_dropout=5e-5, lr=0.0007):\n  input_cat = Input((len(cat_features_hash), ))\n  input_num = Input((len(num_features), ))\n  input_words = Input((MAX_WORDS, ))\n  \n  x_cat = Embedding(max_cat_hash_size, cat_embed_output_dim)(input_cat)    \n  x_cat = SpatialDropout1D(0.3)(x_cat)\n  x_cat = Flatten()(x_cat)\n  \n  x_words = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                      weights=[embedding_matrix],\n                      trainable=trainable)(input_words)\n  x_words = SpatialDropout1D(0.25)(x_words)\n  \n  # https://github.com/mattmotoki/toxic-comment-classification/blob/master/code/modeling/Refine CapsuleNet.ipynb\n  x_words = Bidirectional(CuDNNGRU(gru_spec[0], return_sequences=True,\n                          kernel_regularizer=regularizers.l2(gru_dropout),\n                          recurrent_regularizer=regularizers.l2(gru_dropout)))(x_words)\n  x_words = PReLU()(x_words)\n  x_words = Capsule(num_capsule=10, dim_capsule=16, routings=5, share_weights=True)(x_words)\n  x_words = Flatten()(x_words)\n  x_words = Dropout(0.15)(x_words)\n     \n  x_cat = Dense(100, activation=\"relu\")(x_cat)\n  \n  # extra dense later to handle >400 numerical features\n  x_num = Dense(200, activation=\"relu\")(input_num)\n  x_num = Dropout(0.25)(x_num)\n  x_num = Dense(100, activation=\"relu\")(x_num)\n\n  x = concatenate([x_cat, x_num, x_words])\n\n  x = Dense(50 + (0 if embedding_matrix_2 is None or gru_spec[1] == 0 else 14), activation=\"relu\")(x) # was 30\n  x = Dropout(0.25)(x)\n  predictions = Dense(1, activation=\"sigmoid\")(x)\n  model = Model(inputs=[input_cat, input_num, input_words], outputs=predictions)\n  model.compile(optimizer=optimizers.Adam(lr, decay=1e-6),\n                loss='binary_crossentropy',\n                metrics=['accuracy'])\n\n  return model\n\nmodel = get_model_capsule()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8f75f6b8-b7cd-474d-ae26-4f759363fc47","_uuid":"186bec71b95ca32d565cf192ab74626bafe3d4fd","trusted":false,"collapsed":true},"cell_type":"code","source":"# Combined Bi-GRU and Conv1D\n# https://www.kaggle.com/fizzbuzz/the-all-in-one-model\n\ndef get_model_bigru_conv1d(cat_embed_output_dim=32, \n                           trainable=False,\n                           recurrent_units = 96,\n                           convolution_filters = 192,\n                           dense_units = [256, 128, 64],\n                           dropout_rate = 0.3,\n                           lr = 0.0006):\n  \n  input_cat = Input((len(cat_features_hash), ))\n  input_num = Input((len(num_features), ))\n  input_words = Input((MAX_WORDS, ))\n  \n  x_cat = Embedding(max_cat_hash_size, cat_embed_output_dim)(input_cat)\n  x_cat = SpatialDropout1D(dropout_rate)(x_cat)\n  x_cat = Flatten()(x_cat)\n  \n  x_words = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], # max_features, maxlen,\n                          weights=[embedding_matrix],\n                          trainable=trainable)(input_words)\n  x_words = SpatialDropout1D(dropout_rate)(x_words)\n  \n  x_words1 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(x_words)\n  x_words1 = Convolution1D(convolution_filters, 3, activation=\"relu\")(x_words1)\n  x_words1_1 = GlobalMaxPool1D()(x_words1)\n  x_words1_2 = GlobalAveragePooling1D()(x_words1)\n  x_words1_3 = AttentionWithContext()(x_words1)\n  \n  x_words2 = Convolution1D(convolution_filters, 2, activation=\"relu\")(x_words)\n  x_words2 = Convolution1D(convolution_filters, 2, activation=\"relu\")(x_words2)\n  x_words2_1 = GlobalMaxPool1D()(x_words2)\n  x_words2_2 = GlobalAveragePooling1D()(x_words2)\n  x_words2_3 = AttentionWithContext()(x_words2)\n  \n  x_num = input_num\n\n  x = concatenate([x_words1_1, x_words1_2, x_words1_3, x_words2_1, x_words2_2, x_words2_3, x_cat, x_num])\n  x = BatchNormalization()(x)\n  x = Dense(dense_units[0], activation=\"relu\")(x)\n  x = Dense(dense_units[1], activation=\"relu\")(x)\n  \n  x = concatenate([x, x_num])\n  x = Dense(dense_units[2], activation=\"relu\")(x)\n  predictions = Dense(1, activation=\"sigmoid\")(x)\n  model = Model(inputs=[input_cat, input_num, input_words], outputs=predictions)\n  model.compile(optimizer=optimizers.Adam(lr, decay=1e-6),\n                loss='binary_crossentropy',\n                metrics=['accuracy'])\n\n  return model  \n\nmodel = get_model_bigru_conv1d()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5ad8a5a2-1d4e-46c5-af56-368c9598eddd","_uuid":"836afe1dff8357eb9bc7d57f1f2b708ba2213b1e"},"cell_type":"markdown","source":"My FlexStacker saves all kinds of information for each model run, including AUC scores, logloss and accuracy.  Here is a plot of the AUC score for each fold, for each epoch, for a run of the Capsule Network that had an overall AUC of 0.81251 on my local machine:\n\n![](https://i.imgur.com/QgSTAcn.png)"},{"metadata":{"_cell_guid":"cf83f987-aa0f-416e-943f-6b64cbb7edf4","_uuid":"1612ee9e15338be55c1ffc0c8a0ea104b32fc194"},"cell_type":"markdown","source":""}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}