{"cells":[{"metadata":{"_cell_guid":"6ba9e4d5-8bfe-4af9-a641-91de7bd36826","_uuid":"beb5c22326f2e89f57507121f851635a9878c342"},"cell_type":"markdown","source":"Teachers, of all people, should be good spellers in order to set an example for their students.  One would think that spelling errors in applications must have a negative impact on their acceptance rate.  And in fact they do!  This kernel implements a spellchecker for the DonorsChoose.org application text, counting spelling errors per application.\n\nThe number of spelling errors can then be used as a feature in your models.  As Ehsan's [Ultimate Feature Engineering](https://www.kaggle.com/safavieh/ultimate-feature-engineering-xgb-lgb-nn) kernel demonstrated, new features can really help your models score better.  In fact, the number of spelling errors used as a feature improved my model AUCs by around 0.0004, a pretty nice improvement for a single feature.\n\nThis kernel runs through the process of spellchecking the text.  I've also attached the resultant CSV for the number of spelling errors per application which you can use in your models to see if it improves your score.\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7a01cdd4f944996e7834b0b2a69fd28f35a0b4bb"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, re\n\n# The end at the beginnging!  Here are # of misspellings per application.\nresult = pd.read_csv('../input/corpus-misspellings/corpus_misspellings_feature.csv')\nresult.tail(10)\n","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"509db0b58b6bc5a661d5787577f82340d91a0191"},"cell_type":"markdown","source":"Lets get started by reading in the train and test sets, concatentate them, and combine the cleaned text for each application into a single \"corpus\":"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","scrolled":true,"trusted":true},"cell_type":"code","source":"text_cols = ['project_title', 'project_essay_1', 'project_essay_2', 'project_essay_3', 'project_essay_4', 'project_resource_summary']\nid_col = 'id'\ntarget_col = 'project_is_approved'\nspell_col = 'misspellings'\n\ninput_folder = '../input/donorschoose-application-screening'\ntrain = pd.read_csv(os.path.join(input_folder, 'train.csv'))[[id_col] + text_cols + [target_col]]\ntest = pd.read_csv(os.path.join(input_folder, 'test.csv'))[[id_col] + text_cols]\n\ndf = pd.concat([train, test], axis=0, ignore_index=True)\n\n# piece together 'project_essay'\ndf.loc[df['project_essay_3'].notnull(), 'project_essay_1'] = df['project_essay_1'] + ' ' + df['project_essay_2']\ndf.loc[df['project_essay_4'].notnull(), 'project_essay_2'] = df['project_essay_3'] + ' ' + df['project_essay_4']\ndf['project_essay'] = df['project_essay_1'] + ' ' + df['project_essay_2']\n\ndef clean_text(phrase):\n  # specific\n  q = \"[\\'\\’\\´\\ʻ]\"\n  \n  phrase = re.sub(re.compile(\"won%st\" % q), \"will not\", phrase)\n  phrase = re.sub(re.compile(\"can%st\" % q), \"can not\", phrase)\n  \n  # general\n  phrase = re.sub(re.compile(\"n%st\" % q), \" not\", phrase)\n  phrase = re.sub(re.compile(\"%sre\" % q), \" are\", phrase)\n  phrase = re.sub(re.compile(\"%ss\" % q), \" is\", phrase)\n  phrase = re.sub(re.compile(\"%sd\" % q), \" would\", phrase)\n  phrase = re.sub(re.compile(\"%sll\" % q), \" will\", phrase)\n  phrase = re.sub(re.compile(\"%st\" % q), \" not\", phrase)\n  phrase = re.sub(re.compile(\"%sve\" % q), \" have\", phrase)\n  phrase = re.sub(re.compile(\"%sm\" % q), \" am\", phrase)\n  \n  phrase = re.sub(r\"\\\\r|\\\\n\", \" \", phrase)\n  phrase = re.sub(r\"\\.\\.+\", \". \", phrase) # ellipsis ... or .. to .\n  \n  # all chars except ;.?!\n  phrase = re.sub(re.compile(q + \"+\"), \"\", phrase)   \n  phrase = re.sub(r\"[\\'\\\"\\#\\$\\%\\&\\(\\)\\*\\+\\,\\-\\/\\:\\<\\=\\>\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~\\“\\”\\″\\ʺ\\¨\\‘\\…\\—\\―\\–\\•\\®]+\", \" \", phrase)   \n  \n  # add space after EOS if missing\n  phrase = re.sub(r\"([\\;\\.\\?\\!])([^\\s])\", \"\\\\1 \\\\2\", phrase)\n  # squeeze space before EOS\n  phrase = re.sub(r\"\\s+([\\;\\.\\?\\!])\", \"\\\\1\", phrase)\n  \n  # space squeezer: \\u200b is UNICODE space\n  phrase = re.sub(r\"['\\u200b\\s]+\", \" \", phrase).strip()\n  \n  return phrase  \n\ntext_cols = ['project_title', 'project_essay', 'project_resource_summary']\nfor col in text_cols:\n  df[col] = df[col].apply(clean_text)\n  df[col] = df[col].apply(lambda x: x if x[-1] in ';.?!' else x + '.') # EOS marker\n\ndf['project_corpus'] = df['project_title'] + ' ' + df['project_essay'] + ' ' + df['project_resource_summary']\ndf = df[[id_col] + ['project_corpus'] + [target_col]]\n\ndf.head()","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"86d9cd5e6ff0ed9dfee08a4649bf011d2c245498"},"cell_type":"markdown","source":"Now we load in the [PyHunSpell](https://github.com/blatinier/pyhunspell) spellchecker with the [en_US large](https://aur.archlinux.org/packages/hunspell-en-us-large) vocabulary. "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import hunspell\nhunspell_folder = '../input/hunspellenuslarge'\n\n# https://aur.archlinux.org/packages/hunspell-en-us-large/\nhobj = hunspell.HunSpell(os.path.join(hunspell_folder, 'en_US-large.dic'), \n                         os.path.join(hunspell_folder, 'en_US-large.aff'))\n\n# give it a try\nhobj.spell('donors')\n","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"c10261cadea62f2372d5a1706f3db8f41dae61d0"},"cell_type":"markdown","source":"The hunspell spellchecker is pretty good, but it is not perfect for our purpose.  It does not include product names like \"chromebook\" or slang like \"looove\".   I wanted to find a way to not flag these as spelling errors.  I came up with a semi-manual process where I used myself as a [Mechanical Turk](https://en.wikipedia.org/wiki/The_Turk).  Here is the process:\n\n1. Track the spelling errors according to hunspell, keeping a count of # of errors per word (**n**) in the Python dictionary **misspell**\n2. Sort **misspell** descending by **n** and write it to a CSV file\n3. Manually set a column value **ok** to 1 if the word is not really a misspelling\n4. Read the CSV back in and run the spellchecker again, not considering **ok** words to be misspelled\n\nIt took me about an hour to run through all misspelled words that occurred 10 or more times.  To save time, I did not look at misspelled words that occurred less than 10 times.  But you can do this if you want to!  I am sure it will help a bit.  \n\nBelow, I am loading in the corpus_misspellings.csv file I marked up in this manner.  But I have also included the code to create it from scratch at the end of this kernel.  As I said, you can continue editing corpus_misspellings.csv if you like.\n"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"7aaf2e5a6dbc4611464589f5009885ca9272df6a"},"cell_type":"code","source":"isok = pd.read_csv('../input/corpus-misspellings/corpus_misspellings.csv', encoding='utf-8')\n\n# pass through these words as OK even though hunspell flags them as not\npassthru = set(isok[isok.ok == 1].word)\ndf[spell_col] = 0\nmisspell = {}\n\nisok.head()","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"9ad0dac1d5a511a63f9d9176688730010f193980"},"cell_type":"markdown","source":"Below we are tracking two things:\n1. **misspell** tracks number of occurrences of a misspelled word\n2. **df[spell_col] **counts the spelling errors per application"},{"metadata":{"trusted":true,"_uuid":"02d95777fe951123b6d731b86f87a20520de93ac"},"cell_type":"code","source":"pd.options.mode.chained_assignment = None\nfor ci, s in enumerate(df.project_corpus):\n  sw = s.split(' ') # split into words\n  \n  w_ok = set()\n  w_misspell = set() # per word misspelled\n  for i, w in enumerate(sw):\n    w = re.sub(r'\\W+', '', w) # clean the word\n    \n    if len(w) <= 3: # ignore short words\n      continue\n    \n    # mark any word that contains chars outside a-z as OK:\n    # if a product name like \"Brite\" is capitalized once, this means other occurrences in the\n    # application like \"brite\" will be considered OK\n    if bool(re.search(r'[^a-z]', w)):\n      w_ok.add(w.upper())\n      continue\n    \n    w = w.upper() # hobj.spell() works better with CAPS\n    if hobj.spell(w.upper()) or \\\n       (w.endswith('S') and (hobj.spell(w[:-1].upper())) or (w[:-1] in passthru)) or \\\n       (w in passthru) or \\\n       ((i > 0) and (sw[i-1] in ['MR', 'MRS'])):\n      continue\n    \n    w_misspell.add(w) # word is misspelled!\n    \n  lw_misspell = list(w_misspell - w_ok)\n  for w in lw_misspell:\n    # print(ci, w)\n    misspell[w] = misspell.get(w, 0) + 1\n    df[spell_col][ci] += 1\n    \n# same as we saw in \"result\"\ndf[[id_col, spell_col]].tail(10)\n","execution_count":28,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"b15c0c45bae92ffd85477d51c47ee75f1f11c921"},"cell_type":"markdown","source":"Count of applications with each number of spelling errors:"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"480cd56427d963fad46d02af92844732b906bbd1"},"cell_type":"code","source":"from collections import Counter\nCounter(df[spell_col])","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"d61d9e0ede383316c780b40af69106bd914bb866"},"cell_type":"markdown","source":"Most applications are spelling error free.  What a relief!  About 9% do contain spelling errors however.  Let's take a look at an application with a lot of errors:"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"2af3357d31ff1d81a037d702073d8ca0cb5da4a5"},"cell_type":"code","source":"list(df[df[spell_col] == 13].project_corpus)","execution_count":40,"outputs":[]},{"metadata":{"_uuid":"dd45cc75d432ec3378256ce58573134c0e484c9a"},"cell_type":"markdown","source":"Pretty bad.  Funny that spellchecking is discussed at the end of the text!  Here is the relationship between spelling errors and ratio of applications approved:"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"ba81502d24572f65354943239f9528821fdce11d"},"cell_type":"code","source":"# clipped at 8\nprint('% approval given # of misspellings:')\nfor n in range(0,9):\n  print('%d: %.4f' % (n, np.mean(df[df[spell_col].clip(0,8) == n][target_col])))","execution_count":41,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"dd99c0cc451b80b99289d10e8f4f147c2db3ddc3"},"cell_type":"markdown","source":"Yep.  Misspellings negatively correlates with approval.\n\nIf you are interested, here is how to generate the corpus_misspellings.csv file I discussed earlier."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"9a8b663f4ee72f44128d6661fef6b6ac935e2253"},"cell_type":"code","source":"# combine plural misspelling with singular\ntodelete = []\nfor w, n in misspell.items():\n  if not w.endswith('S') or not w[:-1] in misspell:\n    continue\n  \n  misspell[w[:-1]] += n\n  todelete.append(w)\n\n# delete plurals\nfor w in todelete:\n  misspell.pop(w)  \n  \nmisspell = pd.DataFrame.from_dict(misspell, orient='index')\nmisspell.reset_index(level=0, inplace=True)\nmisspell.columns = ['word', 'n']\nmisspell['ok'] = 0 # not OK, unless manually set to 1\nmisspell = misspell.sort_values(by='n', ascending=False)  \n\n# misspell.to_csv('../output/corpus_misspellings.csv', index=False)\n\nmisspell.head()","execution_count":46,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}