{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"},"cell_type":"markdown","source":"Hi everyone!\n\nThis is my first public kernel and I figured I would start with a deep dive into the data and try to make it as newbie friendly as possible. Hopefully this helps y'all in your model building!\n\nPlease leave a comment if you found this helpful/interesting and if you have any suggestions!\n\nUpdates:\nI added a few more examples of using pandas directly instead of writing code to do things manually. And I've added a heatmap of some correlations at the end."},{"metadata":{"_uuid":"78fb98077f2c4f80ba9f7ed986aee7cbe0435577","collapsed":true,"_cell_guid":"9b84caf6-eeee-4d00-a59a-7b33c7bed104","trusted":true},"cell_type":"code","source":"# Let's import everything we'll be using. Keep it all at the top to make your life easy.\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"8424cdac462cbea2c8d7968c34b256075079102b","_cell_guid":"c09a9440-d5ea-4c27-8f55-949f8f869650","trusted":true},"cell_type":"code","source":"# Where are our files? Check down below! Don't forget the ../input when you try to load them in.\nprint(os.listdir(\"../input\"))","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\nresource = pd.read_csv('../input/resources.csv')","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"8c5300a6c4e57824cc4ab7a7cd12e981bfd2a720","_cell_guid":"4ed2d3de-d511-4b4d-b155-e3840431f33c","trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(resource.shape)","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"187cc8abc3773641e8a81f30dba636e2fbcbbe08","_cell_guid":"9ba471fe-6452-4e87-a514-38a85b219413"},"cell_type":"markdown","source":"I always like to do a quick shape on any data I read in since that can tell me a little bit of what to expect.  Right off the bat I can see that our training data has 182,080 examples and a total of 16 categories. However, our resource dataset has 1,541,272 rows and 4 categories. Since the number of rows in each aren't the same then we immediately know that these aren't 1-to-1 and I'll have to be careful using the resource data in my final algorithm.\n\nNow let's look at the head of both data sets to quickly orient ourselves."},{"metadata":{"_uuid":"1ce8552ec69dbcbc9fc27499ef832e00f3836244","_cell_guid":"536663c8-a159-42fe-a864-3435f03ee6d2","trusted":true},"cell_type":"code","source":"train.head()","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"97ec99355254b70f0bd04494eecd9204fff0aa81","_cell_guid":"9d409c1d-29dc-42e3-b49f-46208ba77e92","trusted":true},"cell_type":"code","source":"resource.head()","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"9353e42b10d23bbd339a73dec39e329affcac342","_cell_guid":"c7c7078b-f1ce-4273-bbde-69566f139181"},"cell_type":"markdown","source":"From looking at the above we probably have a decent sense of the data types in our data set, but we should know for sure and not make assumptions."},{"metadata":{"_uuid":"f2aa08d1589ac6a770ad71b9391a49b0346e9534","_cell_guid":"46f936d2-9c05-458c-b9d4-059cc7053085","trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"8093bdf7e79a31bbb7fc00f23c82d098e34b2fe2","_cell_guid":"8383d2b1-21d5-4546-925f-c61e1aea528e","trusted":true},"cell_type":"code","source":"resource.dtypes","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"80175aa6e704e4ccade114270e498cb1e36d6941","_cell_guid":"d0df1b3b-5ef2-44fe-bbf2-024089df70cc"},"cell_type":"markdown","source":"\"object\" is pandas speak for a str data type. So we see that we have a lot of strings, but not exclusively strings. The things we expect to be numbers (like quantity and price) are loaded in as numbers and of the types that make most sense (int and float, respectively for quantity and price)."},{"metadata":{"_uuid":"59b80eb74aa57e8a192e34c67b07c5dabccfcbe7","_cell_guid":"9b387487-784e-4be4-be50-16cac5f461c4"},"cell_type":"markdown","source":"# Categories in the DonorChoose Data Set\n\nLet's look at the descriptions of each field taken directly from the competition page:\n\n## Data fields\n\ntrain.csv (and test.csv):\n\n*     id - unique id of the project application\n*     teacher_id - id of the teacher submitting the application\n*     teacher_prefix - title of the teacher's name (Ms., Mr., etc.)\n*     school_state - US state of the teacher's school\n*     project_submitted_datetime - application submission timestamp\n*     project_grade_category - school grade levels (PreK-2, 3-5, 6-8, and 9-12)\n*     project_subject_categories - category of the project (e.g., \"Music & The Arts\")\n*     project_subject_subcategories - sub-category of the project (e.g., \"Visual Arts\")\n*     project_title - title of the project\n*     project_essay_1 - first essay*\n*     project_essay_2 - second essay*\n*     project_essay_3 - third essay*\n*     project_essay_4 - fourth essay*\n*     project_resource_summary - summary of the resources needed for the project\n*     teacher_number_of_previously_posted_projects - number of previously posted applications by the submitting teacher\n*     project_is_approved - whether DonorsChoose proposal was accepted (0=\"rejected\", 1=\"accepted\") (train.csv only)\n\nNote: Prior to May 17, 2016, the prompts for the essays were as follows:\n\n*      project_essay_1: \"Introduce us to your classroom\"\n*      project_essay_2: \"Tell us more about your students\"\n*      project_essay_3: \"Describe how your students will use the materials you're requesting\"\n*      project_essay_4: \"Close by sharing why your project will make a difference\"\n\nStarting on May 17, 2016, the number of essays was reduced from 4 to 2, and the prompts for the first 2 essays were changed to the following:\n\n*     project_essay_1: \"Describe your students: What makes your students special? Specific details about their background, your neighborhood, and your school are all helpful.\"\n*     project_essay_2: \"About your project: How will these materials make a difference in your students' learning and improve their school lives?\"\n\nFor all projects with project_submitted_datetime of 2016-05-17 and later, the values of project_essay_3 and project_essay_4 will be NaN.\n\nresources.csv:\n\nProposals also include resources requested. Each project may include multiple requested resources. Each row in resources.csv corresponds to a resource, so multiple rows may tie to the same project by id.\n\n*     id - unique id of the project application; joins with test.csv. and train.csv on id\n*     description - description of the resource requested\n*     quantity - quantity of resource requested\n*     price - price of resource requested\n"},{"metadata":{"_uuid":"e73279659f97375c7cd1c90a21d55e07c4500e82","_cell_guid":"01421d8d-4f64-4290-8e8a-d24fbc949591"},"cell_type":"markdown","source":"The description immediately tells us that we're going to have NaN's (Not a Number) in our data. When we looked at the train.head() we saw a bunch of NaN's, and now we know why. It isn't missing data from a teacher not including it, it is simply the guidelines. That means we'll have to implement a way of dealing with it. If we read the essay descriptions it sounds like essay 1 and 2 in the old format correspond well to essay 1 in the new format and likewise for old essays 3 and 4 with the new essay 2. \n\nWhen it comes to making an algorithm I will tentatively choose to merge the essays as appropriate.\n\nDo we have other NaNs?"},{"metadata":{"_uuid":"9fb9601323a8fb22b25dcdc18f7bfc0a16ec86fb","_cell_guid":"ddcc40ee-0132-4073-958b-009b1a9de373","trusted":true},"cell_type":"code","source":"for label in list(train.columns.values):\n    print(f'{label} has {sum(train[label].isna())}')","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fed87b05418e64d452ffa4d5371795e54a41e73"},"cell_type":"code","source":"# Using this pandas code you can infer how many nan's there are. \n# One line of code instead of my 2 above, plus this gives more info!\n\n#train.info()","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"f880b7aa5a15866bc003af499c8ce1c16291bc42","_cell_guid":"47220ee3-365a-4ce1-b68b-34164e867f24"},"cell_type":"markdown","source":"We're good on the NaN front! Except there are 4 in teacher_prefix."},{"metadata":{"_uuid":"a009aa8e6c31ef3d5aa4696e9640559938ff5e77","collapsed":true,"_cell_guid":"cf17ceed-5d15-423e-a7f5-3490e30f6b59","trusted":false},"cell_type":"code","source":"# If we wanted to look at them in depth then we'd use the following code. \n# For the sake of length I'll omit the output since it isn't particularly interesting.\n#idx = np.where(train['teacher_prefix'].isna() == True)[0]\n#print(train.iloc[idx])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cafea9a4e0912abe62a63f96c37b6c67e7b41366","_cell_guid":"a893a3b0-86cb-418b-bc3c-f9fd74cf34cb"},"cell_type":"markdown","source":"Let's look at the categorical columns. How many different categories do we have? Do we see anything weird?"},{"metadata":{"_uuid":"e19e3877d3d5e743c504787a21a681a6e36bb00c","_cell_guid":"454cbb7e-079f-4261-8555-b11e64a17faa","trusted":true},"cell_type":"code","source":"for label in ['teacher_prefix', 'school_state', 'project_grade_category', 'project_subject_categories', 'project_subject_subcategories']:\n    total = 0\n    print(f'=== {label} ===')\n    for i, item in enumerate(train[label].unique()):\n        count = len(np.where(train[label] == item)[0])\n        print('{}: {}'.format(item, count))\n        total += count\n    print(f'== Total categories: {i+1} and {total} values out of {len(train)}===\\n')","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1c81c0dc03755f41fa5a745337c3da5d425d94ca"},"cell_type":"code","source":"# Here is the slick pandas way to do the above code. 2 lines versus 8! \n# Note that doing it this way doesn't show the nan's in the teacher_prefix, \n# so you still want to make sure you use something to count nan's!\n\n#for label in ['teacher_prefix', 'school_state', 'project_grade_category', 'project_subject_categories', 'project_subject_subcategories']:\n#    print(train[label].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14d09c839d4075d2bcdbadcd7f1a6d2e904c69a3"},"cell_type":"code","source":"train.describe(include=['O'])","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"93a3928b3af33d12368f56f097c129449b24cbc3"},"cell_type":"markdown","source":"It's interesting to note that there are repeated essays. Clearly there are teachers sending in the same, or similar, applications multiple times. I can imagine an argument for filtering these out, but currently I don't plan to do so."},{"metadata":{"_uuid":"1279b4a0686a83048ff119d92f93643b258076a9","_cell_guid":"364f63cc-0413-4d1e-9e4d-22bbddcdbe46"},"cell_type":"markdown","source":"Let's go through feature by feature and see what we learn!\n\n## Teacher Prefix\nThere are 6 different prefixes, but at the end of the list we see a \"nan\" with 0 instances. Those are the same nan we saw above but it is odd that they aren't being counted when done this way, but this is why I implemented the total counts just to be sure.  We also see that Mr is dwarfed by Ms + Mrs. Pending investigation into the nans, I'm contemplating throwing out this column anyways since I like the idea of my algorithm being gender-blind.\n## School States\nAll 50 states are represnted plus 1 for Washington D.C.! From a quck skim it is clear that the applications are not uniform in amount across states. To go one step further in our analysis we could plot these numbers vs the population of each state and see if there is any correlation.\n\n## Grades\nThere are 4 grade categories and each one has a healthy amount of data though note that as the grade level goes up the number of entries go down.\n\n## Subject Categories\nThere are a lot of subject categories at 51! Some of them have tens of thousands of entries, but there is also one with only a single entry. That might be problematic and one should consider combining categories since there is strong overlap.\n\n## Subject Subcategories\nAnd finally we have 407 subcategories. No category has a whopping amount from first glance, but many categories have 10 or less entries. My gut reaction is that subcategory won't be a very good feature. Also, in the subcategories I notice two blank lines. This seems odd since it is a blank line and doesn't have a corresponding number so it doesn't seem to just be an instance of an empty string as a subcategory. This also warrants further investigation, especially if we want to use subcategories as a feature!"},{"metadata":{"_uuid":"8ae141f2d0592eb28a9cdae46d000e30206e8b7c","_cell_guid":"7145c1d4-43da-46b7-8254-5cc7d616c225"},"cell_type":"markdown","source":"Now let's take a peek at some numerical data such as from describe() and some histograms of how many teachers have previously submitted projects."},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"ee448826a979481775f150aef50c68855f30c626"},"cell_type":"code","source":"train.describe()","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"42327daa4152433e1e5bd6f23997035f64df78c8"},"cell_type":"markdown","source":"On average a teacher submits about a dozen projects, but there's a pretty big spread.  Also a huge percentage of applications are accepted."},{"metadata":{"_uuid":"e822eaaf3bb7af385980f81ce39010ee0e77d250","_cell_guid":"b1b9f41a-5a52-4c93-9633-3e87d3333128","trusted":true},"cell_type":"code","source":"plt.figure()\nplt.hist(train['teacher_number_of_previously_posted_projects'], bins=30)\nplt.title('Histogram Counting # of Teachers that Previously Posted Projects')\nplt.xlabel('Projects')\nplt.ylabel('Count')\nplt.show()\n\nnonzeroidx = np.where(train['teacher_number_of_previously_posted_projects'] != 0 )[0]\nplt.figure()\nplt.hist(train['teacher_number_of_previously_posted_projects'].iloc[nonzeroidx], bins=30)\nplt.title('Histogram Counting # of Teachers that Previously Posted Projects (teachers with 0 removed)')\nplt.xlabel('Projects')\nplt.ylabel('Count')\nplt.show()\n\ntestidx = np.where(train['teacher_number_of_previously_posted_projects'] > 5 )[0]\nplt.figure()\nplt.hist(train['teacher_number_of_previously_posted_projects'].iloc[testidx], bins=30)\nplt.title('Histogram Counting # of Teachers that Previously Posted Projects (teachers with 0 removed)')\nplt.xlabel('Projects')\nplt.ylabel('Count')\nplt.show()","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"6079a5fb73599c088746f5ba27c0421820183355","_cell_guid":"fdce2140-acdf-48a6-80b8-f06716dcc245"},"cell_type":"markdown","source":"From the first histogram we see that a huge number of teachers have submitted just a few projects and then it falls off heavily from there. For the second, I filtered out all teachers who have never submitted before (roughly 50,000) and we still see the same behavior. If we were to dig into the numbers more we'd know that ~26,000 have submitted once before, ~17,000 twice, and ~12,000 thrice."},{"metadata":{"_uuid":"35893bce8508fff1eeaae4230024a3a7a2ecf44f","_cell_guid":"b36e9637-5078-474a-b35b-ff8f737bc9b4"},"cell_type":"markdown","source":"This gets me curious, we have a number corresponding to each teacher regarding how many projects they've submitted before. Are all of these previous projects in the dataset? Let's figure that out! I wouldn't anticipate it telling us something actionable, but I'm certainly intrigued! \n\nFrom the initial head() command we know that the first teacher in the set has submitted 26 times before as of that submission. How many times does their teacher_id appear in the data set?"},{"metadata":{"_uuid":"7b5004cfd04eecf8e475f8e1c215f7f43b2a916b","_cell_guid":"2d4479aa-a22a-48a3-bd78-f93f0b3b4355","trusted":true},"cell_type":"code","source":"len(np.where(train['teacher_id'] == train['teacher_id'][0])[0])","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"6c2ac252f5a6ff8b62a557e1a5576e5cdaff1d59","_cell_guid":"8810cf0b-6613-4d29-884e-59efde37563e"},"cell_type":"markdown","source":"It appears only ten times! So there are at least 16 of their applications not in this data set. Let's look to see at the number of previously posted projects they have for each entry."},{"metadata":{"_uuid":"c81a60970db681090074818ba15494366589e722","_cell_guid":"fd8fb549-c494-4e1a-8c8c-2e744f621e7a","trusted":true},"cell_type":"code","source":"train['teacher_number_of_previously_posted_projects'].iloc[np.where(train['teacher_id'] == train['teacher_id'][0])]","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"2d2798376ab7b99eba219675eb08a0729edaa9f2","_cell_guid":"10a74aab-45ef-4120-b790-fc13d326ec3a"},"cell_type":"markdown","source":"We seem to have a random assortment of their submissions."},{"metadata":{"_uuid":"fd64e7ba1ac598d46dedbef968f531d41f0f2e4a","_cell_guid":"98a8268d-4d1b-4dfc-afc8-de1e4e09d456"},"cell_type":"markdown","source":"Is there a pattern in acceptance vs number of previous submissions?"},{"metadata":{"_uuid":"3c3cd4620ba3cdf6bccf38f30363e6436a8b2888","_cell_guid":"056ece63-5612-4505-be58-a2a5e0193001","trusted":true},"cell_type":"code","source":"plt.figure()\nplt.scatter(train['teacher_number_of_previously_posted_projects'], train['project_is_approved'])\nplt.show()","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"63e2732b3c624333a8fea9b18ec8cd13528fd74e","_cell_guid":"294646b1-ec28-4b32-9f80-c22ada78f75f"},"cell_type":"markdown","source":"We see that there are people with past submissions in the hundreds who do sometimes still fail. Experience alone is not a sufficient metric. Let's dig into a single one a little bit more: "},{"metadata":{"_uuid":"2477fdb1b9cbb683a51ccaa2d2f8312dc15fce2a","_cell_guid":"d1440214-7a21-4c2c-891d-b5afd3b0b9ac","trusted":true},"cell_type":"code","source":"reject = np.where(train['project_is_approved'] == 0)[0] # Indexes where an application failed.\nbig_submit = np.where(train['teacher_number_of_previously_posted_projects'] > 375)[0] # Indexes where a teacher has sent more than 350 applications before.\nidx = np.intersect1d(reject, big_submit)\nid_bigsubmit_but_fail = train['teacher_id'].iloc[idx] # All the ids for teachers that have submitted a bunch but didn't suceed on one.\n\nprint(id_bigsubmit_but_fail.head(1)) # Let's look at just a single id.\nprint(train['teacher_number_of_previously_posted_projects'].iloc[np.where(train['teacher_id'] == id_bigsubmit_but_fail.iloc[0])])\nprint(train['project_is_approved'].iloc[np.where(train['teacher_id'] == id_bigsubmit_but_fail.iloc[0])])","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"26b02bc3252bb7988047a77af8f4f43a192f6436","_cell_guid":"d82ecc5c-59f2-4c67-bd77-4572ced78d1f"},"cell_type":"markdown","source":"From this we can see they failed on their 380th submission (they had 379 before that) but they succeeded on many before and after that submission."},{"metadata":{"_uuid":"e2888bbd3e9094d7bc3235076b8cec465cfbb425","_cell_guid":"12d293cb-61d0-484c-bfe8-b04c7a344e90","trusted":true},"cell_type":"code","source":"# Now that I've changed my analysis above, this is redundant. But I'll keep it in for posterity.\n\nrejected_rate = len(reject)/len(train)\nprint(f'The acceptance rate is {(1 - rejected_rate)*100}%')","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"f126311735d5f2be04c2c2fe1879bf64fd187f4f","_cell_guid":"fbb5339c-a8ae-460f-b42b-c6631dabada8"},"cell_type":"markdown","source":"# Summary of Train\n\nWe've learned a few things that could be useful in our model building.\n\n* There are 4 missing values in the prefix column.\n* Not all submissions from a teacher are in our dataset.\n* We need to appropriately deal with the fact that the number of essays change part way through our data set.\n* We have a ton of subcategories and many of them have very few submissions. There are a few categories that also lack enough submissions to really make any good claims.\n* Simply having a large number of previous submissions does not guarantee future success. Likewise few submissions does not guarantee failure.\n* The acceptrance rate in our dataset is 84.768%.\n\n## Some Choices I'll Make After This Analysis\n\n* I prefer a gender blind algorithm so I'm going to remove the prefix column. This conveniently means we don't need to deal with the NaN.\n* I'm going to try and combine categories and subcategories in a reasonable way and see how many entries of each type that leaves us. If I still have subcategories with only a handful of entries then I may throw that out depending on my future analysis."},{"metadata":{"_uuid":"76af20b171f67b12896c11d198c27ef6f5d49e03","_cell_guid":"3552e651-6508-41a8-910d-cfa1d5a0c458"},"cell_type":"markdown","source":"# Quick Exploration of Resource\nLet's do a quick look at resources now."},{"metadata":{"_uuid":"7c27167e3e7b29d1ed82fc9d33305f9080bdbf9a","_cell_guid":"1827aa86-360c-4a04-aa7f-640eac64124d","trusted":true},"cell_type":"code","source":"for label in list(resource.columns.values):\n    print(f'{label} has {sum(resource[label].isna())}')","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"f3d796c794722a49f5f54c655bfdc8a3589d750e","_cell_guid":"0aff21aa-9a5f-4234-8730-c2143e30ec50"},"cell_type":"markdown","source":"We have a decent number of NaN in our description. If we want to use this data then we'll have to deal with that.  What I'm most interested in is plotting the total cost of the resources requested vs. acceptance and plotting total cost vs. how many previous submissions.\n\nLet's make a new data frame that aggregates all the ids and their associated total costs (quantity * price)."},{"metadata":{"_uuid":"1b677e6b3c7978c3bf842dcdeb40c87c435bacc8","collapsed":true,"_cell_guid":"7a7ef143-291f-40af-9c17-bd3b7cf994a1","trusted":true},"cell_type":"code","source":"id_cost = pd.DataFrame({'id': resource['id'], 'total_cost': resource['quantity'] * resource['price']})","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"cd653c67a8e3ade4c1832a8f410ab444e73384ea","_cell_guid":"d2c36290-0f78-414a-9ec7-ed509e76963b","trusted":true},"cell_type":"code","source":"id_cost.head()","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"5ada465730ef4608ea2938691ea290896a480cbd","collapsed":true,"_cell_guid":"9b3e4227-e46e-48ae-8ecb-39fa7a56761d","trusted":true},"cell_type":"code","source":"id_total_cost = id_cost.groupby(id_cost['id'], sort=False).sum().reset_index()\n\n# Small note, I originally wrote the above code as a for loop that looped \n# through all unique ids and and summed up every instance.\n# However, it ran slooowwww. I projected it'd take aboout 3.5 hours to run. \n# The above code runs in under a second. Use pandas built-in methods!","execution_count":33,"outputs":[]},{"metadata":{"_uuid":"a6ca46de076c93e7795a21dfa8060997e507c1ce","_cell_guid":"92b30012-4c1d-49c3-8aa4-719fd48fd2fe","trusted":true},"cell_type":"code","source":"id_total_cost.head()","execution_count":34,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8204384c1e55dc5501d3fd3b308da8ee1fbfdae6"},"cell_type":"code","source":"id_total_cost.describe()","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"8006ebcf42dfe020a5ff50c46d84a930b29a3cb5","_cell_guid":"73c7cf75-34bb-4794-a813-9d94bc8b55fc","trusted":true},"cell_type":"code","source":"id_total_cost.sort_values(by=['total_cost'])","execution_count":36,"outputs":[]},{"metadata":{"_uuid":"8356dd2543801590e7736296fc01f4a91dd582e6","_cell_guid":"3790dab9-1f24-4d45-bd7b-f9fb301bacfc","trusted":true},"cell_type":"code","source":"plt.figure()\nplt.hist(id_total_cost['total_cost'], bins=50)\nplt.show()","execution_count":37,"outputs":[]},{"metadata":{"_uuid":"c299f724c524f1325b5b385b03d49f03127696a0","_cell_guid":"105eaade-88c6-4eb1-89a4-acd509479dba"},"cell_type":"markdown","source":"It seems safe to say that the minimum cost that can be requested is $100. Most total costs are clustered around the lower range -- sub 1000 bucks. However there is a long and skinny tail extending all the way to 17,901.94! We'll want to normalize these values if we use them in a machine learning algorithm."},{"metadata":{"_uuid":"2d28e42a11880efc49b311ad9d187f58eae64a3d","_cell_guid":"07cc99ec-ffcc-414d-bde7-dd6f24a2a592","trusted":true},"cell_type":"code","source":"print(len(id_total_cost['id']))\nprint(len(train['id']))","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"3435346bd7e9f42f17f787b90703660dbc7efcca","_cell_guid":"11003994-ffa2-4a3b-a3bc-548d1f6cf5f5"},"cell_type":"markdown","source":"After these manipulations we see that there are still more id's in our collected resources then there are in train. Let's figure out if every id in train has an id in in_total_cost"},{"metadata":{"_uuid":"7d16fbaf871f008cf0662a6d34a0e05e9af95a1e","_cell_guid":"7cb2f9ce-9475-48f6-b5d5-f0ac36691ef4","trusted":true},"cell_type":"code","source":"train['id'].isin(id_total_cost['id']).head()","execution_count":39,"outputs":[]},{"metadata":{"_uuid":"a6ec0711879b6ffabe8c634416f4f6cc6c75224b","_cell_guid":"800e783a-5dbf-433b-ad54-a3fde7805b46","trusted":true},"cell_type":"code","source":"print(sum(train['id'].isin(id_total_cost['id'])))\nprint(len(train['id'].isin(id_total_cost['id'])))","execution_count":40,"outputs":[]},{"metadata":{"_uuid":"a35ff69b18a7709abf835758669586b156c0e1e6","_cell_guid":"a19716e5-7c8c-449d-bd86-b4dada696012"},"cell_type":"markdown","source":"It looks like all the ids in train have a corresponding id in id_total_cost! Lets use pandas to merge these appropriately! To reiterate from above, use pandas' built in methods whenever possible. I struggled doing this a cumbersome way and it either wouldn't work or would run forever. However, once again, using pandas resulted in a super fast and clean solution."},{"metadata":{"_uuid":"750a01c79f927c12d6364ac96397cbd00cf5c957","collapsed":true,"_cell_guid":"7fff3d37-f73b-4d7a-9c1d-e528ae6bab88","trusted":true},"cell_type":"code","source":"train_aug = pd.merge(train, id_total_cost, on='id', sort=False)","execution_count":41,"outputs":[]},{"metadata":{"_uuid":"360e6f0718beb26e2de481ea4e10e74ad8cf0124","_cell_guid":"9e2ed12e-f092-48aa-b208-506d49c6d83d","trusted":true},"cell_type":"code","source":"sum(pd.merge(train, id_total_cost, on='id', sort=False)['total_cost'].isna())","execution_count":42,"outputs":[]},{"metadata":{"_uuid":"9b273201148f1faa7ace925b658ae3fa0cb47d01","_cell_guid":"2ad4a26a-8d52-4a76-a9a9-2c270a238af6","trusted":true},"cell_type":"code","source":"train_aug.head()","execution_count":43,"outputs":[]},{"metadata":{"_uuid":"ad2ae756b7a211e1d193ea4cfc15707b38fd0016","_cell_guid":"e2cdc7f6-02c2-49de-9f99-0f91892a8957","trusted":true},"cell_type":"code","source":"plt.figure()\nplt.scatter(train_aug['teacher_number_of_previously_posted_projects'], train_aug['total_cost'])\nplt.ylabel('Total Cost')\nplt.xlabel('Number of Previous Submissions')\nplt.show()\n\nplt.figure()\nplt.scatter(train_aug['project_is_approved'], train_aug['total_cost'])\nplt.ylabel('Total Cost')\nplt.xlabel('Approved')\nplt.show()","execution_count":44,"outputs":[]},{"metadata":{"_uuid":"bcc172a60a8e4d0af5d5164bc5426f921ba35de9","_cell_guid":"aacc51cd-3523-4f9b-8a2c-eb814b558205"},"cell_type":"markdown","source":"We see that there appears to be a rough correlation with number of submissions and funds requested, specifically that the more submissions you send in the less you're going to ask for. However, in the second plot we see that there doesn't seem to be much impact on approval based off the total cost of resources. Some very expensive proposals get approved and some do not.\n\nLet's look at these correlations a bit more rigorously."},{"metadata":{"trusted":true,"_uuid":"aa643bfe8c7f643914787772b4e7e302e1a5aeca"},"cell_type":"code","source":"ax = sns.heatmap(train_aug.corr(), annot=True, cmap='coolwarm')","execution_count":66,"outputs":[]},{"metadata":{"_uuid":"cf9f3c8645368c03f2be566e2baed80654d9c620"},"cell_type":"markdown","source":"There is a weak positive correlation that if you've submitted more projects than your chances of approval are higher and there's a weak correlation with total cost and the approval of projects AND how many projects have been submitted in the past. This seems reasonable since if you ask for way too much money than you won't get approved and also the more you've won (or failed to win) the less you'll ask for."},{"metadata":{"_uuid":"dfd625164461f0bfbed43e4c5fcb53061ed0a4e7","_cell_guid":"176390fd-1dda-42b8-a7a8-d615bcd69be7"},"cell_type":"markdown","source":"# Wrap Up\n\nI hope this analysis helped! I definitely think this sort of careful analysis is an important starting point before diving into building a machine learning algorithm. I'm excited to start building now that I have a strong sense of what the underlying data looks like.\n\nOnce again, please comment with any suggestions or questions! \n\nHappy building"}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}