{"cells":[{"metadata":{"_uuid":"430212e0f0bbdea36da51f86947c4fb22309d4f9","_cell_guid":"b36536cf-095f-4f61-b669-245fdfbad59d"},"cell_type":"markdown","source":"# Mastering the basics: How to get ~80% using scikit-learn.\n\n**Author:** tommyod, mattemagisk\n\n> **Scored 0.79241 on Kaggle using logistic regression.**\n\n**Abstract:** In this notebook, we will demonstrate how to get close to $80\\%$ ROC AUC using the [scikit-learn](http://scikit-learn.org/stable/) library for Python. In contrast to some of the other kernels, the primary intention of this notebook is not to demonstrate new, novel methods, but rather to show that one can get far by observing the basics. Specifically, we will choose numerical, categorical and text features, build a scikit-learn pipeline, perform a hyperparameter search and present the results.\n\n\n# Table of contents\n\n- <a href=\"#choosing\">Choosing features</a>: Importing data and preparing features\n- <a href=\"#building\">Building a scikit-learn pipeline</a>: How to build a data processing pipeline\n- <a href=\"#hyperparameter\">Hyperparameter search</a>: Easy search for hyperparameters over the pipeline\n- <a href=\"#results\">Results and references</a>: The results and some further reading"},{"metadata":{"_uuid":"0f5b5055930d4138c65b4c22b2119c7e46ce22d4","_cell_guid":"1733cc72-28a7-4193-af91-7aea114ae72f"},"cell_type":"markdown","source":"# <a id=\"choosing\">Choosing features</a>"},{"metadata":{"_uuid":"4aca9e09d47d0702294310b33ffe2a157a512190","_cell_guid":"3c0b4823-0dd6-4eba-b773-62d181081296"},"cell_type":"markdown","source":"## Importing, setting up packages and loading data"},{"metadata":{"_uuid":"affc98aa79cb39378ff392cef98ed3a8845e83d1","_cell_guid":"bd85d43d-52b1-4fef-bcb4-02438e367d80"},"cell_type":"markdown","source":"### Library imports and settings"},{"metadata":{"_uuid":"c0c3373e71485a4be4fd96a5d57645abd469eed2","collapsed":true,"_cell_guid":"714b4f0f-8904-4cd3-9db2-af13d96fbed4","trusted":true},"cell_type":"code","source":"# Python library imports\nimport numpy as np # All numerical libraries in Python built on NumPy\nimport pandas as pd # Pandas provides DataFrames for data wrangling\nimport matplotlib.pyplot as plt # The de facto plotting library in Python\nimport itertools # For iterations\nimport string # For strings\nimport re # For regular expression (regex)\nimport os # Operating system functions\n\nplt.style.use('Solarize_Light2') # Set a non-default visual aesthetic for plots\n%matplotlib inline\npd.set_option(\"display.max_columns\", 2**10) # View more columns than pandas default","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"f14c52d50efbb4527ed62ee0dfef51da62cf16e8","_cell_guid":"73ca7618-ae84-4b8a-bdd0-f6b2ff06ab7e"},"cell_type":"markdown","source":"### Importing the test and train data, concatenate together"},{"metadata":{"_uuid":"e44f8a1aa98475daa6bde109990908e78941491c","_cell_guid":"9a8f3f98-bc2d-4ba6-8525-0ee88e2908c9"},"cell_type":"markdown","source":"We start by importing the data as [pd.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) objects."},{"metadata":{"_uuid":"49c374a80b3e3591e475a747c2ae7cc16e517cf3","collapsed":true,"_cell_guid":"135e9721-0ad5-4062-9c40-0833a07d9fe3","trusted":true},"cell_type":"code","source":"# Import the train and test data as pandas DataFrame instances\ndate_cols = ['project_submitted_datetime'] # Convert to datetime format automatically\ntrain = pd.read_csv(os.path.join(r'../input', 'train.csv'), low_memory=False, parse_dates=date_cols)\ntest = pd.read_csv(os.path.join(r'../input', 'test.csv'), low_memory=False, parse_dates=date_cols)","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"a6299b4a0c2f30ade082271b4b5394a62f5f1088","_cell_guid":"edfc9ac2-6e10-417b-9735-6dfbf2b01171"},"cell_type":"markdown","source":"We want to apply a set of identical functions to both. The most efficient way is to make a new column in each data set indicating the source and concatenating the DataFrames with [pd.concat()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html)."},{"metadata":{"_uuid":"3d02c613bb777212821b64b43ad752f9240f5c88","_cell_guid":"74e93791-cfd0-4bfe-b016-d6d4f446e09d","trusted":true},"cell_type":"code","source":"# Keep track of the original data source by adding a 'source' column\ntrain['source'] = 'train'\ntest['source'] = 'test'\ntest_train = pd.concat((test, train))\nprint(f'The shape of the data is {test_train.shape}.') # Showing off Python f-strings","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"9b813fd9f4773d21ec3039be6df69a70e1eb43a1","_cell_guid":"ee4e6b31-8826-4960-a50a-b16e95f09d60","trusted":true},"cell_type":"code","source":"# Checking for missing values\nprint(\"=\"*60)\nprint(\"Detecting NaN values in data:\")\nprint(\"=\"*60)\nprint(test_train.isnull().sum(axis=0)[test_train.isnull().sum(axis=0) > 0])","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"e93dd074bff5cf828c429feca99cb27328073cc9","_cell_guid":"2f194e67-8e15-4e26-ab5d-00bec55b2e72"},"cell_type":"markdown","source":"There are many missing third and fourth essays, as expected (see [competition page](https://www.kaggle.com/c/donorschoose-application-screening) for more info. We are also missing $78035$ target values. These are from the test data. Finally, there are $5$ missing values in the `teacher_prefix` column, fill it using [DataFrame.value_counts](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html), [DataFrame.idxmax](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.idxmax.html) (this is the [arg max function](https://en.wikipedia.org/wiki/Arg_max)) and [DataFrame.fillna](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html):"},{"metadata":{"_uuid":"1ac1f65d803d5d8bea881109ce5945d837a92a79","collapsed":true,"_cell_guid":"6b5a7c7e-a669-4d77-8c0a-4c63caa736bd","trusted":true},"cell_type":"code","source":"most_common = test_train.teacher_prefix.value_counts().idxmax() # Compute argument maximizing value counts\ntest_train.teacher_prefix = test_train.teacher_prefix.fillna(most_common)","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"d9ec868ceb5fe8a381ed43527fa4ddc410ae5f78","_cell_guid":"9ce5f066-1971-46d0-b368-517c6cfd2642"},"cell_type":"markdown","source":"## Features selection"},{"metadata":{"_uuid":"381ce3fd3120f00f7b40b5480a58cdacd8234247","_cell_guid":"ab3da851-de4f-4bf2-b883-2ae45c949fec"},"cell_type":"markdown","source":"Algorithms aren't very useful without good data. Going from a raw data set to a set of usable features for an algorithm is is process of [feature selection](https://en.wikipedia.org/wiki/Feature_selection). There are three types of available features in this competition:\n\n- **Numerical**: E.g. the total price of the request\n- **Categorical**: E.g. the gender of the requester\n- **Text**: E.g. the written text of the second essay\n\nWe will build a data processing pipeline which heeds these types of variables."},{"metadata":{"_uuid":"1320fb356c94edb19931aebd83a07072616daeaf","collapsed":true,"_cell_guid":"15228736-e809-4f9f-9b97-10aac7b62075","trusted":true},"cell_type":"code","source":"numerical_cols = []\ndummy_categorical_cols = []\ntext_cols = []","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"18ca55dca0f994581b7acf4cd3e177e509b4596a","_cell_guid":"0e522ab4-a431-4c5f-bb7b-f90c0f1ca292"},"cell_type":"markdown","source":"### Features selection"},{"metadata":{"_uuid":"3f9226d439ba16526db56111027daa90790921f1","_cell_guid":"aff3cb8e-cd0b-48b3-85e1-d49f55a6b5c7"},"cell_type":"markdown","source":"Since essays 3 and 4 were dropped, we will apply the following mapping to the essays:\n\n- $\\text{Essay}_1 := \\text{Essay}_1 \\oplus \\text{Essay}_2$\n- $\\text{Essay}_2 := \\text{Essay}_3 \\oplus \\text{Essay}_4$\n\nWhere $\\oplus$ denotes concatenation. This is easily accomplished using a mask in pandas, along with the clever [DataFrame.assign](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.assign.html) method for assigning new columns as part of a [pandas method chain](https://tomaugspurger.github.io/method-chaining). For more resources related to pandas, see [awesome-pandas](https://github.com/tommyod/awesome-pandas). Also note the use of `lambda` keyword. `lambda` is used to create [anonymous functions](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions), and is very useful."},{"metadata":{"_uuid":"ab254b7cea174cbee3f232c6f568b88d922f782f","collapsed":true,"_cell_guid":"9dc9e453-e622-4d1d-8bc2-ab8476914951","trusted":true},"cell_type":"code","source":"# Find the rows where essays 3 and 4 are not null\nmask_four_essays = ~(test_train.project_essay_3.isnull() & test_train.project_essay_4.isnull())\n\n# Assign them to columns 1 and 2 by concatenation\ntest_train[mask_four_essays] = (test_train[mask_four_essays]\n                 .assign(project_essay_1 = lambda df: df.project_essay_1 + df.project_essay_2)\n                 .assign(project_essay_2 = lambda df: df.project_essay_3 + df.project_essay_4))\n\n# Drop columns related to essay 3 and 4\ntest_train = test_train.drop(columns=['project_essay_3', 'project_essay_4'])","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"accab16e3498aac15c97e0880dd0799137e70714","_cell_guid":"c7b601c4-f051-4098-bbf4-9776783d5fa9"},"cell_type":"markdown","source":"### Importing data from `resources.csv`, join in "},{"metadata":{"_uuid":"311487281829581666d33b820e51c293b3f6401e","_cell_guid":"951fc9d1-faa1-4b32-9695-f6a662987d69"},"cell_type":"markdown","source":"Let's import data from `resources.csv`:"},{"metadata":{"_uuid":"fb4411ab4ab611fec5ef9106f88e43743ddb0b47","_cell_guid":"c794bf02-d3b4-4ee3-8d73-dd1a25917c18","trusted":true},"cell_type":"code","source":"# Load resources\nresources = pd.read_csv(os.path.join(r'../input', 'resources.csv'), low_memory=False)\nprint(f'The shape of the data is {resources.shape}.')","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"d6ebe0a911b90764aab6cc7aec03a2388f094ebd","_cell_guid":"aa9d4cc1-30f7-4f6c-a0ad-0913d6d32675","trusted":true},"cell_type":"code","source":"# Checking for missing values\nprint(\"=\"*30)\nprint(\"Detecting NaN values in data:\")\nprint(\"=\"*30)\nprint(resources.isnull().sum(axis=0)[resources.isnull().sum(axis=0) > 0])","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"ba8f7e168265a75cdc0506c18235e9e8b16006eb","_cell_guid":"070a78ad-0ca3-42d5-b851-49b75ddabc90"},"cell_type":"markdown","source":"We see that 292 descriptions are missing text. Our approach is to simply fill these with the character 'X':"},{"metadata":{"_uuid":"b6ce2d6c876a75a9a802f4c215e04713af6c1339","collapsed":true,"_cell_guid":"5129d029-6f31-407a-adbe-b5156ac91fb2","trusted":true},"cell_type":"code","source":"# Fill NAs\nresources = resources.fillna('X')","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"91beaa9b0c0299c70d81610f36352452f0d3a85e","_cell_guid":"301c6bb2-7178-4c0c-8187-f05e80017d32","trusted":true},"cell_type":"code","source":"# Previewing data\nresources.head(3)","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"e0260529d9d15fcf309443425fab8eeea9d0e5ed","_cell_guid":"a7b863c5-2c43-436a-9c5e-e4cebdae013e"},"cell_type":"markdown","source":"Now it's time to show off [method chaining](https://tomaugspurger.github.io/method-chaining):"},{"metadata":{"_uuid":"ad78246105f98cdfe7e22eeb4a851ececc0c16c8","collapsed":true,"_cell_guid":"4f9e6147-fbac-40b7-9d4f-43e1180dd127","trusted":true},"cell_type":"code","source":"def concatenate(series, sep=' '):\n    return sep.join(series) # Preferred to lambda, since this function has a name\n\n# Create a lot of possible numerical features, each starting with 'p_'\nresource_stats = (resources\n.assign(p_desc_len = lambda df: df.description.str.len()) # Length of description text\n.assign(p_total_price = lambda df: df.quantity * df.price) # Total price per item\n.groupby('id') # Grouping by teacher ID and aggregating the following columns (note we pass function as a list): \n.agg({'description': [pd.Series.nunique, concatenate], # Number of unique items asked for\n'quantity': [np.sum], # Total number of items asked for\n'price': [np.sum, np.mean], # Prices per item added and averaged (quantity not included)\n'p_desc_len': [np.mean, np.min, np.max], # Average description length\n'p_total_price': [np.mean, np.min, np.max]})\n)","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"84dc93427262c8279c1b9ea2ff7f64ad26904809","_cell_guid":"b3e7328f-e03e-49ba-8508-4e66906b6831"},"cell_type":"markdown","source":"Again, we check for missing values:"},{"metadata":{"_uuid":"00c5cc7231083aac11e4d50a48a629bc4d9212c8","_cell_guid":"ca3b612f-7d53-4d1b-a826-14c0b7d3262f","trusted":true},"cell_type":"code","source":"# Checking for missing values\nprint(\"=\"*30)\nprint(\"Detecting NaN values in data:\")\nprint(\"=\"*30)\nprint(resource_stats.isnull().sum(axis=0)[resource_stats.isnull().sum(axis=0) > 0])","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"96c76a8d0dd737e4314639e73e2fb3a0f5470d86","_cell_guid":"26a7ea14-6783-486e-be7d-adb69b030aa5"},"cell_type":"markdown","source":"Note that the new DataFrame has a MultiIndex. In the following we collapse it to a flat index:"},{"metadata":{"_uuid":"b1d959795cc232cd8b3784be95243e7f00573fa0","collapsed":true,"_cell_guid":"419ae5ec-f5b0-4199-a4c9-f936547c5612","trusted":true},"cell_type":"code","source":"# Collaps to flat index\nresource_stats.columns = ['_'.join([col, func]) for col, func in resource_stats.columns.values]\nnumerical_cols += list(resource_stats.columns.values)","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"23567ea1edbe0977f84c509161270db2bc42c63b","_cell_guid":"0df4c2a1-ebca-4415-81a2-aa4608ac0de9"},"cell_type":"markdown","source":"Let's join in information from `resources.csv`. We'll use [DataFrame.merge](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) (which corresponds to a [SQL JOIN](https://blog.codinghorror.com/a-visual-explanation-of-sql-joins/) operation)."},{"metadata":{"_uuid":"57c1dd24e464c51c48455395889ceb8d845c9996","_cell_guid":"07014e23-28cc-4d8e-bd9a-0ac0f113e381","trusted":true},"cell_type":"code","source":"# Merge the resources statistics into the test and train sets\ntest_train = test_train.merge(resource_stats, how='left', left_on='id', right_index=True)\ntest_train.sample(1).T.tail(6)","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"29a136eddaf831a33c30ceb4122bc596b21f946f","_cell_guid":"40b318f9-41db-4c54-a596-387003268c48"},"cell_type":"markdown","source":"## Encode categorical features"},{"metadata":{"_uuid":"1b728411564637452e2affeca695bcbad9289c3d","_cell_guid":"3420415e-c773-478b-993b-50984799f404"},"cell_type":"markdown","source":"At this point we have some numerical features. Now it's time to look at the categorical ones. We'll try adding the following:\n\n- The **month of the request**, added using the [pandas.dt accessor](https://pandas.pydata.org/pandas-docs/stable/basics.html#dt-accessor). Pandas has some pretty powerful datetime (`dt`) functionality. Check out the [documentation](https://pandas.pydata.org/pandas-docs/stable/timeseries.html) for details.\n- The **time of day**. Using [np.where](https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html) for a blazingly fast, vectorized `IF-THEN-ELSE` condition, we will find out if the application was sent during the **morning hours** or not.\n- The **gender of the applicant** and the **school state**, using [pandas.get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) which converts a categorical variable to a dummy variable representation."},{"metadata":{"_uuid":"6f79d88faa8295c3a2855dfd72c9c9eb9ed66494","collapsed":true,"_cell_guid":"f9de3ede-9e8c-4c83-af63-75d8bb3201a5","trusted":true},"cell_type":"code","source":"# Get the month from the datetime, convert it to a string, and add it as a new column\ntest_train['month'] = test_train.project_submitted_datetime.dt.month.apply(str)\n\n# Does submitting during the morning hours help?\ntest_train['daytime'] = pd.Series(np.where( \n    ((7 <= test_train.project_submitted_datetime.dt.hour) & \n     (test_train.project_submitted_datetime.dt.hour <= 10)), 1, 0)).apply(str)","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"15c29dd812d66d40d0da49576a654472d4bfcd71","collapsed":true,"_cell_guid":"43eff5f4-d3fa-4e2c-918a-e098b5d22bdd","trusted":true},"cell_type":"code","source":"# Simple dummy variables, i.e. every entry has one value\ndummy_colnames = ['teacher_prefix', 'month', 'school_state', 'daytime']\ndummies = pd.get_dummies(test_train.loc[:, dummy_colnames])\ndummy_categorical_cols += dummies.columns.tolist()\n\n# Concatenate along the columns\ntest_train = pd.concat((test_train, dummies), axis=1)","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"57fc08584d17582c4cb6ac5bfeb2d7e8ab0b7abe","_cell_guid":"25cc8fe4-c283-4b10-821e-f3c350ba98ba"},"cell_type":"markdown","source":"### Create categorical features from `project_subject_categories` and `project_subject_subcategories`"},{"metadata":{"_uuid":"3d8b1727a38e19ee6834c24b730b7e10f6e6e357","_cell_guid":"cb13cd27-e31b-4f51-b37a-9ab4b43f4561"},"cell_type":"markdown","source":"The following columns are special:\n\n- `project_subject_categories`\n- `project_subject_subcategories`\n\nEach entry may contain several categories - clearly a violation of the priniples of [tidy data](https://en.wikipedia.org/wiki/Tidy_data) if you consider a single category as a variable. There are $51$ different combinations of categories, and $416$ different combinations of subcategories. In reality, there are way less categories to choose from: $9$ categories and $30$ subcategories in total.\n\nIn the following we will define a function that creates a list of unique categories and sub-categories. Then we will create dummy variables from the resulting variables:    "},{"metadata":{"_uuid":"1494f2e70284935bc32a38840ca4ae233b8374cd","_cell_guid":"dd2ed91d-9c84-41a1-b13f-0cbf47bfe37f","trusted":true},"cell_type":"code","source":"# The following libray is for representing (printing) long strings and lists\nimport reprlib\n\ndef set_of_categories(col_name):\n    \"\"\"Retrieve a set of category names from a column\"\"\"\n    list_train = test_train[col_name].tolist()\n    list_test = test_train[col_name].tolist()\n    return set(', '.join(list_train + list_test).split(', '))\n\nunique_categories = set_of_categories('project_subject_categories')\nunique_subcategories = set_of_categories('project_subject_subcategories')\nunique_cats_total = list(unique_categories.union(unique_subcategories))\ndummy_categorical_cols += unique_cats_total\nprint('Categories:', reprlib.repr(unique_cats_total))","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"587c944f4f4d7e913b0c144d314b6389a820ce45","_cell_guid":"0d701d08-09d0-4f55-b182-45cf84d0e2ba"},"cell_type":"markdown","source":"Let's create the dummy encoding:"},{"metadata":{"_uuid":"c224d7c3e9dc4842fcf7f6f3e14d194a51ba21dc","_cell_guid":"95d5c0d3-51db-445f-bca5-024d0f713b7d","trusted":true},"cell_type":"code","source":"project_cat_colnames = ['project_subject_categories', 'project_subject_subcategories']\n\ndf_cats = test_train.loc[:, project_cat_colnames]\n\n# Create a new column for each category: put 1 if it's mentioned, 0 if not\nfor category in unique_categories:\n    df_cats[category] = np.where(df_cats.project_subject_categories.str.contains(category), 1, 0)\nfor category in unique_subcategories:\n    df_cats[category] = np.where(df_cats.project_subject_subcategories.str.contains(category), 1, 0)\n    \ndf_cats = df_cats.drop(columns=project_cat_colnames)\ndf_cats.head(1).T.head(5)","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"99aff4b69e6ac7b17d775d2673ce3d39aa97dfc9","_cell_guid":"2492152e-ef58-4388-9939-ad1f257091ce"},"cell_type":"markdown","source":"Again we'll use [pandas.concat](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html) to concatenate the results back into the DataFrame."},{"metadata":{"_uuid":"3e120aba0d9ceed9750ad5ef6c66e1eb58198b0c","_cell_guid":"ea018e28-5ee9-4fde-b75e-62e444fdca38","trusted":true},"cell_type":"code","source":"test_train = pd.concat((test_train, df_cats), axis=1)\nprint(f'The dataset now has ~{len(test_train.columns)} features.')\ntest_train.head(1)","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"3198812f08b606e5e1186d195bff49943d906b37","_cell_guid":"87565a2f-7174-4e42-8224-ff87ab12691d"},"cell_type":"markdown","source":"## Investigate text features"},{"metadata":{"_uuid":"673839b74f08ee56dcc55fc0ea40ffec5a194d38","collapsed":true,"_cell_guid":"720e9372-4820-4f8a-bc77-7871e27e8248","trusted":true},"cell_type":"code","source":"# Extracting text columns\ntext_cols += ['project_essay_1', 'project_essay_2', 'project_resource_summary', 'description_concatenate']","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"e0ada16ba025fed4784cb8f29443aa9952d08193","_cell_guid":"8b9f486b-b736-4c54-9cf4-1d91853629aa"},"cell_type":"markdown","source":"### Does polarity and subjectivity matter?"},{"metadata":{"_uuid":"f5e2e945e68fab9367d852af793263b1ed2429ce","_cell_guid":"94390c03-831b-4cdf-9104-195cf49956d1"},"cell_type":"markdown","source":"The TextBlog library includes functions for [sentiment analysis](http://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis), namely scoring **polarity** and **subjectivity** from a text. We'll investigate if these could be a useful features:"},{"metadata":{"_uuid":"90c10dff61278b9cacd31bbf3e8b745b40d8032b","_cell_guid":"e0030284-646b-45c2-bbf3-cb1e72cb70df","trusted":true},"cell_type":"code","source":"from textblob import TextBlob\n\ndf_subset = test_train[test_train.source == 'train']\n\n# Create a plot\nplt.figure(figsize=(14, 10))\n\nplt_num = 1\nfor column in text_cols:\n    \n    # Get data corresponding to the columns which we will use\n    data = df_subset.loc[:, ('project_is_approved', column)]\n        \n    for feature in ['polarity', 'subjectivity']:\n    \n        # Create a new subplot, set the title\n        plt.subplot(4, 2, plt_num)\n        plt.title(f'{column} - {feature.capitalize()}', fontsize=12)\n        plt_num += 1\n\n        # Function to get features from text using TextBlob\n        feature_from_txt = lambda x : getattr(TextBlob(x).sentiment, feature)\n\n        # Sample some data, apply the feature extraction function\n        approved_mask = (data.project_is_approved == 1)\n        approved = data[approved_mask].sample(1000).assign(feat=lambda df: df[column].apply(feature_from_txt))\n        not_approved = data[~approved_mask].sample(1000).assign(feat=lambda df: df[column].apply(feature_from_txt))\n        \n        # Plot the subplot\n        bandwidth = 0.225\n        ax = approved.feat.plot.kde(bw_method=bandwidth, label='Approved')\n        ax = not_approved.feat.plot.kde(ax=ax, bw_method=bandwidth, label='Not approved')\n        plt.xlim([-.5, 1])\n        plt.legend(loc='best')\n        \n# Show the full figure\nplt.tight_layout()\nplt.show()","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"a41e72530c01df904bf8b0b569b84426db25132a","_cell_guid":"8ef48444-510b-40a9-93bc-5a8e99c34fb8"},"cell_type":"markdown","source":"It seems like the following could be reasonable predictors of whether a request is approved or not:\n\n- Subjectivity of `description`\n- Polarity of `description`\n- Subjectivity of `project_resource_summary`\n- Polarity of `project_resource_summary`\n- Polarity of `project_essay_1`\n- Polarity of `project_essay_2`\n\n**WARNING:** Slow code ahead. Applying the polarity and subjectivity scoring takes a while. Skip ahead if you do not wish to run slow cells."},{"metadata":{"_uuid":"4edfb820d225386052a7b9d37998b8ef747ac3f5","_cell_guid":"2e8d539e-1115-4714-9ac4-17425467d4bf"},"cell_type":"markdown","source":"--------------------"},{"metadata":{"_uuid":"a21e024ab9b1dd1ecf8b4a9fcab44e401db79188","_cell_guid":"9daebd34-9474-4ce8-ab21-f285834186da","trusted":true},"cell_type":"code","source":"%%time\n\nsubj = lambda x: TextBlob(x).sentiment.subjectivity\npolar = lambda x: TextBlob(x).sentiment.polarity\n\ntest_train['description_subjectivity'] = test_train['description_concatenate'].apply(subj)\ntest_train['description_polarity'] = test_train['description_concatenate'].apply(polar)\n\ntest_train['project_resource_summary_subjectivity'] = test_train['project_resource_summary'].apply(subj)\ntest_train['project_resource_summary_polarity'] = test_train['project_resource_summary'].apply(polar)\n\ntest_train['project_essay_2_polarity'] = test_train['project_essay_2'].apply(polar)\ntest_train['project_essay_1_polarity'] = test_train['project_essay_1'].apply(polar)","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"82676764b9b55383db6aff18237c7490aecb7bc8","_cell_guid":"d12d8458-f993-4e7c-864e-7afba8943cff"},"cell_type":"markdown","source":"There are a few more obvious numerical features we can extract from the text data.\nWe'll extract:\n\n- The number of unique words.\n- The total number of words.\n- Their ratio, defined as the *vocabularity* of a request.\n\nTo accomplish this in a fast way, we'll make use of the scikit-learn [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), which converts a collection of text documents into a (sparse) matrix of token counts. The matrix will be very sparse, so scikit-learn will return a [scipy Compressed Sparse Row matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html). I mistakenly called [.todense()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.todense.html#scipy.sparse.csr_matrix.todense) on it, and it crashed my computer. \n\nFirst we will take a quick look at `text_cols` to remind ourselves of which features we're iterating over:"},{"metadata":{"_uuid":"246a9c9fef9aa6c8f2b7669792d2181f4273a626","_cell_guid":"0dd081fd-301a-4842-b424-f710fa0763e6","trusted":true},"cell_type":"code","source":"text_cols += ['project_title']\nprint(text_cols)","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"085171c2d8e8d3805d2bac96166a85b74f8c3193","_cell_guid":"05a1bb0b-157e-4e89-8a3c-fb9905fa375e","trusted":true},"cell_type":"code","source":"%%time\n\n# This code is a bit slow: approximately ~50 seconds on my computer\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\n\nfor text_col in text_cols: # Title, essays, summary, title\n    \n    # Get a sparse SciPy matrix of words counts\n    X = vectorizer.fit_transform(test_train[text_col].values)\n    \n    col_new_name = text_col.replace('project_', '')\n    \n    # Compute some basic statistics and add to dataset\n    unique_words = (X > 0).sum(axis=1) # Sum of words appearing more than zero times\n    num_words = (X).sum(axis=1) # Sum of occurences of words\n    test_train[col_new_name + '_unique_words'] = unique_words\n    test_train[col_new_name + '_num_words'] = num_words\n    test_train[col_new_name + '_vocab'] = np.exp(unique_words / (num_words + 10e-10))","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"73730f97b1a55d947462697f0263086161203dbc","_cell_guid":"4b0445a8-5d97-4e65-be96-34beb45464a0"},"cell_type":"markdown","source":"### Save and load the data \n\nThe code above is slow. We do not wish to run it every time we run the notebook, so\nwe'll store the data, then re-load it from the hard drive."},{"metadata":{"_uuid":"0bb9ad274d27b2910890e612cb9ac846d474bfee","collapsed":true,"_cell_guid":"bdf6b599-ca17-46aa-a5e4-447f4653034c","trusted":true},"cell_type":"code","source":"from csv import QUOTE_ALL\n\n# We're going to quote the fields using \", so remove it from the text just in case\nfor text_col in text_cols:\n    test_train[text_col] = test_train[text_col].str.replace('\"', ' ')\n   \n# Save the data\n# test_train.to_csv('preprocessed_test_train.csv', quoting=QUOTE_ALL)","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"beec2ca2313d14a762fdc42744e03feb18f2e487","collapsed":true,"_cell_guid":"864066cb-bf72-4809-8dcd-50e199bdf632","trusted":true},"cell_type":"code","source":"# Load the data from disk\n# test_train = pd.read_csv('preprocessed_test_train.csv', index_col=0, quoting=QUOTE_ALL)","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"3951a22d7d77ddcb81380f28bd5772cd7045798d","collapsed":true,"_cell_guid":"e51d7db1-a5f3-4553-a00f-8ff28c556aa1","trusted":true},"cell_type":"code","source":"numeric_cols = [\n 'teacher_number_of_previously_posted_projects',\n 'description_nunique',\n 'quantity_sum',\n 'price_sum',\n 'price_mean',\n 'p_desc_len_mean',\n 'p_desc_len_amin',\n 'p_desc_len_amax',\n 'p_total_price_mean',\n 'p_total_price_amin',\n 'p_total_price_amax',\n 'project_resource_summary_subjectivity',\n 'project_resource_summary_polarity',\n 'project_essay_2_polarity',\n 'project_essay_1_polarity',\n 'essay_1_unique_words',\n 'essay_1_num_words',\n 'essay_1_vocab',\n 'essay_2_unique_words',\n 'essay_2_num_words',\n 'essay_2_vocab',\n 'resource_summary_unique_words',\n 'resource_summary_num_words',\n 'resource_summary_vocab',\n 'description_concatenate_unique_words',\n 'description_concatenate_num_words',\n 'description_concatenate_vocab',\n 'title_unique_words',\n 'title_num_words',\n 'title_vocab']","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"09e3a2558b87ac10109ae405dba531d6907f1edc","_cell_guid":"a420ba45-95ca-42fa-90ea-d270fdd8e528"},"cell_type":"markdown","source":"# <a id=\"building\">Building a scikit-learn pipeline</a>\n\n## General theory on scikit-learn estimators and pipelines\n\nWe have three types of data: \n\n1. **numerical**, \n2. **categorical** (now encoded using dummy variables), and \n3. **text**.\n\nThe next step is to build a scikit-learn [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to handle the flow of data.\nThere are at least three good reasons for building pipelines, they include:\n\n1. **Modular components**: Individual estimators may be switched out for other components easily.\n2. **Object oriented**: Clean, readable code.\n3. **Efficient hyperparameter search**: Hyperparameter searches for every estimator in the pipeline may be automatized.\n\nThe genius of scikit-learn is it's API, described in detail in the paper [API design for machine learning software](https://arxiv.org/abs/1309.0238) on [arXiv.org](https://arxiv.org/). The three important objects are:\n\n1. **Estimators**: Subclasses [BaseEstimator](http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html), must implement a **fit** method.\n2. **Transformers**: Subclasses [BaseEstimator](http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html) and [TransformerMixin](http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html), must implement a **fit** and **transform** method.\n3. **Predictors**: Subclasses [BaseEstimator](http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html) and [RegressorMixin](http://scikit-learn.org/stable/modules/generated/sklearn.base.RegressorMixin.html) (or another predictor-mixin), must implement a **fit** and **predict** method.\n\nEvery transformer is an estimator, and every predictor is an estimator. The converse is not true.\nA [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) is simply a composition:\n\n$$\\text{Pipeline}(D) = (\\text{Estimator} \\circ \\text{Transformer}_N \\circ \\dots \\circ \\text{Transformer}_2 \\circ \\text{Transformer}_1 )(D)$$\n\nWhere $\\circ$ denotes composition of functions and $D$ denotes data. \n\nThis may all be a bit abstract, but hopefully the following image may clarify things (it is just a visual representation of the pipeline described above):\n\n![pipe.png](https://raw.githubusercontent.com/skaug/Kaggle-Club/master/DonorsChoose/python/pipe.png?token=AJm_qDboqiFisXoZ4e04f-L6fTA3fIsmks5a26vvwA%3D%3D)\n\nThere are two types of pipelines: pipelines where the final estimator is a **transformer** (making the entire pipeline a transformer), and pipelines where the final estimator is a **predictor** (making the entire pipeline a predictor).\n\n$$\\text{Pipeline}_\\text{trans}(D) = (\\text{Transformer}_{N+1} \\circ \\text{Transformer}_N \\circ \\dots \\circ \\text{Transformer}_2 \\circ \\text{Transformer}_1 )(D)$$\n\n$$\\text{Pipeline}_\\text{pred}(D) = (\\text{Predictor} \\circ \\text{Transformer}_N \\circ \\dots \\circ \\text{Transformer}_2 \\circ \\text{Transformer}_1 )(D)$$\n\nPipelines are sequential. Their complement is the [FeatureUnion](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html), which combines data from pipelines by concatenation of the columns.\n\n\n## Our pipeline - an overview\n\nWe're going to build the following pipeline.\n\n![full_pipe.png](https://raw.githubusercontent.com/skaug/Kaggle-Club/master/DonorsChoose/python/full_pipe.png?token=AJm_qBC8s6OAl_bVWB3lgxfZ6AlA0l7Dks5a26uowA%3D%3D)"},{"metadata":{"_uuid":"065e47eabb6ab0889b1bd39bd603c8a5d77674a6","_cell_guid":"709c2767-144c-485d-8f0e-fefe825651ec"},"cell_type":"markdown","source":"We'll import most of the estimators: \n   1. [RobustScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) (computes robust statistics by scaling the data according to some given quantile range),\n   2. [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) (a technique to re-weight words so that more meaningful words are given more weight), and \n   3. [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) (used when we want to model a the probability of a binary outcome).<br>\n    \nWe'll create some custom transformers: \n   1. **ColSplitter** to split the columns of the dataset to send data to parallel paths in the main pipeline, \n   2. **LogTransform** which computes $f(x;\\alpha) = \\alpha \\log(1 + x) + (1 - \\alpha) x$ on the numerical data, where we can search for optimal $\\alpha$ using hyperparameter search, and \n   3. **ParallelPipe**, which will let us apply Tfidf to each text feature individually.\n\nWe'll make liberal use of [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) and [FeatureUnion](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) objects to control data flow."},{"metadata":{"_uuid":"8f8f4f2cde142b42750a9bd4ded1f1157f63c0b4","collapsed":true,"_cell_guid":"3cbeac1b-5e29-4113-bae3-d0b1101cb935","trusted":true},"cell_type":"code","source":"# Imports for the pipeline\nfrom tempfile import mkdtemp\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc\nfrom scipy.sparse import coo_matrix, hstack\nfrom sklearn.preprocessing import RobustScaler","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"5b96599c0470d95c5f40cf4c42d40b86914632bf","_cell_guid":"eae19c70-e41e-418d-b828-baade03a1be9"},"cell_type":"markdown","source":"## Custom classes for the pipeline"},{"metadata":{"_uuid":"d7272d9aef6e4022ceff8b2833f668065bf5602b","collapsed":true,"_cell_guid":"96be42af-0bb3-4ce5-980b-2dfa2b0f705f","trusted":true},"cell_type":"code","source":"class ColSplitter(BaseEstimator, TransformerMixin):\n    \"\"\"Estimator to split the columns of a pandas dataframe.\"\"\"\n    \n    def __init__(self, cols, ravel=False):\n        self.cols = cols\n        self.ravel = ravel # If it's a text features, we ravel for TF-IDF\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, x):\n        return (x.loc[:, self.cols].values.ravel() if self.ravel\n                else x.loc[:, self.cols].values)\n    \nclass LogTransform(BaseEstimator, TransformerMixin):\n    \"\"\"Take linear combination of f(x) = x and g(x) = ln(x)\"\"\"\n    \n    def __init__(self, alpha):\n        \"\"\"alpha = 1 -> log\n        alpha = 0 -> linear\"\"\"\n        self.alpha = alpha\n        \n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, x):\n        return self.alpha * np.log(np.log(x + 2.001) + 1.001) + (1 - self.alpha) * x\n    \nclass ParallelPipe(BaseEstimator, TransformerMixin):\n    \"\"\"Put similar pipes in parallel. This has two effects:\n    (1) When transforming, the final result is concatenated (Feature Union)\n    (2) Setting hyperparameters on this pipe will set it on every sub-pipe.\"\"\"\n    \n    def __init__(self, pipes, *args, **kwargs):\n        self.pipes = pipes\n        \n    def fit(self, x, y=None):\n        [p.fit(x) for p in self.pipes]\n        return self\n\n    def transform(self, x):\n        try:\n            return hstack([p.transform(x) for p in self.pipes])\n        except:\n            return np.concatenate(tuple([p.transform(x) for p in self.pipes]), axis=1)\n    \n    def _get_param_names(self, *args, **kwargs):\n        return ['pipes']\n    \n    def set_params(self, *args, **kwargs):\n        [p.set_params(*args, **kwargs) for p in self.pipes]\n        return None\n    ","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"4d542197a52f557f463c4b2b69d010dc1d7d3416","_cell_guid":"861dccdd-43ef-4ee0-9346-e50b804bc2ac"},"cell_type":"markdown","source":"## Building the pipeline"},{"metadata":{"_uuid":"acc1a67ba44c8709b1257e1b06bb2d7708183cd2","collapsed":true,"_cell_guid":"998e2a34-c58f-4ef6-a636-a0406249ef28","trusted":true},"cell_type":"code","source":"# --------------------------------------------------\n# ----- (1) Numerical pipeline ---------------------\n# --------------------------------------------------\nnum_colsplitter = ColSplitter(cols=numeric_cols)\nlogtransform = LogTransform(alpha=1)\nscaler = RobustScaler(quantile_range=(5.0, 95.0))\nnumerical_pipe = Pipeline([('num_colsplitter', num_colsplitter), \n                           ('logtransform', logtransform),\n                           ('scaler', scaler)])\n\n# --------------------------------------------------\n# ----- (2) Categorical pipeline -------------------\n# --------------------------------------------------\ndummy_colsplitter = ColSplitter(cols=dummy_categorical_cols)\ncategorical_pipe = Pipeline([('dummy_colsplitter', dummy_colsplitter)])\n\n# --------------------------------------------------\n# ----- (3) Text pipeline --------------------------\n# --------------------------------------------------\ntext_subpipes = []\nfor text_col in text_cols:\n    text_colsplitter = ColSplitter(cols=text_col, ravel=True)\n    \n    tf_idf = TfidfVectorizer(sublinear_tf=False, norm='l2', stop_words=None, \n                             ngram_range=(1, 1), max_features=None)\n    \n    text_col_pipe = Pipeline([('text_colsplitter', text_colsplitter),\n                     ('tf_idf', tf_idf)])\n    text_subpipes.append(text_col_pipe)\n\ntext_pipe = ParallelPipe(text_subpipes)\n\n# --------------------------------------------------\n# ----- (4) Final pipeline - Logistic regression ---\n# --------------------------------------------------\n#cachedir = mkdtemp() # Creates a temporary directory\nestimator = LogisticRegression(penalty=\"l2\", C=0.21428)\n\npipeline_logreg = Pipeline([('union', FeatureUnion(transformer_list=[\n    ('numerical_pipe', numerical_pipe),\n    ('categorical_pipe', categorical_pipe),\n    ('text_pipe', text_pipe)\n])), \n                     ('estimator', estimator)])","execution_count":40,"outputs":[]},{"metadata":{"_uuid":"d09e9175bf2b7c84746fd40776c066864bc3ecd2","_cell_guid":"02975d5e-14c9-42e5-b86f-6c58963f8f61"},"cell_type":"markdown","source":"Note the penalty term in `estimator` in the final Logistic Regression. [Regularization](https://en.wikipedia.org/wiki/Regularization_%28mathematics%29) is very important in machine learning. Here we use $L_2$-regularization, also known as **weight decay** or **ridge regression**."},{"metadata":{"_uuid":"1c5721bafc8140d1478b86fa83ffa9385e6e09ec","_cell_guid":"1dd577cc-52c8-4915-af37-f41430983379"},"cell_type":"markdown","source":"# <a id=\"hyperparameter\">Hyperparameter search </a>"},{"metadata":{"_uuid":"c1a093dca4e65028fb161345fed84a1df7cee4f2","collapsed":true,"_cell_guid":"c2509929-0b25-45f3-aa22-0992135435bd","trusted":true},"cell_type":"code","source":"# Testing that all numeric values are finite\nassert np.all(np.isfinite(test_train[numeric_cols].values))","execution_count":41,"outputs":[]},{"metadata":{"_uuid":"8493f5f3638b6e26d695c5226bdc1c75d5c45097","_cell_guid":"164464c0-bc0a-4b8a-91f0-10f1ebe02bc0"},"cell_type":"markdown","source":"Time to run hyperparameter search using [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) on the entire pipeline. \n\n**WARNING:** The code below is not representative of what we tried. We experimented with several hyperparameters and several estimators which we liberally removed and added to the full pipeline."},{"metadata":{"_uuid":"ba224c754ab7962b7032e58bd47c109b73dcb83a","_cell_guid":"41bbb479-6d8e-4cde-80c5-6c32a838a33d"},"cell_type":"markdown","source":"## Prepare grid search"},{"metadata":{"_uuid":"3b94c6c46fc6147e35fd601cb32c97f7f3e54147","_cell_guid":"097963fc-8be1-4470-8063-86c09af20c89"},"cell_type":"markdown","source":"The [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) (Receiver operating characteristic) curve is a useful model validation tool when dealing with binary classifiers. We will use the [area under the curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve) (AUC) of the ROC curve to determine the best hyperparameters:"},{"metadata":{"_uuid":"238bf5b618c636ab1a1182aa80aebe1dc2a3510d","collapsed":true,"_cell_guid":"3aed1f74-d1d4-45bf-b9e7-4f2fab4996af","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Dictionary with parameters names to try during search\n# We tried a lot of parameters, you may uncomment the code an experiment\nparam_grid = {\"estimator__C\": np.linspace(0.24285-0.1, 0.24285+0.1, num=6)\n             # \"union__numerical_pipe__logtransform__alpha\": [0.8, 1],\n             # \"union__text_pipe__tf_idf__stop_words\": [None, 'english']\n             }\n\n# run randomized search\ngrid_search = GridSearchCV(pipeline_logreg, param_grid=param_grid,\n                                    scoring='roc_auc',\n                                    n_jobs=1,\n                                    verbose=1,\n                                    cv=3)","execution_count":45,"outputs":[]},{"metadata":{"_uuid":"4ef28754aa25227a8e4b714b67f33ebc61c714e9","_cell_guid":"613c89e4-85c7-4225-9578-5687d43982dc"},"cell_type":"markdown","source":"## Run grid search\n\nWe'll grab an equal amount of data: $50\\%$ approved and $50\\%$ not approved."},{"metadata":{"_uuid":"551ae91824a9e52b8aa6497358139e2a50a0cc10","_cell_guid":"8bc63833-9bee-432c-9b47-1aad7049f6dc","trusted":true,"collapsed":true},"cell_type":"code","source":"# Grab 2 subsets of the data\nn = 25000\nsubset_A = test_train.loc[lambda df: (df.project_is_approved == 1)].sample(n)\nsubset_B = test_train.loc[lambda df: (df.project_is_approved == 0)].sample(n)\ntest_train_subset = pd.concat((subset_A, subset_B))\ntest_X = test_train_subset\ntest_y = test_X.project_is_approved","execution_count":46,"outputs":[]},{"metadata":{"_uuid":"87c68810374d553a2c53f7ed7f119987554e8cae","_cell_guid":"59d25fe8-c2ae-4808-9a0d-efedeb8b37ca","trusted":true},"cell_type":"code","source":"import warnings\nperform_grid_search = True\n\nif perform_grid_search:\n    with warnings.catch_warnings():\n        # UserWarning: Persisting input arguments took 0.70s to run.\n        warnings.simplefilter(\"ignore\", category=UserWarning)\n        grid_search.fit(test_X, test_y.values.ravel())\n    best_estimator = grid_search.best_estimator_\n    print('Best score:', grid_search.best_score_)\n    print('Best params:\\n', grid_search.best_params_)\nelse:\n    # If we do not run grid search, we use the pipeline as initialized\n    best_estimator = pipeline_logreg","execution_count":47,"outputs":[]},{"metadata":{"_uuid":"5b88059272da1d321572176c6a8bff5b39cf10c6","_cell_guid":"b41136bc-a6eb-4179-b5ff-c075b4862086"},"cell_type":"markdown","source":"## Fit on 100% of the data and predict on test data"},{"metadata":{"_uuid":"71a554a3c66581821a2537d2bae8bfbea5366da0","collapsed":true,"_cell_guid":"29458011-915a-44b5-9cc8-1067c39f716b","trusted":true},"cell_type":"code","source":"# Grab 100% of the train data, run fitting using the best estimator\nfull_train = test_train.loc[lambda df: df.source == 'train']\ny_pred = best_estimator.fit(full_train, full_train.project_is_approved.ravel())","execution_count":48,"outputs":[]},{"metadata":{"_uuid":"cdaa11d53cd9fc13474dd242d891a9a018fee271","_cell_guid":"d2f05d8a-d6b8-4918-a335-ff75aeae0ae7","trusted":true},"cell_type":"code","source":"# Predict on test data\ntest = test_train[test_train.source == 'test']\ntest.loc[:, ('project_is_approved', )] = best_estimator.predict_proba(test)[:, 1]","execution_count":52,"outputs":[]},{"metadata":{"_uuid":"7a109b2fa80b4441b907def7f619151309e3b747","_cell_guid":"4ccff4f3-2faf-4849-88c7-20024ce79336"},"cell_type":"markdown","source":"Finally, we save the predictions to a .csv-file and submit it to Kaggle:"},{"metadata":{"trusted":true,"_uuid":"c8960960dd841970cdc624409a8e57f274259a19"},"cell_type":"code","source":"test[['id','project_is_approved']].shape","execution_count":53,"outputs":[]},{"metadata":{"_uuid":"a6876efb56d0c3e9e1da5b7ab01a529ff690c7ac","_cell_guid":"20e8a2d0-360f-4532-af07-540c0a5c794f","trusted":true,"scrolled":true},"cell_type":"code","source":"test[['id','project_is_approved']].to_csv('submission.csv', index=False)","execution_count":54,"outputs":[]},{"metadata":{"_uuid":"d80de7e9bef955d652892df6fcd12f6d8e2a1b30","_cell_guid":"25b9a033-71df-4011-8923-95e4b81f897e"},"cell_type":"markdown","source":"# <a id=\"results\">Results and references</a>\n\n## Results and further work\n\nIn the end, this kernel (notebook) achieved a ROC AUC of $\\sim 80\\%$ using relatively well known, simple techniques.\nWhen we drop parts of the pipeline (by commenting out a line of code), we saw that $\\geq 70\\%$ could be achieved using only simple numerical features.\nAlthough this kernel is still a few percent from the highest scoring submissions, it is our hope that it can be intructional for people wanting to learn about Python, pandas and scikit-learn.\nIf time allows, it would be interesting to experiment with other models than logistic regression. To do so efficiently, we might have to reduce the number of features from the text pipeline.\n\n## References\n\n- [awesome-pandas](https://github.com/tommyod/awesome-pandas) - A collection of resources for pandas (Python) and related subjects.\n- [API design for machine learning software: experiences from the scikit-learn project](https://arxiv.org/abs/1309.0238) - Paper describing the scikit-learn API.\n- [Official pandas documentation](https://pandas.pydata.org/pandas-docs/stable/)\n- [Official scikit-learn documentation](http://scikit-learn.org/stable/)\n\n**Question, comment or feedback? Please leave a comment below!**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}