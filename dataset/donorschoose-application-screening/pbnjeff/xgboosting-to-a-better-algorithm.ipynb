{"cells":[{"metadata":{"_uuid":"be5c585a54c1475627381706b661f3a31a839374","_cell_guid":"e630b602-b58a-466b-9ee5-b3be9eabf301"},"cell_type":"markdown","source":"# Imports and initial data fetching"},{"metadata":{"collapsed":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":false},"cell_type":"code","source":"# data storage and viz\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import hstack\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# text processing\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom nltk import word_tokenize\nfrom nltk.stem.snowball import EnglishStemmer\nfrom textblob import TextBlob\n\n# model imports\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n# model evaluation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import make_scorer\n\n# for diagnostics\nnrows = None\n\ndata = pd.read_csv('../input/train.csv', nrows=nrows).drop('teacher_id', axis=1)\nresource_data = pd.read_csv('../input/resources.csv')\n\ny = data['project_is_approved']\nX = data.drop('project_is_approved', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"217ec64a3f95d1b67bb28ce0efba91acb597c16a","_cell_guid":"5202b657-4cc1-483a-851f-86c1f75563bd"},"cell_type":"markdown","source":"# Making the resource data actually useful\n\nSumming up the prices and text. The text will be treated as just a large blob of text from which we create a model. I don't actually care too much about the quantities, but it could be a feature that is useful later."},{"metadata":{"collapsed":true,"_uuid":"eee8cb6f148a73b63aa4f2981decb0234f72fbdb","_cell_guid":"448e5601-92db-4b26-963b-9df65c4fd112","trusted":false},"cell_type":"code","source":"resource_data = pd.read_csv('../input/resources.csv')\nresource_data['total_price'] = resource_data['quantity'] * resource_data['price']\nresource_data['description'] = resource_data['description'] + ' '\nresource_data = pd.DataFrame(resource_data.groupby('id').apply(lambda x: x.sum()))\nresource_data = resource_data.reset_index()\nresource_data = resource_data.rename(columns={'level_1':'attribute', 0:'value'})\nresource_data = resource_data.pivot(index='id', columns='attribute', values='value').drop(columns=['id','price']).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"286fb07ff4867abec1f609ac0bda31f3e3cfbee8","_cell_guid":"f5a4592d-1bc6-48fb-b79f-f93450615d70"},"cell_type":"markdown","source":"# Modifying the Tfidf Vectorizer\n\nAn example of using a stemmer to modify the tf-idf vectorizer. Useful so that if words like \"student\" and \"students\" appear frequently, we don't overcount them; intuitively, they're basically the same word."},{"metadata":{"collapsed":true,"_uuid":"e687e6ca1dd49cb44d3184e2c1c175d1f911f289","_cell_guid":"2b40d91f-e1d2-419a-9a99-7a867de15206","trusted":false},"cell_type":"code","source":"class StemmedTfidfVectorizer(TfidfVectorizer):\n    def __init__(self, stemmer, stop_words, max_features):\n        super(StemmedTfidfVectorizer, self).__init__(stop_words=stop_words, max_features=max_features)\n        self.stemmer = stemmer\n        \n    def build_analyzer(self):\n        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n        return lambda doc:(self.stemmer.stem(w) for w in analyzer(doc))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ce436d165bfdc3f98778d3e30bad66417b48208","_cell_guid":"37166380-bd52-4acb-9808-ebe8108489c7"},"cell_type":"markdown","source":"# Data processing\n\nCombining a lot of the features together. Look at the description for reasoning about `combine_essays`. The `max_features` is something that could be improved on a lot."},{"metadata":{"collapsed":true,"_uuid":"0351106495e600789785a294a2440a4d7a97821a","_cell_guid":"5c996b13-0977-4e88-9754-f4acdf680760","trusted":false},"cell_type":"code","source":"def join_resource_features(df, resource_data):\n    joined_df = df.merge(resource_data, on='id', how='left')\n    return joined_df\n\n\ndef combine_essays(row):\n    \"\"\"Cleans up the project essays by combining 4 essays into 2.\n    \n    See competition description for details.\n    \"\"\"\n    if row['project_submitted_datetime'] < '2016-05-17':\n        row['project_essay_1'] = row['project_essay_1'] \\\n                                        + ' ' + row['project_essay_2']\n        row['project_essay_2'] = row['project_essay_3'] \\\n                                        + ' ' + row['project_essay_4']\n    return row\n\ndef preprocess_text(X_train, X_test):\n    \"\"\"Cleans and processes the text data for BOTH train and test sets.\n    \n    This combines essays 1/2 & 3/4 then fits the vectorizer to\n    the training data.\n    \"\"\"\n    \n    X_train.apply(lambda row: combine_essays(row), axis=1)\n    X_test.apply(lambda row: combine_essays(row), axis=1)\n    \n    X_train['description'] = X_train['description'].fillna(' ')\n    X_test['description'] = X_test['description'].fillna(' ')\n    \n    X_train['description'] = X_train['description'].astype(str)\n    X_test['description'] = X_test['description'].astype(str)\n    \n    X_train = X_train.drop(columns=['project_essay_3','project_essay_4'], axis=1)\n    X_test = X_test.drop(columns=['project_essay_3','project_essay_4'], axis=1)\n    \n    stemmer = EnglishStemmer()\n    \n    count_vec_title = CountVectorizer(stop_words='english', max_features=50)\n    count_vec_title.fit(X_train['project_title'])\n    tfidf_vec_title = StemmedTfidfVectorizer(stemmer=stemmer, stop_words='english', max_features=500)\n    tfidf_vec_title.fit(X_train['project_title'])\n    \n    count_vec_essay1 = CountVectorizer(stop_words='english', max_features=50)\n    count_vec_essay1.fit(X_train['project_essay_1'])\n    tfidf_vec_essay1 = StemmedTfidfVectorizer(stemmer=stemmer, stop_words='english', max_features=500)\n    tfidf_vec_essay1.fit(X_train['project_essay_1'])\n    \n    count_vec_essay2 = CountVectorizer(stop_words='english', max_features=50)\n    count_vec_essay2.fit(X_train['project_essay_2'])\n    tfidf_vec_essay2 = StemmedTfidfVectorizer(stemmer=stemmer, stop_words='english', max_features=500)\n    tfidf_vec_essay2.fit(X_train['project_essay_2'])\n    \n    count_vec_resource = CountVectorizer(stop_words='english', max_features=50)\n    count_vec_resource.fit(X_train['project_resource_summary'])\n    tfidf_vec_resource = StemmedTfidfVectorizer(stemmer=stemmer, stop_words='english', max_features=500)\n    tfidf_vec_resource.fit(X_train['project_resource_summary'])\n    \n    count_vec_resource_desc = CountVectorizer(stop_words='english', max_features=50)\n    count_vec_resource_desc.fit(X_train['description'])\n    tfidf_vec_resource_desc = StemmedTfidfVectorizer(stemmer=stemmer, stop_words='english', max_features=500)\n    tfidf_vec_resource_desc.fit(X_train['description'])\n    \n    return (X_train, X_test,\n            count_vec_title, tfidf_vec_title,\n            count_vec_essay1, tfidf_vec_essay1,\n            count_vec_essay2, tfidf_vec_essay2,\n            count_vec_resource, tfidf_vec_resource,\n            count_vec_resource_desc, tfidf_vec_resource_desc)\n\n\ndef process_features(df,\n                    count_vec_title, tfidf_vec_title,\n                    count_vec_essay1, tfidf_vec_essay1,\n                    count_vec_essay2, tfidf_vec_essay2,\n                    count_vec_resource, tfidf_vec_resource,\n                    count_vec_resource_desc, tfidf_vec_resource_desc):\n    \"\"\"Takes features and changes them into model-usable format.\n    \n        Returns a numpy array.\n    \"\"\"\n    \n    dropped_columns = ['project_submitted_datetime','project_subject_categories','project_subject_subcategories',\n                      'project_title','project_essay_1','project_essay_2','project_resource_summary','description']\n    \n    ids = df['id']\n    df = df.drop('id', axis=1)\n    \n    df['quantity'] = df['quantity'].astype(float)\n    df['total_price'] = df['total_price'].astype(float)\n    \n    df = pd.get_dummies(df, columns=['teacher_prefix'])\n    df = pd.get_dummies(df, columns=['school_state'])\n    df = pd.get_dummies(df, columns=['project_grade_category'])\n    \n    # Subjects and subject subcategories taken from donorschoose.org\n    \n    subjects = ['Applied Learning', 'Health & Sports', 'History & Civics', 'Literacy & Language', 'Math & Science', 'Music & The Arts', 'Special Needs', 'Warmth, Care & Hunger']\n    \n    subject_subcategories = { 'Applied Learning': ['Character Education', 'College & Career Prep', 'Community Service', 'Early Development', 'Extracurricular', 'Other', 'Parent Involvement'],\n                            'Health & Sports': ['Gym & Fitness', 'Health & Wellness', 'Nutrition Education', 'Team Sports'],\n                            'History & Civics': ['Civics & Government', 'Economics', 'Financial Literacy', 'History & Geography', 'Social Sciences'],\n                            'Literacy & Language': ['ESL', 'Foreign Languages', 'Literacy', 'Literature & Writing'],\n                            'Math & Science': ['Applied Sciences', 'Environmental Science', 'Health & Life Science', 'Mathematics'],\n                            'Music & The Arts': ['Music', 'Performing Arts', 'Visual Arts'],\n                            'Special Needs': [],\n                            'Warmth, Care & Hunger': [] }\n    \n    for subject in subjects:\n        df['Subject: ' + subject] = df['project_subject_categories'].str.contains(subject).astype(int)\n        for subcategory in subject_subcategories[subject]:\n            df['Subcategory: ' + subcategory] = df['project_subject_subcategories'].str.contains(subcategory).astype(int)\n\n    df['project_submitted_datetime'] = pd.to_datetime(df['project_submitted_datetime'])\n    df['submit_year'] = df['project_submitted_datetime'].dt.year\n    df['submit_month'] = df['project_submitted_datetime'].dt.month\n    df['submit_day'] = df['project_submitted_datetime'].dt.day\n    df['submit_dayofweek'] = df['project_submitted_datetime'].dt.weekday_name\n    df['submit_hour'] = df['project_submitted_datetime'].dt.hour\n    \n    df = pd.get_dummies(df, columns=['submit_dayofweek'])\n    \n    pattern = r'\\\\([a-zA-Z]|\")'\n    df['project_title'] = df['project_title'].str.replace(pattern,'')\n    \n    df['title_polarity'] = df['project_title'].apply(lambda x: TextBlob(x).sentiment.polarity)\n    df['project1_polarity'] = df['project_essay_1'].apply(lambda x: TextBlob(x).sentiment.polarity)\n    df['project2_polarity'] = df['project_essay_2'].apply(lambda x: TextBlob(x).sentiment.polarity)\n    df['resource_polarity'] = df['project_resource_summary'].apply(lambda x: TextBlob(x).sentiment.polarity)\n    \n    title_dtm = count_vec_title.transform(df['project_title'])\n    project1_dtm = count_vec_essay1.transform(df['project_essay_1'])\n    project2_dtm = count_vec_essay2.transform(df['project_essay_2'])\n    resource_dtm = count_vec_resource.transform(df['project_resource_summary'])\n    resource_desc_dtm = count_vec_resource_desc.transform(df['description'])\n    \n    title_tfidf_dtm = tfidf_vec_title.transform(df['project_title'])\n    project1_tfidf_dtm = tfidf_vec_essay1.transform(df['project_essay_1'])\n    project2_tfidf_dtm = tfidf_vec_essay2.transform(df['project_essay_2'])\n    resource_tfidf_dtm = tfidf_vec_resource.transform(df['project_resource_summary'])\n    resource_desc_tfidf_dtm = tfidf_vec_resource_desc.transform(df['description'])\n    \n    df['project_title'] = df['project_title'].apply(str.lower)\n    df['project_title'] = df['project_title'].apply(word_tokenize)\n    \n    df['has_exclamation'] = df['project_title'].apply(lambda x: '!' in x).astype(int)\n    df['title_token_count'] = df['project_title'].apply(len)\n    \n    df = df.drop(dropped_columns, axis=1)\n    \n    #processed_array = hstack( (title_dtm, project1_dtm, project2_dtm,\n    #                           resource_dtm, title_tfidf_dtm,\n    #                           project1_tfidf_dtm, project2_tfidf_dtm,\n    #                           resource_tfidf_dtm, resource_desc_dtm, resource_desc_tfidf_dtm, np.array(df)[:,:]) )\n    \n    processed_array = hstack( (title_tfidf_dtm,\n                               project1_tfidf_dtm, project2_tfidf_dtm,\n                               resource_tfidf_dtm, resource_desc_dtm, resource_desc_tfidf_dtm, np.array(df)[:,:]) )\n    \n    return (ids, processed_array)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abb8b63c03b3cbd7dc1f83876797255bd3bc82ac","_cell_guid":"19786f3c-e70a-4c00-9c59-7f2ff878d15a"},"cell_type":"markdown","source":"# For predicting with the test set"},{"metadata":{"_uuid":"a3c82360bf4d572db8e034adaea857fb3ebeca3b","_cell_guid":"f6cd0f7b-4089-42c1-b15c-316b877f4194","trusted":false,"collapsed":true},"cell_type":"code","source":"X_train = X\nX_test = pd.read_csv('../input/test.csv', nrows=nrows).drop('teacher_id', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79e5361e158e96007406e77646a8670bddde1303","_cell_guid":"32cebb2d-55a2-4cc5-b481-e27b68de087c"},"cell_type":"markdown","source":"# Data processing"},{"metadata":{"collapsed":true,"_uuid":"6114aefb10de35b29a59252e7a36dff2ed50aa2d","_cell_guid":"4b26d79f-b265-454f-bae9-0dfb9bc01a42","trusted":false},"cell_type":"code","source":"X_train = join_resource_features(X_train, resource_data)\nX_test = join_resource_features(X_test, resource_data)\n\n(X_train_preprocessed, X_test_preprocessed,\ncount_vec_title, tfidf_vec_title,\ncount_vec_essay1, tfidf_vec_essay1,\ncount_vec_essay2, tfidf_vec_essay2,\ncount_vec_resource, tfidf_vec_resource,\ncount_vec_resource_desc, tfidf_vec_resource_desc) = preprocess_text(X_train, X_test)\n\n(X_train_ids, X_train) = process_features(X_train_preprocessed,\n                                        count_vec_title, tfidf_vec_title,\n                                        count_vec_essay1, tfidf_vec_essay1,\n                                        count_vec_essay2, tfidf_vec_essay2,\n                                        count_vec_resource, tfidf_vec_resource,\n                                        count_vec_resource_desc, tfidf_vec_resource_desc)\n\n(X_test_ids, X_test) = process_features(X_test_preprocessed,\n                                        count_vec_title, tfidf_vec_title,\n                                        count_vec_essay1, tfidf_vec_essay1,\n                                        count_vec_essay2, tfidf_vec_essay2,\n                                        count_vec_resource, tfidf_vec_resource,\n                                        count_vec_resource_desc, tfidf_vec_resource_desc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"849efdb48e73172bc5d99b8f08a87ebfa8d1c8dd","_cell_guid":"e4f64b71-44e8-4da8-abb8-be67b23b4c0a"},"cell_type":"markdown","source":"# For validating models\n\nGrid searching on XGBoost"},{"metadata":{"collapsed":true,"_uuid":"4e3ff799c2ff92288a35088fb3414a94a2c327e4","_cell_guid":"445a5844-d222-4549-ae6b-31574b035361","scrolled":false,"trusted":false},"cell_type":"code","source":"#xgb_model = XGBClassifier(max_depth=2, learning_rate=0.20, objective='binary:logistic')\n#parameters = {\n#                'num_estimators': range(650)\n#             }\n#clf = GridSearchCV(xgb_model, parameters, verbose=10, cv=6, scoring='roc_auc')\n#clf.fit(X_train, y)\n#print(clf.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9134fd081d5f73e5fb1cc8f3295d3169e8dcca49","_cell_guid":"1b169b5f-c803-40a1-9c7e-e6ec5c0621c1"},"cell_type":"markdown","source":"# For making predictions with the test set"},{"metadata":{"collapsed":true,"_uuid":"7db3fd43cbb4ac1f21980af392e680515be99bdb","_cell_guid":"6cf71c07-8416-4745-b52e-4c445e56ad8b","scrolled":true,"trusted":false},"cell_type":"code","source":"xgb_model = XGBClassifier(max_depth=2, n_estimators=1000, learning_rate=0.20, objective='binary:logistic')\nxgb_model.fit(X_train, y)\npredictions = xgb_model.predict_proba(X_test)\noutput_df = pd.DataFrame(columns=['id','project_is_approved'])\noutput_df['id'] = X_test_ids\noutput_df['project_is_approved'] = predictions[:,1]\noutput_df.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"d0e63b28fe393101b7690f4a7a4b81d462f3d0c2","_cell_guid":"067a2640-25e2-4306-934e-66bca72e4cba","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"35eff759f764882d80ec51c170896b9e9271f50f","_cell_guid":"a5e0e5cb-3594-4200-b63f-102fb582a8ab","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}