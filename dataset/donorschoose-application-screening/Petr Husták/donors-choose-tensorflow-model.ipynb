{"cells":[{"metadata":{"_cell_guid":"44d470f2-3ac4-4ba1-8964-5cb5324233f2","_uuid":"c5e5866b57e177b743ff958b9bdd40f12a79e199"},"cell_type":"markdown","source":"# Donors Choose - TensorFlow Model\n\n## 1. Introduction\nI have create this project in order to gain practical experience with TensorFlow after finishing the Google Machine Learning Crash Course. It has been a rewarding experience, as it touched on several aspects of TensorFlow modeling, feature engineering, natural language processing, as well as time series analysis.\n\nI have developed my model as an exercise with the goal of getting a high AUC in the test dataset. If I was developing a production model with practical applicability in mind, I would adjust my my methodology as well as the problem definition. I explain these considerations in section 2.\n\nThe main technical limitation I have encountered was the 1GB hard drive space in the Kaggle environment, which was easily filled up in the NN training phase. Even though I managed to reduce the hard drive footprint with some data preprocessing, I was still not able to utilize the full training data due to this constrain. My plan is to learn more about the different ways to input data into TensorFlow that may overcome this limitation.\n\n## 2. Considerations for Practical Applicability\n\n### 2.1 Discussion of Test Dataset and Repeatability\nThe test dataset for this exercise was created as a random selection from the full application population. While this is generally a safe way to test if the model will perform in practice, in this particular case it may introduce unwanted leakage of information. Here are two examples how this can happen... \n - Example 1: Since test records can be pulled from any point in the history of a teacher's submission list, the training and scoring process has access to future approval performance of given teacher. For example the model knows if the teacher's next project was approved or not. Apparently this will not be known in real world application.\n - Example 2: For each test record, we have a number of training records submitted at the same time (e.g same day, same week) with approval results. Including this data can contain approval rate information related to some unknown factor like for example volunteer shortage at the time. Again, this information will be known in the test data, but will not be available in the future.\n\nI explicitly create features in my kernel exploiting both of the examples above, since for this exercise I only worry about the test dataset performance. Nonetheless, even without creating explicit features, complex models like DNNs may pick up on these patterns, therefore care needs to be taken to make sure the model is not learning from leaked information. One way to do this is would be to create an out-of-time test dataset including all applications submitted after a set date, while train and validation data would contain applications from before that date. Performance on this type of test dataset would be a better reflection of the final usability of the model.\n\n### 2.2 Discussion of Fairness, Explicability, Stability\nAlthough this competition dataset and task is a wonderful resource for practicing modeling with Tensorflow (and this is what I came for personally), I can see three possible issues with an approach that uses NNs to simply replicate observed approvals from the past. \n- Fairness: We understand that each volunteer has potentially their own bias in approval decisions, but in addition to this, there may also be unwanted overall bias (cultral, geographical, ... ). While building the model on a large set of volunteers will improve consistency, the model will still reflect the overall bias if there was one. Furhtermore, if it is put into action, it may even reinforce this bias in a positive feedback loop. It is therefore very important to conduct a through analysis and continuous monitoring to ensure that the model is fair.\n- Explicability: Ideally, the model should assist volunteers to identify applications that may be problematic, and need further attention. Natural extension of this process is to notify the teacher if their project was rejected and for what reason. Therefore it would be very useful if the model can provide not only probability of rejection, but also  the most likely rejection reason. This would be best achived by building a multi-class NN that would be modeled on rejection reason rather than a simple indicator of approval. If rejection reason is not available as a catgorical column (was not collected), one option would be to compile correspondence from volunteers to teachers whose projects were rejected and use NLP to classify rejection reasons.\n- Stability: One of the predictors of approval are keywords in the requested resource descriptions. These can thing like \"wobble chair\" or \"Apple iPad\". Understandably, these terms can age. New techologies emerge, products can be created by alternative brands and so on. If a model is trained on outdated data, it may be looking for resources that are not requested anymore and lose strength because of that. This instability cannot be completely overcome, but it can be controlled for and understood by analyzing the durability of terms and approval rates as a time series. "},{"metadata":{"_cell_guid":"56a83e9e-eec0-4a32-8d85-9032eb77b228","_uuid":"55ba96cf457f9c7f5ea99e6d6bf90966bc595795"},"cell_type":"markdown","source":"## 3. Implementation\n\n### 3.1 Include libraries and basic setup"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.data import Dataset\nimport numpy as np\nimport re\nimport sklearn.metrics as metrics\nimport pandas as pd\nfrom matplotlib import cm\nfrom matplotlib import gridspec\nfrom matplotlib import pyplot as plt\nfrom collections import OrderedDict\ntf.logging.set_verbosity(tf.logging.ERROR)","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"f33fc56b-4ee7-46d1-9681-9419bda52449","_uuid":"e2fe89fd57c21d7089df2cb9bb2b296a3f8e1651"},"cell_type":"markdown","source":"### 3.2 Load all data files"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-output":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load data files\ntraining_dataset = pd.read_csv('../input/train.csv', sep=',')\nresources_dataset = pd.read_csv('../input/resources.csv', sep=',')\ntest_dataset = pd.read_csv('../input/test.csv', sep=',')","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"ff256910-2712-4cb1-bd9e-b9d993a44ac3","_uuid":"e3f5431f3d30326e953e735107d40da7823a58a1"},"cell_type":"markdown","source":"### 3.3 Create time-context features\nThese features capture the context of each application relative to other applications. These include ...  \n- Is this the first/last application of the teacher? \n- How long since/to the previous/next application of the teacher? \n- Was the teachers's previous/next application approved?\n- What was the approval rate of applications within the same day?\n\nAs I discuss above in section 2.1, most of these features are only possible because of the nature of the test dataset in this exercise. In practice, we will not have this context for new applications coming in."},{"metadata":{"_cell_guid":"bf5706fe-db55-46b0-9232-8ac1cf74f591","_uuid":"7cd492f0b7d9edd6c350db3e562a58c65fdff5ad","trusted":true},"cell_type":"code","source":"# Join train and test data. Context features are based on the full dataset \ndfall = training_dataset[[\"id\",\"teacher_id\",\"project_submitted_datetime\",\"project_is_approved\"]].append(\n            test_dataset[[\"id\",\"teacher_id\",\"project_submitted_datetime\"]])\n\n# Parse project submitted date and sort the data frame\ndfall[\"project_submitted_datetime\"] = pd.to_datetime(dfall[\"project_submitted_datetime\"])\ndfall = dfall.sort_values(by=[\"project_submitted_datetime\"])\ndfall = dfall.set_index(\"project_submitted_datetime\")\n\n# Calculate rolling 1 day approval rate feature (roll_approved_pct)\n#  - Calculated as approved applications within the last 24 hours / total applications within the last 24 hours\n#  - Always exclude the current application from calculation\ndfall[\"project_is_approved1\"] = dfall[\"project_is_approved\"].fillna(0)\ndfall[\"train_data\"] =1-( dfall[\"project_is_approved\"].isna()*1)\ndfall[[\"roll_approved\",\"roll_total\"]] = dfall.rolling('1d')[\"project_is_approved1\",\"train_data\"].sum()\ndfall[\"roll_approved_pct\"] = (dfall[\"roll_approved\"]-dfall[\"project_is_approved1\"])/(dfall[\"roll_total\"]-dfall[\"train_data\"])\ndfall = dfall.reset_index(level=\"project_submitted_datetime\")\n\n# Create teacher-context features\n#  - Sort by teacher ID + project submitted datetime and shift forward and backward to get information about next/previous application\ndfall = dfall.sort_values(by=[\"teacher_id\",\"project_submitted_datetime\"])\ndfall[[\"last_project_is_approved\",\"last_dt\"]] = dfall.groupby(\"teacher_id\")[\"project_is_approved\",\"project_submitted_datetime\"].shift(1)\ndfall[[\"next_project_is_approved\",\"next_dt\"]] = dfall.groupby(\"teacher_id\")[\"project_is_approved\",\"project_submitted_datetime\"].shift(-1)\ndfall[\"last_project_is_rejected\"] = 1-dfall[\"last_project_is_approved\"]\ndfall[\"next_project_is_rejected\"] = 1-dfall[\"next_project_is_approved\"]\ndfall[\"time_since_last_project\"] = (dfall[\"project_submitted_datetime\"] - dfall[\"last_dt\"])/np.timedelta64(1, 'h')\ndfall[\"time_to_next_project\"] = (dfall[\"next_dt\"] - dfall[\"project_submitted_datetime\"])/np.timedelta64(1, 'h')\ndfall[\"first_project_ind\"] = dfall[\"time_since_last_project\"].isna()*1\ndfall[\"last_project_ind\"] = dfall[\"time_to_next_project\"].isna()*1\n#  Use rank to get the position of the application in teacher's submission history\ndfall[\"project_number\"] = dfall.groupby(\"teacher_id\")[\"project_submitted_datetime\"].rank().astype(int)\n\n#  Clean up variables\ndfall = dfall.fillna({\"last_project_is_approved\":0,\"next_project_is_approved\":0,\n              \"last_project_is_rejected\":0,\"next_project_is_rejected\":0,})\nfor col in [\"last_project_is_approved\",\"next_project_is_approved\",\"last_project_is_rejected\",\"next_project_is_rejected\"]:\n    dfall[col] = dfall[col].fillna(0)\n    dfall[col] = dfall[col].astype(int)\ndfall.head(5)","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"39396e51-c149-41e9-99c1-6f8f61d37ea0","_uuid":"6d345b1317539fc4e19f4182520d26c052cccb87"},"cell_type":"markdown","source":"### 3.4 Split Training and Validation Data\nDue to disk space constraints during NN training phase, I am only using a subset of the available data for training and validation. "},{"metadata":{"_cell_guid":"fc1ac189-2563-4505-a47e-06e54f0fd84d","_uuid":"9352b21b82cd871266a59e12c5884291c16fd96f","collapsed":true,"trusted":true},"cell_type":"code","source":"# Split training and validation data\nN_TRAINING = 60000\nN_VALIDATION = 20000 \n\n# Create separate training and validation datasets\ntraining_dataset = training_dataset.reindex(np.random.RandomState(seed=67).permutation(training_dataset.index))\ntraining_data = training_dataset.head(N_TRAINING).copy()\nvalidation_data = training_dataset.tail(N_VALIDATION).copy()","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"e5848376-bd4d-4e98-923f-d03d96dbd54f","_uuid":"c3934c41f5d3f30f90fff3196c8a310f22697a16"},"cell_type":"markdown","source":"### 3.5 Preprocess the resources dataset\nThis includes imputing missing descriptions and aggregating the data on request id level. Three aggregated variables are created:\n - resource_count - number of different items requested \n - resource_price - total price for all resources\n - resource_descriptions - Appended descriptions from all resource rows"},{"metadata":{"_cell_guid":"8c1837ae-f36c-43ce-b7ba-be654884cb76","_uuid":"e7dd90e710e6772568d5711bb8d973bb94d1456d","trusted":true},"cell_type":"code","source":"# Impute missing descriptions \nresources_dataset = resources_dataset.fillna({'description':'N/A'})\n# Calculate total price for each resource\nresources_dataset['total_price'] = resources_dataset['quantity'] * resources_dataset['price']\n# Aggregate resources on id level: count rows, add total price, append descriptions\ngrp = resources_dataset.groupby(['id'])\nresources_dataset_grp = grp.apply(lambda row: pd.Series(dict(\n    resource_count=row['total_price'].count(),\n    resource_price=row['total_price'].sum(),\n    resource_descriptions=' '.join(row['description']) )) ).reset_index()\nresources_dataset_grp.head(5)","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"cb5b94fe-6b77-4e55-8e14-526fe734e67c","_uuid":"0e6b8cc9c9b7996995e7eb1a889040ad6d3ea051"},"cell_type":"markdown","source":"### 3.6 Calculate Teacher Statistics\nTeacher statistics is another set of features that relies on the fact that the test dataset is selected randomly from the full timeline of submissions (for more detail, see section 2.1). The statistics are calculated separately for each teacher and are in three levels:\n- All-time teacher statistics\n- Same-month teacher statistics\n- Same-day teacher statistics\n\nFor each of these periods, I calculate \n- Count of applications\n- Count of approved applications\n- Count of rejected applications\n- Percent of approved applications\n\nTo avoid the approval label leakage, the stats are alwas calculated based on all applications in the give time frame except the one in question."},{"metadata":{"_cell_guid":"c89a92f0-b2bc-4cd1-b7c2-b82a44946fa5","_uuid":"b37ac9315c94d9de2e57234d7b587593c8fcbfb4","scrolled":false,"trusted":true},"cell_type":"code","source":"# Calculation of teacher statistics is based on all training data\ntraining_data_ext = training_dataset[[\"teacher_id\",\"project_submitted_datetime\",\"project_is_approved\"]].copy()\n\n# Discretize submitted datetime on day and month level\nfor ds in [training_data,validation_data,test_dataset,training_data_ext]:\n    ds[\"project_submitted_date\"] = pd.to_datetime(ds[\"project_submitted_datetime\"]).dt.date\n    ds[\"project_submitted_month\"] = pd.to_datetime(ds[\"project_submitted_datetime\"]).dt.to_period('M')\n\n# Caluclate all-time teacher stats\nteacher_stats = pd.DataFrame(training_data_ext.groupby(\"teacher_id\")[\"project_is_approved\"].agg([\"count\",\"sum\"]).reset_index())\nteacher_stats = teacher_stats.rename(index=str,columns={\"count\":\"app_cnt\",\"sum\":\"approved_cnt\"})\nteacher_stats[\"approved_pct\"] = teacher_stats[\"approved_cnt\"]/teacher_stats[\"app_cnt\"]\nteacher_stats[\"rejected_cnt\"] =teacher_stats[\"app_cnt\"] - teacher_stats[\"approved_cnt\"]\ntraining_data = pd.merge(training_data, teacher_stats, on='teacher_id', how=\"left\")\nvalidation_data = pd.merge(validation_data, teacher_stats, on='teacher_id', how=\"left\")\ntest_dataset = pd.merge(test_dataset, teacher_stats, on='teacher_id', how=\"left\")\n\n# Caluclate teacher stats for each day and month\nfor period in [\"date\",\"month\"]:\n    teacher_stats = pd.DataFrame(training_data_ext.groupby([\"teacher_id\",\"project_submitted_\" + period])[\"project_is_approved\"].agg([\"count\",\"sum\"]).reset_index())\n    teacher_stats = teacher_stats.rename(index=str,columns={\"count\":\"same_\" + period + \"_app_cnt\",\"sum\":\"same_\" + period + \"_approved_cnt\"})\n    teacher_stats[\"same_\" + period + \"_approved_pct\"] = teacher_stats[\"same_\" + period + \"_approved_cnt\"]/teacher_stats[\"same_\" + period + \"_app_cnt\"]\n    teacher_stats[\"same_\" + period + \"_rejected_cnt\"] = teacher_stats[\"same_\" + period + \"_app_cnt\"] - teacher_stats[\"same_\" + period + \"_approved_cnt\"]\n    training_data = pd.merge(training_data, teacher_stats, on=['teacher_id','project_submitted_' + period], how=\"left\")\n    validation_data = pd.merge(validation_data, teacher_stats, on=['teacher_id','project_submitted_' + period], how=\"left\")\n    test_dataset = pd.merge(test_dataset, teacher_stats, on=['teacher_id','project_submitted_' + period], how=\"left\")\n\n# Inpute zeroes for all missing counts\nfillNaDict = {\"app_cnt\":0,\"approved_cnt\":0,\"rejected_cnt\":0,\n              \"same_date_app_cnt\":0,\"same_date_approved_cnt\":0,\"same_date_rejected_cnt\":0,\n              \"same_month_app_cnt\":0,\"same_month_approved_cnt\":0,\"same_month_rejected_cnt\":0}\ntraining_data = training_data.fillna(fillNaDict)\nvalidation_data = validation_data.fillna(fillNaDict)\ntest_dataset = test_dataset.fillna(fillNaDict)\n\n# Adjust all stats, so that current row (and its approval) is excluded from the calculation\nfor ds in [training_data,validation_data]:\n    for prd in [\"\",\"same_date_\",\"same_month_\"]:\n        ds[prd + \"app_cnt\"] = ds[prd + \"app_cnt\"] - 1\n        ds[prd + \"approved_cnt\"] = ds[prd + \"approved_cnt\"] - ds[\"project_is_approved\"]\n        ds[prd + \"rejected_cnt\"] = ds[prd + \"app_cnt\"] - ds[prd + \"approved_cnt\"]\n        ds[prd + \"approved_pct\"] = ds[prd + \"approved_cnt\"]/ds[prd + \"app_cnt\"]\n\ntraining_data.head(5)","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"98f524cc-c003-43e8-a786-7d05e32636b8","_uuid":"b7d157aebdddbfdfe550dcf12815f358f6f2e18c"},"cell_type":"markdown","source":"### 3.7  Merge resource and time-context data to the training and test datasets "},{"metadata":{"_cell_guid":"9cbb26de-d109-42d9-a46e-3603846ac618","_uuid":"3957ee9e45fa12dd0d6016908626a654717768de","scrolled":true,"trusted":true},"cell_type":"code","source":"# Merge grouped resource data to the training and test datasets \ntraining_data = pd.merge(training_data, resources_dataset_grp, on='id', how=\"left\")\nvalidation_data = pd.merge(validation_data, resources_dataset_grp, on='id', how=\"left\")\ntest_dataset = pd.merge(test_dataset, resources_dataset_grp, on='id', how=\"left\")\n\n# Merge time-context resource data to the training and test datasets \ndropList = [\"teacher_id\",\"project_submitted_datetime\",\"project_is_approved\"]\ntraining_data = pd.merge(training_data, dfall.drop(columns=dropList), on='id', how=\"left\")\nvalidation_data = pd.merge(validation_data, dfall.drop(columns=dropList), on='id', how=\"left\")\ntest_dataset = pd.merge(test_dataset, dfall.drop(columns=dropList), on='id', how=\"left\")\ntraining_data.head(5)","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"7eba9bbb-3ea6-4e7e-93cd-83de5cf8d1e6","_uuid":"c141e4ecfb9ad6b93ccbbf572a4b8f3c5dd460eb"},"cell_type":"markdown","source":"### 3.8 Text Feature Preprocessing\nThis section contains standard preprocessing steps for text columns. All modifications are done to the training, validation, and test datasets to make sure the situation is equivalient between training and scoring.\n\n**Multi-Category Columns**\n- Project_subject_categories and project_subject_subcategories contain comma-separated key phrases\n- To deal with these columns in a standard way, they are converted to a pseudo-text: 1. remove spaces and 2. replace commas by space  \n- After this transformation, each keyphrase becomes one word. This allows us to treat these columns as standard text columns\n\n**Text Columns**\n- Columns are cleaned to remove punctuation and special characters\n- Derived features are created for each text feature: ..._is_na and ..._word_count\n- Length of each text is limited to 500 words"},{"metadata":{"_cell_guid":"dd269e5e-06af-4853-aec9-3671b7be3f40","_uuid":"f7adb1d9cde48337b79f7b32778cd0077ffb1787","scrolled":false,"trusted":true},"cell_type":"code","source":"datasets = [training_data,validation_data,test_dataset]\n# 1. Multi-Category Columns\n#   These columns include arbitrary number of comma-separated key phrases.\n#    ->  We will remove spaces and then replace commas with spaces. That will allow us to treat the data as text with each key phrase being one \"word\"\nmulti_cat_cols = [\"project_subject_categories\",\"project_subject_subcategories\"]\nfor col in multi_cat_cols:\n    for ds in datasets:\n        ds[col] = ds[col].str.replace(\" \",\"\").str.replace(\",\",\" \")\n\n# 2. Text columns\n#  - Clean: Remove punctuation, convert to lower case\n#  - Engineer features: word_count, is_na\ntext_cols = [\"project_title\",\"project_essay_1\",\"project_essay_2\", #\"project_essay_3\",\"project_essay_4\",\n             \"project_resource_summary\",\"project_subject_categories\",\"project_subject_subcategories\",\"resource_descriptions\"]\nmax_word_count = 500\nfor col in text_cols:\n    for ds in datasets:\n        ds[col + \"_is_na\"] = ds[col].isnull() * 1\n        ds[col] = ds[col].fillna('').str.lower().str.replace('[^\\w\\s]','')\n        ds[col + \"_word_count\"] = ds[col].str.count(' ') + 1\n        ds[col] = ds[col].str.split(' ',max_word_count).str[0:max_word_count].str.join(' ')\n\n# Impute missing data for the teacher_prefix feature\nfor ds in datasets:\n    ds[\"teacher_prefix\"] = ds[\"teacher_prefix\"].fillna('')\n\n# Project Submitted Datetime Features\nfor ds in datasets:\n    ds[\"project_submitted_datetime_dt\"] = pd.to_datetime(ds[\"project_submitted_datetime\"])\n    ds[\"submitted_year\"] = ds[\"project_submitted_datetime_dt\"].dt.year\n    ds[\"submitted_month\"] = ds[\"project_submitted_datetime_dt\"].dt.month\n    ds[\"submitted_dow\"] = ds[\"project_submitted_datetime_dt\"].dt.weekday_name\n    ds[\"submitted_dom\"] = ds[\"project_submitted_datetime_dt\"].dt.day\n\ntraining_data.head(5)","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"2c331707-28db-402c-a59b-e8368dad1941","_uuid":"2fdd7778781a90d3417df6187b6f94cdf472e61e"},"cell_type":"markdown","source":"### 3.9 Create Features Definition\nI have defined 6 types of features for this project, each with a different representation in the neural network\n - Numeric features - This is a standard numeric input signal. Used for numeric columns without NaN values.\n - Numeric features bucketized - By default, these get bucketized into 20 bins of equal population. Alternatively, a custom bucket cutoff list can be specified\n - Categorical features - These categorical columns are represented as one-hot indicators feeding directly into the model. Used for catgorical columns with small number of values. \n - Categorical features with embedding - These categorical columns are represented as one-hot indicators feeding into a 2-node embedding.\n - Binary featues - Input columns have values 0 and 1, and are represented as categorical column with identity\n - Text features - Each text feature is represented as categorical with dictionary list, feeding into an embeding with 4 cells. Dictionary is determined from training data separately for each feature and includes top 250 terms that are the most overrepresented and 250 most underrepresented in the approved applications.  Terms must also meet minimum threshold count in the approved applications in training data (200). To reduce disk space footprint, additional preprocessing step is done at this stage, which removes all non-dictionary words from the respective text columns."},{"metadata":{"_cell_guid":"73a43ca4-2a0c-4b64-8396-573442dd4853","_uuid":"e15581c49fd91cd1fbddb90181f2ac34a6892236","scrolled":false,"trusted":true},"cell_type":"code","source":"# Define lists of all feature types\nnumeric_features = [\"app_cnt\",\"approved_cnt\",\"rejected_cnt\",\n                    \"same_date_app_cnt\",\"same_date_approved_cnt\",\"same_date_rejected_cnt\",\n                    \"same_month_app_cnt\",\"same_month_approved_cnt\",\"same_month_rejected_cnt\"]\nnumeric_features_bucket = [\"teacher_number_of_previously_posted_projects\",\"resource_price\",\n                    \"time_since_last_project\",\"time_to_next_project\"\n                    ,\"approved_pct\",\"same_date_approved_pct\",\"same_month_approved_pct\",\"project_number\",\n                    \"roll_approved_pct\"] + [col+\"_word_count\" for col in text_cols]\n# Custom cutoffs for selected bucketized numeric features\nnumeric_features_bucket_cutoffs = {}\nnumeric_features_bucket_cutoffs[\"time_since_last_project\"] = [0.1,0.2,0.3,0.5,1.0,6.0,24.0,168.0,336.0,720.0,8760.0]\nnumeric_features_bucket_cutoffs[\"time_to_next_project\"] = numeric_features_bucket_cutoffs[\"time_since_last_project\"]\nnumeric_features_bucket_cutoffs[\"project_number\"] = [1.0,2.0,3.0,4.0,10.0,20.0]\n\ncategorical_features = [\"project_grade_category\",\"teacher_prefix\",\"submitted_year\",\"submitted_month\",\"submitted_dow\"]\ncategorical_features_embed = [\"school_state\",\"submitted_dom\"]\nbinary_features = [\"last_project_is_approved\",\"next_project_is_approved\",\n                   \"last_project_is_rejected\",\"next_project_is_rejected\",\n                   \"first_project_ind\", \"last_project_ind\"] + [col + \"_is_na\" for col in text_cols]\ntext_features = text_cols\ntarget = \"project_is_approved\"\n\n# Based on the lists of column names above, create list of TensorFlow features \nfeatures = []\nfor feature in binary_features:\n    features.append( tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_identity(feature, 2) ))\n\nfor feature in numeric_features_bucket:\n    quantiles = []\n    if feature in numeric_features_bucket_cutoffs:\n        quantiles = numeric_features_bucket_cutoffs[feature]\n    else:\n        num_buckets = 20\n        quantiles = training_data[feature].quantile(np.arange(1.0, num_buckets) / num_buckets)\n        quantiles = [quantiles[q] for q in quantiles.keys()]\n        quantiles = list(OrderedDict.fromkeys(quantiles))\n    #print(feature)\n    #print(quantiles)\n    features.append( tf.feature_column.bucketized_column(tf.feature_column.numeric_column(feature), boundaries=quantiles) )\n\nfor feature in numeric_features:\n    features.append( tf.feature_column.numeric_column(feature) )\n\nfor feature in categorical_features:\n    dictionary = training_data[feature].unique()\n    features.append(tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list(feature,dictionary)))\n\nfor feature in categorical_features_embed:\n    embedding_count = 2\n    dictionary = training_data[feature].unique()\n    features.append(tf.feature_column.embedding_column(tf.feature_column.categorical_column_with_vocabulary_list(feature,dictionary),embedding_count))\n\n# Helper function for text feature definition: Create dictionary of significant terms (return a list)\ndef getSignificantTerms(feature,posCountThreshold,wordCount):\n    # Count term frequencies among accepted and among rejected applications\n    tokenizerPos = Tokenizer()\n    tokenizerNeg = Tokenizer()\n    tokenizerPos.fit_on_texts(training_data.loc[training_data[target]==1][feature])\n    tokenizerNeg.fit_on_texts(training_data.loc[training_data[target]==0][feature])\n    posCounts = tokenizerPos.word_counts\n    negCounts = tokenizerNeg.word_counts\n    words = []\n    # Iterate over all terms in the accepted submissions\n    for w in posCounts:\n        p = posCounts[w]\n        # Term must appear at least posCountThreshold-times in the accepted submissions to be considered\n        if p > posCountThreshold:\n            n = 0\n            if w in negCounts:\n                n = negCounts[w]\n            # Add term to candidate list with some basic stats\n            # - word, freq\n            words.append([w,p,n,p/(p+n)])    \n    if len(words) <= wordCount:\n        return [x[0] for x in words]\n    words = sorted(words,key=lambda x: -x[3])\n    wordCountPart = int(wordCount/2)\n    ret = [x[0] for x in words[0:wordCountPart]]  # Most overrepresentd in accepted\n    ret += ret + [x[0] for x in words[-wordCountPart:]]  # Most underrepresented in accepted\n    return set(ret)\n\n# Helper function for text feature definition: Prune text fields to keep only dictionary words\ndef dropNonDictionaryWords(feature,dictionary):\n    dict_set = set(dictionary)\n    for ds in [training_data,validation_data,test_dataset]:\n        ds[feature] = ds[feature].apply(lambda x: set([y for y in x.split(' ') if y in dict_set])).str.join(' ')\n    #print(training_data[feature][:10])\n    \nfor feature in text_features:\n    max_words = 500\n    min_pos_frequency = 200\n    embedding_count = 4\n    dictionary = getSignificantTerms(feature,min_pos_frequency,max_words)\n    #print(feature)\n    dropNonDictionaryWords(feature,dictionary)\n    features.append(tf.feature_column.embedding_column(tf.feature_column.categorical_column_with_vocabulary_list(feature,dictionary),embedding_count))","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"d5118b13-27e6-4d59-a973-e726a410ceb4","_uuid":"c73081680c11d0ea95e62da4ec9bb6bb1c01ec85"},"cell_type":"markdown","source":"### 3.10 Create Input Function for DNNClassifier \n\nInput function must be in sync with the feature list created above."},{"metadata":{"_cell_guid":"5d6351e5-99e9-4977-a3dc-a007c9e0f3ea","_uuid":"661fda4331a7c815cd384d2960d9f642c92be42c","collapsed":true,"trusted":true},"cell_type":"code","source":"# Helper function for text feature ipnut: Translate text (string with words) into a list of words \ndef _parse_text(features,targets):\n    for key in text_features:\n        features[key] = tf.string_split([features[key]]).values\n        #print(features[key])\n    return (features,targets)\n    \ndef my_input_fn( data, batch_size=1, shuffle=True, num_epochs=None ):\n    # Create dictionary of all columns that are used by the features as defined above\n    features = {key:np.array(data[key]) for key in (binary_features + numeric_features + numeric_features_bucket + categorical_features \n                                                    + categorical_features_embed + text_features)}\n    targets = data[target]\n    \n    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n    ds = ds.map(_parse_text)\n    ds = ds.padded_batch(batch_size, ds.output_shapes).repeat(num_epochs)\n    # Shuffle the data, if specified\n    if shuffle:\n      ds = ds.shuffle(10000)\n    \n    # Return the next batch of data\n    features, labels = ds.make_one_shot_iterator().get_next()\n    return features, labels","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"6c76872cec5b3eff95a064c040fbfc0efe238a6f"},"cell_type":"markdown","source":"### 3.11 Train Model and Score Test and Validation Data"},{"metadata":{"_cell_guid":"e3ae3a37-b7c5-4dfe-87dc-b83c2a88a4d7","_uuid":"0b52174225f3204cc94190d09d3a10e870145f6d","scrolled":false,"trusted":true},"cell_type":"code","source":"# Define Estimator\nestimator = tf.estimator.DNNClassifier(\n    feature_columns=features,\n    hidden_units=[10,10],\n    optimizer=tf.train.ProximalAdagradOptimizer(\n      learning_rate=0.1,\n      l1_regularization_strength=0.0001\n    ))\n# Create training and scoring versions of the input function \ntrain_fn = lambda:my_input_fn(data=training_data,batch_size=100)\ntrainev_fn = lambda:my_input_fn(data=training_data,num_epochs=1,shuffle=False)\nvalid_fn = lambda:my_input_fn(data=validation_data,num_epochs=1,shuffle=False)\n# Train model\nestimator.train(input_fn=train_fn, steps=2000)\n# Calculate training and validation metrics\ntraining_metrics = estimator.evaluate(input_fn=trainev_fn)\nvalidation_metrics = estimator.evaluate(input_fn=valid_fn)\nprint(\"AUC train/test: {}/{}\".format(training_metrics['auc'],validation_metrics['auc']))","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"37be6a48afd6c841be62bdd18330a534d6250a19"},"cell_type":"markdown","source":"### 3.12 Score Test Dataset and Save Results "},{"metadata":{"_cell_guid":"a419e28b-0422-4da3-8641-0ab0e8f4a1a5","_uuid":"1d417ea7eec814d8c211d49d797236288fb944ef","trusted":true},"cell_type":"code","source":"# Make predictions\ntest_dataset[\"project_is_approved\"] = 0\ntest_fn = lambda:my_input_fn(data=test_dataset,num_epochs=1,shuffle=False)\npredictions_generator = estimator.predict(input_fn=test_fn)\npredictions_list = list(predictions_generator)\n\n# Extract probabilities\nprobabilities = [p[\"probabilities\"][1] for p in predictions_list]\n\nmy_submission = pd.DataFrame({'id': test_dataset[\"id\"], 'project_is_approved': probabilities})\n\nmy_submission.to_csv('my_submission.csv', index=False)","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"f9be4f45-ee63-4909-aa8f-6033407e1990","_uuid":"aed294380d52503f5ebdd23c32f50eef0e3484a8"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}