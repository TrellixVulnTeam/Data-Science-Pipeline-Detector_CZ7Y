{"cells":[{"metadata":{"_uuid":"bea7ceb46375be0126c48f7a315566a49a0cf677","_cell_guid":"c579b7f4-3b2b-4604-9ffc-5d231c93abce"},"cell_type":"markdown","source":"I have been in kaggle for half a year, I learned a lot from kaggle kernels, thanks to kagglers, I like these kernels!\n\nIn this kernel, I will show you different ways to do text classifier, including LogisticRegression, RandomForest, lightgbm and neural networks.\n\nThen,  I'll introduce you a powerful tool for model stacking."},{"metadata":{"collapsed":true,"_uuid":"7bfd9b12770d2e4f64129e079c59d064244d20e3","_cell_guid":"18d49c54-5161-4be7-b450-b02c4d9072ac","trusted":false},"cell_type":"code","source":"# kernel params config\n\n# I set quick_run to True to run a little part of training datasets because of space and time limit, \n# you can run the whole datasets on you local machine.\nquick_run = False\n\nproject_tfidf_features = 30000\nresouse_tfidf_features = 1000\n\nmax_features = 80000\nembed_size = 300\n\npj_repeat = 3\nrs_repeat = 1\ndpcnn_folds = 5\nbatch_size = 64\nepochs = 5\nproject_maxlen = 240\nresouse_max_len = 30\nmaxlen = project_maxlen + resouse_max_len\n\nif quick_run == True:\n    max_features = 1000\n    epochs = 2\n    project_tfidf_features = 5000\n    resouse_tfidf_features = 1000\n    \nEMBEDDING_FILE = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":false},"cell_type":"code","source":"import os; os.environ['OMP_NUM_THREADS'] = '4'\nimport gc\nimport numpy as np\nimport pandas as pd\n\nfrom functools import reduce\nfrom functools import partial\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"d5caaa77c01fc6936f7080408d6c278859085a5a","_cell_guid":"ba33803d-4eaf-4268-9b9c-8587c4fe9dce","trusted":false},"cell_type":"code","source":"train_df = pd.read_csv('../input/donorschoose-application-screening/train.csv')\ntest_df = pd.read_csv('../input/donorschoose-application-screening/test.csv')\nresouse_df = pd.read_csv('../input/donorschoose-application-screening/resources.csv')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_kg_hide-input":false,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"%%time\nresouse_df['description'].fillna('', inplace=True)\nres_nums = pd.DataFrame(resouse_df[['id', 'price']].groupby('id').price.agg(['count', \n                                                                             'sum', \n                                                                             'min', \n                                                                             'max', \n                                                                             'mean',  \n                                                                             'std', \n                                                                             lambda x: len(np.unique(x)),])).reset_index()\nres_nums = res_nums.rename(columns={'count': 'res_count', \n                                    'sum': 'res_sum',\n                                    'min':  'res_min', \n                                    'max':  'res_max',\n                                    'mean': 'res_mean', \n                                    'std':  'res_std',\n                                    '<lambda>': 'res_unique' })\nres_descp = resouse_df[['id', 'description']].groupby('id').description.agg([ lambda x: ' '.join(x) ]).reset_index().rename(columns={'<lambda>':'res_description'})\nresouse_df = res_nums.merge(res_descp, on='id', how='left')\ntrain_df = train_df.merge(resouse_df, on='id', how='left')\ntest_df = test_df.merge(resouse_df, on='id', how='left')\ndel res_nums\ndel res_descp\ndel resouse_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"dbd09d3ed687ad83c6447de1b97661edca3d95ff","_cell_guid":"6f2c0a7e-72c5-4833-8f07-2bbb6c4997ce","trusted":false},"cell_type":"code","source":"if quick_run == True:\n    train_df = train_df[:10000]\n    test_df = test_df[:100]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"644988d39f268dfc80e9e2253e799969728494a3","_cell_guid":"6bdb19c5-1a11-422b-9f5d-9f01a479773b","trusted":false},"cell_type":"code","source":"train_target = train_df['project_is_approved'].values\ntrain_df = train_df.drop('project_is_approved', axis=1)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"dae3466a38682a7a11e911eae5d21653de149704","_cell_guid":"d97fe127-e590-4bfb-aafd-3fd71286f2af","trusted":false},"cell_type":"code","source":"train_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"71c06db13a94ed78eb5dd400a1b9883a559635d5","_cell_guid":"12999e41-0bc3-4762-b9f7-b340eb69418a","trusted":false},"cell_type":"code","source":"essay_cols = ['project_essay_1', 'project_essay_2','project_essay_3', 'project_essay_4']\nessay_length_cols = [item+'_len' for item in essay_cols]\n\ndef count_essay_length(df):\n    for col in essay_cols:\n        df[col] = df[col].fillna('')\n        df[col+'_len'] = df[col].apply(len)\n    return df\ntrain_df = count_essay_length(train_df)\ntest_df = count_essay_length(test_df)\n\ntrain_df['project_essay'] = ''\ntest_df['project_essay'] = ''\nfor col in essay_cols:\n    train_df['project_essay'] += train_df[col] + 'unknown'\n    test_df['project_essay'] += test_df[col] + 'unknown'\ntrain_df = train_df.drop(essay_cols, axis=1)\ntest_df = test_df.drop(essay_cols, axis=1)\ntrain_df[['project_essay']].head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"a6ed50988d7b3d36534ffe09593e949ed5604ece","_cell_guid":"6874ec1f-1377-48e0-aba8-7e7912d945ba","trusted":false},"cell_type":"code","source":"time_cols = ['sub_year', 'sub_month', 'sub_day', 'sub_hour', 'sub_minute', 'sub_dayofweek', 'sub_dayofyear']\ndef time_stamp_features(df):\n    time_df = pd.to_datetime(df['project_submitted_datetime'])\n    df['sub_year'] = time_df.apply(lambda x: x.year)\n    df['sub_month'] = time_df.apply(lambda x: x.month)\n    df['sub_day'] = time_df.apply(lambda x: x.day)\n    df['sub_hour'] = time_df.apply(lambda x: x.hour)\n    df['sub_minute'] = time_df.apply(lambda x: x.minute)\n    df['sub_dayofweek'] = time_df.apply(lambda x: x.dayofweek)\n    df['sub_dayofyear'] = time_df.apply(lambda x: x.dayofyear)\n    return df\ntrain_df = time_stamp_features(train_df)\ntest_df = time_stamp_features(test_df)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"3d198e580f340b9ced11c91bba0b004856e85b4c","_cell_guid":"b3cbcaaf-9357-45c3-ae24-884a257e3be1","trusted":false},"cell_type":"code","source":"str_cols = ['teacher_id', 'teacher_prefix', 'school_state',\n       'project_submitted_datetime', 'project_grade_category',\n       'project_subject_categories', 'project_subject_subcategories',\n       'project_title', 'project_resource_summary','res_description', 'project_essay']\nnum_cols = ['teacher_number_of_previously_posted_projects', \n            'res_count', 'res_sum', 'res_min', 'res_max', 'res_mean', 'res_std', 'res_unique'] + essay_length_cols + time_cols\ntrain_df[str_cols] =train_df[str_cols].fillna('unknown')\ntrain_df[num_cols] = train_df[num_cols].fillna(0)\ntest_df[str_cols] =test_df[str_cols].fillna('unknown')\ntest_df[num_cols] = test_df[num_cols].fillna(0)\nfor col in str_cols:\n    train_df[col] = train_df[col].str.lower()\n    test_df[col] = test_df[col].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"f014e44495ec06e16576a28e3a90ae8436ef3e01","_cell_guid":"d40ea164-a70a-4d6d-b3f7-82ee0f28d9f8","trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nstd_scaler = MinMaxScaler()\ntrain_none_text_features = std_scaler.fit_transform(train_df[num_cols].values)\ntest_none_text_features = std_scaler.transform(test_df[num_cols].values)\n\ntrain_df = train_df.drop(num_cols, axis=1)\ntest_df = test_df.drop(num_cols, axis=1)\ndel std_scaler\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"c0b7917d7b38aef50ae985de79a1c3a82eaf50ce","_cell_guid":"2e1d5210-6614-446d-bc0e-39d33c1cef60","trusted":false},"cell_type":"code","source":"train_df['project_descp'] = train_df['project_subject_categories'] + ' ' + train_df['project_subject_subcategories'] + ' ' + train_df['project_title'] + ' ' + train_df['project_resource_summary'] + ' ' + train_df['project_essay']\ntest_df['project_descp'] = test_df['project_subject_categories'] + ' ' + test_df['project_subject_subcategories'] + ' ' + test_df['project_title'] + ' ' + test_df['project_resource_summary'] + ' ' + test_df['project_essay']\ntrain_df = train_df.drop(['project_title', 'project_resource_summary', 'project_essay'], axis=1)\ntest_df = test_df.drop(['project_title', 'project_resource_summary', 'project_essay'], axis=1)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"64653cb8f5d731039fac2777bae0decc4760fa6d","_cell_guid":"65f79999-3f92-4e47-8753-9b398ccdca89","scrolled":true,"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_cols = [\n    # 'teacher_id',\n    'teacher_prefix', \n    'school_state', \n    'project_grade_category', \n    'project_subject_categories', \n    'project_subject_subcategories'\n]\n\nfor col in label_cols:\n    le = LabelEncoder()\n    le.fit(np.hstack([train_df[col].values, test_df[col].values]))\n    train_df[col] = le.transform(train_df[col])\n    test_df[col] = le.transform(test_df[col])\ntrain_label_features = train_df[label_cols].values\ntest_label_features = test_df[label_cols].values\n\ntrain_df = train_df.drop(label_cols, axis=1)\ntest_df = test_df.drop(label_cols, axis=1)\ndel le\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"bccb3a91c63337abb774b7730bcf4a68d05ba456","_cell_guid":"d2642662-a1ad-45cc-a1b0-c2b89e050850","trusted":false},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"20c1f1d90ef68f2339c88ae0613de452401b4eb4","_cell_guid":"48d21f37-f24d-4736-8f24-229fc986a741","trusted":false},"cell_type":"code","source":"%%time\nimport re\nfrom nltk.corpus import stopwords\nstopwords = stopwords.words('english')\n\ndef clean_descp(descp):\n    low_case = re.compile('([a-z]*)')\n    words = low_case.findall(descp)\n    words = [item for item in filter(lambda x: x not in stopwords, words)]\n    return ' '.join(words)\n\ntrain_df['project_descp']  = train_df['project_descp'].apply(clean_descp)\ntest_df['project_descp']  = test_df['project_descp'].apply(clean_descp)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"b2120c989d1c4ce65b8932a656d4b3a8f462088e","_cell_guid":"f23244ee-55eb-40de-b95b-df936bc8feef","trusted":false},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nproject_tfidf = TfidfVectorizer(max_features=project_tfidf_features, token_pattern='\\w+', ngram_range=(1, 2))\ntrain_pj_tfidf_features = project_tfidf.fit_transform(train_df['project_descp'])\ntest_pj_tfidf_features = project_tfidf.transform(test_df['project_descp'])\ndel project_tfidf\ngc.collect()\n\nresouse_tfidf= CountVectorizer(max_features=resouse_tfidf_features, token_pattern='\\w+', ngram_range=(1, 2))\ntrain_rs_tfidf_features = resouse_tfidf.fit_transform(train_df['res_description'])\ntest_rs_tfidf_features = resouse_tfidf.transform(test_df['res_description'])\ndel resouse_tfidf\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"988b194d6526b4910f532b49bc4124a473642df7","_cell_guid":"b0a57dba-a0f3-4429-9fca-394f7769831a","trusted":false},"cell_type":"code","source":"from scipy.sparse import csr_matrix, hstack\n\ntrain_features = hstack([train_none_text_features, train_label_features, train_pj_tfidf_features, train_rs_tfidf_features]).tocsr()\ntest_features = hstack([test_none_text_features, test_label_features, test_pj_tfidf_features, test_rs_tfidf_features]).tocsr()\n\ndel train_pj_tfidf_features\ndel test_pj_tfidf_features\ndel train_rs_tfidf_features\ndel test_rs_tfidf_features\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c9a254a3a2457556644b3de3ad5480dd0ecd451","_cell_guid":"23259f07-5a65-4cb7-a739-559d0011ee70"},"cell_type":"markdown","source":"It's convenient to do stacking on any model with the class below.\n\nI named it qiaofeng to pay tribute to my big bother, sun e phone."},{"metadata":{"collapsed":true,"_uuid":"9419f8d208258a986665e8959268271347e0fdd0","_cell_guid":"8e142ad5-a877-463f-b8a3-5b3e1987a87f","trusted":false},"cell_type":"code","source":"from functools import reduce\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nclass qiaofeng_kfold_stack:\n    def __init__(self, train, train_target, test, split_target, model, preprocess_func=None, score_func=None, kfolds=5, random_seed=9527, logger=None):\n        self.train = train\n        self.train_target = train_target\n        self.test = test\n        self.split_target = split_target\n        self.model = model\n        self.preprocess_func = preprocess_func\n        self.score_func = score_func\n        self.kfolds = kfolds\n        self.random_seed = random_seed\n        self.logger= logger\n        self.skf = KFold(n_splits=self.kfolds, random_state= self.random_seed)\n        self.predict_test_kfolds = []\n        self.predict_valid_kfolds = np.zeros((self.train.shape[0]))\n    def print_params(self):\n        print('kfolds : %s' % self.kfolds)\n        print('random seed : %s' % self.random_seed)\n    def preprocess(self):\n        if self.preprocess_func != None:\n            self.train, self.test = self.preprocess_func(self.train, self.test)\n    def score(self, target, predict):\n        return self.score_func(target, predict)\n    def model_fit(self, train, train_target):\n        self.model.fit(train, train_target)\n    def model_predict(self, dataset):\n        return self.model.predict(dataset)\n    def model_fit_predict(self, train, train_target, dataset):\n        self.model_fit(train, train_target)\n        predict_train = None#self.model_predict(train)\n        predict_valid = self.model_predict(dataset)\n        predict_test = self.model_predict(self.test)\n        return predict_train, predict_valid, predict_test\n    def clear_predicts(self):\n        self.predict_test_kfolds = []\n        self.predict_valid_kfolds = np.zeros((self.train.shape[0]))\n    def model_train_with_kfold(self):\n        self.clear_predicts()\n        for (folder_index, (train_index, valid_index)) in enumerate(self.skf.split(self.train)):\n            x_train, x_valid = self.train[train_index], self.train[valid_index]\n            y_train, y_valid = self.train_target[train_index], self.train_target[valid_index]\n            predict_train, predict_valid, predict_test = self.model_fit_predict(x_train, y_train, x_valid)\n            self.predict_test_kfolds.append(predict_test)\n            self.predict_valid_kfolds[valid_index] = predict_valid\n            if self.logger != None:\n                valid_score = self.score(y_valid, predict_valid)\n                # train_score = self.score(y_train, predict_train)\n                self.logger('Fold: %s, valid score: %s' % (folder_index, valid_score))\n    def predict_test_mean(self):\n        return reduce(lambda x,y:x+y, self.predict_test_kfolds)  / self.kfolds","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"4b964c4a53eb45c48e03ab1c4ac2613e89f24f89","_cell_guid":"b962c1b7-4762-4280-887c-51c2e2fd15c7","trusted":false},"cell_type":"code","source":"class qiaofeng_predict_prob(qiaofeng_kfold_stack):\n    def model_predict(self, dataset):\n        return self.model.predict_proba(dataset)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"ce17905f2bf339e1efee89d5260e1bd8dc3ce0a3","_cell_guid":"81014f88-b89d-4f43-87d6-ebe24ebe7ef6","trusted":false},"cell_type":"code","source":"%%time\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier( n_jobs=4, \n                                criterion=\"entropy\",\n                                max_depth=30, \n                                n_estimators=400, \n                                max_features='sqrt', \n                                random_state=233,\n                                min_samples_leaf = 50\n                                )\nqiaofeng_rf = qiaofeng_predict_prob(train=train_features, train_target=train_target, test=test_features, kfolds=5,split_target=train_target,\n                                          score_func=roc_auc_score, logger=print, model=model)\nqiaofeng_rf.model_train_with_kfold()\npred_valid_rf = qiaofeng_rf.predict_valid_kfolds\npred_test_avg_rf = qiaofeng_rf.predict_test_mean()\ndel qiaofeng_rf\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"4b2d60511b0f2ce213a0b5d3c3ae4016d3bc1a70","_cell_guid":"430735a3-594c-4438-9fb3-b9e542769105","scrolled":true,"trusted":false},"cell_type":"code","source":"%%time\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\nclass qiaofeng_lgb_reg(qiaofeng_kfold_stack):\n    def model_fit_predict(self, train, train_target, dataset):\n        params = {\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc',\n        'max_depth': 5,\n        'num_leaves': 31,\n        'learning_rate': 0.025,\n        'feature_fraction': 0.85,\n        'bagging_fraction': 0.85,\n        'bagging_freq': 5,\n        'verbose': 0,\n        'num_threads': 4,\n        'lambda_l2': 1,\n        'min_gain_to_split': 0,\n        }  \n\n        X_tra, X_val, y_tra, y_val = train_test_split(train, train_target, train_size=0.95, random_state=233)\n        model = lgb.train(\n            params,\n            lgb.Dataset(X_tra, y_tra),\n            num_boost_round=500,\n            valid_sets=[lgb.Dataset(X_val, y_val)],\n            early_stopping_rounds=50,\n            verbose_eval=100,\n        )\n        return None, model.predict(dataset), model.predict(self.test)\n        \nqf_lgb = qiaofeng_lgb_reg(train=train_features, train_target=train_target, test=test_features, kfolds=5,split_target=train_target,\n                                          score_func=roc_auc_score, logger=print, model=None)\nqf_lgb.model_train_with_kfold()\npred_valid_qflgb = qf_lgb.predict_valid_kfolds\npred_test_avg_qflgb = qf_lgb.predict_test_mean()\ndel qf_lgb\ngc.collect() ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"59dc83e7ab84a95510c4cc94105c2c9723990a93","_cell_guid":"b7122c1d-0b66-494c-b61e-068ad1b59daa","scrolled":true,"trusted":false},"cell_type":"code","source":"%%time\nimport lightgbm as lgb\nmodel = lgb.LGBMClassifier(  n_jobs=4,\n                             max_depth=4,\n                             metric=\"auc\",\n                             n_estimators=400,\n                             num_leaves=15,\n                             boosting_type=\"gbdt\",\n                             learning_rate=0.1,\n                             feature_fraction=0.45,\n                             colsample_bytree=0.45,\n                             bagging_fraction=0.8,\n                             bagging_freq=5,\n                             reg_lambda=0.2)\nqiaofeng_lgb = qiaofeng_predict_prob(train=train_features, train_target=train_target, test=test_features, kfolds=5,split_target=train_target,\n                                          score_func=roc_auc_score, logger=print, model=model)\nqiaofeng_lgb.model_train_with_kfold()\npred_valid_lgb = qiaofeng_lgb.predict_valid_kfolds\npred_test_avg_lgb = qiaofeng_lgb.predict_test_mean()\ndel qiaofeng_lgb\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"0184c46f5315906577e9b68d14437e773de7e633","_cell_guid":"ba45286a-f860-4043-8d41-41d86552e302","scrolled":true,"trusted":false},"cell_type":"code","source":"%%time\nfrom wordbatch.models import FTRL, FM_FTRL\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\npreds = []\nclass qiaofeng_ftrl(qiaofeng_kfold_stack):\n    def model_predict(self, dataset):\n        predict = self.model.predict(dataset)\n        pred_nan = np.isnan(predict)\n        if pred_nan.shape[0] == predict.shape[0]:\n            predict[pred_nan] = 0\n        else:\n            predict[pred_nan] = np.nanmean(predict)\n        preds.append(predict)\n        return sigmoid(predict)\n        \nmodel = FTRL(alpha=0.01, beta=0.1, L1=0.001, L2=1.0, D=train_features.shape[1], iters=10, \n                 inv_link=\"identity\", threads=4)\nqiaofeng_ftrl = qiaofeng_ftrl(train=train_features, train_target=train_target, test=test_features, kfolds=5,split_target=train_target,\n                                          score_func=roc_auc_score, logger=print, model=model)\nqiaofeng_ftrl.model_train_with_kfold()\npred_valid_ftrl = qiaofeng_ftrl.predict_valid_kfolds\npred_test_avg_ftrl = qiaofeng_ftrl.predict_test_mean()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"b70c6797132126e8f03a3014b8ab6497885b9820","_cell_guid":"acc47529-bf13-4dd9-8d65-a0615c841188","trusted":false},"cell_type":"code","source":"del train_features\ndel test_features\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"d74649f7e3f4c164abcc4b9a789536b78737a2d5","_cell_guid":"05c3a488-9d55-4667-be44-1a8e643b72e4","trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.utils import shuffle\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\nfrom keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, LSTM, Dropout, BatchNormalization\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_df['project_descp']) + list(test_df['project_descp']))\ntrain_pj = sequence.pad_sequences(tokenizer.texts_to_sequences(train_df['project_descp']), maxlen=project_maxlen)\ntest_pj = sequence.pad_sequences(tokenizer.texts_to_sequences(test_df['project_descp']), maxlen=project_maxlen)\n\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_df['res_description']) + list(test_df['res_description']))\ntrain_res = sequence.pad_sequences(tokenizer.texts_to_sequences(train_df['res_description']), maxlen=resouse_max_len)\ntest_res = sequence.pad_sequences(tokenizer.texts_to_sequences(test_df['res_description']), maxlen=resouse_max_len)\n\ntrain_seq = np.hstack([train_pj, train_res])\ntest_seq = np.hstack([test_pj, test_res])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"336cae178bdfc03747cbaf4fa1cf061a6d857663","_cell_guid":"003e827a-3044-4ba1-9b34-bd9ec5680509","trusted":false},"cell_type":"code","source":"def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\n\nclass RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            logs['roc_auc_val'] = score\n            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"11bcef2d935111d6ab90a6473650ad65f8a0dfb7","_cell_guid":"0f760020-e05b-4a64-8a12-62e73c5e8dd5","scrolled":true,"trusted":false},"cell_type":"code","source":"train_num_features = np.hstack([train_none_text_features, train_label_features])\ntest_num_features = np.hstack([test_none_text_features, test_label_features])\n\ndel train_none_text_features\ndel train_label_features\ndel test_none_text_features\ndel test_label_features\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb37c1447f397a1a37d6ac3f80c499388e062b03","_cell_guid":"9889e479-4247-4203-84ff-4bc4f8493d1f"},"cell_type":"markdown","source":"# NN Networks\nI learned these nn models from  [https://github.com/neptune-ml/kaggle-toxic-starter](http://), thanks to @Jakub Czakon\n\nFor details, you may refer to:\n\n[http://ai.tencent.com/ailab/media/publications/ACL3-Brady.pdf](http://)\n\n[http://www.aclweb.org/anthology/E17-1104](http://)"},{"metadata":{"collapsed":true,"_uuid":"e73e999bf6e447e087be85d61c51f87b1bbb2df0","_cell_guid":"d651767b-0364-4387-98f7-3520e3ad644e","trusted":false},"cell_type":"code","source":"train_seq = np.hstack([train_seq, train_num_features])\ntest_seq = np.hstack([test_seq, test_num_features])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"1e4d994acf87282cf386163d99bf9145972fbb3d","_cell_guid":"59432f01-3e5e-4705-aca9-8f9adffa834f","trusted":false},"cell_type":"code","source":"gc.collect()\ngc.disable()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"18e543ab8fa832b0ef6f9ad8e2255e8b32e814fd","_cell_guid":"13545cde-eb77-43eb-a9cf-78c9319140e8","trusted":false},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\nfrom keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, LSTM, Dropout, BatchNormalization,Conv1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\nfrom keras.layers import Input, Dense, Embedding, MaxPooling1D, Conv1D, SpatialDropout1D\nfrom keras.layers import add, Dropout, PReLU, BatchNormalization, GlobalMaxPooling1D\n\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras import optimizers\nfrom keras import initializers, regularizers, constraints, callbacks\n\nif 1:\n    def get_model():\n        session_conf = tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4)\n        K.set_session(tf.Session(graph=tf.get_default_graph(), config=session_conf))\n\n        filter_nr = 32\n        filter_size = 3\n        max_pool_size = 3\n        max_pool_strides = 2\n        dense_nr = 64\n        spatial_dropout = 0.2\n        dense_dropout = 0.05\n        train_embed = False\n        \n        project = Input(shape=(project_maxlen,), name='project')\n        emb_project = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=train_embed)(project)\n        emb_project = SpatialDropout1D(spatial_dropout)(emb_project)\n        \n        pj_block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(emb_project)\n        pj_block1 = BatchNormalization()(pj_block1)\n        pj_block1 = PReLU()(pj_block1)\n        pj_block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(pj_block1)\n        pj_block1 = BatchNormalization()(pj_block1)\n        pj_block1 = PReLU()(pj_block1)\n        \n        #we pass embedded comment through conv1d with filter size 1 because it needs to have the same shape as block output\n        #if you choose filter_nr = embed_size (300 in this case) you don't have to do this part and can add emb_comment directly to block1_output\n        pj_resize_emb = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear')(emb_project)\n        pj_resize_emb = PReLU()(pj_resize_emb)\n            \n        pj_block1_output = add([pj_block1, pj_resize_emb])\n        # pj_block1_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(pj_block1_output)\n        for _ in range(pj_repeat):  \n            pj_block1_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(pj_block1_output)\n            pj_block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(pj_block1_output)\n            pj_block2 = BatchNormalization()(pj_block2)\n            pj_block2 = PReLU()(pj_block2)\n            pj_block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(pj_block2)\n            pj_block2 = BatchNormalization()(pj_block2)\n            pj_block2 = PReLU()(pj_block2)\n            pj_block1_output = add([pj_block2, pj_block1_output])\n        \n        resouse = Input(shape=(resouse_max_len,), name='resouse')\n        emb_resouse = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=train_embed)(resouse)\n        emb_resouse = SpatialDropout1D(spatial_dropout)(emb_resouse)\n        \n        rs_block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(emb_resouse)\n        rs_block1 = BatchNormalization()(rs_block1)\n        rs_block1 = PReLU()(rs_block1)\n        rs_block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(rs_block1)\n        rs_block1 = BatchNormalization()(rs_block1)\n        rs_block1 = PReLU()(rs_block1)\n\n        #we pass embedded comment through conv1d with filter size 1 because it needs to have the same shape as block output\n        #if you choose filter_nr = embed_size (300 in this case) you don't have to do this part and can add emb_comment directly to block1_output\n        rs_resize_emb = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear')(emb_resouse)\n        rs_resize_emb = PReLU()(rs_resize_emb)\n\n        rs_block1_output = add([rs_block1, rs_resize_emb])\n        # rs_block1_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(rs_block1_output)\n        for _ in range(rs_repeat):  \n            rs_block1_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(rs_block1_output)\n            rs_block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(rs_block1_output)\n            rs_block2 = BatchNormalization()(rs_block2)\n            rs_block2 = PReLU()(rs_block2)\n            rs_block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(rs_block2)\n            rs_block2 = BatchNormalization()(rs_block2)\n            rs_block2 = PReLU()(rs_block2)\n            rs_block1_output = add([rs_block2, rs_block1_output])\n            \n\n        pj_output = GlobalMaxPooling1D()(pj_block1_output)\n        pj_output = BatchNormalization()(pj_output)\n        rs_output = GlobalMaxPooling1D()(rs_block1_output)\n        rs_output = BatchNormalization()(rs_output)\n        inp_num = Input(shape=(train_seq.shape[1]-maxlen, ), name='num_input')\n        bn_inp_num = BatchNormalization()(inp_num)\n        conc = concatenate([pj_output, rs_output, bn_inp_num])\n        \n        output = Dense(dense_nr, activation='linear')(conc)\n        output = BatchNormalization()(output)\n        output = PReLU()(output)\n        output = Dropout(dense_dropout)(output)\n        output = Dense(1, activation='sigmoid')(output)\n        model = Model(inputs=[project, resouse, inp_num], outputs=output)\n        model.compile(loss='binary_crossentropy', \n                    optimizer='nadam',\n                    metrics=['accuracy'])\n\n        return model","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"e1c05cf45ddebb3a1e69c45fa2ab22df06311761","_cell_guid":"25401240-d69c-401d-a073-1583cc01724e","scrolled":true,"trusted":false},"cell_type":"code","source":"class qiaofeng_dpcnn(qiaofeng_kfold_stack):\n    def model_fit_predict(self, train, train_target, valid):\n        self.model = get_model()\n        early_stopping = EarlyStopping(monitor='roc_auc_val', patience=1, mode='max',min_delta=0.0005)  \n        X_tra, X_val, y_tra, y_val = train_test_split(train, train_target, train_size=0.98, random_state=233)\n        X_tra = { 'project' : X_tra[:,:project_maxlen], 'resouse' : X_tra[:,project_maxlen:project_maxlen+resouse_max_len], 'num_input' : X_tra[:,maxlen:]  }\n        X_val = { 'project' : X_val[:,:project_maxlen], 'resouse' : X_val[:,project_maxlen:project_maxlen+resouse_max_len], 'num_input' : X_val[:,maxlen:]  }\n        x_test = { 'project' : self.test[:,:project_maxlen], 'resouse' : self.test[:,project_maxlen:project_maxlen+resouse_max_len], 'num_input' : self.test[:,maxlen:]  }\n        valid = { 'project' : valid[:,:project_maxlen], 'resouse' : valid[:,project_maxlen:project_maxlen+resouse_max_len], 'num_input' : valid[:,maxlen:]  }\n        \n        RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n        hist = self.model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n                             callbacks=[RocAuc, early_stopping], verbose=2)\n        predict_train = None#self.model.predict(X_tra, batch_size=1024)[:, 0]\n        predict_valid = self.model.predict(valid, batch_size=1024)[:, 0]\n        predict_test = self.model.predict(x_test, batch_size=1024)[:, 0]\n        return predict_train, predict_valid, predict_test            \n\ndpcnn_kfold_model = qiaofeng_dpcnn(train=train_seq, train_target=train_target, test=test_seq, kfolds=dpcnn_folds, split_target=train_target,\n                                          score_func=roc_auc_score, logger=print, model=None)\ndpcnn_kfold_model.model_train_with_kfold()\npred_valid_dpcnn = dpcnn_kfold_model.predict_valid_kfolds\npred_test_avg_dpcnn = dpcnn_kfold_model.predict_test_mean()\n    \ndel dpcnn_kfold_model\ngc.enable()\ngc.collect()\ngc.disable()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"6e4b18aad13e092aeccfa14d025af34e1d678c21","_cell_guid":"d7e8a236-ef77-4294-8f2d-2d768132a86f","trusted":false},"cell_type":"code","source":"del embedding_matrix\ngc.enable()\ngc.collect()\ngc.disable()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"f5a81f2d31866444b90ddba473d46b8662d38d91","_cell_guid":"d50918b9-d038-470b-9b26-f93f2c1ff66e","scrolled":true,"trusted":false},"cell_type":"code","source":"#predict_valid_list = [pred_valid_rf, pred_valid_lr, pred_valid_lgb, pred_valid_dpcnn, pred_valid_scnn, pred_valid_et, pred_valid_ftrl]\n#predict_test_list = [pred_test_avg_rf, pred_test_avg_lr, pred_test_avg_lgb, pred_test_avg_dpcnn, pred_test_avg_scnn, pred_test_avg_et, pred_test_avg_ftrl]\npredict_valid_list = [pred_valid_rf, pred_valid_lgb, pred_valid_dpcnn, pred_valid_ftrl, pred_valid_qflgb]\npredict_test_list = [pred_test_avg_rf, pred_test_avg_lgb, pred_test_avg_dpcnn, pred_test_avg_ftrl, pred_test_avg_qflgb]\n\nvalid_results = np.hstack([item.reshape((item.shape[0], 1)) for item in predict_valid_list])\ntest_results = np.hstack([item.reshape((item.shape[0], 1)) for item in predict_test_list])\ntrain_features = np.hstack([valid_results, train_num_features])\ntest_features = np.hstack([test_results, test_num_features])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"9fde920dc191798a90672af995a4961a39bc95c0","_cell_guid":"fa534578-c814-49a0-b400-f1288e7a6780","trusted":false},"cell_type":"code","source":"lgb_model = lgb.LGBMClassifier(  n_jobs=4,\n                                 max_depth=4,\n                                 metric=\"auc\",\n                                 n_estimators=400,\n                                 num_leaves=10,\n                                 boosting_type=\"gbdt\",\n                                 learning_rate=0.1,\n                                 feature_fraction=0.45,\n                                 colsample_bytree=0.45,\n                                 bagging_fraction=0.8,\n                                 bagging_freq=5,\n                                 reg_lambda=0.2)\nX_tra, X_val, y_tra, y_val = train_test_split(train_features, train_target, train_size=0.8, random_state=233)\nlgb_model.fit(X=X_tra, y=y_tra,\n              eval_set=[(X_val, y_val)],\n              verbose=False)\nprint('Valid Score is %.4f' % roc_auc_score(y_val, lgb_model.predict_proba(X_val)[:,1]))\nfinal_predict = lgb_model.predict_proba(test_features)[:,1]\n\nif quick_run == False:\n    sample_df = pd.read_csv('../input/donorschoose-application-screening/sample_submission.csv')\n    sample_df['project_is_approved'] = final_predict\n    sample_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"c871f0df9f9259cec4dfef88a08c14eb2547aec4","_cell_guid":"48eabd09-354f-42e5-8569-53075c75e6b5","trusted":false},"cell_type":"code","source":"predict_valid_list = [pred_valid_rf, pred_valid_lgb, pred_valid_dpcnn, pred_valid_qflgb]\npredict_test_list = [pred_test_avg_rf, pred_test_avg_lgb, pred_test_avg_dpcnn, pred_test_avg_qflgb]\n\nvalid_results = np.hstack([item.reshape((item.shape[0], 1)) for item in predict_valid_list])\ntest_results = np.hstack([item.reshape((item.shape[0], 1)) for item in predict_test_list])\ntrain_features = np.hstack([valid_results, train_num_features])\ntest_features = np.hstack([test_results, test_num_features])\n\nlgb_model = lgb.LGBMClassifier(  n_jobs=4,\n                                 max_depth=4,\n                                 metric=\"auc\",\n                                 n_estimators=400,\n                                 num_leaves=10,\n                                 boosting_type=\"gbdt\",\n                                 learning_rate=0.1,\n                                 feature_fraction=0.45,\n                                 colsample_bytree=0.45,\n                                 bagging_fraction=0.8,\n                                 bagging_freq=5,\n                                 reg_lambda=0.2)\nX_tra, X_val, y_tra, y_val = train_test_split(train_features, train_target, train_size=0.8, random_state=233)\nlgb_model.fit(X=X_tra, y=y_tra,\n              eval_set=[(X_val, y_val)],\n              verbose=False)\nprint('Valid Score is %.4f' % roc_auc_score(y_val, lgb_model.predict_proba(X_val)[:,1]))\nfinal_predict = lgb_model.predict_proba(test_features)[:,1]\n\nif quick_run == False:\n    sample_df = pd.read_csv('../input/donorschoose-application-screening/sample_submission.csv')\n    sample_df['project_is_approved'] = final_predict\n    sample_df.to_csv('submission_without_ftrl.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"949e32652b401ef695bd3b77000ed1e5ff372802","_cell_guid":"eb233238-b4d5-4299-bcbb-7fe536fe4efc"},"cell_type":"markdown","source":"# TODO LIST:\n\n1.  Text propressing, move stop words, word stem.\n2. Make use of submit datetime.\n3.  Meta features of texts,  text length, word length and so on.\n4. More nn networks, such as BiRNN, RCNN, which are widely used in Toxic Comment Classification Challenge.\n5. Try MLP and bool features, like [https://www.kaggle.com/lopuhin/mercari-golf-0-3875-cv-in-75-loc-1900-s](http://)"}],"metadata":{"language_info":{"pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python","name":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","version":"3.6.4"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}