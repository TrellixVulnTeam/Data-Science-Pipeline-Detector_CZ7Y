{"cells":[{"metadata":{"collapsed":true,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"# Understanding Approval:- Donor Choose EDA\n![](https://cdn.donorschoose.net/images/media-logo-tagline-reversed@2x.jpg)\n\n# Contents:\n* Introduction\n    * About Donors Choose\n    * Competition Objective\n    * Kernel objective\n* Imports and overview\n* Custom Helper Functions\n    * Plotting Functions\n    * Text functions\n        * Extract text stats\n        * Make Wordclouds\n* Individual Feature impact on Approval rates\n    * Categorical features - Teacher-prefix, Gender, Grade/class\n    * Cleaning up - Subject category and Subject sub-category\n* Text columns exploration\n    * Title\n    * Student description\n    * Project description\n    * Resource summary\n* Resources dataset\n* Price points\n    * Exploring some costly items\n* Pre-processing and cleaning text\n* Feature Engineering\n    * Label encoding\n    * Create date features\n    * Custom Vectorizer for ELI5 compatability\n* Baseline Models -- XGBoost and LightGBM\n    * ROC curve and \n* Understanding how the model predicts - ELI5\n    * Explore correct classifications\n    * Explore mis-classifications\n\n# 1. Introduction:\n## 1.1 About Donors Choose:\n[Donorschoose.org](https://www.donorschoose.org/about) is a crowdfunding platform which connects Public school teachers and Donors. \n![](http://stuffonix.com/wp-content/uploads/2017/09/donorschoose-how-it-work.jpg)\n\n\nAs per their [website](https://www.donorschoose.org/about/impact.html), they have raised $645,575,280 till date and claim that 77 percent of all the public schools in America have at least one teacher who has posted a project on DonorsChoose.org. Amazing!\n\nWith such high numbers, the number of applications they receive is increasing every year and the current screening process is manually vetting the applications by a team of volunteers. As a result, there are three main problems they need to solve:\n\n* How to scale current manual processes and resources to screen 500,000 projects so that they can be posted as quickly and as efficiently as possible\n* How to increase the consistency of project vetting across different volunteers to improve the experience for teachers\n* How to focus volunteer time on the applications that need the most assistance\n\n## 1.2 Competition Objective:\n\nThe goal of the competition is to predict whether or not a DonorsChoose.org project proposal submitted by a teacher will be approved, using the text of project descriptions as well as additional metadata about the project, teacher, and school. DonorsChoose.org can then use this information to identify projects most likely to need further review before approval.\n\n## 1.3 Kernel objective:\n\nTo explore and understand factors that make a successful project and hopefully create an approval process pipeline/algorithm to help Donorschoose.org with the vetting process of approving a project.\n\n# 2. Imports and overview:\n\nLets get started by importing all the required packages and performing basic sanity checks like the test-train split ratio, Missing value checks,etc."},{"metadata":{"collapsed":true,"_cell_guid":"8eb0d250-9b77-42e0-b7b1-28b2cd49dc2d","_uuid":"4a935348420caa39131b6eef5f1111a8c3fd81bb","trusted":false},"cell_type":"code","source":"#peak\n!ls -l ../input/*","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"#import required packages\n#basics\nimport pandas as pd \nimport numpy as np\n\n#viz\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec \nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\n\n#nlp\nimport re    #for regex\nimport nltk\nfrom nltk.corpus import stopwords\nimport gensim\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nlem = WordNetLemmatizer()\neng_stopwords = set(stopwords.words(\"english\"))\n\n\n#misc\nimport gc\nimport time\nimport warnings\n\n#settings\nstart_time=time.time()\ncolor = sns.color_palette()\nsns.set_style(\"dark\")\nwarnings.filterwarnings(\"ignore\")\n\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"25b024b8-3f7e-4bb4-b09a-9422ae7f20da","_uuid":"1dd2e29fc5edaed70d331c6e736ce26a8bfb3820","trusted":false},"cell_type":"code","source":"#import all the files!\ntrain=pd.read_csv(\"../input/train.csv\")\nresources=pd.read_csv(\"../input/resources.csv\")\ntest=pd.read_csv(\"../input/test.csv\")\nsample_sub=pd.read_csv(\"../input/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"cf239116-319f-41ff-b7ec-44a759be9221","_uuid":"12138214a709a1c067290f89e02805f5d43470b0","trusted":false},"cell_type":"code","source":"# peak at the data\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e508d53d-70e0-4377-91b9-fee38e284b16","_uuid":"2819e28785a3dab1f6ab2acee589f71eb065ab65","trusted":false},"cell_type":"code","source":"#take a peak\nresources.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"253f4654-0fd6-4608-9abd-26ab082b1836","_uuid":"73389ec993176e4cc170c38f62c467ac070bc696"},"cell_type":"markdown","source":"### Check the test-train split ratio:"},{"metadata":{"collapsed":true,"_kg_hide-input":true,"_cell_guid":"c1399e25-354f-4117-a562-ed1d853358f0","_uuid":"8a06997ee2e4a651f9b80080320a5586fd7312cf","trusted":false},"cell_type":"code","source":"#check test train split\nnrow_train=train.shape[0]\nnrow_test=test.shape[0]\nsum=nrow_train+nrow_test\nprint(\"Checking proportion of Test-train split\")\nprint(\"       : train  : test\")\nprint(\"rows   :\",nrow_train,\":\",nrow_test)\nprint(\"perc   :\",round(nrow_train*100/sum),\"    :\",round(nrow_test*100/sum))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_kg_hide-input":true,"_cell_guid":"10b6f6f5-d47b-4d6d-b0fb-edbf2911d7d5","_uuid":"64de7052847f16e0e61b9fd973a19c28b0eb327a","trusted":false},"cell_type":"code","source":"# check for missing values\nprint(\"Check for Percent of missing values in Train dataset\")\nnull_check=train.isnull().sum()\n(null_check/len(train))*100","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_kg_hide-input":true,"_cell_guid":"38fbe8bf-40f2-450a-b294-a77f0c7fd9f2","_uuid":"192f2d736cbbc08c8505e14f90144a6ca221448d","trusted":false},"cell_type":"code","source":"# check for missing values\nprint(\"Check for Percent of missing values in RESOURCES file\")\nnull_check=resources.isnull().sum()\n(null_check/len(resources))*100","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b0056363-bb57-476e-b29a-97b459919427","_uuid":"c6639a9001ee0bfcb925837c9e28ab01bbdc92e3"},"cell_type":"markdown","source":"### Target Variable:\nThe target variable for this competition is a Binary variable which indicates if the project was **approved to be hosted on the site** or not.\n\nNote that this does not indicate if the project was **funded** or not! "},{"metadata":{"collapsed":true,"_kg_hide-input":true,"_cell_guid":"8fa83b8b-3a63-404b-9f99-72b1fbdc6c63","_uuid":"8b34694655f0daa013923eeb3a364c9dd6f99d0c","trusted":false},"cell_type":"code","source":"x=train.project_is_approved.value_counts()\n#plot\nplt.figure(figsize=(8,4))\nax= sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"Target Variable\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('Project is approved?', fontsize=12)\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()\nprint(\"Approval rate:\",x[1]/(x[0]+x[1])*100)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"7306a6ae-b736-4d83-9023-84aebc6dcd3e","_uuid":"416fdec3b835e4a8c01b8f546ca17e7337b5be69"},"cell_type":"markdown","source":"\nDuring the training period, there is an impressive **84%** approval rate! \n\nThere are some null values in some fields, Project essays 3,4 and Resource description.\n\nAs per the data description, the Project essays 3,4 are just optional descriptive fields that the teachers can enter. Hence, it is ok that we observe around 96.5% empty values.\n\nBut the null entries in the resources dataset is fishy. Let's explore more on that."},{"metadata":{"collapsed":true,"_cell_guid":"bcd92d24-b03c-4980-8790-2c232ec07dd3","_uuid":"89497b6439a983f2d53d41d6a25c356688ce788b","trusted":false},"cell_type":"code","source":"# take IDs of the projects which have null description\nprint(\"There are\",resources.description.isnull().sum(),\"NULL entries in description column of Resources dataset\")\nnull_ids=resources[resources.description.isnull()].id\nprint(\"Those Null entries are from\",len(null_ids.unique()),\"projects\")\nnull_entries_train=train[train.id.isin(null_ids)]\nprint(\"There are\",len(null_entries_train),\"of these projects are in train\")\nnull_entries_test=test[test.id.isin(null_ids)]\nprint(\"There are\",len(null_entries_test),\"of these projects are in test\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c9102f30-93e4-4f9d-9279-a2e44300a12f","_uuid":"d2cadc5e3f29a1fe87767816fcf95d1c102b9aab","trusted":false},"cell_type":"code","source":"x=null_entries_train.project_is_approved.value_counts()\nprint(\"Approval rate of projects with NULL as the description under Resources:\",x[1]/(x[0]+x[1])*100)\nx=null_entries_train[null_entries_train.teacher_number_of_previously_posted_projects==0].project_is_approved.value_counts()\nprint(\"Approval rate of projects with NULL as the description under Resources and 0 previous project submissions:\",x[1]/(x[0]+x[1])*100)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9fd02a3e-a20a-4995-b6ce-79cd91312643","_uuid":"c656dbac686a116a28cd253d14897dc66bf7779e"},"cell_type":"markdown","source":"There is a significant dip in approval rates (from 85% to 62%) if there is no description of the resources and if the project is the first submission by a teacher."},{"metadata":{"collapsed":true,"_cell_guid":"9fac0fe9-731c-43ff-be39-66e041e1233a","_uuid":"7cbf02f36bc52f0ff37d83a6040655da09d54f4d","trusted":false},"cell_type":"code","source":"null_entries_train[null_entries_train.project_is_approved==0].head(5)\n# No obvious predictable pattern :(","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"819ccd3f-3b03-4442-92d3-1184c725f3a2","_uuid":"c8108d10f7ff6fc55be53e28d3053b201e1dfd2e","trusted":false},"cell_type":"code","source":"null_entries_train[null_entries_train.project_is_approved==0].project_resource_summary.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"13fac155-2cd0-40d1-9b40-c214ebf1585b","_uuid":"f50d22319203cf557f789530dbec841ece6e2417","trusted":false},"cell_type":"code","source":"null_entries_train[null_entries_train.project_is_approved==0].project_resource_summary.iloc[1]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b6e4ff12-cf94-4c7d-84ed-3574762e2f32","_uuid":"b90d464b77ff425f619c65b890627acdaabe488b","trusted":false},"cell_type":"code","source":"null_entries_train[null_entries_train.project_is_approved==0].project_resource_summary.iloc[6]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c1b4bd25-7ac6-4dd5-93bf-934c51416ae0","_uuid":"52307667a16ce049cf728e2b564bc6be40033484","trusted":false},"cell_type":"code","source":"null_entries_train[null_entries_train.project_is_approved==0].project_resource_summary.iloc[9]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"56252be6-aa73-430a-81a0-4eb7e9dadca0","_uuid":"53570095da233ce0e06cade56327b1135d13c2bf"},"cell_type":"markdown","source":"A lot of them seem to be **Art Supplies!? **. Is that supposed to mean something? Anyways, let's continue on...\n\n\n# Custom helper functions:\nCreating some functions here that would be used across multiple analysis.\n\n## Plotting functions:\nCreating a simple function to create two plots.\n* Frequency plot\n* Approval rate across the target column entries\n"},{"metadata":{"collapsed":true,"_kg_hide-input":false,"_cell_guid":"3acb8ea6-cd40-4f9d-8af8-3ee3facf943c","_uuid":"aa9e90f56191e4d68347606cdb1bd724b889b555","trusted":false},"cell_type":"code","source":"#making this for easy subsetting later\napprovals=train[train.project_is_approved==1]\nrejects=train[train.project_is_approved==0]\n\n# lets make a simple re-usable function to make the plots!\n# This lets us add more functionality if needed later and it would be replicated across all plots!\ndef make_custom_plot(target_column_name='',title='',total_counts=None,approvals_counts=None,rejects_counts=None,x_rotation_angle=0):\n    \"\"\"        \n    Description:\n        Creates a 1x2 plot of 1. the # of projects across the variable and the 2. A stacked Percentage bar chart of the Approval rates across the variable\n    Useage: 1) make_custom_plot('gender','Analyzing Gender')\n            2) make_custom_plot(total_counts=x,approvals_counts=x1,rejects_counts=x2)\n    \"\"\"\n    if(target_column_name!=''):\n        x=train[target_column_name].value_counts()\n        x1=approvals[target_column_name].value_counts()\n        x2=rejects[target_column_name].value_counts()\n    else:\n        x=total_counts\n        x1=approvals_counts\n        x2=rejects_counts\n        target_column_name=title\n    #plot initiate\n    plt.figure(figsize=(16,6))\n    \n    #super title\n    plt.suptitle(title,fontsize=18)\n    plt.subplot(121)\n    #title and labels for plot1\n    plt.title('Total Projects Submitted',fontsize=12)\n    plt.ylabel('# of Projects', fontsize=12)\n    plt.xlabel(target_column_name, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.setp(labels, rotation=x_rotation_angle)\n    # Barplot\n    ax= sns.barplot(x.index, x.values, alpha=0.8)\n\n    #adding the text labels\n    rects = ax.patches\n    labels = x.values\n    for rect, label in zip(rects, labels):\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n    \n    \n    plt.subplot(122)\n    #title and labels for plot2\n    plt.title('Approval Rate',fontsize=12)\n    plt.ylabel('Percent Approved Projects', fontsize=12)\n    plt.xlabel(target_column_name, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.setp(labels, rotation=x_rotation_angle)\n    # https://python-graph-gallery.com/13-percent-stacked-barplot/\n    r=np.arange(len(x))\n    totals=x\n    greenBars = [i / j * 100 for i,j in zip(x1, totals)]\n    redBars = [i / j * 100 for i,j in zip(x2, totals)]\n\n    barWidth = 0.85\n    names = x.index\n    # Create green Bars\n    plt.bar(r, greenBars, color='#b5ffb9', edgecolor='white', width=barWidth, label=\"Approved\")\n    # Create red Bars\n    plt.bar(r, redBars, bottom=greenBars, color='red', edgecolor='white', width=barWidth, label=\"Rejected\")\n    # Custom x axis\n    plt.xticks(r, names)\n    # Add a legend\n    plt.legend(loc='upper left', bbox_to_anchor=(1,1), ncol=1)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8506eed7-9ec5-46af-a093-5351cc58cfbe","_uuid":"86c679ed6c768e7f45f5ce7aabf5a7450770100a"},"cell_type":"markdown","source":"## Text Functions:\nCreating custom functions to be used in the various text fields in the dataset.\n### Extract text stats:\nThis function does the following\n* Gets basic text statistics (Word count, Unique word count) from the text column\n* Plot Violin plot(Extension of box plot) across Project approval for the computed variables\n* Create a KDE plot for unique word percent\n"},{"metadata":{"collapsed":true,"_kg_hide-input":true,"_cell_guid":"c3d020c1-1012-4f54-9699-d7811b45974f","_uuid":"c9994f1f597335aa1ee57edaf192513a80aa06aa","trusted":false},"cell_type":"code","source":"# Making a function instead of writing code for single text columns so that its more scalable and can be applied to all text cols\ndef get_text_stats(text_col):\n    \"\"\"\n    Get Wordcount,Unique Wordcount and WordCount Percent and make appropriate visuals\n    Todo: Add more text stats\n    \"\"\"\n    title=\"Text Stats of \" + text_col\n    target_col='project_is_approved'\n    # Borrowed from previous work at https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda\n    text_stats=pd.DataFrame()\n    text_stats[target_col]=train[target_col]\n    #Word count \n    text_stats['word_count']=train[text_col].apply(lambda x: len(str(x).split()))\n    #Unique word \n    text_stats['count_unique_word']=train[text_col].apply(lambda x: len(set(str(x).split())))\n    #Word count percent in each comment:\n    text_stats['word_unique_percent']=(text_stats['count_unique_word']*100)/text_stats['word_count']\n    \n    temp_df = pd.melt(text_stats, value_vars=['word_count', 'count_unique_word'], id_vars=target_col)\n    \n    print(\"------ Sample from an Approved project ------\\n\")\n    print(approvals[text_col].iloc[0])\n    \n    print(\"\\n------ Sample from a Rejected project ------\\n\")\n    print(rejects[text_col].iloc[0])\n    \n    \n    #plotting\n    plt.figure(figsize=(16,5))\n    plt.subplot(121)\n    plt.suptitle(title,fontsize=16)\n    #re-shaping as required\n    plt.title(\"Word Count\")\n    sns.violinplot(x='variable', y='value', hue=str(target_col), data=temp_df,inner='quartile')\n    plt.ylabel('# of projects', fontsize=12)\n    \n    plt.subplot(122)\n    plt.title(\"Percentage of Unique words - effect on Approval\")\n    ax=sns.kdeplot(text_stats[text_stats.project_is_approved == 0].word_unique_percent, label=\"Not Approved\",shade=True,color='r')\n    ax=sns.kdeplot(text_stats[text_stats.project_is_approved == 1].word_unique_percent, label=\"Approved\")\n    plt.legend()\n    plt.xlabel('Percent unique words', fontsize=12)\n    plt.ylabel('# of projects', fontsize=12)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8a8264ff-df93-45b4-8aa6-7e1993ad1aa1","_uuid":"25506a0a6e787d7af1fc750e48264f9f23e47ede"},"cell_type":"markdown","source":"### Make Wordclouds:\nA simple function that takes in a text column/field and makes separate word-clouds for approved and rejected projects."},{"metadata":{"collapsed":true,"_cell_guid":"57ad1a12-99c6-4064-8305-94a90c4e672d","_uuid":"0cc7351d2cb532346f2cc93fce3ca5770a509126","trusted":false},"cell_type":"code","source":"# for the wordcloud\nstopword=set(STOPWORDS)\n\n# Custom Adding some stop words to make better and more meaningful wordclouds\nstopword.add('will')\nstopword.add('student')\nstopword.add('students')\nstopword.add('class')\nstopword.add('classroom')\nstopword.add('child')\nstopword.add('children')\nstopword.add('teacher')\nstopword.add('school')\nstopword.add('need')\nstopword.add('needs')\n\ndef make_word_clouds(text_col):\n    \"\"\"\n    Makes two wordclouds : one for Approvals and one for rejects\n    \n    Todo: Think of faceted clouds across categorical var (Eg:Grade category)\n    \"\"\"\n    plt.figure(figsize=(25,8))\n    plt.subplot(211)\n    # Get text col from approvals subset\n    text=approvals[text_col].values\n    # make wordcloud\n    wc= WordCloud(background_color=\"black\",max_words=100,stopwords=stopword)\n    wc.generate(\" \".join(text))\n\n    plt.axis(\"off\")\n    plt.title(\"Words frequented in Approved Projects\", fontsize=12)\n    #https://matplotlib.org/examples/color/colormaps_reference.html for colormaps\n    plt.imshow(wc.recolor(colormap='Pastel1',random_state=17), alpha=0.98)\n    \n    plt.subplot(212)\n    # Get text col from Rejects subset\n    text=rejects[text_col].values\n\n    # make wordcloud\n    wc= WordCloud(background_color=\"white\",max_words=100,stopwords=stopword)\n    wc.generate(\" \".join(text))\n    \n    plt.axis(\"off\")\n    plt.title(\"Words frequented in Rejected Projects\", fontsize=12)\n    plt.imshow(wc.recolor(colormap='inferno',random_state=17), alpha=0.98)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"1d7fc0d5-8000-403a-b273-1bffee597a87","_uuid":"216c7844cf0d2e412e7531c9158358b5149cef0e"},"cell_type":"markdown","source":"# Individual Feature's impact on Approval:\n\nLet's explore the impact of our descriptive features on Project approval rates.\n\n## 1) Teacher-prefix:\nThis variable gives information of the title of the teacher submitting the request. \n\nAlso, indirectly we can infer the gender from this variable."},{"metadata":{"collapsed":true,"_cell_guid":"6c53f1dc-6640-4e74-bfd4-dbe5824a5f0a","_uuid":"46f8f8eaa4c9db6c49353d62e74074f29b04ccc5","trusted":false},"cell_type":"code","source":"make_custom_plot('teacher_prefix','Does a Title affect approval?')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c6a6e611-3ee2-417d-bbbe-c11497b5e0c7","_uuid":"994cba037de3d05c1a4a32db9126cbfd1cc97368"},"cell_type":"markdown","source":"### 2) Gender:\nThis is a created field from the teacher prefix field. The mapping is as follows,\n* Mrs, Ms --> Female\n* Mr. --> Male\n* Teacher,Dr --> Unknown"},{"metadata":{"collapsed":true,"_cell_guid":"aee5cbe1-7d09-4273-bcdf-711b18622e94","_uuid":"db188e2635517ca558eeccb9a9010f61025febd8","trusted":false},"cell_type":"code","source":"# Creating the gender column\ngender_mapping = {\"Ms.\": \"Female\", \"Mrs.\":\"Female\", \"Mr.\":\"Male\", \"Teacher\":\"Unknown\", \"Dr.\":\"Unknown\", np.nan:\"Unknown\"  }\ntrain[\"gender\"] = train.teacher_prefix.map(gender_mapping)\napprovals[\"gender\"] = approvals.teacher_prefix.map(gender_mapping)\nrejects[\"gender\"] = rejects.teacher_prefix.map(gender_mapping)\ntest['gender'] = test.teacher_prefix.map(gender_mapping)\nmake_custom_plot('gender','Analyzing Gender')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4f5f515c-8ec0-407e-b552-b6f08da8109d","_uuid":"926175cbe3b4b9fae4cbe5c1c215bafb5abda5a3"},"cell_type":"markdown","source":"Unsurprisingly, there are more Female teachers. But that does not affect/bias the Approval rate at all.\n\n### 3) Project Grade Category:\nThis variable shows us the class/grade of the students that would benefit from the donation!\n"},{"metadata":{"collapsed":true,"_kg_hide-input":true,"_cell_guid":"18992042-8e91-412a-8eec-9fc58922b5dd","_uuid":"aa17e4b924c602002ee7c03ea0361fb60685f356","trusted":false},"cell_type":"code","source":"make_custom_plot('project_grade_category','Do smaller kids get more approval rates?')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a7ee09f5-aff2-42ec-9228-fbfd47a50f3a","_uuid":"dee9b62b6abb5d8f28586c6a8afebca545497dfa"},"cell_type":"markdown","source":"While Smaller kids seem to get more projects, there is no significant effect of Grade/class on Approval rates.\n\n## 4) Subject Category:\nThis field shows us the Subject category/categories that this project aims to help at. Note that sometimes, there are multiple subject categories tagged to one project."},{"metadata":{"collapsed":true,"_kg_hide-input":true,"_cell_guid":"7f95948b-653d-49e2-bccd-b35c8a257cc0","_uuid":"b689384aaacbda3f8a88f042d3ac862dcf949d98","trusted":false},"cell_type":"code","source":"x= train.project_subject_categories.value_counts()\n#prep for chart\nx=x.sort_values(ascending=False)\nx=x.iloc[0:20]\n\n#chart\nplt.figure(figsize=(16,4))\nax = sns.barplot(x.index, x.values, alpha=0.8,color=color[0])\nplt.title(\"What are the frequent subject categories?\",fontsize=16)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=80)\nplt.ylabel('# Projects', fontsize=12)\nplt.xlabel('Subject Category', fontsize=12)\n\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()\nprint(\"There are \",len(train.project_subject_categories.unique()),\"unique Subject Categories\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"927cc573-5599-46c5-bcc0-4853cc7c8c52","_uuid":"edf9aa15542add7cc869ea4fe856f2da9807a637"},"cell_type":"markdown","source":" The subject categories can be cleaned a bit further.\n \n For example, the third most popular category can be broken down into 1)\"Literacy & language\", 2)\"Math & science\" separately"},{"metadata":{"collapsed":true,"_cell_guid":"fd82b246-359c-47c2-87bc-fd0af1265025","_uuid":"7f1621f3f5a67f13c4d9f2a5c89a4793f08765e0","trusted":false},"cell_type":"code","source":"# Grouping similar categories for overall\nsubject_cats=','.join(train['project_subject_categories'])\ncats=pd.DataFrame(subject_cats.split(','),columns=['project_subject_categories']) # to split on \",\"\ncats.project_subject_categories=cats.project_subject_categories.str.strip() # to remove unwanted spaces\nx=cats.project_subject_categories.value_counts()\nprint(\"There are\",len(x),\"different subject categories after cleaning\")\n\n# repeat for approved group and rejected group\n# Grouping similar categories for approved \nsubject_cats=','.join(approvals['project_subject_categories'])\ncats=pd.DataFrame(subject_cats.split(','),columns=['project_subject_categories']) # to split on \",\"\ncats.project_subject_categories=cats.project_subject_categories.str.strip() # to remove unwanted spaces\nx1=cats.project_subject_categories.value_counts()\n\n# Grouping similar categories for rejected\nsubject_cats=','.join(rejects['project_subject_categories'])\ncats=pd.DataFrame(subject_cats.split(','),columns=['project_subject_categories']) # to split on \",\"\ncats.project_subject_categories=cats.project_subject_categories.str.strip() # to remove unwanted spaces\nx2=cats.project_subject_categories.value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"f198e034-1cad-41d1-a02a-dfc8736ff27e","_uuid":"fbd3881632732873dea6c337c8d0062b0f9466c2","trusted":false},"cell_type":"code","source":"make_custom_plot(title='Subject Category',total_counts=x,approvals_counts=x1,rejects_counts=x2,x_rotation_angle=80)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ac4b54a6-dc91-4b8c-9979-1debe0944f8e","_uuid":"3b1ff8ef1a6746b6b7c4c61277da0e881d802e77"},"cell_type":"markdown","source":"Literature/Language supplies seem to be in high demand. I guess books would fall into this category. Let's explore that more in detail in another section.\n\n## 5) Project Sub-Category:\nPerforming a similar analysis for the project sub-category."},{"metadata":{"collapsed":true,"_cell_guid":"773a7a56-f416-452a-9e55-17a1f3ec3e66","_uuid":"105df86992f83a8427920e52ca574d07c998d478","trusted":false},"cell_type":"code","source":"x= train.project_subject_subcategories.value_counts()\n#prep for chart\nx=x.sort_values(ascending=False)\nx=x.iloc[0:20]\n\n#chart\nplt.figure(figsize=(16,4))\nax = sns.barplot(x.index, x.values, alpha=0.8,color=color[0])\nplt.title(\"What are the frequent subject sub categories?\",fontsize=16)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=80)\nplt.ylabel('# Projects', fontsize=12)\nplt.xlabel('Subject sub-Category', fontsize=12)\n\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()\nprint(\"There are\",len(train.project_subject_subcategories.unique()),\"unique Subject Categories\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"479866a9-9787-4243-a202-ce60fc6fa807","_uuid":"7cc59338b798903989671b4539c2b7435d7704ce","trusted":false},"cell_type":"code","source":"# Grouping similar categories for overall\nsubject_cats=','.join(train['project_subject_subcategories'])\ncats=pd.DataFrame(subject_cats.split(','),columns=['project_subject_subcategories']) # to split on \",\"\ncats.project_subject_subcategories=cats.project_subject_subcategories.str.strip() # to remove unwanted spaces\nx=cats.project_subject_subcategories.value_counts()\nprint(\"There are \",len(x),\" different subject sub-categories after Cleaning\")\nprint(\"They are:- \",x.index.values)\n# repeat for approved group and rejected group\n# Grouping similar categories for approved \nsubject_cats=','.join(approvals['project_subject_subcategories'])\ncats=pd.DataFrame(subject_cats.split(','),columns=['project_subject_subcategories']) # to split on \",\"\ncats.project_subject_subcategories=cats.project_subject_subcategories.str.strip() # to remove unwanted spaces\nx1=cats.project_subject_subcategories.value_counts()\n\n# Grouping similar categories for rejected\nsubject_cats=','.join(rejects['project_subject_subcategories'])\ncats=pd.DataFrame(subject_cats.split(','),columns=['project_subject_subcategories']) # to split on \",\"\ncats.project_subject_subcategories=cats.project_subject_subcategories.str.strip() # to remove unwanted spaces\nx2=cats.project_subject_subcategories.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"d584122a-14fa-4f73-b63c-9d9a2486a659","_uuid":"4699462e5d00b90b8e2e5f22081e2b775bc2903e","trusted":false},"cell_type":"code","source":"# Plotting top 8 to avoid clutter\nmake_custom_plot(title='Subject Sub-Category',total_counts=x.iloc[0:8],approvals_counts=x1.iloc[0:8],rejects_counts=x2.iloc[0:8],x_rotation_angle=80)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"8ca64411-5c93-4aa8-9bcb-59b6cc8f715e","_uuid":"0ab2cee2dd369ffea2335cf2740afdb74a75de39","trusted":false},"cell_type":"code","source":"end_preprocess=time.time()\nprint(\"Time till sub-category:\",end_preprocess-start_time,\"s\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"452a303d-78b0-42fb-9c72-0f8e14d851e3","_uuid":"f3ea5e063733ccde43b88f1d6a7708f294512189"},"cell_type":"markdown","source":"## Title:\n\nProject title. \n\n"},{"metadata":{"collapsed":true,"_cell_guid":"f01e392f-40f7-4baf-8842-4c3787147efc","_uuid":"be0ebc7fed562ba6e308d71ecf7985c1bc938945","trusted":false},"cell_type":"code","source":"text_col='project_title'\ntarget_col='project_is_approved'\n# Borrowed from previous work at https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda\ntext_stats=pd.DataFrame()\ntext_stats[target_col]=train[target_col]\n#Word count \ntext_stats['word_count']=train[text_col].apply(lambda x: len(str(x).split()))\n#Unique word \ntext_stats['count_unique_word']=train[text_col].apply(lambda x: len(set(str(x).split())))\ntext_stats.groupby('project_is_approved').mean()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c024959f-bb2b-43de-8040-d2e23ec21d22","_uuid":"6488855039941f6e69beb39c7ef15034003abea6","trusted":false},"cell_type":"code","source":"temp=train.groupby('project_title')['project_is_approved'].agg(['sum','count'])\ntemp['approval_rate']=(temp['sum']*100)/temp['count']\ntemp.columns=['# of projects approved','# of total projects','Approval rate']\ntemp=temp.sort_values(by='# of total projects',ascending=False)\ntemp=temp.iloc[0:25]\ntemp","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0bfca6fe-d557-4c1e-a867-73f915c0aac0","_uuid":"6b32da06a093f8f14388cda457471dc1beda034b"},"cell_type":"markdown","source":"### Looks like **Wiggle while you work!** is a really famous phrase on the platform. It's having several entries with minor changes. \n### Also, the Approval rates are impressive **~91%** for projects with that title."},{"metadata":{"collapsed":true,"_cell_guid":"4aa4aaba-eb09-403f-8983-958636f61ff7","_uuid":"bb919ee8df34682d7b56b882c0a909fe8382ae37","trusted":false},"cell_type":"code","source":"make_word_clouds(text_col='project_title')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1b72ff1f-f34b-424d-8da8-4bd8b7471f5f","_uuid":"10bb91cfef1775b322711795b94df5b2be4f363d"},"cell_type":"markdown","source":"# Project Essays:\n\nOn May 17th, 2016, the DonorsChoose.org application switched from having 4 essay prompts to just 2 prompts, so from that point forward, only project_essay_1 and project_essay_2 contain text, and project_essay_3 and project_essay_4 have NaNs.\n\nHere's a summary of the essay prompts before and after that date.\n\n**Before May 17th, 2016:**\n\n* project_essay_1: \"Introduce us to your classroom\"\n* project_essay_2: \"Tell us more about your students\"\n* project_essay_3: \"Describe how your students will use the materials you're requesting\"\n* project_essay_4: \"Close by sharing why your project will make a difference\"\n\n**May 17th, 2016 and beyond: **\n\n* project_essay_1: \"Describe your students: What makes your students special? Specific details about their background, your neighborhood, and your school are all helpful.\"\n* project_essay_2: \"About your project: How will these materials make a difference in your students' learning and improve their school lives?\"\n\n\nAs @HeadsorTails explains in this [discussion post](https://www.kaggle.com/c/donorschoose-application-screening/discussion/51352#292941), performing the following changes to clean up the NaNs.\n* Combine essay_1 and essay_2 before May 17th to make \"student_description\" and use essay_1 after May 17th directly\n* Combine essay_3 and essay_4 before May 17th to make \"project_description\" and use essay_2 after May 17th directly\n"},{"metadata":{"collapsed":true,"_cell_guid":"6bfe831e-cc9e-41e3-bd3d-71507e540a7d","_uuid":"9a5c893667de0b9595cf12633bbe7b0e89b91a68","trusted":false},"cell_type":"code","source":"# Before performing changes , simple check\nx=train[train.project_essay_3.notnull()]\nprint(\"The last time an entry occured in Project essay 3 -- \",x['project_submitted_datetime'].max())","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"078371bc-f839-40b8-810b-caf759cace16","_uuid":"36514e53f1a3a52e6e40ea780fe12d83d631a10a","trusted":false},"cell_type":"code","source":"# Making the First essay column :student_description\ntrain['student_description']=train['project_essay_1']\n#performing the adjustment\n# df.loc[selection criteria, columns I want] = value\ntrain.loc[train.project_essay_3.notnull(),'student_description']=train.loc[train.project_essay_3.notnull(),'project_essay_1']+train.loc[train.project_essay_3.notnull(),'project_essay_2']","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b085cd08-04fd-4b3f-bd0c-381f295de197","_uuid":"c4285120082cdd4ee790eaca8e172da6ec8e3dce","trusted":false},"cell_type":"code","source":"#repeat for test dataset\ntest['student_description']=test['project_essay_1']\ntest.loc[test.project_essay_3.notnull(),'student_description']=test.loc[test.project_essay_3.notnull(),'project_essay_1']+test.loc[test.project_essay_3.notnull(),'project_essay_2']","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"02695448-2ab0-4ab7-9e9b-e69d7bc25ac1","_uuid":"6c310ebed44c0df6871e69094c5508bdf45fded8","trusted":false},"cell_type":"code","source":"# Making the second essay column : project_description\ntrain['project_description']=train['project_essay_2']\n#performing the adjustment\n# df.loc[selection criteria, columns I want] = value\ntrain.loc[train.project_essay_3.notnull(),'project_description']=train.loc[train.project_essay_3.notnull(),'project_essay_3']+train.loc[train.project_essay_3.notnull(),'project_essay_4']","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e46a2a3c-6ec6-4cad-bcf3-7c242b102b94","_uuid":"07041dce297f3d811f69e22d6fd26d098e112374","trusted":false},"cell_type":"code","source":"test['project_description']=test['project_essay_2']\ntest.loc[test.project_essay_3.notnull(),'project_description']=test.loc[test.project_essay_3.notnull(),'project_essay_3']+test.loc[test.project_essay_3.notnull(),'project_essay_4']","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"aff19242-5a19-46ec-a8b6-63a6f5fb6a24","_uuid":"6445fb37b5ad80cd89ae6f93b87b038dbb730de1","trusted":false},"cell_type":"code","source":"#check\ntrain[train.project_essay_3.notnull()].head(2)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"324f1bca-f98c-4153-951f-51958b8af2da","_uuid":"b300d8cfbafa2b333ff889ad0f428494dfedcc42","trusted":false},"cell_type":"code","source":"# check\ntest[test.project_essay_3.notnull()].head(1).project_description.values","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"27bdcf1f-ed8f-4eea-8b5a-9d62b3f106a3","_uuid":"2d068d41f073f189156c130c7025502e4067e3ff","trusted":false},"cell_type":"code","source":"#remove unwanted colunms\ndel(train['project_essay_1'])\ndel(train['project_essay_2'])\ndel(train['project_essay_3'])\ndel(train['project_essay_4'])\ndel(test['project_essay_1'])\ndel(test['project_essay_2'])\ndel(test['project_essay_3'])\ndel(test['project_essay_4'])\n\n#update the subsets\napprovals=train[train.project_is_approved==1]\nrejects=train[train.project_is_approved==0]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dbcc8b59-c3ca-4b11-9fdc-e0d3a6d9e168","_uuid":"62f6e79258dffcc6579a3a04e5cc4ca2a60c68d2"},"cell_type":"markdown","source":"## Student Description (Project essay 1):\n\n\"Describe your students: What makes your students special? Specific details about their background, your neighborhood, and your school are all helpful.\"\n\nNote: The data has been altered to account for the change at May 17th, 2016."},{"metadata":{"collapsed":true,"_cell_guid":"80465467-823e-4803-99c7-26bc3bef4681","_uuid":"2275338dc060503c3a59646b4867e8dfcca16123","trusted":false},"cell_type":"code","source":"get_text_stats(text_col='student_description')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d86acbae-76e8-4055-867d-98cbe9f1d9b7","_uuid":"d6043acfcad5d14390122553b0379199312a8bf2"},"cell_type":"markdown","source":"Almost Identical plots across project approval in both word count and unique word count! Looks like they won't be much use here. "},{"metadata":{"collapsed":true,"_cell_guid":"8c26c310-1393-4e61-9e14-22bc684af12d","_uuid":"06abbbb63346c392d2698083caefa11d5fd9e122","trusted":false},"cell_type":"code","source":"make_word_clouds(text_col='student_description')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"de7ad12e-02ef-40da-8487-b9a43d7f1808","_uuid":"a83fe08536d6c472b464f7a5a99a08fcccb15336"},"cell_type":"markdown","source":"## Project Description (Project essay 2):\n\"About your project: How will these materials make a difference in your students' learning and improve their school lives?\"\n\nNote: The data has been altered to account for the change at May 17th, 2016."},{"metadata":{"collapsed":true,"_cell_guid":"94827ea2-ae50-474a-9f2f-70981f108521","_uuid":"7033a8cbdb68462778ee97b1d73ed2ae075c4920","trusted":false},"cell_type":"code","source":"get_text_stats(text_col='project_description')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"5c6c6bef-1734-4160-925b-1cb06022de79","_uuid":"78caeb8e51f0cc0d96b3863ff63e8fbcc6601d21","trusted":false},"cell_type":"code","source":"make_word_clouds(text_col='project_description')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eeb13982-0853-4b41-8523-416bac3b8d23","_uuid":"8a5a57b5caaea9d9e70bd281fd3f9fef45f0ac32"},"cell_type":"markdown","source":"## Project resource summary:\nThis variable contains a short summary of the resources needed for the project."},{"metadata":{"collapsed":true,"_cell_guid":"06d953e1-a5ff-41da-94f6-25c9df54e393","_uuid":"2fdae6b8d3847f5be356c3f4d768f379f0ab0586","trusted":false},"cell_type":"code","source":"get_text_stats('project_resource_summary')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"707d318b-bdef-4bd2-bf18-a80fa78177e6","_uuid":"15494c74bfb5b4d1aa5fa6cfc0f167f4afc8a028","trusted":false},"cell_type":"code","source":"make_word_clouds('project_resource_summary')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"419319c1-2289-4083-80f5-7245c0563f36","_uuid":"55b1b04d4be41e123ce445d48c9f88161c91b6b4","trusted":false},"cell_type":"code","source":"end_time=time.time()\nprint(\"Time till plotting section\",end_time-start_time,\"s\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ff5ae261-07e7-45a6-98e8-929682eadb99","_uuid":"16313dc4b64431867f1855d9d9302adff658f96c"},"cell_type":"markdown","source":"# Adding features from the Resources Dataset:\n"},{"metadata":{"collapsed":true,"_cell_guid":"2a9459e4-454c-4607-a156-6e2b01c8aded","_uuid":"2e2c198415ab256a2a2f1515576a32c1063e0625","trusted":false},"cell_type":"code","source":"resources['total_cost']=resources['quantity']*resources['price']","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"86663953-1421-4afd-b53b-0849f2e51dd0","_uuid":"c3d8e0ac30a7fd318e9d45443ef1c6e21c3d5448","trusted":false},"cell_type":"code","source":"# Group by and get concat of the description\nresources['description']=resources['description'].astype(str)\nx=resources.groupby('id')['description'].apply(lambda x: \"%s\" % ', '.join(x))   #https://stackoverflow.com/questions/17841149/pandas-groupby-how-to-get-a-union-of-strings\nx.head(2)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"6bc51390-3e74-4284-a4c8-426f034dc7eb","_uuid":"c668edf6f8d06a676f731c93b4592f310abfd63a","trusted":false},"cell_type":"code","source":"# project level resource stats\nresources_agg=resources.groupby('id')['quantity','price','total_cost'].agg({'quantity':['sum','count'],'price':['mean'],'total_cost':['sum']})\nresources_agg.columns=['item_quantity_sum','variety_of_items','avg_price_per_item','total_cost']\nresources_agg['collated_description']=x\nresources_agg=resources_agg.reset_index()\n#resources_agg=resources_agg.sort_values(\"total_cost\",ascending=False)\nresources_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b31f8c6a-aed9-476f-8f3f-521c4183dea0","_uuid":"696afb761253d1eb98b6d230731d8d3ca5b43298","scrolled":true,"trusted":false},"cell_type":"code","source":"train_merge=pd.merge(left=train,right=resources_agg,on='id',how='left')\ntrain_merge.sort_values(\"total_cost\",ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"87de45a9-4cd6-4689-8847-cb42bf42814a","_uuid":"dc1fd21e616e03de32fe950c365011121b2daa45","trusted":false},"cell_type":"code","source":"x=train_merge.total_cost.value_counts()\n#sort by price\nx=x.sort_index(ascending=False)\n#subset for alteast 5 entries\nx=x[x>5]\n# get the top 20\nx=x.iloc[0:20]\n#plot\nplt.figure(figsize=(16,5))\nax= sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"Frequent(more than 5 projects) Price points\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('Price point', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d1b5ef80-88b9-4508-9ddf-5c13cb6ab421","_uuid":"cb110f1988e080c448f661b784c783dd4b0db978"},"cell_type":"markdown","source":"There are some pricey items on the list. \n\nLet's explore some of the most costly items requested to hopefully find a useful pattern.\n\n# Google Expeditions Kit : 9999, 6999, 3999 :\nThis seems to be the costliest item under DonorsChoose. Its a set of 30 Virtual Reality glasses for the entire classroom. \n\nI would've loved to have this during my schooling days! [Demo](https://support.google.com/edu/expeditions/answer/7375176?hl=en&ref_topic=6334250)\n\n![](https://images.bbycastatic.ca/sf/projects/bestbuyforbusiness/education/contents/google-expeditions/assets/featured-kit-size-30.jpg)\n\nThe item comes at three price points (9999,6999,3999).\n\nThere were 34 requests for this item and only two rejections at 9999\n\nThere were 13 requests for the item and 0 rejects at 6999\n\n57 requests for this item at 3999 and 5 rejects"},{"metadata":{"collapsed":true,"_cell_guid":"b26ee9ef-6f92-460d-a3aa-27bca6710383","_uuid":"8bd8cacd57865b20ba38610390d253a9dfad9b66","trusted":false},"cell_type":"code","source":"# subset expedition kit requests\nsubset=train_merge[train_merge.total_cost==9999]\nprint(\"Number of times requested:\",len(subset))\nsubset[subset.project_is_approved==0]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"a8b94b0a-d4ce-491d-b95e-606abb95c17c","_uuid":"cd5e90dffceaa3572e9e018d4cbfb9b385e2a3be","trusted":false},"cell_type":"code","source":"# subset expedition kit requests\nsubset=train_merge[train_merge.total_cost==6999]\nprint(\"Number of times requested:\",len(subset))\nsubset[subset.project_is_approved==0]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"79e0777b-2f2b-47e9-9db1-893bf3ae5c13","_uuid":"b3fc20e8b1be6cb60d82aa7091905fa4fa44844b","trusted":false},"cell_type":"code","source":"# subset expedition kit requests\nsubset=train_merge[train_merge.total_cost==3999]\nprint(\"Number of times requested:\",len(subset))\nsubset[subset.project_is_approved==1]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f2da85d2-b5fe-40f6-9f09-a5fb7abb189e","_uuid":"2dd52217fff87d7b2223dbb24d9c2352b577df8c"},"cell_type":"markdown","source":"# Engage 2 - Interactive table - 4995.95 :\n\n![](https://images.kaplanco.com/images/products/engage2-interactive-table2015.jpg)\n\nInteractive table for kindergardeners!\n\nRequested 7 times and no rejects!"},{"metadata":{"collapsed":true,"_cell_guid":"a7c65eaa-b020-4eb8-85d5-b7188ba7e016","_uuid":"6e4172c4943e7105004de68cff925a47822a837b","trusted":false},"cell_type":"code","source":"# subset expedition kit requests\nsubset=train_merge[train_merge.total_cost==4995.95]\nprint(\"Number of times requested:\",len(subset))\nsubset[subset.project_is_approved==1]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"171678c9-ac9c-4443-a5d0-50b95d1ce8f2","_uuid":"96162e3f58d9341f2947e2383782c1a7c28b6b7f"},"cell_type":"markdown","source":"# Apple products:\n\nIpad , Ipad mini, Macbook seem to be frequent in the wishlist of teachers."},{"metadata":{"collapsed":true,"_cell_guid":"001f57ef-845e-4498-9303-59ffccadfdc2","_uuid":"bdbb3b37ab50bff70a8dfd8fe12be6677c3e8230","trusted":false},"cell_type":"code","source":"\nprice_points=[1999.99, 1999.96, 1999.95,1999.9]\n# subset expedition kit requests\nsubset=train_merge[train_merge.total_cost.isin(price_points)]\nprint(\"Number of times requested:\",len(subset))\nprint(\"Number of times approved:\",len(subset[subset.project_is_approved==1]))\nsubset[subset.project_is_approved==0].head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"0fb80e24-0f3a-4246-bb8c-c1bb0a3792e9","_uuid":"1c79c131ea50b939fd3673a451724f6e10f80207"},"cell_type":"markdown","source":"# Feature Engineering (ELI5 version):\n\nThe competition is more focussed on explainability (ie) understanding why a project gets approved, so that they can pre-approve some applications and pass on some of the difficult ones to Human volunteers.\n\nHence, Ive built the model to be compatable with [ELI5](https://www.kaggle.com/lopuhin/eli5-for-mercari), so that we can hopefully understand why the model thinks that certain projects are rejected!\n\n## Pre-processing/Cleaning text fields:\n\nThe following steps have been done for pre-processing.\n\n* Tokenization (Splitting into seperate words )\n* Basic pre-processing ( convert to lower,etc) by Gensim\n* Remove stop words\n* Lemmatization (Converting word to its root form : babies --> baby ; children --> child)\n\n"},{"metadata":{"collapsed":true,"_cell_guid":"11bafe2f-0ef0-4d3f-90d8-45f66b00f811","_uuid":"a681e399802bda43f471cbdbefbe555b9bda61c0","trusted":false},"cell_type":"code","source":"# Using cleaning functions from previous work --> https://www.kaggle.com/jagangupta/understanding-the-topic-of-toxicity/notebook\ndef preprocess_and_clean(text_col):\n    \"\"\"\n    Function to build tokenized texts from input comment and the clean them\n    Following transformations will be done\n    1) Stop words removal from the nltk stopword list\n   #commenting out for speed issues 2) Bigram collation (Finding common bigrams and grouping them together using gensim.models.phrases) (Eg: new + york --> new_york )\n    3) Lemmatization (Converting word to its root form : babies --> baby ; children --> child)\n    \"\"\"\n    \n    word_list = gensim.utils.simple_preprocess(text_col, deacc=True)\n    #Phrases help us group together bigrams :  new + york --> new_york\n    #bigram = gensim.models.Phrases(text_col)\n    \n    #remove stop words\n    clean_words = [w for w in word_list if not w in eng_stopwords]\n    #collect bigrams\n    #clean_words = bigram[clean_words]\n    #Lemmatize\n    clean_words=[lem.lemmatize(word, \"v\") for word in clean_words]\n    return(' '.join(clean_words))  \n\n#check clean function\nprint(\"Before clean:\",train.project_description.iloc[16])\nprint(\"After clean:\",preprocess_and_clean(train.project_description.iloc[16]))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"eec87b09-03fb-4602-901a-fac7b560994e","_uuid":"c455973bd11320fece07bb06f45cea7fa5259d5b","trusted":false},"cell_type":"code","source":"# Null treatment\ntrain.teacher_prefix=train.teacher_prefix.fillna('Unknown')\ntest.teacher_prefix=test.teacher_prefix.fillna('Unknown')\ntrain.gender=train.gender.fillna('Unknown')\ntest.gender=test.gender.fillna('Unknown')\n\nprint(train.shape)\nprint(test.shape)\ny=train['project_is_approved']\ntrain_id=train['id']\ntest_id=test['id']\ndel(train['project_is_approved'])\nall_data=pd.concat([train,test],axis=0)\nprint(all_data.shape)\nall_data_merge=pd.merge(left=all_data,right=resources_agg,on='id',how='left')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"8c8f8f23-1297-4fe9-aec2-f0d8b9250d5d","_uuid":"7309255a3e66ca38eaa2650a6b4e47049713ce97","trusted":false},"cell_type":"code","source":"# taking some FE ideas from public kernals\n# thanks owl, --> https://www.kaggle.com/the1owl/the-choice-is-yours\n# and jmbull --> https://www.kaggle.com/jmbull/xtra-credit-xgb-w-tfidf-feature-stacking","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"736141b6-f226-4c36-b5a0-e043f87e068a","_uuid":"f461e74060625e49dc662c19e33dc20d57daa1a4","trusted":false},"cell_type":"code","source":"from sklearn import *\nfrom tqdm import tqdm\n# Label encode some columns\ncols = [\n    'teacher_id', \n    'teacher_prefix', \n    'school_state', \n    'project_grade_category',\n    'project_subject_categories', \n    'project_subject_subcategories',\n    'gender']\nfor c in tqdm(cols):\n    le = preprocessing.LabelEncoder()\n    le.fit(all_data_merge[c].astype(str))\n    all_data_merge[c] = le.transform(all_data_merge[c].astype(str))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"a502977b-5cd1-4b59-8331-23828f2198f8","_uuid":"a4722e331c0888a4691128c8ff31862cb5280bb1","trusted":false},"cell_type":"code","source":"# Log1p transform price columns\nall_data_merge['avg_price_per_item']=np.log1p(all_data_merge['avg_price_per_item'])\nall_data_merge['total_cost']=np.log1p(all_data_merge['total_cost'])\n\n# date features\nall_data_merge['project_submitted_datetime'] = pd.to_datetime(train['project_submitted_datetime'])\nall_data_merge['datetime_dow'] = all_data_merge['project_submitted_datetime'].dt.dayofweek\nall_data_merge['datetime_year'] = all_data_merge['project_submitted_datetime'].dt.year\nall_data_merge['datetime_month'] = all_data_merge['project_submitted_datetime'].dt.month\nall_data_merge['datetime_hour'] = all_data_merge['project_submitted_datetime'].dt.hour\nall_data_merge['datetime_day'] = all_data_merge['project_submitted_datetime'].dt.day","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"bd0f18d5-47e6-47ef-b03f-e32c005f1d99","_uuid":"ae8983f4cb77a5b54d8086179aa0f2e040a9a6df","trusted":false},"cell_type":"code","source":"#process text cols\ntext_cols=['project_title', \n           'collated_description',\n           'project_resource_summary',\n           'student_description', \n           'project_description']\nfor c in tqdm(text_cols):\n    all_data_merge[c+'_len']=all_data_merge[c].apply(len)               # get length (ie) letter count\n    all_data_merge[c+'_word_count']=all_data_merge[c].apply(lambda x: len(str(x).split())) # get word count\n    all_data_merge[c]=all_data_merge[c].apply(preprocess_and_clean)\nend_time=time.time()\nprint(\"Time till end\",end_time-start_time,\"s\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"4d331675-1086-46c5-a16d-b0b8af882e16","_uuid":"8a07bf52903c9f790b032dfd28ed87020e617e8a","trusted":false},"cell_type":"code","source":"train_shape=train.shape\ntest_shape=test.shape\n# del(train)\n# del(test)\ndel(train_merge)\ndel(resources)\ndel(resources_agg)\ndel(subset)\ndel(approvals)\ndel(rejects)\ndel(all_data)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_kg_hide-input":true,"_cell_guid":"1344301e-e247-4cbb-91a9-bf40a56c8439","_uuid":"349dcbb0646321e506e6608a209c2e4067bee135","trusted":false},"cell_type":"code","source":"############################\n# needs debugging\n########################\n# from sklearn.pipeline import FeatureUnion,TransformerMixin,Pipeline\n# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n# from sklearn.feature_extraction import DictVectorizer\n# from sklearn.metrics import auc\n\n# # https://www.kaggle.com/lopuhin/eli5-for-mercari\n# # https://github.com/scikit-learn/scikit-learn/issues/2034\n\n# class GetItemTransformer(TransformerMixin):\n#     \"\"\"\n#     Custom class to fetch just the column needed from the numpy nd array from pandas.values that is passed to the vectorizer\n#     \"\"\"\n#     def __init__(self, field):\n#         self.field = field\n#     def fit(self, X, y=None):\n#         return self\n#     def transform(self,X):\n#         field_idx = list(all_data_merge.columns).index(self.field)\n#         return X[:,field_idx]\n    \n\n# vectorizer = FeatureUnion([\n#     ('project_title',\n#          Pipeline([\n#             ('get', GetItemTransformer('project_title')),\n#             ('vectorize',CountVectorizer(\n#             ngram_range=(1, 2),\n#             max_features=2000))\n#          ])),\n#     ('project_resource_summary',\n#          Pipeline([\n#             ('get', GetItemTransformer('project_resource_summary')) ,\n#             ('vectorize',TfidfVectorizer(\n#             ngram_range=(1, 2),\n#             max_features=5000))\n#          ])),\n#     ('student_description',\n#          Pipeline([\n#             ('get', GetItemTransformer('student_description')) ,\n#             ('vectorize',TfidfVectorizer(\n#             ngram_range=(1, 2),\n#             max_features=20000))\n#          ])),\n#     ('project_description',\n#          Pipeline([\n#             ('get', GetItemTransformer('project_description')),\n#             ('vectorize',TfidfVectorizer(\n#             ngram_range=(1, 2),\n#             max_features=20000))\n#          ])),\n#     ('collated_description',                                                 # using count vect here as this coulmn contains mostly product descr. \n#          Pipeline([\n#             ('get', GetItemTransformer('collated_description')),\n#             ('vectorize',CountVectorizer(\n#             ngram_range=(1, 2),\n#             max_features=10000))\n#          ]))\n# ])\n# error due to pipeline not having get feature names!! --> https://github.com/scikit-learn/scikit-learn/issues/6424\n# # todo : to add other fields(non-text) into the pipeline itself","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"3f2a6a73-1b19-40b5-9eae-21b671dcd264","_uuid":"53e1bb15eb2789cabcfcd351539a6d9e0049a320","trusted":false},"cell_type":"code","source":"del(all_data_merge['project_submitted_datetime'])\n#https://stackoverflow.com/questions/29815129/pandas-dataframe-to-list-of-dictionaries\nall_data_merge_1=all_data_merge.to_dict('records')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"15d468bd-51d2-4b02-a41e-08c7e8023568","_uuid":"d741ff017ac3cd1ebb1b060ceea6d2c016bbe378","trusted":false},"cell_type":"code","source":"from sklearn.pipeline import FeatureUnion,TransformerMixin,Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.metrics import auc\n\n# https://www.kaggle.com/lopuhin/eli5-for-mercari\n# https://github.com/scikit-learn/scikit-learn/issues/2034\n\nvectorizer = FeatureUnion([\n        ('project_title',CountVectorizer(\n            ngram_range=(1, 2),\n            max_features=5000,\n            preprocessor=lambda x: x['project_title'])),\n        ('project_resource_summary',TfidfVectorizer(\n            ngram_range=(1, 2),\n            max_features=10000,\n            preprocessor=lambda x: x['project_resource_summary'])),\n        ('student_description',TfidfVectorizer(\n            ngram_range=(1, 2),\n            max_features=40000,\n            preprocessor=lambda x: x['student_description'])),\n        ('project_description',TfidfVectorizer(\n            ngram_range=(1, 2),\n            max_features=40000,\n            preprocessor=lambda x: x['project_description'])),\n        ('collated_description',TfidfVectorizer(\n            ngram_range=(1, 2),\n            max_features=30000,\n            preprocessor=lambda x: x['collated_description'])),\n        ('Non_text',DictVectorizer())\n    ])\n\n# todo : to add other fields(non-text) into the pipeline itself","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"5717b93f-8d1b-4963-9801-2c44584c705b","_uuid":"28eb8efb670c6fe4446d2cac8762ba7376312c52","trusted":false},"cell_type":"code","source":"all_data_vectorized = vectorizer.fit_transform(all_data_merge_1)\n# split train and test \n# train_text_data_vectorizer=vectorizer.fit_transform(all_data_merge.iloc[:train_shape[0]])\n# test_text_data_vectorizer=vectorizer.fit_transform(all_data_merge.iloc[train_shape[0]:])\n\nfrom scipy.sparse import csr_matrix, hstack\nfinal_dataset=all_data_vectorized.tocsr()\nend_time=time.time()\nprint(\"total time till Sparse mat creation\",end_time-start_time,\"s\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_kg_hide-input":true,"_cell_guid":"2a2664c5-ac38-4e13-9346-79cfcf8500f3","_uuid":"d5a15de86225f5f2c968858ef486b948e6185007","trusted":false},"cell_type":"code","source":"# older version\n# all_text_data_vectorized = vectorizer.fit_transform(all_data_merge.values)\n# train_x_only_text=all_text_data_vectorized[0:train_shape[0]]\n# test_x_only_text=all_text_data_vectorized[train_shape[0]:]\n# all_cols=all_data_merge.columns.values\n# non_text_cols = [col for col in all_cols if col not in text_cols]\n# non_text_cols.remove('id')\n# non_text_cols.remove('project_submitted_datetime')\n# from scipy.sparse import csr_matrix, hstack\n# all_non_text_data = csr_matrix(all_data_merge[non_text_cols].values)\n# final_dataset=hstack((all_text_data_vectorized,all_non_text_data)).tocsr()\n# end_time=time.time()\n# print(\"total time till Sparse mat creation\",end_time-start_time,\"s\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"7dd8a0d8-2a43-4533-91e0-4111c4673709","_uuid":"7b6c670b5ebb343a97441946047e00df89e397e9","trusted":false},"cell_type":"code","source":"train_x=final_dataset[0:train_shape[0]]\ntest_x=final_dataset[train_shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"35606cb1-6893-46a9-a6ca-921deba995de","_uuid":"29598c74e2f65b6b6e3532161681ec25b6afb716","trusted":false},"cell_type":"code","source":"# del final_dataset,all_text_data_vectorized,all_non_text_data\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ff128a71-b7ce-4f23-8ad1-e4e77a48e846","_uuid":"26f94c81404a237d1fc56c4ccde712be273e7a73","trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(train_x, y, test_size=0.33, random_state=2018)\n# Using LGBM params from https://www.kaggle.com/opanichev/lightgbm-and-tf-idf-starter/code\nparams = {\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc',\n        'max_depth': 16,\n        'num_leaves': 31,\n        'learning_rate': 0.25,\n        'feature_fraction': 0.85,\n        'bagging_fraction': 0.85,\n        'bagging_freq': 5,\n        'verbose': 1,\n        'num_threads': 4,\n        'lambda_l2': 1,\n        'min_gain_to_split': 0,\n        'seed':1234\n}  ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"0a5c7bb9-d99e-43d2-b7f7-81cb911bf24e","_uuid":"59c77aef8f35c9213eceff3bcaf57cbac8caea18","trusted":false},"cell_type":"code","source":"import lightgbm as lgb\n\nmodel = lgb.train(\n        params,\n        lgb.Dataset(X_train, y_train),\n        num_boost_round=10000,\n        valid_sets=[lgb.Dataset(X_valid, y_valid)],\n        early_stopping_rounds=100,\n        verbose_eval=25)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"551c9409-c826-4e37-96d7-91e612e61f8e","_uuid":"b9831d0b479f4677ad59697e48b45a8b322d0ff1","trusted":false},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nvalid_preds = model.predict(X_valid, num_iteration=model.best_iteration)\ntest_preds = model.predict(test_x, num_iteration=model.best_iteration)\nauc = roc_auc_score(y_valid, valid_preds)\nprint('AUC:',auc)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"f23e916f-ab65-4c5b-9207-0d64370b6195","_uuid":"5d951dbd0b526b16df575ec8b9086ed061e2db06","trusted":false},"cell_type":"code","source":"end_time=time.time()\nprint(\"total time till LGB model\",end_time-start_time,\"s\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"3bfb193f-88d9-4845-a40c-67b4ad29278b","_uuid":"e63341d38a2b4722317ea3ba8b7238c8d50b367a","trusted":false},"cell_type":"code","source":"import xgboost as xgb\nxgb_params = {'eta': 0.2, \n                  'max_depth': 5, \n                  'subsample': 0.8, \n                  'colsample_bytree': 0.8, \n                  'objective': 'binary:logistic', \n                  'eval_metric': 'auc', \n                  'seed': 1234\n                 }\n# d_train = xgb.DMatrix(X_train, y_train)\n# d_valid = xgb.DMatrix(X_valid, y_valid)\n# d_test = xgb.DMatrix(test_x)\nX_train, X_valid, y_train, y_valid = train_test_split(train_x, y, test_size=0.33, random_state=2018)\n#for eli5\nd_train = xgb.DMatrix(X_train, y_train)\nd_valid = xgb.DMatrix(X_valid, y_valid)\nd_test = xgb.DMatrix(test_x)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"cb81db91-4cdd-4b80-8157-1b248ddf7cca","_uuid":"533b32d0f650fb246ac49f4010c7e848273758ed","trusted":false},"cell_type":"code","source":"watchlist = [(d_train, 'train'), (d_valid, 'valid')]\nmodel_xgb = xgb.train(xgb_params, d_train, 500, watchlist, verbose_eval=50, early_stopping_rounds=20)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ea841800-ab7f-4df4-8559-2a8f5c5b7fa9","_uuid":"c11618f992b5d87c3e7ac1a4123e02a1ad804935","trusted":false},"cell_type":"code","source":"xgb_pred_test = model_xgb.predict(d_test)\nxgb_pred_valid = model_xgb.predict(d_valid)\nauc = roc_auc_score(y_valid, xgb_pred_valid)\nprint('AUC:',auc)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"bc15a6b0-38bb-49ef-b852-81c2421ef463","_uuid":"21da68bffa82fc95a8fb310c09d5ed5daaa463c2","trusted":false},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfpr,tpr,thresholds=roc_curve(y_valid, xgb_pred_valid)\nroc_auc = metrics.auc(fpr, tpr)\nfpr_1,tpr_1,thresholds_1=roc_curve(y_valid,valid_preds)\nroc_auc_1 = metrics.auc(fpr_1, tpr_1)\nplt.figure(figsize=(8,6))\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'XGBoost-AUC = %0.2f' % roc_auc)\nplt.plot(fpr_1, tpr_1, 'g', label = 'LGBM-AUC = %0.2f' % roc_auc_1)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n# end_time=time.time()\n# print(\"total time till XBG model\",end_time-start_time,\"s\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"252aa002-ac95-49d8-9308-3c25b9a4e495","_uuid":"97f60f0f77601fc84d2815dfe52d30a25cd2ca9d","trusted":false},"cell_type":"code","source":"xgb_pred_train = model_xgb.predict(d_train)\nimport eli5","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"d3418ccb-569c-4bea-b24d-4701a78f4959","_uuid":"64608855364e82c4c49120509783840bbe0f42de","trusted":false},"cell_type":"code","source":"# eli5.explain_weights_lgb(model_xgb, vec=vectorizer)     # out of bounds error\n# text_features=vectorizer.get_feature_names()\n# other_features=non_text_cols\n# all_features=text_features+non_text_cols\n# eli5.explain_weights_xgboost(model_xgb, feature_names=all_features)          ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"f2990970-cbb1-4dd5-ad73-a4519262961f","_uuid":"c1e882477c80cc0b33dbc082130c36b9962f99ba","trusted":false},"cell_type":"code","source":"eli5.show_weights(model_xgb,vec=vectorizer)    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"09e3547b-22bc-452e-939b-d10820ea05a5","_uuid":"1410a2335d34257031045cc5f33d23802eb009db"},"cell_type":"markdown","source":"Lets use ELI5 to understand why a particular entry was selected or rejected"},{"metadata":{"collapsed":true,"_cell_guid":"4af65bec-37e5-4a67-a9b2-fcf1dc3b9679","_uuid":"5c778379583cb7e721d667bef3201ed6a78a5afc","trusted":false},"cell_type":"code","source":"# random entry\nprint(\"Project is Approved?:Actual\",y[100])\nprint(\"Project is Approved?:Predicted prob:\",xgb_pred_train[100])\ndisplay(eli5.show_prediction(model_xgb, doc=all_data_merge_1[100], vec=vectorizer,show_feature_values=True,top=20))    ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"94830976-dfd2-4cd0-9be4-98a5d16a0603","_uuid":"e84f06130ed20468a8948f4e2f2c33f80dc29d4f","trusted":false},"cell_type":"code","source":"# random entry\nprint(\"Project is Approved?:Actual\",y[500])\nprint(\"Project is Approved?:Predicted prob:\",xgb_pred_train[500])\ndisplay(eli5.show_prediction(model_xgb, doc=all_data_merge_1[500], vec=vectorizer,show_feature_values=True,top=20))    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ef00c90a-2a42-4c49-b2c6-c681f920e9dc","_uuid":"f43ecc4767f90291e05879631222ef4c433ff9d7"},"cell_type":"markdown","source":"## Some interesting observations here.\nThe fact that some words(set,ipad,materials) are missing contributes to the model thinking that the project is approved!!  I am not sure, if I am interpreting this correctly here though.\n\n\nAlso,The sentences are cleaned(ie) Stop words removed, Lemmatized.  Need to think of a way to perform the cleaning in the vectorizer itself, to display the original sentence. Do let me know in the comments section if you have any ideas :)\n\n\n"},{"metadata":{"collapsed":true,"_cell_guid":"268f103f-8d8a-4263-97fd-b2e7e9986ffd","_uuid":"e4d7b9b5c3730d4c3f3d0373e8b7aef48b031af7","scrolled":false,"trusted":false},"cell_type":"code","source":"from IPython.display import display\nno_missing = lambda feature_name, feature_value: not np.isnan(feature_value)\nfor i in range(5):\n    print(\"Project is Approved?:Actual\",y[i])\n    print(\"Project is Approved?:Predicted prob:\",xgb_pred_train[i])\n    display(eli5.show_prediction(model_xgb, doc=all_data_merge_1[i], vec=vectorizer,show_feature_values=True,top=30,feature_filter=no_missing))  ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ec2e239e-bca1-43ed-ad1e-cea2e0f5256a","_uuid":"dc45f6a1ee2708b3e8c7069830d1b8244e1e32eb","trusted":false},"cell_type":"code","source":"final_preds=0.4*xgb_pred_test+0.6*test_preds","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b31caed1-07d3-4ac9-9e20-2f9763d7e70b","_uuid":"f5dca11063777ebc1769517e5931963b96ff52f7","trusted":false},"cell_type":"code","source":"# Making submission\nx_preds = pd.DataFrame(final_preds)\nx_preds.columns = ['project_is_approved']\nsub_id=sample_sub['id']\nsubmission = pd.concat([sub_id, x_preds], axis=1)\nsubmission.to_csv('lgbm_xgb_blend.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"42027866-e177-44e1-9569-d2705555a1a7","_uuid":"72cbe52db52cf8cc134eea6c6d47eb38734ca8f8"},"cell_type":"markdown","source":"# Footnotes:\nTo be done:\n* Tune XGB,LGBM better\n* Explore other models\n* Interpret ELi5 output and make changes to the model\n* Explore more on State and Time variables\n* Explore interactions\n* Explore Topic Modeling, text clustering\n* Explore vector based features\n* Add more functionality to text stats, wordcloud functions\n* Add confidence interval to the plotting function\n* Think of a way to perform cleaning in the vectorizer itself , so that ELI5 displays the original sentence instead of the cleaned one\n\n### To be continued....\n\n\n## Do leave an upvote if you liked the content :) \n"},{"metadata":{"collapsed":true,"_cell_guid":"72158ace-3047-4a4d-8ec3-d15190cef21e","_uuid":"4d86c2d0444e2095f30241386f43d7d3c835937d","trusted":false},"cell_type":"code","source":"# To be continued....","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}