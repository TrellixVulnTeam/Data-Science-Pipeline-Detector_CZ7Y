{"cells":[{"metadata":{"_cell_guid":"8438d86a-fa5d-4874-9df4-3009ae179866","_uuid":"549f7d61aaeeff907a1a48696284ff431bcfd9b5"},"cell_type":"markdown","source":"Start off by reading in the necessary inputs for later"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#lgbm is what we choose for this fork\nimport lightgbm as gbm\n\n#let us have training corpus split done for us\nfrom sklearn.model_selection import train_test_split\n\n#to save the model\nfrom sklearn.externals import joblib\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\n# set seed for reproducibility\nnp.random.seed(0) ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ad321379-8722-49b5-a036-319eb42e49e8","_uuid":"56d316d43b4eb2fb1bd9bd5ec48b8b22d73ab120","collapsed":true},"cell_type":"markdown","source":"Read in the data from both our train and test sets together.  We don''t actually need the test set until later"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"# read in our train and resources data that can be used for predictions\ntrain_data = pd.read_csv(\"../input/train.csv\")\nresources_data = pd.read_csv(\"../input/resources.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ddf49635-1069-4245-afee-ca1dbfed3fb9","_uuid":"fa4d1db70ad79f8e3eecf74f88947956377f1729"},"cell_type":"markdown","source":"Let us expand train_data a bit.  In particular, we want to get the month from the project_submitted_datetime field"},{"metadata":{"_cell_guid":"d2683354-95df-4b71-b3ee-83b5e47eb32a","_uuid":"95ec145012c47f36130f9ca95e381168b6dda747","collapsed":true,"trusted":true},"cell_type":"code","source":"train_data['month'] = pd.DatetimeIndex(train_data['project_submitted_datetime']).month\ntrain_data['month'].sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"493db74a-11dc-4c5d-b755-74a469961eef","_uuid":"75b0df7e7b3d62f3d899ad3e38afb8bf945eadea"},"cell_type":"markdown","source":"Now let us combine the train and resources data set into one"},{"metadata":{"_cell_guid":"5e38f2c0-3bda-4ccf-95b5-167f6ef8dfe9","_uuid":"99db8446cb130d795126777152292217d3e312dd","collapsed":true,"trusted":true},"cell_type":"code","source":"full_combined = train_data.merge(resources_data, on='id', how='inner', sort=False)\n# we cannot have multiple rows\n# our current strategy will be to just keep the first row\n# later I would like to try the following:\n# combine descriptions into a single string column and combine price into a sum total of all price expenses\ncombined = full_combined[~full_combined.id.duplicated(keep='first')]\ndufus = full_combined.groupby('id', as_index=False)['price'].sum()\ncombined.loc['price'] = dufus['price']\ndufus2 = full_combined.groupby('id', as_index=False)['quantity'].sum()\ncombined.loc['quantity'] = dufus2['quantity']\ncombined.dropna(subset=['id'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b18aa0c9-fd97-4a44-ace7-ae7e649d1b7e","_uuid":"a1224e9063d2abd4a47f1d397c6b0fafbc28a87f"},"cell_type":"markdown","source":"Take a look at our resources data"},{"metadata":{"_cell_guid":"fd566216-168d-4164-bb5f-c7c9b4344ee1","_uuid":"9bcfe51a52d1d9840107e83cf038a7edbc393f0b","collapsed":true,"trusted":true},"cell_type":"code","source":"#peek at resources\nresources_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"997cb27e-8efb-4fb8-88e5-634d603781ca","_uuid":"2ec2b08acd3d9596c6be34f095a8e06705da4081"},"cell_type":"markdown","source":"Take a look at out combined data to make sure we did it right"},{"metadata":{"_cell_guid":"82c6d8a8-14ab-4601-8473-4f3da55d1b06","_uuid":"e0f0f570c22d05fdf8788bbcf3364c22d52f87ea","collapsed":true,"trusted":true},"cell_type":"code","source":"combined.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bbc813fa-18df-4037-964a-1744e91eb3ca","_uuid":"59441062e3cc04f16fa8015c5e86c2e92e01df12"},"cell_type":"markdown","source":"Are there any missing data points in our data set?"},{"metadata":{"_cell_guid":"7d7523cc-a408-4519-af9e-1336ab9ab0c3","_uuid":"1498b1ca79b4a1c2b14f1cf65a91939687a88dc7","collapsed":true,"trusted":true},"cell_type":"code","source":"# get the number of missing data points per column\nmissing_values_count = combined.isnull().sum()\n\n# look at the # of missing points in the first ten columns sorted descending\nmissing_values_count[:].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7323ad34-c51a-4289-9cc7-5823ef60a01a","_uuid":"963ae994b975fc6c9e69924516f539a312aea464","collapsed":true,"trusted":true},"cell_type":"code","source":"#fill in NAs\ncombined['teacher_number_of_previously_posted_projects'].fillna(0.0, inplace=True)\ncombined['school_state'].fillna('UNK', inplace=True)\ncombined['price'].fillna(0.0, inplace=True)\ncombined['quantity'].fillna(0.0, inplace=True)\n\n#scale previous posted projects\nmin_pp = min(combined['teacher_number_of_previously_posted_projects'])\nmax_pp = max(combined['teacher_number_of_previously_posted_projects'])\ncombined['teacher_number_of_previously_posted_projects'] = (combined['teacher_number_of_previously_posted_projects'] - min_pp) / (min_pp + max_pp)\ncombined['teacher_number_of_previously_posted_projects'].head(5)\n\n#scale price\nmin_price = min(combined['price'])\nmax_price = max(combined['price'])\ncombined['price'] = (combined['price'] - min_price) / (min_price + max_price)\ncombined['price'].head(5)\n\n#scale quantity\nmin_quantity = min(combined['quantity'])\nmax_quantity = max(combined['quantity'])\ncombined['quantity'] = (combined['quantity'] - min_quantity) / (min_quantity + max_quantity)\ncombined['quantity'].head(5)\n\n# school state and month are categorical, the others are continuous.  Oops ...\ntrain_data_exp = combined.join(pd.get_dummies(combined['school_state']))\ntrain_data_expanded = train_data_exp.join(pd.get_dummies(train_data_exp['month']))\n\nprint(train_data_expanded.columns[:])\n\n# now construct the feature vectors for the model\n#X = train_data_expanded[list(train_data_expanded.columns[19:82]) + ['teacher_number_of_previously_posted_projects'] + ['quantity'] + ['price']]\nX = train_data_expanded[list(train_data_expanded.columns[19:82]) + ['teacher_number_of_previously_posted_projects']]\ny = train_data_expanded['project_is_approved']\n\n# lgbm\nparams = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc',\n        'max_depth': 14,\n        'num_leaves': 31,\n        'learning_rate': 0.025,\n        'feature_fraction': 0.85,\n        'bagging_fraction': 0.85,\n        'bagging_freq': 5,\n        'verbose': 0,\n        'num_threads': 4,\n        'lambda_l2': 1.0,\n        'min_gain_to_split': 0\n}  \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nlgb = gbm.train(params,\n                gbm.Dataset(X_train, y_train), \n                valid_sets=[gbm.Dataset(X_test, y_test)],\n                num_boost_round=10000,\n                early_stopping_rounds=100)\n\n# save the model for later use\njoblib.dump(lgb, 'gbm_model_1.pkl') ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5a53bc3d-e391-48a6-8500-261e9655678f","_uuid":"8b253609a7f5b6fd4b261cdb75bfadbbb836f879"},"cell_type":"markdown","source":"And now let us do the prediction since training really does not take that long."},{"metadata":{"_cell_guid":"d8784d33-4500-4da2-b6a9-a86e0280cec9","_uuid":"6a17307e9a67f74a5eedb553c08a960229457614","trusted":true},"cell_type":"code","source":"# read in test and submission data\ntest_data = pd.read_csv(\"../input/test.csv\")\nsubm = pd.read_csv('../input/sample_submission.csv')\n\n# extract month\ntest_data['month'] = pd.DatetimeIndex(test_data['project_submitted_datetime']).month\ntest_data['month'].sample(5)\n\n#merge in supplemental information from resources data file\nfull_combined_test = test_data.merge(resources_data, on='id', how='inner', sort=False)\n# we cannot have multiple rows\n# our current strategy will be to just keep the first row\n# later I would like to try the following:\n# combine descriptions into a single string column and combine price into a sum total of all price expenses\ncombined_test = full_combined_test[~full_combined_test.id.duplicated(keep='first')]\ndufus_test = full_combined_test.groupby('id', as_index=False)['price'].sum()\ncombined_test['price'] = dufus_test['price']\ndufus_test2 = full_combined_test.groupby('id', as_index=False)['quantity'].sum()\ncombined_test['quantity'] = dufus_test2['quantity']\ncombined_test.dropna(subset=['id'], inplace=True)\n\n#fill in NAs\ncombined_test['teacher_number_of_previously_posted_projects'].fillna(0.0, inplace=True)\ncombined_test['school_state'].fillna('UNK', inplace=True)\ncombined_test['price'].fillna(0.0, inplace=True)\ncombined_test['quantity'].fillna(0.0, inplace=True)\n\n#scale previous posted projects\ncombined_test['teacher_number_of_previously_posted_projects'] = (combined_test['teacher_number_of_previously_posted_projects'] - min_pp) / (min_pp + max_pp)\ncombined_test['teacher_number_of_previously_posted_projects'].head(5)\n\n#scale price\ncombined_test['price'] = (combined_test['price'] - min_price) / (min_price + max_price)\ncombined_test['price'].head(5)\n\n#scale quantity\ncombined_test['quantity'] = (combined_test['quantity'] - min_quantity) / (min_quantity + max_quantity)\ncombined_test['quantity'].head(5)\n\n# school state is categorical, the other is continuous.  Oops ...\ntest_data_exp = combined_test.join(pd.get_dummies(test_data['school_state']))\ntest_data_expanded = test_data_exp.join(pd.get_dummies(test_data_exp['month']))\n\n# now construct the feature vector for the svm\n#X_t = test_data_expanded[list(test_data_expanded.columns[18:81]) + ['teacher_number_of_previously_posted_projects'] + ['quantity'] + ['price']]\nX_t = test_data_expanded[list(test_data_expanded.columns[18:81]) + ['teacher_number_of_previously_posted_projects']]\n\npd.set_option('display.max_columns', 500)\nX_t[:18].head(20)\n\n# ok this is fast, right now.  Let us just classify\n#y_t = lgb.predict(X_t, num_iteration=lgb.best_iteration)\n\n#create submission file???\n#submid = pd.DataFrame({'id': subm[\"id\"]})\n#label_cols = ['project_is_approved']\n#submission = pd.concat([submid, pd.DataFrame(y_t, columns = label_cols)], axis=1)\n#submission.to_csv('submission.csv', index=False)","execution_count":1,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}