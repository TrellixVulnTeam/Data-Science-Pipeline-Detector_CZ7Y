{"cells":[{"metadata":{"_cell_guid":"240a43e0-2af6-4e18-9ff8-434bfbc94728","_uuid":"e3f3d4be89198b2482e0145c76723e94585ef4bc"},"cell_type":"markdown","source":"**Word Vectors and Features**\n\nI have been wanting to try out a model that combines word vectors with feature engineering. I also wanted my model to use KFold CV with a validation set within each fold. And, finally, I wanted to build it myself, as much as possible, in order to strengthen my writing and thinking in python.\n\nThis isn't the prettiest kernel but I hope that it's useful for those less proficient than I am. For those farther along, please suggest improvements!\n\nI will add more annotations, explanations, modify parameters and make changes over the course of the competition. Stay tuned..."},{"metadata":{"_cell_guid":"7d164d0a-4f2f-4644-a219-362ede276dad","_uuid":"d9c6df010090ed32d948e1208443927ec34f54c3","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nfrom sklearn import *\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nfrom scipy.sparse import hstack, vstack, csr_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\nimport nltk\nimport re\nimport gc\n\n# This kernel is my attempt to accomplish a personal\n# goal, \"from scratch.\" For a clean example that achieves\n# the same thing (and more), see t35khan's kernel:\n# https://www.kaggle.com/t35khan/tfidf-driven-xgboost","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"6d9644bf-23aa-4aa0-9d99-775599d1467b","_uuid":"949a2fd713e95bfb7b47f61c9aa108dcc0f921d7"},"cell_type":"markdown","source":"**Load the Files**\n\nTwo of the columns in **train** and **test** seem to have values that cause pandas some confusion. I have set them to \"object\" type on import.\n\nWe carve out the target variable, called \"project_is_approved,\" from **train** right away. Drop the target column from **train**.\n\nRead in the sample submission file. We'll use the IDs from that file later.\n\nFinally, fill in NA values for text columns with \"unk\" for \"unknown.\" The string \"unk\" isn't a word but it will add information to your word vectors : )"},{"metadata":{"collapsed":true,"_cell_guid":"8943b5ed-e94f-4494-9275-0f2b7c5f7e77","_uuid":"dda99a56c785ff7488502ca34e77ba426b0d4133","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv', dtype={\"project_essay_3\": object, \"project_essay_4\": object})\ntarget = train['project_is_approved']\ntrain = train.drop('project_is_approved', axis=1)\n\ntest = pd.read_csv('../input/test.csv', dtype={\"project_essay_3\": object, \"project_essay_4\": object})\n\nsub = pd.read_csv('../input/sample_submission.csv')\n\nresources = pd.read_csv('../input/resources.csv')\n\ntrain.fillna(('unk'), inplace=True) \ntest.fillna(('unk'), inplace=True)\n","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"d53d6003-7971-4804-a85b-d594c1b8bcf9","_uuid":"07bf832f2856d5f53ba0818e86c9325924890474"},"cell_type":"markdown","source":"**Feature Engineering**\n\nThe first step in feature engineering is to turn labels, or categorical values, into integers. Generally, XGBoost does better with label encoding than with one hot encoding.\n\n@opanichev provided a great piece of code so let's use it!"},{"metadata":{"collapsed":true,"_cell_guid":"549333f9-fe47-48c8-97b5-e48f7a4be1cb","_uuid":"cd7ed48f5c9c386b05b2f6bd49db32251ef5d9e0","trusted":true},"cell_type":"code","source":"# Feature engineering (before encoding)\n# Thanks to @coronate\n# https://www.kaggle.com/coronate/donorschoose-exploratory-analysis\n# Clever approach to feature engineering but no improvement\n#genderDictionary = {\"Ms.\": \"Female\", \"Mrs.\":\"Female\", \"Mr.\":\"Male\", \"Teacher\":\"Neutral\", \"Dr.\":\"Neutral\", np.nan:\"Neutral\"}\n#train[\"gender\"] = train.teacher_prefix.map(genderDictionary)\n#test[\"gender\"] = test.teacher_prefix.map(genderDictionary)\n\n#titleDictionary = {\"Ms.\": \"Na\", \"Mrs.\":\"Na\", \"Mr.\":\"Na\", \"Teacher\":\"Teacher\", \"Dr.\":\"Dr.\", np.nan:\"Na\"}\n#train[\"title\"] = train.teacher_prefix.map(titleDictionary)\n#test[\"title\"] = test.teacher_prefix.map(titleDictionary)","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"bf5ef5bc-63f8-4368-9990-6206111f2ede","_uuid":"5d783b09e2076ee6c7b545dcd49eeb84c34c22ea","trusted":true},"cell_type":"code","source":"# Thanks to opanichev for saving me a few minutes\n# https://www.kaggle.com/opanichev/lightgbm-and-tf-idf-starter/code\n\n# Label encoding\n\ndf_all = pd.concat([train, test], axis=0)\n\ncols = [\n    'teacher_id', \n    'teacher_prefix', \n    'school_state', \n    'project_grade_category', \n    'project_subject_categories', \n    'project_subject_subcategories'\n]\n\nfor c in tqdm(cols):\n    le = LabelEncoder()\n    le.fit(df_all[c].astype(str))\n    train[c] = le.transform(train[c].astype(str))\n    test[c] = le.transform(test[c].astype(str))\n    \ndel df_all; gc.collect()\n","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"299514bf-44c4-40a0-b9b9-fa3ff47c34c2","_uuid":"3840e1fa90bd3ff84387d72be582ca580c62d0d9"},"cell_type":"markdown","source":"When it comes to feature engineering, get creative! What metrics can we create that may provide more signal to the model than noise?"},{"metadata":{"collapsed":true,"_cell_guid":"3f70f89c-7318-410a-893d-857e03d70e47","_uuid":"28a84aeaa328f11795516340ef8e6b3da9af6ff2","trusted":true},"cell_type":"code","source":"# Feature engineering\n\n# Date and time\ntrain['project_submitted_datetime'] = pd.to_datetime(train['project_submitted_datetime'])\ntest['project_submitted_datetime'] = pd.to_datetime(test['project_submitted_datetime'])\n\n# Date as int may contain some ordinal value\ntrain['datetime_int'] = train['project_submitted_datetime'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\ntest['datetime_int'] = test['project_submitted_datetime'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n\n# Date parts\n\ntrain['datetime_day'] = train['project_submitted_datetime'].dt.day\ntrain['datetime_dow'] = train['project_submitted_datetime'].dt.dayofweek\ntrain['datetime_year'] = train['project_submitted_datetime'].dt.year\ntrain['datetime_month'] = train['project_submitted_datetime'].dt.month\ntrain['datetime_hour'] = train['project_submitted_datetime'].dt.hour\ntrain = train.drop('project_submitted_datetime', axis=1)\n\ntest['datetime_day'] = test['project_submitted_datetime'].dt.day\ntest['datetime_dow'] = test['project_submitted_datetime'].dt.dayofweek\ntest['datetime_year'] = test['project_submitted_datetime'].dt.year\ntest['datetime_month'] = test['project_submitted_datetime'].dt.month\ntest['datetime_hour'] = test['project_submitted_datetime'].dt.hour\ntest = test.drop('project_submitted_datetime', axis=1)\n\n# Essay length\ntrain['e1_length'] = train['project_essay_1'].apply(len)\ntest['e1_length'] = train['project_essay_1'].apply(len)\n\ntrain['e2_length'] = train['project_essay_2'].apply(len)\ntest['e2_length'] = train['project_essay_2'].apply(len)\n\n# Has more than 2 essays?\ntrain['has_gt2_essays'] = train['project_essay_3'].apply(lambda x: 0 if x == 'unk' else 1)\ntest['has_gt2_essays'] = test['project_essay_3'].apply(lambda x: 0 if x == 'unk' else 1)","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"88ce84bf-76e4-42d6-aa3f-9b322db1d2b1","_uuid":"08cc2b151cb54cf85437416596f9df01e89361d6"},"cell_type":"markdown","source":"Let's create some features from the numerical columns in the **resources.csv** file. You could try merging the descriptive text into the content that gets vectorized but we'll leave that behind for now."},{"metadata":{"collapsed":true,"_cell_guid":"70f14197-c383-45a1-bb3a-59c44f0a6952","_uuid":"4560b1d1db5287d123ca452bd9d0776cb5f351ee","trusted":true},"cell_type":"code","source":"# Combine resources file\n# Thanks, the1owl! \n# https://www.kaggle.com/the1owl/the-choice-is-yours\n\nresources['resources_total'] = resources['quantity'] * resources['price']\n\ndfr = resources.groupby(['id'], as_index=False)[['resources_total']].sum()\ntrain = pd.merge(train, dfr, how='left', on='id').fillna(-1)\ntest = pd.merge(test, dfr, how='left', on='id').fillna(-1)\n\ndfr = resources.groupby(['id'], as_index=False)[['resources_total']].mean()\ndfr = dfr.rename(columns={'resources_total':'resources_total_mean'})\ntrain = pd.merge(train, dfr, how='left', on='id').fillna(-1)\ntest = pd.merge(test, dfr, how='left', on='id').fillna(-1)\n\ndfr = resources.groupby(['id'], as_index=False)[['quantity']].count()\ndfr = dfr.rename(columns={'quantity':'resources_quantity_count'})\ntrain = pd.merge(train, dfr, how='left', on='id').fillna(-1)\ntest = pd.merge(test, dfr, how='left', on='id').fillna(-1)\n\ndfr = resources.groupby(['id'], as_index=False)[['quantity']].sum()\ndfr = dfr.rename(columns={'quantity':'resources_quantity_sum'})\ntrain = pd.merge(train, dfr, how='left', on='id').fillna(-1)\ntest = pd.merge(test, dfr, how='left', on='id').fillna(-1)\n\n# We're done with IDs for now\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"e3fd2b0f-93ad-474e-b708-9dfb11b5433b","_uuid":"ce2035b96052c8c1f0351895e8bbdfde4b910b80"},"cell_type":"markdown","source":"Concatenate text columns. This may not be optimal but it is efficient."},{"metadata":{"collapsed":true,"_cell_guid":"1e93bb0b-0cf3-4abd-a5b5-293811a5d148","_uuid":"a7a9b8ef1f952f4fdb055cd7c6386ff191bc1a87","trusted":true},"cell_type":"code","source":"# Thanks to opanichev for saving me a few minutes\n# https://www.kaggle.com/opanichev/lightgbm-and-tf-idf-starter/code\n\ntrain['project_essay'] = train.apply(lambda row: ' '.join([\n    str(row['project_title']),\n    str(row['project_essay_1']), \n    str(row['project_essay_2']), \n    str(row['project_essay_3']),\n    str(row['project_essay_4']),\n    str(row['project_resource_summary'])]), axis=1)\ntest['project_essay'] = test.apply(lambda row: ' '.join([\n    str(row['project_title']),\n    str(row['project_essay_1']), \n    str(row['project_essay_2']), \n    str(row['project_essay_3']),\n    str(row['project_essay_4']),\n    str(row['project_resource_summary'])]), axis=1)\n\ntrain = train.drop([\n    'project_title',\n    'project_essay_1', \n    'project_essay_2', \n    'project_essay_3', \n    'project_essay_4',\n    'project_resource_summary'], axis=1)\ntest = test.drop([\n    'project_title',\n    'project_essay_1', \n    'project_essay_2', \n    'project_essay_3', \n    'project_essay_4',\n    'project_resource_summary'], axis=1)","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"ad46def3-4ded-4f3a-8902-f888173d1ff9","_uuid":"e861f460ac57bf1530f98e0c77ff0fad05b48eb0"},"cell_type":"markdown","source":"How are we doing with our training data transformations and feature engineering?"},{"metadata":{"_cell_guid":"ef61113f-96a6-417a-a6d8-dd7f9e682f57","_uuid":"1c8b131baf69b61118ab521d4fc4a205f5f4b180","trusted":true},"cell_type":"code","source":"train.head()","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"85bf9705-907f-4aae-9340-af0359c7600e","_uuid":"31217a6e11f3d73683cbfb02bc40637b50cb0458"},"cell_type":"markdown","source":"As you can see, our data is now mostly numeric. Let's transform the concatenated text into numbers, too.\n\nNext, we'll clean the text up a bit and [lemmatize](https://en.wikipedia.org/wiki/Lemmatisation) it. This step improves local CV by about .0025."},{"metadata":{"collapsed":true,"_cell_guid":"8251b607-c880-4525-804f-7fd170a5d309","_uuid":"8e2b8b9021add58bcf861fe75e898ba67974754e","trusted":true},"cell_type":"code","source":"w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\ndef prep_text(text):\n    text = text.strip().lower()\n    text = re.sub('\\r','', text) # \\r\\n returns\n    text = re.sub('\\n','', text) # \\r\\n returns\n    text = re.sub('\\W+',' ', text)\n    text = re.sub(' i m ',' i\\'m ', text)\n    text = re.sub('n t ','n\\'t ', text)\n    text = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n    return text\n\ntrain['project_essay'] = train['project_essay'].apply(lambda x: prep_text(x))\ntest['project_essay'] = test['project_essay'].apply(lambda x: prep_text(x))","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"ea044b7e-90cd-4de0-8c64-3ae8eee5f2e7","_uuid":"7d07c0c46e90952501bd93638d627905f737b03d"},"cell_type":"markdown","source":"Here's what our new text objects look like:"},{"metadata":{"_cell_guid":"3d302702-f076-4078-b696-340af48a9911","_uuid":"7885b31dbae07e7ed111eb5a1d19bff64bc33d96","trusted":true},"cell_type":"code","source":"# Note that stop words are handled by the TFIDF vectorzer, below\ntrain['project_essay'][0:20]","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"776b6411-25b9-4ffb-b716-d654ea9200d2","_uuid":"85d6de865281242b237d68eaad5ff556724857dc"},"cell_type":"markdown","source":"I won't try to explain [TFIDF](http://https://en.wikipedia.org/wiki/Tf%E2%80%93idf) or text vectorization in general. Follow the link in the comment below, and the links from the link, to learn more."},{"metadata":{"collapsed":true,"_cell_guid":"3ad22cb4-2f2e-4a3c-8f6f-32cc5be8b7a8","_uuid":"90a710b0be345467a390cda68b7fffd990275b27","trusted":true},"cell_type":"code","source":"# Learn more about NLP from Abishek: \n# https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle\ntfv = TfidfVectorizer(norm='l2', min_df=0,  max_features=8000, \n            strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n            ngram_range=(1,2), use_idf=True, smooth_idf=False, sublinear_tf=True,\n            stop_words = 'english')\n","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"b7ecfb80-bcd7-4bd9-9830-58e49572405f","_uuid":"ce22d85a99409bd38dd6c7c1da371cda98982ccf"},"cell_type":"markdown","source":"Should we fit on **train** and **test** individually, to avoid leakyness? Maybe, but I'm not going to do it that way.\n\nNote that **hstack** is the method by which we combine our engineered features with the TFIDF vectorized text."},{"metadata":{"collapsed":true,"_cell_guid":"f40e8b36-71e4-430f-bfd1-417b16e96e28","_uuid":"533df4ca39188252275308df9505bb6f13e0c52e","trusted":true},"cell_type":"code","source":"train_text = train['project_essay'].apply(lambda x: ' '.join(x))\ntest_text = test['project_essay'].apply(lambda x: ' '.join(x))\n\n# Fitting tfidf on train + test might be leaky\ntfv.fit(list(train_text.values) + list(test_text.values))\ntrain_tfv = tfv.transform(train_text)\ntest_tfv = tfv.transform(test_text)","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"5564c17b-880f-4a8d-a439-1134db39acd2","_uuid":"47bf00303b6609ed19216bbac85f8d44ed06a2fa","trusted":true},"cell_type":"code","source":"# Combine text vectors and features\nfeat_train = train.drop('project_essay', axis=1)\nfeat_test = test.drop('project_essay', axis=1)\n\nfeat_train = csr_matrix(feat_train.values)\nfeat_test = csr_matrix(feat_test.values)\n\nX_train_stack = hstack([feat_train, train_tfv[0:feat_train.shape[0]]])\nX_test_stack = hstack([feat_test, test_tfv[0:feat_test.shape[0]]])\n\nprint('Train shape: ', X_train_stack.shape, '\\n\\nTest Shape: ', X_test_stack.shape)\n\ndel train, test, train_tfv, test_tfv; gc.collect()","execution_count":13,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"38a0efe4-9b62-4897-bcef-1c924d4a4d0d","_uuid":"cd9b67175436211c8fba8c011e8a118a6b9c999c","trusted":true},"cell_type":"code","source":"seed = 28 # Get your own seed","execution_count":14,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"a7f9163e-3906-47bc-9c48-d4c194ac6de2","_uuid":"9020b85ddbde156cf9836e1eafed5794dbdd5fc8","trusted":true},"cell_type":"code","source":"K = 3 # How many folds do you want? \nkf = KFold(n_splits = K, random_state = seed, shuffle = True)","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"3f6b9ac7-2ffc-4941-9974-b42e492a3915","_uuid":"52d6de7a52ee90578ed5716e31d2f78c468ba004"},"cell_type":"markdown","source":"**The XGBoost Model**\n\nWe'll set up arrays to capture our CV scores and the predictions made agains the **test** set. Those will be blended to give us more robust predictions and to prevent overfitting."},{"metadata":{"_cell_guid":"122bed83-d388-414e-b72f-c55b56889ec5","scrolled":true,"_uuid":"fbed60fa0f56c136c14fac948075ecfe458f1ccd","trusted":true},"cell_type":"code","source":"cv_scores = []\nxgb_preds = []\n\nfor train_index, test_index in kf.split(X_train_stack):\n    \n    # Split out a validation set\n    X_train, X_valid, y_train, y_valid = train_test_split(X_train_stack, target, test_size=0.25, random_state=random.seed(seed))\n    \n    # params are tuned with kaggle kernels in mind\n    xgb_params = {'eta': 0.165, \n                  'max_depth': 5, \n                  'subsample': 0.825, \n                  'colsample_bytree': 0.825, \n                  'objective': 'binary:logistic', \n                  'eval_metric': 'auc', \n                  'seed': seed\n                 }\n    \n    d_train = xgb.DMatrix(X_train, y_train)\n    d_valid = xgb.DMatrix(X_valid, y_valid)\n    d_test = xgb.DMatrix(X_test_stack)\n    \n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n    model = xgb.train(xgb_params, d_train, 300, watchlist, verbose_eval=30, early_stopping_rounds=10)\n    cv_scores.append(float(model.attributes()['best_score']))\n    xgb_pred = model.predict(d_test)\n    xgb_preds.append(list(xgb_pred))\n    \n    del X_train, X_valid, y_train, y_valid, d_train, d_valid, d_test; gc.collect()","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"26891a8d-f8b3-4950-b2b6-689f6b35490d","_uuid":"04222b12319b2efe073501714347b1b46acfa9a1"},"cell_type":"markdown","source":"I like to get a look at the CV scores for each fold. Keep track of the average CV score to determine how parameter changes affect your model."},{"metadata":{"_cell_guid":"723fa695-5619-48ea-99b5-b6428af4c941","_uuid":"a0792e65baa56a8c7dee3a47e33d09b055a3ef0d","trusted":true},"cell_type":"code","source":"print(cv_scores)\nprint(np.mean(cv_scores))","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"2f5a97d6-3013-4deb-aefb-c77333314373","_uuid":"fe117f1d94e05d8500a2cfeb1647dfaad52d9008"},"cell_type":"markdown","source":"Blend predictions..."},{"metadata":{"collapsed":true,"_cell_guid":"807cab9c-779a-4b3b-b5f8-17d1f71595f1","_uuid":"4edda3d6c2ff324b872922c55da0fc8923e00e42","trusted":true},"cell_type":"code","source":"x_preds=[]\nfor i in range(len(xgb_preds[0])):\n    sum=0\n    for j in range(K):\n        sum+=xgb_preds[j][i]\n    x_preds.append(sum / K)","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"1dd4ff8a-f88c-4012-bc0b-49cb8cc31735","_uuid":"fcf096952fcc14544662c8730177c4436e58d7b8"},"cell_type":"markdown","source":"... and then take a peek to see how they compare to your last effort or other models. "},{"metadata":{"_cell_guid":"5ea55502-f3c3-495d-aa2a-053cf6f34768","_uuid":"7a0cf4876c5050c1247565cdceb51bcfc246a411","trusted":true},"cell_type":"code","source":"# Peek at first 10 predictions\nx_preds[0:10]","execution_count":19,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"1561e60e-1454-42f5-894f-040e73ee0fbf","_uuid":"8bb230ee417c9d8034133b08641630a1ca72c043","trusted":true},"cell_type":"code","source":"# XGB preds\nx_preds = pd.DataFrame(x_preds)\nx_preds.columns = ['project_is_approved']","execution_count":20,"outputs":[]},{"metadata":{"_cell_guid":"66492ab8-7e49-457a-8ccb-42b4e9892b27","_uuid":"7aa133b970125c06756924bfdc0cad2e405a161f"},"cell_type":"markdown","source":"**Combine and Save**"},{"metadata":{"collapsed":true,"_cell_guid":"08f3441b-46f8-4ed8-8d1c-78eb0d4638c4","_uuid":"f7bd7c1a9701279319cf8b5678f394334a0324d5","trusted":true},"cell_type":"code","source":"submid = sub['id']\n\nsubmission = pd.concat([submid, x_preds], axis=1)\n\nsubmission.to_csv('xgb_submission.csv', index=False)","execution_count":21,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}