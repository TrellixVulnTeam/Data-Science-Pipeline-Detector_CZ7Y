{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":false,"collapsed":true},"cell_type":"code","source":"# Common/Standard packages\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Filepath to main training dataset.\ntrain_file_path = '../input/train.csv'\n\n# Read data and store in DataFrame.\ntrain_data = pd.read_csv(train_file_path, sep=',')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf317f2584ed04639af2448d669b3827b438653a","_cell_guid":"26e96c41-9e6d-4bdd-9a03-b1735d8d84d6"},"cell_type":"markdown","source":"Datasets include:\n* resources.csv - gives id, description, quantity, and price for all items requested in the projects\n* train.csv - inspect this dataset below\n* test.csv - features similar to train.csv, minus the approval outcome (no project_is_approved column)\n* sample_submission.csv - cursory Kaggle example submission\n"},{"metadata":{"_uuid":"17563a667d30f68c0276cfe81d8274cbe7ad9c9b","_cell_guid":"40a857be-b91c-465b-aca4-a46edce2c0e5","trusted":false,"collapsed":true},"cell_type":"code","source":"train_data.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63706046602de1b7b8da3d26ab9a406412aa13ae","_cell_guid":"d448569c-0b81-4ed8-8007-cc19f89570ec"},"cell_type":"markdown","source":"train.csv contains the following variables:\nid\nteacher_id\nteacher_prefix\nschool_state\nproject_submitted_datetime\nproject_grade_category\nproject_subject_categories\nproject_subject_subcategories\nproject_title\nproject_essay_1\nproject_essay_2\nproject_essay_3\nproject_essay_4\nproject_resource_summary\nteacher_number_of_previously_posted_projects\nproject_is_approved\n\nINITIAL OBSERVATIONS\n\n*Domain Knowledge Insights*\n\nI can't help noticing certain limitations in the data based on personal experience trying to get my own DonorsChoose classroom projects approved. Variables that could be significant contributors, but that are not included (some of which may be more difficult to track):\n\n**Match Offer projects** - my projects submitted under match offers were approved more reliably and quickly than standard projects.\n\n**Social Media connections** - most of my donors found my project on social media, so it may be helpful to track what social media accounts teachers have connected to their DonorsChoose accounts and possibly how many friends/followers each account has in all connected accounts, aggregated.\n\n*Data Insights/Ideas*\n\nProject essays 1 through 4 are all in one submission, but not every application essay is broken up into 4 paragraphs. Requirements define a word count, not a paragraph number. May want to explore the size of paragraphs, whether or not projects have 4 paragraphs, etc.\n\nCertain sections begin with uneditable sentence stems. Check to see if these are included as data. Possibly remove this common sentence stem as a possible confounding variable? May be too minimal to matter, but I'm going to say it's worth keeping in mind."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"trusted":false},"cell_type":"code","source":"# Additional packages for these models as suggested by Kaggle\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset\nimport sklearn.metrics as metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1a4c901f908739245cc33949b40eb68365a49a07"},"cell_type":"code","source":"# Define predictor feature(s); start with a simple example with one feature.\nmy_feature_name = 'teacher_number_of_previously_posted_projects'\nmy_feature = train_data[[my_feature_name]]\n\n# Specify the label to predict.\nmy_target_name = 'project_is_approved'\n\n# Prepare training and validation sets.\nN_TRAINING = 160000\nN_VALIDATION = 100000\n\n# Choose examples and targets for training.\ntraining_examples = train_data.head(N_TRAINING)[[my_feature_name]].copy()\ntraining_targets = train_data.head(N_TRAINING)[[my_target_name]].copy()\n\n# Choose examples and targets for validation.\nvalidation_examples = train_data.tail(N_VALIDATION)[[my_feature_name]].copy()\nvalidation_targets = train_data.tail(N_VALIDATION)[[my_target_name]].copy()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7811e5df3be5be04dcf2e84290386314d511500"},"cell_type":"markdown","source":"Datasets API"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7fc7c19506eb1c345b09e8fc90c72710fb4a6a31"},"cell_type":"code","source":"def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n    \"\"\"Trains a linear regression model of one feature.\n  \n    Args:\n      features: pandas DataFrame of features\n      targets: pandas DataFrame of targets\n      batch_size: Size of batches to be passed to the model\n      shuffle: True or False. Whether to shuffle the data.\n      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n    Returns:\n      Tuple of (features, labels) for next data batch\n    \"\"\"\n    \n    # Convert pandas data into a dict of np arrays.\n    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n \n    # Construct a dataset, and configure batching/repeating\n    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n    ds = ds.batch(batch_size).repeat(num_epochs)\n    \n    # Shuffle the data, if specified\n    if shuffle:\n      # Shuffle with a buffer size of 10000\n      ds = ds.shuffle(10000)\n    \n    # Return the next batch of data\n    features, labels = ds.make_one_shot_iterator().get_next()\n    return features, labels","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8af3edc594feebde3fa5e54907fc501e2bc43ac8"},"cell_type":"markdown","source":"Linear Classifier"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"daa34a96261aff3a7ada77bc84e90bfb4f039dcf"},"cell_type":"code","source":"# Learning rate for training.\nlearning_rate = 0.00001\n\n# Function for constructing feature columns from input features\ndef construct_feature_columns(input_features):\n  \"\"\"Construct the TensorFlow Feature Columns.\n  Args:\n    input_features: The names of the numerical input features to use.\n  Returns:\n    A set of feature columns\n  \"\"\"\n  return set([tf.feature_column.numeric_column(my_feature)\n              for my_feature in input_features])\n\n# Create a linear classifier object.\nmy_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n# Set a clipping ratio of 5.0\nmy_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)  \nlinear_classifier = tf.estimator.LinearClassifier(\n    feature_columns=construct_feature_columns(training_examples),\n    optimizer=my_optimizer\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56ab664c60eabdf91a48cbf49c73f4eade717d62"},"cell_type":"markdown","source":"Create input functions for training the model, predicting on the prediction data, and predicting on the validation data:"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"863c4998e353ed5b98c5d450cc42337a2ea6808d"},"cell_type":"code","source":"batch_size = 10\n\n# Create input function for training\ntraining_input_fn = lambda: my_input_fn(training_examples, \n                                        training_targets[my_target_name],\n                                        batch_size=batch_size)\n\n# Create input function for predicting on training data\npredict_training_input_fn = lambda: my_input_fn(training_examples,\n                                                training_targets[my_target_name],\n                                                num_epochs=1, \n                                                shuffle=False)\n\n# Create input function for predicting on validation data\npredict_validation_input_fn = lambda: my_input_fn(validation_examples,\n                                                  validation_targets[my_target_name],\n                                                  num_epochs=1, \n                                                  shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56fe9fbc4f06ddf218cc3a0b46e560d7ab150310"},"cell_type":"markdown","source":"Training the model"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"14ea2b41200a249889d93ad963b36e615f79c7a9"},"cell_type":"code","source":"# Train for 200 steps\nlinear_classifier.train(\n  input_fn=training_input_fn,\n  steps=200\n)\n\n# Compute predictions.    \ntraining_probabilities = linear_classifier.predict(\n    input_fn=predict_training_input_fn)\ntraining_probabilities = np.array(\n      [item['probabilities'] for item in training_probabilities])\n    \nvalidation_probabilities = linear_classifier.predict(\n    input_fn=predict_validation_input_fn)\nvalidation_probabilities = np.array(\n    [item['probabilities'] for item in validation_probabilities])\n    \ntraining_log_loss = metrics.log_loss(\n    training_targets, training_probabilities)\nvalidation_log_loss = metrics.log_loss(\n    validation_targets, validation_probabilities)\n  \n# Print the training and validation log loss.\nprint(\"Training Loss: %0.2f\" % training_log_loss)\nprint(\"Validation Loss: %0.2f\" % validation_log_loss)\n\nauc = metrics.auc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07395b62afe4b67ee9b5bae7f2d645ccd772add8"},"cell_type":"markdown","source":"Next, let's calculate the AUC (area under the curve), which is the metric this competition uses to assess the accuracy of prediction. This may take a few minutes. When calculation is complete, the training and validation AUC values will be output:"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"266b4d28774c6a746c2299c40d2f8f328410a98a"},"cell_type":"code","source":"training_metrics = linear_classifier.evaluate(input_fn=predict_training_input_fn)\nvalidation_metrics = linear_classifier.evaluate(input_fn=predict_validation_input_fn)\n\nprint(\"AUC on the training set: %0.2f\" % training_metrics['auc'])\nprint(\"AUC on the validation set: %0.2f\" % validation_metrics['auc'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83aea0b3ebfeeff1049c7b0aa345d38b8eaa768c"},"cell_type":"markdown","source":"We've achieved AUC values of 0.56, which is slightly better than random. This is a good start, but can you improve the model to achieve better results?\n\nWhat to Try Next\nA couple ideas for model refinements you can try to see if you can improve model accuracy:\n\nTry adjusting the learning_rate and steps hyperparameters on the existing model.\nTry adding some text features to the model, such as the content of the project essays (project_essay_1, project_essay_2, project_essay_3, project_essay_4). You may want to try building a vocabulary from these strings; see the Machine Learning Crash Course Intro to Sparse Data and Embeddings exercise for some practice on working with text data and vocabularies.\n\nHERE ENDS \"Getting Started\" CODE SNAG"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}