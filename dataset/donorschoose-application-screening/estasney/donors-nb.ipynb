{"cells":[{"metadata":{"_cell_guid":"63a2d98f-c295-4923-9560-1a15d344b31f","_uuid":"14fc719320660db08112d331bb4cd1a7aaf24aff","collapsed":true,"trusted":true},"cell_type":"code","source":"import logging\nimport multiprocessing\nfrom operator import itemgetter\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom gensim.parsing.preprocessing import preprocess_string\nfrom nltk import word_tokenize\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn_pandas import DataFrameMapper\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\nONLINE = True\nTOY = False\n\nif ONLINE:\n    TRAIN_FILE = \"../input/train.csv\"\n    RESOURCE_FILE = \"../input/resources.csv\"\n    TEST_FILE = \"../input/test.csv\"\n    WORKERS = 32\n\nelse:\n    TRAIN_FILE = \"train.csv\"\n    RESOURCE_FILE = \"resources.csv\"\n    TEST_FILE = \"test.csv\"\n    WORKERS = 2\n\ndef _apply_df(args):\n    df, func, num, kwargs = args\n    return num, df.apply(func, **kwargs)\n\n\ndef apply_by_multiprocessing(df, func, **kwargs):\n    workers = kwargs.pop('workers')\n    pool = multiprocessing.Pool(processes=workers)\n    result = pool.map(_apply_df, [(d, func, i, kwargs) for i, d in enumerate(np.array_split(df, workers))])\n    pool.close()\n    result = sorted(result, key=lambda x: x[0])\n    return pd.concat([i[1] for i in result])\n\n\ndef tokenize(x):\n    return word_tokenize(x)\n\n\ndef count_punctuation(tokens, punctuation_char):\n    return len([token for token in tokens if token == punctuation_char])\n\n\ndef preprocess_df(df, workers):\n    if __name__ == \"__main__\":\n        dfr = pd.read_csv(RESOURCE_FILE)\n        dfr['total'] = dfr['price'] * dfr['quantity']\n        dfr['has_zero'] = dfr['price'].apply(lambda x: 1 if x == 0 else 0)\n        dfr = dfr.groupby('id').agg('sum').reset_index()\n        \n        # merging essays\n        df['student_description'] = df['project_essay_1']\n        df.loc[df.project_essay_3.notnull(), 'student_description'] = df.loc[\n                                                                          df.project_essay_3.notnull(), 'project_essay_1'] + \\\n                                                                      df.loc[\n                                                                          df.project_essay_3.notnull(), 'project_essay_2']\n        df['project_description'] = df['project_essay_2']\n\n        df.loc[df.project_essay_3.notnull(), 'project_description'] = df.loc[\n                                                                          df.project_essay_3.notnull(), 'project_essay_3'] + \\\n                                                                      df.loc[\n                                                                          df.project_essay_3.notnull(), 'project_essay_4']\n\n        df['project_subject_categories'] = df['project_subject_categories'].apply(lambda x: x.split(\", \"))\n        df['project_subject_subcategories'] = df['project_subject_subcategories'].apply(lambda x: x.split(\", \"))\n        df['teacher_prefix'] = df['teacher_prefix'].fillna('None')\n        df = df.merge(dfr, how='inner', on='id')\n\n        df['student_tokens'] = apply_by_multiprocessing(df['student_description'], tokenize, workers=workers)\n        df['student_word_count'] = df['student_tokens'].apply(lambda x: len(x))\n        df['student_unique_words'] = df['student_tokens'].apply(lambda x: len(set(x)))\n        df['student_n_periods'] = df['student_tokens'].apply(lambda x: count_punctuation(x, '.'))\n        df['student_n_commas'] = df['student_tokens'].apply(lambda x: count_punctuation(x, ','))\n        df['student_n_questions'] = df['student_tokens'].apply(lambda x: count_punctuation(x, '?'))\n        df['student_n_exclamations'] = df['student_tokens'].apply(lambda x: count_punctuation(x, '!'))\n        df['student_word_len'] = df['student_tokens'].apply(lambda x: np.mean([len(token) for token in x]))\n\n        del (df['student_tokens'])\n\n        df['project_tokens'] = apply_by_multiprocessing(df['project_description'], tokenize, workers=workers)\n        df['project_word_count'] = df['project_tokens'].apply(lambda x: len(x))\n        df['project_unique_words'] = df['project_tokens'].apply(lambda x: len(set(x)))\n\n        df['project_n_periods'] = df['project_tokens'].apply(lambda x: count_punctuation(x, '.'))\n        df['project_n_commas'] = df['project_tokens'].apply(lambda x: count_punctuation(x, ','))\n        df['project_n_questions'] = df['project_tokens'].apply(lambda x: count_punctuation(x, '?'))\n        df['project_n_exclamations'] = df['project_tokens'].apply(lambda x: count_punctuation(x, '!'))\n        df['project_word_len'] = df['project_tokens'].apply(lambda x: np.mean([len(token) for token in x]))\n        del (df['project_tokens'])\n        del (df['project_essay_1'])\n        del (df['project_essay_2'])\n        del (df['project_essay_3'])\n        del (df['project_essay_4'])\n        return df\n\nif __name__ == \"__main__\":\n\n    train = pd.read_csv(TRAIN_FILE)\n    if TOY is True:\n        train = train.sample(frac=.1)\n    print(\"Read Train\")\n\n    train = preprocess_df(train, workers=WORKERS)\n\n    print(\"Complete Preprocessing\")\n    \n    from sklearn.decomposition import PCA\n    from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\n\n    raw_X = train\n    raw_y = train['project_is_approved'].values\n\n    X_train, X_test, y_train, y_test = train_test_split(raw_X, raw_y, test_size=0.2)\n\n    mapper = DataFrameMapper([\n        (['teacher_number_of_previously_posted_projects'], StandardScaler()),\n        (['project_grade_category'], LabelBinarizer()),\n        (['student_word_count', 'project_word_count', 'student_n_periods', 'student_n_commas', 'student_n_exclamations', 'student_n_periods', 'project_n_periods', 'project_n_commas',\n         'project_n_questions', 'project_n_exclamations'], PCA(1)),\n        (['total', 'price', 'quantity'], PCA(1)),\n        (['has_zero'], StandardScaler()),\n        ('student_description',\n         [TfidfVectorizer(use_idf=True, ngram_range=(1, 2), stop_words='english'),\n          NMF(n_components=20)]),\n        ('project_description',\n         [TfidfVectorizer(use_idf=True, ngram_range=(1, 2), stop_words='english'),\n          NMF(n_components=20)]),\n    ], sparse=True)\n\n    X_train = mapper.fit_transform(X_train)\n    X_test = mapper.transform(X_test)\n\n    print(\"Mapping Complete\")\n    \n    X_trainR, y_trainR = X_train, y_train\n\n    xgb_params = {'eta': 0.2,\n                  'max_depth': 10,\n                  'max_delta_step': 6,\n                  'subsample': 0.8,\n                  'colsample_bytree': 0.8,\n                  'objective': 'binary:logistic',\n                  'eval_metric': 'auc'\n                  }\n\n    d_train = xgb.DMatrix(X_trainR, y_trainR)\n    d_test = xgb.DMatrix(X_test, y_test)\n\n    watchlist = [(d_train, 'train'), (d_test, 'valid')]\n    model_xgb = xgb.train(xgb_params, d_train, 1000, watchlist, verbose_eval=5, early_stopping_rounds=25)\n    \n    df_test = pd.read_csv(TEST_FILE)\n    df_test = preprocess_df(df_test, WORKERS)\n    X_test_actual = mapper.transform(df_test)\n    X_test_actual = xgb.DMatrix(X_test_actual)\n    y_pred_actual = model_xgb.predict(X_test_actual)\n    my_submission = pd.DataFrame({'id': df_test.id, 'project_is_approved': y_pred_actual})\n    my_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4733956f-2be7-4dd9-bb7e-2a7e749b0b06","_uuid":"48a05f64514dda957335df9a618173b19898a365","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"81028b93-9354-48fa-822a-3597a5d0854c","_uuid":"2e99ca469d5040082c9d9b35c8a0864412dfad96","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"my_submission","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}