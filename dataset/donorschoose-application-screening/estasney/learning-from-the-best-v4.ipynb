{"cells":[{"metadata":{"_uuid":"33b57620d31efc0f5ab667019a6dcf38ba62d138","collapsed":true,"_cell_guid":"92cfb4b9-096c-4c7f-b8b8-0cf1e42a4208","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom operator import itemgetter\nfrom gensim.parsing.preprocessing import preprocess_string\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom gensim.models.phrases import Phrases\n\nfrom xgboost import XGBRegressor\n\nfrom sklearn_pandas import DataFrameMapper\n\nfrom sklearn.preprocessing import QuantileTransformer, LabelBinarizer, StandardScaler, MinMaxScaler, RobustScaler, MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics import accuracy_score, explained_variance_score, r2_score\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = [8, 8]\n\nfrom nltk.tag import pos_tag\nfrom nltk import word_tokenize\n\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"61e866614e13afbd0b3b7da908709c11338eb613","collapsed":true,"_cell_guid":"28e3dd51-a4c8-45b9-90bc-7be8d0473783","trusted":true},"cell_type":"code","source":"f = \"../input/train.csv\"\n# f = \"train.csv\"\nfr = \"../input/resources.csv\"\ntrain = pd.read_csv(f)\ndfr = pd.read_csv(fr)","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"67e07f225cbd450ff204aca73fcc6d8c30c4d551","collapsed":true,"_cell_guid":"18ab5a08-9670-4746-a60b-8019812f7d6a","trusted":true},"cell_type":"code","source":"import multiprocessing\n\ndef _apply_df(args):\n    df, func, num, kwargs = args\n    return num, df.apply(func, **kwargs)\n\ndef apply_by_multiprocessing(df,func,**kwargs):\n    workers=kwargs.pop('workers')\n    pool = multiprocessing.Pool(processes=workers)\n    result = pool.map(_apply_df, [(d, func, i, kwargs) for i,d in enumerate(np.array_split(df, workers))])  \n    pool.close()\n    result=sorted(result,key=lambda x:x[0])\n    return pd.concat([i[1] for i in result])\n\ndef tokenize(x):\n    return word_tokenize(x)\n\ndef count_punctuation(tokens, punctuation_char):\n             return len([token for token in tokens if token == punctuation_char])\n  ","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"8c8d127b0ce6f757614bec267321bdfe514d7e13","collapsed":true,"_cell_guid":"efaa3ead-07b8-4b34-b0ce-182ce3e17df6","trusted":true},"cell_type":"code","source":"def preprocess_df(df, workers):\n    if __name__ == \"__main__\":\n        dfr = pd.read_csv(fr)\n        dfr['total'] = dfr['price'] * dfr['quantity']\n        dfr['has_zero'] = dfr['price'].apply(lambda x: 1 if x==0 else 0)\n        dfr = dfr.groupby('id').agg('sum').reset_index()\n\n        # merging essays\n        df['student_description']=df['project_essay_1']\n        df.loc[df.project_essay_3.notnull(),'student_description']=df.loc[df.project_essay_3.notnull(),'project_essay_1']+df.loc[df.project_essay_3.notnull(),'project_essay_2']\n        df['project_description']=df['project_essay_2']\n\n        df.loc[df.project_essay_3.notnull(),'project_description']=df.loc[df.project_essay_3.notnull(),'project_essay_3']+df.loc[df.project_essay_3.notnull(),'project_essay_4']\n\n        df['project_subject_categories'] = df['project_subject_categories'].apply(lambda x: x.split(\", \"))\n        df['project_subject_subcategories'] = df['project_subject_subcategories'].apply(lambda x: x.split(\", \"))\n        df['teacher_prefix'] = df['teacher_prefix'].fillna('None')\n        df = df.merge(dfr, how='inner', on='id')\n\n\n        df['student_tokens'] = apply_by_multiprocessing(df['student_description'], tokenize, workers=workers)\n        df['student_word_count'] = df['student_tokens'].apply(lambda x: len(x))\n        df['student_unique_words'] = df['student_tokens'].apply(lambda x: len(set(x)))\n        df['student_n_periods'] = df['student_tokens'].apply(lambda x: count_punctuation(x, '.'))\n        df['student_n_commas'] = df['student_tokens'].apply(lambda x: count_punctuation(x, ','))\n        df['student_n_questions'] = df['student_tokens'].apply(lambda x: count_punctuation(x, '?'))\n        df['student_n_exclamations'] = df['student_tokens'].apply(lambda x: count_punctuation(x, '!'))\n        df['student_word_len'] = df['student_tokens'].apply(lambda x: np.mean([len(token) for token in x]))\n        \n        del(df['student_tokens'])\n    \n        df['project_tokens'] = apply_by_multiprocessing(df['project_description'], tokenize, workers=workers)\n        df['project_word_count'] = df['project_tokens'].apply(lambda x: len(x))\n        df['project_unique_words'] = df['project_tokens'].apply(lambda x: len(set(x)))\n\n        \n        \n        df['project_n_periods'] = df['project_tokens'].apply(lambda x: count_punctuation(x, '.'))\n        df['project_n_commas'] = df['project_tokens'].apply(lambda x: count_punctuation(x, ','))\n        df['project_n_questions'] = df['project_tokens'].apply(lambda x: count_punctuation(x, '?'))\n        df['project_n_exclamations'] = df['project_tokens'].apply(lambda x: count_punctuation(x, '!'))\n        df['project_word_len'] = df['project_tokens'].apply(lambda x: np.mean([len(token) for token in x]))\n        del(df['project_tokens'])\n        del(df['project_essay_1'])\n        del(df['project_essay_2'])\n        del(df['project_essay_3'])\n        del(df['project_essay_4'])\n        return df","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"2947a83275eb941d8c36174ba0a265f068f75c32","_cell_guid":"e2072e97-1840-4ce0-a197-c169351ee312","trusted":true},"cell_type":"code","source":"%%time\ntrain = preprocess_df(train, 32)","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"78bc05b561c1b9e8d0cee1f1c528b054a1027849","collapsed":true,"_cell_guid":"1c510f46-402e-4fb0-97c2-5c825947fa30","trusted":true},"cell_type":"code","source":"from gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom gensim.models.phrases import Phrases\n\ndef read_corpus(df, tokens_only=False):\n    for i, row in df.iterrows():\n        tag = row.project_is_approved\n        docs = [row.student_description, row.project_description]\n        docs = [preprocess_string(doc) for doc in docs]\n        if tokens_only:\n            for doc in docs:\n                yield doc\n        else:\n            for doc in docs:\n                yield TaggedDocument(doc, [tag])\n                # For training data, add tags\n\nclass DocStreamer(object):\n    def __init__(self, df):\n        self.df = df\n    \n    def __iter__(self):\n        for i, row in self.df.iterrows():\n            tag = row.project_is_approved\n            docs = [row.student_description, row.project_description]\n            docs = [preprocess_string(doc) for doc in docs]\n            for doc in docs:\n                yield TaggedDocument(doc, [\"{}-{}\".format(tag, i)])\n","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"3781d889568306841536dbcdfc33cd91decfb35f","_cell_guid":"441be143-f758-4687-93d3-6e7fc190a617","trusted":true},"cell_type":"code","source":"%%time\n\n# Better results when tags are equal?\nnot_approved = train.loc[train['project_is_approved']==0]\nn_na = len(not_approved)\napproved = train.loc[train['project_is_approved']==1]\napproved = approved.sample(n_na)\n\ndocs_df = pd.concat([approved, not_approved], axis=0, copy=True)\n\n# Shuffles the dataframe\ndocs = DocStreamer(docs_df.sample(frac=1))\ndoc_model = Doc2Vec(docs, vector_size=200, window=5, min_count=5, workers=32, epochs=1)\ndel docs\ndel not_approved\ndel approved","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"66186c394291d4603ec75adc3430d5e82a838b49","collapsed":true,"_cell_guid":"d88196df-3f16-4fc7-b705-5fa64f8b9d94","trusted":true},"cell_type":"code","source":"def doc2vec_classify(text, model, binary=False):\n    doc = preprocess_string(text)\n    inf_vector = model.infer_vector(doc)\n    doc_sims = model.docvecs.most_similar([inf_vector])\n    if binary:\n        # Return most similar class, i.e. 0, 1\n        doc_sims = sorted(doc_sims, key=itemgetter(1), reverse=True)\n        return int(doc_sims[0][0].split(\"-\")[0])\n    else:\n        # Return similarity to project_is_accepted\n        return len([tag for tag, score in doc_model.docvecs.most_similar([inf_vector]) if tag.startswith('1')]) / 10","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"3aeff81e096efbb2525fedb155f37383613ed397","_cell_guid":"c3bcb59c-17cd-4d91-bd5d-507f32b650df","trusted":true,"collapsed":true},"cell_type":"code","source":"%%time\nif __name__ == \"__main__\":\n    train['student_description_sim'] = apply_by_multiprocessing(train['student_description'], doc2vec_classify, model=doc_model, binary=False, workers=16)\n    train['project_description_sim'] = apply_by_multiprocessing(train['student_description'], doc2vec_classify, model=doc_model, binary=False, workers=16)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0bcf2d6e9289bc2668d07e720c0b5b7f20da4c80","collapsed":true,"_cell_guid":"5e14d915-9bf7-4568-9700-99694a15d1ea","trusted":false},"cell_type":"code","source":"%%time\nraw_X = train\nraw_y = train['project_is_approved'].values\n\nX_train, X_test, y_train, y_test = train_test_split(raw_X, raw_y, test_size=0.2)\n\nmapper = DataFrameMapper([\n    (['teacher_number_of_previously_posted_projects'], StandardScaler()),\n    (['student_word_count'], StandardScaler()),\n    (['project_word_count'], StandardScaler()),\n    (['student_n_periods'], StandardScaler()),\n    (['student_n_commas'], StandardScaler()),\n    (['student_n_questions'], StandardScaler()),\n    (['student_n_exclamations'], StandardScaler()),\n    (['project_n_periods'], StandardScaler()),\n    (['project_n_commas'], StandardScaler()),\n    (['project_n_questions'], StandardScaler()),\n    (['project_n_exclamations'], StandardScaler()),\n    (['total'], StandardScaler()),\n    (['quantity'], StandardScaler()),\n    ('student_description', [TfidfVectorizer(use_idf=True, ngram_range=(1,2), stop_words='english', max_features=10000),\n                                                      NMF(n_components=20)]),\n    ('project_description', [TfidfVectorizer(use_idf=True, ngram_range=(1,2), stop_words='english', max_features=10000),\n                                                      NMF(n_components=20)]),\n    (['student_description_sim'], StandardScaler()),\n    (['project_description_sim'], StandardScaler()),\n], sparse=True)\n\nX_train = mapper.fit_transform(X_train)\nX_test = mapper.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f62305b84645821116add6688dcdec83ab44ac97","collapsed":true,"_cell_guid":"c9f3bd98-12b6-4608-a68f-cd9fad76f541","trusted":false},"cell_type":"code","source":"from imblearn.under_sampling import NeighbourhoodCleaningRule\nsampler = NeighbourhoodCleaningRule(ratio='majority', n_jobs=32)\n# sampler = RandomUnderSampler()\n\nfrom collections import Counter\nCounter(y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f09b1fb0a391e5720ef0975b1aa534488fcc8911","collapsed":true,"_cell_guid":"f736952d-23d5-4327-8b70-b4d7d7572292","trusted":false},"cell_type":"code","source":"%%time\n# X_trainR, y_trainR = sampler.fit_sample(X_train, y_train)\nX_trainR, y_trainR = X_train, y_train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8eccca675424526c456e70a62470014bd9309995","collapsed":true,"_cell_guid":"4d49d6cf-0b96-490a-b02f-d2232e96d6e2","trusted":false},"cell_type":"code","source":"# The scaled data\nCounter(y_trainR)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1f118a5f3bf74cf5ae88ebf2ad9900fc957c749","collapsed":true,"_cell_guid":"bd8f5efa-3f54-4e80-ae69-9b52b15fb195","trusted":false},"cell_type":"code","source":"import xgboost as xgb\n\nxgb_params = {'eta': 0.001, \n                  'max_depth': 8,\n                  'max_delta_step': 6,\n                  'subsample': 0.8, \n                  'colsample_bytree': 0.8, \n                  'objective': 'binary:logistic', \n                  'eval_metric': 'auc'\n                  }\n\nd_train = xgb.DMatrix(X_trainR, y_trainR)\nd_test = xgb.DMatrix(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21ccf1c8d1802fbd87ce102d625a9e5b3fffc1dd","collapsed":true,"_cell_guid":"ba5cb251-ac90-433d-8fc3-d517d250875f","trusted":false},"cell_type":"code","source":"watchlist = [(d_train, 'train'), (d_test, 'valid')]\nmodel_xgb = xgb.train(xgb_params, d_train, 500, watchlist, verbose_eval=50, early_stopping_rounds=25)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91396b0b5775dd0ee113cae490a35f27f2f2991d","collapsed":true,"_cell_guid":"ada038a0-0f10-4a61-bde1-bf51a774584f","trusted":false},"cell_type":"code","source":"test = pd.read_csv(\"../input/test.csv\")\ntest = preprocess_df(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"798aa40b7a34ff7a1532e00a5ed1e081a7ee6027","collapsed":true,"_cell_guid":"0017913a-2a98-4a48-b8eb-880a2218f4e1","trusted":false},"cell_type":"code","source":"if __name__ == \"__main__\":\n    test['project_description_sim'] = apply_by_multiprocessing(test['project_description'], doc2vec_classify, model=doc_model, binary=False, workers=32)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87896c109a8f902b7a543ecd39be46fe76f9aff5","collapsed":true,"_cell_guid":"ed51e178-6122-4199-a4de-f33a352ab3df","trusted":false},"cell_type":"code","source":"if __name__ == \"__main__\":\n    test['project_description_bin'] = apply_by_multiprocessing(test['project_description'], doc2vec_classify, model=doc_model, binary=True, workers=32)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0f485723b9a993b0a52314d45567f43f092be97","collapsed":true,"_cell_guid":"c50c2254-9683-49d6-93a7-5d5b5df0924c","trusted":false},"cell_type":"code","source":"if __name__ == \"__main__\":\n    test['student_description_sim'] = apply_by_multiprocessing(test['student_description'], doc2vec_classify, model=doc_model, binary=False, workers=32)    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1621cad8a7de00d897a3f5803c00805b764e1b16","collapsed":true,"_cell_guid":"6273eba8-7357-4feb-864d-7bd796331018","trusted":false},"cell_type":"code","source":"if __name__ == \"__main__\":\n    test['student_description_bin'] = apply_by_multiprocessing(test['student_description'], doc2vec_classify, model=doc_model, binary=True, workers=32)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e27bae0224a183e8b19d1f73def6f6f7b3db66b","collapsed":true,"_cell_guid":"9e3e38fa-14b8-4d8c-9bb8-6f6f4b447d99","trusted":false},"cell_type":"code","source":"X_test_actual = mapper.transform(test)\ny_pred_actual = model_xgb.predict(X_test_actual)\nmy_submission = pd.DataFrame({'id': df_test.id, 'project_is_approved': y_pred_actual})\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f04ff9b25bfdb34458eabbeedd062e82401a374b","collapsed":true,"_cell_guid":"7f9c14c7-bf5e-4c8b-b423-0d230a1e59fe","trusted":false},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/test.csv\")\ndf_test = preprocess_df(df_test, 32)\n\nX_test_actual = mapper.transform(df_test)\ny_pred_actual = random_search.predict(X_test_actual)\n\nmy_submission = pd.DataFrame({'id': df_test.id, 'project_is_approved': y_pred_actual})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}