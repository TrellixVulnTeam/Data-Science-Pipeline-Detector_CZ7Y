{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport os\nfrom imblearn.over_sampling import SMOTE\n\n\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom gensim.models import Word2Vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/donorschoose-application-screening/train.csv',nrows=70000)\ntest=pd.read_csv('/kaggle/input/donorschoose-application-screening/test.csv')\nresources=pd.read_csv('/kaggle/input/donorschoose-application-screening/resources.csv')\nsubmissions=pd.read_csv('/kaggle/input/donorschoose-application-screening/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing three null values as i will be using teacher prefix\ntrain=train.loc[train.iloc[:,3].notnull()]\nprint(train.shape)\ntrain.drop(['project_essay_3','project_essay_4'],inplace=True,axis=1)\ntest.drop(['project_essay_3','project_essay_4'],inplace=True,axis=1)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.merge(train,resources.groupby('id').agg({'quantity':'sum','price':'sum'}),on='id',how='left')\ntest=pd.merge(test,resources.groupby('id').agg({'quantity':'sum','price':'sum'}),on='id',how='left')\ntrain.dropna(inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"shape of train data:\",train.shape)\nprint(\"*\"*50)\nprint(\"shape of test data\",test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.value_counts(train.project_is_approved)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"class_0=train.loc[train['project_is_approved']==0,][:16000]\nclass_1=train.loc[train['project_is_approved']==1,][:16000]\ntotal=pd.concat([class_0,class_1],axis=0)\ntrain=total.sample(frac=1)\ntrain.head()\npd.value_counts(total.project_is_approved)"},{"metadata":{"trusted":true},"cell_type":"code","source":"y=train.project_is_approved\ntrain.drop(['project_is_approved'],inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test=train_test_split(train,y,test_size=0.2,stratify=y ,random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train\nprint(test.shape)\npd.value_counts(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#under sampling\ntotal=pd.concat([x_train,y_train],axis=1)\nclass_1= total.loc[total['project_is_approved']==1].sample(13234)\nclass_0= total.loc[total['project_is_approved']==0][:13234]\nnew_df=pd.concat([class_0,class_1],axis=0)\nprint(pd.value_counts(new_df.project_is_approved))\nnew_df=new_df.sample(frac=1)\ny_train=new_df['project_is_approved']\nnew_df.drop(['project_is_approved'],inplace=True,axis=1)\nx_train=new_df"},{"metadata":{"trusted":true},"cell_type":"code","source":"#extracting only dates from datetime as datetime would lead to lots of unique data points\nx_train.project_submitted_datetime=pd.to_datetime(x_train.project_submitted_datetime).dt.date\nx_train.rename(columns={'project_submitted_datetime':'project_submitted_date'})\nprint(\"training done!\")\n\nx_test.project_submitted_datetime=pd.to_datetime(x_test.project_submitted_datetime).dt.date\nx_test.rename(columns={'project_submitted_datetime':'project_submitted_date'})\nprint(\"testing done!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#subjectlist\ndef clean_categories(dataframe,column):\n    my_list=list(column)\n    sublist=[]\n    for i in my_list:\n        temp=\"\"\n        for j in i.split(','):\n            #print(j)\n            j=j.replace(\" \",'')\n            temp+=j.strip()+\" \"\n            temp=temp.replace('&','_')\n        sublist.append(temp)\n    dataframe['clean_categories']=sublist\n    dataframe.drop(['project_subject_categories'],inplace=True,axis=1)\n    return sublist\n\n\nta=clean_categories(x_train,x_train.project_subject_categories)\nmy_subs=[j for i in ta for j in i.rstrip().split(\" \")]\nsubject_dict=dict( Counter(my_subs))\n\nclean_categories( x_test,x_test.project_subject_categories)\nclean_categories( test,test.project_subject_categories)\n\n\nprint('done!')\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=subject_dict.keys() ,lowercase=False, binary=True)\ncategories_one_hot_train = vectorizer.fit_transform(x_train.clean_categories.values)\ncategories_one_hot_test = vectorizer.transform(x_test.clean_categories.values)\ncategories_test_data=vectorizer.transform(test.clean_categories.values)\nprint(vectorizer.get_feature_names())\nprint(\"Shape of matrix after one hot encodig \",categories_one_hot_train.shape)\ncategories_one_hot_train.toarray()[1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#subject subcategories\ndef clean_subcategories(dataframe,column):\n    my_list=list(column)\n    subj_sub_list=[]\n    for i in my_list:\n        temp=\"\"\n        for j in i.split(','):\n            #print(j)\n            j=j.replace(\" \",'')\n            temp+=j.strip()+\" \"\n            temp=temp.replace('&','_')\n        subj_sub_list.append(temp)\n    dataframe['clean_subcategories']=subj_sub_list\n    dataframe.drop(['project_subject_subcategories'],inplace=True,axis=1)\n    return subj_sub_list\n\n    \n\nsubj_sub_list=clean_subcategories(x_train,x_train.project_subject_subcategories)\nmy_subs=[j for i in subj_sub_list for j in i.rstrip().split(\" \")]\nsubject_sub_dict=dict( Counter(my_subs))\n\nclean_subcategories( x_test,x_test.project_subject_subcategories)\nclean_subcategories( test,test.project_subject_subcategories)\nprint('done!')\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=subject_sub_dict.keys() ,lowercase=False, binary=True)\nsubcategories_one_hot_train = vectorizer.fit_transform(x_train.clean_subcategories.values)\nsubcategories_one_hot_test = vectorizer.transform(x_test.clean_subcategories.values)\nsubcategories_test_data = vectorizer.transform(test.clean_subcategories.values)\nprint(vectorizer.get_feature_names())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#teacher prefix\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder=OneHotEncoder(handle_unknown='ignore')\nonehot_teacherprefix_train=encoder.fit_transform(x_train.iloc[:,2].values.reshape(-1,1))\nonehot_teacherprefix_test=encoder.transform(x_test.iloc[:,2].values.reshape(-1,1))\nonehot_teacherprefix_actual_test=encoder.transform(test.iloc[:,2].values.reshape(-1,1))\nonehot_teacherprefix_test.toarray()[0]"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.iloc[1,3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encoding schoolstate for all three datasets\nfrom sklearn.preprocessing import OneHotEncoder\nencoder=OneHotEncoder(handle_unknown='ignore')\nonehot_schoolstate_train=encoder.fit_transform(x_train.iloc[:,3].values.reshape(-1,1))\nonehot_schoolstate_test=encoder.transform(x_test.iloc[:,3].values.reshape(-1,1))\nonehot_schoolstate_actual_test=encoder.transform(test.iloc[:,3].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#grade prefix sort of a label encoding\nx_train['project_grade_encoded']=x_train.project_grade_category.map({'Grades PreK-2':0, 'Grades 6-8':2, 'Grades 3-5':1,'Grades 9-12':3})\nx_test['project_grade_encoded']=x_test.project_grade_category.map({'Grades PreK-2':0, 'Grades 6-8':2, 'Grades 3-5':1,'Grades 9-12':3})\ntest['project_grade_encoded']=x_test.project_grade_category.map({'Grades PreK-2':0, 'Grades 6-8':2, 'Grades 3-5':1,'Grades 9-12':3})\n\n#label_grade=encoder.transform(train.project_grade_category)\n\n#label_grade.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]\n\nfrom tqdm import tqdm\ndef get_essay(train):\n    train['essay']=train[\"project_essay_1\"].map(str) + train[\"project_essay_2\"].map(str) \n# tqdm is for printing the status bar\ndef preprocessing_essay(column):\n    preprocessed_essays = []\n    for sentance in tqdm(column.values):\n        sent = decontracted(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n        preprocessed_essays.append(sent.lower().strip())\n    return preprocessed_essays\nget_essay(x_train)\npreprocessed_essays_train=preprocessing_essay(x_train.essay)\n\nget_essay(x_test)\npreprocessed_essays_test=preprocessing_essay(x_test.essay)\n\nget_essay(test)\npreprocessed_essays_actual_test=preprocessing_essay(test.essay)\n \n\nprint('done!')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n#vectorizer = TfidfVectorizer(max_features=5000)\n#text_tfidf_train = vectorizer.fit_transform(preprocessed_essays_train)\n#text_tfidf_test = vectorizer.transform(preprocessed_essays_test)\n#print(\"Shape of matrix after one hot encodig \",text_tfidf_cv.shape)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=[]\nfor i in tqdm(preprocessed_essays_train):\n    corpus.append(i.split())\ncorpus\n\nimport gensim\n\nw2v_essay=Word2Vec(corpus,size=300,workers=3)\n#w2v_essay=gensim.models.Word2Vec.load(\"w2v_essay\")\nw2v_essay.save(\"w2v_essay\")\ndel corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_w2v_vectors_essay_train = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(preprocessed_essays_train): # for each review/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if word in w2v_essay.wv.vocab.keys():\n            vector += w2v_essay[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector /= cnt_words\n    avg_w2v_vectors_essay_train.append(vector)\n    \n\n    \n    #Average w2vec for test data\navg_w2v_vectors_essay_test = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(preprocessed_essays_test): # for each review/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if word in w2v_essay.wv.vocab.keys():\n            vector += w2v_essay[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector /= cnt_words\n    avg_w2v_vectors_essay_test.append(vector)\n    \navg_w2v_vectors_essay_actual_test = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(preprocessed_essays_actual_test): # for each review/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if word in w2v_essay.wv.vocab.keys():\n            vector += w2v_essay[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector /= cnt_words\n    avg_w2v_vectors_essay_actual_test.append(vector)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#processing project title with TF_IDF\ndef get_title(column):\n    preprocessed_title = []\n    for sentance in tqdm(column.values):\n        sent = decontracted(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n        preprocessed_title.append(sent.lower().strip())\n    return preprocessed_title\n\npreprocessed_title_train=get_title(x_train.project_title)\n\npreprocessed_title_test=get_title(x_test.project_title)\n\npreprocessed_title_actual_test=get_title(test.project_title)\n\n\n#from sklearn.feature_extraction.text import TfidfVectorizer\n#vectorizer = TfidfVectorizer(max_features=2000)\n#title_tfidf_train = vectorizer.fit_transform(preprocessed_title_train)\n#title_tfidf_test = vectorizer.transform(preprocessed_title_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=[]\nfor i in preprocessed_title_train:\n    corpus.append(i.split())\ncorpus\n\n\n#w2v_title=gensim.models.Word2Vec.load(\"w2v_title\")\nw2v_title=Word2Vec(corpus,size=300,workers=3)\nw2v_title.save(\"w2v_title\")\ndel corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_w2v_vectors_title_train = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(preprocessed_title_train): # for each review/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if word in w2v_title.wv.vocab.keys():\n            vector += w2v_title[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector /= cnt_words\n    avg_w2v_vectors_title_train.append(vector)\n    \n    #Average w2vec for test data\navg_w2v_vectors_title_test = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(preprocessed_title_test): # for each review/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if word in w2v_title.wv.vocab.keys():\n            vector += w2v_title[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector /= cnt_words\n    avg_w2v_vectors_title_test.append(vector)\n\navg_w2v_vectors_title_actual_test = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(preprocessed_title_actual_test): # for each review/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if word in w2v_title.wv.vocab.keys():\n            vector += w2v_title[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector /= cnt_words\n    avg_w2v_vectors_title_actual_test.append(vector)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scaling the numerical variables: price  \nfrom sklearn.preprocessing import StandardScaler\nprice=StandardScaler()\nprice_train=price.fit_transform(x_train['price'].values.reshape(-1,1))\nprice_test=price.transform(x_test['price'].values.reshape(-1,1))\nprice_actual_test=price.transform(test['price'].values.reshape(-1,1))\nprint(\"done!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scaling the numerical variables: price  \nfrom sklearn.preprocessing import StandardScaler\nprice=StandardScaler()\npreviously_posted_projects_train=price.fit_transform(x_train['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\npreviously_posted_projects_test=price.transform(x_test['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\npreviously_posted_projects_actual_test=price.transform(test['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\n\nprint(\"done!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scaling the numerical variables: price  \nfrom sklearn.preprocessing import StandardScaler\nprice=StandardScaler()\nquantity_train=price.fit_transform(x_train['quantity'].values.reshape(-1,1))\nquantity_test=price.transform(x_test['quantity'].values.reshape(-1,1))\nquantity_actual_test=price.transform(test['quantity'].values.reshape(-1,1))\n\nprint(\"done!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.project_grade_encoded.shape\nprint(quantity_actual_test.shape)\nprint(previously_posted_projects_actual_test.shape)\nprint(price_actual_test.shape)\nprint(subcategories_test_data.shape)\nprint(categories_test_data.shape)\nprint(onehot_schoolstate_actual_test.shape)\n#print(onehot_teacherprefix_actual_test.shape)\nprint(np.asarray( avg_w2v_vectors_essay_actual_test).shape)\nprint(np.asarray(avg_w2v_vectors_essay_actual_test).shape)\nprint(pd.DataFrame(test.project_grade_encoded).shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#since all the variables are done now it's time to stack them Average word2vec\nfrom scipy.sparse import hstack\nprint(quantity_train.shape)\nprint(previously_posted_projects_train.shape)\nprint(price_train.shape)\nprint(subcategories_one_hot_train.shape)\nprint(categories_one_hot_train.shape)\nprint(onehot_schoolstate_train.shape)\n#print(onehot_teacherprefix_train.shape)\nprint(np.asarray( avg_w2v_vectors_essay_train).shape)\nprint(np.asarray(avg_w2v_vectors_essay_train).shape)\nprint(pd.DataFrame(x_train.project_grade_encoded).shape)\nx_tr_w2v=hstack((pd.DataFrame(x_train.project_grade_encoded),np.asarray(avg_w2v_vectors_essay_train),np.asarray(avg_w2v_vectors_title_train),quantity_train,previously_posted_projects_train,price_train,subcategories_one_hot_train,categories_one_hot_train,onehot_schoolstate_train)).tocsr()\nx_te_w2v=hstack((pd.DataFrame(x_test.project_grade_encoded),np.asarray(avg_w2v_vectors_essay_test),np.asarray(avg_w2v_vectors_title_test),quantity_test,previously_posted_projects_test,price_test,subcategories_one_hot_test,categories_one_hot_test,onehot_schoolstate_test)).tocsr()\nx_real_test=hstack((pd.DataFrame(test.project_grade_encoded),np.asarray(avg_w2v_vectors_essay_actual_test),np.asarray(avg_w2v_vectors_title_actual_test),quantity_actual_test,previously_posted_projects_actual_test,price_actual_test,subcategories_test_data,categories_test_data,onehot_schoolstate_actual_test)).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_tr_w2v.shape)\nprint(x_te_w2v.shape)\nprint(x_real_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"total=pd.DataFrame( x_tr.todense()).join(y_train, how='inner')\nprint(pd.value_counts(total1.project_is_approved))\nclass_1= total.loc[total['project_is_approved']==1].sample(6882)\nclass_0= total.loc[total['project_is_approved']==0][:6882]\nnew_df=pd.concat([class_0,class_1],axis=0)\nnew_df=new_df.sample(frac=1)\nnew_y_train=new_df['project_is_approved']\nnew_df.drop(['project_is_approved'],inplace=True,axis=1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.value_counts(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train1=pd.DataFrame(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Activation,Dropout\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\n\nn_inputs = x_tr_w2v.shape[1]\n\nmodel = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, )),\n    Dropout(.5),\n\n    Dense(256, activation='relu'),\n    Dropout(.5),\n   \n    Dense(1, activation='sigmoid')\n])\nmodel.summary()\nmodel.compile(Adam(lr=0.001),loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x_tr_w2v, y_train,validation_split=0.1 ,batch_size=256,class_weight={0:3.2,1:.86}, epochs=3,verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport numpy as np\n#y_pred = regressor.predict(x_cv)\ncv_pred=np.argmax(model.predict_proba(x_te_w2v),axis=1)\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(model.predict_classes(x_te_w2v),y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\ncv_pred=np.argmax(model.predict(x_te_w2v),axis=1)\nprint(\"Train confusion matrix\")\nprint(classification_report(model.predict_classes(x_te_w2v),y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npd.value_counts(y_test)\nmodel.predict_proba(x_te_w2v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver=\"sag\", max_iter=200)\nlr.fit(x_tr_w2v,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\ncv_pred=np.argmax(model.predict_proba(x_te_w2v),axis=1)\nprint(\"Train confusion matrix\")\nprint(classification_report(y_test,cv_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds=model.predict_proba(x_real_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds\n#pd.DataFrame(model.predict_proba(x_real_test)[:, 1],columns=[\"project_is_approved\"],index=test.id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm = pd.DataFrame()\nsubm['id'] = test.id\nsubm['project_is_approved'] = preds\nsubm.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.value_counts(subm.project_is_approved)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":1}