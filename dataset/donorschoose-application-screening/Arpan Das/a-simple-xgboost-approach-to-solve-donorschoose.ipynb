{"cells":[{"metadata":{"colab_type":"text","id":"-Ab-X-IrCyl0"},"cell_type":"markdown","source":"# A Simple XGBoost Approach to solve DonorsChoose Application Screening"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing nesessary Libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","execution_count":187,"outputs":[]},{"metadata":{"colab_type":"text","id":"MjknLZQqCymQ"},"cell_type":"markdown","source":"## Reading Data"},{"metadata":{"colab":{},"colab_type":"code","id":"abtcqBS8CymT","trusted":true},"cell_type":"code","source":"# Reading Data\nproject_data = pd.read_csv('../input/train.csv')\nresource_data = pd.read_csv('../input/resources.csv')\ntest_data=pd.read_csv('../input/test.csv')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"merging two dataframers in order to prepare train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging two dataframes \n\nprice_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\nprice_data.head(2)\nproject_data = pd.merge(project_data, price_data, on='id', how='left')\ntest_data=pd.merge(test_data,price_data,on='id',how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now Let's take a first look of our data\n"},{"metadata":{"colab":{},"colab_type":"code","id":"Prf2ghjDCymX","outputId":"226c1b3c-35c9-48bd-afc9-7a819c871e8d","trusted":true},"cell_type":"code","source":"# Data Overview\n\nprint(\"Number of data points in train data\", project_data.shape)\nprint('-'*50)\nprint(\"The attributes of data :\", project_data.columns.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have 'project_submitted_datetime available' with us we are sorting the data by date of project submission"},{"metadata":{"colab":{},"colab_type":"code","id":"BTGMDQoPCymc","outputId":"5b3c47b3-d5d0-416b-b512-a59b77f60f0b","trusted":true},"cell_type":"code","source":"\ncols = ['Date' if x=='project_submitted_datetime' else x for x in list(project_data.columns)]\nproject_data['Date'] = pd.to_datetime(project_data['project_submitted_datetime'])\nproject_data.drop('project_submitted_datetime', axis=1, inplace=True)\nproject_data.sort_values(by=['Date'], inplace=True)\nproject_data = project_data[cols]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['Date' if x=='project_submitted_datetime' else x for x in list(test_data.columns)]\ntest_data['Date'] = pd.to_datetime(test_data['project_submitted_datetime'])\ntest_data.drop('project_submitted_datetime', axis=1, inplace=True)\ntest_data.sort_values(by=['Date'], inplace=True)\ntest_data = test_data[cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we are merging essay columns in to one to preprocess easily"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nproject_data[\"essay\"] = project_data[\"project_essay_1\"].map(str) +\\\n                        project_data[\"project_essay_2\"].map(str) + \\\n                        project_data[\"project_essay_3\"].map(str) + \\\n                        project_data[\"project_essay_4\"].map(str)\nproject_data.drop(['project_essay_1'], axis=1, inplace=True)\nproject_data.drop(['project_essay_2'], axis=1, inplace=True)\nproject_data.drop(['project_essay_3'], axis=1, inplace=True)\nproject_data.drop(['project_essay_4'], axis=1, inplace=True)\n\ntest_data[\"essay\"] = test_data[\"project_essay_1\"].map(str) +\\\n                        test_data[\"project_essay_2\"].map(str) + \\\n                        test_data[\"project_essay_3\"].map(str) + \\\n                        test_data[\"project_essay_4\"].map(str)\ntest_data.drop(['project_essay_1'], axis=1, inplace=True)\ntest_data.drop(['project_essay_2'], axis=1, inplace=True)\ntest_data.drop(['project_essay_3'], axis=1, inplace=True)\ntest_data.drop(['project_essay_4'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we need to check for NaN values in our data , as it may lead to inconsistancy "},{"metadata":{"trusted":true},"cell_type":"code","source":"project_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we are getting 4 'Na' values in teacher prefix column. let's replace those with 'undefined'"},{"metadata":{"trusted":true},"cell_type":"code","source":"project_data.fillna(value='undefined',inplace=True)\ntest_data.fillna(value='undefined',inplace=True)\nproject_data.isna().sum()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Spliting Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = project_data['project_is_approved'].values\nproject_data.drop(['project_is_approved'], axis=1, inplace=True)\nproject_data.head(1)\nx=project_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's split our data in to Train and CV to cross validate performance of our model before actual submission "},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_cv,y_train,y_cv=train_test_split(x,y,test_size=0.33,stratify=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"colab_type":"text","id":"6WZaYhwJCymp"},"cell_type":"markdown","source":"## preprocessing of `project_subject_categories`"},{"metadata":{"colab":{},"colab_type":"code","id":"Mdkhq7PRCymr","trusted":true},"cell_type":"code","source":"# Function to Pre Process project subject Categories\n\ndef clean_categories(df,col='project_subject_categories'):\n    catogories = list(df[col].values)\n    cat_list = []\n    for i in catogories:\n        temp = \"\"\n        for j in i.split(','): \n            if 'The' in j.split(): \n                j=j.replace('The','') \n            j = j.replace(' ','') \n            temp+=j.strip()+\" \" \n            temp = temp.replace('&','_')\n        cat_list.append(temp.strip())\n    \n    df['clean_categories'] = cat_list\n    df.drop([col], axis=1, inplace=True)\n\n    from collections import Counter\n    my_counter = Counter()\n    for word in df['clean_categories'].values:\n        my_counter.update(word.split())\n\n    cat_dict = dict(my_counter)\n    sorted_cat_dict = dict(sorted(cat_dict.items(), key=lambda kv: kv[1]))\n    return sorted_cat_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_dict_key_x_train=clean_categories(x_train)\nsorted_dict_key_x_cv=clean_categories(x_cv)\nsorted_dict_key_test=clean_categories(test_data)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"386yx3T2Cymv"},"cell_type":"markdown","source":"## preprocessing of `project_subject_subcategories`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to Pre Process project subject Sub Categories\n\ndef clean_subcategories(df,col='project_subject_subcategories'):\n    catogories = list(df[col].values)\n    sub_cat_list = []\n    for i in catogories:\n        temp = \"\"\n        for j in i.split(','): \n            if 'The' in j.split(): \n                j=j.replace('The','')\n            j = j.replace(' ','') \n            temp+=j.strip()+\" \" \n            temp = temp.replace('&','_') \n        sub_cat_list.append(temp.strip())\n    \n    df['clean_subcategories'] = sub_cat_list\n    df.drop([col], axis=1, inplace=True)\n\n    from collections import Counter\n    my_counter = Counter()\n    for word in df['clean_subcategories'].values:\n        my_counter.update(word.split())\n\n    sub_cat_dict = dict(my_counter)\n    sorted_sub_cat_dict = dict(sorted(sub_cat_dict.items(), key=lambda kv: kv[1]))\n    return sorted_sub_cat_dict","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"4QSP0r8XCymw","trusted":true},"cell_type":"code","source":"sorted_sub_dict_key_x_train=clean_subcategories(x_train)\nsorted_sub_dict_key_x_cv=clean_subcategories(x_cv)\nsorted_sub_dict_key_test=clean_subcategories(test_data)\n","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"NANzhWlLCynN"},"cell_type":"markdown","source":"## Text preprocessing"},{"metadata":{"colab":{},"colab_type":"code","id":"Yqj4vGVoCynh","trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/a/47091490/4084039\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"hhyPw-8wCyny","trusted":true},"cell_type":"code","source":"# https://gist.github.com/sebleier/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing Essay Column\nfrom nltk.stem.snowball import SnowballStemmer\n#from tqdm import tqdm_notebook as tqdm\nstemmer=SnowballStemmer('english')\ndef preprocess_essay(data):\n    preprocessed_data=[]\n    for sentance in (data.values):\n        sent = decontracted(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        # https://gist.github.com/sebleier/554280\n        sent=' '.join(stemmer.stem(word) for word in sent.split() if word not in stopwords)\n        preprocessed_data.append(sent.lower().strip())\n    return preprocessed_data\n\npreprocessed_essays_x_train=preprocess_essay(x_train['essay'])\npreprocessed_essays_x_cv=preprocess_essay(x_cv['essay'])\npreprocessed_essays_test=preprocess_essay(test_data['essay'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing project title column\n\ndef preprocess_title(data):\n    preprocessed_data=[]\n    for sentance in (data.values):\n        sent = decontracted(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        sent=' '.join(stemmer.stem(word) for word in sent.split() if word not in stopwords)\n        preprocessed_data.append(sent.lower().strip())\n    return preprocessed_data\n\npreprocessed_title_x_train=preprocess_title(x_train['project_title'])\npreprocessed_title_x_cv=preprocess_title(x_cv['project_title'])\npreprocessed_title_test=preprocess_title(test_data['project_title'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Project Grade Category"},{"metadata":{"trusted":true},"cell_type":"code","source":"# data overview of project grade category\nproject_data['project_grade_category'].tail(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing Grade\n\ndef preprocess_grade(data):\n    preprocessed_data=[]\n    for sentence in (data.values):\n        sentence=sentence.replace('Grades','')\n        sentence=sentence.replace('-','to')\n        preprocessed_data.append(sentence)\n    return preprocessed_data\n    \npreprocessed_grade_x_train=preprocess_grade(x_train['project_grade_category'])\npreprocessed_grade_x_cv=preprocess_grade(x_cv['project_grade_category'])\npreprocessed_grade_test=preprocess_grade(test_data['project_grade_category'])\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train['clean_grade']=preprocessed_grade_x_train\nx_cv['clean_grade']=preprocessed_grade_x_cv\ntest_data['clean_grade']=preprocessed_grade_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.columns","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"0d0QeeQ-CyoD"},"cell_type":"markdown","source":"## Vectorizing Categorical data"},{"metadata":{},"cell_type":"markdown","source":"As we are using Tree based Classifier we can not use One Hot Encoding to vectorize our text data ,instead we will use Label Encoding of Sckit Learn . It also comes with the limitation that it can not handle efficiently if a new category occurs at test data that was not seen in train data. To remove that probability we will consolidate the Train , Cross Validation and Test Data in order to get all categorical values."},{"metadata":{"trusted":true},"cell_type":"code","source":"consolidated=pd.concat([x_train,x_cv,test_data],axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoder\ncols = [\n    'teacher_prefix', \n    'school_state', \n    'clean_categories', \n    'clean_subcategories', \n    'clean_grade'\n]\n\nfor c in cols:\n    le = LabelEncoder()\n    le.fit(consolidated[c])\n    x_train[c] = le.transform(x_train[c].astype(str))\n    x_cv[c] = le.transform(x_cv[c].astype(str))\n    test_data[c] = le.transform(test_data[c].astype(str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding categorical features of Train Data\ncat_encoded_x_train=x_train['clean_categories'].values.reshape(-1,1)\nsub_cat_encoded_x_train=x_train['clean_subcategories'].values.reshape(-1,1)\nstate_encoded_x_train=x_train['school_state'].values.reshape(-1,1)\nprefix_encoded_x_train=x_train['teacher_prefix'].values.reshape(-1,1)\ngrade_encoded_x_train=x_train['clean_grade'].values.reshape(-1,1)\nprint(cat_encoded_x_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding categorical features of Cross Validation Data\ncat_encoded_x_cv=x_cv['clean_categories'].values.reshape(-1,1)\nsub_cat_encoded_x_cv=x_cv['clean_subcategories'].values.reshape(-1,1)\nstate_encoded_x_cv=x_cv['school_state'].values.reshape(-1,1)\nprefix_encoded_x_cv=x_cv['teacher_prefix'].values.reshape(-1,1)\ngrade_encoded_x_cv=x_cv['clean_grade'].values.reshape(-1,1)\nprint(cat_encoded_x_cv.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding categorical features of Test Data\ncat_encoded_test=test_data['clean_categories'].values.reshape(-1,1)\nsub_cat_encoded_test=test_data['clean_subcategories'].values.reshape(-1,1)\nstate_encoded_test=test_data['school_state'].values.reshape(-1,1)\nprefix_encoded_test=test_data['teacher_prefix'].values.reshape(-1,1)\ngrade_encoded_test=test_data['clean_grade'].values.reshape(-1,1)\nprint(cat_encoded_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"5YnkzKnmCyoN"},"cell_type":"markdown","source":"## Vectorizing Text data"},{"metadata":{"colab_type":"text","id":"gK_SHRpTCyol"},"cell_type":"markdown","source":"#### TFIDF vectorizer"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We will use TF-IDF Vectorizer to vectorize our text data"},{"metadata":{"colab":{},"colab_type":"code","id":"l0gzc2iwCyoo","outputId":"3ada03da-5eec-4a16-c7bd-915d1c9352ae","trusted":true},"cell_type":"code","source":"# TFIDF Encoding of essay text\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer=TfidfVectorizer(min_df=10,max_features=5000)\nvectorizer.fit(preprocessed_essays_x_train)\nX_train_essay_tfidf = vectorizer.transform(preprocessed_essays_x_train)\nX_cv_essay_tfidf = vectorizer.transform(preprocessed_essays_x_cv)\ntest_essay_tfidf = vectorizer.transform(preprocessed_essays_test)\nprint(\"After vectorizations\")\nprint(X_train_essay_tfidf.shape, y_train.shape)\nprint(X_cv_essay_tfidf.shape, y_cv.shape)\nprint(test_essay_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TFIDF encoding of project_title ,We are considering only the words which appeared in at least 10 documents(rows or projects).\nvectorizer=TfidfVectorizer(min_df=10)\nvectorizer.fit(preprocessed_title_x_train)\nX_train_title_tfidf = vectorizer.transform(preprocessed_title_x_train)\nX_cv_title_tfidf = vectorizer.transform(preprocessed_title_x_cv)\ntest_title_tfidf = vectorizer.transform(preprocessed_title_test)\nprint(\"After vectorizations\")\nprint(X_train_title_tfidf.shape, y_train.shape)\nprint(X_cv_title_tfidf.shape, y_cv.shape)\nprint(test_title_tfidf.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"6Em6Kb2-CypR"},"cell_type":"markdown","source":"### Vectorizing Numerical features"},{"metadata":{"colab":{},"colab_type":"code","id":"owkbYbowCypV","trusted":true},"cell_type":"code","source":"# Normalizing price\nfrom sklearn.preprocessing import Normalizer\nnormalizer = Normalizer()\nnormalizer.fit(x_train['price'].values.reshape(-1,1))\n\nX_train_price_norm = normalizer.transform(x_train['price'].values.reshape(-1,1))\nX_cv_price_norm = normalizer.transform(x_cv['price'].values.reshape(-1,1))\ntest_price_norm = normalizer.transform(test_data['price'].values.reshape(-1,1))\nprint(\"After vectorizations\")\nprint(X_train_price_norm.shape, y_train.shape)\nprint(X_cv_price_norm.shape, y_cv.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalizing teacher_number_of_previously_posted_projects\nnormalizer = Normalizer()\nnormalizer.fit(x_train['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\nX_train_teacher_number_of_previously_posted_projects_norm = normalizer.transform(x_train['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\nX_cv_teacher_number_of_previously_posted_projects_norm = normalizer.transform(x_cv['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\ntest_teacher_number_of_previously_posted_projects_norm = normalizer.transform(test_data['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\nprint(\"After vectorizations\")\nprint(X_train_teacher_number_of_previously_posted_projects_norm.shape, y_train.shape)\nprint(X_cv_teacher_number_of_previously_posted_projects_norm.shape, y_cv.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"5UbaJH21Cypa"},"cell_type":"markdown","source":" ### Merging all the above features"},{"metadata":{},"cell_type":"markdown","source":"Let's merge categorical,Text and Numerical Data to prepare final the evaluation data matrixes"},{"metadata":{"colab":{},"colab_type":"code","id":"J7uuEmryCype","outputId":"b0360c1d-592a-4bd7-b8c7-a20d91219fa8","trusted":true},"cell_type":"code","source":"from scipy.sparse import hstack\nx_train=hstack((cat_encoded_x_train,sub_cat_encoded_x_train,state_encoded_x_train,prefix_encoded_x_train,grade_encoded_x_train,X_train_title_tfidf,\nX_train_essay_tfidf,X_train_price_norm,X_train_teacher_number_of_previously_posted_projects_norm)).tocsr()\nx_cv=hstack((cat_encoded_x_cv,sub_cat_encoded_x_cv,state_encoded_x_cv,prefix_encoded_x_cv,grade_encoded_x_cv,X_cv_title_tfidf,\nX_cv_essay_tfidf,X_cv_price_norm,X_cv_teacher_number_of_previously_posted_projects_norm)).tocsr()\ntest_eval=hstack((cat_encoded_test,sub_cat_encoded_test,state_encoded_test,prefix_encoded_test,grade_encoded_test,test_title_tfidf,\ntest_essay_tfidf,test_price_norm,test_teacher_number_of_previously_posted_projects_norm)).tocsr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take a look at the shapes of our final matrixes"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_cv.shape)\nprint(y_cv.shape)\nprint(test_eval.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost Classifier"},{"metadata":{},"cell_type":"markdown","source":"Here we will use XGBoost Classifier which is a Sckit Learn Wrapper for original XGBoost algorithm . For simplicity let's take 1000 estimators with learning rate as 0.01 .For better result we can use KFoldCrossValidation using GridSearch."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trainning XGBoost Model\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_curve\n\nclf = XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.01,   \n                      n_estimators=1000, \n                      reg_alpha = 0.3,\n                      max_depth=5, \n                      )\nclf.fit(x_train,y_train)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting Train and AUC Scores using our model\ny_pred_train=clf.predict(x_train)\ntrain_fpr,train_tpr,train_threshold=roc_curve(y_pred_train,y_train)\ny_pred_cv=clf.predict(x_cv)\ncv_fpr,cv_tpr,cv_threshold=roc_curve(y_pred_cv,y_cv)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot results obtained from the model \nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(cv_fpr, cv_tpr, label=\"CV AUC =\"+str(auc(cv_fpr, cv_tpr)))\nplt.grid()\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('FPR vs TPR')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\ntest_pred=clf.predict_proba(test_eval)[:,1]\nsubmission['id'] = test_data['id']\nsubmission['project_is_approved'] = test_pred\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Endnotes:"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The main intention of this kernel is to get familier with the XGBoost algorithm with simple featurization techniques .I am adding some interesting reads  on the topic :\n\n[1] https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ \n\n[2] https://stats.stackexchange.com/questions/173390/gradient-boosting-tree-vs-random-forest"}],"metadata":{"colab":{"collapsed_sections":["0d0QeeQ-CyoD","5YnkzKnmCyoN","W544CoFtCyoN","gK_SHRpTCyol","YHwGesZUCyo1","_s3QN_ZNCypD","B1dIm8PiCypw","s85gPOAbCyp1","oq3NW2CbCyp3","TJLjbsNYCyp5"],"name":"3_DonorsChoose_KNN.ipynb","provenance":[],"version":"0.3.2"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}