{"cells":[{"metadata":{"_cell_guid":"d3b04218-0413-4e6c-8751-5d8a404d73a9","_uuid":"0bca9739b82d5d51e1229243e03ea1b6db35c17e"},"cell_type":"markdown","source":"## Introduction\n\nThis kernel shows how to use NBSVM (Naive Bayes - Support Vector Machine) to create (at the very least) a strong baseline for the [DonorsChoose.org Application Screening](http://https://www.kaggle.com/c/donorschoose-application-screening) playground competition. NBSVM was introduced by Sida Wang and Chris Manning in the paper [Baselines and Bigrams: Simple, Good Sentiment and Topic Classiﬁcation](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf). However we have to use sklearn's logistic regression in this kernel instead of SVM, although in practice the two are nearly identical (sklearn uses the liblinear library behind the scenes).\n\nIf you're not familiar with [Naive Bayes](http://https://en.wikipedia.org/wiki/Naive_Bayes_classifier) or [Bag-of-words matrices](http://https://en.wikipedia.org/wiki/Bag-of-words_model), another competitor made a preview available of one of fast.ai's upcoming *Practical Machine Learning* course videos, which [introduces this topic](https://youtu.be/37sFIak42Sc?t=3745)."},{"metadata":{"_cell_guid":"ef06cd19-66b6-46bc-bf45-184e12d3f7d4","_uuid":"cca038ca9424a3f66e10262fc9129de807b5f855","collapsed":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"a494f561-0c2f-4a38-8973-6b60c22da357","_kg_hide-output":true,"_uuid":"f70ebe669fcf6b434c595cf6fb7a76120bf7809c","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsubm = pd.read_csv('../input/sample_submission.csv')","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"3996a226-e1ca-4aa8-b39f-6524d4dadb07","_uuid":"2c18461316f17d1d323b1959c8eb4e5448e8a44e"},"cell_type":"markdown","source":"## Quickly Exploring the Data\n\nThe training data contains a row per application, with an id, the text of the essays, and 1 binary label that we'll try to predict."},{"metadata":{"_cell_guid":"5ddb337b-c9b2-4fec-9652-cb26769dc3c6","_uuid":"5f5269c56ea6ded273881b0d4dcdb6af83a3e089","scrolled":true,"trusted":true},"cell_type":"code","source":"train.head()","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"b3b071fb-7a2c-4195-9817-b01983d11c0e","_uuid":"004d2e823056e98afc5adaac433b7afbfe93b82d"},"cell_type":"markdown","source":"Here's a couple of examples of essays (essay 1 and essay 2)."},{"metadata":{"_cell_guid":"d57f0b31-c09b-4305-a0b0-0b864e944fd1","_uuid":"1ba9522a65227881a3a55aefaee9de93c4cfd792","trusted":true},"cell_type":"code","source":"train['project_essay_1'][0]","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"9caf5da3-33bb-422d-81c4-fef20fbda1a8","_uuid":"b0d70e9d745411ea6228c95c5f19bd3a2ca6dd55","scrolled":true,"trusted":true},"cell_type":"code","source":"train['project_essay_2'][0]","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"2ea37597-02f7-43cf-ad16-a3d50aac1aba","_uuid":"5c4c716de98a4b1c2ecc0e516e67813b4fc1473e"},"cell_type":"markdown","source":"There is a little bit of variance in the length of the essays.  However, I think there is a word limit on the essays."},{"metadata":{"_cell_guid":"fd3fe158-4d7f-4b30-ac15-42605240ea4f","_uuid":"9c1a3f81397199fa250a2b642edc7fbc5f9f504e","trusted":true},"cell_type":"code","source":"lens1 = train.project_essay_1.str.len()\nlens2 = train.project_essay_2.str.len()\nlens3 = train.project_title.str.len()\nlens4 = train.project_resource_summary.str.len()\nlens1.mean(), lens1.std(), lens1.max()\nlens2.mean(), lens2.std(), lens2.max()\nlens3.mean(), lens3.std(), lens3.max()\nlens4.mean(), lens4.std(), lens4.max()","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"d2e55012-4736-425f-84f3-c148ac1f4852","_uuid":"eb68f1c83a5ad11e652ca5f2150993a06d43edb4","trusted":true},"cell_type":"code","source":"lens1.hist(grid=False, bins=30);\nlens2.hist(grid=False, bins=30);\nlens3.hist(grid=False, bins=30);\nlens4.hist(grid=False, bins=30);","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"b8515824-b2dd-4c95-bbf9-dc74c80355db","_uuid":"0151ab55887071aed82d297acb2c6545ed964c2b"},"cell_type":"markdown","source":"We'll create a list of all the labels to predict, and we'll also create a 'none' label so we can see how many comments have no labels. We can then summarize the dataset."},{"metadata":{"_cell_guid":"c66f79d1-1d9f-4d94-82c1-8026af198f2a","_uuid":"4ba6ef86c82f073bf411785d971a694348c3efa9","trusted":true},"cell_type":"code","source":"label_cols = ['project_is_approved']\ntrain.describe()","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"9f6316e3-7e29-431b-abef-73acf4a08637","_uuid":"b7b0d391248f929a026b16fc38936b7fc0176351","trusted":true},"cell_type":"code","source":"len(train), len(test)\ntrain_length = len(train)\ntest_length = len(test)\nprint(\"Train set has \", train_length, \"pieces of data.\")\nprint(\"Test set has \", test_length, \"pieces of data.\")","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"1b221e62-e23f-422a-939d-6747edf2d613","_uuid":"bfdcf59624717b37ca4ffc0c99d2c28a2d419b06"},"cell_type":"markdown","source":"There are a few empty essays that we need to get rid of, otherwise sklearn will complain. It's worth mentioning here that we're only looking at essays 1 and 2, since it seems essays 3 and 4 aren't as important to the test dataset, and if they were, there aren't a lot to train on anyways."},{"metadata":{"_cell_guid":"fdba531c-7ef2-4967-88e2-fc2b04f6f2ef","_uuid":"1e1229f403225f1889c7a7b4fc9be90fda818af5","collapsed":true,"trusted":true},"cell_type":"code","source":"ESSAY1 = 'project_essay_1'\ntrain[ESSAY1].fillna(\"unknown\", inplace=True)\ntest[ESSAY1].fillna(\"unknown\", inplace=True)\nESSAY2 = 'project_essay_2'\ntrain[ESSAY2].fillna(\"unknown\", inplace=True)\ntest[ESSAY2].fillna(\"unknown\", inplace=True)\nTITLE = 'project_title'\ntrain[TITLE].fillna(\"unknown\", inplace=True)\ntest[TITLE].fillna(\"unknown\", inplace=True)\nRESOURCES = 'project_resource_summary'\ntrain[RESOURCES].fillna(\"unknown\", inplace=True)\ntest[RESOURCES].fillna(\"unknown\", inplace=True)","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"480780f1-00c0-4f9a-81e5-fc1932516a80","_uuid":"f2e77e8e6df5e29b620c7a2a0add1438c35af932"},"cell_type":"markdown","source":"## Building the Model and Making a Submission (essay 1)\n\nWe'll start by creating a *bag of words* representation as a [*term document matrix*](http://https://en.wikipedia.org/wiki/Document-term_matrix). Because the paper we wrote of earlier suggested the use of [*n*-grams](http://https://en.wikipedia.org/wiki/N-gram), we will use *n*-grams."},{"metadata":{"_cell_guid":"b7f11db7-5c12-4eb8-9f2d-0323d629fed9","_uuid":"b043a3fb66c443fab0129e863c134ec813dadb87","collapsed":true,"trusted":true},"cell_type":"code","source":"import re, string\nre_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"bfdebf11-133c-4b12-8664-8bf64757d6cc","_uuid":"941759df15c71d42853515e4d1006f4ab000ce75"},"cell_type":"markdown","source":"It turns out that using TFIDF gives even better priors than the binarized features used in the paper. I don't think this has been mentioned in any published literature before, but it improves the public leaderboard score."},{"metadata":{"_cell_guid":"31ad6c98-d054-426c-b3bd-b3b18f52eb6f","_uuid":"75f3f27d56fb2d7d539e65c292d9e77c92ceead3","collapsed":true,"trusted":true},"cell_type":"code","source":"n = train.shape[0]\nvec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\ntrn_term_doc = vec.fit_transform(train[ESSAY1])\ntest_term_doc = vec.transform(test[ESSAY1])","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"4cf3ec26-8237-452b-90c9-831cb0297955","_uuid":"6d215bc460e64d88b08f501d5c5a67c290e40635"},"cell_type":"markdown","source":"This creates a [*sparse matrix*](http://https://en.wikipedia.org/wiki/Sparse_matrix) with only a small number of non-zero elements (*stored elements* in the representation  below)."},{"metadata":{"_cell_guid":"4c7bdbcc-4451-4477-944c-772e99bac777","_uuid":"8816cc35f66b9fed9c12978fbdef5bb68fae10f4","trusted":true},"cell_type":"code","source":"trn_term_doc, test_term_doc","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"59131479-a861-4f46-add9-b2af09a51976","_uuid":"5fc487461f4c6fdaea25f2cd471fc801856c6689"},"cell_type":"markdown","source":"Here's the basic Naive Bayes feature equation:"},{"metadata":{"_cell_guid":"45fc6070-ba13-455b-9274-5c2611e2809c","_uuid":"8b277f01cecd575ed4fcae2e630c0dd8ce979793","collapsed":true,"trusted":true},"cell_type":"code","source":"def pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) / ((y==y_i).sum()+1)","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"2299d24b-5515-4d37-92d9-e7f6b16a290a","_uuid":"926eaa2e40e588f4ef2b86e0a28f8e575c9ed5f4","collapsed":true,"trusted":true},"cell_type":"code","source":"x = trn_term_doc\ntest_x = test_term_doc","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"c0b494ac-0dfc-4faa-a909-0a6d7696d1fc","_uuid":"dc5cafeab86d17ac4f036d58658437636a885a87"},"cell_type":"markdown","source":"Fit our model for \"project is approved\" binary classifier:"},{"metadata":{"_cell_guid":"b756c889-a383-4952-9ee9-eca79fd3454f","_uuid":"8652ab2f5f84e77fa395252be9b60be1e44fd583","collapsed":true,"trusted":true},"cell_type":"code","source":"def get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) / pr(0,y))\n    m = LogisticRegression(C=4, dual=True)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"33fd5f8c-adfc-45a1-9fde-1769a0993e76","_uuid":"0fa103b5406aabdc36ea9ef21612d343e4982fc4","trusted":true},"cell_type":"code","source":"preds1 = np.zeros((len(test), len(label_cols)))\n\nfor i, j in enumerate(label_cols):\n    print('Fitting for ', j, '.')\n    m,r = get_mdl(train[j])\n    preds1[:,i] = m.predict_proba(test_x.multiply(r))[:,1]\n\nprint(preds1[0:20])","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"1a99c4d9-916f-4189-9a25-fedcb7700336","_uuid":"5525045116474e6d12b6edc890250d30c0790f06"},"cell_type":"markdown","source":"And finally, create the submission file."},{"metadata":{"_cell_guid":"bc6a4575-fbbb-47ea-81ac-91fa702dc194","_uuid":"5dd033a93e6cf32cdbdaa0a8b05cd8d27de2b21d","collapsed":true,"trusted":true},"cell_type":"code","source":"submid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds1, columns = label_cols)], axis=1)\nsubmission.to_csv('essay1_output.csv', index=False)","execution_count":19,"outputs":[]},{"metadata":{"_cell_guid":"6a9d6a3d-cb6b-473b-be1a-6473f67739aa","_uuid":"90f65e1d69749f3177266803fd5cca810ba57d28"},"cell_type":"markdown","source":"## Building the Model and Making a Submission (essay 2)\n\nWe're going to do the exact same thing we did before for the first project essay, but this time with the second project essay."},{"metadata":{"_cell_guid":"565c9ecf-7fb1-4d35-97ae-3eec9ce50c60","_uuid":"f9e41dbef078d961b7169f3df88f4251d877c82e","collapsed":true,"trusted":true},"cell_type":"code","source":"import re, string\nre_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","execution_count":20,"outputs":[]},{"metadata":{"_cell_guid":"5edfef2d-f920-4eb4-b5ca-fabdd6b889cb","_uuid":"4cfa5650945ad41a9431af817cb0ffef852964b2","collapsed":true,"trusted":true},"cell_type":"code","source":"n = train.shape[0]\nvec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\ntrn_term_doc = vec.fit_transform(train[ESSAY2])\ntest_term_doc = vec.transform(test[ESSAY2])","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"2ee94748-dcd7-4f36-a158-23a904407885","_uuid":"7b37556c5187fd0733f3f7bae597f3df6218f0ac","trusted":true},"cell_type":"code","source":"trn_term_doc, test_term_doc","execution_count":22,"outputs":[]},{"metadata":{"_cell_guid":"4ed594f8-0961-42e9-856b-af336931f7dc","_uuid":"2094c340922b2f39f10957b8659c8e35d4380a6e","collapsed":true,"trusted":true},"cell_type":"code","source":"def pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) / ((y==y_i).sum()+1)","execution_count":23,"outputs":[]},{"metadata":{"_cell_guid":"62690230-7fb3-4015-9652-651d95022b07","_uuid":"757996df9ddff61902569b5d39e21a175d7a2cda","collapsed":true,"trusted":true},"cell_type":"code","source":"x = trn_term_doc\ntest_x = test_term_doc","execution_count":24,"outputs":[]},{"metadata":{"_cell_guid":"ce496a56-5b91-4661-bb56-5ec2e092b6fd","_uuid":"eb847ecddcac42932dbcfed6dd425c6128ea0769","collapsed":true,"trusted":true},"cell_type":"code","source":"def get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) / pr(0,y))\n    m = LogisticRegression(C=4, dual=True)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r","execution_count":25,"outputs":[]},{"metadata":{"_cell_guid":"75d5617f-f0ea-406e-b1bf-de7fd30a1aec","_uuid":"9159e2b682a59e9bba9e9d52520687125ea1e215","trusted":true},"cell_type":"code","source":"preds2 = np.zeros((len(test), len(label_cols)))\n\nfor i, j in enumerate(label_cols):\n    print('Fitting for ', j)\n    m,r = get_mdl(train[j])\n    preds2[:,i] = m.predict_proba(test_x.multiply(r))[:,1]\n    \nprint(preds2[0:20])","execution_count":26,"outputs":[]},{"metadata":{"_cell_guid":"ab13aef6-21b5-4f5d-ba13-ecb05c8a453b","_uuid":"7230ee4239415ddfd75ad9032b80d44cf3f7db55","collapsed":true,"trusted":true},"cell_type":"code","source":"submid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds2, columns = label_cols)], axis=1)\nsubmission.to_csv('essay2_output.csv', index=False)","execution_count":27,"outputs":[]},{"metadata":{"_cell_guid":"ac09d2f7-c37f-4660-9be9-e0610ca3e92f","_uuid":"c30c6c8a73b46f847a6a349d9f2b2ad20a0e8787"},"cell_type":"markdown","source":"## Blending Essay1 and Essay2\nWe use the geometric mean function here, because it's likely that one bad essay would weigh the other one down."},{"metadata":{"_cell_guid":"e7d35c5c-2776-4ca6-9b70-1efc1e04de78","_uuid":"a65f849eda9caae4a0008067845fdb6da4bbd33f","collapsed":true,"trusted":true},"cell_type":"code","source":"i = 0\n\nessaypreds = np.zeros((len(test), len(label_cols)))\n\nfor i in range(len(preds1)):\n    essaypreds[i] = math.sqrt( preds1[i] * preds2[i] ) # geometric mean","execution_count":28,"outputs":[]},{"metadata":{"_cell_guid":"93bab5cc-19cf-4595-ab93-2ba5e2ce5675","_uuid":"9c9ad2733889c8e68af307a9da07b30c89cb85c5","collapsed":true,"trusted":true},"cell_type":"code","source":"submid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(essaypreds, columns = label_cols)], axis=1)\nsubmission.to_csv('essaysonlysubmission.csv', index=False)","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"b3c800998ffab39c1c99a2c796ffee4a349c2032"},"cell_type":"markdown","source":"## Building the Model and Making a Submission (title)\n\nWe're going to do the exact same thing we did before for the essays, but this time with the title."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"38654a1c7b9c56ee009c3e68fe03f0937bcd4a2b"},"cell_type":"code","source":"import re, string\nre_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","execution_count":30,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"2dba8882328c83454aa2f828da134a10c8504602"},"cell_type":"code","source":"n = train.shape[0]\nvec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\ntrn_term_doc = vec.fit_transform(train[TITLE])\ntest_term_doc = vec.transform(test[TITLE])","execution_count":31,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"161c1d9945b2f88fe16177556d6a7923b9ea3d1a"},"cell_type":"code","source":"trn_term_doc, test_term_doc","execution_count":32,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"91dd6dfb34e9fa1ba44a329e8ae1ee24835353c0"},"cell_type":"code","source":"def pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) / ((y==y_i).sum()+1)","execution_count":33,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"1a7f1c050377ae520e06182acdc05198d708667a"},"cell_type":"code","source":"x = trn_term_doc\ntest_x = test_term_doc","execution_count":34,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"5625eda7c0afe415f2c62e58af4cb61a82b8140d"},"cell_type":"code","source":"def get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) / pr(0,y))\n    m = LogisticRegression(C=4, dual=True)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r","execution_count":35,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4022af5e5b976b215bf9f7c96f5cd9beced2616"},"cell_type":"code","source":"preds3 = np.zeros((len(test), len(label_cols)))\n\nfor i, j in enumerate(label_cols):\n    print('Fitting for ', j)\n    m,r = get_mdl(train[j])\n    preds3[:,i] = m.predict_proba(test_x.multiply(r))[:,1]\n    \nprint(preds3[0:20])","execution_count":36,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"d00727a13315981a2162a28e47758933c125cc9d"},"cell_type":"code","source":"submid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds3, columns = label_cols)], axis=1)\nsubmission.to_csv('title_output.csv', index=False)","execution_count":37,"outputs":[]},{"metadata":{"_uuid":"055c80ed40f79d08d52b36539809b5660a46190f"},"cell_type":"markdown","source":"Project Resource Summary"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"8995459507bcf887e78855bbe7a0e342b07f6cdf"},"cell_type":"code","source":"import re, string\nre_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","execution_count":38,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"acfca54e01d52cd2a1d77117eaddeebf4774b706"},"cell_type":"code","source":"n = train.shape[0]\nvec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\ntrn_term_doc = vec.fit_transform(train[RESOURCES])\ntest_term_doc = vec.transform(test[RESOURCES])","execution_count":39,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fbc6e3c2830c7618fe9be07d0b5de7b5f1860f6"},"cell_type":"code","source":"trn_term_doc, test_term_doc","execution_count":40,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"e956bb9517b92d5f2c8a77d76e7937855d02228b"},"cell_type":"code","source":"def pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) / ((y==y_i).sum()+1)","execution_count":41,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"32d20f9f90e263b04c425b07f98d65f23a6d4c35"},"cell_type":"code","source":"x = trn_term_doc\ntest_x = test_term_doc","execution_count":42,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"94b93ac029c99a9196da2698b87f7662604d0e5c"},"cell_type":"code","source":"def get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) / pr(0,y))\n    m = LogisticRegression(C=4, dual=True)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r","execution_count":43,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bab7dcb37f967bf637b018ec4e1564b862404de5"},"cell_type":"code","source":"preds4 = np.zeros((len(test), len(label_cols)))\n\nfor i, j in enumerate(label_cols):\n    print('Fitting for ', j)\n    m,r = get_mdl(train[j])\n    preds4[:,i] = m.predict_proba(test_x.multiply(r))[:,1]\n    \nprint(preds4[0:20])","execution_count":44,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"8ffd5d7e763aea888267919f16ed89d26224251e"},"cell_type":"code","source":"submid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds4, columns = label_cols)], axis=1)\nsubmission.to_csv('resourcesummary_output.csv', index=False)","execution_count":45,"outputs":[]},{"metadata":{"_uuid":"aec482bb3b7c31b8ee231a5d24a417ddeb6e3f85"},"cell_type":"markdown","source":"Blend it all"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"6137bed1d4716bfdd599325ca75095bc6f49f428"},"cell_type":"code","source":"i = 0\n\nfinalpreds = np.zeros((len(test), len(label_cols)))\n\nfor i in range(len(preds3)):\n    finalpreds[i] = ( essaypreds[i] + preds4[i] + preds3[i] ) / 3\n    \n# essays = 71.515\n# resource summary =\n# title =","execution_count":46,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"060b80d750ce6c61d2745f822fd60d2d7bb11c9f"},"cell_type":"code","source":"submid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(finalpreds, columns = label_cols)], axis=1)\nsubmission.to_csv('submission.csv', index=False)","execution_count":47,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}