{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"<h1> Introduction: Home Credit Default Risk Competition\n\nIn this notebook, we will take an initial look at the Home Credit default risk machine learning competition currently hosted on Kaggle. The objective of this competition is to use historical loan application data to predict whether or not an applicant will be able to repay a loan. This is a standard supervised classification task:\n\nSupervised: The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features\nClassification: The label is a binary variable, 0 (will repay loan on time), 1 (will have difficulty repaying loan). Related post to deal Imbalance in Data problem: https://www.kaggle.com/tini9911/imbalance-in-data\n\n\n"},{"metadata":{"_uuid":"5c8c1b514680f7d65ccc1eaa0e6548bba37f6967"},"cell_type":"markdown","source":"<h1> Objective"},{"metadata":{"_uuid":"194e98a5060c85dab9cd8e88b5912d74dc92864a"},"cell_type":"markdown","source":"\n<b> Predict how capable each applicant is of repaying a loan"},{"metadata":{"_uuid":"a77070e2b5eecd82cf36e12dde5be69acb21361c"},"cell_type":"markdown","source":"<h1> Data\n\nThe data is provided by Home Credit, a service dedicated to provided lines of credit (loans) to the unbanked population. Predicting whether or not a client will repay a loan or have difficulty is a critical business need, and Home Credit is hosting this competition on Kaggle to see what sort of models the machine learning community can develop to help them in this task.\n\n\napplication_train/application_test: the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the feature SK_ID_CURR. The training application data comes with the TARGET indicating 0: the loan was repaid or 1: the loan was not repaid.\n"},{"metadata":{"_uuid":"23ba43e4d26c1be5d1dd4cc1a02cfbe233c6d824"},"cell_type":"markdown","source":"<h1> Data Wrangling"},{"metadata":{"trusted":true,"_uuid":"6759608ae3ceef782e6769acfe33a6ebd8a9e1fb","collapsed":true},"cell_type":"code","source":"#Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\ncolor = sns.color_palette()\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.offline as offline\noffline.init_notebook_mode()\nimport os\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70d6c0df8a1cc08ed7f78f3ef41f64f9697afae9","collapsed":true},"cell_type":"code","source":"# List files available\nprint(os.listdir(\"../input/\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d850b4e0fd261b29d380ba91b5e2ce02df87324","collapsed":true},"cell_type":"code","source":"#Importing the dataset\ndf_train = pd.read_csv('../input/application_train.csv')\ndf_test=pd.read_csv('../input/application_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"#Shape of dataset\ndf_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d0509738ed579cdd411ad55cb268b5ae1694d88","collapsed":true},"cell_type":"code","source":"df_train.head(8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2cc6c915096307bdb29507044fadf7df294f2b1","collapsed":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fece869cf3c3188e7d82f288164e04931d18b4e","collapsed":true},"cell_type":"code","source":"df_train['NAME_FAMILY_STATUS'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f20ff03f9194602159c02d9942ba1789708eb44","collapsed":true},"cell_type":"code","source":"df_test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21eff61623013a1f23d1534652ed46e1c9642359","collapsed":true},"cell_type":"code","source":"df_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2e0edeaad35218701992d0eddd48e30dd7e63bb","collapsed":true},"cell_type":"code","source":"df_train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67aa6a63b04c2321f35d41c47158e2bbf56aefa9","collapsed":true},"cell_type":"code","source":"df_train['TARGET'].astype(int).plot.hist();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"253194a52b9dacc7f958b9c174e7685008a51049"},"cell_type":"markdown","source":"From this information, we see this is an imbalanced class problem(http://www.chioka.in/class-imbalance-problem/). There are far more loans that were repaid on time than loans that were not repaid. Once we get into more sophisticated machine learning models, we can weight the classes by their representation in the data to reflect this imbalance."},{"metadata":{"_uuid":"7546441193b119232136eee9b9584fd1ac1674e4"},"cell_type":"markdown","source":"<h1> Examine Missing Values"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fcc27026e60f00fb8d9b28a0e866bab37fa577e2"},"cell_type":"code","source":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba02dcf17f81ad06e611663d74782e800bb1ed92","collapsed":true},"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_values_table(df_train)\nmissing_values.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df812123bbc87204a61a0f2a1bf29899eca7a832"},"cell_type":"markdown","source":"When it comes time to build our machine learning models, we will have to fill in these missing values (known as imputation). In later work, we will use models such as XGBoost that can handle missing values with no need for imputation. Another option would be to drop columns with a high percentage of missing values, although it is impossible to know ahead of time if these columns will be helpful to our model. Therefore, we will keep all of the columns for now."},{"metadata":{"_uuid":"cd8dc6fb7a019e16ace23fc837da4f585aa6bc88"},"cell_type":"markdown","source":"<h1> Column Types"},{"metadata":{"_uuid":"a1ccbc332ed35f01aa5b2451e7eaaf81b51646fd"},"cell_type":"markdown","source":"Let's look at the number of columns of each data type. int64 and float64 are numeric variables (which can be either discrete or continuous). object columns contain strings and are categorical features. ."},{"metadata":{"trusted":true,"_uuid":"3f37308f985dfbc0707515c9e8ee34d9b1336e64","collapsed":true},"cell_type":"code","source":"# Number of each type of column\ndf_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bbf81d7753bde182f3b665a9dc340453a415b4d"},"cell_type":"markdown","source":"Let's now look at the number of unique entries in each of the object (categorical) columns."},{"metadata":{"trusted":true,"_uuid":"41ba3276684a700cf1ea3f7ede9232f6716626ce","collapsed":true},"cell_type":"code","source":"# Number of unique classes in each object column\ndf_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d05e3913ddb217ba88e1aada87cc3e37bd8af9a5"},"cell_type":"markdown","source":"Most of the categorical variables have a relatively small number of unique entries. We will need to find a way to deal with these categorical variables!"},{"metadata":{"_uuid":"5ef5f78bfc0032e61a09322dc054ae2e898ac9b3"},"cell_type":"markdown","source":"<h1> Encoding Categorical Variables "},{"metadata":{"_uuid":"d26c1da0f753fe347b7a468bef2fa6eb9e33428f"},"cell_type":"markdown","source":"Before we go any further, we need to deal with pesky categorical variables. A machine learning model unfortunately cannot deal with categorical variables (except for some models such as LightGBM). Therefore, we have to find a way to encode (represent) these variables as numbers before handing them off to the model. There are two main ways to carry out this process:\nLabel encoding: assign each unique category in a categorical variable with an integer. No new columns are created. An example is shown below image image"},{"metadata":{"_uuid":"cc7a9fe9ca243aaa029c28143d06347b6227dfc5"},"cell_type":"markdown","source":"![image](https://raw.githubusercontent.com/WillKoehrsen/Machine-Learning-Projects/master/label_encoding.png)"},{"metadata":{"_uuid":"bd98ebd427e0650670ac97bb79e9f9fa128a63ba"},"cell_type":"markdown","source":"One-hot encoding: create a new column for each unique category in a categorical variable. Each observation recieves a 1 in the column for its corresponding category and a 0 in all other new columns.\nimage"},{"metadata":{"_uuid":"3d72dceed5c6366c1508a63346bc703d11d892da"},"cell_type":"markdown","source":"![image](https://raw.githubusercontent.com/WillKoehrsen/Machine-Learning-Projects/master/one_hot_encoding.png)"},{"metadata":{"_uuid":"7004f943f7fa0b80c81dfb58d286ca1eb9bf7571"},"cell_type":"markdown","source":"we will use Label Encoding for any categorical variables with only 2 categories and One-Hot Encoding for any categorical variables with more than 2 categories. This process may need to change as we get further into the project, but for now, we will see where this gets us. (We will also not use any dimensionality reduction in this notebook but will explore in future iter"},{"metadata":{"_uuid":"8866573a85fc70c0cd657ea00c3c9acf663eed1d"},"cell_type":"markdown","source":"<h1> Label Encoding and One-Hot Encoding"},{"metadata":{"_uuid":"fdb4086cb05266449822758000ed677d50eb94fe"},"cell_type":"markdown","source":"Let's implement the policy described above: for any categorical variable (dtype == object) with 2 unique categories, we will use label encoding, and for any categorical variable with more than 2 unique categories, we will use one-hot encoding.\nFor label encoding, we use the Scikit-Learn LabelEncoder and for one-hot encoding, the pandas get_dummies(df) function."},{"metadata":{"trusted":true,"_uuid":"6ab7e0cdc53e19175fa7895a49b04c27537df5b9","collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in df_train:\n    if df_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(df_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(df_train[col])\n            # Transform both training and testing data\n            df_train[col] = le.transform(df_train[col])\n            df_test[col] = le.transform(df_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f4714af27da5879ad531fc82888c47a08a92251","collapsed":true},"cell_type":"code","source":"# one-hot encoding of categorical variables\ndf_train = pd.get_dummies(df_train)\ndf_test = pd.get_dummies(df_test)\n\nprint('Training Features shape: ', df_train.shape)\nprint('Testing Features shape: ', df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87213e08787502e4f4717031e521f0355a513b00"},"cell_type":"markdown","source":"<h1> Aligning Training and Testing Data"},{"metadata":{"_uuid":"7c9c34831385729c31f1999d5c9e6c568fe3e1d3"},"cell_type":"markdown","source":"Aligning Training and Testing Data There need to be the same features (columns) in both the training and testing data. One-hot encoding has created more columns in the training data because there were some categorical variables with categories not represented in the testing data. To remove the columns in the training data that are not in the testing data, we need to align the dataframes. First we extract the target column from the training data (because this is not in the testing data but we need to keep this information). When we do the align, we must make sure to set axis = 1 to align the dataframes based on the columns and not on the rows!"},{"metadata":{"trusted":true,"_uuid":"734c5d47a73567ef88fc4c9ac2be7041b42ca13b","collapsed":true},"cell_type":"code","source":"train_labels = df_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\ndf_train, df_test = df_train.align(df_test, join = 'inner', axis = 1)\n\n# Add the target back in\ndf_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', df_train.shape)\nprint('Testing Features shape: ', df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0358a77c17621d8a2f84ebe9ed04c321cd2e8303"},"cell_type":"markdown","source":"The training and testing datasets now have the same features which is required for machine learning. The number of features has grown significantly due to one-hot encoding. At some point we probably will want to try dimensionality reduction (removing features that are not relevant) to reduce the size of the datasets."},{"metadata":{"_uuid":"28f5b2f0b98efc7b438957845f5f9b296d12b740"},"cell_type":"markdown","source":"<h1> Anomalies"},{"metadata":{"_uuid":"57628625d3e490c8085b4ede0fba3e386aae11dc"},"cell_type":"markdown","source":"One problem we always want to be on the lookout for when doing EDA is anomalies within the data. These may be due to mis-typed numbers, errors in measuring equipment, or they could be valid but extreme measurements. One way to support anomalies quantitatively is by looking at the statistics of a column using the describe method. The numbers in the DAYS_BIRTH column are negative because they are recorded relative to the current loan application. To see these stats in years, we can mutliple by -1 and divide by the number of days in a year:"},{"metadata":{"trusted":true,"_uuid":"4dffa4e9efa4c2f944c7ff6f389e9b0ea807d03b","collapsed":true},"cell_type":"code","source":"(df_train['DAYS_BIRTH'] / -365).describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77243ed9104a5c34e804440222c3ddddc02a35ef"},"cell_type":"markdown","source":"Those ages look reasonable. There are no outliers for the age on either the high or low end. How about the days of employment?"},{"metadata":{"trusted":true,"_uuid":"ff280c8c178bb259da1f0f1f72e3dbe224c46ef8","collapsed":true},"cell_type":"code","source":"df_train['DAYS_EMPLOYED'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8d93a00f09d558fea3633ac1313e6bcf85a1c9e"},"cell_type":"markdown","source":"That doesn't look right! The maximum value (besides being positive) is about 1000 years!"},{"metadata":{"trusted":true,"_uuid":"2f5bf31da4825c866ecee6b3d425dc591fb290fa","collapsed":true},"cell_type":"code","source":"df_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3751ea4fc36fffd1b68f3c65616602a9e3316c4f"},"cell_type":"markdown","source":"Just out of curiousity, let's subset the anomalous clients and see if they tend to have higher or low rates of default than the rest of the clients."},{"metadata":{"_uuid":"e36b7162a1719cad26784423c42721067a7da0a3"},"cell_type":"markdown","source":"<h1> Missing Values Strategy # 1 - Identify Features with Missing Values -> Replace with NaN -> Remove all Features with Missing Value -> Assess Model using Logistic Regression "},{"metadata":{"trusted":true,"_uuid":"64e2f5b4636e2b45b624d0ec28dde9869778e163","collapsed":true},"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_values_table(df_train)\nmissing_values.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"406c09051d7334f9366a5512c474ac0fcd73160b","collapsed":true},"cell_type":"code","source":"#Omitting TARGET from Column list\ntrain= df_train.drop(columns = ['TARGET'])\n# Replace Nulls with NaN\n# mark zero values as missing or NaN\ntrain.iloc[:, :] = train.iloc[:, :].replace('' , np.NaN)\n# count the number of NaN values in each column\nprint(train.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8152bbb172b2342f2247a40fc67986cc4e7ec5d4","collapsed":true},"cell_type":"code","source":"# print the first 20 rows of data\nprint(df_train.head(20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d4a904867f2db2b6c2443832c7b9b9c877c0143","collapsed":true},"cell_type":"code","source":"print (train.head(10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0bb065e1e1fa1bb45a83c2e10ce41fa56ff96d16"},"cell_type":"markdown","source":"<h1> Baseline Model "},{"metadata":{"_uuid":"99b6ab0fbbe1a615383964dfdf0b8983ff3a454f"},"cell_type":"markdown","source":"<h3> Dropping Rows with Missing Values ->Baselining model with Logistic Regression "},{"metadata":{"trusted":true,"_uuid":"0ac48b0cbed75589cf7028a5a359bcbe43327f00","collapsed":true},"cell_type":"code","source":"# drop rows with missing values\ntrain.dropna(inplace=True)\n# summarize the number of rows and columns in the dataset\n#Add in the TARGET\ntrain['TARGET']=df_train['TARGET']\nprint(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6613c566ce8a3f8e5b8127eab8d7fea8d5f3c108","collapsed":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3e8c5eddedfb8bd5eefdf6a7f53ad4793c134c6","collapsed":true},"cell_type":"code","source":"#Deploying Logistic Regression\n#Splitting the dataset\n\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt \nplt.rc(\"font\", size=14)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.cross_validation import train_test_split\n\nX = train.iloc[:, :1]\ny = train.iloc[:,1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nfrom sklearn import metrics\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01e3a58a4b0eee846a258f7f82b6ffa0a89f282a","collapsed":true},"cell_type":"code","source":"y_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4bc40f2e109bd610cf71ca3add8ec7680c72688"},"cell_type":"markdown","source":"<h3> Classification Report for Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"a06c843a75d1ee146681f95ae24081bc0bf87590","collapsed":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,accuracy_score\nprint(classification_report(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68ad1c3069a847e380b5c55bafba3cf2fb2578f1"},"cell_type":"markdown","source":"<h1> Another Approach: Identify Features with Missing Values -> Replace with NaN -> Impute all Features with Missing Value -> Assess Model using Logistic Regression "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"55b3e1ebb8a4370900b42e1f2489be0a25419a01"},"cell_type":"code","source":"df_train = pd.read_csv('../input/application_train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8a163688cfe36b73d109ebcc5065dcddbaeb5ca","collapsed":true},"cell_type":"code","source":"missing_values = missing_values_table(df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4b89f235b23c649720a27a281b122eccd52e093","collapsed":true},"cell_type":"code","source":"print (df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e530213feb17a48f9a141ee5fb217bf997a6dfb","collapsed":true},"cell_type":"code","source":"# Number of unique classes in each object column\ndf_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f16fbabe54204e0264ee7f825ea3b3ee7d3b0a6e","collapsed":true},"cell_type":"code","source":"print (list(df_train[col].unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ae95a95a141e287056a5cabdff90d39c73be070","collapsed":true},"cell_type":"code","source":"df_train = pd.get_dummies(df_train)\nprint('Training Features shape: ', df_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"215367253d365e7850e77d973f79e02a2463322e","collapsed":true},"cell_type":"code","source":"train_labels = df_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\ndf_train, df_test = df_train.align(df_test, join = 'inner', axis = 1)\n\n# Add the target back in\ndf_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', df_train.shape)\nprint('Testing Features shape: ', df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0aa7b6a5453b071c8452e11e3576a10168dd6443"},"cell_type":"markdown","source":"<h1> Missing Values Strategy # 2 - Identify Features with Missing Values -> Replace with NaN -> Impute all Features with Missing Value -> Assess Model using Logistic Regression "},{"metadata":{"trusted":true,"_uuid":"0c6446799969c60e295bfc3ee47bcaeecff78156","collapsed":true},"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_values_table(df_train)\nmissing_values.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdbaf5f71d300c48fdca1ba17910b788160e21e2","collapsed":true},"cell_type":"code","source":"df_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe3e6ba11a36e7edb9b2a8e327465e6614392a9f","collapsed":true},"cell_type":"code","source":"# Replace Nulls with NaN\n# mark zero values as missing or NaN\n\ndf_train.iloc[:, :1] = df_train.iloc[:, :1].replace('', np.NaN)\n# count the number of NaN values in each column\nprint(df_train.isnull().sum())\n\n# fill missing values with mean column values\ndf_train.fillna(df_train.mean(), inplace=True)\n# count the number of NaN values in each column\nprint(df_train.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69e5c40b0acadf3be5cd6bd074a7068f5d22d065","collapsed":true},"cell_type":"code","source":"df_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0980ccb85271908b1a68930ecb1a58b4095d770","collapsed":true},"cell_type":"code","source":"#LDA\n\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n# split dataset into inputs and outputs\n#values = dataset.values\nX = df_train.iloc[:, :-1]\ny = df_train.iloc[:, -1]\n# fill missing values with mean column values\nimputer = Imputer()\ntransformed_X = imputer.fit_transform(X)\n# evaluate an LDA model on the dataset using k-fold cross validation\nmodel = LinearDiscriminantAnalysis()\nkfold = KFold(n_splits=3, random_state=7)\nresult = cross_val_score(model, transformed_X, y, cv=kfold, scoring='accuracy')\nprint(result.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1de2f2b74570c79e20877ec45a8a312e284c2909","collapsed":true},"cell_type":"code","source":"# Deploying Logistic Regression\n#Splitting the dataset\n#Keep the following 6 features (variables) which are important\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt \nplt.rc(\"font\", size=14)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.cross_validation import train_test_split\n\nX = df_train.iloc[:, :1]\ny = df_train.iloc[:,1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nfrom sklearn import metrics\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a33f4dcb5b7ae1e9021058e6d8f88e772aa05de","collapsed":true},"cell_type":"code","source":"y_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f908b7d0f68508efbc7db9e0da20e7fb8b0334cd","collapsed":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,accuracy_score\nprint(classification_report(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d6af49d1e40d4eedfed7ced6d8073299c73cc977"},"cell_type":"markdown","source":"<h1> Baseline with Feature Scaling "},{"metadata":{"_uuid":"b818d426f35909956ee6f8b4fcc035ba4cde03a6"},"cell_type":"markdown","source":"To get a baseline, we will use all of the features after encoding the categorical variables. We will preprocess the data by filling in the missing values (imputation) and normalizing the range of the features (feature scaling). The following code performs both of these preprocessing steps.\nTo get a baseline, we will use all of the features after encoding the categorical variables. We will preprocess the data by filling in the missing values (imputation) and normalizing the range of the features (feature scaling). The following code performs both of these preprocessing steps."},{"metadata":{"trusted":true,"_uuid":"c6bc6f5bc31cbf875635a09a427073177b605c1f","collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, Imputer\n\n# Drop the target from the training data\nif 'TARGET' in df_train:\n    train = df_train.drop(columns = ['TARGET'])\nelse:\n    train = df_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = df_test.copy()\n\n# Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(df_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ae9f0201b618937f6684bbe85e494a9c5d85ce7"},"cell_type":"markdown","source":"We will use LogisticRegressionfrom Scikit-Learn for our first model. The only change we will make from the default model settings is to lower the regularization parameter, C, which controls the amount of overfitting (a lower value should decrease overfitting). This will get us slightly better results than the default LogisticRegression, but it still will set a low bar for any future models.\nHere we use the familiar Scikit-Learn modeling syntax: we first create the model, then we train the model using .fit and then we make predictions on the testing data using .predict_proba (remember that we want probabilities and not a 0 or 1)."},{"metadata":{"trusted":true,"_uuid":"ddb4218a3a9b373b227372db12e65e2bacced68d","collapsed":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cc1ced7996559cee108a17af37abc26b8610ffb"},"cell_type":"markdown","source":"Now that the model has been trained, we can use it to make predictions. We want to predict the probabilities of not paying a loan, so we use the model predict.proba method. This returns an m x 2 array where m is the number of observations. The first column is the probability of the target being 0 and the second column is the probability of the target being 1 (so for a single row, the two columns must sum to 1). We want the probability the loan is not repaid, so we will select the second column.\nThe following code makes the predictions and selects the correct column."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ebeca52b06cb9b97200e9ead71a7e90844cd182d"},"cell_type":"code","source":"# Make predictions\n# Make sure to select the last column only\nlog_reg_pred = log_reg.predict_proba(test)[:, -1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b12974e6f8014bff5fa9406c6177709696aa6bfe"},"cell_type":"markdown","source":"The predictions must be in the format shown in the sample_submission.csv file, where there are only two columns: SK_ID_CURR and TARGET. We will create a dataframe in this format from the test set and the predictions called submit."},{"metadata":{"trusted":true,"_uuid":"11a0aa9836c9514070856224041d4b37f75bbef2","collapsed":true},"cell_type":"code","source":"# Submission dataframe\nsubmit = df_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae647bbafc07c681209c47b3e68746998ac8fcf6"},"cell_type":"markdown","source":"The predictions represent a probability between 0 and 1 that the loan will not be repaid. If we were using these predictions to classify applicants, we could set a probability threshold for determining that a loan is risky."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d9cfbbcfcb9c1d92832889325e0d5e24e8839c9f"},"cell_type":"code","source":"# Save the submission to a csv file\nsubmit.to_csv('log_reg_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"165d78f1f184d4c39e19edff19ec1e51432b2453"},"cell_type":"markdown","source":"The submission has now been saved to the virtual environment in which our notebook is running. This runs the entire notebook and then lets us download any files that are created during the run.\nOnce we run the notebook, the files created are available in the Versions tab under the Output sub-tab.\nThis is Data Wrangling complete with Baseline Model determined with Logistic Regresssion; next we may proceed with EDA & Inferential Statistics."},{"metadata":{"_uuid":"81e3c6b4357733c20080f29d32be2da898aa219f"},"cell_type":"markdown","source":"<h1> EDA & Inferential Statistics"},{"metadata":{"_uuid":"e534f6635b8764e458d26830eea0a87077638ba5"},"cell_type":"markdown","source":"<h3> Function to Explore Numeric Data"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0c6b9c64c765a2ab5ee5e3b08a2261f3879bf6b0"},"cell_type":"code","source":"def numeric(col):\n    plt.figure(figsize=(12,5))\n    plt.title(\"Distribution of \"+col)\n    ax = sns.distplot(df_train[col].dropna())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4eb2b5714cdc84ad4565e8a04fa386d07fba84c","collapsed":true},"cell_type":"code","source":"numeric(\"AMT_CREDIT\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4ff0b0f9506e705a98356cce5ad85525b816f10","collapsed":true},"cell_type":"code","source":"numeric(\"AMT_INCOME_TOTAL\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"881da9b24a9e340f41982cbd1557cb8b97575be1","collapsed":true},"cell_type":"code","source":"numeric(\"AMT_ANNUITY\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ee618b3a225eb4ea955da7fb67678591768f0a3","collapsed":true},"cell_type":"code","source":"anom = df_train[df_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = df_train[df_train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"009bcdc3455a6816872b6ceeeb9941bb2a9d8b83"},"cell_type":"markdown","source":"Well that is extremely interesting! It turns out that the anomalies have a lower rate of default. Handling the anomalies depends on the exact situation, with no set rules. One of the safest approaches is just to set the anomalies to a missing value and then have them filled in (using Imputation) before machine learning. In this case, since all the anomalies have the exact same value, we want to fill them in with the same value in case all of these loans share something in common. The anomalous values seem to have some importance, so we want to tell the machine learning model if we did in fact fill in these values. As a solution, we will fill in the anomalous values with not a number (np.nan) and then create a new boolean column indicating whether or not the value was anomalous."},{"metadata":{"trusted":true,"_uuid":"9c96db156143f532484ad0aa974ee93425902976","collapsed":true},"cell_type":"code","source":"# Create an anomalous flag column\ndf_train['DAYS_EMPLOYED_ANOM'] = df_train[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\ndf_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\ndf_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66f3a1c977c551eee80be671c546f13b9c26dd97"},"cell_type":"markdown","source":"The distribution looks to be much more in line with what we would expect, and we also have created a new column to tell the model that these values were originally anomalous (becuase we will have to fill in the nans with some value, probably the median of the column). The other columns with DAYS in the dataframe look to be about what we expect with no obvious outliers. As an extremely important note, anything we do to the training data we also have to do to the testing data. Let's make sure to create the new column and fill in the existing column with np.nan in the testing data. The distribution looks to be much more in line with what we would expect, and we also have created a new column to tell the model that these values were originally anomalous (becuase we will have to fill in the nans with some value, probably the median of the column). The other columns with DAYS in the dataframe look to be about what we expect with no obvious outliers. As an extremely important note, anything we do to the training data we also have to do to the testing data. Let's make sure to create the new column and fill in the existing column with np.nan in the testing data."},{"metadata":{"trusted":true,"_uuid":"fa16e0464af3e5200da7fad6d3ef27bc1d3e2cb8","collapsed":true},"cell_type":"code","source":"df_test['DAYS_EMPLOYED_ANOM'] = df_test[\"DAYS_EMPLOYED\"] == 365243\ndf_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (df_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(df_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e804198961ff8327880ef3597c6c4180d90fe3e","collapsed":true},"cell_type":"code","source":"(df_train['DAYS_BIRTH']/365.0).describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"302928f11c2e5cb93fc663738f02e4b81c32530a"},"cell_type":"markdown","source":"<h2> Dataset Prep for Another set of EDA "},{"metadata":{"trusted":true,"_uuid":"c279a337198661b4d43fa55d0977e0b0d6f569f4","collapsed":true},"cell_type":"code","source":"from plotly.offline import init_notebook_mode, iplot\nfrom wordcloud import WordCloud\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nimport plotly.plotly as py\nfrom plotly import tools\nfrom datetime import date\nimport pandas as pd\nimport numpy as np \nimport seaborn as sns\nimport random \nimport warnings\nwarnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)\n\npath = \"../input/\"\n\ndef bar_hor(df, col, title, color, w=None, h=None, lm=0, limit=100, return_trace=False, rev=False, xlb = False):\n    cnt_srs = df[col].value_counts()\n    yy = cnt_srs.head(limit).index[::-1] \n    xx = cnt_srs.head(limit).values[::-1] \n    if rev:\n        yy = cnt_srs.tail(limit).index[::-1] \n        xx = cnt_srs.tail(limit).values[::-1] \n    if xlb:\n        trace = go.Bar(y=xlb, x=xx, orientation = 'h', marker=dict(color=color))\n    else:\n        trace = go.Bar(y=yy, x=xx, orientation = 'h', marker=dict(color=color))\n    if return_trace:\n        return trace \n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n\ndef bar_hor_noagg(x, y, title, color, w=None, h=None, lm=0, limit=100, rt=False):\n    trace = go.Bar(y=x, x=y, orientation = 'h', marker=dict(color=color))\n    if rt:\n        return trace\n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n\n\ndef bar_ver_noagg(x, y, title, color, w=None, h=None, lm=0, rt = False):\n    trace = go.Bar(y=y, x=x, marker=dict(color=color))\n    if rt:\n        return trace\n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n    \ndef gp(col, title):\n    df1 = df_train[df_train[\"TARGET\"] == 1]\n    df0 = df_train[df_train[\"TARGET\"] == 0]\n    a1 = df1[col].value_counts()\n    b1 = df0[col].value_counts()\n    \n    total = dict(df_train[col].value_counts())\n    x0 = a1.index\n    x1 = b1.index\n    \n    y0 = [float(x)*100 / total[x0[i]] for i,x in enumerate(a1.values)]\n    y1 = [float(x)*100 / total[x1[i]] for i,x in enumerate(b1.values)]\n\n    trace1 = go.Bar(x=a1.index, y=y0, name='Target : 1', marker=dict(color=\"#96D38C\"))\n    trace2 = go.Bar(x=b1.index, y=y1, name='Target : 0', marker=dict(color=\"#FEBFB3\"))\n    return trace1, trace2 \n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"310e27d6e8b91601f8f486cdf1ff3f3576fe9a6e"},"cell_type":"markdown","source":"<h1> Imbalance of Data"},{"metadata":{"trusted":true,"_uuid":"b3b6ccda858bd3e4315468c814cec874c98762bc","collapsed":true},"cell_type":"code","source":"target_distribution = df_train['TARGET'].value_counts()\ntarget_distribution.plot.pie(figsize=(10, 10),\n                             title='Target Distribution',\n                             fontsize=15, \n                             legend=True, \n                             autopct=lambda v: \"{:0.1f}%\".format(v))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a29d37ff1d6769bfeb2b8a44521b33184bc1505e"},"cell_type":"markdown","source":"<h1> Correlations "},{"metadata":{"_uuid":"835215f02d7c35e73eba8aa8ed5792eb165f9061"},"cell_type":"markdown","source":"Now that we have dealt with the categorical variables and the outliers, let's continue with the EDA. One way to try and understand the data is by looking for correlations between the features and the target. We can calculate the Pearson correlation coefficient between every variable and the target using the .corr dataframe method.\nThe correlation coefficient is not the greatest method to represent \"relevance\" of a feature, but it does give us an idea of possible relationships within the data. Some general interpretations of the absolute value of the correlation coefficent are:\n.00-.19 “very weak”\n.20-.39 “weak”\n.40-.59 “moderate”\n.60-.79 “strong”\n.80-1.0 “very strong”"},{"metadata":{"trusted":true,"_uuid":"d257f6a29a1cfd0c3b7f9c7ea7f7497b1f9e63c6","collapsed":true},"cell_type":"code","source":"# Find correlations with the target and sort\ncorrelations = df_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b9dd5e77b79e3689e71f6e68cab76c8cdda1f17"},"cell_type":"markdown","source":"Let's take a look at some of more significant correlations: the DAYS_BIRTH is the most positive correlation. (except for TARGET because the correlation of a variable with itself is always 1!) Looking at the documentation, DAYS_BIRTH is the age in days of the client at the time of the loan in negative days (for whatever reason!). The correlation is positive, but the value of this feature is actually negative, meaning that as the client gets older, they are less likely to default on their loan (ie the target == 0). That's a little confusing, so we will take the absolute value of the feature and then the correlation will be negative."},{"metadata":{"_uuid":"60385250cc6d6e6a4766ccb32bc47fbccf199fc8"},"cell_type":"markdown","source":"<h1> Effect of Age on Repayment"},{"metadata":{"trusted":true,"_uuid":"554bd7b6e5983ca8e272558ab4c4d31250c50ce9","collapsed":true},"cell_type":"code","source":"# Find the correlation of the positive days since birth and target\ndf_train['DAYS_BIRTH'] = abs(df_train['DAYS_BIRTH'])\ndf_train['DAYS_BIRTH'].corr(df_train['TARGET'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"734885bc1894c1ed23a212c02ab58a3a7a3b425a"},"cell_type":"markdown","source":"As the client gets older, there is a negative linear relationship with the target meaning that as clients get older, they tend to repay their loans on time more often.\nLet's start looking at this variable. First, we can make a histogram of the age. We will put the x axis in years to make the plot a little more understandable."},{"metadata":{"trusted":true,"_uuid":"63b64954a26498787cea9f5b4bc30a9f5bae000c","collapsed":true},"cell_type":"code","source":"# Set the style of plots\nplt.style.use('fivethirtyeight')\n\n# Plot the distribution of ages in years\nplt.hist(df_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aebdd1c879e6b1ad8623cda6a3edaee2983c4e35"},"cell_type":"markdown","source":"By itself, the distribution of age does not tell us much other than that there are no outliers as all the ages are reasonable. To visualize the effect of the age on the target, we will next make a kernel density estimation plot (KDE) colored by the value of the target. A kernel density estimate plot shows the distribution of a single variable and can be thought of as a smoothed histogram (it is created by computing a kernel, usually a Gaussian, at each data point and then averaging all the individual kernels to develop a single smooth curve). We will use the seaborn kdeplot for this graph."},{"metadata":{"trusted":true,"_uuid":"c6779807d914ed65979d8882831e01c053ea671d","collapsed":true},"cell_type":"code","source":"import seaborn as sns\nplt.figure(figsize = (10, 8))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(df_train.loc[df_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(df_train.loc[df_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n\n# Labeling of plot\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76a8c8b0428d5dcddc42667c3ad1a34c01cb118d"},"cell_type":"markdown","source":"The target == 1 curve skews towards the younger end of the range. Although this is not a significant correlation (-0.07 correlation coefficient), this variable is likely going to be useful in a machine learning model because it does affect the target. Let's look at this relationship in another way: average failure to repay loans by age bracket.\nTo make this graph, first we cut the age category into bins of 5 years each. Then, for each bin, we calculate the average value of the target, which tells us the ratio of loans that were not repaid in each age category."},{"metadata":{"trusted":true,"_uuid":"8d81788617cb4c569d52f94bcbaa17f1a8834a9f","collapsed":true},"cell_type":"code","source":"# Age information into a separate dataframe\nage_data = df_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"550b8dd1f001ffa8edaf40b53296e3fc9ab44e1f","collapsed":true},"cell_type":"code","source":"# Group by the bin and calculate averages\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af9e5cbb2daafc4ddca7d3d664637ca3eb2d5df6","collapsed":true},"cell_type":"code","source":"plt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88033dcb7be2621f81a3ad5c650307cb4d8fc583"},"cell_type":"markdown","source":"There is a clear trend: younger applicants are more likely to not repay the loan! The rate of failure to repay is above 10% for the youngest three age groups and beolow 5% for the oldest age group. This is information that could be directly used by the bank: because younger clients are less likely to repay the loan, maybe they should be provided with more guidance or financial planning tips. This does not mean the bank should discriminate against younger clients, but it would be smart to take precautionary measures to help younger clients pay on time."},{"metadata":{"_uuid":"79d7523c548807795a47ec71938a4ab836a28525"},"cell_type":"markdown","source":"<h2> Exterior Sources "},{"metadata":{"_uuid":"15fd533813c4d3051a80b975512f3a789956359f"},"cell_type":"markdown","source":"Exterior Sources The 3 variables with the strongest negative correlations with the target are EXT_SOURCE_1, EXT_SOURCE_2, and EXT_SOURCE_3. According to the documentation, these features represent a \"normalized score from external data source\". I'm not sure what this exactly means, but it may be a cumulative sort of credit rating made using numerous sources of data.\nLet's take a look at these variables.\nFirst, we can show the correlations of the EXT_SOURCE features with the target and with each other."},{"metadata":{"trusted":true,"_uuid":"f34e04f44c1a53e77807f9053968551821461751","collapsed":true},"cell_type":"code","source":"# Extract the EXT_SOURCE variables and show correlations\next_data = df_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'NAME_EDUCATION_TYPE_Higher education', 'CODE_GENDER_F']]\next_data_corrs = ext_data.corr()\next_data_corrs\n\nplt.figure(figsize = (25, 36))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38c3a64373c0c22be57d562fd6d0ce18bc329745","collapsed":true},"cell_type":"code","source":"# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.02, annot = True, vmax = 0.5)\nplt.title('Correlation Heatmap');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12a06ba9cad34ab795db6c8522d25200430a0ed4"},"cell_type":"markdown","source":"All three EXT_SOURCE featureshave negative correlations with the target, indicating that as the value of the EXT_SOURCE increases, the client is more likely to repay the loan. We can also see that DAYS_BIRTH is positively correlated with EXT_SOURCE_1 indicating that maybe one of the factors in this score is the client age.\nNext we can look at the distribution of each of these features colored by the value of the target. This will let us visualize the effect of this variable on the target."},{"metadata":{"trusted":true,"_uuid":"1c03f08c35939dd0883924e62cef6a9d21fcda63","collapsed":true},"cell_type":"code","source":"plt.figure(figsize = (10, 12))\n# iterate through the sources\nfor i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    \n    # create a new subplot for each source\n    plt.subplot(3, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(df_train.loc[df_train['TARGET'] == 0, source], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(df_train.loc[df_train['TARGET'] == 1, source], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e205d7a41863d339e70b9f0e853cdf66fa27df55"},"cell_type":"markdown","source":"EXT_SOURCE_3 displays the greatest difference between the values of the target. We can clearly see that this feature has some relationship to the likelihood of an applicant to repay a loan. The relationship is not very strong (in fact they are all considered very weak, but these variables will still be useful for a machine learning model to predict whether or not an applicant will repay a loan on time."},{"metadata":{"_uuid":"f9b0a4331364952b9b3981594bfd8f9d4a04a32b"},"cell_type":"markdown","source":"<h1> Pairs Plot"},{"metadata":{"_uuid":"3465db7aaf5c52bf6c26537f37083a8e6e7ec595"},"cell_type":"markdown","source":"As a final exploratory plot, we can make a pairs plot of the EXT_SOURCE variables and the DAYS_BIRTH variable. The Pairs Plot is a great exploration tool because it lets us see relationships between multiple pairs of variables as well as distributions of single variables. Here we are using the seaborn visualization library and the PairGrid function to create a Pairs Plot with scatterplots on the upper triangle, histograms on the diagonal, and 2D kernel density plots and correlation coefficients on the lower triangle.\nIf you don't understand this code, that's all right! Plotting in Python can be overly complex, and for anything beyond the simplest graphs, I usually find an existing implementation and adapt the code (don't repeat yourself)!"},{"metadata":{"trusted":true,"_uuid":"820b3a7f567be414be7e0edb01f4a8b3473d29b3","collapsed":true},"cell_type":"code","source":"\n# Copy the data for plotting\nplot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n\n# Add in the age of the client in years\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n\n# Drop na values and limit to first 100000 rows\nplot_data = plot_data.dropna().loc[:100000, :]\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)\n\n# Create the pairgrid object\ngrid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n                    hue = 'TARGET', \n                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.2)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee81153a33606eb366eb8eaae032bc4f7e640c59"},"cell_type":"markdown","source":"In this plot, the red indicates loans that were not repaid and the blue are loans that are paid. We can see the different relationships within the data. There does appear to be a moderate positive linear relationship between the EXT_SOURCE_1 and the DAYS_BIRTH (or equivalently YEARS_BIRTH), indicating that this feature may take into account the age of the client."},{"metadata":{"trusted":true,"_uuid":"7d82ca34e338f97ad6cda1c0e5e99a12bd8bc0e9","collapsed":true},"cell_type":"code","source":"n= len(df_train)\nprint(n)\nx = np.sort(df_train['TARGET'])\ny = np.arange(1,len(x)+1)/float(len(x)) \nprint (y)\n\n_= plt.plot(x, y, marker = '.', linestyle = 'none')\n_= plt.xlabel('Customer Credibility to Repay Loan')\n_= plt.ylabel('ECDF')\n_= plt.margins(.02)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19833ce39d3cbd8deccb9987294da70413163694","collapsed":true},"cell_type":"code","source":"# Checking ECDF Distribution of Ability to Repay Loan across the real data and theoretical samples of data\ndef ecdf(data):\n    x= np.sort(data)\n    n= float(len(data))\n    y = np.arange(1, n+1)/n\n    return x,y\n\n%matplotlib inline\nplt.figure(figsize=(25,20))\n\n# Seed the random number generator:\nnp.random.seed(15)\n#Sample data for theortical normal dist\nsamples = np.random.normal(np.mean(df_train.TARGET), np.std(df_train.TARGET), size=10000)\nsamples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5753b981711189300f5a998f8076df77760acdd2","collapsed":true},"cell_type":"code","source":"#find ecdf of data\nx_count, y_count = ecdf(df_train.TARGET)\nx_theor, y_theor = ecdf(samples)\n\nfig = plt.plot(x_count, y_count, marker='.', linestyle='none')\nfig = plt.plot(x_theor, y_theor, marker='.', linestyle='none')\n\n# Label axes and add legend and a title:\nfig = plt.title('Customer Credibility to Repay Loan VS Theoretical Normal Dist')\nfig = plt.xlabel('Customer to Repay Loan')\nfig = plt.ylabel('ECDF')\n\n# Save and display the plots:\n#plt.savefig('reports/figures/cdf_body_temps.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92276a1244aadba65c60dd52746e0def382a0f03"},"cell_type":"markdown","source":"Compare the distribution of the data to the theoretical distribution of the data. This is done by comparing the ecdf First define a function for computing the ecdf from a data set. Next use np.random.normal to sample the theoretical normal distribution and overlay the ecdf of both data sets to compare distribution. Since theoretical ECDF is continuous curve while real data set is contiguous bar for 0 & 1 since it's classification problem but we may consider any data points closer to value '0' indicates 'will repay loan on time', 1 (will have difficulty repaying loan)"},{"metadata":{"trusted":true,"_uuid":"b98116f15a013fdae19c934b82b0a37508f404ef","collapsed":true},"cell_type":"code","source":"np.percentile(df_train['TARGET'], [25, 50, 75, 90, 98, 100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01f82a52493996ee4b4bd95c5bebc539feb938e7","collapsed":true},"cell_type":"code","source":"pd.DataFrame.hist(df_train, column='TARGET')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17055875e97f088025004d433bff372e5c6c9989"},"cell_type":"markdown","source":"<h2> Variance"},{"metadata":{"trusted":true,"_uuid":"710a10070f728f57752dae179196957669a55284","collapsed":true},"cell_type":"code","source":"np.var(df_train['TARGET'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88e13440625a0b5b248ddbcd3331405083722684"},"cell_type":"markdown","source":"<h2> Standard Deviation"},{"metadata":{"trusted":true,"_uuid":"867db84458daa00c3b5fff10d27103d0bbc66fc0","collapsed":true},"cell_type":"code","source":"np.std(df_train['TARGET'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0b18f135589035bb6d2bcbb9ac3596629ece580"},"cell_type":"markdown","source":"<h2> Covariance"},{"metadata":{"trusted":true,"_uuid":"fb956b6c83d49742a9eb62fc7374bce5acc3c21d","collapsed":true},"cell_type":"code","source":"np.cov(df_train['TARGET'], df_train['EXT_SOURCE_1'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e83b753faeaf6827d63eea7df0abbef693bfcdb","collapsed":true},"cell_type":"code","source":"np.cov(df_train['TARGET'], df_train['DAYS_BIRTH'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"526229d837507623c6329d078ac860d9556ff881"},"cell_type":"markdown","source":"<h2> Pearson Correlation Coeffient"},{"metadata":{"trusted":true,"_uuid":"038bc823a2718489a8eed041333676b79e84f871","collapsed":true},"cell_type":"code","source":"np.corrcoef(df_train['TARGET'], df_train['DAYS_BIRTH'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"378b4f965b899570f49be2c0f881615a34a94980"},"cell_type":"markdown","source":"<h1> Confidence Interval"},{"metadata":{"_uuid":"38f0ef0161ee4ec40c37b3da76ef6d28af3789f0"},"cell_type":"markdown","source":"<h3> Step 1: Build a function to create a bootstrap replicate"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4b107f61ad2e156170d6bae142dcd318171bd763"},"cell_type":"code","source":"def bootstrap_replicate_1d(data, func):\n    return func(np.random.choice(df_train['TARGET'], size=len(df_train['TARGET'])))\n#np.random.choice() works on linear model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a280085dd1837df2559e13c11d8610edad439e39"},"cell_type":"markdown","source":"<h3> Step 2: Another function to generate multiple such bootstrap samples"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"358e53c9416c2ac6fe6383d2c714da3901f0ebd9"},"cell_type":"code","source":"def draw_bs_reps(data, func, size=1):\n    \"\"\"Draw bootstrap replicates.\"\"\"\n\n    # Initialize array of replicates: bs_replicates\n    bs_replicates = np.empty(size)\n\n    # Generate replicates\n    for i in range(size):\n        bs_replicates[i] = bootstrap_replicate_1d(data, func)\n\n    return bs_replicates","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"090c9e5cd0e64acec72ddc9d3709d4a1d8f053e8"},"cell_type":"markdown","source":"<h3> Step 3: Plot the histogram for bootstrap replicates"},{"metadata":{"trusted":true,"_uuid":"07ab57bb83dc872151882056bd842bb2599b213f","collapsed":true},"cell_type":"code","source":"# Take 10,000 bootstrap replicates of the mean: bs_replicates\nbs_replicates = draw_bs_reps(df_train['TARGET'], np.mean, 10000)\n\n# Compute and print SEM Standard Error of the Mean\nsem = np.std(df_train['TARGET']) / np.sqrt(len(df_train['TARGET']))\nprint(sem)\n\n# Compute and print standard deviation of bootstrap replicates\nbs_std = np.std(bs_replicates)\nprint(bs_std)\n\n# Make a histogram of the results\n_ = plt.hist(bs_replicates, bins=50, normed=True)\n_ = plt.xlabel('Credit Loan Default Risk')\n_ = plt.ylabel('ECDF')\n\n# Show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae003c96617f2705b9310d28d74009422d198114"},"cell_type":"markdown","source":"<b> This is bootstrap estimate of the probability distribution function of the mean of 'Credit Loan Default Risk'at the Home Credit Group. Remember, we are estimating the mean 'Credit Loan Default Risk' we would get if the Home Credit Group could repeat all of the measurements over and over again. This is a probabilistic estimate of the mean. I plot the PDF as a histogram, and I see that it is not Normal as it has slightly longer right tail.\nIn fact, it can be shown theoretically that under not-too-restrictive conditions, the value of the mean will always be Normally distributed. (This does not hold in general, just for the mean and a few other statistics.) The standard deviation of this distribution, called the standard error of the mean, or SEM, is given by the standard deviation of the data divided by the square root of the number of data points. I.e., for a data set. Notice that the SEM we got from the known expression and the bootstrap replicates is the same and the distribution of the bootstrap replicates of the mean is Normal.\nAssuming 95% Confidence interval i.e. give the 2.5th and 97.5th percentile of bootstrap replicates is stored as bs_replicates"},{"metadata":{"trusted":true,"_uuid":"5794be645deb545390f7aa56e3de85e43ed33286","collapsed":true},"cell_type":"code","source":"np.percentile(bs_replicates, [2.5, 97.5])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6d6c0ea476c7f295ceddc5a61392b54b39f44c6"},"cell_type":"markdown","source":"The above steps may be repeated to show for Variance function as well"},{"metadata":{"_uuid":"4579fce13c0843df3f5b5866360904bca063a38c"},"cell_type":"markdown","source":"<h1> Extending Confidence Interval Concept to Pairs Bootstrap "},{"metadata":{"trusted":true,"_uuid":"c50cabc08b950c182ab0c508685a6f8f0a54fa98","collapsed":true},"cell_type":"code","source":"#Finding pairs bootstrap for slope & intercept of a linear function between Bike REntal Count and Registered User Type\ndef draw_bs_pairs_linreg(x, y, size=1):\n    \"\"\"Perform pairs bootstrap for linear regression.\"\"\"\n\n    # Set up array of indices to sample from: inds\n    inds = np.arange(len(x))\n\n    # Initialize replicates: bs_slope_reps, bs_intercept_reps\n    bs_slope_reps = np.empty(size)\n    bs_intercept_reps = np.empty(size)\n\n    # Generate replicates\n    for i in range(size):\n        bs_inds = np.random.choice(inds, size=len(inds))\n        bs_x, bs_y = x[bs_inds], y[bs_inds]\n        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)\n\n    return bs_slope_reps, bs_intercept_reps\n\n# Generate replicates of slope and intercept using pairs bootstrap\nbs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(df_train['EXT_SOURCE_1'], df_train['TARGET'], 1000)\n\n# Compute and print 95% CI for slope\nprint(np.percentile(bs_slope_reps, [2.5, 97.5]))\n# Plot the histogram\n_ = plt.hist(bs_slope_reps, bins=50, normed=True)\n_ = plt.xlabel('slope')\n_ = plt.ylabel('PDF')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"119a89db7e0492a52e5e4b4cb1bf1c7de3e44733"},"cell_type":"markdown","source":"<h1> Hypothesis Testing "},{"metadata":{"_uuid":"19d5f4ecc85f86fc8bfededd8737eab887fa82e0"},"cell_type":"markdown","source":"<b> Null Hypothesis- There is no significant difference between EXT_SOURCE_1 and EXT_SOURCE_2 mean on 'Ability to Repay Loan' \nH0: μEXT_SOURCE_1−μEXT_SOURCE_2=0 Significance Level: 95% Confidence α=0.05 Alternate Hypothesis - There is significant difference between EXT_SOURCE_1 and EXT_SOURCE_2 mean on 'Ability to Repay Loan' \nHA : μEXT_SOURCE_1−μEXT_SOURCE_2 != 0"},{"metadata":{"trusted":true,"_uuid":"520142f278ae2f69429dabec160035d1710af62d","collapsed":true},"cell_type":"code","source":"def permutation_sample(data1, data2):\n    \"\"\"Generate a permutation sample from two data sets.\"\"\"\n\n    # Concatenate the data sets: data\n    data = np.concatenate((data1, data2))\n\n    # Permute the concatenated array: permuted_data\n    permuted_data = np.random.permutation(data)\n\n    # Split the permuted array into two: perm_sample_1, perm_sample_2\n    perm_sample_1 = permuted_data[:len(data1)]\n    perm_sample_2 = permuted_data[len(data1):]\n\n    return perm_sample_1, perm_sample_2\n\nfor _ in range(50):\n    # Generate permutation samples\n    perm_sample_1, perm_sample_2 = permutation_sample(\n                                    df_train['EXT_SOURCE_1'], df_train['EXT_SOURCE_2'])\n\n    # Compute and plot ECDF from permutation sample 1 \n    x1 = np.sort(perm_sample_1)\n    y1 = np.arange(1,len(x1)+1)/float(len(x1)) \n    \n    # Compute and plot ECDF from permutation sample 2\n    x2 = np.sort(perm_sample_2)\n    y2 = np.arange(1,len(x2)+1)/float(len(x2))\n\n\n    # Plot ECDFs of permutation sample\n    _ = plt.plot(x1, y1, marker='.', linestyle='none',\n                 color='red', alpha=0.02)\n    _ = plt.plot(x2, y2, marker='.', linestyle='none',\n                 color='blue', alpha=0.02)\n# Compute and plot ECDF from original 'registered'\nx11 = np.sort(df_train['EXT_SOURCE_1'])\ny11 = np.arange(1,len(x11)+1)/float(len(x11)) \n\n_ = plt.plot(x11, y11, marker='.', color= 'red')\n\n# Compute and plot ECDF from original 'casual'\nx22 = np.sort(df_train['EXT_SOURCE_2'])\ny22 = np.arange(1,len(x22)+1)/float(len(x22)) \n\n_ = plt.plot(x22, y22, marker='.', color= 'blue')\n# Make margins and label axes\nplt.margins(0.02)\n_ = plt.xlabel('External Data Source Influence')\n_ = plt.ylabel('ECDF')\n\n# Show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d906b89f7d5ac393a31e804954bed2bbda2cee47"},"cell_type":"markdown","source":"Permutation samples ECDFs overlap and give a purple haze. Few of the ECDFs from the permutation samples overlap with the observed External Source Data1 data towards right of the graph & even fewer overlap towards left, suggesting that the hypothesis is not commensurate with the data. External Source Data1 & External Source Data2 are not identically distributed and do not influence data in similar way. So Null Hypothesis is rejected.\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}