{"cells":[{"metadata":{"_uuid":"46b8f8d4614ac73e8ef1e6fda30ef07032b9370e"},"cell_type":"markdown","source":"# A collection of useful (for me) functions"},{"metadata":{"_uuid":"e907c04955f81df3df2a7519d59e441075710ed5"},"cell_type":"markdown","source":"This is a collection of scripts which can be useful for this and next competitions, as I think.\n\nThere is an example of baseline at the end of this notebook."},{"metadata":{"_uuid":"7f6812c560eaccd3c78644f7aea0f71fdf85b550","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport datetime, time\nimport warnings\nwarnings.simplefilter(action = 'ignore', category = FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7936b6e2feaf8f5ea6b246571ad6435b767b086","trusted":true,"collapsed":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25daa4ec258223659d9328317dd7e512d7f2b89d","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, roc_curve, precision_score, recall_score\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import scale","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73a799ae3433c036622a7c1674eb169491158e1e","trusted":true,"collapsed":true},"cell_type":"code","source":"from scipy.stats import ranksums","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bafa1f36642ebcfb1db7143196a81b55a9f2fd28","trusted":true,"collapsed":true},"cell_type":"code","source":"from bayes_opt import BayesianOptimization","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e029efdc88b336e13eb667bd97b8d9cd93d928b5"},"cell_type":"markdown","source":"## Service functions"},{"metadata":{"_uuid":"1a7124912000a9150adb1d63181c9ee5c734a5e4","trusted":true,"collapsed":true},"cell_type":"code","source":"def reduce_mem_usage(df_):\n    start_mem = df_.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe: {:.2f} MB'.format(start_mem))\n    \n    for c in df_.columns[df_.dtypes != 'object']:\n        col_type = df_[c].dtype\n        \n        c_min = df_[c].min()\n        c_max = df_[c].max()\n        if str(col_type)[:3] == 'int':\n            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                df_[c] = df_[c].astype(np.int8)\n            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                df_[c] = df_[c].astype(np.int16)\n            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                df_[c] = df_[c].astype(np.int32)\n            else:\n                df_[c] = df_[c].astype(np.int64)  \n        else:\n            if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                df_[c] = df_[c].astype(np.float16)\n            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                df_[c] = df_[c].astype(np.float32)\n            else:\n                df_[c] = df_[c].astype(np.float64)\n\n    end_mem = df_.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54235cc8d7cbbe421f2eeb2424dc12650cd10796"},"cell_type":"markdown","source":"## For EDA"},{"metadata":{"_uuid":"708d0574f55b5a04c18cc21f4a2806d56fc2e2f0","trusted":true,"collapsed":true},"cell_type":"code","source":"# For exploring missing values across train and test sets\ndef get_missing_report(df_, target_name = 'TARGET'):\n    # Divide in training/validation and test data\n    df_train_ = df_[df_[target_name].notnull()].drop(target_name, axis = 1)\n    df_test_ = df_[df_[target_name].isnull()].drop(target_name, axis = 1)\n    \n    count_missing_train = df_train_.isnull().sum().values\n    ratio_missing_train = count_missing_train / df_train_.shape[0]\n    count_missing_test = df_test_.isnull().sum().values\n    ratio_missing_test = count_missing_test / df_test_.shape[0]\n    \n    return pd.DataFrame(data = {'count_missing_train': count_missing_train, \n                                'ratio_missing_train': ratio_missing_train,\n                                'count_missing_test': count_missing_test, \n                                'ratio_missing_test': ratio_missing_test}, \n                        index = df_test_.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5265a80c6eda9d260e657896fa7f86eb06a2a7fa","trusted":true,"collapsed":true},"cell_type":"code","source":"# For comparing distributions of two features\ndef corr_feature_with_target(feature, target):\n    c0 = feature[target == 0].dropna()\n    c1 = feature[target == 1].dropna()\n        \n    if set(feature.unique()) == set([0, 1]):\n        diff = abs(c0.mean(axis = 0) - c1.mean(axis = 0))\n    else:\n        diff = abs(c0.median(axis = 0) - c1.median(axis = 0))\n        \n    p = ranksums(c0, c1)[1] if ((len(c0) >= 20) & (len(c1) >= 20)) else 2\n        \n    return [diff / feature.mean(), p]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea9e441632c324b140427abf629e6a8b4c7416c6","trusted":true,"collapsed":true},"cell_type":"code","source":"# For selecting the best model for train set by simple launching the several classic models\ndef plot_roc_curves(df_train_, target_name, random_state = 0):\n    warnings.simplefilter('ignore')\n    \n    f_imp = pd.DataFrame(index = df_train_.columns.drop(target_name))\n    \n    X_trn, X_tst, y_trn, y_tst = train_test_split(df_train_.drop(target_name, axis = 1), \n                                                  df_train_[target_name], \n                                                  test_size = 0.2, random_state = random_state)\n\n    plt.figure(figsize = (7, 7))\n    plt.plot([0, 1], [0, 1], 'k--')\n\n    estimator = LGBMClassifier(random_state = random_state)\n    estimator.fit(X_trn, y_trn)\n    y_pred_xgb = estimator.predict_proba(X_tst)[:, 1]\n    fpr_xgb, tpr_xgb, _ = roc_curve(y_tst, y_pred_xgb)\n    f_imp['LGBM'] = pd.Series(estimator.feature_importances_, index = X_trn.columns)\n    plt.plot(fpr_xgb, tpr_xgb, label = 'LGBM: ' + str(roc_auc_score(y_tst, y_pred_xgb)))\n    \n    X_trn.fillna(X_trn.mean(axis = 0), inplace = True)\n    X_tst.fillna(X_tst.mean(axis = 0), inplace = True)\n        \n    estimator = RandomForestClassifier(random_state = random_state)\n    estimator.fit(X_trn, y_trn)\n    y_pred_rf = estimator.predict_proba(X_tst)[:, 1]\n    fpr_rf, tpr_rf, _ = roc_curve(y_tst, y_pred_rf)\n    f_imp['RF'] = estimator.feature_importances_\n    plt.plot(fpr_rf, tpr_rf, label = 'RF: ' + str(roc_auc_score(y_tst, y_pred_rf)))\n    \n    estimator = LogisticRegression(random_state = random_state)\n    estimator.fit(X_trn, y_trn)\n    y_pred_lrg = estimator.predict_proba(X_tst)[:, 1]\n    fpr_lrg, tpr_lrg, _ = roc_curve(y_tst, y_pred_lrg)\n    plt.plot(fpr_lrg, tpr_lrg, label = 'LogR: ' + str(roc_auc_score(y_tst, y_pred_lrg)))\n    \n    X_trn = pd.DataFrame(scale(X_trn), index = X_trn.index, columns = X_trn.columns)\n    X_tst = pd.DataFrame(scale(X_tst), index = X_tst.index, columns = X_tst.columns)\n    \n    estimator = KNeighborsClassifier()\n    estimator.fit(X_trn, y_trn)\n    y_pred_knn = estimator.predict_proba(X_tst)[:, 1]\n    fpr_knn, tpr_knn, _ = roc_curve(y_tst, y_pred_knn)\n    plt.plot(fpr_knn, tpr_knn, label = 'KNN: ' + str(roc_auc_score(y_tst, y_pred_knn)))\n    \n    del X_trn, X_tst, y_trn, y_tst\n    gc.collect()\n    \n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve')\n    plt.legend(loc = 'best')\n    plt.show()\n    \n    f_imp['mean'] = f_imp.mean(axis = 1)\n    return f_imp","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"116ac4509a5dde5e3d19458ffe9a35ddbae4266a"},"cell_type":"markdown","source":"## For cross-validation"},{"metadata":{"_uuid":"a53912f27ff407d306f88e66eee5139499956c16","trusted":true,"collapsed":true},"cell_type":"code","source":"# For saving scores and metrics\nscores_index = [\n        'roc_auc_train', 'roc_auc_test', \n        'precision_train_0', 'precision_test_0', \n        'precision_train_1', 'precision_test_1', \n        'recall_train_0', 'recall_test_0', \n        'recall_train_1', 'recall_test_1', \n        'LB'\n]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a66f8c36d1b94a3116649753af57f6957edf059","trusted":true,"collapsed":true},"cell_type":"code","source":"# For visual analysis of the metrics\ndef display_scores(df_scores_):\n    _, axes = plt.subplots(3, 2, figsize = (25, 10))\n    df_scores_.T[[scores_index[0]]].plot(ax = axes[0, 0]); # roc-auc train\n    df_scores_.T[[scores_index[1], scores_index[10]]].plot(ax = axes[0, 1]); # roc-auc test & LB\n    df_scores_.T[[scores_index[2], scores_index[3]]].plot(ax = axes[1, 0]);  # precision class 0\n    df_scores_.T[[scores_index[4], scores_index[5]]].plot(ax = axes[1, 1]);  # precision class 1\n    df_scores_.T[[scores_index[6], scores_index[7]]].plot(ax = axes[2, 0]);  # recall class 0\n    df_scores_.T[[scores_index[8], scores_index[9]]].plot(ax = axes[2, 1]);  # recall class 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38ea872db7eb9535dd9b3a428b7c28e292119987","trusted":true,"collapsed":true},"cell_type":"code","source":"# For cleaning float LGBM parameters after Bayesian optimization\ndef int_lgbm_params(params):\n    for p in params:\n        if p in ['num_leaves', 'max_depth', 'n_estimators', 'subsample_for_bin', 'min_child_samples', \n                 'subsample_freq', 'random_state']:\n            params[p] = int(np.round(params[p], decimals = 0))\n    return params","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4227e51680d196abd59e8e8c856141632d0eb614","trusted":true,"collapsed":true},"cell_type":"code","source":"# For cross-validation with LGBM classifier\ndef cv_lgbm_scores(df_, num_folds, params, \n                   target_name = 'TARGET', index_name = 'SK_ID_CURR',\n                   stratified = False, rs = 1001, verbose = -1):\n    \n    warnings.simplefilter('ignore')\n    \n    # Cleaning and defining parameters for LGBM\n    params = int_lgbm_params(params)\n    clf = LGBMClassifier(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n\n    # Divide in training/validation and test data\n    df_train_ = df_[df_[target_name].notnull()]\n    df_test_ = df_[df_[target_name].isnull()]\n    print(\"Starting LightGBM cross-validation at {}\".format(time.ctime()))\n    print(\"Train shape: {}, test shape: {}\".format(df_train_.shape, df_test_.shape))\n\n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits = num_folds, shuffle = True, random_state = rs)\n    else:\n        folds = KFold(n_splits = num_folds, shuffle = True, random_state = rs)\n        \n    # Create arrays to store results\n    train_pred = np.zeros(df_train_.shape[0])\n    train_pred_proba = np.zeros(df_train_.shape[0])\n\n    test_pred = np.zeros(df_train_.shape[0])\n    test_pred_proba = np.zeros(df_train_.shape[0])\n    \n    prediction = np.zeros(df_test_.shape[0]) # prediction for test set\n    \n    feats = df_train_.columns.drop([target_name, index_name])\n    \n    df_feat_imp_ = pd.DataFrame(index = feats)\n    \n    # Cross-validation cycle\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df_train_[feats], df_train_[target_name])):\n        print('--- Fold {} started at {}'.format(n_fold, time.ctime()))\n        \n        train_x, train_y = df_train_[feats].iloc[train_idx], df_train_[target_name].iloc[train_idx]\n        valid_x, valid_y = df_train_[feats].iloc[valid_idx], df_train_[target_name].iloc[valid_idx]\n\n        clf.fit(train_x, train_y, \n                eval_set = [(valid_x, valid_y)], eval_metric = 'auc', \n                verbose = verbose, early_stopping_rounds = 100)\n\n        train_pred[train_idx] = clf.predict(train_x, num_iteration = clf.best_iteration_)\n        train_pred_proba[train_idx] = clf.predict_proba(train_x, num_iteration = clf.best_iteration_)[:, 1]\n        test_pred[valid_idx] = clf.predict(valid_x, num_iteration = clf.best_iteration_)\n        test_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration = clf.best_iteration_)[:, 1]\n        \n        prediction += clf.predict_proba(df_test_[feats], \n                                        num_iteration = clf.best_iteration_)[:, 1] / folds.n_splits\n\n        df_feat_imp_[n_fold] = pd.Series(clf.feature_importances_, index = feats)\n        \n        del train_x, train_y, valid_x, valid_y\n        gc.collect()\n\n    # Computation of metrics\n    roc_auc_train = roc_auc_score(df_train_[target_name], train_pred_proba)\n    precision_train = precision_score(df_train_[target_name], train_pred, average = None)\n    recall_train = recall_score(df_train_[target_name], train_pred, average = None)\n    \n    roc_auc_test = roc_auc_score(df_train_[target_name], test_pred_proba)\n    precision_test = precision_score(df_train_[target_name], test_pred, average = None)\n    recall_test = recall_score(df_train_[target_name], test_pred, average = None)\n\n    print('Full AUC score {:.6f}'.format(roc_auc_test))\n    \n    # Filling the feature_importance table\n    df_feat_imp_.fillna(0, inplace = True)\n    df_feat_imp_['mean'] = df_feat_imp_.mean(axis = 1)\n    \n    # Preparing results of prediction for saving\n    prediction_train = df_train_[[index_name]]\n    prediction_train[target_name] = test_pred_proba\n    prediction_test = df_test_[[index_name]]\n    prediction_test[target_name] = prediction\n    \n    del df_train_, df_test_\n    gc.collect()\n    \n    # Returning the results and metrics in format for scores' table\n    return df_feat_imp_, prediction_train, prediction_test, \\\n           [roc_auc_train, roc_auc_test,\n            precision_train[0], precision_test[0], precision_train[1], precision_test[1],\n            recall_train[0], recall_test[0], recall_train[1], recall_test[1], 0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6dfe02a7bc94b5d0965a5e5c8a6f3b3e9ba3d1f","trusted":true,"collapsed":true},"cell_type":"code","source":"# For visual analysis of the fearure importances\ndef display_feature_importances(df_feat_imp_):\n    n_columns = 3\n    n_rows = (df_feat_imp_.shape[1] + 1) // n_columns\n    _, axes = plt.subplots(n_rows, n_columns, figsize=(8 * n_columns, 8 * n_rows))\n    for i, c in enumerate(df_feat_imp_.columns):\n        sns.barplot(x = c, y = 'index', \n                    data = df_feat_imp_.reset_index().sort_values(c, ascending = False).head(20), \n                    ax = axes[i // n_columns, i % n_columns] if n_rows > 1 else axes[i % n_columns])\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0243fe2d8ebcca6a2f7f4939f0a7e6cc03d5a995","trusted":true,"collapsed":true},"cell_type":"code","source":"# For selection of parameters for LGBM with Bayesian optimization\ndef get_best_params_for_lgbm(df_train_, seed_cv_, seed_bo_, target_name = 'TARGET', \n                             init_points = 5, n_iter = 5):\n    def lgbm_auc_evaluate(**params):\n        warnings.simplefilter('ignore')\n    \n        params = int_lgbm_params(params)   \n        clf = LGBMClassifier(**params, n_estimators = 10000, nthread = 4, n_jobs = -1)\n\n        folds = KFold(n_splits = 2, shuffle = True, random_state = params['random_state'])\n        \n        test_pred_proba = np.zeros(df_train_.shape[0])\n    \n        feats = df_train_.columns.drop(target_name)\n    \n        for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df_train_[feats], df_train_[target_name])):\n            train_x, train_y = df_train_[feats].iloc[train_idx], df_train_[target_name].iloc[train_idx]\n            valid_x, valid_y = df_train_[feats].iloc[valid_idx], df_train_[target_name].iloc[valid_idx]\n\n            clf.fit(train_x, train_y, \n                    eval_set = [(valid_x, valid_y)], eval_metric = 'auc', \n                    verbose = False, early_stopping_rounds = 100)\n\n            test_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration = clf.best_iteration_)[:, 1]\n        \n            del train_x, train_y, valid_x, valid_y\n            gc.collect()\n        \n        roc_auc_test = roc_auc_score(df_train_[target_name], test_pred_proba)\n        \n        return roc_auc_test\n    \n    params = {'learning_rate': (.001, .02), \n          'colsample_bytree': (0.3, 1),\n          'subsample_for_bin' : (20000, 500000),\n          'subsample': (0.3, 1), \n          'num_leaves': (2, 100), \n          'max_depth': (3, 9), \n          'reg_alpha': (.0, 1.), \n          'reg_lambda': (.0, 1.), \n          'min_split_gain': (.01, 1.),\n          'min_child_weight': (1, 50),\n          'min_child_samples': (10, 1000),\n          'random_state': (seed_cv_, seed_cv_)}\n    bo = BayesianOptimization(lgbm_auc_evaluate, params, random_state = seed_bo_)\n    bo.maximize(init_points = init_points, n_iter = n_iter)\n\n    return bo.res['max']['max_val'], bo.res['max']['max_params']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35f912dfdeaf101af217de988a38a286176dbd9f"},"cell_type":"markdown","source":"## For blending predictions"},{"metadata":{"_uuid":"a1121833998db9d77f21ba6a08b251c5f5367ce4","trusted":true,"collapsed":true},"cell_type":"code","source":"# For metadata about predictions\nblending_index = ['date', 'to_blend', 'folder', 'file_name', 'auc_train', 'auc_test', 'auc_LB', 'Comments']\nsuffix_train = '_train.csv'\nsuffix_test = '_test.csv'\nblending_folder = '../input/tmp-preds'\nblending_file_name = 'predictions_for_blending.csv'\n\ndf_blend = pd.DataFrame(index = blending_index)\ndf_blend.index.name = 'index'\n#df_blend.to_csv(blending_folder + '/' + blending_file_name) # Commented for Kaggle","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d99ef0e527343c4790466e10a87b1202b25d99c1","trusted":true,"collapsed":true},"cell_type":"code","source":"# For saving prediction into file\ndef save_prediction(df_, file_name, index_col = 'SK_ID_CURR', prediction_col = 'TARGET'):\n    df_.columns = [index_col, prediction_col]\n    df_.to_csv(file_name, index = False) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41c7f023f4954f59b89209125cbb61658d1fb72a","collapsed":true},"cell_type":"code","source":"# For saving files and metadata about predictions for blending\ndef store_predictions_for_blending(df_train_, df_test_, file_name, scor, comments, \n                                   index_col = 'SK_ID_CURR', prediction_col = 'TARGET', \n                                   folder = blending_folder, b_file_name = blending_file_name):\n    \n    full_file_name = folder + '/' + file_name\n    save_prediction(df_train_, full_file_name + suffix_train, index_col, prediction_col)\n    save_prediction(df_test_, full_file_name + suffix_test, index_col, prediction_col)\n    \n    df_blending = pd.read_csv(b_file_name, index_col = 'index')\n    df_blending[df_blending.shape[1]] = [\n        datetime.datetime.today(),\n        True,\n        folder, file_name,\n        scor[0], scor[1], scor[-1],\n        comments\n    ]\n    df_blending.to_csv(b_file_name)\n    del df_blending\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03d2ae48ad753b4b64c6152c2ffa1db4150212c6","collapsed":true},"cell_type":"code","source":"# For loading previous prediction results\ndef load_predictions_for_blending(b_folder = blending_folder, b_file_name = blending_file_name):\n    def load_prediction(file_name):\n        tmp = pd.read_csv(file_name)\n        return tmp.set_index(tmp.columns[0])\n\n    df_blend_ = pd.read_csv(b_folder + '/' + b_file_name, index_col = 'index')\n    df_train_ = []\n    df_test_ = []\n    for c in df_blend_.columns:\n        #full_file_name = df_blend_.loc['folder', c] + '/' + df_blend_.loc['file_name', c]\n        full_file_name = '../input/tmp-preds' + '/' + df_blend_.loc['file_name', c] # Only for Kaggle\n        df_train_.append(load_prediction(full_file_name + suffix_train))\n        df_test_.append(load_prediction(full_file_name + suffix_test))\n        \n    df_train_ = pd.concat(df_train_, axis = 1)\n    df_train_.columns = df_blend_.columns\n    df_test_ = pd.concat(df_test_, axis = 1)\n    df_test_.columns = df_blend_.columns\n\n    return df_train_, df_test_, df_blend_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f60bca412af025ce9ecc7ac6ff98b81d4811e0b","collapsed":true},"cell_type":"code","source":"# For blending flagged predictions\ndef get_blended_prediction(df_train_, flag, params_):\n    warnings.simplefilter('ignore')\n    \n    test_pred_proba = pd.Series(np.zeros(df_train_.shape[0]), index = df_train_.index)\n    \n    for f in df_train_.columns[flag.values.astype(bool)]:\n        test_pred_proba += df_train_[f] * params_[f]\n        \n    min_pr = test_pred_proba.min()\n    max_pr = test_pred_proba.max()\n    test_pred_proba = (test_pred_proba - min_pr) / (max_pr - min_pr)\n    return test_pred_proba","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0410a2b503b47389cd75e5a2a7d874d0edd62db1","trusted":true,"collapsed":true},"cell_type":"code","source":"# For selection of parameters with Bayesian optimization\ndef get_best_params_for_blending(df_train_, flag, target, seed_bo_, init_points = 10, n_iter = 10):\n    def blend_auc_evaluate(**params):\n        return roc_auc_score(target, get_blended_prediction(df_train_, flag, params))    \n    \n    params = {}\n    for c in df_train_.columns[flag.values.astype(bool)]:\n        params[c] = (0, 1)\n\n    bo = BayesianOptimization(blend_auc_evaluate, params, random_state = seed_bo_)\n    bo.maximize(init_points = init_points, n_iter = n_iter)\n\n    return bo.res['max']['max_val'], bo.res['max']['max_params']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f04a7c8011383493192b83eb0c8a5bf6674bbe5"},"cell_type":"markdown","source":"# Example of baseline"},{"metadata":{"_uuid":"8b6540dfc1f9996a6d1d6b7c4d5a817ace02a598"},"cell_type":"markdown","source":"This is just an example!"},{"metadata":{"_uuid":"3089330d2e3e9e795cdbd9da3a925aae32642532","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf0ba34c779f17e6db1266ec0b8e3958395f1c02"},"cell_type":"markdown","source":"### Loading datasets"},{"metadata":{"_uuid":"f0199f60c16631b05693c2aaee2f0f9cc35b9e8d","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\ndf_test = pd.read_csv('../input/home-credit-default-risk/application_test.csv')\nprint('Train shape: {}, test shape: {}'.format(df_train.shape, df_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbb4006fec2e21a74af4a4d7b07f58ad37ccba91","trusted":true},"cell_type":"code","source":"df_all = pd.concat([df_train, df_test], axis = 0, ignore_index = True)\nprint('Сombined shape: {}'.format(df_all.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"777dd3869f9988da3f88669956f5a882d03865e7","trusted":true},"cell_type":"code","source":"train_index = df_all[df_all['TARGET'].notnull()].index\ntest_index = df_all[df_all['TARGET'].isnull()].index\nprint('Train shape: {}, test shape: {}'.format(df_all.loc[train_index].shape, df_all.loc[test_index].shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b7e153d6dac4b1041f39a81d0f7563738e321c2","trusted":true},"cell_type":"code","source":"del df_train, df_test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de5082bc76b58f0e5f7efce3d817d44a69f63f9c"},"cell_type":"markdown","source":"### Convert categorical features\n\nOnly Label encoding for this example"},{"metadata":{"_uuid":"70c798c29b3011123a439947aca90c9b8b4ab428","trusted":true,"collapsed":true},"cell_type":"code","source":"le = LabelEncoder()\ncategorical = df_all.columns[df_all.dtypes == 'object']\nprint('{} categorical features were'.format(len(categorical)))\nfor c in categorical:\n    df_all[c].fillna('NaN', inplace = True)\n    df_all[c] = le.fit_transform(df_all[c])\nprint('{} categorical features left'.format(len(df_all.columns[df_all.dtypes == 'object'])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5e621f36eaec59a1ca99d9f387dcd7ac1a8cbfb","trusted":true,"collapsed":true},"cell_type":"code","source":"df_all = reduce_mem_usage(df_all)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cf84c25a23a5551427f3245ee2a1439706ebbd2"},"cell_type":"markdown","source":"### Exploring missing values"},{"metadata":{"_uuid":"a769128a08fcaa410b9668431f2b89391d07b7eb","trusted":true,"collapsed":true},"cell_type":"code","source":"missing_values = get_missing_report(df_all)\nmissing_values.sort_values('ratio_missing_train', ascending = False) \\\n                                    [['ratio_missing_train', 'ratio_missing_test']].plot(figsize = (25, 7));","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0ef0e9b25b1b7a80f6e0ccd1bd46670a7234b61","trusted":true,"collapsed":true},"cell_type":"code","source":"missing_values[abs(missing_values['ratio_missing_train'] - missing_values['ratio_missing_test']) > .1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8765c617f9038a5f091f70bde96c7604474e0028"},"cell_type":"markdown","source":"To drop `EXT_SOURCE_1` feature if it's not usefull in next explorations"},{"metadata":{"_uuid":"bbfce0888130d384f098d38784157fc685938acb"},"cell_type":"markdown","source":"### Exploring correlation of features between the train set and target"},{"metadata":{"_uuid":"772a036e55054fb2b2042a540b9797268eac3eb3","trusted":true,"collapsed":true},"cell_type":"code","source":"corr_target = pd.DataFrame(index = ['diff', 'p'])\nfor c in df_all.columns.drop('TARGET'):\n    corr_target[c] = corr_feature_with_target(df_all.loc[train_index, c], df_all.loc[train_index, 'TARGET'])\ncorr_target = corr_target.T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"148b294d238912c79302ac4a3fcb7138804cdd16","trusted":true,"collapsed":true},"cell_type":"code","source":"bad_features = corr_target[(corr_target['diff'] < 1e-5) & (corr_target['p'] > .05)].index\nprint('There are {} uninformative features'.format(len(bad_features)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a210c01870d15e88237c2c785c4387b648738aa6","trusted":true,"collapsed":true},"cell_type":"code","source":"bad_features","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd3dd87370a9376437460c4b9df021152a289758","trusted":true,"collapsed":true},"cell_type":"code","source":"df_all.drop(bad_features, axis = 1, inplace = True)\ndf_all.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5376cb8f1e88acebd25dee41ed1a88a1a5a6b068"},"cell_type":"markdown","source":"### Exploring correlation of features between the train and test sets"},{"metadata":{"_uuid":"a332eb210e1719ec018e967f3e2c186323115b29","trusted":true,"collapsed":true},"cell_type":"code","source":"target_test = (df_all['TARGET'].notnull()).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0eaa3ddd5393dfa52adffa82ff44fd32b06ebe43","trusted":true,"collapsed":true},"cell_type":"code","source":"corr_test = pd.DataFrame(index = ['diff', 'p'])\nfor c in df_all.columns.drop('TARGET'):\n    corr_test[c] = corr_feature_with_target(df_all[c], target_test)\ncorr_test = corr_test.T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87bde8f617b88096d021923d502682387ad43539","trusted":true,"collapsed":true},"cell_type":"code","source":"bad_features = corr_test[(corr_test['diff'] > 1) & (corr_test['p'] < .05)].index\nprint('There are {} features with different distribution on the train and test sets'.format(len(bad_features)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea3fb6d7766fdc3dd14dafceadec53ac0d367cc8","trusted":true,"collapsed":true},"cell_type":"code","source":"bad_features","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3fef118aa6fa9a30ff92ee30757c9e6e114d3b3","trusted":true,"collapsed":true},"cell_type":"code","source":"df_all.drop(bad_features, axis = 1, inplace = True)\ndf_all.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86b664cfad6674980f5b6e2dc9afb49c1254d99f"},"cell_type":"markdown","source":"### Selection the best classic model for this dataset"},{"metadata":{"_uuid":"eb3d9427738791a9a0f69b83c14631249987cb85","trusted":true,"collapsed":true},"cell_type":"code","source":"feature_importance = plot_roc_curves(df_all.loc[train_index], 'TARGET', random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abc12ab6e4c71873c9b5e8093bd4a00b199da3d8"},"cell_type":"markdown","source":"The most interesting model is LGBM with the first draft score .757. \n\nThe least interesting one is KNearest."},{"metadata":{"_uuid":"56dfbceefefa7b45c13ac163ec1c9fe4ec7d510d","trusted":true,"collapsed":true},"cell_type":"code","source":"feature_importance.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2388e6dcbcf1025aff5c0b766a447d5ae7f150d","trusted":true,"collapsed":true},"cell_type":"code","source":"display_feature_importances(feature_importance)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d2d681796a45c29471f7b9eb57408dc887065f4"},"cell_type":"markdown","source":"### Calculating the first metrics without Bayesian Optimization"},{"metadata":{"_uuid":"f15665dda4f28000560436b1f6cd87925643c51b","trusted":true,"collapsed":true},"cell_type":"code","source":"step = 'first_prediction'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2260fef4019d30b6d3c858d81df296d38102c1d6","trusted":true,"collapsed":true},"cell_type":"code","source":"seed_cv = 0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4b1176f64c5462ddf91e04ae9f9e2e0ab2ebb77","trusted":true,"collapsed":true},"cell_type":"code","source":"score_table = pd.DataFrame(index = scores_index)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a3cd3c206932719b6ba92c93ca8bc4acb6b9c25","trusted":true,"collapsed":true},"cell_type":"code","source":"f_importance, pred_train, pred_test, score = cv_lgbm_scores(df_all, \n                                                            num_folds = 5, params = {'random_state': seed_cv}, \n                                                            rs = seed_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fad1dae9fa9c86cf2bf4942d80946555432e93f0","collapsed":true},"cell_type":"code","source":"display_feature_importances(f_importance)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce2c23fc274431e2e86ef5c5d930f272128830c4","trusted":true,"collapsed":true},"cell_type":"code","source":"#save_prediction(pred_test, 'pred_' + step + suffix_test) # Commented for Kaggle","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f9687b0fb172bccd817e7cf7c856ef640a78ade"},"cell_type":"markdown","source":"To submit `pred_test` prediction and manually add real LB score in the next cell."},{"metadata":{"_uuid":"bf7e6505b134bf7b64b9d749ea47772282c79415","trusted":true,"collapsed":true},"cell_type":"code","source":"score[-1] = .745\nscore_table[step] = score\nscore_table.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc1343a5264f002b4c4e43e1e605750055e6aa70","collapsed":true},"cell_type":"code","source":"#store_predictions_for_blending(pred_train, pred_test, step, score, # Commented for Kaggle\n#                               comments = 'The first prediction without tuning')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b644296c8ebde91f48c348ca2e805ab6be261be5"},"cell_type":"markdown","source":"### Calculating the metrics with Bayesian Optimization (initial seeds)"},{"metadata":{"_uuid":"498960a38f83a21249423aee8385eb51356c039e","trusted":true,"collapsed":true},"cell_type":"code","source":"step = 'bo_seed_0'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0512ab55e13b7d34bb193801aa154f8c6a8a99c2","trusted":true,"collapsed":true},"cell_type":"code","source":"seed_bo = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0e5ebad60cb2ef31cb8a1c5028a440dcf1f3724","collapsed":true},"cell_type":"code","source":"best_score, best_params = get_best_params_for_lgbm(df_all.loc[train_index], seed_cv, seed_bo)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9ebf8fd5423705b6cf3eaa0ca3abb4994eb1fb3","trusted":true,"collapsed":true},"cell_type":"code","source":"print('Best score:', best_score)\nprint('Best params:', best_params)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"379e8a95c6368eac539cd7e84f4d86420204bc96","trusted":true,"collapsed":true},"cell_type":"code","source":"f_importance, pred_train, pred_test, score = cv_lgbm_scores(df_all, \n                                                            num_folds = 5, params = best_params, \n                                                            rs = seed_cv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"833294f54449520e44c2aed1abb6490a5a5a2605","trusted":true,"collapsed":true},"cell_type":"code","source":"#save_prediction(pred_test, 'pred_' + step + suffix_test) # Commented for Kaggle","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49afc3160cd77e005ecdd383be79a252157cf703","trusted":true,"collapsed":true},"cell_type":"code","source":"score[-1] = .748\nscore_table[step] = score\nscore_table.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee77f5626fb4ba7c3da10d15dc228912e2d29f2d","collapsed":true},"cell_type":"code","source":"display_scores(score_table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ef3bef18c820fd2962abeffa7bef3d5a5ffd9a3","collapsed":true},"cell_type":"code","source":"#store_predictions_for_blending(pred_train, pred_test, step, score, # Commented for Kaggle\n#                               comments = 'Best params for LGBM (seed_bo = 0)')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"191256e9720b6f9a78bfcf8a35cfe20930b531bb"},"cell_type":"markdown","source":"You can select the best seed for Bayesian Optimization and for CV. I cannot do it in this example."},{"metadata":{"_uuid":"b539ddbc78e0f810a92adddf294b837d286b44de"},"cell_type":"markdown","source":"### Blending predictions"},{"metadata":{"trusted":true,"_uuid":"6b01f1c936dbb32f223927afc02ce1c3d2610667"},"cell_type":"code","source":"df_train, df_test, df_blending = load_predictions_for_blending()\ndf_blending.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c8a92c906a5c611ad5526e4d31e46f9effb1e79"},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac3652c2d5af2797555b9aa677f1110256eacf8c"},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8558d6493f7d02e11aabe637656006eb42d8c8bd"},"cell_type":"code","source":"best_score, best_params = get_best_params_for_blending(df_train, df_blending.loc['to_blend'], \n                                                       df_all.loc[train_index, 'TARGET'], seed_bo)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"935293e4072fa01eba1b8962f29549928fdc18af"},"cell_type":"code","source":"print('Best score:', best_score)\nprint('Best params:', best_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13959ec6b84aab2d5ae71d7a1cb1c399c29b777f"},"cell_type":"code","source":"pred = get_blended_prediction(df_test, df_blending.loc['to_blend'], best_params)\npred.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee66ca101df333113d3b3f456cc6e3124ed37c01","collapsed":true},"cell_type":"code","source":"#save_prediction(pred.reset_index(), 'final.csv') # Commented for Kaggle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba084a666875ce43d46bc5e0de21cf0b6d96124c","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}