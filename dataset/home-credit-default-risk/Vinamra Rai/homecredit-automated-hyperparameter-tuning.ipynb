{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# Will Koehrsen's notebook used as a reference\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport lightgbm as lgb # modelling\n\n# evaluating the model\n\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n# visualization\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['font.size'] = 18\n%matplotlib inline\n\n# governing choices for search\n\nN_FOLDS = 5\nMAX_EVALS = 5\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"# immporting datasets and its splitting\n\nfeatures = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\n\n# sampling 16000 rows (10000 for training, 6000 for testing)\n\nfeatures = features.sample(n = 16000, random_state = 42)\n\n# selecting only numeric features\n\nfeatures = features.select_dtypes('number')\n\n# extracting labels\n\nlabels = np.array(features['TARGET'].astype(np.int32)).reshape((-1, ))\nfeatures = features.drop(columns = ['TARGET', 'SK_ID_CURR'])\n\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 6000, random_state = 42)\n\nprint('Train shape: ', train_features.shape)\nprint('Test shape: ', test_features.shape)\n\ntrain_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42125004fd314e7fbbacebfa3146d23ac3910320","collapsed":true},"cell_type":"code","source":"model = lgb.LGBMClassifier(random_state = 50)\n\n# training set\ntrain_set = lgb.Dataset(train_features, label = train_labels)\ntest_set = lgb.Dataset(test_features, label = test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc98d8bddaa4bb09f55a62c35c0630c19cd7ba84","collapsed":true},"cell_type":"code","source":"# default hyperparameters\nhyperparameters = model.get_params()\n\n# using early stopping to determine number of estimators and therefore deleting the default value\ndel hyperparameters['n_estimators']\n\n# performing cross validation with early stopping\ncv_results = lgb.cv(hyperparameters, train_set, num_boost_round=10000, nfold = N_FOLDS, metrics = 'auc', early_stopping_rounds=100, verbose_eval=False, seed = 42)\n\n# highest score\nbest = cv_results['auc-mean'][-1]\n\n# standard deviation of best score\nbest_std = cv_results['auc-stdv'][-1]\n\nprint('The maximium ROC AUC in cross validation was {:.5f} with std of {:.5f}.'.format(best, best_std))\nprint('The ideal number of iterations was {}.'.format(len(cv_results['auc-mean'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6563b35435ffcc3f8103d326f417f62d7e03ac00","collapsed":true},"cell_type":"code","source":"# evaluating the baseline model on the testing data\n# optimal number of estimators found in cv\n\nmodel.n_estimators = len(cv_results['auc-mean'])\n\n# training and making predictions with model\nmodel.fit(train_features, train_labels)\npreds = model.predict_proba(test_features)[:, 1]\nbaseline_auc = roc_auc_score(test_labels, preds)\n\nprint('The baseline model scores {:.5f} ROC-AUC on the test set.'.format(baseline_auc) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae0ba7165ee0d0d4daa71bc8c581c41a464f8c37","collapsed":true},"cell_type":"code","source":"# Objective Function which takes a set of Hyperparameter values and returns the cross validation score on the training data\n\n# the following excerpt about objective function in Hyperopt taken from Will Koehrsen's notebook for future reference\n# An objective function in Hyperopt must return either a single real value to minimize, or a dictionary with a key \"loss\" \n# with the score to minimize (and a key \"status\" indicating if the run was successful or not).\n\n# Optimization is typically about minimizing a value, and because our metric is Receiver Operating Characteristic Area Under the Curve (ROC AUC) \n# where higher is better, the objective function will return  1âˆ’ROC AUC Cross Validation\n\nimport csv\nfrom hyperopt import STATUS_OK\nfrom timeit import default_timer as timer\n\ndef objective(hyperparameters):\n    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Optimization.\n       Writes a new line to `outfile` on every iteration\n    \"\"\"\n    \n    # keeping track of evals\n    global ITERATION\n    ITERATION += 1\n    \n    # using early stopping to find number of trees trained\n    if 'n_estimators' in hyperparameters:\n        del hyperparameters['n_estimators']\n    \n    # retrieving the subsample\n    subsample = hyperparameters['boosting_type'].get('subsample', 1.0)\n    \n    # extracting the boosting types and subsample to top level keys\n    hyperparameters['boosting_type'] = hyperparameters['boosting_type']['boosting_type']\n    hyperparameters['subsample'] = subsample\n    \n    # ensuring the integer parameters remain as intergers\n    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n        hyperparameters[parameter_name] = int(hyperparameters[parameter_name])\n        \n    start = timer()\n    \n    # performing n_folds cv\n    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 1000, nfold = N_FOLDS, early_stopping_rounds = 100, metrics = 'auc', seed = 50)\n    \n    run_time = timer() - start\n    \n    # extracting the best score\n    best_score = cv_results['auc-mean'][-1]\n    \n    # minimizing loss\n    loss = 1-best_score\n    \n    # boosting rounds that returned the highest score\n    n_estimators = len(cv_results['auc-mean'])\n    \n    # adding the number of estimators to the hyperparameters\n    hyperparameters['n_estimators'] = n_estimators \n    \n    # writing to the csv file. 'a' mean append\n    of_connection = open(OUT_FILE, 'a')\n    writer = csv.writer(of_connection)\n    writer.writerow([loss, hyperparameters, ITERATION, run_time, best_score])\n    of_connection.close()\n    \n    # dictionary with information for evaluation\n    return{'loss': loss, 'hyperparameters': hyperparameters, 'iteration': ITERATION, 'train_time': run_time, 'status': STATUS_OK}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6be5c5e92c934046f6f90f260c91de75c3e5e1fa"},"cell_type":"code","source":"# specifying domain (known as space in Hyperopt) is different from grid search. In Hyperopt and other Bayesian opt. frameworks\n# the domain is not a discrete grid but it consists of probability distribution of different hyperparameters\n\nfrom hyperopt import hp\nfrom hyperopt.pyll.stochastic import sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bf982d57942db4b20c5e47ea8e82ffffccc13ef6"},"cell_type":"code","source":"# again like grid search we will go through example of learning rate which is defined on a log scale\n\n# creating the learning rate\nlearning_rate = {'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.2))}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83e5f29289550faa5a1842fa7fdc2d41dc5a177d","collapsed":true},"cell_type":"code","source":"# visualization of the learning rate\nlearning_rate_dist = []\n\n# drawing 1000 samples from the learning rate domain\nfor _ in range(10000):\n    learning_rate_dist.append(sample(learning_rate)['learning_rate'])\n    \nplt.figure(figsize = (8,6))\nsns.kdeplot(learning_rate_dist, color = 'red', linewidth = 2, shade = True);\nplt.title('Learning Rate Dist.', size = 18); plt.xlabel('Learning Rate', size = 16); plt.ylabel('Density', size = 16);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8dcdf0d2a67c953c2d3d5b79c43359f88071f530","collapsed":true},"cell_type":"code","source":"# on the other, no. of leaves is an uniform distribution\n\nnum_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\nnum_leaves_dist = []\n\n# sampling 10000 times from the number of leaves distribution\nfor _ in range(10000):\n    num_leaves_dist.append(sample(num_leaves)['num_leaves'])\n    \nplt.figure(figsize = (8,6))\nsns.kdeplot(num_leaves_dist, linewidth = 2, shade = True);\nplt.title('Number of Leaves Distribution', size = 18); \nplt.xlabel('Number of Leaves', size = 16); plt.ylabel('Density', size = 16);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b76e276fffbe9cc4d2447744cc81518d83f3ee7c","collapsed":true},"cell_type":"code","source":"# conditional domain\n# using nested conditional statements to indicate hyperparameters that depend on other hyperparameters\n# for example, \"goss\" boosting_type cannot use subsampling, therefore we have to preset the \"subsample\" value as 1.0 in its case\n\n# boosting type domain\nboosting_type = {'boosting_type': hp.choice('boosting_type', [{'boosting_type': 'gbdt', 'subsample': hp.uniform('subsample', 0.5, 1)}, \n                                                             {'boosting_type': 'dart', 'subsample': hp.uniform('subsample', 0.5, 1)},\n                                                             {'boosting_type': 'goss', 'subsample':1.0}])}\n\n# drawing a sample\nhyperparams = sample(boosting_type)\nhyperparams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd86d13d7b658bb580e505764db3476238425997","collapsed":true},"cell_type":"code","source":"# retrieve the subsample if present otherwise set it to a default of 1.0\nsubsample = hyperparams['boosting_type'].get('subsample', 1.0)\n\n# extracting the boosting type\nhyperparams['boosting_type'] = hyperparams['boosting_type']['boosting_type']\nhyperparams['subsample'] = subsample\n\nhyperparams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39173a00f0b389b4e96312c8929d82cfeba4a5d6","collapsed":true},"cell_type":"code","source":"# the gbm cannot use the distionary, therefore 'boosting_type' and 'subsample' have to be set as toplevel keys\n\n# COMPLETE BAYESIAN DOMAIN\n# defining the search space\n\nspace = {\n    'boosting_type': hp.choice('boosting_type', \n                                            [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n                                             {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n                                             {'boosting_type': 'goss', 'subsample': 1.0}]),\n    'num_leaves': hp.quniform('num_leaves', 20, 150, 1),\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),\n    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n    'is_unbalance': hp.choice('is_unbalance', [True, False]),\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c712bb7c858f1840911a90a6981dc173a20984f2","collapsed":true},"cell_type":"code","source":"# sampling from the full space (domain)\nx = sample(space)\n\n# conditional logic to assign top-level keys\nsubsample = x['boosting_type'].get('subsample', 1.0)\nx['boosting_type'] = x['boosting_type']['boosting_type']\nx['subsample'] = subsample\n\n# will different outputs each time its run\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"980e7a93c99b35795360f096ba3a8b3ed12adbfc","collapsed":true},"cell_type":"code","source":"# testing objective function with domain\n# creating a new file and opening a connection\n\nOUT_FILE = 'bayes_test.csv'\nof_connection = open(OUT_FILE, 'w')\nwriter = csv.writer(of_connection)\n\nITERATION = 0\n\n# writing column names\n\nheaders = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score']\nwriter.writerow(headers)\nof_connection.close()\n\n# testing the objective function\n# results vary everytime\nresults = objective(sample(space))\nprint('The cross validation loss = {:.5f}'.format(results['loss']))\nprint('The optimal number of estimators was {}.'.format(results['hyperparameters']['n_estimators']))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"91cfc924f7a6502b7d5cc3515647f4b3477ff315"},"cell_type":"code","source":"# OPTIMIZATION ALGORITHM\n\nfrom hyperopt import tpe\n\n# creating the algorithm\ntpe_algorithm = tpe.suggest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7b782ed3cb7093083de2a80748b6250143168f62"},"cell_type":"code","source":"# automated hyperparameter tuning is informed unlike grid and random methods\n# hyperopt internally keeps a track of the results for the algos to use, but if we want to monitor the results and have a saved copy of the search, storage of results is required\n# Trials object stores the dictionary returned from the objective function\n\nfrom hyperopt import Trials\n\n# recording results\ntrials = Trials()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f59a9eebf8f1f9213461817355d66fb1399b9716"},"cell_type":"code","source":"# Trials object will hold everything returned from objective functions in the .results attribute\n\n# Create a file and open a connection\nOUT_FILE = 'bayes_test.csv'\nof_connection = open(OUT_FILE, 'w')\nwriter = csv.writer(of_connection)\n\nITERATION = 0\n\n# Write column names\nheaders = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score']\nwriter.writerow(headers)\nof_connection.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7dfc0e40bad3cb24e24c6b1f8bdc03cfce09686c"},"cell_type":"code","source":"# now we have all the four parts of the hyperparameter optimization\n\nfrom hyperopt import fmin ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35031e88eb7fca10fe87e7968bf72b1b788d6275","collapsed":true},"cell_type":"code","source":"# fmin takes the four parts defined above as well as the maximum number of iterations max_evals\n\nglobal ITERATION\n\nITERATION = 0\n\n# running optimization\nbest = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials, max_evals = MAX_EVALS)\n\nbest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c500a95eba10dd5e55f2009bf1859ac324bf5d22","collapsed":true},"cell_type":"code","source":"# the best object holds only the hyperparameter which returned the lowest loss function\n\n# for understanding how each search progresses, inspection of Trials object or csv file is required\n\n# sorting the trials with the lowest loss (highest AUC) first\ntrials_dict = sorted(trials.results, key = lambda x: x['loss'])\ntrials_dict[:1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"acb3fd50887c11beaa4fcd49d3fd90f5282dffc2"},"cell_type":"code","source":"# reading the csv file\nresults = pd.read_csv(OUT_FILE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3a5ccbccb94294d965c01b0c7d9c65b3c91d0722"},"cell_type":"code","source":"# this function takes in the results, trains a model on the training data and evaluates on the testing data\n# returns a df of hyperparameters from the search. \n# saving the results to a csv file converts the dictionary of hyperparameters to a string\n# mapping that back to a dictionary using ast.literal_eval\n\nimport ast\n\ndef evaluate(results, name):\n    \"\"\"\n    Evaluate model on test data using hyperparameters in results\n    Return dataframe of hyperparameters\n    \"\"\"\n    new_results = results.copy()\n    # string to dictionary\n    new_results['hyperparameters'] = new_results['hyperparameters'].map(ast.literal_eval)\n    \n    # sorting with the best values on top\n    new_results = new_results.sort_values('score', ascending = False).reset_index(drop = True)\n    \n    # printing out cross validation high score\n    print('The highest cross validation score from {} was {:.5f} found on iteration {}.'.format(name, new_results.loc[0, 'score'], new_results.loc[0, 'iteration']))\n    \n    # using the best parameters to create a model\n    hyperparameters = new_results.loc[0, 'hyperparameters']\n    model = lgb.LGBMClassifier(**hyperparameters)\n    \n    # training and making predictions\n    model.fit(train_features, train_labels)\n    preds = model.predict_proba(test_features)[:, 1]\n    \n    print('ROC AUC from {} on test data = {:.5f}.'.format(name, roc_auc_score(test_labels, preds)))\n    \n    # creating dataframes of hyperparameters\n    hyp_df = pd.DataFrame(columns = list(new_results.loc[0, 'hyperparameters'].keys()))\n    \n    # iterating through each set of hyperparameters that were evaluated\n    for i, hyp in enumerate(new_results['hyperparameters']):\n        hyp_df = hyp_df.append(pd.DataFrame(hyp, index = [0]), ignore_index = True)\n        \n    # putting the iteration and score in hyperparameter dataframe\n    hyp_df['iteration'] = new_results['iteration']\n    hyp_df['score'] = new_results['score']\n    \n    return hyp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4abfa1a20497c590dbc1ac45a6e912906ff7ca29","collapsed":true},"cell_type":"code","source":"bayes_results = evaluate(results, name = 'Bayesian')\nbayes_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"45724319a018409911b450df847117574e67ace3"},"cell_type":"code","source":"# continuing optimization\n# Hyperopt can continue searching where a previous search left off if we pass in a  Trials object that already has results\nMAX_EVALS = 10\n\nbest = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials, max_evals = MAX_EVALS)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a5f21425982cb9de6496327278084ab7d62a21c3"},"cell_type":"code","source":"# saving the Trials object so it can be realater for more training\n\nimport json\n\nwith open('trials.json', 'w') as f:\n    f.write(json.dumps(trials_dict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"337fd1aaad74f5ee6d8499d000bff1c1dc76d2c8","collapsed":true},"cell_type":"code","source":"# to start the training from where it left off, simply load in the Trials object and pass it to an instance of fmin\nMAX_EVALS = 300\n\n# Create a new file and open a connection\nOUT_FILE = 'bayesian_trials_300.csv'\nof_connection = open(OUT_FILE, 'w')\nwriter = csv.writer(of_connection)\n\n# Write column names\nheaders = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score']\nwriter.writerow(headers)\nof_connection.close()\n\n# Record results\ntrials = Trials()\n\nglobal ITERATION\n\nITERATION = 0 \n\nbest = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials, max_evals = MAX_EVALS)\n\n# Sort the trials with lowest loss (highest AUC) first\ntrials_dict = sorted(trials.results, key = lambda x: x['loss'])\n\nprint('Finished, best results')\nprint(trials_dict[:1])\n\n# Save the trial results\nwith open('trials.json', 'w') as f:\n    f.write(json.dumps(trials_dict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dac0abd0c1e34a0b5076a09d3d740d65f82e09d","collapsed":true},"cell_type":"code","source":"# going through the results from 300 search iterations on the reduced dataset. Looking at the scores, the distribution of hyperparameter values tried, \n# the evolution of values over time, and compare the hyperparameters values to those from random search.\n\nbayes_results = pd.read_csv('../input/bayes-300/bayesian_trials_300.csv').sort_values('score', ascending = False).reset_index()\nbayes_params = evaluate(bayes_results, name = 'Bayesian')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3670b902231e36aeae0fbc5dda3aba559bc79fc2","collapsed":true},"cell_type":"code","source":"# getting all the score in a df in order to plot them over the course of training\n\nscores = pd.DataFrame({'ROC AUC': bayes_params['score'], 'iteration': bayes_params['iteration'], 'search': 'Bayesian'})\nscores['ROC AUC'] = scores['ROC AUC'].astype(np.float32)\nscores['iteration'] = scores['iteration'].astype(np.int32)\n\nscores.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0c7069db922921e3b9f22b993bc6c27a447af9f5"},"cell_type":"code","source":"# best scores for plotting the best hyperparameter values\n\nbest_bayes_params = bayes_params.iloc[bayes_params['score'].idxmax(), :].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95df1cc312c13b4090c6819d3b226d36b695865c","collapsed":true},"cell_type":"code","source":"# plot of scores over the course of searching\n\nsns.lmplot('iteration', 'ROC AUC', hue = 'search', data = scores, size = 8);\nplt.scatter(best_bayes_params['iteration'], best_bayes_params['score'], marker = '*', s = 40, c = 'orange', edgecolors='k')\nplt.xlabel('Iteration'); plt.ylabel('ROC AUC'); plt.title(\"Validation ROC AUC v/s Iteration\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c46c51cad503e4eaa55241010254ae3b5bbb1f4","collapsed":true},"cell_type":"code","source":"plt.figure(figsize = (20, 8))\nplt.rcParams['font.size'] = 18\n\n# density plots of the learning rate distribution\nsns.kdeplot(learning_rate_dist, label = 'Sampling Distribution', linewidth = 4)\nsns.kdeplot(bayes_params['learning_rate'], label = 'Bayesian Optimization', linewidth = 4)\nplt.legend()\nplt.xlabel('Learning Rate'); plt.ylabel('Density'); plt.title('Learning Rate Distribution');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56177650f40540768dc49c25c9fd7fb46a85a58f","collapsed":true},"cell_type":"code","source":"# we can make the same plot now for all the hyperparameters\n\n# iterating through each hyperparameter\nfor i, hyper in enumerate(bayes_params.columns):\n    if hyper not in ['class_weight', 'n_estimators', 'score', 'is_unbalance',\n                    'boosting_type', 'iteration', 'subsample', 'metric', 'verbose', 'loss', 'learning_rate']:\n        plt.figure(figsize = (14, 6))\n        if hyper != 'loss':\n            sns.kdeplot([sample(space[hyper]) for _ in range(1000)], label = 'Sampling Dist.', linewidth = 4)\n            sns.kdeplot(bayes_params[hyper], label = 'Bayes Opt.', linewidth = 4)\n            plt.legend()\n            plt.title('{} Distribution'.format(hyper))\n            plt.xlabel('{}'.format(hyper)); plt.ylabel('Density');\n            plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05a3f5fdd24dec48150ac79e94ac084594a22c63","collapsed":true},"cell_type":"code","source":"# evolution of hyperparameters over search\n\nfig, axs = plt.subplots(1, 4, figsize = (24, 6))\ni=0\n\nfor i, hyper in enumerate(['colsample_bytree', 'learning_rate', 'min_child_samples', 'num_leaves']):\n    sns.regplot('iteration', hyper, data = bayes_params, ax = axs[i])\n    axs[i].scatter(best_bayes_params['iteration'], best_bayes_params[hyper], marker = '*', s = 20, c = 'k')\n    axs[i].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = '{} over Search'.format(hyper));\n    \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c92b6c40a857e78dcd522d45fadc2bb370e22f0f","collapsed":true},"cell_type":"code","source":"# Read in full dataset\ntrain = pd.read_csv('../input/data-will/simple_features_train.csv')\ntest = pd.read_csv('../input/homecred-final-test/simple_features_test.csv')\n\n# Extract the test ids and train labels\ntest_ids = test['SK_ID_CURR']\ntrain_labels = np.array(train['TARGET'].astype(np.int32)).reshape((-1, ))\n\ntrain = train.drop(columns = ['SK_ID_CURR', 'TARGET'])\ntest = test.drop(columns = ['SK_ID_CURR'])\n\nprint('Training shape: ', train.shape)\nprint('Testing shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0c0fed6186404f62d292a7aa31044ec9abdb06d","collapsed":true},"cell_type":"code","source":"# APPLYING TO FULL DATASET\nbayes_results['hyperparameters'] = bayes_results['hyperparameters'].map(ast.literal_eval)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3af29281e9d1061801b98ae65c8b6b5206b9c6d6","collapsed":true},"cell_type":"code","source":"# bayesian optimization on full dataset\n\nhyperparameters = dict(**bayes_results.loc[0, 'hyperparameters'])\ndel hyperparameters['n_estimators']\n\n# cross validation with n_folds and early stopping\ncv_results = lgb.cv(hyperparameters, train_set, num_boost_round=10000, early_stopping_rounds=100, metrics = 'auc', nfold = N_FOLDS)\nprint('The cross validation score on the full dataset for Bayes Opt. = {:.5f} with std: {:.5f}.'.format(cv_results['auc-mean'][-1], cv_results['auc-stdv'][-1]))\nprint('Number of estimators = {}.'.format(len(cv_results['auc-mean'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5593bf3a854131bad53c0c39497e6b321b07d96a","collapsed":true},"cell_type":"code","source":"model = lgb.LGBMClassifier(n_estimators = len(cv_results['auc-mean']), **hyperparameters)\nmodel.fit(train, train_labels)\n\npreds = model.predict_proba(test)[:, 1]\n\nsubmission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': preds})\nsubmission.to_csv('submission_bayesian_optimization2.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}