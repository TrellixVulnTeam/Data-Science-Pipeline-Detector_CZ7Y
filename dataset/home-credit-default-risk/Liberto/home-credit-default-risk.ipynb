{"cells":[{"metadata":{"_uuid":"a8b7420e19fd3e8e1e9bf4c755fe6393d3b9825b"},"cell_type":"markdown","source":"# Introduction\nThis notebook is based on Will Koehrsen's kernel titled [Start Here: A Gentle Introduction](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction). This kernel is for practice only, to learn about Kaggle competition first hand."},{"metadata":{"_uuid":"eb286b10e9f2176530aff02da78f34da4d0d0c6d"},"cell_type":"markdown","source":"# Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# numpy and pandas for data manipulation \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system management\nimport os\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib.and seaborn for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30d264c5688135f71b5180a5ed8cf42c48f9aa9d"},"cell_type":"markdown","source":"# Read in Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# List available files\nprint(os.listdir('../input/'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72e34a874adde12d8198c74b522e107c74686562"},"cell_type":"code","source":"# Read training data\napp_train = pd.read_csv('../input/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb40ea8dd5dfc9d697140d36ec625a8f17127aaa"},"cell_type":"markdown","source":"The training data has 307511 observations (each one a separate loan) and 122 features, including the TARGET label (which we want to predict)."},{"metadata":{"trusted":true,"_uuid":"9a1c027f809c634e2d6eb90e65069ae94cefcacf"},"cell_type":"code","source":"# Read testing data\napp_test = pd.read_csv('../input/application_test.csv')\nprint('Testing data shape:', app_test.shape)\napp_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59776e617a7e7458d6adab8f99c678fb8fe084c4"},"cell_type":"markdown","source":"# Exploratory Data Analysis\n"},{"metadata":{},"cell_type":"markdown","source":"## Examine the Distribution of the Target Column"},{"metadata":{"trusted":true,"_uuid":"89d7aa85c06c5505aba57939aaf4d220798c7a53"},"cell_type":"code","source":"app_train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['TARGET'].astype(int).plot.hist();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imbalanced class problem: there are far more repaid loan than loans that were not repaid."},{"metadata":{},"cell_type":"markdown","source":"## Examine Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_values_table(df):\n    '''\n    Function to calculate missing values by column\n    Arguments:\n        df: dataframe\n    Output:\n        miss_val_table_renamed_cols: dataframe with missing values\n    '''\n    \n    # Total missing values:\n    miss_value = df.isnull().sum()\n    \n    # Missing values in percent:\n    miss_value_pct = 100 * df.isnull().sum() / len(df)\n    \n    # Make a table with the results\n    miss_val_table = pd.concat([miss_value, miss_value_pct], axis=1)\n    \n    # Rename the columns\n    miss_val_table_renamed_cols = miss_val_table.rename(columns={0: 'Missing Values', 1: '% of Total Values'})\n    \n    # Sort the table by percentage of missing, descending\n    miss_val_table_renamed_cols = miss_val_table_renamed_cols[\n                                  miss_val_table_renamed_cols.iloc[:, 1] != 0].sort_values('% of Total Values', ascending = False).round(1)\n    \n    # Print some summary information\n    print('Your selected dataframe has ' + str(df.shape[1]) \n          + ' columns.\\n')\n    print('There are' + str(miss_val_table_renamed_cols.shape[0]) +\n          ' columns that have missing values')\n    \n    return miss_val_table_renamed_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values = missing_values_table(app_train)\nmissing_values.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Column Types"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of each type of column\napp_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of unique entries in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most object-type columns have small number of unique entries."},{"metadata":{},"cell_type":"markdown","source":"## Encoding Categorical Variables"},{"metadata":{},"cell_type":"markdown","source":"### Label Encoding\nFor any categorical variable (`dtype == object`) with 2 unique categories, we will use label encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype =='object':\n        \n        # If 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            \n            # Train on the training data\n            le.fit(app_train[col])\n            \n            # Transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            print(col + ' column was label encoded.')\n            \nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One-Hot Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Align training and test data\n\ntrain_labels = app_train['TARGET']\n\napp_train, app_test = app_train.align(app_test, join='inner', axis=1)\n\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Back to EDA"},{"metadata":{},"cell_type":"markdown","source":"### Anomalies\nUse the `describe` method to find anomalies."},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['DAYS_BIRTH'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DAYS_BIRTH column was negative because it was recorded against current loan application. Multiply by -1 and divide by 365.25 to get the number of years."},{"metadata":{"trusted":true},"cell_type":"code","source":"(app_train['DAYS_BIRTH'] / -365.25).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(app_train['DAYS_EMPLOYED'] / -365.25).describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Minimum employed is *minus* 1000 years!"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Anayze the default rate for anomalies:"},{"metadata":{"trusted":true},"cell_type":"code","source":"anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The anomalies have a lower level of default. They might have some importance.\nSolution: fill the anomalies with `np.nan` and then create a new boolean column to indicate the anomaly."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create an anomalous flag olumn\napp_train['DAYS_EMPLOYED_ANOM'] = app_train['DAYS_EMPLOYED'] == 365243\n\n# Replace the anomalous values with NaN\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace=True)\n\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do the same with test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find correlations with the target and sort\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the client gets older, the repayment level is better, but the correlation is kinda weak."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert DAYS_BIRTH column to absolute value\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\n\nplt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor='k', bins=25)\nplt.title('Age of Client');\nplt.xlabel('Age (years)');\nplt.ylabel('Count');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To visualize the effect of the age on the target, use kernel density estimation plot (KDE)."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\n\n# KDE plot on loans that were repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, \n                          'DAYS_BIRTH'] / 365, label = 'target =0')\n\n# KDE plot on loans which were not repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1,\n                          'DAYS_BIRTH'] / 365, label = 'target = 1')\n\n# Labeling of plot\nplt.xlabel('Age (years)')\nplt.ylabel = 'Density'\nplt.title = 'Distribution of Ages'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Age information into a separate dataframe\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH']/365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], \n                                  bins=np.linspace(20, 70, num=11))\nage_data.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group by the bind and calculate averages\nage_groups = age_data.groupby('YEARS_BINNED').mean()\nage_groups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exterior Sources"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6))\n\n# Heatmap of correlation\nsns.heatmap(ext_data_corrs, cmap=plt.cm.RdYlBu_r, vmin=-0.25,\n            annot=True, vmax=0.6)\nplt.title('Correlation HeatMap')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"## Polynomial Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a new datafrane for polynomial features\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n                           'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n                           'DAYS_BIRTH']]\n\n# Imputer for handling missing values\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy='median')\npoly_target = poly_features['TARGET']\npoly_features = poly_features.drop(columns=['TARGET'])\n\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Create the polynomial object with specified degreee\npoly_transformer = PolynomialFeatures(degree= 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the polynomial\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe of the features \npoly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Add in the target\npoly_features['TARGET'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Merge polynomial features into training dataframe\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n\n# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Domain Knowledge Features\n\n* `CREDIT_INCOME_PERCENT`: the percentage of the credit amount relative to a client's income\n* `ANNUITY_INCOME_PERCENT`: the percentage of the loan annuity relative to a client's income\n* `CREDIT_TERM`: the length of the payment in months (since the annuity is the monthly amount due\n* `DAYS_EMPLOYED_PERCENT`: the percentage of the days employed relative to the client's age"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline"},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression Implementation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, Imputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns=['TARGET'])\nelse:\n    train = app_train.copy\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# Median imputation of missing values\nimputer = Imputer(strategy='median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range=(0,1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain  = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape : ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(C=0.0001)\nlog_reg.fit(train, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_red_pred = log_reg.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_red_pred\n\nsubmit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.to_csv('log_reg_baseline.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}