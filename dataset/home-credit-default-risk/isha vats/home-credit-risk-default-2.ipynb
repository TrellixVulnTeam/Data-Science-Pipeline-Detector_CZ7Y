{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#loading the train data\ntrain=pd.read_csv(\"../input/application_train.csv\")\nprint('Size of application_train data', train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f32c26f5422b2e19767a4030b90ca4a1953fe1e"},"cell_type":"code","source":"#loading the test data\ntest=pd.read_csv(\"../input/application_test.csv\")\nprint('Size of application_test data', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e9b90db7bbf8702fa90b26607ce002feb8cc91e"},"cell_type":"code","source":"#Distribution of target variables\ntrain['TARGET'].value_counts()\ntrain['TARGET'].astype(int).plot.hist();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb20d18bef181a01b050effa0b3e8942a4d4149e"},"cell_type":"code","source":"#Finding missing values\ntotal = train.isnull().sum().sort_values(ascending = False)\npercent = (train.isnull().sum()/train.isnull().count()*100).sort_values(ascending = False)\nmissing_train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_train_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c96eebc281f535bf8b8bb21f727e40d399eeaa0"},"cell_type":"code","source":"total = test.isnull().sum().sort_values(ascending = False)\npercent = (test.isnull().sum()/test.isnull().count()*100).sort_values(ascending = False)\nmissing_test_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_test_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37df7a97605af378d9fd8764231593c31fd5ef0b"},"cell_type":"code","source":"train.dtypes.value_counts()\ntrain.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cdbe75fadc19342ba9eabd9392fb8d25aa78261"},"cell_type":"code","source":"# Create a label encoder object\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in train:\n    if train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(train[col])\n            # Transform both training and testing data\n            train[col] = le.transform(train[col])\n            test[col] = le.transform(test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab4151eaf33d4065b1d91c76f9a7adcc9f2785b0"},"cell_type":"code","source":"# one-hot encoding of categorical variables\ntrain = pd.get_dummies(train)\ntest = pd.get_dummies(test)\n\nprint('Training Features shape: ', train.shape)\nprint('Testing Features shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2984afa5fd8b355c0e9bdbe3c88889e08c4fb115"},"cell_type":"code","source":"train_labels = train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\ntrain, test = train.align(test, join = 'inner', axis = 1)\n\n# Add the target back in\ntrain['TARGET'] = train_labels\n\nprint('Training Features shape: ', train.shape)\nprint('Testing Features shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b669b4f7dc2858a4c38276887cd11cda5cfbb3d"},"cell_type":"code","source":"# Find correlations with the target and sort\ncorrelations = train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84e6e56257bbb738263c51783a4abcab3aa2ea74"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, Imputer\n\n# Drop the target from the training data\nif 'TARGET' in train:\n    train_data = train.drop(columns = ['TARGET'])\nelse:\n    train_data = train.copy()\n    \n# Feature names\nfeatures = list(train_data.columns)\n\n# Copy of the testing data\ntest_data = test.copy()\n\n# Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(train_data)\n\n# Transform both training and testing data\ntrain_data = imputer.transform(train_data)\ntest_data = imputer.transform(test)\n# Repeat with the scaler\nscaler.fit(train_data)\ntrain_data = scaler.transform(train_data)\ntest_data = scaler.transform(test_data)\n\nprint('Training data shape: ', train_data.shape)\nprint('Testing data shape: ', test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c05ba75b61f7632ab680e0bdf992ed55d2b1fc14"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(train_data,train_labels,test_size=0.25,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef38f0298901c9c0a89ad16981f6cb0648c7f4e5"},"cell_type":"code","source":"#Building the logistice regression model\nfrom sklearn.linear_model import LogisticRegression\n\n# instantiate the model (using the default parameters)\nlogreg = LogisticRegression()\n\n# fit the model with data\nlogreg.fit(X_train,y_train)\n\n#\ny_pred=logreg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b8947e72b4d3f0043e9870308cd46298027b89f"},"cell_type":"code","source":"from sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\ncnf_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c453c67a08efefb3bf1705381fa9ffde4f45792"},"cell_type":"code","source":"#Analysis of confusion matrix\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0661b6806ffc296bb5f0e715dd545fa646161a92"},"cell_type":"code","source":"#ROC curve\ny_pred_proba = logreg.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f2303408a824755656993722f9ff4c76ee17358"},"cell_type":"code","source":"submit = test[['SK_ID_CURR']]\nsubmit['TARGET'] = y_pred\n\nsubmit.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}