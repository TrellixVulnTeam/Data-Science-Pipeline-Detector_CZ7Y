{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Calculated the running avg of predicted probability with OOF Prediction","metadata":{"id":"aldQtXhkCeVR"}},{"cell_type":"markdown","source":"### Load Package","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os, sys \n\n%matplotlib inline\n\npd.set_option('display.max_rows', 300)\npd.set_option('display.max_columns', 200)","metadata":{"id":"C09IrTNHCeVX","executionInfo":{"status":"ok","timestamp":1599725019377,"user_tz":-540,"elapsed":1230,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"outputId":"9fa884e3-3a0b-47dc-baab-8dcb9a09d9fb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 코랩 버전은 Google Drive에서 데이터 세트를 로딩","metadata":{"id":"4ld91aBtCsnw"}},{"cell_type":"code","source":"platform = 'kaggle'\ndefault_dir = None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if platform == 'google':\n    # google drive\n    from google.colab import drive \n    drive.mount('/content/gdrive')\n    default_dir = \"/content/gdrive/My Drive\"\nelse:\n    default_dir = \"../input/home-credit-default-risk/\"","metadata":{"id":"ZfQUza7rCsBA","executionInfo":{"status":"ok","timestamp":1599725172675,"user_tz":-540,"elapsed":20138,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"outputId":"effd0963-fa2d-4145-d80c-7020d817ade8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### load data as pandas DataFrame","metadata":{"id":"exeQaOvKCeVp"}},{"cell_type":"code","source":"def get_dataset():\n    \"\"\"\n    Minimize memory allocated usage\n    \"\"\"\n    prev_dtype = {\n        'SK_ID_PREV':np.uint32, 'SK_ID_CURR':np.uint32, 'HOUR_APPR_PROCESS_START':np.int32, 'NFLAG_LAST_APPL_IN_DAY':np.int32,\n        'DAYS_DECISION':np.int32, 'SELLERPLACE_AREA':np.int32, 'AMT_ANNUITY':np.float32, 'AMT_APPLICATION':np.float32,\n        'AMT_CREDIT':np.float32, 'AMT_DOWN_PAYMENT':np.float32, 'AMT_GOODS_PRICE':np.float32, 'RATE_DOWN_PAYMENT':np.float32,\n        'RATE_INTEREST_PRIMARY':np.float32, 'RATE_INTEREST_PRIVILEGED':np.float32, 'CNT_PAYMENT':np.float32,\n        'DAYS_FIRST_DRAWING':np.float32, 'DAYS_FIRST_DUE':np.float32, 'DAYS_LAST_DUE_1ST_VERSION':np.float32,\n        'DAYS_LAST_DUE':np.float32, 'DAYS_TERMINATION':np.float32, 'NFLAG_INSURED_ON_APPROVAL':np.float32\n    }\n    \n    bureau_dtype = {\n        'SK_ID_CURR':np.uint32, 'SK_ID_BUREAU':np.uint32, 'DAYS_CREDIT':np.int32,'CREDIT_DAY_OVERDUE':np.int32,\n        'CNT_CREDIT_PROLONG':np.int32, 'DAYS_CREDIT_UPDATE':np.int32, 'DAYS_CREDIT_ENDDATE':np.float32,\n        'DAYS_ENDDATE_FACT':np.float32, 'AMT_CREDIT_MAX_OVERDUE':np.float32, 'AMT_CREDIT_SUM':np.float32,\n        'AMT_CREDIT_SUM_DEBT':np.float32, 'AMT_CREDIT_SUM_LIMIT':np.float32, 'AMT_CREDIT_SUM_OVERDUE':np.float32,\n        'AMT_ANNUITY':np.float32\n    }\n    \n    bureau_bal_dtype = {\n        'SK_ID_BUREAU':np.int32, 'MONTHS_BALANCE':np.int32,\n    }\n    \n    pos_dtype = {\n        'SK_ID_PREV':np.uint32, 'SK_ID_CURR':np.uint32, 'MONTHS_BALANCE':np.int32, 'SK_DPD':np.int32,\n        'SK_DPD_DEF':np.int32, 'CNT_INSTALMENT':np.float32,'CNT_INSTALMENT_FUTURE':np.float32\n    }\n    \n    install_dtype = {\n        'SK_ID_PREV':np.uint32, 'SK_ID_CURR':np.uint32, 'NUM_INSTALMENT_NUMBER':np.int32, 'NUM_INSTALMENT_VERSION':np.float32,\n        'DAYS_INSTALMENT':np.float32, 'DAYS_ENTRY_PAYMENT':np.float32, 'AMT_INSTALMENT':np.float32, 'AMT_PAYMENT':np.float32\n    }\n    \n    card_dtype = {\n        'SK_ID_PREV':np.uint32, 'SK_ID_CURR':np.uint32, 'MONTHS_BALANCE':np.int16,\n        'AMT_CREDIT_LIMIT_ACTUAL':np.int32, 'CNT_DRAWINGS_CURRENT':np.int32, 'SK_DPD':np.int32,'SK_DPD_DEF':np.int32,\n        'AMT_BALANCE':np.float32, 'AMT_DRAWINGS_ATM_CURRENT':np.float32, 'AMT_DRAWINGS_CURRENT':np.float32,\n        'AMT_DRAWINGS_OTHER_CURRENT':np.float32, 'AMT_DRAWINGS_POS_CURRENT':np.float32, 'AMT_INST_MIN_REGULARITY':np.float32,\n        'AMT_PAYMENT_CURRENT':np.float32, 'AMT_PAYMENT_TOTAL_CURRENT':np.float32, 'AMT_RECEIVABLE_PRINCIPAL':np.float32,\n        'AMT_RECIVABLE':np.float32, 'AMT_TOTAL_RECEIVABLE':np.float32, 'CNT_DRAWINGS_ATM_CURRENT':np.float32,\n        'CNT_DRAWINGS_OTHER_CURRENT':np.float32, 'CNT_DRAWINGS_POS_CURRENT':np.float32, 'CNT_INSTALMENT_MATURE_CUM':np.float32\n    }\n    \n    app_train = pd.read_csv(os.path.join(default_dir, 'application_train.csv'))\n    app_test = pd.read_csv(os.path.join(default_dir, 'application_test.csv'))\n    apps = pd.concat([app_train, app_test])\n    prev = pd.read_csv(os.path.join(default_dir, 'previous_application.csv'), dtype=prev_dtype)\n    bureau = pd.read_csv(os.path.join(default_dir, 'bureau.csv'), dtype=bureau_dtype)\n    bureau_bal = pd.read_csv(os.path.join(default_dir, 'bureau_balance.csv'), dtype=bureau_bal_dtype)\n    pos_bal = pd.read_csv(os.path.join(default_dir, 'POS_CASH_balance.csv'), dtype=pos_dtype)\n    install = pd.read_csv(os.path.join(default_dir, 'installments_payments.csv'), dtype=install_dtype)\n    card_bal = pd.read_csv(os.path.join(default_dir, 'credit_card_balance.csv'), dtype=card_dtype)\n\n    return apps, prev, bureau, bureau_bal, pos_bal, install, card_bal","metadata":{"id":"RBOWiVA-CeVt","executionInfo":{"status":"ok","timestamp":1599725316475,"user_tz":-540,"elapsed":1662,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modulization for Feature Engineering","metadata":{"id":"5ocmIjSwCeWD"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_apps_processed(apps):\n    '''\n    feature engineering for apps - current loan\n    including customer information\n    '''\n    # EXT_SOURCE_X FEATURE\n    apps['APPS_EXT_SOURCE_MEAN'] = apps[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n    apps['APPS_EXT_SOURCE_STD'] = apps[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n    apps['APPS_EXT_SOURCE_STD'] = apps['APPS_EXT_SOURCE_STD'].fillna(apps['APPS_EXT_SOURCE_STD'].mean())\n    \n    # AMT_ANNUITY - amount should be paid per month.\n    # AMT_CREDIT  - total amount of loan.\n    # AMT_GOODS_PRICE : consumer loadn.eg) car purchase installment.\n    apps['APPS_ANNUITY_CREDIT_RATIO'] = apps['AMT_ANNUITY']/apps['AMT_CREDIT']\n    apps['APPS_GOODS_CREDIT_RATIO'] = apps['AMT_GOODS_PRICE']/apps['AMT_CREDIT']\n    \n    # AMT_INCOME_TOTAL : income \n    # CNT_FAM_MEMBERS  : the number of family members\n    apps['APPS_ANNUITY_INCOME_RATIO'] = apps['AMT_ANNUITY']/apps['AMT_INCOME_TOTAL']\n    apps['APPS_CREDIT_INCOME_RATIO'] = apps['AMT_CREDIT']/apps['AMT_INCOME_TOTAL']\n    apps['APPS_GOODS_INCOME_RATIO'] = apps['AMT_GOODS_PRICE']/apps['AMT_INCOME_TOTAL']\n    apps['APPS_CNT_FAM_INCOME_RATIO'] = apps['AMT_INCOME_TOTAL']/apps['CNT_FAM_MEMBERS']\n    \n    # DAYS_BIRTH : Client's age in days at the time of application\n    # DAYS_EMPLOYED : How many days before the application the person started current employment\n    apps['APPS_EMPLOYED_BIRTH_RATIO'] = apps['DAYS_EMPLOYED']/apps['DAYS_BIRTH']\n    apps['APPS_INCOME_EMPLOYED_RATIO'] = apps['AMT_INCOME_TOTAL']/apps['DAYS_EMPLOYED']\n    apps['APPS_INCOME_BIRTH_RATIO'] = apps['AMT_INCOME_TOTAL']/apps['DAYS_BIRTH']\n    apps['APPS_CAR_BIRTH_RATIO'] = apps['OWN_CAR_AGE'] / apps['DAYS_BIRTH']\n    apps['APPS_CAR_EMPLOYED_RATIO'] = apps['OWN_CAR_AGE'] / apps['DAYS_EMPLOYED']\n    \n    return apps\n\ndef get_prev_processed(prev):\n    \"\"\"\n    feature engineering for previous credit amount.\n    \"\"\"\n    # AMT_APPLICATION : For how much credit did client ask on the previous application\n    # AMT_GOODS_PRICE : Goods price of good that client asked for (if applicable) on the previous application\n    prev['PREV_CREDIT_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n    prev['PREV_GOODS_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_GOODS_PRICE']\n    prev['PREV_CREDIT_APPL_RATIO'] = prev['AMT_CREDIT']/ prev['AMT_APPLICATION']\n    # prev['PREV_ANNUITY_APPL_RATIO'] = prev['AMT_ANNUITY']/prev['AMT_APPLICATION']\n    prev['PREV_GOODS_APPL_RATIO'] = prev['AMT_GOODS_PRICE'] / prev['AMT_APPLICATION']   \n    \n    # data cleansing\n    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n    \n    # important features for determine days overdue\n    # 1.PREV_INTERESTS_RATE : interest ratio\n    \n    prev['PREV_DAYS_LAST_DUE_DIFF'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_LAST_DUE']\n    \n    # 매월 납부 금액과 납부 횟수 곱해서 전체 납부 금액 구함. \n    # AMT_ANNUITY : Annuity of previous application\n    all_pay = prev['AMT_ANNUITY'] * prev['CNT_PAYMENT']\n    # 전체 납부 금액 대비 AMT_CREDIT 비율을 구하고 여기에 다시 납부횟수로 나누어서 이자율 계산. \n    prev['PREV_INTERESTS_RATE'] = (all_pay/prev['AMT_CREDIT'] - 1)/prev['CNT_PAYMENT']\n        \n    return prev\n    \n    \ndef get_prev_amt_agg(prev):\n    '''\n    A newly creatd feature aggregation \n    새롭게 생성된 대출 신청액 대비 다른 금액 차이 및 비율로 aggregation 수행. \n    '''\n    agg_dict = {\n         # 기존 컬럼 aggregation. \n        'SK_ID_CURR':['count'],\n        'AMT_CREDIT':['mean', 'max', 'sum'],\n        'AMT_ANNUITY':['mean', 'max', 'sum'], \n        'AMT_APPLICATION':['mean', 'max', 'sum'],\n        'AMT_DOWN_PAYMENT':['mean', 'max', 'sum'],\n        'AMT_GOODS_PRICE':['mean', 'max', 'sum'],\n        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n        'DAYS_DECISION': ['min', 'max', 'mean'],\n        'CNT_PAYMENT': ['mean', 'sum'],\n        # 가공 컬럼 aggregation\n        'PREV_CREDIT_DIFF': ['mean', 'max', 'sum'],\n        'PREV_CREDIT_APPL_RATIO':['mean', 'max'],\n        'PREV_GOODS_DIFF':['mean', 'max', 'sum'],\n        'PREV_GOODS_APPL_RATIO':['mean', 'max'],\n        'PREV_DAYS_LAST_DUE_DIFF':['mean', 'max', 'sum'],\n        'PREV_INTERESTS_RATE':['mean', 'max']\n    }\n    prev_group = prev.groupby('SK_ID_CURR')\n    prev_amt_agg = prev_group.agg(agg_dict)\n    \n    # multi index 컬럼을 '_'로 연결하여 컬럼명 변경\n    prev_amt_agg.columns = [\"PREV_\"+ \"_\".join(x).upper() for x in prev_amt_agg.columns.ravel()]\n    return prev_amt_agg\n\ndef get_prev_refused_appr_agg(prev):\n    '''\n    Groupby SK_ID_CURR + MANE_CONTRACT_STATUS\n    원래 groupby 컬럼 + 세부 기준 컬럼으로 groupby 수행. 세분화된 레벨로 aggregation 수행 한 뒤에 unstack()으로 컬럼레벨로 변형\n    '''\n    prev_refused_appr_group = prev[prev['NAME_CONTRACT_STATUS'].isin(['Approved', 'Refused'])].groupby(['SK_ID_CURR', 'NAME_CONTRACT_STATUS'])\n    prev_refused_appr_agg = prev_refused_appr_group['SK_ID_CURR'].count().unstack()\n    \n    # rename column\n    prev_refused_appr_agg.columns = ['PREV_APPROVED_COUNT', 'PREV_REFUSED_COUNT']\n    \n    # NaN값은 모두 0으로 변경. \n    prev_refused_appr_agg = prev_refused_appr_agg.fillna(0)    \n    return prev_refused_appr_agg\n\ndef get_prev_agg(prev):\n    '''\n    Aggregation for previous credit\n    '''\n    prev = get_prev_processed(prev)\n    prev = get_prev_processed(prev)\n    prev_amt_agg = get_prev_amt_agg(prev)\n    \n    # Refused or Approved previous credit\n    prev_refused_appr_agg = get_prev_refused_appr_agg(prev)\n    \n    # prev_amt_agg와 조인. \n    prev_agg = prev_amt_agg.merge(prev_refused_appr_agg, on='SK_ID_CURR', how='left')\n    \n    # SK_ID_CURR별 과거 대출건수 대비 APPROVED_COUNT 및 REFUSED_COUNT 비율 생성. \n    prev_agg['PREV_REFUSED_RATIO'] = prev_agg['PREV_REFUSED_COUNT']/prev_agg['PREV_SK_ID_CURR_COUNT']\n    prev_agg['PREV_APPROVED_RATIO'] = prev_agg['PREV_APPROVED_COUNT']/prev_agg['PREV_SK_ID_CURR_COUNT']\n    \n    # 'PREV_REFUSED_COUNT', 'PREV_APPROVED_COUNT' 컬럼 drop \n    prev_agg = prev_agg.drop(['PREV_REFUSED_COUNT', 'PREV_APPROVED_COUNT'], axis = 1)\n    \n    return prev_agg\n\ndef get_bureau_processed(bureau):\n    '''\n    feature engineering for bureau\n    예정 채무 시작 및 완료일과 실제 채무 완료일간의 차이 및 날짜 비율 가공.  \n    '''\n    \n    # DAYS_CREDIT_ENDDATE : CB 크레딧 채무 완료까지 남아있는 일수(신청일 기준) Remaining duration of CB credit (in days) at the time of application in Home Credit\n    # DAYS_ENDDATE_FACT : CB 크레딧 채무 완료까지 걸린 실제 일수(신청일 기준, 상태가 Close일때만) Days since CB credit ended at the time of application in Home Credit (only for closed credit)\n    # DAYS_CREDIT : 현재 대출 신청 일 기준 과거 대출 신청 지난 기간(How many days before current application did client apply for Credit Bureau credit)\n    bureau['BUREAU_ENDDATE_FACT_DIFF'] = bureau['DAYS_CREDIT_ENDDATE'] - bureau['DAYS_ENDDATE_FACT']\n    bureau['BUREAU_CREDIT_FACT_DIFF'] = bureau['DAYS_CREDIT'] - bureau['DAYS_ENDDATE_FACT']\n    bureau['BUREAU_CREDIT_ENDDATE_DIFF'] = bureau['DAYS_CREDIT'] - bureau['DAYS_CREDIT_ENDDATE']\n  \n    # 채무 금액 대비/대출 금액 비율 및 차이 가공\n    bureau['BUREAU_CREDIT_DEBT_RATIO'] = bureau['AMT_CREDIT_SUM_DEBT'] / bureau['AMT_CREDIT_SUM']\n    #bureau['BUREAU_CREDIT_DEPT_DIFF'] =  bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_DEBT']\n    bureau['BUREAU_CREDIT_DEBT_DIFF'] = bureau['AMT_CREDIT_SUM_DEBT'] - bureau['AMT_CREDIT_SUM']\n    \n    # 연체 여부 및 120일 이상 연체 여부 가공\n    # CREDIT_DAY_OVERDUE : 대출 신청 시 CB 크레딧 연체 일수\n    # Number of days past due on CB credit at the time of application for related loan in our sample\n    bureau['BUREAU_IS_DPD'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 0 else 0)\n    bureau['BUREAU_IS_DPD_OVER120'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 120 else 0)\n    \n    return bureau\n\ndef get_bureau_day_amt_agg(bureau):\n    '''\n    Aggregationn for bureau\n    bureau 주요 컬럼 및 앞에서 채무 및 대출금액 관련 컬럼들로 SK_ID_CURR 레벨의 aggregation 컬럼 생성. \n    '''        \n    bureau_agg_dict = {\n    'SK_ID_BUREAU':['count'],\n    'DAYS_CREDIT':['min', 'max', 'mean'],\n    'CREDIT_DAY_OVERDUE':['min', 'max', 'mean'],\n    'DAYS_CREDIT_ENDDATE':['min', 'max', 'mean'],\n    'DAYS_ENDDATE_FACT':['min', 'max', 'mean'],\n    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n    'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n    'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n    'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean', 'sum'],\n    'AMT_ANNUITY': ['max', 'mean', 'sum'],\n    # 추가 가공 컬럼\n    'BUREAU_ENDDATE_FACT_DIFF':['min', 'max', 'mean'],\n    'BUREAU_CREDIT_FACT_DIFF':['min', 'max', 'mean'],\n    'BUREAU_CREDIT_ENDDATE_DIFF':['min', 'max', 'mean'],\n    'BUREAU_CREDIT_DEBT_RATIO':['min', 'max', 'mean'],\n    'BUREAU_CREDIT_DEBT_DIFF':['min', 'max', 'mean'],\n    'BUREAU_IS_DPD':['mean', 'sum'],\n    'BUREAU_IS_DPD_OVER120':['mean', 'sum']\n    }\n\n    bureau_grp = bureau.groupby('SK_ID_CURR')\n    bureau_day_amt_agg = bureau_grp.agg(bureau_agg_dict)\n    bureau_day_amt_agg.columns = ['BUREAU_'+('_').join(column).upper() for column in bureau_day_amt_agg.columns.ravel()]\n    # 조인을 위해 SK_ID_CURR을 reset_index()로 컬럼화 \n    bureau_day_amt_agg = bureau_day_amt_agg.reset_index()\n    #print('bureau_day_amt_agg shape:', bureau_day_amt_agg.shape)\n    return bureau_day_amt_agg\n\n\ndef get_bureau_active_agg(bureau):\n    '''\n    CREDIT_ACTIVE='Active' 인 데이터만 filtering\n    Bureau의 CREDIT_ACTIVE='Active' 인 데이터만 filtering 후 주요 컬럼 및 앞에서 채무 및 대출금액 관련 컬럼들로 SK_ID_CURR 레벨의 aggregation 컬럼 생성\n    '''\n    cond_active = bureau['CREDIT_ACTIVE'] == 'Active'\n    bureau_active_grp = bureau[cond_active].groupby(['SK_ID_CURR'])\n    bureau_agg_dict = {\n        'SK_ID_BUREAU':['count'],\n        'DAYS_CREDIT':['min', 'max', 'mean'],\n        'CREDIT_DAY_OVERDUE':['min', 'max', 'mean'],\n        'DAYS_CREDIT_ENDDATE':['min', 'max', 'mean'],\n        'DAYS_ENDDATE_FACT':['min', 'max', 'mean'],\n        'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n        'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean', 'sum'],\n        'AMT_ANNUITY': ['max', 'mean', 'sum'],\n        # 추가 가공 컬럼\n        'BUREAU_ENDDATE_FACT_DIFF':['min', 'max', 'mean'],\n        'BUREAU_CREDIT_FACT_DIFF':['min', 'max', 'mean'],\n        'BUREAU_CREDIT_ENDDATE_DIFF':['min', 'max', 'mean'],\n        'BUREAU_CREDIT_DEBT_RATIO':['min', 'max', 'mean'],\n        'BUREAU_CREDIT_DEBT_DIFF':['min', 'max', 'mean'],\n        'BUREAU_IS_DPD':['mean', 'sum'],\n        'BUREAU_IS_DPD_OVER120':['mean', 'sum']\n        }\n\n    bureau_active_agg = bureau_active_grp.agg(bureau_agg_dict)\n    bureau_active_agg.columns = ['BUREAU_ACT_'+('_').join(column).upper() for column in bureau_active_agg.columns.ravel()]\n    # 조인을 위해 SK_ID_CURR을 reset_index()로 컬럼화 \n    bureau_active_agg = bureau_active_agg.reset_index()\n    #print('bureau_active_agg shape:', bureau_active_agg.shape)\n    return bureau_active_agg\n\ndef get_bureau_bal_agg(bureau, bureau_bal):\n    '''\n    1.BUREAU_BAL join with BUREAU\n    2.Aggregation for added features\n    bureau_bal을 SK_ID_CURR 레벨로 건수와 MONTHS_BALANCE의 aggregation 가공 \n    '''\n    bureau_bal = bureau_bal.merge(bureau[['SK_ID_CURR', 'SK_ID_BUREAU']], on='SK_ID_BUREAU', how='left')\n    \n    # Status of Credit Bureau loan during the month (active, closed, DPD0-30,?[C means closed, X means status unknown, 0 means no DPD, \n    # 1 means maximal did during month between 1-30, 2 means DPD 31-60,?5 means DPD 120+ or sold or written off ] )\n    bureau_bal['BUREAU_BAL_IS_DPD'] = bureau_bal['STATUS'].apply(lambda x:1 if x in ['1', '2', '3', '4', '5'] else 0)\n    bureau_bal['BUREAU_BAL_IS_DPD_OVER120'] = bureau_bal['STATUS'].apply(lambda x: 1 if x =='5'  else 0)\n    \n    bureau_bal_grp = bureau_bal.groupby('SK_ID_CURR')\n    \n    # SK_ID_CURR 레벨로 건수와 MONTHS_BALANCE의 aggregation 가공 \n    bureau_bal_agg_dict = {\n        'SK_ID_CURR':['count'],\n        'MONTHS_BALANCE':['min', 'max', 'mean'],\n        'BUREAU_BAL_IS_DPD':['mean','sum'],\n        'BUREAU_BAL_IS_DPD_OVER120':['mean','sum']\n    }\n    bureau_bal_agg = bureau_bal_grp.agg(bureau_bal_agg_dict)\n    bureau_bal_agg.columns = ['BUREAU_BAL_'+ '_'.join(column).upper() for column in bureau_bal_agg.columns.ravel()]\n    \n    # 조인을 위해 SK_ID_CURR을 reset_index()로 컬럼화 \n    bureau_bal_agg = bureau_bal_agg.reset_index()\n    #print('bureau_bal_agg shape:', bureau_bal_agg.shape)\n    return bureau_bal_agg\n    \n\ndef get_bureau_agg(bureau, bureau_bal):\n    '''\n    가공된 bureau관련 aggregation 컬럼들을 모두 결합   \n    '''\n    bureau = get_bureau_processed(bureau)\n    bureau_day_amt_agg = get_bureau_day_amt_agg(bureau)\n    bureau_active_agg = get_bureau_active_agg(bureau)\n    bureau_bal_agg = get_bureau_bal_agg(bureau, bureau_bal)\n    \n    # bureau_day_amt_agg와 bureau_active_agg 조인.  \n    bureau_agg = bureau_day_amt_agg.merge(bureau_active_agg, on='SK_ID_CURR', how='left')\n    \n    # STATUS가 ACTIVE IS_DPD RATIO관련 비율 재가공. \n    #bureau_agg['BUREAU_IS_DPD_RATIO'] = bureau_agg['BUREAU_BUREAU_IS_DPD_SUM']/bureau_agg['BUREAU_SK_ID_BUREAU_COUNT']\n    #bureau_agg['BUREAU_IS_DPD_OVER120_RATIO'] = bureau_agg['BUREAU_BUREAU_IS_DPD_OVER120_SUM']/bureau_agg['BUREAU_SK_ID_BUREAU_COUNT']\n    bureau_agg['BUREAU_ACT_IS_DPD_RATIO'] = bureau_agg['BUREAU_ACT_BUREAU_IS_DPD_SUM']/bureau_agg['BUREAU_SK_ID_BUREAU_COUNT']\n    bureau_agg['BUREAU_ACT_IS_DPD_OVER120_RATIO'] = bureau_agg['BUREAU_ACT_BUREAU_IS_DPD_OVER120_SUM']/bureau_agg['BUREAU_SK_ID_BUREAU_COUNT']\n    \n    # bureau_agg와 bureau_bal_agg 조인. \n    bureau_agg = bureau_agg.merge(bureau_bal_agg, on='SK_ID_CURR', how='left')\n    \n    #print('bureau_agg shape:', bureau_agg.shape)\n    \n    return bureau_agg\n\ndef get_pos_bal_agg(pos_bal):  \n    '''\n    Aggregation for datasets from consumer load and balance information\n    '''\n    # 연체여부,  연체일수 0~ 120 사이 여부, 연체 일수 120보다 큰 여부 \n    pos_bal['POS_IS_DPD'] = pos_bal['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n    pos_bal['POS_IS_DPD_UNDER_120'] = pos_bal['SK_DPD'].apply(lambda x:1 if (x > 0) & (x <120) else 0 )\n    pos_bal['POS_IS_DPD_OVER_120'] = pos_bal['SK_DPD'].apply(lambda x:1 if x >= 120 else 0)\n\n    # 기존 컬럼과 신규 컬럼으로 SK_ID_CURR 레벨로 신규 aggregation 컬럼 생성\n    pos_bal_grp = pos_bal.groupby('SK_ID_CURR')\n    pos_bal_agg_dict = {\n        'SK_ID_CURR':['count'], \n        'MONTHS_BALANCE':['min', 'mean', 'max'], \n        'SK_DPD':['min', 'max', 'mean', 'sum'],\n        'CNT_INSTALMENT':['min', 'max', 'mean', 'sum'],\n        'CNT_INSTALMENT_FUTURE':['min', 'max', 'mean', 'sum'],\n        # 추가 컬럼. \n        'POS_IS_DPD':['mean', 'sum'],\n        'POS_IS_DPD_UNDER_120':['mean', 'sum'],\n        'POS_IS_DPD_OVER_120':['mean', 'sum']\n    }    \n    pos_bal_agg = pos_bal_grp.agg(pos_bal_agg_dict)\n    \n    pos_bal_agg.columns = ['POS_'+'_'.join(column).upper() for column in pos_bal_agg.columns.ravel()]\n    \n    # MONTHS_BALANCE가 최근(20개월 이하)인 데이터 세트 별도 가공. \n    # MONTHS_BALANCE < 20 : having tendency to fall into DPD\n    cond_months = pos_bal['MONTHS_BALANCE'] > -20\n    pos_bal_m20_grp = pos_bal[cond_months].groupby('SK_ID_CURR')\n    \n    pos_bal_m20_agg_dict = {\n        'SK_ID_CURR':['count'], \n        'MONTHS_BALANCE':['min', 'mean', 'max'], \n        'SK_DPD':['min', 'max', 'mean', 'sum'],\n        'CNT_INSTALMENT':['min', 'max', 'mean', 'sum'],\n        'CNT_INSTALMENT_FUTURE':['min', 'max', 'mean', 'sum'],\n        # 추가 컬럼. \n        'POS_IS_DPD':['mean', 'sum'],\n        'POS_IS_DPD_UNDER_120':['mean', 'sum'],\n        'POS_IS_DPD_OVER_120':['mean', 'sum']\n    }\n\n    pos_bal_m20_agg = pos_bal_m20_grp.agg(pos_bal_m20_agg_dict)\n    # 컬럼 변경 \n    pos_bal_m20_agg.columns = [('POS_M20')+('_').join(column).upper() for column in pos_bal_m20_agg.columns.ravel()]\n    pos_bal_agg = pos_bal_agg.merge(pos_bal_m20_agg, on='SK_ID_CURR', how='left')\n    \n    # SK_ID_CURR을 reset_index()를 이용하여 컬럼으로 변환\n    pos_bal_agg = pos_bal_agg.reset_index()\n    \n    \n    return pos_bal_agg\n\ndef get_install_agg(install):\n    \"\"\"\n    예정 납부 금액 대비 실제 납부 금액 관련 데이터 가공. 예정 납부 일자 대비 실제 납부 일자 비교를 DPD 일자 생성  \n    \"\"\"\n    # AMT_INSTALMENT : 예정납부금액\n    # AMT_PAYMENT - 납부금액\n    \n    # DAYS_ENTRY_PAYMENT : When was the installments of previous credit paid actually (relative to application date of current loan - 현 대출신청일 대비 과거 대출건 대출납입 실제일자)\n    # DAYS_INSTALMENT - When the installment of previous credit was supposed to be paid (relative to application date of current loan-현 대출신청일 대비 이전 대출납입 예정일자)\n    \n    install['AMT_DIFF'] = install['AMT_INSTALMENT'] - install['AMT_PAYMENT']\n    install['AMT_RATIO'] =  (install['AMT_PAYMENT'] +1)/ (install['AMT_INSTALMENT'] + 1)\n    install['SK_DPD'] = install['DAYS_ENTRY_PAYMENT'] - install['DAYS_INSTALMENT']\n\n    # 연체여부,  연체일수 30~ 120 사이 여부, 연체 일수 100보다 큰 여부 데이터 가공. \n    install['INS_IS_DPD'] = install['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n    install['INS_IS_DPD_UNDER_120'] = install['SK_DPD'].apply(lambda x: 1 if (x>0) & (x < 120) else 0)\n    install['INS_IS_DPD_OVER_120'] = install['SK_DPD'].apply(lambda x: 1 if (x >= 120) else 0)\n\n    # 기존 컬럼과 신규 컬럼으로 SK_ID_CURR 레벨로 신규 aggregation 컬럼 생성. \n    install_grp = install.groupby('SK_ID_CURR')\n\n    install_agg_dict = {\n        'SK_ID_CURR':['count'],\n        'NUM_INSTALMENT_VERSION':['nunique'], \n        'DAYS_ENTRY_PAYMENT':['mean', 'max', 'sum'],\n        'DAYS_INSTALMENT':['mean', 'max', 'sum'],\n        'AMT_INSTALMENT':['mean', 'max', 'sum'],\n        'AMT_PAYMENT':['mean', 'max','sum'],\n        #  추가 컬럼\n        'AMT_DIFF':['mean','min', 'max','sum'],\n        'AMT_RATIO':['mean', 'max'],\n        'SK_DPD':['mean', 'min', 'max'],\n        'INS_IS_DPD':['mean', 'sum'],\n        'INS_IS_DPD_UNDER_120':['mean', 'sum'],\n        'INS_IS_DPD_OVER_120':['mean', 'sum']    \n    }\n\n    install_agg = install_grp.agg(install_agg_dict)\n    install_agg.columns = ['INS_'+('_').join(column).upper() for column in install_agg.columns.ravel()]\n\n    \n    # 실제 납부 일자(DAYS_ENTRY_PAYMENT)가 비교적 최근(1년 이내) 데이터만 별도로 가공\n    cond_day = install['DAYS_ENTRY_PAYMENT'] >= -365\n    install_d365_grp = install[cond_day].groupby('SK_ID_CURR')\n    install_d365_agg_dict = {\n        'SK_ID_CURR':['count'],\n        'NUM_INSTALMENT_VERSION':['nunique'], \n        'DAYS_ENTRY_PAYMENT':['mean', 'max', 'sum'],\n        'DAYS_INSTALMENT':['mean', 'max', 'sum'],\n        'AMT_INSTALMENT':['mean', 'max', 'sum'],\n        'AMT_PAYMENT':['mean', 'max','sum'],\n        #  추가 컬럼\n        'AMT_DIFF':['mean','min', 'max','sum'],\n        'AMT_RATIO':['mean', 'max'],\n        'SK_DPD':['mean', 'min', 'max'],\n        'INS_IS_DPD':['mean', 'sum'],\n        'INS_IS_DPD_UNDER_120':['mean', 'sum'],\n        'INS_IS_DPD_OVER_120':['mean', 'sum']    \n    }\n    \n    install_d365_agg = install_d365_grp.agg(install_d365_agg_dict)\n    install_d365_agg.columns = ['INS_D365'+('_').join(column).upper() for column in install_d365_agg.columns.ravel()]\n    \n    install_agg = install_agg.merge(install_d365_agg, on='SK_ID_CURR', how='left')\n    install_agg = install_agg.reset_index()\n    \n    return install_agg\n\ndef get_card_bal_agg(card_bal):\n    '''\n    # 월별 카드 허용한도에 따른 잔고와 인출 금액 비율 \n    '''\n    card_bal['BALANCE_LIMIT_RATIO'] = card_bal['AMT_BALANCE']/card_bal['AMT_CREDIT_LIMIT_ACTUAL']\n    card_bal['DRAWING_LIMIT_RATIO'] = card_bal['AMT_DRAWINGS_CURRENT'] / card_bal['AMT_CREDIT_LIMIT_ACTUAL']\n\n    # DPD에 따른 가공 컬럼 생성.\n    card_bal['CARD_IS_DPD'] = card_bal['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n    card_bal['CARD_IS_DPD_UNDER_120'] = card_bal['SK_DPD'].apply(lambda x:1 if (x > 0) & (x <120) else 0 )\n    card_bal['CARD_IS_DPD_OVER_120'] = card_bal['SK_DPD'].apply(lambda x:1 if x >= 120 else 0)\n\n    # 기존 컬럼과 가공 컬럼으로 SK_ID_CURR 레벨로 aggregation 신규 컬럼 생성. \n    card_bal_grp = card_bal.groupby('SK_ID_CURR')\n    card_bal_agg_dict = {\n        'SK_ID_CURR':['count'],\n         #'MONTHS_BALANCE':['min', 'max', 'mean'],\n        'AMT_BALANCE':['max'],\n        'AMT_CREDIT_LIMIT_ACTUAL':['max'],\n        'AMT_DRAWINGS_ATM_CURRENT': ['max', 'sum'],\n        'AMT_DRAWINGS_CURRENT': ['max', 'sum'],\n        'AMT_DRAWINGS_POS_CURRENT': ['max', 'sum'],\n        'AMT_INST_MIN_REGULARITY': ['max', 'mean'],\n        'AMT_PAYMENT_TOTAL_CURRENT': ['max','sum'],\n        'AMT_TOTAL_RECEIVABLE': ['max', 'mean'],\n        'CNT_DRAWINGS_ATM_CURRENT': ['max','sum'],\n        'CNT_DRAWINGS_CURRENT': ['max', 'mean', 'sum'],\n        'CNT_DRAWINGS_POS_CURRENT': ['mean'],\n        'SK_DPD': ['mean', 'max', 'sum'],\n        #  추가 컬럼\n        'BALANCE_LIMIT_RATIO':['min','max'],\n        'DRAWING_LIMIT_RATIO':['min', 'max'],\n        'CARD_IS_DPD':['mean', 'sum'],\n        'CARD_IS_DPD_UNDER_120':['mean', 'sum'],\n        'CARD_IS_DPD_OVER_120':['mean', 'sum']    \n    }\n    card_bal_agg = card_bal_grp.agg(card_bal_agg_dict)\n    card_bal_agg.columns = ['CARD_'+('_').join(column).upper() for column in card_bal_agg.columns.ravel()]\n\n    card_bal_agg = card_bal_agg.reset_index()\n    \n    # MONTHS_BALANCE가 비교적 최근 데이터( 3개월 이하)만 별도로 가공.  \n    cond_month = card_bal.MONTHS_BALANCE >= -3\n    card_bal_m3_grp = card_bal[cond_month].groupby('SK_ID_CURR')\n    card_bal_m3_agg = card_bal_m3_grp.agg(card_bal_agg_dict)\n    card_bal_m3_agg.columns = ['CARD_M3'+('_').join(column).upper() for column in card_bal_m3_agg.columns.ravel()]\n    \n    card_bal_agg = card_bal_agg.merge(card_bal_m3_agg, on='SK_ID_CURR', how='left')\n    card_bal_agg = card_bal_agg.reset_index()\n    \n    return card_bal_agg\n\ndef get_apps_all_encoded(apps_all):\n    \"\"\"\n    factorize for object columns and return apps_all\n    \"\"\"\n    object_columns = apps_all.dtypes[apps_all.dtypes == 'object'].index.to_list()\n    for column in object_columns:\n        apps_all[column] = pd.factorize(apps_all[column])[0]\n    return apps_all\ndef get_apps_all_train_test(apps_all):\n    \"\"\"\n    split train, test and return train datasets, test datasets\n    \"\"\"\n    apps_all_train = apps_all[~apps_all['TARGET'].isnull()]\n    apps_all_test = apps_all[apps_all['TARGET'].isnull()]\n    apps_all_test = apps_all_test.drop('TARGET', axis = 1)\n    return apps_all_train, apps_all_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_apps_all_with_all_agg(apps, prev, bureau, bureau_bal, pos_bal, install, card_bal):\n    '''\n    apps와 prev_agg, bureau_agg, pos_bal_agg, install_agg, card_bal_agg를 개별 함수 호출하여 생성후 조인 결합\n    '''\n    apps_all =  get_apps_processed(apps)\n    prev_agg = get_prev_agg(prev)\n    bureau_agg = get_bureau_agg(bureau, bureau_bal)\n    pos_bal_agg = get_pos_bal_agg(pos_bal)\n    install_agg = get_install_agg(install)\n    card_bal_agg = get_card_bal_agg(card_bal)\n    print('prev_agg shape:', prev_agg.shape, 'bureau_agg shape:', bureau_agg.shape )\n    print('pos_bal_agg shape:', pos_bal_agg.shape, 'install_agg shape:', install_agg.shape, 'card_bal_agg shape:', card_bal_agg.shape)\n    print('apps_all before merge shape:', apps_all.shape)\n    \n    # 생성된 prev_agg, bureau_agg, pos_bal_agg, install_agg, card_bal_agg를 apps와 조인하여 최종 학습/테스트 집합 생성. \n    apps_all = apps_all.merge(prev_agg, on='SK_ID_CURR', how='left')\n    apps_all = apps_all.merge(bureau_agg, on='SK_ID_CURR', how='left')\n    apps_all = apps_all.merge(pos_bal_agg, on='SK_ID_CURR', how='left')\n    apps_all = apps_all.merge(install_agg, on='SK_ID_CURR', how='left')\n    apps_all = apps_all.merge(card_bal_agg, on='SK_ID_CURR', how='left')\n    \n    print('apps_all after merge with all shape:', apps_all.shape)\n    \n    return apps_all","metadata":{"id":"uifgJbUWCeWR","executionInfo":{"status":"ok","timestamp":1599725320638,"user_tz":-540,"elapsed":2097,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creation of the final train and test datasets","metadata":{"id":"lDD1hDA0CeWe"}},{"cell_type":"code","source":"apps, prev, bureau, bureau_bal, pos_bal, install, card_bal = get_dataset()\n\n# application, previous, bureau, bureau_bal 관련 데이터셋 가공 및 취합. \napps_all = get_apps_all_with_all_agg(apps, prev, bureau, bureau_bal, pos_bal, install, card_bal)\n\n# Category 컬럼을 모두 Label 인코딩 수행. \napps_all = get_apps_all_encoded(apps_all)\n\n# 학습과 테스트 데이터로 분리. \napps_all_train, apps_all_test = get_apps_all_train_test(apps_all)","metadata":{"id":"rO2ml4oCCeWf","executionInfo":{"status":"ok","timestamp":1599725481976,"user_tz":-540,"elapsed":161849,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"outputId":"da51b95d-2849-42d8-f5ef-60a30070b9c3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"apps_all_train.shape, apps_all_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ouf Of Fold Prediction","metadata":{"id":"kQm5zVNaCeWt"}},{"cell_type":"code","source":"np.zeros(10)  # note that 1 dim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_apps_all_with_oof(apps_all_train, apps_all_test, nfolds=5):\n    \"\"\"\n    Predict credit risk using OOF(Out of Fold) method \n    Parameter : apps_all_train, apps_all_test, nfolds count\n    \"\"\"\n    ftr_app = apps_all_train.drop(['SK_ID_CURR', 'TARGET'], axis=1)  # feature dateset\n    target_app = apps_all_train['TARGET']                           # target datasets\n\n    # nfolds 개의 cross validatin fold set을 가지는 KFold 생성 \n    folds = KFold(n_splits = nfolds, shuffle=True, random_state = 2020)\n    \n    #  Out of Folds로 학습된 모델의 validation set을 예측하여 결과 확률을 담을 array 생성.\n    # validation set가 n_split갯수만큼 있으므로 크기는 ftr_app의 크기가 되어야 함. \n    oof_preds = np.zeros(ftr_app.shape[0])\n    \n    # Ouf of Folds로 학습된 모델의 test dataset을 예측하여 결과 확률을 담을 array 생성. \n    test_preds = np.zeros(apps_all_test.shape[0])\n    \n    # n_estimators를 4000까지 확대. \n    # 현재 데이터셋으로 하이터 파라미터를 튜닝하는것이 바람직.\n    clf = LGBMClassifier(\n                nthread=4,\n                n_estimators=4000,\n                learning_rate=0.01,\n                max_depth = 11,\n                num_leaves=58,\n                colsample_bytree=0.613,\n                subsample=0.708,\n                max_bin=407,\n                reg_alpha=3.564,\n                reg_lambda=4.930,\n                min_child_weight= 6,\n                min_child_samples=165,\n                silent=-1,\n                verbose=-1,\n                )\n\n    # nfolds 번 cross validation Iteration 반복하면서 OOF 방식으로 학습 및 테스트 데이터 예측\n    for fold_idx, (train_idx , valid_idx) in enumerate(folds.split(ftr_app)):\n        print(\"##### iteration \", fold_idx, 'starts')\n        # 학습용 데이터 세트의 인덱스와 검증용 데이터 세트의 인덱스 추출하여 이를 기반으로 학습/검증 데이터 추출\n        train_x = ftr_app.iloc[train_idx, :]\n        train_y = target_app.iloc[train_idx]\n        valid_x = ftr_app.iloc[valid_idx, :]\n        valid_y = target_app.iloc[valid_idx]\n\n        # 추출된 학습/검증 데이터 세트로 모델 학습. early_stopping은 200으로 증가. \n        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'auc', verbose= 200, \n                early_stopping_rounds= 200)\n        \n        # 검증 데이터 세트로 예측된 확률 저장 사용되지는 않음. \n        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration = clf.best_iteration_)[:, 1]\n        \n        # 학습된 모델로 테스트 데이터 세트에 예측 확률 계산. \n        # nfolds 번 반복 실행하므로 평균 확률을 구하기 위해 개별 수행시 마다 수행 횟수로 나눈 확률을 추후에 더해서 최종 평균 확률 계산. \n        # num_iteration  - stops at the time of early stopping\n        # [0 - normal , 1 - overdue] : fetch the probability of overdue\n        # Predecited probabily should be mean value \n        test_preds += clf.predict_proba(apps_all_test.drop('SK_ID_CURR', axis = 1),num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n        \n    return clf, test_preds","metadata":{"id":"RDAo4NwLCeWv","executionInfo":{"status":"ok","timestamp":1599725481981,"user_tz":-540,"elapsed":157565,"user":{"displayName":"권철민","photoUrl":"","userId":"03917677622451543916"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\n \nprint(\"Starting Time \" , datetime.datetime.now())\n\nclf, test_preds = train_apps_all_with_oof(apps_all_train, apps_all_test, nfolds=5)\n\nprint(\"End Time : \", datetime.datetime.now())\n\n'''\nPrivate Score: 0.79440, Public Score: 0.80178\n'''\n","metadata":{"id":"bAMSJm3gCeW6","outputId":"5b1fa266-e728-415a-b3bb-772c8f647f89","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CVS for predicted result","metadata":{"id":"L1_qwC2QD3jt"}},{"cell_type":"code","source":"display(test_preds.shape, test_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"apps_all_test['TARGET'] = test_preds\nif platform =='google':\n    default_dir = \"/content/gdrive/My Drive\"\nelse:\n    default_dir = \"../working/\"\napps_all_test[['SK_ID_CURR', 'TARGET']].to_csv(os.path.join(default_dir,'oof_all_01.csv'), index=False)","metadata":{"id":"hqPQR_rhCeXA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"apps_all_test = apps_all_test.drop(['TARGET'], axis=1)","metadata":{"id":"ywtBePJUCeXM","trusted":true},"execution_count":null,"outputs":[]}]}