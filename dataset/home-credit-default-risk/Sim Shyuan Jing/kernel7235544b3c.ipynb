{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Executive Summary"},{"metadata":{},"cell_type":"markdown","source":"This project aims to classify whether the applicant will default on the loan given applicantion information and past credit histories data. This project will approach it as a binary classification problem. \n\nData Cleaning:\n\nEach applicant past credit data are given if available. However, each applicant can have more than 2 installments or credit records thus need to summarise the past credit records per applicant. \nRecords like number of days past due for installment, percentage of payment made, max number of credit card days due etc are summarised for each applicant.\nRecords like who accompanied the applicant to apply, if given area different from residential area, if email is given are some of the columns that are deem not important. These are removed.\nCategorical data are converted to labels.\n\nModeling\n\nThe model used for classification problem. The train and test data at 75% & 25% split. All the features are used as input. \n\n1. Logistics Regression \n2. Decision Tree with Pruning \n3. Random Forest \n4. Neural Network \n"},{"metadata":{},"cell_type":"markdown","source":"# Problem Statement / Research Topic"},{"metadata":{},"cell_type":"markdown","source":"Data Source: https://www.kaggle.com/c/home-credit-default-risk/data\n\nThis was a data competition hosted at Kaggle.com. \n\nThere are clients who are unable to get a loan due to their limited credit histories. In order not to penalize this group of clients, Home Credit wanted to explore other transactional data if it can help to determine whether the applicant is able to repay a loan.\n\nThis research can bring a win win situation for both Home Credit and the clients. Clients are able to loan and get their home and Home Credit can increase their loan take up rate at reduced risk.\n\nThis project is to predict whether the applicant will default on the loan. There will be 2 outcomes (Yes/No)."},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning\n"},{"metadata":{},"cell_type":"markdown","source":"- previous_application.csv\n- installments_payments.csv\n- credit_card_balance.csv\n\nConslidate the payment histories for installments and credit card of the applicants \n\n#### Output\n- Merge both data files\n- \"SK_ID_CURR\" is the key\n- csv file to be merged with other data files"},{"metadata":{"trusted":true},"cell_type":"code","source":"import csv\nimport math\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom graphviz import Source\n\nfrom scipy.stats import entropy\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DIMS=(16, 15)\n\n# matrix bar chart \ndef drawbar(n, df):\n    fig = plt.figure(figsize=DIMS)\n    drow = math.ceil(n/2)\n    for i in range(n):\n        s = df.iloc[:, i].groupby(df.iloc[:, i]).size()\n        fig.tight_layout()\n        ax = fig.add_subplot(drow, 2,i+1)\n        s.plot.bar()\n        ax.set_title(df.columns[i], fontsize = 12)\n    plt.show()\n        \n# data proportion\ndef showpro(n, df):\n    for i in range(n):\n        s = df.iloc[:, i].groupby(df.iloc[:, i]).size()/len(df)*100\n        print (s)\n        print (\"------\")\n\n        \n# matrix box plot\ndef drawbox(n,df):\n    fig = plt.figure(figsize=DIMS)\n    drow = math.ceil(n/2)\n    for i in range(n):\n        fig.tight_layout()\n        ax = fig.add_subplot(drow, 2,i+1)\n        df.iloc[:, i].plot(kind='box', ax=ax)\n        ax.set_title(df.columns[i], fontsize = 12)\n    plt.show()\n    \n# matrix histogram    \ndef drawdistplot(n, df, bins):\n    fig = plt.figure(figsize=DIMS)\n    drow = math.ceil(n/2)\n    for i in range(n):\n        fig.tight_layout()\n        ax = fig.add_subplot(drow, 2,i+1)\n        sns.distplot(df.iloc[:, i],kde=False, bins = bins)\n        ax.set_title(df.columns[i])\n    plt.show()\n\n    \n# data proportion with respect to Y (TARGET)    \ndef crosst(n, df):\n    for i in range(n-1):\n        print (pd.crosstab(df.iloc[:, i+1], df.TARGET, normalize='index'))\n        print (\"--------\")\n\n# percentage of non-null value in each column        \ndef summary(df, p):\n    n = len(df.columns)\n    totalrow = df.shape[0]\n    print (totalrow)\n    for i in range(n):\n        col = df.columns[i]\n        cnt = df.iloc[:,i].count()\n        percentage = (cnt/totalrow*100)\n        if (percentage <= p):\n            print ('column: ', col, ' - ', cnt, '@' , percentage)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read csv file\n\ndf_prevapp = pd.read_csv('../input/previous_application.csv')\ndf_install = pd.read_csv('../input/installments_payments.csv')\ndf_credit = pd.read_csv('../input/credit_card_balance.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clean Installments Payments"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the columns name in installments payments file\nprint (df_install.columns)\nprint (\" ------- \")\nprint (df_install.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# box plot visualization for'DAYS_INSTALMENT', 'DAYS_ENTRY_PAYMENT', 'AMT_INSTALMENT', 'AMT_PAYMENT'\n\npic = df_install[['DAYS_INSTALMENT', 'DAYS_ENTRY_PAYMENT',\n       'AMT_INSTALMENT', 'AMT_PAYMENT']]        \nn = len(pic.columns)\ndrawbox(n,pic)\n\n# so many outliers for intallment payments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histogram plot visualization for'DAYS_INSTALMENT', 'DAYS_ENTRY_PAYMENT', 'AMT_INSTALMENT', 'AMT_PAYMENT'\n\npic = df_install[['DAYS_INSTALMENT', 'DAYS_ENTRY_PAYMENT',\n       'AMT_INSTALMENT', 'AMT_PAYMENT']]        \nn = len(pic.columns)\ndrawdistplot(n,pic,100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Calculate Days Past Due \n\n- 'DAYS_INSTALMENT' - 'DAYS_ENTRY_PAYMENT'\n- If > 0 means clients pay on time  \n- If < 0 means clients did not pay on time\n    \n### Calculate Proportion of the Installment Paid\n\n- 'AMT_PAYMENT' / 'AMT_INSTALMENT'\n- Proportion of the installment paid\n- If 1 means 100% paid\n- If 0.9 means 90% paid only"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Calculate Days Past Due\n\ndef cal_days_past(dfdueday,dfpayday):\n    late = dfdueday-dfpayday\n    if late >= 0:\n        return np.NaN\n    else:\n        return abs(late)\n    \n# Calculate Proportion of the Installment Paid\n\ndef cal_percent_paid(dfpay,dfinstall):\n    mis=dfpay-dfinstall\n    if mis >=0:\n        return 1.0\n    else:\n        return abs(dfpay)/dfinstall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# copy the dataframe into df_i\ndf_i = df_install.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check how many 0 installment amount\n# there are 290 rows with 0 installment needed\n# remove these 290 rows\n\nlen(df_i[df_i['AMT_INSTALMENT']==0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove 'AMT_INSTALMENT' = 0\n\ndf_i = df_i[df_i.AMT_INSTALMENT != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# New column : Calculate Days Past Due\n\ndf_i['Days_Past']=  df_i.apply(lambda x: cal_days_past(x[\"DAYS_INSTALMENT\"],\n                                                                           x[\"DAYS_ENTRY_PAYMENT\"]),axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# New column : Calculate Proportion of the Installment Paid\n\ndf_i['Payment_Made']=  df_i.apply(lambda x: cal_percent_paid(x[\"AMT_PAYMENT\"],\n                                                                           x[\"AMT_INSTALMENT\"]),axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# groupby SK_ID_PREV\n# Get the mean of installment amout, mean number of days past due and mean of the proportion \n# 0 values are already in nan form so that it is not part of the mean calculation\n\ndf_i1 = df_i.groupby([\"SK_ID_PREV\", \"SK_ID_CURR\"]).agg({'AMT_INSTALMENT': 'mean' ,\n                                                                    'Days_Past':'mean', \n                                                                    'Payment_Made':'mean'}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace all Nan to 0\n\ndf_i1 = df_i1.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Cleaned Installment Data groupby SK_ID_PREV\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_i1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clean Previous Application"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the columns name in previous application file\nprint (df_prevapp.columns)\nprint (\" ------- \")\nprint (df_prevapp.describe())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first draft of keeping columns that looks useful\n# rate and usage of the loans are removed\n# reduce the columns for analysis\n\ntokeep =['SK_ID_PREV', 'SK_ID_CURR', 'AMT_CREDIT', 'FLAG_LAST_APPL_PER_CONTRACT', \n         'NFLAG_LAST_APPL_IN_DAY', 'NAME_CONTRACT_STATUS', 'CNT_PAYMENT', \n         'PRODUCT_COMBINATION','NFLAG_INSURED_ON_APPROVAL']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reduced columns \ndf_pa = df_prevapp[tokeep]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merge installments and previous application"},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge installment and previous application data by SK_ID_PREV\n\ndf_pi = pd.merge(df_pa,df_i1, on=\"SK_ID_PREV\", how= \"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop duplicate column 'SK_ID_CURR_y'\n\ndf_pi.drop(['SK_ID_CURR_y'], axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check \n\nprint (df_pi[\"NAME_CONTRACT_STATUS\"].unique())\nprint (df_pi[\"FLAG_LAST_APPL_PER_CONTRACT\"].unique())\n\npic = df_pi[[\"NAME_CONTRACT_STATUS\",\"FLAG_LAST_APPL_PER_CONTRACT\" ]]        \nn = len(pic.columns)\nshowpro(n,pic)\n\n# Only keep approved and refused application\n# Only keep FLAG_LAST_APPL_PER_CONTRACT == Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keep only approved and refused application\n# keep FLAG_LAST_APPL_PER_CONTRACT == Y\n\n# approved and refused application\ncond2 = df_pi[\"NAME_CONTRACT_STATUS\"] == \"Approved\"\ncond3 = df_pi[\"NAME_CONTRACT_STATUS\"] == \"Refused\"\n\ndf_pi = df_pi[cond2|cond3]\n\n# keep Y\ncond4 = df_pi[\"FLAG_LAST_APPL_PER_CONTRACT\"] == 'Y'\ndf_pi = df_pi[cond4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Groupby SK_ID_CURR_x\n# MAX Credit Requested, SUM insurance bought\n# MEAN of installment amount\n# MEAN of days due past and proportion of payment made\n\ndf_pi_01 = df_pi.groupby(\"SK_ID_CURR_x\").agg({'AMT_CREDIT':'max',\n                                          'NFLAG_INSURED_ON_APPROVAL': 'sum',\n                                          'AMT_INSTALMENT': 'mean',\n                                          'Days_Past': 'mean',\n                                          'Payment_Made': 'mean'}).reset_index()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pi_01.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count number of installments per ID_CURR\n# count number of approved and refused applications for each ID_CURR\n\ndf_pi_02 = pd.concat([df_pi[['SK_ID_CURR_x', 'SK_ID_PREV']].groupby('SK_ID_CURR_x').count().rename(columns={'SK_ID_PREV' :'count'}),\n                   pd.crosstab(df_pi.SK_ID_CURR_x, df_pi.NAME_CONTRACT_STATUS)]\n                  ,1).reset_index()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge the 2 dataframes\n\ndf_prei = pd.merge(df_pi_01,df_pi_02, on=\"SK_ID_CURR_x\", how= \"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rename the SK_ID_CURR_x to SK_ID_CURR_\ndf_prei = df_prei.rename(columns={'SK_ID_CURR_x': 'SK_ID_CURR_'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clean Credit Card Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Remove columns that is not useful to reduce the complexity\n* Keep 'SK_ID_PREV', 'SK_ID_CURR',\n* Keep 'MONTHS_BALANCE', 'AMT_BALANCE', 'AMT_CREDIT_LIMIT_ACTUAL'\n* Keep 'AMT_DRAWINGS_CURRENT'\n* Keep 'AMT_INST_MIN_REGULARITY' 'AMT_PAYMENT_TOTAL_CURRENT'\n* Keep 'AMT_RECEIVABLE_PRINCIPAL', 'AMT_TOTAL_RECEIVABLE'\n* Keep Total CNT_Drawings_Current\n* Remove 'CNT_INSTALMENT_MATURE_CUM'\n* Keep 'NAME_CONTRACT_STATUS'\n* Remove 'SK_DPD', Keep 'SK_DPD_DEF'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keep relevant columns \nkeepcol =['SK_ID_PREV', 'SK_ID_CURR', 'MONTHS_BALANCE', 'AMT_BALANCE',\n       'AMT_CREDIT_LIMIT_ACTUAL', \n       'AMT_DRAWINGS_CURRENT', \n       'AMT_INST_MIN_REGULARITY',\n       'AMT_PAYMENT_TOTAL_CURRENT',\n       'AMT_RECEIVABLE_PRINCIPAL', 'AMT_TOTAL_RECEIVABLE',\n       'CNT_DRAWINGS_CURRENT',\n       'NAME_CONTRACT_STATUS','SK_DPD_DEF']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reduced columns dataframe \n\ndf_c = df_credit[keepcol]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove rows with 0 'AMT_BALANCE' : no amt owed\n\ndf_c = df_c[df_c.AMT_BALANCE != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace 0 with nan\n# to exclude 0 from calculation\n\ncols = [\"AMT_DRAWINGS_CURRENT\",'SK_DPD_DEF']\ndf_c[cols] = df_c.loc[:,cols].replace({0:np.nan})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# group by SK_ID_PREV\n# extract frequency of credit card monthly use, and avg credit used \n# Max number of Day Due Past alert\n\ndf_c1 = (\n    df_c.groupby([\"SK_ID_PREV\", \"SK_ID_CURR\"])\n    .agg({'AMT_DRAWINGS_CURRENT': ['count' , 'mean'] , 'SK_DPD_DEF':'max'})\n    .reset_index()\n)\n\ndf_c1.columns = [\"_\".join(x) for x in df_c1.columns.ravel()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace all nan to 0\n\ndf_c1.fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (len(df_c1['SK_ID_PREV_'].unique()))\nprint (len(df_c1['SK_ID_CURR_'].unique()))\n\n# There are ID_CURR with more than 1 ID_PREV# group by SK_ID_CURR \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# group by SK_ID_CURR \n# avg amt credit used, total freq monthly usage, \n# to merge with installment and preapp data, max number of days past due alert\n\ndf_c2 = (\n    df_c1.groupby('SK_ID_CURR_')\n    .agg({'AMT_DRAWINGS_CURRENT_count' :'sum', 'AMT_DRAWINGS_CURRENT_mean' :'mean', 'SK_DPD_DEF_max': 'max' })\n    .reset_index()\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merge Installment, Previous Application and Credit Card Data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge credit , installment and pre app data into 1 dataframe\n\ndf_precredinstall = pd.merge(df_prei, df_c2, on=\"SK_ID_CURR_\", how= \"left\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_precredinstall.to_csv(\"precredinstall.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clean Application File "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/application_train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# pandas.core.series.Series\ndatatype = df.dtypes\n\nprint (datatype.unique())\n\ncond1 = datatype == 'float64'\nfloatcol = datatype[cond1].index.tolist()\n\ncond2 = datatype == 'int64'\nintcol = datatype[cond2].index.tolist()\n\ncond3 = datatype == 'O'\nobjcol = datatype[cond3].index.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove strong correlated colummns (Float and Int Data)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Float Data Columns\n\ndf_float = df[floatcol]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlated features\n#Using Pearson Correlation\n\nplt.figure(figsize=(16,16))\ncorf = df_float.corr()\nsns.heatmap(corf, annot=True, cmap=plt.cm.Reds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nupperf = corf.where(np.triu(np.ones(corf.shape), k=1)\n                          .astype(np.bool))\n\n#print (upper)\nto_dropf = [column for column in upperf\n           .columns if any(upperf[column] > 0.9)]\n\nprint (to_dropf)\nprint (len(to_dropf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Int Data Columns\ndf_int =df[intcol]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlated features\n#Using Pearson Correlation\n\n\nplt.figure(figsize=(16,16))\ncori = df_int.corr()\nsns.heatmap(cori, annot=True, cmap=plt.cm.Reds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"upperi = cori.where(np.triu(np.ones(cori.shape), k=1)\n                          .astype(np.bool))\n\n#print (upper)\nto_dropi = [column for column in upperi\n           .columns if any(upperi[column] > 0.90)]\n\nprint (to_dropi)\nprint (len(to_dropi))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# where 1 is the axis number (0 for rows and 1 for columns.)\nto_dropf.extend(to_dropi)\n\nprint (to_dropf)\ndf_xcorr = df.drop(to_dropf, 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_xcorr.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove features with many null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(df_xcorr,35)\n\n# remove columns of data with more than 65% null values\n\ncolnull = ['OWN_CAR_AGE', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG', \n           'FLOORSMIN_AVG', 'NONLIVINGAPARTMENTS_AVG', 'FONDKAPREMONT_MODE']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_xcorr1 = df_xcorr.drop(colnull, 1)\nprint (df_xcorr1.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Go through remaining features"},{"metadata":{"trusted":true},"cell_type":"code","source":"print (df_xcorr1.columns[:20])\nprint (df_xcorr1.columns[20:40])\nprint (df_xcorr1.columns[40:60])\nprint (df_xcorr1.columns[60:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Remove (random behavior)\n* 'REGION_POPULATION_RELATIVE'\n* 'WEEKDAY_APPR_PROCESS_START' \n* 'HOUR_APPR_PROCESS_START'  \n* 'DAYS_REGISTRATION'\n* 'DAYS_ID_PUBLISH'\n* 'NAME_TYPE_SUITE'\n* 'DAYS_LAST_PHONE_CHANGE'"},{"metadata":{"trusted":true},"cell_type":"code","source":"col =['TARGET', 'CODE_GENDER', 'NAME_CONTRACT_TYPE', 'REGION_RATING_CLIENT']\n\npic = df_xcorr[col]\nn = len(pic.columns)\ndrawbar(n,pic)\n\nshowpro(n,pic)\n\n# remove XNA code_gender","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col =['CNT_CHILDREN', 'CNT_FAM_MEMBERS', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE']\n\npic = df_xcorr[col]\nn = len(pic.columns)\ndrawbar(n,pic)\n\nshowpro(n,pic)\n\n\n# keep 'CNT_FAM_MEMBERS'\n# 70% have 0 children","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = [ 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']\n\npic =df_xcorr[col]\n#pic.head()\n\nn = len(pic.columns)\ndrawbar(n,pic)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = [ 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY']\n\npic = df_xcorr[col]        \nn = len(pic.columns)\ndrawdistplot(n,pic,10)\n\n\n\n# keep basic information","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = [ 'REGION_POPULATION_RELATIVE', \n        'DAYS_BIRTH',  'DAYS_EMPLOYED',  'DAYS_REGISTRATION','DAYS_ID_PUBLISH',\n        'OWN_CAR_AGE']\n\n\npic = df_xcorr[col]        \nn = len(pic.columns)\ndrawdistplot(n,pic,20)\n\n# Remove \n# DAYS_REGISTRATION','DAYS_ID_PUBLISH',  -  It should not affect credibility","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col =['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']\n\npic = df_xcorr[col]        \nn = len(pic.columns)\ndrawdistplot(n,pic,10)\n\n# Keep all 3 columns,\n# unknown data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pic = df_xcorr1[['REG_REGION_NOT_LIVE_REGION','REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION',\n'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY']]\n\nn = len(pic.columns)\n\nshowpro(n,pic)\n\npic = df_xcorr1[['TARGET', 'REG_REGION_NOT_LIVE_REGION','REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION',\n'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY']]\n\n\ncrosst(n,pic)\n\nplt.figure(figsize=(16,16))\ncorx = pic.corr()\nsns.heatmap(corx, annot=True, cmap=plt.cm.Reds)\nplt.show()\n\n\nupperx = corx.where(np.triu(np.ones(corx.shape), k=1)\n                          .astype(np.bool))\n\n#print (upper)\nto_dropx = [column for column in upperx\n           .columns if any(upperx[column] > 0.80)]\n\nprint (to_dropx)\nprint (len(to_dropx))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pic = df_xcorr[['FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE',\n       'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL']]        \n\nn = len(pic.columns)\ndrawbar(n,pic)\n\n\nshowpro(n,pic)\n\n# Since 100% provided mobile\n# All columns not needed\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pic = df_xcorr1[['AMT_REQ_CREDIT_BUREAU_HOUR',\n       'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK',\n       'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT',\n       'AMT_REQ_CREDIT_BUREAU_YEAR']]\n\nn = len(pic.columns)\n\ndrawbar(n,pic)\n\n\n#keep 'AMT_REQ_CREDIT_BUREAU_YEAR'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pic = df_xcorr1[['FLAG_DOCUMENT_2',\n       'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5',\n       'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8',\n       'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11',\n       'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14',\n       'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17',\n       'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20',\n       'FLAG_DOCUMENT_21']]\n\nn = len(pic.columns)\n\nshowpro(n,pic)\n\n\n\n# Remove 99% are 0, 'FLAG_DOCUMENT_2','FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_9' , 'FLAG_DOCUMENT_10',\n#  'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14',\n#'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', \n#'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21'\n\n\n#Keep'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6', , 'FLAG_DOCUMENT_8',\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" col = ['APARTMENTS_AVG', 'BASEMENTAREA_AVG',\n'YEARS_BEGINEXPLUATATION_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG',\n'FLOORSMAX_AVG', 'LANDAREA_AVG', 'NONLIVINGAREA_AVG']\n        \npic = df_xcorr1[col]        \nn = len(pic.columns)\ndrawdistplot(n,pic,10)\n\ncol = ['HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE']\npic = df_xcorr1[col]        \nn = len(pic.columns)\ndrawbar(n,pic)\nshowpro(n,pic)\n\n#remove 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE'\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = ['OBS_30_CNT_SOCIAL_CIRCLE',\n       'DEF_30_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE']\n\npic = df_xcorr1[col]\n\nn = len(pic.columns)\ndrawbar(n,pic)\n\n\n# Keep 'DEF_30_CNT_SOCIAL_CIRCLE'\n# interested in defaulted behaviour","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Columns kept for modelling "},{"metadata":{"trusted":true},"cell_type":"code","source":"finalcol = ['SK_ID_CURR', 'TARGET', 'CODE_GENDER', 'NAME_CONTRACT_TYPE', 'REGION_RATING_CLIENT', \n            'CNT_FAM_MEMBERS', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE','NAME_INCOME_TYPE', \n            'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',\n            'DAYS_BIRTH', 'DAYS_EMPLOYED', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', \n            'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION', \n            'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY', 'AMT_REQ_CREDIT_BUREAU_YEAR', \n            'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_8', \n            'APARTMENTS_AVG', 'BASEMENTAREA_AVG','YEARS_BEGINEXPLUATATION_AVG', 'ELEVATORS_AVG', \n            'ENTRANCES_AVG', 'FLOORSMAX_AVG', 'LANDAREA_AVG', 'NONLIVINGAREA_AVG', 'DEF_30_CNT_SOCIAL_CIRCLE']\n\ndf_app = df[finalcol]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = df_app['CODE_GENDER'].isin(['F', 'M'])\ndf_app = df_app[col]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### AMT ANNUITY\n\n- There are null amt annuity \n- To be removed\n\n### Replace Nan to 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop nan AMT_ANNUITY\ndf_app.dropna(subset=['AMT_ANNUITY'], inplace = True)\n\ndf_app.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_app.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Replace categorical data into labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_app.NAME_CONTRACT_TYPE.replace(['Cash loans', 'Revolving loans'],[0,1], inplace = True)\ndf_app.CODE_GENDER.replace(['M', 'F'], [0, 1], inplace = True)\ndf_app.FLAG_OWN_CAR.replace(['Y', 'N'], [1,0], inplace = True)\ndf_app.FLAG_OWN_REALTY.replace(['Y', 'N'], [1,0], inplace = True)\ndf_app.NAME_INCOME_TYPE.replace(['Businessman', 'State servant', 'Commercial associate'\n                                 , 'Working', 'Maternity leave', 'Pensioner', 'Unemployed', 'Student'],\n                               [0,1,2,3,4,5,6,7], inplace = True)\n\ndf_app.NAME_EDUCATION_TYPE.replace(['Academic degree', 'Higher education', 'Incomplete higher',\n                            'Secondary / secondary special', 'Lower secondary'],\n                           [0,1,2,3,4], inplace = True)\n\ndf_app.NAME_FAMILY_STATUS.replace(['Married','Civil marriage', 'Single / not married', 'Separated', \n                                   'Widow', 'Unknown'],[0,1,2,3,4,5], inplace = True)\n\ndf_app.NAME_HOUSING_TYPE.replace(['House / apartment',  'Municipal apartment', 'Office apartment', \n                          'Co-op apartment', 'Rented apartment', 'With parents'],\n                        [0,1,2,3,4,5], inplace = True)\n\ndf_app.OCCUPATION_TYPE.replace(['Laborers', 'Core staff', 'Accountants', 'Managers', \n                                 'Drivers', 'Sales staff', 'Cleaning staff', 'Cooking staff',\n                                 'Private service staff', 'Medicine staff', 'Security staff',\n                                 'High skill tech staff', 'Waiters/barmen staff', \n                                 'Low-skill Laborers', 'Realty agents', 'Secretaries', 'IT staff','HR staff'],\n                              [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ndf_app['ORGANIZATION_TYPE'] = labelencoder.fit_transform(df_app['ORGANIZATION_TYPE'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clean POS Cash Balance"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_pos = pd.read_csv('../input/POS_CASH_balance.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_pos.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Retrieve the max value of days due past\n\ndf_pos1 = (\n    df_pos.groupby([\"SK_ID_PREV\", \"SK_ID_CURR\"])\n    .agg({'SK_DPD_DEF':'max'})\n    .reset_index()\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Groupby ID_CURR\n\ndf_pos2 = (\n    df_pos1.groupby('SK_ID_CURR')\n    .agg({'SK_DPD_DEF': 'max' })\n    .reset_index()\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# write POS data to csv\n\ndf_pos2.to_csv(\"POS.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merge All Data to Application Data"},{"metadata":{"trusted":false},"cell_type":"code","source":"# read application data\ndf_01 = pd.read_csv('precredinstall.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_01 = df_01.rename(columns={'SK_ID_CURR_': 'SK_ID_CURR'})\ndf_01.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# read POS Cash data\ndf_02 =  pd.read_csv('POS.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_02 = df_02.rename(columns={'SK_DPD_DEF': 'DPD_DEF_POS'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Merging of all csv files"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_f1 = pd.merge(df_app,df_01, on=\"SK_ID_CURR\", how= \"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_f2 = pd.merge(df_f1,df_02, on=\"SK_ID_CURR\", how= \"left\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## After merging\n\n- Replace Nan with 0"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_f2.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_f2.to_csv(\"final.csv\", index =False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_f2.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate Predictive Models"},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.read_csv('final.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"s = df['TARGET'].groupby(df['TARGET']).size()\nprint (s)\n\nfig = plt.figure(figsize=(10,6))\ns.plot.bar()\n\nplt.show()\n\n# data Imbalance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### Data Imbalance\n\nTarget 1 & 0\nHowever, 92% are 0 and 8% are 1 as shown in the graph\n\n#### To balance the data for training and testing\n\n- Target 1 : Select All (24,825 rows)\n- Target 0 : Random pick 25,000 rows\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"# extract target = 1\ncond1 = df['TARGET'] == 1\ndf_1 = df[cond1]\n\n# extract target = 0\ncond2 = df['TARGET'] == 0\ndf_0 = df[cond2]\n\n# random pick 25,000 rows of target = 0\ndf_0 = df_0.sample(n = 25000, random_state = 1)\n\nprint (len(df_1), len(df_0))\n\n# concat both target = 1 and 0 into dataframe\ndf_c = pd.concat([df_1,df_0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Assign X & Y"},{"metadata":{"trusted":false},"cell_type":"code","source":"X = df_c[['CODE_GENDER', 'NAME_CONTRACT_TYPE',\n       'REGION_RATING_CLIENT', 'CNT_FAM_MEMBERS', 'OCCUPATION_TYPE',\n       'ORGANIZATION_TYPE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE',\n       'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'FLAG_OWN_CAR',\n       'FLAG_OWN_REALTY', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'AMT_INCOME_TOTAL',\n       'AMT_CREDIT_x', 'AMT_ANNUITY', 'EXT_SOURCE_1', 'EXT_SOURCE_2',\n       'EXT_SOURCE_3', 'REG_REGION_NOT_LIVE_REGION',\n       'REG_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY',\n       'REG_CITY_NOT_WORK_CITY', 'AMT_REQ_CREDIT_BUREAU_YEAR',\n       'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6',\n       'FLAG_DOCUMENT_8', 'APARTMENTS_AVG', 'BASEMENTAREA_AVG',\n       'YEARS_BEGINEXPLUATATION_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG',\n       'FLOORSMAX_AVG', 'LANDAREA_AVG', 'NONLIVINGAREA_AVG',\n       'DEF_30_CNT_SOCIAL_CIRCLE', 'AMT_CREDIT_y', 'NFLAG_INSURED_ON_APPROVAL',\n       'AMT_INSTALMENT', 'Days_Past', 'Payment_Made', 'count', 'Approved',\n       'Refused', 'AMT_DRAWINGS_CURRENT_count', 'AMT_DRAWINGS_CURRENT_mean',\n       'SK_DPD_DEF_max', 'DPD_DEF_POS']]\n\ny = df_c['TARGET']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split data into train (75%) and test data (25%)"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print (len(X_train), len(X_test))\nprint (len(y_train), len(y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling with all the columns as Input"},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nlogreg = LogisticRegression(solver='lbfgs', max_iter=1000)\nlogreg.fit(X_train,y_train)\ny_pred = logreg.predict(X_train)\nprint('Train accuracy score:',accuracy_score(y_train,y_pred))\nprint('Test accuracy score:', accuracy_score(y_test,logreg.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree - No Pruning"},{"metadata":{"trusted":false},"cell_type":"code","source":"dtree = DecisionTreeClassifier(criterion=\"entropy\")\ndtree.fit(X_train, y_train)\ny_pred = dtree.predict(X_train)\nprint('Train accuracy score:',accuracy_score(y_train,y_pred))\nprint('Test accuracy score:', accuracy_score(y_test,dtree.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree - Pruning"},{"metadata":{"trusted":false},"cell_type":"code","source":"dptree = DecisionTreeClassifier(max_depth=5, criterion=\"entropy\")\ndptree.fit(X_train, y_train)\ny_pred = dptree.predict(X_train)\nprint('Train accuracy score:',accuracy_score(y_train,y_pred))\nprint('Test accuracy score:', accuracy_score(y_test,dptree.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest "},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier as RFC\nrfc_b = RFC(n_estimators=100)\nrfc_b.fit(X_train,y_train)\ny_pred = rfc_b.predict(X_train)\nprint('Train accuracy score:',accuracy_score(y_train,y_pred))\nprint('Test accuracy score:', accuracy_score(y_test,rfc_b.predict(X_test)))\n\n\n# overfit on the training data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NN Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"# standardize the data\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_trainS = scaler.transform(X_train)\nX_testS = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# run the model\n\nfrom sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=3000)\nmlp.fit(X_trainS,y_train)\ny_pred = mlp.predict(X_trainS)\n\nprint('Train accuracy score:',accuracy_score(y_train,y_pred))\nprint('Test accuracy score:', accuracy_score(y_test,mlp.predict(X_testS)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling with selected 20 features"},{"metadata":{},"cell_type":"markdown","source":"## Feature selection"},{"metadata":{"trusted":false},"cell_type":"code","source":"features = [x for i,x in enumerate(X_train.columns)]\n# Find the full list of features in the dataset\nprint(features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Use Random Forest for Feature Selection"},{"metadata":{"trusted":false},"cell_type":"code","source":"\nn_features = len(features)\n\nplt.figure(figsize=(8,15))\nplt.barh(range(n_features), rfc_b.feature_importances_, align='center')\nplt.yticks(np.arange(n_features), features)\nplt.xlabel(\"Feature importance\")\nplt.ylabel(\"Feature\")\nplt.title(\"Feature Importance Plot\")\nplt.ylim(-1, n_features)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Selected Features"},{"metadata":{"trusted":false},"cell_type":"code","source":"# sort the more important features from random forest model\n# select the top 20 features for modeling again\n\n\nfi = rfc_b.feature_importances_\ncol = np.array(features)\n\ndf_features = pd.DataFrame({'c': col , 'f':fi})\n\nselected = df_features.sort_values('f', ascending = False)[:20]['c'].tolist()\nprint (selected)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_trainA = X_train[selected]\nX_testA  = X_test[selected]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rerun Random Forest Model with Reduced 20 Features"},{"metadata":{"trusted":false},"cell_type":"code","source":"rfc_bA = RFC(n_estimators=100)\nrfc_bA.fit(X_trainA,y_train)\ny_pred = rfc_bA.predict(X_trainA)\nprint('Train accuracy score:',accuracy_score(y_train,y_pred))\nprint('Test accuracy score:', accuracy_score(y_test,rfc_bA.predict(X_testA)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rerun Neural Networ with Reduced 20 Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(X_trainA)\n\nX_trainS = scaler.transform(X_trainA)\nX_testS = scaler.transform(X_testA)\n\n\nfrom sklearn.neural_network import MLPClassifier\nmlpf = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=3000)\nmlpf.fit(X_trainS,y_train)\ny_pred = mlpf.predict(X_trainS)\n\nprint('Train accuracy score:',accuracy_score(y_train,y_pred))\nprint('Test accuracy score:', accuracy_score(y_test,mlpf.predict(X_testS)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}