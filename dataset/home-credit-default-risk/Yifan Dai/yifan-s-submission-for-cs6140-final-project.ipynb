{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a1af2e718c3a113ac205641175d5374e77c6df3"},"cell_type":"markdown","source":"## Read tables"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"app_train = pd.read_csv('../input/application_train.csv')\napp_test = pd.read_csv('../input/application_test.csv')\nbureau = pd.read_csv('../input/bureau.csv')\nbureau_balance = pd.read_csv('../input/bureau_balance.csv')\npos_cash_balance = pd.read_csv('../input/POS_CASH_balance.csv')\n\nprevious_app = pd.read_csv('../input/previous_application.csv')\ninstallments_payments = pd.read_csv('../input/installments_payments.csv')\ncredit_card_balance = pd.read_csv('../input/credit_card_balance.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7367d34fccbb6efb46b59663667e4e534ee2b49e"},"cell_type":"markdown","source":"## Get the previous loan counts of the users."},{"metadata":{"trusted":true,"_uuid":"e6a4e123538aa62ada569d87e95aa7ce71f0ed0a"},"cell_type":"code","source":"previous_loan_counts = bureau.groupby('SK_ID_CURR', as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU': 'previous_loan_counts'})\nprevious_loan_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3fa927b98e58b81dd5d6ae8a27f1fd230efba48b"},"cell_type":"markdown","source":"## Deal with the numerical columns"},{"metadata":{"trusted":true,"_uuid":"79845776ffc97531122b73bc3a1475cc7c5bc0db"},"cell_type":"code","source":"def agg_numeric(df, group_var, df_name):\n    \"\"\"Aggregates the numeric values in a dataframe. This can\n    be used to create features for each instance of the grouping variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the dataframe to calculate the statistics on\n        group_var (string): \n            the variable by which to group df\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated for \n            all numeric columns. Each instance of the grouping variable will have \n            the statistics (mean, min, max, sum; currently supported) calculated. \n            The columns are also renamed to keep track of features created.\n    \n    \"\"\"\n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != group_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    group_ids = df[group_var]\n    numeric_df = df.select_dtypes('number')\n    numeric_df[group_var] = group_ids\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n\n    # Need to create new column names\n    columns = [group_var]\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        # Skip the grouping variable\n        if var != group_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1][:-1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n\n    agg.columns = columns\n    return agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa72aefa6a1efc7ae6843dcf6b4de203eae6a805"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebfddde92ba4a1cf13ced374cc5dd072f12fb62e"},"cell_type":"code","source":"bureau_agg = agg_numeric(bureau.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d39aab87e129a30ec5949b7d0c6e61bf6fc08e86"},"cell_type":"code","source":"app_train['CREDIT_INCOME_PERCENT'] = app_train['AMT_CREDIT'] / app_train['AMT_INCOME_TOTAL']\napp_train['ANNUITY_INCOME_PERCENT'] = app_train['AMT_ANNUITY'] / app_train['AMT_INCOME_TOTAL']\napp_train['CREDIT_TERM'] = app_train['AMT_ANNUITY'] / app_train['AMT_CREDIT']\napp_train['DAYS_EMPLOYED_PERCENT'] = app_train['DAYS_EMPLOYED'] / app_train['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0767966ff6cc6ff4bfdd9ae092d2a716d163999d"},"cell_type":"code","source":"app_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18134a9803ac4aac6597d249d97bf0abf5cd2e75"},"cell_type":"code","source":"app_train = app_train.join(bureau_agg, how='left', on='SK_ID_CURR', lsuffix='1', rsuffix='2') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e67f835c36cdf56142831381289d974b981c4e7"},"cell_type":"code","source":"app_train = app_train.rename(index=str, columns={\"SK_ID_CURR1\": \"SK_ID_CURR\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85932c3b844205be83cda41f120624816470823e"},"cell_type":"code","source":"app_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3932eebd4a9990ef0e03548753ca483dbcd0515c"},"cell_type":"code","source":"app_test['CREDIT_INCOME_PERCENT'] = app_test['AMT_CREDIT'] / app_test['AMT_INCOME_TOTAL']\napp_test['ANNUITY_INCOME_PERCENT'] = app_test['AMT_ANNUITY'] / app_test['AMT_INCOME_TOTAL']\napp_test['CREDIT_TERM'] = app_test['AMT_ANNUITY'] / app_test['AMT_CREDIT']\napp_test['DAYS_EMPLOYED_PERCENT'] = app_test['DAYS_EMPLOYED'] / app_test['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4b5c129deb3016698780536bcb37d6d772446db"},"cell_type":"code","source":"app_test = app_test.join(bureau_agg, how='left', on='SK_ID_CURR', lsuffix='1', rsuffix='2') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"270e338f26e1940e02593c7e55216fb2d1a69b41"},"cell_type":"code","source":"app_test = app_test.rename(index=str, columns={\"SK_ID_CURR1\": \"SK_ID_CURR\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86172f4bf7b7f318976500aec1381b6b634e614e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f724f024ca694c401f0494095586c51d1a7d55dc"},"cell_type":"markdown","source":"# Deal with the categorical columns"},{"metadata":{"trusted":true,"_uuid":"0bdc8d8a432c0c07a21e6a97b0d79495336f8b56"},"cell_type":"code","source":"def normalize_categorical(df, group_var, col_name):\n    \n    \"\"\"Computes counts and normalized counts for each observation\n    of `group_var` for each unique category in every categorical variable\n    \n    Parameters \n    ----------\n    df - DataFrame for which we will calculate count\n    \n    group_var  = string\n        The variable by which to group the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    col_name = string\n            Variable added to the front of column names to keep track of columns\n            \n            \"\"\"\n    # select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('object'))\n    \n    # Make sure to put the identifying id on the column\n    categorical[group_var] = df[group_var]\n    \n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])                                              \n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['count', 'count_norm']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (col_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    return categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf31d83ecb53f9347b33d170cce3d05636a6a828"},"cell_type":"code","source":"bureau_counts = normalize_categorical(bureau, group_var = 'SK_ID_CURR', col_name = 'bureau')\nbureau_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f319318cfd276f78746b429245ceb2bbfe04a2fc"},"cell_type":"code","source":"data_bureau_agg=bureau.groupby(by='SK_ID_CURR').mean()\ndata_credit_card_balance_agg=credit_card_balance.groupby(by='SK_ID_CURR').mean()\ndata_previous_application_agg=previous_app.groupby(by='SK_ID_CURR').mean()\ndata_installments_payments_agg=installments_payments.groupby(by='SK_ID_CURR').mean()\ndata_POS_CASH_balance_agg=pos_cash_balance.groupby(by='SK_ID_CURR').mean()\n\ndef merge(df):\n    df = df.join(data_bureau_agg, how='left', on='SK_ID_CURR', lsuffix='1', rsuffix='2') \n    df = df.join(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n    df = df.merge(previous_loan_counts, on = 'SK_ID_CURR', how = 'left')\n    df = df.join(data_credit_card_balance_agg, how='left', on='SK_ID_CURR', lsuffix='1', rsuffix='2')    \n    df = df.join(data_previous_application_agg, how='left', on='SK_ID_CURR', lsuffix='1', rsuffix='2')   \n    df = df.join(data_installments_payments_agg, how='left', on='SK_ID_CURR', lsuffix='1', rsuffix='2') \n    \n    return df\n\ntrain = merge(app_train)\ntest = merge(app_test)\ndisplay(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9aff5e96def6e33a43e9315ff4df6ed81c6df8e"},"cell_type":"code","source":"#combining the data\nntrain = train.shape[0]\nntest = test.shape[0]\n\ny_train = train.TARGET.values\n\n#train_df = train_df.drop\n\nall_data = pd.concat([train, test]).reset_index(drop=True)\nall_data.drop(['TARGET'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fd04fea8cb42c01a80e8f2f80865424448a4f46"},"cell_type":"markdown","source":"## Adjust some features"},{"metadata":{"trusted":true,"_uuid":"b167c613454c68229f30a1c7bc17055838ce7c02"},"cell_type":"code","source":"# Now we will convert days employed and days registration and days id publish to a positive no. \ndef correct_birth(df):\n    \n    df['DAYS_BIRTH'] = round((df['DAYS_BIRTH'] * (-1))/365)\n    return df\n\ndef convert_abs(df):\n    df['DAYS_EMPLOYED'] = abs(df['DAYS_EMPLOYED'])\n    df['DAYS_REGISTRATION'] = abs(df['DAYS_REGISTRATION'])\n    df['DAYS_ID_PUBLISH'] = abs(df['DAYS_ID_PUBLISH'])\n    df['DAYS_LAST_PHONE_CHANGE'] = abs(df['DAYS_LAST_PHONE_CHANGE'])\n    return df\n\n# Now we will fill misisng values in OWN_CAR_AGE. \n#Most probably there will be missing values if the person does not own a car. So we will fill with 0\n\ndef missing(df):\n    \n    features = ['previous_loan_counts','NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAPARTMENTS_AVG','NONLIVINGAREA_MEDI','OWN_CAR_AGE']\n    \n    for f in features:\n        df[f] = df[f].fillna(0 )\n    return df\n\ndef transform_app(df):\n    df = correct_birth(df)\n    df = convert_abs(df)\n    df = missing(df)\n    return df\n\n   \n\nall_data = transform_app(all_data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1aad6590191d510a42c06ef0822b1453e30ffcc6"},"cell_type":"code","source":"# counting no of phones given by the company and delete the irrelevant features\nall_data['NO_OF_CLIENT_PHONES'] = all_data['FLAG_MOBIL'] + all_data['FLAG_EMP_PHONE'] + all_data['FLAG_WORK_PHONE']\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c76f459c4792ed8b077cd5ee0f17a1600982db0d"},"cell_type":"code","source":"# add a feature to determine if client's permanent city does not match with contact/work city\nall_data['FLAG_CLIENT_OUTSIDE_CITY'] = np.where((all_data['REG_CITY_NOT_WORK_CITY']==1) & (all_data['REG_CITY_NOT_LIVE_CITY']==1),1,0)\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c0e5d1952f88e62694c785ed22f3144d7584fa7"},"cell_type":"code","source":" # add a feature to determine if client's permanent city does not match with contact/work region\nall_data['FLAG_CLIENT_OUTSIDE_REGION'] = np.where((all_data['REG_REGION_NOT_LIVE_REGION']==1) & (all_data['REG_REGION_NOT_WORK_REGION']==1),1,0)\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91e48b5fa9cced9a43fb204d023e3584a1ca3f03"},"cell_type":"markdown","source":"## Delete useless features"},{"metadata":{"trusted":true,"_uuid":"a185f2f6015fa56d265c7585c422027bb2c7be65"},"cell_type":"code","source":"# deleting useless features\ndef delete(df):\n   # useless=['FLAG_MOBIL', 'FLAG_EMP_PHONE' ,'FLAG_WORK_PHONE','REG_CITY_NOT_WORK_CITY','REG_CITY_NOT_LIVE_CITY','REG_REGION_NOT_LIVE_REGION','REG_REGION_NOT_WORK_REGION']\n    #for feature in useless:\n     return df.drop(['FLAG_MOBIL', 'FLAG_EMP_PHONE' ,'FLAG_WORK_PHONE','REG_CITY_NOT_WORK_CITY','REG_CITY_NOT_LIVE_CITY','REG_REGION_NOT_LIVE_REGION','REG_REGION_NOT_WORK_REGION'], axis=1)\ndef transform(df):\n   # df = convert_abs(df)\n    df = delete(df)\n   \n    return df\n\nall_data = transform(all_data)\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3e45ee09a6220d0b2b726c883019989f9024168"},"cell_type":"code","source":"# delete Ids\n\ndef delete_id(df):\n    return df.drop(['SK_ID_CURR', 'SK_ID_PREV','SK_ID_BUREAU'], axis = 1)\n\nall_data = delete_id(all_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c67b7a014f6a62c8c3da709d6ebfd9b2dd72b34e"},"cell_type":"markdown","source":"## Handling missing values"},{"metadata":{"trusted":true,"_uuid":"7d7d5cd922ac480dc957d5f490adb7c2a7416f32"},"cell_type":"code","source":"def miss_numerical(df):\n    \n    features = ['previous_loan_counts','NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAPARTMENTS_AVG','NONLIVINGAREA_MEDI','OWN_CAR_AGE']\n    numerical_features = all_data.select_dtypes(exclude = [\"object\"] ).columns\n    #print(numerical_features)\n    for f in numerical_features:\n        #print(f)\n        if f not in features:\n            df[f] = df[f].fillna(df[f].median())\n      \n    return df\n\ndef miss_categorical(df):\n    \n    categorical_features = all_data.select_dtypes(include = [\"object\"]).columns\n    \n    for f in categorical_features:\n        df[f] = df[f].fillna(df[f].mode()[0])\n        \n    return df\n\ndef transform_feature(df):\n    df = miss_numerical(df)\n    df = miss_categorical(df)\n    #df = fill_cabin(df)\n    return df\n\nall_data = transform_feature(all_data)\n\n\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ab5533d17bd1e40edb01af0c318ea643c0c5414"},"cell_type":"markdown","source":"# Logistic regression Model"},{"metadata":{"trusted":true,"_uuid":"78d85f8583c4755c671641acc450e2eb67948ae5"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\ndef encoder(df):\n    scaler = MinMaxScaler()\n    numerical = all_data.select_dtypes(exclude = [\"object\"]).columns\n    features_transform = pd.DataFrame(data= df)\n    features_transform[numerical] = scaler.fit_transform(df[numerical])\n    display(features_transform.head(n = 5))\n    return df\n\nall_data = encoder(all_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15c3766bb669cab6222c75bc3bfbcecf84d52658"},"cell_type":"code","source":"# Converting into categorical features\n\n# Create a label encoder object\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle_count = 0\n\n\n# Iterate through the columns\nfor col in all_data:\n    if all_data[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(all_data[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(all_data[col])\n            # Transform both training and testing data\n            all_data[col] = le.transform(all_data[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n           \nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9562ee4b20cd1e9cd63d208717d2acc475d3db36"},"cell_type":"code","source":"all_data = pd.get_dummies(all_data)\ndisplay(all_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e82364afcae25204d29806e6749104198ac9c31"},"cell_type":"code","source":"train = all_data[:ntrain]\ntest = all_data[ntrain:]\n\nprint(\"Training shape\", train.shape)\nprint(\"Testing shape\", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"b8c7463dc1b8f96c1f02c0cb19fcae334db46eff"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(train, y_train, test_size = 0.1, random_state = 100)\nprint(\"X Training shape\", X_train.shape)\nprint(\"X Testing shape\", X_test.shape)\nprint(\"Y Training shape\", Y_train.shape)\nprint(\"Y Testing shape\", Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"5de700507255cef34f0afa08874b7b888593901c"},"cell_type":"code","source":"from sklearn.metrics import make_scorer\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(random_state=0, class_weight='balanced', C=500)\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict_proba(X_test)[:,1]\n\nprint('Train/Test split results:')\nprint(\"ROC\",  roc_auc_score(Y_test, Y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e49c51ed629071f2a9c9d4084da9489784b15f3e"},"cell_type":"code","source":"pred_test = logreg.predict_proba(train)\nsubmission = pd.DataFrame({'SK_ID_CURR' : app_train['SK_ID_CURR'], \n                           'TARGET' : pred_test[:,0]})\npd.DataFrame(submission, columns=['SK_ID_CURR','TARGET'],index=None).to_csv('homecreditada.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f77fee4411bf759bd9e9c0a9a2bc55905810d35f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}