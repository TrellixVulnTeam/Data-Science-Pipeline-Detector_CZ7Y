{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Import Libs**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Introduction**","metadata":{}},{"cell_type":"markdown","source":"При выполнении EDA я хочу поэтапно решить такие проблемы : \n* Переполнение одинаковыми и пропущенными данными \n* Категориальные столбцы\n* Пропущенные данные в столбцах\n* Высокая корреляция данных ","metadata":{}},{"cell_type":"markdown","source":"# **Load Data**","metadata":{}},{"cell_type":"code","source":"workDatasets = {\n    \"test\": \"/kaggle/input/home-credit-default-risk/application_test.csv\",\n    \"train\": \"/kaggle/input/home-credit-default-risk/application_train.csv\"\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(workDatasets[\"train\"], sep=',', header=0)\ntarget = train_data['TARGET']\ntest_data = pd.read_csv(workDatasets[\"test\"], sep=',', header=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Check Data**","metadata":{}},{"cell_type":"code","source":"print(train_data.info(max_cols=122))\nprint(train_data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_data.info(max_cols=122))\nprint(test_data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Cразу видно, что часть данных неполная и часть — категориальная, они отображаются как object. \nПоэтому надо будет их как то обработать.**","metadata":{}},{"cell_type":"markdown","source":"**Также поскольку колонок очень много, я постараюсь уменьшить их количество,чтоб можно было это нормально анализировать.**","metadata":{}},{"cell_type":"markdown","source":"# **Frequency distribution of data**","metadata":{}},{"cell_type":"markdown","source":"**Давайте посмотрим на частотное распределение данных и удалим те колонки, в которых более 90% строк занимают одинаковые значения.**","metadata":{}},{"cell_type":"code","source":"drop_freq_features = []\nfor feature in train_data:\n    if (train_data[feature].value_counts()*100/len(train_data[feature])).max() > 90 and feature not in [\"TARGET\", \"NAME_CONTRACT_TYPE\"]:\n        drop_freq_features.append(feature)\nprint(len(drop_freq_features), \" столбцов подлежат уничтожению.\")\nfor feature in drop_freq_features:\n    print(f\"Feature : {feature}\\nmoda :\\n{(train_data[feature].value_counts()*100/len(train_data[feature])).max()}\\n\" + \"-\"*50)\ntrain_data = train_data.drop(drop_freq_features, axis=1)\ntest_data = test_data.drop(drop_freq_features, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Таких оказалось 27 столбцов. Почти все они о том какую информацию человек предоставил,  и о совпадении мест жительства, регистрации и работы.**","metadata":{}},{"cell_type":"markdown","source":"# **Missing Values**","metadata":{}},{"cell_type":"code","source":"def missing_values_table(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = df.isnull().sum() * 100 / len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing', 1 : '% of All'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of All', ascending=False).round(1)\n        print (str(mis_val_table_ren_columns.shape[0]) +\n              \" столбцов с неполными данными.\")\n        return mis_val_table_ren_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values = missing_values_table(train_data)\nmissing_values.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Видно, что присутствует 66 столбцов с неполными данными. Столбцы, в которых отсутствует >= 50% данных я решил удалить.**","metadata":{}},{"cell_type":"code","source":"mis = missing_values[missing_values.iloc[:,1] >= 50].iloc[:,1]\ndrop_missing_values = []\nfor missing_value in mis.index:\n    drop_missing_values.append(missing_value)\nprint(len(drop_missing_values), \" столбец подлежит уничтожению\")\nfor feature in drop_missing_values:\n    print(f\"Feature :\\n{feature}\\nMissing values % : {mis[feature]}\")\ntrain_data = train_data.drop(drop_missing_values, axis=1)\ntest_data = test_data.drop(drop_missing_values, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fill_missing_values = list(set(missing_values.index).difference(set(drop_missing_values)))\nprint(f\"{len(fill_missing_values)} столбцов с пропущенными данными осталось.\")\nfor feature in fill_missing_values:\n    print(f\"Feature : {feature}\\ntype : {train_data[feature].dtype}\\n\", \"-\"*50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Поскольку 41 столбец удален, осталось решить проблему лишь с 25. Перед тем как что-то делать с ними, я разберусь с категориальными данными, потому что среди них тоже есть столбцы с пропущенными данными**","metadata":{}},{"cell_type":"markdown","source":"# **Catecorical Data**","metadata":{}},{"cell_type":"code","source":"categorical_columns = train_data.select_dtypes(include=[object]).apply(pd.Series.nunique, axis = 0)\nprint(categorical_columns)\nprint(\"Categorical columns count : \", len(categorical_columns))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**У нас есть 13 столбцов категориальных данных, из которых 5 с количеством уникальных категорий <= 3. Есть 2 варината того, что можно сделать с этими столбцами.**\n1. Label Encoding\n2. One-Hot-encoding\n**Я воспользуюсь Label Encoding для столбцов, количеством уникальных категорий <= 3, а для остальных One-Hot-encoding**","metadata":{}},{"cell_type":"markdown","source":"**Но прежде заполним пропуски. Для этого посмотрим на наши категориальные столбцы поближе.**","metadata":{}},{"cell_type":"code","source":"categorical_data = train_data[train_data.select_dtypes(include=[object]).columns]\nfor feature in categorical_data:\n    print(\"Values %\")\n    print( 100 * train_data[feature].value_counts() / len(train_data[feature]))\n    if feature in fill_missing_values:\n        print(\"Missing values : \", missing_values.at[feature, '% of All'], \"%\")  \n    train_data[feature].value_counts().plot(kind='bar')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Пропущенные данные есть в столбцах : [OCCUPATION_TYPE, EMERGENCYSTATE_MODE, NAME_TYPE_SUITE].\n* NAME_TYPE_SUITE\nЗдесь можно просто заполнить пропущенные значения Модой, поскольку missing % 0.4, а Мода составляет 80% от всех данных.\n* EMERGENCYSTATE_MODE\nЕе я решил удалить, потому что на 51.8% он заполнен одинаковыми данными и при этом 47.4% данных пропущено.\n* OCCUPATION_TYPE\nЗаполним потом.","metadata":{}},{"cell_type":"code","source":"train_data.drop(['EMERGENCYSTATE_MODE'], axis=1)\ntest_data.drop(['EMERGENCYSTATE_MODE'], axis=1)\nmod = train_data['NAME_TYPE_SUITE'].mode()\ntrain_data['NAME_TYPE_SUITE'] = train_data['NAME_TYPE_SUITE'].fillna(mod[0])\ntest_data['NAME_TYPE_SUITE'] = test_data['NAME_TYPE_SUITE'].fillna(mod[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for feature in categorical_columns.index:\n    if train_data[feature].nunique()  <= 3:\n        train_data[feature] = train_data[feature].astype('category')\n        train_data[feature] = train_data[feature].cat.codes\n    if test_data[feature].nunique()  <= 3:\n        test_data[feature] = test_data[feature].astype('category')\n        test_data[feature] = test_data[feature].cat.codes\ntrain_data = pd.get_dummies(train_data)\ntest_data = pd.get_dummies(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train data : ', train_data.shape)\nprint('Test data : ', test_data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Так как количество вариантов в столбцах выборок не равное, количество столбцов теперь не совпадает. Требуется выравнивание — нужно убрать из тренировочной выборки столбцы, которых нет в тестовой.**","metadata":{}},{"cell_type":"code","source":"train_data, test_data = train_data.align(test_data, join = 'inner', axis = 1)\n\nprint('Train data : ', train_data.shape)\nprint('Test data : ', test_data.shape)\n\ntrain_data['TARGET'] = target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values_rest = missing_values_table(train_data)\nprint(missing_values_rest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Осталось лишь 22 столбца с неполными данными, а не 25, это можно обьяснить тем что было удалено 3 столбца во время выравнивания train_data к test_data.**","metadata":{}},{"cell_type":"markdown","source":"**Заполним их медианой.**","metadata":{}},{"cell_type":"code","source":"for feature in missing_values_rest.index:\n    mean = train_data[feature].mean()\n    train_data[feature] = train_data[feature].fillna(mean)\n    test_data[feature] = test_data[feature].fillna(mean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Correlation Data**","metadata":{}},{"cell_type":"code","source":"corrMatt = train_data.corr()\ncorrelations = corrMatt['TARGET'].sort_values()\nprint('Наивысшая позитивная корреляция: \\n', correlations.tail(15))\nprint('\\nНаивысшая негативная корреляция: \\n', correlations.head(15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Таким образом, все данные слабо коррелируют с TARGET. Выделяется EXT_SOURCE_2 EXT_SOURCE_3 и DAYS_BIRTH, но их корреляционный коэфициент все равно меньше 0.7 и больше -0.7, так что их можно и не трогать.**","metadata":{}},{"cell_type":"code","source":"mask = np.array(corrMatt)\nmask[np.tril_indices_from(mask)] = False\nfig,ax= plt.subplots()\nfig.set_size_inches(20,10)\nsns.heatmap(corrMatt, cmap=\"Greens\", mask= mask, vmax=.8, square=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Видно, что данные особо между собой не коррелируют. Так что на этом EDA можно завершить**","metadata":{}},{"cell_type":"markdown","source":"# **Model**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nif 'TARGET' in train_data:\n    train = train_data.drop(labels = ['TARGET'], axis=1)\nelse:\n    train = train_data.copy()\n\nfeatures = list(train.columns)\ntest = test_data.copy()\n\nscaler = MinMaxScaler(feature_range = (0, 1))\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Формат тренировочной выборки: ', train.shape)\nprint('Формат тестовой выборки: ', test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Вначале мы очистили train_data от столбца TARGET. Далее мы нормализировали наши датасеты. И видим, что до начала учебы у нас осталось 158 столбцов.**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(C = 0.0001)\n\nlog_reg.fit(train, target)\nLogisticRegression(C=0.0001, class_weight=None, dual=False,\n          fit_intercept=True, intercept_scaling=1, max_iter=100,\n          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n\n\nlog_reg_pred = log_reg.predict_proba(test)[:, 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Теперь модель можно использовать для предсказаний. Метод predict_proba даст на выходе массив m x 2, где m - количество наблюдений, первый столбец - вероятность 0, второй - вероятность 1. Нам нужен второй (вероятность невозврата).**","metadata":{}},{"cell_type":"code","source":"submit = pd.DataFrame(log_reg_pred, columns = ['TARGET'])\nsubmit['SK_ID_CURR'] = test_data['SK_ID_CURR']\nsubmit.to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Записали результат в submission.csv. Теперь можно сабмитить :)**","metadata":{}},{"cell_type":"markdown","source":"**Во время выполнения работы, я подумал, что стоило копировать датасет и уже в копиях удалять столбцы, а не в оригинале, потому что было пару раз когда хочу внести изменение вначале ноутбука, но не могу это сделать быстро, из-за того что нужно по новой запускать ноутбук. Но мне эта мысль пришла уже когда я почти все доделал и переделывать не был готов. Но в следующей работе планирую это применить. Так же стоило больше времени уделить корреляции столбов друг к другу, но их слишком много. Также заполнение пропущенных данных можно проводить учитывая корреляцию этих столбцов к столбцу TARGET, но опять все упиралось в их количество. А так работа получилась достаточно интересная.**","metadata":{}}]}