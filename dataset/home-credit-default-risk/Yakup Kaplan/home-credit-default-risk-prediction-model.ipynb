{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# HOME CREDIT DEFAULT RISK PREDICTION\n\n\"\"\"\nThis script is copied from https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features and then updated, improved.\nMost features are created by applying min, max, mean, sum and var functions to grouped tables.\nLittle feature selection is done and overfitting might be a problem since many features are related.\nThe following key ideas were used:\n    - Divide or subtract important features to get rates (like annuity and income)\n    - In Bureau Data: create specific features for Active credits and Closed credits\n    - In Previous Applications: create specific features for Approved and Refused applications\n    - Modularity: one function for each table (except bureau_balance and application_test)\n    - One-hot encoding for categorical features\nAll tables are joined with the application DF using the SK_ID_CURR key (except bureau_balance).\nYou can use LightGBM with KFold or Stratified KFold.\n- Use standard KFold CV (not stratified)\n\n--> Run LightGBM with kfold - done in 4651s=77,52 min | Full model run - done in 5031s = 83,85 min\n\n\"\"\"\n\n# Import dependencies\nimport numpy as np\nimport pandas as pd\nimport gc\nimport time\nfrom contextlib import contextmanager\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport re\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n\n\n# One-hot encoding for categorical columns with get_dummies\ndef one_hot_encoder(df, nan_as_category=True):\n    \"\"\"\n    One-hot encoding for categorical columns with get_dummies\n    Returns dataframe with one-hot encoded columns and list for new created columns\n\n    :param df: dataframe\n        dataframe whose categorical columns will be one hot encoded\n\n    :param nan_as_category: bool\n        boolean indicating if the missing values will be shown separately or not.\n\n    :return: dataframe, list for new_columns\n\n    \"\"\"\n    original_columns = list(df.columns)\n    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n    df = df.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))\n    # df.columns = [\"\".join(c if c.isalnum() else \"_\" for c in str(x)) for x in df.columns]\n    new_columns = [c for c in df.columns if c not in original_columns]\n    return df, new_columns\n\n\n# Rare encoding function for rare labels\ndef rare_encoder(dataframe, rare_perc):\n    \"\"\"\n    Rare encoding function for rare labels\n    Returns dataframe with 'Rare' encoded labels\n\n    :param dataframe: dataframe\n        dataframe to be rare encoded\n\n    :param rare_perc: float\n        percentage for lables to be accepted as 'Rare'\n\n    :return: dataframe\n\n    \"\"\"\n    temp_df = dataframe.copy()\n    rare_columns = [col for col in temp_df.columns if temp_df[col].dtypes == 'O'\n                    and (temp_df[col].value_counts() / len(temp_df) < rare_perc).any(axis=None)]\n    for var in rare_columns:\n        tmp = temp_df[var].value_counts() / len(temp_df)\n        rare_labels = tmp[tmp < rare_perc].index\n        temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])\n    return temp_df\n\n\n# Feature Engineering steps for application_train and application_test.\ndef feature_eng_application_train(df):\n    \"\"\"\n    Feature Engineering steps for application_train and application_test.\n    Returns dataframe with FE steps implemented.\n\n    :param df: dataframe\n        dataframe(application_train) for FE-steps\n\n    :return: dataframe\n\n    \"\"\"\n    # Optional: Remove 4 applications with XNA CODE_GENDER (train set) and Remove 2 applications with Unknown NAME_FAMILY_STATUS (train set)\n    df = df[df['CODE_GENDER'] != 'XNA']\n    df = df[df['NAME_FAMILY_STATUS'] != 'Unknown']\n\n    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n    # Some simple new features (percentages)\n    df['NEW_DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n    df['NEW_INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n    df['NEW_INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n    df['NEW_ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n    df['NEW_PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n    df['NEW_LOAN_VALUE_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n    df['NEW_INCOME_PER_PERSON_PERC_AMT_ANNUITY'] = df['NEW_INCOME_PER_PERSON'] / df['AMT_ANNUITY']\n    df['NEW_INCOME_PER_PERSON_PERC_PAYMENT_RATE_INCOME_PER_PERSON'] = df['NEW_INCOME_PER_PERSON'] / df[\n        'NEW_PAYMENT_RATE']\n    df.loc[(df['AMT_CREDIT'] <= df['AMT_GOODS_PRICE']), 'NEW_FLAG_CREDIT_MORE_THAN_GOODSPRICE'] = 0\n    df.loc[(df['AMT_CREDIT'] > df['AMT_GOODS_PRICE']), 'NEW_FLAG_CREDIT_MORE_THAN_GOODSPRICE'] = 1\n    df['NEW_AGE_RANGE'] = pd.cut(x=df['DAYS_BIRTH'] / -365, bins=[0, 27, 40, 50, 65, 99], labels=[1, 2, 3, 4, 5])\n    df['NEW_WORKING_YEAR_RANGE'] = pd.cut(x=df['DAYS_EMPLOYED'] / -365, bins=[0, 3, 5, 15, 50], labels=[1, 2, 3, 4])\n    df[\"NEW_TOTAL_CONTACT_INFORMATION\"] = df['FLAG_MOBIL'] + df['FLAG_EMP_PHONE'] + df['FLAG_WORK_PHONE'] + df[\n        'FLAG_CONT_MOBILE'] + df['FLAG_EMAIL']\n    df['NEW_YEAR_LAST_PHONE_CHANGE'] = pd.cut(x=df['DAYS_LAST_PHONE_CHANGE'] / -365,\n                                              bins=[-0.1, 0, 1, 2, 3, 4, 5, 11.75], labels=[0, 1, 2, 3, 4, 5, 6])\n    df[\"NEW_MISS_DOCUMENTS_20\"] = df['FLAG_DOCUMENT_2'] + df['FLAG_DOCUMENT_3'] + df['FLAG_DOCUMENT_4'] + \\\n                                  df['FLAG_DOCUMENT_5'] + df['FLAG_DOCUMENT_6'] + df['FLAG_DOCUMENT_7'] + \\\n                                  df['FLAG_DOCUMENT_8'] + df['FLAG_DOCUMENT_9'] + df['FLAG_DOCUMENT_10'] + \\\n                                  df['FLAG_DOCUMENT_11'] + df['FLAG_DOCUMENT_12'] + df['FLAG_DOCUMENT_13'] + \\\n                                  df['FLAG_DOCUMENT_14'] + df['FLAG_DOCUMENT_15'] + df['FLAG_DOCUMENT_16'] + \\\n                                  df['FLAG_DOCUMENT_17'] + df['FLAG_DOCUMENT_18'] + df['FLAG_DOCUMENT_19'] + \\\n                                  df['FLAG_DOCUMENT_20'] + df['FLAG_DOCUMENT_21']\n    df['NEW_FLAG_MISS_DOCUMENTS'] = df['NEW_MISS_DOCUMENTS_20'].apply(lambda x: 1 if x > 0 else 0)\n    df[\"NEW_AMT_REQ_CREDIT_BUREAU_YEAR\"] = df[\"AMT_REQ_CREDIT_BUREAU_HOUR\"] + df[\"AMT_REQ_CREDIT_BUREAU_DAY\"] + \\\n                                           df[\"AMT_REQ_CREDIT_BUREAU_WEEK\"] + df[\"AMT_REQ_CREDIT_BUREAU_MON\"] + \\\n                                           df[\"AMT_REQ_CREDIT_BUREAU_QRT\"] + df[\"AMT_REQ_CREDIT_BUREAU_YEAR\"]\n    df.loc[(df[\"NEW_AMT_REQ_CREDIT_BUREAU_YEAR\"] >= 7), \"NEW_AMT_REQ_CREDIT_BUREAU_YEAR\"] = 7\n\n    # Define the list for variables to drop.\n    drop_list = ['FLAG_DOCUMENT_9', 'LANDAREA_MODE', 'FLAG_WORK_PHONE', 'FLAG_DOCUMENT_8', 'FLOORSMIN_MEDI',\n                 'ELEVATORS_MODE', 'COMMONAREA_MODE', 'NONLIVINGAPARTMENTS_AVG',\n                 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_19', 'AMT_REQ_CREDIT_BUREAU_MON', 'NONLIVINGAPARTMENTS_MEDI',\n                 'REG_REGION_NOT_LIVE_REGION', 'FLAG_DOCUMENT_16', 'ENTRANCES_MODE', 'CNT_FAM_MEMBERS',\n                 'ENTRANCES_MEDI', 'YEARS_BUILD_MEDI', 'YEARS_BEGINEXPLUATATION_AVG', 'FLAG_DOCUMENT_7',\n                 'FLOORSMAX_AVG', 'FLAG_DOCUMENT_18', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'LIVINGAPARTMENTS_MODE',\n                 'FLAG_DOCUMENT_2', 'BASEMENTAREA_MODE', 'BASEMENTAREA_MEDI', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_10',\n                 'ELEVATORS_AVG', 'YEARS_BUILD_MODE', 'COMMONAREA_AVG', 'AMT_REQ_CREDIT_BUREAU_DAY', 'LIVINGAREA_MODE',\n                 'FLAG_DOCUMENT_3', 'LANDAREA_MEDI', 'DAYS_LAST_PHONE_CHANGE',\n                 'REG_REGION_NOT_WORK_REGION', 'COMMONAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI',\n                 'REGION_RATING_CLIENT_W_CITY', 'FLAG_DOCUMENT_5', 'APARTMENTS_MEDI', 'LIVINGAPARTMENTS_AVG',\n                 'FLOORSMAX_MODE', 'FLOORSMAX_MEDI', 'NONLIVINGAREA_MEDI', 'FLAG_DOCUMENT_21',\n                 'YEARS_BEGINEXPLUATATION_MODE', 'APARTMENTS_AVG', 'ENTRANCES_AVG', 'FLAG_PHONE',\n                 'LIVE_REGION_NOT_WORK_REGION', 'FLAG_DOCUMENT_6', 'BASEMENTAREA_AVG', 'FLAG_DOCUMENT_14',\n                 'FLAG_EMP_PHONE', 'NONLIVINGAPARTMENTS_MODE', 'FLOORSMIN_AVG', 'FLAG_MOBIL', 'LIVINGAREA_AVG',\n                 'FLAG_CONT_MOBILE', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'YEARS_BUILD_AVG', 'NONLIVINGAREA_AVG',\n                 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_4', 'AMT_REQ_CREDIT_BUREAU_QRT', 'LIVINGAPARTMENTS_MEDI',\n                 'FLAG_DOCUMENT_11', 'NONLIVINGAREA_MODE', 'AMT_REQ_CREDIT_BUREAU_YEAR',\n                 'DEF_60_CNT_SOCIAL_CIRCLE', 'FLAG_DOCUMENT_20', 'FLOORSMIN_MODE', 'FLAG_EMAIL',\n                 'OBS_60_CNT_SOCIAL_CIRCLE', 'ELEVATORS_MEDI', 'LANDAREA_AVG', 'APARTMENTS_MODE', 'FLAG_DOCUMENT_17',\n                 'LIVINGAREA_MEDI', 'TOTALAREA_MODE',\n                 'EMERGENCYSTATE_MODE', 'WALLSMATERIAL_MODE', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE']\n    df.drop(drop_list, axis=1, inplace=True)\n\n    return df\n\n\n# Preprocess application_train.csv and application_test.csv\ndef application_train_test(num_rows=None, nan_as_category=False):\n    \"\"\"\n    Loads, merges datasets. Afterwards, feature_eng_application_train and one_hot_encoder functions are called.\n    Returns dataframe with one-hot encoded and feature engineering implemented columns.\n\n    :param num_rows: int\n        int that shows number of rows to be loaded for the dataset\n\n    :param nan_as_category: bool\n        boolean that shows, if nan values will be created as separate columns or not.\n\n    :return: dataframe\n\n    \"\"\"\n    # Read data and merge\n    df = pd.read_csv('../input/home-credit-default-risk/application_train.csv', nrows=num_rows)\n    test_df = pd.read_csv('../input/home-credit-default-risk/application_test.csv', nrows=num_rows)\n    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n    df = df.append(test_df).reset_index()\n    # Apply feature engineering for application_train\n    df = feature_eng_application_train(df)\n    # Categorical features with Binary encode (0 or 1; two categories)\n    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n    # Categorical features with One-Hot encode\n    df, cat_cols = one_hot_encoder(df, nan_as_category)\n\n    del test_df\n    gc.collect()\n\n    return df\n\n\n# Feature Engineering steps for bureau_and_balance\ndef feature_eng_bureau_and_balance(df):\n    \"\"\"\n    Feature Engineering steps for bureau_and_balance.\n    Returns dataframe with FE steps implemented.\n\n    :param df: dataframe\n        dataframe(bureau_and_balance) for FE-steps\n\n    :return:dataframe\n\n    \"\"\"\n    df.fillna(0, inplace=True)\n\n    grp = df[['SK_ID_CURR', 'DAYS_CREDIT']].groupby(by=['SK_ID_CURR'])['DAYS_CREDIT'].count().reset_index().rename(\n        index=str, columns={'DAYS_CREDIT': 'NEW_BUREAU_LOAN_COUNT'})\n    df = df.merge(grp, on=['SK_ID_CURR'], how='left')\n\n    grp = df[['SK_ID_CURR', 'CREDIT_TYPE']].groupby(by=['SK_ID_CURR'])['CREDIT_TYPE'].nunique().reset_index().rename(\n        index=str, columns={'CREDIT_TYPE': 'NEW_BUREAU_LOAN_TYPES'})\n    df = df.merge(grp, on=['SK_ID_CURR'], how='left')\n\n    df['CREDIT_ACTIVE_BINARY'] = df['CREDIT_ACTIVE'].apply(lambda x: 1 if x == 'Active' else 0)\n    grp = df.groupby(by=['SK_ID_CURR'])['CREDIT_ACTIVE_BINARY'].mean().reset_index().rename(index=str, columns={\n        'CREDIT_ACTIVE_BINARY': 'NEW_ACTIVE_LOANS_PERCENTAGE'})\n    df = df.merge(grp, on=['SK_ID_CURR'], how='left')\n    del df['CREDIT_ACTIVE_BINARY']\n    gc.collect()\n\n    df['CREDIT_ENDDATE_BINARY'] = df['DAYS_CREDIT_ENDDATE'].apply(lambda x: 0 if x <= 0 else 1)\n    grp = df.groupby(by=['SK_ID_CURR'])['CREDIT_ENDDATE_BINARY'].mean().reset_index().rename(index=str, columns={\n        'CREDIT_ENDDATE_BINARY': 'NEW_CREDIT_ENDDATE_PERCENTAGE'})\n    df = df.merge(grp, on=['SK_ID_CURR'], how='left')\n    del df['CREDIT_ENDDATE_BINARY']\n    gc.collect()\n\n    grp1 = df[['SK_ID_CURR', 'AMT_CREDIT_SUM_DEBT']].groupby(by=['SK_ID_CURR'])['AMT_CREDIT_SUM_DEBT'].sum().\\\n        reset_index().rename(index=str, columns={'AMT_CREDIT_SUM_DEBT': 'TOTAL_CUSTOMER_DEBT'})\n    grp2 = df[['SK_ID_CURR', 'AMT_CREDIT_SUM']].groupby(by=['SK_ID_CURR'])['AMT_CREDIT_SUM'].sum().reset_index().rename(\n        index=str, columns={'AMT_CREDIT_SUM': 'TOTAL_CUSTOMER_CREDIT'})\n    df = df.merge(grp1, on=['SK_ID_CURR'], how='left')\n    df = df.merge(grp2, on=['SK_ID_CURR'], how='left')\n    del grp1, grp2\n    gc.collect()\n\n    df['NEW_DEBT_CREDIT_RATIO'] = df['TOTAL_CUSTOMER_DEBT'] / df['TOTAL_CUSTOMER_CREDIT']\n    del df['TOTAL_CUSTOMER_DEBT'], df['TOTAL_CUSTOMER_CREDIT']\n    gc.collect()\n\n    return df\n\n\n# Aggregation operations for bureau_and_balance\ndef aggregations_bureau_and_balance(bureau, bb, bureau_cat, bb_cat):\n    \"\"\"\n    Aggregation operations for bureau_and_balance\n    Returns dataframe after completing specific aggregations for numerical and categorical variables for bureau\n    and bureau_balance tables and finally joins these two tables.\n\n    :param bureau: dataframe\n        dataframe(bureau) for applying aggregations\n\n    :param bb: dataframe\n        dataframe(bureau_balance) for applying aggregations\n\n    :param bureau_cat: list\n        list that holds categorical variables for bureau table\n\n    :param bb_cat: list\n        list that holds categorical variables for bureau_balance table\n\n    :return: dataframe\n\n    \"\"\"\n    # Bureau balance: Perform aggregations and merge with bureau.csv\n    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n    for col in bb_cat:\n        bb_aggregations[col] = ['mean']\n    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n    bb_agg.columns = pd.Index(['BB_' + e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace=True)\n    del bb, bb_agg\n    gc.collect()\n\n    # Bureau and bureau_balance numeric features\n    num_aggregations = {\n        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n        'DAYS_CREDIT_UPDATE': ['mean'],\n        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n        'AMT_ANNUITY': ['max', 'mean'],\n        'CNT_CREDIT_PROLONG': ['sum'],\n        'BB_MONTHS_BALANCE_MIN': ['min'],\n        'BB_MONTHS_BALANCE_MAX': ['max'],\n        'BB_MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n        'NEW_BUREAU_LOAN_COUNT': ['mean'],\n        'NEW_BUREAU_LOAN_TYPES': ['mean'],\n        'NEW_ACTIVE_LOANS_PERCENTAGE': ['max', 'mean'],\n        'NEW_CREDIT_ENDDATE_PERCENTAGE': ['max', 'mean'],\n        'NEW_DEBT_CREDIT_RATIO': ['max', 'mean']\n    }\n    # Bureau and bureau_balance categorical features\n    cat_aggregations = {}\n    for cat in bureau_cat:\n        cat_aggregations[cat] = ['mean']\n    for cat in bb_cat:\n        cat_aggregations[\"BB_\" + cat + \"_MEAN\"] = ['mean']\n    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n\n    # Bureau: Active credits - using only numerical aggregations\n    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n    del active, active_agg\n    gc.collect()\n\n    # Bureau: Closed credits - using only numerical aggregations\n    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n    del closed, closed_agg, bureau\n    gc.collect()\n\n    return bureau_agg\n\n\n# Preprocess bureau.csv and bureau_balance.csv\ndef bureau_and_balance(num_rows=None, nan_as_category=True):\n    \"\"\"\n    Loads, merges datasets. Afterwards, feature_eng_application_train, rare_encoding, one_hot_encoder and aggregations\n    functions are called.\n    Returns dataframe with one-hot encoded, rare-encoded, feature engineering and aggregations implemented columns.\n\n    :param num_rows: int\n        int that shows number of rows to be loaded for the dataset\n\n    :param nan_as_category: bool\n        boolean that shows, if nan values will be created as separate columns or not.\n\n    :return: dataframe\n\n    \"\"\"\n    # Load the datasets\n    bureau = pd.read_csv('../input/home-credit-default-risk/bureau.csv', nrows=num_rows)\n    bb = pd.read_csv('../input/home-credit-default-risk/bureau_balance.csv', nrows=num_rows)\n    # Apply feature engineering steps for bureau_and_balance\n    bureau = feature_eng_bureau_and_balance(bureau)\n    # Implement rare encoding\n    bureau = rare_encoder(bureau, 0.01)\n    # Apply one hot encoding\n    bb, bb_cat = one_hot_encoder(bb, nan_as_category=nan_as_category)\n    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category=nan_as_category)\n    # Apply aggregation operations to the dataset\n    bureau_agg = aggregations_bureau_and_balance(bureau, bb, bureau_cat, bb_cat)\n\n    return bureau_agg\n\n\n# Feature Engineering steps for previous_applications.\ndef feature_eng_previous_applications(df):\n    \"\"\"\n    Feature Engineering steps for previous_applications.\n    Returns dataframe with FE steps implemented.\n\n    :param df: dataframe\n        dataframe(previous_applications) for FE-steps\n\n    :return:dataframe\n\n    \"\"\"\n    accompanied = ['Family', 'Spouse, partner', 'Children', 'Other_B', 'Other_A', 'Group of people']\n    df[\"NAME_TYPE_SUITE\"] = df[\"NAME_TYPE_SUITE\"].replace(accompanied, 'Accompanied')\n\n    # Otherization\n    name_others = ['Auto Accessories', 'Jewelry', 'Homewares', 'Medical Supplies', 'Vehicles', 'Sport and Leisure',\n                   'Gardening', 'Other', 'Office Appliances', 'Tourism', 'Medicine', 'Direct Sales', 'Fitness',\n                   'Additional Service', 'Education', 'Weapon', 'Insurance', 'House Construction', 'Animals']\n    df[\"NAME_GOODS_CATEGORY\"] = df[\"NAME_GOODS_CATEGORY\"].replace(name_others, 'others')\n\n    channel_others = ['AP+ (Cash loan)', 'Channel of corporate sales', 'Car dealer']\n    df[\"CHANNEL_TYPE\"] = df[\"CHANNEL_TYPE\"].replace(channel_others, 'Other_Channel')\n\n    seller_others = ['Auto technology', 'Jewelry', 'MLM partners', 'Tourism']\n    df[\"NAME_SELLER_INDUSTRY\"] = df[\"NAME_SELLER_INDUSTRY\"].replace(seller_others, 'Others')\n\n    loan_others = ['Refusal to name the goal', 'Money for a third person', 'Buying a garage',\n                   'Gasification / water supply',\n                   'Hobby', 'Business development', 'Buying a holiday home / land', 'Furniture', 'Car repairs',\n                   'Buying a home', 'Wedding / gift / holiday']\n    df[\"NAME_CASH_LOAN_PURPOSE\"] = df[\"NAME_CASH_LOAN_PURPOSE\"].replace(loan_others, 'Other_Loan')\n\n    df['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n    df['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n    df['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n    df['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n    df['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n\n    df['NEW_APP_CREDIT_RATE'] = df['AMT_APPLICATION'] / df['AMT_CREDIT']\n    df[\"NEW_APP_CREDIT_RATE_RATIO\"] = df[\"NEW_APP_CREDIT_RATE\"].apply(lambda x: 1 if (x <= 1) else 0)\n    df['NEW_AMT_PAYMENT_RATE'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n    df['NEW_APP_GOODS_RATE'] = df['AMT_APPLICATION'] / df['AMT_GOODS_PRICE']\n    df['NEW_CREDIT_GOODS_RATE'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n    df['NEW_RETURN_DAY'] = df['DAYS_DECISION'] + df['CNT_PAYMENT'] * 30\n    df['NEW_DAYS_TERMINATION_DIFF'] = df['DAYS_TERMINATION'] - df['NEW_RETURN_DAY']\n    df['NEW_DAYS_DUE_DIFF'] = df['DAYS_LAST_DUE_1ST_VERSION'] - df['DAYS_FIRST_DUE']\n    df[\"NEW_CNT_PAYMENT\"] = pd.cut(x=df['CNT_PAYMENT'], bins=[0, 12, 60, 120], labels=[\"Short\", \"Middle\", \"Long\"])\n    df[\"NEW_END_DIFF\"] = df[\"DAYS_TERMINATION\"] - df[\"DAYS_LAST_DUE\"]\n\n    weekend = [\"SATURDAY\", \"SUNDAY\"]\n    df[\"WEEKDAY_APPR_PROCESS_START\"] = df[\"WEEKDAY_APPR_PROCESS_START\"].apply(lambda x: \"WEEKEND\" if (x in weekend) else \"WEEKDAY\")\n\n    df['NFLAG_LAST_APPL_IN_DAY'] = df['NFLAG_LAST_APPL_IN_DAY'].astype(\"O\")\n    df['FLAG_LAST_APPL_PER_CONTRACT'] = df['FLAG_LAST_APPL_PER_CONTRACT'].astype(\"O\")\n    df[\"NEW_CNT_PAYMENT\"] = df['NEW_CNT_PAYMENT'].astype(\"O\")\n    df['NEW_APP_CREDIT_RATE_RATIO'] = df['NEW_APP_CREDIT_RATE_RATIO'].astype('O')\n    new_coding = {\"0\": \"Yes\", \"1\": \"No\"}\n    df['NEW_APP_CREDIT_RATE_RATIO'] = df['NEW_APP_CREDIT_RATE_RATIO'].replace(new_coding)\n\n    return df\n\n\n# Aggregation operations for previous_applications\ndef aggregations_previous_applications(df, cat_cols):\n    \"\"\"\n    Aggregation operations for previous_applications\n    Returns dataframe after completing specific aggregations for numerical and categorical variables for previous_applications.\n\n    :param df: dataframe\n        dataframe(bureau) for applying aggregations\n\n    :param cat_cols: list\n        list that holds categorical variables for bureau table\n\n    :return: dataframe\n\n    \"\"\"\n    # Aggregation for numeric features\n    num_aggregations = {\n        'SK_ID_PREV': 'count',\n        'AMT_ANNUITY': ['min', 'max', 'median', 'mean'],\n        'AMT_APPLICATION': ['min', 'max', 'mean', 'median'],\n        'AMT_CREDIT': ['min', 'max', 'mean', 'median'],\n        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean', 'median'],\n        'AMT_GOODS_PRICE': ['min', 'max', 'mean', 'median'],\n        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean', 'median'],\n        'DAYS_DECISION': ['min', 'max', 'mean', 'median'],\n        'NEW_APP_CREDIT_RATE': ['min', 'max', 'mean', 'var'],\n        'NEW_AMT_PAYMENT_RATE': ['min', 'max', 'mean'],\n        'NEW_APP_GOODS_RATE': ['min', 'max', 'mean'],\n        'NEW_CREDIT_GOODS_RATE': ['min', 'max', 'mean'],\n        'NEW_RETURN_DAY': ['min', 'max', 'mean', 'var'],\n        'NEW_DAYS_TERMINATION_DIFF': ['min', 'max', 'mean'],\n        'NEW_END_DIFF': ['min', 'max', 'mean'],\n        'NEW_APP_CREDIT_RATE_RATIO': ['min', 'max', 'mean'],\n        'NEW_DAYS_DUE_DIFF': ['min', 'max', 'mean']\n    }\n\n    # Aggregation for categorical features\n    cat_aggregations = {}\n    for cat in cat_cols:\n        cat_aggregations[cat] = ['mean']\n\n    prev_agg = df.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n\n    # Approved Applications - Aggregation for numeric features\n    approved = df[df['NAME_CONTRACT_STATUS_Approved'] == 1]\n    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n\n    # Refused Applications - Aggregation for numeric features\n    refused = df[df['NAME_CONTRACT_STATUS_Refused'] == 1]\n    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n\n    del refused, refused_agg, approved, approved_agg, df\n    gc.collect()\n\n    return prev_agg\n\n\n# Preprocess previous_applications.csv\ndef previous_applications(num_rows=None, nan_as_category=True):\n    \"\"\"\n    Loads previous_applications dataset. Afterwards, feature_eng_application_train, one_hot_encoder and aggregations\n    functions are called.\n    Returns dataframe with one-hot encoded, feature engineering and aggregations implemented columns.\n\n    :param num_rows: int\n        int that shows number of rows to be loaded for the dataset\n\n    :param nan_as_category: bool\n        boolean that shows, if nan values will be created as separate columns or not.\n\n    :return: dataframe\n\n    \"\"\"\n    df_prev = pd.read_csv('../input/home-credit-default-risk/previous_application.csv', nrows=num_rows)\n    # Implement feature engineering operations for df_prev\n    df_prev = feature_eng_previous_applications(df_prev)\n    # Apply one hot encoding\n    df_prev, cat_cols = one_hot_encoder(df_prev, nan_as_category=nan_as_category)\n    # Apply aggregation operations to the dataset\n    prev_agg = aggregations_previous_applications(df_prev, cat_cols)\n\n    return prev_agg\n\n\n# Preprocess POS_CASH_balance.csv\ndef pos_cash(num_rows=None, nan_as_category=True):\n    \"\"\"\n    Loads pos_cash dataset. Afterwards, one_hot_encoder and aggregations steps are implemented.\n    Returns dataframe with one-hot encoded and aggregations implemented columns.\n\n    :param num_rows: int\n        int that shows number of rows to be loaded for the dataset\n\n    :param nan_as_category: bool\n        boolean that shows, if nan values will be created as separate columns or not.\n\n    :return: dataframe\n\n    \"\"\"\n    pos = pd.read_csv('../input/home-credit-default-risk/POS_CASH_balance.csv', nrows=num_rows)\n    pos, cat_cols = one_hot_encoder(pos, nan_as_category=nan_as_category)\n    # Features\n    aggregations = {\n        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n        'SK_DPD': ['max', 'mean'],\n        'SK_DPD_DEF': ['max', 'mean']\n    }\n    for cat in cat_cols:\n        aggregations[cat] = ['mean']\n\n    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n    # Count pos cash accounts\n    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n    del pos\n    gc.collect()\n    return pos_agg\n\n\n# Preprocess installments_payments.csv\ndef installments_payments(num_rows=None, nan_as_category=True):\n    \"\"\"\n    Loads installments_payments dataset. Afterwards, one_hot_encoder, feature engineering and aggregations steps are implemented.\n    Returns dataframe with one-hot encoded, feature engineering and aggregations implemented columns.\n\n    :param num_rows: int\n        int that shows number of rows to be loaded for the dataset\n\n    :param nan_as_category: bool\n        boolean that shows, if nan values will be created as separate columns or not.\n\n    :return: dataframe\n    \"\"\"\n    ins = pd.read_csv('../input/home-credit-default-risk/installments_payments.csv', nrows=num_rows)\n    ins, cat_cols = one_hot_encoder(ins, nan_as_category=nan_as_category)\n    # Percentage and difference paid in each installment (amount paid and installment value)\n    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n    # Days past due and days before due (no negative values)\n    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n    # Features: Perform aggregations\n    aggregations = {\n        'NUM_INSTALMENT_VERSION': ['nunique'],\n        'DPD': ['max', 'mean', 'sum'],\n        'DBD': ['max', 'mean', 'sum'],\n        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n    }\n    for cat in cat_cols:\n        aggregations[cat] = ['mean']\n    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n    # Count installments accounts\n    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n    del ins\n    gc.collect()\n    return ins_agg\n\n\n# Preprocess credit_card_balance.csv\ndef credit_card_balance(num_rows=None, nan_as_category=True):\n    \"\"\"\n    Loads credit_card_balance dataset. Afterwards, one_hot_encoder, feature engineering and aggregations steps are implemented.\n    Returns dataframe with one-hot encoded, feature engineering and aggregations implemented columns.\n\n    :param num_rows: int\n        int that shows number of rows to be loaded for the dataset\n\n    :param nan_as_category: bool\n        boolean that shows, if nan values will be created as separate columns or not.\n\n    :return: dataframe\n    \"\"\"\n    cc = pd.read_csv('../input/home-credit-default-risk/credit_card_balance.csv', nrows=num_rows)\n    cc, cat_cols = one_hot_encoder(cc, nan_as_category=nan_as_category)\n    # General aggregations\n    cc.drop(['SK_ID_PREV'], axis=1, inplace=True)\n    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n    # Count credit card lines\n    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n    del cc\n    gc.collect()\n\n    return cc_agg\n\n\n# LightGBM GBDT with KFold or Stratified KFold\n# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\ndef kfold_lightgbm(df, num_folds, stratified=False, debug=False):\n    \"\"\"\n    LightGBM GBDT with KFold or Stratified KFold.\n    Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n    Separates train and test sets. Trains the model with tuned hyperparameters(found by Bayesian optimization) and\n    creates feature importance dataframe.\n\n    Returns a dataframe that shows hightest 40 feature importances.\n\n    :param df: dataframe\n        dataframe to be trained\n\n    :param num_folds: int\n        int that shows the number of splits for cross validation.\n\n    :param stratified: bool\n        boolean that indicates, if cross validation will be applied stratified or not.\n\n    :param debug: bool\n        boolean that indicates, if the model will be run debug mode or not.\n\n    :return: dataframe\n\n    \"\"\"\n    # Divide in training/validation and test data\n    train_df = df[df['TARGET'].notnull()]\n    test_df = df[df['TARGET'].isnull()]\n    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n    del df\n    gc.collect()\n\n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1001)\n    else:\n        folds = KFold(n_splits=num_folds, shuffle=True, random_state=1001)\n\n    # Create arrays and dataframes to store results\n    oof_preds = np.zeros(train_df.shape[0])\n    sub_preds = np.zeros(test_df.shape[0])\n    feature_importance_df = pd.DataFrame()\n    feats = [f for f in train_df.columns if f not in ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV', 'index']]\n\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n\n        # LightGBM parameters found by Bayesian optimization\n        clf = LGBMClassifier(\n            nthread=4,\n            n_estimators=10000,\n            learning_rate=0.02,\n            num_leaves=34,\n            colsample_bytree=0.9497036,\n            subsample=0.8715623,\n            max_depth=8,\n            reg_alpha=0.041545473,\n            reg_lambda=0.0735294,\n            min_split_gain=0.0222415,\n            min_child_weight=39.3259775,\n            silent=-1,\n            verbose=-1, )\n\n        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],\n                eval_metric='auc', verbose=200, early_stopping_rounds=200)\n\n        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = clf.feature_importances_\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n        del clf, train_x, train_y, valid_x, valid_y\n        gc.collect()\n\n    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n    # Write submission file and plot feature importance\n    if not debug:\n        test_df['TARGET'] = sub_preds\n        test_df[['SK_ID_CURR', 'TARGET']].to_csv(submission_file_name, index=False)\n    display_importances(feature_importance_df)\n    return feature_importance_df\n\n\n# Display/plot feature importance\ndef display_importances(feature_importance_df_):\n    \"\"\"\n    Displays/plots and saves feature importances.\n\n    :param feature_importance_df_: dataframe\n        dataframe for feature importances to be plotted and saved.\n\n    \"\"\"\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\",ascending=False)[:40].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances01.png')\n\n\ndef main(debug=True):\n    \"\"\"\n    Main function for Home Credit Default Risk Prediction.\n\n    :param debug: bool\n        boolean that indicates, if the model will be run debug mode or not.\n\n    \"\"\"\n    num_rows = 10000 if debug else None\n    df = application_train_test(num_rows)\n    with timer(\"Process bureau and bureau_balance\"):\n        bureau = bureau_and_balance(num_rows)\n        print(\"Bureau df shape:\", bureau.shape)\n        df = df.join(bureau, how='left', on='SK_ID_CURR')\n        del bureau\n        gc.collect()\n    with timer(\"Process previous_applications\"):\n        prev = previous_applications(num_rows)\n        print(\"Previous applications df shape:\", prev.shape)\n        df = df.join(prev, how='left', on='SK_ID_CURR')\n        del prev\n        gc.collect()\n    with timer(\"Process POS-CASH balance\"):\n        pos = pos_cash(num_rows)\n        print(\"Pos-cash balance df shape:\", pos.shape)\n        df = df.join(pos, how='left', on='SK_ID_CURR')\n        del pos\n        gc.collect()\n    with timer(\"Process installments payments\"):\n        ins = installments_payments(num_rows)\n        print(\"Installments payments df shape:\", ins.shape)\n        df = df.join(ins, how='left', on='SK_ID_CURR')\n        del ins\n        gc.collect()\n    with timer(\"Process credit card balance\"):\n        cc = credit_card_balance(num_rows)\n        print(\"Credit card balance df shape:\", cc.shape)\n        df = df.join(cc, how='left', on='SK_ID_CURR')\n        del cc\n        gc.collect()\n    with timer(\"Run LightGBM with kfold\"):\n        feat_importance = kfold_lightgbm(df, num_folds=10, stratified=False, debug=debug)\n\n\nif __name__ == \"__main__\":\n    submission_file_name = \"submission_kernel.csv\"\n    with timer(\"Full model run\"):\n        main()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}