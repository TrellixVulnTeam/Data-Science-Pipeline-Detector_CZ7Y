{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**[Problem 1] Cross Validation**","metadata":{"id":"yMPAdNGI5rge"}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-06-18T09:18:38.494621Z","iopub.execute_input":"2021-06-18T09:18:38.494956Z","iopub.status.idle":"2021-06-18T09:18:38.506942Z","shell.execute_reply.started":"2021-06-18T09:18:38.494927Z","shell.execute_reply":"2021-06-18T09:18:38.505739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import accuracy_score\n\n# loading the csv of the dataset\ndf = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\ndf = df.select_dtypes('number')\n\n# cleaning the dataset by filling the empy data(null)\n\ncleaned_df = df.fillna(0)\n\n# get only existing data with no missing values\ncleaned_df = cleaned_df[cleaned_df.columns[~cleaned_df.isnull().all()]]\n\n# separating them into variables\ny = cleaned_df['TARGET']\nX = cleaned_df.drop(['TARGET'], axis=1)\n\nX = X.to_numpy()\n\nkf = KFold(n_splits=2)\n\nfor train_index, test_index in kf.split(X):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]","metadata":{"id":"2XuVdlgI5mDU","outputId":"66f8a2cf-920d-4109-a2f0-0f50f51084c6","execution":{"iopub.status.busy":"2021-06-18T09:18:38.508368Z","iopub.execute_input":"2021-06-18T09:18:38.508801Z","iopub.status.idle":"2021-06-18T09:18:43.896249Z","shell.execute_reply.started":"2021-06-18T09:18:38.50877Z","shell.execute_reply":"2021-06-18T09:18:43.895492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train_trans = scaler.transform(X_train)\nX_test_trans = scaler.transform(X_test)","metadata":{"id":"LSBFNStODvgw","execution":{"iopub.status.busy":"2021-06-18T09:18:43.89728Z","iopub.execute_input":"2021-06-18T09:18:43.897693Z","iopub.status.idle":"2021-06-18T09:18:44.376254Z","shell.execute_reply.started":"2021-06-18T09:18:43.897663Z","shell.execute_reply":"2021-06-18T09:18:44.375279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**[Problem 2] Grid search**","metadata":{"id":"WanBR3CWD5xw"}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\n# checking which model and params are best\nmodel_params = {\n    'random_forest':{\n        'model': RandomForestClassifier(),\n        'params': {\n            'n_estimators': [1,5,10]\n        }\n    },\n    'logic_regression':{\n        'model': LogisticRegression(solver=\"liblinear\",multi_class=\"auto\"),\n        'params': {\n            'C': [1,5,10]\n        }\n    }\n}\n\n# defining an array to store the scores\nscores = []\n\nfor model_name,mp in model_params.items():\n    clf = GridSearchCV(mp['model'],mp['params'], return_train_score=False)\n    clf.fit(X_train_trans,y_train)\n    scores.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n\nbest_model_params = pd.DataFrame(scores,columns=['model','best_score','best_params'])\nbest_model_params","metadata":{"id":"gNj0i93VECDQ","outputId":"6c28020b-8d64-45d2-937d-4c85d369ad03","execution":{"iopub.status.busy":"2021-06-18T09:18:44.377353Z","iopub.execute_input":"2021-06-18T09:18:44.377714Z","iopub.status.idle":"2021-06-18T09:29:33.017141Z","shell.execute_reply.started":"2021-06-18T09:18:44.377681Z","shell.execute_reply":"2021-06-18T09:29:33.016308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**[Problem 3] Survey from Kaggle Notebooks**","metadata":{"id":"vYEhcFn8JWdL"}},{"cell_type":"markdown","source":"Model Hyperparameter Optimization is the  points of choice or configuration that allow a machine learning model to be customized for a specific task or dataset.\n\nParameters are different from hyperparameters. Parameters are learned automatically; hyperparameters are set manually to help guide the learning process.\n\nthe Gradient Boosting Algorithm for Machine Learning The origin of boosting from learning theory and AdaBoost.\nHow gradient boosting works including the loss function, weak learners and the additive model.\nHow to improve performance over the base algorithm with various regularization schemes.\nModal explainability \nEarly Stopping to Avoid Overtraining Neural Networks A compromise is to train on the training dataset but to stop training at the point when performance on a validation dataset starts to degrade. This simple, effective, and widely used approach to training neural networks is called early stopping.","metadata":{"id":"0Mkea_L0YxUE"}},{"cell_type":"markdown","source":"**[Problem 4] Creating a model with high generalization performance**","metadata":{"id":"cp89x6Faea-g"}},{"cell_type":"code","source":"import lightgbm as lgb\n\n# creating an instance of the model\nmodel = lgb.LGBMClassifier()\n\n# save the default params\ndefault_params = model.get_params()\n\n# number of folds\nN_FOLDS = 5\n\n# creating a dataset\ntrain_set = lgb.Dataset(data = X_train)\n\n# Cross validation results when avoid overfitting\ncv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, metrics = 'auc', nfold = N_FOLDS, seed = 42)\n\n# displaying the results\nprint('The maximum validation ROC AUC was: {:.5f}.'.format(cv_results['auc-mean'][-1]))\nprint('The optimal number of boosting rounds (estimators) was {}.'.format(len(cv_results['auc-mean'])))","metadata":{"id":"Oi1oPfs1JY5r","outputId":"f3dd4c3f-90d3-4fa5-ffbc-0e4a66248e8c","execution":{"iopub.status.busy":"2021-06-18T09:29:33.018916Z","iopub.execute_input":"2021-06-18T09:29:33.019393Z","iopub.status.idle":"2021-06-18T09:29:38.45968Z","shell.execute_reply.started":"2021-06-18T09:29:33.019341Z","shell.execute_reply":"2021-06-18T09:29:38.458757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"imported the whole dataset\ncreated a subset of only numbers\nsplit the data using kfold\nused gridsearchCV to find the best model and params to fine tune my classiffiers","metadata":{"id":"nfwv39UifE6w"}},{"cell_type":"markdown","source":"**[Problem 5] Final model selection**","metadata":{"id":"kFDwyBnKfhM-"}},{"cell_type":"code","source":"# loading the csv of the test dataset\ntest_df = pd.read_csv('../input/home-credit-default-risk/application_test.csv')\n\n# cleaning the dataset by removing the empy data(null)\ntest_cleaned_df = test_df.fillna(0)\n\n# separating them into variables\ntest_X = test_cleaned_df.select_dtypes('number')\n\n# standardizing the data\ntest_scaler = StandardScaler()\ntest_X_test_trans = scaler.fit_transform(test_X)\n\n# predicting\ntest_reg_pred = clf.predict(test_X_test_trans)\n\nkgl_submission = pd.concat([test_df['SK_ID_CURR'], pd.Series(test_reg_pred, name='TARGET')], axis=1)\nkgl_submission.to_csv(' ', index=False) ","metadata":{"id":"sQHboWkhflVt","outputId":"1a59edf6-e150-4da4-a252-09245de29341","execution":{"iopub.status.busy":"2021-06-18T09:29:38.461225Z","iopub.execute_input":"2021-06-18T09:29:38.461923Z","iopub.status.idle":"2021-06-18T09:29:39.547856Z","shell.execute_reply.started":"2021-06-18T09:29:38.46187Z","shell.execute_reply":"2021-06-18T09:29:39.546989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kgl_submission.to_csv(\"kaggle_submission.csv\", index=False, header =1)","metadata":{"id":"85Ssp1zDD8Y6","execution":{"iopub.status.busy":"2021-06-18T09:29:39.549234Z","iopub.execute_input":"2021-06-18T09:29:39.549876Z","iopub.status.idle":"2021-06-18T09:29:39.648031Z","shell.execute_reply.started":"2021-06-18T09:29:39.549828Z","shell.execute_reply":"2021-06-18T09:29:39.647116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nkgl_submission.to_csv(os.path.join('','chervis.csv'), index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T09:33:03.803908Z","iopub.execute_input":"2021-06-18T09:33:03.804252Z","iopub.status.idle":"2021-06-18T09:33:03.898592Z","shell.execute_reply.started":"2021-06-18T09:33:03.804222Z","shell.execute_reply":"2021-06-18T09:33:03.897762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}