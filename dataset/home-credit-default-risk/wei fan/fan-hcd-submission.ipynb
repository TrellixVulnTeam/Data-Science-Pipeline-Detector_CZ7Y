{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"# numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd \n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read in Data \n","metadata":{"_uuid":"ded520f73b9e94ed47ac2e994a5fb1bcb9093d0f","_cell_guid":"a5e67831-4751-4f11-8e07-527e3e092671"}},{"cell_type":"code","source":"# List files available\nprint(os.listdir(\"../input/\"))","metadata":{"_uuid":"c54e1559611512ebd447ac24f2226c2fffd61dcd","_cell_guid":"2cdca894-e637-43a9-8f80-5791c2bb9041","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training data\napp_train = pd.read_csv('../input/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training data has 307511 observations (each one a separate loan) and 122 features (variables) including the `TARGET` (the label we want to predict).","metadata":{"_uuid":"4695541966d3d29e8a7a8975b072d01caff1631d"}},{"cell_type":"code","source":"# Testing data features\napp_test = pd.read_csv('../input/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","metadata":{"_uuid":"cbd1c4111df6f07bc0d479b51f50895e728b717a","_cell_guid":"d077aee0-5271-440e-bc07-6087eab40b74","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The test set is considerably smaller and lacks a `TARGET` column. ","metadata":{"_uuid":"e351f02c8a5886756507a2d4f1ddba4791220f12"}},{"cell_type":"markdown","source":"## Examine the Distribution of the Target Column\n\nThe target is what we are asked to predict: either a 0 for the loan was repaid on time, or a 1 indicating the client had payment difficulties. We can first examine the number of loans falling into each category.","metadata":{"_uuid":"7c006a09627df1333c557dc11a09f372bde34dda","_cell_guid":"23b20e53-3484-4c4b-bec9-2d8ac2ac918d"}},{"cell_type":"code","source":"app_train['TARGET'].value_counts()","metadata":{"_uuid":"2163ca09678b53dbe88388ccbc7d0e0f7d6c6230","_cell_guid":"5fb6ab16-1b38-4ecf-8123-e48c7c061773","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train['TARGET'].astype(int).plot.hist();","metadata":{"_uuid":"1b2611fb3cf392023c3f40fd2f7b96f56f5dee7d","_cell_guid":"0e93c1e2-f6b8-4a0b-82b6-7dad8df56048","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this plot, we can see it is an imbalanced class problem.","metadata":{"_uuid":"119106000875202a0030109f14b73245fc4285e1","_cell_guid":"48f008ff-d81e-46b2-80a3-e58f2a6627ca"}},{"cell_type":"markdown","source":"## Examine Missing Values\n\nNext we can look at the number and percentage of missing values in each column. ","metadata":{"_uuid":"58851dfef481f32b3026e89b086534ea3683440d","_cell_guid":"507ec6b1-99d0-4324-a3ed-bdea2f916227"}},{"cell_type":"code","source":"total = app_train.isnull().sum().sort_values(ascending = False)\npercent = (app_train.isnull().sum()/app_train.isnull().count()*100).sort_values(ascending = False)\nmiss_train = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\nmiss_train.head(10)","metadata":{"_uuid":"7a2f5c72c45fa04d9fa95e8051ae595be806e9a2","_cell_guid":"fc4c675f-e4a1-4e4f-9ece-3c59e5c8f7fd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now look at the number of unique entries in each of the `object` (categorical) columns.","metadata":{"_uuid":"5859303c9acc63f7ff7acce063a9cd022a6d38cd"}},{"cell_type":"code","source":"# Number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","metadata":{"_uuid":"2d021eda10939a19b141292d34491b357acd201a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Label Encoding and One-Hot Encoding\n\nFor any categorical variable (`dtype == object`) with 2 unique categories, use label encoding, and for any categorical variable with more than 2 unique categories, use one-hot encoding. \n\nFor label encoding, we use the Scikit-Learn `LabelEncoder` and for one-hot encoding, the pandas `get_dummies(df)` function.","metadata":{"_uuid":"46f5bf9a6de52e270aa911ffd895e704da5426ec","_cell_guid":"95627792-157e-457a-88a8-3b3875c7e1d5"}},{"cell_type":"code","source":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(app_train[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","metadata":{"_uuid":"ddfaae5c3dcc7ec6bb47a2dffc10d364e8d25355","_cell_guid":"70641d4d-1075-4837-8972-e58d70d8f242","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","metadata":{"_uuid":"6796c6dc793a08e162b6e20c6f185ef37bdf51f3","_cell_guid":"0851773b-39fd-4cf0-9a66-e30adeef3e57","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Aligning Training and Testing Data\n","metadata":{"_uuid":"1b2c4198638ec8e5155097d112249de8754eb5c0","_cell_guid":"61d910b5-84f5-4655-bd8a-d29672c13741"}},{"cell_type":"code","source":"train_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n\n# Add the target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","metadata":{"_uuid":"e0d12a13cb95521c19b10d8829e8abe2b1118396","_cell_guid":"d99ca215-e893-490c-a6a4-83f3e8a067b3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Find outlier for column\"DAYS_BIRTH\" and \"DAYS_EMPLOYED\"\n\n\n","metadata":{"_uuid":"4d7c8dd1d5bb5a0ef84cb78e6bff927249e62145","_cell_guid":"13918211-0e6b-4d72-955b-f997db19eea2"}},{"cell_type":"code","source":"(app_train['DAYS_BIRTH'] / -365).describe()","metadata":{"_uuid":"a60be93c2d7d63855e6d65c1109f408ad85da134","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].describe()","metadata":{"_uuid":"600c59dd5d970d3ccfea3a6af0036d85958adc91","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","metadata":{"_uuid":"2878bfb3a2be4554f33e03e1a04d4c1978b52a08","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check the anomalous clients to see they have higher or low rates of default than the rest of the clients.","metadata":{"_uuid":"d28ca1e799c0a6113cc5e920297e1dc93d380af4"}},{"cell_type":"code","source":"anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","metadata":{"_uuid":"67ea87d9ef6974b1780a7db1eefd13f90f81b5be","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fill in the anomalous values with not a number (`np.nan`) in training and test dataset, then create a new boolean column indicating whether or not the value was anomalous.\n\n","metadata":{"_uuid":"1edfcf786aadb004f083e9896989a29e43bf80da"}},{"cell_type":"code","source":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\n# Check values distribution after repalcementin in train dataset\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","metadata":{"_uuid":"e23ec3cb89428f3dd994b572f718cc729740cfab","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))","metadata":{"_uuid":"a0d7c77b2adecaa878f39cf86ffddcfbbe51a190","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Find correlations\n\n\n","metadata":{"_uuid":"fd656b392faad3b34ecfa448b55ad03e75449e0a"}},{"cell_type":"code","source":"# Find correlations with the target and sort\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","metadata":{"_uuid":"d39d15d64db1f2c9015c6f542911ef9a9cac119e","_cell_guid":"02acdb8d-d95f-41b9-8ad1-e2b6cb26f398","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`DAYS_BIRTH` (the age in days of the client at the time of the loan in negative days) is the most positive correlation, meaning that as the client gets older, they are less likely to default on their loan (ie the target == 0). ","metadata":{"_uuid":"67e1f0f22ec8e26c38827c24ca1e9409d73c9c64","_cell_guid":"8cfa409c-ec74-4fa4-8093-e7d00596c9c5"}},{"cell_type":"markdown","source":"### Effect of Age on Repayment","metadata":{"_uuid":"c1b831b6d1c3221efb123fbc1a4882aa1f598ec0","_cell_guid":"0f7b1cfb-9e5c-4720-9618-ad326940f3f3"}},{"cell_type":"code","source":"# Find the correlation of the positive days since birth and target\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","metadata":{"_uuid":"f705c7aa49486ec3bf119c4edc4e4af58861b88d","_cell_guid":"b0ab583c-dfbb-4ff7-80e5-d747fc408499","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Age information into a separate dataframe\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)","metadata":{"_uuid":"6c50572f095bff250bfed1993e2c53118277b5dd","_cell_guid":"4296e926-7245-40df-bb0a-f6e59d8e566a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group by the bin and calculate averages\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups","metadata":{"_uuid":"7082483e5fd9114856926de28968e5ae0b478b36","_cell_guid":"18873d6b-3877-4c77-830e-0f3e10e5e7fb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","metadata":{"_uuid":"823b5032f472b05ce079ae5a7680389f31ddd8b7","_cell_guid":"004d1021-d73f-4356-9ef8-0464c95d1708","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the client gets older, there is a negative linear relationship with the target meaning that as clients get older, they tend to repay their loans on time more often. Although this is not a significant correlation (-0.07 correlation coefficient), this variable is likely going to be useful in a machine learning model because it does affect the target. ","metadata":{"_uuid":"eb2bd6392ed6d6f7e002bc8dbea6aab0f30487d9","_cell_guid":"2dad060f-bcab-4fe3-aa19-29fbf3e6fdab"}},{"cell_type":"markdown","source":"## Domain Knowledge Features\n\n\n","metadata":{"_uuid":"9b27fad1522263c32b57a8127c84ad0e08ff9d8f"}},{"cell_type":"code","source":"app_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']","metadata":{"_uuid":"c8d4b165b45da6c3120911de18e9348d8726c70c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']","metadata":{"_uuid":"d017103871bd4935a8c29599d6be33e0e74b2f83","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_domain.shape)\nprint('Testing data with polynomial features shape:  ', app_test_domain.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read in bureau and Bureau Balance","metadata":{}},{"cell_type":"code","source":"bureau = pd.read_csv('../input/bureau.csv')\nbureau.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bureau_balance = pd.read_csv('../input/bureau_balance.csv')\nbureau_balance.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function for Numeric Aggregations","metadata":{}},{"cell_type":"code","source":"def agg_numeric(df, group_var, df_name):\n\n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != group_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    group_ids = df[group_var]\n    numeric_df = df.select_dtypes('number')\n    numeric_df[group_var] = group_ids\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n\n    # Need to create new column names\n    columns = [group_var]\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        # Skip the grouping variable\n        if var != group_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1][:-1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n\n    agg.columns = columns\n    return agg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function to Handle Categorical Variables","metadata":{}},{"cell_type":"code","source":"def count_categorical(df, group_var, df_name):\n \n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('object'))\n\n    # Make sure to put the identifying id on the column\n    categorical[group_var] = df[group_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['count', 'count_norm']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    return categorical","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate value count statistics for each `SK_ID_CURR` in bureau\nbureau_agg = agg_numeric(bureau.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_agg.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Counts of bureau\nbureau_counts = count_categorical(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_counts.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate value count statistics for each `SK_ID_CURR` in butrau_balance\nbureau_balance_agg = agg_numeric(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_agg.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Counts of each type of status for each previous loan\nbureau_balance_counts = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_counts.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Aggregated Stats of Bureau Balance by loan","metadata":{}},{"cell_type":"code","source":"# Dataframe grouped by the loan\nbureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n\n# Merge to include the SK_ID_CURR\nbureau_by_loan = bureau_by_loan.merge(bureau[['SK_ID_BUREAU', 'SK_ID_CURR']], on = 'SK_ID_BUREAU', how = 'left')\n\nbureau_by_loan.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Aggregated Stats of Bureau Balance by Client","metadata":{}},{"cell_type":"code","source":"bureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n\n# Merge to include the SK_ID_CURR\nbureau_by_loan = bureau[['SK_ID_BUREAU', 'SK_ID_CURR']].merge(bureau_by_loan, on = 'SK_ID_BUREAU', how = 'left')\n\n# Aggregate the stats for each client\nbureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insert Computed Features into app_train_domain Data","metadata":{}},{"cell_type":"code","source":"# Merge with the value counts of bureau\ntrain_domain = app_train_domain.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the stats of bureau\ntrain_domain = train_domain.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the monthly information grouped by client\ntrain_domain = train_domain.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_features = list(train_domain.columns)\nprint('Number of features using previous loans from other institutions data with domain features: ', len(new_features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculate Information for test_domain Data","metadata":{}},{"cell_type":"code","source":"# Merge with the value counts of bureau\ntest_domain = app_test_domain.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the stats of bureau\ntest_domain = test_domain.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the value counts of bureau balance\ntest_domain = test_domain.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Shape of test_domain Data: ', test_domain.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Align the testing and training dataframes","metadata":{}},{"cell_type":"code","source":"train_labels = train_domain['TARGET']\n\n# Align the dataframes, this will remove the 'TARGET' column\ntrain_domain, test_domain = train_domain.align(test_domain, join = 'inner', axis = 1)\n\ntrain_domain['TARGET'] = train_labels\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train = train_domain\ntrain = app_train\napp_test = test_domain\ntest = app_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Free up memory by deleting old objects\nimport gc\ngc.enable()\ndel train_domain, bureau_balance, bureau_agg,  bureau_balance_agg, bureau_balance_counts, bureau_by_loan, bureau_balance_by_client, bureau_counts\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression Implementation¶","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, Imputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns = ['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions\nlog_reg_pred = log_reg.predict_proba(test)[:, 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the submission to a csv file\nsubmit.to_csv('logistic_regression.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### This model scores 0.6837.","metadata":{}},{"cell_type":"markdown","source":"##  Random Forest with Domain Features\n","metadata":{"_uuid":"92687ac866441f6ee2919aa5e5c935490c172afc","_cell_guid":"462ea34f-3f66-490a-a61f-24a991271f69"}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\napp_train_domain = app_train_domain.drop(columns = 'TARGET')\n\ndomain_features_names = list(app_train_domain.columns)\n\n# Impute the domainnomial features\nimputer = Imputer(strategy = 'median')\n\ndomain_features = imputer.fit_transform(app_train_domain)\ndomain_features_test = imputer.transform(app_test_domain)\n\n# Scale the domainnomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\ndomain_features = scaler.fit_transform(domain_features)\ndomain_features_test = scaler.transform(domain_features_test)\n\nrandom_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n# Train on the training data\nrandom_forest_domain.fit(domain_features, train_labels)\n\n# Extract feature importances\nfeature_importance_values_domain = random_forest_domain.feature_importances_\nfeature_importances_domain = pd.DataFrame({'feature': domain_features_names, 'importance': feature_importance_values_domain})\n\n# Make predictions on the test data\npredictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]","metadata":{"_uuid":"cf05e2318904b8f3575ae233c185cd995fd07643","_cell_guid":"6643479e-7980-431c-a6a2-9087acdb0f42","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest.csv', index = False)","metadata":{"_uuid":"1da4b02502388d2b8a2bc5376027c5bef50272f3","_cell_guid":"25145966-669e-426d-89a3-98e30b861057","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### This model scores 0.6781.","metadata":{}},{"cell_type":"markdown","source":"### Light Gradient Boosting Machine","metadata":{"_uuid":"a8bc307f9be27bfabbc3891deddbd94293ca03fa","_cell_guid":"d12452cd-347e-4269-b3d4-f5f0589f4c5c"}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport gc\n\ndef model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","metadata":{"_uuid":"2719663ed461422fce26b5dd55a31ab9718df47a","_cell_guid":"60208a3f-947f-42d9-8f46-2159afd2eb7d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission, fi, metrics = model(app_train, app_test)\nprint('Light GBM metrics')\nprint(metrics)","metadata":{"_uuid":"89e02dcbb23e47e3504ed1f61431b182e2011ba5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('LightBMG.csv', index = False)","metadata":{"_uuid":"d71f9d7b9b322824704eec9dc82e38a480d4f76c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This submission scores 0.7628. ","metadata":{"_uuid":"2aca0b9ea31dfef1ca3221dc6424fe31e829cbbf"}}]}