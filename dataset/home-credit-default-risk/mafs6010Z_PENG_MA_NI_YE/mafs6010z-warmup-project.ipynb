{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport lightgbm as lgb\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\ndf_train = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\ndf_test = pd.read_csv('../input/home-credit-default-risk/application_test.csv')\n#when we exploring or cleaning the data we need to do it on both trian and test case\n#drop all features that have >60% missing data\ndf_train=df_train.dropna(thresh=len(df_train)*0.4, axis=1)\n#create a new feature states the completeness of application file for each cilent\ndf_train['incomplete'] = 1\ndf_train.loc[df_train.isnull().sum(axis=1) < 40, 'incomplete'] = 0\ndf_test['incomplete'] = 1\ndf_test.loc[df_test.isnull().sum(axis=1) < 40, 'incomplete'] = 0\n#deal with unknown data in gender feature\nprint(list(df_train['CODE_GENDER'].unique()))\ndf_train['CODE_GENDER'].replace({'XNA': np.nan}, inplace = True)\ndf_test['CODE_GENDER'].replace({'XNA': np.nan}, inplace = True)\n#tramsform daily date to years, and replace abnormal employ date with NaN，and label a new feature state abnormal issue\ndf_train['YEARS_EMPLOYED'] = df_train['DAYS_EMPLOYED'].apply(lambda x: int(x/-365))\ndf_test['YEARS_EMPLOYED'] = df_test['DAYS_EMPLOYED'].apply(lambda x: int(x/-365))\ndf_train['YEARS_EMPLOYED'].replace({-1000: np.nan}, inplace = True)\ndf_test['YEARS_EMPLOYED'].replace({-1000: np.nan}, inplace = True)\n\ndf_train['DAYS_EMPLOYED_ANOM'] = df_train[\"DAYS_EMPLOYED\"] == 365243\ndf_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\ndf_test['DAYS_EMPLOYED_ANOM'] = df_test[\"DAYS_EMPLOYED\"] == 365243\ndf_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n#drop daily date we dont need\ndf_train = df_train.drop(['DAYS_EMPLOYED'], axis=1)\ndf_test = df_test.drop(['DAYS_EMPLOYED'], axis=1)\n#transform ojective type feature to numerical feature by using labelencoder in sklearn package\n#objective type feature encoding，and store/count those objective feature\nfrom sklearn import preprocessing\nlabel = preprocessing.LabelEncoder()\nlabel_count = 0\nlabel_list = []\n#check out objective feature that cant use pd.get_dummies\nfor col in df_test:\n    if df_test[col].dtype == 'object' or df_test[col].dtype == 'bool':\n        try:\n            label.fit(df_train[col])\n            # Transform both training and testing data\n            df_train[col] = label.transform(df_train[col])\n            df_test[col] = label.transform(df_test[col])\n        except:#if we cant encode the objective, we need to drop it\n            print('error exist')# print the error messages out\n            if col in df_train.columns.values.tolist():\n                df_train = df_train.drop([col], axis=1)\n            if col in df_test.columns.values.tolist():\n                df_test = df_test.drop([col], axis=1)\n        else:\n            # counting number and storing features\n            label_count += 1\n            label_list.append(col)\n        \n#print the num of features we transform\nprint('%d columns were label encoded.' % label_count)   \n\n#create some self-design features that is meaningful\n#term : credit/annuity\ndf_train['TERM'] = df_train['AMT_CREDIT'] / df_train['AMT_ANNUITY']\ndf_test['TERM'] = df_test['AMT_CREDIT'] / df_test['AMT_ANNUITY']\n#percentage of credit by income\ndf_train['CREDIT_INCOME_PERCENT'] = df_train['AMT_CREDIT'] / df_train['AMT_INCOME_TOTAL']\n#percentage of annuity by income\ndf_train['ANNUITY_INCOME_PERCENT'] = df_train['AMT_ANNUITY'] / df_train['AMT_INCOME_TOTAL']\n\n#transform days_birth/days_employed to years type and take its lower boundary\ndf_train['YEARS_BIRTH'] = df_train['DAYS_BIRTH'].apply(lambda x: int(x/-365))\ndf_test['YEARS_BIRTH'] = df_test['DAYS_BIRTH'].apply(lambda x: int(x/-365))\ndf_train = df_train.drop(['DAYS_BIRTH'], axis=1)\ndf_test = df_test.drop(['DAYS_BIRTH'], axis=1)\n\n#percentage of employed day by age\ndf_train['YEARS_EMPLOYED_PERCENT'] = df_train['YEARS_EMPLOYED'] / df_train['YEARS_BIRTH']\n\n\n#input data and rename the colname into legal type\nimport re\ndf_train = df_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\ndf_test = df_test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n#checking missing data and print it out\ndef find_missing(data):\n    # number of missing values\n    count_missing = data.isnull().sum().values\n    # total records\n    total = data.shape[0]\n    # percentage of missing\n    ratio_missing = count_missing/total\n    # return a dataframe to show: feature name, # of missing and % of missing\n    return pd.DataFrame(data={'missing_count':count_missing, 'missing_ratio':ratio_missing}, index=data.columns.values)\n#sort the missing_percent descending\nmissing_percent=find_missing(df_train).sort_values(by=['missing_ratio'],ascending=False)\nprint(missing_percent.head(12))\n#impute missing part by fillna method with strategy median\ndef refill(data):\n    for col in data.columns.values.tolist():\n        data[col] = data[col].fillna(data[col].median())\n    return data\ndf_train = refill(df_train)\ndf_test = refill(df_test)\n#set up data for modeling,and drop prediction col and index col\nX = df_train.drop(['SK_ID_CURR', 'TARGET'], axis=1)\ny = df_train.TARGET\npredition_X = df_test.drop(['SK_ID_CURR'], axis=1)\n#using 5-folds cv to improve the modeling\n#using package to randomly split data into 5 piece\nfolds = StratifiedKFold(n_splits=5,shuffle=True,random_state=6)\npreds = np.zeros(X.shape[0])\npredictions = np.zeros(predition_X.shape[0])\n\n#perform the cv\nvalid_score = 0\nfor n_fold, (train_idx, eval_idx) in enumerate(folds.split(X, y)):\n    train_x, train_y = X.iloc[train_idx], y[train_idx]\n    eval_x, eval_y = X.iloc[eval_idx], y[eval_idx]    \n    \n    train_data = lgb.Dataset(data=train_x, label=train_y,categorical_feature=label_list)\n    eval_data = lgb.Dataset(data=eval_x, label=eval_y)\n    \n    params = {'application':'binary','num_iterations':4000, 'learning_rate':0.05, 'num_leaves':24, \n             'feature_fraction':0.8, 'bagging_fraction':0.9,\n             'lambda_l1':0.1, 'lambda_l2':0.1, 'min_split_gain':0.01, 'min_data_in_leaf':20,\n             'early_stopping_round':100, 'max_depth':7, \n             'min_child_weight':40, 'metric':'auc','verbose' : -1}\n    #train the model\n    lgb_es_model = lgb.train(params, train_data, valid_sets=[train_data, eval_data], categorical_feature=label_list,verbose_eval=100) \n    #pick the best one\n    preds[eval_idx] = lgb_es_model.predict(eval_x, num_iteration=lgb_es_model.best_iteration)\n    #store the best prediction result \n    predictions += lgb_es_model.predict(predition_X, num_iteration=lgb_es_model.best_iteration,predict_disable_shape_check=True) / folds.n_splits\n    #print out aue score to do the model assessment\n    print('AUC of Fold %2d  : %.3f' % (n_fold + 1, roc_auc_score(eval_y, preds[eval_idx])))\n    valid_score += roc_auc_score(eval_y, preds[eval_idx])\n\n#calculate out the mean of cv score\nprint('valid score:', str(round(valid_score/folds.n_splits,4)))\n#plot the feature importance graph to see which features is most important\nlgb.plot_importance(lgb_es_model, height=0.5, max_num_features=20, ignore_zero = False, figsize = (12,6), importance_type ='gain')\n\n#output the predictions as csv file\noutput = pd.DataFrame({'SK_ID_CURR':df_test.SK_ID_CURR, 'TARGET': predictions})\noutput.to_csv('./submission.csv', index=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-18T06:42:16.971774Z","iopub.execute_input":"2021-09-18T06:42:16.972124Z","iopub.status.idle":"2021-09-18T06:45:39.754466Z","shell.execute_reply.started":"2021-09-18T06:42:16.972037Z","shell.execute_reply":"2021-09-18T06:45:39.753503Z"},"trusted":true},"execution_count":null,"outputs":[]}]}