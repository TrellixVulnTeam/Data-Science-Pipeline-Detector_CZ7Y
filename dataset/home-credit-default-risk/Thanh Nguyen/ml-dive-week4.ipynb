{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/home-credit-default-risk/application_train.csv\", index_col='SK_ID_CURR')\ndf = data.drop(\"TARGET\", axis=1)\ndf.head()","metadata":{"id":"RviOdiUECYI2","outputId":"145ebcf0-0407-4892-d3a3-251f6eac9ff7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX, y = data.drop('TARGET', axis=1), data['TARGET']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=1)\nprint(X_train.shape, X_val.shape, y_train.shape, y_val.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check missing value ratios\npd.set_option('display.max_row', 122)\npd.set_option('display.max_column', 122)\n\nmissing_ratio = (df.isna().sum() / df.shape[0] * 100).sort_values(ascending=False)\nmissing_df = missing_ratio.to_frame(name=\"missing_ratio\")\nmissing_df[\"dtype\"] = [df[col].dtype for col in missing_df.index]\nprint(missing_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discrete_cols = df.select_dtypes(include='int').columns.tolist()\n\n# Bring high cordinality columns to numerical pipeline\nnumerical_cols = df.select_dtypes(include='float').columns.tolist() \\\n                    + [col for col in discrete_cols if df[col].nunique() >= 30]\nlow_card_discrete_cols = [col for col in discrete_cols if df[col].nunique() < 30]\ncategorical_cols = df.select_dtypes(include='object').columns.tolist()","metadata":{"id":"EU2H45nI4rXq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_cols(df, cols):\n    discrete_cols = df[cols].select_dtypes(include='int').columns.tolist()\n\n    # Bring high cordinality columns to numerical pipeline\n    numerical_cols = df[cols].select_dtypes(include='float').columns.tolist() \\\n                        + [col for col in discrete_cols if df[col].nunique() >= 30]\n    low_card_discrete_cols = [col for col in discrete_cols if df[col].nunique() < 30]\n    categorical_cols = df[cols].select_dtypes(include='object').columns.tolist()\n    return numerical_cols, low_card_discrete_cols, categorical_cols\n\n# missing_df is sorted by missing_ratio\ncol_groups = []\ngroup = []\nfor col in missing_df.index:\n    if len(col_groups) == 0 or missing_df.loc[col, \"missing_ratio\"] <= 0.0001:\n        col_groups.append([col])\n        continue\n    diff = abs(missing_df.loc[col_groups[-1][-1], \"missing_ratio\"] - missing_df.loc[col, \"missing_ratio\"])\n    if diff <= 0.00001:\n        col_groups[-1].append(col)\n    else:\n        col_groups.append([col])\n\ncompress_able_cols = [group for group in col_groups if len(group) == 3]\ncompress_cols = [group[0] for group in compress_able_cols]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flatten_list = [col for cols in compress_able_cols for col in cols]\nselected_cols = [col for col in df.columns if col not in flatten_list] + compress_cols\n\nnum_cols, low_cols, cat_cols = split_cols(df, selected_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import roc_auc_score, plot_roc_curve\n\n# Column preprocessing\nnumerical_pipeline = Pipeline([('numerical_imputer', SimpleImputer(strategy=\"median\")),\n                               ('numerical_scaler', StandardScaler()),\n                               ('numerical_selector', SelectKBest(k=10))])\n\nlow_card_discrete_pipeline = Pipeline([('low_card_imputer', SimpleImputer(strategy=\"most_frequent\")),\n                                       ('low_card_selector', SelectKBest(chi2, k=10))])\n\ncategorical_pipeline = Pipeline([('categorical_imputer', SimpleImputer(strategy=\"most_frequent\")),\n                                 ('categorical_encoder', OneHotEncoder()),\n                                 ('categorical_selector', SelectKBest(chi2, k=10))])\n\npreprocessor = make_column_transformer((numerical_pipeline, num_cols),\n                                       (low_card_discrete_pipeline, low_cols),\n                                       (categorical_pipeline, cat_cols))\n\n# Model pipeline\nmodel = Pipeline([('preprocessor', preprocessor),\n                  ('classifier', SGDClassifier(loss='log', random_state=0))])\n\nmodel.fit(X_train, y_train)\ny_score = model.predict_proba(X_val)\nscore = roc_auc_score(y_val, y_score[:,1])\nprint(\"ROC score:\", score)\nplot_roc_curve(model, X_val, y_val, drop_intermediate=False, name=\"Baseline\")\nplt.plot([0, 1], [0, 1], linestyle='--', color='r')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv(\"../input/home-credit-default-risk/application_test.csv\", index_col='SK_ID_CURR')\n\npreds_test = model.predict(df_test)\noutput_df = pd.DataFrame({\"SK_ID_CURR\": df_test.index,\n                          \"TARGET\": preds_test})\noutput_df.to_csv(\"thanh_dive_submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}