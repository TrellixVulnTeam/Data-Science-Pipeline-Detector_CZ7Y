{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Data manipulation\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n# Modeling\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\n\n# Data Viz\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Ignore warnings\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-29T23:04:39.050699Z","iopub.execute_input":"2022-05-29T23:04:39.051024Z","iopub.status.idle":"2022-05-29T23:04:39.072331Z","shell.execute_reply.started":"2022-05-29T23:04:39.050987Z","shell.execute_reply":"2022-05-29T23:04:39.071251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### I. Import Data","metadata":{}},{"cell_type":"code","source":"#import train and test data\ntrain = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\ntest = pd.read_csv('../input/home-credit-default-risk/application_test.csv')\n\n#import other tables\npos = pd.read_csv('../input/home-credit-default-risk/POS_CASH_balance.csv')\ninstallment = pd.read_csv('../input/home-credit-default-risk/installments_payments.csv')\ncredit = pd.read_csv('../input/home-credit-default-risk/credit_card_balance.csv')\np_app = pd.read_csv('../input/home-credit-default-risk/previous_application.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T23:08:10.311426Z","iopub.execute_input":"2022-05-29T23:08:10.31183Z","iopub.status.idle":"2022-05-29T23:09:00.020864Z","shell.execute_reply.started":"2022-05-29T23:08:10.311791Z","shell.execute_reply":"2022-05-29T23:09:00.019127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### II. Data Cleaning","metadata":{}},{"cell_type":"code","source":"#identifying the data types\ntrain.dtypes.unique()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-29T23:05:25.389594Z","iopub.status.idle":"2022-05-29T23:05:25.39002Z","shell.execute_reply.started":"2022-05-29T23:05:25.389777Z","shell.execute_reply":"2022-05-29T23:05:25.389799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataframe of only categorical types\ncat_train = train.select_dtypes(['object'])\ncat_test = test.select_dtypes(['object'])\n\n#dataframe of numerical types\nnum_train = train.select_dtypes(['int64','float64'])\nnum_test = test.select_dtypes(['int64','float64'])","metadata":{"execution":{"iopub.status.busy":"2022-05-29T23:05:25.391864Z","iopub.status.idle":"2022-05-29T23:05:25.392711Z","shell.execute_reply.started":"2022-05-29T23:05:25.39241Z","shell.execute_reply":"2022-05-29T23:05:25.392444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sanity Check: Numerical Values\nnum_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T23:05:25.393857Z","iopub.status.idle":"2022-05-29T23:05:25.394813Z","shell.execute_reply.started":"2022-05-29T23:05:25.394527Z","shell.execute_reply":"2022-05-29T23:05:25.394559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sanity Check: Categorical Data Values\ncat_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T23:05:25.396705Z","iopub.status.idle":"2022-05-29T23:05:25.397403Z","shell.execute_reply.started":"2022-05-29T23:05:25.397129Z","shell.execute_reply":"2022-05-29T23:05:25.397161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Identifying uniqueness\ncat_train.apply(pd.Series.nunique, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T23:05:25.398608Z","iopub.status.idle":"2022-05-29T23:05:25.39893Z","shell.execute_reply.started":"2022-05-29T23:05:25.398764Z","shell.execute_reply":"2022-05-29T23:05:25.398781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Convert Binary to Boolean","metadata":{}},{"cell_type":"code","source":"binary_col=[]\nfor col in cat_train.columns:\n    if len(list(cat_train[col].unique())) <=2:\n        binary_col.append(col)\ncat_train[binary_col].head # Sanity Check","metadata":{"execution":{"iopub.status.busy":"2022-05-29T23:05:25.400336Z","iopub.status.idle":"2022-05-29T23:05:25.400767Z","shell.execute_reply.started":"2022-05-29T23:05:25.40051Z","shell.execute_reply":"2022-05-29T23:05:25.400536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Label Encoding for Train and Test","metadata":{}},{"cell_type":"code","source":"lb_mkr = LabelEncoder()\nfor col in binary_col:\n    cat_train[col] = lb_mkr.fit_transform(cat_train[col])\n    cat_test[col] = lb_mkr.fit_transform(cat_test[col])","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:53:54.329995Z","iopub.execute_input":"2022-05-29T22:53:54.330225Z","iopub.status.idle":"2022-05-29T22:53:54.765661Z","shell.execute_reply.started":"2022-05-29T22:53:54.330197Z","shell.execute_reply":"2022-05-29T22:53:54.76463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_train[binary_col].head() # Sanity Check","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:53:54.76697Z","iopub.execute_input":"2022-05-29T22:53:54.7672Z","iopub.status.idle":"2022-05-29T22:53:54.785565Z","shell.execute_reply.started":"2022-05-29T22:53:54.767171Z","shell.execute_reply":"2022-05-29T22:53:54.784517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### One Hot Encoding ","metadata":{}},{"cell_type":"code","source":"cat_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:53:54.790359Z","iopub.execute_input":"2022-05-29T22:53:54.790729Z","iopub.status.idle":"2022-05-29T22:53:54.810866Z","shell.execute_reply.started":"2022-05-29T22:53:54.790689Z","shell.execute_reply":"2022-05-29T22:53:54.809771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_train = pd.get_dummies(cat_train)\ncat_test = pd.get_dummies(cat_test)\ncat_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:53:54.81196Z","iopub.execute_input":"2022-05-29T22:53:54.81222Z","iopub.status.idle":"2022-05-29T22:53:55.751825Z","shell.execute_reply.started":"2022-05-29T22:53:54.812186Z","shell.execute_reply":"2022-05-29T22:53:55.750728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combining the dataframes into one\nencoded_train = pd.concat([num_train,cat_train], axis=1)\nencoded_test = pd.concat([num_test, cat_test], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:53:55.753647Z","iopub.execute_input":"2022-05-29T22:53:55.754658Z","iopub.status.idle":"2022-05-29T22:53:55.902521Z","shell.execute_reply.started":"2022-05-29T22:53:55.754599Z","shell.execute_reply":"2022-05-29T22:53:55.901501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Outlier Investigation\nencoded_train['DAYS_EMPLOYED'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:53:55.904251Z","iopub.execute_input":"2022-05-29T22:53:55.905067Z","iopub.status.idle":"2022-05-29T22:53:55.935863Z","shell.execute_reply.started":"2022-05-29T22:53:55.905009Z","shell.execute_reply":"2022-05-29T22:53:55.934969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anom = encoded_train[encoded_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = encoded_train[encoded_train['DAYS_EMPLOYED'] != 365243]\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:53:55.937685Z","iopub.execute_input":"2022-05-29T22:53:55.93804Z","iopub.status.idle":"2022-05-29T22:53:56.545144Z","shell.execute_reply.started":"2022-05-29T22:53:55.938008Z","shell.execute_reply":"2022-05-29T22:53:56.544195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an anomalous flag column\nencoded_train['DAYS_EMPLOYED_ANOM'] = encoded_train[\"DAYS_EMPLOYED\"] == 365243\nencoded_test['DAYS_EMPLOYED_ANOM'] = encoded_test[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\nencoded_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\nencoded_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:53:56.54636Z","iopub.execute_input":"2022-05-29T22:53:56.546881Z","iopub.status.idle":"2022-05-29T22:53:56.600469Z","shell.execute_reply.started":"2022-05-29T22:53:56.546839Z","shell.execute_reply":"2022-05-29T22:53:56.59912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Aggregating Numericals","metadata":{"execution":{"iopub.status.busy":"2022-05-23T04:58:56.410781Z","iopub.execute_input":"2022-05-23T04:58:56.411079Z","iopub.status.idle":"2022-05-23T04:58:56.416125Z","shell.execute_reply.started":"2022-05-23T04:58:56.411049Z","shell.execute_reply":"2022-05-23T04:58:56.41482Z"}}},{"cell_type":"code","source":"def agg_numeric(df, group_var, df_name):\n\n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != group_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    numeric_df = df.select_dtypes('number')\n    numeric_df[group_var] = df[group_var]\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'sum']).reset_index()\n\n    # Need to create new column names\n    columns = [group_var]\n\n\n    for var in agg.columns.levels[0]:\n        if var != group_var:\n            \n            # [:-1] because the index column is ''\n            for stat in agg.columns.levels[1][:-1]:\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n\n    agg.columns = columns\n    return agg","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:53:56.601979Z","iopub.execute_input":"2022-05-29T22:53:56.602561Z","iopub.status.idle":"2022-05-29T22:53:56.611414Z","shell.execute_reply.started":"2022-05-29T22:53:56.602522Z","shell.execute_reply":"2022-05-29T22:53:56.610536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Aggregating Categoricals","metadata":{}},{"cell_type":"code","source":"# We have four supplemental table and we need to combine them together and merge it with main table using SK_ID_CURR\n\n# count_categorical function will do a OHE for categorical data and for each column, calculate sum and proportion of each value\ndef count_categorical(df, group_var, df_name):\n    \n    # get the categorical data and do OHE\n    cat = pd.get_dummies(df.select_dtypes('object'))\n    \n    # add the id\n    cat[group_var] = df[group_var]\n    \n    # aggregate by id\n    cat = cat.groupby(group_var).agg(['sum', 'mean'])\n    \n    column_names = []\n    \n    # for each first level column name, we define a new column name\n    for var in cat.columns.levels[0]:\n        for stat in ['count', 'count_norm']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    cat.columns = column_names\n    \n    return cat","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:53:56.612862Z","iopub.execute_input":"2022-05-29T22:53:56.613312Z","iopub.status.idle":"2022-05-29T22:53:56.630937Z","shell.execute_reply.started":"2022-05-29T22:53:56.613277Z","shell.execute_reply":"2022-05-29T22:53:56.629822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Aggregating Tables","metadata":{}},{"cell_type":"code","source":"# Pos\npos_counts = count_categorical(pos, group_var = 'SK_ID_CURR', df_name = 'os')\npos_agg = agg_numeric(pos, group_var = 'SK_ID_CURR', df_name = 'pos')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:53:56.632451Z","iopub.execute_input":"2022-05-29T22:53:56.632988Z","iopub.status.idle":"2022-05-29T22:54:12.968887Z","shell.execute_reply.started":"2022-05-29T22:53:56.632951Z","shell.execute_reply":"2022-05-29T22:54:12.967931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Installment\ninstallment_agg = agg_numeric(installment, group_var = 'SK_ID_CURR', df_name = 'installment')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:54:12.970472Z","iopub.execute_input":"2022-05-29T22:54:12.970851Z","iopub.status.idle":"2022-05-29T22:54:18.260773Z","shell.execute_reply.started":"2022-05-29T22:54:12.970799Z","shell.execute_reply":"2022-05-29T22:54:18.259737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Credit\ncredit_counts = count_categorical(credit, group_var = 'SK_ID_CURR', df_name = 'credit')\ncredit_agg = agg_numeric(credit, group_var = 'SK_ID_CURR', df_name = 'credit')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:54:18.262346Z","iopub.execute_input":"2022-05-29T22:54:18.263101Z","iopub.status.idle":"2022-05-29T22:54:24.612402Z","shell.execute_reply.started":"2022-05-29T22:54:18.263044Z","shell.execute_reply":"2022-05-29T22:54:24.611301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# p_app\np_app_counts = count_categorical(p_app, group_var = 'SK_ID_CURR', df_name = 'p_app')\np_app_agg = agg_numeric(p_app, group_var = 'SK_ID_CURR', df_name = 'p_app')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:54:24.61363Z","iopub.execute_input":"2022-05-29T22:54:24.613884Z","iopub.status.idle":"2022-05-29T22:54:56.520081Z","shell.execute_reply.started":"2022-05-29T22:54:24.613855Z","shell.execute_reply":"2022-05-29T22:54:56.518873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Merging with Test and Train ","metadata":{}},{"cell_type":"code","source":"# Merge with the train\nencoded_train = encoded_train.merge(pos_counts, on = 'SK_ID_CURR', how = 'left')\nencoded_train = encoded_train.merge(pos_agg, on = 'SK_ID_CURR', how = 'left')\n\nencoded_train = encoded_train.merge(installment_agg, on = 'SK_ID_CURR', how = 'left')\n\nencoded_train = encoded_train.merge(credit_counts, on = 'SK_ID_CURR', how = 'left')\nencoded_train = encoded_train.merge(credit_agg, on = 'SK_ID_CURR', how = 'left')\n\nencoded_train = encoded_train.merge(p_app_counts, on = 'SK_ID_CURR', how = 'left')\nencoded_train = encoded_train.merge(p_app_agg, on = 'SK_ID_CURR', how = 'left')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:54:56.522107Z","iopub.execute_input":"2022-05-29T22:54:56.522697Z","iopub.status.idle":"2022-05-29T22:56:10.649686Z","shell.execute_reply.started":"2022-05-29T22:54:56.522624Z","shell.execute_reply":"2022-05-29T22:56:10.648523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge with the test\nencoded_test = encoded_test.merge(pos_counts, on = 'SK_ID_CURR', how = 'left')\nencoded_test = encoded_test.merge(pos_agg, on = 'SK_ID_CURR', how = 'left')\n\nencoded_test = encoded_test.merge(installment_agg, on = 'SK_ID_CURR', how = 'left')\n\nencoded_test = encoded_test.merge(credit_counts, on = 'SK_ID_CURR', how = 'left')\nencoded_test = encoded_test.merge(credit_agg, on = 'SK_ID_CURR', how = 'left')\n\nencoded_test = encoded_test.merge(p_app_counts, on = 'SK_ID_CURR', how = 'left')\nencoded_test = encoded_test.merge(p_app_agg, on = 'SK_ID_CURR', how = 'left')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:56:10.652789Z","iopub.execute_input":"2022-05-29T22:56:10.653163Z","iopub.status.idle":"2022-05-29T22:56:31.782758Z","shell.execute_reply.started":"2022-05-29T22:56:10.653114Z","shell.execute_reply":"2022-05-29T22:56:31.781566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training set full shape: ', encoded_train.shape)\nprint('Testing set full shape: ' , encoded_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:56:31.784097Z","iopub.execute_input":"2022-05-29T22:56:31.784507Z","iopub.status.idle":"2022-05-29T22:56:31.791446Z","shell.execute_reply.started":"2022-05-29T22:56:31.784432Z","shell.execute_reply":"2022-05-29T22:56:31.790217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"copy_encoded_train = encoded_train.copy()\ncopy_encoded_test = encoded_test.copy()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:56:31.793778Z","iopub.execute_input":"2022-05-29T22:56:31.794273Z","iopub.status.idle":"2022-05-29T22:56:34.140068Z","shell.execute_reply.started":"2022-05-29T22:56:31.794215Z","shell.execute_reply":"2022-05-29T22:56:34.138751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### III. Feature Selection","metadata":{}},{"cell_type":"code","source":"# Calculating proportion by summing NA values and dividing by length of DF\nprop_na = encoded_train.isna().sum()/len(encoded_train)\n# Filtering out columns with less than 5% NA values to clean up the visualization below\nprop_na = prop_na[prop_na > 0.3]\nprop_na = prop_na.sort_values(0, ascending=True).rename('missing_perc').reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:56:34.142112Z","iopub.execute_input":"2022-05-29T22:56:34.142461Z","iopub.status.idle":"2022-05-29T22:56:34.633379Z","shell.execute_reply.started":"2022-05-29T22:56:34.142413Z","shell.execute_reply":"2022-05-29T22:56:34.632088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prop_na.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:56:34.634961Z","iopub.execute_input":"2022-05-29T22:56:34.635203Z","iopub.status.idle":"2022-05-29T22:56:34.646871Z","shell.execute_reply.started":"2022-05-29T22:56:34.635175Z","shell.execute_reply":"2022-05-29T22:56:34.645796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 20))\n\nbarh = plt.barh(prop_na['index'], prop_na['missing_perc'], alpha=0.85, color='green')\n\nplt.title('Proportion of NA Values')\nplt.xticks(np.arange(.1, 1.01, .1))\n\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:56:34.652083Z","iopub.execute_input":"2022-05-29T22:56:34.652376Z","iopub.status.idle":"2022-05-29T22:56:37.440743Z","shell.execute_reply.started":"2022-05-29T22:56:34.652341Z","shell.execute_reply":"2022-05-29T22:56:37.439676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_missing(df_train, df_test, thredhold):\n    \n    # get the missing column and missing values percentage\n    train_missing = (df_train.isnull().sum() / len(df_train)).sort_values(ascending = False)\n    test_missing = (df_test.isnull().sum() / len(df_test)).sort_values(ascending = False)\n    \n    # filter the missing values by thredhold\n    train_missing = train_missing.index[train_missing > thredhold]\n    test_missing = test_missing.index[test_missing > thredhold]\n    \n    # combine the missing values columns from train and test\n    all_missing = list(set(set(train_missing) | set(test_missing)))\n    print('There are %d columns with more than %s%% missing values' % (len(all_missing), thredhold))\n    \n    # save the target column\n    train_labels = train[\"TARGET\"]\n    \n    # drop the missing values columns\n    df_train = df_train.drop(columns = all_missing)\n    df_test = df_test.drop(columns = all_missing)\n    \n    # align the columns from both table\n    df_train, df_test = df_train.align(df_test, join = 'inner', axis = 1)\n    df_train, df_test = df_train.align(df_test, join = 'inner', axis = 1)\n    \n    df_train[\"TARGET\"] = train_labels\n    \n    print('Training set full shape: ', df_train.shape)\n    print('Testing set full shape: ' , df_test.shape)\n    \n    return df_train, df_test","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:56:37.442445Z","iopub.execute_input":"2022-05-29T22:56:37.442929Z","iopub.status.idle":"2022-05-29T22:56:37.455773Z","shell.execute_reply.started":"2022-05-29T22:56:37.442888Z","shell.execute_reply":"2022-05-29T22:56:37.454876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_train, encoded_test = remove_missing(encoded_train, encoded_test, 0.60)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T23:04:19.454436Z","iopub.execute_input":"2022-05-29T23:04:19.454773Z","iopub.status.idle":"2022-05-29T23:04:19.477589Z","shell.execute_reply.started":"2022-05-29T23:04:19.454739Z","shell.execute_reply":"2022-05-29T23:04:19.47621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance","metadata":{}},{"cell_type":"code","source":"# store and remove the id column\n\ntrain_id = encoded_train['SK_ID_CURR']\ntest_id = encoded_test['SK_ID_CURR']\n\nencoded_train = encoded_train.drop('SK_ID_CURR', axis=1)\nencoded_test = encoded_test.drop('SK_ID_CURR', axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T23:04:16.785015Z","iopub.execute_input":"2022-05-29T23:04:16.785357Z","iopub.status.idle":"2022-05-29T23:04:16.809312Z","shell.execute_reply.started":"2022-05-29T23:04:16.785318Z","shell.execute_reply":"2022-05-29T23:04:16.807992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# store and remove the train data target\ntarget_train = encoded_train['TARGET']\nencoded_train = encoded_train.drop('TARGET', axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T23:04:13.451887Z","iopub.execute_input":"2022-05-29T23:04:13.452207Z","iopub.status.idle":"2022-05-29T23:04:13.472189Z","shell.execute_reply.started":"2022-05-29T23:04:13.452169Z","shell.execute_reply":"2022-05-29T23:04:13.47118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:56:43.173846Z","iopub.execute_input":"2022-05-29T22:56:43.174299Z","iopub.status.idle":"2022-05-29T22:56:43.208643Z","shell.execute_reply.started":"2022-05-29T22:56:43.174245Z","shell.execute_reply":"2022-05-29T22:56:43.20755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# format the column name\nencoded_train.columns = [''.join (c if c.isalnum() else '_' for c in str(x)) for x in encoded_train.columns]\nencoded_test.columns = [''.join (c if c.isalnum() else '_' for c in str(x)) for x in encoded_test.columns]","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:56:43.210703Z","iopub.execute_input":"2022-05-29T22:56:43.211042Z","iopub.status.idle":"2022-05-29T22:56:43.228058Z","shell.execute_reply.started":"2022-05-29T22:56:43.210999Z","shell.execute_reply":"2022-05-29T22:56:43.226899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build a LightGBM model to select important features\nd_train = lgb.Dataset(encoded_train, label=target_train)\nparam = {'max_depth': 5, 'learning_rate' : 0.1, 'num_leaves': 900, 'n_estimators': 100}\nmodel = lgb.train(params=param,train_set=d_train)\nax = lgb.plot_importance(model, max_num_features=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T23:04:05.720036Z","iopub.execute_input":"2022-05-29T23:04:05.720384Z","iopub.status.idle":"2022-05-29T23:04:05.8724Z","shell.execute_reply.started":"2022-05-29T23:04:05.720326Z","shell.execute_reply":"2022-05-29T23:04:05.871058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation","metadata":{}},{"cell_type":"code","source":"correlation_threshold = 0.8\n\n# Calculating an absolute value correlation matrix\ncorr_mat = encoded_train[LGBM_features_columns].corr().abs()\n\n# Getting upper triangle of this matrix only\nupper = pd.DataFrame(np.triu(corr_mat, k=1), columns=encoded_train[LGBM_features_columns].columns)\n\n# Select columns with correlations above threshold\ncorr_col_drop = [col for col in upper.columns if any(upper[col] > correlation_threshold)]\n\nprint(f'There are {len(corr_col_drop)} columns to remove out of {len(encoded_train[LGBM_features_columns].columns)}.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nencoded_train_if = encoded_train[LGBM_features_columns].drop(corr_col_drop, axis=1)\nencoded_test_if = encoded_test[LGBM_features_columns].drop(corr_col_drop, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### IV. PCA","metadata":{}},{"cell_type":"code","source":"# define the number of components\nn_comp=.95\n\n# create a pca pipeline with median imputation\npipeline = Pipeline(steps = [('scaler', StandardScaler()),\n                             ('imputer', SimpleImputer(strategy = 'median')),\n                             ('pca', PCA(n_components=n_comp, svd_solver='full', random_state=1))])\n\npca = pipeline.named_steps['pca']\n\npipeline.fit(encoded_train)\n\ntrain_pca = pipeline.transform(encoded_train)\ntest_pca = pipeline.transform(encoded_test)\n\nplt.figure(figsize = (8, 5))\nplt.plot(list(range(train_pca.shape[1])), np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of PC'); plt.ylabel('Cumulative Variance Explained');\nplt.title('Cumulative Variance Explained with PCA');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select top n princple features\ntotal_variance_explained = 0\nprint('Individual variance contributions:')\n\nfor j in range(12):\n    print(pca.explained_variance_ratio_[j])\n    total_variance_explained += pca.explained_variance_ratio_[j]\nprint('Explained variance: %.4f' % total_variance_explained)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pca_train = pd.DataFrame(data=train_pca)\ndf_pca_test = pd.DataFrame(data=test_pca)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}