{"cells":[{"metadata":{"trusted":true,"_uuid":"e19bfdab74156f301da91c59041ed4a2e8d0f2c4"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nimport math\nimport os\n\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }</style>\"))\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a9fd7418868e93cd60405b453505f37f52e0984"},"cell_type":"markdown","source":"We try to find client will pay their repayment if we give them credit in our bank. We try to solve this problem according to client bureau, installment payments and current payments history. We try to analyze clients and their behaviours about their debts. By using that we can determine that clients are reliable or not. By using this system, Bank can choose their client more efficient way and they do not need to suffer days past due payments. In this notebook following index is followed;\n\n**INDEX : **\n    \n    1) Extra Data Sources Analysis and Scoring\n        1.1) Installment Payments\n        1.2) Bureau\n    2) Exploratory Data Analysis\n        2.1) Missing Values\n        2.2) Anomalies in Features\n            2.2.1) DAYS_BIRTH\n            2.2.2) DAYS_EMPLOYED\n            2.2.3) DAYS_REGISTRATION\n        2.3) Relationship with Target Variables\n            2.3.1) EXT_SOURCE_2\n            2.3.2) DAYS_EMPLOYED\n    3) Feature Engineering\n    4) Feature Importance and Target Variable Analysis\n        4.1) RandomForestClassifier for Feature Importance\n        4.2) Logistic Regression\n            4.2.1) DAYS_ID_PUBLISH\n            4.2.2) DAYS_LAST_PHONE_CHANGE\n    5) Models Implementation and Feature Elimination\n        5.1) Principal Component Analysis without Boostrap Sampling\n        5.2) Principal Component Analysis with Boostrap Resampling\n    6) Conclusion\n            "},{"metadata":{"_uuid":"38f480dfb5144fa41a79a7bf70bcc23a5f1e95a0"},"cell_type":"markdown","source":"# 1) Extra Data Sources Analysis and Scoring\n\nIn order to analyze credit risk of our clients, we have to analyze their behaviours. In this notebook we analyze their installment payments and bureau data sources. We analyze these data sources and merge them with main table. All of client history is important for us but a lot of features misdirect us for knowledge of client behaviour. Then we have to decrease features by analyzing extra data sources. We try to decrease features in data sources by scoring clients.\n\n## 1.1) Installment Payments\n\nIn order to score their installment payment behaviour that we need to segment our client and then we score them. We can determine this by using DAYS_ columns and AMT_ columns. First we need to look client payment status. We look most reliable client in this records. For example one client pay his all debt and pay it 2700 day before to due date then this client take 100 from installment payments. Then score decrease according to their performance.\n\nWe score our clients according to this formula:\n$$Score =100 - (\\frac{(\\frac {Days past Due Date}{MaximumPast} + \\frac {Remaining Debt}{Maximum Remaining Debt})}{2} * 100) $$"},{"metadata":{"trusted":true,"_uuid":"06a18c82310e55e4a53da16afed285d7a896c4e4"},"cell_type":"code","source":"installment_payments = pd.read_csv(\"../input/installments_payments.csv\")\ninstallment_payments.dropna(inplace=True)\n\ninstallment_payments_new = installment_payments.groupby(['SK_ID_CURR','SK_ID_PREV','NUM_INSTALMENT_NUMBER'],as_index=False).mean()\ninstallment_payments_new['AMT_INSTALMENT'] = np.round(installment_payments.groupby(['SK_ID_CURR','SK_ID_PREV','NUM_INSTALMENT_NUMBER'],as_index=False).sum()['AMT_INSTALMENT'],decimals=3)\n\nl = installment_payments_new[installment_payments_new.AMT_PAYMENT>installment_payments_new.AMT_INSTALMENT].index\ninstallment_payments_new.drop(index=l,inplace=True)\n\ninstallment_payments_new['payment'] = installment_payments_new.AMT_INSTALMENT-installment_payments_new.AMT_PAYMENT\ninstallment_payments_new['days'] = installment_payments_new.iloc[:,4]-installment_payments_new.iloc[:,5]\nearly_max = np.min(installment_payments_new.DAYS_INSTALMENT-installment_payments_new.DAYS_ENTRY_PAYMENT)\ninstallment_payments_new.days = installment_payments_new.days - early_max\n\ninstallment_payments_new['score'] = 100 - (((installment_payments_new.days/np.max(installment_payments_new.days)) + (installment_payments_new.payment/np.max(installment_payments_new.payment)))/2*100)\n\nscore_last_installment = installment_payments_new.groupby(['SK_ID_CURR'],as_index=False)['score'].mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca8ecf0128dfd552690d7e66923ce17ab28ad4b5"},"cell_type":"markdown","source":"## 1.2) Bureau\n\nIn order to score their bureau payment behaviour, we need to look for their current debt, days overdue and account is active or closed. Our score metric will be;\n$$IF(CREDITDAYOVERDUE != 0) \\quad score = 100-(\\frac{(\\frac {CREDITDAYOVERDUE}{MAXCREDITDAYOVERDUE} + \\frac {AMTCREDITSUMOVERDUE}{MAXAMTCREDITSUMOVERDUE})}{2}*100) $$\nELSE\n$$IF(CREDITDAYOVERDUE == 0) \\quad AND \\quad IF(CREDITACTIVE == CLOSED) \\quad score = (\\frac {\\frac {AMTCREDITSUM}{DAYSCREDITENDDATE-DAYSCREDIT}}{MAX(\\frac {AMTCREDITSUM}{DAYSCREDITENDDATE-DAYSCREDIT})})*100$$\nELSE\n$$IF(CREDITDAYOVERDUE == 0) \\quad AND \\quad IF(CREDITACTIVE == ACTIVE) \\quad score = (\\frac {\\frac {AMTCREDITSUM-AMTCREDITSUMDEBT}{DAYSCREDITENDDATE-DAYSCREDIT}}{MAX(\\frac {AMTCREDITSUM-AMTCREDITSUMDEBT}{DAYSCREDITENDDATE-DAYSCREDIT})})*100$$\n<br>\nAfter looking clients installment payments and bureau history, we can use these score variables in our main table. Now we can start analysis of data towards to target variable.\n<br>\n<br>"},{"metadata":{"trusted":true,"_uuid":"f243fe9531472133f833d4ed2b9f3693a7997ea7"},"cell_type":"code","source":"bureau = pd.read_csv(\"../input/bureau.csv\")\n\nbureau_new = bureau.groupby(['SK_ID_CURR','SK_ID_BUREAU','CREDIT_ACTIVE'],as_index=False).mean()\nbureau_new['score'] = np.nan\n\nl1 = bureau_new[bureau_new.CREDIT_DAY_OVERDUE!=0].index\nbureau_new.loc[l1,'score'] =100-(((bureau_new.loc[l1,'CREDIT_DAY_OVERDUE']/np.max(bureau_new.loc[l1,'CREDIT_DAY_OVERDUE']))+(bureau_new.loc[l1,'AMT_CREDIT_SUM_OVERDUE']/np.max(bureau_new.loc[l1,'AMT_CREDIT_SUM_OVERDUE'])))/2*100) \n\nl2 = bureau_new[(bureau_new.CREDIT_DAY_OVERDUE==0)&(bureau_new.CREDIT_ACTIVE=='Closed')].index\ndivisionl2 = bureau_new.loc[l2,'AMT_CREDIT_SUM']/(bureau_new.loc[l2,'DAYS_CREDIT_ENDDATE']-bureau_new.loc[l2,'DAYS_CREDIT'])\ndivisionl2.replace([np.inf, -np.inf], np.nan,inplace=True)\nbureau_new.loc[l2,'score'] = (divisionl2 / np.max(divisionl2))*100\n\n\nl3 = bureau_new[(bureau_new.CREDIT_DAY_OVERDUE==0)&(bureau_new.CREDIT_ACTIVE=='Active')].index\ndivisionl3 = (bureau_new.loc[l3,'AMT_CREDIT_SUM']-bureau_new.loc[l3,'AMT_CREDIT_SUM_DEBT'])/(bureau_new.loc[l3,'DAYS_CREDIT_ENDDATE']-bureau_new.loc[l3,'DAYS_CREDIT'])\ndivisionl3.replace([np.inf, -np.inf], np.nan,inplace=True)\nbureau_new.loc[l3,'score'] = (divisionl3 / np.max(divisionl3))*100\n\nscore_last_bureau = bureau_new.groupby(['SK_ID_CURR'],as_index=False)['score'].mean()\nscore_last_bureau.dropna(subset=['score'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5558e452c7c7af9e19c8f435ba27a0dd3bcb5e47"},"cell_type":"markdown","source":"# 2) Exploratory Data Analysis"},{"metadata":{"trusted":true,"_uuid":"0ceee2a7024042c2cfb5d9f3758fbbd0d9cf8698"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\napplication_train = pd.read_csv(\"../input/application_train.csv\")\napplication_test = pd.read_csv(\"../input/application_test.csv\")\ntarget_variable = application_train.TARGET\napplication_train.drop('TARGET',axis=1,inplace=True)\n\n#Bureau and installment score matching\napplication_train['bureau_score']=np.nan\napplication_train['installment_score']=np.nan\n\nl = list(score_last_bureau.SK_ID_CURR)\nl_last = list(application_train[application_train.SK_ID_CURR.isin(l)]['SK_ID_CURR'])\napplication_train.loc[application_train.SK_ID_CURR.isin(l_last),'bureau_score']=score_last_bureau.loc[score_last_bureau.SK_ID_CURR.isin(l_last),'score'].values\n\nl2 = list(score_last_installment.SK_ID_CURR)\nl2_last = list(application_train[application_train.SK_ID_CURR.isin(l2)]['SK_ID_CURR'])\napplication_train.loc[application_train.SK_ID_CURR.isin(l2_last),'installment_score']=score_last_installment.loc[score_last_installment.SK_ID_CURR.isin(l2_last),'score'].values\n\napplication_test['bureau_score']=np.nan\napplication_test['installment_score']=np.nan\n\nl = list(score_last_bureau.SK_ID_CURR)\nl_last = list(application_test[application_test.SK_ID_CURR.isin(l)]['SK_ID_CURR'])\napplication_test.loc[application_test.SK_ID_CURR.isin(l_last),'bureau_score']=score_last_bureau.loc[score_last_bureau.SK_ID_CURR.isin(l_last),'score'].values\n\nl2 = list(score_last_installment.SK_ID_CURR)\nl2_last = list(application_test[application_test.SK_ID_CURR.isin(l2)]['SK_ID_CURR'])\napplication_test.loc[application_test.SK_ID_CURR.isin(l2_last),'installment_score']=score_last_installment.loc[score_last_installment.SK_ID_CURR.isin(l2_last),'score'].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9072a2d23b24c2ff61652d76e488daf13f149ac5"},"cell_type":"markdown","source":"## 2.1) Missing Values\n\nFirst of all we needed to look missing values in data. Without that we cannot determine which variable is more effect on target variable. We start with calculate missing values, if variable have %20 NaN then we can drop this variable. Because if we try to impute these variables, we change characteristics of data and this can mislead us. If variable have less than %20 Nan values, we look correlation matrix for this variable and try to fill these values according to that. But before we fill missing values we need to encode categorical variables(We use Will Koehrsen notebook codes). Then we look for missing values and use following formula for handling missing values;\n\n$IF(NANPERCENTAGE > 0.2)$ Then Drop Columns\n\n$IF(NANPERCENTAGE <= 0.2 \\& NANPERCENTAGE > 0)$ Then Fill missing values with median of columns\n\nWe do last method because small amount of Nan values do not change characteristics of predictive variable."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"fae981f4975de2400f934f34e6d9c864edbe9db7"},"cell_type":"code","source":"# %20 or more missing values handling\nmissing_values = pd.DataFrame(np.ones(shape=(123,2)),columns=['Columns','Total NaNs'])\nmissing_values['Columns'] = application_train.isnull().sum().index\nmissing_values['Total NaNs'] = application_train.isnull().sum().values\n\nmissing_values['NanP'] = missing_values['Total NaNs'] / len(application_train)\n\ndroped_columns = missing_values.loc[missing_values.NanP>0.2,'Columns']\napplication_train.drop(droped_columns,inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"38cfe38591414703c9b0f4f11fbc7d5d94c1a3c4"},"cell_type":"code","source":"missing_values = pd.DataFrame(np.ones(shape=(123,2)),columns=['Columns','Total NaNs'])\nmissing_values['Columns'] = application_test.isnull().sum().index\nmissing_values['Total NaNs'] = application_test.isnull().sum().values\n\nmissing_values['NanP'] = missing_values['Total NaNs'] / len(application_test)\n\ndroped_columns = missing_values.loc[missing_values.NanP>0.2,'Columns']\napplication_test.drop(droped_columns,inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df9017a9a6f519557997610829429d8b693e909d"},"cell_type":"code","source":"application_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"07577514a335a86dc2af0e924d4830f4be9d4c6d"},"cell_type":"code","source":"application_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9eae52011cbf0b399c39a771a70a39daf7531ba"},"cell_type":"code","source":"# Categorical Variable Handling\n\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\n# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in application_train:\n    if application_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(application_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(application_train[col])\n            # Transform both training and testing data\n            application_train[col] = le.transform(application_train[col])\n            application_test[col] = le.transform(application_test[col])\n\n# one-hot encoding of categorical variables\napplication_train = pd.get_dummies(application_train)\napplication_test = pd.get_dummies(application_test)\n\n# %10 or less missing values handling\nmissing_values = pd.DataFrame(np.ones(shape=(165,2)),columns=['Columns','Total NaNs'])\nmissing_values['Columns'] = application_train.isnull().sum().index\nmissing_values['Total NaNs'] = application_train.isnull().sum().values\n\nmissing_values['NanP'] = missing_values['Total NaNs'] / len(application_train)\n\nmedian_columns = missing_values[(missing_values.NanP>0)&(missing_values.NanP<=0.2)]['Columns']\nfor col in median_columns:\n    application_train[col].fillna(application_train[col].median(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47947437175b80221391aa803a9088b18ebb7fe2"},"cell_type":"code","source":"# Categorical Variable Handling\n\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\n# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in application_test:\n    if application_test[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(application_test[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(application_test[col])\n            # Transform both training and testing data\n            application_test[col] = le.transform(application_test[col])\n            application_test[col] = le.transform(application_test[col])\n\n# one-hot encoding of categorical variables\napplication_test = pd.get_dummies(application_test)\napplication_test = pd.get_dummies(application_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b4506e7a505e2d2bbaa21faf88e133db65c0f7a"},"cell_type":"code","source":"# %10 or less missing values handling\nmissing_values = pd.DataFrame(np.ones(shape=(162,2)),columns=['Columns','Total NaNs'])\nmissing_values['Columns'] = application_test.isnull().sum().index\nmissing_values['Total NaNs'] = application_test.isnull().sum().values\n\nmissing_values['NanP'] = missing_values['Total NaNs'] / len(application_test)\n\nmedian_columns = missing_values[(missing_values.NanP>0)&(missing_values.NanP<=0.2)]['Columns']\nfor col in median_columns:\n    application_test[col].fillna(application_test[col].median(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d325f45087f04bd835ba232823879719d228fe60"},"cell_type":"markdown","source":"## 2.2) Anomalies in Features\n\nAfter fill or drop missing values, we can start to look for anomalies in data. We need to drop these outliers because this type of outliers mislead our machine learning models. First of all we start with describe method to gather all statistics of variables. Then we try to decide which variable have anomalies. After that we either drop these rows or fill these values with column median values."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"a3f323a81385b3a3227ebdeeb9145412f86d979d"},"cell_type":"code","source":"anomaly_df = application_train.describe()\n\nanomaly_columns=anomaly_df.columns[anomaly_df.iloc[7,:]!=1]\nanomaly_df[anomaly_columns]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3faa30cea5a6a14d7d4746eaa9a1a8bf46dd4104"},"cell_type":"markdown","source":"### 2.2.1) DAYS_BIRTH"},{"metadata":{"trusted":true,"_uuid":"54b4acf3cb101e4349536ec36eed2b6948c51015"},"cell_type":"code","source":"25229/365","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae9b5b1f56a8c34d64631d88e376fe7402405b2c"},"cell_type":"code","source":"7489/365","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9f033d5e0694830cb94f4b8d0cb81432ffe00f1"},"cell_type":"markdown","source":"According to data most youngest person is 20 years old and oldest person is 69 years old. This show that, we have not anomalies in DAYS_BIRTH column."},{"metadata":{"_uuid":"fd24c86ab37c0566ee8b139a59ab21e275ef82f5"},"cell_type":"markdown","source":"### 2.2.2) DAYS_EMPLOYED"},{"metadata":{"trusted":true,"_uuid":"b20b2ad9b519e49c1bd920d24e959a968fb86cce"},"cell_type":"code","source":"application_train.DAYS_EMPLOYED = application_train.DAYS_EMPLOYED - np.min(application_train.DAYS_EMPLOYED)\napplication_test.DAYS_EMPLOYED = application_test.DAYS_EMPLOYED - np.min(application_test.DAYS_EMPLOYED)\n\nanomaly_df = application_train.describe()\n\nanomaly_columns=anomaly_df.columns[anomaly_df.iloc[7,:]!=1]\nanomaly_df[anomaly_columns]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc2eee9b239733d96c8a8a2fb8301ccbab1b5247"},"cell_type":"markdown","source":"In days employed columns we need to change it positive variables. So we add minumum amount to all rows to gather positive variables. Because person can not employed negative days. Also oldest person in data is 69 years old then max years employed must be 45-50 years. We determine these days. If person days employed higher than 45-50 years we replace with Nan then we fill these values with column's median value."},{"metadata":{"trusted":true,"_uuid":"29e4b45c694f4df76132e902e10da50c49347fb5"},"cell_type":"code","source":"50*365","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"ec970dc1e8935d5c774de16e5a4f62000824e910"},"cell_type":"code","source":"l = application_train[application_train.DAYS_EMPLOYED>18250]['DAYS_EMPLOYED'].index\napplication_train.loc[l,'DAYS_EMPLOYED'] = np.nan\napplication_train['DAYS_EMPLOYED'].fillna(application_train['DAYS_EMPLOYED'].median(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d52e36d8b99a48951a3b9827d655ba621e5aa249"},"cell_type":"code","source":"l = application_test[application_test.DAYS_EMPLOYED>18250]['DAYS_EMPLOYED'].index\napplication_test.loc[l,'DAYS_EMPLOYED'] = np.nan\napplication_test['DAYS_EMPLOYED'].fillna(application_test['DAYS_EMPLOYED'].median(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1df128e27b459db9a19aa1f71732300cff988734"},"cell_type":"markdown","source":"### 2.2.3) DAYS_REGISTRATION"},{"metadata":{"_uuid":"2c72b8d7abd36ba30389d7d2273a947a6d89d60d"},"cell_type":"markdown","source":"Since we know oldest client in data is 69 years old. Again registration cannot higher than 45-50 years. We use same method for this anomaly."},{"metadata":{"trusted":true,"_uuid":"1883895eff5fb0e67ebb05b51a33220356b9539b"},"cell_type":"code","source":"l = application_train[application_train.DAYS_REGISTRATION>18250]['DAYS_REGISTRATION'].index\napplication_train.loc[l,'DAYS_REGISTRATION'] = np.nan\napplication_train['DAYS_REGISTRATION'].fillna(application_train['DAYS_REGISTRATION'].median(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b20da6121c3d231a1e9776a7d3ef45f336a93b5c"},"cell_type":"code","source":"l = application_test[application_test.DAYS_REGISTRATION>18250]['DAYS_REGISTRATION'].index\napplication_test.loc[l,'DAYS_REGISTRATION'] = np.nan\napplication_test['DAYS_REGISTRATION'].fillna(application_test['DAYS_REGISTRATION'].median(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa8e31dd670e0c6fdb677ae8aa9c7d4da3dbb964"},"cell_type":"markdown","source":"## 2.3) Relationship with Target Variables\n\nWe handle missing values and outliers so far. Now we can determine correlation between target variable and predictive variables. We can check that is the relationship between these variables make sense or something wrong in data. We can look 2 most correlated variables to decide that. We use plots from Koehrsen notebook. "},{"metadata":{"trusted":true,"_uuid":"935f92c1cb08de617a4873f161ad012283feafa8"},"cell_type":"code","source":"application_train['TARGET'] = target_variable\ncorr = application_train.corr()['TARGET'].sort_values()\n\ncorr.tail(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f48b3d47e5fa81aa82b61fbaff1f56d8cc934da1"},"cell_type":"code","source":"corr.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eabfe66de56882d1a0cfd9794ecbfb82ae3e6e04"},"cell_type":"markdown","source":"### 2.3.1) EXT_SOURCE_2"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"7edf5543fbf5a5833ec9494e5ec0555a41c07cd9"},"cell_type":"code","source":"plt.figure(figsize = (6, 4))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(application_train.loc[application_train['TARGET'] == 0, 'EXT_SOURCE_2'], label = 'target == 0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(application_train.loc[application_train['TARGET'] == 1, 'EXT_SOURCE_2'], label = 'target == 1')\n\n# Labeling of plot\nplt.xlabel('EXT_SOURCE_2'); plt.ylabel('Density'); plt.title('Distribution EXT_SOURCE');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8fe43df5e033a4b17ea4796e2491499fbcaaed8"},"cell_type":"markdown","source":"We can easily say that between 0.6 and 0.8 values of EXT_SOURCE_2, client pay their repayment."},{"metadata":{"_uuid":"58d752daa8d0f14cf98cf38aa5115d8a4c54cb2f"},"cell_type":"markdown","source":"### 2.3.2) DAYS_EMPLOYED"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"2c48ab7bed1558b6e890ea2d9062d1b845cd63bd"},"cell_type":"code","source":"plt.figure(figsize = (6, 4))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(application_train.loc[application_train['TARGET'] == 0, 'DAYS_EMPLOYED'] / 365, label = 'target == 0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(application_train.loc[application_train['TARGET'] == 1, 'DAYS_EMPLOYED'] / 365, label = 'target == 1')\n\n# Labeling of plot\nplt.xlabel('DAYS_EMPLOYED'); plt.ylabel('Density'); plt.title('Distribution DAYS_EMPLOYED');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"640171dc79dd47233b45d2f8c50befed5730b9c0"},"cell_type":"markdown","source":"We can easily say that if client is employed more than 40 years, client paid their repayment."},{"metadata":{"_uuid":"5ca656237c2d9293d1af9afc54e63a679c325a9d"},"cell_type":"markdown","source":"# 3) Feature Engineering\n\nTo get better model to predict client is either pay his/her repayment or do not pay repayment, we need to create better features. Every possible new feature can be lead our model to success. New features and their formulas are following(Some of features taken from Koehrsen notebook);\n\n    -INCOME_PER_PERSON: Total income divided by family members count.\n    -CREDIT_INCOME_PERCENT: The percentage of the credit amount relative to a client's income.\n    -ANNUITY_INCOME_PERCENT: The percentage of the loan annuity relative to a client's income.\n    -CREDIT_TERM: The length of the payment in months (since the annuity is the monthly amount due.\n    -DAYS_EMPLOYED_PERCENT: The percentage of the days employed relative to the client's age.\n    -GOOD_PRICE_PERCENT: The percentage of the good price relative to client total income.\n    -CHILDREN_PERCENT: The percentage of children count relative to client family members count.\n    -OBS_30_CNT_SOCIAL_CIRCLE_REGION: Regional ratio of observable 30 day past due\n    -DEF_30_CNT_SOCIAL_CIRCLE_REGION: Regional ratio of default 30 day past due\n    -OBS_60_CNT_SOCIAL_CIRCLE_REGION: Regional ratio of observable 60 day past due\n    -DEF_60_CNT_SOCIAL_CIRCLE_REGION: Regional ratio of observable 60 day past due\n"},{"metadata":{"trusted":true,"_uuid":"d8dd1a2582649e6716f313bc0af338fdfc1617de"},"cell_type":"code","source":"application_train_new = application_train.copy()\napplication_test_new = application_test.copy()\n\napplication_train_new['INCOME_PER_PERSON'] = application_train['AMT_INCOME_TOTAL'] / application_train['CNT_FAM_MEMBERS']\napplication_train_new['CREDIT_INCOME_PERCENT'] = application_train_new['AMT_CREDIT'] / application_train_new['AMT_INCOME_TOTAL']\napplication_train_new['ANNUITY_INCOME_PERCENT'] = application_train_new['AMT_ANNUITY'] / application_train_new['AMT_INCOME_TOTAL']\napplication_train_new['CREDIT_TERM'] = application_train_new['AMT_ANNUITY'] / application_train_new['AMT_CREDIT']\napplication_train_new['DAYS_EMPLOYED_PERCENT'] = application_train_new['DAYS_EMPLOYED'] / application_train_new['DAYS_BIRTH']\napplication_train_new['GOOD_PRICE_PERCENT'] = application_train_new['AMT_GOODS_PRICE'] / application_train_new['AMT_INCOME_TOTAL']\napplication_train_new['CHILDREN_PERCENT'] = application_train_new['CNT_CHILDREN'] / application_train_new['CNT_FAM_MEMBERS']\napplication_train_new['OBS_30_CNT_SOCIAL_CIRCLE_REGION'] = application_train_new.OBS_30_CNT_SOCIAL_CIRCLE * application_train_new.REGION_POPULATION_RELATIVE\napplication_train_new['DEF_30_CNT_SOCIAL_CIRCLE_REGION'] = application_train_new.DEF_30_CNT_SOCIAL_CIRCLE * application_train_new.REGION_POPULATION_RELATIVE\napplication_train_new['OBS_60_CNT_SOCIAL_CIRCLE_REGION'] = application_train_new.OBS_60_CNT_SOCIAL_CIRCLE * application_train_new.REGION_POPULATION_RELATIVE\napplication_train_new['DEF_60_CNT_SOCIAL_CIRCLE_REGION'] = application_train_new.DEF_60_CNT_SOCIAL_CIRCLE * application_train_new.REGION_POPULATION_RELATIVE\n\napplication_test_new['INCOME_PER_PERSON'] = application_train['AMT_INCOME_TOTAL'] / application_train['CNT_FAM_MEMBERS']\napplication_test_new['CREDIT_INCOME_PERCENT'] = application_test_new['AMT_CREDIT'] / application_test_new['AMT_INCOME_TOTAL']\napplication_test_new['ANNUITY_INCOME_PERCENT'] = application_test_new['AMT_ANNUITY'] / application_test_new['AMT_INCOME_TOTAL']\napplication_test_new['CREDIT_TERM'] = application_test_new['AMT_ANNUITY'] / application_test_new['AMT_CREDIT']\napplication_test_new['DAYS_EMPLOYED_PERCENT'] = application_test_new['DAYS_EMPLOYED'] / application_test_new['DAYS_BIRTH']\napplication_test_new['GOOD_PRICE_PERCENT'] = application_test_new['AMT_GOODS_PRICE'] / application_test_new['AMT_INCOME_TOTAL']\napplication_test_new['CHILDREN_PERCENT'] = application_test_new['CNT_CHILDREN'] / application_test_new['CNT_FAM_MEMBERS']\napplication_test_new['OBS_30_CNT_SOCIAL_CIRCLE_REGION'] = application_test_new.OBS_30_CNT_SOCIAL_CIRCLE * application_test_new.REGION_POPULATION_RELATIVE\napplication_test_new['DEF_30_CNT_SOCIAL_CIRCLE_REGION'] = application_test_new.DEF_30_CNT_SOCIAL_CIRCLE * application_test_new.REGION_POPULATION_RELATIVE\napplication_test_new['OBS_60_CNT_SOCIAL_CIRCLE_REGION'] = application_test_new.OBS_60_CNT_SOCIAL_CIRCLE * application_test_new.REGION_POPULATION_RELATIVE\napplication_test_new['DEF_60_CNT_SOCIAL_CIRCLE_REGION'] = application_test_new.DEF_60_CNT_SOCIAL_CIRCLE * application_test_new.REGION_POPULATION_RELATIVE","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"612452d11442d54aed9b5f6389c60d10a061f94b"},"cell_type":"markdown","source":"After creating these features, we need to check missing and infinite values for them again."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"352b394013e77ff86f83a097139570a049d74d5a"},"cell_type":"code","source":"application_train_new.replace([np.inf,-np.inf],np.nan,inplace=True)\napplication_train_new.isnull().sum().tail(11)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"f5a88ca4aa78402dfef2c27949210d99f82fd901"},"cell_type":"code","source":"application_test_new.replace([np.inf,-np.inf],np.nan,inplace=True)\napplication_test_new.isnull().sum().tail(11)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44991d967385d330040e7808eb7cc103a7140c0d"},"cell_type":"markdown","source":"# 4) Feature Importance and Target Variable Analysis\n\nAfter gather and clean all features, now we can start to determine best features that explain target variable. We are going to use RandomForest and Logistic Regression to determine these features. After implementation these alghoritms, we select for these alghoritms best to features and look distribution of target variable.\n\n## 4.1) RandomForestClassifier for Feature Importance"},{"metadata":{"trusted":true,"_uuid":"e1f96da28c60bf0682512c96604e112b5b18ab48"},"cell_type":"code","source":"a = application_train_new.columns.difference(application_test_new.columns)\na= a.drop('TARGET')\n\napplication_train_new.drop(a,inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"85050b09f80a94253677d62d4e8389877c445b90"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\n\nscalar = StandardScaler()\nX = application_train_new.drop('TARGET',axis=1)\ny = application_train_new.TARGET\nX_scaled = scalar.fit_transform(X)\n\nrf = RandomForestClassifier(n_jobs=-1,n_estimators=30)\nrf.fit(X_scaled,y)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"7ec6008fa9e01155312002843ebf10121e3bdb38"},"cell_type":"code","source":"result_rf = pd.DataFrame()\nresult_rf['Features'] = X.columns\nresult_rf ['Values'] = rf.feature_importances_\nresult_rf.sort_values('Values',inplace=True, ascending = False)\nresult_rf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e73a2a1a55b2d82778de1c668fbe10e7e128cfbc"},"cell_type":"markdown","source":"We already look realation between EXTRA_SOURCE and Target variable."},{"metadata":{"_uuid":"51016367a4aa69c7279ac33fac834962038b70de"},"cell_type":"markdown","source":"## 4.2) Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"ef55e905016bac4eb5d98a770042146463275e2e"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(X_scaled,y)\na = lr.coef_[0]\ncoef = pd.Series(a, index=X.columns)\ncoef.sort_values(inplace=True)\n\ncoef.tail(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f977ed4df90c75c109f7a9cc579b3de566c50cd"},"cell_type":"markdown","source":"### 4.2.1) DAYS_ID_PUBLISH"},{"metadata":{"trusted":true,"_uuid":"0038556448cf6a470ffc6fed4b1705e634767cf8"},"cell_type":"code","source":"plt.figure(figsize = (6, 4))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(application_train.loc[application_train['TARGET'] == 0, 'DAYS_ID_PUBLISH'], label = 'target == 0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(application_train.loc[application_train['TARGET'] == 1, 'DAYS_ID_PUBLISH'], label = 'target == 1')\n\n# Labeling of plot\nplt.xlabel('DAYS_ID_PUBLISH'); plt.ylabel('Density'); plt.title('Distribution DAYS_ID_PUBLISH');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"715aefc0fe3d2959e51d72ff24482c6815ebe614"},"cell_type":"markdown","source":"### 4.2.2) DAYS_LAST_PHONE_CHANGE"},{"metadata":{"trusted":true,"_uuid":"e9457ba64c9137a27a2b3a52dcc88e38e0ba8237"},"cell_type":"code","source":"plt.figure(figsize = (6, 4))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(application_train.loc[application_train['TARGET'] == 0, 'DAYS_LAST_PHONE_CHANGE'], label = 'target == 0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(application_train.loc[application_train['TARGET'] == 1, 'DAYS_LAST_PHONE_CHANGE'], label = 'target == 1')\n\n# Labeling of plot\nplt.xlabel('DAYS_LAST_PHONE_CHANGE'); plt.ylabel('Density'); plt.title('Distribution DAYS_LAST_PHONE_CHANGE');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24394276af235871cf42d8f92cb68c7fde227cd8"},"cell_type":"markdown","source":"# 5) Models Implementation and Feature Elimination\n\nNow our dataset is ready for build machine learning models. We use Principal Component Analysis and Recursive Feature Elimination techniques for feature elimination. PCA take best features and combine them to gather best explained features. Also we implement boostrap sampling because our dataset is rare event. There is few ones in dataset. We try to increase number of the ones by using boostrap sampling. We are going to implement Ridge Classifier, Logistic Regression, Random Forest Classifier, Linear Discriminant Analysis and Quadratic Discriminant Analysis alghoritms. We select best alghoritm or combine them for better results.\n\n## 5.1) Principal Component Analysis without Boostrap Sampling"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"d1e8ce00801e8dede2c1a00a0c17b76823d40d34"},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import ElasticNet, LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nimport datetime\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\nresult_df = pd.DataFrame(data=np.ones(shape=(5,9)),columns=['Model','TruePositive','FalsePositive','FalseNegative','TrueNegative','Accuracy'\n                                                         ,'Precision','Recall','Specifity'])\n\npca = PCA(n_components = 100)\npca_X_train = pd.DataFrame(pca.fit_transform(X_train))\npca_X_train.index = X_train.index\n\npca_X_test = pd.DataFrame(pca.transform(X_test))\npca_X_test.index = X_test.index\n\nclf_1 = RidgeClassifier()\nclf_1.fit(pca_X_train, y_train)\ntn, fn,fp, tp = confusion_matrix(y_test,clf_1.predict(pca_X_test)).ravel()\nresult_df.loc[0,'Model'] = str(clf_1).split('(')[0]\nresult_df.loc[0,'TruePositive'] = tp\nresult_df.loc[0,'FalsePositive'] = fp\nresult_df.loc[0,'FalseNegative'] = fn\nresult_df.loc[0,'TrueNegative'] = tn\nresult_df.loc[0,'Accuracy'] = (tp+tn)/(tp+tn+fp+fn)\nresult_df.loc[0,'Precision'] = tp/(tp+fp)\nresult_df.loc[0,'Recall'] = tp / (tp+fn)\nresult_df.loc[0,'Specifity'] = tn / (tn+fp)\nprint(datetime.datetime.now())\n\nclf_4 = LogisticRegression(C=0.05)\nclf_4.fit(pca_X_train, y_train)\ntn, fn,fp, tp = confusion_matrix(y_test,clf_4.predict(pca_X_test)).ravel()\nresult_df.loc[1,'Model'] = str(clf_4).split('(')[0]\nresult_df.loc[1,'TruePositive'] = tp\nresult_df.loc[1,'FalsePositive'] = fp\nresult_df.loc[1,'FalseNegative'] = fn\nresult_df.loc[1,'TrueNegative'] = tn\nresult_df.loc[1,'Accuracy'] = (tp+tn)/(tp+tn+fp+fn)\nresult_df.loc[1,'Precision'] = tp/(tp+fp)\nresult_df.loc[1,'Recall'] = tp / (tp+fn)\nresult_df.loc[1,'Specifity'] = tn / (tn+fp)\nprint(datetime.datetime.now())\n\nclf_5 = RandomForestClassifier(n_jobs=-1)\nclf_5.fit(pca_X_train,y_train)\ntn, fn,fp, tp = confusion_matrix(y_test,clf_5.predict(pca_X_test)).ravel()\nresult_df.loc[2,'Model'] = str(clf_5).split('(')[0]\nresult_df.loc[2,'TruePositive'] = tp\nresult_df.loc[2,'FalsePositive'] = fp\nresult_df.loc[2,'FalseNegative'] = fn\nresult_df.loc[2,'TrueNegative'] = tn\nresult_df.loc[2,'Accuracy'] = (tp+tn)/(tp+tn+fp+fn)\nresult_df.loc[2,'Precision'] = tp/(tp+fp)\nresult_df.loc[2,'Recall'] = tp / (tp+fn)\nresult_df.loc[2,'Specifity'] = tn / (tn+fp)\nprint(datetime.datetime.now())\n\nclf_6 = LinearDiscriminantAnalysis()\nclf_6.fit(pca_X_train, y_train)\ntn, fn,fp, tp= confusion_matrix(y_test,clf_6.predict(pca_X_test)).ravel()\nresult_df.loc[3,'Model'] = str(clf_6).split('(')[0]\nresult_df.loc[3,'TruePositive'] = tp\nresult_df.loc[3,'FalsePositive'] = fp\nresult_df.loc[3,'FalseNegative'] = fn\nresult_df.loc[3,'TrueNegative'] = tn\nresult_df.loc[3,'Accuracy'] = (tp+tn)/(tp+tn+fp+fn)\nresult_df.loc[3,'Precision'] = tp/(tp+fp)\nresult_df.loc[3,'Recall'] = tp / (tp+fn)\nresult_df.loc[3,'Specifity'] = tn / (tn+fp)\nprint(datetime.datetime.now())\n\nclf_7 = QuadraticDiscriminantAnalysis()\nclf_7.fit(pca_X_train,y_train)\ntn, fn,fp, tp = confusion_matrix(y_test,clf_7.predict(pca_X_test)).ravel()\nresult_df.loc[4,'Model'] = str(clf_7).split('(')[0]\nresult_df.loc[4,'TruePositive'] = tp\nresult_df.loc[4,'FalsePositive'] = fp\nresult_df.loc[4,'FalseNegative'] = fn\nresult_df.loc[4,'TrueNegative'] = tn\nresult_df.loc[4,'Accuracy'] = (tp+tn)/(tp+tn+fp+fn)\nresult_df.loc[4,'Precision'] = tp/(tp+fp)\nresult_df.loc[4,'Recall'] = tp / (tp+fn)\nresult_df.loc[4,'Specifity'] = tn / (tn+fp)\nprint(datetime.datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"28fa57c56d096037bcc0343637cdc8c2d581de5f"},"cell_type":"code","source":"result_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8ec4b4beddc6957ac0edfcb7b9a48cf735ca3b3"},"cell_type":"markdown","source":"## 5.2) Principal Component Analysis with Boostrap Resampling"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"7e8dd90ffbe60f4a866d0579d3d11f7016d4aefc"},"cell_type":"code","source":"from sklearn.utils import resample\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\ndf7 = application_train_new.copy()\ndf7.drop('TARGET',inplace=True,axis=1)\ndf7['TARGET'] = application_train_new['TARGET']\n\nvalues = df7[df7.TARGET==1].values\ndf8 = df7.copy()\n# configure bootstrap\nn_iterations = 10\nn_size = int(len(df7[df7.TARGET==1]) * 0.001)\n# run bootstrap\nfor i in range(n_iterations):\n    # prepare train and test sets\n    train = resample(values, n_samples=n_size)\n    test = np.array([a for a in values if a.tolist() not in train.tolist()])\n    test_df = pd.DataFrame(test, columns = df7.columns)\n    # fit model\n    model = DecisionTreeClassifier()\n    model.fit(train[:,:-1], train[:,-1])\n    # evaluate model\n    predictions = model.predict(test[:,:-1])\n    score = accuracy_score(test[:,-1], predictions)\n    if(score >0.9):\n        df8 = pd.concat([df8,test_df])\n        df8.reset_index(drop=True, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"ab31608d1feae23c28a3f2e700a02a2865602b62"},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import ElasticNet, LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nimport datetime\n\nX_train, X_test, y_train, y_test = train_test_split(df8.iloc[:,:-1],df8.iloc[:,-1],test_size=0.2,random_state=42)\nresult_df_boostrap = pd.DataFrame(data=np.ones(shape=(5,9)),columns=['Model','TruePositive','FalsePositive','FalseNegative','TrueNegative','Accuracy'\n                                                         ,'Precision','Recall','Specifity'])\n\nscalar = StandardScaler()\nX_train_scaled = scalar.fit_transform(X_train)\nX_test_scaled = scalar.transform(X_test)\n\npca = PCA(n_components = 100)\npca_X_train = pd.DataFrame(pca.fit_transform(X_train_scaled))\npca_X_train.index = X_train.index\n\npca_X_test = pd.DataFrame(pca.transform(X_test_scaled))\npca_X_test.index = X_test.index\n\nclf_1 = RidgeClassifier()\nclf_1.fit(pca_X_train, y_train)\ntn, fn,fp, tp = confusion_matrix(y_test,clf_1.predict(pca_X_test)).ravel()\nresult_df_boostrap.loc[0,'Model'] = str(clf_1).split('(')[0]\nresult_df_boostrap.loc[0,'TruePositive'] = tp\nresult_df_boostrap.loc[0,'FalsePositive'] = fp\nresult_df_boostrap.loc[0,'FalseNegative'] = fn\nresult_df_boostrap.loc[0,'TrueNegative'] = tn\nresult_df_boostrap.loc[0,'Accuracy'] = (tp+tn)/(tp+tn+fp+fn)\nresult_df_boostrap.loc[0,'Precision'] = tp/(tp+fp)\nresult_df_boostrap.loc[0,'Recall'] = tp / (tp+fn)\nresult_df_boostrap.loc[0,'Specifity'] = tn / (tn+fp)\nprint(datetime.datetime.now())\n\n\nclf_4 = LogisticRegression(C=0.05)\nclf_4.fit(pca_X_train, y_train)\ntn, fn,fp, tp = confusion_matrix(y_test,clf_4.predict(pca_X_test)).ravel()\nresult_df_boostrap.loc[1,'Model'] = str(clf_4).split('(')[0]\nresult_df_boostrap.loc[1,'TruePositive'] = tp\nresult_df_boostrap.loc[1,'FalsePositive'] = fp\nresult_df_boostrap.loc[1,'FalseNegative'] = fn\nresult_df_boostrap.loc[1,'TrueNegative'] = tn\nresult_df_boostrap.loc[1,'Accuracy'] = (tp+tn)/(tp+tn+fp+fn)\nresult_df_boostrap.loc[1,'Precision'] = tp/(tp+fp)\nresult_df_boostrap.loc[1,'Recall'] = tp / (tp+fn)\nresult_df_boostrap.loc[1,'Specifity'] = tn / (tn+fp)\nprint(datetime.datetime.now())\n\nclf_5 = RandomForestClassifier(n_jobs=-1)\nclf_5.fit(pca_X_train,y_train)\ntn, fn,fp, tp = confusion_matrix(y_test,clf_5.predict(pca_X_test)).ravel()\nresult_df_boostrap.loc[2,'Model'] = str(clf_5).split('(')[0]\nresult_df_boostrap.loc[2,'TruePositive'] = tp\nresult_df_boostrap.loc[2,'FalsePositive'] = fp\nresult_df_boostrap.loc[2,'FalseNegative'] = fn\nresult_df_boostrap.loc[2,'TrueNegative'] = tn\nresult_df_boostrap.loc[2,'Accuracy'] = (tp+tn)/(tp+tn+fp+fn)\nresult_df_boostrap.loc[2,'Precision'] = tp/(tp+fp)\nresult_df_boostrap.loc[2,'Recall'] = tp / (tp+fn)\nresult_df_boostrap.loc[2,'Specifity'] = tn / (tn+fp)\nprint(datetime.datetime.now())\n\nclf_6 = LinearDiscriminantAnalysis()\nclf_6.fit(pca_X_train, y_train)\ntn, fn,fp, tp= confusion_matrix(y_test,clf_6.predict(pca_X_test)).ravel()\nresult_df_boostrap.loc[3,'Model'] = str(clf_6).split('(')[0]\nresult_df_boostrap.loc[3,'TruePositive'] = tp\nresult_df_boostrap.loc[3,'FalsePositive'] = fp\nresult_df_boostrap.loc[3,'FalseNegative'] = fn\nresult_df_boostrap.loc[3,'TrueNegative'] = tn\nresult_df_boostrap.loc[3,'Accuracy'] = (tp+tn)/(tp+tn+fp+fn)\nresult_df_boostrap.loc[3,'Precision'] = tp/(tp+fp)\nresult_df_boostrap.loc[3,'Recall'] = tp / (tp+fn)\nresult_df_boostrap.loc[3,'Specifity'] = tn / (tn+fp)\nprint(datetime.datetime.now())\n\nclf_7 = QuadraticDiscriminantAnalysis()\nclf_7.fit(pca_X_train,y_train)\ntn, fn,fp, tp = confusion_matrix(y_test,clf_7.predict(pca_X_test)).ravel()\nresult_df_boostrap.loc[4,'Model'] = str(clf_7).split('(')[0]\nresult_df_boostrap.loc[4,'TruePositive'] = tp\nresult_df_boostrap.loc[4,'FalsePositive'] = fp\nresult_df_boostrap.loc[4,'FalseNegative'] = fn\nresult_df_boostrap.loc[4,'TrueNegative'] = tn\nresult_df_boostrap.loc[4,'Accuracy'] = (tp+tn)/(tp+tn+fp+fn)\nresult_df_boostrap.loc[4,'Precision'] = tp/(tp+fp)\nresult_df_boostrap.loc[4,'Recall'] = tp / (tp+fn)\nresult_df_boostrap.loc[4,'Specifity'] = tn / (tn+fp)\nprint(datetime.datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"f62e7643b5f56ba0a0575dfe120611c8c2d98a44"},"cell_type":"code","source":"result_df_boostrap","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d53f272e5f1ddacee4998c6658bd1e582e273a21"},"cell_type":"markdown","source":"# 6) Conclusion\n\nAccording to these result our best model is Random Forest Classifier by using boostrap resampling and PCA for dimesonality reduction."},{"metadata":{"trusted":true,"_uuid":"c0fd1fbbb4b97070d295fee890492da6a428d679"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.5"}},"nbformat":4,"nbformat_minor":1}