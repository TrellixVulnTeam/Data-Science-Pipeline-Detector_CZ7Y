{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport random\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder, FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\nfrom sklearn.metrics import make_scorer, log_loss, f1_score, roc_auc_score, average_precision_score, \\\n    precision_recall_curve, roc_curve, confusion_matrix\n\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 220)\npd.set_option('display.max_colwidth', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Set seed"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_seed = 100\nrandom.seed(random_seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_folder = '/kaggle/input/home-credit-default-risk/'\n\ndf_desc = pd.read_csv(input_folder + 'HomeCredit_columns_description.csv', encoding='cp1252')\ndf_app_train = pd.read_csv(input_folder + 'application_train.csv')\ndf_app_test = pd.read_csv(input_folder + 'application_test.csv')\ndf_pos_bal = pd.read_csv(input_folder + 'POS_CASH_balance.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## List column descriptions"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_desc[['Table', 'Row', 'Description', 'Special']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inspect application_train"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_app_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Percentage of positive target in training data:', round(100 * df_app_train.TARGET.sum() / len(df_app_train), 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By inspection, it seems the features can be divided into categorical, ordinal and numerical features in roughly the following way:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_app_train.NAME_EDUCATION_TYPE.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recurring_cat_cols = [col for col in df_app_train.columns if col[:4] in ['NAME', 'FLAG', 'REG_', 'LIVE']]\ncat_cols = ['CODE_GENDER', 'OCCUPATION_TYPE', 'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY', 'WEEKDAY_APPR_PROCESS_START',\n            'ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE', \n            'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON',\n            'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR'] + recurring_cat_cols\n\nnum_cols = [col for col in df_app_train.columns if col not in cat_cols + ['SK_ID_CURR', 'TARGET']]\n\n# Features that don't seem too important:\ndrop_cols = ['WEEKDAY_APPR_PROCESS_START', 'HOUR_APPR_PROCESS_START', 'WALLSMATERIAL_MODE'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of categorical features:', len(cat_cols))\nprint('Number of numerical features:', len(num_cols))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Are there any obvious outliers in the numerical data? Should we consider transforming some of the numerical features?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_app_train[num_cols].hist(bins=20, figsize=(50, 50))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Min value in AMT_INCOME_TOTAL:', df_app_train['AMT_INCOME_TOTAL'].min())\nprint('Min value in AMT_CREDIT:', df_app_train['AMT_CREDIT'].min())\nprint('Min value in AMT_ANNUITY:', df_app_train['AMT_ANNUITY'].min())\nprint('Min value in AMT_GOODS_PRICE:', df_app_train['AMT_GOODS_PRICE'].min())\nprint('Max values in DAYS_EMPLOYED less than 350k:', df_app_train[df_app_train['DAYS_EMPLOYED'] < 3.5*10**5].DAYS_EMPLOYED.max())\nprint('Min value in OBS_30_CNT_SOCIAL_CIRCLE:', df_app_train['OBS_30_CNT_SOCIAL_CIRCLE'].min())\nprint('Min value in DEF_30_CNT_SOCIAL_CIRCLE:', df_app_train['DEF_30_CNT_SOCIAL_CIRCLE'].min())\nprint('Min value in OBS_60_CNT_SOCIAL_CIRCLE:', df_app_train['OBS_60_CNT_SOCIAL_CIRCLE'].min())\nprint('Min value in DEF_60_CNT_SOCIAL_CIRCLE:', df_app_train['DEF_60_CNT_SOCIAL_CIRCLE'].min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_app_train[df_app_train['DAYS_EMPLOYED'] < 3.5*10**5].DAYS_EMPLOYED.plot(kind='hist', bins=20)\nplt.title('Distribution of DAYS_EMPLOYED less than 350k')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some observations:  \n- There are definitely some outliers in the DAYS_EMPLOYED feature, as values > 350k would mean having worked for ~1k years. Also, when these values are removed, there are no values greater than zero. This indicates that these values should, like many of the other features relative to the time of applying, be negative.\n- Some columns look like they they are feasible for log transformation, for example: AMT_INCOME_TOTAL, AMT_CREDIT, AMT_ANNUITY and AMT_GOODS_PRICE.\n- Other features might be suitable for log(x + 1)-transformations as these include zeros. For example: DAYS_EMPLoYED (capped at zero to remove outliers), ...SOCIAL_CIRCLE features and many of the features representing statistics on housing.\n- There is sudden spike in values at ~65 years (?) for the OWN_CAR_AGE feature. We can probably remove these values."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['AMT_INCOME_TOTAL', 'AMT_CREDIT','AMT_ANNUITY', 'AMT_GOODS_PRICE']:  \n    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n    ax = ax.flatten()\n    \n    df_app_train[col].plot(kind='hist', bins=100, ax=ax[0])\n    ax[0].set_title('Original ' + col)\n    \n    try:\n        df_app_train[col].apply(np.log).plot(kind='hist', color='r', bins=75, ax=ax[1])\n        ax[1].set_title('Log-transformed ' + col)\n    except ValueError:\n        print('Feature includes zero(s):' + col)\n        \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inspect POS_CASH_balance"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pos_bal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_desc[df_desc.Table == 'POS_CASH_balance.csv'][['Row', 'Description', 'Special']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pos_bal[df_pos_bal.SK_DPD_DEF != 0].sort_values(by=['SK_ID_CURR', 'MONTHS_BALANCE']).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pos_bal[df_pos_bal.SK_ID_CURR == 182943].sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE']).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Build feature pipeline and split data"},{"metadata":{"trusted":true},"cell_type":"code","source":"class PreProcessor(BaseEstimator, TransformerMixin):\n    \n    # Transformer which currently only thresholds specified features \n    \n    def __init__(self, thresh_feature_names: list, thresholds: list):\n        # Each feature has one (upper) threshold\n        self.thresh_feature_names = thresh_feature_names\n        self.thresholds = thresholds\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        \n        X_t = X.copy()\n        \n        for col, thresh in zip(self.thresh_feature_names, self.thresholds):\n            X_t.loc[X_t[col] > thresh, col] = np.nan\n        \n        return X_t\n    \nclass CatEncoderNan(BaseEstimator, TransformerMixin):\n    \n    # Encode (nominal) categorical features while ignoring NaNs\n    \n    def __init__(self, features: list):\n        self.features = features\n        self.encoder_dict = dict()\n        for feat in features:\n            self.encoder_dict[feat] = LabelEncoder()\n    \n    def fit(self, X, y=None):\n        for feat in self.features:\n            self.encoder_dict[feat].fit(X[~X[feat].isna()][feat])\n        return self\n    \n    def transform(self, X, y=None):\n        X_t = X.copy()\n        for feat in self.features:\n            X_t.loc[~X[feat].isna(), feat] = self.encoder_dict[feat].transform(X[~X[feat].isna()][feat])\n\n        return X_t\n    \nclass FeatureGeneratorApplication(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        \n        X['CREDIT_LENGTH'] = X['AMT_CREDIT'] / X['AMT_ANNUITY']\n        \n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thresh_feats = ['DAYS_EMPLOYED']\nthreshs = [0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_transform_cols = ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE']\n\ncol_tf = ColumnTransformer(\n    [\n        ('num_log_tf', FunctionTransformer(func=np.log), log_transform_cols),\n        ('cat_encoder', CatEncoderNan(cat_cols), cat_cols)\n    ],\n    remainder='passthrough'\n)\n\nfeature_pipeline = Pipeline([\n    ('preprocessor', PreProcessor(thresh_feats, threshs)),\n    ('feature_generator', FeatureGeneratorApplication()),\n    ('col_tf', col_tf)\n])\n\noutput_feature_names = log_transform_cols + cat_cols + [col for col in df_app_train if col not in (log_transform_cols + cat_cols)] + \\\n    ['CREDIT_LENGTH']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.DataFrame(feature_pipeline.fit_transform(df_app_train), columns=output_feature_names, dtype='float32')\ndf_train[cat_cols] = df_train[cat_cols].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameter tuning by cross-validation"},{"metadata":{},"cell_type":"markdown","source":"### Cross-validation with early stopping per fold"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train = df_train.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n# y_train = df_train.TARGET","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# %%time\n\n# num_iters = 3\n# num_splits = 5\n\n# param_space = {\n#     'max_depth': list(range(2, 63)) + [None],\n#     'num_leaves': range(7, 4096),\n#     'subsample': [0.4, 1],\n#     'colsample_bytree': [0.4, 1],\n#     'reg_lambda': [0, 1 , 2],\n#     'scale_pos_weight': [1, 6, 12]\n# }\n\n# max_depth = random.choices(param_space['max_depth'], k=num_iters)\n# num_leaves = random.choices(param_space['num_leaves'], k=num_iters)\n# subsample = random.choices(param_space['subsample'], k=num_iters)\n# colsample_by_tree = random.choices(param_space['colsample_bytree'], k=num_iters)\n# reg_lambda = random.choices(param_space['reg_lambda'], k=num_iters)\n# scale_pos_weight = random.choices(param_space['scale_pos_weight'], k=num_iters)\n    \n# cv = KFold(n_splits=num_splits, shuffle=True, random_state=random_seed)\n\n# scores_per_iter = list()\n# for i in range(num_iters):\n#     scores_per_iter.append(list())\n    \n# for train_index, valid_index in cv.split(X_train):\n\n#     X_train_cv, X_valid_cv = X_train.iloc[train_index], X_train.iloc[valid_index]\n#     y_train_cv, y_valid_cv = y_train[train_index], y_train[valid_index]\n    \n#     for i in range(num_iters):\n        \n#         train_set_cv = lgb.Dataset(X_train_cv, y_train_cv, categorical_feature=cat_cols)\n#         valid_set_cv = lgb.Dataset(X_valid_cv, y_valid_cv, categorical_feature=cat_cols)\n        \n#         estimator = lgb.train(params={'metric': 'auc',\n#                                       'num_iterations': 99999,\n#                                       'max_depth': max_depth[i],\n#                                       'num_leaves': num_leaves[i],\n#                                       'subsample': subsample[i],\n#                                       'colsample_by_tree': colsample_by_tree[i],\n#                                       'reg_lambda': reg_lambda[i],\n#                                       'scale_pos_weight': scale_pos_weight[i]},\n#                               train_set=train_set_cv,\n#                               valid_sets=[train_set_cv, valid_set_cv],\n#                               valid_names=['training', 'validation'],\n#                               early_stopping_rounds=20,\n#                               verbose_eval=100)\n        \n#         auc_valid_score = estimator.best_score['validation']['auc']\n#         scores_per_iter[i].append(auc_valid_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mean_scores = list()\n# std_scores = list()\n# for i in range(num_iters):\n#     mean_scores.append(np.mean(scores_per_iter[i]))\n#     std_scores.append(np.std(scores_per_iter[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mean_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# std_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### scikit-learn RandomizedSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[cat_cols] = df_train[cat_cols].astype('category')\n\nX_train = df_train.drop(['SK_ID_CURR', 'TARGET'], axis=1)\ny_train = df_train.TARGET","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_iters = 10\nnum_splits = 5\n\nparam_space = {\n    'n_estimators': range(50, 300, 50),\n    'max_depth': range(3, 9, 2),\n    'subsample': [0.4, 1],\n    'colsample_bytree': [0.4, 1],\n    'reg_lambda': [0, 1 , 2],\n    'scale_pos_weight': [1, 6, 12]\n}\n\ncv = KFold(n_splits=num_splits, shuffle=True, random_state=random_seed)\n\nestimator = lgb.LGBMClassifier(random_state=random_seed)\n\nrandom_search_cv = RandomizedSearchCV(estimator=estimator,\n                                      param_distributions=param_space,\n                                      n_iter=num_iters,\n                                      scoring={'f1_score': make_scorer(f1_score),\n                                               'roc_auc': make_scorer(roc_auc_score)},\n                                      n_jobs=-1,\n                                      cv=cv,\n                                      refit='roc_auc',\n                                      verbose=20,\n                                      random_state=random_seed,\n                                      return_train_score=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nrandom_search_cv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search_cv.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train with validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid  = train_test_split(df_train.drop(['SK_ID_CURR', 'TARGET'], axis=1),\n                                                       df_train.TARGET,\n                                                       test_size=0.2,\n                                                       random_state=random_seed,\n                                                       shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search_cv.best_params_['n_estimators'] = 9999\n\nestimator = lgb.LGBMClassifier(objective='binary',\n                               **random_search_cv.best_params_,\n                               random_state=random_seed)\nestimator.fit(X_train, y_train,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_names=['train', 'valid'],\n              eval_metric='auc',\n              verbose=25,\n              callbacks=[lgb.early_stopping(stopping_rounds=200,\n                                            first_metric_only=True)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_thresh = 0.5\n\ny_pred_proba = estimator.predict_proba(X_valid)[:, 1]\n\nprec, rec, _ = precision_recall_curve(y_valid, y_pred_proba)\nfpr, tpr, _ = roc_curve(y_valid, y_pred_proba)\n\nfig, ax = plt.subplots(1, 3, figsize=(18, 6))\nax = ax.flatten()\n\nax[0].plot(rec, prec)\nax[0].set_title('Average precision on validation set: ' + str(round(average_precision_score(y_valid, y_pred_proba), 2)),\n                fontsize=11, weight='bold')\nax[0].set_xlabel('Recall')\nax[0].set_ylabel('Precision')\nax[0].grid()\n\nax[1].plot(fpr, tpr)\nax[1].set_title('ROC-AUC on validation set: ' + str(round(roc_auc_score(y_valid, y_pred_proba), 2)),\n                fontsize=11, weight='bold')\nax[1].set_xlabel('False positive rate')\nax[1].set_ylabel('True positive rate')\nax[1].grid()\n\ncm = confusion_matrix(y_valid, [1 if pred >= class_thresh else 0 for pred in y_pred_proba])\nsns.heatmap(cm, ax=ax[2], cmap='cividis', annot=True, fmt='d', annot_kws={'size': 11, 'weight': 'bold'}, cbar=False)\nax[2].set_title('Confusion matrix, threshold: ' + str(100 * class_thresh) + '%', fontsize=11, weight='bold')\nplt.yticks(rotation=0)\nax[2].set_xlabel('Predicted')\nax[2].set_ylabel('Truth', rotation=0)\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}