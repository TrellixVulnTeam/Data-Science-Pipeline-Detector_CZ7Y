{"cells":[{"metadata":{},"cell_type":"markdown","source":"今回は特徴量エンジニアリングまでを扱う","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"POS_CASH_balance=pd.read_csv('../input/home-credit-default-risk/POS_CASH_balance.csv',encoding='cp932')\napplication_test=pd.read_csv('../input/home-credit-default-risk/application_test.csv',encoding='cp932')\napplication_train=pd.read_csv('../input/home-credit-default-risk/application_train.csv',encoding='cp932')\nbureau=pd.read_csv('../input/home-credit-default-risk/bureau.csv',encoding='cp932')\nbureau_balance=pd.read_csv('../input/home-credit-default-risk/bureau_balance.csv',encoding='cp932')\ncredit_card_balance=pd.read_csv('../input/home-credit-default-risk/credit_card_balance.csv',encoding='cp932')\ninstallments_payments=pd.read_csv('../input/home-credit-default-risk/installments_payments.csv',encoding='cp932')\nprevious_application=pd.read_csv('../input/home-credit-default-risk/previous_application.csv',encoding='cp932')\nsample_submission=pd.read_csv('../input/home-credit-default-risk/sample_submission.csv',encoding='cp932')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_list = [application_test, application_train, bureau, bureau_balance, POS_CASH_balance, credit_card_balance, previous_application, installments_payments, sample_submission]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"今回扱う異常値はDAYS_EMPLOYEDのみ(異常値の基準わからず・異常値を見つける方法曖昧)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"application_train['DAYS_EMPLOYED_ANOM'] = application_train[\"DAYS_EMPLOYED\"] == 365243\napplication_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"testデータにも同じものがあるかもしれないので、trainと同様の処置をする","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"application_test['DAYS_EMPLOYED_ANOM'] = application_test[\"DAYS_EMPLOYED\"] == 365243\napplication_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ここでデータの欠損率を確認","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values = missing_values_table(application_train)\nmissing_values.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"欠損率60%以上は足切り\n\nどれほどの欠損率で足切りするべきかはわからず\n\nもっといいコードの書き方あったはず","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"delete_columns = ['COMMONAREA_MODE'\n, 'COMMONAREA_AVG'\n, 'COMMONAREA_MEDI'\n, 'NONLIVINGAPARTMENTS_MEDI'\n,'NONLIVINGAPARTMENTS_MODE'\n,'NONLIVINGAPARTMENTS_AVG'\n,'FONDKAPREMONT_MODE'\n,'LIVINGAPARTMENTS_MEDI'\n,'LIVINGAPARTMENTS_AVG'\n,'LIVINGAPARTMENTS_MODE'\n,'FLOORSMIN_AVG'\n,'FLOORSMIN_MEDI'\n,'FLOORSMIN_MODE'\n,'YEARS_BUILD_MEDI'\n,'YEARS_BUILD_MODE'\n,'YEARS_BUILD_AVG'\n,'OWN_CAR_AGE']\n\napplication_train.drop(delete_columns, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"足切りした特徴量以外の相関を確認する\n\n目的変数との相関が強い特徴量を挙げる","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find correlations with the target and sort\ncorrelations = application_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(6))\nprint('\\nMost Negative Correlations:\\n', correlations.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"上記の特徴量はある程度目的変数との関係がみられるため、採用する\n\nあとは、個人的に関係ありそうな特徴量を複数選ぶ\n\nAMT_INCOME_TOTAL(クライアントの収入)　NAME_HOUSING_TYPE(クライアントの住宅情報)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features=['DAYS_LAST_PHONE_CHANGE'\n,'REGION_RATING_CLIENT' \n,'REGION_RATING_CLIENT_W_CITY'\n,'DAYS_BIRTH'\n,'EXT_SOURCE_3'\n,'EXT_SOURCE_2'\n,'EXT_SOURCE_1'\n,'DAYS_EMPLOYED'\n,'FLOORSMAX_AVG'\n,'AMT_INCOME_TOTAL'\n,'NAME_HOUSING_TYPE']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"とりあえずtrainデータとtestデータを作ってみる","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = application_train[features]\ntest = application_test[features]\ndata = pd.concat([train, test], sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_df_name(df):\n    name =[x for x in globals() if globals()[x] is df][0]\n    return name","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"今回使う特徴量の中にカテゴリカル変数があるので、ラベルエンコーディングとワンホットエンコーディングを行う\n\nこの辺も少し曖昧だが、今回は特徴量のカテゴリカル変数が2個以下ならラベルエンコーディング、それ以上ならワンホットエンコーディングとする(決定木においてラベルエンコーディングは有効らしいので、LightGBMを最初から使う予定なら、メモリも節約できるので、すべてをラベルエンコーディングにしてもいいかも？)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle_count = 0\nfor col in data:\n    if data[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(data[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(data[col])\n            # Transform both training and testing data\n            data[col] = le.transform(data[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ワンホットエンコーディングはpandasのget_dummiesにより可能","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.get_dummies(data)\n\nprint('Training Features shape: ', data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"今回使う特徴量には欠損しているデータがあるので、埋めていく\n\n適切なデータの埋め方が現在曖昧なので、今回は欠損している数値を平均値で埋めた","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['EXT_SOURCE_1'].fillna(data['EXT_SOURCE_1'].median(), inplace=True)\ndata['EXT_SOURCE_3'].fillna(data['EXT_SOURCE_3'].median(), inplace=True)\ndata['FLOORSMAX_AVG'].fillna(data['FLOORSMAX_AVG'].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"最後に、トレーニングデータとテストデータを作成して特徴量エンジニアリング終了","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train=data[:len(train)]\ntest=data[len(train):]\n\ny_train = application_train['TARGET']\nX_train = train\nX_test = test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\n\ny_preds = []\nmodels = []\n\n#初期値が0で長さがトレーニングデータ分の長さの配列\noof_train = np.zeros((len(X_train),))\n#データを5個に分ける。random_state＝0は初期seed、shuffle=Trueは要素がシャッフルする\ncv = KFold(n_splits=10, shuffle=True, random_state=0)\n\n\n\nparams = {\n    'objective': 'binary',\n    'max_bin': 300,\n    'learning_rate': 0.05,\n    'num_leaves': 40\n}\n#enumerateはfold_id をindex番号としてそのまま使える\nfor fold_id, (train_index, valid_index) in enumerate(cv.split(X_train)):\n#     print(train_index, valid_index)\n    # : は全てのラベル、つまり、train_index行の全ての列ラベルを選択している\n    X_tr = X_train.loc[train_index, :]\n    X_val = X_train.loc[valid_index, :]\n    y_tr = y_train[train_index]\n    y_val = y_train[valid_index]\n    \n    #ここから機械学習に通す\n    lgb_train = lgb.Dataset(X_tr, y_tr)\n    lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n\n    model = lgb.train(\n        params, lgb_train,\n        valid_sets=[lgb_train, lgb_eval],\n        verbose_eval=10,\n        num_boost_round=1000,\n        early_stopping_rounds=10\n    )\n\n    oof_train[valid_index] = model.predict(X_val, num_iteration=model.best_iteration)\n    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n    #複数回やってるから分割ごとのモデルと予測結果保存するよ\n    y_preds.append(y_pred)\n    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_oof = (oof_train > 0.5).astype(int)\nroc_auc_score(y_train, y_pred_oof)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sample_submission\n\ny_pred = (y_pred > 0.5).astype(int)\n\nsub['TARGET'] = y_pred\nsub.to_csv(\"submission_lightgbm_optuna.csv\", index=False)\n\nsub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}