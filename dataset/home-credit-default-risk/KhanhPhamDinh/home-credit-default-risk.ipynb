{"cells":[{"metadata":{"_uuid":"d3cdd142f774389a9d1d6d80315144d224fede4a"},"cell_type":"markdown","source":"# 1. Giới thiệu\n\nHome credit là một cuộc thi phân tích dữ liệu do kaggle tổ chức. Đề tài cần giải quyết là một bài toán thuộc lớp mô hình phân loại và học có giám sát. Yêu cầu của các đội thi là xây dựng mô hình phân loại các hợp đồng tín dụng đã được gán nhãn repaid (trả nợ) và no repaid  (không trả nợ) sao cho mức độ chính xác trong phân loại là lớn nhất. Tiêu chí được sử dụng để đánh giá mô hình là chỉ số AUC, một chỉ số khá quen thuộc trong lĩnh vực scorecard. Về mặt toán học AUC chính là phần diện tích nằm dưới đường cong ROC mà giá trị của nó nằm trong khoảng từ [0, 1]. Một mô hình càng có sức mạnh phân loại tốt khi AUC càng gần 1 và trái lại. Trường hợp AUC = 0.5 kết quả mô hình bằng với việc dự báo ngẫu nhiên. AUC < 0.5 đảo ngược kết quả của mô hình bạn sẽ thu được một mô hình mạnh hơn.\n\nĐây là một bài toán thuộc lĩnh vực credit risk được sử dụng nhiều trong các tổ chức tài chính, ngân hàng. Chính vì vai trò và mức độ quan trọng của bài toán nên Home Credit thu hút được trên 7000 đội tham dự. Dữ liệu của bài toán thuộc dạng có cấu trúc (SQL) gồm nhiều bảng liên hệ với nhau bởi các primary key và foriegn key theo sơ đồ bên dưới:\n\n![image](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)\n\nCác bảng dữ liệu bao quát các khía cạnh của một hồ sơ tín dụng như:\n\n* Thông tin hồ sơ khách hàng hiện tại: Mỗi một hồ sơ được xác định bằng một khóa SK_ID_CURR. Trong thông tin hồ sơ chúng ta sẽ biết được thu nhập, số lượng người phụ thuộc, nghề nghiệp, trình độ giáo dục, ngày sinh, có sở hữu xe hay không,.... và rất nhiều các thông tin khác. Các thông tin này có thể chia thành nhóm nhân khẩu học (demographic) liên quan đến cá nhân vay và thông tin tín dụng liên quan đến tình trạng tín dụng của người vay. Các bảng application_train.csv (sử dụng để huấn luyện) và bảng application_test.csv (sử dụng để test). Về mặt cấu trúc các trường trong bảng application_train.csv và application_test.csv là giống nhau ngoại trừ biến mục tiêu TARGET được giấu ở application_test.csv để đánh giá kết quả mô hình sau cùng.\n\n* Thông tin giao dịch: Các lịch sử giao dịch và hành vi mua sắm qua thẻ tín dụng của người tiêu dùng tại home credit. Bảng POSH_CASH_balance.csv\n\n* Thông tin trả nợ: Lịch sử trả nợ của khách hàng trong quá khứ nếu họ đã có các khoản vay trước đó tại home credit. Bảng installments_payment.csv\n\n* Thông tin hồ sơ các khoản vay trước đây tại home credit. Bảng previous_application.csv\n\n* Thông tin các khoản vay tại tổ chức tài chính khác được lưu trữ tại cục tín dụng. Bảng bureau_balance.csv và bureau.csv.\n\n# 2. Tiền xử lý dữ liệu\n\nDo dữ liệu home credit được chia thành nhiều khía cạnh dữ liệu nhỏ và quản lý tại mỗi bảng riêng biệt nên ta cần thống kê các thông tin của dữ liệu theo key là ID hồ sơ và join các thông tin thu được từ mỗi bảng theo key để thu được bảng tổng hợp làm input cho mô hình. Tại bước này chúng ta cần các kĩ năng về data manipulation trên pandas. Cụ thể bạn đọc có thể tham khảo thêm hướng dẫn xử lý dữ liệu với [pandas](https://www.kaggle.com/phamdinhkhanh/gi-i-thi-u-pandas).\n\nMột điểm cần lưu ý đó là thông tin về các hồ sơ tín dụng thường không đầy đủ. Do đó chúng ta cần kiểm tra tỷ lệ các quan sát bị missing ở mỗi trường. Việc kiểm tra tỷ lệ missing giúp đánh giá mức độ đầy đủ về mặt thông tin mà một trường dữ liệu cung cấp. Khi tỷ lệ missing quá lớn, trường dữ liệu thường không đáng tin cậy trong phân loại mô hình và chúng ta cần phải loại những biến này khỏi mô hình.\n\n## 2.1. Khảo sát dữ liệu\n\nBên dưới ta sẽ kiểm tra số trường và kích thước mẫu của các tập dữ liệu train và test."},{"metadata":{"trusted":true,"_uuid":"e9760c5860ff588e82a4e98099c80736f25ce4dc"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\n\n%matplotlib inline\n\n# Checking for all file\nos.listdir('../input')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"315b0b488fb15cbd41bb2636c9cc713f711b81c1"},"cell_type":"code","source":"# Training data\napp_train = pd.read_csv('../input/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7e84e563a19e9b843a21ddfbec59bc638473998b"},"cell_type":"code","source":"# Testing data features\napp_test = pd.read_csv('../input/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"211f4d3baa7cdbfe1a384ee14f86779c3684ddce"},"cell_type":"markdown","source":"Biến TARGET của tập test đã được giấu đi để đánh giá mô hình được xây dựng từ tập train. Số lượng quan sát của train lớn gấp khoảng 6 lần train. \n\nBên dưới ta sẽ thống kê số lượng và tỷ lệ các class Repaid và Not repaid trong tập train."},{"metadata":{"trusted":false,"_uuid":"eb26e39d1c132c2a6e44e81a09d366a03338c8ae"},"cell_type":"code","source":"app_train['TARGET'].value_counts().plot.bar()\nn_group = app_train['TARGET'].value_counts()\nn_group_sum = n_group.sum()\n\nprint('Repaid: {}'.format(n_group[0]))\nprint('Not repaid: {}'.format(n_group[1]))\n\nprint('Repaid: {:.2f} {}'.format(n_group[0]/n_group_sum*100, '%'))\nprint('Not repaid: {:.2f} {}'.format(n_group[1]/n_group_sum*100, '%'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fae745f5f1c520f7d73061eec207b8320bc25381"},"cell_type":"markdown","source":"Mẫu có hiện tượng mất cân bằng khi nhóm thiểu chỉ chiếm 8.07% và nhóm đa số chiếm tới 91.93%. Mẫu mất cân bằng có thể dẫn tới một số tác hại đối với mô hình như:\n\n1. Kết quả dự báo của mô hình chỉ thiên về một class. Thậm chí trong một số trường hợp mô hình dự báo chỉ đưa ra một class duy nhất là class đa số.\n2. Dễ dàng ngộ nhận mô hình tốt do Accuracy của mô hình đối với mẫu mất cân bằng thường rất cao. Trong trường hợp này ta cần sử dụng đến các chỉ số thay thế như precision, recall, F1-Score, Kappa, ROC Curve. Về nội dung và ý nghĩa của các chỉ số này các bạn có thể tham khảo tại [đánh giá hệ thống phân lớp](https://machinelearningcoban.com/2017/08/31/evaluation/).\n\nKhi gặp hiện tượng mất cân bằng mẫu chúng ta có thể sử dụng nhiều phương pháp khác nhau để biến đổi mẫu về cân bằng. Một trong những phương pháp đó là:\n\n1.\tThu thập thêm nhiều dữ liệu cho mẫu thiểu số nếu việc thu thập thêm dữ liệu là khả thi trên thực tế.\n2.\tSử dụng các phương pháp tái chọn mẫu có lặp lại (resampling) nhằm gia tăng số lượng mẫu thiểu số.\n3.\tSử dụng kĩ thuật tạo mẫu tổng hợp cho mẫu thiểu (Synthetic Minority Over-sampling Technique - SMOTE). Thay vì lấy các quan sát lập lại, thuật toán sẽ tạo ra các quan sát tổng hợp dựa trên hai hoặc nhiều quan sát mẫu. Các mẫu được lựa chọn có sự tương đương nhau dựa trên thước đo khoảng cách. Các thuộc tính của quan sát tổng hợp sau đó sẽ được tạo ra bằng cách thêm một đại lượng ngẫu nhiên chênh lệch so với các điểm lân cận.\n4.\tThêm các hệ số phạt vào mô hình để gia tăng hàm mất mát nhiều hơn nếu dự báo sai các mẫu thiểu. \n\nBên dưới ta sẽ thống kê dữ liệu missing và các kiểu dữ liệu numeric, categorical."},{"metadata":{"trusted":false,"_uuid":"33f950baeac2319de873b339c95d2a7edb4316c3"},"cell_type":"code","source":"def summary_missing(dataset):\n    n_miss = dataset.isnull().sum()\n    n_obs = dataset.shape[0]\n    n_miss_per = n_miss/n_obs*100\n    n_miss_tbl = pd.concat([n_miss, n_miss_per], axis = 1).sort_values(1, ascending = False).round(1)\n    n_miss_tbl = n_miss_tbl[n_miss_tbl[1] != 0]\n    print('No fields: ', dataset.shape[0])\n    print('No missing fields: ', n_miss_tbl.shape[0])\n    n_miss_tbl = n_miss_tbl.rename(columns = {0:'Number mising Value', 1:'Percentage missing Value'})\n    return n_miss_tbl\n\nsummary_missing(app_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8efb1c35a3d2f8f7866ce2dba46f87e505fcc483"},"cell_type":"code","source":"def _tbl_dtype(dataset):\n    sum_dtype = pd.DataFrame(dataset.dtypes).sort_values(0).rename(columns = {0:'Data Type'})\n    return sum_dtype\n\ntable_dtype = _tbl_dtype(app_train)\ntable_dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7768d9c51ae0c9a337eb7925d356d2c484f9277e"},"cell_type":"code","source":"table_dtype['Data Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0f28241d767a045e89cd9f82951a2ac98096afae"},"cell_type":"code","source":"# Các dòng dữ liệu dạng object\napp_train.select_dtypes('object').head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35579c0e9440503874c6f51f393ab35e46abcab0"},"cell_type":"markdown","source":"Thống kê số lượng các class trong mỗi nhóm đối với dữ liệu dạng object."},{"metadata":{"trusted":false,"_uuid":"b3d114bf360bc122e747436cdee1a0b4b0dec741"},"cell_type":"code","source":"app_train.select_dtypes('object').apply(pd.Series.nunique)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2adeb0c3df0f55d9eb2fe6db40c962d980f1ecd3"},"cell_type":"markdown","source":"Vẽ biểu đồ phân phối số lượng các quan sát theo nhóm đối với dữ liệu dạng object. Đối với các biến ORGANIZATION_TYPE, OCCUPATION_TYPE ta sẽ nghiên cứu riêng do số lượng class lớn."},{"metadata":{"trusted":false,"_uuid":"3f672fcb74565260a812af80f04ee68b9941a74a"},"cell_type":"code","source":"dtypes_object = table_dtype[table_dtype['Data Type'] == 'object'].index.tolist()\ndtypes_object = [col for col in dtypes_object if col not in ['OCCUPATION_TYPE', 'ORGANIZATION_TYPE']]\n\n\ndef _plot_bar_classes(cols):\n    app_train[cols].value_counts().plot.bar()\n\nplt.figure(figsize = (20, 15))    \nfor i in range(1, 15, 1):\n    plt.subplot(5, 3, i)\n    _plot_bar_classes(dtypes_object[i-1])\n    plt.title(dtypes_object[i-1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3921c3abc59412efdfa6bb069fdf1d46872eca3a"},"cell_type":"markdown","source":"Thống kê tỷ lệ Repaid/Not Repaid theo các biến dự báo dạng object."},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"11a61e4c99cb30557e6e9843395ebd614f5f8a0f"},"cell_type":"code","source":"def _per_categorical(col):\n    tbl_per = pd.pivot_table(app_train[['TARGET', col]], index = ['TARGET'], columns = [col], aggfunc = len)\n    per_categorical = (tbl_per.iloc[0, :]/tbl_per.iloc[1, :]).sort_values(ascending = True)\n    print(per_categorical)\n    print('-------------------------------------\\n')\n    return per_categorical\n\nfor col in dtypes_object:\n    _per_categorical(col)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e5116b8a2eb94ab4de619fad37dd35a06f32489"},"cell_type":"markdown","source":"Vẽ biểu đồ tỷ lệ Repaid/Not Repaid theo các biến dự báo dạng object."},{"metadata":{"trusted":false,"_uuid":"8eba7c107799497bf050717f58997d39b44157e6"},"cell_type":"code","source":"def _plot_per_categorical(col):\n    tbl_per = pd.pivot_table(app_train[['TARGET', col]], index = ['TARGET'], columns = [col], aggfunc = len)\n    per_categorical = (tbl_per.iloc[0, :]/tbl_per.iloc[1, :]).sort_values(ascending = True)\n    per_categorical.plot.bar()\n    plt.title(col)\n    return per_categorical\n\nplt.figure(figsize = (20, 15))\ni = 0\nfor col in dtypes_object:\n    i += 1\n    plt.subplot(5, 3, i)\n    _plot_per_categorical(col)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95a3a73d138b02e4b3dc088fc4c0ab0ad2b81590"},"cell_type":"markdown","source":"Thông qua biểu đồ ta cũng hình dung được một vài biến phân loại có sự khác biệt lớn giữa tỷ lệ Repaid/Non Repaid lớn như NAME_CONTRACT_TYPE, GENDER, FLAG_OWN_REALITY do đó đây là những biến có tác động lớn đến biến mục tiêu.\n\nKiểm tra riêng cho 2 biến ORGANIZATION_TYPE, OCCUPATION_TYPE."},{"metadata":{"trusted":false,"_uuid":"bf2d4620553b9a670c902ff5a3a211a0d135ab8b"},"cell_type":"code","source":"plt.figure(figsize = (15, 7))\ni = 0\nfor col in ['ORGANIZATION_TYPE', 'OCCUPATION_TYPE']:\n    i += 1\n    plt.subplot(2, 1, i)\n    _plot_per_categorical(col)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8b1f86e133f60dec707e64f2a21c30e867c5e32"},"cell_type":"markdown","source":"## 2.2. Nhóm các đặc trưng theo tỷ lệ Repaid/Not Repaid\nĐể giảm thiểu số lượng đặc trưng (features) ta có thể nhóm những biến có tỷ lệ Repaid/Not Repaid gần bằng nhau vào một nhóm bởi những nhóm này có đặc tính gần giống nhau trong phân loại hợp đồng."},{"metadata":{"trusted":false,"_uuid":"3ea52c2d52dca6732342b420efcb9895043391a3"},"cell_type":"code","source":"for col in ['ORGANIZATION_TYPE', 'OCCUPATION_TYPE']:\n    _per_categorical(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f4a7bc0fdef83f2cb89641111f62a045a5622c64"},"cell_type":"code","source":"# Nhóm các giá trị rate gần bằng nhau vào 1 nhóm theo schedule_div.\ndef _devide_group(col, schedule_div = None, n_groups = 3, *kwargs):\n    cols = []\n    tbl_per_cat = _per_categorical(col)\n    \n    if schedule_div is None:\n        n_cats = len(tbl_per_cat)\n        n_val_incat = int(n_cats/n_groups)\n        n_odd = n_cats - n_groups*n_val_incat\n\n        for i in range(n_groups):\n            if i == n_groups - 1:\n                el = tbl_per_cat[(n_val_incat*i):(n_val_incat*(i+1)+n_odd)].index.tolist()\n            else:\n                el = tbl_per_cat[(n_val_incat*i):n_val_incat*(i+1)].index.tolist()    \n            cols.append(el)\n    else:\n        idx = 0\n        for n_cols in schedule_div:\n            el_cols = tbl_per_cat[idx:(idx+n_cols)].index.tolist()\n            cols.append(el_cols)\n            idx += n_cols\n                \n    return cols\n\ncols_OCCUPATION_TYPE = _devide_group(col = 'OCCUPATION_TYPE', schedule_div = [1, 7, 9, 1])\ncols_OCCUPATION_TYPE","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ad0a5ea80cea43b148638210bf1d037ec0147585"},"cell_type":"code","source":"cols_ORGANIZATION_TYPE = _devide_group(col = 'ORGANIZATION_TYPE')\ncols_ORGANIZATION_TYPE","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3478b9347358ac0d35861b8295bb7a9b25589335"},"cell_type":"markdown","source":"Sau khi đã nhóm các features trong một biến phân loại thành các features tổng hợp chúng ta cần cập nhật lại các trường của bảng dữ liệu theo các features mới."},{"metadata":{"trusted":false,"_uuid":"18618ce5761e89e0c742746323b4bb6ceb2fef3b"},"cell_type":"code","source":"def _map_lambda_cats(cols_list, colname, x): \n    cats = list(map(lambda x:colname + '_' + str(x), np.arange(len(cols_list)).tolist()))\n    for i in range(len(cols_ORGANIZATION_TYPE)):\n        if x in cols_list[i]:\n            return cats[i]\n        \ndef _map_cats(cols_list, colname, dataset):                    \n    return list(map(lambda x: _map_lambda_cats(cols_list, colname, x), \n                    dataset[colname]))\n\napp_train['ORGANIZATION_TYPE'] = _map_cats(cols_ORGANIZATION_TYPE, 'ORGANIZATION_TYPE', app_train)\npd.Series.unique(app_train['ORGANIZATION_TYPE'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"168f2d6bdc3a6b8c7632be0a2ab88086576e5afd"},"cell_type":"markdown","source":"Áp dụng trên tập test."},{"metadata":{"trusted":false,"_uuid":"b7778bfe1a4304967a9f553d0924e9310856090a"},"cell_type":"code","source":"app_test['ORGANIZATION_TYPE'] = _map_cats(cols_ORGANIZATION_TYPE, 'ORGANIZATION_TYPE', app_test)\npd.Series.unique(app_test['ORGANIZATION_TYPE'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"288de1172ea8f56130ed1371bd0ba6bb8636a3b5"},"cell_type":"markdown","source":"Gán lại các biến này bằng dữ liệu sau khi map."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"2e403db6797cecd5a4a6624e13e1c410af134b01"},"cell_type":"code","source":"app_train['OCCUPATION_TYPE'] = _map_cats(cols_OCCUPATION_TYPE, 'OCCUPATION_TYPE', app_train)\napp_test['OCCUPATION_TYPE'] = _map_cats(cols_OCCUPATION_TYPE, 'OCCUPATION_TYPE', app_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa47cf913b39bbe8988ff324c3d82bbd31a19c61"},"cell_type":"markdown","source":"Sau các xử lý trên, dữ liệu trên app_train và app_test sẽ được map theo các nhóm phân loại mới. Chúng ta có thể kiểm tra lại tỷ lệ Repaid/No Repaid của các nhóm biến mới như bên dưới."},{"metadata":{"trusted":false,"_uuid":"78c839a867dd6f14166df7e23fe6498173e20ff2"},"cell_type":"code","source":"i = 0\nplt.figure(figsize = (16, 8))\nfor col in ['ORGANIZATION_TYPE', 'OCCUPATION_TYPE']:\n    i += 1\n    plt.subplot(2, 1, i)\n    _plot_per_categorical(col)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7c771aaa8961398f47169bf3a9300dbdfe1ccd9"},"cell_type":"markdown","source":"Sau khi đã nhóm xong các biến category thành những features tổng hợp, chúng ta cần biến đổi các biến này sao cho chúng có thể được sử dụng trong mô hình hồi qui. Có 2 cách biến đổi chính có thể áp dụng đó là one-hot coding và tạo biến thứ bậc. Tạo biến thứ bậc thường được áp dụng khi các nhóm có sự khác biệt về mức độ chẳng hạn như xếp hạng học lực: Giỏi, Khá, Trung bình, Yếu. Trong khi đó one-hot coding sẽ biểu diễn một biến category bởi một vector sparse (các phần tử của vector là 0 hoặc 1) có độ dài bằng số lượng các nhóm sao cho đặc trưng tương ứng với giá trị của biến sẽ được gán là 1 và các đặc trưng còn lại là 0. Chúng ta có thể sử dụng hàm `get_dummies()` trong pandas để tự động biến đổi các biến category trong data frame về dạng one-hot coding."},{"metadata":{"trusted":false,"_uuid":"048a15792427f26efc9bf834a9b2b3a436259141"},"cell_type":"code","source":"app_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bacbad80820dcae353b3b7eeff8087f01438f64"},"cell_type":"markdown","source":"Kiểm tra kích thước dữ liệu sau biến đổi."},{"metadata":{"trusted":false,"_uuid":"89ca1f3403dcdc8b56d1643fa69af23e2d5f1215"},"cell_type":"code","source":"print('app_train shape: ', app_train.shape)\nprint('app_test shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74410bede9e171fa42237e480c8c380023fde628"},"cell_type":"markdown","source":"Số lượng các biến đã tăng lên do biến đổi one-hot coding, tuy nhiên có sự chênh lệch giữa tập train và test. Kiểm tra các biến ở app_train không có trong app_test."},{"metadata":{"trusted":false,"_uuid":"ceb30d62de971bee74cf803d9b8e0fa0e8167191"},"cell_type":"code","source":"for fea_name in app_train.columns:\n    if fea_name not in app_test.columns:\n        print(fea_name)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dec536c03549401d51e3ad2e4ff22ed2d1b94581"},"cell_type":"markdown","source":"Những biến này không xuất hiện trong tập test là do có một số biến không xảy ra đầy đủ các khả năng. Chúng ta cần loại bỏ những biến không có trong tập train. Chỉ dữ lại những biến có trong cả app_train và app_test:"},{"metadata":{"trusted":false,"_uuid":"019532530fad812a2f254d429b2706bf89294de6"},"cell_type":"code","source":"TARGET = app_train['TARGET']\n\n# Lệnh align theo axis = 1 sẽ lấy những trường xuất hiện đồng thời trong app_train và app_test\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n# Sau lệnh align biến TARGET bị mất, do đó ta cần gán lại biến này\napp_train['TARGET'] = TARGET\n\nprint('app_train shape: ', app_train.shape)\nprint('app_test shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7a5b6569268703e2ad3cc3517842d0d9f468fc6"},"cell_type":"markdown","source":"## 2.3. Xử lý outlier\n\nTrong một bộ dữ liệu thường có những quan sát bất thường. Nguyên nhân có thể đến từ dữ liệu bị sai định dạng, quá trình nhập liệu sai, quan sát có tính chất đặc biệt,.... Việc tra soát các dữ liệu bất thường có thể giúp khám phá ra một số tình chất của dữ liệu và đồng thời hiệu chỉnh dữ liệu trong trường hợp nhập liệu sai. Kiếm tra dữ liệu bất thường có thể được thực hiện bằng các thống kê mô tả."},{"metadata":{"trusted":false,"_uuid":"40fe48299da22ab35ff406667d4ae7ab4f72f76b"},"cell_type":"code","source":"app_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"08e9094b0656a73968abaf0979ee4d0187004651"},"cell_type":"code","source":"app_train['AMT_INCOME_TOTAL'].describe().plot.box()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df5826cac6a0ad81bbdbdb1462a459b5198a6e3d"},"cell_type":"markdown","source":"Ta nhận thấy biến thu nhập có dấu hiệu bất thường dữ liệu khi hầu hết các khoảng ngũ phân vị (quintile) của biến đều thấp quan gốc 0 ngoại trừ một vài trường hợp cao đặc biệt. Điều này cho thấy có sự chênh lệch trong mức thu nhập của những người đi vay."},{"metadata":{"trusted":false,"_uuid":"a7babe24cafd008f72f0fe8853bd00d25f906d94"},"cell_type":"code","source":"app_train['AMT_INCOME_TOTAL'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"044c9831b01791e3c8225301defab4abc4732b99"},"cell_type":"markdown","source":"Giá trị cao nhất của thu nhập là khoảng 117 triệu USD trong khi trung bình thu nhập chỉ là 168 nghìn USD. Chúng ta sẽ kiểm tra phân phối của biến TARGET theo biến AMT_INCOME_TOTAL ứng với các trường hợp Repaid và No repaid."},{"metadata":{"trusted":false,"_uuid":"9cb1a859555d8d43f039ffe9f6ad607990429321"},"cell_type":"code","source":"def _plot_density(colname):\n    plt.figure(figsize = (10, 8))\n    sns.kdeplot(app_train[colname][app_train['TARGET'] == 0], label = 'Target = 0')\n    sns.kdeplot(app_train[colname][app_train['TARGET'] == 1], label = 'Target = 1')\n    plt.xlabel(colname)\n    plt.ylabel('Density')\n    plt.title('Distribution of %s'%colname)\n\n_plot_density('AMT_INCOME_TOTAL')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b765b99d2adf1bda01ca511086cd86bb466c6d83"},"cell_type":"markdown","source":"Sử dụng phương pháp 3 sigma để điều chỉnh lại các giá trị nằm ngoài miền $[\\mu - 3\\sigma, \\mu + 3\\sigma]$ về trong miền giá trị đó. Đối với giá trị lớn hơn $\\mu+3\\sigma$ sẽ được gán bằng $\\mu+3\\sigma$ và tương tự với giá trị nhỏ hơn $\\mu - 3\\sigma$. Phương pháp này giúp điều chỉnh các outlier về trong khoảng biến thiên cho phép và làm giảm ảnh hưởng chệch gây ra bởi chúng."},{"metadata":{"trusted":false,"_uuid":"e38e46bfcc56dcbcc0ab29cc436b76f47b9a4835"},"cell_type":"code","source":"def _zoom_3sigma(col, dataset, dataset_apl):\n    '''\n    col: Tên cột dữ liệu\n    dataset: Bảng dữ liệu gốc sử dụng để tính khoảng 3 sigma\n    dataset_apl: Bảng dữ liệu mới áp dụng khoảng 3 sigma được lấy từ dataset.\n    '''\n    xs = dataset[col]\n    mu = xs.mean()\n    sigma = xs.std()\n    low =  mu - 3*sigma\n#     low =  0 if low < 0 else low\n    high = mu + 3*sigma\n    \n    def _value(x):\n        if x < low: return low\n        elif x > high: return high\n        else: return x\n    xapl = dataset_apl[col]    \n    xnew = list(map(lambda x: _value(x), xapl))\n    n_low = len([i for i in xnew if i == low])\n    n_high = len([i for i in xnew if i == high])\n    n = len(xapl)\n    print('Percentage of low: {:.2f}{}'.format(100*n_low/n, '%'))\n    print('Percentage of high: {:.2f}{}'.format(100*n_high/n, '%'))\n    print('Low value: {:.2f}'.format(low))\n    print('High value: {:.2f}'.format(high))\n    return xnew\n\n# Kiểm tra với biến FLAG_MOBIL\nx = _zoom_3sigma('FLAG_MOBIL', app_train, app_train)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ea7a6a8f1f141d18234290fb305aac99ec0edea6"},"cell_type":"code","source":"app_train.dtypes.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5ac9cb5800840bd308c2ba1a58a2534fe891b04d"},"cell_type":"code","source":"# Thống kê các giá trị khác biệt trong toàn bộ các biến.\ndef _count_unique(x):\n    return pd.Series.nunique(x)\n\ntbl_dis_val = app_train.apply(_count_unique).sort_values(ascending = False)\ntbl_dis_val[tbl_dis_val > 500]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e29614cf272f03e2a2865cb0e6d4ae4a27bdebd4"},"cell_type":"markdown","source":"Ta coi các biến có số lượng giá trị khác biệt > 500 là các biến liên tục. Áp dụng nguyên lý 3 sigma cho các biến này."},{"metadata":{"trusted":false,"_uuid":"6354fd9ab2dcd8ac71f58bd122c0d52f21878de1"},"cell_type":"code","source":"cols_3sigma = tbl_dis_val[tbl_dis_val > 500].index.tolist()\n# Loại bỏ biến key là SK_ID_CURR ra khỏi danh sách:\ncols_3sigma = cols_3sigma[1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c07fe918d2d70b0138a7c34c7aabd09c0be039ac"},"cell_type":"code","source":"# Loại bỏ các outlier bằng 3 sigma\nfor col in cols_3sigma:\n    print(col)\n    app_train[col] = _zoom_3sigma(col, app_train, app_train) \n    print('------------------------\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ea59c9cf4580b3ba54ecc2b6b34f5b3bd41064f2"},"cell_type":"code","source":"for col in cols_3sigma:\n    print(col)\n    app_test[col] = _zoom_3sigma(col, app_train, app_test) \n    print('------------------------\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5c206c28546b3b8b5ff5613a9744b4749ec893b6"},"cell_type":"code","source":"# Kiểm tra lại biến AMT_INCOME_TOTAL sau khi loại bỏ outlier\napp_train['AMT_INCOME_TOTAL'].describe().plot.box()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"65d53c32365c9d705bb56c1233a911e21127393a"},"cell_type":"code","source":"_plot_density('AMT_INCOME_TOTAL')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1e532a52c4cc0982735f31889eacfb344c09638"},"cell_type":"markdown","source":"Như vậy sau khi loại bỏ oulier, đồ thị phân phối của AMT_INCOME_TOTAL đã không còn thiên lệch. Các điểm mốc Q1, Q2, Q3, Q4 trong tứ phân vị (quintile) đã không quá nhỏ so với giá trị lớn nhất trong biểu đồ box plot như ban đầu. Biểu đồ hàm mật độ cũng cho thấy một phân phối không có các giá trị dị biệt như trước khi điều chỉnh."},{"metadata":{"_uuid":"d0a16bf484fad562f909c6bca516dab7a4060023"},"cell_type":"markdown","source":"## 2.4. Xử lý dữ liệu missing\n\nXử lý dữ liệu missing là một trong những mảng rất quan trọng của tiền xử lý dữ liệu (pre-processing data). Các phương pháp xử lý dữ liệu missing cũng đa dạng, từ đơn giản đến phức tạp. Những phương pháp thông thường nhất đó là thay thế các điểm dữ liệu bị missing bằng các giá trị đại diện chẳng hạn như median, mean, hoặc giá trị có tần xuất xuất hiện lớn nhất (mode). Một số phương pháp phức tạp hơn sẽ dựa trên khoảng cách của các điểm dữ liệu để thay các dữ liệu missing bằng giá trị của quan sát gần nhất với nó hoặc thực hiện voting dữ liệu trong một tập hợp gần nhất bằng trung bình có trọng số hoặc không có trọng số của các điểm này. \nTrong phân tích này tôi sẽ áp dụng MinMaxScaler để chuẩn hóa biến dự báo và sử dụng mean để xử lý dữ liệu missing."},{"metadata":{"trusted":false,"_uuid":"6bffd550295de6655cb41a8c78faab1dffe12600"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, Imputer\n\nif 'TARGET' in app_train.columns:\n    TARGET = app_train.pop('TARGET')\n\n# Gán train và test vào app_train và app_test; train, test được sử dụng để scale dữ liệu\ntrain = app_train\ntest = app_test\n\n# Khởi tạo inputer theo phương pháp trung bình\ninputer = Imputer(strategy = 'mean')\ninputer.fit(train)\n\n# Điền các giá trị NA bằng trung bình\ntrain = inputer.transform(train)\ntest = inputer.transform(test)\n\n# Khởi tạo scaler theo phương pháp MinMaxScaler trong khoảng [-1, 1]\nscaler = MinMaxScaler(feature_range = (-1, 1))\nscaler.fit(train)\n\n# Scale dữ liệu trên train và test\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\n# Loại bỏ cột SK_ID_CURR đầu tiên do cột này là key. Khi cần lấy từ app_train và app_test sang\ntrain = train[:, 1:]\ntest = test[:, 1:]\n\nprint('train shape: ', train.shape)\nprint('test shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"730c39086064d163200199cf207d6ff1b76599b1"},"cell_type":"markdown","source":"# 3. Xây dựng mô hình\n## 3.1. Đánh giá mức độ tương quan\n\nTrong một mô hình có quá nhiều biến input, chúng ta cần loại bỏ các biến không quan trọng để giảm chi phí tính toán và hạn chế khả năng overfiting của mô hình. Một cách phổ biến nhất trong đánh giá mối quan hệ giữa biến dự báo và biến mục tiêu đó là sử dụng bảng hệ số tương quan. Có một vài phương pháp tiếp cận khác cao cấp hơn như:\n\n* Thông qua các mô hình ensemble để xếp hạng mức độ quan trọng của các biến trong tree boosting hoặc random forest.\n* Sử dụng chỉ số Information Value trong Scorecard. Cách này được sử dụng phổ biến trong các mô hình credit risk.\n* Sử dụng chỉ số Akaike Inforamtion Criterion kết hợp với phương pháp step wise áp dụng trong các mô hình thuộc lớp hồi qui.\n\nCó thời gian mình sẽ trình bày thêm về các phương pháp này. Tạm thời chúng ta đánh giá quan hệ giữa các biến thông qua mức độ tương quan."},{"metadata":{"trusted":false,"_uuid":"35fd99fedf513ebe2f60cef522f7ba851ebeb638"},"cell_type":"code","source":"app_train['TARGET'] = TARGET\ncorr_tbl = app_train.corr()\ncorr_tbl","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7454f547b40eb87dc1b874d777fb2ba5662e39c5"},"cell_type":"code","source":"corr_tbl['TARGET'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"449e1709b5794f316d27b17c2105fb2f0c72c682"},"cell_type":"markdown","source":"Chúng ta có thể sử dụng hàm np.corrcoef(arr) để tính ma trận hệ số tương quan cho các biến. Tuy nhiên tính toán correlation trên numpy cần nhiều tài nguyên hơn so với pandas. Với lệnh trên, máy của mình đã bung memory. Một lưu ý nhỏ đó là các chiều dữ liệu được coi như các dòng và các quan sát là các cột trong lệnh np.corrcoef(arr). Chuyển ma trận qua data frame và tính correlation."},{"metadata":{"trusted":false,"_uuid":"c59ef6f0ce6cf98f8b46844bc29a99c6820b08ed"},"cell_type":"code","source":"pd_train = pd.DataFrame(train, columns = app_train.columns[1:-1])\npd_train['TARGET'] = TARGET\npd_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f1a5f21f0c3424faaa4c000595f3a18cd29ed585"},"cell_type":"code","source":"corr_tbl_train = pd_train.corr()\ncorr_tbl_train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36f532951defdb5db0939fb2b408b55b813d8e2e"},"cell_type":"markdown","source":"Mức độ tương quan của các biến sẽ được xếp hạng một cách tương đối dựa trên giá trị tuyệt đối của chúng. Các khoảng đánh giá như sau:\n\n* 0-0.19: Rất yếu.\n* 0.2-0.39: Yếu.\n* 0.4-0.59: Trung bình.\n* 0.6-0.79: Cao.\n* 0.8-1: Rất cao.\n\nDựa trên bảng hệ số tương quan chúng ta có thể tìm ra những biến có mối liên hệ lớn tới biến TARGET."},{"metadata":{"trusted":false,"_uuid":"6defeae46178d2a66f764f8caa4c0e14961cabcb"},"cell_type":"code","source":"corr_tbl_train['TARGET'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b47673ef03c6c028fa98c68021efed9b771e3eea"},"cell_type":"markdown","source":"Chọn 15 biến có mức độ tương quan lớn nhất đến biến mục tiêu và biểu diễn ma trận hệ số tương quan của chúng."},{"metadata":{"trusted":false,"_uuid":"11a4f6f60af072e82a40688c993c2aa5fa1691e8"},"cell_type":"code","source":"# Lấy ra danh sách 15 biến có tương quan lớn nhất tới biến mục tiêu theo trị tuyệt đối.\ncols_corr_15 = np.abs(corr_tbl_train['TARGET']).sort_values()[-16:].index.tolist()\n\n# Tính ma trận hệ số tương quan\ncols_tbl_15 = pd_train[cols_corr_15].corr()\n\n# Biểu diễn trên biểu đồ heatmap\nplt.figure(figsize = (13, 10))\nsns.heatmap(cols_tbl_15, cmap = plt.cm.RdYlBu_r, annot = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f43b2200f149dffb4d07129a02a888d4bfed6ac0"},"cell_type":"markdown","source":"Phân phối xác xuất của các biến trên theo các nhóm của biến mục tiêu."},{"metadata":{"trusted":false,"_uuid":"64f7c5e03ba11959c379c9fd12799198eaf2fad3"},"cell_type":"code","source":"plt.figure(figsize = (20, 5))\nfor i in range(5):\n    _plot_density(cols_corr_15[i])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c493930f346a095dda5c5df41f5be38b3ef74728"},"cell_type":"markdown","source":"DAYS_BIRTH là một biến liên tục có range nằm trong khoảng từ 20 - 70. Trục hoành biểu diễn độ tuổi theo ngày, giá trị âm thể hiện số ngày chênh lệch từ thời điểm sinh so với hiện tại. Như vậy người trẻ sẽ nằm ở bên phải của trục hoành và người già sẽ nằm bên trái. Từ biểu đồ ta có thể thấy những người trẻ có xu hướng không trả nợ nhiều hơn những người già. Bằng chứng là tại đồ thị mật độ ta nhận thấy phân phối lệch phải. Để làm rõ hơn nhận định, chúng ta đi tìm hiểu hành vi trả nợ theo các nhóm tuổi."},{"metadata":{"trusted":false,"_uuid":"8b5fc0257ec84c23382923a8d5104f5dbd171397"},"cell_type":"code","source":"age_bin = app_train[['TARGET', 'DAYS_BIRTH']]\nage_bin['YEAR_OLD'] = -app_train['DAYS_BIRTH']/365\n\n# Phân chia khoảng tuổi thanh 10 khoảng bằng nhau\nage_bin['DAYS_BIN'] = pd.cut(age_bin['YEAR_OLD'], bins = np.linspace(20, 70, num = 11))\nage_bin.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cf4980d859d25b6ce91b72f29b3a2d930c26b75"},"cell_type":"markdown","source":"Thống kê số lượng không trả nợ theo các khoảng tuổi."},{"metadata":{"trusted":false,"_uuid":"5ff6c9df5a0a403eae6482ce1a086d05bfb738c4"},"cell_type":"code","source":"age_bin.groupby(['DAYS_BIN']).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d69aab0fa515fa10c39e23e624ad4312b8705551"},"cell_type":"markdown","source":"Biểu đồ tỷ lệ trả nợ các hợp đồng theo từng nhóm tuổi."},{"metadata":{"trusted":false,"_uuid":"793d795cfe52957910c3d867b131435f1563b219"},"cell_type":"code","source":"plt.figure(figsize = (8, 6))\nage_bin.groupby(['DAYS_BIN']).mean()['TARGET'].plot.barh(color = 'b')\nplt.xticks(rotation = '75')\nplt.xlabel('Not Repaid rate')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5805163076402f3cec9ea38d572234e4df8ace2"},"cell_type":"markdown","source":"Biểu đồ cho thấy tỷ lệ không trả nợ ở những người thuộc nhóm tuổi cao thấp hơn so với nhóm tuổi thấp. Hành vi này có thể là do người trẻ từ 20-25 tuổi chưa có thu nhập cao và các khoản tiết kiệm trong khi người gia có nhiều tiền tiết kiệm. \n\nSau khi xử lý dữ liệu missing và tìm hiểu phân phối tỷ lệ Repaid/ Not Repaid của các biến numeric. Chúng ta sẽ hồi qui mô hình."},{"metadata":{"_uuid":"35de7a15c448d9bd72163e9dd545301357c475d7"},"cell_type":"markdown","source":"## 3.2. Logistic regression\n\nMặc dù tên mô hình là logistic regression nhưng logistic không phải là mô hình hồi qui mà trái lại, là mô hình thuộc lớp phân loại. Ngoài ra logistic không phải là một phương pháp mạnh. Đây là lớp mô hình có đường biên phân loại tuyến tính (linear seperable) nên khả năng phân loại dữ liệu trong trường hợp biên của các nhóm chồng lấn sẽ yếu. Tuy nhiên đây là phương pháp được sử dụng phổ biến nhất trong tính toán scorecard. Một trong những nguyên nhân giúp logistic được ưa chuộng trong scorecard đó là:\n\n* Giá trị của logistic nằm giới hạn trong khoảng [0, 1] và phù hợp với miền giá trị mà một xác xuất rơi vào.\n* Đạo hàm của hàm sigmoid rất đẹp mắt..\n* Hàm mất mát là một hàm lồi.\n* Có thể giải thích được tác động của các biến giải thích lên biến mục tiêu dựa vào hệ số ước lượng. Điều mà trường phái thống kê rất coi trọng.\n\nChính vì thế, khi làm quen với Machine Learning, hồi qui logistic sẽ là một trong những lớp bài toán được tiếp cận đầu tiên sau hồi qui tuyến tính. Bên dưới là mô hình hồi qui logistic áp dụng cho bài toán này."},{"metadata":{"trusted":false,"_uuid":"f1641712d2d86a80ea374fcb39b21f54eddee8f1"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Xây dựng mô hình logistic với tham số kiểm soát C = 0.0001\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Huấn luyện mô hình\nlog_reg.fit(train, TARGET)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"da9afc8847702545f31bdbc30d84f85eb45fe219"},"cell_type":"code","source":"train_pred_prob = log_reg.predict_proba(train)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"956de905c64db46f70d7136d5d4661d6e0099231"},"cell_type":"code","source":"TARGET.value_counts()/TARGET.value_counts().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8721c1c6959927000f33dc92a16a9dcc251e2ba"},"cell_type":"markdown","source":"ROC curve (Receiver Operating Characteristic) là một đường cong thể hiện mối liên hệ giữa tỷ lệ mắc sai lầm loại I (false positive rate) và tỷ lệ dự báo đúng positive (true positive rate). \n\nTrong thống kê, chúng ta chia các sai lầm của giả thuyết thành 2 loại: sai lầm loại I và sai lầm loại II. Khi thực hiện một mô hình phân loại nợ mục tiêu của chúng ta là tìm ra những hồ sơ nợ xấu. Một kết luận từ mô hình có thể rơi vào 2 trạng thái sai lầm: nhận định một hồ sơ xấu là tốt - loại I, hoặc trái lại, coi một hồ sơ tốt là xấu - loại II (giết nhầm còn hơn bỏ sót). \n\nTác hại của sai lầm loại I sẽ lớn hơn sai lầm loại II bởi ảnh hưởng do hồ sơ xấu gây là lớn hơn nhiều so với việc bạn kiếm được một hồ sơ tốt. Đó cũng là lý do Tào Tháo thường hay đa nghi bởi ông biết được mức độ tác hại của 2 loại sai lầm này là khác nhau (mặc dù ông cũng chẳng có một chứng chỉ thống kê nào).\n\nNếu chúng ta chấp nhận một tỷ dự báo đúng hồ sơ tốt cao hơn thì chúng ta sẽ phải hạ thấp ngưỡng threshold xác định loại hồ sơ (mặc định là 0.5). Điều này dẫn đến các hồ sơ xấu có khả năng bị nhận định là hồ sơ tốt cao hơn. Điều này cho thấy luôn có sự đánh đổi giữa tỷ lệ true positive rate và false positive rate. \n\nMột mô hình phân loại tốt là mô hình mà ở các threshold ta phân loại được nhiều nhất các hồ sơ tốt nhưng chỉ phải chấp nhận một lượng rất nhỏ các hồ sơ xấu. Các mô hình như vậy đều có chung một tính chất, đó là đường cong ROC lồi lên phía trên so với trục hoành. Như vậy các bạn đã hiểu được ý nghĩa của đường cong ROC rồi chứ? Trên thực tiễn ROC là một biểu đồ trực quan đánh giá phẩm chất của mô hình. ROC càng lồi mô hình càng phân loại tốt và trái lại. Mức độ lồi của đường cong ROC được đo bằng phần diện tích nằm dưới đường cong ROC và trục hoành hay còn gọi là chỉ số AUC (Area under curve).\n\nTrong tín dụng sau khi đã tìm được mô hình sở hữu đường cong ROC tốt nhất, ngân hàng sẽ xây dựng một hàm mất mát dựa trên phương trình ước lượng tổn thất:\n$$EL = PD*LGD$$\nTrong đó EL - Expected Loss, PD - Probability Default, LGD - Loss given default.\n\nĐể chọn ra một threshold tối ưu mang lại tổn thất về mặt lợi ích cho ngân hàng là nhỏ nhất. Về EL là gì? PD, LGD là gì? Mình sẽ không giải thích thêm ở bài viết này. Các bạn muốn đi sâu vào ngành Credit Risk có thể tìm thấy trong rất nhiều các tài liệu và khóa học về mảng đề tài này.\n\nBên dưới ta sẽ xây dựng một đường cong ROC."},{"metadata":{"trusted":false,"_uuid":"6189c840e29bfae4f47626e066d112b4d5fb7c0e"},"cell_type":"code","source":"from sklearn.metrics import roc_curve, precision_recall_curve\nfpr, tpr, thres = precision_recall_curve(TARGET, train_pred_prob)\n\ndef _plot_roc_curve(fpr, tpr, thres):\n    plt.figure(figsize = (10, 8))\n    plt.plot(fpr, tpr, 'b-', label = 'ROC')\n    plt.plot([0, 1], [0, 1], '--')\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n\n_plot_roc_curve(fpr, tpr, thres)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14195809f0cbb60eac45a82ea5ccc2c63aa891ce"},"cell_type":"markdown","source":"Trong các mô hình phân loại chúng ta thường quan tâm đến tỷ lệ Accuracy. Đây là chỉ số đại diện nhất vì nó phản ánh trực tiếp hiệu quả của một mô hình thông qua số trường hợp phân loại đúng /tổng số trường hợp phân loại. Tuy nhiên chỉ số này tỏ ra kém hiệu quả đối với trường hợp mẫu mất cân bằng nghiêm trọng. Trong trường hợp này kết quả dự báo của mô hình thường thiên lệch về một lớp đa số. Do đó Accuracy hiển nhiên cao đối với những mô hình phẩm chất kém bởi số trường hợp hồ sơ tốt là quá nhiều dẫn đến một một mô hình cảm tính quyết định toàn bộ là hồ sơ tốt cũng dẫn tới Accuracy lớn. Một modeler thiếu kinh nghiệm sẽ rất dễ hài lòng với kết quả này. Nếu đưa vào áp dụng những mô hình như vậy sẽ ảnh hưởng không nhỏ tới các tổ chức tài chính, kinh doanh. Vậy trong trường hợp này tỷ lệ nào được sử dụng thay thể để đánh giá mô hình?\nPrecision và Recall là một lựa chọn thay thế phù hợp nhất bởi nó thỏa mãn các tính chất:\n\n* Cả 2 đều đánh mức độ dự báo chính xác của positive tức hồ sơ phân loại là xấu. Đây là nhóm được ưu tiên phân loại chính xác hơn vì thiệt hại gây ra bởi nó lớn hơn.\n* Precision đánh giá tỷ lệ dự báo chính xác hồ sơ xấu trong tổng số trường hợp được dự báo là xấu.\n* Recall đánh giá tỷ lệ dự báo chính xác hồ sơ xấu khi hồ sơ về mặt bản chất là xấu.\n\nBên cạnh đó còn có các chỉ số khác chúng ta có thể cân nhắc như F1-Score, Kappa, Gini.\n\nKhi nhìn vào biểu đồ của Precision và Recall ta sẽ biết được tại mỗi một ngưỡng threshold sẽ trả về tỷ lệ phân loại hồ sơ xấu là bao nhiêu trên lần lượt tổng số các hồ sơ được dự báo là xấu và tổng số các hồ sơ xấu trên thực tế."},{"metadata":{"trusted":false,"_uuid":"61327c44e12cf6ac8772e6d193fa67f81fc62387"},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nprec, rec, thres = precision_recall_curve(TARGET, train_pred_prob)\n\ndef _plot_prec_rec_curve(prec, rec, thres):\n    plt.figure(figsize = (10, 8))\n    plt.plot(thres, prec[:-1], 'b--', label = 'Precision')\n    plt.plot(thres, rec[:-1], 'g-', label = 'Recall')\n    plt.xlabel('Threshold')\n    plt.ylabel('Probability')\n    plt.title('Precsion vs Recall Curve')\n    plt.legend()\n\n_plot_prec_rec_curve(prec, rec, thres)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b29c11a3e933b5aa7c10b4ce710d62ca89e35a2"},"cell_type":"markdown","source":"Đường ROC curve không lồi lên phía trên so với trục hoành cho thấy sức mạnh phân loại của mô hình tương đối yếu. Tỷ lệ precision và recall theo các ngưỡng threshold cũng không phải là các đường cong lồi dẫn đến khi thay đổi threshold có thể làm tăng chi phí đánh đổi giữa tỷ lệ precision và recall đáng kể. Ở mỗi mức threshold không đạt được đồng thời tỷ lệ cao ở precision và recall. Thay vào đó ta sẽ phải đánh đổi giữa precision cao hoặc recall cao. Đây không phải là một mô hình đủ tốt để áp dụng vào thực tiễn."},{"metadata":{"_uuid":"675c5c2280b2165b81f4d0224d295c94f5c989a8"},"cell_type":"markdown","source":"## 3.3. Sử dụng kĩ thuật feature engineering\n\nHầu hết bí quyết để chiến thắng trong các cuộc thi phân tích dữ liệu đó là feature engineering. Feature engineering quan trọng đến mức thầy Andrew Ng đã nói rằng: 'Áp dụng học máy cơ bản là sử dụng feature engineering'. Và theo kinh nghiệm của mình, feature engineering thường mang lại kết quả tốt hơn so với các phương pháp hyparameter tunning. Tất nhiên sử dụng kết hợp cả 2 phương pháp này là tốt nhất nhưng mình vẫn thường có xu hướng thực hiện feature engineering trước khi lựa chọn các phương pháp phức tạp. Tránh tình trạng garbage in garbage out. Một mô hình phức tạp nhưng không có các biến đầu vào tin cậy thì kết quả mô hình sẽ không thể tốt.\n\nVới mô hình hiện tại chúng ta nhận thấy các biến không đủ mạnh để tạo ra một kết quả chuẩn xác. Trong trường hợp này mình nghĩ ngay đến việc áp dụng feature engineering như một giải pháp cải thiện tình hình. feature engineering sẽ tạo ra những biến mới mà khả năng giải thích của nó sẽ tốt hơn đáng kể nếu áp dụng vào mô hình. \n\nVề kĩ thuật feature engineering chúng ta có 2 nhánh chính là:\n\n* feature selection: lựa chọn các biến tốt nhất từ các biến đã có. Phương pháp này có thể dựa trên ranking của các feature theo các tiêu chí đánh giá sức mạnh hoặc dựa trên kinh nghiệm và hiểu biết về đề tài nghiên cứu.\n\n* thêm mới: Tạo ra các feature mới từ các feature sẵn có. Các biến đổi có thể là scale, chuẩn hóa dữ liệu, sử dụng lũy thừa, logarith,....\n\nTrong bài này mình sẽ sử dụng feature engineering theo phương pháp thêm mới bằng đa thức bậc cao (polynormal feature). Các biến được lựa chọn để feature chỉ bao gồm 15 biến có tương quan cao nhất. Bậc được lựa chọn cao nhất là 3. Thông qua polynormal feature các biến bậc cao và biến tích chéo sẽ được tạo thành. Chẳng hạn từ 2 biến EXT_SOURCE_1 và EXT_SOURCE_2 chúng ta có thể tạo ra EXT_SOURCE_1^2, EXT_SOURCE_2^2 và EXT_SOURCE_1xEXT_SOURCE_2.\n\nTrong sklearn chúng ta có thể dễ dàng sử dụng kĩ thuật này thông qua hàm `PolynormialFeatures()`."},{"metadata":{"trusted":false,"_uuid":"5b45aaabde4c4a1c703c1db848df3a24a1a22fd9"},"cell_type":"code","source":"print(cols_corr_15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6b77739a5b5ed727a6035f603543eb87e63a5f6b"},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures, Imputer, MinMaxScaler\n\n# Khởi tạo các preprocessing. Trong đó inputer theo mean, minmax scaler theo khoảng 0, 1 và polynomial features bậc 3.\ninputer = Imputer(strategy = 'mean')\nminmax_scaler = MinMaxScaler(feature_range = (0, 1))\npoly_engineer = PolynomialFeatures(degree = 3)\n\n# Lấy các feature có tương quan lớn nhất đến biến mục tiêu từ app_train và app_test\nTARGET = app_train[cols_corr_15[-1]]\ntrain_poly_fea = app_train[cols_corr_15[:-1]]\ntest_poly_fea = app_test[cols_corr_15[:-1]]\n\n# input dữ liệu missing\ninputer = inputer.fit(train_poly_fea)\ntrain_poly_fea = inputer.transform(train_poly_fea)\ntest_poly_fea = inputer.transform(test_poly_fea)\n\n# Minmax scaler dữ liệu\nminmax_scaler = minmax_scaler.fit(train_poly_fea)\ntrain_poly_fea = minmax_scaler.transform(train_poly_fea)\ntest_poly_fea = minmax_scaler.transform(test_poly_fea)\n\nprint('train_poly_fea shape: ', train_poly_fea.shape)\nprint('test_poly_fea shape: ', test_poly_fea.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"31212d5d0afcd3290c0af5bb4fd0473e8b7ee903"},"cell_type":"code","source":"# Polynormial features dữ liệu\npoly_engineer = poly_engineer.fit(train_poly_fea)\ntrain_poly_fea = poly_engineer.transform(train_poly_fea)\ntest_poly_fea = poly_engineer.transform(test_poly_fea)\n\nprint('train_poly_fea shape: ', train_poly_fea.shape)\nprint('test_poly_fea shape: ', test_poly_fea.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b496d5ab005c061f5aedc9636d9e781ce3ceef21"},"cell_type":"markdown","source":"Sau feature engineering, số lượng các biến đã tăng từ 15 lên 816 biến. Chúng ta có thể xem danh sách các features được tạo mới như sau:"},{"metadata":{"trusted":false,"_uuid":"e3d15b38a7cf77d528d6fc749b8e69a6f51b8a66"},"cell_type":"code","source":"features = poly_engineer.get_feature_names(input_features = cols_corr_15[:-1])\nfeatures[:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24455269440a2011e6d840295aeb418cadd153f9"},"cell_type":"markdown","source":"Để đánh giá liệu rằng sau khi thực hiện features engineering có giúp cải thiện kết quả hay không chúng ta thực hiện hồi qui logistic theo những features mới."},{"metadata":{"trusted":false,"_uuid":"f2cc25deca90e874b0e036a3a5bb74ed1280b40b"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Xây dựng mô hình hồi qui logistic với tham số kiểm soát là C = 0.0001\nlg_reg = LogisticRegression(C = 0.0001)\nlg_reg.fit(train_poly_fea, TARGET)\nlg_reg","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fd4066c32f86a6c4d83b7b121b07f1561c4b5daa"},"cell_type":"code","source":"# Dự báo xác xuất logistic\ntrain_pred_prob = lg_reg.predict_proba(train_poly_fea)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"190dbbbfb38f3761c3128762070c9eba3eca2413"},"cell_type":"code","source":"# Biểu diễn đường roc_curve\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thres = roc_curve(TARGET, train_pred_prob)\n\n\ndef _plot_roc_curve(fpr, tpr, thres):\n    roc = plt.figure(figsize = (10, 8))\n    plt.plot(fpr, tpr, 'b-', label = 'ROC')\n    plt.plot([0, 1], [0, 1], '--')\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    return roc\n\n# Lưu biểu đồ vào p1\np1 = _plot_roc_curve(fpr, tpr, thres)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"79f74b5dda618f79e3b2b7c0072412356d444151"},"cell_type":"code","source":"from sklearn.metrics import auc\n#0.7127599620726505\nauc(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c212384b3f78cdcb925d7b71b0b5a42164b8a8d"},"cell_type":"markdown","source":"So với trước khi thực hiện feature engineering đường cong ROC sau khi thực hiện feature engineering lồi hơn. Điều đó cho thấy sức mạnh phân loại của mô hình tốt hơn sau feature engineering."},{"metadata":{"trusted":false,"_uuid":"878a162a0633936d083214fa99fbd1ca21e231a6"},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve, accuracy_score\n\nprec, rec, thres = precision_recall_curve(TARGET, train_pred_prob)\n\ndef _plot_prec_rec_curve(prec, rec, thres):\n    plot_pr = plt.figure(figsize = (10, 8))\n    plt.plot(thres, prec[:-1], 'b--', label = 'Precision')\n    plt.plot(thres, rec[:-1], 'g-', label = 'Recall')\n    plt.xlabel('Threshold')\n    plt.ylabel('Probability')\n    plt.title('Precsion vs Recall Curve')\n    return plot_pr\n\n_plot_prec_rec_curve(prec, rec, thres)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0b46bdba035959a9d55f0c6343a0234586a9e208"},"cell_type":"code","source":"# Accuracy\ntrain_pred_label = lg_reg.predict(train_poly_fea)\naccuracy_score(TARGET, train_pred_label)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95056f971cae2879b87861f21147e0c7a396186a"},"cell_type":"markdown","source":"## 3.4. Random Forest\n\nRandom Forest là mô hình thuộc lớp kết hợp (ensemble model) tức kết quả được đưa ra dựa trên không chỉ một mô hình mà từ nhiều mô hình khác nhau. \n\nRandom Forest sẽ xây dựng một rừng cây ngẫu nhiên dựa trên các node và nhánh. Đại diện cho mỗi node là một câu hỏi mà giá trị trả về là YES hoặc NO. Các nhánh sẽ có tác dụng kết nối các nodes để tạo ra một kịch bản đường đi (routine).\n\nNode bắt đầu của Random Forest là root node. Từ root node, mô hình sẽ xây dựng một bộ câu hỏi Yes/No dựa trên thông tin được cung cấp từ biến dự báo. Các nhánh YES, NO sẽ rẽ đến các node mới được gọi là internal node. Chẳng hạn đối với biến liên lục như YEAR_OLD mô hình có thể đặt ra câu hỏi YEAR_OLD > 30 hay không? Dựa trên giá trị của quan sát đối với biến YEAR_OLD mà một quan sát có thể rẽ theo nhánh YES hoặc NO. \n\nTại phía cuối của các nhánh YES/NO mô hình tiếp tục khởi tạo những internal node ở tầng thấp hơn với các biến khác. Thứ tự các biến được lựa chọn là ngẫu nhiên. Quá trình rẽ nhánh được thực hiện liên tục cho đến khi mô hình đi đến node cuối. Tại node này không có nhánh nào được rẽ thêm. \n\nSơ đồ của mô hình rất giống một cái cây. Xuất phát từ gốc cây rẽ vào các cành to, cành nhỏ và kết thúc ở lá. Bởi thế node cuối cùng còn được gọi là leaf node. Tại leaf node mô hình sẽ đưa ra kết quả thống kê của kịch bản đường đi (routine) từ gốc tới lá là một giá trị xác xuất của positive và negative. Mỗi một kịch bản rẽ nhánh từ root node tới leaf node được gọi là một cây.\n\nLưu ý rằng mỗi một cây được sẽ được áp dụng trên nhiều mẫu dữ liệu con được lựa chọn ngẫu nhiên để quyết định nhãn cho biến mục tiêu tại leaf node. Một quan sát được dự báo trên rất nhiều cây khác nhau và kết quả nhãn trả về từ các cây sẽ là cơ sở để voting nhãn cho quan sát.\n\nKết quả từ mô hình Random Forest được kết hợp từ nhiều cây quyết định con và được thử nghiệm trên nhiều bộ dữ liệu con nên sai số dự báo thông thường nhỏ hơn so với những mô hình phân loại tuyến tính như logistic hoặc linear regression. \n\nBên cạnh Random Forest thì Gradient Boosting và AdaBoost cũng là các mô hình thuộc lớp mô hình kết hợp thường được áp dụng và mang lại hiệu quả bất ngờ tại nhiều cuộc thi.\n\nĐể sử dụng Random Forest trong python chúng ta có thể khai thác module `sklearn.ensemble`"},{"metadata":{"trusted":false,"_uuid":"7053701aae4f1976459df61517aca2f1d1e5e0fc"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Khởi tạo rừng cây\nrd_classifier = RandomForestClassifier(n_estimators = 100, # Số cây trong rừng cây\n                                       max_depth = 5, # Độ sâu của cây\n                                       random_state = 123, # Khai báo seed để mô hình không đổi cho các lần chạy sau\n                                       verbose = 1, # In log của quá trình huấn luyện\n                                       n_jobs = -1 # Sử dụng đa luồng để vận hành mô hình\n                                      )\nrd_classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7e5cdf550bd4a3f35d778fa1a7f7071abe4ae2cb"},"cell_type":"code","source":"# Huấn luyện mô hình\nrd_classifier.fit(train_poly_fea, TARGET)\n\n# Dự báo trên tập train\ntrain_prob_rd = rd_classifier.predict_proba(train_poly_fea)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81a496dbeea432d28fcae4c8dc75f1bd39fb5d0f"},"cell_type":"markdown","source":"Vẽ biểu đồ đường ROC curve"},{"metadata":{"trusted":false,"_uuid":"c0346c416065c9f05341c4000d7c471e9d9e4e28"},"cell_type":"code","source":"fpr2, tpr2, thres2 = roc_curve(TARGET, train_prob_rd)\np2 = _plot_roc_curve(fpr, tpr, thres)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa5d20cae7946461daf332c6005f0592bdc6534f"},"cell_type":"markdown","source":"Biểu diễn cả 2 đường ROC của hồi qui logistic và random forest trên cùng một đồ thị."},{"metadata":{"trusted":false,"_uuid":"9897961a57b07809d7bc04ca6b80a7f90daf8327"},"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nplt.plot(fpr2, tpr2, 'b-', label = 'Random Forest')\nplt.plot(fpr, tpr, 'r-', label = 'Logistic')\nplt.plot([0, 1], [0, 1], '--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"aa0a4d62f95721e11a771d58fc2545382ebd338c"},"cell_type":"code","source":"from sklearn.metrics import auc\n#0.7300858815170105\nauc(fpr2, tpr2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d8e30b05c29b078ec8444d5d4112ea0f4bc8fb6"},"cell_type":"markdown","source":"Ta nhận thấy đường ROC của Random Forest lồi hơn so với Logistic. Điều đó cho thấy sức mạnh phân loại của Random Forest là tốt hơn Logistic.\n\nBiểu đồ precision và recall curve"},{"metadata":{"trusted":false,"_uuid":"805375f9fa7001c9019045c056f0787ced74cade"},"cell_type":"code","source":"prec, rec, thres = precision_recall_curve(TARGET, train_pred_prob)\n_plot_prec_rec_curve(prec, rec, thres)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"130e5397f630c933e92f1692bf96c381910688a1"},"cell_type":"markdown","source":"Một trong những ưu thế của mô hình random forest đó là khả năng đánh giá mức độ quan trọng của các biến dự báo dựa trên mức độ ảnh hưởng của nó tới xác xuất của biến mục tiêu. Đây là một trong những phương pháp feature selection thường được sử dụng trong machine learning để tuyển chọn biến. Thông tin về mức độ quan trọng có thể được khai thác thông qua thuộc tính `feature_importances_` như sau:"},{"metadata":{"trusted":false,"_uuid":"0e8967dcd557318d0fffebbfa8f290baf45043b0"},"cell_type":"code","source":"# Lấy thông tin về mức độ quan trọng các biến tác động lên biến mục tiêu\nfeature_importance = rd_classifier.feature_importances_\nfeature_importance = pd.DataFrame({'importance values': feature_importance})\nfeature_importance.index = features\nfeature_importance = feature_importance.sort_values('importance values', ascending = False)\nfeature_importance[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e73be64ef348a8d022dd82e5630a56704c10ccac"},"cell_type":"code","source":"feature_importance[:10].sort_values('importance values', ascending = True).plot.barh(figsize = (8, 6))\nplt.yticks(rotation = 15)\nplt.xlabel('Importance values')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ae5d34168f11d7f299ab6e5b21578be1de930203"},"cell_type":"code","source":"feature_importance.iloc[:5, 0].tolist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17fbea296c04cc76b2dcd856cae4daabbe397937"},"cell_type":"markdown","source":"Tính toán mức độ chính xác của mô hình dự báo."},{"metadata":{"trusted":false,"_uuid":"0791e066c0fb218278016ba44ce1b6d6e0fceafa"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ntrain_label_rd = rd_classifier.predict(train_poly_fea)\naccuracy_score(train_label_rd, TARGET)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9caaaa7e18a8e41220af417d48049f9f11dc433a"},"cell_type":"code","source":"np.unique(train_label_rd, return_counts = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5d0f4229156f380b3b0cb649d5fdc5aaf6a8532"},"cell_type":"markdown","source":"## 3.5. Gradient Boosting"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"4605e8ef39fa93142b52bb57b9098122d8754dcc"},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"243aae2d41d2b3a8b52482ad1749236983662d05"},"cell_type":"code","source":"lgb_classifier = lgb.LGBMClassifier(n_estimator = 10000, \n                                    objective = 'binary', \n                                    class_weight = 'balanced',\n                                    learning_rate = 0.05,\n                                    reg_alpha = 0.1,\n                                    reg_lambda = 0.1,\n                                    subsample = 0.8,\n                                    n_job = -1,\n                                    random_state = 12\n                                   )\nlgb_classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0f7ae3076f218c067d72d83c786c949d469d7e67"},"cell_type":"code","source":"kfold = KFold(n_splits = 10, shuffle = True, random_state = 12)\nvalid_scores = []\ntrain_scores = []\ncount = 0\nfor train_idx, valid_idx in kfold.split(train_poly_fea):\n    count += 1\n    # Split train, valid\n    train_features, train_labels = train_poly_fea[train_idx], TARGET[train_idx]\n    valid_features, valid_labels = train_poly_fea[valid_idx], TARGET[valid_idx]\n    lgb_classifier.fit(train_features, train_labels, eval_metric = 'auc',\n              eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n              eval_names = ['valid', 'train'], \n              early_stopping_rounds = 100, verbose = 200)\n    \n    valid_score = lgb_classifier.best_score_['valid']['auc'] \n    train_score = lgb_classifier.best_score_['train']['auc'] \n    \n    valid_scores.append(valid_score)\n    train_scores.append(train_score)\n    \n    print('fold time: {}; train score: {}; valid score: {}'.format(count, valid_score, train_score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3631ee8f625f02bf43b2b63939dc48ac9d9bace6"},"cell_type":"markdown","source":"## 3.6. Neural network"},{"metadata":{"trusted":false,"_uuid":"6e8cec5360bef7118ad854146a737b65ec5e0441"},"cell_type":"code","source":"# Deep learning với Keras\nfrom keras.layers import Input, Dense, Flatten, Concatenate, concatenate, Dropout, Lambda\nfrom keras.models import Model\nfrom keras.layers.embeddings import Embedding","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7e52b88491da5437a8d840e54b1a11960d5492cc"},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"686b9d067975bb32387237d44dd1df821e9c92e7"},"cell_type":"code","source":"# design neural network\ninput_els = []\nencode_els = []\n\n# Generate a list include many Input layers\n\nfor i in range(train.shape[1]):\n    # input alway have the shape (*, 1)\n    input_els.append(Input(shape = (1,)))\n    encode_els.append(input_els[-1])\n# encode_els","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9a257f0971d3ddbd25395de2ab3916f8aaa804de"},"cell_type":"code","source":"# concate nate all layers\nencode_els = concatenate(encode_els) \n\n# After completed the input layers, we design the hidden layers\nhidden1 = Dense(units = 128, kernel_initializer = 'normal', activation = 'relu')(encode_els)\ndroplayer1 = Dropout(0.2)(hidden1)\nhidden2 = Dense(64, kernel_initializer = 'normal', activation = 'relu')(droplayer1)\ndroplayer2 = Dropout(0.2)(hidden2)\noutputlayer = Dense(1, kernel_initializer = 'normal', activation = 'sigmoid')(droplayer2)\n\nclassifier = Model(input = input_els, outputs = [outputlayer])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"68a2afa626e069aa66355ae5446e7543f9d45306"},"cell_type":"code","source":"classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nclassifier.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"05ee6924295866968bd390018cf1f8ec15d3d268"},"cell_type":"code","source":"# split train/valid\nfrom sklearn.model_selection import KFold\ncount = 0\nkfold = KFold(n_splits = 10, shuffle = True, random_state = 12)\nvalid_scores = []\ntrain_scores = []\nfor train_idx, valid_idx in kfold.split(train_poly_fea):\n    while count < 1:\n        count += 1\n        # Split train, valid\n        train_features, train_labels = train[train_idx], TARGET[train_idx]\n        valid_features, valid_labels = train[valid_idx], TARGET[valid_idx]\n        classifier.fit(\n            [train_features[:, i] for i in range(train.shape[1])], #lấy list toàn bộ các cột\n            train_labels,\n            epochs=1,\n            batch_size=128,\n            shuffle=True,\n            validation_data=([valid_features[:, i] for i in range(train.shape[1])], valid_labels) \n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"852d07c78ddc9ff3d34a34d42bca421ea91275b2"},"cell_type":"code","source":"# DỰ báo trên tập train.\ntrain_prob_nn = classifier.predict([train[:, i] for i in range(train.shape[1])])\ntrain_prob_nn","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c895cd6bdb8cbcd1c6ddc8b5553b900f8a715725"},"cell_type":"code","source":"# np.save('train_prob_nn.npy',train_prob_nn)\n\nfpr4, tpr4, thres4 = roc_curve(TARGET, train_prob_nn)\n_plot_roc_curve(fpr4, tpr4, thres4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"00536759d2057d2cc27c1427b989a8e019ae186a"},"cell_type":"code","source":"from sklearn.metrics import auc\nauc(fpr4, tpr4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"349ded4cf84816c2c72880a840784eb79d5b6986"},"cell_type":"code","source":"prec, rec, thres = precision_recall_curve(TARGET, train_prob_nn)\n_plot_prec_rec_curve(prec, rec, thres)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"0e10cfef41ce5ec386a1774aa4540f1143e0be9a"},"cell_type":"code","source":"# Save data\n# np.save('train1.npy', train)\n# np.save('test1.npy', test)\n# np.save('TARGET.npy', TARGET)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"51e0f62f79a253decfce9dd6a47d19eeec881738"},"cell_type":"code","source":"# import numpy as np\n# train = np.load('train.npy')\n# print(train.shape)\n# test = np.load('test.npy')\n# print(test.shape)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":1}