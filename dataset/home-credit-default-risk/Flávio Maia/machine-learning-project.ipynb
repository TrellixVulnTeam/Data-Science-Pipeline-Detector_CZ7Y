{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#TESTING\n#Once more\n\n#numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd\n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List files available\nprint(os.listdir(\"../input/\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Training data\napp_train = pd.read_csv('../Jupyter/input/application_train.csv')\nprint('Training data rows and columns: ', app_train.shape)\napp_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Testing data features\napp_test = pd.read_csv('../Jupyter/input/application_test.csv')\nprint('Testing data rows and columns: ', app_test.shape)\napp_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis\nTrying figure out if there are some trends, anomalies, patterns or relationships within the data.\nBasically, learn what our data can tell us."},{"metadata":{"trusted":false},"cell_type":"code","source":"# loans paid off(0) and the one with difficulties(1)\napp_train[\"TARGET\"].value_counts() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"app_train[\"TARGET\"].astype(int).plot.hist();","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Function to calculate missing values by column\ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# missing vales statistics \nmissing_values = missing_values_table(app_train)\nmissing_values.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# number of each type of column\napp_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Create a label ecnoder object\nle = LabelEncoder()\nle_count = 0\n\n#Iterate thorugh the columns\nfor col in app_train:\n    if app_train[col].dtype =='object':\n        # if 2 or fewer unqiue categories \n        if len(list(app_train[col].unique())) <= 2:\n            # train on the training data\n            le.fit(app_train[col])\n            #transform both training and testing data\n            app_train[col]=le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            #keep track of how many columns were label encoded\n            le_count += 1\nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Training Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_labels = app_train['TARGET']\n\n# Align the traingin and testing data, keep only columns o resent in both dataframes\napp_train, app_test = app_train.align(app_test, join ='inner', axis =1 )\n\n# Add the target back in \napp_train[\"TARGET\"] = train_labels\n\nprint(\"Training Features shape: \", app_train.shape)\nprint(\"Testing Features shape: \", app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"(app_train[\"DAYS_BIRTH\"]/-365).describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].plot.hist(title=\"Days Employment Histogram\");\nplt.xlabel(\"Days Employment\");\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"anom = app_train[app_train[\"DAYS_EMPLOYED\"]==365243]\nnon_anom = app_train[app_train[\"DAYS_EMPLOYED\"]!=365243]\nprint('The non-anomalies default on %0.2f%% of loans' %(100*non_anom[\"TARGET\"].mean()))\nprint('The anomalies default on %0.02f%% of loans'%(100*anom[\"TARGET\"].mean()))\nprint('There are %d anomalous days of employment' %len(anom))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tescik = app_train[app_train[\"DAYS_EMPLOYED\"]==365243]\ntescik.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create an anomalous flag column\napp_train[\"DAYS_EMPLOYED_ANOM\"] = app_train[\"DAYS_EMPLOYED\"]== 365243\n\n# Replace the anomalous values with nan\napp_train[\"DAYS_EMPLOYED\"].replace({365243: np.nan},inplace =True)\n\napp_train[\"DAYS_EMPLOYED\"].plot.hist(title = \"Days Employment Histogram\");\nplt.xlabel(\"Days Employment\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlations\nlooking for correlations between the features and th target\nhence we calculate correlation coefficient, which absolute values are categorised as follows\n- .00-.19 “very weak”\n- .20-.39 “weak”\n- .40-.59 “moderate”\n- .60-.79 “strong”\n- .80-1.0 “very strong”"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Find correlations with the target and sort\ncorrelations = app_train.corr(method=\"pearson\")['TARGET'].sort_values()\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Find the correlation of the positive days since birth and target\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Set the style of plots\nplt.style.use('fivethirtyeight')\n\n# Plot the distribution of ages in years\nplt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Kernel density estimation plot"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize = (8, 6))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n\n # KDE plot of loans which were not repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n\n# Labeling of plot\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## average failure to repay loans by age bracket."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Age information into a separate dataframe\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Group by the bin and calculate averages\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TREND:\nyounger applicants are more likely to not repay the loan! The rate of failure to repay is above 10% for the youngest three age groups and beolow 5% for the oldest age group."},{"metadata":{},"cell_type":"markdown","source":"## INFO\n\nNow we want to investigate correlation between credit rating('EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3'), age and repaying loan on time.\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Heatmap \n\nto show correlations properly\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize = (8, 6))\n\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reflection:\nWe can see that DAYS_BIRTH is positively correlated with EXT_SOURCE_1(strong correlation) and EXT_SOURCE_1(weak), indicating that maybe one of the factors in this score is the client age."},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10,12))\n\n# iterate through the sources\nfor i, source in enumerate(['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']):\n    # create a new subplot for each source\n    plt.subplot(3, 1, i+1)\n    #plot repaid loans\n    sns.kdeplot(app_train.loc[app_train[\"TARGET\"]==0, source], label ='target == 0')\n    #plot loans that were not repaid\n    sns.kdeplot(app_train.loc[app_train[\"TARGET\"]==1, source], label = 'target == 1')\n    \n    # label the plots\n    plt.title(\"Distribution of %s by Target Value\" %source)\n    plt.xlabel('%s'% source); plt.ylabel('Density');\n    plt.legend();\nplt.tight_layout(h_pad=2.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The EXT_SOURCE_3 displays the greates diffrences between the values of the target. We can clearly see that this featoure has some relationship with the repaying the loan. It is very weak but still exists."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Copy the data for plotting\nplot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n\n# Add in the age of the client in years\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n\n# Drop na values and limit to first 100000 rows\nplot_data = plot_data.dropna().loc[:100000, :]\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)\n\n# Create the pairgrid object\ngrid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n                    hue = 'TARGET', \n                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.2)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature enigneering \n1. Polynomial features\n2. Domain knowledge features"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Make a new dataframe for polynomial features\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\n# imputer for handling missing values\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy = 'median')\n\npoly_target = poly_features['TARGET']\n\npoly_features = poly_features.drop(columns = ['TARGET'])\n\n# Need to impute missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n                                  \n# Create the polynomial object with specified degree\npoly_transformer = PolynomialFeatures(degree = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create a dataframe of the features \npoly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Add in the target\npoly_features['TARGET'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Merge polynomial features into training dataframe\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n\n# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}