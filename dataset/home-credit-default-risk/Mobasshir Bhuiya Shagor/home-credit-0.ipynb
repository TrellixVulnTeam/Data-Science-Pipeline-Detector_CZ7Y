{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# numpy and pandas for data manipulation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# File system manangement\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List files available\nprint(os.listdir(\"../input/home-credit-default-risk\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dir_name = \"../input/home-credit-default-risk\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train = pd.read_csv(os.path.join(dir_name,'application_train.csv'))\nprint(\"Training data shape: \", app_train.shape)\napp_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Application train column names: \")\nprint(\"------------------------------\")\nfor _ in app_train.columns.values:\n    print(_, end=' , ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **The training data has 307511 observations (each one a separate loan) **"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_test = pd.read_csv(os.path.join(dir_name,'application_test.csv'))\nprint(\"Test data shape: \", app_test.shape)\napp_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Application test column names: \")\nprint(\"------------------------------\")\nfor _ in app_test.columns.values:\n    print(_, end=' , ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Exploratory Data Analysis**\n> Exploratory Data Analysis (EDA) is an open-ended process where we calculate\n* statistics\n* make figures to find trends\n* anomalies\n* patterns\n* relationships\nwithin the data. "},{"metadata":{},"cell_type":"markdown","source":"**Emining the distributions of the target columns**\n> Here we will determine the distributions of loan repayment or not related data"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Distributions of repaid and non-repaid loan record**"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['TARGET'].astype(int).plot.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Missing values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_values_table(df):\n    # Total missing values\n    missing_values = df.isnull().sum()\n#     print(missing_values)\n    # Percentage of missing values\n    missing_values_percentage = 100 * missing_values / len(df)\n#     print(missing_values_percentage)\n    missing_values_table = pd.concat([missing_values,missing_values_percentage],axis=1)\n    print(\"Missing values and percentage shape: \",missing_values_table.shape)\n#     print(missing_values_table)\n    # Renaming the column names\n    missing_values_rename_columns = missing_values_table.rename(columns = {0: 'Missing Values', 1: 'Missing % of total values'})\n#     print(missing_values_rename_columns)\n    # Sorting the table by percentage of missing descendents\n    missing_values_rename_columns = missing_values_rename_columns[missing_values_rename_columns.iloc[:,1] != 0].sort_values('Missing % of total values', ascending = False).round(1)\n#     print(missing_values_rename_columns)\n    print(f'This dataframe has {df.shape[1]} columns. There are {missing_values_rename_columns.shape[0]} columns that have missing values.')\n    \n    return missing_values_rename_columns\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train_missing_values = missing_values_table(app_train)\napp_train_missing_values.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Checking column types**"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train.select_dtypes('object').apply(pd.Series.nunique,axis = 0) # here axis = 1 means row in the dataframe and 0 means column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Encoding Categorical Variables**\n\n> * Label encoding\n> * One hot encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a label encoder object\ndef label_encoder_train_test(train, test):\n\n    le = LabelEncoder()\n    le_count = 0\n\n    # Iterate through the columns\n    for col in train:\n        if train[col].dtype == 'object':\n            if len(list(train[col].unique())) <= 2:\n                le.fit(train[col])\n                train[col] = le.transform(train[col])\n                test[col] = le.transform(test[col])\n                le_count += 1\n\n    print(f'{le_count} columns were encoded.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder_train_test(app_train,app_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One hot encoding for more than two categorical values\ndef one_hot_encoding_train_test(train, test):\n    train = pd.get_dummies(train)\n    test = pd.get_dummies(test)\n    \n    print(f\"Training feature shape: {train.shape}\")\n    print(f\"Testing feature shape: {test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_encoding_train_test(app_train,app_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Aligning Train and Test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = app_train['TARGET']\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join='inner', axis=1) # axis = 1 for column based alignment\napp_train['TARGET'] = train_labels\n\nprint(f'Training feature shape: {app_train.shape}')\nprint(f'Testing feature shape: {app_test.shape}')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Back to Exploratory Data Analysis\n> **Anomalies**\n* One way to support anomalies quantitatively is by looking at the statistics of a column using the describe method\n* These may be due to **mis-typed numbers**, **errors in measuring equipment**, or **they could be valid but extreme measurements**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The numbers in the DAYS_BIRTH column are negative because they are recorded relative to the current loan application\nage = (app_train['DAYS_BIRTH']/ -365).describe()\nage","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train.DAYS_EMPLOYED.plot.hist(title='Days employment histogram');\nplt.xlabel('Days employment');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Anomalous value eradication  "},{"metadata":{"trusted":true},"cell_type":"code","source":"anomalous = app_train[app_train.DAYS_EMPLOYED == 365243]\nnon_anomalous = app_train[app_train.DAYS_EMPLOYED != 365243]\n\nprint(f'The non-anomalous default on {non_anomalous.TARGET.mean() * 100}% of loans')\nprint(f'The anomalous default on {anomalous.TARGET.mean() * 100}% of loans')\nprint(f'There are {len(anomalous)} anomalous days of employment')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train.DAYS_EMPLOYED_ANOM = app_train.DAYS_EMPLOYED == 365243\napp_train.DAYS_EMPLOYED.replace({365243: np.nan}, inplace = True)\napp_train.DAYS_EMPLOYED.plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_test.DAYS_EMPLOYED_ANOM = app_test.DAYS_EMPLOYED == 365243\napp_test.DAYS_EMPLOYED.replace({365243: np.nan}, inplace = True)\nprint(f'There are {app_test.DAYS_EMPLOYED_ANOM.sum()} anomalies in the test data out of {len(app_test)} entries.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlations\n>  Pearson correlation coefficient\n\n* .00-.19 “very weak”\n* .20-.39 “weak”\n* .40-.59 “moderate”\n* .60-.79 “strong”\n* .80-1.0 “very strong”"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = app_train.corr()['TARGET'].sort_values()\n\nprint('Most positive correlations:\\n', correlations.tail(15))\nprint('\\nMost negative correlations:\\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Effect of age on repayment**"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train.DAYS_BIRTH = abs(app_train.DAYS_BIRTH)\napp_train.DAYS_BIRTH.corr(app_train.TARGET)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\nplt.hist(app_train['DAYS_BIRTH']/365, edgecolor='k', bins=25);\nplt.title('Age of client');plt.xlabel('Age (years)');plt.ylabel('Count');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**kernel density estimation plot (KDE)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"## TODO: What KDE actually depicts here? Find out.\n\nplt.figure(figsize = (10,8))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(app_train.loc[app_train.TARGET == 0, 'DAYS_BIRTH']/365, label='target == 0')\n#KDE plot of loans that were not repaid om time\nsns.kdeplot()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}