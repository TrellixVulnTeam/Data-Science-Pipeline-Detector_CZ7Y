{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center>Level 3. Home Credit Default Risk</center>\n### <center>- Start Here: A Gentle Introduction -</center>\n# <center>**주택 대출 미상환 고객 예측**</center>\n## <center>제작자 : 서상훈</center>"},{"metadata":{},"cell_type":"markdown","source":"---\n\n - 커리큘럼 : https://kaggle-kr.tistory.com/32?category=868318\n\n - 원문 : https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction\n---"},{"metadata":{},"cell_type":"markdown","source":"# 목차\n1. 소개\n<br>\n\n2. 문제 정의, 데이터 설명\n<br>2-1. 문제 정의\n<br>2-2. 데이터 정의\n<br>2-3. 데이터 불러오기\n<br>\n\n3. EDA ( 탐색적 데이터 분석 )\n\n4. Feature Engineering\n<br>4-1. Polynomial Features\n<br>4-2. Domain Knowledge Features\n\n5. 데이터 모델링"},{"metadata":{},"cell_type":"markdown","source":"# 1. 소개\n>본 노트는 머신러닝에 익숙하지 않거나 문제에 쉽게 이해할 수 있는 사람들을 위한 것이다. \n\n이에 따라 복잡한 모델을 사용하거나 많은 데이터를 결합하는 것을 하지 않는다.\n\n이 챌린지의 목적은 대출 신청자가 대출을 상환할 수 있는지 \n여부를 예측하기 위해 과거 대출 신청 데이터를 활용한다.\n\n본 노트의 목표는 최대한 쉽게 예측값을 도출할 수 있도록 한다.\n레이블의 분류는 다음과 같은 이진 분류로 이루어진다.\n - 0 ( 정시 대출 상환 )\n - 1 ( 정시 대출 상환 어려움 )"},{"metadata":{},"cell_type":"markdown","source":"# 2. 문제 정의, 데이터 정의\n## 2-1. 문제 정의\n여기서 프로젝트의 목표는 이미 제시되었다.\n\n> <span style=\"color:red\">\" 각 신청자가 대출을 상환 할 수있는 능력을 예측하시오.\"</span>\n\n아래는 Home Credit 프로젝트의 요약본이다.\n\n> 1997년 체코에서 설립된 비은행 금융 기관입니다.<br> \n신용기록이 적거나 전혀 없는 사람들을 대상으로 대출에 중점을 두고 있습니다.<br>\n본 사의 Home Credit 은 긍정적이고 안전한 대출 경험을 제공함으로써 고객을 확보하려 노력합니다.<br>\n이러한 인구들이 긍정적인 대출 경험을 갖도록 하기 위해 Home Credit은 고객의 상환 능력을 예측하기 위해 <br>\n전화 및 거래 정보를 포함한 다양한 데이터를 활용합니다. 이를 통해 상환 가능한 고객이 거부되지 않고 <br>\n대출에 성공할 수 있도록 하는 것이 목표입니다. Home Credit은 현재 이러한 통계를 예측하기 위해 <br>\n다양한 통계 및 머신 러닝 방법을 사용하고 있지만, Kagglers들의 방법을 보기 위해 이 챌린지를 제안합니다."},{"metadata":{},"cell_type":"markdown","source":"## 2-2. 데이터 정의\n\n데이터는 Home Credit 에 의해서 제공되며, 총 7가지로 구성된다.\n\n - application_train/application_test <br>\n Home Credit의 각 대출 신청에 대한 정보를 가진 데이터\n 모든 대출내역은 각 행을 가지며, SK_ID_CURR feature로 판단하며, 그 기준은\n 0 : 대출상환 되었음, 1 : 대출상환 되지 않았음 으로 나뉜다.\n - bureau <br>\n 다른 금융 기관으로부터의 고객의 이전 대출 내역\n - bureau_balance <br>\n bureau에 대한 월별 데이터\n - previous_application <br>\n Home Credit 으로부터의 고객의 이전 대출 내역\n SK_ID_PREV feature 로 판단한다.\n - POS_CASH_balance <br>\n application data 에 대출이 있는 고객의 이전 대출 신청 내역\n - credit_card_balance <br>\n Home Credit 을 사용하는 고객의 신용 카드 내역, 월별 데이터\n - installments_payment <br>\n Home Credit 을 사용했던 고객의 대출 상환 내역\n \n 아래의 표는 모든 데이터가 어떻게 연관되어있는지 보여준다.\n\n![home_credit](https://user-images.githubusercontent.com/53182751/69926570-4f1d8100-14f8-11ea-8111-1f6dfc90f867.png)\n\n이를 범주로 나타내면 6가지로 나누어 볼 수 있다.\n1. 인구통계적 특성\n2. 직업 및 소득 관련 특성\n3. 기본 자산 현황\n4. 거주지 관련 특성\n5. 외부 신용평가 자료\n6. 신청한 대출의 특성\n\n우리는 이 경쟁에서 순위를 높이기 위한 것이 아닌 입문자로서\n최대한 하나씩 이해하는 것이 목표이므로, 본 노트에서는 \napplication_train application_test 만 사용할 것이다."},{"metadata":{},"cell_type":"markdown","source":"## 2-3. 데이터 불러오기\n\n파이썬 패키지로는 numpy, pandas, sklearn, matplotlib 를 사용할 예정이다."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# 오류 무시\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# 시각화를 위한 패키지\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(os.listdir(\"../input/\"))\n\n# 파일명 확인\nprint(os.listdir(\"../input/home-credit-default-risk/\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training data 확인\napp_train = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training data 는 307,511 개의 행과 122개의 열(feature)을 가지고 있다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing data 확인\napp_test = pd.read_csv('../input/home-credit-default-risk/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test data 는 Training 보다 행이 적으며, 목표(TARGET) 열이 없다."},{"metadata":{},"cell_type":"markdown","source":"# 3. EDA (탐색적 데이터 분석)\n\nEDA 는 데이터를 보고 데이터의 추세, 이상, 패턴 또는 관계를 찾기 위한 것이다.<br>\n이를 통해 데이터가 무엇을 말하는지 파악하고 모델링 선택에 도움을 줄 수 있다.\n\n각 데이터가 어떻게 이루어졌는지 알아보자."},{"metadata":{},"cell_type":"markdown","source":"### Target 의 분포 조사"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['TARGET'].astype(int).plot.hist();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"위의 0(대출 정시 상환)이 1(대출 정시 상환 x) 에 비해 높다 라는 <br>\n데이터를 통해 이 문제가 지난 2번 주제인 Porto 의 데이터 셋과 마찬가지로<br>\n불균형 문제인 것을 알 수 있다."},{"metadata":{},"cell_type":"markdown","source":"### 결측값 조사"},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_values_table(df):\n        # 총 결측값\n        mis_val = df.isnull().sum()\n        \n        # 결측값의 비율\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # 결과를 테이블에 저장\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # 열의 이름 수정\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # 결측값 내림차순으로 정렬\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # 요약\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # return\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 결측값 통계\nmissing_values = missing_values_table(app_train)\nmissing_values.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"머신러닝 모델링을 할 때, 위의 결측값을 지우거나 채워야 한다.\n우리는 결측값이 높은 feature 를 없애지 않고 최대한 모두 이용할 것이다.\n\n추후 XGBoost 같은 모델을 사용할 때에는 결측값을 처리하는 방법이 보다 다양하다."},{"metadata":{},"cell_type":"markdown","source":"### 각 열의 타입 파악"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 각 열의 타입 개수\napp_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of unique classes in each object column\n# object 열(범주형 변수)의 고유 항목 수 파악\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 범주형 변수 인코딩\n\n더 나아가기 전에, 범주형 변수들을 다룰 필요가 있다.\n머신러닝의 대부분 모델들은 범주형 변수를 다룰 수 없다. (LightGBM 등을 제외)\n따라서 이러한 범주형 변수들을 숫자로 인코딩 할 수 있는 방법을 찾아야 한다.\n인코딩 하는 방법은 2가지가 있다.\n\n - Label encoding <br>\n ![label_encoding](https://user-images.githubusercontent.com/53182751/69928041-ba1d8680-14fd-11ea-91c6-c502b8638dc1.png)\n\n - one-hot encoding <br>\n ![one_hot_encoding](https://user-images.githubusercontent.com/53182751/69928068-d4576480-14fd-11ea-8ba6-ff8e93312f6e.png)\n \n 일반적으로 범주형 변수(남성/여성 등)가 2개의 고유 값만 가지고 있다면\n Lable encoding 은 괜찮지만, 2개 이상의 고유 값을 가지고 있다면\n One-hot encoding 을 사용하는 것이 괜찮으므로 본 노트에서는\n 2개의 변수를 가지고 있는 범주형 변수는 Label encoding\n 2개 이사으이 변수를 가지고 있는 범주형 변수는 one-hot encoding 을 진행한다.\n"},{"metadata":{},"cell_type":"markdown","source":"### Label Encoding and One-Hot Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(app_train[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train, Test 데이터의 열 정렬\n\nTrain, Test 데이터가 동일한 열의 개수를 가지고 있어야 한다.\n\none-hot encoding 을 진행하면서 생긴 dummy 로 컬럼이 더 생겨 이를 Test 열의 수와 동일하게 만들어주자."},{"metadata":{"trusted":true},"cell_type":"code","source":"# TARGET 을 보존하기 위해 추출\ntrain_labels = app_train['TARGET']\n\n# 데이터 프레임으로 Train, Test 데이터 정렬 ( 열의 개수 맞춤 )\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n\n# TARGET 추가\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 다시 EDA \n\n#### 이상 유무 파악\n\nEDA를 수행할 때 항상 조심해야 할 것은 데이터 내의 이상이다.<br>\n이는 측정 장비의 오류, 유효하지만 극단적인 측정 등이 있다.<br>\n\nDAYS_BIRTH 를 살펴보자\n여기서 DAYS_BIRTH 란, 나이를 의미한다.<br>\n이 외에도 DAYS_EMPLOYED 등 DAYS 로 시작되는 변수가 있는데,<br>\n이런 변수들은 \"대출 신청 시점 - 해당 사건 발생 시점\"으로 계산하기 떄문에<br>\n마이너스(-) 형태로 표시되는 것에 주의 해야 한다.\n\n이 데이터를 잘 활용하면 많은 분석을 할 수 있는데, 가령 DAYS_BIRTH 를<br>\n양수로 바꾸고 365로 나누게 되면 나이가 계산되고, 이를 10살 씩 묶게 되면<br>\n연령대별 특성이 어떻게 나타나는지 분석이 가능하다.\n\n이제 나이에 따른 이상 유무를 파악해보자."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 나이 특이치\n(app_train['DAYS_BIRTH'] / -365).describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"위의 결과를 보면 나이대가 합리적으로 보인다.<br>\n높은 쪽이나 낮은 쪽에 특이치가 없어 보인다.\n\n다음으로 고용일을 보자"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"최대 가치가 1000년이므로 이 데이터는 이상이 있다고 볼 수 있다."},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"다음으로 변칙적인 고객들이 일반적인 고객에 비해 <br>\n대출 상환률의 경향이 어떤지 파악해보자."},{"metadata":{"trusted":true},"cell_type":"code","source":"anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이상 고객의 대출 상환률이 일반적인 고객보다 낮다는 것을 알 수 있다!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 상관관계\n\n범주형 변수와 이상치를 다루었으니 EDA를 계속해보자.\n데이터를 이해하기 위해서는 feature 간의 상관관계를 파악하는 것이다.\n\n상관계수의 절대값에 대한 일반적인 해석은 다음과 같다.\n\n - .00-.19 “매우 약한 상관관계”\n - .20-.39 “약한 상관관계”\n - .40-.59 “중간 상관관계”\n - .60-.79 “강한 상관관계”\n - .80-1.0 “매우 강한 상관관계”"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find correlations with the target and sort\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 나이가 대출 상황에 미치는 영향 파악"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the correlation of the positive days since birth and target\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이를 해석해보면 고객의 나이가 들수록 대출금을 더 자주<br>\n상환하는 경향이 있다는 목표 의미와 부정적인 선형 관계가 있다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the style of plots\nplt.style.use('fivethirtyeight')\n\n# Plot the distribution of ages in years\nplt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 8))\n\n# 정시 상환된 대출의 나이 분포\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n\n# 정시 상환되지 않은 대출의 나이 분포\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n\n# Labeling of plot\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"연령이 낮을 수록 대출 상환 능력이 떨어지는 것을 볼 수 있다.\n\n좀 더 제대로 파악하기 위해 구간을 분할해보자."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Age information into a separate dataframe\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group by the bin and calculate averages\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이를 해석해보면 나이가 젊을수록 대출금을 <br>\n상환하지 않을 가능성이 높다는 것을 알 수 있다."},{"metadata":{},"cell_type":"markdown","source":"### 외부 금융기관으로부터의 데이터\n\nTARGET 과 가장 강한 음의 상관관계를 갖는 3개의 변수는\nEXT_SURCE_1, EXT_SURCE_2, EXT_SURCE_3 문서에 따르면\n이 feature 들은 신용등급을 의미한다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 6))\n\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3 가지 EXT_SOURCE 기능 모두 TARGET 과 음의 상관관계를 가지고 있어<br>\nEXT_SOURCE 의 가치가 증가함에 따라 고객이 대출금을 상환할 가능성이<br>\n높다는 것을 나타낸다.\n\n또한 DayS_BIRTH와 EXT_SOURCE_1 과 양의 상관관계를 보임으로써 이 점수의\n요인 중 하나가 고객 연령임을 알 수 있다."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 12))\n\n# iterate through the sources\nfor i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    \n    # create a new subplot for each source\n    plt.subplot(3, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"위의 그래프를 해석해보면 EXT_SOURCE_3이 대상 값 사이의 큰 차이를 보이므로,<br>\n이 특징이 신청자가 대출금을 상환할 가능성과 어떤 관계가 있을을 알 수 있다."},{"metadata":{},"cell_type":"markdown","source":"## Pairs Plot\n\n최종 탐색적 그래프로서, EXT_SOURCE 변수와 DatS_BIRTH 변수의 그래프를 만들 수 있다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy the data for plotting\nplot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n\n# Add in the age of the client in years\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n\n# Drop na values and limit to first 100000 rows\nplot_data = plot_data.dropna().loc[:100000, :]\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)\n\n# Create the pairgrid object\ngrid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n                    hue = 'TARGET', \n                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.2)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Feature Engineering\n\n## 4-1. Polynomial Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 교호 작용 변수들을 만들기 위해 아래의 변수들을 불러오자\n# 결측값 채우기\n\n# Make a new dataframe for polynomial features\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\n# imputer for handling missing values\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'median')\n\npoly_target = poly_features['TARGET']\n\npoly_features = poly_features.drop(columns = ['TARGET'])\n\n# Need to impute missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n                                  \n# Create the polynomial object with specified degree\npoly_transformer = PolynomialFeatures(degree = 3) # 여기서 degree 는 다항식을 의미","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe of the features \npoly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Add in the target\npoly_features['TARGET'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"새롭게 생성한 변수 중 몇몇 변수는 기존의 변수보다 TARGET 과 <br>\n더 큰 상관 관계를 갖는다. 이를 Train, Test 데이터에 <br>\n넣었을 때와 넣지 않았을 때를 비교 후 효과를 시험해 볼 수 있다. <br>\n\n새롭게 생성한 값들이 영향이 있을지 모르지만, 본 커널의 원 저자는 merge를 시켜준다.\n기존의 app_train 이라는 원본 데이터에서 SK_ID_CURR을 가져온 뒤 poly_feature에 넣어주고\n조인을 시켜준다. 그리고 다시 정렬하자.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Merge polynomial features into training dataframe\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n\n# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4-2. Domain Knowledge Features\n\n교호 작용 변수 외에도 고객의 대출 반환 여부를 판단하는 데에\n몇 가지 feature를 더 만들 수 있다.\n\n - CREDIT_INCOME_PERCENT <br>\n 고객 소득에 대한 신용 금액의 백분율\n - ANNUILY_INCOME_PERCENT <br>\n 고객 소득에 대한 대출 연금의 백분율\n - CREDIT_TERM <br>\n 월 지불의기간\n - DAYS_EMPLOYEM_PERCENT \n 고객의 연령에 비해 고용된 일의 백분율"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"새롭게 추가된 변수와 TARGET 을 시각화 해보자"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 20))\n# iterate through the new features\nfor i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n    \n    # create a new subplot for each source\n    plt.subplot(4, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % feature)\n    plt.xlabel('%s' % feature); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"새롭게 추가한 feature의 효과를 시험하려면 테스트 하는 수 밖에 없다."},{"metadata":{},"cell_type":"markdown","source":"## 5. 데이터 모델링\n\n### 랜덤 포레스트"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, Imputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns = ['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# 결측값의 중앙값으로 채우기\nimputer = Imputer(strategy = 'median')\n\n# 스케일을 0~1로 지정\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train on the training data\nrandom_forest.fit(train, train_labels)\n\n# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = random_forest.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest.score(train, train_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이제 결과를 제출하자"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"랜덤 포레스트 모델에서 어떤 변수가 중요하게 쓰였는지<br>\nfeature_importances_를 확인해보자"},{"metadata":{"trusted":true},"cell_type":"code","source":"a = feature_importances.sort_values('importance'\n                                   , ascending=False).reset_index().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.barplot(a['importance'], a['feature'])\nsns.despine(left=True, bottom=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}