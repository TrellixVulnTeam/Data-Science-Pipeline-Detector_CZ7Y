{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nHomeCredit_columns_description = pd.read_csv(\"../input/home-credit-default-risk/HomeCredit_columns_description.csv\",encoding= 'unicode_escape')\nPOS_CASH_balance = pd.read_csv(\"../input/home-credit-default-risk/POS_CASH_balance.csv\",encoding= 'unicode_escape')\napplication_test = pd.read_csv(\"../input/home-credit-default-risk/application_test.csv\",encoding= 'unicode_escape')\napplication_train = pd.read_csv(\"../input/home-credit-default-risk/application_train.csv\",encoding= 'unicode_escape')\nbureau = pd.read_csv(\"../input/home-credit-default-risk/bureau.csv\",encoding= 'unicode_escape')\nbureau_balance = pd.read_csv(\"../input/home-credit-default-risk/bureau_balance.csv\",encoding= 'unicode_escape')\ncredit_card_balance = pd.read_csv(\"../input/home-credit-default-risk/credit_card_balance.csv\",encoding= 'unicode_escape')\ninstallments_payments = pd.read_csv(\"../input/home-credit-default-risk/installments_payments.csv\",encoding= 'unicode_escape')\nprevious_application = pd.read_csv(\"../input/home-credit-default-risk/previous_application.csv\",encoding= 'unicode_escape')\nsample_submission = pd.read_csv(\"../input/home-credit-default-risk/sample_submission.csv\",encoding= 'unicode_escape')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 상환 가능한 고객이 거부되지 않고 고객이 성공할 수 있도록 원금, 만기 및 상환 일정으로 대출"},{"metadata":{},"cell_type":"markdown","source":"- application - train / test\n  main table, statci Data\n\n  하나의 row가 하나의 대출을 나타낸다.\n\n- bureau\n\n  Credit Bureau에보고 된 다른 금융 기관에서 제공 한 모든 고객의 이전 크레딧\n\n  샘플의 모든 대출에는 신청 날짜 이전에 고객이 신용 관리국에 보유한 크레딧 수만큼의 행이 존재\n\n- bureau_balance\n  Credit Bureau에서 이전 크레딧의 월별 잔액\n\n  Bureau에 보고된 모든 이전 크레딧의 월별 이력에 대한 행\n\n- credit_card_balance\n  \n  신청자가 홈 크레딧으로 보유한 이전 신용 카드의 월별 잔액\n\n- previous_application.csv\n  \n  대출이있는 고객의 Home Credit 대출에 대한 이전의 모든 신청\n\n- installments_payments\n  \n  샘플의 대출과 관련하여 Home Credit에서 이전에 지불 된 크레딧에 대한 상환 내역\n\n- HomeCredit_columns_description"},{"metadata":{},"cell_type":"markdown","source":"- bureau : \nHome Credit에보고 된 다른 금융 기관과의 고객의 이전 대출에 관한 정보. 이전의 각 대출에는 자체 행이 있습니다.\n- bureau_balance : \n이전 대출에 대한 월간 정보. 매월 자체 행이 있습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Counts of a client's previous loans\n- 이전 대출 횟수\n- 먼저 다른 금융 기관에서 고객의 이전 대출 수를 가져온다.\n\n\n- groupby : SK_ID_CURR 열을 기준으로 그룹화\n- agg : 열의 평균을 취하는 등 그룹화 된 데이터에 대한 계산을 수행\n- merge : SK_ID_CURR 열의 계산 된 통계와 병합 , 해당 값 없는 셀에 Nan값"},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Credit Bureau에보고 된 다른 금융 기관에서 제공 한 모든 고객의 이전 크레딧"},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"previous_loan_counts = bureau.groupby('SK_ID_CURR', as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU' : 'previous_loan_counts'})\nprevious_loan_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau.groupby('SK_ID_CURR')['SK_ID_BUREAU'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min(bureau['SK_ID_CURR'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/home-credit-default-risk/application_train.csv\",encoding= 'unicode_escape')\ntrain = train.merge(previous_loan_counts, on = 'SK_ID_CURR',how='left')\n\ntrain['previous_loan_counts'] = train['previous_loan_counts'].fillna(0)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Assessing Usefulness of New Variable with r value\n새 변수가 유용한 지 확인하기 위해 변수와 대상 간의 Pearson 상관계수 계산\n\n두 변수 사이의 선형관계의 강도를 측정하며 범위는 -1 ~ +1 이다.\n\n변수의 r-값이 클수록 변수의 변경이 대상의 값에 영향을 줄 수 있다.\n\n목표 대비 절대값 r-값이 가장 큰 변수를 찾는다."},{"metadata":{},"cell_type":"markdown","source":"#### KDE PLOT - 단일 변수의 분포를 보여준다."},{"metadata":{"trusted":true},"cell_type":"code","source":"def kde_target(var_name, df):\n    corr = df['TARGET'].corr(df[var_name])\n    \n    avg_repaid = df.ix[df['TARGET']==0, var_name].median()\n    avg_not_repaid = df.ix[df['TARGET']==1, var_name].median()\n    \n    plt.figure(figsize=(12,6))\n    \n    sns.kdeplot(df.ix[df['TARGET']==0,var_name],label='TARGET==0')\n    sns.kdeplot(df.ix[df['TARGET'] == 1, var_name], label = 'TARGET == 1')\n    \n    plt.xlabel(var_name)\n    plt.ylabel('Density')\n    plt.title('%s Distribution' % var_name)\n    plt.legend()\n    \n    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid)\n    print('Median value for loan that was repaid =     %0.4f' % avg_repaid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kde_target('EXT_SOURCE_3', train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kde_target('previous_loan_counts', train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Numeric Columns 집계"},{"metadata":{},"cell_type":"markdown","source":"숫자형태의 열에 대한 통계를 계산하기 위해 ID별 그룹화해서 훈련데이터에 다시 병합"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 클라이언트 id별로 그룹화, 집계 통계 계산\nbureau_agg = bureau.drop(columns = ['SK_ID_BUREAU']).groupby('SK_ID_CURR', as_index = False).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\nbureau_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of column names\ncolumns = ['SK_ID_CURR']\n\n# Iterate through the variables names\nfor var in bureau_agg.columns.levels[0]:\n    # Skip the id name\n    if var != 'SK_ID_CURR':\n        \n        # Iterate through the stat names\n        for stat in bureau_agg.columns.levels[1][:-1]:\n            # Make a new column name for the variable and stat\n            columns.append('bureau_%s_%s' % (var, stat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#열 이름 목록을 데이터 프레임 열 이름으로\nbureau_agg.columns = columns\nbureau_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train데이터와 합병\ntrain = train.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#새로운 상관관계\nnew_corrs = []\n\n\nfor col in columns:\n    corr = train['TARGET'].corr(train[col])\n    new_corrs.append((col, corr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#상관관계 정렬\nnew_corrs = sorted(new_corrs, key = lambda x: abs(x[1]), reverse = True)\nnew_corrs[:15]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"새로 만든 변수는 target값과 상관관계가 없음, bureau_DAYS_CREDIT_mean과의 관계 파악"},{"metadata":{"trusted":true},"cell_type":"code","source":"kde_target('bureau_DAYS_CREDIT_mean', train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" 변수의 평균과 과거에 추가로 대출을 신청 한 고객이 Home Credit에서 대출을 상환 할 가능성이 높다는 의미 사이에 매우 약한 긍정적 관계"},{"metadata":{},"cell_type":"markdown","source":"### 이전 작업 캡슐화"},{"metadata":{"trusted":true},"cell_type":"code","source":"def agg_numeric(df, group_var, df_name):\n    \"\"\"Aggregates the numeric values in a dataframe. This can\n    be used to create features for each instance of the grouping variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the dataframe to calculate the statistics on\n        group_var (string): \n            the variable by which to group df\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated for \n            all numeric columns. Each instance of the grouping variable will have \n            the statistics (mean, min, max, sum; currently supported) calculated. \n            The columns are also renamed to keep track of features created.\n    \n    \"\"\"\n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != group_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    group_ids = df[group_var]\n    numeric_df = df.select_dtypes('number')\n    numeric_df[group_var] = group_ids\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n\n    # Need to create new column names\n    columns = [group_var]\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        # Skip the grouping variable\n        if var != group_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1][:-1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n\n    agg.columns = columns\n    return agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_agg_new = agg_numeric(bureau.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_agg_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_agg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"값을 살펴보고 살펴보면 값이 같다는 것을 알게됩니다. 다른 데이터 프레임의 수치 통계를 계산하기 위해이 기능을 재사용 할 수 있습니다."},{"metadata":{},"cell_type":"markdown","source":"### Correlation Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def target_corrs(df):\n\n    # List of correlations\n    corrs = []\n\n    # Iterate through the columns \n    for col in df.columns:\n        print(col)\n        # Skip the target column\n        if col != 'TARGET':\n            # Calculate correlation with the target\n            corr = df['TARGET'].corr(df[col])\n\n            # Append the list as a tuple\n            corrs.append((col, corr))\n            \n    # Sort by absolute magnitude of correlations\n    corrs = sorted(corrs, key = lambda x: abs(x[1]), reverse = True)\n    \n    return corrs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical Variables"},{"metadata":{},"cell_type":"markdown","source":"각 범주 형 변수 내에서 각 범주의 값 카운트를 계산"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = pd.get_dummies(bureau.select_dtypes('object'))\ncategorical['SK_ID_CURR'] = bureau['SK_ID_CURR']\ncategorical.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_grouped = categorical.groupby('SK_ID_CURR').agg(['sum', 'mean'])\ncategorical_grouped.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- sum : 연결된 클라이언트에 대한 해당 범주의 개수를 나타낸다\n- mean : 정규화 된 개수를 나타낸다"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_grouped.columns.levels[0][:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_grouped.columns.levels[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group_var = 'SK_ID_CURR'\n\n# Need to create new column names\ncolumns = []\n\n# Iterate through the variables names\nfor var in categorical_grouped.columns.levels[0]:\n    # Skip the grouping variable\n    if var != group_var:\n        # Iterate through the stat names\n        for stat in ['count', 'count_norm']:\n            # Make a new column name for the variable and stat\n            columns.append('%s_%s' % (var, stat))\n\n#  Rename the columns\ncategorical_grouped.columns = columns\n\ncategorical_grouped.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.merge(categorical_grouped, left_on = 'SK_ID_CURR', right_index = True, how = 'left')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.iloc[:10, 123:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Function to Handle Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_categorical(df, group_var, df_name):\n    \"\"\"Computes counts and normalized counts for each observation\n    of `group_var` of each unique category in every categorical variable\n    \n    Parameters\n    --------\n    df : dataframe \n        The dataframe to calculate the value counts for.\n        \n    group_var : string\n        The variable by which to group the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    df_name : string\n        Variable added to the front of column names to keep track of columns\n\n    \n    Return\n    --------\n    categorical : dataframe\n        A dataframe with counts and normalized counts of each unique category in every categorical variable\n        with one row for every unique value of the `group_var`.\n        \n    \"\"\"\n    \n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('object'))\n\n    # Make sure to put the identifying id on the column\n    categorical[group_var] = df[group_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['count', 'count_norm']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    return categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_counts = count_categorical(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_counts.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 다른 데이터프레임에 연산 적용"},{"metadata":{},"cell_type":"markdown","source":"- 다른 금융 기관과의 각 고객의 이전 대출에 대한 월별 정보 존재\n- 이 데이터 프레임을 클라이언트 ID 인 SK_ID_CURR로 그룹화하는 대신 먼저 이전 대출의 ID 인 SK_ID_BUREAU로 데이터 프레임을 그룹화\n- 각 대출에 대한 하나의 데이터 프레임 행을 제공\n- SK_ID_CURR로 그룹화하고 각 클라이언트의 대출에 대한 집계를 계산\n- 최종 결과는 각 고객에 대해 하나의 행이있는 데이터 프레임이되고 대출에 대한 통계가 계산"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in bureau balance\nbureau_balance = pd.read_csv(\"../input/home-credit-default-risk/bureau_balance.csv\",encoding= 'unicode_escape')\nbureau_balance.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"각가의 대출에 대한 status 계산"},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_balance_counts = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_counts.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- MONTHS_BALANCE 열에는 \"신청일에 대한 월 잔고\""},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate value count statistics for each `SK_ID_CURR` \nbureau_balance_agg = agg_numeric(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_agg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"각 대출에 대한 계산이 있습니다. 이제 각 클라이언트마다이를 집계해야합니다. 먼저 데이터 프레임을 병합하여이 작업을 수행 한 다음 모든 변수가 숫자이므로 통계를 다시 집계하면됩니다. 이번에는 SK_ID_CURR로 그룹화합니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataframe grouped by the loan\nbureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n\n# Merge to include the SK_ID_CURR\nbureau_by_loan = bureau_by_loan.merge(bureau[['SK_ID_BUREAU', 'SK_ID_CURR']], on = 'SK_ID_BUREAU', how = 'left')\n\nbureau_by_loan.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client')\nbureau_balance_by_client.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### bureau_balance\n- 각 대출별로 계산 된 통계 수치 그룹화\n- 대출을 기준으로 각 범주 형 변수 그룹화의 가치 계산\n- 통계와 융자 가치 합병\n- 클라이언트 ID별로 결과 데이터 프레임 그룹화에 대한 계산 된 숫자 통계"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Free up memory by deleting old objects\nimport gc\ngc.enable()\ndel train, bureau, bureau_balance, bureau_agg, bureau_agg_new, bureau_balance_agg, bureau_balance_counts, bureau_by_loan, bureau_balance_by_client, bureau_counts\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/home-credit-default-risk/application_train.csv\",encoding= 'unicode_escape')\nbureau = pd.read_csv(\"../input/home-credit-default-risk/bureau.csv\",encoding= 'unicode_escape')\nbureau_balance = pd.read_csv(\"../input/home-credit-default-risk/bureau_balance.csv\",encoding= 'unicode_escape')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Counts of Bureau Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_counts = count_categorical(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_counts.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Aggregated Stats of Bureau Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_agg = agg_numeric(bureau.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_agg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Value counts of Bureau Balance dataframe by loan"},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_balance_counts = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_counts.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bureau Balance 데이터 프레임을 대출로 집계"},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_balance_agg = agg_numeric(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_agg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### client로 집계"},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n\n# Merge to include the SK_ID_CURR\nbureau_by_loan = bureau[['SK_ID_BUREAU', 'SK_ID_CURR']].merge(bureau_by_loan, on = 'SK_ID_BUREAU', how = 'left')\n\n# Aggregate the stats for each client\nbureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_features = list(train.columns)\nprint('Original Number of Features: ', len(original_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the stats of bureau\ntrain = train.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the monthly information grouped by client\ntrain = train.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_features = list(train.columns)\nprint('Number of features using previous loans from other institutions data: ', len(new_features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_train = missing_values_table(train)\nmissing_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"결 측값이 90 %보다 큰 학습 또는 테스트 데이터의 열을 제거"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_train_vars = list(missing_train.index[missing_train['% of Total Values'] > 90])\nlen(missing_train_vars)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"결 측값을 제거하기 전에 테스트 데이터에서 결 측값 백분율을 찾습니다. 그런 다음 학습 또는 테스트 데이터에서 결 측값이 90 %보다 큰 모든 열을 제거\n\n테스트 데이터를 읽고 동일한 작업을 수행하고 테스트 데이터에서 누락 된 값을 살펴보기\n\n모든 수와 집계 통계를 계산 했으므로 테스트 데이터를 적절한 데이터와 병합"},{"metadata":{},"cell_type":"markdown","source":"### Calculate Information for Testing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/home-credit-default-risk/application_test.csv\",encoding= 'unicode_escape')\n\n# Merge with the value counts of bureau\ntest = test.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the stats of bureau\ntest = test.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the value counts of bureau balance\ntest = test.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of Testing Data: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = train['TARGET']\n\n# Align the dataframes, this will remove the 'TARGET' column\ntrain, test = train.align(test, join = 'inner', axis = 1)\n\ntrain['TARGET'] = train_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Data Shape: ', train.shape)\nprint('Testing Data Shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- train data와 test data 프레임 동일한 열 갖게 정렬"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_test = missing_values_table(test)\nmissing_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_test_vars = list(missing_test.index[missing_test['% of Total Values'] > 90])\nlen(missing_test_vars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_columns = list(set(missing_test_vars + missing_train_vars))\nprint('There are %d columns with more than 90%% missing in either the training or testing data.' % len(missing_columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the missing columns\ntrain = train.drop(columns = missing_columns)\ntest = test.drop(columns = missing_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.to_csv('train_bureau_raw.csv', index = False)\ntest.to_csv('test_bureau_raw.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlations"},{"metadata":{},"cell_type":"markdown","source":"target 과 변수 간의 상관관계"},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs = train.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs = corrs.sort_values('TARGET', ascending = False)\n\n# Ten most positive correlations\npd.DataFrame(corrs['TARGET'].head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(corrs['TARGET'].dropna().tail(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target 과 상관계수가 높은 변수는 만든 변수, 그러나 상관계수가 높다 유용한 것은 아님 "},{"metadata":{},"cell_type":"markdown","source":"### Collinear Variables"},{"metadata":{},"cell_type":"markdown","source":"대상과 변수의 상관 관계뿐만 아니라 각 변수와 다른 모든 변수의 상관 관계도 계산할 수 있습니다. 이를 통해 데이터에서 제거해야 할 공선 변수가 있는지 확인할 수 있습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = 0.8\n\n# Empty dictionary to hold correlated variables\nabove_threshold_vars = {}\n\n# For each column, record the variables that are above the threshold\nfor col in corrs:\n    above_threshold_vars[col] = list(corrs.index[corrs[col] > threshold])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_remove = []\ncols_seen = []\ncols_to_remove_pair = []\n\n# Iterate through columns and correlated columns\nfor key, value in above_threshold_vars.items():\n    # Keep track of columns already examined\n    cols_seen.append(key)\n    for x in value:\n        if x == key:\n            next\n        else:\n            # Only want to remove one in a pair\n            if x not in cols_seen:\n                cols_to_remove.append(x)\n                cols_to_remove_pair.append(key)\n            \ncols_to_remove = list(set(cols_to_remove))\nprint('Number of columns to remove: ', len(cols_to_remove))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_corrs_removed = train.drop(columns = cols_to_remove)\ntest_corrs_removed = test.drop(columns = cols_to_remove)\n\nprint('Training Corrs Removed Shape: ', train_corrs_removed.shape)\nprint('Testing Corrs Removed Shape: ', test_corrs_removed.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_corrs_removed.to_csv('train_bureau_corrs_removed.csv', index = False)\ntest_corrs_removed.to_csv('test_bureau_corrs_removed.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\nimport gc\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Control"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_control  = pd.read_csv(\"../input/home-credit-default-risk/application_test.csv\",encoding= 'unicode_escape')\ntrain_control  = pd.read_csv(\"../input/home-credit-default-risk/application_train.csv\",encoding= 'unicode_escape')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission, fi, metrics = model(train_control, test_control)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}