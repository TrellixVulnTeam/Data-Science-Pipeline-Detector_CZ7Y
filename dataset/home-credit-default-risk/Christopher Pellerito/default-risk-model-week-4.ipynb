{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Home Credit Default Risk - Team 3 (Kahsai, Nichols, Pellerito)","metadata":{}},{"cell_type":"markdown","source":"### Import packages","metadata":{}},{"cell_type":"code","source":"# standard Python tools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# special tools for working in Kaggle\nimport joblib   # save and load ML models\nimport gc       # garbage collection\nimport os \nimport sklearn\n\n# preprocessing steps\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# machine learning models and tools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\n\n# cross validation and metrics - remember this competition is scored as area under curve\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n# surely there will be a lot more packages loaded by the time we are done!","metadata":{"execution":{"iopub.status.busy":"2021-11-14T12:42:23.537251Z","iopub.execute_input":"2021-11-14T12:42:23.537567Z","iopub.status.idle":"2021-11-14T12:42:23.546888Z","shell.execute_reply.started":"2021-11-14T12:42:23.537535Z","shell.execute_reply":"2021-11-14T12:42:23.545753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Don't do this!\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-11-14T12:42:23.549126Z","iopub.execute_input":"2021-11-14T12:42:23.549748Z","iopub.status.idle":"2021-11-14T12:42:23.566819Z","shell.execute_reply.started":"2021-11-14T12:42:23.549617Z","shell.execute_reply":"2021-11-14T12:42:23.565957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First look at training data set","metadata":{}},{"cell_type":"markdown","source":"### Read the training data","metadata":{}},{"cell_type":"code","source":"MainDir = \"../input/../input/home-credit-default-risk\"\nprint(os.listdir(MainDir))\n\n# Main table\ntrain = pd.read_csv(f'{MainDir}/application_train.csv')\n\ntrain.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T12:42:23.568477Z","iopub.execute_input":"2021-11-14T12:42:23.568973Z","iopub.status.idle":"2021-11-14T12:42:27.652008Z","shell.execute_reply.started":"2021-11-14T12:42:23.568931Z","shell.execute_reply":"2021-11-14T12:42:27.650941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### data cleansing and feature engineering: create new features based on ratios, logs, etc.","metadata":{}},{"cell_type":"code","source":"# what is going on with days_employed? Over 50,000 entries have the value 365,243 days! Let's replace those with NaN and let the imputer deal with them.\ntrain['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\n# ratio features\ntrain['CI_ratio'] = train['AMT_CREDIT'] / train['AMT_INCOME_TOTAL']        # credit-to-income ratio\ntrain['AI_ratio'] = train['AMT_ANNUITY'] / train['AMT_INCOME_TOTAL']       # annuity-to-income ratio\ntrain['AC_ratio'] = train['AMT_CREDIT'] / train['AMT_ANNUITY']             # credit to annuity - basically the term of the loan in years\ntrain['CG_ratio'] = train['AMT_CREDIT'] / train['AMT_GOODS_PRICE']         # credit to goods price ratio - how much was financed?\n\n# log features\ntrain['log_INCOME'] = np.log(train['AMT_INCOME_TOTAL'])                    # log of income\ntrain['log_ANNUITY'] = np.log(train['AMT_ANNUITY'])                        # log of annuity\ntrain['log_CREDIT'] = np.log(train['AMT_CREDIT'])                          # log of credit\ntrain['log_GOODS'] = np.log(train['AMT_GOODS_PRICE'])                      # log of goods price\n\n# flag features\ntrain['MissingBureau'] = train.iloc[:, 41:44].isnull().sum(axis=1).astype(\"category\")   # number of bureaus with no score\ntrain['FLAG_CG_ratio'] = train['AMT_CREDIT'] > train['AMT_GOODS_PRICE']                 # FLAG if you borrowed more than the price of the item\n\n# EXT_SOURCE_x variables are very important - let's not leave missing values up to the imputer!\n# Instead of imputing missing values by column mean or median, let's fill in missing values by row\n# i.e. missing scores are replaced with the average of the scores we do have. If there are no scores at all\n# let's just give them a value of 0.2 for now.\ntrain['AVG_EXT'] = train.iloc[:, 41:44].sum(axis=1)/(3- train.iloc[:,41:44].isnull().sum(axis=1))   # average of the (at most) three scores\ntrain['AVG_EXT'].replace(np.nan, 0.2, inplace = True)   # get rid of any /0 errors generated from previous step\n\ntrain.EXT_SOURCE_1.fillna(train.AVG_EXT, inplace=True)\ntrain.EXT_SOURCE_2.fillna(train.AVG_EXT, inplace=True)\ntrain.EXT_SOURCE_3.fillna(train.AVG_EXT, inplace=True)\n\ntrain.drop(['AVG_EXT'], axis = 1)   # let's not make AVG_EXT a feature - it will be too highly correlated to the three components\n\n# drop these variables based on poor feature significance (< 0.0001)\ntrain.drop(['REG_REGION_NOT_LIVE_REGION','AMT_REQ_CREDIT_BUREAU_WEEK','HOUSETYPE_MODE','OCCUPATION_TYPE','FLAG_MOBIL','FLAG_CONT_MOBILE',\n           'NAME_TYPE_SUITE', 'FLAG_DOCUMENT_4','ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_16',\n           'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11','FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'AMT_REQ_CREDIT_BUREAU_DAY',\n           'AMT_REQ_CREDIT_BUREAU_HOUR', 'FLAG_DOCUMENT_21','FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_17','FLAG_DOCUMENT_2'],\n           axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T12:42:27.653498Z","iopub.execute_input":"2021-11-14T12:42:27.653746Z","iopub.status.idle":"2021-11-14T12:42:28.15474Z","shell.execute_reply.started":"2021-11-14T12:42:27.653716Z","shell.execute_reply":"2021-11-14T12:42:28.153708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### split out our training data - start with about 10% or 30k out of 300k","metadata":{}},{"cell_type":"code","source":"y = train['TARGET'].values\nX_train, X_valid, y_train, y_valid = train_test_split(train.drop(['TARGET', 'SK_ID_CURR'], axis = 1), y, stratify = y, test_size=0.9, random_state=1)\nprint('Shape of X_train:',X_train.shape)\nprint('Shape of y_train:',y_train.shape)\nprint('Shape of X_valid:',X_valid.shape)\nprint('Shape of y_valid:',y_valid.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T12:42:28.157079Z","iopub.execute_input":"2021-11-14T12:42:28.157332Z","iopub.status.idle":"2021-11-14T12:42:28.826646Z","shell.execute_reply.started":"2021-11-14T12:42:28.157285Z","shell.execute_reply":"2021-11-14T12:42:28.825694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### make lists of cat and num features for pipeline, based on dtype","metadata":{}},{"cell_type":"code","source":"types = np.array([z for z in X_train.dtypes])        # array([dtype('float64'), dtype('float64'), dtype('O'), dtype('O') ...])\nall_columns = X_train.columns.values                 # list of all column names\nis_num = types != 'object'                           # returns array([False, False, False, False,  True,  True, ...) where True is a numeric variable\nnum_features = all_columns[is_num].tolist()          # list of all numeric columns\ncat_features = all_columns[~is_num].tolist()         # list of all categorical columns\n\nprint(len(num_features), \"numeric features\")\nprint(len(cat_features), \"categorical features\")","metadata":{"execution":{"iopub.status.busy":"2021-11-14T12:42:28.827701Z","iopub.execute_input":"2021-11-14T12:42:28.828488Z","iopub.status.idle":"2021-11-14T12:42:28.837556Z","shell.execute_reply.started":"2021-11-14T12:42:28.828434Z","shell.execute_reply":"2021-11-14T12:42:28.836123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### build model pipeline based on num_cols and cat_cols lists","metadata":{}},{"cell_type":"code","source":"features = num_features + cat_features\n\nPipe_num = Pipeline(\n    steps=[\n    ('imputer', SimpleImputer(strategy = 'median')),        # tried median, mean, constant strategies\n    ('scaler', StandardScaler())       ])\n\nPipe_cat = Pipeline(\n    steps=[\n    ('imputer', SimpleImputer(strategy = 'constant', fill_value = 'Unknown')),\n    ('onehot', OneHotEncoder())        ])\n\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('num', Pipe_num, num_features),\n        ('cat', Pipe_cat, cat_features)])\n\npreprocessor.fit(train[features])\nX_train = preprocessor.transform(X_train[features])\nX_valid = preprocessor.transform(X_valid[features])\n\nprint('Shape of X_train:',X_train.shape)\nprint('Shape of y_train:',y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T12:42:28.839224Z","iopub.execute_input":"2021-11-14T12:42:28.839861Z","iopub.status.idle":"2021-11-14T12:42:59.462126Z","shell.execute_reply.started":"2021-11-14T12:42:28.839824Z","shell.execute_reply":"2021-11-14T12:42:59.46116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build Model Scoreboard","metadata":{}},{"cell_type":"code","source":"# set up table - new rows will be appended as models are run\n\npd.set_option('display.max_colwidth', None)             # LGBM in particular has long hyperparameters and I want to see them all\nresults = pd.DataFrame(columns = ['Model Type','AUC - 10xv', 'AUC - Valid', 'Hyperparameters'])","metadata":{"execution":{"iopub.status.busy":"2021-11-14T12:42:59.463634Z","iopub.execute_input":"2021-11-14T12:42:59.46388Z","iopub.status.idle":"2021-11-14T12:42:59.471211Z","shell.execute_reply.started":"2021-11-14T12:42:59.463849Z","shell.execute_reply":"2021-11-14T12:42:59.470192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"%%time\n# This model is running slow as a dog right now - might need to run it overnight\nlr_clf = LogisticRegression(max_iter=2000, solver='saga', penalty = 'elasticnet')\nlr_parameters = {'l1_ratio':[1], 'C': [1]}\nlr_grid = GridSearchCV(lr_clf, lr_parameters, cv=10, refit='True', n_jobs=-1, verbose=1, scoring='roc_auc')\nlr_grid.fit(X_train, y_train)\n\nlr_model = lr_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Logistic Regression',\n                          'AUC - 10xv' : lr_grid.best_score_,\n                          'AUC - Valid' : roc_auc_score(y_valid, lr_model.predict_proba(X_valid)[:, 1]),\n                          'Hyperparameters' : lr_grid.best_params_},\n                        ignore_index=True)\nresults","metadata":{"execution":{"iopub.status.busy":"2021-11-14T12:42:59.472718Z","iopub.execute_input":"2021-11-14T12:42:59.473074Z","iopub.status.idle":"2021-11-14T12:56:29.393418Z","shell.execute_reply.started":"2021-11-14T12:42:59.473032Z","shell.execute_reply":"2021-11-14T12:56:29.392612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest","metadata":{}},{"cell_type":"code","source":"%%time\nrf_clf = RandomForestClassifier(random_state=1, n_estimators=100)\nrf_parameters = {'max_depth': [28, 30, 32],  'min_samples_leaf': [30, 32, 34, 36]}\nrf_grid = GridSearchCV(rf_clf, rf_parameters, cv=10, refit='True', n_jobs=-1, verbose=1, scoring='roc_auc')\nrf_grid.fit(X_train, y_train)\nrf_model = rf_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Random Forest',\n                          'AUC - 10xv' : rf_grid.best_score_,\n                          'AUC - Valid' : roc_auc_score(y_valid, rf_model.predict_proba(X_valid)[:, 1]),\n                          'Hyperparameters' : rf_grid.best_params_},\n                        ignore_index=True)\nresults","metadata":{"execution":{"iopub.status.busy":"2021-11-14T12:56:29.395341Z","iopub.execute_input":"2021-11-14T12:56:29.395625Z","iopub.status.idle":"2021-11-14T13:03:08.471894Z","shell.execute_reply.started":"2021-11-14T12:56:29.395589Z","shell.execute_reply":"2021-11-14T13:03:08.470776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree","metadata":{}},{"cell_type":"code","source":"%%time\ndt_clf = DecisionTreeClassifier(random_state=1)\ndt_parameters = {\n    'max_depth': [4, 8, 12, 16, 20, 24],\n    'min_samples_leaf': [2, 4, 6, 8]\n}\n\ndt_grid = GridSearchCV(dt_clf, dt_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring='roc_auc')\ndt_grid.fit(X_train, y_train)\n\ndt_model = dt_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Decision Tree',\n                          'AUC - 10xv' : dt_grid.best_score_,\n                          'AUC - Valid' : roc_auc_score(y_valid, dt_model.predict_proba(X_valid)[:, 1]),\n                          'Hyperparameters' : dt_grid.best_params_},\n                        ignore_index=True)\nresults","metadata":{"execution":{"iopub.status.busy":"2021-11-14T13:03:08.473518Z","iopub.execute_input":"2021-11-14T13:03:08.473782Z","iopub.status.idle":"2021-11-14T13:05:23.314885Z","shell.execute_reply.started":"2021-11-14T13:03:08.473747Z","shell.execute_reply":"2021-11-14T13:05:23.313966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Light Gradient Boost Machine","metadata":{}},{"cell_type":"code","source":"%%time\nXGB_clf = XGBClassifier(objective='binary:logistic', use_label_encoder=False)\nXGB_parameters = {\n    'max_depth': range(1, 3, 5),\n    'n_estimators': range(10, 50, 100),\n    'learning_rate': [0.25, 0.5, 0.75, 1, 1.25, 1.5]\n}\n\nXGB_grid = GridSearchCV(XGB_clf, XGB_parameters, cv=10, n_jobs=10, verbose=True, scoring= 'roc_auc')\nXGB_grid.fit(X_train, y_train)\n\nXGB_model = XGB_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Light GBM',\n                          'AUC - 10xv' : XGB_grid.best_score_,\n                          'AUC - Valid' : roc_auc_score(y_valid, XGB_model.predict_proba(X_valid)[:, 1]),\n                          'Hyperparameters' : XGB_grid.best_params_},\n                        ignore_index=True)\nresults","metadata":{"execution":{"iopub.status.busy":"2021-11-14T13:05:23.316163Z","iopub.execute_input":"2021-11-14T13:05:23.316421Z","iopub.status.idle":"2021-11-14T13:06:06.70916Z","shell.execute_reply.started":"2021-11-14T13:05:23.31639Z","shell.execute_reply":"2021-11-14T13:06:06.708147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting the ROC curve (for AUC score on validation data)","metadata":{}},{"cell_type":"code","source":"probabilities = rf_model.predict_proba(X_valid)[:,1]\nfpr, tpr, thresholds = roc_curve(y_valid, probabilities)\nauc = roc_auc_score(y_valid, probabilities)               # AUC on validation data was .7403 per table above\nplt.figure(figsize=(6,6))\nplt.plot(fpr, tpr)                                        # plot the blue curve\nplt.plot([0, 1], [0, 1])                                  # plot the orange 45 degree line\nplt.title('Receiver operating characteristic curve')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.legend([\"AUC = %.6f\"%auc])\nplt.show()\n\n# hat tip:\n# https://medium.com/@praveenkotha/home-credit-default-risk-end-to-end-machine-learning-project-1871f52e3ef2\n# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html","metadata":{"execution":{"iopub.status.busy":"2021-11-14T13:06:06.710875Z","iopub.execute_input":"2021-11-14T13:06:06.711847Z","iopub.status.idle":"2021-11-14T13:06:11.71822Z","shell.execute_reply.started":"2021-11-14T13:06:06.711788Z","shell.execute_reply":"2021-11-14T13:06:11.716997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance (random forest)","metadata":{}},{"cell_type":"code","source":"importance_DF = pd.DataFrame(zip(rf_model.feature_importances_, features), columns=['Value','Feature']).sort_values(by=\"Value\", ascending=False)\nimportance_plot = importance_DF[importance_DF['Value'] > 0.01]\nplt.figure(figsize=[10,6])\nsns.barplot(importance_plot['Value'], importance_plot['Feature'], orient = \"h\", color = \"lightsteelblue\")\nplt.title(\"Most important features (min 1% of total)\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-14T13:10:29.222243Z","iopub.execute_input":"2021-11-14T13:10:29.222632Z","iopub.status.idle":"2021-11-14T13:10:29.670817Z","shell.execute_reply.started":"2021-11-14T13:10:29.222597Z","shell.execute_reply":"2021-11-14T13:10:29.669698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Factors with little explanatory power - consider dropping these","metadata":{}},{"cell_type":"code","source":"# Consider dropping these\ndrop_list = importance_DF[importance_DF['Value'] < 1E-4]['Feature'].to_list()\ndrop_list","metadata":{"execution":{"iopub.status.busy":"2021-11-14T13:10:46.690784Z","iopub.execute_input":"2021-11-14T13:10:46.691095Z","iopub.status.idle":"2021-11-14T13:10:46.698872Z","shell.execute_reply.started":"2021-11-14T13:10:46.691064Z","shell.execute_reply":"2021-11-14T13:10:46.697868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Boxplot on validation set","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfinal_model = RandomForestClassifier(random_state=1, n_estimators=100, max_depth=24, min_samples_leaf=24)\nscores = cross_val_score(rf_model, X_train, y_train, cv=10, scoring = 'roc_auc')\nprint(scores.round(2))\nprint(scores.mean())\nprint(scores.std(ddof=1))\nplt.figure(figsize=[12,3])\nsns.boxplot(scores, orient = \"h\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-14T13:06:12.172407Z","iopub.execute_input":"2021-11-14T13:06:12.172655Z","iopub.status.idle":"2021-11-14T13:07:35.652828Z","shell.execute_reply.started":"2021-11-14T13:06:12.172628Z","shell.execute_reply":"2021-11-14T13:07:35.651677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating separation between classes","metadata":{}},{"cell_type":"code","source":"boxdata = pd.DataFrame({'prediction' : lr_model.predict_proba(X_valid)[:,1], 'target' : y_valid})\nplt.figure(figsize=[12,3])\nsns.boxplot(boxdata.prediction, boxdata.target, orient = \"h\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-14T13:11:09.584556Z","iopub.execute_input":"2021-11-14T13:11:09.584872Z","iopub.status.idle":"2021-11-14T13:11:09.955657Z","shell.execute_reply.started":"2021-11-14T13:11:09.584832Z","shell.execute_reply":"2021-11-14T13:11:09.954549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Final Model Selection - save data","metadata":{}},{"cell_type":"code","source":"final_model = LogisticRegression(max_iter=2000, solver='saga', penalty = 'elasticnet', C = 1, l1_ratio = 1)\nfinal_model.fit(X_train, y_train)\n\njoblib.dump(preprocessor, 'default_preprocessor_06.joblib') \njoblib.dump(final_model, 'default_model_06.joblib')","metadata":{"execution":{"iopub.status.busy":"2021-11-14T13:11:43.659461Z","iopub.execute_input":"2021-11-14T13:11:43.659744Z","iopub.status.idle":"2021-11-14T13:14:22.25507Z","shell.execute_reply.started":"2021-11-14T13:11:43.659714Z","shell.execute_reply":"2021-11-14T13:14:22.253919Z"},"trusted":true},"execution_count":null,"outputs":[]}]}