{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Home Credit Default Risk - Team 3 (Kahsai, Nichols, Pellerito)","metadata":{}},{"cell_type":"markdown","source":"### Import packages","metadata":{}},{"cell_type":"code","source":"# standard Python tools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as ss   # need this for chi-squared function\n\n# special tools for working in Kaggle\nimport joblib   # save and load ML models\nimport gc       # garbage collection\nimport os \n\n# preprocessing steps\nfrom sklearn import preprocessing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n# machine learning models and tools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# surely there will be a lot more packages loaded by the time we are done!","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:44:46.383056Z","iopub.execute_input":"2021-11-08T00:44:46.383336Z","iopub.status.idle":"2021-11-08T00:44:47.724997Z","shell.execute_reply.started":"2021-11-08T00:44:46.383302Z","shell.execute_reply":"2021-11-08T00:44:47.724415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First look at training data set","metadata":{}},{"cell_type":"markdown","source":"### Read the training data","metadata":{}},{"cell_type":"code","source":"MainDir = \"../input/../input/home-credit-default-risk\"\nprint(os.listdir(MainDir))\n\n# Main table\ntrain = pd.read_csv(f'{MainDir}/application_train.csv')\n\n# Supplemental data - we can create additional feature sets by analyzing these.\nbureau = pd.read_csv(f'{MainDir}/bureau_balance.csv')\ncc = pd.read_csv(f'{MainDir}/credit_card_balance.csv')\n# and so on - not going to worry about these just yet","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:44:47.726919Z","iopub.execute_input":"2021-11-08T00:44:47.727408Z","iopub.status.idle":"2021-11-08T00:45:21.888844Z","shell.execute_reply.started":"2021-11-08T00:44:47.727336Z","shell.execute_reply":"2021-11-08T00:45:21.887331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bureau - separate data set with history for each customer. Let's make a crosstable and then \n# left-join it into the training data.\n\n#bureau.shape           # 27299925, 3\n#bureau.nunique()       # 817,395 unique customers, 8 unique statuses\nbureau_table = pd.crosstab(bureau.SK_ID_BUREAU, bureau.STATUS)\nbureau_table.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:45:21.891189Z","iopub.execute_input":"2021-11-08T00:45:21.891515Z","iopub.status.idle":"2021-11-08T00:45:39.699794Z","shell.execute_reply.started":"2021-11-08T00:45:21.891472Z","shell.execute_reply":"2021-11-08T00:45:39.698881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cc - credit card data. Let's get average balance by customer\n#cc.shape           # 3840312, 23\n#cc.nunique()       \ncc_table = cc.groupby(['SK_ID_CURR']).agg(np.mean)['AMT_BALANCE'].to_frame()\ncc_table.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:45:39.701362Z","iopub.execute_input":"2021-11-08T00:45:39.701702Z","iopub.status.idle":"2021-11-08T00:45:41.318763Z","shell.execute_reply.started":"2021-11-08T00:45:39.70166Z","shell.execute_reply":"2021-11-08T00:45:41.317923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### View the training data set","metadata":{}},{"cell_type":"code","source":"print('Shape of training data:', train.shape)\npd.set_option(\"display.max_columns\", None)            # makes it scrollable horizontally instead of suppressing columns\ntrain.head(5)\n# over 300,000 records in the training set, and 121 features (plus whatever other features we end up importing\n# from the supplemental tables.) Target variable is \"TARGET.\"","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:45:41.321254Z","iopub.execute_input":"2021-11-08T00:45:41.321485Z","iopub.status.idle":"2021-11-08T00:45:41.419818Z","shell.execute_reply.started":"2021-11-08T00:45:41.321459Z","shell.execute_reply":"2021-11-08T00:45:41.419003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Proportion Table for target variable","metadata":{}},{"cell_type":"code","source":"# Proportion - about 91.9% are zero (not default) and 8.1% are one (default.)\n# Project is being scored as AUC (area under curve) i.e. confidence matters. The best scores of all time are around 80-81%\n# so you can't score high by making a model that naively guesses that everything is a no-default.\n(train['TARGET'].value_counts() / len(train)).to_frame()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:45:41.421222Z","iopub.execute_input":"2021-11-08T00:45:41.421456Z","iopub.status.idle":"2021-11-08T00:45:41.4327Z","shell.execute_reply.started":"2021-11-08T00:45:41.42143Z","shell.execute_reply":"2021-11-08T00:45:41.432074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### How many categorical variables do we have, and how many levels in each?","metadata":{}},{"cell_type":"code","source":"train.dtypes.value_counts()\n# There are 16 categorical variables in our model.\n# We should also take a look at the 41 integer variables - some could be counting statistics (e.g. family size) but others could be integer-encoded categorical variables","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:45:41.433789Z","iopub.execute_input":"2021-11-08T00:45:41.434126Z","iopub.status.idle":"2021-11-08T00:45:41.447864Z","shell.execute_reply.started":"2021-11-08T00:45:41.434098Z","shell.execute_reply":"2021-11-08T00:45:41.44728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.select_dtypes('object').apply(pd.Series.nunique, axis = 0).to_frame()\n# These are the sixteen categorical variables: there would be 140 dummy variables in our model if we one-hot encoded all of these.","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:45:41.448992Z","iopub.execute_input":"2021-11-08T00:45:41.44941Z","iopub.status.idle":"2021-11-08T00:45:41.966743Z","shell.execute_reply.started":"2021-11-08T00:45:41.449359Z","shell.execute_reply":"2021-11-08T00:45:41.965903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing Data","metadata":{}},{"cell_type":"code","source":"train.isna().sum().to_frame().sort_values(0, ascending = False).head(50)\n# Lots of variables have missing values. We need to come up with a strategy for imputing missing values.","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:45:41.968255Z","iopub.execute_input":"2021-11-08T00:45:41.96874Z","iopub.status.idle":"2021-11-08T00:45:42.529888Z","shell.execute_reply.started":"2021-11-08T00:45:41.968698Z","shell.execute_reply":"2021-11-08T00:45:42.52903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Some Visualizations","metadata":{}},{"cell_type":"code","source":"# Let's create a sample of 1000 rows from the training data, so that these graphics can render in a reasonable amount of time.\ntrain_1K = train.sample(n=1000, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:45:42.531635Z","iopub.execute_input":"2021-11-08T00:45:42.531939Z","iopub.status.idle":"2021-11-08T00:45:42.5524Z","shell.execute_reply.started":"2021-11-08T00:45:42.531898Z","shell.execute_reply":"2021-11-08T00:45:42.5516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Income by occupation type\nfig, ax = plt.subplots(figsize=(15, 6))\nax = sns.boxplot(y = \"OCCUPATION_TYPE\", x = \"AMT_INCOME_TOTAL\", orient = \"h\", data = train_1K)\nplt.xlim([0, 1e6])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:45:42.553398Z","iopub.execute_input":"2021-11-08T00:45:42.554293Z","iopub.status.idle":"2021-11-08T00:45:43.091336Z","shell.execute_reply.started":"2021-11-08T00:45:42.554246Z","shell.execute_reply":"2021-11-08T00:45:43.090516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I like two-sided violin plots for categorical classification problems. They can help you see whether different groups\n# have different sensitivities.\n\nfig, ax = plt.subplots(figsize=(12, 6))\nax = sns.violinplot(x=\"WEEKDAY_APPR_PROCESS_START\", y=\"AMT_CREDIT\", hue = \"TARGET\", split = True, data=train_1K)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:45:43.092477Z","iopub.execute_input":"2021-11-08T00:45:43.092698Z","iopub.status.idle":"2021-11-08T00:45:43.510739Z","shell.execute_reply.started":"2021-11-08T00:45:43.092672Z","shell.execute_reply":"2021-11-08T00:45:43.50978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pairplot - I just picked a few continuous variables to show. Blue = 0 (not default), orange = 1 (default)\nNumRows = train_1K.iloc[:,[1, 7,8,9]]\nax = plt.figure(figsize = (8, 8))\nax=sns.pairplot(NumRows, hue = \"TARGET\", plot_kws={'s':20})\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:45:43.512147Z","iopub.execute_input":"2021-11-08T00:45:43.512487Z","iopub.status.idle":"2021-11-08T00:45:46.407418Z","shell.execute_reply.started":"2021-11-08T00:45:43.512434Z","shell.execute_reply":"2021-11-08T00:45:46.406557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlations","metadata":{}},{"cell_type":"code","source":"correl = train.corr()\nfig, ax = plt.subplots(figsize=(24, 24))\nsns.heatmap(correl, annot = False, cmap = \"BuPu\", label = 'small', cbar = False)\nax.set_title('Correlation Matrix'); \nplt.show()\n\n# Big ol' correlation matrix shows that there are some highly correlated variables. This data set could be\n# a candidate for feature reduction using Principal Component Analysis.","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:45:46.410989Z","iopub.execute_input":"2021-11-08T00:45:46.411474Z","iopub.status.idle":"2021-11-08T00:46:02.081056Z","shell.execute_reply.started":"2021-11-08T00:45:46.411428Z","shell.execute_reply":"2021-11-08T00:46:02.080157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### Categorical association - Cramer's V","metadata":{}},{"cell_type":"code","source":"# https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\n\n# Cramer's V (φ) is similar to Pearson's R (correlation coefficient) but it works with categorical data.\n# While R has a range from -1 to +1, V has a range of 0 to +1. We are going to use this to build a\n# heatmap that will help us evaluate whether the categorical variables are independent.\n\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n    rcorr = r-((r-1)**2)/(n-1)\n    kcorr = k-((k-1)**2)/(n-1)\n    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:46:02.082579Z","iopub.execute_input":"2021-11-08T00:46:02.083233Z","iopub.status.idle":"2021-11-08T00:46:02.091316Z","shell.execute_reply.started":"2021-11-08T00:46:02.083192Z","shell.execute_reply":"2021-11-08T00:46:02.090455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I don't see a big opportunity for feature reduction here, but it was worth taking a look. And the heatmap looks cool.\n\nCatFeatures = ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_TYPE_SUITE', \n               'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'WEEKDAY_APPR_PROCESS_START', \n               'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE']\n\nLL = len(CatFeatures)\ncramers_outputs = []\nfor x in range(0,LL):\n    first = train.loc[:,CatFeatures[x]]\n    for y in range(0,LL):\n        second = train.loc[:,CatFeatures[y]]\n        result = round(cramers_v(first,second),4)\n        cramers_outputs.append(result.tolist())\narray = np.array(cramers_outputs)\nreshaped = array.reshape(LL,LL)\n\nfig = plt.figure(figsize = (8, 8))  # instanciate figure for heat map\nax = sns.heatmap(reshaped, annot = True,  cmap = \"BuPu\", fmt=\".0%\", cbar = False)\nax.set_xticklabels(CatFeatures)\nax.set_yticklabels(CatFeatures)\nax.tick_params(axis = 'x', labelrotation = 90)\nax.tick_params(axis = 'y', labelrotation = 0)\nax.set_title(\"Heatmap of Cramer's V on categorical variables\");","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:46:02.092841Z","iopub.execute_input":"2021-11-08T00:46:02.093069Z","iopub.status.idle":"2021-11-08T00:46:27.896048Z","shell.execute_reply.started":"2021-11-08T00:46:02.093044Z","shell.execute_reply":"2021-11-08T00:46:27.895155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Some barplots","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ndef DisplayBreakdown(vars) :\n    df_nb = []\n    df_cols = []\n    for var in vars :\n        df_nb.append(pd.crosstab(train_1K['TARGET'], train_1K[var], normalize='index').reset_index())\n        df_cols.append(train_1K[var].unique())\n\n    for idx in range(len(df_nb)) :\n        fig = px.bar(df_nb[idx], y='TARGET', x=df_cols[idx], orientation='h')\n        fig.update_layout(height=275, width=800, xaxis_tickformat = '.0%', title_text=vars[idx], legend_title='', xaxis_title='', yaxis_type='category', legend=dict(orientation='h'))\n        fig.show()\n\nDisplayBreakdown(['NAME_EDUCATION_TYPE', 'NAME_HOUSING_TYPE', 'NAME_FAMILY_STATUS', 'WEEKDAY_APPR_PROCESS_START'])","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:46:27.897454Z","iopub.execute_input":"2021-11-08T00:46:27.89769Z","iopub.status.idle":"2021-11-08T00:46:30.511169Z","shell.execute_reply.started":"2021-11-08T00:46:27.897661Z","shell.execute_reply":"2021-11-08T00:46:30.510297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Principal Component Analysis","metadata":{}},{"cell_type":"code","source":"# from sklearn.preprocessing import StandardScaler    <- already loaded\nfrom sklearn.decomposition import PCA                 \n\nPCAFeatures = train_1K.iloc[:,np.r_[41:85]]\nPCAFeatures.replace(np.nan,0, inplace = True)\nPCAFeaturesScaled = preprocessing.scale(PCAFeatures)\n\n# display(round(pd.DataFrame(PCAFeaturesScaled).describe(),4))  # check to see if these columns have mean 0 and std 1: looks good\n\npca = PCA()\npca.fit(PCAFeaturesScaled)\npca_data = pca.transform(PCAFeaturesScaled)\n\n# Plot component importance - maybe keep the top 10 or so?\nfig = plt.figure(figsize = (22, 3))\npcvar = np.round(pca.explained_variance_ratio_ * 100,1)\nlabels = ['PC'+str(x) for x in range(1,len(pcvar)+1)]\nplt.bar(x=range(1,len(pcvar)+1), height = pcvar, tick_label = labels)\nplt.ylabel('%age of explained variance')\nplt.xlabel('Principal Component')\nplt.title('PCA Components')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:46:30.512704Z","iopub.execute_input":"2021-11-08T00:46:30.513017Z","iopub.status.idle":"2021-11-08T00:46:31.104948Z","shell.execute_reply.started":"2021-11-08T00:46:30.512975Z","shell.execute_reply":"2021-11-08T00:46:31.1036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA data sets are completely uncorrelated\npca_df = pd.DataFrame(pca_data, columns = labels)\nfig = plt.figure(figsize = (26,10))\nsns.heatmap(pca_df.corr(), cmap = 'BuPu', annot = True, fmt=\".0%\")\nplt.title('Correlation Heatmap for principal components')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:46:31.106231Z","iopub.execute_input":"2021-11-08T00:46:31.106555Z","iopub.status.idle":"2021-11-08T00:46:38.31044Z","shell.execute_reply.started":"2021-11-08T00:46:31.106523Z","shell.execute_reply":"2021-11-08T00:46:38.309825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_df = pd.DataFrame(pca_data, columns = labels)\npca_df['TARGET'] = train_1K.TARGET\n\npca_df.head(5)\n\n# Scatterplot of PC1 and PC2 components.\n#Yes = pca_df.loc[(pca_df.TARGET == 1),:]\n#No = pca_df.loc[(pca_df.TARGET == 0),:]\n#plt.scatter('PC1','PC2', data = No, s=4, label = '0')\n#plt.scatter('PC1','PC2', data = Yes, s=4, label = '1')\n#plt.xlabel('PC1')\n#plt.ylabel('PC2')\n#plt.title('Scatter Plot - LeftUnion by PC1 and PC2')\n#plt.legend()\n\n#plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-11-08T01:08:10.143795Z","iopub.execute_input":"2021-11-08T01:08:10.144676Z","iopub.status.idle":"2021-11-08T01:08:10.193036Z","shell.execute_reply.started":"2021-11-08T01:08:10.144633Z","shell.execute_reply":"2021-11-08T01:08:10.192179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build Model Scoreboard","metadata":{}},{"cell_type":"code","source":"# set up table\nresults = pd.DataFrame(columns = ['Model Type','Accuracy','Hyperparameters'])\n\n# each time you run a model, run this code\nresults = results.append({'Model Type' : 'Logistic Regression',                              # logistic regression, random forest, etc\n                          'Accuracy' : 0.7243,                                               # variable that contains best model run\n                          'Hyperparameters' : \"{'max_depth': 9, 'min_samples_leaf': 1}\"},    # variable that contains hyperparameters from best model run\n                        ignore_index=True)    \nresults","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:46:38.55977Z","iopub.execute_input":"2021-11-08T00:46:38.56059Z","iopub.status.idle":"2021-11-08T00:46:38.577606Z","shell.execute_reply.started":"2021-11-08T00:46:38.560541Z","shell.execute_reply":"2021-11-08T00:46:38.576744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Appendix - data descriptions","metadata":{}},{"cell_type":"code","source":"# Description table contains characters that are unprintable with UTF8 encoding, so we need to open it this way:\n\nwith open(f'{MainDir}/HomeCredit_columns_description.csv', 'r', encoding = 'ISO-8859-1') as csvfile:\n    desc = pd.read_csv(csvfile)\npd.set_option(\"display.max_rows\", None)               # print entire thing, not just first and last rows\npd.options.display.max_colwidth = 100                 # description column\ndesc","metadata":{"execution":{"iopub.status.busy":"2021-11-08T00:46:38.578938Z","iopub.execute_input":"2021-11-08T00:46:38.579435Z","iopub.status.idle":"2021-11-08T00:46:38.647404Z","shell.execute_reply.started":"2021-11-08T00:46:38.579394Z","shell.execute_reply":"2021-11-08T00:46:38.646505Z"},"trusted":true},"execution_count":null,"outputs":[]}]}