{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Model Submission Notebook - Home Credit Default Risk","metadata":{}},{"cell_type":"markdown","source":"### initialization - load packages and data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport joblib\nimport numpy as np\nimport gc\n\nMainDir = \"../input/../input/home-credit-default-risk\"\ntest = pd.read_csv(f'{MainDir}/application_test.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-19T21:38:12.154533Z","iopub.execute_input":"2021-11-19T21:38:12.154796Z","iopub.status.idle":"2021-11-19T21:38:12.777536Z","shell.execute_reply.started":"2021-11-19T21:38:12.154769Z","shell.execute_reply":"2021-11-19T21:38:12.776741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Model","metadata":{}},{"cell_type":"code","source":"preprocessor = joblib.load('../input/defaultdata08/default_preprocessor_08.joblib')\nmodel = joblib.load('../input/defaultdata08/default_model_08.joblib')\nprint(type(model))","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:38:12.779367Z","iopub.execute_input":"2021-11-19T21:38:12.779586Z","iopub.status.idle":"2021-11-19T21:38:12.853028Z","shell.execute_reply.started":"2021-11-19T21:38:12.779558Z","shell.execute_reply":"2021-11-19T21:38:12.851955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing","metadata":{}},{"cell_type":"code","source":"# Load Bureau table\nbureau = pd.read_csv(f'{MainDir}/bureau.csv')\nprint(bureau.shape, \"- shape of bureau table\")\n\n# Load bureau_balance table and merge its features into bureau\nbureau_balance = pd.read_csv(f'{MainDir}/bureau_balance.csv')\nbb_status = pd.crosstab(bureau_balance.SK_ID_BUREAU, bureau_balance.STATUS) # , normalize = 'index')   let's try it as counts instead of row proportions\n\nbb_status.columns = ['BB_'+column for column in bb_status.columns]                                  # add BB_ prefix for identification\nbureau = bureau.merge(bb_status, left_on = 'SK_ID_BUREAU', right_on = 'SK_ID_BUREAU')               # merge the tables\nbureau = bureau.drop(['SK_ID_BUREAU'], axis = 1)                                                    # no longer need this\nprint(bureau.shape, \"- shape of bureau table after merge\")\n\nbureau.columns = ['BU_'+column if column !='SK_ID_CURR' else column for column in bureau.columns]   # this way we can recognize these columns later\n\n# Create numeric features by grouping on SK_ID_CURR and finding group means\nbureau_num = bureau.groupby(by=['SK_ID_CURR']).mean().reset_index()         # group the numeric features by SK_ID_CURR\nprint(bureau_num.shape, \"- shape of numeric features (incl index)\")         # should be 305,811 x 13\n\n# Create categorical features by creating dummies and then taking group means\nbureau_cat = pd.get_dummies(bureau.select_dtypes('object'))                 # this got rid of the SK_ID_CURR column ...\nbureau_cat['SK_ID_CURR'] = bureau['SK_ID_CURR']                             # so we have to replace it\nbureau_cat = bureau_cat.groupby(by = ['SK_ID_CURR']).mean().reset_index()   # could try sum as well.\nprint(bureau_cat.shape, \"- shape of categorical features (incl index)\")     # should be 305,811 x 24\n\n# Number of past loans per customer\nbureau_count = bureau.groupby(by = ['SK_ID_CURR'])['BU_CREDIT_ACTIVE'].count().reset_index()\nbureau_count.rename(columns={'BU_CREDIT_ACTIVE':'COUNT_of_BUREAU'})\nbureau_count.head(5)\n\n# merge bureau_num and bureau_cat into the training data\ntest = test.merge(bureau_num, on='SK_ID_CURR', how='left')                # merge numeric features\ntest = test.merge(bureau_cat, on='SK_ID_CURR', how='left')                # merge categorical features\ntest = test.merge(bureau_count, on='SK_ID_CURR', how='left')              # merge count features\nprint(test.shape, \"- shape of training data after merges\")                 # added 35 new features.\n\n# This process will add some NaNs to the main data, but we can let the imputer take care of that later.\n\n# no longer need bureau, bureau_num, bureau_car, bureau_count, bureau_balance\nlist = ['bureau', 'bureau_num', 'bureau_cat', 'bureau_balance']\ndel list\ngc.collect()\n\n# ---------\n\n# merge pos_cash_balance, installments_payments and credit_card_balance into previous_application\nprevious = pd.read_csv(f'{MainDir}/previous_application.csv')\nprint(previous.shape, \"- shape of previous_application\")\npos = pd.read_csv(f'{MainDir}/POS_CASH_balance.csv')\n\npos.columns = ['PO_'+column if column !='SK_ID_PREV' else column for column in pos.columns]   # this way we can recognize these columns later\n\n# Create numeric features of pos by grouping on SK_ID_PREV and finding group means\npos_num = pos.groupby(by=['SK_ID_PREV']).mean().reset_index()         # group the numeric features by SK_ID_CURR\nprint(pos_num.shape, \"- shape of numeric features (incl index)\")         # should be 305,811 x 13\n\n# Create categorical features by creating dummies and then taking group means\npos_cat = pd.get_dummies(pos.select_dtypes('object'))                 # this got rid of the SK_ID_PREV column ...\npos_cat['SK_ID_PREV'] = pos['SK_ID_PREV']                             # so we have to replace it\npos_cat = pos_cat.groupby(by = ['SK_ID_PREV']).mean().reset_index()   # could try sum as well.\nprint(pos_cat.shape, \"- shape of categorical features (incl index)\")     # should be 305,811 x 24\n\n# merge pos_num and pos_cat into the previous_application data\nprevious = previous.merge(pos_num, on='SK_ID_PREV', how='left')                # merge numeric features\nprevious = previous.merge(pos_cat, on='SK_ID_PREV', how='left')                # merge categorical features\nprint(previous.shape, \"- shape of previous data after merges\")                 # added 35 new features.\n\n# don't need these anymore: pos, pos_num, pos_cat\nlist = ['pos', 'pos_num', 'pos_cat']\ndel list\ngc.collect()\n\ninst = pd.read_csv(f'{MainDir}/installments_payments.csv')\ninst.columns = ['IP_'+column if column !='SK_ID_PREV' else column for column in inst.columns]   # this way we can recognize these columns later\n\ninst_num = inst.groupby(by=['SK_ID_PREV']).mean().reset_index()         # group the numeric features by SK_ID_CURR\nprint(inst_num.shape, \"- shape of numeric features (incl index)\")         # should be 305,811 x 13\n\n# installments_payments only has numeric features\n\n# merge pos_num into the previous_application data\nprevious = previous.merge(inst_num, left_on='SK_ID_PREV', right_on = 'SK_ID_PREV', how='left')                # merge numeric features\nprint(previous.shape, \"- shape of previous data after merges\")                 # added 35 new features.\n\n# don't need these anymore: inst, inst_num\nlist = ['inst', 'inst_num']\ndel list\ngc.collect()\n\nccb = pd.read_csv(f'{MainDir}/credit_card_balance.csv')\nccb.columns = ['CC_'+column if column !='SK_ID_PREV' else column for column in ccb.columns]   # this way we can recognize these columns later\n\n# Create numeric features of pos by grouping on SK_ID_PREV and finding group means\nccb_num = ccb.groupby(by=['SK_ID_PREV']).mean().reset_index()         # group the numeric features by SK_ID_CURR\nprint(ccb_num.shape, \"- shape of numeric features (incl index)\")         # should be 305,811 x 13\n\n# Create categorical features by creating dummies and then taking group means\nccb_cat = pd.get_dummies(ccb.select_dtypes('object'))                 # this got rid of the SK_ID_PREV column ...\nccb_cat['SK_ID_PREV'] = ccb['SK_ID_PREV']                             # so we have to replace it\nccb_cat = ccb_cat.groupby(by = ['SK_ID_PREV']).mean().reset_index()   # could try sum as well.\nprint(ccb_cat.shape, \"- shape of categorical features (incl index)\")     # should be 305,811 x 24\n\n# merge ccb_num and ccb_cat into the previous_application data\nprevious = previous.merge(ccb_num, on='SK_ID_PREV', how='left')                # merge numeric features\nprevious = previous.merge(ccb_cat, on='SK_ID_PREV', how='left')                # merge categorical features\nprint(previous.shape, \"- shape of previous data after merges\")                 # added 35 new features.\n\n# don't need these anymore: pos, pos_num, pos_cat\nlist = ['ccb', 'ccb_num', 'ccb_cat']\ndel list\ngc.collect()\n\n# final step: merge previous into main table\n\nprevious.columns = ['PR_'+column if column !='SK_ID_CURR' else column for column in previous.columns]   # this way we can recognize these columns later\nprevious['PR_DAYS_LAST_DUE'].replace({365243: np.nan}, inplace = True)\nprevious['PR_DAYS_TERMINATION'].replace({365243: np.nan}, inplace = True)\nprevious['PR_DAYS_FIRST_DRAWING'].replace({365243: np.nan}, inplace = True)\n\n# Create numeric features by grouping on SK_ID_CURR and finding group means\nprevious_num = previous.groupby(by=['SK_ID_CURR']).mean().reset_index()         # group the numeric features by SK_ID_CURR\nprint(previous_num.shape, \"- shape of numeric features (incl index)\")         # should be 305,811 x 13\n\n# Create categorical features by creating dummies and then taking group means\nprevious_cat = pd.get_dummies(previous.select_dtypes('object'))                 # this got rid of the SK_ID_CURR column ...\nprevious_cat['SK_ID_CURR'] = previous['SK_ID_CURR']                             # so we have to replace it\nprevious_cat = bureau_cat.groupby(by = ['SK_ID_CURR']).mean().reset_index()   # could try sum as well.\nprint(previous_cat.shape, \"- shape of categorical features (incl index)\")     # should be 305,811 x 24\n\n# merge bureau_num and bureau_cat into the training data\ntest = test.merge(previous_num, on='SK_ID_CURR', how='left')                # merge numeric features\ntest = test.merge(previous_cat, on='SK_ID_CURR', how='left')                # merge categorical features\nprint(test.shape, \"- shape of training data after merges\")                 # added 35 new features.\n\n# This process will add some NaNs to the main data, but we can let the imputer take care of that later.\n\n# no longer need bureau, bureau_num, bureau_car, bureau_count, bureau_balance\nlist = ['previous', 'previous_num', 'previous_cat']\ndel list\ngc.collect()\n\n# -------------\n\n# what is going on with days_employed? Over 50,000 entries have the value 365,243 days! Let's replace those with NaN and let the imputer deal with them.\ntest['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\n# ratio features\ntest['CI_ratio'] = test['AMT_CREDIT'] / test['AMT_INCOME_TOTAL']        # credit-to-income ratio\ntest['AI_ratio'] = test['AMT_ANNUITY'] / test['AMT_INCOME_TOTAL']       # annuity-to-income ratio\ntest['AC_ratio'] = test['AMT_CREDIT'] / test['AMT_ANNUITY']             # credit to annuity - basically the term of the loan in years\ntest['CG_ratio'] = test['AMT_CREDIT'] / test['AMT_GOODS_PRICE']         # credit to goods price ratio - how much was financed?\n\n# log features\ntest['log_INCOME'] = np.log(test['AMT_INCOME_TOTAL'])                    # log of income\ntest['log_ANNUITY'] = np.log(test['AMT_ANNUITY'])                        # log of annuity\ntest['log_CREDIT'] = np.log(test['AMT_CREDIT'])                          # log of credit\ntest['log_GOODS'] = np.log(test['AMT_GOODS_PRICE'])                      # log of goods price\n\n# flag features\ntest['MissingBureau'] = test.iloc[:, 41:44].isnull().sum(axis=1).astype(\"category\")   # number of bureaus with no score\ntest['FLAG_CG_ratio'] = test['AMT_CREDIT'] > test['AMT_GOODS_PRICE']                 # FLAG if you borrowed more than the price of the item\ntest['DAYS_ID_4200'] = test['DAYS_ID_PUBLISH'] < -4200                             # IDs more than about 14 years old are from USSR\n\n# EXT_SOURCE_x variables are very important - let's not leave missing values up to the imputer!\n# Instead of imputing missing values by column mean or median, let's fill in missing values by row\n# i.e. missing scores are replaced with the average of the scores we do have. If there are no scores at all\n# let's just give them a value of 0.2 for now.\ntest['AVG_EXT'] = test.iloc[:, 41:44].sum(axis=1)/(3- test.iloc[:,41:44].isnull().sum(axis=1))   # average of the (at most) three scores\ntest['AVG_EXT'].replace(np.nan, 0.2, inplace = True)   # get rid of any /0 errors generated from previous step\n\ntest.EXT_SOURCE_1.fillna(test.AVG_EXT, inplace=True)\ntest.EXT_SOURCE_2.fillna(test.AVG_EXT, inplace=True)\ntest.EXT_SOURCE_3.fillna(test.AVG_EXT, inplace=True)\n\ntest.drop(['AVG_EXT'], axis = 1)   # let's not make AVG_EXT a feature - it will be too highly correlated to the three components\n\n# drop these variables based on poor feature significance (< 0.0001)\n#train.drop(['REG_REGION_NOT_LIVE_REGION','AMT_REQ_CREDIT_BUREAU_WEEK','HOUSETYPE_MODE','OCCUPATION_TYPE','FLAG_MOBIL','FLAG_CONT_MOBILE',\n#           'NAME_TYPE_SUITE', 'FLAG_DOCUMENT_4','ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_16',\n#           'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11','FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'AMT_REQ_CREDIT_BUREAU_DAY',\n#           'AMT_REQ_CREDIT_BUREAU_HOUR', 'FLAG_DOCUMENT_21','FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_17','FLAG_DOCUMENT_2'],\n#           axis=1, inplace=True)\n\ntest.drop(['ORGANIZATION_TYPE'], axis = 1)  # 58 dummies, doesn't do jackshit\n\n# Ratio Features\ntest['OD_ratio'] = test['BU_AMT_CREDIT_SUM_OVERDUE'] / test['BU_AMT_CREDIT_SUM_DEBT']   # proportion of debt that is overdue\ntest['OD_ratio'].replace([np.nan, np.inf, -np.inf], 0, inplace = True)\ntest['Credit_ratio'] = test['BU_AMT_CREDIT_SUM'] / test['BU_AMT_CREDIT_SUM_LIMIT']      # proportion of credit line used\ntest['Credit_ratio'].replace([np.nan, np.inf, -np.inf], 0, inplace = True)\ntest['Debt_ratio'] = test['BU_AMT_CREDIT_SUM_DEBT'] / test['BU_AMT_CREDIT_SUM']         # debt percentage\ntest['Debt_ratio'].replace([np.nan, np.inf, -np.inf], 0, inplace = True)\ntest['PR_term'] = test['PR_IP_AMT_PAYMENT'] / test['PR_IP_AMT_INSTALMENT']             # term\ntest['PR_term'].replace([np.nan, np.inf, -np.inf], 0, inplace = True)\n\n\nX_test = preprocessor.transform(test)\nprint(X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:38:12.854873Z","iopub.execute_input":"2021-11-19T21:38:12.855106Z","iopub.status.idle":"2021-11-19T21:38:16.582621Z","shell.execute_reply.started":"2021-11-19T21:38:12.855079Z","shell.execute_reply":"2021-11-19T21:38:16.581469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test Predictions","metadata":{}},{"cell_type":"code","source":"test_pred = model.predict_proba(X_test)\nprint(test_pred.shape)\nprint(test_pred[:5])","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:38:16.58405Z","iopub.status.idle":"2021-11-19T21:38:16.584392Z","shell.execute_reply.started":"2021-11-19T21:38:16.584223Z","shell.execute_reply":"2021-11-19T21:38:16.584241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('../input/home-credit-default-risk/sample_submission.csv')\nsubmission.head(10)  # We need the probability of default (column [1] from test_pred)\nsubmission.TARGET = test_pred[:,1]   # replace the default values with our predictions\nsubmission.head(10)\nsubmission.to_csv('default_submission_08.csv', index=False, header = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T21:38:16.585761Z","iopub.status.idle":"2021-11-19T21:38:16.586105Z","shell.execute_reply.started":"2021-11-19T21:38:16.585929Z","shell.execute_reply":"2021-11-19T21:38:16.585946Z"},"trusted":true},"execution_count":null,"outputs":[]}]}