{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Home Credit Default Risk - Team 3 (Kahsai, Nichols, Pellerito)","metadata":{}},{"cell_type":"markdown","source":"### Import packages","metadata":{}},{"cell_type":"code","source":"# standard Python tools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as ss   # need this for chi-squared function\n\n# special tools for working in Kaggle\nimport joblib   # save and load ML models\nimport gc       # garbage collection\nimport os \n\n# preprocessing steps\nfrom sklearn import preprocessing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n# machine learning models and tools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# surely there will be a lot more packages loaded by the time we are done!","metadata":{"execution":{"iopub.status.busy":"2021-11-14T04:03:37.896776Z","iopub.execute_input":"2021-11-14T04:03:37.897093Z","iopub.status.idle":"2021-11-14T04:03:39.337305Z","shell.execute_reply.started":"2021-11-14T04:03:37.897014Z","shell.execute_reply":"2021-11-14T04:03:39.336501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First look at training data set","metadata":{}},{"cell_type":"markdown","source":"### Read the training data","metadata":{}},{"cell_type":"code","source":"MainDir = \"../input/../input/home-credit-default-risk\"\nprint(os.listdir(MainDir))\n\n# Main table\ntrain = pd.read_csv(f'{MainDir}/application_train.csv')\n\n# Supplemental data - we can create additional feature sets by analyzing these.\nbureau = pd.read_csv(f'{MainDir}/bureau.csv')\n","metadata":{"execution":{"iopub.status.busy":"2021-11-14T04:03:39.339115Z","iopub.execute_input":"2021-11-14T04:03:39.339586Z","iopub.status.idle":"2021-11-14T04:03:49.86116Z","shell.execute_reply.started":"2021-11-14T04:03:39.339541Z","shell.execute_reply":"2021-11-14T04:03:49.860359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reminder - these are the columns in train","metadata":{}},{"cell_type":"code","source":"train.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T04:03:49.862472Z","iopub.execute_input":"2021-11-14T04:03:49.863815Z","iopub.status.idle":"2021-11-14T04:03:49.913124Z","shell.execute_reply.started":"2021-11-14T04:03:49.86376Z","shell.execute_reply":"2021-11-14T04:03:49.912307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bureau table","metadata":{}},{"cell_type":"markdown","source":"### first look - head of bureau table, size and shape, etc.","metadata":{}},{"cell_type":"code","source":"print(bureau.shape)                                        # size of table - 17 columns x 1.72 million rows\nprint(bureau.SK_ID_CURR.nunique(), \"unique SK_ID_CURR\")    # number of unique SK_ID_CURR is 305,811, similar to size of train.csv\n\n# identifying column types\ntypes = np.array([z for z in bureau.dtypes])               # array([dtype('float64'), dtype('float64'), dtype('O'), dtype('O') ...])\nall_columns = bureau.columns.values                        # list of all column names\nis_num = types != 'object'                                 # returns array([False, False, False, False,  True,  True, ...) where True is a numeric variable\nnum_features = all_columns[is_num].tolist()                # list of all numeric columns\ncat_features = all_columns[~is_num].tolist()               # list of all categorical columns\nprint(len(num_features), \"numeric features\")               # looks like we have 14 numeric features (including the two key fields)\nprint(len(cat_features), \"categorical features\")           # ... and three categorical features\n\nbureau.head(5)\n# SK_ID_CURR key field will let us merge this data into the train.csv table; SK_ID_BUREAU key field will let us merge with bureau_balance.csv.","metadata":{"execution":{"iopub.status.busy":"2021-11-14T04:03:49.915234Z","iopub.execute_input":"2021-11-14T04:03:49.915667Z","iopub.status.idle":"2021-11-14T04:03:49.978257Z","shell.execute_reply.started":"2021-11-14T04:03:49.915623Z","shell.execute_reply":"2021-11-14T04:03:49.977195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### wrangling new features from bureau.csv table","metadata":{}},{"cell_type":"code","source":"# check for missing values:\nbureau.isna().sum().to_frame().sort_values(0, ascending = False)\n\n# no categorical variables have missing values. For now, let's assume that missing numeric values are zeroes.\nbureau.fillna(0, inplace = True)\n\n# let's get some stats grouped on SK_ID_CURR\nGrouped = (bureau\n           .groupby('SK_ID_CURR')\n           .agg(\n               {'SK_ID_CURR': 'count', \n                'AMT_CREDIT_SUM': 'sum',\n                'AMT_CREDIT_SUM_DEBT': 'sum',\n                'AMT_CREDIT_SUM_OVERDUE': 'sum',\n                'AMT_ANNUITY' : 'sum',\n                'DAYS_CREDIT': 'max',\n                'CREDIT_DAY_OVERDUE' : 'max',\n                'AMT_CREDIT_MAX_OVERDUE' : 'sum'\n               }\n           )\n          )\nGrouped","metadata":{"execution":{"iopub.status.busy":"2021-11-14T04:03:49.980954Z","iopub.execute_input":"2021-11-14T04:03:49.981183Z","iopub.status.idle":"2021-11-14T04:03:51.574727Z","shell.execute_reply.started":"2021-11-14T04:03:49.981154Z","shell.execute_reply":"2021-11-14T04:03:51.573831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### merge our new features into the training data","metadata":{}},{"cell_type":"code","source":"#Merge into train.csv\ntrain['SK_ID_CURR'] = train['SK_ID_CURR'].astype(str)\nGrouped['SK_ID_CURR'] = Grouped['SK_ID_CURR'].astype(str)\n\ntrain = train.merge(Grouped, on = 'SK_ID_CURR', how = 'left')\n\n# getting all NA on merged columns\ntrain.isna().sum().to_frame().sort_values(0, ascending = False)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-14T04:03:51.576215Z","iopub.execute_input":"2021-11-14T04:03:51.577066Z","iopub.status.idle":"2021-11-14T04:03:52.547655Z","shell.execute_reply.started":"2021-11-14T04:03:51.577024Z","shell.execute_reply":"2021-11-14T04:03:52.546231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Some Visualizations","metadata":{}},{"cell_type":"markdown","source":"# Appendix - data descriptions","metadata":{}},{"cell_type":"code","source":"# Description table contains characters that are unprintable with UTF8 encoding, so we need to open it this way:\n\nwith open(f'{MainDir}/HomeCredit_columns_description.csv', 'r', encoding = 'ISO-8859-1') as csvfile:\n    desc = pd.read_csv(csvfile)\npd.set_option(\"display.max_rows\", None)               # print entire thing, not just first and last rows\npd.options.display.max_colwidth = 100                 # description column\ndesc","metadata":{"execution":{"iopub.status.busy":"2021-11-14T04:03:52.548821Z","iopub.status.idle":"2021-11-14T04:03:52.549344Z","shell.execute_reply.started":"2021-11-14T04:03:52.548974Z","shell.execute_reply":"2021-11-14T04:03:52.54899Z"},"trusted":true},"execution_count":null,"outputs":[]}]}