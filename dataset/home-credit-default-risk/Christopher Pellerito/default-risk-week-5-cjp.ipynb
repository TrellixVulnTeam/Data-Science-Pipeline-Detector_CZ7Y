{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Home Credit Default Risk - Team 3 (Kahsai, Nichols, Pellerito)","metadata":{}},{"cell_type":"markdown","source":"### Import packages","metadata":{}},{"cell_type":"code","source":"# standard Python tools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# special tools for working in Kaggle\nimport joblib   # save and load ML models\nimport gc       # garbage collection\nimport os \nimport sklearn\n\n# preprocessing steps\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# machine learning models and tools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\n\n# cross validation and metrics - remember this competition is scored as area under curve\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n# clear out any old junk\ngc.collect()\n\n# Don't do this!\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Let's put this someplace where it's easy to find - the proportion split between training and validation data\ntrain_size = 0.75\n\n# define the directory where our stuff lives\nMainDir = \"../input/../input/home-credit-default-risk\"\nprint(os.listdir(MainDir))","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:13:21.495878Z","iopub.execute_input":"2021-11-26T23:13:21.497078Z","iopub.status.idle":"2021-11-26T23:13:23.319532Z","shell.execute_reply.started":"2021-11-26T23:13:21.496877Z","shell.execute_reply":"2021-11-26T23:13:23.318567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building the training data set","metadata":{}},{"cell_type":"markdown","source":"### Read the training data","metadata":{}},{"cell_type":"code","source":"# Main table\npd.options.display.max_columns = None\ntrain = pd.read_csv(f'{MainDir}/application_train.csv')\ntrain.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:13:23.321298Z","iopub.execute_input":"2021-11-26T23:13:23.321567Z","iopub.status.idle":"2021-11-26T23:13:29.578582Z","shell.execute_reply.started":"2021-11-26T23:13:23.321536Z","shell.execute_reply":"2021-11-26T23:13:29.577775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### First look at bureau.csv table","metadata":{}},{"cell_type":"code","source":"# Load Bureau table\n# Notice that we have two different keys: SK_ID_CURR helps us link to the train.csv table\n# while SK_ID_BUREAU helps us link to the bureau_balance table.\nbureau = pd.read_csv(f'{MainDir}/bureau.csv')\nprint(bureau.shape, \"- shape of bureau table\")\nbureau.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:13:29.57978Z","iopub.execute_input":"2021-11-26T23:13:29.580422Z","iopub.status.idle":"2021-11-26T23:13:33.916875Z","shell.execute_reply.started":"2021-11-26T23:13:29.580389Z","shell.execute_reply":"2021-11-26T23:13:33.916045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### first look at bureau_balance.csv","metadata":{}},{"cell_type":"code","source":"# Load bureau_balance table. Just a few columns here.\n# Strategy: make a pivot table with SK_ID_BUREAU as rows and count of STATUS as columns.\n# That should give us nine features, since there are eight unique status (plus we'll add a total column.)\n\nbureau_balance = pd.read_csv(f'{MainDir}/bureau_balance.csv')\nprint(bureau_balance.shape, \"- shape of bureau_balance table\")\nprint(bureau_balance.STATUS.nunique(), \"unique codes in STATUS column\")\nbureau_balance.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:13:33.919323Z","iopub.execute_input":"2021-11-26T23:13:33.919781Z","iopub.status.idle":"2021-11-26T23:13:45.736373Z","shell.execute_reply.started":"2021-11-26T23:13:33.919726Z","shell.execute_reply":"2021-11-26T23:13:45.735412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create features from bureau_balance","metadata":{}},{"cell_type":"code","source":"# get bureau_status features by creating a crosstab\n# BB_ prefix identifies features that came from bureau_balance\nbb_status = pd.crosstab(bureau_balance.SK_ID_BUREAU, bureau_balance.STATUS, margins = True)       # get count of 0, 1, 2, 3, 4, 5, C, X by SK_ID_BUREAU\nbb_status.columns = ['BB_'+column for column in bb_status.columns]\nprint(bb_status.shape, \"- shape of bb_status table\")\nbb_status.head(5)\n\n# I had also tried the proportion by row (e.g. the 5001709 row would be 0 , 0, 0, 0, 0, 0. 0.887, 0.113) but these factors didn't help the model.\n# The count data proved to be much more valuable in improving model score.","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:13:45.737658Z","iopub.execute_input":"2021-11-26T23:13:45.737903Z","iopub.status.idle":"2021-11-26T23:14:16.98696Z","shell.execute_reply.started":"2021-11-26T23:13:45.737874Z","shell.execute_reply":"2021-11-26T23:14:16.986046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Merge our bb_status feature table into bureau table on SK_ID_BUREAU","metadata":{}},{"cell_type":"code","source":"# bureau_balance only has SK_ID_BUREAU key field, so the only thing we can link it to is bureau.csv\nbureau = bureau.merge(bb_status, left_on = 'SK_ID_BUREAU', right_on = 'SK_ID_BUREAU')               # merge the tables\nbureau = bureau.drop(['SK_ID_BUREAU'], axis = 1)                                                    # no longer need this\nprint(bureau.shape, \"- shape of bureau table after merging in bureau_balance\")\n\n# BU_ prefix identifies features that came from bureau\nbureau.columns = ['BU_'+column if column !='SK_ID_CURR' else column for column in bureau.columns]\nbureau.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:14:16.98854Z","iopub.execute_input":"2021-11-26T23:14:16.989105Z","iopub.status.idle":"2021-11-26T23:14:18.427837Z","shell.execute_reply.started":"2021-11-26T23:14:16.989067Z","shell.execute_reply":"2021-11-26T23:14:18.427042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create features for bureau data","metadata":{}},{"cell_type":"code","source":"# I also tried aggregating by sum instead of mean - virtually no difference in final result\n\n# Create numeric features by grouping on SK_ID_CURR and finding group means\nbureau_num = bureau.groupby(by=['SK_ID_CURR']).mean().reset_index()                                 # group the numeric features by SK_ID_CURR\nprint(bureau_num.shape, \"- shape of numeric bureau features (incl index)\")                          # should be 134,542 x 22\n\n# Create categorical features by creating dummies and then taking group means\nbureau_cat = pd.get_dummies(bureau.select_dtypes('object'))                                         # this got rid of the SK_ID_CURR column ...\nbureau_cat['SK_ID_CURR'] = bureau['SK_ID_CURR']                                                     # so we have to replace it\nbureau_cat = bureau_cat.groupby(by = ['SK_ID_CURR']).mean().reset_index()                           # tried sum - didn't change anything\nprint(bureau_cat.shape, \"- shape of categorical bureau features (incl index)\")                      # should be 134,542 x 23\n\n# Number of past loans per customer (just one feature)\nbureau_count = bureau.groupby(by = ['SK_ID_CURR'])['BU_CREDIT_ACTIVE'].count().reset_index()\nbureau_count.rename(columns={'BU_CREDIT_ACTIVE':'COUNT_of_BUREAU'})                                ","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:14:18.428944Z","iopub.execute_input":"2021-11-26T23:14:18.429387Z","iopub.status.idle":"2021-11-26T23:14:19.572052Z","shell.execute_reply.started":"2021-11-26T23:14:18.429344Z","shell.execute_reply":"2021-11-26T23:14:19.571199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Merge our new features into training data on SK_ID_CURR","metadata":{}},{"cell_type":"code","source":"# merge bureau_num and bureau_cat into the training data\ntrain = train.merge(bureau_num, on='SK_ID_CURR', how='left')                                        # merge numeric features\ntrain = train.merge(bureau_cat, on='SK_ID_CURR', how='left')                                        # merge categorical features\ntrain = train.merge(bureau_count, on='SK_ID_CURR', how='left')                                      # merge count features\nprint(train.shape, \"- shape of training data after merges with bureau features\")                    # added 35 new features.\n\n# no longer need bureau, bureau_num, bureau_cat, bureau_count, bureau_balance\ndel bureau\ndel bureau_num\ndel bureau_cat\ndel bureau_count\ndel bureau_balance\ndel bb_status\ngc.collect()\ntrain.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:14:19.573202Z","iopub.execute_input":"2021-11-26T23:14:19.573513Z","iopub.status.idle":"2021-11-26T23:14:21.96459Z","shell.execute_reply.started":"2021-11-26T23:14:19.573482Z","shell.execute_reply":"2021-11-26T23:14:21.96364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### First look at previous.csv","metadata":{}},{"cell_type":"markdown","source":"### Extra features on previous, including total interest paid and interest rate on loan (heavy calculation)","metadata":{}},{"cell_type":"code","source":"%%time\n# Wall time for refreshing this was around 38 minutes with no accelerator. I want to run this ONE time and save the output.\nprevious = pd.read_csv(f'{MainDir}/previous_application.csv')\ndef calc_rate(row):\n    return np.rate(row['CNT_PAYMENT'], -row['AMT_ANNUITY'], row['AMT_CREDIT'], 0, guess = 0.05, maxiter = 10)\nprevious['CALC_RATE'] = previous.apply(calc_rate, axis=1)\nprevious['INTEREST_PAID'] = previous['AMT_ANNUITY'] * previous['CNT_PAYMENT'] - previous['AMT_CREDIT']\nprevious['INT_PRINC'] = previous['INTEREST_PAID'] / previous['AMT_CREDIT']\n\n# import numpy_financial as npf           # financial functions are supposedly deprecated in numpy - might need this package in the future\n# Example for SK_ID_PREV 2030495\n# pv = 17145                                # AMT_CREDIT\n# pmt = 1730.43                             # AMT_ANNUITY\n# nper = 12                                 # CNT_PAYMENT\n# print(np.rate(nper, -pmt, pv, 0), \"is the interest rate per period\")\n\n#https://numpy.org/numpy-financial/latest/\n#rate(nper, pmt, pv, fv[, when, guess, tol, …])   Compute the rate of interest per period.","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:14:21.965846Z","iopub.execute_input":"2021-11-26T23:14:21.966121Z","iopub.status.idle":"2021-11-26T23:22:16.209195Z","shell.execute_reply.started":"2021-11-26T23:14:21.966089Z","shell.execute_reply":"2021-11-26T23:22:16.208242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### POS_CASH_balance","metadata":{}},{"cell_type":"code","source":"# POS_CASH_balance contains both SK_ID_PREV and SK_ID_CURR. We could merge this data into previous_application on SK_ID_PREV\n# and then later merge previous_application into the training data, or we could just merge this directly into the training data.\n# We are going to try the former approach for now, although I might also try appending this data directly to training instead.\n\npos = pd.read_csv(f'{MainDir}/POS_CASH_balance.csv')\npos.drop(['SK_ID_CURR'], axis=1, inplace = True)\npos.columns = ['PO_'+column if column != 'SK_ID_PREV' else column for column in pos.columns]\npos.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:22:16.212444Z","iopub.execute_input":"2021-11-26T23:22:16.212725Z","iopub.status.idle":"2021-11-26T23:22:27.135966Z","shell.execute_reply.started":"2021-11-26T23:22:16.21269Z","shell.execute_reply":"2021-11-26T23:22:27.13503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create features from POS_CASH_balance","metadata":{}},{"cell_type":"code","source":"# I tried aggregating by mean and by sum - doesn't make much of a difference.\n\n# Create numeric features of pos by grouping on SK_ID_PREV and finding group means\npos_num = pos.groupby(by=['SK_ID_PREV']).mean().reset_index()                  # group the numeric features by SK_ID_PREV\nprint(pos_num.shape, \"- shape of numeric features (incl index)\")               # should be 936,325 x 7\n\n# Create categorical features by creating dummies and then taking group means\npos_cat = pd.get_dummies(pos.select_dtypes('object'))                          # this got rid of the SK_ID_PREV column ...\npos_cat['SK_ID_PREV'] = pos['SK_ID_PREV']                                      # so we have to replace it\npos_cat = pos_cat.groupby(by = ['SK_ID_PREV']).mean().reset_index()            # could try sum as well.\nprint(pos_cat.shape, \"- shape of categorical features (incl index)\")           # should be 936,325 x 10\n\n# merge pos_num and pos_cat into the previous_application data\nprevious = previous.merge(pos_num, on='SK_ID_PREV', how='left')                # merge numeric features\nprevious = previous.merge(pos_cat, on='SK_ID_PREV', how='left')                # merge categorical features\nprint(previous.shape, \"- shape of previous data after merges\")                 # added 35 new features.\n\n# don't need these anymore: pos, pos_num, pos_cat\ndel pos\ndel pos_num\ndel pos_cat\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:22:27.137224Z","iopub.execute_input":"2021-11-26T23:22:27.137729Z","iopub.status.idle":"2021-11-26T23:22:40.583127Z","shell.execute_reply.started":"2021-11-26T23:22:27.13768Z","shell.execute_reply":"2021-11-26T23:22:40.582267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### installment_payments","metadata":{}},{"cell_type":"code","source":"# note that this table contains only numeric features, nothing categorical.\ninst = pd.read_csv(f'{MainDir}/installments_payments.csv')\ninst.drop(['SK_ID_CURR'], axis=1, inplace = True)\ninst.columns = ['IP_'+column if column != 'SK_ID_PREV' else column for column in inst.columns]\ninst.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:22:40.58465Z","iopub.execute_input":"2021-11-26T23:22:40.58521Z","iopub.status.idle":"2021-11-26T23:22:59.383013Z","shell.execute_reply.started":"2021-11-26T23:22:40.58517Z","shell.execute_reply":"2021-11-26T23:22:59.382133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create features from installment_payments","metadata":{}},{"cell_type":"code","source":"inst_num = inst.groupby(by=['SK_ID_PREV']).mean().reset_index()         # group the numeric features by SK_ID_PREV\nprint(inst_num.shape, \"- shape of numeric features (incl index)\")       # should be 997,752 x 8\n\n# installments_payments only has numeric features\n\n# merge pos_num into the previous_application data\nprevious = previous.merge(inst_num, left_on='SK_ID_PREV', right_on = 'SK_ID_PREV', how='left')         \nprint(previous.shape, \"- shape of previous data after merges\")          # should be 1,670,214 x 59\n\n# don't need these anymore: inst, inst_num\ndel inst\ndel inst_num\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:22:59.384169Z","iopub.execute_input":"2021-11-26T23:22:59.384725Z","iopub.status.idle":"2021-11-26T23:23:04.151775Z","shell.execute_reply.started":"2021-11-26T23:22:59.384687Z","shell.execute_reply":"2021-11-26T23:23:04.150938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's try installment_payments direct to training","metadata":{}},{"cell_type":"code","source":"# Instead of merging installment_payments into previous_application and then merging previous_application into application_train,\n# we can also try merging these features directly into application_train because this dataset has both keys. I am leaving these out\n# because they aren't improving the model.\n\n#inst2 = pd.read_csv(f'{MainDir}/installments_payments.csv')\n#inst2.drop(['SK_ID_PREV'], axis=1, inplace = True)\n#inst2.columns = ['IPX_'+column if column != 'SK_ID_CURR' else column for column in inst2.columns]\n#inst2.head(5)\n\n#inst2_num = inst2.groupby(by=['SK_ID_CURR']).mean().reset_index()         # group the numeric features by SK_ID_CURR\n#print(inst2_num.shape, \"- shape of numeric features (incl index)\")       # should be 339,587 x 7\n\n#train = train.merge(inst2_num, left_on='SK_ID_CURR', right_on = 'SK_ID_CURR', how='left')         \n#print(train.shape, \"- shape of train data after merges\")          # should be 1,670,214 x 83\n\n# don't need these anymore: inst, inst_num\n#del inst2\n#del inst2_num\n#gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:23:04.153317Z","iopub.execute_input":"2021-11-26T23:23:04.154058Z","iopub.status.idle":"2021-11-26T23:23:04.158668Z","shell.execute_reply.started":"2021-11-26T23:23:04.154023Z","shell.execute_reply":"2021-11-26T23:23:04.157723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### POS_CASH_balance direct to training","metadata":{}},{"cell_type":"code","source":"# Likewise I am leaving these out of the model\n#pos2 = pd.read_csv(f'{MainDir}/POS_CASH_balance.csv')\n#pos2.drop(['SK_ID_PREV'], axis=1, inplace = True)\n#pos2.columns = ['POX_'+column if column != 'SK_ID_CURR' else column for column in pos2.columns]\n#pos2.head(5)\n\n#pos2_num = pos2.groupby(by=['SK_ID_CURR']).mean().reset_index()         # group the numeric features by SK_ID_CURR\n#print(pos2_num.shape, \"- shape of numeric features (incl index)\")       # should be 339,587 x 7\n\n#train = train.merge(pos2_num, left_on='SK_ID_CURR', right_on = 'SK_ID_CURR', how='left')         \n#print(train.shape, \"- shape of train data after merges\")          # should be 1,670,214 x 83\n\n# don't need these anymore: inst, inst_num\n#del pos2\n#del pos2_num\n#gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:23:04.160197Z","iopub.execute_input":"2021-11-26T23:23:04.160429Z","iopub.status.idle":"2021-11-26T23:23:04.171121Z","shell.execute_reply.started":"2021-11-26T23:23:04.160402Z","shell.execute_reply":"2021-11-26T23:23:04.170405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### credit_card_bureau","metadata":{}},{"cell_type":"code","source":"# Note that there is not a lot of data here - only about 100,000 SK_ID_PREVs out of a million possible. \nccb = pd.read_csv(f'{MainDir}/credit_card_balance.csv')\nccb.drop(['SK_ID_CURR'], axis=1, inplace = True)\nccb.columns = ['CC_'+column if column != 'SK_ID_PREV' else column for column in ccb.columns]\nccb.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:23:04.172498Z","iopub.execute_input":"2021-11-26T23:23:04.173258Z","iopub.status.idle":"2021-11-26T23:23:17.167483Z","shell.execute_reply.started":"2021-11-26T23:23:04.173214Z","shell.execute_reply":"2021-11-26T23:23:17.166577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create features from credit_card_bureau","metadata":{}},{"cell_type":"code","source":"# Create numeric features of pos by grouping on SK_ID_PREV and finding group means\nccb_num = ccb.groupby(by=['SK_ID_PREV']).mean().reset_index()         # group the numeric features by SK_ID_PREV\nprint(ccb_num.shape, \"- shape of numeric features (incl index)\")      # should be 104,307 x 22\n\n# Create categorical features by creating dummies and then taking group means\nccb_cat = pd.get_dummies(ccb.select_dtypes('object'))                 # this got rid of the SK_ID_PREV column ...\nccb_cat['SK_ID_PREV'] = ccb['SK_ID_PREV']                             # so we have to replace it\nccb_cat = ccb_cat.groupby(by = ['SK_ID_PREV']).mean().reset_index()   # could try sum as well.\nprint(ccb_cat.shape, \"- shape of categorical features (incl index)\")     # should be 305,811 x 24\n\n# merge ccb_num and ccb_cat into the previous_application data\nprevious = previous.merge(ccb_num, on='SK_ID_PREV', how='left')                # merge numeric features\nprevious = previous.merge(ccb_cat, on='SK_ID_PREV', how='left')                # merge categorical features\nprint(previous.shape, \"- shape of previous data after merges\")                 # added 35 new features.\n\n# don't need these anymore: pos, pos_num, pos_cat\ndel ccb\ndel ccb_num\ndel ccb_cat\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:23:17.168911Z","iopub.execute_input":"2021-11-26T23:23:17.169243Z","iopub.status.idle":"2021-11-26T23:23:22.067231Z","shell.execute_reply.started":"2021-11-26T23:23:17.1692Z","shell.execute_reply":"2021-11-26T23:23:22.066439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Merge previous table into training data","metadata":{}},{"cell_type":"code","source":"# don't need this anymore - all further joins will be on SK_ID_CURR\nprevious.drop(['SK_ID_PREV'], axis = 1, inplace = True)\n\nprevious.columns = ['PR_'+column if column !='SK_ID_CURR' else column for column in previous.columns]\nprevious['PR_DAYS_LAST_DUE'].replace({365243: np.nan}, inplace = True)\nprevious['PR_DAYS_TERMINATION'].replace({365243: np.nan}, inplace = True)\nprevious['PR_DAYS_FIRST_DRAWING'].replace({365243: np.nan}, inplace = True)\n\n# Create numeric features by grouping on SK_ID_CURR and finding group means\nprevious_num = previous.groupby(by=['SK_ID_CURR']).mean().reset_index()         # group the numeric features by SK_ID_CURR\nprint(previous_num.shape, \"- shape of numeric features (incl index)\")         # should be 305,811 x 13\n\n# Create categorical features by creating dummies and then taking group means\nprevious_cat = pd.get_dummies(previous.select_dtypes('object'))                 # this got rid of the SK_ID_CURR column ...\nprevious_cat['SK_ID_CURR'] = previous['SK_ID_CURR']                             # so we have to replace it\nprevious_cat = previous_cat.groupby(by = ['SK_ID_CURR']).mean().reset_index()   # could try sum as well.\nprint(previous_cat.shape, \"- shape of categorical features (incl index)\")     # should be 305,811 x 24\n\n# merge bureau_num and bureau_cat into the training data\ntrain = train.merge(previous_num, on='SK_ID_CURR', how='left')                # merge numeric features\ntrain = train.merge(previous_cat, on='SK_ID_CURR', how='left')                # merge categorical features\nprint(train.shape, \"- shape of training data after ALL merges\")                 # added 35 new features.\n\n# no longer need bureau, bureau_num, bureau_car, bureau_count, bureau_balance\ndel previous\ndel previous_num\ndel previous_cat\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:23:22.068558Z","iopub.execute_input":"2021-11-26T23:23:22.068773Z","iopub.status.idle":"2021-11-26T23:23:38.241552Z","shell.execute_reply.started":"2021-11-26T23:23:22.068746Z","shell.execute_reply":"2021-11-26T23:23:38.240646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### data cleansing and feature engineering: create new features based on ratios, logs, etc.","metadata":{}},{"cell_type":"code","source":"# what is going on with days_employed? Over 50,000 entries have the value 365,243 days! Let's replace those with NaN and let the imputer deal with them.\ntrain['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\n# ratio features\ntrain['CI_ratio'] = train['AMT_CREDIT'] / train['AMT_INCOME_TOTAL']        # credit-to-income ratio\ntrain['AI_ratio'] = train['AMT_ANNUITY'] / train['AMT_INCOME_TOTAL']       # annuity-to-income ratio\ntrain['AC_ratio'] = train['AMT_CREDIT'] / train['AMT_ANNUITY']             # credit to annuity - basically the term of the loan in years\ntrain['CG_ratio'] = train['AMT_CREDIT'] / train['AMT_GOODS_PRICE']         # credit to goods price ratio - how much was financed?\n\n# log features\ntrain['log_INCOME'] = np.log(train['AMT_INCOME_TOTAL'])                    # log of income\ntrain['log_ANNUITY'] = np.log(train['AMT_ANNUITY'])                        # log of annuity\ntrain['log_CREDIT'] = np.log(train['AMT_CREDIT'])                          # log of credit\ntrain['log_GOODS'] = np.log(train['AMT_GOODS_PRICE'])                      # log of goods price\n\n# flag features\ntrain['MissingBureau'] = train.iloc[:, 41:44].isnull().sum(axis=1).astype(\"category\")   # number of bureaus with no score\ntrain['FLAG_CG_ratio'] = train['AMT_CREDIT'] > train['AMT_GOODS_PRICE']                 # FLAG if you borrowed more than the price of the item\ntrain['DAYS_ID_4200'] = train['DAYS_ID_PUBLISH'] < -4200                             # IDs more than about 14 years old are from USSR\n\n# EXT_SOURCE_x variables are very important - let's not leave missing values up to the imputer!\n# Instead of imputing missing values by column mean or median, let's fill in missing values by row\n# i.e. missing scores are replaced with the average of the scores we do have. If there are no scores at all\n# let's just give them a value of 0.2 for now.\ntrain['AVG_EXT'] = train.iloc[:, 41:44].sum(axis=1)/(3- train.iloc[:,41:44].isnull().sum(axis=1))   # average of the (at most) three scores\ntrain['AVG_EXT'].replace(np.nan, 0.2, inplace = True)   # get rid of any /0 errors generated from previous step\n\ntrain.EXT_SOURCE_1.fillna(train.AVG_EXT, inplace=True)\ntrain.EXT_SOURCE_2.fillna(train.AVG_EXT, inplace=True)\ntrain.EXT_SOURCE_3.fillna(train.AVG_EXT, inplace=True)\ntrain.EXT_SOURCE_MAX = np.max(train.iloc[:,41:44], axis = 1)\n# train.drop(['AVG_EXT'], axis = 1, inplace = True)\n\ntrain.drop(['ORGANIZATION_TYPE'], axis = 1, inplace = True)  # 58 dummies, doesn't do jackshit - let's just get rid of it now\n\n# Ratio Features\ntrain['OD_ratio'] = train['BU_AMT_CREDIT_SUM_OVERDUE'] / train['BU_AMT_CREDIT_SUM_DEBT']   # proportion of debt that is overdue\ntrain['OD_ratio'].replace([np.nan, np.inf, -np.inf], 0, inplace = True)\ntrain['Credit_ratio'] = train['BU_AMT_CREDIT_SUM'] / train['BU_AMT_CREDIT_SUM_LIMIT']      # proportion of credit line used\ntrain['Credit_ratio'].replace([np.nan, np.inf, -np.inf], 0, inplace = True)\ntrain['Debt_ratio'] = train['BU_AMT_CREDIT_SUM_DEBT'] / train['BU_AMT_CREDIT_SUM']         # debt percentage\ntrain['Debt_ratio'].replace([np.nan, np.inf, -np.inf], 0, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:23:38.242896Z","iopub.execute_input":"2021-11-26T23:23:38.243126Z","iopub.status.idle":"2021-11-26T23:23:39.32388Z","shell.execute_reply.started":"2021-11-26T23:23:38.243098Z","shell.execute_reply":"2021-11-26T23:23:39.323039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### split out our training data - start with about 10% or 30k out of 300k","metadata":{}},{"cell_type":"code","source":"y = train['TARGET'].values\nX_train, X_valid, y_train, y_valid = train_test_split(train.drop(['TARGET', 'SK_ID_CURR'], axis = 1), y, stratify = y, test_size=1 - train_size, random_state=1)\nprint('Shape of X_train:',X_train.shape)\nprint('Shape of y_train:',y_train.shape)\nprint('Shape of X_valid:',X_valid.shape)\nprint('Shape of y_valid:',y_valid.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:23:39.325262Z","iopub.execute_input":"2021-11-26T23:23:39.325499Z","iopub.status.idle":"2021-11-26T23:23:41.453436Z","shell.execute_reply.started":"2021-11-26T23:23:39.325473Z","shell.execute_reply":"2021-11-26T23:23:41.452495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### make lists of cat and num features for pipeline, based on dtype","metadata":{}},{"cell_type":"code","source":"types = np.array([z for z in X_train.dtypes])        # array([dtype('float64'), dtype('float64'), dtype('O'), dtype('O') ...])\nall_columns = X_train.columns.values                 # list of all column names\nis_num = types != 'object'                           # returns array([False, False, False, False,  True,  True, ...) where True is a numeric variable\nnum_features = all_columns[is_num].tolist()          # list of all numeric columns\ncat_features = all_columns[~is_num].tolist()         # list of all categorical columns\n\nprint(len(num_features), \"numeric features\")\nprint(len(cat_features), \"categorical features\")","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:23:41.45669Z","iopub.execute_input":"2021-11-26T23:23:41.457053Z","iopub.status.idle":"2021-11-26T23:23:41.466426Z","shell.execute_reply.started":"2021-11-26T23:23:41.457015Z","shell.execute_reply":"2021-11-26T23:23:41.465371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### build model pipeline based on num_cols and cat_cols lists","metadata":{}},{"cell_type":"code","source":"features = num_features + cat_features\n\nPipe_num = Pipeline(\n    steps=[\n    ('imputer', SimpleImputer(strategy = 'median')),        # tried median, mean, constant strategies\n    ('scaler', StandardScaler())       ])\n\nPipe_cat = Pipeline(\n    steps=[\n    ('imputer', SimpleImputer(strategy = 'constant', fill_value = 'Unknown')),\n    ('onehot', OneHotEncoder())        ])\n\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('num', Pipe_num, num_features),\n        ('cat', Pipe_cat, cat_features)])\n\npreprocessor.fit(train[features])\nX_train = preprocessor.transform(X_train[features])\nX_valid = preprocessor.transform(X_valid[features])\n\nprint('Shape of X_train:',X_train.shape)\nprint('Shape of y_train:',y_train.shape)\n\ngc.collect()\n\ndel train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:23:41.467877Z","iopub.execute_input":"2021-11-26T23:23:41.46835Z","iopub.status.idle":"2021-11-26T23:25:29.551515Z","shell.execute_reply.started":"2021-11-26T23:23:41.468312Z","shell.execute_reply":"2021-11-26T23:25:29.550548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build Model Scoreboard\nEach time we run a new model, we will append a new row to the scoreboard.","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)             # LGBM in particular has long hyperparameters and I want to see them all\nresults = pd.DataFrame(columns = ['Model Type','AUC - 10xv', 'AUC - Valid', 'Hyperparameters'])","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:25:29.553025Z","iopub.execute_input":"2021-11-26T23:25:29.553338Z","iopub.status.idle":"2021-11-26T23:25:29.560808Z","shell.execute_reply.started":"2021-11-26T23:25:29.553296Z","shell.execute_reply":"2021-11-26T23:25:29.560213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Memory Management\nThe only data frame open right now should be the model scoreboard 'results' that we just created. Even the train dataframe can be closed now because the information it contained is stored in arrays X_train, X_valid, etc. We should have about 12 GB free at this point.","metadata":{"execution":{"iopub.status.busy":"2021-11-24T01:42:29.954859Z","iopub.execute_input":"2021-11-24T01:42:29.955175Z","iopub.status.idle":"2021-11-24T01:42:29.958884Z","shell.execute_reply.started":"2021-11-24T01:42:29.955141Z","shell.execute_reply":"2021-11-24T01:42:29.957868Z"}}},{"cell_type":"code","source":"%who_ls DataFrame","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:25:29.561957Z","iopub.execute_input":"2021-11-26T23:25:29.562366Z","iopub.status.idle":"2021-11-26T23:25:29.575683Z","shell.execute_reply.started":"2021-11-26T23:25:29.562323Z","shell.execute_reply":"2021-11-26T23:25:29.574502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import psutil\nprint(psutil.virtual_memory()[1]/1E9, \"GB free\")","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:25:29.577211Z","iopub.execute_input":"2021-11-26T23:25:29.577612Z","iopub.status.idle":"2021-11-26T23:25:29.587793Z","shell.execute_reply.started":"2021-11-26T23:25:29.577566Z","shell.execute_reply":"2021-11-26T23:25:29.586925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"markdown","source":"### Logistic Regression\nI would very much like for this model to finish running before the sun turns into a red giant and swallows up the earth, so I am limiting the training data to 10,000 records. Obviously we could improve our score with a larger training set.","metadata":{}},{"cell_type":"code","source":"%%time\n\nlr_clf = LogisticRegression(max_iter=1000, solver='saga', penalty = 'elasticnet')\nlr_parameters = {'l1_ratio':[1], 'C': [1]}\nlr_grid = GridSearchCV(lr_clf, lr_parameters, cv=10, refit='True', n_jobs=-1, verbose=1, scoring='roc_auc')\nlr_grid.fit(X_train[0:10000], y_train[0:10000])\n\nlr_model = lr_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Logistic Regression',\n                          'AUC - 10xv' : lr_grid.best_score_,\n                          'AUC - Valid' : roc_auc_score(y_valid, lr_model.predict_proba(X_valid)[:, 1]),\n                          'Hyperparameters' : lr_grid.best_params_},\n                        ignore_index=True)\nresults","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:25:29.589245Z","iopub.execute_input":"2021-11-26T23:25:29.589733Z","iopub.status.idle":"2021-11-26T23:32:19.378645Z","shell.execute_reply.started":"2021-11-26T23:25:29.589689Z","shell.execute_reply":"2021-11-26T23:32:19.377576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Support Vector Machine\nOnce again we need to limit the amount of training data. SVM model runtime varies with the square of the number of training observations (so training on 2000 obs should take 4x as long as training on 1000, etc.) But SVM does have potential for training a 'wide' (many-featured) data set without using too many observations.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nsv_clf = SVC(probability = True)\nsv_parameters = {\n    'kernel' : ['linear'],   # 'sigmoid', 'poly', 'rbf'],\n    'gamma' : [0.3],\n    'C' : [0.01]\n}\nsv_grid = GridSearchCV(sv_clf, sv_parameters, cv=10, refit='True', n_jobs=-1, verbose=1, scoring='roc_auc')\nsv_grid.fit(X_train[0:5000], y_train[0:5000])\nsv_model = sv_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Support Vector Machine',\n                          'AUC - 10xv' : sv_grid.best_score_,\n                          'AUC - Valid' : roc_auc_score(y_valid, sv_model.predict_proba(X_valid)[:, 1]),\n                          'Hyperparameters' : sv_grid.best_params_},\n                        ignore_index=True)\ngc.collect()\nresults","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:32:19.381104Z","iopub.execute_input":"2021-11-26T23:32:19.381787Z","iopub.status.idle":"2021-11-26T23:36:21.522205Z","shell.execute_reply.started":"2021-11-26T23:32:19.381729Z","shell.execute_reply":"2021-11-26T23:36:21.521319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest\nThis is our best performing model so far.","metadata":{}},{"cell_type":"code","source":"%%time\nrf_clf = RandomForestClassifier(random_state=1, n_estimators=100)\nrf_parameters = {'max_depth': [32],  'min_samples_leaf': [34]}        # results from previous run - save a little time\nrf_grid = GridSearchCV(rf_clf, rf_parameters, cv=10, refit='True', n_jobs=-1, verbose=1, scoring='roc_auc')\nrf_grid.fit(X_train, y_train)\nrf_model = rf_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Random Forest',\n                          'AUC - 10xv' : rf_grid.best_score_,\n                          'AUC - Valid' : roc_auc_score(y_valid, rf_model.predict_proba(X_valid)[:, 1]),\n                          'Hyperparameters' : rf_grid.best_params_},\n                        ignore_index=True)\ngc.collect()\nresults","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:36:21.526243Z","iopub.execute_input":"2021-11-26T23:36:21.526496Z","iopub.status.idle":"2021-11-26T23:49:56.873724Z","shell.execute_reply.started":"2021-11-26T23:36:21.526465Z","shell.execute_reply":"2021-11-26T23:49:56.872704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree","metadata":{}},{"cell_type":"code","source":"%%time\ndt_clf = DecisionTreeClassifier(random_state=1)\ndt_parameters = {\n    'max_depth': [12],\n    'min_samples_leaf': [8]\n}\n\ndt_grid = GridSearchCV(dt_clf, dt_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring='roc_auc')\ndt_grid.fit(X_train, y_train)\n\ndt_model = dt_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Decision Tree',\n                          'AUC - 10xv' : dt_grid.best_score_,\n                          'AUC - Valid' : roc_auc_score(y_valid, dt_model.predict_proba(X_valid)[:, 1]),\n                          'Hyperparameters' : dt_grid.best_params_},\n                        ignore_index=True)\ngc.collect()\nresults","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:49:56.875273Z","iopub.execute_input":"2021-11-26T23:49:56.875544Z","iopub.status.idle":"2021-11-26T23:53:15.907151Z","shell.execute_reply.started":"2021-11-26T23:49:56.875512Z","shell.execute_reply":"2021-11-26T23:53:15.906044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XG Boost","metadata":{}},{"cell_type":"code","source":"%%time\nXGB_clf = XGBClassifier(objective='binary:logistic', use_label_encoder=False)\nXGB_parameters = {\n    'max_depth': [10],\n    'n_estimators': [100],\n    'learning_rate': [0.5]\n}\n\nXGB_grid = GridSearchCV(XGB_clf, XGB_parameters, cv=10, n_jobs=1, verbose=True, scoring= 'roc_auc')\nXGB_grid.fit(X_train, y_train)\n\nXGB_model = XGB_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'XG Boost',\n                          'AUC - 10xv' : XGB_grid.best_score_,\n                          'AUC - Valid' : roc_auc_score(y_valid, XGB_model.predict_proba(X_valid)[:, 1]),\n                          'Hyperparameters' : XGB_grid.best_params_},\n                        ignore_index=True)\ngc.collect()\nresults","metadata":{"execution":{"iopub.status.busy":"2021-11-26T23:53:15.908843Z","iopub.execute_input":"2021-11-26T23:53:15.909693Z","iopub.status.idle":"2021-11-27T01:02:47.257856Z","shell.execute_reply.started":"2021-11-26T23:53:15.909639Z","shell.execute_reply":"2021-11-27T01:02:47.256881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Light GBM\nLight Gradient Boost Machine runs fast and produces our best predictive model.","metadata":{}},{"cell_type":"code","source":"%%time\nimport lightgbm as lgb\n\nparams = {'boosting_type': 'gbdt', 'objective': 'binary',                           # choices for boosting type are 'gbdt', 'rf', 'dart', 'goss'\n          'nthread': -1, 'num_leaves': 35, 'learning_rate': 0.02,                                  # default learning rate is 0.1 - lower numbers are working well\n          'max_bin': 512, 'subsample_for_bin': 200, 'subsample': 0.88,\n          'subsample_freq': 1, 'colsample_bytree': 0.8, 'reg_alpha': 20,\n          'reg_lambda': 20, 'min_split_gain': 0.5, 'min_child_weight': 1,\n          'min_child_samples': 10, 'scale_pos_weight': 11.5, 'num_class' : 1,       # about 92% target=0 to 8% target=1 - ratio is about 11.5 to 1\n          'metric' : 'auc'\n          }\n    \nLGB_clf = lgb.LGBMClassifier(**params)\nLGB_parameters = {\n    'max_depth': [15],\n    'n_estimators': [2000]\n}\n\nLGB_grid = GridSearchCV(LGB_clf, LGB_parameters, cv=10, scoring= 'roc_auc')\nLGB_grid.fit(X_train, y_train)\nLGB_model = LGB_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Light GBM',\n                          'AUC - 10xv' : LGB_grid.best_score_,\n                          'AUC - Valid' : roc_auc_score(y_valid, LGB_model.predict_proba(X_valid)[:, 1]),\n                          'Hyperparameters' : LGB_grid.best_params_},\n                        ignore_index=True)\ngc.collect()\nresults","metadata":{"execution":{"iopub.status.busy":"2021-11-27T01:02:47.259264Z","iopub.execute_input":"2021-11-27T01:02:47.259595Z","iopub.status.idle":"2021-11-27T01:26:05.561741Z","shell.execute_reply.started":"2021-11-27T01:02:47.259562Z","shell.execute_reply":"2021-11-27T01:26:05.560652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing Model Outputs","metadata":{}},{"cell_type":"markdown","source":"### Plotting the ROC curve (for AUC score on validation data)\nGood information on derivation of roc_auc: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n","metadata":{}},{"cell_type":"code","source":"probabilities = LGB_model.predict_proba(X_valid)[:,1]\nfpr, tpr, thresholds = roc_curve(y_valid, probabilities)\nauc = roc_auc_score(y_valid, probabilities)               # AUC on validation data was .7802 per table above\nplt.figure(figsize=(6,6))\nplt.plot(fpr, tpr)                                        # plot the blue curve\nplt.plot([0, 1], [0, 1])                                  # plot the orange 45 degree line\nplt.title('Receiver operating characteristic curve')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.legend([\"AUC = %.6f\"%auc])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T01:26:05.563582Z","iopub.execute_input":"2021-11-27T01:26:05.564404Z","iopub.status.idle":"2021-11-27T01:26:12.580623Z","shell.execute_reply.started":"2021-11-27T01:26:05.564355Z","shell.execute_reply":"2021-11-27T01:26:12.579636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance\nAC_ratio (basically the term of the loan) is the most important feature, followed by age, length of time since ID changed, then the credit bureau scores. Creating the right engineered features was the key to a good performing model; the top 50 contains many of the features that we created.","metadata":{}},{"cell_type":"code","source":"LGB_model.fit(X_train, y_train)\nfeature_imp = pd.DataFrame(zip(LGB_model.feature_importances_, features), columns=['Value','Feature']).sort_values(by=\"Value\", ascending=False)\nfeature_imp = feature_imp.iloc[0:50,:]\nplt.figure(figsize=[6,10])\nsns.barplot(feature_imp['Value'], feature_imp['Feature'], orient = \"h\", color = \"lightsteelblue\")\nplt.title(\"Most important features (top 50)\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T01:26:12.581867Z","iopub.execute_input":"2021-11-27T01:26:12.582117Z","iopub.status.idle":"2021-11-27T01:28:25.809152Z","shell.execute_reply.started":"2021-11-27T01:26:12.582088Z","shell.execute_reply":"2021-11-27T01:28:25.80811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating separation between classes","metadata":{}},{"cell_type":"code","source":"output_data = pd.DataFrame({'prediction' : LGB_model.predict_proba(X_valid)[:,1], 'target' : y_valid})\nplt.figure(figsize=[12,3])\nsns.boxplot(output_data.prediction, output_data.target, orient = \"h\")\nplt.title('Distribution of predicted probabilities - validation data')\nplt.show()\n\n# This is more like it - earlier models were showing that the '0' class had a 7% chance of default and the '1' class had a 9% chance, something like that.\n# By adding the class weights, we have a lot more separation between the classes.","metadata":{"execution":{"iopub.status.busy":"2021-11-27T01:30:00.430175Z","iopub.execute_input":"2021-11-27T01:30:00.431268Z","iopub.status.idle":"2021-11-27T01:30:06.588029Z","shell.execute_reply.started":"2021-11-27T01:30:00.431216Z","shell.execute_reply":"2021-11-27T01:30:06.587093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# alternate plot, not sure which I prefer\nplt.figure(figsize=[12,3])\nsns.kdeplot(x= LGB_model.predict_proba(X_valid)[:,1], hue=y_valid, common_norm = False, fill = True)\nplt.xlabel('probability')\nplt.xlim(0,1)\nplt.title('KDE plots by of predicted probabilities by target - validation data')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T01:28:31.933213Z","iopub.execute_input":"2021-11-27T01:28:31.934061Z","iopub.status.idle":"2021-11-27T01:28:38.677888Z","shell.execute_reply.started":"2021-11-27T01:28:31.933997Z","shell.execute_reply":"2021-11-27T01:28:38.677201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion, recall, precision matrices\nRECALL is much more important than precision or accuracy for this application, i.e. if we predict that someone will pay, we need to be right! It is quite easy to make a model that achieves 90-92% \"accuracy\" because the targets are unbalanced; but that is not what we are trying to do.","metadata":{}},{"cell_type":"code","source":"labels = ['Paid -0-','Default -1-']\n\nboxdata['binpred'] = np.floor(boxdata['prediction']+0.5)     # one of many ways to convert probabilistic predictions to 0/1 binary predictions\nCM = confusion_matrix(boxdata.target, boxdata.binpred)       # confusion matrix\nRM = (((CM.T)/(CM.sum(axis=1))).T)                           # recall matrix\nPM = (CM/CM.sum(axis=0))                                     # precision matrix\n\nplots = [CM, RM, PM]\nplot_titles = ['Confusion Matrix', 'Recall Matrix', 'Precision Matrix']\nformats = [\"d\", \".4f\", \".4f\"]\nplt.figure(figsize=(12,4))\nfor i in range(0,3):\n    plt.subplot(1,3,i+1)\n    sns.heatmap(plots[i], annot=True, cmap='BuPu', fmt = formats[i], xticklabels = labels, yticklabels=labels, cbar = False)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Orignal Class')\n    plt.title(plot_titles[i])\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T01:28:38.679248Z","iopub.execute_input":"2021-11-27T01:28:38.680244Z","iopub.status.idle":"2021-11-27T01:28:39.421419Z","shell.execute_reply.started":"2021-11-27T01:28:38.680194Z","shell.execute_reply":"2021-11-27T01:28:39.420318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Store model results for outputs notebook","metadata":{}},{"cell_type":"code","source":"output_data = pd.DataFrame({'target': y_valid, 'prediction' : LGB_model.predict_proba(X_valid)[:,1]})\noutput_data['binary'] = np.floor(output_data['prediction']+0.5)     # one of many ways to convert probabilistic predictions to 0/1 binary predictions\noutput_data.head(5)\noutput_data.to_pickle(\"./output.pkl\", compression='infer', storage_options=None)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T01:38:39.623396Z","iopub.execute_input":"2021-11-27T01:38:39.624296Z","iopub.status.idle":"2021-11-27T01:38:45.625694Z","shell.execute_reply.started":"2021-11-27T01:38:39.624252Z","shell.execute_reply":"2021-11-27T01:38:45.624896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Model Selection - save data","metadata":{}},{"cell_type":"code","source":"gc.collect()\n\n#final_model = lgb.LGBMClassifier(**params, n_estimators= 2000, max_depth = 15)\n#final_model.fit(X_train, y_train)\n\n#joblib.dump(preprocessor, 'default_preprocessor_08.joblib') \n#joblib.dump(final_model, 'default_model_08.joblib')\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T01:28:39.423285Z","iopub.execute_input":"2021-11-27T01:28:39.423615Z","iopub.status.idle":"2021-11-27T01:28:39.589288Z","shell.execute_reply.started":"2021-11-27T01:28:39.42357Z","shell.execute_reply":"2021-11-27T01:28:39.588622Z"},"trusted":true},"execution_count":null,"outputs":[]}]}