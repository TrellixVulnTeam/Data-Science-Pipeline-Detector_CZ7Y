{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# japanize-matplotlibはkaggleのデフォライブラリーではないので別途インストールする必要があります\n## kaggle kernelで使用するようにするための方法：https://bit.ly/3kViqBX\n\n!pip install japanize-matplotlib","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:49:48.866549Z","iopub.execute_input":"2022-05-10T05:49:48.86731Z","iopub.status.idle":"2022-05-10T05:50:05.2831Z","shell.execute_reply.started":"2022-05-10T05:49:48.867261Z","shell.execute_reply":"2022-05-10T05:50:05.28179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 必要なライブラリのimport\nimport pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nimport os\nimport re\nimport glob\nimport shutil\nfrom pathlib import Path\n\n# matplotlibの日本語化対応\nimport japanize_matplotlib\n\n# データフレーム表示用関数\nfrom IPython.display import display\n\n# 表示オプション調整\n# numpyの浮動小数点の表示精度\nnp.set_printoptions(suppress=True, precision=4)\n\n# pandasでの浮動小数点の表示精度\npd.options.display.float_format = '{:.4f}'.format\n\n# データフレームですべての項目を表示\npd.set_option(\"display.max_columns\",None)\n\n# グラフのデフォルトフォント指定\nplt.rcParams[\"font.size\"] = 14\n\n# 乱数の種\nrandom_seed = 123","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:50:07.497323Z","iopub.execute_input":"2022-05-10T05:50:07.497627Z","iopub.status.idle":"2022-05-10T05:50:08.831817Z","shell.execute_reply.started":"2022-05-10T05:50:07.497585Z","shell.execute_reply":"2022-05-10T05:50:08.830702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# セッション時間表示\n\n# https://github.com/nyk510/vivid/blob/master/vivid/utils.py\nfrom contextlib import contextmanager\nfrom time import time\n\nclass Timer:\n    def __init__(self, logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None, sep=' '):\n\n        if prefix: format_str = str(prefix) + sep + format_str\n        if suffix: format_str = format_str + sep + str(suffix)\n        self.format_str = format_str\n        self.logger = logger\n        self.start = None\n        self.end = None\n\n    @property\n    def duration(self):\n        if self.end is None:\n            return 0\n        return self.end - self.start\n\n    def __enter__(self):\n        self.start = time()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.end = time()\n        out_str = self.format_str.format(self.duration)\n        if self.logger:\n            self.logger.info(out_str)\n        else:\n            print(out_str)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:50:17.801774Z","iopub.execute_input":"2022-05-10T05:50:17.80225Z","iopub.status.idle":"2022-05-10T05:50:17.812701Z","shell.execute_reply.started":"2022-05-10T05:50:17.802212Z","shell.execute_reply":"2022-05-10T05:50:17.812058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 今回のコンペについて\n## コンペの外観\n\n* [Home Credit とは](https://www.homecredit.net/)：信用力が足りずに融資を受けることができない顧客にも融資を行う会社のことです。\n* 目的：個人のクレジット情報や以前の応募情報などから、各データが債務不履行になるかどうかを予測する問題です。\n\n\n* 評価方法：[AUC (Aread Under the ROC curve)](https://blog.kikagaku.co.jp/roc-auc)\n  * 閾値を変化させた際に描かれるROC曲線の下の面積(=AUC)を0~1の範囲で判断し、1に近づくほど良いとされます。\n  * つまり positive or negative, Yes or No みたいなのをきちんと分類されていれば良いとされます。\n\n","metadata":{}},{"cell_type":"markdown","source":"# 機械学習とは\n機械学習とは、ざっと言ってしまうとあるデータ X を入力として対応する予測値 y を取り出すような対応関係を作成することです。\n\n例：タイタニック号で、乗客が生きるか死ぬかを予測する問題だと X は乗客の年齢, 性別, 船室のグレード… など乗客に紐づく情報のことを指します。通常、この情報のことを特徴量とよびます。\n\n特徴量 X と 予測値 y が用意できれば学習用データ (X - y の関係がわかっているデータ) を元にして X をいれて y になるようにモデルを調整する。この調整の段階を学習とよびます。学習には様々なアルゴリズムがあるが、X, y を用意しなくてはならない部分は基本的に変わらないです。","metadata":{}},{"cell_type":"markdown","source":"# データ読み込み","metadata":{}},{"cell_type":"code","source":"# ローカルで使うときはこっち！\n\n# # input_dir（input directory） を作ります\n# current_note_path = os.path.dirname(os.path.abspath('__file__'))\n# INPUT_DIR = os.path.join(current_note_path, \"data\")\n\n# # INPUT_DIRがまだ作られていなければ作成\n# if not os.path.isdir(INPUT_DIR):\n#     os.mkdir(INPUT_DIR)\n\n# # csvファイルを `data` ディレクトリ（=フォルダー） に移動させます\n# unique_dir_names = []\n# # ローカルで使用する場合は f'{current_note_path}' に変えます\n# for f in Path(f'{current_note_path}').rglob('*.csv'):\n#     unique_dir_names.append(f)\n\n# for file in list(set(unique_dir_names)):\n#     print(f'moved file: {file}')\n#     shutil.move(f'{file}', f'{INPUT_DIR}')","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:50:17.814054Z","iopub.execute_input":"2022-05-10T05:50:17.81485Z","iopub.status.idle":"2022-05-10T05:50:17.856211Z","shell.execute_reply.started":"2022-05-10T05:50:17.81481Z","shell.execute_reply":"2022-05-10T05:50:17.855276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kaggle notebookで使うときはこっち！\n\n# INPUT_DIR を指定します\nINPUT_DIR = '../input/home-credit-default-risk'\n\n# output_dir(output directory) を作ります\n## notbookがあるディレクトリパスを `current_note_path` に渡します\ncurrent_note_path = os.path.dirname(os.path.abspath('__file__'))\nOUTPUT_DIR = os.path.join(current_note_path, 'outputs')\n\n# OUTPUT_DIRがまだ作られていなければ作成\nif not os.path.isdir(OUTPUT_DIR):\n    os.mkdir(OUTPUT_DIR)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:50:17.857615Z","iopub.execute_input":"2022-05-10T05:50:17.857841Z","iopub.status.idle":"2022-05-10T05:50:17.867849Z","shell.execute_reply.started":"2022-05-10T05:50:17.857815Z","shell.execute_reply":"2022-05-10T05:50:17.867168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# csv を読み取る関数を設定したあげると、pathや拡張子を書かずに読み込めるので入力が楽になります（Kaggle notebookだとされないかも）\ndef read_csv(name, **kwrgs):\n    path = os.path.join(INPUT_DIR, name + '.csv')\n    print(f'Load: {path}')\n    return pd.read_csv(path, **kwrgs)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:50:17.869818Z","iopub.execute_input":"2022-05-10T05:50:17.870359Z","iopub.status.idle":"2022-05-10T05:50:17.885034Z","shell.execute_reply.started":"2022-05-10T05:50:17.870314Z","shell.execute_reply":"2022-05-10T05:50:17.884362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>application_{train|test}</b>\n\n* 基本となるファイルです。顧客ごとに1つずつローンの情報とtrainにはデフォルトしたかという情報(Target)が含まれています。\n* ER図で見ても分かる通り、`SK_ID_CURR` で `previous_appliation.csv` や `bureau.csv` と紐づけられます。\n* application_testデータはtestデータとして使用されます。\n\n>以下のテーブルデータをtrain_df, test_dfに正確に加えるためにはSK_ID_CURRごとにデータをまとめる必要があります\n\n<b>bureau_balance</b>\n\n* bureau.csvの毎月の残高データです。bureauと組み合わせて使います。\n\n<b>bureau</b>\n\n* 調査局のデータで、全ての顧客が過去に借りた他の金融機関からのローン情報となります。他の金融機関から Home Credit 社に情報提供されたものです。\n\n<b>credit_card_balance</b>\n\n* Home Credit 社のもつ申請者の月次クレジットカード残高のスナップショットになります。\n\n<b>installments_payments</b>\n\n* サンプルローンに関連する Home Credit 社にある過去実績になります。\n\n<b>POS_CASH_balance</b>\n\n* Home Credit 社のもつ申請者の月次クレジットカード残高を販売拠点ごとにまとめたものです。\n\n<b>previous_application</b>\n\n* 申請者の Home Credit 社でのローン履歴となります。\n\n<b>sample_submission</b>\n\n* サンプルcsvです。\n\n","metadata":{}},{"cell_type":"code","source":"# application_train.csv\napp_train = read_csv('application_train')\napp_test = read_csv('application_test')\nbureau_balance = read_csv('bureau_balance')\nbureau = read_csv('bureau')\ncredit_balance = read_csv('credit_card_balance')\n# カラムの説明csvは手元でutf-8形式に直してあげないと読み込めなさそうです。\n# カラムの日本語説明の記事：https://www.ritzcolor.net/?p=235\n# column_desc = read_csv('HomeCredit_columns_description')\ninstal_payment = read_csv('installments_payments')\npos_cash = read_csv('POS_CASH_balance')\nprev_app = read_csv('previous_application')\nsamp_sub = read_csv('sample_submission')","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:50:17.88635Z","iopub.execute_input":"2022-05-10T05:50:17.887027Z","iopub.status.idle":"2022-05-10T05:51:59.809672Z","shell.execute_reply.started":"2022-05-10T05:50:17.886983Z","shell.execute_reply":"2022-05-10T05:51:59.808447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 以下のカラムは頻出で、毎回入力するのはめんどくさいので、ポップアップされるように定義します\nSK_ID_CURR = 'SK_ID_CURR'\nSK_ID_PREV = 'SK_ID_PREV'\nSK_ID_BUREAU = 'SK_ID_BUREAU'","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:51:59.811894Z","iopub.execute_input":"2022-05-10T05:51:59.812198Z","iopub.status.idle":"2022-05-10T05:51:59.818657Z","shell.execute_reply.started":"2022-05-10T05:51:59.812159Z","shell.execute_reply":"2022-05-10T05:51:59.817603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"各データのサイズを見てみます","metadata":{}},{"cell_type":"markdown","source":"## データ確認","metadata":{}},{"cell_type":"markdown","source":"app_train, app_testを例に見てみると、\n* 各ユーザーで1行\n* app_testには `TARGET` が抜けているとわかる\n* 欠損値が散見される\n* カテゴリ関数がある\n* 日付が時間差表記されている\n* カラム多すぎるし、何なのかわからん (* ただありがたいことに `HomeCredit_columns_description.csv` で説明がされている)","metadata":{}},{"cell_type":"code","source":"app_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:51:59.820066Z","iopub.execute_input":"2022-05-10T05:51:59.820402Z","iopub.status.idle":"2022-05-10T05:51:59.908932Z","shell.execute_reply.started":"2022-05-10T05:51:59.820369Z","shell.execute_reply":"2022-05-10T05:51:59.908097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:51:59.910271Z","iopub.execute_input":"2022-05-10T05:51:59.910836Z","iopub.status.idle":"2022-05-10T05:51:59.981193Z","shell.execute_reply.started":"2022-05-10T05:51:59.910799Z","shell.execute_reply":"2022-05-10T05:51:59.980392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# データの統計量\ndisplay(app_train.describe())","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:51:59.985182Z","iopub.execute_input":"2022-05-10T05:51:59.985774Z","iopub.status.idle":"2022-05-10T05:52:02.012032Z","shell.execute_reply.started":"2022-05-10T05:51:59.985724Z","shell.execute_reply":"2022-05-10T05:52:02.011081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"bureauも見てみる\n","metadata":{}},{"cell_type":"code","source":"bureau.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:02.013552Z","iopub.execute_input":"2022-05-10T05:52:02.013822Z","iopub.status.idle":"2022-05-10T05:52:02.032555Z","shell.execute_reply.started":"2022-05-10T05:52:02.01379Z","shell.execute_reply":"2022-05-10T05:52:02.031536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bureau_balance.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:02.034169Z","iopub.execute_input":"2022-05-10T05:52:02.03468Z","iopub.status.idle":"2022-05-10T05:52:02.049485Z","shell.execute_reply.started":"2022-05-10T05:52:02.034641Z","shell.execute_reply":"2022-05-10T05:52:02.048903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"credit_balance.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:02.050737Z","iopub.execute_input":"2022-05-10T05:52:02.051392Z","iopub.status.idle":"2022-05-10T05:52:02.082127Z","shell.execute_reply.started":"2022-05-10T05:52:02.05135Z","shell.execute_reply":"2022-05-10T05:52:02.0808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prev_app.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:02.084445Z","iopub.execute_input":"2022-05-10T05:52:02.084779Z","iopub.status.idle":"2022-05-10T05:52:02.119411Z","shell.execute_reply.started":"2022-05-10T05:52:02.084738Z","shell.execute_reply":"2022-05-10T05:52:02.118487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"instal_payment.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:02.120822Z","iopub.execute_input":"2022-05-10T05:52:02.12431Z","iopub.status.idle":"2022-05-10T05:52:02.144456Z","shell.execute_reply.started":"2022-05-10T05:52:02.124235Z","shell.execute_reply":"2022-05-10T05:52:02.143419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_cash.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:02.145976Z","iopub.execute_input":"2022-05-10T05:52:02.146538Z","iopub.status.idle":"2022-05-10T05:52:02.165513Z","shell.execute_reply.started":"2022-05-10T05:52:02.146489Z","shell.execute_reply":"2022-05-10T05:52:02.164691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"欠損値状況を見てみると、かなり欠損値が多いカラムがあることが伺えます","metadata":{}},{"cell_type":"code","source":"# 欠損値の確認関数\ndef missing_values_summary(df):\n    mis_val = df.isnull().sum()\n    mis_val_percent = 100 * df.isnull().sum() / len(df)\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'mis_val_count', 1 : 'mis_val_percent'})\n    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('mis_val_percent', ascending=False).round(1)\n    print (\"カラム数：\" + str(df.shape[1]) + \"\\n\" + \"欠損値のカラム数： \" + str(mis_val_table_ren_columns.shape[0]))\n    return mis_val_table_ren_columns","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:02.166761Z","iopub.execute_input":"2022-05-10T05:52:02.16773Z","iopub.status.idle":"2022-05-10T05:52:02.178211Z","shell.execute_reply.started":"2022-05-10T05:52:02.167694Z","shell.execute_reply":"2022-05-10T05:52:02.177231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values_summary(app_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:02.179882Z","iopub.execute_input":"2022-05-10T05:52:02.180558Z","iopub.status.idle":"2022-05-10T05:52:03.283449Z","shell.execute_reply.started":"2022-05-10T05:52:02.180505Z","shell.execute_reply":"2022-05-10T05:52:03.282377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values_summary(bureau)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:03.286997Z","iopub.execute_input":"2022-05-10T05:52:03.287265Z","iopub.status.idle":"2022-05-10T05:52:04.474841Z","shell.execute_reply.started":"2022-05-10T05:52:03.287237Z","shell.execute_reply":"2022-05-10T05:52:04.473529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values_summary(prev_app)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:04.476372Z","iopub.execute_input":"2022-05-10T05:52:04.476614Z","iopub.status.idle":"2022-05-10T05:52:10.154517Z","shell.execute_reply.started":"2022-05-10T05:52:04.476587Z","shell.execute_reply":"2022-05-10T05:52:10.153517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## スキーマ（構造）の理解","metadata":{}},{"cell_type":"markdown","source":"各データのサイズを見てみます","metadata":{}},{"cell_type":"code","source":"# サイズ\nprint(f'Size of application_train: {app_train.shape}')\nprint('>Unique counts of current loan ID:', app_train[SK_ID_CURR].nunique())\nprint(f'Size of application_test: {app_test.shape}')\nprint('>Unique counts of current loan ID:', app_test[SK_ID_CURR].nunique())\nprint('--------')\nprint(f'Size of bureau_balance: {bureau_balance.shape}')\nprint('>Unique counts of bureau ID:', bureau_balance[SK_ID_BUREAU].nunique())\nprint(f'Size of bureau: {bureau.shape}')\nprint('>Unique counts of bureau ID:', bureau[SK_ID_BUREAU].nunique())\nprint('--------')\nprint(f'Size of previous_application: {prev_app.shape}')\nprint('>Unique counts of current loan ID:', prev_app[SK_ID_CURR].nunique())\nprint('>Unique counts of previous loan ID:', prev_app[SK_ID_PREV].nunique())\nprint('--------')\nprint(f'Size of credit_card_balance: {credit_balance.shape}')\nprint('>Unique counts of current loan ID:', prev_app[SK_ID_CURR].nunique())\nprint('>Unique counts of previous loan ID:', prev_app[SK_ID_PREV].nunique())\nprint('--------')\nprint(f'Size of installments_payments: {instal_payment.shape}')\nprint('>Unique counts of current loan ID:', instal_payment[SK_ID_CURR].nunique())\nprint('>Unique counts of previous loan ID:', instal_payment[SK_ID_PREV].nunique())\nprint('--------')\nprint(f'Size of POS_CASH_balance: {pos_cash.shape}')\nprint('>Unique counts of current loan ID:', pos_cash[SK_ID_CURR].nunique())\nprint('>Unique counts of previous loan ID:', pos_cash[SK_ID_PREV].nunique())\nprint('--------')\n# サブミットするサンプルcsv\nprint(f'Size of sample_submission: {samp_sub.shape}')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:10.155797Z","iopub.execute_input":"2022-05-10T05:52:10.156047Z","iopub.status.idle":"2022-05-10T05:52:12.162433Z","shell.execute_reply.started":"2022-05-10T05:52:10.156011Z","shell.execute_reply":"2022-05-10T05:52:12.161502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"データフレームの特徴を自動的に作ってくれる pandas-profiling を利用する方法もある\\\nしかしカラム数が多すぎると、ファイルサイズが大きくなり、表示失敗しやすくなるため注意(？)","metadata":{}},{"cell_type":"code","source":"# https://qiita.com/wakame1367/items/39faf5d91e20a5cf5772#_reference-a12e589891bdd7bf36e5\n# from pandas_profiling import ProfileReport\n\n# カラム数が多いせいか、htmlファイルのロードで失敗することがあるので、dataframe を半分にします\n# app_train_first_half = app_train.iloc[:, :60]\n# app_train_second_half = app_train.iloc[:, 60:]\n# app_train_second_half = pd.concat([app_train.iloc[:, :2], app_train_second_half], axis=1)\n\n# 分割したカラムそれぞれでレポートを作成します\n# report1 = ProfileReport(app_train_first_half)\n# report1.to_file(os.path.join(OUTPUT_DIR, \"report_of_application_train_first_half.html\"))\n# report2 = ProfileReport(app_train_second_half)\n# report2.to_file(os.path.join(OUTPUT_DIR, \"report_of_application_train_second_half.html\"))","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:12.163954Z","iopub.execute_input":"2022-05-10T05:52:12.164281Z","iopub.status.idle":"2022-05-10T05:52:12.169756Z","shell.execute_reply.started":"2022-05-10T05:52:12.164236Z","shell.execute_reply":"2022-05-10T05:52:12.168751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 学習データ(=入力データ & 正解データ)の作成\n\nある程度データを理解したところで学習データを作っていきましょう\n\n* 基本方針として、`app_train` でとりあえず使えそうなカラムと、その他のテーブルデータの基本統計量をマージして使用します\n* Light GBMといったGBDT系のモデルを使用します\n* 欠損値は一旦そのままにします\n* カテゴリデータはラベルエンコーディングします","metadata":{}},{"cell_type":"code","source":"credit_balance.groupby([SK_ID_CURR], as_index=False).mean()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:12.175548Z","iopub.execute_input":"2022-05-10T05:52:12.175941Z","iopub.status.idle":"2022-05-10T05:52:14.158095Z","shell.execute_reply.started":"2022-05-10T05:52:12.175895Z","shell.execute_reply":"2022-05-10T05:52:14.157047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 各ｄｆの基本統計量のうち、一旦平均値を取って集計する -> SK_ID_CURRでユニークな値が生まれた\ncd_b = credit_balance.groupby([SK_ID_CURR], as_index=False).mean()\nins_p = instal_payment.groupby([SK_ID_CURR], as_index=False).mean()\npos_c = pos_cash.groupby([SK_ID_CURR], as_index=False).mean()\nprv_a = prev_app.groupby([SK_ID_CURR], as_index=False).mean()\n\n# SK_ID_PREV は不要なため削除する\ncd_b = cd_b.drop(SK_ID_PREV, axis=1)\nins_p = ins_p.drop(SK_ID_PREV, axis=1)\npos_c = pos_c.drop(SK_ID_PREV, axis=1)\nprv_a = prv_a.drop(SK_ID_PREV, axis=1)\n\n# カラム名が重複しないように名称を変更する\ncd_b2 = cd_b.rename(columns={'MONTHS_BALANCE':'MONTHS_BALANCE_CREDIT', 'SK_DPD' : 'SK_DPD_CRE_CREDIT', 'SK_DPD_DEF' : 'SK_DPD_DEF_CREDIT'})\nprv_a2 = prv_a.rename(columns={'AMT_CREDIT':'AMT_CREDIT_PRE_APP', 'AMT_ANNUITY' : 'AMT_ANNUITY_PRE_APP', 'AMT_GOODS_PRICE' : 'AMT_GOODS_PRICE_PRE_APP'})\n","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:14.159982Z","iopub.execute_input":"2022-05-10T05:52:14.160326Z","iopub.status.idle":"2022-05-10T05:52:21.896731Z","shell.execute_reply.started":"2022-05-10T05:52:14.160262Z","shell.execute_reply":"2022-05-10T05:52:21.895942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"前処理（カテゴリ変数の変換）\n\nカテゴリデータは基本的にそのまま特徴量として扱えないので、数値化する必要があります。\\\nカテゴリ変数の変換にはいくつもの手法がありますが、よくやるものでもデータや使うモデルによって向き不向きがあるので気をつけましょう。\n\n* One-Hot Encoding -> gbdt系以外（線形モデル etc..）におすすめ\n* Label Encoding-> gbdt系 にもおすすめ\\\n[【sklearn】LabelEncoderの使い方を丁寧に](https://gotutiyan.hatenablog.com/entry/2020/09/08/122621)\n* Target Encoding -> gbdt系にはより効果的らしい\\\n[Target Encoding はなぜ有効なのか](https://speakerdeck.com/hakubishin3/target-encoding-hanazeyou-xiao-nafalseka)","metadata":{}},{"cell_type":"code","source":"# LabelEncoder()は，文字列や数値で表されたラベルを，0~(ラベル種類数-1)までの数値に変換してくれるものです\nfrom sklearn.preprocessing import LabelEncoder\n\ntmp_app_train = app_train.copy()\ntmp_app_test = app_test.copy()\n# 今回は簡単に使えそうなカラムをラベルエンコーディングします\nfor c in ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE']:\n    # LabelEncoderを宣言します\n    le = LabelEncoder()\n    # ラベルとラベルIDの対応づけを行います。positiveは0にしよう，みたいなことを決めます\n    le.fit(tmp_app_train[c].fillna('NA'))\n\n    # 学習データとテストデータそれぞれのdf内のカラムを変換します\n    tmp_app_train[c] = le.transform(tmp_app_train[c].fillna('NA'))\n    tmp_app_test[c] = le.transform(tmp_app_test[c].fillna('NA'))","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:21.898179Z","iopub.execute_input":"2022-05-10T05:52:21.898436Z","iopub.status.idle":"2022-05-10T05:52:24.684148Z","shell.execute_reply.started":"2022-05-10T05:52:21.898407Z","shell.execute_reply":"2022-05-10T05:52:24.683375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp_app_train = tmp_app_train.drop(['FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE'], axis=1)\ntmp_app_test = tmp_app_test.drop(['FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:24.68535Z","iopub.execute_input":"2022-05-10T05:52:24.685589Z","iopub.status.idle":"2022-05-10T05:52:24.899676Z","shell.execute_reply.started":"2022-05-10T05:52:24.685561Z","shell.execute_reply":"2022-05-10T05:52:24.898688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:24.900852Z","iopub.execute_input":"2022-05-10T05:52:24.901083Z","iopub.status.idle":"2022-05-10T05:52:24.969971Z","shell.execute_reply.started":"2022-05-10T05:52:24.901057Z","shell.execute_reply":"2022-05-10T05:52:24.969078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# app_trainと比べて、`Target` カラムがなくなっていることがわかると思います\ntmp_app_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:24.971109Z","iopub.execute_input":"2022-05-10T05:52:24.971366Z","iopub.status.idle":"2022-05-10T05:52:25.029949Z","shell.execute_reply.started":"2022-05-10T05:52:24.971336Z","shell.execute_reply":"2022-05-10T05:52:25.02918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"テーブルデータをマージします","metadata":{}},{"cell_type":"code","source":"#　マージ\n## `instalments_payments` に `credit_card_balance` テーブルをマージします\ntmp1 = pd.merge(ins_p, cd_b2, on=SK_ID_CURR, how='left')\n## `tmp1` に `pos_c` テーブルをマージします\ntmp2 = pd.merge(tmp1, pos_c, on=SK_ID_CURR, how='left')\n## `tmp2` に `previous_application` テーブルをマージします\ntmp3 = pd.merge(tmp2, prv_a2, on=SK_ID_CURR, how='left')\n\n## train と test にもマージします\n### 今回は app_{train|test} のカラムの削除を敢えてせずに全て使用します。\ntrain = pd.merge(tmp_app_train, tmp3, on=SK_ID_CURR, how='left')\ntest = pd.merge(tmp_app_test, tmp3, on=SK_ID_CURR, how='left')","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:25.031321Z","iopub.execute_input":"2022-05-10T05:52:25.032168Z","iopub.status.idle":"2022-05-10T05:52:27.039511Z","shell.execute_reply.started":"2022-05-10T05:52:25.032121Z","shell.execute_reply":"2022-05-10T05:52:27.03847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"予測対象となる `TARGET`カラムを抜き出し、 `y` とします\n\n同時にtrainから `TARGET`カラムを削除します","metadata":{}},{"cell_type":"code","source":"y_train = train['TARGET']\n\nX_train = train.drop('TARGET', axis=1)\n\nX_test = test.copy()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:27.0407Z","iopub.execute_input":"2022-05-10T05:52:27.041022Z","iopub.status.idle":"2022-05-10T05:52:27.567704Z","shell.execute_reply.started":"2022-05-10T05:52:27.040983Z","shell.execute_reply":"2022-05-10T05:52:27.566716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# numpy 配列に直します\nX, y = X_train.values, y_train.values\nX_test2 = X_test.values","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:27.56904Z","iopub.execute_input":"2022-05-10T05:52:27.569336Z","iopub.status.idle":"2022-05-10T05:52:27.760389Z","shell.execute_reply.started":"2022-05-10T05:52:27.569282Z","shell.execute_reply":"2022-05-10T05:52:27.759397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_trainとy_trainの行数が、X_train, X_testのカラム数がそれぞれ同数なのでOK\nprint(X.shape, y.shape, X_test2.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:27.761626Z","iopub.execute_input":"2022-05-10T05:52:27.761903Z","iopub.status.idle":"2022-05-10T05:52:27.768669Z","shell.execute_reply.started":"2022-05-10T05:52:27.761872Z","shell.execute_reply":"2022-05-10T05:52:27.767511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 簡単に学習・予測・サブミットまでしてみる","metadata":{}},{"cell_type":"markdown","source":"## 学習","metadata":{}},{"cell_type":"markdown","source":"特徴量を作成できたので次にモデルの学習を行っていきます。この時大事になるのが交差検証 (Cross Validation) という考え方です。\n\n* Cross Validation とは\n\nCross Validation とは学習用のデータセットを複数に分割してそれぞれの分割で学習・検証のデータセットを作り、モデルの性能を見積もる枠組みのことです。\n\n* なんで Cross Validation するの?\n\nなぜわざわざ分割するの? (そのまま全部学習で使っちゃえばいいじゃない?) と思われるのが普通だと思います。なぜ分割するかというと学習データの中で今の枠組みの性能(枠組みと言っているのは特徴量・モデルの構成もろもろ全部が含まれるためです)を評価したいからです。手元で評価ができないとLBに出してみて一喜一憂するしかなくなり、結果publicLBにオーバーフィットしてしまうので良くないです。\n\n>仕事的な観点で言ってもLBに出すというのはデプロイ(本番へ反映すること)だから、本番に出さないとモデルの良し悪しがわからないのはよろしくないのと一緒\n\n一番ナイーブな戦略は Random と呼ばれるものです。これは何も考えずにとにかくランダムに学習データを分割します。\n\nその他にターゲットの分布が同じになるように分割する Stratified と呼ばれる方法もあります。\n\nあとは「各分割で特定のグループが重ならないようにする」Group もよく使われます。\n\nその他にも時系列で区切る TimeSeriesSplit という方法もあります。\n\n### どの分割方法がいいの?\nどの分割方法が一番良いのかは一概に言えません。\n\nまず良い分割とはなにかを考えてみると、良い分割とは今のモデルがテストデータでどのぐらいの性能を出すかを、検証データで確認できる分割だと考えられます。\n\nつまり、分割された学習/検証用データが、全体の学習/テストデータとの対応関係と一致していることといえます。\nしたがって本来、分割を決める際には学習データとテストデータの関係性をしらべ、それと同じ分割を採用する必要があります。","metadata":{}},{"cell_type":"code","source":"# 今回は王道の Stratified K Fold (層化抽出)を利用します\n## stratified K Foldはテストデータに含まれる各クラスの割合は、学習データに含まれる各クラスの割合とほぼ同じであろうという仮説に基づき、バリデーションの評価を安定させようとする手法です。\n## 多クラス分類のような極端に頻度の少ないクラスがある場合は、層化抽出を行うのが重要です。ただ今回のような二値分類で偏りが大きくない場合はあまり効用を感じないかもしれないです。\nfrom sklearn.model_selection import StratifiedKFold\n\nfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=510)\ncv = fold.split(X, y)\n# split の返り値は generator だから、list 化して何度も iterate できるようにしておく\ncv = list(cv)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:27.770122Z","iopub.execute_input":"2022-05-10T05:52:27.770394Z","iopub.status.idle":"2022-05-10T05:52:27.910673Z","shell.execute_reply.started":"2022-05-10T05:52:27.770363Z","shell.execute_reply":"2022-05-10T05:52:27.909972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## モデル構築","metadata":{}},{"cell_type":"markdown","source":"今回はlightgbmを使用します。\nGBDT(Gradient Boosting Decision Tree: 勾配ブースティング木)と呼ばれる決定技をベースとしたアルゴリズムの一種でテーブルデータで性能が高いことが知られています。\n\nlightgbmの特徴として\n* 数値の大きさ自体に意味がなく、大小関係のみが影響する\n* 欠損値が存在している場合にも自然に取り扱えるため特に処理が必要ない\n* 決定技の分岐の繰り返しによって、変数間の相互作用を反映する\n* 特徴重要度(`feature importance`)をさっと確認できる\n* CPU 環境でも高速に学習・推論が行える\n\nほかにも理由はありますが u++ さんの [「初手LightGBM」をする7つの理由](https://upura.hatenablog.com/entry/2019/10/29/184617) などが参考になります","metadata":{}},{"cell_type":"markdown","source":"### LightGBM による CrossValidation を用いた学習\n\nこちらも参考に\\\n[LightGBMで交差検証を実装してみるよ](https://potesara-tips.com/lightgbm-k-fold-cross-validation/)","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nimport lightgbm as lgbm\n\ndef fit_lgbm(X, y, \n                cv, \n                params: dict=None, \n                verbose: int=50,\n                seed=random_seed):\n\n    \"\"\"lightGBM を CrossValidation の枠組みで学習を行うための関数を定義します\"\"\"\n\n    # パラメータがない時は、空の dict で置き換える\n    if params is None:\n        params = {}\n    \n    models = []\n    n_records = len(X)\n    # training data の target と同じだけのゼロ配列を用意\n    oof_pred = np.zeros((n_records,), dtype=np.float32)\n\n    for i, (idx_train, idx_valid) in enumerate(cv):\n        # この部分が交差検証のところ。データセットを `cv instance` によって分割します\n        # training data を train/valid に分割\n        x_train, y_train = X[idx_train], y[idx_train]\n        x_valid, y_valid = X[idx_valid], y[idx_valid]\n\n        clf = lgbm.LGBMClassifier(**params)\n\n        with Timer(prefix='fit fold={}'.format(i)):\n            clf.fit(x_train,\n                    y_train,\n                    eval_set = [(x_valid, y_valid)],\n                    early_stopping_rounds=100,\n                    verbose=verbose)\n        \n        pred_i = clf.predict_proba(x_valid)[:, 1]\n        oof_pred[idx_valid] = pred_i\n        models.append(clf)\n\n        # 今回の指標の `roc_auc_score` で計算する\n        score = roc_auc_score(y_valid, pred_i)\n        print(f'{score:.4f}')\n\n    score = roc_auc_score(y, oof_pred)\n    print(f'{score:.4f}')\n    \n    return oof_pred, models","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:27.911974Z","iopub.execute_input":"2022-05-10T05:52:27.912927Z","iopub.status.idle":"2022-05-10T05:52:29.15616Z","shell.execute_reply.started":"2022-05-10T05:52:27.912881Z","shell.execute_reply":"2022-05-10T05:52:29.155168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### parameter について\nLightGBM などの GBDT のパラメータは、そこまでセンシティブではないです。しかし、内部的にどういう意味を持つのかを知っておくと、問題ごとにどういうパラメータが良いかの感覚がわかったり、チューニングする際にも有効なパラメータに絞ってチューニングできるので、重要な変数に関してはその意味についてざっと目を通しておくことと良いと言われています。\n\n参考文献。\n\n[LightGBM 徹底入門 – LightGBMの使い方や仕組み、XGBoostとの違いについて](https://www.codexa.net/lightgbm-beginner/)\\\n[Parameters Tuning](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html): lightGBM 公式のパラメータチューニングガイド。英語。\\\n[勾配ブースティングで大事なパラメータの気持ち](https://bit.ly/3L2xmcN): gotoさんが書いた記事。日本語。","metadata":{}},{"cell_type":"code","source":"lgbm_params = {\n    # 目的関数、これの意味で最小となるようなパラメータを探します\n    'objective': 'binary',\n\n    # 学習率, 小さいほど滑らかな決定境界が作られて性能向上につながる場合が多いです\n    # 一方でそれだけ木をつくるため、学習に時間がかります\n    'learning_rate': .1,\n\n    # L2 Reguralization\n    'reg_lambda': .1,\n\n    # L1\n    'reg_alpha': 0,\n\n    # 木の深さ、深い木を許容するほどより複雑な交互作用を考慮することになります\n    'max_depth': 5,\n\n    # 木の最大数, early_stopping という枠組みで木の数は制御されるようにしているので、とても大きい値を指定しておきます\n    'n_estimators': 10000,\n\n    # 木を作る際に考慮する特徴量の割合. 1以下を指定すると特徴をランダムに欠落させます \n    # 小さくすることで満遍なく特徴を使うという効果があるそうです\n    'colsample_samples': 10,\n\n    # 最小分割でのデータ数. 小さいとより細かい粒度の分割方法を許容します\n    'min_child_samples': 10,\n\n    # bagging の頻度と割合\n    'subsample_freq': 3,\n    'subsample': .9,\n\n    # 特徴重要度計算のロジック\n    'importance_type': 'gain',\n    'random_state': 71,\n\n}","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:29.157742Z","iopub.execute_input":"2022-05-10T05:52:29.158114Z","iopub.status.idle":"2022-05-10T05:52:29.165475Z","shell.execute_reply.started":"2022-05-10T05:52:29.158071Z","shell.execute_reply":"2022-05-10T05:52:29.164364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## モデル評価","metadata":{}},{"cell_type":"markdown","source":"モデルを実行し、結果を見てみます。\\\nCross Validationしたスコアの平均は最後に表示されており、訓練データで `0.7727` でした（乱数指定しているので毎回同じスコアが出ると思います）。\\\nlightgbmを使うと、それなりのスコアを出せることがわかります。\\\nサブミットしていないのでかなり安直ですが、メダル圏内まであと `0.02` ほど必要なので、まだまだ改善の余地があります。","metadata":{}},{"cell_type":"code","source":"oof, models =fit_lgbm(X=X, y=y, cv=cv, params=lgbm_params)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:52:29.166847Z","iopub.execute_input":"2022-05-10T05:52:29.167232Z","iopub.status.idle":"2022-05-10T05:54:29.873149Z","shell.execute_reply.started":"2022-05-10T05:52:29.167127Z","shell.execute_reply":"2022-05-10T05:54:29.872142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 特徴重要度（feature importance） の確認","metadata":{}},{"cell_type":"code","source":"def visualize_importance(models, X_train):\n    \"\"\"lightGBM の model 配列の feature_importance を plot する関数です\n    CVごとのブレを boxen plot として表現します\n\n    args:\n        models:\n            List of lightGBM models\n        X_train:\n            学習時に使った DataFrame\n    \"\"\"\n\n    feature_importance_df = pd.DataFrame()\n    for i, model in enumerate(models):\n        _df = pd.DataFrame()\n        _df['feature_importance'] = model.feature_importances_\n        _df['column'] = X_train.columns\n        _df['fold'] = i + 1\n        feature_importance_df = pd.concat(\n            [feature_importance_df, _df],\n            axis=0,\n            ignore_index=True\n        )\n    \n    order = feature_importance_df.groupby('column')\\\n        .sum()[['feature_importance']]\\\n        .sort_values('feature_importance', ascending=False).index[:50]\n\n    \n    fig, ax = plt.subplots(figsize=(8, max(6, len(order) * .25)))\n    sns.boxenplot(data=feature_importance_df,\n                  x='feature_importance',\n                  y='column',\n                  order=order,\n                  ax=ax,\n                  palette='viridis',\n                  orient='h'\n                  )\n\n    ax.tick_params(axis='x', rotation=90)\n    ax.set_title('Importance')\n    ax.grid()\n    fig.tight_layout()\n    return fig, ax","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:54:29.875035Z","iopub.execute_input":"2022-05-10T05:54:29.875756Z","iopub.status.idle":"2022-05-10T05:54:29.88885Z","shell.execute_reply.started":"2022-05-10T05:54:29.875703Z","shell.execute_reply":"2022-05-10T05:54:29.88775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"以下のコードで重要度が可視化できます。\\\nfeature_importance は値が大きいほど有効な分割であることを意味します。\\\nしかし、想定とは異なる部分が重要となっています（EXT_SOURCE_#ってなんや！？）\\\nてきとーにカラムを選択すると特徴重要度を見たときに有用な示唆を得にくくなる例ですね汗\\\nカラムを取捨選択したり、掛け合わせて作成したり、自分なりの仮説を持って特徴量生成をした後に再度可視化をするとまた面白い示唆を得られるかもしれないです。","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nfig, ax = visualize_importance(models, X_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:54:29.890585Z","iopub.execute_input":"2022-05-10T05:54:29.890941Z","iopub.status.idle":"2022-05-10T05:54:31.940311Z","shell.execute_reply.started":"2022-05-10T05:54:29.890893Z","shell.execute_reply":"2022-05-10T05:54:31.939385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:54:31.941497Z","iopub.execute_input":"2022-05-10T05:54:31.941854Z","iopub.status.idle":"2022-05-10T05:54:31.955246Z","shell.execute_reply.started":"2022-05-10T05:54:31.941823Z","shell.execute_reply":"2022-05-10T05:54:31.954326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 訓練データとテストデータでの予測結果の傾向差を見る\nテストデータではどのような予測結果が出されるのか可視化してみます。\\\n今回のデータは訓練データとテストデータでユーザー特性が大きく異なることはあまりないと考えられます。\\\nなので訓練データとテストデータをもとにした予測結果もある程度は近くなると予測されます。","metadata":{}},{"cell_type":"code","source":"# K 個のモデルの予測確率（predict_proba） を作成します。 shape = (k, N_test, n_classes) になるはずです。\npred_prob = np.array([model.predict_proba(X_test2) for model in models])\nprint(f\"1. shape: {pred_prob.shape}\")\n\n# k 個のモデルの平均を計算\npred_prob = np.mean(pred_prob, axis=0) # axis=0 なので shape の `k` が潰れます\nprint(f\"2. shape: {pred_prob.shape}\")\n\n\n# 欲しいのは y=1 の確率なので全要素の 1 次元目を取ってきます\npred_prob = pred_prob[:, 1]\nprint(f'3. shape: {pred_prob.shape}')\n\n# ついでにsample_submissionのshapeとも比較しましょう\nprint('4. shape:', samp_sub.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:54:31.956573Z","iopub.execute_input":"2022-05-10T05:54:31.956918Z","iopub.status.idle":"2022-05-10T05:54:33.817417Z","shell.execute_reply.started":"2022-05-10T05:54:31.956874Z","shell.execute_reply":"2022-05-10T05:54:33.816732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_prob","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:54:33.8189Z","iopub.execute_input":"2022-05-10T05:54:33.819427Z","iopub.status.idle":"2022-05-10T05:54:33.825954Z","shell.execute_reply.started":"2022-05-10T05:54:33.819388Z","shell.execute_reply":"2022-05-10T05:54:33.825217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"どういったラベルが予測されているか、などの傾向を知っておきましょう。\\\nまた学習時とテスト時で出力の乖離が無いか、を見ることも大事です。乖離が大きい場合には、入力する値自体が大きく異なっているなどで性能悪化が起こっている可能性があるのでサブミットする前に注意です。","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nfig, ax = plt.subplots(figsize=(10, 6))\n\nsns.distplot(pred_prob, ax=ax, label=\"Test\")\nsns.distplot(oof, ax=ax, label=\"Out Of Fold (Train)\")\n\nax.legend()\nax.grid()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:54:33.827885Z","iopub.execute_input":"2022-05-10T05:54:33.828225Z","iopub.status.idle":"2022-05-10T05:54:35.791656Z","shell.execute_reply.started":"2022-05-10T05:54:33.828183Z","shell.execute_reply":"2022-05-10T05:54:35.790703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### NOTE: テストでの乖離が大きい とは\nテストの予測値の乖離が大きい場合の原因はいくつか考えられますが「テスト時に使えない情報を特徴量としてつかっていないか?」を最も警戒した方が良いでう。\n\nテスト時に使えない特徴 A を利用してモデルを作っていると、学習時に A をみるようなモデルが出来る可能性があり、テスト時にそれを参照できないことで予測が上手く行かない場合があります。\n\nもっとも極端なのは A が予測ラベルそのものである場合です。学習時は予測ラベルを参照できるため、それこそ精度100%で予測できるようなモデルができますが、テスト時には当然予測ラベルはわからないため、精度は大きく悪化します。このように予測ラベルの情報が学習時の特徴量に染み出してしまった結果学習が上手く行かないことをリークとよびます（今回は予測対象が `TARGET` と分かりやすいので大丈夫ですね）。\n\nテストでの乖離が起こるのはリークの場合だけではないですが、あまりに大きく異なる場合にはリークを含め、特徴量の選定に問題がないかを検討しましょう。","metadata":{}},{"cell_type":"markdown","source":"### サブミットファイルの作成","metadata":{}},{"cell_type":"code","source":"samp_sub2 = samp_sub.copy()\nsamp_sub2.TARGET = pred_prob\nsamp_sub2.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:54:35.793267Z","iopub.execute_input":"2022-05-10T05:54:35.793551Z","iopub.status.idle":"2022-05-10T05:54:35.804552Z","shell.execute_reply.started":"2022-05-10T05:54:35.793518Z","shell.execute_reply":"2022-05-10T05:54:35.803798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samp_sub2.to_csv(os.path.join(OUTPUT_DIR, 'submission.csv'), index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:54:35.806118Z","iopub.execute_input":"2022-05-10T05:54:35.806411Z","iopub.status.idle":"2022-05-10T05:54:35.992474Z","shell.execute_reply.started":"2022-05-10T05:54:35.80638Z","shell.execute_reply":"2022-05-10T05:54:35.991521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('./'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-05-10T05:54:35.993697Z","iopub.execute_input":"2022-05-10T05:54:35.993938Z","iopub.status.idle":"2022-05-10T05:54:36.000172Z","shell.execute_reply.started":"2022-05-10T05:54:35.993908Z","shell.execute_reply":"2022-05-10T05:54:35.999189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 機械学習モデルを改善してみる","metadata":{}},{"cell_type":"markdown","source":"ここからスコアを上げるためにやることが大きく分けて3つあります。\n\n雑な共有メモですが、、\n\n* 特にGBDTでは差や比率を直接表現することが苦手なので明示的に作成したほうが精度が向上することがあります．\n* 一方で大量に特徴量を作成した後に，LightGBMなどで学習させて，そのfeature importanceの上位の変数だけ用いるといったことをされる方もいます\n* そもそも決定木を使用する際は，特徴量に対する対数化や，0から1の範囲に正規化するような大小関係が保存される変換の影響はほとんどありません．数値の大小関係で学習するモデルであるため，基本的にスケーリングを行う必要がありません．\n* 一方で，線形回帰などはスケールの大きい変数ほど回帰係数が小さくなり，正則化がかかりにくいといった問題が生じてしまうので，NN含めスケーリングを行ったほうがいいことが多いです．(後述しますが二値変数や疎ベクトルに対してはスケーリングをしないほうが良い場合もあります．)\n\nhttps://zenn.dev/colum2131/articles/fffac4654e7c7c\n\nhttps://www.nogawanogawa.com/entry/mlflow_lgbm\n\nhttps://kakeami.github.io/viz-madb/index.html\n\nhttps://github.com/FavioVazquez/ds-cheatsheets/blob/master/Python/Others/mementopython3-english.pdf","metadata":{}},{"cell_type":"markdown","source":"## 前処理","metadata":{}},{"cell_type":"markdown","source":"## EDA（探索的データ分析）& 特徴量生成","metadata":{}},{"cell_type":"markdown","source":"## モデル選択 & パラメーターチューニング","metadata":{}}]}