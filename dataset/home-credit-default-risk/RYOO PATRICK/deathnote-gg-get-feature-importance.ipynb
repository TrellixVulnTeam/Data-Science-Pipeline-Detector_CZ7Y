{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deathnote.gg Feature 가중치 구하기\n\nIn this notebook we will apply feature engineering to the manual engineered features built in two previous kernels. We will reduce the number of features using several methods and then we will test the performance of the features using a fairly basic gradient boosting machine model. \n\nWe will use three methods for feature selection:\n\n1. Remove collinear features\n2. Remove features with greater than a threshold percentage of missing values\n3. Keep only the most relevant features using feature importances from a model","metadata":{"_uuid":"748e245881e9a26d54da4c9431786b3f874734de"}},{"cell_type":"markdown","source":"Standard imports for data science work. The LightGBM library is used for the gradient boosting machine.","metadata":{"_uuid":"97b0d94920dc9e25a5f1dbfd56702124ebc07769"}},{"cell_type":"code","source":"# pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# featuretools for automated feature engineering\n# import featuretools as ft\n\n# matplotlit and seaborn for visualizations\nimport matplotlib.pyplot as plt\nplt.rcParams['font.size'] = 22\nimport seaborn as sns\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# modeling \nimport lightgbm as lgb\n\n# utilities\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\n# memory management\nimport gc","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-07-24T08:13:49.94452Z","iopub.execute_input":"2021-07-24T08:13:49.944922Z","iopub.status.idle":"2021-07-24T08:13:49.967031Z","shell.execute_reply.started":"2021-07-24T08:13:49.94485Z","shell.execute_reply":"2021-07-24T08:13:49.966087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Call preprocessed dataset saved in data.csv \n80% Training set, 20% Test set","metadata":{"_uuid":"06695931219858e57330b8574af5d9028c9e50c2"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndf = pd.read_csv('../input/dataset/data.csv')\ntrain, test = train_test_split(df, test_size=0.2)\n\ntrain_columns = list(train.columns)\ntest_columns = list(test.columns)","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2021-07-24T08:13:49.968044Z","iopub.execute_input":"2021-07-24T08:13:49.96832Z","iopub.status.idle":"2021-07-24T08:13:50.033914Z","shell.execute_reply.started":"2021-07-24T08:13:49.968261Z","shell.execute_reply":"2021-07-24T08:13:50.033268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('There are %d train features.' % len(train_columns))\nprint('Size of train %d' % len(train))\n\nprint('There are %d test features.' % len(test_columns))\nprint('Size of test %d' % len(test))","metadata":{"_uuid":"af78939941b510855d5a972ca24e33037c003977","execution":{"iopub.status.busy":"2021-07-24T08:13:50.03516Z","iopub.execute_input":"2021-07-24T08:13:50.035474Z","iopub.status.idle":"2021-07-24T08:13:50.042993Z","shell.execute_reply.started":"2021-07-24T08:13:50.035414Z","shell.execute_reply":"2021-07-24T08:13:50.042237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That gives us the number of features in each dataframe.","metadata":{"_uuid":"951caa5d7f4a0033ff4148039d73ba153034cb61"}},{"cell_type":"code","source":"print('Training shape: ', train.shape)\nprint('Testing shape: ', test.shape)","metadata":{"_uuid":"b0da702050879ddfb8aecc4c23186e7ee97923ba","execution":{"iopub.status.busy":"2021-07-24T08:13:50.044261Z","iopub.execute_input":"2021-07-24T08:13:50.044668Z","iopub.status.idle":"2021-07-24T08:13:50.058193Z","shell.execute_reply.started":"2021-07-24T08:13:50.044601Z","shell.execute_reply":"2021-07-24T08:13:50.057353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (train.info()) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html\n#data_raw.head() #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html\n#data_raw.tail() #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.html\ntrain.sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:13:50.059464Z","iopub.execute_input":"2021-07-24T08:13:50.060051Z","iopub.status.idle":"2021-07-24T08:13:50.103612Z","shell.execute_reply.started":"2021-07-24T08:13:50.059996Z","shell.execute_reply":"2021-07-24T08:13:50.102801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lane is an object type!\nWe convert the values in lane column to be of type int","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\nlbl = preprocessing.LabelEncoder()\ntrain['lane'] = lbl.fit_transform(train['lane'].astype(str))\ntest['lane'] = lbl.fit_transform(test['lane'].astype(str))\nprint(test['lane'])","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:13:50.104798Z","iopub.execute_input":"2021-07-24T08:13:50.105146Z","iopub.status.idle":"2021-07-24T08:13:50.228176Z","shell.execute_reply.started":"2021-07-24T08:13:50.105073Z","shell.execute_reply":"2021-07-24T08:13:50.227377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Remove Collinear Variables\n\nCollinear variables are those which are highly correlated with one another. These can decrease the model's availablility to learn, decrease model interpretability, and decrease generalization performance on the test set. Clearly, these are three things we want to increase, so removing collinear variables is a useful step. We will establish an admittedly arbitrary threshold for removing collinear variables, and then remove one out of any pair of variables that is above that threshold. \n\nThe code below identifies the highly correlated variables based on the absolute magnitude of the Pearson correlation coefficient being greater than 0.9. Again, this is not entirely accurate since we are dealing with such a limited section of the data. This code is for illustration purposes, but if we read in the entire dataset, it would work (if the kernels allowed it)! \n\nThis code is adapted from [work by Chris Albon](https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/).","metadata":{"_uuid":"c402926c8d265ad46e8f9bc5b1683563352cb496"}},{"cell_type":"markdown","source":"### Identify Correlated Variables","metadata":{"_uuid":"5100a4ad6c6586b2637139db8ef4e67df5ad49a8"}},{"cell_type":"code","source":"# Threshold for removing correlated variables\nthreshold = 0.8\n\n# Absolute value correlation matrix\ncorr_matrix = train.corr().abs()\ncorr_matrix.head()","metadata":{"_uuid":"59259cf2be69f0c5e699109f9177287599836354","execution":{"iopub.status.busy":"2021-07-24T08:13:50.229538Z","iopub.execute_input":"2021-07-24T08:13:50.22987Z","iopub.status.idle":"2021-07-24T08:13:50.289316Z","shell.execute_reply.started":"2021-07-24T08:13:50.229792Z","shell.execute_reply":"2021-07-24T08:13:50.288523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Upper triangle of correlations\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.head()","metadata":{"_uuid":"7c33392948540b626cf213fa6dea316766627eb9","execution":{"iopub.status.busy":"2021-07-24T08:13:50.290588Z","iopub.execute_input":"2021-07-24T08:13:50.290906Z","iopub.status.idle":"2021-07-24T08:13:50.321666Z","shell.execute_reply.started":"2021-07-24T08:13:50.29084Z","shell.execute_reply":"2021-07-24T08:13:50.320815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint('There are %d columns to remove.' % (len(to_drop)))\nprint(to_drop)","metadata":{"_uuid":"1517d554f80a8eed2c0e03303fe7d1fb4a469131","execution":{"iopub.status.busy":"2021-07-24T08:13:50.323361Z","iopub.execute_input":"2021-07-24T08:13:50.323713Z","iopub.status.idle":"2021-07-24T08:13:50.337638Z","shell.execute_reply.started":"2021-07-24T08:13:50.323634Z","shell.execute_reply":"2021-07-24T08:13:50.336892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Drop Correlated Variables","metadata":{"_uuid":"fc6bf1f1a4c98749727522bb5a3ae89adf145f8d"}},{"cell_type":"code","source":"train = train.drop(columns = to_drop)\ntest = test.drop(columns = to_drop)\n\nprint('Training shape: ', train.shape)\nprint('Testing shape: ', test.shape)","metadata":{"_uuid":"2c18834968dac305317a5a881308a0c9a3cce353","execution":{"iopub.status.busy":"2021-07-24T08:13:50.339006Z","iopub.execute_input":"2021-07-24T08:13:50.339425Z","iopub.status.idle":"2021-07-24T08:13:50.352546Z","shell.execute_reply.started":"2021-07-24T08:13:50.339269Z","shell.execute_reply":"2021-07-24T08:13:50.351719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Remove Missing Values\n\nA relatively simple choice of feature selection is removing missing values. Well, it seems simple, at least until we have to decide what percentage of missing values is the minimum threshold for removing a column. Like many choices in machine learning, there is no right answer, and not even a general rule of thumb for making this choice. In this implementation, if any columns have greater than 75% missing values, they will be removed. \n\nMost models (including those in Sk-Learn) cannot handle missing values, so we will have to fill these in before machine learning. The Gradient Boosting Machine ([at least in LightGBM](https://github.com/Microsoft/LightGBM/blob/master/docs/Advanced-Topics.rst)) can handle missing values. Imputing missing values always makes me a little uncomfortable because we are adding information that actually isn't in the dataset. Since we are going to be evaluating several models (in a later notebook), we will have to use some form of imputation. For now, we will focus on removing columns above the threshold.","metadata":{"_uuid":"e9c652ca3eb3a8770195469abf77ad71d15b3fe0"}},{"cell_type":"markdown","source":"Let's drop the columns, one-hot encode the dataframes, and then align the columns of the dataframes.","metadata":{"_uuid":"fc8e7bdb14e7e65a257c93d5c45a882098971360"}},{"cell_type":"markdown","source":"# Feature Selection through Feature Importances\n\nThe next method we can employ for feature selection is to use the feature importances of a model. Tree-based models (and consequently ensembles of trees) can determine an \"importance\" for each feature by measuring the reduction in impurity for including the feature in the model. I'm not really sure what that means (any explanations would be welcome) and the absolute value of the importance can be difficult to interpret. However, the relative value of the importances can be used as an approximation of the \"relevance\" of different features in a model. Moreover, we can use the feature importances to remove features that the model does not consider important. \n\nOne method for doing this automatically is the [Recursive Feature Elimination method](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html) in Scikit-Learn. This accepts an estimator (one that either returns feature weights such as a linear regression, or feature importances such as a random forest) and a desired number of features. In then fits the model repeatedly on the data and iteratively removes the lowest importance features until the desired number of features is left. This means we have another arbitrary hyperparameter to use in out pipeline: the number of features to keep! \n\nInstead of doing this automatically, we can perform our own feature removal by first removing all zero importance features from the model. If this leaves too many features, then we can consider removing the features with the lowest importance. We will use a Gradient Boosted Model from the LightGBM library to assess feature importances. If you're used to the Scikit-Learn library, the LightGBM library has an API that makes deploying the model very similar to using a Scikit-Learn model. ","metadata":{"_uuid":"d4d38ab010cddd88e714894e09d8733cc60daafd"}},{"cell_type":"markdown","source":"Since the LightGBM model does not need missing values to be imputed, we can directly `fit` on the training data. We will use Early Stopping to determine the optimal number of iterations and run the model twice, averaging the feature importances to try and avoid overfitting to a certain set of features.","metadata":{"_uuid":"6525f690933a50ab1eb7f360629b98d9a2f6df37"}},{"cell_type":"code","source":"train_labels = train[\"isWin\"]\ntest_labels = test[\"isWin\"]\ntrain = train.drop(columns = [\"isWin\"])\ntest = test.drop(columns = [\"isWin\"])\n\nprint(train.shape)\nprint(test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:13:50.353844Z","iopub.execute_input":"2021-07-24T08:13:50.354203Z","iopub.status.idle":"2021-07-24T08:13:50.366051Z","shell.execute_reply.started":"2021-07-24T08:13:50.354061Z","shell.execute_reply":"2021-07-24T08:13:50.365285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBMClassifier\n\nn_estimators : 반복 수행하는 트리의 개수를 의미한다.\n\n","metadata":{}},{"cell_type":"code","source":"# Initialize an empty array to hold feature importances\nfeature_importances = np.zeros(train.shape[1])\n\n# Create the model with several hyperparameters\nmodel = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, class_weight = 'balanced')","metadata":{"_uuid":"4741055961c566f3a2e9d60893950cade5a57330","execution":{"iopub.status.busy":"2021-07-24T08:13:50.367557Z","iopub.execute_input":"2021-07-24T08:13:50.36794Z","iopub.status.idle":"2021-07-24T08:13:50.374286Z","shell.execute_reply.started":"2021-07-24T08:13:50.367855Z","shell.execute_reply":"2021-07-24T08:13:50.373566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model twice to avoid overfitting\nfor i in range(2):\n    \n    # Split into training and validation set\n    train_features, valid_features, train_y, valid_y = train_test_split(train, train_labels, test_size = 0.25, random_state = i)\n    \n    # Train using early stopping\n    model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], \n              eval_metric = 'auc', verbose = 200)\n    \n    # Record the feature importances\n    feature_importances += model.feature_importances_","metadata":{"_uuid":"4535a84ea5b2c55f7b443832c7a5b2b77894e511","execution":{"iopub.status.busy":"2021-07-24T08:13:50.375729Z","iopub.execute_input":"2021-07-24T08:13:50.376241Z","iopub.status.idle":"2021-07-24T08:13:56.298356Z","shell.execute_reply.started":"2021-07-24T08:13:50.376188Z","shell.execute_reply":"2021-07-24T08:13:56.297595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make sure to average feature importances! \nfeature_importances = feature_importances / 2\nfeature_importances = pd.DataFrame({'feature': list(train.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\n\nfeature_importances.head()","metadata":{"_uuid":"68a0ce264d8182bffa27fca668d81040fdc3b9ba","execution":{"iopub.status.busy":"2021-07-24T08:13:56.29964Z","iopub.execute_input":"2021-07-24T08:13:56.30003Z","iopub.status.idle":"2021-07-24T08:13:56.313532Z","shell.execute_reply.started":"2021-07-24T08:13:56.299898Z","shell.execute_reply":"2021-07-24T08:13:56.312687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the features with zero importance\nzero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\nprint('There are %d features with 0.0 importance' % len(zero_features))\nfeature_importances.tail()","metadata":{"_uuid":"54f05dafeccd5f3800967fa304cbcf3b3076a5c2","execution":{"iopub.status.busy":"2021-07-24T08:13:56.314761Z","iopub.execute_input":"2021-07-24T08:13:56.315275Z","iopub.status.idle":"2021-07-24T08:13:56.332027Z","shell.execute_reply.started":"2021-07-24T08:13:56.315083Z","shell.execute_reply":"2021-07-24T08:13:56.331256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that one of our features made it into the top 5 most important! That's a good sign for all of our hard work making the features. It also looks like many of the features we made have literally 0 importance. For the gradient boosting machine, features with 0 importance are not used at all to make any splits. Therefore, we can remove these features from the model with no effect on performance (except for faster training). ","metadata":{"_uuid":"68f738649fcc774a3eeceb2c602d00d9f4ff841d"}},{"cell_type":"code","source":"def plot_feature_importances(df, threshold = 0.9):\n    \"\"\"\n    Plots 15 most important features and the cumulative importance of features.\n    Prints the number of features needed to reach threshold cumulative importance.\n    \n    Parameters\n    --------\n    df : dataframe\n        Dataframe of feature importances. Columns must be feature and importance\n    threshold : float, default = 0.9\n        Threshold for prining information about cumulative importances\n        \n    Return\n    --------\n    df : dataframe\n        Dataframe ordered by feature importances with a normalized column (sums to 1)\n        and a cumulative importance column\n    \n    \"\"\"\n    \n    plt.rcParams['font.size'] = 18\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    # Cumulative importance plot\n    plt.figure(figsize = (8, 6))\n    plt.plot(list(range(len(df))), df['cumulative_importance'], 'r-')\n    plt.xlabel('Number of Features'); plt.ylabel('Cumulative Importance'); \n    plt.title('Cumulative Feature Importance');\n    plt.show();\n    \n    importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n    print('%d features required for %0.2f of cumulative importance' % (importance_index + 1, threshold))\n    \n    return df","metadata":{"_uuid":"144732dd9d88d13b06983590b9d34970ff6c9a69","execution":{"iopub.status.busy":"2021-07-24T08:13:56.3332Z","iopub.execute_input":"2021-07-24T08:13:56.333507Z","iopub.status.idle":"2021-07-24T08:13:56.400737Z","shell.execute_reply.started":"2021-07-24T08:13:56.333419Z","shell.execute_reply":"2021-07-24T08:13:56.400133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"norm_feature_importances = plot_feature_importances(feature_importances)","metadata":{"_uuid":"ea0dcbff339b6ffdfd654d165cdb96268311ba95","execution":{"iopub.status.busy":"2021-07-24T08:13:56.401991Z","iopub.execute_input":"2021-07-24T08:13:56.402318Z","iopub.status.idle":"2021-07-24T08:13:56.715618Z","shell.execute_reply.started":"2021-07-24T08:13:56.402259Z","shell.execute_reply":"2021-07-24T08:13:56.714721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's remove the features that have zero importance.","metadata":{"_uuid":"e8df33430fe5bd31d83e5d2ca5050c0864b34488"}},{"cell_type":"code","source":"train = train.drop(columns = zero_features)\ntest = test.drop(columns = zero_features)\n\nprint('Training shape: ', train.shape)\nprint('Testing shape: ', test.shape)","metadata":{"_uuid":"e3849b51615d52eee53e3c919c17c6066ae29397","execution":{"iopub.status.busy":"2021-07-24T08:13:56.717102Z","iopub.execute_input":"2021-07-24T08:13:56.717434Z","iopub.status.idle":"2021-07-24T08:13:56.729574Z","shell.execute_reply.started":"2021-07-24T08:13:56.717371Z","shell.execute_reply":"2021-07-24T08:13:56.728639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this point, we can re-run the model to see if it identifies any more features with zero importance. In a way, we are implementing our own form of recursive feature elimination. Since we are repeating work, we should probably put the zero feature importance identification code in a function.","metadata":{"_uuid":"c67926fdcb81d3f438510f06cadbba1edbc70879"}},{"cell_type":"code","source":"def identify_zero_importance_features(train, train_labels, iterations = 2):\n    \"\"\"\n    Identify zero importance features in a training dataset based on the \n    feature importances from a gradient boosting model. \n    \n    Parameters\n    --------\n    train : dataframe\n        Training features\n        \n    train_labels : np.array\n        Labels for training data\n        \n    iterations : integer, default = 2\n        Number of cross validation splits to use for determining feature importances\n    \"\"\"\n    \n    # Initialize an empty array to hold feature importances\n    feature_importances = np.zeros(train.shape[1])\n\n    # Create the model with several hyperparameters\n    model = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, class_weight = 'balanced')\n    \n    # Fit the model multiple times to avoid overfitting\n    for i in range(iterations):\n\n        # Split into training and validation set\n        train_features, valid_features, train_y, valid_y = train_test_split(train, train_labels, test_size = 0.25, random_state = i)\n\n        # Train using early stopping\n        model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], \n                  eval_metric = 'auc', verbose = 200)\n\n        # Record the feature importances\n        feature_importances += model.feature_importances_ / iterations\n    \n    feature_importances = pd.DataFrame({'feature': list(train.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\n    \n    # Find the features with zero importance\n    zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n    print('\\nThere are %d features with 0.0 importance' % len(zero_features))\n    \n    return zero_features, feature_importances","metadata":{"_uuid":"d09ec2ad863688243bc95c51aa43a559f56ad4fb","execution":{"iopub.status.busy":"2021-07-24T08:13:56.731163Z","iopub.execute_input":"2021-07-24T08:13:56.731686Z","iopub.status.idle":"2021-07-24T08:13:56.773923Z","shell.execute_reply.started":"2021-07-24T08:13:56.731613Z","shell.execute_reply":"2021-07-24T08:13:56.772982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"second_round_zero_features, feature_importances = identify_zero_importance_features(train, train_labels)","metadata":{"_uuid":"f69eede0e8e04f1f4c850e5c102dcdebcdb107e7","execution":{"iopub.status.busy":"2021-07-24T08:13:56.775213Z","iopub.execute_input":"2021-07-24T08:13:56.775504Z","iopub.status.idle":"2021-07-24T08:14:02.789607Z","shell.execute_reply.started":"2021-07-24T08:13:56.775446Z","shell.execute_reply":"2021-07-24T08:14:02.788956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are now no 0 importance features left (I guess we should have expected this). If we want to remove more features, we will have to start with features that have a non-zero importance. One way we could do this is by retaining enough features to account for a threshold percentage of importance, such as 95%. At this point, let's keep enough features to account for 95% of the importance. Again, this is an arbitrary decision! ","metadata":{"_uuid":"c877ed509374d9494c508a8b920e92c926453f84"}},{"cell_type":"markdown","source":"## Threshold of 0.95","metadata":{}},{"cell_type":"code","source":"norm_feature_importances = plot_feature_importances(feature_importances, threshold = 0.95)","metadata":{"_uuid":"bb6dd17a7417f3b67017e7e8e18c09725df6e434","execution":{"iopub.status.busy":"2021-07-24T08:14:02.790878Z","iopub.execute_input":"2021-07-24T08:14:02.791188Z","iopub.status.idle":"2021-07-24T08:14:03.095591Z","shell.execute_reply.started":"2021-07-24T08:14:02.791118Z","shell.execute_reply":"2021-07-24T08:14:03.094839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(feature_importances)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:14:03.096849Z","iopub.execute_input":"2021-07-24T08:14:03.097224Z","iopub.status.idle":"2021-07-24T08:14:03.106777Z","shell.execute_reply.started":"2021-07-24T08:14:03.097091Z","shell.execute_reply":"2021-07-24T08:14:03.105995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Threshold of 0.3","metadata":{}},{"cell_type":"code","source":"norm_feature_importances = plot_feature_importances(feature_importances, threshold = 0.3)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:14:03.108227Z","iopub.execute_input":"2021-07-24T08:14:03.108626Z","iopub.status.idle":"2021-07-24T08:14:03.413484Z","shell.execute_reply.started":"2021-07-24T08:14:03.10855Z","shell.execute_reply":"2021-07-24T08:14:03.412613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\n31개의 Feature들에 대해서 중요도를 알아낼 수 있었다.\n\n이를 Deathtnote 알고리즘에 반영하였다.\n\n더 작업해야 하는 내용들은\n1. 챔피언ID 별로 가중치 알아내기 ( 의미가 있으려면 주기별로 데이터를 가져와서 결과를 도출해야 한다. )\n2. 티어별로 따로 조사해보기 ( 현재는 탑랭커 유저들의 데이터만 가져왔다. )\n\n등이 있다.","metadata":{}}]}