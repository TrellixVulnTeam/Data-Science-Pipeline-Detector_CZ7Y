{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import PolynomialFeatures\nimport os\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n'''Read Data'''\napp_train=pd.read_csv('../input/application_train.csv')\napp_test=pd.read_csv('../input/application_test.csv')\n\n'''EDA'''\n#1.Target Distribution\n#app_train['TARGET'].value_counts()\n    #app_train['TARGET'].astype(int).plot.hist():数据分布的直方图\n#plt.hist(app_train['TARGET'].astype(int))\n\n#2.examine misssing values\n    #实现的目的是确定有多少colunm是有缺失值的；每一列的缺失数量以及缺失率。\ndef missing_values_table(df):\n    mis_val=df.isnull().sum()\n    mis_val_percent=100*df.isnull().sum()/len(df)\n    mis_val_table=pd.concat([mis_val,mis_val_percent],axis=1)\n    #列的重命名\n    mis_val_table_ren_columns=mis_val_table.rename(columns={0:'Missing Values',1:'% of Total Values'})\n    mis_val_table_ren_columns=mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1]!=0].sort_values('% of Total Values',ascending=False).round(1)\n\n    print('our selected dataframe has '+str(df.shape[1])+' columns.\\n'\n          'There are '+str(mis_val_table_ren_columns.shape[0])+' columns that have missing values.')\n\n    return mis_val_table_ren_columns\n\n#3.columns type\napp_train.dtypes.value_counts()\napp_train.select_dtypes('object').apply(pd.Series.nunique,axis=0)\n\n#4.encoding categorical variables\n    #对object中是类别变量的encoding\nle=LabelEncoder()\n    #函数对2类的类别变量进行label编码即0或者1\ndef object_cat_encoding(app_train,app_test,lecount):\n    for col in app_train:\n        if app_train[col].dtype=='object':\n            if len(list(app_train[col].unique()))<=2:\n                le.fit(app_train[col])\n            \n                app_train[col]=le.transform(app_train[col])\n                app_test[col]=le.transform(app_test[col])\n            \n                lecount+=1\n    \n    print('%d columns were label encoded.' % lecount)\n    return app_train,app_test\n    #这两个操作是对上面的编码为0或者1变量进行ont-hot-encoding \napp_train,app_test=object_cat_encoding(app_train,app_test,0)\napp_train=pd.get_dummies(app_train)\napp_test=pd.get_dummies(app_test)  \n\n#5.Aligning Training and Testing Data\ndef align_train_test_data(app_train,app_test):\n    train_labels=app_train['TARGET']\n    app_train,app_test=app_train.align(app_test,join='inner',axis=1)\n    app_train['TARGET']=train_labels\n    print('Training Features shape:',app_train.shape)\n    print('Training Features shape:',app_test.shape)\n    return app_train,app_test\napp_train,app_test=align_train_test_data(app_train,app_test)\n\n#6.Anomalies\ndef anom_dectect(app_train,app_test):\n    #未出现异常点\n    (app_train['DAYS_BIRTH']/-365).describe()\n    #出现异常点\n    app_train['DAYS_EMPLOYED'].describe()\n    app_train['DAYS_EMPLOYED'].plot.hist(title='Days Emploment Histogram')\n    plt.xlabel('Days Emploment')\n    plt.ylabel('Frequency')\n    \n    anom=app_train[app_train['DAYS_EMPLOYED']==365243]\n    non_anom=app_train[app_train['DAYS_EMPLOYED']!=365243]\n    print('The anom default on %0.2f%% of loans' % (100*anom['TARGET'].mean()))\n    print('the non_anom default on %0.2f%% of loans' % (100*non_anom['TARGET'].mean()))\n    print('There are %d anom days of emploment' % len(anom))\n#anomaly_dectect(app_train,app_test)\n\ndef delet_anom_hist_train(app_train):\n    app_train['DAYS_EMPLOMED_ANOM']=app_train['DAYS_EMPLOYED']==365243\n    app_train['DAYS_EMPLOYED'].replace({365243:np.nan},inplace=True)\n    \n    '''print('There are %d anom in the train data out of %d entries' % (app_train['DAYS_EMPLOMED_ANOM'].sum(),len(app_train)))\n    app_train['DAYS_EMPLOYED'].plot.hist(title='Days Empoment Hist')\n    plt.xlabel('Days Emploment')'''\n    return app_train\napp_train=delet_anom_hist_train(app_train)\nprint('app_train的行列：',app_train.shape)\n\ndef delet_anom_hist_test(app_test):\n    app_test['DAYS_EMPLOMED_ANOM']=app_test['DAYS_EMPLOYED']==365243\n    app_test['DAYS_EMPLOYED'].replace({365243:np.nan},inplace=True)\n    '''print('There are %d anom in the test data out of %d entries' % (app_test['DAYS_EMPLOMED_ANOM'].sum(),len(app_test)))\n    app_test['DAYS_EMPLOYED'].plot.hist(title='Days Empoment Hist')\n    plt.xlabel('Days Emploment')'''\n    return app_test\napp_test=delet_anom_hist_test(app_test)\nprint('app_test的行列：',app_test.shape)\n\n#7.correlatons\ndef correlation(app_train):\n    correlations=app_train.corr()['TARGET'].sort_values()\n    print('Most Positive Correlations:\\n',correlations.tail(15))\n    print('Most Negative Correlations:\\n',correlations.head(15))\n#correlation(app_train)\n \n#7.1正相关的变量DAYS_BIRTH的EDA \ndef daybirth_target_corr(app_train):\n    app_train['DAYS_BIRTH']=abs(app_train['DAYS_BIRTH'])\n    a=app_train['DAYS_BIRTH'].corr(app_train['TARGET'])\n    print('The corr about abs of DAYS_BIRTH and TARGET is \\n', a)\n    \n    #hist直方图分析年龄的分布\n    '''plt.style.use('fivethirtyeight')\n    plt.hist(abs(app_train['DAYS_BIRTH']/365),edgecolor='k',bins=25)\n    plt.title('Age of client')\n    plt.xlabel('Age(years)')\n    plt.ylabel('count')'''\n    \n    #senborn kdeplot\n    sns.kdeplot(app_train.loc[app_train['TARGET']==0,'DAYS_BIRTH']/365,label='target==0')\n    sns.kdeplot(app_train.loc[app_train['TARGET']==1,'DAYS_BIRTH']/365,label='target==1')\n    plt.xlabel('Age(years)')\n    plt.ylabel('Density')\n    plt.title('Distribution of Ages')\n    \n    #in another way to look at the relationship(DAYS_BIRTH,TARGET)\n    age_data=app_train[['TARGET','DAYS_BIRTH']]\n    age_data['YEARS_BIRTH']=age_data['DAYS_BIRTH']/365\n    age_data['YEARS_BINNED']=pd.cut(age_data['YEARS_BIRTH'],bins=np.linspace(20,70,num=11))\n    age_data.head(10)\n    age_groups=age_data.groupby('YEARS_BINNED').mean()\n    #print(age_groups)\n    \n    #fail to repay by age group\n    plt.figure(figsize=(8,8))\n    plt.bar(age_groups.index.astype(str),100*age_groups['TARGET'])\n    plt.xlabel('Age Group(years)')\n    plt.ylabel('Failure to Repay(%)')\n    #通过年龄的分组和target的关系得出年龄越小还款失败率越高\n#daybirth_target_corr(app_train)\n#7.2负相关变量EXterior Sources(1/2/3)的EDA\ndef extsource_target_corr(app_train):\n    ext_data=app_train[['TARGET','EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']]\n    ext_data_corrs=ext_data.corr()\n    \n    #heatpmap(热图)反应各个变量之间的corr\n        #热图反应EXT_SOURCE与target负相关且EXT_SOURCE_1与DAYS_BIRTHU有很大相关\n    plt.figure(figsize=(8,6))\n    sns.heatmap(ext_data_corrs,cmap=plt.cm.RdYlBu_r,vmin= -0.25,annot=True,vmax=0.6)\n    plt.title('Correlation Heatmap')\n    \n    #senborn kdeplot:EXT_SOURCE_1/2/3 and TARGET\n        #分别画出三个变量与target的kdeplot\n    plt.figure(figsize=(10,12))\n    for i,source in enumerate(['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']):\n        plt.subplot(3,1,i+1)\n        sns.kdeplot(app_train.loc[app_train['TARGET']==0,source],label='target==0')\n        sns.kdeplot(app_train.loc[app_train['TARGET']==1,source],label='target==1')\n        \n        print('Distribution of %s by Target Value' % source)\n        plt.xlabel('%s' % source)\n        plt.ylabel('DEnsity')\n    plt.tight_layout(h_pad=2.5)\n    \n    age_data=app_train[['TARGET','DAYS_BIRTH']]\n    age_data['YEARS_BIRTH']=abs(age_data['DAYS_BIRTH'])/365\n    age_data['YEARS_BINNED']=pd.cut(age_data['YEARS_BIRTH'],bins=np.linspace(20,70,num=11))\n    plot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n    # Add in the age of the client in years\n    plot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n    # Drop na values and limit to first 100000 rows\n    plot_data = plot_data.dropna().loc[:100000, :]\n    # Function to calculate correlation coefficient between two columns\n    def corr_func(x, y, **kwargs):\n        r = np.corrcoef(x, y)[0][1]\n        ax = plt.gca()\n        ax.annotate(\"r = {:.2f}\".format(r),\n                    xy=(.2, .8), xycoords=ax.transAxes,\n                    size = 20)\n    # Create the pairgrid object\n    grid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n                        hue = 'TARGET', \n                        vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n    # Upper is a scatter plot\n    grid.map_upper(plt.scatter, alpha = 0.2)\n    # Diagonal is a histogram\n    grid.map_diag(sns.kdeplot)\n    # Bottom is density plot\n    grid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r)\n    plt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05)\n#extsource_target_corr(app_train)\n    \n    \n'''Feature Engineering:(feature construction)and(feature selection)'''\n#1.feature construction:Polynomial features and Domain knowledge features\n#1.1 Polynomial features\ndef feature_construction_polynominal(app_train,app_test):\n    #app_train['DAYS_BIRTH']必须是绝对值。\n    app_train['DAYS_BIRTH']=abs(app_train['DAYS_BIRTH'])\n    poly_features=app_train[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH','TARGET']]\n    poly_features_test=app_test[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']]\n    imputer=Imputer(strategy='median')\n    poly_target=poly_features['TARGET']\n    poly_features=poly_features.drop(columns=['TARGET'])\n    \n    #impute missing value\n    poly_features=imputer.fit_transform(poly_features)\n    poly_features_test=imputer.transform(poly_features_test)\n    \n    #create the polynomial object with specified degree\n    poly_transform=PolynomialFeatures(degree=3)\n    \n    #train the polynomial features\n    poly_transform.fit(poly_features)\n    \n    poly_features=poly_transform.transform(poly_features)\n    poly_features_test=poly_transform.transform(poly_features_test)\n    print('Polynomial Features train shape: ',poly_features.shape,app_train.shape)\n    print('Polynomial Features test shape: ',poly_features_test.shape,app_test.shape)\n    #此时返回的poly_features和poly_features_test都是数组，转换成DataFRame进行操作\n    \n    #create a dataframe fo the poly_features\n    poly_features=pd.DataFrame(poly_features,columns=poly_transform.get_feature_names(\n            ['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']))\n    poly_features['TARGET']=poly_target\n    poly_corrs=poly_features.corr()['TARGET'].sort_values()\n    \n    #create a dataframe of the poly_features_test\n    poly_features_test=pd.DataFrame(poly_features_test,columns=poly_transform.get_feature_names(\n            ['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']))\n    \n    #merge polynomial features into training dataframe\n    poly_features['SK_ID_CURR']=app_train['SK_ID_CURR']\n    app_train_poly=app_train.merge(poly_features,on='SK_ID_CURR',how='left')\n    #merge polynomial features into testing dataframe\n    poly_features_test['SK_ID_CURR']=app_test['SK_ID_CURR']\n    app_test_poly=app_test.merge(poly_features_test,on='SK_ID_CURR',how='left')\n    \n    app_train_poly,app_test_poly=app_train_poly.align(app_test_poly,join='inner',axis=1)\n    print('Training data with polynomial features shape:',app_train_poly.shape)\n    print('Testing data with polynomial features shape:',app_test_poly.shape) \n    return app_train_poly,app_test_poly\n#app_train,app_test=feature_construction_polynominal(app_train,app_test)#此时是275个特征\n#2.Domain knowledge features\ndef domain_knowledge_features(app_train,app_test):\n    app_train_domain=app_train.copy()\n    app_test_domain=app_test.copy()\n    \n    app_train_domain['CREDIT_INCOME_PERCENT']=app_train_domain['AMT_CREDIT']/app_train_domain['AMT_INCOME_TOTAL']\n    app_train_domain['ANNUITY_INCOME_PERCENT']=app_train_domain['AMT_ANNUITY']/app_train_domain['AMT_INCOME_TOTAL']\n    app_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\n    app_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']\n    \n    app_test_domain['CREDIT_INCOME_PERCENT']=app_test_domain['AMT_CREDIT']/app_test_domain['AMT_INCOME_TOTAL']\n    app_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\n    app_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\n    app_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']\n    \n    plt.figure(figsize = (12, 20))\n    # iterate through the new features\n    for i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n        \n        # create a new subplot for each source\n        plt.subplot(4, 1, i + 1)\n        # plot repaid loans\n        sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature], label = 'target == 0')\n        # plot loans that were not repaid\n        sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature], label = 'target == 1')\n        \n        # Label the plots\n        plt.title('Distribution of %s by Target Value' % feature)\n        plt.xlabel('%s' % feature); plt.ylabel('Density');   \n    plt.tight_layout(h_pad = 2.5)\n#domain_knowledge_features(app_train,app_test)\n\n'''baseline'''\n#1.logistic regression implementation\nfrom sklearn.preprocessing import MinMaxScaler,Imputer\nfrom sklearn.linear_model import LogisticRegression\ndef baseline_logistic(app_train,app_test):\n    train_label=app_train['TARGET']\n    if 'TARGET' in app_train:\n        train=app_train.drop(columns=['TARGET'])\n    else:\n        train=app_train.copy()\n    test=app_test.copy()\n   \n    imputer=Imputer(strategy='median')\n    scaler=MinMaxScaler(feature_range=(0,1))\n    \n    imputer.fit(train)\n    train=imputer.transform(train)\n    test=imputer.transform(app_test)\n    \n    scaler.fit(train)\n    train=scaler.transform(train)\n    test=scaler.transform(test)\n    print('Training data shape:',train.shape)\n    print('Testing data shape:',test.shape)\n    \n    log_reg=LogisticRegression(C=0.0001, class_weight=None, dual=False,\n          fit_intercept=True, intercept_scaling=1, max_iter=100,\n          multi_class='ovr', n_jobs=None, penalty='l2', random_state=None,\n          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n    log_reg.fit(train,train_label)\n    log_reg_pred=log_reg.predict_proba(test)[:,1]\n    submit=app_test[['SK_ID_CURR']]\n    submit['TARGET']=log_reg_pred\n    submit.to_csv('log_reg_output.csv',index=False)\n\nbaseline_logistic(app_train,app_test)\n    \n    \n    \n    \n    \n    \n    \n\n\n        \n\n\n    \n        \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}