{"cells":[{"metadata":{},"cell_type":"markdown","source":"1 Понимание проблемы и ознакомление с данными\n2 Чистка данных и форматирование\n3 EDA\n4 Базовая модель\n5 Улучшение модели\n6 Интерпретация модели\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Просмотрим наличие файлов в каталоге\nimport os\nPATH=\"../input/home-credit-default-risk/\"\nprint(os.listdir(PATH))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Подругажаем файлы и выводим размер обучающей и тестовой выборки\napp_train = pd.read_csv(PATH + 'application_train.csv')\napp_test = pd.read_csv(PATH + 'application_test.csv')\nprint ('формат обучающей выборки:', app_train.shape)\nprint ('формат тестовой выборки:', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', None) # иначе pandas не покажет все столбцы\napp_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train.info(max_cols=122)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"EDA исследовательский анализ данных"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Распределение целевой переменной\napp_train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\nplt.rcParams[\"figure.figsize\"] = [8,5]\nplt.hist(app_train.TARGET)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Проверим недостающие данные\n# Функция для подсчета недостающих столбцов\ndef missing_values_table(df):\n    \n        # Всего недостает\n        mis_val = df.isnull().sum()\n        \n        # Процент недостающих данных\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Таблица с результатами\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Переименование столбцов\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Сортировка про процентажу\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Инфо\n        print (\"В выбранном датафрейме \" + str(df.shape[1]) + \" столбцов.\\n\"      \n            \"Всего \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" столбцов с неполными данными.\")\n        \n        # Возврат таблицы с данными\n        return mis_val_table_ren_columns\n    \nmissing_values = missing_values_table(app_train)\nmissing_values.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# В графическом формате\nplt.style.use('seaborn-talk')\nfig = plt.figure(figsize=(18,6))\nmiss_train = pd.DataFrame((app_train.isnull().sum())*100/app_train.shape[0]).reset_index()\nmiss_test = pd.DataFrame((app_test.isnull().sum())*100/app_test.shape[0]).reset_index()\nmiss_train[\"type\"] = \"тренировочная\"\nmiss_test[\"type\"]  =  \"тестовая\"\nmissing = pd.concat([miss_train,miss_test],axis=0)\nax = sns.pointplot(\"index\",0,data=missing,hue=\"type\")\nplt.xticks(rotation =90,fontsize =7)\nplt.title(\"Доля отсуствующих значений в данных\")\nplt.ylabel(\"Доля в %\")\nplt.xlabel(\"Столбцы\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Типы столбцов и кодирование категориальных данных"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train.select_dtypes(include=[object]).apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# кодировать категориальнцые признаки можно с помощью one hot encoding либо label encoding\n# можно совершить кодирование с помощью pandas \napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"так как количество столбцов теперь не совпадает, нужно удалить столбцы в трейне"},{"metadata":{"trusted":true},"cell_type":"code","source":"#сохраним лейблы, их же нет в тестовой выборке и при выравнивании они потеряются. \ntrain_labels = app_train['TARGET']\n# Выравнивание - сохранятся только столбцы. имеющиеся в обоих датафреймах\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\nprint('Формат тренировочной выборки: ', app_train.shape)\nprint('Формат тестовой выборки: ', app_test.shape)\n# Add target back in to the data\napp_train['TARGET'] = train_labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*  Корреляция в данных\n* \n* Хороший метод понять данные — рассчитать коэффициенты корреляции Пирсона для данных относительно целевого признака. Это не лучший метод показать релевантность признаков, но он прост и позволяет составить представление о данных. Интерпретировать коэффициенты можно следующим образом:\n* \n* 00-.19 “очень слабая”\n* 20-.39 “слабая”\n* 40-.59 “средняя”\n* 60-.79 “сильная”\n* 80-1.0 “очень сильная”\n* "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Корреляция и сортировка\ncorrelations = app_train.corr()['TARGET'].sort_values()\n# Отображение\nprint('Наивысшая позитивная корреляция: \\n', correlations.tail(15))\nprint('\\nНаивысшая негативная корреляция: \\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Возраст\n\nПонятно, что чем старше клиент, тем выше вероятность возврата (до определенного предела, конечно). Но возраст почему-то указан в отрицательных днях до выдачи кредита, поэтому он положительно коррелирует с невозвратом (что выглядит несколько странно). Приведем его к положительному значению и посмотрим на корреляцию."},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Гистограмма распределения возраста в годах, всего 25 столбцов\nplt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Сама по себе гистограмма распределения может сказать немного полезного, кроме того что мы не видим особых выбросов и все выглядит более-менее правдоподобно. Чтобы показать эффект влияния возраста на результат, можно построить график kernel density estimation (KDE) — распределение ядерной плотности, раскрашенный в цвета целевого признака. Он показывает распределение одной переменной и может быть истолкован как сглаженная гистограмма (рассчитывается как Гауссианское ядро по каждой точке, которое затем усредняется для сглаживания)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# KDE займов, выплаченных вовремя\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n# KDE проблемных займов\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n# Обозначения\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Как видно, доля невозвратов выше для молодых людей и снижается с ростом возраста. Это не повод отказывать молодым людям в кредите всегда, такая «рекомендация» приведет лишь к потере доходов и рынка для банка. Это повод задуматься о более тщательном отслеживании таких кредитов, оценке и, возможно, даже каком-то финансовом образовании для молодых заемщиков."},{"metadata":{},"cell_type":"markdown","source":"Внешние источники\n\nПосмотрим внимательнее на «внешние источники данных» EXT_SOURCE и их корреляцию."},{"metadata":{"trusted":true},"cell_type":"code","source":"ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 12))\n# итерация по источникам\nfor i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    \n    # сабплот\n    plt.subplot(3, 1, i + 1)\n    # отрисовка качественных займов\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n    # отрисовка дефолтных займов\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n    \n    # метки\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Парный график\n\nДля лучшего понимания взаимоотношений этих переменных можно построить парный график, в нем мы сможем увидеть взаимоотношения каждой пары и гистограмму распределения по диагонали. Выше диагонали можно показать диаграмму рассеяния, а ниже — 2d KDE."},{"metadata":{"trusted":true},"cell_type":"code","source":"#вынесем данные по возрасту в отдельный датафрейм\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n# копирование данных для графика\nplot_data = ext_data.drop(labels = ['DAYS_BIRTH'], axis=1).copy()\n# Добавим возраст\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n# Уберем все незаполненнные строки и ограничим таблицу в 100 тыс. строк\nplot_data = plot_data.dropna().loc[:100000, :]\n# Функиця для расчет корреляции\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)\n# Создание объекта pairgrid object\ngrid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n                    hue = 'TARGET', \n                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n# Сверху - скаттерплот\ngrid.map_upper(plt.scatter, alpha = 0.2)\n# Диагональ - гистограмма\ngrid.map_diag(sns.kdeplot)\n# Внизу - распределение плотности\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\nplt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Исследование прочих признаков\n\nРассмотрим более подробно другие признаки и их зависимость от целевой переменной. Так как среди них много категориальных (а мы уже успели их закодировать), нам снова понадобятся исходные данные. Назовем их немного по-другому во избежание путаницы"},{"metadata":{"trusted":true},"cell_type":"code","source":"application_train = pd.read_csv(PATH + \"application_train.csv\")\napplication_test = pd.read_csv(PATH + \"application_test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Также нам понадобится пара функций для красивого отображения распределений и их влияния на целевую переменную"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_stats(feature,label_rotation=False,horizontal_layout=True):\n    temp = application_train[feature].value_counts()\n    df1 = pd.DataFrame({feature: temp.index,'Количество займов': temp.values})\n    # Расчет доли target=1 в категории\n    cat_perc = application_train[[feature, 'TARGET']].groupby([feature],as_index=False).mean()\n    cat_perc.sort_values(by='TARGET', ascending=False, inplace=True)\n    \n    if(horizontal_layout):\n        fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\n    else:\n        fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(12,14))\n    sns.set_color_codes(\"pastel\")\n    s = sns.barplot(ax=ax1, x = feature, y=\"Количество займов\",data=df1)\n    if(label_rotation):\n        s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    \n    s = sns.barplot(ax=ax2, x = feature, y='TARGET', order=cat_perc[feature], data=cat_perc)\n    if(label_rotation):\n        s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    plt.ylabel('Доля проблемных', fontsize=10)\n    plt.tick_params(axis='both', which='major', labelsize=10)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Итак, рассмотрим основные признаки колиентов\n\nТип займа"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stats('NAME_CONTRACT_TYPE')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Пол клиента\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stats('CODE_GENDER')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Владение машиной и недвижимостью\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stats('FLAG_OWN_CAR')\nplot_stats('FLAG_OWN_REALTY')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Семейный статус\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stats('NAME_FAMILY_STATUS',True, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Количество детей"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stats('CNT_CHILDREN')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"application_train.CNT_CHILDREN.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Количество членов семьи"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stats('CNT_FAM_MEMBERS',True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Тип дохода"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stats('NAME_INCOME_TYPE',False,False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Вид деятельности"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stats('OCCUPATION_TYPE',True, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"application_train.OCCUPATION_TYPE.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Образование\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stats('NAME_EDUCATION_TYPE',True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Тип организации — работодателя"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stats('ORGANIZATION_TYPE',True, False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Распределение суммы кредитования\n\nРассмотрим распределение сумм кредитов и влияние их на возвратность"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.title(\"Распределение AMT_CREDIT\")\nax = sns.distplot(app_train[\"AMT_CREDIT\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\n# KDE займов, выплаченных вовремя\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'AMT_CREDIT'], label = 'target == 0')\n# KDE проблемных займов\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'AMT_CREDIT'], label = 'target == 1')\n# Обозначения\nplt.xlabel('Сумма кредитования'); plt.ylabel('Плотность'); plt.title('Суммы кредитования');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Распределение по плотности проживания"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.title(\"Распределение REGION_POPULATION_RELATIVE\")\nax = sns.distplot(app_train[\"REGION_POPULATION_RELATIVE\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\n# KDE займов, выплаченных вовремя\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'REGION_POPULATION_RELATIVE'], label = 'target == 0')\n# KDE проблемных займов\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'REGION_POPULATION_RELATIVE'], label = 'target == 1')\n# Обозначения\nplt.xlabel('Плотность'); plt.ylabel('Плотность населения'); plt.title('Плотность населения');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature Engineering — преобразование признаков\n\nСоревнования на Kaggle выигрываются преобразованием признаков — побеждает тот, кто смог создать самые полезные признаки из данных. По меньшей мере для структурированных данных выигрышные модели — это сейчас в основном разные варианты градиентного бустинга. Чаще всего эффективнее потратить время на преобразование признаков, чем на настройку гиперпараметров или подбор моделей. Модель все-таки может обучиться только по тем данным, которые ей переданы. Убедиться, что данные релевантны задаче — главная ответственность дата саентиста.\n\nПроцесс преобразования признаков может включать создание новых из имеющихся данных, выбор наиболее важных из имеющихся и т.д. Опробуем на этот раз полиномиальные признаки.\n"},{"metadata":{},"cell_type":"markdown","source":"Полиномиальные признаки\n\nПолиномиальный метод конструирования признаков заключается в то, что мы просто создаем признаки, которые являются степенью имеющихся признаков и их произведениями. В некоторых случаях такие сконструированные признаки могут иметь более сильную корреляцию с целевой переменной, чем их «родители». Хотя такие методы часто используются в статистических моделях, в машинном обучении они встречаются значительно реже. Впрочем. ничего не мешает нам их попробовать, тем более что Scikit-Learn имеет класс специально для этих целей — PolynomialFeatures — который создает полиномиальные признаки и их произведения, нужно указать лишь сами исходные признаки и максимальную степень, в которую их нужно возводить. Используем самые мощные по силе воздействия на результат 4 признака и степень 3, чтобы не слишком сильно усложнять модель и избежать оверфиттинга (перетренированности модели — её излишней подстройки под обучающую выборку)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# создадим новый датафрейм для полиномиальных признаков\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n# обработаем отуствующие данные\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy = 'median')\npoly_target = poly_features['TARGET']\npoly_features = poly_features.drop('TARGET', axis=1)\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n                                  \n# Создадим полиномиальный объект степени 3\npoly_transformer = PolynomialFeatures(degree = 3)\n\n# Тренировка полиномиальных признаков\npoly_transformer.fit(poly_features)\n\n# Трансформация признаков\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Формат полиномиальных признаков: ', poly_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Итого 35 полиномиальных и производных признаков. Проверим их корреляцию с таргетом.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Датафрейм для новых фич \npoly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n# Добавим таргет\npoly_features['TARGET'] = poly_target\n# рассчитаем корреляцию\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n# Отобразим признаки с наивысшей корреляцией\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Итак, некоторые признаки показывают более высокую корреляцию, чем исходные. Есть смысл попробовать обучение с ними и без них (как и многое другое в машинном обучении, это можно выяснить экспериментально). Для этого создадим копию датафреймов и добавим туда новые фичи.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# загрузим тестовые признаки в датафрейм\npoly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n# объединим тренировочные датафреймы\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n# объединим тестовые датафреймы\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n# Выровняем датафреймы\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n# Посмотрим формат\nprint('Тренировочная выборка с полиномиальными признаками: ', app_train_poly.shape)\nprint('Тестовая выборка с полиномиальными признаками: ', app_test_poly.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Тренировка модели\n\nБазовый уровень\n\nВ расчетах нужно отталкиваться от какого-то базового уровня модели, ниже которого упасть уже нельзя. В нашем случае это могло бы быть 0,5 для всех тестовых клиентов — это показывает, что мы совершенно не представляем, вернет кредит клиент или нет. В нашем случае предварительная работа уже проведена и можно использовать более сложные модели.\n\nЛогистическая регрессия\n\nДля расчета логистической регрессии нам нужно взять таблицы с закодированными категориальными признаками, заполнить недостающие данные и нормализовать их (привести к значениям от 0 до 1). Все это выполняет следующий код:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n# Уберем таргет из тренировочных данных\nif 'TARGET' in app_train:\n    train = app_train.drop(labels = ['TARGET'], axis=1)\nelse:\n    train = app_train.copy()\nfeatures = list(train.columns)\n# копируем тестовые данные\ntest = app_test.copy()\n# заполним недостающее по медиане\nimputer = SimpleImputer(strategy = 'median')\n# Нормализация\nscaler = MinMaxScaler(feature_range = (0, 1))\n# заполнение тренировочной выборки\nimputer.fit(train)\n# Трансофрмация тренировочной и тестовой выборок\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n# то же самое с нормализацией\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\nprint('Формат тренировочной выборки: ', train.shape)\nprint('Формат тестовой выборки: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Используем логистическую регрессию из Scikit-Learn как первую модель. Возьмем дефольную модель с одной поправкой — понизим параметр регуляризации C во избежание оверфиттинга. Синтаксис обычный — создаем модель, тренируем ее и пресказываем вероятность при помощи predict_proba (нам же нужна вероятность, а не 0/1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n# Создаем модель\nlog_reg = LogisticRegression(C = 0.0001)\n# Тренируем модель\nlog_reg.fit(train, train_labels)\nLogisticRegression(C=0.0001, class_weight=None, dual=False,\n          fit_intercept=True, intercept_scaling=1, max_iter=100,\n          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n# Теперь модель можно использовать для предсказаний. Метод prdict_proba даст на выходе массив m x 2, где m - количество наблюдений, первый столбец - вероятность 0, второй - вероятность 1. Нам нужен второй (вероятность невозврата).\n\n\nlog_reg_pred = log_reg.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\nsubmit.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Улучшенная модель — случайный лес\n\nЛогрег показывает себя не очень хорошо, попробуем использовать улучшенную модель — случайный лес. Это гораздо более мощная модель, которая может строить сотни деревьев и выдавать куда более точный результат. Используем 100 деревьев. Схема работы с моделью все та же, совершенно стандартная — загрузка классификатора, тренировка. предсказание."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n# Создадим классификатор\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50)\n# Тренировка на тернировочных данных\nrandom_forest.fit(train, train_labels)\n# Предсказание на тестовых данных\npredictions = random_forest.predict_proba(test)[:, 1]\n# Создание датафрейма для загрузки\n#submit = app_test[['SK_ID_CURR']]\n#submit['TARGET'] = predictions\n# Сохранение\n#submit.to_csv('random_forest_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Градиентный бустинг\n\nГрадиентный бустинг — «серьёзная модель» для машинного обучения. Практически все последние состязания «затаскиваются» именно. Построим простую модель и проверим её производительность.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nclf = LGBMClassifier()\nclf.fit(train, train_labels)\n\npredictions = clf.predict_proba(test)[:, 1]\nprint(predictions)\n\n# Датафрейм для загрузки\n#submit = app_test[['SK_ID_CURR']]\n#submit['TARGET'] = predictions\n\n# Сохранение датафрейма\n#submit.to_csv('lightgbm_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}