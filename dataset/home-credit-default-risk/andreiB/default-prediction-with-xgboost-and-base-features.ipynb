{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imports \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import (train_test_split, StratifiedKFold, \n                                     cross_val_score, cross_validate, \n                                     cross_val_predict, GridSearchCV, RandomizedSearchCV)\nfrom sklearn import (preprocessing as pp,\n                    feature_selection as fs)\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier)\nfrom sklearn.linear_model import Lasso, LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import (roc_auc_score, auc, roc_curve, \n                            confusion_matrix, f1_score, log_loss, \n                            classification_report)\n\nimport missingno as msno \n\nfrom scipy.stats import chi2_contingency\n\nfrom statsmodels.stats import weightstats as stests\n\nfrom scipy import stats\n\nfrom xgboost import XGBClassifier\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/home-credit-default-risk/application_train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_test = train_test_split(df, test_size=0.20, random_state=42, stratify=df['TARGET'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Identify feature types","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"set(df_train.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#separating columns by type\n\nbinary_features = [col for col in df_train.drop(columns=['TARGET', 'SK_ID_CURR']).columns.values if df_train[col].nunique()<=2]\n\nnum_features = df_train.drop(columns=['TARGET', 'SK_ID_CURR']).select_dtypes(include=['int64', 'float64']).columns \nnum_features = list(set(num_features) - set(binary_features))\n\ncat_features = df_train.drop(columns=['TARGET', 'SK_ID_CURR']).select_dtypes(include='O').columns\ncat_features = list(set(cat_features) - set(binary_features))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Data Skeweness","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"title = str(df_train['TARGET'].value_counts(normalize=True))\ndf_train['TARGET'].value_counts(normalize=True).plot.bar(title=title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Numerical features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting missing numerical features values\nplt.figure(figsize=(10,40))\nsns.barplot(x=df_train.count()[:],y=df_train.count().index)\nplt.xlabel('Non-Null Values Count')\nplt.ylabel('Numerical Features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# showing percentage of missing data per feature\nmissing_p_feat= {feat: round(df_train[feat].isnull().mean(), 3) for feat in df_train.columns.values if df_train[feat].isnull().sum()>0}\n\n# frequency of columns per missing percentage\nplt.hist( np.array(list(missing_p_feat.values())), bins=70)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above we can see that there're lots of features with more than 20% of missing data. These are best to be removed, but first, let's investigate if these features are Missing Completly at Random (MCAR).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted([(value,key) for (key,value) in missing_p_feat.items()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's an interesting pattern I've noticed from the list above. That is, most of the missing features seem to be household related. They could be purposely missing due to aplicants' living situtions (ex living with parents). Let's further investigate with a nullity correlation dendogram.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.dendrogram(df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housig_features_w_missing = [ \n        'LANDAREA_AVG',\n        'LANDAREA_MEDI',\n        'LANDAREA_MODE',\n        'BASEMENTAREA_AVG',\n        'BASEMENTAREA_MEDI',\n        'BASEMENTAREA_MODE',\n        'YEARS_BEGINEXPLUATATION_AVG',\n        'YEARS_BEGINEXPLUATATION_MEDI',\n        'YEARS_BEGINEXPLUATATION_MODE',\n        'TOTALAREA_MODE',\n        'EMERGENCYSTATE_MODE',\n        'LIVINGAREA_AVG',\n        'LIVINGAREA_MEDI',\n        'LIVINGAREA_MODE',\n        'FLOORSMAX_AVG',\n        'FLOORSMAX_MEDI',\n        'FLOORSMAX_MODE',\n        'ENTRANCES_AVG',\n        'ENTRANCES_MEDI',\n        'ENTRANCES_MODE',\n        'WALLSMATERIAL_MODE',\n        'HOUSETYPE_MODE',\n        'APARTMENTS_AVG',\n        'APARTMENTS_MEDI',\n        'APARTMENTS_MODE',    \n        'ELEVATORS_AVG',\n        'ELEVATORS_MEDI',\n        'ELEVATORS_MODE',\n        'NONLIVINGAPARTMENTS_AVG',\n        'NONLIVINGAPARTMENTS_MEDI',\n        'NONLIVINGAPARTMENTS_MODE',\n        'LIVINGAPARTMENTS_AVG',\n        'YEARS_BUILD_AVG',\n        'YEARS_BUILD_MEDI',\n        'YEARS_BUILD_MODE',\n        'FLOORSMIN_AVG',\n        'FLOORSMIN_MEDI',\n        'FLOORSMIN_MODE',\n        'NONLIVINGAREA_AVG',\n        'NONLIVINGAREA_MEDI',\n        'NONLIVINGAREA_MODE',\n        'LIVINGAPARTMENTS_MEDI',\n        'LIVINGAPARTMENTS_MODE',\n        'FONDKAPREMONT_MODE',\n        'COMMONAREA_AVG',\n        'COMMONAREA_MEDI',\n        'COMMONAREA_MODE']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Let's see if there's any difference between aplicants that have missing housing information and the ones that do not.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_housing_missing = df_train[housig_features_w_missing].isnull().all(axis=1)\nratio_on_housing_miss = df_train['TARGET'][all_housing_missing].mean()\nratio_on_housing_avail = df_train['TARGET'][~all_housing_missing].mean()\n\nprint(f'default rate on missing housing data aplicants: {ratio_on_housing_miss:.2%}')\nprint(f'default rate on available housing data aplicants: {ratio_on_housing_avail:.2%}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results above do indicate a difference. The question is whether this difference relevant to default or is due to random chance. To answer this question I am using the chi_squared statistical hypothesis test. If the p values is less than 0.03, then it means that missing housing information is in fact informative in determining default.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cont = pd.crosstab( all_housing_missing, df_train['TARGET'])\n\ntest = chi2_contingency(cont, lambda_=\"log-likelihood\")\np_val = test[1]\nis_stats_diff = p_val<0.03\n\nprint(f'is there statistical significance: {is_stats_diff}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_housing = list(set(num_features).intersection(set(housig_features_w_missing)))\ndf_train[num_housing].hist(figsize=(25,18), bins=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above demonstrates that missing housing data does have an effect over the outcome to be predicted (default).\nSince I am planing on using a desission tree based model, I will be replacing numerical housing values with an extreme value(9999). This way, these data points will be better sepparated in their own group, adding meaning to the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xtreme_inputer = SimpleImputer(fill_value=9999, strategy=\"constant\")\n\ndf_train[num_housing] = xtreme_inputer.fit_transform(df_train[num_housing])\ndf_test[num_housing] = xtreme_inputer.transform(df_test[num_housing])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replacing missing values in categorical features\ncat_housing = list(set(cat_features).intersection(set(housig_features_w_missing)))\ncat_housing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=3, figsize=(10, 8))\nfor i, col in enumerate(cat_housing):\n    df_train[col].hist(ax=axes[i])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[cat_housing] = df_train[cat_housing].fillna(\"MISSING\")\ndf_test[cat_housing] = df_test[cat_housing].fillna(\"MISSING\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=3, figsize=(10, 8))\nfor i, col in enumerate(cat_housing):\n    df_train[col].hist(ax=axes[i])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Listing the rest of the feature with missing values\n\nsorted([(value,key) for (key,value) in missing_p_feat.items() if key not in housig_features_w_missing])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"For features with less than 1% missing values, it is safe to simply drop missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"to_drop_missing = sorted([key for (key,value) in missing_p_feat.items() if key not in housig_features_w_missing and value < 0.01])\nto_drop_missing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping values where missing <1%\n\ndf_train = df_train.dropna(axis=0, how='any', subset=to_drop_missing)\ndf_test = df_test.dropna(axis=0, how='any', subset=to_drop_missing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rest_of_missing = sorted([key for (key,value) in missing_p_feat.items() if key not in housig_features_w_missing and value>0.01])\nrest_of_missing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['OCCUPATION_TYPE'].value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OCCUPATION_TYPE does not have an \"Unemployed\" option it could be that the missing values indicate unemployment. I am labeling all missing values with \"missing\" to signal this observation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['OCCUPATION_TYPE'] = df_train['OCCUPATION_TYPE'].fillna(\"MISSING\")\ndf_test['OCCUPATION_TYPE'] = df_test['OCCUPATION_TYPE'].fillna(\"MISSING\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['OCCUPATION_TYPE'].value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[rest_of_missing[:6]].hist(figsize=(8,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AMT_REQ_CREDIT_BUREAU series show the Number of enquiries to Credit Bureau about the client at a certain amount of time unit before application. Missing values in this case can have a predictive power over target. I am using the chi-square test to find out if thsi is the case.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_req_credit = df_train[rest_of_missing[:6]].isnull().any(axis=1)\nratio_on_missing_req_credit = df_train['TARGET'][missing_req_credit].mean()\nratio_on_avail_req_credit = df_train['TARGET'][~missing_req_credit].mean()\n\nprint(f'default rate on missing credit requests: {ratio_on_missing_req_credit:.2%}')\nprint(f'default rate on available credit requests: {ratio_on_avail_req_credit:.2%}')\n\ncont = pd.crosstab(missing_req_credit, df_train['TARGET'])\n\ntest = chi2_contingency(cont, lambda_=\"log-likelihood\")\np_val = test[1]\nis_stats_diff = p_val<0.03\n\nprint(f'is there statistical significance: {is_stats_diff}')\nprint(f'p: {p_val}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The test above confirms that AMT_REQ_CREDIT_BUREAU does influence outcome of default. To represent this in the model, I replace the missing values with an extreme value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"req_credit = [\n    'AMT_REQ_CREDIT_BUREAU_DAY',\n    'AMT_REQ_CREDIT_BUREAU_HOUR',\n    'AMT_REQ_CREDIT_BUREAU_MON',\n    'AMT_REQ_CREDIT_BUREAU_QRT',\n    'AMT_REQ_CREDIT_BUREAU_WEEK',\n    'AMT_REQ_CREDIT_BUREAU_YEAR'\n]\n\ndf_train[req_credit] = xtreme_inputer.fit_transform(df_train[req_credit])\ndf_test[req_credit] = xtreme_inputer.transform(df_test[req_credit])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking if there's statistical significance when the remaining columns are missing, EXT_SOURCE_1, EXT_SOURCE_3 and OWN_CAR_AGE.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['EXT_SOURCE_1','EXT_SOURCE_3','OWN_CAR_AGE']:\n    missing = df_train[col].isnull()\n    missing_rate = df_train['TARGET'][missing].mean()\n    not_missing_rate = df_train['TARGET'][~missing].mean()\n    \n    print(col)\n    print(f'default rate on missing: {missing_rate:.2%}')\n    print(f'default rate on available: {not_missing_rate:.2%}')\n\n    cont = pd.crosstab(missing, df_train['TARGET'])\n\n    test = chi2_contingency(cont, lambda_=\"log-likelihood\")\n    p_val = test[1]\n    is_stats_diff = p_val<0.03\n\n    print(f'Is there statistical significance: {is_stats_diff}')\n    print(f'p val: {p_val}')\n    print('_'*20, '\\n')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missingness in the features above features do show meaning in predicting defualt. Therefore I am going to replace missing values with an extreme in order to single these cases out.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[['EXT_SOURCE_1','EXT_SOURCE_3','OWN_CAR_AGE']].hist(figsize=(8,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['EXT_SOURCE_1','EXT_SOURCE_3','OWN_CAR_AGE']\n\ndf_train[cols] = xtreme_inputer.fit_transform(df_train[cols])\ndf_test[cols] = xtreme_inputer.transform(df_test[cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking if there's any remaining features with missing values\nremaining_missing = df_train.columns[np.where(df_train.isnull().sum()!=0)]\nremaining_missing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['EMERGENCYSTATE_MODE'].hist(figsize=(5, 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.dropna(axis=0, how='any', subset=remaining_missing)\ndf_test = df_test.dropna(axis=0, how='any', subset=remaining_missing)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Categorical Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[cat_features].head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"whitegrid\")\n\ndef show_plots(feature, figsize=None):\n    if not figsize:\n        figsize = (20, 5)\n    fig, ax = plt.subplots(1, 2, figsize=figsize)\n    fig.suptitle(feature)\n    \n    order = df_train[[feature,'TARGET']].groupby(feature)['TARGET'].mean().sort_values().keys()\n\n    ct = sns.countplot(data=df_train, y=feature, ax=ax[0], order=order)\n    ct.set_title(\"COUNT\")\n    ax[0].set_xlabel('')\n    ax[0].set_ylabel('')\n    \n\n    dfr = sns.barplot(y=feature, x=\"TARGET\", data=df_train, ax=ax[1], order=order)\n    dfr.set_title(\"DEFAULT %\")\n    dfr.set(yticklabels=list())\n    ax[1].set_xlabel('')\n    ax[1].set_ylabel('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_train = df_train.drop(df_train.loc[~df_train['CODE_GENDER'].isin(['F','M'])].index)\ndf_test = df_test.drop(df_test.loc[~df_test['CODE_GENDER'].isin(['F','M'])].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_plots(\"CODE_GENDER\", figsize = (15,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"CODE_GENDER\"] = df_train[\"CODE_GENDER\"].replace({'F':0, 'M':1})\ndf_test[\"CODE_GENDER\"] = df_test[\"CODE_GENDER\"].replace({'F':0, 'M':1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_plots(\"WALLSMATERIAL_MODE\", figsize = (15,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the variables in WALLSMATERIAL_MODE are rare labels, which leads to model overfitment especially in tree based algorithms where variables with lots of labels dominate the ones with fewer labels. Therefore I merge all rare labels into one group titled \"Rare\".","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vc = dict(df_train[\"WALLSMATERIAL_MODE\"].value_counts(normalize=True))\nrare_lb = [k for k,v in vc.items() if v<=0.05]\n\ndf_train[\"WALLSMATERIAL_MODE\"] = df_train[\"WALLSMATERIAL_MODE\"].replace(rare_lb,'RARE')\ndf_test[\"WALLSMATERIAL_MODE\"] = df_test[\"WALLSMATERIAL_MODE\"].replace(rare_lb,'RARE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_plots(\"WALLSMATERIAL_MODE\", figsize = (15,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#One hot encode WALLSMATERIAL_MODE\n\ndf_train = pd.concat([df_train.drop('WALLSMATERIAL_MODE', axis=1), \n                       pd.get_dummies(df_train['WALLSMATERIAL_MODE'], prefix='WALLSMATERIAL_MODE')], \n                          axis=1)\n\ndf_test = pd.concat([df_test.drop('WALLSMATERIAL_MODE', axis=1), \n                       pd.get_dummies(df_test['WALLSMATERIAL_MODE'], prefix='WALLSMATERIAL_MODE')], \n                          axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_plots(\"HOUSETYPE_MODE\", figsize=(15, 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vc = dict(df_train[\"HOUSETYPE_MODE\"].value_counts(normalize=True))\nrare_lb = [k for k,v in vc.items() if v<=0.5]\n\ndf_train[\"HOUSETYPE_MODE\"] = df_train[\"HOUSETYPE_MODE\"].replace(rare_lb,'RARE')\ndf_test[\"HOUSETYPE_MODE\"] = df_test[\"HOUSETYPE_MODE\"].replace(rare_lb,'RARE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_plots(\"HOUSETYPE_MODE\", figsize=(15, 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#One hot encode HOUSETYPE_MODE\n\ndf_train = pd.concat([df_train.drop('HOUSETYPE_MODE', axis=1), \n                       pd.get_dummies(df_train['HOUSETYPE_MODE'], prefix='HOUSETYPE_MODE')], \n                          axis=1)\n\ndf_test = pd.concat([df_test.drop('HOUSETYPE_MODE', axis=1), \n                       pd.get_dummies(df_test['HOUSETYPE_MODE'], prefix='HOUSETYPE_MODE')], \n                          axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_plots(\"NAME_INCOME_TYPE\", figsize=(10, 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['NAME_INCOME_TYPE'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"NAME_INCOME_TYPE\"] = np.where(df_train[\"NAME_INCOME_TYPE\"]=='Working', 1, 0)\ndf_test[\"NAME_INCOME_TYPE\"] = np.where(df_test[\"NAME_INCOME_TYPE\"]=='Working', 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_plots(\"ORGANIZATION_TYPE\", figsize=(20, 15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ORGANIZATION_TYPE has very high cardinality. Therefore I will simply drop this feature.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.drop(\"ORGANIZATION_TYPE\", axis=1)\ndf_test = df_test.drop(\"ORGANIZATION_TYPE\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_plots(\"NAME_FAMILY_STATUS\", figsize=(15, 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(df_train[\"NAME_FAMILY_STATUS\"])^set(df_test[\"NAME_FAMILY_STATUS\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encode NAME_FAMILY_STATUS\n\ndf_train = pd.concat([df_train.drop('NAME_FAMILY_STATUS', axis=1), \n                       pd.get_dummies(df_train['NAME_FAMILY_STATUS'], prefix='NAME_FAMILY_STATUS')], \n                          axis=1)\n\ndf_test = pd.concat([df_test.drop('NAME_FAMILY_STATUS', axis=1), \n                       pd.get_dummies(df_test['NAME_FAMILY_STATUS'], prefix='NAME_FAMILY_STATUS')], \n                          axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_plots(\"FONDKAPREMONT_MODE\", figsize=(15, 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given the high percentage of missing values in FONDKAPREMONT_MODE and also the fact that I am not sure what it means and I cannot find much information on the topic, I am going to simply remove this feature.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vc = dict(df_train[\"FONDKAPREMONT_MODE\"].value_counts(normalize=True))\nrare_lb = [k for k,v in vc.items() if v<=0.1]\n\ndf_train[\"FONDKAPREMONT_MODE\"] = df_train[\"FONDKAPREMONT_MODE\"].replace(rare_lb,'RARE')\ndf_test[\"FONDKAPREMONT_MODE\"] = df_test[\"FONDKAPREMONT_MODE\"].replace(rare_lb,'RARE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.drop(\"FONDKAPREMONT_MODE\", axis=1)\ndf_test = df_test.drop(\"FONDKAPREMONT_MODE\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_plots(\"OCCUPATION_TYPE\", figsize=(15, 5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OCCUPATION_TYPE has high cardinality and a high percentage of missing values, therefore I will drop this feature.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['OCCUPATION_TYPE'].value_counts(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['OCCUPATION_TYPE'] = np.where(df_train['OCCUPATION_TYPE']=='MISSING',0,1)\ndf_test['OCCUPATION_TYPE'] = np.where(df_test['OCCUPATION_TYPE']=='MISSING',0,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_plots(\"WEEKDAY_APPR_PROCESS_START\", figsize=(15, 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encode WEEKDAY_APPR_PROCESS_START\n\ndf_train = pd.concat([df_train.drop('WEEKDAY_APPR_PROCESS_START', axis=1), \n                       pd.get_dummies(df_train['WEEKDAY_APPR_PROCESS_START'], prefix='WEEKDAY_APPR_PROCESS_START')], \n                          axis=1)\n\ndf_test = pd.concat([df_test.drop('WEEKDAY_APPR_PROCESS_START', axis=1), \n                       pd.get_dummies(df_test['WEEKDAY_APPR_PROCESS_START'], prefix='WEEKDAY_APPR_PROCESS_START')], \n                          axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_plots(\"NAME_TYPE_SUITE\", figsize=(15,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"NAME_TYPE_SUITE\"] = np.where(df_train[\"NAME_TYPE_SUITE\"]=='Unaccompanied', 0, 1) \ndf_test[\"NAME_TYPE_SUITE\"] = np.where(df_test[\"NAME_TYPE_SUITE\"]=='Unaccompanied', 0, 1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_plots(\"NAME_HOUSING_TYPE\", figsize=(15,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(df_train['NAME_HOUSING_TYPE'])^set(df_train['NAME_HOUSING_TYPE'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vc = dict(df_train[\"NAME_HOUSING_TYPE\"].value_counts(normalize=True))\nrare_lb = [k for k,v in vc.items() if v<=0.04]\n\ndf_train = df_train[~df_train[\"NAME_HOUSING_TYPE\"].isin(rare_lb)]\ndf_test = df_test[~df_test[\"NAME_HOUSING_TYPE\"].isin(rare_lb)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_plots(\"NAME_HOUSING_TYPE\", figsize=(15,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encode NAME_HOUSING_TYPE\n\ndf_train = pd.concat([df_train.drop('NAME_HOUSING_TYPE', axis=1), \n                       pd.get_dummies(df_train['NAME_HOUSING_TYPE'], prefix='NAME_HOUSING_TYPE')], \n                          axis=1)\n\ndf_test = pd.concat([df_test.drop('NAME_HOUSING_TYPE', axis=1), \n                       pd.get_dummies(df_test['NAME_HOUSING_TYPE'], prefix='NAME_HOUSING_TYPE')], \n                          axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_plots(\"NAME_EDUCATION_TYPE\", figsize=(15,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['NAME_EDUCATION_TYPE'].value_counts(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(df_train['NAME_EDUCATION_TYPE'])^set(df_train['NAME_EDUCATION_TYPE'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['NAME_EDUCATION_TYPE'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mapping = {\n'Higher education': 2,\n'Secondary / secondary special' : 1,\n'Incomplete higher' : 1,\n'Lower secondary' : 1,\n'Academic degree': 2\n}\n\ndf_train[\"NAME_EDUCATION_TYPE\"] = df_train[\"NAME_EDUCATION_TYPE\"].map(mapping)\ndf_test[\"NAME_EDUCATION_TYPE\"] = df_test[\"NAME_EDUCATION_TYPE\"].map(mapping)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[['NAME_EDUCATION_TYPE','TARGET']].groupby('NAME_EDUCATION_TYPE')['TARGET'].value_counts(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list(set(cat_features).intersection(set(df_train.columns.values)))\ncols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Binary Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[binary_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in binary_features:\n    show_plots(feature, figsize=(15,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"NAME_CONTRACT_TYPE\"] = df_train[\"NAME_CONTRACT_TYPE\"].replace({'Cash loans':1, 'Revolving loans':0})\ndf_test[\"NAME_CONTRACT_TYPE\"] = df_test[\"NAME_CONTRACT_TYPE\"].replace({'Cash loans':1, 'Revolving loans':0}) \n\ndf_train[\"FLAG_OWN_CAR\"] = df_train[\"FLAG_OWN_CAR\"].replace({'Y':1, 'N':0})\ndf_test[\"FLAG_OWN_CAR\"] = df_test[\"FLAG_OWN_CAR\"].replace({'Y':1, 'N':0})\n\ndf_train[\"FLAG_OWN_REALTY\"] = df_train[\"FLAG_OWN_REALTY\"].replace({'Y':1, 'N':0})\ndf_test[\"FLAG_OWN_REALTY\"] = df_test[\"FLAG_OWN_REALTY\"].replace({'Y':1, 'N':0})\n\ndf_train[\"EMERGENCYSTATE_MODE\"] = df_train[\"EMERGENCYSTATE_MODE\"].replace({'Yes':1, 'No':0})\ndf_test[\"EMERGENCYSTATE_MODE\"] = df_test[\"EMERGENCYSTATE_MODE\"].replace({'Yes':1, 'No':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We observe that lots of the features are constant, therefore I am going to remove them as they do not add any information to the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"var = fs.VarianceThreshold(0.02)\nvar.fit(df_train)\n\nnon_informative = df_train.columns[~var.get_support()]\nnon_informative","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[non_informative].hist(bins=30, figsize=(20,20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"non_informative = list(filter(lambda x: x!='REGION_POPULATION_RELATIVE', non_informative))\n\ndf_train = df_train.drop(non_informative, 1)\ndf_test = df_test.drop(non_informative, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = df_train.corr().abs()\nsns.clustermap(correlation, cmap='coolwarm', \n               vmin=0, vmax=0.8, center=0, \n               square=True, linewidths=.5, \n               figsize=(50,50), yticklabels=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_mat = correlation.unstack() \ncorr_mat = corr_mat.sort_values(ascending=False)\ncorr_mat = corr_mat[(corr_mat >= 0.8) & (corr_mat < 1)]\ncorr_mat = pd.DataFrame(corr_mat).reset_index()\ncorr_mat.columns = ['f1', 'f2', 'correlation']\ncorr_mat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_feature = set()\ncorrelated_groups = list()\n\nfor feature in corr_mat.f1.unique():\n    if feature not in grouped_feature:\n        correlated_block = corr_mat[corr_mat.f1==feature]\n        grouped_feature = grouped_feature | set(correlated_block.f2) | set(feature)\n        correlated_groups.append(correlated_block)\n        \nprint(f'correlated groups count: {len(correlated_groups)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, group in enumerate(correlated_groups):\n    print(f'\\tgroup {i+1}:\\n{group}\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance_groups = list()\n\nfor group in correlated_groups:\n    group_features = list(set(group.f1) | set(group.f2))\n    \n    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4, verbose=0)\n    \n    f_import = rf.fit(df_train[group_features], \n                       df_train['TARGET']).feature_importances_\n    imp_g = pd.DataFrame({'feature': group_features,\n                            'importance': f_import}).sort_values(by='importance', ascending=False)\n    importance_groups.append(imp_g)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'total importance groups: {len(importance_groups)}\\n\\n')\nfor i, g in enumerate(importance_groups):\n    print(f'img_g: {i}\\n {g}\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"not_imp_features = set()\n\nfor g in importance_groups:\n    not_imp_features |= set(g.feature[1:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.drop(columns=list(not_imp_features)).reset_index()\ndf_test = df_test.drop(columns=list(not_imp_features)).reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Numerical features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"updated_numericals = list(set(num_features).intersection(set(df_train.columns.values)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[updated_numericals].hist(figsize=(20,20), bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"updated_numericals.remove('CNT_CHILDREN')\nupdated_numericals.remove('REGION_RATING_CLIENT_W_CITY')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss = pp.StandardScaler()\nss.fit(df_train[updated_numericals])\n\ndf_train[updated_numericals] = ss.transform(df_train[updated_numericals])\ndf_test[updated_numericals] = ss.transform(df_test[updated_numericals])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_plots(\"CNT_CHILDREN\", figsize=(15,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vc = dict(df_train[\"CNT_CHILDREN\"].value_counts(normalize=True))\nrare_lb = [k for k,v in vc.items() if v<=0.05]\n\ndf_train = df_train[~df_train[\"CNT_CHILDREN\"].isin(rare_lb)]\ndf_test = df_test[~df_test[\"CNT_CHILDREN\"].isin(rare_lb)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_plots(\"CNT_CHILDREN\", figsize=(15,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[['CNT_CHILDREN', 'TARGET']].groupby('CNT_CHILDREN')['TARGET'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oh = pp.OneHotEncoder(handle_unknown='ignore')\noh.fit(df_train[['CNT_CHILDREN']])\n\ndf_train = pd.concat([df_train.drop('CNT_CHILDREN', axis=1).reset_index(drop=True), \n                       pd.DataFrame( oh.transform(df_train['CNT_CHILDREN'].values.reshape(-1,1)).toarray(), \n                                        columns=oh.get_feature_names(['CNT_CHILDREN']) \n                                    )  \n                      ], \n                    axis=1).drop('index', axis=1)\n\ndf_test = pd.concat([df_test.drop('CNT_CHILDREN', axis=1).reset_index(drop=True), \n                       pd.DataFrame( oh.transform(df_test['CNT_CHILDREN'].values.reshape(-1,1)).toarray(), \n                                        columns=oh.get_feature_names(['CNT_CHILDREN']) \n                                    )  \n                      ], \n                    axis=1).drop('index', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_plots(\"REGION_RATING_CLIENT_W_CITY\", figsize=(15,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['REGION_RATING_CLIENT_W_CITY'].value_counts(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[['REGION_RATING_CLIENT_W_CITY', 'TARGET']].groupby('REGION_RATING_CLIENT_W_CITY')['TARGET'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Selecting features using Lasso Regularization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sample = df_train.sample(frac=0.1,replace=False, random_state=1)\n\nX_f = df_sample[[col for col in df_sample.columns if col not in ['TARGET', 'SK_ID_CURR']]]\ny_f = df_sample['TARGET']\n\nX_f_train, X_f_test, y_f_train, y_f_test = train_test_split(X_f, y_f, test_size=0.3, random_state=0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l1',solver='liblinear'))\nsel_.fit(X_f_train, y_f_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"not_select_feat = X_f_train.columns[~(sel_.get_support())]\nnot_select_feat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.drop('FLAG_WORK_PHONE', axis=1)\ndf_test = df_test.drop('FLAG_WORK_PHONE', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Informative features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"AUC_values = list()\n\nfor feature in X_f.columns:\n    clf = DecisionTreeClassifier()\n    clf.fit(X_f_train[[feature]], y_f_train)\n    pred = clf.predict_proba(X_f_test[[feature]])\n    score = roc_auc_score(y_f_test,pred[:,1])\n    AUC_values.append(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUC_df = pd.Series(AUC_values)\nAUC_df.index = X_f_train.columns\nAUC_df = AUC_df.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(AUC_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,20))\nsns.barplot(y=AUC_df.index, x=AUC_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"informative_features = list(AUC_df[AUC_df>0.5].index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature importancy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = RandomForestClassifier()\nrf_model.fit(X_f, y_f)\n\nfv = dict(zip(X_f.columns,rf_model.feature_importances_))\nfv_dict = {k: v for k, v in sorted(fv.items(), key=lambda item: item[1], reverse=True)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,20))\nsns.barplot(y=list(fv_dict.keys()), x=list(fv_dict.values()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"important_features = [k for k,v in fv_dict.items() if v>0.01]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trying out multiple models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nmetrics = ['neg_log_loss', 'accuracy', 'f1', 'precision', 'roc_auc']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"informative_and_important_features = list(set(informative_features).union(set(important_features)))\nall_features = [col for col in df_train.columns if col not in ['SK_ID_CURR','TARGET']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = dict()\n\nfor set_name, feature_set in dict(zip(['', 'important_features', 'informative_features', 'informative_and_important_features'], \n                                      [all_features, important_features, informative_features, informative_and_important_features])).items():\n    \n    for model_name, model in dict(zip(['XGBClassifier', 'RandomForestClassifier', 'AdaBoostClassifier', 'GradientBoostingClassifier'], \n                               [XGBClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier])).items():\n\n        validator = cross_validate(model(), df_train[feature_set], df_train['TARGET'], \n                                   cv=kfold, scoring=metrics, return_train_score=True,\n                                   n_jobs=-1, verbose=10)\n        results[f'{model_name}_{set_name}'] = validator\n        print(f'{model_name}_{set_name}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = pd.DataFrame(results.values())\nd.index = results.keys()\nd = d.applymap(np.mean)\nd = d.sort_values(by='test_roc_auc',ascending=False)\nd['diff'] = (d['train_roc_auc'] - d['test_roc_auc']) *100\nd\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = { \n                'n_estimators' : range(50, 150, 50),\n                'max_depth' : range(4, 5, 1),\n                'min_samples_split': range(200,400,200),\n                \"reg_alpha\": [1.5, 2, 2.5],\n                \"reg_lambda\": [3.5, 4, 4.5]\n             }\n\n\ngrid_search = GridSearchCV(\n            XGBClassifier(tree_method='gpu_hist'),\n            parameters,\n            scoring=['roc_auc', 'neg_log_loss'],\n            refit='roc_auc',\n            n_jobs=-1, \n            cv = kfold, \n            verbose=10,\n            return_train_score = True \n) \n\ngrid_search.fit(df_train[all_features], df_train['TARGET'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_search.best_score_)\nprint(grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = grid_search.cv_results_['mean_train_roc_auc']\ntest = grid_search.cv_results_['mean_test_roc_auc']\nparams = grid_search.cv_results_['params']\n\nsns.lineplot(y=test, x=list(range(0,len(test),1)))\nsns.lineplot(y=train, x=list(range(0,len(train),1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for train, test,  param in zip(train, test, params):\n    print(\"test: %f train: %f with: %r\" % (test, train, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"mean_train_roc_auc -----------------{}\".format( np.mean(grid_search.cv_results_['mean_train_roc_auc']) ))\nprint(\"mean_test_roc_auc ------------------{}\".format( np.mean(grid_search.cv_results_['mean_test_roc_auc']) ))\n\nprint(\"mean_train_neg_log_loss ------------{}\".format( np.mean(grid_search.cv_results_['mean_train_neg_log_loss'])))\nprint(\"mean_test_neg_log_loss -------------{}\".format( np.mean(grid_search.cv_results_['mean_test_neg_log_loss'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(grid_search.cv_results_['mean_test_roc_auc'])\nsns.distplot(grid_search.cv_results_['mean_train_roc_auc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = XGBClassifier(**grid_search.best_params_)\n\nbest_model.fit(df_train[all_features], df_train['TARGET'])\n\ny_predict = best_model.predict_proba( df_test[all_features] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_results = pd.DataFrame({'true':df_test['TARGET'],\n                            'predict': y_predict[:,1]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(df_results['true'], df_results['predict'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot( df_results.predict[df_results.true==1] )\nsns.distplot( df_results.predict[df_results.true==0] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( confusion_matrix(df_results.true, \n                        np.where(df_results.predict>0.5, 1, 0)\n                       ) \n     )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_results.true.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( classification_report( df_results.true, \n                        np.where(df_results.predict>0.5, 1, 0)\n                       ) \n     )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( log_loss( df_results.true, df_results.predict) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['TARGET'].value_counts(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the Dumb Logloss curve from below, the model's logloss slightly falls under the curve.\nTherefore, this model is informative.\n\n![](https://i.stack.imgur.com/54KwE.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}