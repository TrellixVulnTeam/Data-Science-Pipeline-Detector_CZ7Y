{"cells":[{"metadata":{"_uuid":"2b249fbfb2ddca306623a0a513d120b7096c6a95"},"cell_type":"markdown","source":"# Home Credit Default Risk- Encyclopedic Data Analysis and Exploration using Python ‚úç\n\n### This note book is work in progress. Stay tuned for more indepth analysis and insights"},{"metadata":{"_uuid":"53c042114e5b0c2faa92f205907cd79c30bead8e"},"cell_type":"markdown","source":"## Give me six hours to chop down a tree and i will spend the first four sharpening the axe - Abraham Lincon.\n#### This makes perfect sense to me in the area of machine learning and statistical modeling and this is exactly what i would do when i build a model. In fact, i would spend five and half in shapening the axe (data prepartion) .\n\n#### In the advent of various machine learning algorithms like Random Forest, XG Boost etc.. you don't have to spend much time in variable selection and treatment, but it's always fun to know what and how you are going to process the features and for the fact that, your results are directly proportional to quality of  your input variables. \n"},{"metadata":{"_uuid":"d38dfdba47217b093cae03d7d2264a54ee40c9b3"},"cell_type":"markdown","source":"#### I'm going to drive this notebook in terms of various steps involved in the process of building a good model and i see it more useful for beginner to intermediate data scientists\n\n#### As always, any comments / suggestions are much appreciated :)"},{"metadata":{"_uuid":"1f7b14970abc80825792d7944a73912ec5e08cc7"},"cell_type":"markdown","source":"## Index:\n- <a href='#pb'>1. Problem Statement</a>\n- <a href='#lb'>2. Libraries</a>\n- <a href='#dl'>3. Get the Data</a>\n    - <a href='#lk'>3.1 Quick look at the data</a>\n- <a href='#dp'>4. Data Exploration</a>\n     - <a href='#vi'>4.1 Variable Identification</a>\n     - <a href='#uv'>4.2 Univariate Analysis</a>\n- <a href='#mv'>5. Missing Value Treatment</a>\n- <a href='#ot'>6. Outlier Treatment</a>\n"},{"metadata":{"_uuid":"7f19e1c74eea857edf2e3aed2dba5ed9e15aef02"},"cell_type":"markdown","source":"## <a id='pb'>1. Problem Statement</a>\n\n  #### As per the description mentioned [here](https://www.kaggle.com/c/home-credit-default-risk),  we are going to build a model for the credit invisibles (whose credit history was very limited / unavailable with the bureau) to estimater whether the given customer will be able to pay the loan installements on time or not. For this, we are going to depend mostly on the interal data that the bank collected at the time of application / alternate data sources than the treditional bureau data. \n  #### Target variable (1 - client with payment difficulties: he/she had late payment more than X days on at least one of the first Y installments of the loan in our sample, 0 - all other cases)\n "},{"metadata":{"_uuid":"d19688c99d09485ef1542e33a25fe4519660f1a0"},"cell_type":"markdown","source":"## <a id='lb'>2. Libraries</a>"},{"metadata":{"trusted":true,"_uuid":"c76923a0742b6a20bdbbd4d6b45aaa3c4f4fa917","collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd   \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\npd.set_option('display.max_row',10000)\npd.set_option('display.max_columns',150)","execution_count":190,"outputs":[]},{"metadata":{"_uuid":"9a7e5ef4c85a8516f8b975ec7f495c93cd8c273c"},"cell_type":"markdown","source":"## <a id='dl'>3. Get the Data</a>"},{"metadata":{"trusted":true,"_uuid":"d36f8174cc7bc63396c1342be8d3e6478f0bd347","collapsed":true},"cell_type":"code","source":"app_train = pd.read_csv('../input/application_train.csv')\napp_test=pd.read_csv('../input/application_test.csv')\nbureau = pd.read_csv('../input/bureau.csv')\nbureau_bal = pd.read_csv('../input/bureau_balance.csv')\npos_cash = pd.read_csv('../input/POS_CASH_balance.csv')\ncc_bal = pd.read_csv('../input/credit_card_balance.csv')\napp_prev = pd.read_csv('../input/previous_application.csv')\ninstal_pay = pd.read_csv('../input/installments_payments.csv')","execution_count":191,"outputs":[]},{"metadata":{"_uuid":"78c5bc07a9e08521edcf1aee607fa48a9934a9b7"},"cell_type":"markdown","source":"## <a id='lk'>3.1 Quick look at the data</a>"},{"metadata":{"trusted":true,"_uuid":"c492b3803f73150d79e1759e63aa943defbc9213"},"cell_type":"code","source":"app_train.shape","execution_count":192,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a5723deaab79dced310551e3c53b0a265fb225e"},"cell_type":"code","source":"app_train.head()","execution_count":193,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdd2e14dae667809675964a9922d5eeee1ddb6a5","collapsed":true},"cell_type":"code","source":"len(app_train['SK_ID_CURR'].unique().tolist())","execution_count":40,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebe38e4191ac639aea1604c41b666a9245817267"},"cell_type":"code","source":"app_test.shape","execution_count":194,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e9e19f6d4e2298dd5ff0c857b1c63456b28a7a1"},"cell_type":"code","source":"len(app_test['SK_ID_CURR'].unique().tolist())","execution_count":195,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df686dd7ca36b75be2e677e7787ad2e65fc67fa7"},"cell_type":"code","source":"bureau.shape","execution_count":196,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a66e504ba9a63663b34f746aa77850dd9935d57c"},"cell_type":"code","source":"bureau.head()","execution_count":197,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e79ba75881891348b1d73d93fe82e17aa07e42b"},"cell_type":"code","source":"len(bureau['SK_ID_CURR'].unique().tolist())","execution_count":198,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8dfea0728dc61e25f6eaffa77b3dad3ac1fb4470"},"cell_type":"code","source":"len(bureau['SK_ID_BUREAU'].unique().tolist())","execution_count":199,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ea6b8bc7f8fe02d7e226cc9ceaccd216f1aa2d8"},"cell_type":"code","source":"pos_cash.shape","execution_count":200,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51d161ee76585192d26e22ba0b5b6a9ccc1873b8"},"cell_type":"code","source":"len(pos_cash['SK_ID_CURR'].unique().tolist())","execution_count":201,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd2af5a39aef505ac98164bbd385df268cc974f8"},"cell_type":"code","source":"cc_bal.shape","execution_count":202,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5e73efbe0a40daaf6c1e1d88c8e60af684f616c"},"cell_type":"code","source":"len(cc_bal['SK_ID_CURR'].unique().tolist())","execution_count":203,"outputs":[]},{"metadata":{"_uuid":"7330051f585bd8d8361b8260a61e98a98934a145"},"cell_type":"markdown","source":"As mentioend in the data description,  excpet for the train and test datasets, all other dataset have duplicates/repeated entries by the ID variable. So need to be carefull when we join the datasets\n\n### Lets have a peek at the distribution of the target variable in the train dataset, "},{"metadata":{"trusted":true,"_uuid":"7781eae75ddb6f47c9cc65e8bca9c4a9112273cd"},"cell_type":"code","source":"bad_dist = app_train['TARGET'].value_counts().reset_index()\nbad_dist.columns = ['val','count']\nbad_dist['percent'] = (bad_dist['count']/app_train.shape[0])*100\nbad_dist","execution_count":204,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9eb35c5b6af547d905cd833595d2aa7f676a6c6"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure()\nplt.bar(app_train['TARGET'].value_counts().index,app_train['TARGET'].value_counts());","execution_count":205,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9c2d3b3ed5c31f42a47f18d79ef4da1d17360b79"},"cell_type":"markdown","source":"\n#### It's an example of imbalanced data with ~92% good and 8% bad observations. Make sense to look at ROC as the evaluation metric than Accuracy"},{"metadata":{"_uuid":"5d46c307827836ec9d78d9a85df874a1d7807429"},"cell_type":"markdown","source":"#### Lets combine the train and test data set for preprocessing"},{"metadata":{"trusted":true,"_uuid":"3ac53d2aa79aa63a7be0fa5fa1007f415063516b","collapsed":true},"cell_type":"code","source":"train_id = app_train['SK_ID_CURR']\ntest_id = app_test['SK_ID_CURR']\ntrain_y = app_train['TARGET']\napp_train['key'] = 'train'\napp_train.drop(['SK_ID_CURR','TARGET'],axis=1,inplace=True)\napp_test.drop(['SK_ID_CURR'],axis=1,inplace=True)\napp_test['key']='test'\ndata=pd.concat((app_train,app_test)).reset_index(drop=True)\n","execution_count":206,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f5e664fcef09340d27dd5d71eb1fb805faf6f04"},"cell_type":"code","source":"data.shape","execution_count":207,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"14c9b0333b26ba5bdd522256d73d5f80832f90ba"},"cell_type":"markdown","source":"## <a id='vi'>4. Variable Identification</a>\n\nLets look at the variable type, data type and also the variable category (categorical/contineous). We will convert the existing variables type to other types if it is necessary"},{"metadata":{"trusted":true,"_uuid":"85ccb446395fcc3ba050e8ba2b92c1fd84ccbc12"},"cell_type":"code","source":"bureau.info()","execution_count":208,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"915de7f4339fb5443cc26f6b3f23efad2cac49ef"},"cell_type":"code","source":"#Randomly looking at the distribution of the variables that i'm curious about\nbureau.groupby(['CREDIT_CURRENCY'])['CREDIT_CURRENCY'].count()","execution_count":209,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f8912425b664b1a6243ee747f9074d12c9ed3ac"},"cell_type":"code","source":"bureau_bal.info()","execution_count":210,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"813dbadaac4877aae0949a00dc0c1c3c8a4acf23"},"cell_type":"code","source":"bureau_bal.groupby(['STATUS'])['STATUS'].count()","execution_count":211,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25434b65b18abae7d1b103fc9d5c8508d4e562e5"},"cell_type":"code","source":"instal_pay.info()","execution_count":212,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8c7b40703e3c685a65c459ca98b8950d73eb2ec"},"cell_type":"code","source":"app_prev.info()","execution_count":213,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8d18828ee001041fc27eccd03f32b91fc197162"},"cell_type":"code","source":"cc_bal.info()","execution_count":214,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5731f7e0db1f0bcdfb158f270c08cae522a2af6"},"cell_type":"code","source":"pos_cash.info()","execution_count":215,"outputs":[]},{"metadata":{"_uuid":"655909685c28643f6bb948f92dd627e59d732da5"},"cell_type":"markdown","source":"Looks like the variables are assigned correctly and we don't have to change anything"},{"metadata":{"_uuid":"78106a66e8f2766d3ffa2d75554291706ba38103"},"cell_type":"markdown","source":"## <a id='uv'>4.2 Univariate Analysis </a>\n\n#### Lets look at the central tendency for continuous variale and the frequncy tables for categorical variables. Before diving into it, let's seggreate the continuous and categorical variables into separate datasets for convenience"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"af49f458aed1e468ac9f6a5cd55c26ae42089d05"},"cell_type":"code","source":"cat_data=data.select_dtypes(include=[\"O\"])","execution_count":216,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8307d6d33e65988552d5962fa5e05e1de0c5859","collapsed":true},"cell_type":"code","source":"num_data = data.select_dtypes(include=[\"number\"])","execution_count":217,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b67385e03d911b35029f281ddd1a0be64cc0fec0"},"cell_type":"code","source":"num_data.shape","execution_count":218,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b15080a6097999a46b95a19dbfce8030ae5c8058"},"cell_type":"code","source":"cat_data.shape","execution_count":219,"outputs":[]},{"metadata":{"_uuid":"5eca0f0374fd554fa4f46ed64fc48b1c8b8f7021"},"cell_type":"markdown","source":"####  So we have 104 numerical columns and 17 categorical columns, which matches to the total 121 columns in the train data. "},{"metadata":{"trusted":true,"_uuid":"94f6dfd312ac8cb8207b23c4c581637fa0802d48"},"cell_type":"code","source":"cat_data.describe()","execution_count":220,"outputs":[]},{"metadata":{"_uuid":"0d0628007d2785495cf2c2da15fcf6c2b2090c0c"},"cell_type":"markdown","source":"#### The above output gives you an indepth information about all categorical variables. It gives you the total count (we can get the missing count from this, if any), unique values in each column, the value repeated the most number of times and the number of time the top value was repeated. It's a very good insight to start with. From this, you can also see if the frequncy value in the about table is more than 90% of the count, then the variable is very unlikely to give us any information about the target variable. Hence we can delete the variable. Anyhow, let's do it in in a while after making sure of that once again. As of now, i see the variables 'NAME_CONTRACT_TYPE' and 'NAME_HOUSING_TYPE' are the one's that have the same value for 90% of the observations"},{"metadata":{"trusted":true,"_uuid":"27a5124bbb512ef7c5531f8131c19e99653a6405"},"cell_type":"code","source":"pd.set_option('display.float_format',lambda x: '%.1f' % x)\nnum_data.describe()","execution_count":221,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_uuid":"d2c217c5fc291487823a9a3b42ca0ad3d52f6e7b"},"cell_type":"markdown","source":"#### This is way too much info and should spend significant amount of time to understand the variables and their distirbution. All the days fields here have -ve values as they are calculated based on the appliation date (howmany days ago from the application date). if you see the same values for 25th, 50th and 75th percentile, then ideally that variable wouldn't give much information about the target variable. \n\n#### From the above distribution, you can see there are outliers for quite a few variables and will treat them in a while. Let's look at the missing values"},{"metadata":{"_uuid":"1d3f537f5c74b88f5af6af7d89905213ac41fe15"},"cell_type":"markdown","source":"## <a id='mv'>5. Missing Value Treatment </a>"},{"metadata":{"trusted":true,"_uuid":"59f21f7071b2707c306fb81cf7fc60fd6a389061"},"cell_type":"code","source":"missing = num_data.isnull().sum(axis=0).reset_index()\nmissing.columns=['column_name','missing_count']\npercent = ((num_data.isnull().sum(axis=0)/num_data.shape[0])*100).reset_index()\npercent.columns=['variable','percent']\nmissing_data = pd.concat([missing,percent['percent']],axis=1)\nmissing_data=missing_data.ix[missing_data['missing_count']>0].sort_values(by='missing_count')","execution_count":222,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e13d64c4706c60959653d67222b70de16d327fdf"},"cell_type":"code","source":"missing_data","execution_count":223,"outputs":[]},{"metadata":{"_uuid":"577855360fcde40b0919b5d10f3b7168f8a9f8a6"},"cell_type":"markdown","source":"#### We have handful number of variables with missing values. I'm going to delete all the variables with more than 45% of the values are missing"},{"metadata":{"trusted":true,"_uuid":"572f768e6371f07448c6276050c5a650f518df05"},"cell_type":"code","source":"missing_col=missing_data['column_name'][missing_data['percent']> 45].tolist()\nfor i in missing_col:\n    num_data.drop([i],axis=1,inplace=True)","execution_count":224,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d27b164e693badcecdc4fb89de6052f849aab06a"},"cell_type":"code","source":"num_data.shape","execution_count":84,"outputs":[]},{"metadata":{"_uuid":"3134c3195760bb5c6c9fa5cce308ba3732b54f14"},"cell_type":"markdown","source":"By looking at the data dictionary, i'm going to impute the missing values using the most appropirate method that i think would make sense"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2277c26d405a53204aedb9cb12ca15753fa42371"},"cell_type":"code","source":"num_data['DAYS_LAST_PHONE_CHANGE']= num_data['DAYS_LAST_PHONE_CHANGE'].fillna(num_data['DAYS_LAST_PHONE_CHANGE'].mode()[0])\nnum_data['CNT_FAM_MEMBERS']= num_data['CNT_FAM_MEMBERS'].fillna(num_data['CNT_FAM_MEMBERS'].mode()[0])\nnum_data['AMT_ANNUITY']= num_data['AMT_ANNUITY'].fillna(num_data['AMT_ANNUITY'].mean())\nnum_data['AMT_GOODS_PRICE']= num_data['AMT_GOODS_PRICE'].fillna(0)\nnum_data['EXT_SOURCE_2']= num_data['EXT_SOURCE_2'].fillna(num_data['EXT_SOURCE_2'].mean())\nnum_data['DEF_60_CNT_SOCIAL_CIRCLE']= num_data['DEF_60_CNT_SOCIAL_CIRCLE'].fillna(num_data['DEF_60_CNT_SOCIAL_CIRCLE'].mode()[0])\nnum_data['OBS_60_CNT_SOCIAL_CIRCLE']= num_data['OBS_60_CNT_SOCIAL_CIRCLE'].fillna(num_data['OBS_60_CNT_SOCIAL_CIRCLE'].mode()[0])\nnum_data['DEF_30_CNT_SOCIAL_CIRCLE']= num_data['DEF_30_CNT_SOCIAL_CIRCLE'].fillna(num_data['DEF_30_CNT_SOCIAL_CIRCLE'].mode()[0])\nnum_data['OBS_30_CNT_SOCIAL_CIRCLE']= num_data['OBS_30_CNT_SOCIAL_CIRCLE'].fillna(num_data['OBS_30_CNT_SOCIAL_CIRCLE'].mode()[0])\nnum_data['AMT_REQ_CREDIT_BUREAU_MON']= num_data['AMT_REQ_CREDIT_BUREAU_MON'].fillna(num_data['AMT_REQ_CREDIT_BUREAU_MON'].mode()[0])\nnum_data['AMT_REQ_CREDIT_BUREAU_WEEK']= num_data['AMT_REQ_CREDIT_BUREAU_WEEK'].fillna(num_data['AMT_REQ_CREDIT_BUREAU_WEEK'].mode()[0])\nnum_data['AMT_REQ_CREDIT_BUREAU_DAY']= num_data['AMT_REQ_CREDIT_BUREAU_DAY'].fillna(num_data['AMT_REQ_CREDIT_BUREAU_DAY'].mode()[0])\nnum_data['AMT_REQ_CREDIT_BUREAU_HOUR']= num_data['AMT_REQ_CREDIT_BUREAU_HOUR'].fillna(num_data['AMT_REQ_CREDIT_BUREAU_HOUR'].mode()[0])\nnum_data['AMT_REQ_CREDIT_BUREAU_QRT']= num_data['AMT_REQ_CREDIT_BUREAU_QRT'].fillna(num_data['AMT_REQ_CREDIT_BUREAU_QRT'].mode()[0])\nnum_data['AMT_REQ_CREDIT_BUREAU_YEAR']= num_data['AMT_REQ_CREDIT_BUREAU_YEAR'].fillna(num_data['AMT_REQ_CREDIT_BUREAU_YEAR'].mode()[0])\nnum_data['EXT_SOURCE_3']= num_data['EXT_SOURCE_3'].fillna(num_data['EXT_SOURCE_3'].mean())","execution_count":105,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"52ffec3e185764fef7591210e72212d401a5b04b"},"cell_type":"code","source":"missing_cat = cat_data.isnull().sum(axis=0).reset_index()\nmissing_cat.columns=['column_name','missing_count']\npercent_cat = ((cat_data.isnull().sum(axis=0)/cat_data.shape[0])*100).reset_index()\npercent_cat.columns=['variable','percent']\nmissing_data_cat = pd.concat([missing_cat,percent_cat['percent']],axis=1)\nmissing_data_cat=missing_data_cat.ix[missing_data_cat['missing_count']>0].sort_values(by='missing_count')","execution_count":108,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ad2f98986bb555c604393c8e6e429e043d0c69a"},"cell_type":"code","source":"missing_data_cat","execution_count":109,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8f0febf8937b5b7ef0fc19e92f4c1fb857cf1cae"},"cell_type":"code","source":"missing_col_cat=missing_data_cat['column_name'][missing_data_cat['percent']> 45].tolist()\nfor i in missing_col_cat:\n    cat_data.drop([i],axis=1,inplace=True)","execution_count":157,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"749ac0c66a595897c92f9d44bf53e0a9c3ba9072"},"cell_type":"code","source":"cat_data.shape","execution_count":158,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bd9f28440438c744072b3ff999035c50f4e2fc16"},"cell_type":"code","source":"cat_data['NAME_TYPE_SUITE']= cat_data['NAME_TYPE_SUITE'].fillna(cat_data['NAME_TYPE_SUITE'].mode()[0])","execution_count":159,"outputs":[]},{"metadata":{"_uuid":"d62cbf9934602aa5951866550f4b2d8ab464b55f"},"cell_type":"markdown","source":"In case of missing values for occupation, we can kind of predict it from their level of eduction. Let's see the distribution of the education level where the occupation was null. "},{"metadata":{"trusted":true,"_uuid":"6c958257033fd2d986d2b6a11da71cdf33ef42b0"},"cell_type":"code","source":"cat_data['NAME_EDUCATION_TYPE'][cat_data.OCCUPATION_TYPE.isnull()].value_counts()","execution_count":125,"outputs":[]},{"metadata":{"_uuid":"16663ab4a99a43897a58c8057116b4dd5aa78aaf"},"cell_type":"markdown","source":"so most of them were with the 'Seconday / secondary special' education level. Lets look at the distribution of the occupation in this education level. From the below distribution, so most of them are Laboreres. I will replace the missing value with corresponding the secondary education level with 'Laborers' "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"16d2455b6f77d52e99685efdd63c12a27082e3cd"},"cell_type":"code","source":"cat_data['OCCUPATION_TYPE'][cat_data['NAME_EDUCATION_TYPE']=='Secondary / secondary special'].value_counts()","execution_count":132,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3bca630b02694442cd1cadc936367bcc8c0fe38"},"cell_type":"code","source":"cat_data['OCCUPATION_TYPE'][cat_data['NAME_EDUCATION_TYPE']=='Secondary / secondary special'] =  cat_data['OCCUPATION_TYPE'][cat_data['NAME_EDUCATION_TYPE']=='Secondary / secondary special'].fillna(cat_data['OCCUPATION_TYPE'][cat_data['NAME_EDUCATION_TYPE']=='Secondary / secondary special'].mode()[0])\ncat_data['OCCUPATION_TYPE'][cat_data['NAME_EDUCATION_TYPE']=='Higher education'] =  cat_data['OCCUPATION_TYPE'][cat_data['NAME_EDUCATION_TYPE']=='Higher education'].fillna(cat_data['OCCUPATION_TYPE'][cat_data['NAME_EDUCATION_TYPE']=='Higher education'].mode()[0])\ncat_data['OCCUPATION_TYPE'][cat_data['NAME_EDUCATION_TYPE']=='Incomplete higher'] = cat_data['OCCUPATION_TYPE'][cat_data['NAME_EDUCATION_TYPE']=='Incomplete higher'].fillna(cat_data['OCCUPATION_TYPE'][cat_data['NAME_EDUCATION_TYPE']=='Incomplete higher'].mode()[0])\ncat_data['OCCUPATION_TYPE'][cat_data['NAME_EDUCATION_TYPE']=='Lower secondary'] = cat_data['OCCUPATION_TYPE'][cat_data['NAME_EDUCATION_TYPE']=='Lower secondary'].fillna(cat_data['OCCUPATION_TYPE'][cat_data['NAME_EDUCATION_TYPE']=='Lower secondary'].mode()[0])\ncat_data['OCCUPATION_TYPE'][cat_data['NAME_EDUCATION_TYPE']=='Academic degree'] = cat_data['OCCUPATION_TYPE'][cat_data['NAME_EDUCATION_TYPE']=='Academic degree'].fillna(cat_data['OCCUPATION_TYPE'][cat_data['NAME_EDUCATION_TYPE']=='Academic degree'].mode()[0])","execution_count":160,"outputs":[]},{"metadata":{"_uuid":"748679cf2355285905aced776c61bfbd63a76791"},"cell_type":"markdown","source":"## <a id='ot'>6. Outlier Treatment </a>"},{"metadata":{"_uuid":"1dd6daf3b600a010b2048f6fac7914aaf59a15f2"},"cell_type":"markdown","source":"So we're done with the missing value imputation. Let's clip the outliers."},{"metadata":{"trusted":true,"_uuid":"23fd5539a45765b41365e80ba015b7c54ba3e53f","collapsed":true},"cell_type":"code","source":"for i in num_data.columns:\n    ll,ul = np.percentile(num_data[i],[1,99])\n    num_data[i].ix[num_data[i]>ul] = ul\n    num_data[i].ix[num_data[i] < ll]=ll\n    ","execution_count":162,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef4872f67eee7eab6ef648953a8f212194afb790"},"cell_type":"code","source":"num_data.describe()","execution_count":163,"outputs":[]},{"metadata":{"_uuid":"88289319f5c61b0358ad0fa5fde3e52369e969f1"},"cell_type":"markdown","source":"We've to repeat the same process of missing and outlier treatment for other datasets aswell.'"},{"metadata":{"_uuid":"d42142d3d528e260006e5e7695ec0285897552e1"},"cell_type":"markdown","source":"## Visualisation and Feature engineering in the next update"},{"metadata":{"_uuid":"e29592cf4e828dbd16b69a728258b31cc654397f"},"cell_type":"markdown","source":"# Stay tuned for more and don't forget to upvote if you like it.\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"afa41e7836d39d910dde5940c64b49596e38835b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}