{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, RocCurveDisplay\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-23T18:05:44.919475Z","iopub.execute_input":"2021-06-23T18:05:44.91986Z","iopub.status.idle":"2021-06-23T18:05:44.925042Z","shell.execute_reply.started":"2021-06-23T18:05:44.919822Z","shell.execute_reply":"2021-06-23T18:05:44.923997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After looking into some discussions of the competition, I decided to focus on the application table. The reason is almost all highest important features are coming from this table. Once the baseline model is created, we could explore other tables more to improve the model.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/home-credit-default-risk/application_train.csv\")\ndf_test = pd.read_csv(\"../input/home-credit-default-risk/application_test.csv\")\n\ndf.head","metadata":{"execution":{"iopub.status.busy":"2021-06-23T18:05:44.929859Z","iopub.execute_input":"2021-06-23T18:05:44.930171Z","iopub.status.idle":"2021-06-23T18:05:49.452925Z","shell.execute_reply.started":"2021-06-23T18:05:44.930144Z","shell.execute_reply":"2021-06-23T18:05:49.451969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first step in any problem solving is to look at the data we have and collect any useful information from high level.","metadata":{}},{"cell_type":"code","source":"df.TARGET.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T18:05:49.456215Z","iopub.execute_input":"2021-06-23T18:05:49.456522Z","iopub.status.idle":"2021-06-23T18:05:49.466858Z","shell.execute_reply.started":"2021-06-23T18:05:49.456494Z","shell.execute_reply":"2021-06-23T18:05:49.465928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training data contains unbalanced target group. There are several options to balance the dataset (undersampling, oversampling, etc.), however in this case, we should first try with the raw data.","metadata":{}},{"cell_type":"code","source":"# Checking missing values in the table\ntotal = df.isnull().sum().sort_values(ascending=False)\nmissing = (df.isnull().sum() / df.isnull().count() * 100).sort_values(ascending=False)\nmissing_percent  = pd.concat([total, missing], axis=1, keys=[\"Total\", \"Missing (%)\"])\nmissing_percent.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T18:05:49.469089Z","iopub.execute_input":"2021-06-23T18:05:49.469481Z","iopub.status.idle":"2021-06-23T18:05:50.928578Z","shell.execute_reply.started":"2021-06-23T18:05:49.469446Z","shell.execute_reply":"2021-06-23T18:05:50.927686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are lots of missing values in some collumns. One option is to use imputation, the other is to leave the missing values as they are. In this kind of problem, missing values are also a valuable information, so they will be left as null.","metadata":{}},{"cell_type":"markdown","source":"Now, I will visualize several features in correlation with the target variable. Since this will only be an example, I chose the highest important features from the existing EDAs in the competition.","metadata":{}},{"cell_type":"code","source":"temp0 = df[\"EXT_SOURCE_3\"][df[\"TARGET\"]==0]\ntemp1 = df[\"EXT_SOURCE_3\"][df[\"TARGET\"]==1]\n\ncon = pd.concat([temp0, temp1], axis=1, ignore_index=False)\ncon.columns=[\"0\", \"1\"]\n\nax = con.plot.hist(by=[\"0\", \"1\"],bins=30, alpha=0.5,figsize=(15,8))\nax.set_xlabel(\"Value\")\nax.set_title(\"External Source 3 Credit Value Histogram\")\n\ndel temp0,temp1,con","metadata":{"execution":{"iopub.status.busy":"2021-06-23T18:05:50.930003Z","iopub.execute_input":"2021-06-23T18:05:50.930307Z","iopub.status.idle":"2021-06-23T18:05:51.482714Z","shell.execute_reply.started":"2021-06-23T18:05:50.930279Z","shell.execute_reply":"2021-06-23T18:05:51.481603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from the figure above, the graph for default and non-default are quite different. The non-default graph is skewed to the higher values while the default graph is skewed slightly to the smaller values. Since we don't know the actual meaning of this external source credit value, we would just use it as it is.","metadata":{}},{"cell_type":"code","source":"temp0 = df[\"DAYS_BIRTH\"][df[\"TARGET\"]==0]\ntemp1 = df[\"DAYS_BIRTH\"][df[\"TARGET\"]==1]\n\ncon = pd.concat([temp0, temp1], axis=1, ignore_index=False)\ncon.columns=[\"0\", \"1\"]\ncon = con/-365\nax = con.plot.hist(by=[\"0\", \"1\"],bins=30, alpha=0.5,figsize=(15,8))\nax.set_xlabel(\"Age\")\nax.set_title(\"Applicant's Age Histogram\")\n\ntemp_total = df[\"DAYS_BIRTH\"]/-365\nbins = [20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 100]\ntemp1 = pd.cut(temp1/-365, bins).value_counts()\ntemp_total = pd.cut(temp_total, bins).value_counts()\n\npercent = temp1.div(temp_total)*100\n\nfig, ax2 = plt.subplots(1, 1,figsize=(15,8))\nax2 = percent.plot.bar(rot=0, ax=ax2)\nax2.set_xlabel(\"Age\")\nax2.set_ylabel(\"Percentage\")\nax2.set_title(\"Defaulting Applicant's Age in Percentage of Total\")\n\ndel temp0,temp1, temp_total,con","metadata":{"execution":{"iopub.status.busy":"2021-06-23T18:05:51.484006Z","iopub.execute_input":"2021-06-23T18:05:51.484332Z","iopub.status.idle":"2021-06-23T18:05:52.112217Z","shell.execute_reply.started":"2021-06-23T18:05:51.484299Z","shell.execute_reply":"2021-06-23T18:05:52.110737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the figures comparing the applicant's age, there is no significant different between the two groups. However, when we use percentage of defaulting in the age groups, we could see a correlation between the age and the default variable. Younger applicants are more likely to default rather than older ones. This feature will most likely be useful for the model.","metadata":{}},{"cell_type":"code","source":"temp1 = df[\"NAME_EDUCATION_TYPE\"][df[\"TARGET\"]==1].value_counts()\ntemp_total = df[\"NAME_EDUCATION_TYPE\"].value_counts()\n\npercent = temp1.div(temp_total)\npercent = percent.sort_values(ascending=False)\n\nfig, ax = plt.subplots(1, 1,figsize=(15,8))\npercent.plot.bar(rot=0, ax=ax)\nax.set_xlabel(\"Education\")\nax.set_ylabel(\"Percentage\")\nax.set_title(\"Defaulting Applicant's Education in Percentage of Total\")\n\ndel temp1, temp_total, percent","metadata":{"execution":{"iopub.status.busy":"2021-06-23T18:05:52.113859Z","iopub.execute_input":"2021-06-23T18:05:52.114283Z","iopub.status.idle":"2021-06-23T18:05:52.357572Z","shell.execute_reply.started":"2021-06-23T18:05:52.114238Z","shell.execute_reply":"2021-06-23T18:05:52.356434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similar to the age figure, here we could see there is a trend in the education groups. Applicants with lower education is more likely to default in the loan rather than the ones with higher education.","metadata":{}},{"cell_type":"markdown","source":"From several feature visualization, we could see that even in higher level, there are some information that we could obtain. However, since there are already a complete EDAs in the competition discussion, I will stop the feature exploration and continue with the feature processing and model building.","metadata":{}},{"cell_type":"code","source":"def one_hot_encoder(df):\n    cat_col = [col for col in df.columns if df[col].dtype == \"object\"]\n    new_df = pd.get_dummies(df, columns=cat_col, dummy_na=True)\n    return new_df","metadata":{"execution":{"iopub.status.busy":"2021-06-23T18:05:52.359045Z","iopub.execute_input":"2021-06-23T18:05:52.359468Z","iopub.status.idle":"2021-06-23T18:05:52.365409Z","shell.execute_reply.started":"2021-06-23T18:05:52.359425Z","shell.execute_reply":"2021-06-23T18:05:52.364471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using one hot encoder, I replace categorical features with each sub-category value 0 or 1. This will elaborate each sub-category rather than clump all into 1 feature.","metadata":{}},{"cell_type":"code","source":"df_all = df.append(df_test).reset_index()\n\n# Replacing N and Y variables with 0 and 1 for binary feature with no missing values\nfor feat in [\"FLAG_OWN_CAR\", \"FLAG_OWN_REALTY\"]:\n    df_all[feat], uniques = pd.factorize(df_all[feat])\n    \n# One-Hot Encoding for the rest categorical features\ndf_all = one_hot_encoder(df_all)\n\n# There seems like to be anomalies in the data where days employed should have negative values, while some have really high positive value (365243)\ndf_all[\"DAYS_EMPLOYED\"].replace(365243, np.nan, inplace= True)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T18:05:52.367661Z","iopub.execute_input":"2021-06-23T18:05:52.367985Z","iopub.status.idle":"2021-06-23T18:05:54.318514Z","shell.execute_reply.started":"2021-06-23T18:05:52.367943Z","shell.execute_reply":"2021-06-23T18:05:54.317452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another feature engineering method is extracting features out of existing features. For example, we have the applicants income and the loan annuity, so we could combine those two features to obtain the ratio of the applicants monthly income to the loan annuity. Another example is to find the ratio between the application credit amount versus the applicants total income. We could extract much more features manually, however, we must also take consideration that more features will bring the curse of dimensionality, which may worsen the performance of the model.","metadata":{}},{"cell_type":"code","source":"# Example of extra features\ndf_all[\"AMT_ANNUITY / AMT_INCOME_TOTAL per month\"] = df_all[\"AMT_ANNUITY\"].divide(df_all[\"AMT_INCOME_TOTAL\"]/12)\ndf_all[\"AMT_CREDIT / AMT_INCOME_TOTAL\"] = df_all[\"AMT_CREDIT\"].divide(df_all[\"AMT_INCOME_TOTAL\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-23T18:05:54.319984Z","iopub.execute_input":"2021-06-23T18:05:54.32026Z","iopub.status.idle":"2021-06-23T18:05:54.329415Z","shell.execute_reply.started":"2021-06-23T18:05:54.320234Z","shell.execute_reply":"2021-06-23T18:05:54.328431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting dataframe\ndf = df_all[df_all[\"TARGET\"].notnull()]\ndf_test = df_all[df_all[\"TARGET\"].isnull()].drop([\"TARGET\"], axis=1)\n\ndel df_all","metadata":{"execution":{"iopub.status.busy":"2021-06-23T18:05:54.330746Z","iopub.execute_input":"2021-06-23T18:05:54.331047Z","iopub.status.idle":"2021-06-23T18:05:55.002838Z","shell.execute_reply.started":"2021-06-23T18:05:54.331002Z","shell.execute_reply":"2021-06-23T18:05:55.001935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Other than extracting features (adding new features), we could also reduce features (called feature selection), which removes irrelevant features from the available ones. The basic technique in feature selection is using filter method, which removes the bad features from preliminary analysis, such as correlation coefficient, Weight of Evidence (WoE), or Information Value (IV). We remove the ones with no correlation or weight to the target variable, then proceed with the left features.\nHowever, for this dataset and problem, corrcoef method is not the effective one as it actually worsen the performance of the model when I used only the 5 highest correlation features.","metadata":{}},{"cell_type":"code","source":"corrcoef = df.corr(method ='pearson')[\"TARGET\"]\ncorrcoef.sort_values().dropna().iloc[np.r_[0:5, -5:-1]]","metadata":{"execution":{"iopub.status.busy":"2021-06-23T18:05:55.004119Z","iopub.execute_input":"2021-06-23T18:05:55.004421Z","iopub.status.idle":"2021-06-23T18:06:54.966833Z","shell.execute_reply.started":"2021-06-23T18:05:55.004393Z","shell.execute_reply":"2021-06-23T18:06:54.965851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Due to time constraint, I train the model using all the features in the application table without many modification. The result showed 0.76 ROC AUC score on average in the train dataset and 0.747 score in the task submission.","metadata":{}},{"cell_type":"code","source":"# Preparing data for kfold cross-validation\ndf_y = df[\"TARGET\"]\ndf_x = df.drop([\"TARGET\"], axis=1)\n# df_x = df[[\"EXT_SOURCE_3\", \"EXT_SOURCE_2\", \"EXT_SOURCE_1\", \"DAYS_BIRTH\", \"DAYS_EMPLOYED\"]]\n\n# Removing special JSON characters so that lgbm could work\ndf_x = df_x.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\nclf = LGBMClassifier(\n            n_estimators=1000,\n            learning_rate=0.05,\n            max_depth=8,\n            min_split_gain=.01,\n            min_child_weight=2,\n            silent=-1,\n        )\n\n# Variable for probability result\npreds = np.zeros(df_test.shape[0])\n\nfold = StratifiedKFold(n_splits=10, shuffle=True)\nfor n, (train_index, test_index) in enumerate(fold.split(df_x,df_y)):\n    X_train, X_test = df_x.iloc[train_index], df_x.iloc[test_index]\n    y_train, y_test = df_y.iloc[train_index], df_y.iloc[test_index]\n    \n    clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)],\n            eval_metric=\"auc\", verbose=200, early_stopping_rounds=200)\n    \n    \n    preds += clf.predict_proba(df_test, num_iteration=clf.best_iteration_)[:, 1] / fold.n_splits\n    print(\"ROC AUC score of fold \" + str(n+1) + \" is: \" + str(roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])))\n\nfpr, tpr, _ = roc_curve(y_test, clf.predict_proba(X_test)[:, 1])\nroc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n    \ndf_test['TARGET'] = preds\ndf_test[['SK_ID_CURR', 'TARGET']].to_csv(\"sub.csv\", index= False)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T18:06:54.968408Z","iopub.execute_input":"2021-06-23T18:06:54.968804Z","iopub.status.idle":"2021-06-23T18:14:12.562047Z","shell.execute_reply.started":"2021-06-23T18:06:54.968764Z","shell.execute_reply":"2021-06-23T18:14:12.561387Z"},"trusted":true},"execution_count":null,"outputs":[]}]}