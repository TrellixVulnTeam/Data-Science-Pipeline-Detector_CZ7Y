{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#setting pdandas to display max rows and max columns\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#checking the running time\ntrain_data = pd.read_csv(\"/kaggle/input/home-credit-default-risk/application_train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/home-credit-default-risk/application_test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking the memory usage and other information about data\nprint(train_data.info())\nprint(test_data.info())","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking the unique data types\nprint(train_data.dtypes.unique())\nprint(test_data.dtypes.unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lets check if downcasting the dtypes changes the memory usage in our case","metadata":{}},{"cell_type":"code","source":"#Downcast types to reduce memory usage.\ndef downcast_dtypes(df):\n    _start = df.memory_usage(deep = True).sum()/1024**2\n    float_cols = [c for c in df if df[c].dtype == \"float32\"]\n    int_cols = [c for c in df if df[c].dtype == \"int8\"]\n    _end = df.memory_usage(deep = True).sum()/1024**2\n    saved = (_start - _end)/ _start*100\n    print(f\"saved {saved:.2f}%\")\n    return df\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = downcast_dtypes(train_data)\ntest = downcast_dtypes(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.info())\nprint(test.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.shape)\nprint(test_data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"checking no. of columns in train_data with any null values: \",train_data.isnull().any().sum())\nprint(\"checking no. of columns in train_data with all null values: \",  train_data.isnull().all().sum())\nprint(\"checking no. of rows in train_data with all null values: \", train_data.isnull().all(axis = 1).sum())\nprint(\"list of columns in train_data with null values: \", train_data.columns[train_data.isna().any()].tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"checking no. of columns in test_data with any null values: \",test_data.isnull().any().sum())\nprint(\"checking no. of columns in test_data with all null values: \",  test_data.isnull().all().sum())\nprint(\"checking no. of rows in test_data with all null values: \", test_data.isnull().all(axis = 1).sum())\nprint(\"list of columns in test_data with null values: \", test_data.columns[test_data.isna().any()].tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Treating negative values","metadata":{}},{"cell_type":"code","source":"train_data['DAYS_BIRTH'] = train_data['DAYS_BIRTH'].abs()\ntrain_data['DAYS_EMPLOYED'] = train_data['DAYS_EMPLOYED'].abs()\ntrain_data['DAYS_REGISTRATION'] = train_data['DAYS_REGISTRATION'].abs()\ntrain_data['DAYS_ID_PUBLISH'] = train_data['DAYS_ID_PUBLISH'].abs()\ntrain_data['DAYS_LAST_PHONE_CHANGE'] = train_data['DAYS_LAST_PHONE_CHANGE'].abs()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['DAYS_BIRTH'] = test_data['DAYS_BIRTH'].abs()\ntest_data['DAYS_EMPLOYED'] = test_data['DAYS_EMPLOYED'].abs()\ntest_data['DAYS_REGISTRATION'] = test_data['DAYS_REGISTRATION'].abs()\ntest_data['DAYS_ID_PUBLISH'] = test_data['DAYS_ID_PUBLISH'].abs()\ntest_data['DAYS_LAST_PHONE_CHANGE'] = test_data['DAYS_LAST_PHONE_CHANGE'].abs()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking the percentage of null values in the train_data columns having null values.\nx=train_data.isnull().sum()\ny=(train_data.isnull().sum()/train_data.shape[0])*100\nz={'Number of missing values':x,'Percentage of missing values':y}\ndf=pd.DataFrame(z,columns=['Number of missing values','Percentage of missing values'])\ndf.sort_values(by='Percentage of missing values',ascending=False).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking the percentage of null values in the test_data columns having null values.\nx=test_data.isnull().sum()\ny=(test_data.isnull().sum()/test_data.shape[0])*100\nz={'Number of missing values':x,'Percentage of missing values':y}\ndf=pd.DataFrame(z,columns=['Number of missing values','Percentage of missing values'])\ndf.sort_values(by='Percentage of missing values',ascending=False).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#assigning the null values of train_data to variables \nval= train_data.isnull().sum()\n#taking the null values greater then 30%\ndrop_column = val[val.values >= 92253]\n##checking the length columns having null values more then 30%\nprint(len(drop_column))\n#checking the list of columns to be dropped\nprint(drop_column.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#assigning the null values of test_data to variables \nval= test_data.isnull().sum()\n#taking the null values greater then 30%\ndrop_column = val[val.values >= 14624]\n##checking the length columns having null values more then 30%\nprint(len(drop_column))\n#checking the list of columns to be dropped\nprint(drop_column.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### before dropping the columns check if any column is under 50% of null values and can still impact the target. We found that column to be \"OCCUPATION_TYPE\".\n### We can see from above table that \"OCCUPATION_TYPE\" have just 31% of the miss data in train_data and 32% in test_data. we know what is the importance of job type while asking for loan. So we must consider this column.","metadata":{}},{"cell_type":"code","source":"#Dropping the null values columns having more then 30% of missing values except \"OCCUPATION_TYPE\" in train_data.\ntrain_data = train_data.drop(['OWN_CAR_AGE', 'EXT_SOURCE_1', 'APARTMENTS_AVG',\n       'BASEMENTAREA_AVG', 'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG',\n       'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG', 'FLOORSMAX_AVG',\n       'FLOORSMIN_AVG', 'LANDAREA_AVG', 'LIVINGAPARTMENTS_AVG',\n       'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG',\n       'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE',\n       'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ELEVATORS_MODE',\n       'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE',\n       'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE',\n       'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI',\n       'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI',\n       'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI',\n       'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI',\n       'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI', 'FONDKAPREMONT_MODE',\n       'HOUSETYPE_MODE', 'TOTALAREA_MODE', 'WALLSMATERIAL_MODE',\n       'EMERGENCYSTATE_MODE'] , axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dropping the null values columns having more then 30% of missing values except \"OCCUPATION_TYPE\" in test_data.\ntest_data = test_data.drop(['OWN_CAR_AGE', 'EXT_SOURCE_1', 'APARTMENTS_AVG',\n       'BASEMENTAREA_AVG', 'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG',\n       'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG', 'FLOORSMAX_AVG',\n       'FLOORSMIN_AVG', 'LANDAREA_AVG', 'LIVINGAPARTMENTS_AVG',\n       'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG',\n       'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE',\n       'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ELEVATORS_MODE',\n       'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE',\n       'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE',\n       'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI',\n       'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI',\n       'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI',\n       'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI',\n       'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI', 'FONDKAPREMONT_MODE',\n       'HOUSETYPE_MODE', 'TOTALAREA_MODE', 'WALLSMATERIAL_MODE',\n       'EMERGENCYSTATE_MODE'] , axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.shape)\nprint(test_data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"checking the datatypes in our train_data: \")\nprint(train_data.dtypes.value_counts())\nprint('\\n')\nprint(\"checking the datatypes in our test_data: \")\nprint(test_data.dtypes.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"checking the maximum value of null values in a row of train_data\")\nprint(train_data.isnull().sum(axis=1).sort_values(ascending  = False).head(1))\nprint('\\n')\nprint(\"checking the maximum value of null values in a row of test_data\")\nprint(test_data.isnull().sum(axis=1).sort_values(ascending  = False).head(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"checking the unique dtypes of null values in train_data: \")\nprint(train_data.dtypes[train_data.isnull().any()].unique())\nprint(\"\\n\")\nprint(\"checking the unique dtypes of null values in test_data: \")\nprint(test_data.dtypes[test_data.isnull().any()].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### In the above cell we can see that null values in our dataset are of two data_types . The object can be easily clasiified as categorical data but in numeric that we can still have ordinal data which is again a categorical data. So, we must seperate the ordinal data from numeric data","metadata":{"execution":{"iopub.status.busy":"2021-06-27T11:23:46.335927Z","iopub.execute_input":"2021-06-27T11:23:46.336249Z","iopub.status.idle":"2021-06-27T11:23:46.699314Z","shell.execute_reply.started":"2021-06-27T11:23:46.336222Z","shell.execute_reply":"2021-06-27T11:23:46.698491Z"}}},{"cell_type":"code","source":"print(train_data.dtypes[train_data.isnull().any()])\nprint(len(train_data.dtypes[train_data.isnull().any()]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_data.dtypes[test_data.isnull().any()])\nprint(len(test_data.dtypes[test_data.isnull().any()]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Parsing","metadata":{}},{"cell_type":"code","source":"train_categorical = train_data.select_dtypes(include = ['object'])\ntest_categorical = test_data.select_dtypes(include = ['object'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_categorical.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_categorical.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"null_train = train_data[train_data.columns[train_data.isnull().any()]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"null_test = test_data[test_data.columns[test_data.isnull().any()]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def value_count():\n    for i in range(len(null_train.columns)):\n        if len(null_train.iloc[:,i].value_counts()) <= 100:\n            print(null_train.columns[[i]])\nvalue_count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def value_count():\n    for i in range(len(null_test.columns)):\n        if len(null_test.iloc[:,i].value_counts()) <= 100:\n            print(null_test.columns[[i]])\nvalue_count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filling numeric null values","metadata":{}},{"cell_type":"code","source":"print(train_data['AMT_ANNUITY'].describe())\nprint('\\n')\nprint(test_data['AMT_ANNUITY'].describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(plt.hist(train_data['AMT_ANNUITY']))\nprint(plt.hist(test_data['AMT_ANNUITY']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['AMT_ANNUITY'] = train_data['AMT_ANNUITY'].fillna(train_data['AMT_ANNUITY'].median())\ntest_data['AMT_ANNUITY'] = test_data['AMT_ANNUITY'].fillna(test_data['AMT_ANNUITY'].median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['AMT_GOODS_PRICE'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(train_data['AMT_GOODS_PRICE'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['AMT_GOODS_PRICE'].value_counts().head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"median: \",train_data['AMT_GOODS_PRICE'].median())\nprint(\"mode: \",train_data['AMT_GOODS_PRICE'].mode()[0])\nprint(\"mean: \",train_data['AMT_GOODS_PRICE'].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['AMT_GOODS_PRICE'] = train_data['AMT_GOODS_PRICE'].fillna(train_data['AMT_GOODS_PRICE'].median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['DAYS_LAST_PHONE_CHANGE'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['DAYS_LAST_PHONE_CHANGE'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['DAYS_LAST_PHONE_CHANGE'].value_counts().head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(train_data['DAYS_LAST_PHONE_CHANGE'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data['DAYS_LAST_PHONE_CHANGE'].mode()[0])\nprint(train_data['DAYS_LAST_PHONE_CHANGE'].median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['DAYS_LAST_PHONE_CHANGE'] = train_data['DAYS_LAST_PHONE_CHANGE'].fillna(train_data['DAYS_LAST_PHONE_CHANGE'].median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['EXT_SOURCE_2'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['EXT_SOURCE_2'].value_counts().head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data['EXT_SOURCE_2'].mean())\nprint(train_data['EXT_SOURCE_2'].median())\nprint(train_data['EXT_SOURCE_2'].mode()[0])\nprint(test_data['EXT_SOURCE_2'].mean())\nprint(test_data['EXT_SOURCE_2'].median())\nprint(test_data['EXT_SOURCE_2'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(train_data['EXT_SOURCE_2'])\nplt.hist(test_data['EXT_SOURCE_2'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['EXT_SOURCE_2'] = train_data['EXT_SOURCE_2'].fillna(train_data['EXT_SOURCE_2'].median())\ntest_data['EXT_SOURCE_2'] = test_data['EXT_SOURCE_2'].fillna(test_data['EXT_SOURCE_2'].median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data['EXT_SOURCE_3'].describe())\nprint('\\n')\nprint(test_data['EXT_SOURCE_3'].describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data['EXT_SOURCE_3'].median())\nprint(train_data['EXT_SOURCE_3'].mean())\nprint(train_data['EXT_SOURCE_3'].mode())\nprint(test_data['EXT_SOURCE_3'].median())\nprint(test_data['EXT_SOURCE_3'].mean())\nprint(test_data['EXT_SOURCE_3'].mode())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(train_data['EXT_SOURCE_3'])\nplt.hist(test_data['EXT_SOURCE_3'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['EXT_SOURCE_3'] = train_data['EXT_SOURCE_3'].fillna(train_data['EXT_SOURCE_3'].median())\ntest_data['EXT_SOURCE_3'] = test_data['EXT_SOURCE_3'].fillna(test_data['EXT_SOURCE_3'].median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filling Categorical null values using function","metadata":{}},{"cell_type":"code","source":"def filling_TrainDataCategoricalNullvalues():\n    for i in range(len(null_train.columns)):\n        if len(null_train.iloc[:,i].value_counts()) <= 100:\n            column = null_train.columns[i]\n            train_data[column] = train_data[column].fillna(train_data[column].mode()[0])\nfilling_TrainDataCategoricalNullvalues()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def filling_TestDataCategoricalNullvalues():\n    for i in range(len(null_test.columns)):\n        if len(null_test.iloc[:,i].value_counts()) <= 100:\n            column = null_test.columns[i]\n            test_data[column] = test_data[column].fillna(test_data[column].mode()[0])\nfilling_TestDataCategoricalNullvalues()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.isnull().sum().sum())\nprint(test_data.isnull().sum().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### checking of duplicate rows and columns","metadata":{}},{"cell_type":"code","source":"#checking for duplicate rows\ntrain_duplicate = train_data[train_data.duplicated()]\ntest_duplicate = test_data[test_data.duplicated()]\ntrain_duplicate.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_duplicate.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking for duplicate columns\ndef getDuplicateColumns(df):\n  \n    # Create an empty set\n    duplicateColumnNames = set()\n      \n    # Iterate through all the columns \n    # of dataframe\n    for x in range(df.shape[1]):\n          \n        # Take column at xth index.\n        col = df.iloc[:, x]\n          \n        # Iterate through all the columns in\n        # DataFrame from (x + 1)th index to\n        # last index\n        for y in range(x + 1, df.shape[1]):\n              \n            # Take column at yth index.\n            otherCol = df.iloc[:, y]\n              \n            # Check if two columns at x & y\n            # index are equal or not,\n            # if equal then adding \n            # to the set\n            if col.equals(otherCol):\n                duplicateColumnNames.add(df.columns.values[y])\n                  \n    # Return list of unique column names \n    # whose contents are duplicates.\n    return list(duplicateColumnNames)\ngetDuplicateColumns(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"getDuplicateColumns(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoding categorical values\n### For categorical data we perform three kinds of encoding . Ordinal enconding , OneHot encoding. These encodings perfromed looking into the data types. lets say we have a categorical data which is related and having rank then we have to perform ordinal encoding. If the categorical data is not displaying any rank order among it self then OneHot encoding can be used. While using OneHot encoding dummy variable trap is taken care.","metadata":{}},{"cell_type":"code","source":"def value_count():\n    for i in range(len(train_categorical.columns)):\n        print(train_categorical.columns[[i]])\n        print(len(train_categorical.iloc[:,i].value_counts()))\n        print('\\n')\n            \nvalue_count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def value_count():\n    for i in range(len(test_categorical.columns)):\n        print(test_categorical.columns[[i]])\n        print(len(test_categorical.iloc[:,i].value_counts()))\n        print('\\n')\n            \nvalue_count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data['NAME_CONTRACT_TYPE'].unique())\nCT = ['Cash loans', 'Revolving loans']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data['CODE_GENDER'].unique())\nprint(test_data['CODE_GENDER'].unique())\ntrain_CG = ['M', 'F' ,'XNA']\ntest_CG = ['M', 'F']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data['FLAG_OWN_CAR'].unique())\nC = ['N', 'Y']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data['FLAG_OWN_REALTY'].unique())\nR = ['N', 'Y']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['NAME_TYPE_SUITE'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data['NAME_INCOME_TYPE'].unique())\nprint(test_data['NAME_INCOME_TYPE'].unique())\ntrain_IT = ['Unemployed', 'Student', 'Pensioner','Working','Commercial associate','State servant','Businessman','Maternity leave' ]\ntest_IT = ['Unemployed', 'Student', 'Pensioner','Working','Commercial associate','State servant','Businessman' ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data['NAME_EDUCATION_TYPE'].unique())\nET = ['Lower secondary','Secondary / secondary special','Incomplete higher','Higher education','Academic degree']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data['NAME_FAMILY_STATUS'].unique())\nprint(test_data['NAME_FAMILY_STATUS'].unique())\ntrain_FS = ['Widow','Separated','Single / not married','Married','Civil marriage','Unknown',]\ntest_FS = ['Widow','Separated','Single / not married','Married','Civil marriage']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['NAME_HOUSING_TYPE'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['OCCUPATION_TYPE'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['WEEKDAY_APPR_PROCESS_START'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['ORGANIZATION_TYPE'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OE_train = OrdinalEncoder(categories = [CT,train_CG, C, R, train_IT, ET, train_FS ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OE_train.fit(train_data[['NAME_CONTRACT_TYPE','CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY',\\\n                         'NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[['NAME_CONTRACT_TYPE','CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY','NAME_INCOME_TYPE',\\\n            'NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS']] =\\\nOE_train.transform(train_data[['NAME_CONTRACT_TYPE','CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY',\\\n                              'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OE_test = OrdinalEncoder(categories = [CT,test_CG, C, R, test_IT, ET, test_FS ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OE_test.fit(test_data[['NAME_CONTRACT_TYPE','CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY',\\\n                         'NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data[['NAME_CONTRACT_TYPE','CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY',\\\n                         'NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS']] =\\\nOE_test.transform(test_data[['NAME_CONTRACT_TYPE','CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY',\\\n                         'NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data[['NAME_CONTRACT_TYPE','CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY',\\\n                         'NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[['NAME_CONTRACT_TYPE','CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY',\\\n                         'NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Applying one hot encoding.\ntrain_data = pd.get_dummies(train_data , drop_first = True)\ntest_data = pd.get_dummies(test_data , drop_first = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train_data['TARGET']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.value_counts().plot.bar()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting our dataset","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(train_data.drop(['TARGET'], axis = 1),\\\n                                            train_data['TARGET'],test_size = 0.2,random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Balancing Training Data","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smote = SMOTE()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain_smote , ytrain_smote = smote.fit_resample(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Before somte: \" , Counter(y_train))\nprint(\"After smote: \", Counter(ytrain_smote))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Applying Feature Scaling","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc = StandardScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain_smote1 = pd.DataFrame(sc.fit_transform(xtrain_smote))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain_smote1.columns = xtrain_smote.columns.values\nxtrain_smote1.index = xtrain_smote.index.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain_smote = xtrain_smote1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain_smote.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test1 = pd.DataFrame(sc.transform(x_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test1.columns = x_test.columns.values\nx_test1.index = x_test.index.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test = x_test1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data1 = pd.DataFrame(sc.transform(test_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data1.columns = test_data.columns.values\ntest_data1.index = test_data.index.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = test_data1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Bulding","metadata":{}},{"cell_type":"code","source":"import warnings\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning,\n                        module=\"sklearn\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_c = LogisticRegression()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_c.fit(xtrain_smote, ytrain_smote)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_prd = lr_c.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#evaluating results\n\nfrom sklearn.metrics import confusion_matrix , accuracy_score, f1_score,\\\nprecision_score, recall_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_prd)\ncm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(accuracy_score(y_test, y_prd))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f1_score(y_test, y_prd))\nprint(precision_score(y_test, y_prd))\nprint(recall_score(y_test, y_prd))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cm = pd.DataFrame(cm , index = (0,1), columns = (0,1))\nplt.figure(figsize=(10,7))\nsns.set(font_scale = 1.4)\nsns.heatmap(df_cm , annot = True , fmt = 'g')\nprint(\"test_data accuracy: %0.4f\" % accuracy_score(y_test , y_prd))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Applying K_Fold cross validation","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.model_selection import cross_val_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies = cross_val_score(estimator = lr_c , \n                             X = xtrain_smote, \n                             y = ytrain_smote,\n                             cv = 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature selection\n### Information gain - mutual information in classification","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking the memory usage and other information about data\nprint(xtrain_smote.info())\nprint(xtrain_smote.dtypes.unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmutual_info = mutual_info_classif(xtrain_smote , ytrain_smote)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mutual_info = pd.Series(mutual_info)\nmutual_info.index = xtrain_smote.columns\nmutual_info.sort_values(ascending = False).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"select_hundred_columns = SelectKBest(mutual_info_classif, k = 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nselect_hundred_columns.fit(xtrain_smote, ytrain_smote)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain_smote[xtrain_smote.columns[select_hundred_columns.get_support()]].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nlr_c.fit(xtrain_smote[xtrain_smote.columns[select_hundred_columns.get_support()]],ytrain_smote)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_prediction2 = lr_c.predict(x_test[x_test.columns[select_hundred_columns.get_support()]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_prediction2,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter optimization \n### Using GridSearchCv","metadata":{}},{"cell_type":"code","source":"params = lr_c.get_params()\nparams","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = [    \n    {'penalty' : ['l1', 'l2'],\n    'C' : [1,2],\n    'solver' : ['lbfgs','liblinear'],\n    'max_iter' : [100,200]\n    }\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Here I have considered very less parameter optimization just to reduce the very long time taken by GridSearchCV to perform the tuning operation. I have just displayed how optimization works focusing less on the result.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = GridSearchCV(lr_c, param_grid = param_grid, cv = 2, verbose=True, n_jobs=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nbest_clf = clf.fit(xtrain_smote[xtrain_smote.columns[select_hundred_columns.get_support()]],ytrain_smote)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_clf.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bulding our model with best parameter values","metadata":{}},{"cell_type":"code","source":"classification = LogisticRegression(C=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classification.fit(xtrain_smote[xtrain_smote.columns[select_hundred_columns.get_support()]],ytrain_smote)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prd = classification.predict(x_test[x_test.columns[select_hundred_columns.get_support()]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(prd, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Applying our test data values on the final model.","metadata":{}},{"cell_type":"code","source":"testprd = classification.predict(test_data[test_data.columns[select_hundred_columns.get_support()]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_prd = pd.DataFrame(testprd)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_prd.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}