{"cells":[{"metadata":{"_uuid":"f87b3a816c6e69060a06c7aa6c8713ada8399442"},"cell_type":"markdown","source":"[Automated Model Tuning\n](https://www.kaggle.com/willkoehrsen/automated-model-tuning)\n\nThank Will Koehresn for the great spirit of sharing\n\n一如既往的学习一下Will Koehresn的代码，Kaggle的分享精神实在太好了，希望伪汉化工作真的很渣，旨在督促自己学习，各位还是尽量看原文。\n\n\n## 关于 LightGBm\n\n梯度提升算法最近成为了顶级机器学习模型之一，特别适用于结构化数据。简单的说模型的思路，不同于随机森林的各个子模型之间相互独立，训练过程也相互平行，GBM每个新的子模型都会借鉴之前模型的错误分类，每次增加的子模型都能够最大降低模型损失。\n\n因为模型结构较为复杂，所以参数也就比较多，虽然原kernel基本上是直接对所有参数进行搜索，但是我觉得可以参考一下[scikit-learn 梯度提升树(GBDT)调参小结](http://www.cnblogs.com/pinard/p/6143927.html)的调参思路,主要是处理框架参数，和弱学习器参数的不同策略。\n\n[参数](http://lightgbm.apachecn.org/cn/latest/Parameters.html), 也有中文文档可以详细了解一下相关参数含义。\n\n## 进行调参的四个步骤\n\n1. 确定目标函数\n2. 定义搜寻范围\n3. 选择调参算法\n4. 输出结果\n\n## 贝叶斯优化调参初步\n\n贝叶斯优化调参是不同于GridSearch和随机调参的另外一种调参思路，最重要的区别是贝叶斯优化是一种 informed methods，也就是利用已经尝试过的调参结果，来决定下一组尝试的参数。这样的好处是是，在更有可能得到最优结果的区域，能够进行更多的尝试。\n\n## 关于Hyperopt\n\n可以进行贝叶斯优化的开源库有很多，Spearmint (基于高斯过程) and SMAC (基于随机森林)。Hyperopt是基于Tree Parzen Estimator算法构建代替函数来选择下一代的参数。作者选择这个是因为文档比较完整。想尽量把用起来，没有纠结算法细节，以后有空看看。。\n\n## 开始试验\n\n基本的实验设定\n\n- 5折交叉验证\n- Early Stopping 为 100\n- 选取10000个样本为训练集，6000为试验集记性实验，这样可以快速迭代实验，学习调参方法，最后看看调好的参数是否能够迁移到整个数据集上\n\n\n\n\n\n\n\n\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ec4117d7204e06e25f750288c48c66be02a7500e"},"cell_type":"code","source":"# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Modeling\nimport lightgbm as lgb\n\n# Evaluation of the model\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['font.size'] = 18\n%matplotlib inline\n\n# Governing choices for search\nN_FOLDS = 5\nMAX_EVALS = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94fd2ddbfe5fb71cf95a548ff7e3eef13396309a","collapsed":true},"cell_type":"code","source":"features = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\n\n# Sample 16000 rows (10000 for training, 6000 for testing)\nfeatures = features.sample(n = 16000, random_state = 42)\n\n# Only numeric features\nfeatures = features.select_dtypes('number')\n\n# Extract the labels\nlabels = np.array(features['TARGET'].astype(np.int32)).reshape((-1, ))\nfeatures = features.drop(columns = ['TARGET', 'SK_ID_CURR'])\n\n# Split into training and testing data\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 6000, random_state = 42)\n\nprint('Train shape: ', train_features.shape)\nprint('Test shape: ', test_features.shape)\n\ntrain_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"20e7184bee30e82b8fbb69215a9e5fa3ee3b66c4"},"cell_type":"code","source":"model = lgb.LGBMClassifier(random_state=50)\n\n# Training set\ntrain_set = lgb.Dataset(train_features, label = train_labels)\ntest_set = lgb.Dataset(test_features, label = test_labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5379543a161cafa280ba27e59781582731842f30"},"cell_type":"markdown","source":"#### 1 用原始参数训练Baseline Model，用来比较模型调参的效果"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"420922342510a1a3ca68240160ab63052ea08ec2"},"cell_type":"code","source":"# Default hyperparamters\nhyperparameters = model.get_params()\n\n# Using early stopping to determine number of estimators.\ndel hyperparameters['n_estimators']\n\n# Perform cross validation with early stopping\ncv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = N_FOLDS, metrics = 'auc', \n           early_stopping_rounds = 100, verbose_eval = False, seed = 42)\n\n# Highest score\nbest = cv_results['auc-mean'][-1]\n\n# Standard deviation of best score\nbest_std = cv_results['auc-stdv'][-1]\n\nprint('The maximium ROC AUC in cross validation was {:.5f} with std of {:.5f}.'.format(best, best_std))\nprint('The ideal number of iterations was {}.'.format(len(cv_results['auc-mean'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0a2ad89689f98312c7c23634a8dfb6ba4c593cc7"},"cell_type":"code","source":"model.n_estimators = len(cv_results['auc-mean'])\n\nmodel.fit(train_features, train_labels)\npreds = model.predict_proba(test_features)[:, 1]\nbaseline_auc = roc_auc_score(test_labels, preds)\n\nprint('The baseline model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21d52f0c0a9ac0ad4a04e0568f1d2276ff0f11ac"},"cell_type":"markdown","source":"#### 2 构建目标函数"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6e9ac6f07ffcad2cf965d0441327a064adc5319f"},"cell_type":"code","source":"import csv\nfrom hyperopt import STATUS_OK\nfrom timeit import default_timer as timer\n\ndef objective(hyperparameters):\n    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Optimization.\n       Writes a new line to `outfile` on every iteration\"\"\"\n    \n    # 确定是哪一组验证集\n    global ITERATION\n    \n    ITERATION += 1\n    \n    # 使用 early stopping 确定弱学习器的数量\n    if 'n_estimators' in hyperparameters:\n        del hyperparameters['n_estimators']\n    \n    # 因为设定参数 不同的boosting type需要不同的subsample参数，所以重新构造调参范围\n    subsample = hyperparameters['boosting_type'].get('subsample', 1.0)\n    hyperparameters['boosting_type'] = hyperparameters['boosting_type']['boosting_type']\n    hyperparameters['subsample'] = subsample\n    \n    # 确认整数参数的情况\n    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n        hyperparameters[parameter_name] = int(hyperparameters[parameter_name])\n        \n    start = timer()\n    \n    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = N_FOLDS, \n                        early_stopping_rounds = 100, metrics = 'auc', seed = 50)\n\n    run_time = timer() - start\n    \n    best_score = cv_results['auc-mean'][-1]\n    \n    # 损失函数 越小越好\n    loss = 1 - best_score\n    \n    # 返回最优结果的 Boosting 轮数\n    n_estimators = len(cv_results['auc-mean'])\n    \n    hyperparameters['n_estimators'] = n_estimators\n\n    # Write to the csv file ('a' means append)\n    of_connection = open(OUT_FILE, 'a')\n    writer = csv.writer(of_connection)\n    writer.writerow([loss, hyperparameters, ITERATION, run_time, best_score])\n    of_connection.close()\n    \n    # Dictionary with information for evaluation\n    return {'loss': loss, 'hyperparameters': hyperparameters, 'iteration': ITERATION,\n            'train_time': run_time, 'status': STATUS_OK}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7754160f9582d357be1830e5c338271caa94be2"},"cell_type":"markdown","source":"#### 2 定义搜寻范围"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fc7a673ff298dcf9e2c9b7431759f681b444609f"},"cell_type":"code","source":"from hyperopt import hp\nfrom hyperopt.pyll.stochastic import sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6d4cdfc61ae7e0cab69354442a91385449301e1f"},"cell_type":"code","source":"learning_rate = {'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.2))}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6264e7851ded5e03f66f38c1202680b776684a3d"},"cell_type":"markdown","source":"- log 均匀分布"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9472f30728d89d8a725b3198db6f457df8047d60"},"cell_type":"code","source":"learning_rate_dist = []\n\n# Draw 10000 samples from the learning rate domain\nfor _ in range(10000):\n    learning_rate_dist.append(sample(learning_rate)['learning_rate'])\n    \nplt.figure(figsize = (8, 6))\nsns.kdeplot(learning_rate_dist, color = 'red', linewidth = 2, shade = True);\nplt.title('Learning Rate Distribution', size = 18); plt.xlabel('Learning Rate', size = 16); plt.ylabel('Density', size = 16);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94c80fbcd950a4f7f10197452defb43040551a70"},"cell_type":"markdown","source":"以上是采样一万个学习率的分布情况，因为是log均匀分布，所以在较小的地方分布比较密集"},{"metadata":{"_uuid":"72a4bbddfebd2ac5666309bd850d61419bb51f8f"},"cell_type":"markdown","source":"- 均匀分布"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"869ac3add2cee696cea1a51e42d5655366ab3c60"},"cell_type":"code","source":"num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\nnum_leaves_dist = []\n\nfor _ in range(10000):\n    num_leaves_dist.append(sample(num_leaves)['num_leaves'])\n    \nplt.figure(figsize = (8, 6))\nsns.kdeplot(num_leaves_dist, linewidth = 2, shade = True);\nplt.title('Number of Leaves Distribution', size = 18); plt.xlabel('Number of Leaves', size = 16); plt.ylabel('Density', size = 16);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1486f329e646d68bdf0303e8f0322221562eb1ee"},"cell_type":"markdown","source":"- 相互影响的参数处理"},{"metadata":{"_uuid":"3805966322248f1f8095d1e6b7d85856504fb970"},"cell_type":"markdown","source":"因为如果设定了boosting_type为goss，subsample就只能为1，所以要把两个参数放到一起设定"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"546dbc2670bffc273507c3a0f06c57ae74ff0c47"},"cell_type":"code","source":"boosting_type = {'boosting_type': hp.choice('boosting_type', \n                                            [{'boosting_type': 'gbdt', 'subsample': hp.uniform('subsample', 0.5, 1)}, \n                                             {'boosting_type': 'dart', 'subsample': hp.uniform('subsample', 0.5, 1)},\n                                             {'boosting_type': 'goss', 'subsample': 1.0}])}\n\nhyperparams = sample(boosting_type)\nhyperparams","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2a487ee5f36eaf4b9a0fe7816aafc9de417e74c"},"cell_type":"markdown","source":"- 其他调参范围的完整设定"},{"metadata":{"_uuid":"a80b55cd9435226d8b9218e0ef4be3c903bf0c7a"},"cell_type":"markdown","source":"[官方文档中关于调参范围各种设置的方式](https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions)，只是上面两种是用的最多的。"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f6225ce8565742cb17023cb5b675c62c312e0cd8"},"cell_type":"code","source":"space = {\n    'boosting_type': hp.choice('boosting_type', \n                                            [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n                                             {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n                                             {'boosting_type': 'goss', 'subsample': 1.0}]),\n    'num_leaves': hp.quniform('num_leaves', 20, 150, 1),\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),\n    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n    'is_unbalance': hp.choice('is_unbalance', [True, False]),\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbdf59b8907d3ca0e3244c4c173532eff76e8606"},"cell_type":"markdown","source":"**对调参范围进行抽样的方式**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0ffe41929c86c7d5e4a13833f4de5f2405f15900"},"cell_type":"code","source":"x = sample(space)\n\n# 因为之前对 相互影响的参数进行了特殊设置，现在也需要特殊处理一下，让整个参数处于一个层级\nsubsample = x['boosting_type'].get('subsample', 1.0)\nx['boosting_type'] = x['boosting_type']['boosting_type']\nx['subsample'] = subsample\n\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d83e6b80c846896cb1064d831a6aa8026862994d"},"cell_type":"code","source":"x = sample(space)\nsubsample = x['boosting_type'].get('subsample', 1.0)\nx['boosting_type'] = x['boosting_type']['boosting_type']\nx['subsample'] = subsample\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"377378a53eb1e3da4fe20f4d5bcf943e1aa25953"},"cell_type":"code","source":"# 创建一个文件，用来存调参结果\nOUT_FILE = 'bayes_test.csv'\nof_connection = open(OUT_FILE, 'w')\nwriter = csv.writer(of_connection)\n\nITERATION = 0\n\n# 写入列名\nheaders = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score']\nwriter.writerow(headers)\nof_connection.close()\n\n# 测试目标函数\nresults = objective(sample(space))\nprint('The cross validation loss = {:.5f}.'.format(results['loss']))\nprint('The optimal number of estimators was {}.'.format(results['hyperparameters']['n_estimators']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a14e25efc37d862943246f89639899ab00315a8c"},"cell_type":"markdown","source":"#### 3 调参算法的选择 Optimization Algorithm"},{"metadata":{"_uuid":"fc175fe6a003bd1cba74d4c6eb2cea119eee2d65"},"cell_type":"markdown","source":"选择调参算法，其实就是选择了一种构建函数模型的方式。然后根据这个算法可以算出下一步尝试的参数，哪个后验概率最大。\n\n以下是两个作者推荐的文档，我反正暂时没看。。。\n\n[技术细节](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)\n\n[概念性的解释](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f)，这个kernel文档作者自己的博文"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"802c4e1be419cbe1733ca93674c4c1121114c76b"},"cell_type":"code","source":"from hyperopt import tpe\n\n# Create the algorithm\ntpe_algorithm = tpe.suggest","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf140471495509698682307960967cea05523a4f"},"cell_type":"markdown","source":"#### 4 结果记录"},{"metadata":{"_uuid":"71b06edffea717145596754401a69b5f269673af"},"cell_type":"markdown","source":"Hyperopt 提供了记录结果的工具，但是我们自己记录，可以方便实时监控。"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"52b6fcbcef21f3a3c7906d3f0c1be328a3d56fd5"},"cell_type":"code","source":"from hyperopt import Trials\n\n# Record results\ntrials = Trials()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1bfce3e4eb7b58e8b772eca7b72744098a4c7b11"},"cell_type":"code","source":"OUT_FILE = 'bayes_test.csv'\nof_connection = open(OUT_FILE, 'w')\nwriter = csv.writer(of_connection)\n\nITERATION = 0\n\nheaders = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score']\nwriter.writerow(headers)\nof_connection.close()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22ca8f080c771fe46e6904690898924cf0c4f015"},"cell_type":"markdown","source":"## 调参实践"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"27418b357d4c589f9ef8e2c0e0a2cca3c31b05a6"},"cell_type":"code","source":"from hyperopt import fmin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5e12e432e12d06d35144f614867f8aed325c2dfa"},"cell_type":"code","source":"global  ITERATION\n\nITERATION = 0\n\n# 这一步就是调参的运行过程\nbest = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials,\n            max_evals = MAX_EVALS)\n\nbest","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"183b0802de03fd3c70ec14aeb07c8019f1412a64"},"cell_type":"markdown","source":"这里运行可能有点小问题，可以参照这个[解决](https://blog.csdn.net/FontThrone/article/details/79012616)"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c71f040d8b772c04905ab38e2f3ab8af01cb01d5"},"cell_type":"code","source":"results = pd.read_csv(OUT_FILE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0e15dd457b3353de25485877c1cc0317f1bc90dc"},"cell_type":"code","source":"import ast\n\ndef evaluate(results, name):\n    \"\"\"evaluate 函数用来评估最佳参数的表现\n    返回的结果是 将之前用csv记录的结果，结构化返回，方便后续统计分析参数的分布\"\"\"\n    \n    new_results = results.copy()\n    # 使用ast.literal_eval str -> dic\n    new_results['hyperparameters'] = new_results['hyperparameters'].map(ast.literal_eval)\n    \n    # Sort with best values on top\n    new_results = new_results.sort_values('score', ascending = False).reset_index(drop = True)\n    \n    # 打印最高分数\n    print('The highest cross validation score from {} was {:.5f} found on iteration {}.'.format(name, new_results.loc[0, 'score'], new_results.loc[0, 'iteration']))\n    \n    # 使用最佳参数建模训练，返回分数\n    hyperparameters = new_results.loc[0, 'hyperparameters']\n    model = lgb.LGBMClassifier(**hyperparameters)\n    model.fit(train_features, train_labels)\n    preds = model.predict_proba(test_features)[:, 1]\n    \n    print('ROC AUC from {} on test data = {:.5f}.'.format(name, roc_auc_score(test_labels, preds)))\n    \n    # 将dict存储的参数转化为 结构化的数据 df\n    hyp_df = pd.DataFrame(columns = list(new_results.loc[0, 'hyperparameters'].keys()))\n\n    for i, hyp in enumerate(new_results['hyperparameters']):\n        hyp_df = hyp_df.append(pd.DataFrame(hyp, index = [0]), \n                               ignore_index = True)\n        \n    # 增加 iteration score 两列 \n    hyp_df['iteration'] = new_results['iteration']\n    hyp_df['score'] = new_results['score']\n    \n    return hyp_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50b27fe90c701217ce89f3a7675f55212737b03f"},"cell_type":"markdown","source":"ast.literal_eval 是升级版的 eval ，能够将字符串解析为python对象，安全性比 eval 高"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d97cf7d2e11b6480758fdb7164153e3a3687ee0c"},"cell_type":"code","source":"bayes_results = evaluate(results, name = 'Bayesian')\nbayes_results","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c437ec9441527027dccef2f4441dca159125c6d0"},"cell_type":"markdown","source":"## 搜索结果的探索"},{"metadata":{"_uuid":"1d51ceb680bc70a245a3caaf4df4b4d1219e8536"},"cell_type":"markdown","source":"作者对 贝叶斯优化 和 随机搜索的各1000组结果进行可视化，更直观的比较两种调参方式的特性。\n\n- 参数分布，观察参数的集中趋势\n- 时序分布，观察随着算法迭代，参数是如何分布的\n\n原作者是先后进行可视化的，我觉得对一个参数同时进行两种可视化可能更有意思点，我用作者思路和代码重新设计一下"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f5c165f4b5d227c6554c7bc087452dd0023110c2"},"cell_type":"code","source":"bayes_results = pd.read_csv('../input/home-credit-model-tuning/bayesian_trials_1000.csv').sort_values('score', ascending = False).reset_index()\nrandom_results = pd.read_csv('../input/home-credit-model-tuning/random_search_trials_1000.csv').sort_values('score', ascending = False).reset_index()\nrandom_results['loss'] = 1 - random_results['score']\n\nbayes_params = evaluate(bayes_results, name = 'Bayesian')\nrandom_params = evaluate(random_results, name = 'random')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8beefc44cdde7602e5700ae807a44f1760d62ced"},"cell_type":"code","source":"# Dataframe of just scores\nscores = pd.DataFrame({'ROC AUC': random_params['score'], 'iteration': random_params['iteration'], 'search': 'Random'})\nscores = scores.append(pd.DataFrame({'ROC AUC': bayes_params['score'], 'iteration': bayes_params['iteration'], 'search': 'Bayesian'}))\n\nscores['ROC AUC'] = scores['ROC AUC'].astype(np.float32)\nscores['iteration'] = scores['iteration'].astype(np.int32)\n\nscores.head()# Dataframe of just scores\nscores = pd.DataFrame({'ROC AUC': random_params['score'], 'iteration': random_params['iteration'], 'search': 'Random'})\nscores = scores.append(pd.DataFrame({'ROC AUC': bayes_params['score'], 'iteration': bayes_params['iteration'], 'search': 'Bayesian'}))\n\nscores['ROC AUC'] = scores['ROC AUC'].astype(np.float32)\nscores['iteration'] = scores['iteration'].astype(np.int32)\n\nscores.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9fe7ef1c314b8aa5a8aa7cd26799ea641372ff61"},"cell_type":"code","source":"best_random_params = random_params.iloc[random_params['score'].idxmax(), :].copy()\nbest_bayes_params = bayes_params.iloc[bayes_params['score'].idxmax(), :].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d4c1d7f81d5355808be43329acb367f1bb091e82"},"cell_type":"code","source":"best_random_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8935e42edee615b998835ff626d34228e3719b13"},"cell_type":"code","source":"# Plot of scores over the course of searching\nsns.lmplot('iteration', 'ROC AUC', hue = 'search', data = scores, size = 8);\nplt.scatter(best_bayes_params['iteration'], best_bayes_params['score'], marker = '*', s = 400, c = 'orange', edgecolor = 'k')\nplt.scatter(best_random_params['iteration'], best_random_params['score'], marker = '*', s = 400, c = 'blue', edgecolor = 'k')\nplt.xlabel('Iteration'); plt.ylabel('ROC AUC'); plt.title(\"Validation ROC AUC versus Iteration\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea731bd853e8626e5a70a08bbd6f87e3dbed2b7c"},"cell_type":"markdown","source":"从这张看出来，其实随机搜索的效果真的很好，如果计算资源有限，在少量的迭代过程中就能得到较好的参数，但是由于随机的效果，随着迭代数量的增加，不能明显提升分数\n\n贝叶斯优化参数效果有一个稳定提升的进程，虽然理论上可能是需要较少的搜索可以得到较好的效果，但还是需要一定量的迭代之后才能发挥。"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d51266c725fcdaa28de8778235059f1c9ce51723"},"cell_type":"code","source":"fig, axs = plt.subplots(3, 1, figsize = (24, 22))\n\n# 第一张参数的分布\nhyper = 'learning_rate'\n# sns.regplot('iteration', 'learning_rate', data = bayes_params, ax = axs[0])\n# axs[i].scatter(best_bayes_params['iteration'], best_bayes_params[hyper], marker = '*', s = 200, c = 'k')\n# axs[i].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = '{} over Search'.format(hyper));\n\nsns.kdeplot(learning_rate_dist, label='Sampling Distribution', linewidth=4, ax=axs[0])\nsns.kdeplot(random_params['learning_rate'], label='Random Search', linewidth=4, ax=axs[0])\nsns.kdeplot(bayes_params['learning_rate'], label='Bayes Optimization', linewidth=4, ax=axs[0])\naxs[0].vlines(x=best_random_params['learning_rate'], ymin=0.0, \n              ymax=20.0, linestyles='--', linewidth=4, colors=['orange', 'green'])\naxs[0].set(xlabel='Learning Rate', ylabel='Learning Rate', title = 'Bayes Optimization Search');\n\n# 第二章 贝叶斯优化的时序分布\nsns.regplot('iteration', hyper, data = bayes_params, ax = axs[1])\naxs[1].scatter(best_bayes_params['iteration'], best_bayes_params[hyper], marker = '*', s = 200, c = 'k')\naxs[1].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = 'Bayes Optimization Search vs Random Search');\n\n# 第三章 随机搜索的时序分布\nsns.regplot('iteration', hyper, data=random_params, ax=axs[2])\naxs[2].scatter(best_random_params['iteration'], best_random_params[hyper], marker='*', s=300, c='k')\naxs[2].set(xlabel='Iteration', ylabel='{}'.format(hyper), title='Random Search')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66dbef9060f92ddd13f6b0ee26e515609e2ffb2c"},"cell_type":"markdown","source":"可以看到贝叶斯优化在前40%左右的搜索模式和随机搜索很相似，到了后期才集中到可能分数较高的区域。"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0372fa777e37213660739838e290013abeeea5cf"},"cell_type":"code","source":"hyper_list = ['colsample_bytree', \n              'min_child_samples', \n              'num_leaves',\n              'reg_alpha',\n              'reg_lambda',\n              'subsample_for_bin']\n\nvline_heights = [10, 0.01, 0.012, 2.6, 1.7, 0.000007]\n\nfor hyper, vheight in zip(hyper_list, vline_heights):\n        \n    fig, axs = plt.subplots(3, 1, figsize = (24, 22))\n\n    # 第一张参数的分布\n    sns.kdeplot([sample(space[hyper]) for _ in range(1000)], label = 'Sampling Distribution', linewidth = 4, ax=axs[0])\n    sns.kdeplot(random_params[hyper], label='Random Search', linewidth=4, ax=axs[0])\n    sns.kdeplot(bayes_params[hyper], label='Bayes Optimization', linewidth=4, ax=axs[0])\n    axs[0].vlines(x=[best_bayes_params[hyper],best_random_params[hyper]], ymin=0.0, \n                  ymax=vheight, linestyles='--', linewidth=4, colors=['orange', 'green'])\n    axs[0].set(xlabel=hyper, ylabel='density', title = 'Bayes Optimization Search vs Random Search');\n\n    # 第二章 贝叶斯优化的时序分布\n    sns.regplot('iteration', hyper, data = bayes_params, ax = axs[1])\n    axs[1].scatter(best_bayes_params['iteration'], best_bayes_params[hyper], marker = '*', s = 200, c = 'k')\n    axs[1].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = 'Bayes Optimization');\n\n    # 第三章 随机搜索的时序分布\n    sns.regplot('iteration', hyper, data=random_params, ax=axs[2])\n    axs[2].scatter(best_random_params['iteration'], best_random_params[hyper], marker='*', s=300, c='k')\n    axs[2].set(xlabel='Iteration', ylabel='{}'.format(hyper), title='Random Search')\n\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bd84decba847f4f03bf33050355e6ced435db93"},"cell_type":"markdown","source":"和原文不同我尝试将每个参数的 概率分布和时序分布放到一起来展示，效果可能比原文清楚点。"},{"metadata":{"_uuid":"c9cacb60b2683a5fcb56b1bddaec5a3857f18edd"},"cell_type":"markdown","source":"## 后续\n\n后续的一些探索不列出来了：\n- 在非数值型参数上，参数值贝叶斯优化过程中也有集中趋势，这甚至可以指导我们进行 GridSearch和RandomSearch\n- Random 在测试集上效果更好，贝叶斯优化在验证集上效果较好\n- 迁移到整体数据集上，贝叶斯优化居然达到了<https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features>给出的 LB分数 0.792，迁移效果居然还不错\n\n## 结论\n\n- 贝叶斯优化在测试集上有更好的泛化效果\n- 相比GridSearch和RandomSearch，贝叶斯优化只需要更少的迭代次数\n\n\n"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"90b57274f70cba2eef08cd91ca0508f960366220"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}