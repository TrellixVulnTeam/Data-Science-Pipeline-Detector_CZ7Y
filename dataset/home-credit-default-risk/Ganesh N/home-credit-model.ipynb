{"cells":[{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Introduction: Home Credit Default Risk Competition\n\n# Data\n\nThe data is provided by [Home Credit](http://www.homecredit.net/about-us.aspx), a service dedicated to provided lines of credit\n(loans) to the unbanked population. The goal is to predict if  client will repay a loan or have difficulty is a critical business need,\n\n\n\n\n![image](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)\n"},{"metadata":{},"cell_type":"markdown","source":"application_{train|test}.csv\n\n    This is the main table, broken into two files for Train (with TARGET) and Test (without TARGET).\n    Static data for all applications. One row represents one loan in our data sample.\n\nbureau.csv\n\n    All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample).\n    For every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.\n\nbureau_balance.csv\n\n    Monthly balances of previous credits in Credit Bureau.\n    This table has one row for each month of history of every previous credit reported to Credit Bureau – i.e the table has (#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows.\n\nPOS_CASH_balance.csv\n\n    Monthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit.\n    This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows.\n\ncredit_card_balance.csv\n\n    Monthly balance snapshots of previous credit cards that the applicant has with Home Credit.\n    This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has (#loans in sample * # of relative previous credit cards * # of months where we have some history observable for the previous credit card) rows.\n\nprevious_application.csv\n\n    All previous applications for Home Credit loans of clients who have loans in our sample.\n    There is one row for each previous application related to loans in our data sample.\n\ninstallments_payments.csv\n\n    Repayment history for the previously disbursed credits in Home Credit related to the loans in our sample.\n    There is a) one row for every payment that was made plus b) one row each for missed payment.\n    One row is equivalent to one payment of one installment OR one installment corresponding to one payment of one previous Home Credit credit related to loans in our sample."},{"metadata":{},"cell_type":"markdown","source":"please check my kernel for simple exploration of all variables - https://www.kaggle.com/ganeshn88/simple-exploration-of-all-200-variables"},{"metadata":{},"cell_type":"markdown","source":"# Loading Libraries\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd \n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Data Preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\n\n#Model building\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nimport lightgbm as lgb\n\n#Model Evaluation\n\nfrom sklearn.metrics import roc_auc_score\n\n#Saving Models\nfrom sklearn.externals import joblib","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List files available\nprint(os.listdir(\"../input/home-credit-default-risk/\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reducing Memory Usage"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def importdata():    \n    print('-' * 80)\n    print('train')\n    train = import_data('../input/home-credit-default-risk/application_train.csv')\n\n    print('-' * 80)\n    print('test')\n    test = import_data('../input/home-credit-default-risk/application_test.csv')\n\n    print('-' * 80)\n    print('bureau_balance')\n    bureau_balance = import_data('../input/home-credit-default-risk/bureau_balance.csv')\n\n    print('-' * 80)\n    print('bureau')\n    bureau = import_data('../input/home-credit-default-risk/bureau.csv')\n\n    print('-' * 80)\n    print('credit_card_balance')\n    credit_card = import_data('../input/home-credit-default-risk/credit_card_balance.csv')\n\n    print('-' * 80)\n    print('installments_payments')\n    installments = import_data('../input/home-credit-default-risk/installments_payments.csv')\n\n    print('-' * 80)\n    print('pos_cash_balance')\n    pos_cash = import_data('../input/home-credit-default-risk/POS_CASH_balance.csv')\n\n    print('-' * 80)\n    print('previous_application')\n    previous_app = import_data('../input/home-credit-default-risk/previous_application.csv')\n    return train,test,bureau_balance,bureau,credit_card,installments,pos_cash,previous_app","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling Categorical data"},{"metadata":{},"cell_type":"markdown","source":"## Label Encoding "},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef one_hot_code(app_train,app_test):\n    \n    app_train = pd.get_dummies(app_train)\n    app_test = pd.get_dummies(app_test)\n    \n    train_labels = app_train['TARGET']\n    print('Aligning Train and Test Data')\n    app_train, app_test = app_train.align(app_test, join='inner', axis=1)\n    \n\n    print('Training Features shape: ', app_train.shape)\n    print('Testing Features shape: ', app_test.shape)\n    \n    return app_train,app_test,train_labels\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scaling Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale_data(traindata,testdata):\n   \n    imputer = SimpleImputer(strategy = 'median')\n\n    # Scale each feature to 0-1\n    scaler = MinMaxScaler(feature_range = (0, 1))\n\n    train = traindata.copy()\n    test = testdata.copy()\n\n    imputer.fit(train)\n    train = imputer.transform(train)\n    test = imputer.transform(test)\n\n    scaler.fit(train)\n    train = scaler.transform(train)\n    test = scaler.transform(test)\n\n    print('Training data shape: ', train.shape)\n    print('Testing data shape: ', test.shape)\n\n    return train,test\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"def featureengineering(df):\n    \n    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n    \n    return df\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_buildmodel(train,test,cvtype):\n    train = featureengineering(train)\n    test = featureengineering(test)\n    print('--------Encoding Categorical Data----------')\n    \n    app_train_encoded,app_test_encoded,target = one_hot_code(train,test)\n    print('-----------Scaling Data------------------------')\n    train_scaled,test_scaled = scale_data(app_train_encoded,app_test_encoded)\n    \n    print('---------Splitting Data-----------------')\n    X_train,X_test,Y_train,Y_test = train_test_split(train_scaled,target,test_size=0.2,random_state = 1)\n    params = {\n    'xgb': { 'learning_rate' : [0.05,0.10] ,'max_depth': [ 3,4,5,6,7] ,'n_estimators' : [500,1000] },\n    'lgb': {  'learning_rate' : [0.05,0.10] ,'max_depth': [ 3,4,5,6,7] ,'n_estimators' : [500,1000]  }\n    }\n    models = {\n    'xgb': XGBClassifier(), \n    'lgb':lgb.LGBMClassifier()\n    }\n    \n    from collections import defaultdict\n    model_result = defaultdict(list)\n    \n    for key in models.keys():\n        \n        model = models[key]\n        param = params[key]\n        \n        if cvtype == 'grid':\n            clf=GridSearchCV(model,param, cv=5, verbose=0)\n        elif cvtype == 'rand':\n            clf = RandomizedSearchCV(model, param, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1)\n    \n        clf.fit(X_train,Y_train,early_stopping_rounds=10,eval_metric = 'auc',eval_set=[(X_train, Y_train), (X_test, Y_test)])\n    \n        aucscoretr = roc_auc_score(Y_train, clf.predict_proba(X_train)[:,1])\n        aucscore = roc_auc_score(Y_test, clf.predict_proba(X_test)[:,1])\n        joblib_file = key + '_' + cvtype + '_model.pkl'\n        joblib.dump(clf, joblib_file)\n        model_result[key] = clf,clf.best_estimator_.get_params(),aucscoretr,aucscore,joblib_file\n        \n    return model_result,X_train,X_test,Y_train,Y_test,test_scaled\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing All data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train,test,bureau_balance,bureau,credit_card,installments,pos_cash,previous_app = importdata()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building Model with only Application Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"data_size = 1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#app_train = train.sample(data_size)\n#app_test = test\n#print('Appl data columns',train.columns)\n#print('Shape',app_train.shape)\n#model_result,X_train,X_test,Y_train,Y_test,test_scaled =preprocess_buildmodel(app_train,app_test,'rand')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print('XGB Train AUC with App data only',model_result['xgb'][2])\n#print('XGB Validation AUC with App data only',model_result['xgb'][3])\n#print('LGB Train AUC with App data only',model_result['lgb'][2])\n#print('LGB Validation AUC with App data only',model_result['lgb'][3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lgb_model = joblib.load(model_result['lgb'][4])\n#xgb_model = joblib.load(model_result['xgb'][4])\n#lgb_pred = lgb_model.predict_proba(test_scaled)[:, 1]\n#xgb_pred = xgb_model.predict_proba(test_scaled)[:, 1]\n#submit = app_test[['SK_ID_CURR']]\n#submit['TARGET'] = (lgb_pred + xgb_pred)/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submit.to_csv('homecredit_withappdata_blend.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Combining Application data with Previous Application Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"prev_apps_avg = previous_app.groupby('SK_ID_CURR').mean()\nprev_apps_avg.columns = ['p_' + col for col in prev_apps_avg.columns]\ntrain_prev_app = train.merge(right=prev_apps_avg.reset_index(), how='left', on='SK_ID_CURR')\n\navg_inst = installments.groupby('SK_ID_CURR').mean()\navg_inst.columns = ['i_' + f_ for f_ in avg_inst.columns]\ntrain_prev_app_inst = train_prev_app.merge(right=avg_inst.reset_index(), how='left', on='SK_ID_CURR')\n\npos_cash = pos_cash.groupby('SK_ID_CURR').mean()\ntrain_prev_app_inst_pos = train_prev_app_inst.merge(right=pos_cash.reset_index(), how='left', on='SK_ID_CURR')\n\navg_cc_bal = credit_card.groupby('SK_ID_CURR').mean()\navg_cc_bal.columns = ['cc_bal_' + f_ for f_ in avg_cc_bal.columns]\ntrain_prev_app_inst_pos_credit = train_prev_app_inst_pos.merge(right=avg_cc_bal.reset_index(), how='left', on='SK_ID_CURR')\n\nbureau_avg = bureau.groupby('SK_ID_CURR').mean()\nbureau_avg.columns = ['B_' + f_ for f_ in bureau_avg.columns]\ntrain_prev_app_inst_pos_credit_bureau = train_prev_app_inst_pos_credit.merge(right=bureau_avg.reset_index(), how='left', on='SK_ID_CURR')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train = train_prev_app_inst_pos_credit_bureau\napp_test = test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#app_train = train_prev_app_inst_pos_credit_bureau.sample(data_size)\napp_train = train_prev_app_inst_pos_credit_bureau\napp_test = test\nprint('Shape',app_train.shape)\nmodel_result,X_train,X_test,Y_train,Y_test,test_scaled =preprocess_buildmodel(app_train,app_test,'rand')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_model = joblib.load(model_result['lgb'][4])\nxgb_model = joblib.load(model_result['xgb'][4])\nlgb_pred = lgb_model.predict_proba(test_scaled)[:, 1]\nxgb_pred = xgb_model.predict_proba(test_scaled)[:, 1]\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = (lgb_pred + xgb_pred)/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('XGB Train AUC with Full data only',model_result['xgb'][2])\nprint('XGB Validation AUC with Full data only',model_result['xgb'][3])\nprint('LGB Train AUC with Full data only',model_result['lgb'][2])\nprint('LGB Validation AUC with Full data only',model_result['lgb'][3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.to_csv('homecredit_withprevapp_blend.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"References:\n\nReducing Memory usage  - https://www.kaggle.com/gemartin/load-data-reduce-memory-usage"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}