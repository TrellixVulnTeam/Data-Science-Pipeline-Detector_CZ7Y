{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"from sklearn.utils import shuffle\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":true},"cell_type":"code","source":"application_train = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\napplication_test  = pd.read_csv('../input/home-credit-default-risk/application_test.csv')\napplication_sub   = pd.read_csv('../input/home-credit-default-risk/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"#Using a for loop in Python to figure out the number of missing values in each column\nmissing_data_application_train = application_train.isnull()\nmost_missing = pd.DataFrame(columns=['Column','Percentage'])\n\nfor column in missing_data_application_train.columns.values.tolist(): \n    #print(column)\n    #print(missing_data_application_train[column].value_counts())\n    #print(\"Percentage of missing values in column:\",np.sum(missing_data_application_train[column])/missing_data_application_train[column].count()*100)\n    #print(\"\")\n    \n    if (np.sum(missing_data_application_train[column])/missing_data_application_train[column].count()*100) > 0:\n        most_missing = most_missing.append({'Column': column,'Percentage':np.sum(missing_data_application_train[column])/missing_data_application_train[column].count()*100}, ignore_index=True)\n    else:\n        continue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"most_missing.sort_values(by='Percentage',ascending=False).round(1)[most_missing['Percentage'] > 20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#Delete non important columns that we won't take into account in our model\nfor row in most_missing.sort_values(by='Percentage',ascending=False).round(1)[most_missing['Percentage'] > 20].iterrows():\n    name_column = list(row)\n    name_column = [str(i).split('\\n',1)[0] for i in name_column]\n    name_column = name_column[1].split()[1]\n    application_train.drop([name_column],axis=1,inplace=True)\n    application_test.drop( [name_column],axis=1,inplace=True)\n\n#TRAIN: Convert categorical variable into dummy/indicator variables with get_dummies and concat them to the df\nfor index, dtype in application_train.dtypes.iteritems():\n    if dtype == object:\n        normalized_column = pd.get_dummies(application_train[index],drop_first=True)\n        application_train = pd.concat([application_train,normalized_column],axis=1)\n        #Drop the old columns with categorical variables no longer useful\n        application_train.drop([index],axis=1,inplace=True)\n\n#TEST: Convert categorical variable into dummy/indicator variables with get_dummies and concat them to the df\nfor index, dtype in application_test.dtypes.iteritems():\n    if dtype == object:\n        normalized_column = pd.get_dummies(application_test[index],drop_first=True)\n        application_test = pd.concat([application_test,normalized_column],axis=1)\n        #Drop the old columns with categorical variables no longer useful\n        application_test.drop([index],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"application_train['TARGET']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def getDuplicatesWithCount(listOfElems):\n    ''' Get frequency count of duplicate elements in the given list '''\n    dictOfElems = dict()\n    # Iterate over each element in list\n    for elem in listOfElems:\n        # If element exists in dict then increment its value else add it in dict\n        if elem in dictOfElems:\n            dictOfElems[elem] += 1\n        else:\n            dictOfElems[elem] = 1    \n \n    # Filter key-value pairs in dictionary. Keep pairs whose value is greater than 1 i.e. only duplicate elements from list.\n    dictOfElems = { key:value for key, value in dictOfElems.items() if value > 1}\n    # Returns a dict of duplicate elements and thier frequency count\n    return dictOfElems","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"dictOfElems = getDuplicatesWithCount(application_train.columns)     \nfor key, value in dictOfElems.items():\n        print(key , ' :: ', value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"application_train.drop(['Maternity leave','Unknown','XNA','Y',], axis=1,inplace=True)\napplication_test.drop(['Y','XNA'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"len(set(application_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"missing = []\nfor column in application_train.columns:\n    if column not in application_test.columns:\n         missing.append(column)\nmissing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"#Clean data from NA values, fill them with the mean()\napplication_train.fillna(application_train.mean(),inplace=True)\napplication_test.fillna(application_test.mean(),inplace=True)\n\n#Put aside TARGET column in app_train\napplication_train_target = pd.DataFrame({'TARGET':application_train['TARGET']})\n\n#Put the columns with titles aside before asigning the scalars\ntrain_columns = application_train.loc[:, application_train.columns != 'TARGET'].columns\ntest_columns  = application_test.columns\n\n#Multiply each train and test dataframes by a scalar\nScaler1 = StandardScaler()\nScaler2 = StandardScaler()\napplication_train = pd.DataFrame(Scaler1.fit_transform(application_train.loc[:, application_train.columns != 'TARGET']))\napplication_test  = pd.DataFrame(Scaler2.fit_transform(application_test))\n\n#Reput the write column titles\napplication_train.columns = train_columns\napplication_test.columns  = test_columns\n\n#Reassign TARGET in app_train\napplication_train['TARGET'] = application_train_target\n\napplication_train\n#Save features and target names in separate variables\nfeatures = application_train.iloc[:,2:].columns.tolist()\ntarget   = application_train.loc[:, 'TARGET'].name\n\n#Create n dimensional arrays with features ('X_train') and the targets for each ('Y_train')\nX_train = application_train.iloc[:,2:].values\ny_train = application_train.loc[:,'TARGET'].values\n\n#Create n dimensional arrays with features ('X_test')\nX_test = application_test.iloc[:,1:].values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"application_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We import everything we need from Pytorch"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We define our data loaders :"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"## train data\nclass trainData(Dataset):\n    \n    def __init__(self, X_data, y_data):\n        self.X_data = X_data\n        self.y_data = y_data\n        \n    def __getitem__(self, index):\n        return self.X_data[index], self.y_data[index]\n        \n    def __len__ (self):\n        return len(self.X_data)\n\n\ntrain_data = trainData(torch.FloatTensor(X_train), \n                       torch.FloatTensor(y_train))\n## test data    \nclass testData(Dataset):\n    \n    def __init__(self, X_data):\n        self.X_data = X_data\n        \n    def __getitem__(self, index):\n        return self.X_data[index]\n        \n    def __len__ (self):\n        return len(self.X_data)\n    \n\ntest_data = testData(torch.FloatTensor(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"batch_size = 200\nn_epochs = 300\nbatch_no = len(X_train) // batch_size\n\ntrain_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_data, batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's examine how much features we get after the normalization of the data, to align this number with the inputs of the NN:"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"len(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.layer1 = nn.Linear(148, 296)\n        self.layer2 = nn.Linear(296, 148)\n        self.layer3 = nn.Linear(148, 74)\n        self.layer_out = nn.Linear(74, 1)\n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.1)\n        self.batchnorm1 = nn.BatchNorm1d(296)\n        self.batchnorm2 = nn.BatchNorm1d(148)\n        self.batchnorm3 = nn.BatchNorm1d(74)\n        \n        \n    def forward(self, inputs):\n        #print('Shape of inputs:',inputs.shape)\n        x = self.relu(self.layer1(inputs))\n        #print('Shape of x after relu.layer1', x.shape)\n        x = self.batchnorm1(x)\n        #print('Shape of x after batchnorm1', x.shape)\n        x = self.relu(self.layer2(x))\n        #print('Shape of x after relu.layer2', x.shape)\n        x = self.batchnorm2(x)\n        #print('Shape of x after batchnorm2', x.shape)\n        x = self.relu(self.layer3(x))\n        #print('Shape of x after relu.layer3', x.shape)\n        x = self.batchnorm3(x)\n        x = self.dropout(x)\n        #print('Shape of x after dropout', x.shape)\n        x = self.layer_out(x)\n        return x\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"ngpu = 1\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\nprint(device)\n\nmodel = Net()\nmodel.to(device)\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In all this configuration of the NN, we also want to check the accuracy of the model, so let's build a function for it:"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def binary_acc(y_pred, y_test):\n    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n\n    correct_results_sum = (y_pred_tag == y_test).sum().float()\n    acc = correct_results_sum/y_test.shape[0]\n    acc = torch.round(acc * 100)\n    \n    return acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"model.train()\nfor e in range(1, n_epochs+1):\n    epoch_loss = 0\n    epoch_acc = 0\n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        \n        output = model(X_batch)\n        loss = criterion(output, y_batch.unsqueeze(1))\n        acc = binary_acc(output, y_batch.unsqueeze(1))\n        \n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        #print(loss.item())\n        \n    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')\n    if epoch_acc < epoch_acc += acc.item() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"scrolled":true},"cell_type":"code","source":"y_pred_list = []\nmodel.eval()\nwith torch.no_grad():\n    for X_batch in test_loader:\n        X_batch = X_batch.to(device)\n        y_test_pred = model(X_batch)\n        y_test_pred = torch.sigmoid(y_test_pred)\n        y_pred_tag = torch.round(y_test_pred)\n        y_pred_list.append(y_pred_tag.cpu().numpy())\n\ny_pred_list = [a.squeeze().tolist() for a in y_pred_list]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"perc = submission.loc[submission['TARGET'] == 1].count()/submission.loc[submission['TARGET'] == 0].count()*100\nperc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(y_pred_list)\nsns.countplot(y_pred_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"submission = pd.DataFrame({'SK_ID_CURR': application_sub['SK_ID_CURR'], 'TARGET': y_pred_list})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}