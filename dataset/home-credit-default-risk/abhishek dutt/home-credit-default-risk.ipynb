{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('fivethirtyeight')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"418df7ab0ffcc72e694504896b907a12d1de4d90"},"cell_type":"code","source":"# read in train application\napplication_train = pd.read_csv('../input/application_train.csv')\nprint('Training data shape: ', application_train.shape)\napplication_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9108e7f069f2f842acc1925cc8b39f922fbbf63"},"cell_type":"code","source":"# Testing data features\napp_test = pd.read_csv('../input/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bef39c732fcecab380d8a249f8d43a305f5de07","scrolled":true},"cell_type":"code","source":"application_train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7863281edd74893193d9e1647680ac0dcdf6ffdd"},"cell_type":"code","source":"application_train['TARGET'].astype(int).plot.hist();\n# http://www.chioka.in/class-imbalance-problem/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16ce972329eeb3d0b97d128568bf6bb2eee751ff"},"cell_type":"code","source":"# practise \nmis_val = application_train.isnull().sum()\nmis_val_percent = 100 * application_train.isnull().sum() / len(application_train)\nmis_val_table = pd.concat([mis_val, mis_val_percent], axis=1) # concat along Columns !\nmis_val_table = mis_val_table[mis_val_table.iloc[:,1] != 0]\nmis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\nmis_val_table_sorted = mis_val_table_ren_columns.sort_values(\n        '% of Total Values', ascending=False).round(1)\nmis_val_table_sorted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54207c05dc67ce34ca6d52ba6750889c9b7a1de5"},"cell_type":"code","source":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b16f27c66b50970492ee9053806462d61c5208fc"},"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_values_table(application_train)\nmissing_values.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8181b3b5d76ebf22e97c992990d606c0a194a14e","_kg_hide-output":false},"cell_type":"code","source":"# Number of each type of column\napplication_train.dtypes.value_counts()\n# Number of unique classes in each object column\napplication_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)\n#application_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"194deb1fcab5f7a668dad4a4334b427601eb0fc1"},"cell_type":"code","source":"\"\"\"\nLet's implement the policy described above: for any categorical variable (`dtype == object`) with 2 unique categories, we will use label encoding, \nand for any categorical variable with more than 2 unique categories, we will use one-hot encoding. \n\nFor label encoding, we use the Scikit-Learn `LabelEncoder` and for one-hot encoding, the pandas `get_dummies(df)` function.\n\"\"\"\n\n# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in application_train:\n    if application_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        #print(col)\n        if len(list(application_train[col].unique())) <= 2: \n            # Train on the training data\n            le.fit(application_train[col])\n            # Transform both training and testing data\n            application_train[col] = le.transform(application_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \n            \nprint('%d columns were label encoded.' % le_count)\n\napplication_train.dtypes.value_counts() # now 16 to 12 left for categorical valriables (object type!)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3c9b3e18487f2a178f56533e4dcd18d93e1431e"},"cell_type":"code","source":"# one-hot encoding of categorical variables\napplication_train = pd.get_dummies(application_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', application_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09e7ad66c8ce135c9627499518ca6f60f2a75948"},"cell_type":"code","source":"train_labels = application_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napplication_train, app_test = application_train.align(app_test, join = 'inner', axis = 1)\n\n# Add the target back in\napplication_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', application_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"604f9350d28d03b6d10629cef28b852c437f99c4"},"cell_type":"code","source":"(application_train).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a185ece7f4d1593e2b40e852833122effb4d3c3"},"cell_type":"code","source":"(application_train['DAYS_BIRTH'] / -365).describe()\napplication_train['DAYS_EMPLOYED'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea75bca4736a27f6a7c40797ba33af7d1c25f280"},"cell_type":"code","source":"#Just out of curiousity, let's subset the anomalous clients and see if they tend to have higher or low rates of default than the rest of the clients.\nanom = application_train[application_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = application_train[application_train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))\n\n# create a flag column\napplication_train['DAYS_EMPLOYED_ANOM'] = application_train[\"DAYS_EMPLOYED\"] == 365243\n# Replace the anomalous values with nan\napplication_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\napplication_train['DAYS_EMPLOYED'].describe()\n\napplication_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1905c786c4d06980b9dec41ec685ca36d468170c"},"cell_type":"code","source":"app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba543330ee4462209ca046a5e132af40705620ad"},"cell_type":"code","source":"# Find correlations with the target and sort\ncorrelations = application_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))\n\n\"\"\"\nLet's take a look at some of more significant correlations: the `DAYS_BIRTH` is the most positive correlation. (except for `TARGET` because \nthe correlation of a variable with itself is always 1!) Looking at the documentation, `DAYS_BIRTH` is the age in days of the client at \nthe time of the loan in negative days (for whatever reason!). The correlation is positive, but the value of this feature is \nactually negative, meaning that as the client gets older, they are less likely to default on their loan (ie the target == 0). \nThat's a little confusing, so we will take the absolute value of the feature and then the correlation will be negative.\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7ae81c6f29442c43cf213003c70e6f73a56a3cc"},"cell_type":"code","source":"app_train = application_train\n# Find the correlation of the positive days since birth and target\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0de566407db2b0f29c7762aa6deda81735dc4573"},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\n\n# Plot the distribution of ages in years\nplt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d79cb7f89d8a3740ca70009e6b2417a1e1690c4"},"cell_type":"code","source":"# practise\napp_train.loc[app_train['TARGET'] == 0, ['DAYS_BIRTH']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"032ef62ec53633b64c513b1d109d1193c7deb1ee"},"cell_type":"code","source":"plt.figure(figsize = (10, 8))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n\n# Labeling of plot\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7d2a1db131ef03b654b7476f9717c6e1989a557"},"cell_type":"code","source":"# Age information into a separate dataframe\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e46429a2ed6d5a533845b2ea0fb5baa0fd36e553"},"cell_type":"code","source":"\n# Group by the bin and calculate averages\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3307eb07bf1565f02202177664b7b4de5998481"},"cell_type":"code","source":"plt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"386b6aabd9e61c7ac3614ed12c6d5f4aed7899b1"},"cell_type":"code","source":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea73453ae7c272610d82966c2601cb8ec57861b4"},"cell_type":"code","source":"plt.figure(figsize = (8, 6))\n\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"525045602c5816e5fb25998044838c0747e772dd"},"cell_type":"code","source":"plt.figure(figsize = (10, 12))\n\n# iterate through the sources\nfor i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    \n    # create a new subplot for each source\n    plt.subplot(3, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec2a44904afdd90605a5ebf4d76fb18d771cc2a4"},"cell_type":"code","source":"# Pairs Plot !\n\n# Copy the data for plotting\nplot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n\n# Add in the age of the client in years\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n\n# Drop na values and limit to first 100000 rows\nplot_data = plot_data.dropna().loc[:100000, :]\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)\n\n# Create the pairgrid object\ngrid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n                    hue = 'TARGET', \n                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.2)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e6f0e45b85722d09df1603ab7672612171c477c"},"cell_type":"code","source":"# Make a new dataframe for polynomial features\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\nprint(poly_features.head())\n\n# imputer for handling missing values\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'median')\n\npoly_target = poly_features['TARGET']\npoly_features = poly_features.drop(columns = ['TARGET'])\n\n# Need to impute missing values\npoly_features_scaled = imputer.fit_transform(poly_features)\npoly_features_test_scaled = imputer.transform(poly_features_test)\n\n# scaled_features_df = pd.DataFrame(scaled_features, index=df.index, columns=df.columns)\npoly_features_scaled_df = pd.DataFrame(poly_features_scaled, index=poly_features.index, columns=poly_features.columns)\n\npoly_features_scaled_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5edef93fa597c58b23400d025a16eca2a19fb19"},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n                                  \n# Create the polynomial object with specified degree\npoly_transformer = PolynomialFeatures(degree = 3)\n\npoly_features = poly_features_scaled\npoly_features_test = poly_features_test_scaled\n\n# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2581859d4c5afb9ed83a215fe2b8ae6c115fec13"},"cell_type":"code","source":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b311ca729b01d50068accf12d203088fd8ff1da"},"cell_type":"code","source":"# Create a dataframe of the features \npoly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Add in the target\npoly_features['TARGET'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))\npoly_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03e7cf4c360b73544e27fb2ae582f215668d2f1b"},"cell_type":"code","source":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n# Merge polynomial features into training dataframe\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n\n# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a16333104cc26833bc10908f678fa632fc185f1d"},"cell_type":"code","source":"app_train.shape\napp_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b592c6a6f6b7dc9448a84e1f4b384bafb2df448"},"cell_type":"code","source":"# Domain Knowledge !\napp_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7e77f5862d8fe7b10cc99eb09f67bfa34eff444"},"cell_type":"code","source":"app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d44a3db5cf2251c88d07ca4d798f1e68c33be3a","collapsed":true},"cell_type":"code","source":"plt.figure(figsize = (12, 20))\n# iterate through the new features\nfor i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n    \n    # create a new subplot for each source\n    plt.subplot(4, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % feature)\n    plt.xlabel('%s' % feature); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91d16c5748b93ff85890f1ab2dfc3305a423fe98"},"cell_type":"code","source":"# Logistic Regression ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b7b69bb657bcf49fe0c4ab343afd12103417a47"},"cell_type":"code","source":"\n\"\"\"\nTo get a baseline, we will use all of the features after encoding the categorical variables. We will preprocess the data by filling in \nthe missing values (imputation) and normalizing the range of the features (feature scaling). The following code performs both of \nthese preprocessing steps.\n\"\"\"\n\nfrom sklearn.preprocessing import MinMaxScaler, Imputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns = ['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"def553541f1014f4c5e1976a8c3241bcad2fabdd"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e928b0423ec7a74ffcca268a088f45979e10b27"},"cell_type":"code","source":"\"\"\"\nNow that the model has been trained, we can use it to make predictions. We want to predict the probabilities of not paying a loan, so we use \nthe model `predict.proba` method. This returns an m x 2 array where m is the number of observations. The first column is the probability\nof the target being 0 and the second column is the probability of the target being 1 (so for a single row, the two columns must sum to 1). \nWe want the probability the loan is not repaid, so we will select the second column.\n\"\"\"\n\n# Make predictions\n# Make sure to select the second column only\nlog_reg_pred = log_reg.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5f55e987d8f846712f62363fe81981faa98e71c"},"cell_type":"code","source":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"265f95ac6454f11eba29c0b39d73cfe52cd6b9ad"},"cell_type":"code","source":"# Save the submission to a csv file\nsubmit.to_csv('log_reg_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"055d4b03fe8ecb8cf02b61e871faa084da548a38"},"cell_type":"code","source":"# Random Forrest \nfrom sklearn.ensemble import RandomForestClassifier\n\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"873626d91ab4831432edf8e4424a704df81e73e7"},"cell_type":"code","source":"# Train on the training data\nrandom_forest.fit(train, train_labels)\n\n# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = random_forest.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b737aa448ee3c263bdb37b70bae55dd468c82bc"},"cell_type":"code","source":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d624d3586bae84696df106b60ace797aa84c5192"},"cell_type":"code","source":"# testing poly features ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e55b128fd862d0989c4cdeef5e6b5289002f6eb7"},"cell_type":"code","source":"poly_features_names = list(app_train_poly.columns)\napp_train_poly.shape\n\n# Impute the polynomial features\nimputer = Imputer(strategy = 'median')\n\npoly_features = imputer.fit_transform(app_train_poly)\npoly_features_test = imputer.transform(app_test_poly)\n\n# Scale the polynomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\npoly_features = scaler.fit_transform(poly_features)\npoly_features_test = scaler.transform(poly_features_test)\n\nrandom_forest_poly = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c8a58d1187880f332f2023bbbf281d8420d8130"},"cell_type":"code","source":"# Train on the training data\nrandom_forest_poly.fit(poly_features, train_labels)\n\n# Make predictions on the test data\npredictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0817f064c9d8f744b926247a3b08b3ae71a4c0ad"},"cell_type":"code","source":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_engineered.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fe0a3fe6245bd20f0280344ee62190c3469f5bd"},"cell_type":"code","source":"# testing domain features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90ece64e2ffebffd79ecef755a66244b10ae714b"},"cell_type":"code","source":"app_train_domain = app_train_domain.drop(columns = 'TARGET')\n\ndomain_features_names = list(app_train_domain.columns)\n\n# Impute the domainnomial features\nimputer = Imputer(strategy = 'median')\n\ndomain_features = imputer.fit_transform(app_train_domain)\ndomain_features_test = imputer.transform(app_test_domain)\n\n# Scale the domainnomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\ndomain_features = scaler.fit_transform(domain_features)\ndomain_features_test = scaler.transform(domain_features_test)\n\nrandom_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n# Train on the training data\nrandom_forest_domain.fit(domain_features, train_labels)\n\n# Extract feature importances\nfeature_importance_values_domain = random_forest_domain.feature_importances_\nfeature_importances_domain = pd.DataFrame({'feature': domain_features_names, 'importance': feature_importance_values_domain})\n\n# Make predictions on the test data\npredictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"862429edba756f73c7112395bf36b3f34b0fc578"},"cell_type":"code","source":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_domain.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3338f8adacee637f01a4145b6677a3afec5cba0b"},"cell_type":"code","source":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a35a47247d6b584c5f61b89e0b17af3116833273"},"cell_type":"code","source":"# Show the feature importances for the default features\nfeature_importances_sorted = plot_feature_importances(feature_importances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1621fa8aabd7f0768b4084756f16fecb085b7d17"},"cell_type":"code","source":"feature_importances_domain_sorted = plot_feature_importances(feature_importances_domain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46ec38a54c5f9091110d2a25098a63748c8f53bd"},"cell_type":"code","source":"# Gradient Boosting Machine ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fb5455f2401dd0c092d1f363aa322e656354d61"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"999f68059bee1c60477ed2c6cfafaae5ce192ed6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"daa19d796a9739be5e163f7c86b09d6be898fa17"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a6f108bdbf5b6d6178ca53ac7f2a6e4457c0977"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e3c4add78e8b4f364c77d05834acd9d102f3119"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2436ba9155d29ff7f64d1c1be6f7cb0b79616ca7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fe72d5fa961f083b6593e3f59bde3addaa165d1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93df14b4c98e9357b7b39cea2ba94757655db7dc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6964da5a586bd99a05b47ad253939ce55b8ecb22"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e749b78be879e51b10098f999aabdcff0f1b1665"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a7c88ecc0d0135a3a2f980f68ec05e6d3a1373f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb7da82c64d2c28a8b305c626bbb84c965ed1487"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b6ab3d65da2ca8ed122084d328b0f72bc4231f1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}