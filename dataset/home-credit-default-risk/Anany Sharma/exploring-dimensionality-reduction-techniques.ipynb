{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns# data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"../input/home-credit-default-risk/application_train.csv\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Percentage of missing values in the set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values = train.isna().sum()/len(train)*100\nmissing_values[missing_values>0].sort_values(ascending  = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variables of less than 60% of missing values in the training set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = train.columns\na = train.isna().sum()/len(train)*100\nvariable = []\nfor i in range(0,len(cols)):\n    if a[i]>=60:\n        variable.append(cols[i])\nprint(variable)        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in cols:\n    train[col].fillna(train[col].mode()[0],inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values = train.isna().sum()/len(train)*100\nmissing_values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Low Variance Filter Technique","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.var().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric = train.select_dtypes(include=[np.number])\nvar = numeric.var()\nvariance = []\nfor i in range(len(var)):\n    if var[i]>=30:\n        variance.append(var[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"variance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"High Correlation between Variables\n1. High Correlation with the target variable is always sought\n2. High correlation between input variables is avoideed as both variables show similar characterstics. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=train.drop('TARGET', 1)\nf , ax = plt.subplots(figsize = (14,12))\nplt.title('Correlation of Features- HeatMap',y=1,size=16)\nsns.heatmap(df.corr(),square = True,  vmax=0.8,annot = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should consider dropping variables having correlations greater than 0.5-0.6 ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Random Forest can be used to explore Feature Importances between  variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\ndf=df.drop(['SK_ID_CURR','DAYS_ID_PUBLISH'], axis=1)\nmodel = RandomForestRegressor(random_state=1, max_depth=10)\ndf=pd.get_dummies(df)\nmodel.fit(df,train.TARGET)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = df.columns\nimportances = model.feature_importances_\nindices = np.argsort(importances[0:20])  # top 20 features\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recursive Feature Selection\n* In this technique we will check the influence of a variable on the overall model by iteratively dropping each variable and model using the remaining variables.Further Comparing the performances and deduce if the variable is feasable to drop.Its recursive/Iterative process.\n* We first take all the n variables present in our dataset and train the model using them\n* We then calculate the performance of the model\n* Now, we compute the performance of the model after eliminating each variable (n times), i.e., we drop one   variable every time and train the model on the remaining n-1 variables\n* We identify the variable whose removal has produced the smallest (or no) change in the performance of the model, and then drop that variable\n* Repeat this process until no variable can be dropped\n* This method can be used when building Linear Regression or Logistic Regression models.","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Define dictionary to store our rankings\nranks = {}\n# Create our function which stores the feature rankings to the ranks dictionary\ndef ranking(ranks, names, order=1):\n    minmax = MinMaxScaler()\n    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n    ranks = map(lambda x: round(x,2), ranks)\n    return dict(zip(names, ranks))\n# Construct our Linear Regression model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nlr = LinearRegression(normalize=True)\nlr.fit(df, train.TARGET)\n#stop the search when only the last feature is left\nrfe = RFE(lr, n_features_to_select=1, verbose =3 )\nrfe.fit(df, train.TARGET)\n\nfrom sklearn.preprocessing import MinMaxScaler\nranks[\"RFE\"] = ranking(list(map(float, rfe.ranking_)), train.columns, order=-1)\n# Create empty dictionary to store the mean value calculated from all the scores\nr = {}\nfor name in train.columns:\n    r[name] = round(np.mean([ranks[method][name] \n                             for method in ranks.keys()]), 2)\n# Put the mean scores into a Pandas dataframe\nmeanplot = pd.DataFrame(list(r.items()), columns= ['Feature','Mean Ranking'])\n# Sort the dataframe\nmeanplot = meanplot.sort_values('Mean Ranking', ascending=False)\n# Let's plot the ranking of the features\nsns.factorplot(x=\"Mean Ranking\", y=\"Feature\", data = meanplot, kind=\"bar\",size=16, aspect=0.75, palette='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dimensionality Reduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" Factor Analysis\n* Factor Analysis is a technique of grouping highly correlated variables such that each group of variables have high correlations between them nad weak correaltions with the other groups.\n* These groups are called factors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/fashionmnist/fashion-mnist_train.csv\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = np.array(train,dtype = 'float32')\nimg = []\nfor i in range(len(train)):\n    image = train_data[i].flatten()\n    img.append(image)\nimg = np.array(img,dtype = 'float32')    \nimage.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/fashionmnist/fashion-mnist_train.csv\",sep=',')    # Give the complete path of your train.csv file\nfeat_cols = [ 'pixel'+str(i) for i in range(img.shape[1]) ]\ndf = pd.DataFrame(img,columns=feat_cols)\ndf['label'] = train['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import FactorAnalysis\nfa = FactorAnalysis(n_components = 3).fit_transform(df[feat_cols].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(16,10))\nplt.title('Factor Analysis Components')\nplt.scatter(fa[:,0], fa[:,1],c='r',s=10)\nplt.scatter(fa[:,1], fa[:,2],c='b',s=10)\nplt.scatter(fa[:,2],fa[:,0],c='g',s=10)\nplt.legend((\"First Factor\",\"Second Factor\",\"Third Factor\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Principle Component Analysis\n* Principal Component Analysis is a technique which is used to group variables into components.The variables are linearly correlated.\n* It works in way that first component gives the highest variance as compared to subsequent components","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=4)\npca_result = pca.fit_transform(df[feat_cols].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,6))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = np.arange(len(pca.explained_variance_ratio_))\nplt.figure(figsize=(14,6))\nplt.title('Principal Component Analysis')\nplt.bar(index, pca.explained_variance_ratio_*100)\nplt.xlabel('Principal Component', fontsize=10)\nplt.ylabel('Explained Variance', fontsize=10)\nplt.xticks(index, pca.explained_variance_ratio_*100, fontsize=10, rotation=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,10))\nplt.plot(range(4), pca.explained_variance_ratio_)\nplt.plot(range(4), np.cumsum(pca.explained_variance_ratio_))\nplt.title(\"PCA - Cumulative Explained Variance vs. Component-Explained Variance \")\nplt.legend((\"Component - Explained Variance\",\"Cumulative Sum - Explained Variance\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above graph, the blue line represents component-wise explained variance while the orange line represents the cumulative explained variance. We are able to explain around 60% variance in the dataset using just four components. Let us now try to visualize each of these decomposed components:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(pca_result[:, 0], pca_result[:, 1],pca_result[:, 2], pca_result[:, 3],\n            edgecolor='none', alpha=0.9,\n            cmap=plt.cm.get_cmap('Spectral', 8))\nplt.colorbar();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Singular Value Decomposition\n* In this type of Decomposition the original variables are decomposed into 3 different matrices.Its main aim is to remove redundant information.SVD uses Eigen Value and Eigen Vectors for calculation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD \nsvd = TruncatedSVD(n_components=3, random_state=42).fit_transform(df[feat_cols].values)\nsvd.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Let us visualize the transformed variables by plotting the first three principal components:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,10))\nplt.title('SVD Components')\nplt.scatter(svd[:,0], svd[:,1],c='r',s=10)\nplt.scatter(svd[:,1], svd[:,2],c='b',s=10)\nplt.scatter(svd[:,2],svd[:,0],c='g',s=10)\nplt.legend((\"Principal Component 1\",\"Principal Component 2\",\"Principal Component 3\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can still see some redundant features in the graph","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> Independent Component Analysis\n* In this analysis the groups or factors are not uncorrelated but independent\n* Variables are independent if its not dependent on other variables but uncorrelated means they cant be linearly correlated\n* Its the most used Dimensionality Reduction methods","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import FastICA \nICA = FastICA(n_components=3, random_state=12) \nX=ICA.fit_transform(df[feat_cols].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.title('ICA Components')\nplt.scatter(X[:,0], X[:,1],c='r',s=10)\nplt.scatter(X[:,1], X[:,2],c='b',s=10)\nplt.scatter(X[:,2], X[:,0],c='g',s=10)\nplt.legend((\"ICA Component 1\",\"ICA Component 2\",\"ICA Component 3\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">  Hello World!! I am just a beginner in the field and still exploring so bear with my mistakes(if any) and i am open to any type of suggestions.Plus if you like it please UPVOTE.It really pushes me to learn more and more.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}