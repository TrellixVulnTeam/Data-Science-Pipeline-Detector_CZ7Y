{"cells":[{"metadata":{},"cell_type":"markdown","source":"This project aims to perform a useful EDA on the data, suggest some future engineering and finally find the best model for the ML part."},{"metadata":{},"cell_type":"markdown","source":"# 1. Importing libraries and reading data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/home-credit-default-risk/application_train.csv\", index_col=0)\ntest = pd.read_csv(\"../input/home-credit-default-risk/application_test.csv\", index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(train.head())\ndisplay(train.tail())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(test.head())\ndisplay(test.tail())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. EDA\nLet's do some charting!"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = [\"TARGET\", \"CODE_GENDER\", \"NAME_CONTRACT_TYPE\", \"FLAG_OWN_CAR\",\n           \"FLAG_OWN_REALTY\", \"CNT_CHILDREN\",\"NAME_TYPE_SUITE\",\n           \"NAME_HOUSING_TYPE\", \"OCCUPATION_TYPE\", \"EMERGENCYSTATE_MODE\",\"FLAG_DOCUMENT_2\",\n           \"FLAG_DOCUMENT_3\",\"FLAG_DOCUMENT_4\",\"FLAG_DOCUMENT_5\",\"FLAG_DOCUMENT_6\",\n           \"FLAG_DOCUMENT_7\",\"FLAG_DOCUMENT_8\",\"FLAG_DOCUMENT_9\",\"FLAG_DOCUMENT_10\",\n           \"FLAG_DOCUMENT_11\",\"FLAG_DOCUMENT_12\",\"FLAG_DOCUMENT_13\",\n           \"FLAG_DOCUMENT_14\",\"FLAG_DOCUMENT_15\",\"FLAG_DOCUMENT_16\",\n           \"FLAG_DOCUMENT_17\",\"FLAG_DOCUMENT_18\",\"FLAG_DOCUMENT_19\",\n           \"FLAG_DOCUMENT_20\",\"FLAG_DOCUMENT_21\"]\nfor column in columns:\n  df = pd.DataFrame(train[column].value_counts())\n  plt.figure(figsize=(12,12))\n  plt.pie(df[column], labels=df.index, autopct='%2.1f%%')\n  plt.title(f\" {column}\")\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As we can see:\n\n 1- the target column **(labels)** in the training dataset is significantly **imbalanced**\n    \n 2- Around  **66%** of the applicants are **women** and the rest are men\n    \n 3- Around **90%** are the loan applications are **cash loans** and the rest are revolving loans\n    \n 4- **34%** of the applicants already **own a car**\n    \n 5- **30%** of the applicants already **own a home or flat**\n    \n 6- **70%** of the applicants **don't have a child**, **20%** have **1**, and **9%** have **2 children**\n    \n 7- **81%** of the applicant were **living alone** while **13%** were **family**\n \n 8- Most of the applicants were **laborers (26%)** which were about **twice of the sales staff and core staff**\n \n 9- **1.4%** of the applicants had an **emergency situation** while applying for the loan\n \n 10- Most of the **columns named \"Documents\"** **don't have any useful data** and we can delet them in the data cleaning part\n    "},{"metadata":{},"cell_type":"markdown","source":"Let's plot some charts based on the number of those who did or didn't pay their loans:"},{"metadata":{},"cell_type":"markdown","source":"### Gender-based :"},{"metadata":{"trusted":true},"cell_type":"code","source":" male_zero = train.loc[(train[\"CODE_GENDER\"]==\"M\")&(train[\"TARGET\"]==0), :].count()[0]\n male_one =  train.loc[(train[\"CODE_GENDER\"]==\"M\")&(train[\"TARGET\"]==1), :].count()[0]\n female_zero =  train.loc[(train[\"CODE_GENDER\"]==\"F\")&(train[\"TARGET\"]==0), :].count()[0]\n female_one =  train.loc[(train[\"CODE_GENDER\"]==\"F\")&(train[\"TARGET\"]==1), :].count()[0]\n\nplt.figure(figsize=(10,5))\nsns.countplot(data=train,x=\"CODE_GENDER\", hue=\"TARGET\", )\nplt.text(-0.3,100000,male_zero)\nplt.text(0.08,15000,male_one)\nplt.text(0.7,190000,female_zero)\nplt.text(1.1,20000,female_one)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, more than **10%** of the **male applicants** **failed** to pay theri loans while **less than 10%** of **the female applicants** did so."},{"metadata":{},"cell_type":"markdown","source":"### Owning a Car:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(data=train,x=\"FLAG_OWN_CAR\", hue=\"TARGET\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Age-based:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nsns.countplot(data=train,x=pd.cut(train.DAYS_BIRTH/-365.25, bins=3,precision=0, right=True,retbins=False), hue=\"TARGET\")\nplt.xlabel(\"YEARS_BIRTH\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, there is a **greater chance** for ** youger applicants** to fail to pay their loans which is a good information for taking precautionary measures."},{"metadata":{},"cell_type":"markdown","source":"### distribution of the employment history:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\n(train[\"DAYS_EMPLOYED\"]/-365.25).plot.hist(bins=10)\nplt.xlabel(\"YEARS_EMPLOYED\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### \"Employment history\"-based:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,10))\nsns.countplot(data=train,x=pd.cut(train.DAYS_EMPLOYED/-365.25, bins=5,precision=0, right=True,retbins=False), hue=\"TARGET\")\nplt.xlabel(\"YEARS_EMPLOYED\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Career-based:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(data=train, x=\"NAME_INCOME_TYPE\", hue=\"TARGET\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Education-bsed:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(data=train,x=\"NAME_EDUCATION_TYPE\", hue=\"TARGET\", )\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### \"Marriage status\"-based:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(data=train,x=\"NAME_FAMILY_STATUS\", hue=\"TARGET\", )\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### based on the last phone number change:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsns.countplot(data=train,x=pd.cut(train.DAYS_LAST_PHONE_CHANGE/-365.25, bins=12,precision=0, right=True,retbins=False), hue=\"TARGET\")\nplt.xlabel(\"YEARS_LAST_PHONE_CHANGE\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Based on Income type:"},{"metadata":{"trusted":true},"cell_type":"code","source":"name_income_type=train.NAME_INCOME_TYPE.unique()\nfor name in name_income_type:\n  plt.figure(figsize=(7,5))\n  data=train.loc[(train.NAME_INCOME_TYPE==name), :]\n  sns.countplot(data = data, x= \"NAME_INCOME_TYPE\", hue=train[\"TARGET\"])\n  plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mssing Values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(train.isnull().sum(), columns=[\"MISSING_VALUES\"]).sort_values(by=\"MISSING_VALUES\", ascending=False).head(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow! what a disaster in data collection! Seems we have lots of work to do!"},{"metadata":{"trusted":true},"cell_type":"code","source":"#categorical columns\npd.DataFrame(train.select_dtypes('object').apply(pd.Series.nunique, axis = 0),\n             columns=[\"Categ_Data\"]).sort_values(by=\"Categ_Data\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#categorical data in test data set\npd.DataFrame(train.select_dtypes('object').apply(pd.Series.nunique, axis = 0),\n             columns=[\"Categ_Data\"]).sort_values(by=\"Categ_Data\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the columns which we need to one-hot code before ML model building"},{"metadata":{},"cell_type":"markdown","source":"### Anomalies"},{"metadata":{},"cell_type":"markdown","source":"Let's see our data anomalies by plotting some distributions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"colomns = [\"HOUR_APPR_PROCESS_START\", \"EXT_SOURCE_1\", \"APARTMENTS_MODE\",\n           \"YEARS_BEGINEXPLUATATION_MODE\",\"DAYS_LAST_PHONE_CHANGE\",\n           \"REGION_POPULATION_RELATIVE\", \"DAYS_REGISTRATION\", \"DAYS_ID_PUBLISH\",\n           \"DAYS_EMPLOYED\", \"AMT_ANNUITY\", \"AMT_CREDIT\", \"AMT_INCOME_TOTAL\"\n           ]\n\nfor colomn in colomns:\n  plt.figure(figsize=(12,6))\n  plt.subplot(1,2,1)\n  train[colomn].plot(kind = \"box\")\n  plt.subplot(1,2,2)\n  train[colomn].plot(kind = \"hist\")\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.AMT_INCOME_TOTAL.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"useless_columns = [\"HOUSETYPE_MODE\", \"WALLSMATERIAL_MODE\",\"FLAG_DOCUMENT_2\",\n                     \"FLAG_DOCUMENT_4\",\"FLAG_DOCUMENT_5\",\"FLAG_DOCUMENT_7\",\n                     \"FLAG_DOCUMENT_9\",\"FLAG_DOCUMENT_10\",\"FLAG_DOCUMENT_12\",\n                     \"FLAG_DOCUMENT_15\",\"FLAG_DOCUMENT_17\",\"FLAG_DOCUMENT_19\",\n                   \"FLAG_DOCUMENT_20\", \"FLAG_DOCUMENT_21\",\"AMT_REQ_CREDIT_BUREAU_HOUR\",\n                   \"YEARS_BUILD_MODE\",\"ELEVATORS_MODE\",\"ENTRANCES_MODE\", \"FLOORSMAX_MODE\",\n                     \"COMMONAREA_MEDI\", \"ELEVATORS_MEDI\", \"ENTRANCES_MEDI\",\n                   \"FLOORSMAX_MEDI\", \"FLOORSMIN_MEDI\",\"ELEVATORS_AVG\",\"ENTRANCES_AVG\",\n                   \"FLOORSMAX_AVG\",\"FLOORSMIN_AVG\",\"LIVINGAPARTMENTS_AVG\",\n                   \"NONLIVINGAPARTMENTS_AVG\",\"ORGANIZATION_TYPE\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we saw in the EDA part, some of the columns don't provide any data (as most of \"FLAG_DOCUMENT_... ones). Some of the columns also don't contain any useful data (such as the walls materials, the elevator type and area and etc...)"},{"metadata":{},"cell_type":"markdown","source":"Let's delet these columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(columns = useless_columns)\ntest = test.drop(columns=useless_columns)\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our training dataset now has 90 columns which is not too bad!"},{"metadata":{},"cell_type":"markdown","source":"## 4.  Imputation"},{"metadata":{},"cell_type":"markdown","source":"Now that we deleted the useless columns, let's impute missing values and anomalies with reasonable values:"},{"metadata":{},"cell_type":"markdown","source":"I chose to fill the categorical missing values with the most frequent ones, and the numerical values with their median.\n(I chose median instead of mean to avoid affected by the anomalies)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fill the missing categorical data with the most frequent velue\ntrain_categorical = train.select_dtypes(include ='object') \ntrain_categorical = train_categorical.apply(lambda x: x.fillna(x.value_counts().index[0]))\n\n#Fill the missing numerical data with the median \ntrain_float = train.select_dtypes(include ='float64') \ntrain_float = train_float.fillna(train_float.median())\n\ntrain_int = train.select_dtypes(include ='int64') \ntrain_int = train_int.fillna(train_int.median())\n\ntrain_final = pd.concat([train_categorical,train_float,train_int], axis=1)\ntrain_final[\"TARGET\"] = train[\"TARGET\"]\ntrain=train_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill missing categorical values in test data set with most frequent values in \n#training data set (don't use any info from test data set)\n\ncategorical_columns = test.select_dtypes(include ='object').columns\ntest_categorical = test.select_dtypes(include ='object') \n\nfor column in categorical_columns:\n  test_categorical[column] = test_categorical[column].fillna(train[column].value_counts().index[0])\n\n#for numerical missing values in test data set, fill them with median value in \n#the training data set (don't use any info from test data set)\ntest_float = test.select_dtypes(include ='float64') \ntest_float = test_float.fillna(train_float.median())\n\ntest_int = test.select_dtypes(include ='int64') \ntest_int = test_int.fillna(train_int.median())\n\ntest_final = pd.concat([test_categorical,test_float,test_int], axis=1)\ntest=test_final","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The other point was that I chose to not to impute the test dataset with its own values and get the help of the median values in the training data. that's how we can prevent overfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape,test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Replacing anomalies with reasonable values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train.DAYS_EMPLOYED==train.DAYS_EMPLOYED.max(), :].NAME_INCOME_TYPE.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.DAYS_EMPLOYED.max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the anomalies for the employment days were those who were either pensioner or umemployed.\nI chose to impute the pensioners employment history anomalies with 50 (the max years) and the unemployed ones with 0 \n(because the don't have any employment history claimed)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#replace anomalies with the extreme value in range of normal data:\n#for years of employment, the maximum number should be 50 years for pensioners,\n#and 0 for unemployed clients\ntrain_dataframe=train.copy()\ntest_dataframe = test.copy()\ndef unemployed_anomaly(x):\n  if x==365243:\n    x= 0\n  return x\n\ndef pensioner_anomaly(x):\n  if x==365243:\n    x=-18263\n  return x\n\ntrain_unemployed = train_dataframe.loc[train_dataframe.NAME_INCOME_TYPE==\"Unemployed\",:]\ntrain_pensioner = train_dataframe.loc[train_dataframe.NAME_INCOME_TYPE==\"Pensioner\",:]\ntrain_other = train_dataframe.loc[(train_dataframe.NAME_INCOME_TYPE!=\"Unemployed\")&(train_dataframe.NAME_INCOME_TYPE!=\"Pensioner\"),:]\ntrain_unemployed.DAYS_EMPLOYED = train_unemployed.DAYS_EMPLOYED.apply(unemployed_anomaly)\ntrain_pensioner.DAYS_EMPLOYED=train_pensioner.DAYS_EMPLOYED.apply(pensioner_anomaly)\ntrain_dataframe=pd.concat([train_unemployed,train_pensioner,train_other],axis=0)\n\ntest_unemployed = test_dataframe.loc[test_dataframe.NAME_INCOME_TYPE==\"Unemployed\",:]\ntest_pensioner = test_dataframe.loc[test_dataframe.NAME_INCOME_TYPE==\"Pensioner\",:]\ntest_other = test_dataframe.loc[(test_dataframe.NAME_INCOME_TYPE!=\"Unemployed\")&(test_dataframe.NAME_INCOME_TYPE!=\"Pensioner\"),:]\ntest_unemployed.DAYS_EMPLOYED=test_unemployed.DAYS_EMPLOYED.apply(unemployed_anomaly)\ntest_pensioner.DAYS_EMPLOYED=test_pensioner.DAYS_EMPLOYED.apply(pensioner_anomaly)\ntest_dataframe=pd.concat([test_unemployed,test_pensioner,test_other],axis=0)\n\ndisplay(test_dataframe.head())\ndisplay(train_dataframe.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataframe.DAYS_EMPLOYED.min()/365.25","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are all good :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train_dataframe\ntest=test_dataframe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"I tried to do some feature engineering based on  domain knowledge and not the feature importance"},{"metadata":{},"cell_type":"markdown","source":"### Ratios:"},{"metadata":{},"cell_type":"markdown","source":"#### 1- Payment Ratio"},{"metadata":{"trusted":true},"cell_type":"code","source":"#RATIOS:\n#1- amount of annual payment to annual income:\ntrain_df = train.copy()\ntrain_df[\"PAYMENT_RATIO\"] = train_df[\"AMT_ANNUITY\"]/train_df[\"AMT_INCOME_TOTAL\"]\ntrain_df = train_df.drop(columns=[\"AMT_ANNUITY\"])\ntest_df = test.copy()\ntest_df[\"PAYMENT_RATIO\"] = test_df[\"AMT_ANNUITY\"]/test_df[\"AMT_INCOME_TOTAL\"]\ntest_df = test_df.drop(columns=[\"AMT_ANNUITY\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2- Years instead of days!"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Years Instead of Days!\ndef year_convertor(dataframe):\n  df = dataframe.copy()\n  df[\"AGE\"] = df[\"DAYS_BIRTH\"].apply(lambda x: np.int(-x/365))\n  df[\"YEARS_EMPLOYED\"] = df[\"DAYS_EMPLOYED\"].apply(lambda x: np.int(-x/365))\n  df[\"YEARS_REGISTERED\"] = df[\"DAYS_REGISTRATION\"].apply(lambda x: np.int(-x/365))\n  df[\"YEARS_ID_PUBLISHED\"] = df[\"DAYS_ID_PUBLISH\"].apply(lambda x: np.int(-x/365))\n  df[\"YEARS_LAST_PHONE_CHANGE\"]=df[\"DAYS_LAST_PHONE_CHANGE\"].apply(lambda x: np.int(-x/365))\n  df = df.drop(columns = [\"DAYS_BIRTH\", \"DAYS_EMPLOYED\", \"DAYS_REGISTRATION\", \"DAYS_ID_PUBLISH\",\"DAYS_LAST_PHONE_CHANGE\"])\n  return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" training_df = year_convertor(train_df)\n testing_df = year_convertor(test_df)\ntraining_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=training_df\ntest=testing_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One-Hot Coding:"},{"metadata":{},"cell_type":"markdown","source":"Before one-hot coding, I tried to replace binary categorical values simply with 0 and 1 to prevent more dimention increase:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1 = train.copy()\ntest_1 = test.copy()\n\n#Binary Categorical Data (Y/N,F/M):\ntrain_1['FLAG_OWN_CAR'] = train_1['FLAG_OWN_CAR'].replace(to_replace = ['Y', 'N'], value = [1,0] )\ntrain_1['FLAG_OWN_REALTY'] = train_1['FLAG_OWN_REALTY'].replace(['Y', 'N'], [1, 0])\ntrain_1['EMERGENCYSTATE_MODE'] = train_1['EMERGENCYSTATE_MODE'].replace(['Yes', 'No'], [1, 0])\ntrain_1['CODE_GENDER'] = train_1['CODE_GENDER'].replace(['M', 'F'], [1, 0])\n\ntest_1['FLAG_OWN_CAR'] = test_1['FLAG_OWN_CAR'].replace(['Y', 'N'], [1, 0])\ntest_1['FLAG_OWN_REALTY'] = test_1['FLAG_OWN_REALTY'].replace(['Y', 'N'], [1, 0])\ntest_1['EMERGENCYSTATE_MODE'] = test_1['EMERGENCYSTATE_MODE'].replace(['Yes', 'No'], [1, 0])\ntest_1['CODE_GENDER'] = test_1['CODE_GENDER'].replace(['M', 'F'], [1, 0])\ntrain_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train_1\ntest=test_1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now we are set for the one-hot coding "},{"metadata":{"trusted":true},"cell_type":"code","source":"#one-hot coding on categorical columns:\ntrain = pd.get_dummies(train)\ntest = pd.get_dummies(test)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape,test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because of more categories in one or more columns in the training dataset, the one-hot coding produced more columns for the training dataset. we can simply align these data sets together"},{"metadata":{"trusted":true},"cell_type":"code","source":"#saving the targets firs:\ntrain_labels = train['TARGET']\n\n# Align the training and testing data\ntrain, test = train.align(test, join = 'inner', axis = 1)\n\n# Add the target\ntrain['TARGET'] = train_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape,test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it's aligned"},{"metadata":{},"cell_type":"markdown","source":"## 6. ML model building"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the data set "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train.drop(columns=\"TARGET\"),\n                                                    train[\"TARGET\"], test_size=0.10,\n                                                    random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape,X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Logistic Regression:"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_algorithm = LogisticRegression()\nreg_algorithm.fit(X_train,y_train)\npredictions = reg_algorithm.predict(X_test)\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems the algorithm doesn't learn any thing from the data. That's probably because of the imbalanced labels."},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree Classifier:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_model=DecisionTreeClassifier()\ndt_model.fit(X_train,y_train)\ndt_pred = dt_model.predict(X_test)\nprint(confusion_matrix(y_test,dt_pred))\nprint(classification_report(y_test,dt_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Classifier:"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model= RandomForestClassifier(n_estimators=300)\nrf_model.fit(X_train,y_train)\nrf_pre=rf_model.predict(X_test)\nprint(confusion_matrix(y_test,rf_pre))\nprint(classification_report(y_test,rf_pre))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost Classifier:"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = XGBClassifier(n_estimators=300)\nxgb_model.fit(X_train,y_train)\nxg_pred = xgb_model.predict(X_test)\nprint(confusion_matrix(y_test,xg_pred))\nprint(classification_report(y_test,xg_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Model Optimization"},{"metadata":{},"cell_type":"markdown","source":"### Grid Search Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=train.drop(columns=\"TARGET\")\ny=train.TARGET\ntuned_parameters = [{'n_estimators': [200,350], 'max_depth' : [5,10]}]\nclf = GridSearchCV(XGBClassifier(), tuned_parameters, cv=5, scoring='roc_auc', return_train_score=True)\nclf.fit(X, y)\nprint(clf.classes_)\nprint (\"best score = \", clf.best_score_)\nprint(\"Best parameters set found on development set: \", clf.best_params_)\nmaxDepth=clf.best_params_[\"max_depth\"]\nnEstimator = clf.best_params_[\"n_estimators\"]\n\nxgb_model=XGBClassifier(max_depth=maxDepth,n_estimators=nEstimator)\nX = train.drop(columns=\"TARGET\")\ny = train.TARGET\nxgb_model=xgb_model.fit(X,y)\ntest_labels = xgb_model.predict_proba(test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n    \"SK_ID_CURR\": test.index,\n    \"TARGET\" : test_labels[:,1]\n})\nsubmission.to_csv('submission.csv', index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}