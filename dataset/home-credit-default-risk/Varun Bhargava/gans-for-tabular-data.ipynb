{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-01T07:59:39.923449Z","iopub.execute_input":"2021-12-01T07:59:39.924203Z","iopub.status.idle":"2021-12-01T07:59:39.934884Z","shell.execute_reply.started":"2021-12-01T07:59:39.924162Z","shell.execute_reply":"2021-12-01T07:59:39.93418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import precision_score, recall_score, f1_score,\\\n                            accuracy_score, balanced_accuracy_score,classification_report,\\\n                            plot_confusion_matrix, confusion_matrix\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nimport lightgbm as lgb\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, Concatenate\nfrom tensorflow.keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.initializers import RandomNormal\nimport tensorflow.keras.backend as K\nfrom sklearn.utils import shuffle\n\nnp.random.seed(1635848)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T07:59:40.88882Z","iopub.execute_input":"2021-12-01T07:59:40.889515Z","iopub.status.idle":"2021-12-01T07:59:40.898405Z","shell.execute_reply.started":"2021-12-01T07:59:40.889472Z","shell.execute_reply":"2021-12-01T07:59:40.897386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass cGAN():\n    \n    \"\"\"\n    Class containing 3 methods (and __init__): generator, discriminator and train.\n    Generator is trained using random noise and label as inputs. Discriminator is trained\n    using real/fake samples and labels as inputs.\n    \"\"\"\n    \n    def __init__(self,latent_dim=32, out_shape=16):\n        \n        self.latent_dim = latent_dim\n        self.out_shape = out_shape \n        self.num_classes = 4\n        # using Adam as our optimizer\n        optimizer = Adam(0.0002, 0.5)\n        \n        # building the discriminator\n        self.discriminator = self.discriminator()\n        self.discriminator.compile(loss=['binary_crossentropy'],\n                                   optimizer=optimizer,\n                                   metrics=['accuracy'])\n\n        # building the generator\n        self.generator = self.generator()\n\n        noise = Input(shape=(self.latent_dim,))\n        label = Input(shape=(1,))\n        gen_samples = self.generator([noise, label])\n        \n        # we don't train discriminator when training generator\n        self.discriminator.trainable = False\n        valid = self.discriminator([gen_samples, label])\n\n        # combining both models\n        self.combined = Model([noise, label], valid)\n        self.combined.compile(loss=['binary_crossentropy'],\n                              optimizer=optimizer,\n                             metrics=['accuracy'])\n\n\n    def generator(self):\n        init = RandomNormal(mean=0.0, stddev=0.02)\n        model = Sequential()\n\n        model.add(Dense(128, input_dim=self.latent_dim))\n        model.add(Dropout(0.2))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n\n        model.add(Dense(256))\n        model.add(Dropout(0.2))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n\n        model.add(Dense(512))\n        model.add(Dropout(0.2))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n\n        model.add(Dense(self.out_shape, activation='tanh'))\n\n        noise = Input(shape=(self.latent_dim,))\n        label = Input(shape=(1,), dtype='int32')\n        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n        \n        model_input = multiply([noise, label_embedding])\n        gen_sample = model(model_input)\n\n        return Model([noise, label], gen_sample, name=\"Generator\")\n\n    \n    def discriminator(self):\n        init = RandomNormal(mean=0.0, stddev=0.02)\n        model = Sequential()\n\n        model.add(Dense(512, input_dim=self.out_shape, kernel_initializer=init))\n        model.add(LeakyReLU(alpha=0.2))\n        \n        model.add(Dense(256, kernel_initializer=init))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.4))\n        \n        model.add(Dense(128, kernel_initializer=init))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.4))\n        \n        model.add(Dense(1, activation='sigmoid'))\n        \n        gen_sample = Input(shape=(self.out_shape,))\n        label = Input(shape=(1,), dtype='int32')\n        label_embedding = Flatten()(Embedding(self.num_classes, self.out_shape)(label))\n\n        model_input = multiply([gen_sample, label_embedding])\n        validity = model(model_input)\n\n        return Model(inputs=[gen_sample, label], outputs=validity, name=\"Discriminator\")\n\n\n    def train(self, X_train, y_train, pos_index, neg_index, epochs, sampling=False, batch_size=32, sample_interval=100, plot=True): \n        \n        # though not recommended, defining losses as global helps as in analysing our cgan out of the class\n        global G_losses\n        global D_losses\n        \n        G_losses = []\n        D_losses = []\n        # Adversarial ground truths\n        valid = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n\n        for epoch in range(epochs):\n            \n            # if sampling==True --> train discriminator with 8 sample from postivite class and rest with negative class\n            if sampling:\n                idx1 = np.random.choice(pos_index, 8)\n                idx0 = np.random.choice(neg_index, batch_size-8)\n                idx = np.concatenate((idx1, idx0))\n            # if sampling!=True --> train discriminator using random instances in batches of 32\n            else:\n                idx = np.random.choice(len(y_train), batch_size)\n            samples, labels = X_train[idx], y_train[idx]\n            samples, labels = shuffle(samples, labels)\n            \n            # Sample noise as generator input\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_samples = self.generator.predict([noise, labels])\n\n            # label smoothing\n            if epoch < epochs//1.5:\n                valid_smooth = (valid+0.1)-(np.random.random(valid.shape)*0.1)\n                fake_smooth = (fake-0.1)+(np.random.random(fake.shape)*0.1)\n            else:\n                valid_smooth = valid \n                fake_smooth = fake\n                \n            # Train the discriminator\n            self.discriminator.trainable = True\n            d_loss_real = self.discriminator.train_on_batch([samples, labels], valid_smooth)\n            d_loss_fake = self.discriminator.train_on_batch([gen_samples, labels], fake_smooth)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n            # Train Generator\n            self.discriminator.trainable = False\n            sampled_labels = np.random.randint(0, 2, batch_size).reshape(-1, 1)\n            # Train the generator\n            g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n\n            if (epoch+1)%sample_interval==0:\n                print('[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n                  % (epoch, epochs, d_loss[0], g_loss[0]))\n            G_losses.append(g_loss[0])\n            D_losses.append(d_loss[0])\n            if plot:\n                if epoch+1==epochs:\n                    plt.figure(figsize=(10,5))\n                    plt.title(\"Generator and Discriminator Loss\")\n                    plt.plot(G_losses,label=\"G\")\n                    plt.plot(D_losses,label=\"D\")\n                    plt.xlabel(\"iterations\")\n                    plt.ylabel(\"Loss\")\n                    plt.legend()\n                    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:05:08.323359Z","iopub.execute_input":"2021-12-01T08:05:08.323929Z","iopub.status.idle":"2021-12-01T08:05:08.361625Z","shell.execute_reply.started":"2021-12-01T08:05:08.323886Z","shell.execute_reply":"2021-12-01T08:05:08.360771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/home-credit-default-risk/bureau.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T07:59:43.824183Z","iopub.execute_input":"2021-12-01T07:59:43.824615Z","iopub.status.idle":"2021-12-01T07:59:48.791609Z","shell.execute_reply.started":"2021-12-01T07:59:43.824582Z","shell.execute_reply":"2021-12-01T07:59:48.790768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = preprocessing.LabelEncoder()\nfor i in ['CREDIT_ACTIVE','CREDIT_CURRENCY','CREDIT_TYPE']:\n    df[i] = le.fit_transform(df[i].astype(str))","metadata":{"execution":{"iopub.status.busy":"2021-12-01T07:59:48.79318Z","iopub.execute_input":"2021-12-01T07:59:48.793415Z","iopub.status.idle":"2021-12-01T07:59:50.802158Z","shell.execute_reply.started":"2021-12-01T07:59:48.793389Z","shell.execute_reply":"2021-12-01T07:59:50.801294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T07:59:50.803565Z","iopub.execute_input":"2021-12-01T07:59:50.803917Z","iopub.status.idle":"2021-12-01T07:59:50.830158Z","shell.execute_reply.started":"2021-12-01T07:59:50.803867Z","shell.execute_reply":"2021-12-01T07:59:50.828115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.CREDIT_ACTIVE.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T07:59:50.832479Z","iopub.execute_input":"2021-12-01T07:59:50.833147Z","iopub.status.idle":"2021-12-01T07:59:50.856155Z","shell.execute_reply.started":"2021-12-01T07:59:50.833098Z","shell.execute_reply":"2021-12-01T07:59:50.855209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\n\nX = scaler.fit_transform(df.drop('CREDIT_ACTIVE', 1))\ny = df['CREDIT_ACTIVE'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T07:59:50.857511Z","iopub.execute_input":"2021-12-01T07:59:50.858923Z","iopub.status.idle":"2021-12-01T07:59:52.409019Z","shell.execute_reply.started":"2021-12-01T07:59:50.858884Z","shell.execute_reply":"2021-12-01T07:59:52.408139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_1 = lgb.LGBMClassifier()\nlgb_1.fit(X_train, y_train)\n\ny_pred = lgb_1.predict(X_test)\n\n# evaluation\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(lgb_1, X_test, y_test)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T07:59:52.410246Z","iopub.execute_input":"2021-12-01T07:59:52.410453Z","iopub.status.idle":"2021-12-01T08:00:23.586651Z","shell.execute_reply.started":"2021-12-01T07:59:52.410428Z","shell.execute_reply":"2021-12-01T08:00:23.58567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cgan = cGAN()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:05:14.584203Z","iopub.execute_input":"2021-12-01T08:05:14.584712Z","iopub.status.idle":"2021-12-01T08:05:14.907514Z","shell.execute_reply.started":"2021-12-01T08:05:14.584676Z","shell.execute_reply":"2021-12-01T08:05:14.906575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = y_train.reshape(-1,1)\npos_index = np.where(y_train==1)[0]\nneg_index = np.where(y_train==0)[0]\ncgan.train(X_train, y_train, pos_index, neg_index, epochs=500)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:05:16.281895Z","iopub.execute_input":"2021-12-01T08:05:16.282206Z","iopub.status.idle":"2021-12-01T08:06:03.887129Z","shell.execute_reply.started":"2021-12-01T08:05:16.282172Z","shell.execute_reply":"2021-12-01T08:06:03.886218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### ","metadata":{}}]}