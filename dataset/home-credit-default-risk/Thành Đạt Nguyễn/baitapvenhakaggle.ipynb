{"cells":[{"metadata":{},"cell_type":"markdown","source":"I use the code from Will Koehrsen. Thanks for Will Koehrsen"},{"metadata":{},"cell_type":"markdown","source":"Data: Có 7 đa ta khác nhau\n1)application_train/application_test:Khoản vay chưa hoàn trả:1, khoản vay chưa hoàn trả:0.\n2)bureau:Khoản vay trước đây từ các tổ chức tài chính khác\n3)bureau_balance:Dữ liệu hàng tháng về các khoản vay trước đây trong văn phòng\n4)previous_application:Dữ liệu các khoản vay trước đây tại Home Credit\n5)POS_CASH_BALANCE:dữ liệu hàng hàng về các khoảng vay của khách hàng với Home Credit\n6)credit_card_balance:dữ liệu hàng tháng về các thẻ tín dụng mà khách hàng sử dụng với Home Credit\n7)installments_payment:lịch sử thanh toán với Home Credit\n"},{"metadata":{},"cell_type":"markdown","source":"Import:"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install scikit-misc\n!pip install scikit-learn==0.21.3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\n# sklearn preprocessing để xử lý các biên phân loại\nfrom sklearn.preprocessing import LabelEncoder\n\n# File quản lý tập tin\nimport os\n\n# loại bỏ cảnh báo \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for vẽ mô hình\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Đọc Data"},{"metadata":{},"cell_type":"markdown","source":"Liệt kê tất cả cá tệp dữ liệu có sẵn"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/home-credit-default-risk\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training Data\napp_train = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Có 307511 quan sát và 122 biến"},{"metadata":{"trusted":true},"cell_type":"code","source":"#TestingData\napp_test = pd.read_csv('../input/home-credit-default-risk/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"Phân tích dữ liệu khám phá (EDA) là một quy trình kết thúc mở, trong đó tính toán số liệu thống kê và đưa ra số liệu để tìm xu hướng, sự bất thường, mô hình hoặc mối quan hệ trong dữ liệu"},{"metadata":{},"cell_type":"markdown","source":"Kiểm tra sự phân bố của \"TARGET\""},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['TARGET'].astype(int).plot.hist();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Có nhiều khoản vay được hoàn trả đúng hạn hơn so với các khoản vay không được hoàn trả"},{"metadata":{},"cell_type":"markdown","source":"# Kiểm tra các giá trị còn thiếu"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hàm tính toán các giá trị còn thiếu theo cột \ndef missing_values_table(df):\n        # Tổng các giá trị còn thiếu\n        mis_val = df.isnull().sum()\n        \n        # Tỷ lệ phần trăm thiếu\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Lập bản với kết quả\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Đổi tên côt\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sắp sếp bảng theo tỉ lệ phần trăm giảm dần\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        #In thông tin cần thiết\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Trả về dữ liệu với thông tin còn thiếu\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values = missing_values_table(app_train)\nmissing_values.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Column Types"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hầu hết các biến phân loại có số lượng mục duy nhất tương đối nhỏ, tìm cách xử lý các biến phân loại này"},{"metadata":{},"cell_type":"markdown","source":"# Mã hóa Categorical Variables"},{"metadata":{},"cell_type":"markdown","source":"Label encoding: gán từng loại duy nhất trong một biến phân loại với một số nguyên"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tạo một nhãn\nle = LabelEncoder()\nle_count = 0\n\n# Lặp lại qua các cột\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # nếu 2 hoặc ít hơn unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(app_train[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One-hot encoding:tạo một cột mới cho mỗi danh mục duy nhất trong một biến phân loại. Mỗi quan sát nhận được 1 trong cột cho loại tương ứng và 0 trong tất cả các cột mới khác."},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sắp xếp Training and Testing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = app_train['TARGET']\n\n# Chỉnh lại dữ liệu train và test chỉ giữ lại các cột có trong ca hai tệp dữ liệu\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n\n# Thêm \"TARGET\"\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# Quay lại phân tích dữ liệu thăm dò"},{"metadata":{},"cell_type":"markdown","source":"Các số trong cột DAYS_BIRTH âm bởi vì chúng được ghi lại liên quan đến đơn xin vay hiện tại. Để xem các số liệu thống kê này theo năm, chúng ta có thể nhân đôi -1 và chia cho số ngày trong một năm"},{"metadata":{"trusted":true},"cell_type":"code","source":"(app_train['DAYS_BIRTH'] / -365).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The maximum value (besides being positive) is about 1000 years!"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"tập hợp các khách hàng dị thường và xem liệu họ có xu hướng có tỷ lệ mặc định cao hơn hoặc thấp hơn so với các khách hàng còn lại."},{"metadata":{"trusted":true},"cell_type":"code","source":"anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Điền vào các giá trị dị thường không phải là một số (np.nan) và sau đó tạo một cột boolean mới cho biết giá trị đó có bất thường hay không."},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tạo cột mới và điền vào cột hiện có với np.nan trong dữ liệu test."},{"metadata":{"trusted":true},"cell_type":"code","source":"app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlations"},{"metadata":{},"cell_type":"markdown","source":"Một cách để thử và hiểu dữ liệu là tìm kiếm mối tương quan giữa các tính năng và mục tiêu. Chúng ta có thể tính hệ số tương quan Pearson giữa mọi biến và mục tiêu bằng phương pháp khung dữ liệu .corr.\nHệ số tương quan không phải là phương pháp tốt nhất để biểu thị \"mức độ liên quan\" của một tính năng, nhưng nó cho chúng ta ý tưởng về các mối quan hệ có thể có trong dữ liệu."},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = app_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DAYS_BIRTH là mối tương quan tích cực nhất. (ngoại trừ TARGET vì tương quan của một biến với chính nó luôn là 1!) Nhìn vào tài liệu, DAYS_BIRTH là độ tuổi tính theo ngày của khách hàng tại thời điểm cho vay trong những ngày âm (vì bất kỳ lý do gì!). Mối tương quan là tích cực, nhưng giá trị của tính năng này thực sự là âm, có nghĩa là khi khách hàng già đi, họ ít có khả năng mặc định cho khoản vay của mình (ie the target == 0)"},{"metadata":{},"cell_type":"markdown","source":"# Ảnh hưởng của tuổi tác với việc hoàn trả"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tìm mối tương qua giữa birth và target\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Khi khách hàng già đi, họ có xu hướng trả nợ đúng hạn thường xuyên hơn\nTạo ra một biểu đồ histogram of the age đặt trục x trong nhiều năm để làm cho nó dễ hiểu hơn một chút."},{"metadata":{},"cell_type":"markdown","source":"Tiếp theo chúng ta sẽ tạo một kernel density estimation plot (KDE) được tô màu bằng giá trị của mục tiêu. A kernel density estimate plot shows the distribution of a single variable(nó được tạo bằng cách tính toán một hạt nhân, thường là một Gaussian, tại mỗi điểm dữ liệu và sau đó lấy trung bình tất cả các hạt nhân riêng lẻ để phát triển một độ mịn đường cong)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 8))\n\n# KDE plot các khoảng vay trả đúng hạng\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n\n# KDE plot các khoảng vay không trả đúng hạng\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n\n# Hiện biểu đồ\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target == 1 nghiêng về phía cuối của phạm vi trẻ.\nMặc dù đây không phải là một mối tương quan đáng kể (-0.07 correlation coefficient), biến này có thể sẽ hữu ích trong mô hình học máy vì nó ảnh hưởng đến"},{"metadata":{},"cell_type":"markdown","source":"Cắt loại tuổi thành từng hộp 5 năm. Sau đó, với mỗi hộp,tính giá trị trung bình của mục tiêu, cho ta biết tỷ lệ các khoản vay không được hoàn trả trong từng loại tuổi."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Điền thông tin tuổi vào 1 separate dataframe\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group by the hộp và tính trung bình\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 8))\n\n# Vẽ đồ thị các thùng tuổi và mức trung bình của mục tiêu dưới dạng biểu đồ thanh\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nCó một xu hướng rõ ràng: những ứng viên trẻ tuổi có nhiều khả năng không trả được khoản vay! Tỷ lệ không trả nợ là trên 10% cho ba nhóm tuổi trẻ nhất và 5% cho nhóm tuổi già nhất.\nĐây là thông tin có thể được ngân hàng sử dụng trực tiếp: bởi vì các khách hàng trẻ tuổi ít có khả năng trả nợ, có lẽ họ nên được cung cấp thêm hướng dẫn hoặc mẹo lập kế hoạch tài chính. Điều này không có nghĩa là ngân hàng nên phân biệt đối xử với khách hàng trẻ tuổi, nhưng sẽ rất thông minh khi thực hiện các biện pháp phòng ngừa để giúp khách hàng trẻ thanh toán đúng hạn."},{"metadata":{},"cell_type":"markdown","source":"# Exterior Sources"},{"metadata":{},"cell_type":"markdown","source":"3 biến có tương quan âm mạnh nhất với mục tiêu là EXT_SOURCE_1, EXT_SOURCE_2 và EXT_SOURCE_3. Theo tài liệu này, các tính năng này thể hiện\"normalized score from external data source\""},{"metadata":{},"cell_type":"markdown","source":"Hiển thị các mối tương quan của các tính năng EXT_SOURCE với mục tiêu và giữa chúng với nhau với nhau."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trích xuất các biến EXT_SOURCE và hiển thị tương quan\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 6))\n\n# Sơ đồ tương quan\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tất cả ba tính năng EXT_SOURCE đều có tương quan âm với mục tiêu, cho thấy rằng khi giá trị của EXT_SOURCE tăng lên, khách hàng có nhiều khả năng trả nợ hơn. Chúng ta cũng có thể thấy rằng DAYS_BIRTH có mối tương quan tích cực với EXT_SOURCE_1 chỉ ra rằng có thể một trong những yếu tố trong điểm số này là tuổi của khách hàng.\n\nTiếp theo chúng ta có thể xem xét sự phân phối của từng tính năng này được tô màu bởi giá trị của mục tiêu. Điều này sẽ cho chúng ta hình dung ảnh hưởng của biến này đến mục tiêu."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 12))\n\n# lặp qua các nguồn\nfor i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    \n    # tạo một subplot mới cho mỗi nguồn\n    plt.subplot(3, 1, i + 1)\n    # plot Trả nợ\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n    # plot các khoản vay không được hoàn trả\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nEXT_SOURCE_3 hiển thị sự khác biệt lớn nhất giữa các giá trị của mục tiêu. Chúng ta có thể thấy rõ rằng tính năng này có một số mối quan hệ với khả năng người nộp đơn trả nợ."},{"metadata":{},"cell_type":"markdown","source":"# Pairs Plot"},{"metadata":{},"cell_type":"markdown","source":"Chúng ta có thể tạo một cặp biểu đồ của các biến EXT_SOURCE và biến DAYS_BIRTH.\nSử dụng thư viện trực quan hóa đáy biển và hàm PairGrid để tạo ra một Lô ghép với các biểu đồ phân tán trên tam giác trên, biểu đồ trên đường chéo và các biểu đồ mật độ hạt nhân 2D và các hệ số tương quan trên tam giác dưới."},{"metadata":{"trusted":true},"cell_type":"code","source":"# sao chép dữ liệu for plotting\nplot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n\n# Thêm tuổi của khách hàng trong năm\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n\n# Giảm giá trị na và giới hạn ở 100000 hàng đầu tiên\nplot_data = plot_data.dropna().loc[:100000, :]\n\n# Hàm tính hệ số tương quan giữa hai cột\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)\n\n# Tạo đối tượng cặp\ngrid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n                    hue = 'TARGET', \n                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.2)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Màu đỏ biểu thị các khoản vay không được hoàn trả và màu xanh là các khoản vay được trả. Chúng ta có thể thấy các mối quan hệ khác nhau trong dữ liệu. Dường như có mối quan hệ tuyến tính tích cực vừa phải giữa EXT_SOURCE_1 và DAYS_BIRTH (hoặc tương đương YEARS_BIRTH), cho thấy tính năng này có thể tính đến tuổi của khách hàng."},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"Feature engineering đề cập đến một quá trình tạo gen và có thể liên quan đến cả việc xây dựng tính năng: thêm các tính năng mới từ dữ liệu hiện có và lựa chọn tính năng: chỉ chọn các tính năng quan trọng nhất hoặc các phương pháp giảm kích thước khác. Có nhiều kỹ thuật chúng ta có thể sử dụng để tạo cả tính năng và chọn tính năng."},{"metadata":{},"cell_type":"markdown","source":"**Polynomial Features**"},{"metadata":{},"cell_type":"markdown","source":"Trong phương pháp này, ta tạo ra các tính năng là sức mạnh của các tính năng hiện có cũng như các điều khoản tương tác giữa các tính năng hiện có. Ví dụ: chúng tôi có thể tạo các biến EXT_SOURCE_1 ^ 2 và EXT_SOURCE_2 ^ 2 và cả các biến như EXT_SOURCE_1 x EXT_SOURCE_2, EXT_SOURCE_1 x EXT_SOURCE_2 ^ 2, EXT_SOURCE_1 ^ 2 x EXT_SOURCE Các tính năng này là sự kết hợp của nhiều biến riêng lẻ được gọi là thuật ngữ tương tác vì chúng nắm bắt được sự tương tác giữa các biến."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tạo một khung dữ liệu mới cho polynomial features\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\n# imputer để xử lý các giá trị còn thiếu\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'median')\n\npoly_target = poly_features['TARGET']\n\npoly_features = poly_features.drop(columns = ['TARGET'])\n\n# Áp đặt các giá trị còn thiếu\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n                                  \n# Tạo 1 đa thức xác định mức độ\npoly_transformer = PolynomialFeatures(degree = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Điều này tạo ra một số lượng đáng kể các tính năng mới. Để có được tên, chúng ta phải sử dụng các tính năng đa thức get_feature_names"},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Xem liệu bất kỳ tính năng mới nào trong số này có tương quan với mục tiêu hay không."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tạo một khung dữ liệu của các tính năng\npoly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Add in the target\npoly_features['TARGET'] = poly_target\n\n# Tìm sự tương quan với target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Hiển thị tính tích cực và tiêu cực\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Một số biến mới có mối tương quan lớn hơn (về độ lớn tuyệt đối) với target hơn là các tính năng đầu.Thêm các tính năng này vào một bản sao của dữ liệu đào tạo và thử nghiệm và sau đó đánh giá các mô hình có và không có các tính năng"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Đặt tính năng kiểm tra vào khung dữ liệu\npoly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Hợp nhất các tính năng đa thức vào khung dữ liệu đào tạo\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n\n# Hợp nhất các tính năng đa ngôn ngữ vào khung dữ liệu thử nghiệm\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n\n# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Domain Knowledge Features"},{"metadata":{},"cell_type":"markdown","source":"Tạo ra một vài tính năng cố gắng nắm bắt những gì có thể quan trọng để cho biết khách hàng có mặc định cho khoản vay hay không\nCREDIT_INCOME_PERCENT: tỷ lệ phần trăm của số tiền tín dụng so với thu nhập của khách hàng\nANNUITY_INCOME_PERCENT: tỷ lệ phần trăm của tiền hằng năm cho vay so với thu nhập của khách hàng\nCREDIT_TERM: thời hạn thanh toán tính theo tháng\nDAYS_EMPLOYED_PERCENT: tỷ lệ phần trăm số ngày làm việc so với tuổi của khách hàng"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make the same KDE plot colored by the value of the TARGET."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 20))\n# Duyệt qua các tính năng mới\nfor i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n    \n    #tạo một subplot mới cho mỗi nguồn\n    plt.subplot(4, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % feature)\n    plt.xlabel('%s' % feature); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Đường cơ sở"},{"metadata":{},"cell_type":"markdown","source":"Đối với đường cơ sở, chúng ta có thể đoán cùng một giá trị cho tất cả các ví dụ trên tập kiểm tra\nĐoán 0,5 cho tất cả các quan sát trên bộ thử nghiệm\nSử dụng hồi quy logistic để dự đoàn đường cơ sở"},{"metadata":{},"cell_type":"markdown","source":"# Thực hiện hồi quy"},{"metadata":{},"cell_type":"markdown","source":"\nĐể có được đường cơ sở, ta sẽ sử dụng tất cả các tính năng sau khi mã hóa các biến phân loại\nXử lý trước dữ liệu bằng cách điền vào các giá trị còn thiếu (chỉ định) và chuẩn hóa phạm vi của các tính năng (tính năng chia tỷ lệ)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, Imputer\n\n# Lấy mục tiêu từ dữ liệu huấn luyện\nif 'TARGET' in app_train:\n    train = app_train.drop(columns = ['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# Trung bình các giá trị thiếu\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Chuyển đổ dữ liệu train và test\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sử dụng cú pháp mô hình Scikit-Learn quen thuộc: trước tiên tạo mô hình, sau đó chúng tôi huấn luyện mô hình bằng cách sử dụng .fit và sau đó đưa ra dự đoán về dữ liệu thử nghiệm bằng cách sử dụng .predict_proba"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Tạo mô hình với tham số chính quy đã chỉ định\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sử dụng phương pháp dự đoán mô hình. Điều này trả về một mảng m x 2 trong đó m là số lượng quan sát. Cột đầu tiên là xác suất của mục tiêu là 0 và cột thứ hai là xác suất của mục tiêu là 1. Do muốn xác nhận khoảng vay không hoàng trả nên chọn cột 2."},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg_pred = log_reg.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Các dự đoán phải ở định dạng được hiển thị trong tệp sample_submission.csv, trong đó chỉ có hai cột: SK_ID_CURR và TARGET"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Các dự đoán đại diện cho xác suất từ 0 đến 1 rằng khoản vay sẽ không được hoàn trả."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the submission to a csv file\nsubmit.to_csv('log_reg_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Đường cơ sở hồi quy logistic sẽ đạt khoảng 0,671 khi được gửi."},{"metadata":{},"cell_type":"markdown","source":"# Cải thiện Model bằng Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train on the training data\nrandom_forest.fit(train, train_labels)\n\n# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = random_forest.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mô hình này sẽ đạt khoảng 0,678 khi gửi."},{"metadata":{},"cell_type":"markdown","source":"# Dự đoán bằng cách sử dụng các tính năng thiết kế"},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_features_names = list(app_train_poly.columns)\n\n# Impute the polynomial features\nimputer = Imputer(strategy = 'median')\n\npoly_features = imputer.fit_transform(app_train_poly)\npoly_features_test = imputer.transform(app_test_poly)\n\n# Scale the polynomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\npoly_features = scaler.fit_transform(poly_features)\npoly_features_test = scaler.transform(poly_features_test)\n\nrandom_forest_poly = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train on the training data\nrandom_forest_poly.fit(poly_features, train_labels)\n\n# Đưa ra dữ liệu thử nghiệm\npredictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_engineered.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This scores 0.679 when submitted "},{"metadata":{},"cell_type":"markdown","source":"# Giải thích mô hình: Tầm quan trọng của Feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importances(df):\n    #Sắp xếp các tính năng theo mức độ quan trọng\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    #Bình thường quá các tính năng quan trọng\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n\n    # Biểu đồ thể hiện tính năng quan trọng\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Đảo ngược chỉ số vẽ đồ thị\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show biểu đồ\nfeature_importances_sorted = plot_feature_importances(feature_importances)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Như mong đợi, các tính năng quan trọng nhất là những tính năng liên quan đến EXT_SOURCE và DAYS_BIRTH."},{"metadata":{},"cell_type":"markdown","source":"# Light Gradient Boosting Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport gc\n\ndef model(features, test_features, encoding = 'ohe', n_folds = 5):\n    # Trích xuất id\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Trích dẫn nhãn\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Căn chỉnh dataframe theo các cột\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # Danh sách lưu trữ các chỉ số phân loại\n        cat_indices = []\n        \n        # Lặp qua từng cột\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Ánh xạ các tính năng phân loại\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Ghi lại các chỉ số phân loại\n                cat_indices.append(i)\n    \n    # Bắt lỗi nếu sơ đồ mã hóa nhãn không hợp lệ\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Trích xuất tên tính năng\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Danh sách ghi lại xác nhận và điểm train\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Thêm điểm tổng thể vào số liệu\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe của điểm xác nhận\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission, fi, metrics = model(app_train, app_test)\nprint('Baseline metrics')\nprint(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi_sorted = plot_feature_importances(fi)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('baseline_lgb.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train_domain['TARGET'] = train_labels\n\n# Test the domain knolwedge features\nsubmission_domain, fi_domain, metrics_domain = model(app_train_domain, app_test_domain)\nprint('Baseline with domain knowledge features metrics')\nprint(metrics_domain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi_sorted = plot_feature_importances(fi_domain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_domain.to_csv('baseline_lgb_domain_features.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}