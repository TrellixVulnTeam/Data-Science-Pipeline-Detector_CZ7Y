{"cells":[{"metadata":{},"cell_type":"markdown","source":"# HomeCredit Default Risk"},{"metadata":{},"cell_type":"markdown","source":"![](https://www.homecredit.co.id/HCID/media/images/HCID_logo.jpg)"},{"metadata":{},"cell_type":"markdown","source":"Due to the large amount of data, I've imported all the tables into my local SQL Server instance and joined them into a view according to the data schema provided. That's not obviously possible here at Kaggle, so I've just used a single table to show you, how I would proceed in treating an imbalance dataset with a hefty amount of attribures. Thus, do not expect the model accuracy to be anywhere beyond random - this is only a methodical approache.\n\nWhat you'll essentialy see here is an object encoding followed by PCA reduction."},{"metadata":{},"cell_type":"markdown","source":"### Steps to solve the problem"},{"metadata":{},"cell_type":"markdown","source":"1. Load libraries\n2. Load data\n3. EDA\n4. Data preparation\n    * Anomaly detection\n    * Outlier detection\n    * Null handling\n5. Object encoding\n    * Option 1 - Dummy variables\n    * Option 2 - OneHotEncoding\n    * Standartization\n    * PCA\n6. Model Baseline\n7. Model Evaluation\n    * Confusion Matrix\n    * F-1 Score\n    * Precision and Recall\n    * Precision-Recall Curve\n    * ROC Curve\n    * AUROC Score"},{"metadata":{},"cell_type":"markdown","source":"## LOAD LIBRARIES"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basic\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Common tools\nimport os\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom collections import Counter\n'''\n# Advanced visualization\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot, init_notebook_mode\n# Using plotly + cufflinks in offline mode\nimport cufflinks\ncufflinks.go_offline(connected=True)\ninit_notebook_mode(connected=True)\n'''\n# Model\nfrom sklearn import ensemble, tree, svm, naive_bayes, neighbors, linear_model, gaussian_process, neural_network\nfrom sklearn.metrics import accuracy_score, f1_score, auc, roc_curve, roc_auc_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\nfrom sklearn.ensemble import VotingClassifier\n\n# Configure Defaults\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\npd.set_option('display.max_colwidth', -1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LOAD DATA "},{"metadata":{},"cell_type":"markdown","source":"I'll load just very basic files here. We're not going to do data merge as mentioned above."},{"metadata":{"trusted":true},"cell_type":"code","source":"# List all files\nprint(os.listdir('../input/'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/application_train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test = pd.read_csv('../input/application_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_submission.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Target distribution "},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.countplot(x='TARGET', data=train)\nprint(train.TARGET.sum()/train.TARGET.count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data-set is cleary imbalanced. We'll have that in mind."},{"metadata":{},"cell_type":"markdown","source":"## Basic EDA "},{"metadata":{},"cell_type":"markdown","source":"The number of features is medium high, so we'll do a general glance of the data and will not dig deep into invidual features."},{"metadata":{},"cell_type":"markdown","source":"### General overview"},{"metadata":{"trusted":false},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Number of categories within a categorical feature\ntrain.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's possibly 41 numerical categories and 16 categorial features which could be treat as nominal. Given the size of the dataset, we'll probably employ OHE encoding, followed by PCA reduction."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Missing values\nn = train.isnull().sum() / len(train)\nn.sort_values(ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's a lot of missing values within numerical features reaching 70%. We can drop them or use some model which can handle Nulls like XGBoost."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Number of features exceeding 1/6 of missing values.\nsum(i>0.1667 for i in n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Almost half of the dataset contains missing values exceeding 1/6 of a feature. Numerical and object categories could be imputed with a default value. Float features < 16% imputed with a mean."},{"metadata":{},"cell_type":"markdown","source":"## DATA WRANGLING"},{"metadata":{},"cell_type":"markdown","source":"### Anomaly detection"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Age\n(train['DAYS_BIRTH'] / -365).describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This feature looks good. Loans are provided to people within an age range 20-70 years."},{"metadata":{},"cell_type":"markdown","source":"#### Days employed"},{"metadata":{"trusted":false},"cell_type":"code","source":"(train['DAYS_EMPLOYED'] / 365).describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Maximal employed time over 1,000 years doesn't look valid. High std confirms there's a huge dispercy in data."},{"metadata":{"trusted":false},"cell_type":"code","source":"train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The value is quite frequently used for some reason, possible as a kind of a default value."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Out of curiosity...\nanom = train[train['DAYS_EMPLOYED'] == 365243]\nnon_anom = train[train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" We will feature engineer the column now here before we'll do outlier removal and dataset merge."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create new feature\ntrain['DAYS_EMPLOYED_BOOL'] = train['DAYS_EMPLOYED'] == 365243\ntrain['DAYS_EMPLOYED'].replace({365243:np.nan}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check orginal feature\ntrain['DAYS_EMPLOYED'].plot.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outlier removal  "},{"metadata":{},"cell_type":"markdown","source":"Money-related features would be are best guess, but let's iterate over the whole dataset as due to the Tukey method."},{"metadata":{"trusted":false},"cell_type":"code","source":"def detect_outliers(df,n,features):\n    \n    outlier_indices = []\n    \n    for col in features:\n        Q1 = df[col].quantile(0.02)\n        Q3 = df[col].quantile(0.98)\n        IQR = Q3 - Q1\n        \n        outliers = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR )].index\n        outlier_indices.extend(outliers)\n        \n    # Select observations with more than n outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Return only float64 features\nnumerical_feature_mask = train.dtypes==float\nnumerical_cols = train.columns[numerical_feature_mask].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Detect outliers\nOutliers_to_drop = detect_outliers(train,2,numerical_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Number of outliers to drop\nlen(Outliers_to_drop)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's an acceptable number."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Remove outliers\ntrain.drop(Outliers_to_drop, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Concat data "},{"metadata":{},"cell_type":"markdown","source":"Create a single dataframe for conviniet feature engineering and futher handling."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Save Id for the submission at the very end.\nId = test['SK_ID_CURR']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Get marker\nsplit = len(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Merge into one dataset\ndata =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We don't need the Id anymore now.\ndata.drop('SK_ID_CURR', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handle nulls "},{"metadata":{},"cell_type":"markdown","source":"Alternatively, we can calculate std and randomely generate a number within, which would allow us to set a higher threshold and potentially spare some features."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Remove mostly sparse features\nfor f in data:\n   if data[f].isnull().sum() / data.shape[0] >= 0.5: del data[f] # Or do a boolean flag here","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check for TARGET data type\ndata['TARGET'].dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Select columns due to theirs data type\nfloat_col = data.select_dtypes('float').drop(['TARGET'], axis=1)\nint_col = data.select_dtypes('int')\nobject_col = data.select_dtypes('object')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Remove and impute numerical features\nfor f in float_col:\n   if data[f].isnull().sum() / data.shape[0] > 0.1667: del data[f] # Remove 1/6+ of NANs\n   else: data[f] = data[f].fillna(data[f].mean()) # Impute others with a mean value","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Impute default value into a numerical category\nfor i in int_col:\n   data[i] = data[i].fillna(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Impute object type with a default\nfor o in object_col:\n   data[o] = data[o].fillna('Unknown')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check\ndata.isnull().sum().sort_values(ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only the target in test dataset should be left with NANs."},{"metadata":{},"cell_type":"markdown","source":"### Obejct encoding"},{"metadata":{},"cell_type":"markdown","source":"One way here would be to use LabelEncoder followed by OHE or to use DictVectorizer which both supports spare matrix output for high model training performance. Another possibility is to use dummy variables which is probably the most easist way of object encoding that I'll prefere here as the number of features isn't yet that huge."},{"metadata":{"trusted":false},"cell_type":"code","source":"data = pd.get_dummies(data, prefix_sep='_', drop_first=True) # Drop originall feature to avoid multi-collinearity","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Object encoding (the other way)"},{"metadata":{"trusted":false},"cell_type":"code","source":"'''# Categorical mask\ncategorical_feature_mask = train.dtypes==object\n# Get categorical columns\ncategorical_cols = train.columns[categorical_feature_mask].tolist()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''# Instantiate LE\nle = LabelEncoder()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''# Apply LE\ntrain[categorical_cols] = train[categorical_cols].apply(lambda col: le.fit_transform(col.astype(str)))'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''# Check\ntrain[categorical_cols].head(10)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''# Instantiate OHE\nohe = OneHotEncoder(categorical_features = categorical_feature_mask, sparse=False ) #Can be enabled True for higher preformance'''","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"'''# Apply OHE\ntrain_ohe = ohe.fit_transform(train) #an numpy array'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standardize data "},{"metadata":{},"cell_type":"markdown","source":"In order to perform PCA, we need to standardize our data."},{"metadata":{"trusted":false},"cell_type":"code","source":"#Split data\ntrain_c = data[:split]\ntest_c = data[split:].drop(['TARGET'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Get variables for a model\nx = train_c.drop([\"TARGET\"], axis=1)\ny = train_c[\"TARGET\"]\n\n#Do train data splitting\nX_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.22, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train) # Fit on training set only.\n\n# Apply transform to both the training set and the test set.\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(.95)\npca.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train = pca.transform(X_train)\nX_test = pca.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model "},{"metadata":{},"cell_type":"markdown","source":"### Baseline"},{"metadata":{},"cell_type":"markdown","source":"I've choise following algorithm because it is fast."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(solver = 'lbfgs').fit(X_train, y_train)\npred = lr.predict(X_test)\nacc = lr.score(X_test, y_test)\n\nprint(\"Accuracy: \", acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Evaluation  "},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix "},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\n\npredicts = cross_val_predict(lr, X_train, y_train, cv=3)\nconfusion_matrix(y_train, predicts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the first row, 218718 clients were correctly predicted as not having a payment difficulties (true negatives) and 17 were wrongly classified as not having a payment difficulty (false negative).\nIn the second row 19192 clients were wrongly classified as having a payment difficulty (false possitive) and 17 of them correcly classified as having a payment difficulty (true negative)."},{"metadata":{},"cell_type":"markdown","source":"### Precision and Recall "},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision:\", precision_score(y_train, predicts))\nprint(\"Recall:\",recall_score(y_train, predicts))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The precission tells a probability with a client will be classified correctly. The recall tells us that it predicted a payment difficulty of 73 % of the clients who actually had the payment difficulty."},{"metadata":{},"cell_type":"markdown","source":"### F-1 Score"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nf1_score(y_train, predicts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The F-score is computed as the harmonic mean of both precision and recall, thus high F1 score is possible only if both precesion and recall are high."},{"metadata":{},"cell_type":"markdown","source":"### Precision Recall Curve"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\ny_scores = lr.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(y_train, y_scores)\n\ndef plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n    plt.xlabel(\"threshold\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(14, 7))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on rapid recall curve fall is possible to set recall/precision trade-off before."},{"metadata":{},"cell_type":"markdown","source":"### ROC Curve "},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, y_scores)\n\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n\nplt.figure(figsize=(14, 7))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is probably the most important measure which is shared among most model-predicting technologies.\nThe more is the blue curve leaning towards upper-left corned (right angle), the better a model is in predicting actual results. "},{"metadata":{},"cell_type":"markdown","source":"### ROC Score"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_scores = lr.predict_proba(X_train)\ny_scores = y_scores[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auroc = roc_auc_score(y_train, y_scores)\nprint(\"ROC-AUC Score:\", auroc)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}