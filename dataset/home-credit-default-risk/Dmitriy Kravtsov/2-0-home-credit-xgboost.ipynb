{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 1000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/home-credit-default-risk/application_train.csv')\ntest = pd.read_csv('/kaggle/input/home-credit-default-risk/application_test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## not improve our score\n# from collections import Counter\n# num_col = train.loc[:,'NAME_CONTRACT_TYPE':'AMT_REQ_CREDIT_BUREAU_YEAR'].select_dtypes(exclude=['object']).columns\n# # Outlier detection \n\n# def detect_outliers(df,n,features):\n#     \"\"\"\n#     Takes a dataframe df of features and returns a list of the indices\n#     corresponding to the observations containing more than n outliers according\n#     to the Tukey method.\n#     \"\"\"\n#     outlier_indices = []\n    \n#     # iterate over features(columns)\n#     for col in features:\n#         # 1st quartile (25%)\n#         Q1 = np.percentile(df[col], 25)\n#         # 3rd quartile (75%)\n#         Q3 = np.percentile(df[col],75)\n#         # Interquartile range (IQR)\n#         IQR = Q3 - Q1\n        \n#         # outlier step\n#         outlier_step = 1.5 * IQR\n        \n#         # Determine a list of indices of outliers for feature col\n#         outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n#         # append the found outlier indices for col to the list of outlier indices \n#         outlier_indices.extend(outlier_list_col)\n        \n#     # select observations containing more than 2 outliers\n#     outlier_indices = Counter(outlier_indices)        \n#     multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n#     return multiple_outliers   \n\n# # detect outliers from Age, SibSp , Parch and Fare\n# Outliers_to_drop = detect_outliers(train,2, num_col)\n# train.loc[Outliers_to_drop] # Show the outliers rows\n# # Drop outliers\n# train = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat((train.loc[:,'NAME_CONTRACT_TYPE':'AMT_REQ_CREDIT_BUREAU_YEAR'], test.loc[:,'NAME_CONTRACT_TYPE':'AMT_REQ_CREDIT_BUREAU_YEAR']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# before tuning\n\ndef basic_details(df):\n    b = pd.DataFrame()\n    b['Missing value, %'] = round(df.isnull().sum()/df.shape[0]*100)\n    b['N unique value'] = df.nunique()\n    b['dtype'] = df.dtypes\n    return b\nbasic_details(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# simplest NaN imputation\n\nfor col in df:\n    if df[col].dtype == 'object':\n        df[col].fillna('N')\n    df[col].fillna(10000, inplace=True) ## one of the best result (0.7599 vs 0.75760 of final result)\n    \n# # polynomial features adding (is not good idea finally)\n# from sklearn.preprocessing import PolynomialFeatures\n# poly_features = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'AMT_ANNUITY']] ## first 5 important features as per baseline model (therefore adding new 56 polynomial  features), but finally it's not give positive result (0.7583 vs 0.75911)\n# poly_transformer = PolynomialFeatures(degree = 3)\n# poly_transformer.fit(poly_features)\n# poly_features = poly_transformer.transform(poly_features)\n# # # # Scale the polynomial features\n# # from sklearn.preprocessing import MinMaxScaler\n# # scaler = MinMaxScaler(feature_range = (0, 1))\n# # poly_features = scaler.fit_transform(poly_features)\n# # Put poly features into dataframe\n# poly_features = pd.DataFrame(poly_features, columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'AMT_ANNUITY']))\n# df = df.merge(poly_features)\n  \n# making category features from some numerical (part 1 - for short shape numerical features)\n\nfor col in df:\n    if df[col].nunique()<=30:\n        df[col] = df[col].astype(str)\n        \n# # making category features from some numerical (part 2 - for long shape numerical features) - not help us to increase score\n# num_col = df.select_dtypes(exclude=['object']).columns\n# for col in num_col:\n#     if df[col].nunique()>30:\n#         df[col+str('_category')] = pd.qcut(df[col].rank(method='first'),q=[0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1],labels=False,precision=1).astype(str)\n        \n\n# simplest encoding for object columns in df\n\ndf = pd.get_dummies(df) ## therefore adding  additional feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# after tuning\n\ndef basic_details(df):\n    b = pd.DataFrame()\n    b['Missing value, %'] = round(df.isnull().sum()/df.shape[0]*100,2)\n    b['N unique value'] = df.nunique()\n    b['dtype'] = df.dtypes\n    return b\nbasic_details(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # it's gives us negative (abt -0.004) result to baseline\n# def descriptive_stat_feat(df):\n#     df = pd.DataFrame(df)\n#     dcol= [c for c in df.columns if df[c].nunique()>=3]\n#     d_median = df[dcol].median(axis=0)\n#     d_mean = df[dcol].mean(axis=0)\n#     q1 = df[dcol].apply(np.float32).quantile(0.25)\n#     q3 = df[dcol].apply(np.float32).quantile(0.75)\n    \n#     #Add mean and median column to data set having more then 3 categories\n#     for c in dcol:\n#         df[c+str('_median_range')] = (df[c].astype(np.float32).values > d_median[c]).astype(np.int8)\n#         df[c+str('_mean_range')] = (df[c].astype(np.float32).values > d_mean[c]).astype(np.int8)\n#         df[c+str('_q1')] = (df[c].astype(np.float32).values < q1[c]).astype(np.int8)\n#         df[c+str('_q3')] = (df[c].astype(np.float32).values > q3[c]).astype(np.int8)\n#     return df\n\n# df = descriptive_stat_feat(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating matrices for feature selection:\nX_train = df[:train.shape[0]]\nX_test_fin = df[train.shape[0]:]\ny = train.TARGET\nX_train['Y'] = y\ndf = X_train\n\nX = df.drop('Y', axis=1)\ny = df.Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n\nx_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=10)\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\nd_test = xgb.DMatrix(X_test_fin)\n\nparams = {\n        'objective':'binary:logistic',\n        'tree_method':'gpu_hist',\n        'eta': 0.3,\n        'max_depth':6,\n        'learning_rate':0.01,\n        'eval_metric':'auc',\n        'min_child_weight':2,\n        'subsample':0.8,\n        'colsample_bytree':0.7,\n        'seed':29,\n        'reg_lambda':0.8,\n        'reg_alpha':0.000001,\n        'gamma':0.1,\n        'scale_pos_weight':1,\n        'n_estimators': 500,\n        'nthread':-1\n}\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\nnrounds=10000 \nmodel = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=50, \n                           maximize=True, verbose_eval=10)\np_test = model.predict(d_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame({'SK_ID_CURR':test['SK_ID_CURR'],'TARGET':p_test})\n\nsub.to_csv('submission.csv',index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nfig,ax = plt.subplots(figsize=(100,75))\nxgb.plot_importance(model,ax=ax,height=0.8,color='r')\n#plt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}