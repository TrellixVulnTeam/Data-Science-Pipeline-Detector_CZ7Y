{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def merge_data():\n    print('merge data ...'.center(50, '*'))\n    gc.enable()\n    bur_bal = pd.read_csv('../input/bureau_balance.csv')\n    print('bureau_balance shape:', bur_bal.shape)\n    #bur_bal.head()\n    bur_bal = pd.concat([bur_bal, pd.get_dummies(bur_bal.STATUS, prefix='bur_bal_status')],\n                       axis=1).drop('STATUS', axis=1)\n    bur_cnts = bur_bal[['SK_ID_BUREAU', 'MONTHS_BALANCE']].groupby('SK_ID_BUREAU').count()\n    bur_bal['bur_cnt'] = bur_bal['SK_ID_BUREAU'].map(bur_cnts['MONTHS_BALANCE'])\n    avg_bur_bal = bur_bal.groupby('SK_ID_BUREAU').mean()\n    avg_bur_bal.columns = ['bur_bal_' + f_ for f_ in avg_bur_bal.columns]\n    del bur_bal\n    gc.collect()\n\n    bur = pd.read_csv('../input/bureau.csv')\n    print('bureau shape:', bur.shape)\n    #bur.head()\n    bur_credit_active_dum = pd.get_dummies(bur.CREDIT_ACTIVE, prefix='ca')\n    bur_credit_currency_dum = pd.get_dummies(bur.CREDIT_CURRENCY, prefix='cc')\n    bur_credit_type_dum = pd.get_dummies(bur.CREDIT_TYPE, prefix='ct')\n\n    bur_full = pd.concat([bur, bur_credit_active_dum, bur_credit_currency_dum, bur_credit_type_dum], axis=1).drop(['CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'CREDIT_TYPE'], axis=1)\n    del bur_credit_active_dum, bur_credit_currency_dum, bur_credit_type_dum\n    gc.collect()\n    bur_full = bur_full.merge(right=avg_bur_bal.reset_index(), how='left', on='SK_ID_BUREAU',suffixes=('', '_bur_bal'))\n    nb_bureau_per_curr = bur_full[['SK_ID_CURR', 'SK_ID_BUREAU']].groupby('SK_ID_CURR').count()\n    bur_full['SK_ID_BUREAU'] = bur_full['SK_ID_CURR'].map(nb_bureau_per_curr['SK_ID_BUREAU'])\n    avg_bur = bur_full.groupby('SK_ID_CURR').mean()\n    avg_bur.columns = ['bur_' + f_ for f_ in avg_bur.columns]\n    del bur, bur_full, avg_bur_bal\n    gc.collect()\n\n    prev = pd.read_csv('../input/previous_application.csv')\n    print('previous_application shape:', prev.shape)\n    #prev.head()\n    prev_cat_features = [f_ for f_ in prev.columns if prev[f_].dtype == 'object']\n    prev_dum = pd.DataFrame()\n    for f_ in prev_cat_features:\n        prev_dum = pd.concat([prev_dum, pd.get_dummies(prev[f_], prefix=f_)], axis=1)\n    prev = pd.concat([prev, prev_dum],axis=1)\n    del prev_dum\n    gc.collect()\n    nb_prev_per_curr = prev[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n    prev['SK_ID_PREV'] = prev['SK_ID_CURR'].map(nb_prev_per_curr['SK_ID_PREV'])\n    avg_prev = prev.groupby('SK_ID_CURR').mean()\n    avg_prev.columns = ['prev_' + f_ for f_ in avg_prev.columns]\n    del prev\n    gc.collect()\n\n    pos = pd.read_csv('../input/POS_CASH_balance.csv')\n    print('pos_cash_balance shape:', pos.shape)\n    #pos.head()\n    pos = pd.concat([pos, pd.get_dummies(pos['NAME_CONTRACT_STATUS'], prefix='ncs')], axis=1)\n    nb_prevs = pos[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n    pos['SK_ID_PREV'] = pos['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n    avg_pos = pos.groupby('SK_ID_CURR').mean()\n    avg_pos.columns = ['pos_' + f_ for f_ in avg_pos.columns]\n    del pos, nb_prevs\n    gc.collect()\n\n    cc_bal = pd.read_csv('../input/credit_card_balance.csv')\n    print('credit_card_balance shape:', cc_bal.shape)\n    cc_bal = pd.concat([cc_bal, pd.get_dummies(cc_bal['NAME_CONTRACT_STATUS'], prefix='ncs')], axis=1)\n    nb_prevs = cc_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n    cc_bal['SK_ID_PREV'] = cc_bal['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n    avg_cc_bal = cc_bal.groupby('SK_ID_CURR').mean()\n    avg_cc_bal.columns = ['cc_bal_' + f_ for f_ in avg_cc_bal.columns]\n    del cc_bal, nb_prevs\n    gc.collect()\n\n    inst = pd.read_csv('../input/installments_payments.csv')\n    print('installment_payment shape:', inst.shape)\n    nb_prevs = inst[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n    inst['SK_ID_PREV'] = inst['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n    avg_inst = inst.groupby('SK_ID_CURR').mean()\n    avg_inst.columns = ['inst_' + f_ for f_ in avg_inst.columns]\n    del inst, nb_prevs\n    gc.collect()\n\n    train = pd.read_csv('../input/application_train.csv')\n    test = pd.read_csv('../input/application_test.csv')\n    print('train shape:', train.shape)\n    print('test shape:', test.shape)\n    y = train['TARGET']\n    del train['TARGET']\n    cat_feats = [f_ for f_ in train.columns if train[f_].dtype == 'object']\n    for f_ in cat_feats:\n        train[f_], indexer = pd.factorize(train[f_])#类似于类似于类似于label encoder\n        test[f_] = indexer.get_indexer(test[f_])\n    train = train.merge(right = avg_bur.reset_index(), how='left', on='SK_ID_CURR')\n    test = test.merge(right = avg_bur.reset_index(), how='left', on='SK_ID_CURR')\n    train = train.merge(right = avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n    test = test.merge(right = avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n    train = train.merge(right = avg_pos.reset_index(), how='left', on='SK_ID_CURR')\n    test = test.merge(right = avg_pos.reset_index(), how='left', on='SK_ID_CURR')\n    train = train.merge(right = avg_cc_bal.reset_index(), how='left', on='SK_ID_CURR')\n    test = test.merge(right = avg_cc_bal.reset_index(), how='left', on='SK_ID_CURR')\n    train = train.merge(right = avg_inst.reset_index(), how='left', on='SK_ID_CURR')\n    test = test.merge(right = avg_inst.reset_index(), how='left', on='SK_ID_CURR')\n    del avg_bur, avg_prev, avg_pos, avg_cc_bal, avg_inst\n    gc.collect()\n    return train, test, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff50d8b79bf3f217861b42a8506cd2cd1fee0da8","collapsed":true},"cell_type":"code","source":"train, test, y = merge_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"974b672ec3debc41ceeebcbe14228a55f0ec5732"},"cell_type":"code","source":"def feat_ext_source(df):\n    x1 = df['EXT_SOURCE_1'].fillna(-1) + 1e-1\n    x2 = df['EXT_SOURCE_2'].fillna(-1) + 1e-1\n    x3 = df['EXT_SOURCE_3'].fillna(-1) + 1e-1\n    \n    df['EXT_SOURCE_1over2_NAminus1_Add0.1'] = x1/x2\n    df['EXT_SOURCE_2over1_NAminus1_Add0.1'] = x2/x1\n    df['EXT_SOURCE_1over3_NAminus1_Add0.1'] = x1/x3\n    df['EXT_SOURCE_3over1_NAminus1_Add0.1'] = x3/x1\n    df['EXT_SOURCE_2over3_NAminus1_Add0.1'] = x2/x3\n    df['EXT_SOURCE_3over2_NAminus1_Add0.1'] = x3/x2\n    df['EXT_SOURCE_1_log'] = np.log(df['EXT_SOURCE_1'] + 1)\n    df['EXT_SOURCE_2_log'] = np.log(df['EXT_SOURCE_2'] + 1)\n    df['EXT_SOURCE_3_log'] = np.log(df['EXT_SOURCE_3'] + 1) \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bf872e3c859590bc0b14f23d8a3f2b8e5e4205dc"},"cell_type":"code","source":"train = feat_ext_source(train)\ntest = feat_ext_source(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"39c75baeb776a9e1aeffe6ec9c3409a5a9a5c898"},"cell_type":"code","source":"def nan_process(train, test):\n    print('NaN process ...'.center(50, '*'))\n    print(train.shape, test.shape)\n    train = train.fillna(-1)\n    test = test.fillna(-1)\n    return train, test, y\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"921e34c117f55538a472fdc3b8aefed65131a606","collapsed":true},"cell_type":"code","source":"train, test, y = nan_process(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b0d1596735cbaeb93d3d82fd5a4e11ab25462e4a"},"cell_type":"code","source":"def smote(train, test, y):\n    print('smote ...'.center(50, '*'))\n    train, val_x, y, val_y = train_test_split(train, y, test_size=0.1, random_state = 14)\n    features = train.columns\n    #sm = SMOTE(random_state=42, kind='borderline2', n_jobs=-1)\n    #train, y = sm.fit_sample(train, y)\n    return train, test, y, val_x, val_y, features\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed13f67b9c340fb0cd53a89b39fc215aa8efa548","collapsed":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\ntrain, test, y, val_x, val_y, features = smote(train, test, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c854a92defa0f9ef7124f93fc7a4d274630bf184","collapsed":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\ndef reduce(train, test, y, val_x, val_y, features):\n    print('reduce memory usage...'.center(50, '*'))\n    train = pd.DataFrame(train)\n    val_x = pd.DataFrame(val_x)\n    test = pd.DataFrame(test)\n    train = reduce_mem_usage(train)\n    val_x = reduce_mem_usage(val_x)\n    test = reduce_mem_usage(test)\n    feats = [f_ for f_ in features if f_ not in ['SK_ID_CURR']]\n    y = pd.DataFrame(y)\n    val_y = pd.DataFrame(val_y)\n    train.columns = features\n    train = train[feats]\n    test.columns = features\n    try:\n        train.to_csv('train_.csv', index=False)\n        y.to_csv('y_.csv', index=False)\n        test.to_csv('test_.csv', index=False)\n        val_x.to_csv('val_x_.csv', index=False)\n        val_y.to_csv('val_y_.csv', index=False)\n    except IOError:\n        print('write to csv failed!')\n    return train, test, y,val_x, val_y, features, feats\n\ntrain, test, y,val_x, val_y, features, feats = reduce(train, test, y, val_x, val_y, features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f734703c03363d2dc11cd3210d489871ea8daf98"},"cell_type":"code","source":"train_x = pd.read_csv('train_.csv')\ntrain_y = pd.read_csv('y_.csv')\nval_x = pd.read_csv('val_x_.csv')\nval_y = pd.read_csv('val_y_.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fccbf87cbe1ece9f8661a4ff546ef6bef06eef95","collapsed":true},"cell_type":"code","source":"train_x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6887a17f0e6818607dfc87199a88b6e654bec775","collapsed":true},"cell_type":"code","source":"val_x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f74477c816098b2a43edf19be24240f3a24afa9","collapsed":true},"cell_type":"code","source":"val_x = val_x.drop(columns=['SK_ID_CURR'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1a06614744ec00ab2436bde7b14f4f632c4be37","collapsed":true},"cell_type":"code","source":"val_x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1385af2bf49e76a1ecfb968fe6392f2944edcb8c"},"cell_type":"code","source":"train_x, train_y, val_x, val_y = train_x.values, train_y.values, val_x.values, val_y.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d4318e338d9f135ce6d20d297b0c3c04938bf1b9"},"cell_type":"code","source":"def train_model(train_x, train_y, val_x_, val_y_, folds_):\n    \n    oof_preds = np.zeros(train_x.shape[0])\n    val_preds = np.zeros(val_x_.shape[0])\n    \n    for n_fold, (trn_idx, val_idx) in enumerate(folds_.split(train_x)):\n        #print(train_.type)\n        trn_x, trn_y = pd.DataFrame(train_x).iloc[trn_idx], pd.DataFrame(train_y).iloc[trn_idx]\n        val_x, val_y = pd.DataFrame(train_x).iloc[val_idx], pd.DataFrame(train_y).iloc[val_idx]\n        \n        clf = LGBMClassifier(\n            n_estimators = 4000,\n            learning_rate = 0.03,\n            num_leaves = 30,\n            colsample_bytree = .8,\n            subsample = .9,\n            max_depth = 7,\n            reg_alpha = .1,\n            min_split_gain = .01,\n            min_child_weight = 2,\n            silent = -1,\n            verbose = -1\n            )\n        clf.fit(trn_x, trn_y, \n            eval_set = [(trn_x, trn_y), (val_x, val_y)],\n            eval_metric = 'auc', verbose = 100, early_stopping_rounds = 100)\n        \n        oof_preds[val_idx] = clf.predict_proba(val_x, num_iteration = clf.best_iteration_)[:, 1]\n        val_preds += clf.predict_proba(pd.DataFrame(val_x_))[:, 1] / folds_.n_splits\n        print('fold %2d validate AUC score %.6f'%(n_fold + 1,roc_auc_score(val_y_, val_preds * folds_.n_splits)))\n        print('fold %2d AUC %.6f'%(n_fold+1, roc_auc_score(val_y, oof_preds[val_idx])))\n        del clf, trn_x, trn_y, val_x, val_y\n        gc.collect()\n    print('validate AUC score %.6f'%roc_auc_score(val_y_, val_preds))\n    print('full AUC score %.6f'%roc_auc_score(train_y, oof_preds))\n    \n    return oof_preds, val_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5cbfd3268451f27cf85d3ea17d6b4fddd41f6a9","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score\nfolds = KFold(n_splits=5, shuffle=True, random_state=0)\noof_preds, val_preds = train_model(train_x, train_y, val_x, val_y, folds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"928cd3ffb35e73740f80aad5175dda1ba0329839"},"cell_type":"code","source":"def rf_model(train_x, train_y, val_x_, val_y_):\n    \n    val_preds = np.zeros(val_x_.shape[0])  \n    clf = RandomForestClassifier(\n        max_features = 'sqrt',\n        n_estimators = 400,\n        min_samples_leaf = 10,\n        n_jobs = -1,\n        random_state = 14,\n        oob_score = True\n        )\n    clf.fit(train_x, train_y)\n    val_preds = clf.predict_proba(pd.DataFrame(val_x_))[:, 1]\n    print('validate AUC score %.6f'%roc_auc_score(val_y_, val_preds))\n    print('full AUC score %.6f'%roc_auc_score(train_y, clf.predict_proba(train_x)[:,1]))\n    \n    return oof_preds, val_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7980cf25f57acd575eeca661734d9c83a2124490"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b30893f56ed5fb758ef1396a9362384f77973ba","collapsed":true},"cell_type":"code","source":"oof_preds, val_preds = rf_model(train_x, train_y, val_x, val_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0df1807ce24727fbccb6dcb0b115a312110729af"},"cell_type":"code","source":"def et_model(train_x, train_y, val_x_, val_y_):\n    \n    val_preds = np.zeros(val_x_.shape[0])  \n    clf = ExtraTreesClassifier(\n        max_features = 'sqrt',\n        n_estimators = 400,\n        min_samples_leaf = 10,\n        n_jobs = -1,\n        random_state = 14,\n        oob_score = True,\n        bootstrap = True\n        )\n    clf.fit(train_x, train_y)\n    val_preds = clf.predict_proba(pd.DataFrame(val_x_))[:, 1]\n    print('validate AUC score %.6f'%roc_auc_score(val_y_, val_preds))\n    try:\n        print('full AUC score %.6f'%roc_auc_score(train_y, clf.predict_proba(train_x)[:,1]))\n    except:\n        print('oob_score error')\n    return oof_preds, val_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fdec4bed676dbcd7bcd05af95b0426d9086596e","collapsed":true},"cell_type":"code","source":"oof_preds, val_preds = et_model(train_x, train_y, val_x, val_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5f0b62d58c6e96e03755327248dd754ce2a2214a"},"cell_type":"code","source":"def gbdt_model(train_x, train_y, val_x_, val_y_):\n    \n    val_preds = np.zeros(val_x_.shape[0])  \n    clf = GradientBoostingClassifier(\n        max_features = 'sqrt',\n        n_estimators = 500,\n        learning_rate = 0.3,\n        subsample = 0.8,\n        min_samples_leaf = 5,\n        random_state = 14\n        )\n    clf.fit(train_x, train_y)\n    val_preds = clf.predict_proba(pd.DataFrame(val_x_))[:, 1]\n    \n    print('validate AUC score %.6f'%roc_auc_score(val_y_, val_preds))\n    try:\n        print('full AUC score %.6f'%roc_auc_score(train_y, clf.predict_proba(train_x)[:,1]))\n    except:\n        print('error!')\n    return oof_preds, val_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3886365c5550a982b22fd6df7862d57a7210398b","collapsed":true},"cell_type":"code","source":"oof_preds, val_preds = gbdt_model(train_x, train_y, val_x, val_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d35b15e4ef0497b98d513798d885b698e029bc17","collapsed":true},"cell_type":"code","source":"def xgb_model(train_x, train_y, val_x_, val_y_):\n    \n    val_preds = np.zeros(val_x_.shape[0])  \n    clf = XGBClassifier(\n        min_child_weight = 0.01,\n        n_jobs = -1,\n        learning_rate = 0.3,\n        n_estimators = 500,\n        gamma = 0.5,\n        subsample = 0.8,\n        colsample_bytree = 0.8,\n        booster = 'gbtree',\n        scale_pos_weight = 2,\n        reg_alpha = 1,\n        random_state = 14\n        )\n    clf.fit(train_x, train_y)\n    val_preds = clf.predict_proba(pd.DataFrame(val_x_))[:, 1]\n    \n    print('validate AUC score %.6f'%roc_auc_score(val_y_, val_preds))\n    try:\n        print('full AUC score %.6f'%roc_auc_score(train_y, clf.predict_proba(train_x)[:,1]))\n    except:\n        print('error!')\n    return oof_preds, val_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f040e197e89b9f90c10604b275c6812f793817ae","collapsed":true},"cell_type":"code","source":"oof_preds, val_preds = xgb_model(train_x, train_y, val_x, val_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"29697b52156e031ef1a649d6666d9ad73625672e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}