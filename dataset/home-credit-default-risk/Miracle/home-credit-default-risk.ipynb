{"cells":[{"metadata":{},"cell_type":"markdown","source":"This project comes from [this Kaggle competition](https://www.kaggle.com/c/home-credit-default-risk). The aim of the project is to build a prediction model for predicting how likely a loan will be defaulted."},{"metadata":{},"cell_type":"markdown","source":"**Import packages**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.utils import shuffle\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom pandas.api.types import CategoricalDtype\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nimport gc\nimport os\nfrom functools import partial\n\nprint(f\"Pandas version: {pd.__version__}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Program Constants**"},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_DIR = \"../input/\"\n\nget_full_path = partial(os.path.join, INPUT_DIR)\n\nPATH_APP_TRAIN = get_full_path(\"application_train.csv\")\nPATH_APP_TEST = get_full_path(\"application_test.csv\")\nPATH_PRE_APP = get_full_path(\"previous_application.csv\")\nPATH_INST_PAY = get_full_path(\"installments_payments.csv\")\nPATH_INST_PAY_PROCESSED = \"installments_payments_preprocessed.csv\"\nPATH_POS_BALANCE = get_full_path(\"POS_CASH_balance\")\nPATH_CREDIT_BALANCE = get_full_path(\"credit_card_balance.csv\")\nPATH_BUREAU = get_full_path(\"bureau.csv\")\nPATH_BUREAU_BALANCE = get_full_path(\"bureau_balance.csv\")\n\nPERFORM_IMPUTATION = True\nCROSS_VALIDATION_FOLD = 5\nRANDOM_SEED = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Define categorical types for data loading**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"LoanType = CategoricalDtype([\"Cash loans\", \"Revolving loans\", \"Consumer loans\"], False)\nHouseType = CategoricalDtype([\"block of flats\", \"terraced house\", \"specific housing\"])\nWeekDayType = CategoricalDtype(['SUNDAY', 'MONDAY', 'TUESDAY', 'WEDNESDAY', 'THURSDAY', 'FRIDAY', 'SATURDAY'], True)\nYesNoType = CategoricalDtype([\"N\", \"Y\"], True)\nLoanStatusType = CategoricalDtype(['Approved', 'Refused', 'Canceled', 'Unused offer'], False)\nEducationType = CategoricalDtype([\"Incomplete higher\", \"Higher education\", \"Lower secondary\",\n                                  \"Secondary / secondary special\", \"Academic degree\"], True)\nFamilyType = CategoricalDtype([\"Civil marriage\", \"Married\", \"Separated\", \"Single / not married\", \"Widow\"], False)\nHousingType = CategoricalDtype([\"House / apartment\", \"Rented apartment\", \"With parents\",\n                                \"Municipal apartment\", \"Office apartment\", \"Co-op apartment\"], False)\nIncomeType = CategoricalDtype([\"Unemployed\", \"Student\", \"State servant\", \"Working\", \"Commercial associate\",\n                               \"Businessman\", \"Maternity leave\", \"Pensioner\"], False)\nAccompanyType = CategoricalDtype([\"Unaccompanied\", \"Spouse\", \"partner\", \"Family\", \"Children\",\n                                  \"Group of people\", \"Other_A\", \"Other_B\"], False)\nOccupationType = CategoricalDtype([\"Accountants\", \"Cleaning staff\", \"Cooking staff\", \"Core staff\",\n                                   \"Drivers\", \"HR staff\", \"High skill tech staff\", \"IT staff\", \"Laborers\",\n                                   \"Low-skill Laborers\", \"Managers\", \"Medicine staff\",\n                                   \"Private service staff\", \"Realty agents\", \"Sales staff\", \"Secretaries\",\n                                   \"Security staff\", \"Waiters/barmen staff\"], False)\nCreditStatusType = CategoricalDtype([\"Closed\", \"Active\", \"Sold\", \"Bad debt\", \"Signed\"], True)\nBureauBalanceStatusType = CategoricalDtype([\"C\", *map(str, range(6))], True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Loading Functions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_app_data(train_only=False):\n    gender_type = CategoricalDtype([\"M\", \"F\"], False)\n    yes_no_type2 = CategoricalDtype([\"No\", \"Yes\"], True)\n    wall_material_type = CategoricalDtype([\"Stone, brick\", \"Wooden\", \"Block\", \"Panel\", \n                                           \"Monolithic\", \"Mixed\", \"Others\"], False)\n    fondkapremont_type = CategoricalDtype([\"reg oper account\", \"org spec account\",\n                                           \"reg oper spec account\", \"not specified\"], False)\n\n    col_types = {\n        \"SK_ID_CURR\": np.uint32, \"TARGET\": np.bool, \"CODE_GENDER\": gender_type, \"NAME_CONTRACT_TYPE\": LoanType,\n        \"FLAG_OWN_CAR\": YesNoType, \"FLAG_OWN_REALTY\": YesNoType, \"CNT_CHILDREN\": np.uint8,\n        \"AMT_INCOME_TOTAL\": np.float32, \"AMT_CREDIT\": np.float32,\n        \"AMT_ANNUITY\": np.float32, \"AMT_GOODS_PRICE\": np.float32,\n        \"NAME_TYPE_SUITE\": AccompanyType, \"NAME_EDUCATION_TYPE\": EducationType, \"NAME_INCOME_TYPE\": IncomeType,\n        \"NAME_FAMILY_STATUS\": FamilyType, \"NAME_HOUSING_TYPE\": HousingType,\n        \"REGION_POPULATION_RELATIVE\": np.float32,\n        \"REGION_RATING_CLIENT\": np.uint8, \"REGION_RATING_CLIENT_W_CITY\": np.uint8,\n        \"WEEKDAY_APPR_PROCESS_START\": WeekDayType, \"HOUR_APPR_PROCESS_START\": np.uint8,\n        \"DAYS_EMPLOYED\": np.float32, \"DAYS_BIRTH\": np.int32,\n        \"DAYS_REGISTRATION\": np.float32, \"DAYS_ID_PUBLISH\": np.float32,\n        \"OWN_CAR_AGE\": np.float16,\n        \"EXT_SOURCE_1\": np.float32, \"EXT_SOURCE_2\": np.float32, \"EXT_SOURCE_3\": np.float32,\n        \"FLAG_MOBIL\": np.bool, \"FLAG_EMP_PHONE\": np.bool, \"FLAG_WORK_PHONE\": np.bool,\n        \"FLAG_CONT_MOBILE\": np.bool, \"FLAG_PHONE\": np.bool, \"FLAG_EMAIL\": np.bool,\n        \"OCCUPATION_TYPE\": OccupationType, \"CNT_FAM_MEMBERS\": np.float16,\n        \"REG_REGION_NOT_LIVE_REGION\": np.bool, \"REG_REGION_NOT_WORK_REGION\": np.bool,\n        \"LIVE_REGION_NOT_WORK_REGION\": np.bool, \"REG_CITY_NOT_LIVE_CITY\": np.bool,\n        \"REG_CITY_NOT_WORK_CITY\": np.bool, \"LIVE_CITY_NOT_WORK_CITY\": np.bool,\n        \"ORGANIZATION_TYPE\": \"category\",\n        \"OBS_30_CNT_SOCIAL_CIRCLE\": np.float16, \"DEF_30_CNT_SOCIAL_CIRCLE\": np.float16,\n        \"OBS_60_CNT_SOCIAL_CIRCLE\": np.float16, \"DEF_60_CNT_SOCIAL_CIRCLE\": np.float16,\n        \"DAYS_LAST_PHONE_CHANGE\": np.float32,\n        \"AMT_REQ_CREDIT_BUREAU_HOUR\": np.float16, \"AMT_REQ_CREDIT_BUREAU_DAY\": np.float16,\n        \"AMT_REQ_CREDIT_BUREAU_WEEK\": np.float16, \"AMT_REQ_CREDIT_BUREAU_MON\": np.float16,\n        \"AMT_REQ_CREDIT_BUREAU_QRT\": np.float16, \"AMT_REQ_CREDIT_BUREAU_YEAR\": np.float16,\n        \"HOUSETYPE_MODE\": HouseType, \"FONDKAPREMONT_MODE\": fondkapremont_type,\n        \"WALLSMATERIAL_MODE\": wall_material_type, \"EMERGENCYSTATE_MODE\": yes_no_type2\n    }\n\n    float_housing_columns = [\n        \"APARTMENTS_AVG\", \"BASEMENTAREA_AVG\", \"YEARS_BEGINEXPLUATATION_AVG\",\n        \"YEARS_BUILD_AVG\", \"COMMONAREA_AVG\", \"ELEVATORS_AVG\", \"ENTRANCES_AVG\",\n        \"FLOORSMAX_AVG\", \"FLOORSMIN_AVG\", \"LANDAREA_AVG\",\n        \"LIVINGAPARTMENTS_AVG\", \"LIVINGAREA_AVG\", \"NONLIVINGAPARTMENTS_AVG\", \"NONLIVINGAREA_AVG\",\n        \"APARTMENTS_MODE\", \"BASEMENTAREA_MODE\", \"YEARS_BEGINEXPLUATATION_MODE\",\n        \"YEARS_BUILD_MODE\", \"COMMONAREA_MODE\", \"ELEVATORS_MODE\", \"ENTRANCES_MODE\",\n        \"FLOORSMAX_MODE\", \"FLOORSMIN_MODE\", \"LANDAREA_MODE\",\n        \"LIVINGAPARTMENTS_MODE\", \"LIVINGAREA_MODE\", \"NONLIVINGAPARTMENTS_MODE\",\n        \"NONLIVINGAREA_MODE\", \"TOTALAREA_MODE\",\n        \"APARTMENTS_MEDI\", \"BASEMENTAREA_MEDI\", \"YEARS_BEGINEXPLUATATION_MEDI\",\n        \"YEARS_BUILD_MEDI\", \"COMMONAREA_MEDI\", \"ELEVATORS_MEDI\", \"ENTRANCES_MEDI\",\n        \"FLOORSMAX_MEDI\", \"FLOORSMIN_MEDI\", \"LANDAREA_MEDI\", \"LIVINGAPARTMENTS_MEDI\",\n        \"LIVINGAREA_MEDI\", \"NONLIVINGAPARTMENTS_MEDI\", \"NONLIVINGAREA_MEDI\",\n        # 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE',\n    ]\n\n    col_types.update((\"FLAG_DOCUMENT_\" + str(v), np.bool) for v in range(2, 22))\n    col_types.update((col, np.float16) for col in float_housing_columns)\n    \n    replace_dict = {\"DAYS_EMPLOYED\": {365243: np.nan}, \"DAYS_LAST_PHONE_CHANGE\": {0: np.nan}}\n    na_values = [\"XNA\", \"Unknown\"]\n    df_app_train = pd.read_csv(PATH_APP_TRAIN, index_col=0, na_values=na_values, dtype=col_types)\n    df_app_train.replace(replace_dict, inplace=True)\n    assert df_app_train.shape == (307511, 121)\n    if train_only:\n        return df_app_train\n    else:\n        df_app_test = pd.read_csv(PATH_APP_TEST, index_col=0, na_values=na_values, dtype=col_types)\n        df_app_test.replace(replace_dict, inplace=True)\n        assert df_app_test.shape == (48744, 120)\n        return df_app_train, df_app_test\n\n    \ndef load_prev_app_data():\n    interest_types = CategoricalDtype([\"low_action\", \"low_normal\", \"middle\", \"high\"], True)\n    col_types = {\n        \"SK_ID_PREV\": np.uint32, \"SK_ID_CURR\": np.uint32, \"NAME_CONTRACT_TYPE\": LoanType,\n        \"AMT_ANNUITY\": np.float32, \"AMT_APPLICATION\": np.float32, \"AMT_CREDIT\": np.float32,\n        \"AMT_DOWN_PAYMENT\": np.float32, \"AMT_GOODS_PRICE\": np.float32,\n        \"WEEKDAY_APPR_PROCESS_START\": WeekDayType, \"HOUR_APPR_PROCESS_START\": np.uint8,\n        \"FLAG_LAST_APPL_PER_CONTRACT\": YesNoType, \"NFLAG_LAST_APPL_IN_DAY\": np.bool,\n        \"RATE_DOWN_PAYMENT\": np.float32, \"RATE_INTEREST_PRIMARY\": np.float32, \"RATE_INTEREST_PRIVILEGED\": np.float32,\n        \"NAME_CASH_LOAN_PURPOSE\": \"category\", \"NAME_CONTRACT_STATUS\": LoanStatusType,\n        \"DAYS_DECISION\": np.int16, \"NAME_PAYMENT_TYPE\": \"category\", \"CODE_REJECT_REASON\": \"category\",\n        \"NAME_TYPE_SUITE\": \"category\", \"NAME_CLIENT_TYPE\": \"category\", \"NAME_GOODS_CATEGORY\": \"category\",\n        \"NAME_PORTFOLIO\": \"category\", \"NAME_PRODUCT_TYPE\": \"category\", \"CHANNEL_TYPE\": \"category\",\n        \"SELLERPLACE_AREA\": np.float32, \"NAME_SELLER_INDUSTRY\": \"category\", \"CNT_PAYMENT\": np.float16,\n        \"NAME_YIELD_GROUP\": interest_types, \"PRODUCT_COMBINATION\": \"category\",\n        \"DAYS_FIRST_DRAWING\": np.float32, \"DAYS_FIRST_DUE\": np.float32,\n        \"DAYS_LAST_DUE_1ST_VERSION\": np.float32, \"DAYS_LAST_DUE\": np.float32, \"DAYS_TERMINATION\": np.float32,\n        \"NFLAG_INSURED_ON_APPROVAL\": np.float16\n    }\n\n    d = {365243: np.nan}\n\n    replace_dict = {\"SELLERPLACE_AREA\": {-1: np.nan},\n                    \"DAYS_FIRST_DRAWING\": d, \"DAYS_FIRST_DUE\": d,\n                    \"DAYS_LAST_DUE_1ST_VERSION\": d, \"DAYS_LAST_DUE\": d, \"DAYS_TERMINATION\": d}\n    df_prev_app = pd.read_csv(PATH_PRE_APP, na_values=[\"XNA\", \"\"], index_col=0,\n                              keep_default_na=False, dtype=col_types)\n    # df_prev_app.set_index(\"SK_ID_PREV\")\n    df_prev_app.replace(replace_dict, inplace=True)\n    assert df_prev_app.shape == (1670214, 36), f\"Incorrect shape for df_prev_app: {df_prev_app.shape}\"\n    return df_prev_app\n\n\ndef load_install_payments(load_processed=True):\n    col_types = {\n        \"SK_ID_PREV\": np.uint32, \"SK_ID_CURR\": np.uint32,\n        \"NUM_INSTALMENT_VERSION\": np.uint8, \"NUM_INSTALMENT_NUMBER\": np.uint16,\n        \"DAYS_INSTALMENT\": np.int16, \"DAYS_ENTRY_PAYMENT\": np.float16,\n        \"AMT_INSTALMENT\": np.float32, \"AMT_PAYMENT\": np.float32,\n        \"NUM_PAYMENTS\": np.uint16, \"AMT_OVERDUE\": np.float32, \"AMT_DPD30\": np.float32\n    }\n\n    if load_processed:\n        return pd.read_csv(PATH_INST_PAY_PROCESSED, dtype=col_types)\n    else:\n        df = pd.read_csv(PATH_INST_PAY, dtype=col_types)\n        assert df.shape == (13605401, 8)\n        return df\n\n\ndef load_pos_balance():\n    col_types = {\n        \"SK_ID_PREV\": np.uint32, \"SK_ID_CURR\": np.uint32, \"MONTHS_BALANCE\": np.int16,\n        \"CNT_INSTALMENT\": np.float16, \"CNT_INSTALMENT_FUTURE\": np.float16,\n        \"NAME_CONTRACT_STATUS\": CreditStatusType,\n        \"SK_DPD\": np.uint16, \"SK_DPD_DEF\": np.uint16\n    }\n    df = pd.read_csv(PATH_POS_BALANCE, na_values=[\"XNA\"], dtype=col_types)\n    assert df.shape == (10001358, 8)\n    return df\n\n\ndef load_credit_balance_data():\n    col_types = {\n        \"SK_ID_PREV\": np.uint32, \"SK_ID_CURR\": np.uint32, \"MONTHS_BALANCE\": np.int16,\n        \"AMT_BALANCE\": np.float32, \"AMT_CREDIT_LIMIT_ACTUAL\": np.float32,\n        \"AMT_DRAWINGS_ATM_CURRENT\": np.float32, \"AMT_DRAWINGS_CURRENT\": np.float32,\n        \"AMT_DRAWINGS_OTHER_CURRENT\": np.float32, \"AMT_DRAWINGS_POS_CURRENT\": np.float32,\n        \"AMT_INST_MIN_REGULARITY\": np.float32, \"AMT_PAYMENT_CURRENT\": np.float32,\n        \"AMT_PAYMENT_TOTAL_CURRENT\": np.float32, \"AMT_RECEIVABLE_PRINCIPAL\": np.float32,\n        \"AMT_RECIVABLE\": np.float32, \"AMT_TOTAL_RECEIVABLE\": np.float32,\n        \"CNT_DRAWINGS_ATM_CURRENT\": np.float16, \"CNT_DRAWINGS_CURRENT\": np.float16,\n        \"CNT_DRAWINGS_OTHER_CURRENT\": np.float16, \"CNT_DRAWINGS_POS_CURRENT\": np.float16,\n        \"CNT_INSTALMENT_MATURE_CUM\": np.float16, \"NAME_CONTRACT_STATUS\": \"category\",\n        \"SK_DPD\": np.uint16, \"SK_DPD_DEF\": np.uint16\n    }\n    df = pd.read_csv(PATH_CREDIT_BALANCE, dtype=col_types)\n    assert df.shape == (3840312, 23)\n    return df\n\n\ndef load_bureau():\n    currency_types = CategoricalDtype([\"currency \" + str(v) for v in range(1, 5)], False)\n    col_types = {\n        \"SK_ID_BUREAU\": np.uint32, \"SK_ID_CURR\": np.uint32, \"CREDIT_ACTIVE\": CreditStatusType,\n        \"CREDIT_CURRENCY\": currency_types, \"DAYS_CREDIT\": np.float16, \"CREDIT_DAY_OVERDUE\": np.uint16,\n        \"DAYS_ENDDATE_FACT\": np.float16, \"DAYS_CREDIT_ENDDATE\": np.float16,\n        \"AMT_CREDIT_MAX_OVERDUE\": np.float32, \"CNT_CREDIT_PROLONG\": np.uint8,\n        \"AMT_CREDIT_SUM\": np.float32, \"AMT_CREDIT_SUM_DEBT\": np.float32, \"AMT_CREDIT_SUM_LIMIT\": np.float32,\n        \"AMT_CREDIT_SUM_OVERDUE\": np.float32, \"AMT_ANNUITY\": np.float32,\n        \"CREDIT_TYPE\": \"category\"\n    }\n    df = pd.read_csv(PATH_BUREAU, index_col=1, dtype=col_types)\n    assert df.shape == (1716428, 16)\n    return df\n\n\ndef load_bureau_balance():\n    col_types = {\n        \"SK_ID_BUREAU\": np.uint32, \"MONTHS_BALANCE\": np.int16, \"STATUS\": BureauBalanceStatusType\n    }\n    df = pd.read_csv(PATH_BUREAU_BALANCE, na_values=[\"X\"], dtype=col_types)\n    assert df.shape == (27299925, 3)\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Preprocessing Functions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def flatten_agg_df_columns(df_agg, prefix=None):\n    if prefix is None:\n        df_agg.columns = ['_'.join([c0, c1.upper()]) for c0, c1 in df_agg.columns]\n    else:\n        df_agg.columns = ['_'.join([prefix, c0, c1.upper()]) for c0, c1 in df_agg.columns]\n    return df_agg\n\ndef clean_inst_pay(df_inst_pay):\n    print(\"Cleaning installment payments data\")\n    df_inst_pay[\"DAYS_ENTRY_PAYMENT\"].fillna(0, inplace=True)\n    df_inst_pay[\"AMT_PAYMENT\"].fillna(-1, inplace=True)\n    df_inst_pay_valid_filter = (df_inst_pay[\"AMT_PAYMENT\"] > 0) | (df_inst_pay[\"AMT_INSTALMENT\"] > 0)\n    print(f\"Remove {(~df_inst_pay_valid_filter).sum():d} invalid records.\")\n    df_inst_pay_group = df_inst_pay[df_inst_pay_valid_filter].groupby([\"SK_ID_PREV\", \"NUM_INSTALMENT_NUMBER\",\n                                                                       \"DAYS_ENTRY_PAYMENT\", \"AMT_PAYMENT\"])\n    del df_inst_pay_valid_filter\n\n    print(\"Aggregate multiple installments for one payment\")\n    df_inst_pay_group_cnt = df_inst_pay_group.size()\n    df_inst_agg = flatten_agg_df_columns(df_inst_pay_group.agg({\n        \"SK_ID_CURR\": [\"min\", \"max\"],\n        \"NUM_INSTALMENT_VERSION\": [\"max\", \"nunique\"],\n        \"DAYS_INSTALMENT\": [\"min\", \"max\"],\n        \"AMT_INSTALMENT\": [\"min\", \"max\", \"sum\"]\n    }))\n    del df_inst_pay_group\n\n    print(\"Processing 1\")\n    assert (df_inst_agg[\"SK_ID_CURR_MIN\"] == df_inst_agg[\"SK_ID_CURR_MAX\"]).all(axis=None), \"Inconsistent SK_ID_CURR\"\n    df_inst_pay_processed = pd.DataFrame(index=df_inst_agg.index)\n    df_inst_pay_processed[\"SK_ID_CURR\"] = df_inst_agg[\"SK_ID_CURR_MIN\"]\n\n    df_inst_pay_group_cnt_distict = df_inst_agg[\"NUM_INSTALMENT_VERSION_NUNIQUE\"]\n    df_inst_pay_group_check = ((df_inst_pay_group_cnt == 2) |\n                               (df_inst_pay_group_cnt_distict == 1))\n    assert df_inst_pay_group_check.all(axis=None)\n    del df_inst_pay_group_cnt, df_inst_pay_group_check\n    df_inst_pay_processed[\"NUM_INSTALMENT_VERSION\"] = df_inst_agg[\"NUM_INSTALMENT_VERSION_MAX\"]\n\n    assert (df_inst_agg[\"DAYS_INSTALMENT_MIN\"] == df_inst_agg[\"DAYS_INSTALMENT_MAX\"]).all(axis=None)\n    df_inst_pay_processed[\"DAYS_INSTALMENT\"] = df_inst_agg[\"DAYS_INSTALMENT_MIN\"]\n\n    df_agg_filter = (df_inst_pay_group_cnt_distict == 2)\n    assert (df_agg_filter | (df_inst_agg[\"AMT_INSTALMENT_MIN\"] == df_inst_agg[\"AMT_INSTALMENT_MAX\"])).all(axis=None)\n    df_inst_pay_processed[\"AMT_INSTALMENT\"] = df_inst_agg[\"AMT_INSTALMENT_MIN\"]\n    df_inst_pay_processed.loc[df_agg_filter, \"AMT_INSTALMENT\"] = df_inst_agg[\"AMT_INSTALMENT_SUM\"]\n    print(\"%d payments aggregated\" % df_agg_filter.sum())\n    del df_inst_pay_group_cnt_distict, df_agg_filter\n\n    df_inst_pay_processed.reset_index(inplace=True)\n    # df_inst_pay_processed[\"DAYS_ENTRY_PAYMENT\"].astype(np.float16, copy=False)\n    df_inst_pay_processed[\"DAYS_ENTRY_PAYMENT\"] = df_inst_pay_processed[\"DAYS_ENTRY_PAYMENT\"].astype(np.float16,\n                                                                                                     copy=False)\n    df_inst_pay_processed[\"AMT_PAYMENT\"] = df_inst_pay_processed[\"AMT_PAYMENT\"].astype(np.float32, copy=False)\n    df_inst_pay_processed[\"AMT_PAYMENT\"].replace(-1, -np.inf, inplace=True)\n    assert ((df_inst_pay_processed[\"AMT_PAYMENT\"] >= 0) |\n            (df_inst_pay_processed[\"DAYS_ENTRY_PAYMENT\"] == 0)).all(axis=None)\n    df_diff_entry_offset = df_inst_pay_processed[\"DAYS_ENTRY_PAYMENT\"] - df_inst_pay_processed[\"DAYS_INSTALMENT\"]\n    df_inst_pay_processed[\"AMT_DUE_PAYMENT\"] = (np.fmax(df_inst_pay_processed[\"AMT_PAYMENT\"], 0) *\n                                                (df_diff_entry_offset <= 0))\n    df_inst_pay_processed[\"AMT_DUE30_PAYMENT\"] = (np.fmax(df_inst_pay_processed[\"AMT_PAYMENT\"], 0) *\n                                                  (df_diff_entry_offset <= 30))\n\n    df_inst_pay_group = df_inst_pay_processed.groupby([\"SK_ID_PREV\", \"NUM_INSTALMENT_NUMBER\", \"NUM_INSTALMENT_VERSION\"])\n    del df_diff_entry_offset, df_inst_pay_processed, df_inst_agg\n\n    print(\"Aggregate multiple payments for one installment\")\n    df_inst_pay_group_cnt = df_inst_pay_group.size()\n    df_inst_agg = flatten_agg_df_columns(df_inst_pay_group.agg({\n        \"SK_ID_CURR\": [\"min\", \"max\"],\n        # \"NUM_INSTALMENT_VERSION\": [\"min\", \"max\"],\n        \"DAYS_INSTALMENT\": [\"min\", \"max\"],\n        \"DAYS_ENTRY_PAYMENT\": [\"min\", \"max\"],\n        \"AMT_INSTALMENT\": [\"min\", \"max\", \"sum\"],\n        \"AMT_PAYMENT\": [\"sum\"],\n        \"AMT_DUE_PAYMENT\": [\"sum\"],\n        \"AMT_DUE30_PAYMENT\": [\"sum\"]\n    }, skipna=False))\n    del df_inst_pay_group\n    print(\"Finish aggregations\")\n    \n    gc.collect()\n\n    print(\"Processing 2\")\n    assert (df_inst_agg[\"SK_ID_CURR_MIN\"] == df_inst_agg[\"SK_ID_CURR_MAX\"]).all(), \"Inconsistent SK_ID_CURR\"\n    df_inst_pay_processed = pd.DataFrame(index=df_inst_agg.index)\n    df_inst_pay_processed[\"SK_ID_CURR\"] = df_inst_agg[\"SK_ID_CURR_MIN\"]\n\n    # df_inst_agg_INST_VER = df_inst_agg[\"NUM_INSTALMENT_VERSION\"]\n    # assert (df_inst_agg_INST_VER[\"min\"] == df_inst_agg_INST_VER[\"max\"]).all(axis=None), \"Inconsistent NUM_INSTALMENT_VERSION\"\n    # df_inst_pay_processed[\"NUM_INSTALMENT_VERSION\"] = df_inst_agg_INST_VER[\"min\"]\n\n    assert (df_inst_agg[\"DAYS_INSTALMENT_MIN\"] ==\n            df_inst_agg[\"DAYS_INSTALMENT_MAX\"]).all(axis=None), \"Inconsistent DAYS_INSTALMENT\"\n    df_inst_pay_processed[\"DAYS_INSTALMENT\"] = df_inst_agg[\"DAYS_INSTALMENT_MIN\"]\n\n    df_inst_pay_processed[\"DAYS_FIRST_PAYMENT\"] = df_inst_agg[\"DAYS_ENTRY_PAYMENT_MIN\"].replace(0, np.nan)\n    df_inst_pay_processed[\"DAYS_LAST_PAYMENT\"] = df_inst_agg[\"DAYS_ENTRY_PAYMENT_MAX\"].replace(0, np.nan)\n\n    assert (df_inst_agg[\"AMT_INSTALMENT_MIN\"] == df_inst_agg[\"AMT_INSTALMENT_MAX\"]).all(axis=None)\n    df_inst_pay_processed[\"AMT_INSTALMENT\"] = df_inst_agg[\"AMT_INSTALMENT_MIN\"]\n\n    # Fix missing installment info\n    # df_prev_app_ann = pd.read_csv(r\"data\\previous_application.csv\", index_col=0, usecols=[0, 3])\n    # df_inst_agg = df_inst_agg.join(df_prev_app_ann, how=\"left\")\n    #\n    # df_annuity_check = ((df_inst_agg.index.get_level_values(2) != 1) | df_inst_agg[\"AMT_ANNUITY\"].isna() |\n    #                     (df_inst_agg[\"AMT_INSTALMENT_min\"] == 0) |\n    #                     ((df_inst_agg[\"AMT_ANNUITY\"] - df_inst_agg[\"AMT_INSTALMENT_min\"]).abs() < 0.01))\n    # assert df_annuity_check.all(axis=None)\n    # inst_fix_filter = ((df_inst_agg[\"NUM_INSTALMENT_VERSION\"] == 1) & (df_inst_agg[\"AMT_INSTALMENT_min\"] == 0))\n    # df_inst_pay_processed.loc[inst_fix_filter, \"AMT_INSTALMENT\"] = df_inst_agg.loc[inst_fix_filter, \"AMT_ANNUITY\"]\n    # del df_annuity_check, inst_fix_filter\n\n    # inst_fix_filter = (df_inst_agg[\"AMT_INSTALMENT_min\"] == 0)\n    # df_inst_pay_processed.loc[inst_fix_filter, \"AMT_INSTALMENT\"] = df_inst_agg.loc[inst_fix_filter, \"AMT_PAYMENT_sum\"]\n    # del inst_fix_filter\n\n    df_inst_pay_invalid_filter = (df_inst_agg[\"AMT_PAYMENT_SUM\"] < 0)\n    assert ((~df_inst_pay_invalid_filter) | (df_inst_pay_group_cnt == 1)).all(axis=None)\n    df_inst_pay_processed[\"AMT_PAYMENT\"] = df_inst_agg[\"AMT_PAYMENT_SUM\"]\n    df_inst_pay_processed.loc[df_inst_pay_invalid_filter, \"AMT_PAYMENT\"] = np.nan\n    assert (df_inst_pay_processed[\"AMT_PAYMENT\"] != 0).all(axis=None)\n\n    df_inst_pay_invalid_filter = df_inst_pay_processed[\"AMT_PAYMENT\"].isnull()\n    df_inst_pay_processed[\"NUM_PAYMENTS\"] = df_inst_pay_group_cnt.astype(np.uint16)\n    df_inst_pay_processed.loc[df_inst_pay_invalid_filter, \"NUM_PAYMENTS\"] = np.uint16(0)\n    print(\"%d installments aggregated\" % (df_inst_pay_group_cnt > 1).sum())\n    del df_inst_pay_group_cnt, df_inst_pay_invalid_filter\n\n    df_inst_pay_processed[\"AMT_OVERDUE\"] = np.fmax(df_inst_pay_processed[\"AMT_INSTALMENT\"] -\n                                                   df_inst_agg[\"AMT_DUE_PAYMENT_SUM\"], 0)\n    df_inst_pay_processed[\"AMT_OVERDUE\"] *= (df_inst_pay_processed[\"AMT_OVERDUE\"] >= 0.01)\n    df_inst_pay_processed[\"AMT_DPD30\"] = np.fmax(df_inst_pay_processed[\"AMT_INSTALMENT\"] -\n                                                 df_inst_agg[\"AMT_DUE30_PAYMENT_SUM\"], 0)\n    df_inst_pay_processed[\"AMT_DPD30\"] *= (df_inst_pay_processed[\"AMT_DPD30\"] >= 0.01)\n    df_inst_pay_processed[\"AMT_UNPAID\"] = np.fmax(df_inst_pay_processed[\"AMT_INSTALMENT\"] -\n                                                  df_inst_pay_processed[\"AMT_PAYMENT\"].fillna(0), 0)\n    df_inst_pay_processed[\"AMT_UNPAID\"] *= (df_inst_pay_processed[\"AMT_UNPAID\"] >= 0.01)\n    df_inst_pay_processed.reset_index(inplace=True)\n    # df_inst_pay_processed.rename(columns={\"NUM_INSTALMENT_NUMBER\": \"NUM_INSTALMENT_NUMBER\",\n    #                                       \"NUM_INSTALMENT_VERSION\": \"INSTALMENT_VER\"})\n    del df_inst_agg\n    \n    print(\"Finish processing\")\n\n    output_columns = [\"SK_ID_PREV\", \"SK_ID_CURR\", \"NUM_INSTALMENT_VERSION\", \"NUM_INSTALMENT_NUMBER\",\n                      \"DAYS_INSTALMENT\", \"DAYS_FIRST_PAYMENT\", \"DAYS_LAST_PAYMENT\", \"NUM_PAYMENTS\",\n                      \"AMT_INSTALMENT\", \"AMT_PAYMENT\", \"AMT_OVERDUE\", \"AMT_DPD30\", \"AMT_UNPAID\"]\n    df_inst_pay_processed.drop(df_inst_pay_processed.columns.drop(output_columns), axis=1, inplace=True)\n\n#     df_inst_pay_processed.to_csv(r\"installments_payments_processed.csv\", index=False)\n    print(\"Finish cleaning installment payment data\")\n    return df_inst_pay_processed\n\n\ndef merge_payment_info():\n    df_inst_pay = load_install_payments()\n    df_pos = load_pos_balance()\n    df_inst_pay[\"MONTHS_BALANCE\"] = df_inst_pay[\"DAYS_INSTALMENT\"] // 30\n    df_pos.set_index([\"SK_ID_PREV\", \"MONTHS_BALANCE\"], inplace=True, drop=True)\n    df_merged = df_inst_pay.join(df_pos, on=[\"SK_ID_PREV\", \"MONTHS_BALANCE\"], how=\"outer\", rsuffix=\"_POS\")\n    t = df_merged[(df_merged.NAME_CONTRACT_STATUS != \"Active\") & df_merged.SK_ID_CURR.notnull()]\n    return t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def group_values(col_orig, new_col_values):\n    return pd.DataFrame({col: col_orig.isin(values) for col, values in new_col_values})\n\ndef get_preprocessed_bureau_data():\n    df_bureau = load_bureau()\n    df_bureau_balance = load_bureau_balance()\n\n    df_bureau_balance[\"STATUS\"] = df_bureau_balance[\"STATUS\"].cat.codes - 1\n    df_bureau_balance[\"HAS_OVERDUE\"] = df_bureau_balance[\"STATUS\"] > 0\n    df_bureau_balance_agg = df_bureau_balance.groupby(\"SK_ID_BUREAU\").agg({\n        \"HAS_OVERDUE\": [\"sum\"],\n        \"STATUS\": [\"max\"]\n    })\n    df_bureau_balance_agg.columns = [\"CNT_OVERDUE\", \"MAX_OVERDUE_TIME\"]\n    \n#     df_bureau_balance_L12_agg = (df_bureau_balance[\"MONTHS_BALANCE\"] > -12).agg({\n#         \"HAS_OVERDUE\": [\"count\", \"sum\"],\n#         \"STATUS\": [\"max\"]\n#     })\n#     df_bureau_balance_L12_agg = [\"NUM_CREDITS\", \"CNT_OVERDUE\", \"MAX_OVERDUE\"]\n    \n    df_bureau = df_bureau.join(df_bureau_balance_agg)\n    df_bureau[\"CNT_OVERDUE\"].where(df_bureau[\"CNT_OVERDUE\"] > 0,\n                                   df_bureau[\"AMT_CREDIT_MAX_OVERDUE\"].gt(0).astype(np.uint8, copy=False),\n                                   inplace=True)\n    \n    df_bureau.loc[df_bureau[\"DAYS_ENDDATE_FACT\"].isna() & (df_bureau[\"AMT_CREDIT_SUM_DEBT\"] == 0) &\n                  (df_bureau[\"DAYS_CREDIT_ENDDATE\"] < 0), \"DAYS_ENDDATE_FACT\"] = df_bureau[\"DAYS_CREDIT_ENDDATE\"]\n    df_bureau[\"DURATION\"] = df_bureau[\"DAYS_ENDDATE_FACT\"].fillna(0) - df_bureau[\"DAYS_CREDIT\"]\n    df_bureau[\"CLOSE_DURATION_RATIO\"] = ((df_bureau[\"DAYS_ENDDATE_FACT\"] - df_bureau[\"DAYS_CREDIT\"]) / \n                                         (df_bureau[\"DAYS_CREDIT_ENDDATE\"] - df_bureau[\"DAYS_CREDIT\"]))\n    \n    bureau_groups = df_bureau.groupby(\"SK_ID_CURR\")\n    df_bureau_agg = flatten_agg_df_columns(bureau_groups.agg({\n        \"CNT_OVERDUE\": [\"sum\"],\n        \"DURATION\": [\"sum\"],\n        \"AMT_CREDIT_MAX_OVERDUE\": [\"max\"],\n        \"AMT_CREDIT_SUM_OVERDUE\": [\"max\", \"sum\"],\n        \"DAYS_CREDIT\": [\"min\", \"max\"],\n        \"CREDIT_DAY_OVERDUE\": [\"max\", \"mean\"],\n        \"MAX_OVERDUE_TIME\": [\"max\"],\n        \"AMT_CREDIT_SUM\": [\"max\", \"mean\", \"sum\"],\n        \"AMT_CREDIT_SUM_DEBT\": [\"max\", \"mean\", \"sum\"],\n        \"CNT_CREDIT_PROLONG\": [\"max\"],\n        \"DURATION\": [\"sum\"],\n        \"DAYS_ENDDATE_FACT\": [\"max\"],\n        \"CLOSE_DURATION_RATIO\": [\"min\", \"max\", \"mean\"]\n\n#         'DAYS_CREDIT': ['mean', 'var'],\n#         'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n#         'DAYS_CREDIT_UPDATE': ['mean'],\n#         'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n#         'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n#         'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n#         'AMT_ANNUITY': ['max', 'mean'],\n#         'MONTHS_BALANCE_MIN': ['min'],\n#         'MONTHS_BALANCE_MAX': ['max'],\n#         'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n    }), \"BUREAU\")\n    df_bureau_agg[\"BUREAU_OVERDUE_FREQ\"] = df_bureau_agg[\"BUREAU_CNT_OVERDUE_SUM\"] / df_bureau_agg[\"BUREAU_DURATION_SUM\"]\n    df_bureau_agg[\"BUREAU_LOAN_CNT\"] = bureau_groups.size()\n\n    # def func_bureau_agg(s):\n    #     total_overdues = (s.CREDIT_DAY_OVERDUE > 0).sum()\n    #     max_overdue = s.AMT_CREDIT_MAX_OVERDUE.max()\n    #     # active_debt = s[s.CREDIT_ACTIVE == \"Active\"].AMT_CREDIT_SUM_DEBT.sum()\n    #     return pd.Series([total_overdues, max_overdue], index=[\"total_overdues\", \"max_overdues\"])\n    # df_bureau_agg = df_bureau.groupby(\"SK_ID_CURR\").apply(func_bureau_agg)\n\n    bureau_active_groups = df_bureau[df_bureau.CREDIT_ACTIVE == \"Active\"].groupby(\"SK_ID_CURR\")\n    df_bureau_active_agg = flatten_agg_df_columns(bureau_active_groups.agg({\n        \"DAYS_CREDIT_ENDDATE\": [\"max\"],\n        \"AMT_ANNUITY\": [\"sum\"]\n    }), \"BUREAU_ACTIVE\")\n    df_bureau_agg = df_bureau_agg.join(df_bureau_active_agg, how=\"left\")\n\n    df_bureau_agg.fillna({\n        \"BUREAU_AMT_CREDIT_MAX_OVERDUE_MAX\": 0,\n        \"BUREAU_AMT_CREDIT_SUM_DEBT_SUM\": 0,\n        \"BUREAU_ACTIVE_AMT_CREDIT_SUM_DEBT_SUM\": 0,\n        # \"BUREAU_ACTIVE_DAYS_CREDIT_ENDDATE_MAX\": 100000\n    }, inplace=True)\n\n    # assert df_bureau_agg.notnull().all(axis=None)\n    \n    del df_bureau, df_bureau_balance\n    return df_bureau_agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_preprocessed_previous_app_data(load_inst_pay=True, load_credit_balance=True):\n    # def func_prev_app_agg(s):\n    #     return pd.Series({\"has_assessed_risk\": s.has_assessed_risk.sum(),\n    #                       \"max_refused\": s[s.is_refused].AMT_APPLICATION.max(),\n    #                       \"total_approved\": s[s.is_approved].size,\n    #                       \"max_prev_annuity\": s[s.is_approved].AMT_ANNUITY.max()})\n\n    df_prev_app = load_prev_app_data()\n    df_prev_app[\"APPROVED_RATIO\"] = df_prev_app[\"AMT_CREDIT\"] / df_prev_app[\"AMT_APPLICATION\"]\n    df_prev_app[\"DOWN_PAYMENT_RATIO\"] = df_prev_app[\"AMT_DOWN_PAYMENT\"] / df_prev_app[\"AMT_CREDIT\"]\n    df_prev_app[\"CNT_ASSESSED_RISK\"] = (df_prev_app[\"NAME_PRODUCT_TYPE\"] == \"x-sell\").astype(np.uint8, copy=False)\n    df_prev_app[\"NAME_YIELD_GROUP_CODE\"] = df_prev_app[\"NAME_YIELD_GROUP\"].cat.codes\n    df_prev_app.loc[df_prev_app[\"NAME_CONTRACT_TYPE\"] != \"Revolving loans\", \"PAYMENT_DURATION_RATIO\"] = (\n        (df_prev_app[\"DAYS_LAST_DUE\"] - df_prev_app[\"DAYS_FIRST_DUE\"] + 30) / (30 * df_prev_app[\"CNT_PAYMENT\"])\n    )\n    \n    total_ratio = df_prev_app[\"AMT_ANNUITY\"] * df_prev_app[\"CNT_PAYMENT\"] / df_prev_app[\"AMT_CREDIT\"]\n    total_ratio = total_ratio[total_ratio.notna() & (df_prev_app[\"CNT_PAYMENT\"] > 0)]\n    interest_rate_est = 1 - 1 / total_ratio\n    for _ in range(7):\n        interest_rate_est = 1 - (1 - interest_rate_est ** df_prev_app[\"CNT_PAYMENT\"]) / total_ratio\n    df_prev_app[\"INTEREST_RATE\"] = interest_rate_est\n    del total_ratio, interest_rate_est\n    \n    prev_app_groups = df_prev_app[df_prev_app[\"NAME_CONTRACT_STATUS\"] != \"Canceled\"].groupby(\"SK_ID_CURR\")\n    df_prev_app_agg = flatten_agg_df_columns(prev_app_groups.agg({\n        \"CNT_ASSESSED_RISK\": [\"sum\"],\n        \"NAME_YIELD_GROUP_CODE\": [\"mean\"],\n        \"DAYS_DECISION\": [\"max\"]\n        \n#         'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n#         'AMT_ANNUITY': ['min', 'max', 'mean'],\n#         'AMT_APPLICATION': ['min', 'max', 'mean'],\n#         'AMT_CREDIT': ['min', 'max', 'mean'],\n#         'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n#         'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n#         'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n#         'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n#         'DAYS_DECISION': ['min', 'max', 'mean'],\n#         'CNT_PAYMENT': ['mean', 'sum'],\n    }), \"PREV\")\n\n    prev_app_refused_groups = df_prev_app[df_prev_app.NAME_CONTRACT_STATUS == \"Refused\"].groupby(\"SK_ID_CURR\")\n    df_prev_refused_agg = flatten_agg_df_columns(prev_app_refused_groups.agg({\n        \"AMT_APPLICATION\": [\"max\"],\n        \"RATE_INTEREST_PRIMARY\": [\"mean\", \"max\"],\n        \"NAME_YIELD_GROUP_CODE\": [\"mean\", \"max\"]\n    }), \"PREV_REFUSED\")\n    df_prev_refused_agg[\"PREV_REFUSED_CNT\"] = prev_app_refused_groups.size()\n    df_prev_app_agg = df_prev_app_agg.join(df_prev_refused_agg, how=\"left\")\n    del df_prev_refused_agg\n\n    prev_app_approved_groups = df_prev_app[df_prev_app.NAME_CONTRACT_STATUS == \"Approved\"].groupby(\"SK_ID_CURR\")\n    df_prev_approved_agg = flatten_agg_df_columns(prev_app_approved_groups.agg({\n        \"AMT_APPLICATION\": [\"max\"],\n        \"AMT_ANNUITY\": [\"max\"],\n        \"DOWN_PAYMENT_RATIO\": [\"min\", \"max\", \"mean\"],\n        \"RATE_INTEREST_PRIMARY\": [\"mean\", \"max\"],\n        \"NAME_YIELD_GROUP_CODE\": [\"mean\", \"max\"],\n        \"INTEREST_RATE\": [\"mean\", \"min\", \"max\"],\n        \"PAYMENT_DURATION_RATIO\": [\"min\", \"max\", \"mean\"]\n    }), \"PREV_APPROVED\")\n    df_prev_approved_agg[\"PREV_APPROVED_CNT\"] = prev_app_approved_groups.size()\n    df_prev_app_agg = df_prev_app_agg.join(df_prev_approved_agg, how=\"left\")\n    del df_prev_approved_agg\n    del df_prev_app\n\n    if load_inst_pay:\n        if os.path.exists(PATH_INST_PAY_PROCESSED):\n            df_inst_pay = load_install_payments()\n        else:\n            df_inst_pay = clean_inst_pay(load_install_payments(False))\n            columns_to_write = [\"SK_ID_PREV\", \"SK_ID_CURR\", \"NUM_INSTALMENT_VERSION\", \"NUM_INSTALMENT_NUMBER\",\n                    \"DAYS_INSTALMENT\", \"DAYS_FIRST_PAYMENT\", \"DAYS_LAST_PAYMENT\", \"NUM_PAYMENTS\",\n                    \"AMT_INSTALMENT\", \"AMT_PAYMENT\", \"AMT_OVERDUE\", \"AMT_DPD30\", \"AMT_UNPAID\"]\n            df_inst_pay.to_csv(PATH_INST_PAY_PROCESSED, index=False, columns=columns_to_write)\n            # df_POS_CASH_balance = load_pos_balance()\n\n        # df_prev_app_processed = df_prev_app[[\"SK_ID_CURR\"]].copy()\n        # df_prev_app_processed[\"\"] = df_prev_app[\"NAME_CONTRACT_TYPE\"]\n\n        df_inst_pay[\"CNT_OVERDUE\"] = (df_inst_pay.AMT_OVERDUE > 0)\n        df_inst_pay[\"CNT_DPD30\"] = (df_inst_pay.AMT_DPD30 > 0)\n        df_inst_pay_groups = df_inst_pay.groupby(\"SK_ID_CURR\")\n\n        df_inst_pay_agg = flatten_agg_df_columns(df_inst_pay_groups.agg({\n            \"AMT_OVERDUE\": [\"max\", \"mean\"],\n            \"CNT_OVERDUE\": [\"sum\", \"mean\"],\n            \"AMT_DPD30\": [\"max\", \"mean\"],\n            \"CNT_DPD30\": [\"sum\", \"mean\"],\n            \"AMT_UNPAID\": [\"sum\"]\n        }), \"INST_PAY\")\n\n        # print_null_columns(df_inst_pay_agg)\n        df_prev_app_agg = df_prev_app_agg.join(df_inst_pay_agg, how=\"outer\")\n        del df_inst_pay,  df_inst_pay_agg\n\n    if load_credit_balance:\n        df_credit_card_balance = load_credit_balance_data()\n        df_credit_card_balance.sort_values([\"SK_ID_CURR\", \"SK_ID_PREV\", \"MONTHS_BALANCE\"], inplace=True)\n         \n        df_credit_card_balance[\"CNT_OVERDUE\"] = (df_credit_card_balance[\"SK_DPD\"] > 0).astype(np.uint8)\n        df_credit_card_balance[\"CNT_OVERDUE_DEF\"] = (df_credit_card_balance[\"SK_DPD_DEF\"] > 0).astype(np.uint8)\n        df_credit_card_balance[\"IS_BALANCE_HIGH\"] = (2 * df_credit_card_balance[\"AMT_BALANCE\"] > df_credit_card_balance[\"AMT_CREDIT_LIMIT_ACTUAL\"]).astype(np.uint8)\n        cc_groups = df_credit_card_balance.groupby(\"SK_ID_CURR\")\n        df_cc_agg = flatten_agg_df_columns(cc_groups.agg({\n            \"MONTHS_BALANCE\": [\"min\", \"max\"],\n            \"SK_DPD\": [\"max\", \"mean\"],\n            \"SK_DPD_DEF\": [\"max\", \"mean\"],\n            \"CNT_OVERDUE\": [\"sum\"],\n            \"CNT_OVERDUE_DEF\": [\"sum\"],\n            \"IS_BALANCE_HIGH\": [\"mean\", \"max\"],\n        }), \"CREDIT_CARD\")\n        \n        df_cc_agg_prev = flatten_agg_df_columns(df_credit_card_balance.groupby(\"SK_ID_PREV\").agg({\n            \"SK_ID_CURR\": [\"last\"],\n            \"MONTHS_BALANCE\": [\"last\"],\n            \"AMT_BALANCE\": [\"last\"],\n            \"AMT_CREDIT_LIMIT_ACTUAL\": [\"max\", \"last\"],\n            \"SK_DPD\": [\"last\"],\n            \"SK_DPD_DEF\": [\"last\"]\n        }))\n        df_cc_agg_prev[\"LAST_LIMIT_RATIO\"] = df_cc_agg_prev[\"AMT_CREDIT_LIMIT_ACTUAL_LAST\"] / df_cc_agg_prev[\"AMT_CREDIT_LIMIT_ACTUAL_MAX\"]\n        \n        df_cc_agg = df_cc_agg.join(flatten_agg_df_columns(df_cc_agg_prev.groupby(\"SK_ID_CURR_LAST\").agg({\n            \"AMT_CREDIT_LIMIT_ACTUAL_LAST\": [\"sum\", \"max\"],\n            \"AMT_BALANCE_LAST\": [\"sum\"],\n            \"LAST_LIMIT_RATIO\": [\"mean\", \"min\"],\n            \"SK_DPD_LAST\": [\"max\", \"sum\"],\n            \"SK_DPD_DEF_LAST\": [\"max\", \"sum\"]\n        }), \"CREDIT_CARD\"), how=\"inner\")\n        df_cc_agg[\"CREDIT_BALANCE_RATIO\"] = (df_cc_agg[\"CREDIT_CARD_AMT_BALANCE_LAST_SUM\"] / \n                                             df_cc_agg[\"CREDIT_CARD_AMT_CREDIT_LIMIT_ACTUAL_LAST_SUM\"]).fillna(0)\n        \n        df_prev_app_agg = df_prev_app_agg.join(df_cc_agg, how=\"outer\")\n        del df_credit_card_balance, df_cc_agg\n\n    df_prev_app_agg.fillna({\n        \"PREV_REFUSED_AMT_APPLICATION_MAX\": 0,\n        \"PREV_REFUSED_CNT\": 0,\n        \"PREV_APPROVED_AMT_APPLICATION_MAX\": 0,\n        \"PREV_APPROVED_AMT_ANNUITY_MAX\": 0,\n        \"PREV_APPROVED_CNT\": 0,\n        \"PREV_CNT_ASSESSED_RISK_SUM\": 0,\n        \"INST_PAY_AMT_OVERDUE_MAX\": 0,\n        \"INST_PAY_CNT_OVERDUE_SUM\": 0,\n        \"INST_PAY_CNT_OVERDUE_MEAN\": 0,\n        \"INST_PAY_AMT_DPD30_MAX\": 0,\n        \"INST_PAY_CNT_DPD30_SUM\": 0,\n        \"INST_PAY_CNT_DPD30_MEAN\": 0,\n        \"INST_PAY_AMT_UNPAID_SUM\": 0,\n        \"CREDIT_CARD_MONTHS_BALANCE_MAX\": 0,\n        \"CREDIT_CARD_MONTHS_BALANCE_MIN\": 0,\n        \"CREDIT_CARD_SK_DPD_MAX\": 0,\n        \"CREDIT_CARD_SK_DPD_DEF_MAX\": 0,\n        \"CREDIT_CARD_CNT_OVERDUE_SUM\": 0,\n        \"CREDIT_CARD_CNT_OVERDUE_DEF_SUM\": 0\n    }, inplace=True)\n\n    # print(df_prev_app_agg.head())\n    # print_null_columns(df_prev_app_agg)\n    return df_prev_app_agg\n\ndef preprocess_app(transformers, df, df_bureau_agg, df_prev_app_agg, impute=True):\n    perform_grouping = True\n    excluded_columns = [\"TARGET\"]\n    PASSTHROUGH_COLS = [\"CNT_CHILDREN\", \"CNT_FAM_MEMBERS\",\n                         \"AMT_INCOME_TOTAL\", \"AMT_CREDIT\", \"AMT_ANNUITY\", \"AMT_GOODS_PRICE\",\n                         \"REGION_POPULATION_RELATIVE\",\n                         \"DAYS_BIRTH\", \"DAYS_EMPLOYED\", \"DAYS_REGISTRATION\", \"DAYS_ID_PUBLISH\",\n                         \"OWN_CAR_AGE\",\n                         \"FLAG_EMP_PHONE\",\n                         # \"FLAG_MOBIL\", \"FLAG_EMP_PHONE\", \"FLAG_WORK_PHONE\", \"FLAG_CONT_MOBILE\", \"FLAG_PHONE\",\n                         # \"FLAG_EMAIL\",\n                         \"REGION_RATING_CLIENT\", \"REGION_RATING_CLIENT_W_CITY\",\n                         # \"HOUR_APPR_PROCESS_START\",\n                         # \"REG_REGION_NOT_LIVE_REGION\", \"REG_REGION_NOT_WORK_REGION\",\n                         # \"REG_CITY_NOT_LIVE_CITY\", \"REG_CITY_NOT_WORK_CITY\", \"LIVE_CITY_NOT_WORK_CITY\",\n                         \"DAYS_LAST_PHONE_CHANGE\",\n                         \"TOTALAREA_MODE\", \"AMT_REQ_CREDIT_BUREAU_MON\"]\n    # PASSTHROUGH_COLS.extend((\"FLAG_DOCUMENT_\" + str(i)) for i in range(2, 22))\n\n    IMPUTER_FIX_VAL_COL_MAP = {\n        0: [\"AMT_GOODS_PRICE\", \"OBS_30_CNT_SOCIAL_CIRCLE\",\n            \"DEF_30_CNT_SOCIAL_CIRCLE\", \"OBS_60_CNT_SOCIAL_CIRCLE\", \"DEF_60_CNT_SOCIAL_CIRCLE\"],\n        -1: [\"OWN_CAR_AGE\"]\n    }\n    MEAN_IMPUTER_COLS = [\"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]\n    MOST_FREQ_IMPUPTER_COLS = [\"CNT_FAM_MEMBERS\", \"AMT_ANNUITY\", \"DAYS_LAST_PHONE_CHANGE\"]\n\n    housing_columns = [\"APARTMENTS_AVG\", \"BASEMENTAREA_AVG\", \"YEARS_BEGINEXPLUATATION_AVG\",\n                       \"YEARS_BUILD_AVG\", \"COMMONAREA_AVG\", \"ELEVATORS_AVG\", \"ENTRANCES_AVG\",\n                       \"FLOORSMAX_AVG\", \"FLOORSMIN_AVG\", \"LANDAREA_AVG\",\n                       \"LIVINGAPARTMENTS_AVG\", \"LIVINGAREA_AVG\", \"NONLIVINGAPARTMENTS_AVG\",\n                       \"NONLIVINGAREA_AVG\",\n                       \"APARTMENTS_MODE\", \"BASEMENTAREA_MODE\", \"YEARS_BEGINEXPLUATATION_MODE\",\n                       \"YEARS_BUILD_MODE\", \"COMMONAREA_MODE\", \"ELEVATORS_MODE\", \"ENTRANCES_MODE\",\n                       \"FLOORSMAX_MODE\", \"FLOORSMIN_MODE\", \"LANDAREA_MODE\",\n                       \"LIVINGAPARTMENTS_MODE\", \"LIVINGAREA_MODE\", \"NONLIVINGAPARTMENTS_MODE\",\n                       \"NONLIVINGAREA_MODE\",\n                       \"APARTMENTS_MEDI\", \"BASEMENTAREA_MEDI\", \"YEARS_BEGINEXPLUATATION_MEDI\",\n                       \"YEARS_BUILD_MEDI\", \"COMMONAREA_MEDI\", \"ELEVATORS_MEDI\", \"ENTRANCES_MEDI\",\n                       \"FLOORSMAX_MEDI\", \"FLOORSMIN_MEDI\", \"LANDAREA_MEDI\", \"LIVINGAPARTMENTS_MEDI\",\n                       \"LIVINGAREA_MEDI\", \"NONLIVINGAPARTMENTS_MEDI\", \"NONLIVINGAREA_MEDI\",\n                       # 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE'\n                       ]\n\n    df_app_processed = df[[]].copy()\n    df_app_processed[\"NAME_CONTRACT_TYPE\"] = df[\"NAME_CONTRACT_TYPE\"].str.startswith(\"C\").astype(np.uint8)\n    df_app_processed[\"IS_MALE\"] = df[\"CODE_GENDER\"].cat.codes\n    df_app_processed[\"FLAG_OWN_CAR\"] = df[\"FLAG_OWN_CAR\"].cat.codes\n    df_app_processed[\"FLAG_OWN_REALTY\"] = df[\"FLAG_OWN_REALTY\"].cat.codes\n    df_app_processed[\"NAME_EDUCATION_TYPE\"] = df[\"NAME_EDUCATION_TYPE\"].cat.codes\n    if perform_grouping:\n        name_type_suite_groups = [(\"Acc_No\", [\"Unaccompanied\"]),\n                                  (\"Acc_Fam_Ch\", [\"Family\", \"Children\", \"Group of people\"]),\n                                  (\"Acc_Spouse\", [\"Spouse, partner\"]),\n                                  (\"Acc_Other\", [\"Other_A\", \"Other_B\"])]\n        df_app_processed = pd.concat(\n            [df_app_processed, group_values(df[\"NAME_TYPE_SUITE\"], name_type_suite_groups)], axis=1, copy=False)\n\n        family_status_groups = [(\"With_family\", [\"Married\", \"Civil marriage\"]),\n                                (\"Without_family\", [\"Single / not married\", \"Separated\", \"Widow\"])]\n        df_app_processed = pd.concat(\n            [df_app_processed, group_values(df[\"NAME_FAMILY_STATUS\"], family_status_groups)], axis=1, copy=False)\n\n        income_type_groups = [(\"Income_Job\", [\"Working\", \"Maternity leave\"]),\n                              (\"Income_Commercial\", [\"Commercial associate\", \"Businessman\"]),\n                              (\"Income_Pensioner\", [\"Pensioner\"]),\n                              (\"Income_Servant\", [\"State servant\"]),\n                              (\"Income_Other\", [\"Unemployed\", \"Student\"])]\n        df_app_processed = pd.concat(\n            [df_app_processed, group_values(df[\"NAME_INCOME_TYPE\"], income_type_groups)], axis=1, copy=False)\n\n        organization_groups = [(\"Org_Missing\", [np.nan]),\n                               (\"Org_Business_1\", [\"Business Entity Type 1\"]),\n                               (\"Org_Business_2\", [\"Business Entity Type 2\"]),\n                               (\"Org_Business_3\", [\"Business Entity Type 3\"]),\n                               (\"Org_Government\", [\"Government\"]),\n                               (\"Org_Self\", [\"Self-employed\"]),\n                               (\"Org_Trade_7\", [\"Trade: type 7\"]),\n                               (\"Org_Transport_3\", [\"Transport: type 3\"]),\n                               (\"Org_Transport_4\", [\"Transport: type 4\"]),\n                               (\"Org_Medicine\", [\"Medicine\"]),\n                               (\"Org_Other\", [\"Other\"]),\n                               (\"Org_Mix_0\", [\"Trade: type 6\", \"Transport: type 1\", \"Industry: type 12\"]),\n                               (\"Org_Mix_1\", [\"Bank\", \"Military\", \"Police\", \"University\", \"Security Ministries\"]),\n                               (\"Org_Mix_2\", [\"School\", \"Insurance\", \"Culture\"]),\n                               (\"Org_Mix_3\", [\"Trade: type 5\", \"Trade: type 4\", \"Religion\"]),\n                               (\"Org_Mix_4\", [\"Hotel\", \"Industry: type 10\", \"Medicine\"]),\n                               (\"Org_Mix_5\", [\"Industry: type 3\", \"Realtor\", \"Agriculture\",\n                                              \"Trade: type 3\", \"Industry: type 4\", \"Security\"]),\n                               (\"Org_Mix_6\", [\"Industry: type 11\", \"Postal\"]),\n                               (\"Org_Mix_7\", [\"Industry: type 13\", \"Industry: type 8\", \"Restaurant\",\n                                              \"Construction\", \"Cleaning\", \"Industry: type 1\"]),\n                               ]\n        df_app_processed = pd.concat(\n            [df_app_processed, group_values(df[\"ORGANIZATION_TYPE\"], organization_groups)], axis=1, copy=False)\n\n        housing_groups = [\n                          (\"Housing_Missing\", [np.nan]),\n                          (\"Housing_Own\", [\"House / apartment\"]),\n                          (\"Housing_Provided\", [\"Municipal apartment\", \"Office apartment\", \"Co-op apartment\"]),\n                          (\"Housing_Rent\", [\"Rented apartment\"]),\n                          (\"Housing_Parent\", [\"With parents\"])\n                         ]\n        df_app_processed = pd.concat(\n            [df_app_processed, group_values(df[\"NAME_HOUSING_TYPE\"], housing_groups)], axis=1, copy=False)\n    else:\n        df_app_processed = append_one_hot_encoding(df_app_processed, df[\"NAME_TYPE_SUITE\"], prefix=\"Acc\")\n        df_app_processed = append_one_hot_encoding(df_app_processed, df[\"NAME_FAMILY_STATUS\"], prefix=\"Fam\")\n        df_app_processed = append_one_hot_encoding(df_app_processed, df[\"NAME_INCOME_TYPE\"], prefix=\"Income\")\n        df_app_processed = append_one_hot_encoding(df_app_processed, df[\"ORGANIZATION_TYPE\"], prefix=\"Org\")\n        df_app_processed = append_one_hot_encoding(df_app_processed, df[\"NAME_HOUSING_TYPE\"], prefix=\"Housing\")\n\n    df_app_processed[PASSTHROUGH_COLS] = df[PASSTHROUGH_COLS]\n#     df_app_processed[\"EXT_SCORE_MEAN\"] = df[[\"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]].mean(axis=1)\n    df_app_processed[\"ANN_PERCENT\"] = df.AMT_ANNUITY / df.AMT_INCOME_TOTAL\n    df_app_processed[\"EMPLOYED_PERCENT\"] = df.DAYS_EMPLOYED / df.DAYS_BIRTH\n    df_app_processed[\"PAYMENT_RATIO\"] = df.AMT_CREDIT / df.AMT_ANNUITY\n    # df_app_processed[\"LEFT_OVER\"] = df.AMT_INCOME_TOTAL - df.AMT_ANNUITY\n\n    if transformers:\n        imputer = transformers[\"imputer\"]\n        imputer_cols = transformers[\"imputer_cols\"]\n    else:\n        imputer_cols = []\n        for cols in IMPUTER_FIX_VAL_COL_MAP.values():\n            imputer_cols.extend(cols)\n        imputer_cols.extend(MEAN_IMPUTER_COLS)\n        imputer_cols.extend(MOST_FREQ_IMPUPTER_COLS)\n        transformers[\"imputer_cols\"] = imputer_cols\n        assert len(imputer_cols) == len(set(imputer_cols))\n\n        if impute:\n            imputer = [(\"FixVal_Imputer_\" + str(v), SimpleImputer(strategy=\"constant\", fill_value=v), cols)\n                       for v, cols in IMPUTER_FIX_VAL_COL_MAP.items()]\n            imputer.append((\"Mean_Imputer\", SimpleImputer(strategy=\"mean\"), MEAN_IMPUTER_COLS))\n            imputer.append((\"Most_Freq_Imputer\", SimpleImputer(strategy=\"most_frequent\"), MOST_FREQ_IMPUPTER_COLS))\n\n            transformers[\"imputer\"] = imputer = ColumnTransformer(imputer)\n\n            imputer.fit(df[imputer_cols])\n        else:\n            transformers[\"imputer\"] = imputer = ColumnTransformer([(\"No_Imputer\", \"passthrough\", imputer_cols)])\n            imputer.fit(df[imputer_cols])\n\n    df_app_processed[imputer_cols] = pd.DataFrame(imputer.transform(df[imputer_cols]), index=df.index, copy=False)\n    # df_app_processed[housing_columns] = df[housing_columns]\n\n    if df_bureau_agg is not None:\n        df_app_processed = df_app_processed.join(df_bureau_agg, how=\"left\")\n    if df_prev_app_agg is not None:\n        df_app_processed = df_app_processed.join(df_prev_app_agg, how=\"left\")\n        df_app_processed[\"ANNUITY_RATIO\"] = df_app_processed.AMT_ANNUITY / df_app_processed.PREV_APPROVED_AMT_ANNUITY_MAX\n\n    # for name, col in df.iteritems():\n    #     if not (col.isnull().any() or col.dtype == \"object\" or name in excluded_columns):\n    #         # df_app_processed[name] = col\n    #         print(name)\n\n    return df_app_processed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_preprocessed_data(impute=True, random_seed=None):\n    print(\"Reading data\")\n    df_app_train, df_app_test = load_app_data()\n    print(\"Finish reading data\")\n\n    print(\"Preprocess training data\")\n    df_bureau_agg = None\n    df_prev_app_agg = None\n    df_bureau_agg = get_preprocessed_bureau_data()\n    print(\"Finish preprocessing bureau data\")\n    df_prev_app_agg = get_preprocessed_previous_app_data(True, True)\n    print(\"Finish preprocessing previous application data\")\n\n    df_app_train = shuffle(df_app_train, random_state=random_seed)\n    transformers = dict()\n    X_train = preprocess_app(transformers, df_app_train, df_bureau_agg, df_prev_app_agg, impute=impute)\n    y_train = df_app_train[\"TARGET\"]\n    print(\"Finish preprocessing application data\")\n\n    print(\"Preprocess test data\")\n    X_test = preprocess_app(transformers, df_app_test, df_bureau_agg, df_prev_app_agg, impute=impute)\n\n    print(\"Training data shape:\", X_train.shape)\n    X_train.info(verbose=5)\n    \n    return X_train, y_train, X_test\n\nX_train, y_train, X_test = get_preprocessed_data(PERFORM_IMPUTATION, RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Modeling building and Execution**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model_and_classify(X_train, y_train, X_test=None, classifier=\"xgb\", early_stopping=True, \n                             cv=CROSS_VALIDATION_FOLD, tune_param=False, random_seed=None, \n                             output_path=\"submission.csv\"):\n    print(\"Initializing classifier\")\n    \n    if classifier == \"xgb\":\n        if tune_param:\n            clf = XGBClassifier(seed=random_seed, tree_method=\"gpu_hist\")\n        else:\n            clf = XGBClassifier(n_estimators=2000, min_child_weight=32, max_depth=6, # max_leaves=64,\n                                min_split_loss=0.08, learning_rate=0.1,\n                                reg_lambda=1, reg_alpha=0.6,\n                                scale_pos_weight=3,\n                                seed=random_seed, tree_method=\"gpu_hist\")\n            # clf = XGBClassifier(max_depth=8, min_child_weight=12, seed=1)\n#             clf = XGBClassifier(n_estimators=10000, learning_rate=0.02, max_leaves=34, max_depth=8,\n#                             colsample_bytree=0.95, subsample=0.872,\n#                             reg_alpha=0.0415, reg_lambda=0.0735, min_split_loss=0.022,\n#                             min_child_weight=39.33,\n#                             seed=random_seed, tree_method=\"gpu_hist\")\n    elif classifier == \"lgbm\":\n#         clf = LGBMClassifier(\n#             n_jobs=8, n_estimators=200, learning_rate=0.02, num_leaves=34, max_depth=8,\n#             colsample_bytree=0.95, subsample=0.872,\n#             reg_alpha=0.0415, reg_lambda=0.0735,\n#             min_split_gain=0.022, min_child_weight=39.33,\n# #             early_stopping_round=10,\n#             metric=\"auc\",\n#             tree_learner=\"data\",\n#             data_random_seed=random_seed,\n# #             silent=-1,\n#             verbose=1)\n        clf = LGBMClassifier(\n            boosting_type=\"goss\", n_estimators=1000, learning_rate=0.00513,\n            num_leaves=54, max_depth=10, subsample_for_bin=240000,\n            reg_alpha=0.436193, reg_lambda=0.479169,\n            min_split_gain=0.025, colsample_bytree=0.52,\n            subsample=1, # 'is_unbalance': False,\n            silent=-1, verbose=-1\n        )\n    else:\n        # clf = GradientBoostingClassifier(max_depth=10, min_samples_split=15, verbose=5)\n        # clf = DecisionTreeClassifier(class_weight=weight_dict, max_depth=15, min_samples_split=4)\n        # clf = LogisticRegression(class_weight=weight_dict)\n        raise ValueError(\"Unsupported classifier: \" + classifier)\n\n    if early_stopping:\n        fit_orig = clf.fit\n        VALIDATION_SIZE = 0.1\n        def fit_wrapped(X, y, **kwargs):\n            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VALIDATION_SIZE)\n            fit_orig(X_train, y_train, eval_set=[(X_val, y_val)], \n                     early_stopping_rounds=50, eval_metric=\"auc\")\n        clf.fit = fit_wrapped\n        \n    if cv > 0:\n        if tune_param:\n            print(\"Tuning classifier parameters\")\n            params = {\n                \"n_estimators\": [200, 400],\n                \"max_depth\": [6, 8],\n                \"max_leaves\": [64, 128],\n                \"min_child_weight\": [24, 32],\n                \"min_split_loss\": [0.02 * (1 << n) for n in range(6)],\n                \"learning_rate\": [0.06 + 0.02 * (1 << n) for n in range(1, 4)],\n                \"reg_lambda\": [0.2 * n for n in range(6)],\n                \"reg_alpha\": [0.2 * n for n in range(6)],\n                \"scale_pos_weight\": [1, 3, 5]\n            }\n#             model_selection_task = GridSearchCV(clf, param_grid=params, scoring=\"roc_auc\", cv=cv, verbose=5)\n            model_selection_task = RandomizedSearchCV(clf, params, n_iter=50, scoring=\"roc_auc\", \n                                                      cv=cv, n_jobs=5, verbose=5)\n            global results\n            results = model_selection_task.fit(X_train, y_train)\n            print(f\"Best score: {results.best_score_}\")\n            print(f\"Best parameters: {results.best_params_}\")\n            print(f\"CV results: {results.cv_results_}\")\n            clf = results.best_estimator_\n        else:\n            print(f\"Perform {cv:d}-fold cross validation\")\n            score_val = sum(cross_val_score(clf, X_train, y_train,\n                                            cv=cv, scoring=\"roc_auc\", verbose=10, n_jobs=5)) / cv\n            print(\"Validation AUC: %.6f\" % score_val)\n    else:\n        test_size = 0.1\n        print(f\"Perform hold-out validation (Test size: {test_size:.0%})\")\n        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size, random_state=random_seed)\n        clf.fit(X_train, y_train)\n        prob_val = clf.predict_proba(X_val)[:, 1]\n        score_val = roc_auc_score(y_val, prob_val)\n        print(\"Validation AUC: %.6f\" % score_val)\n    # print(clf.feature_importances_)\n\n    if X_test is None:\n        return\n\n    print(\"Training classifier\")\n    clf.fit(X_train, y_train)\n\n#     print(\"Dumping trained classifier\")\n#     from joblib import dump\n#     dump(clf, 'boost_tree_gpu_0.joblib')\n\n    print(\"Classify training and test set\")\n#     train_prob_df = pd.DataFrame(clf.predict_proba(X_train)[:, 1], index=X_train.index, columns=[\"PRED_PROB\"])\n#     train_prob_df.to_csv(\"train_prob.csv\")\n    test_prob_df = pd.DataFrame(clf.predict_proba(X_test)[:, 1], index=X_test.index, columns=[\"TARGET\"])\n    test_prob_df.to_csv(output_path)\n    print(\"Finished classifying\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%timeit -n 1 -r 1\n\n# Training with LGBM\nbuild_model_and_classify(X_train, y_train, X_test, \"lgbm\", cv=CROSS_VALIDATION_FOLD, tune_param=True,\n                         random_seed=RANDOM_SEED, output_path=\"submission_lgbm.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build_model_and_classify(X_train, y_train, X_test, random_seed=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"build_model_and_classify(X_train, y_train, X_test, tune_param=True, random_seed=RANDOM_SEED,\n                         output_path=\"submission_xgb.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_result = pd.DataFrame.from_records(results.cv_results_[\"params\"])\ndf_result[\"rank\"] = results.cv_results_[\"rank_test_score\"]\ndf_result[\"score\"] = results.cv_results_[\"mean_test_score\"]\ndf_result.sort_values(by=\"rank\", inplace=True)\ndf_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in df_result.columns[:-2]:\n    print(df_result.groupby(c)[\"score\"].mean().sort_values())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}