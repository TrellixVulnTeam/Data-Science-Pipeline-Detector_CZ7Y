{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"*This notebook is the second part of a series of notebooks that comprise a larger project. This is the full list of notebooks:*\n1. [Understanding the problem](https://www.kaggle.com/timvh2/understanding-the-problem/edit)\n2. Data cleaning\n3. [Exploratory data analysis](https://www.kaggle.com/timvh2/data-exploratie-versie-2/edit)\n4. [Building a model](https://www.kaggle.com/timvh2/building-a-model/edit)\n5. [Model evaluation and interpretation](https://www.kaggle.com/timvh2/model-evaluation-and-interpretation/edit)\n\nThe data and Home Credit Default competition can be found [here](https://www.kaggle.com/c/home-credit-default-risk).\n\n**Note**: we should remove the /edit from the links when we publicize this. Also, if we change the title of the notebook the link will change!","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-06T10:53:05.232992Z","iopub.execute_input":"2021-07-06T10:53:05.233559Z","iopub.status.idle":"2021-07-06T10:53:05.243235Z","shell.execute_reply.started":"2021-07-06T10:53:05.233513Z","shell.execute_reply":"2021-07-06T10:53:05.241925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Path of the file to read\napplication_train_file_path = '../input/home-credit-default-risk/application_train.csv'\napplication_test_file_path = '../input/home-credit-default-risk/application_test.csv'\n\nbureau_file_path = '../input/home-credit-default-risk/bureau.csv'\nbureau_balance_file_path = '../input/home-credit-default-risk/bureau_balance.csv'\n\nprevious_application_file_path = '../input/home-credit-default-risk/previous_application.csv'\nPOS_CASH_balance_file_path = '../input/home-credit-default-risk/POS_CASH_balance.csv'\ninstallments_payments_file_path = '../input/home-credit-default-risk/installments_payments.csv'\ncredit_card_balance_file_path = '/kaggle/input/home-credit-default-risk/credit_card_balance.csv'\n\n# Read the file\napplication_train_data = pd.read_csv(application_train_file_path, index_col = \"SK_ID_CURR\")\napplication_test_data = pd.read_csv(application_test_file_path, index_col = \"SK_ID_CURR\")\n\n# bureau_data = pd.read_csv(bureau_file_path)\n# bureau_balance_data = pd.read_csv(bureau_balance_file_path)\n\n# previous_application_data = pd.read_csv(previous_application_file_path)\n# POS_CASH_balance_data = pd.read_csv(POS_CASH_balance_file_path)\n# installments_payments_data = pd.read_csv(installments_payments_file_path)\n# credit_card_balance_data = pd.read_csv(credit_card_balance_file_path)\n\n# Copy the permutable dataframes\n\nX_train_full = application_train_data.copy()\nX_test = application_test_data.copy()\n\n# Define target\ny = X_train_full.TARGET\n\n# Drop target from training set\nX_train_full = X_train_full.drop(['TARGET'], axis = 1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-06T10:53:05.244928Z","iopub.execute_input":"2021-07-06T10:53:05.245613Z","iopub.status.idle":"2021-07-06T10:53:12.495759Z","shell.execute_reply.started":"2021-07-06T10:53:05.245559Z","shell.execute_reply":"2021-07-06T10:53:12.494812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train_full","metadata":{"execution":{"iopub.status.busy":"2021-07-02T12:18:07.218744Z","iopub.execute_input":"2021-07-02T12:18:07.219148Z","iopub.status.idle":"2021-07-02T12:18:07.222127Z","shell.execute_reply.started":"2021-07-02T12:18:07.219119Z","shell.execute_reply":"2021-07-02T12:18:07.221268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data cleaning\n\nIn the previous chapter we have examined the problem and seen how the data are structured. We now wish to take a first look at the data, and prepare them for exploratory data analysis. To this end, we will take the following steps:\n\n1. We examine the distribution of all variables. We check this distribution is consistent with our expectations of the variable, and deal with any anomalies.\n2. We examine the missing values in the data, and either impute or drop them.\n3. We examine the categorical variables, and apply either one-hot encoding or label encoding to them.\n\nThese steps will be put together into a pipeline with which we will preprocess the data.","metadata":{}},{"cell_type":"markdown","source":"# 2.1 Distributions","metadata":{}},{"cell_type":"code","source":"# X_train_full.loc[114967, [\"AMT_INCOME_TOTAL\"]] = None\n\n# outliers = X_train_full.loc[X_train_full.AMT_INCOME_TOTAL > 50000000]\n# # X_train_full.AMT_INCOME_TOTAL.describe()\n\n# print(outliers)\n\n# # X_train_full.loc[]\n\n# X_train_full.iloc[12840].AMT_INCOME_TOTAL\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T12:18:07.223331Z","iopub.execute_input":"2021-07-02T12:18:07.223786Z","iopub.status.idle":"2021-07-02T12:18:07.235623Z","shell.execute_reply.started":"2021-07-02T12:18:07.223757Z","shell.execute_reply":"2021-07-02T12:18:07.234552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.2 Missing values\n\n","metadata":{}},{"cell_type":"markdown","source":"First, let's take inventory of the missing values. We will start with the application_train_data dataset.","metadata":{}},{"cell_type":"code","source":"# find the columns that contain missing values\ncols_missing_vals = [col for col in application_train_data.columns\n                     if application_train_data[col].isnull().any()]\n\nn_missing_vals = application_train_data.isnull().sum()[cols_missing_vals]\n\nn_missing_vals_percent = (n_missing_vals / len(application_train_data) * 100).sort_values(ascending = False)\n\n# # Percentages of missing values w.r.t. the whole column\n# with pd.option_context('display.max_rows', None):\n#   display(n_missing_vals_percent)\n\n# pd.DataFrame([cols_missing_vals, n_missing_vals]), index = cols_missing_vals, columns = [\"\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-06T10:53:12.497371Z","iopub.execute_input":"2021-07-06T10:53:12.49768Z","iopub.status.idle":"2021-07-06T10:53:13.415399Z","shell.execute_reply.started":"2021-07-06T10:53:12.497644Z","shell.execute_reply":"2021-07-06T10:53:13.414433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Matrix and Dendrogram of relations between columns in term of missing values\n# import missingno as mn\n\n# mn.matrix(X_train_full.sample(500), figsize = (10,6))\n\n# mn.dendrogram(X_train_full, figsize = (20,50))","metadata":{"execution":{"iopub.status.busy":"2021-07-02T12:18:08.240519Z","iopub.execute_input":"2021-07-02T12:18:08.241131Z","iopub.status.idle":"2021-07-02T12:18:08.245277Z","shell.execute_reply.started":"2021-07-02T12:18:08.241076Z","shell.execute_reply":"2021-07-02T12:18:08.244314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that there are 57 variables that have a large amount of missing values, and 10 more with small amounts of missing values. Those with large amounts of missing data are almost all variables describing the building the client lives in. Though we do not know how these data were collected or often even exactly what they describe, it is probably the case that these data are not missing completely at random. But as we suspect the properties of the building one lives in are usually related to for instance income and number of children (which are known), we will use imputation to fill in missing values.\n\nFor now we will use a method of imputation known as *k-nearest neighbor(kNN)*.","metadata":{}},{"cell_type":"code","source":"# from sklearn.impute import KNNImputer\n\n# imputer = KNNImputer(n_neighbors = 5)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T12:18:08.248162Z","iopub.execute_input":"2021-07-02T12:18:08.248629Z","iopub.status.idle":"2021-07-02T12:18:08.258807Z","shell.execute_reply.started":"2021-07-02T12:18:08.248596Z","shell.execute_reply":"2021-07-02T12:18:08.257682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distinguish categorical and numerical columns\ncat_cols = [col for col in X_train_full.columns if X_train_full[col].dtype == \"object\"]\nnum_cols = [col for col in X_train_full.columns if X_train_full[col].dtype in [\"int64\", \"float64\"]]\n\ncat_cols_with_two_vals = [col for col in cat_cols if len(X_train_full[col].unique()) == 2]\n\ncat_cols_with_more_vals = [col for col in cat_cols if len(X_train_full[col].unique()) > 2]\n\nprint(cat_cols_with_more_vals)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T10:53:13.41722Z","iopub.execute_input":"2021-07-06T10:53:13.417631Z","iopub.status.idle":"2021-07-06T10:53:14.263114Z","shell.execute_reply.started":"2021-07-06T10:53:13.417586Z","shell.execute_reply":"2021-07-06T10:53:14.262357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import TransformerMixin , BaseEstimator\n\nclass KillVampires(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, feature_name):\n        self.feature_name = feature_name\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        X_ = X.copy() # We create a copy to prevent meddling with the original dataset\n        X_[self.feature_name] = X_[self.feature_name].replace(to_replace = 365243, value = np.nan)\n        return X_","metadata":{"execution":{"iopub.status.busy":"2021-07-06T10:53:14.264152Z","iopub.execute_input":"2021-07-06T10:53:14.26452Z","iopub.status.idle":"2021-07-06T10:53:15.176598Z","shell.execute_reply.started":"2021-07-06T10:53:14.26449Z","shell.execute_reply":"2021-07-06T10:53:15.175667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rating(rij):\n    if rij > 10**7:\n        return np.nan \n    else:\n        return rij","metadata":{"execution":{"iopub.status.busy":"2021-07-06T10:53:15.177935Z","iopub.execute_input":"2021-07-06T10:53:15.178269Z","iopub.status.idle":"2021-07-06T10:53:15.184418Z","shell.execute_reply.started":"2021-07-06T10:53:15.178233Z","shell.execute_reply":"2021-07-06T10:53:15.183466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class KillLiars(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, feature_name):\n#         print('\\n>>>>>>>init() called.\\n')\n        self.feature_name = feature_name\n    \n    def fit(self, X, y = None):\n#         print('\\n>>>>>>>fit() called.\\n')\n        return self\n    \n    def transform(self, X, y = None):\n#         print('\\n>>>>>>>transform() called.\\n')\n        X_ = X.copy() # We create a copy to prevent meddling with the original dataset\n        X_[self.feature_name] = X_[self.feature_name].apply(lambda x: rating(x))\n        return X_","metadata":{"execution":{"iopub.status.busy":"2021-07-06T10:53:15.186376Z","iopub.execute_input":"2021-07-06T10:53:15.186685Z","iopub.status.idle":"2021-07-06T10:53:15.198277Z","shell.execute_reply.started":"2021-07-06T10:53:15.186646Z","shell.execute_reply":"2021-07-06T10:53:15.197445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import KNNImputer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nnum_transformer = Pipeline(steps = [\n    (\"killVamp\", KillVampires('DAYS_EMPLOYED')),\n    (\"killLiar\", KillLiars('AMT_INCOME_TOTAL')),\n    (\"scaler\", StandardScaler()),\n    (\"imputer\", SimpleImputer(strategy = \"mean\"))\n])\n\n# preprocessing for categorical data with two values\ncat_transformer_two_vals = Pipeline(steps = [\n    (\"imputer\", SimpleImputer(strategy = \"constant\", fill_value = \"missing\")),\n    (\"label_encoder\", OrdinalEncoder())\n])\n\n\n# preprocessing for categorical data with more than two values\ncat_transformer_more_vals = Pipeline(steps = [\n    (\"imputer\", SimpleImputer(strategy = \"constant\", fill_value = \"missing\")), \n    (\"onehot_encoder\", OneHotEncoder(handle_unknown = \"ignore\"))\n])\n\n# bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('num', num_transformer, num_cols),\n        ('cat_with_two_vals', cat_transformer_two_vals, cat_cols_with_two_vals),\n        ('cat_with_more_than_two_vals', cat_transformer_more_vals, cat_cols_with_more_vals)\n])","metadata":{"execution":{"iopub.status.busy":"2021-07-06T10:53:15.199709Z","iopub.execute_input":"2021-07-06T10:53:15.200236Z","iopub.status.idle":"2021-07-06T10:53:15.530415Z","shell.execute_reply.started":"2021-07-06T10:53:15.200202Z","shell.execute_reply":"2021-07-06T10:53:15.529361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.metrics import roc_auc_score\n\nmodel = LogisticRegressionCV(class_weight = \"balanced\", max_iter = 1000, cv = 5, scoring = \"roc_auc\")\n\npipeline = Pipeline(steps = [('preprocessor', preprocessor),\n                              ('model', model)\n                             ])","metadata":{"execution":{"iopub.status.busy":"2021-07-06T11:25:31.210594Z","iopub.execute_input":"2021-07-06T11:25:31.211103Z","iopub.status.idle":"2021-07-06T11:25:31.215718Z","shell.execute_reply.started":"2021-07-06T11:25:31.21107Z","shell.execute_reply":"2021-07-06T11:25:31.215034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train_split, X_test_split, y_train_split, y_test_split = train_test_split(\n    X_train_full, y, test_size = 0.2, train_size = 0.8, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T11:12:21.89187Z","iopub.execute_input":"2021-07-06T11:12:21.892248Z","iopub.status.idle":"2021-07-06T11:12:22.389429Z","shell.execute_reply.started":"2021-07-06T11:12:21.892213Z","shell.execute_reply":"2021-07-06T11:12:22.388559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline.fit(X_train_split, y_train_split)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T11:25:38.123034Z","iopub.execute_input":"2021-07-06T11:25:38.123451Z","iopub.status.idle":"2021-07-06T11:36:26.033082Z","shell.execute_reply.started":"2021-07-06T11:25:38.123416Z","shell.execute_reply":"2021-07-06T11:36:26.032004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = model.coef_\n\nprint(params)\n\ntype(params)\n\nonehot_features = pipeline.named_steps[\"preprocessor\"].transformers_[2][1].named_steps[\"onehot_encoder\"].get_feature_names(cat_cols_with_more_vals)\n\none_hot_col_names = X_train_split.copy().columns.tolist()\nfor col in cat_cols_with_more_vals:\n    one_hot_col_names.remove(col)\none_hot_col_names.extend(onehot_features)\n\nprint(len(num_cols))\n\nprint(one_hot_col_names)\nlen(one_hot_col_names)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T11:37:27.526009Z","iopub.execute_input":"2021-07-06T11:37:27.526448Z","iopub.status.idle":"2021-07-06T11:37:27.689037Z","shell.execute_reply.started":"2021-07-06T11:37:27.526402Z","shell.execute_reply":"2021-07-06T11:37:27.688074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nkeys = one_hot_col_names\nvals = params.tolist()\n\nprint(keys)\nprint(vals[0])\n\nprint(len(keys))\nprint(len(vals[0]))\n\nplt.figure(figsize=(20,6))\n\nsns.barplot(x = keys, y = vals[0])","metadata":{"execution":{"iopub.status.busy":"2021-07-06T11:37:35.060917Z","iopub.execute_input":"2021-07-06T11:37:35.06144Z","iopub.status.idle":"2021-07-06T11:37:40.966323Z","shell.execute_reply.started":"2021-07-06T11:37:35.061405Z","shell.execute_reply":"2021-07-06T11:37:40.965211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coefficients = dict(zip(keys, vals[0]))\nwith pd.option_context(\"display.max_rows\", None,\"display.min_rows\", None):\n    print(pd.Series(coefficients).sort_values())","metadata":{"execution":{"iopub.status.busy":"2021-07-06T11:37:46.948385Z","iopub.execute_input":"2021-07-06T11:37:46.9488Z","iopub.status.idle":"2021-07-06T11:37:46.966917Z","shell.execute_reply.started":"2021-07-06T11:37:46.948759Z","shell.execute_reply":"2021-07-06T11:37:46.964926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\npreds = pipeline.predict_proba(X_test_split)[: ,1]\nprint(roc_auc_score(y_test_split, preds, average = None))","metadata":{"execution":{"iopub.status.busy":"2021-07-06T11:43:30.883589Z","iopub.execute_input":"2021-07-06T11:43:30.883954Z","iopub.status.idle":"2021-07-06T11:43:31.740198Z","shell.execute_reply.started":"2021-07-06T11:43:30.883922Z","shell.execute_reply":"2021-07-06T11:43:31.73904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nimport time\n\ntic = time.time()\n# preprocess and fit model\npipeline.fit(X_train_full, y)\ntoc = time.time()\nprint(toc-tic)\n\ntic = time.time()\n# make predictions for validation set\npreds = pipeline.predict_proba(X_test)[:,1]\ntoc = time.time()\nprint(toc-tic)\n\n# # evaluate model\n# score = roc_auc_score(y, preds, average = None)\n# print('ROC AUC score:', score)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T11:45:29.380314Z","iopub.execute_input":"2021-07-06T11:45:29.380861Z","iopub.status.idle":"2021-07-06T11:59:42.298031Z","shell.execute_reply.started":"2021-07-06T11:45:29.380813Z","shell.execute_reply":"2021-07-06T11:59:42.295159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds","metadata":{"execution":{"iopub.status.busy":"2021-07-06T12:00:09.027858Z","iopub.execute_input":"2021-07-06T12:00:09.028286Z","iopub.status.idle":"2021-07-06T12:00:09.035467Z","shell.execute_reply.started":"2021-07-06T12:00:09.02822Z","shell.execute_reply":"2021-07-06T12:00:09.034451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds = preds.copy()\nsubmission = pd.concat([pd.Series(test_preds, index = X_test.index)], axis = 1)\nsubmission.columns = ['TARGET']\nsubmission.index.names = ['SK_ID_CURR']\nsubmission.to_csv('submission.csv', index=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T12:00:12.35418Z","iopub.execute_input":"2021-07-06T12:00:12.354633Z","iopub.status.idle":"2021-07-06T12:00:12.560064Z","shell.execute_reply.started":"2021-07-06T12:00:12.354556Z","shell.execute_reply":"2021-07-06T12:00:12.558894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2021-07-02T12:18:25.039935Z","iopub.status.idle":"2021-07-02T12:18:25.040577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_case = X_train_full.iloc[0:-1]\n# test_y = y.iloc[0:-1]\n\n# test_case = X_train_full\n# test_y = y\n\n# # print(test_y)\n# import time\n# tic = time.time()\n# result = pd.DataFrame(preprocessor.fit_transform(test_case,test_y))\n# toc = time.time()\n# print(toc-tic)\n\n# preprocessor.fit_transform(test_case, test_y)\n\n\n# # ----------------------------------------------------------------------------------------------------------------\n# import time\n\n# time_elapsed = {}\n\n# for t in range(20000, 40000, 10000):\n#     begin_time = time.time()\n#     test_case = X_train_full.iloc[0 : t]\n#     transformed_test_case = pd.DataFrame(preprocessor.fit_transform(test_case), index = test_case.index)\n#     end_time = time.time()\n#     time_elapsed[t] = end_time - begin_time\n    \n# import seaborn as sns\n# import matplotlib.pyplot as plt\n\n# keys = list(time_elapsed.keys())\n\n# values = list(time_elapsed.values())\n\n# plt.figure(figsize = (10, 10))\n# plt.title(\"{} HOURS for N=5 Nearest Neighbours\".format(round(37*values[-1]/60,2)))\n# bar = sns.barplot(x = keys, y = values)\n# for p in range(len(values)):\n#     bar.text(p, values[p], str(round(values[p], 2)),\n#             horizontalalignment = 'center')\n# # ----------------------------------------------------------------------------------------------------------------\n\n# # test = pd.DataFrame(preprocessor.fit_transform(test_case, test_y)\n# # pipeline.fit(X_train_full, y)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T12:18:25.041413Z","iopub.status.idle":"2021-07-02T12:18:25.041822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_preds = pipeline.predict_proba(X_test)[:,1]\n# submission = pd.concat([pd.Series(test_preds, index = X_test['SK_ID_CURR'])], axis = 1)\n# submission.columns = ['TARGET']\n# submission.index.names = ['SK_ID_CURR']\n# submission.to_csv('submission.csv', index=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T12:18:25.042658Z","iopub.status.idle":"2021-07-02T12:18:25.043045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}