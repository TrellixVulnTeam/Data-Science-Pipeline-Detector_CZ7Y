{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 简介\n在本部分，我们探索为Home Credit Default Risk竞赛题目手工创建特征。在前一个notebook中，我们仅使用贷款申请数据来构建模型。我们从这些数据中获得的最佳模型在排行榜上获得了大约0.74的分数。为了得到更高的分数，我们需要利用其他数据表中的信息。这里，我们会使用表 bureau 和 bureau_balance,这两个表达含义如下：\n* bureau: 客户在以前在其他金融机构的贷款记录\n* bureau_balance: 客户以前在其他经融机构的贷款的月度数据记录\n\n手动特征工程是一个繁琐的过程，通常依赖于领域专业知识。由于我对信贷领域的专业知识有限，我将加入尽可能多的信息到最终的训练表中。我们的想法是，模型将会知道哪些特征重要，哪些不重要，而不需要我们自己来做决定。基本上，我们的方法是尽可能多地创建特征，然后将它们全部交给模型使用！之后，我们可以使用模型中的特征重要性或PCA等其他技术来执行特征降维。"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 客户之前的贷款记录计数\n为了说明手工特征工程的一般过程，我们首先简单地了解一个客户以前在其他金融机构贷款的数量。"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Read in bureau\nbureau = pd.read_csv('../input/bureau.csv')\nbureau.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Groupby the client id (SK_ID_CURR), count the number of previous loans, and rename the column\nprevious_loan_counts = bureau.groupby('SK_ID_CURR', as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU': 'previous_loan_counts'})\nprevious_loan_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Join to the training dataframe\ntrain = pd.read_csv('../input/application_train.csv')\ntrain = train.merge(previous_loan_counts, on = 'SK_ID_CURR', how = 'left')\n\n# Fill the missing values with 0 \ntrain['previous_loan_counts'] = train['previous_loan_counts'].fillna(0)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 用r值评估新变量的有用性\n为了判断一个新变量是否有用，我们可以计算新变量和target的皮尔森相关系数（r值）。r值可以衡量两个变量之间的线性相关性，虽然他不是衡量一个变量是否有用的最佳指标，但是它可以给出变量是否有助于机器学习模型的第一个近似估计。r值绝对值越大，表明新变量越有可能影响target值。\n\n我们还可以使用核密度估计（KDE）图来直观地观察变量与目标的关系。"},{"metadata":{},"cell_type":"markdown","source":"## 核密度估计图\n核密度估计图显示单个变量的分布（将其视为平滑的直方图）。要变量在不同target值下的分布，我们可以根据target不同的取值对分布进行不同的着色。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plots the disribution of a variable colored by value of the target\ndef kde_target(var_name, df):\n    \n    # Calculate the correlation coefficient between the new variable and the target\n    corr = df['TARGET'].corr(df[var_name])\n    \n    # Calculate medians for repaid vs not repaid\n    avg_repaid = df.ix[df['TARGET'] == 0, var_name].median()\n    avg_not_repaid = df.ix[df['TARGET'] == 1, var_name].median()\n    \n    plt.figure(figsize = (12, 6))\n    \n    # Plot the distribution for target == 0 and target == 1\n    sns.kdeplot(df.ix[df['TARGET'] == 0, var_name], label = 'TARGET == 0')\n    sns.kdeplot(df.ix[df['TARGET'] == 1, var_name], label = 'TARGET == 1')\n    \n    # label the plot\n    plt.xlabel(var_name)\n    plt.ylabel('Density')\n    plt.title('%s Distribution' % var_name)\n    plt.legend()\n    \n    # print out the correlation\n    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n    # Print out average values\n    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid)\n    print('Median value for loan that was repaid =     %0.4f' % avg_repaid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"我们可以使用EXT_SOURCE_3变量来测试这个函数，我们在之前的notebook中发现它是最重要的变量之一。"},{"metadata":{"trusted":true},"cell_type":"code","source":"kde_target('EXT_SOURCE_3', train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kde_target('previous_loan_counts', train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"由此很难判断这个变量是否重要。 相关系数极弱，分布几乎没有明显差异。\n\n接下来我们继续从bureau表中创建一些变量，对表中的每一个数值列进行 min，max，mean运算。"},{"metadata":{},"cell_type":"markdown","source":"# 聚合数值列\n为了使用bureau表中的数值信息，我们计算表中所有数值列的统计特征。我们按客户id进行 分组，然后在分组的数据上进行 聚合，结果合并到训练表中。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group by the client id, calculate aggregation statistics\nbureau_agg = bureau.drop(columns = ['SK_ID_BUREAU']).groupby('SK_ID_CURR', as_index = False).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\nbureau_agg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"我们需要对这些列重新命名。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of column names\ncolumns = ['SK_ID_CURR']\n\n# Iterate through the variables names\nfor var in bureau_agg.columns.levels[0]:\n    # Skip the id name\n    if var != 'SK_ID_CURR':\n        \n        # Iterate through the stat names\n        for stat in bureau_agg.columns.levels[1][:-1]:\n            # Make a new column name for the variable and stat\n            columns.append('bureau_%s_%s' % (var, stat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_agg.columns = columns\nbureau_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge with the training data\ntrain = train.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 聚合值与target之间的相关性"},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of new correlations\nnew_corrs = []\n\n# Iterate through the columns \nfor col in columns:\n    # Calculate correlation with the target\n    corr = train['TARGET'].corr(train[col])\n    \n    # Append the list as a tuple\n\n    new_corrs.append((col, corr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sort the correlations by the absolute value\n# Make sure to reverse to put the largest values at the front of list\nnew_corrs = sorted(new_corrs, key = lambda x: abs(x[1]), reverse = True)\nnew_corrs[:15]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"没有新变量与TARGET有显著的相关性。 我们可以看一下最高相关性变量bureau_DAYS_CREDIT_mean的KDE图。"},{"metadata":{"trusted":true},"cell_type":"code","source":"kde_target('bureau_DAYS_CREDIT_mean', train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"此列的定义是：在其他金融机构申请上一笔贷款到在Home Credit申请贷款之间的天数。 因此，较大的负数表示贷款在当前贷款申请之前很久没申请过贷款。我们看到这个变量的平均值与target之间存在极其微弱的正相关关系，这意味着过去很久没申请贷款的客户更有可能在Home Credit偿还贷款。虽然这种相关性很弱，它也有可能是噪声。"},{"metadata":{},"cell_type":"markdown","source":"# 数值聚合函数\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def agg_numeric(df, group_var, df_name):\n    \"\"\"Aggregates the numeric values in a dataframe. This can\n    be used to create features for each instance of the grouping variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the dataframe to calculate the statistics on\n        group_var (string): \n            the variable by which to group df\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated for \n            all numeric columns. Each instance of the grouping variable will have \n            the statistics (mean, min, max, sum; currently supported) calculated. \n            The columns are also renamed to keep track of features created.\n    \n    \"\"\"\n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != group_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    group_ids = df[group_var]\n    numeric_df = df.select_dtypes('number')\n    numeric_df[group_var] = group_ids\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n\n    # Need to create new column names\n    columns = [group_var]\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        # Skip the grouping variable\n        if var != group_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1][:-1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n\n    agg.columns = columns\n    return agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_agg_new = agg_numeric(bureau.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_agg_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"为了确保函数按预期工作，我们应该与手工构建的聚合数据表进行比较。"},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_agg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 相关系数函数"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to calculate correlations with the target for a dataframe\ndef target_corrs(df):\n\n    # List of correlations\n    corrs = []\n\n    # Iterate through the columns \n    for col in df.columns:\n        print(col)\n        # Skip the target column\n        if col != 'TARGET':\n            # Calculate correlation with the target\n            corr = df['TARGET'].corr(df[col])\n\n            # Append the list as a tuple\n            corrs.append((col, corr))\n            \n    # Sort by absolute magnitude of correlations\n    corrs = sorted(corrs, key = lambda x: abs(x[1]), reverse = True)\n    \n    return corrs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 类别变量\n对于离散的字符串变量，我们不能计算统计量，例如均值和最大值，它们仅适用于数值变量。相反，我们将对每个类别变量中每个类别的值计数。例如，如果我们有以下数据表：\n\nSK_ID_CURR | Loan type |  \n-|-|-\n1 | home |\n1 | home |\n1 |\thome |\n1 | credit |\n2 | credit |\n3 | credit |\n3 | cash |\n3 |\tcash |\n4 | credit | \n4 | home |\n4 | home |\n\n每个类别贷款数量如下：\n\nSK_ID_CURR | credit count | cash count | home count | total count\n-|-|-|-|-\n1 | 1 | 0 | 3 | 4 |\n2|\t1|\t0|\t0|\t1|\n3|\t1|\t2|\t0|\t3|\n4|\t1|\t0|\t2|\t3|\n\n然后对各个类别计数进行归一化：\n\nSK_ID_CURR | credit count | cash count | home count | total count|credit count norm|cash count norm|\thome count norm\n-|-|-|-|-|-|-|\n1 | 1 | 0 | 3 | 4 |0.25|\t0|\t0.75|\n2|\t1|\t0|\t0|\t1|1.00|\t0|\t0|\n3|\t1|\t2|\t0|\t3|0.33|\t0.66|\t0|\n4|\t1|\t0|\t2|\t3|0.33|\t0|\t0.66|**"},{"metadata":{},"cell_type":"markdown","source":"首先，对数据类型为‘object'的列进行one_hot编码"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = pd.get_dummies(bureau.select_dtypes('object'))\ncategorical['SK_ID_CURR'] = bureau['SK_ID_CURR']\ncategorical.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 求和与归一化\ncategorical_grouped = categorical.groupby('SK_ID_CURR').agg(['sum', 'mean'])\ncategorical_grouped.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"对列进行重命名"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_grouped.columns.levels[0][:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_grouped.columns.levels[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group_var = 'SK_ID_CURR'\n\n# Need to create new column names\ncolumns = []\n\n# Iterate through the variables names\nfor var in categorical_grouped.columns.levels[0]:\n    # Skip the grouping variable\n    if var != group_var:\n        # Iterate through the stat names\n        for stat in ['count', 'count_norm']:\n            # Make a new column name for the variable and stat\n            columns.append('%s_%s' % (var, stat))\n\n#  Rename the columns\ncategorical_grouped.columns = columns\n\ncategorical_grouped.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"sum列记录计数，而mean列记录归一化后的计数。\n\n我们可以将此数据框合并到训练数据中。"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.merge(categorical_grouped, left_on = 'SK_ID_CURR', right_index = True, how = 'left')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.iloc[:5, 123:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 处理类别变量的函数\n把类别变量的处理封装成函数"},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_categorical(df, group_var, df_name):\n    \"\"\"Computes counts and normalized counts for each observation\n    of `group_var` of each unique category in every categorical variable\n    \n    Parameters\n    --------\n    df : dataframe \n        The dataframe to calculate the value counts for.\n        \n    group_var : string\n        The variable by which to group the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    df_name : string\n        Variable added to the front of column names to keep track of columns\n\n    \n    Return\n    --------\n    categorical : dataframe\n        A dataframe with counts and normalized counts of each unique category in every categorical variable\n        with one row for every unique value of the `group_var`.\n        \n    \"\"\"\n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('object'))\n\n    # Make sure to put the identifying id on the column\n    categorical[group_var] = df[group_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['count', 'count_norm']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    return categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_counts = count_categorical(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_counts.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 处理其他数据表\nbureau balance 包含每个客户以前在其他金融机构的贷款的月度信息。我们将首先按SK_ID_BUREAU（即先前贷款的ID）对数据帧进行分组，而不是按客户的ID(ID SK_ID_CURR)分组。这将为每笔贷款提供一行数据。 然后，我们可以按SK_ID_CURR进行分组，并计算每个客户贷款的汇总。最终结果将是每个客户一行数据，并计算其贷款的统计数据。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in bureau balance\nbureau_balance = pd.read_csv('../input/bureau_balance.csv')\nbureau_balance.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Counts of each type of status for each previous loan\nbureau_balance_counts = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 处理数值型变量\nbureau_balance_agg = agg_numeric(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_agg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"现在我们需要为每个客户聚合贷款信息。首先将数据表合并在一起，由于所有变量都是数字，我们只需要再次聚合统计数据，按SK_ID_CURR进行分组。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataframe grouped by the loan\nbureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n\n# Merge to include the SK_ID_CURR\nbureau_by_loan = bureau_by_loan.merge(bureau[['SK_ID_BUREAU', 'SK_ID_CURR']], on = 'SK_ID_BUREAU', how = 'left')\n\nbureau_by_loan.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client')\nbureau_balance_by_client.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* client_bureau_balance_MONTHS_BALANCE_mean_mean：对于每笔贷款，计算MONTHS_BALANCE的平均值。 然后为每个客户计算所有贷款的该值的平均值。\n* client_bureau_balance_STATUS_X_count_norm_sum：对于每笔贷款，计算STATUS == X的出现次数除以贷款的总STATUS值的数量。 然后，对每个客户贷款的值求和。"},{"metadata":{},"cell_type":"markdown","source":"# 重新用函数执行\n 让我们重置所有变量，然后使用我们构建的函数从头开始执行此操作。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Free up memory by deleting old objects\nimport gc\ngc.enable()\ndel train, bureau, bureau_balance, bureau_agg, bureau_agg_new, bureau_balance_agg, bureau_balance_counts, bureau_by_loan, bureau_balance_by_client, bureau_counts\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in new copies of all the dataframes\ntrain = pd.read_csv('../input/application_train.csv')\nbureau = pd.read_csv('../input/bureau.csv')\nbureau_balance = pd.read_csv('../input/bureau_balance.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Counts of Bureau Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_counts = count_categorical(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_counts.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aggregated Stats of Bureau Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_agg = agg_numeric(bureau.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_agg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Value counts of Bureau Balance dataframe by loan"},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_balance_counts = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_counts.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aggregated stats of Bureau Balance dataframe by loan"},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_balance_agg = agg_numeric(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_agg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aggregated Stats of Bureau Balance by Client"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataframe grouped by the loan\nbureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n\n# Merge to include the SK_ID_CURR\nbureau_by_loan = bureau[['SK_ID_BUREAU', 'SK_ID_CURR']].merge(bureau_by_loan, on = 'SK_ID_BUREAU', how = 'left')\n\n# Aggregate the stats for each client\nbureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 将计算出的特征插入训练数据表中"},{"metadata":{"trusted":true},"cell_type":"code","source":"original_features = list(train.columns)\nprint('Original Number of Features: ', len(original_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge with the value counts of bureau\ntrain = train.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the stats of bureau\ntrain = train.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the monthly information grouped by client\ntrain = train.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_features = list(train.columns)\nprint('Number of features using previous loans from other institutions data: ', len(new_features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 特征工程成果\n完成所有这些工作之后，现在我们想看看我们创建的变量。我们可以看一下缺失值的百分比，变量与目标的相关性，以及变量与其他变量的相关性。变量之间的相关性可以显示我们是否具有共线变量，即彼此高度相关的变量。通常，我们希望删除一对共线变量中的一个。我们还可以使用缺失值的百分比来删除缺失值过多的变量。\n\n特征选择将是一个重要的焦点，因为减少特征的数量可以帮助模型在训练期间更好地学习，并且更好地泛化到测试数据。“维度的诅咒”是由于具有太多特征（维度太高）而导致的问题。随着变量数量的增加，学习这些变量与目标值之间关系所需的数据点数量呈指数增长。"},{"metadata":{},"cell_type":"markdown","source":"### 缺失值\n删除具有太多缺失值的列。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_train = missing_values_table(train)\nmissing_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"我们看到有许多列具有较高的缺失值百分比。没有明确的阈值来删除缺失值，最佳的行动方案取决于具体问题。在这里，为了减少特征的数量，我们将删除训练或测试数据中缺失值超过90％的列。"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_train_vars = list(missing_train.index[missing_train['% of Total Values'] > 90])\nlen(missing_train_vars)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"在删除缺失值之前，我们将在测试数据中找到缺失的值百分比。 然后，我们将删除训练或测试数据中缺失值超过90％的任何列。 现在让我们读入测试数据，执行相同的操作，并查看测试数据中的缺失值。 我们已经计算了所有计数和聚合统计数据，因此我们只需要将测试数据与适当的数据合并。"},{"metadata":{},"cell_type":"markdown","source":"# 处理测试数据"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in the test dataframe\ntest = pd.read_csv('../input/application_test.csv')\n\n# Merge with the value counts of bureau\ntest = test.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the stats of bureau\ntest = test.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the value counts of bureau balance\ntest = test.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of Testing Data: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = train['TARGET']\n\n# Align the dataframes, this will remove the 'TARGET' column\ntrain, test = train.align(test, join = 'inner', axis = 1)\n\ntrain['TARGET'] = train_labels\n\nprint('Training Data Shape: ', train.shape)\nprint('Testing Data Shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_test = missing_values_table(test)\nmissing_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_test_vars = list(missing_test.index[missing_test['% of Total Values'] > 90])\nlen(missing_test_vars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_columns = list(set(missing_test_vars + missing_train_vars))\nprint('There are %d columns with more than 90%% missing in either the training or testing data.' % len(missing_columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the missing columns\ntrain = train.drop(columns = missing_columns)\ntest = test.drop(columns = missing_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"我们最终在此回合中没有删除任何列，因为没有列缺失值超过90％的列。 我们可能必须应用另一种特征选择方法来减少维度。"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.to_csv('train_bureau_raw.csv', index = False)\ntest.to_csv('test_bureau_raw.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 相关性\n首先让我们看一下变量与目标的相关性。可以看到，我们创建的任何变量都具有比最初的训练数据中已存在的变量更大的相关性。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate all correlations in dataframe\ncorrs = train.corr()\n\ncorrs = corrs.sort_values('TARGET', ascending = False)\n\n# Ten most positive correlations\npd.DataFrame(corrs['TARGET'].head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ten most negative correlations\npd.DataFrame(corrs['TARGET'].dropna().tail(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"与目标的最高相关变量是我们创建的变量。 然而，仅仅因为变量是相关的并不意味着它将是有用的，我们必须记住，如果我们生成数百个新变量，一些变量将与目标相关，仅仅是因为随机噪声。\n\n看起来确实有几个新创建的变量可能有用。 为了评估变量的“有用性”，我们将查看模型返回的特征重要性。我们可以创建两个新创建的变量的kde图看看。"},{"metadata":{"trusted":true},"cell_type":"code","source":"kde_target(var_name='bureau_DAYS_CREDIT_mean', df=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kde_target(var_name='bureau_CREDIT_ACTIVE_Active_count_norm', df=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 共线变量（Collinear Variables）"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the threshold\nthreshold = 0.8\n\n# Empty dictionary to hold correlated variables\nabove_threshold_vars = {}\n\n# For each column, record the variables that are above the threshold\nfor col in corrs:\n    above_threshold_vars[col] = list(corrs.index[corrs[col] > threshold])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"对于这些高度相关变量对中的每一对，我们只想删除其中一个变量。 以下代码通过仅添加每对中的一个来创建要删除的变量集。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Track columns to remove and columns already examined\ncols_to_remove = []\ncols_seen = []\ncols_to_remove_pair = []\n\n# Iterate through columns and correlated columns\nfor key, value in above_threshold_vars.items():\n    # Keep track of columns already examined\n    cols_seen.append(key)\n    for x in value:\n        if x == key:\n            next\n        else:\n            # Only want to remove one in a pair\n            if x not in cols_seen:\n                cols_to_remove.append(x)\n                cols_to_remove_pair.append(key)\n            \ncols_to_remove = list(set(cols_to_remove))\nprint('Number of columns to remove: ', len(cols_to_remove))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"我们可以从训练和测试数据集中删除这些列，然后对删除这些变量后将性能与保持这些变量（我们之前保存的原始csv文件）的性能进行比较。"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_corrs_removed = train.drop(columns = cols_to_remove)\ntest_corrs_removed = test.drop(columns = cols_to_remove)\n\nprint('Training Corrs Removed Shape: ', train_corrs_removed.shape)\nprint('Testing Corrs Removed Shape: ', test_corrs_removed.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_corrs_removed.to_csv('train_bureau_corrs_removed.csv', index = False)\ntest_corrs_removed.to_csv('test_bureau_corrs_removed.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 模型"},{"metadata":{},"cell_type":"markdown","source":"* control: only the data in the **application** files.\n* test one: the data in the application files with all of the data recorded from the **bureau** and **bureau_balance** files\n* test two: the data in the application files with all of the data recorded from the **bureau** and **bureau_balance** files with highly correlated variables removed."},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\nimport gc\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Control\n任何实验的第一步是建立一个控制变量。 为此，我们将使用上面定义的函数（定义模型）和单个主数据源（application表）。"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_control = pd.read_csv('../input/application_train.csv')\ntest_control = pd.read_csv('../input/application_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission, fi, metrics = model(train_control, test_control)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"出现了过拟合现象，因为训练分数比验证分数高很多。 我们可以在后面的笔记本中解决这个问题（我们已经通过使用reg_lambda和reg_alpha以及提前停止在此模型中采取了一些正则化措施）。"},{"metadata":{"trusted":true},"cell_type":"code","source":"fi_sorted = plot_feature_importances(fi)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('control.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The control scores **0.745** when submitted to the competition."},{"metadata":{},"cell_type":"markdown","source":"### Test One"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_raw, fi_raw, metrics_raw = model(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_raw","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi_raw_sorted = plot_feature_importances(fi_raw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"看起来我们构建的一些特征在最重要的特征中。 让我们找一下制作的前100个最重要特征的百分比。但是，我们需要与独热编码的原始特征进行比较，而不是仅仅与原始特征进行比较。这些已经记录在fi中（来自原始数据）。"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_100 = list(fi_raw_sorted['feature'])[:100]\nnew_features = [x for x in top_100 if x not in list(fi['feature'])]\n\nprint('%% of Top 100 Features created from the bureau data = %d.00' % len(new_features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"重要性前100的特征中超过一半是由我们制作的！这应该让我们相信，我们所做的所有努力都是值得的。"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_raw.to_csv('test_one.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Test one scores 0.759 when submitted to the competition.**"},{"metadata":{},"cell_type":"markdown","source":"### Test Two"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_corrs, fi_corrs, metrics_corr = model(train_corrs_removed, test_corrs_removed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_corr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These results are better than the control, but slightly lower than the raw features."},{"metadata":{"trusted":true},"cell_type":"code","source":"fi_corrs_sorted = plot_feature_importances(fi_corrs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_corrs.to_csv('test_two.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Test Two scores 0.753 when submitted to the competition.**"},{"metadata":{},"cell_type":"markdown","source":"# 结果\n完成所有这些工作之后，我们可以说包含额外信息确实提高了性能！让我们正式总结一下：\n\nExperiment| Train AUC|\tValidation AUC|\tTest AUC\n-|-|-|-|\nControl|\t0.815|\t0.760|\t0.745|\nTest One|\t0.837|\t0.767|\t0.759|\nTest Two|\t0.826|\t0.765|\t0.753|"},{"metadata":{},"cell_type":"markdown","source":"我们所有的努力工作都转化为比原始测试数据高0.014 ROC AUC的改进。 删除高共线变量会略微降低性能，因此我们需要考虑不同的特征选择方法。 此外，我们可以说，我们构建的一些特征是模型判断最重要的特征之一。"},{"metadata":{},"cell_type":"markdown","source":"# 下一步\n我们现在可以在其他数据集中使用我们在此笔记本中开发的特征。我们的模型中还有4个其他数据文件可供使用！在下一个笔记本中，我们将把这些数据文件（包含Home Credit的先前贷款信息）中的信息合并到我们的训练数据中。然后我们可以构建相同的模型并运行更多实验来确定我们的特征工程的效果。"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}