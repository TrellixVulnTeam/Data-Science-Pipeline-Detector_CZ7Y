{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"_uuid":"16dc7fa2-6e21-43fe-9ff8-82af8d06ca2d","_cell_guid":"3f0aaacd-8c17-438d-b262-eaa9b1a6407f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score","metadata":{"_uuid":"59cf922c-ac70-48a7-b1ae-d2c8120954fa","_cell_guid":"9062e9d9-77f6-4334-9dc1-b38c7d4d6497","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-22T11:56:11.22729Z","iopub.execute_input":"2021-08-22T11:56:11.227665Z","iopub.status.idle":"2021-08-22T11:56:11.232876Z","shell.execute_reply.started":"2021-08-22T11:56:11.227633Z","shell.execute_reply":"2021-08-22T11:56:11.232049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load data\nstart_time = time.time()\n#input_dir = os.path.join(os.pardir, 'input/home-credit-default-risk')\ninput_dir = os.path.join(os.pardir, 'input/home-credit-loan-better-data-processing')\n#../input/home-credit-loan-better-data-processing\nprint('Loading data...')\nrows_read = None\napp_train_df = pd.read_csv(os.path.join(input_dir, 'train.csv'), nrows=rows_read)\napp_test_df = pd.read_csv(os.path.join(input_dir, 'test.csv'), nrows=rows_read)\napp_labels_df = pd.read_csv(os.path.join(input_dir, 'labels.csv'), nrows=rows_read)\nprint('Time elapsed %.0f sec'%(time.time()-start_time))\nprint('Pre-processing data...')","metadata":{"_uuid":"daec9e33-ef73-4706-a3c8-eebfc0ade534","_cell_guid":"c1f2c200-4b92-43fb-92e5-f44195f3c973","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-22T11:56:11.234269Z","iopub.execute_input":"2021-08-22T11:56:11.234793Z","iopub.status.idle":"2021-08-22T11:56:44.772604Z","shell.execute_reply.started":"2021-08-22T11:56:11.234761Z","shell.execute_reply":"2021-08-22T11:56:44.771781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge the datasets into a single one\ntarget = app_labels_df.pop('TARGET')\nlen_train = len(app_labels_df)\nmerged_df = pd.concat([app_train_df, app_test_df])\nmeta_df = merged_df.pop('SK_ID_CURR')\ndel app_test_df, app_train_df\ngc.collect()","metadata":{"_uuid":"072c0e04-200a-4fb2-9ca9-d13e9868b5ad","_cell_guid":"1d9830fb-05cc-411f-be18-4dd3b689200b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-22T11:56:44.774052Z","iopub.execute_input":"2021-08-22T11:56:44.774378Z","iopub.status.idle":"2021-08-22T11:56:46.219834Z","shell.execute_reply.started":"2021-08-22T11:56:44.774348Z","shell.execute_reply":"2021-08-22T11:56:46.218753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encode categoricals: 1-hot\ncategorical_feats = merged_df.columns[merged_df.dtypes == 'object']\nprint('Using %d prediction variables'%(merged_df.shape[1]))\nprint('Encoding %d non-numeric columns...'%(merged_df.columns[merged_df.dtypes == 'object'].shape))\nfor feat in categorical_feats:\n    merged_df[feat].fillna('MISSING', inplace=True) # populate missing labels\n    encoder = LabelBinarizer() # works with text\n    new_columns = encoder.fit_transform(merged_df[feat])\n    i=0\n    for u in merged_df[feat].unique():\n        if i<new_columns.shape[1]:\n            merged_df[feat+'_'+u]=new_columns[:,i]\n            i+=1\n    merged_df.drop(feat, axis=1, inplace=True)\nprint('Now using %d prediction variables'%(merged_df.shape[1]))\nprint('Time elapsed %.0f sec'%(time.time()-start_time))","metadata":{"_uuid":"e0c362c4-d546-4ebe-94a5-84f96fa9229d","_cell_guid":"d5fcd2f5-ad67-45b7-9886-55ee22dd724e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-22T11:56:46.221377Z","iopub.execute_input":"2021-08-22T11:56:46.221665Z","iopub.status.idle":"2021-08-22T11:56:46.234322Z","shell.execute_reply.started":"2021-08-22T11:56:46.221637Z","shell.execute_reply":"2021-08-22T11:56:46.233032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# handle missing values\nnull_counts = merged_df.isnull().sum()\nnull_counts = null_counts[null_counts > 0]\nnull_ratios = null_counts / len(merged_df)","metadata":{"_uuid":"f2c2ae34-7e9e-4fe0-b5c1-42b59ca49220","_cell_guid":"9542e095-dcf6-4620-a9cf-8adec9151298","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-22T11:56:46.235693Z","iopub.execute_input":"2021-08-22T11:56:46.236002Z","iopub.status.idle":"2021-08-22T11:56:46.690995Z","shell.execute_reply.started":"2021-08-22T11:56:46.235972Z","shell.execute_reply":"2021-08-22T11:56:46.69003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop columns over x% null\nnull_thresh = .8\nnull_cols = null_ratios[null_ratios > null_thresh].index\nmerged_df.drop(null_cols, axis=1, inplace=True)\nif null_cols.shape[0] > 0:\n    print('Columns dropped for being over %.2f null:'%(null_thresh))\n    for col in null_cols:\n        print(col)","metadata":{"_uuid":"eee8b987-5902-40cd-8c4b-df2698bd2895","_cell_guid":"c59c03a7-0033-4b6c-bc45-65114ed98478","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-22T11:56:46.692444Z","iopub.execute_input":"2021-08-22T11:56:46.692741Z","iopub.status.idle":"2021-08-22T11:56:48.60135Z","shell.execute_reply.started":"2021-08-22T11:56:46.692713Z","shell.execute_reply":"2021-08-22T11:56:48.600343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill the rest with 0\nmerged_df.fillna(0, inplace=True)","metadata":{"_uuid":"ad2082cf-d1c1-41af-89d7-d1d60349069b","_cell_guid":"91e58316-20eb-420b-ab5a-d091036f3a83","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-22T11:56:48.60266Z","iopub.execute_input":"2021-08-22T11:56:48.602953Z","iopub.status.idle":"2021-08-22T11:56:49.362962Z","shell.execute_reply.started":"2021-08-22T11:56:48.602924Z","shell.execute_reply":"2021-08-22T11:56:49.361964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scale continuous features\n# first, convert large ingegers into floats.\nfor feat in merged_df.columns:\n    if (merged_df[feat].max() > 100) | (merged_df[feat].min() < -100):\n        merged_df[feat]=merged_df[feat].astype(np.float64)\nscaler = StandardScaler()\ncontinuous_feats = merged_df.columns[merged_df.dtypes == 'float64']\nprint('Scaling %d features...'%(continuous_feats.shape))\ns1 = merged_df.shape[0],1\nfor feat in continuous_feats:\n    merged_df[feat] = scaler.fit_transform(merged_df[feat].values.reshape(s1))","metadata":{"_uuid":"a015de67-8635-421b-a9ed-0c68e1229cf9","_cell_guid":"155a9f92-e3ac-49b2-93f5-3442352cfd06","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-22T11:56:49.365392Z","iopub.execute_input":"2021-08-22T11:56:49.365711Z","iopub.status.idle":"2021-08-22T11:56:54.147306Z","shell.execute_reply.started":"2021-08-22T11:56:49.36568Z","shell.execute_reply":"2021-08-22T11:56:54.146235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Re-separate into train and test\ntrain_df = merged_df[:len_train]\ntest_df = merged_df[len_train:]\ndel merged_df\ngc.collect()\n\nprint('Time elapsed %.0f sec'%(time.time()-start_time))\nprint('Starting training...')","metadata":{"_uuid":"a6e952c3-7b26-4e07-b4ab-c92d2cf687db","_cell_guid":"727ffdce-922b-4bc3-af4e-bf7be0e57244","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-22T11:56:54.148993Z","iopub.execute_input":"2021-08-22T11:56:54.149325Z","iopub.status.idle":"2021-08-22T11:56:54.320484Z","shell.execute_reply.started":"2021-08-22T11:56:54.149286Z","shell.execute_reply":"2021-08-22T11:56:54.319313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define train parameters\nL2c = 4e-4                    # loss, with L2\nlr0 = 0.02                    # starting learning rate\nlr_decay = 0.90               # lr decay rate\niterations = 41               # full passes over data\nROWS = train_df.shape[0]      # rows in input data\nVARS = train_df.shape[1]      # vars used in the model\nNUMB = 10000                  # batch size\nNN = int(ROWS/NUMB)           # number of batches","metadata":{"_uuid":"13b5854f-4e7a-40af-b513-4e104ffde2a1","_cell_guid":"9faaac43-065b-4fdf-85ac-d2ecaffdb012","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-22T11:56:54.322398Z","iopub.execute_input":"2021-08-22T11:56:54.322816Z","iopub.status.idle":"2021-08-22T11:56:54.339063Z","shell.execute_reply.started":"2021-08-22T11:56:54.322773Z","shell.execute_reply":"2021-08-22T11:56:54.33806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the model\nimport tensorflow as tf\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\ny_ = tf.placeholder(tf.float32, [None, 1])\nx  = tf.placeholder(tf.float32, [None, VARS])\n\n# model: logistic + 1 hidden layer\nW      = tf.Variable(tf.truncated_normal([VARS,1],mean=0.0,stddev=0.001),dtype=np.float32)\nNUML1  = 64\nW1     = tf.Variable(tf.truncated_normal([VARS,NUML1],mean=0.0,stddev=0.0001),dtype=np.float32)\nW1f    = tf.Variable(tf.truncated_normal([NUML1,1],mean=0.0,stddev=0.0001),dtype=np.float32)\nlogit1 = tf.matmul( x, W ) + tf.matmul(tf.nn.relu(tf.matmul( x, W1 )), W1f)\ny      = tf.nn.sigmoid( logit1 )","metadata":{"_uuid":"34aba53d-5195-4ac3-9ef9-116731469be2","_cell_guid":"d237fe03-f88c-4abe-9cae-1da7bf787eb8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-22T11:56:54.340333Z","iopub.execute_input":"2021-08-22T11:56:54.340627Z","iopub.status.idle":"2021-08-22T11:56:54.371788Z","shell.execute_reply.started":"2021-08-22T11:56:54.340598Z","shell.execute_reply":"2021-08-22T11:56:54.370732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss/optimizer\nloss0 = tf.reduce_mean( (y_-y)*(y_-y) )\nloss1 = L2c * (tf.nn.l2_loss( W ) + tf.nn.l2_loss( W1 ) + tf.nn.l2_loss( W1f ))\nloss  = loss0 + loss1\nglobal_step = tf.Variable(0, trainable=False)\nlearning_rate = tf.train.exponential_decay(lr0, global_step, NN, lr_decay)\ntrain_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss,global_step=global_step)\nsess = tf.InteractiveSession()\ntf.global_variables_initializer().run()","metadata":{"_uuid":"a915ef79-bc72-4b5c-ab31-01cd15246334","_cell_guid":"bb5fe6fc-111a-4061-b61c-62cbc38808b9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-22T11:56:54.373254Z","iopub.execute_input":"2021-08-22T11:56:54.373654Z","iopub.status.idle":"2021-08-22T11:56:54.506473Z","shell.execute_reply.started":"2021-08-22T11:56:54.373613Z","shell.execute_reply":"2021-08-22T11:56:54.505422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# main training loop\ny0=target.values.astype(np.float32)\nx0=train_df.values.astype(np.float32)\ndel train_df\ngc.collect()\ny0_1=np.where(y0[0:int(NN*0.8)*NUMB] == 1)[0] # reserve last 20% for testing\ny0_0=np.where(y0[0:int(NN*0.8)*NUMB] == 0)[0] # reserve last 20% for testing\nfor i in range(iterations):\n    for j in range(int(NN*0.8)): # reserve last 20% for testing\n        pos_ratio = 0.5\n        pos_idx = np.random.choice(y0_1, size=int(np.round(NUMB*pos_ratio)))\n        neg_idx = np.random.choice(y0_0, size=int(np.round(NUMB*(1-pos_ratio))))\n        idx = np.concatenate([pos_idx, neg_idx])\n        fd = {y_: y0[idx].reshape(NUMB,1),x:  x0[idx,:]}\n        _= sess.run( [train_step], feed_dict=fd )\n    if i%10 == 0:\n        # get area under the ROC curve\n        fd   = {y_: y0.reshape(y0.shape[0],1),x: x0}\n        y1   = sess.run( y, feed_dict=fd )\n        lim  = int(NN*0.8) * NUMB\n        auc1 = roc_auc_score(y0[0:lim],y1[0:lim,0])\n        #auc2 = roc_auc_score(y0[lim:y0.shape[0]],y1[lim:y0.shape[0],0])\n        print('iteration %d, auc train/validatet %.5f'%(i,auc1))","metadata":{"_uuid":"07fbffd1-24f0-4fe2-9d22-ec60a5c47bf7","_cell_guid":"1d3c168f-e9d1-4a48-b1ea-23d0aaa1b4c6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-22T11:56:54.507708Z","iopub.execute_input":"2021-08-22T11:56:54.507984Z","iopub.status.idle":"2021-08-22T11:59:03.516525Z","shell.execute_reply.started":"2021-08-22T11:56:54.507958Z","shell.execute_reply":"2021-08-22T11:59:03.515317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predict on test set and create submission\nx0     = test_df.values.astype(np.float32)\nfd     = {y_: np.zeros([x0.shape[0],1]),x: x0}\ny_pred = sess.run( y, feed_dict=fd )\nout_df = pd.DataFrame({'SK_ID_CURR': meta_df[len_train:], 'TARGET': y_pred[:,0]})\nout_df.to_csv('submission.csv', index=False)\nprint('Time elapsed %.0f sec'%(time.time()-start_time))","metadata":{"_uuid":"11e69e7f-aa41-4b38-840d-46dbb59b132d","_cell_guid":"c2506347-d121-4d21-88b3-da78261658e7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-22T11:59:03.51792Z","iopub.execute_input":"2021-08-22T11:59:03.518225Z","iopub.status.idle":"2021-08-22T11:59:03.969282Z","shell.execute_reply.started":"2021-08-22T11:59:03.518177Z","shell.execute_reply":"2021-08-22T11:59:03.968294Z"},"trusted":true},"execution_count":null,"outputs":[]}]}