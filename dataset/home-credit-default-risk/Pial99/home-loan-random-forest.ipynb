{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"# numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd \n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:21:42.992284Z","iopub.execute_input":"2021-05-22T21:21:42.992668Z","iopub.status.idle":"2021-05-22T21:21:42.99794Z","shell.execute_reply.started":"2021-05-22T21:21:42.992636Z","shell.execute_reply":"2021-05-22T21:21:42.997027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List files available\nDATA_DIRECTORY = \"../input/home-credit-default-risk\"\nprint(os.listdir(\"../input/\"))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T20:46:01.672132Z","iopub.execute_input":"2021-05-22T20:46:01.672792Z","iopub.status.idle":"2021-05-22T20:46:01.677105Z","shell.execute_reply.started":"2021-05-22T20:46:01.672756Z","shell.execute_reply":"2021-05-22T20:46:01.676426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training data\napp_train = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T20:47:02.012959Z","iopub.execute_input":"2021-05-22T20:47:02.013414Z","iopub.status.idle":"2021-05-22T20:47:07.694579Z","shell.execute_reply.started":"2021-05-22T20:47:02.01337Z","shell.execute_reply":"2021-05-22T20:47:07.693844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing data features\napp_test = pd.read_csv('../input/home-credit-default-risk/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T20:49:25.261424Z","iopub.execute_input":"2021-05-22T20:49:25.262031Z","iopub.status.idle":"2021-05-22T20:49:26.199027Z","shell.execute_reply.started":"2021-05-22T20:49:25.261995Z","shell.execute_reply":"2021-05-22T20:49:26.198147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train['TARGET'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T20:50:36.026765Z","iopub.execute_input":"2021-05-22T20:50:36.027178Z","iopub.status.idle":"2021-05-22T20:50:36.041381Z","shell.execute_reply.started":"2021-05-22T20:50:36.027145Z","shell.execute_reply":"2021-05-22T20:50:36.040096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train['TARGET'].astype(int).plot.hist();","metadata":{"execution":{"iopub.status.busy":"2021-05-22T20:50:55.184748Z","iopub.execute_input":"2021-05-22T20:50:55.185129Z","iopub.status.idle":"2021-05-22T20:50:55.423043Z","shell.execute_reply.started":"2021-05-22T20:50:55.185094Z","shell.execute_reply":"2021-05-22T20:50:55.422022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","metadata":{"execution":{"iopub.status.busy":"2021-05-22T20:51:57.132844Z","iopub.execute_input":"2021-05-22T20:51:57.133454Z","iopub.status.idle":"2021-05-22T20:51:57.140148Z","shell.execute_reply.started":"2021-05-22T20:51:57.133405Z","shell.execute_reply":"2021-05-22T20:51:57.139344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_values_table(app_train)\nmissing_values.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T20:52:40.177958Z","iopub.execute_input":"2021-05-22T20:52:40.178489Z","iopub.status.idle":"2021-05-22T20:52:41.116912Z","shell.execute_reply.started":"2021-05-22T20:52:40.178458Z","shell.execute_reply":"2021-05-22T20:52:41.115692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of each type of column\napp_train.dtypes.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T20:53:21.235926Z","iopub.execute_input":"2021-05-22T20:53:21.236271Z","iopub.status.idle":"2021-05-22T20:53:21.244286Z","shell.execute_reply.started":"2021-05-22T20:53:21.236242Z","shell.execute_reply":"2021-05-22T20:53:21.243161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T20:53:42.707339Z","iopub.execute_input":"2021-05-22T20:53:42.707655Z","iopub.status.idle":"2021-05-22T20:53:43.47475Z","shell.execute_reply.started":"2021-05-22T20:53:42.707628Z","shell.execute_reply":"2021-05-22T20:53:43.47395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T20:55:53.051215Z","iopub.execute_input":"2021-05-22T20:55:53.05157Z","iopub.status.idle":"2021-05-22T20:55:53.326781Z","shell.execute_reply.started":"2021-05-22T20:55:53.05153Z","shell.execute_reply":"2021-05-22T20:55:53.32601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(app_train[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T20:59:01.774431Z","iopub.execute_input":"2021-05-22T20:59:01.774756Z","iopub.status.idle":"2021-05-22T20:59:02.540808Z","shell.execute_reply.started":"2021-05-22T20:59:01.774731Z","shell.execute_reply":"2021-05-22T20:59:02.539866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:00:25.015815Z","iopub.execute_input":"2021-05-22T21:00:25.016199Z","iopub.status.idle":"2021-05-22T21:00:25.921723Z","shell.execute_reply.started":"2021-05-22T21:00:25.016167Z","shell.execute_reply":"2021-05-22T21:00:25.920852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n\n# Add the target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:00:51.918703Z","iopub.execute_input":"2021-05-22T21:00:51.919222Z","iopub.status.idle":"2021-05-22T21:00:52.292702Z","shell.execute_reply.started":"2021-05-22T21:00:51.91919Z","shell.execute_reply":"2021-05-22T21:00:52.29168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(app_train['DAYS_BIRTH'] / -365).describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:01:44.168972Z","iopub.execute_input":"2021-05-22T21:01:44.169293Z","iopub.status.idle":"2021-05-22T21:01:44.192539Z","shell.execute_reply.started":"2021-05-22T21:01:44.169267Z","shell.execute_reply":"2021-05-22T21:01:44.191555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:02:39.942042Z","iopub.execute_input":"2021-05-22T21:02:39.942387Z","iopub.status.idle":"2021-05-22T21:02:39.960962Z","shell.execute_reply.started":"2021-05-22T21:02:39.942359Z","shell.execute_reply":"2021-05-22T21:02:39.960143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:02:57.4043Z","iopub.execute_input":"2021-05-22T21:02:57.404763Z","iopub.status.idle":"2021-05-22T21:02:57.593812Z","shell.execute_reply.started":"2021-05-22T21:02:57.404734Z","shell.execute_reply":"2021-05-22T21:02:57.593132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:03:27.710054Z","iopub.execute_input":"2021-05-22T21:03:27.710582Z","iopub.status.idle":"2021-05-22T21:03:28.094622Z","shell.execute_reply.started":"2021-05-22T21:03:27.710549Z","shell.execute_reply":"2021-05-22T21:03:28.093955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:03:54.325157Z","iopub.execute_input":"2021-05-22T21:03:54.325446Z","iopub.status.idle":"2021-05-22T21:03:54.562863Z","shell.execute_reply.started":"2021-05-22T21:03:54.325421Z","shell.execute_reply":"2021-05-22T21:03:54.56197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:04:34.671011Z","iopub.execute_input":"2021-05-22T21:04:34.671366Z","iopub.status.idle":"2021-05-22T21:04:34.687773Z","shell.execute_reply.started":"2021-05-22T21:04:34.671338Z","shell.execute_reply":"2021-05-22T21:04:34.686573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find correlations with the target and sort\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:05:07.522339Z","iopub.execute_input":"2021-05-22T21:05:07.522659Z","iopub.status.idle":"2021-05-22T21:05:54.98758Z","shell.execute_reply.started":"2021-05-22T21:05:07.522632Z","shell.execute_reply":"2021-05-22T21:05:54.986751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the correlation of the positive days since birth and target\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:06:45.05867Z","iopub.execute_input":"2021-05-22T21:06:45.059023Z","iopub.status.idle":"2021-05-22T21:06:45.074038Z","shell.execute_reply.started":"2021-05-22T21:06:45.058994Z","shell.execute_reply":"2021-05-22T21:06:45.073087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the style of plots\nplt.style.use('fivethirtyeight')\n\n# Plot the distribution of ages in years\nplt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:06:57.992745Z","iopub.execute_input":"2021-05-22T21:06:57.993075Z","iopub.status.idle":"2021-05-22T21:06:58.176462Z","shell.execute_reply.started":"2021-05-22T21:06:57.993046Z","shell.execute_reply":"2021-05-22T21:06:58.175648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10, 8))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n\n# Labeling of plot\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:07:29.368217Z","iopub.execute_input":"2021-05-22T21:07:29.368556Z","iopub.status.idle":"2021-05-22T21:07:31.051328Z","shell.execute_reply.started":"2021-05-22T21:07:29.368529Z","shell.execute_reply":"2021-05-22T21:07:31.050497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Age information into a separate dataframe\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:07:55.759405Z","iopub.execute_input":"2021-05-22T21:07:55.759721Z","iopub.status.idle":"2021-05-22T21:07:55.907718Z","shell.execute_reply.started":"2021-05-22T21:07:55.759694Z","shell.execute_reply":"2021-05-22T21:07:55.906816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group by the bin and calculate averages\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:08:09.309531Z","iopub.execute_input":"2021-05-22T21:08:09.30992Z","iopub.status.idle":"2021-05-22T21:08:09.331487Z","shell.execute_reply.started":"2021-05-22T21:08:09.309886Z","shell.execute_reply":"2021-05-22T21:08:09.330839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:08:23.250453Z","iopub.execute_input":"2021-05-22T21:08:23.250994Z","iopub.status.idle":"2021-05-22T21:08:23.434545Z","shell.execute_reply.started":"2021-05-22T21:08:23.250931Z","shell.execute_reply":"2021-05-22T21:08:23.433611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:08:41.59859Z","iopub.execute_input":"2021-05-22T21:08:41.598997Z","iopub.status.idle":"2021-05-22T21:08:41.649265Z","shell.execute_reply.started":"2021-05-22T21:08:41.598965Z","shell.execute_reply":"2021-05-22T21:08:41.648427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (8, 6))\n\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:08:56.88931Z","iopub.execute_input":"2021-05-22T21:08:56.889641Z","iopub.status.idle":"2021-05-22T21:08:57.205323Z","shell.execute_reply.started":"2021-05-22T21:08:56.889614Z","shell.execute_reply":"2021-05-22T21:08:57.204544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10, 12))\n\n# iterate through the sources\nfor i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    \n    # create a new subplot for each source\n    plt.subplot(3, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5) ","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:09:46.9045Z","iopub.execute_input":"2021-05-22T21:09:46.904916Z","iopub.status.idle":"2021-05-22T21:09:50.911925Z","shell.execute_reply.started":"2021-05-22T21:09:46.904881Z","shell.execute_reply":"2021-05-22T21:09:50.91097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Copy the data for plotting\nplot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n\n# Add in the age of the client in years\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n\n# Drop na values and limit to first 100000 rows\nplot_data = plot_data.dropna().loc[:100000, :]\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)\n\n# Create the pairgrid object\ngrid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n                    hue = 'TARGET', \n                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.2)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:10:07.163644Z","iopub.execute_input":"2021-05-22T21:10:07.163992Z","iopub.status.idle":"2021-05-22T21:13:06.591374Z","shell.execute_reply.started":"2021-05-22T21:10:07.163963Z","shell.execute_reply":"2021-05-22T21:13:06.590266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a new dataframe for polynomial features\nimport numpy as np\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\n# imputer for handling missing values\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy = 'median')\n\npoly_target = poly_features['TARGET']\n\npoly_features = poly_features.drop(columns = ['TARGET'])\n\n# Need to impute missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n                                  \n# Create the polynomial object with specified degree\npoly_transformer = PolynomialFeatures(degree = 3)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:29:21.801095Z","iopub.execute_input":"2021-05-22T21:29:21.801626Z","iopub.status.idle":"2021-05-22T21:29:21.999186Z","shell.execute_reply.started":"2021-05-22T21:29:21.80158Z","shell.execute_reply":"2021-05-22T21:29:21.998328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns = ['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# Median imputation of missing values\nimputer = SimpleImputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:30:59.483716Z","iopub.execute_input":"2021-05-22T21:30:59.484262Z","iopub.status.idle":"2021-05-22T21:31:33.946622Z","shell.execute_reply.started":"2021-05-22T21:30:59.484213Z","shell.execute_reply":"2021-05-22T21:31:33.94564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:35:16.602548Z","iopub.execute_input":"2021-05-22T21:35:16.602917Z","iopub.status.idle":"2021-05-22T21:35:19.88073Z","shell.execute_reply.started":"2021-05-22T21:35:16.602886Z","shell.execute_reply":"2021-05-22T21:35:19.879706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions\n# Make sure to select the second column only\nlog_reg_pred = log_reg.predict_proba(test)[:, 1]","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:35:48.80983Z","iopub.execute_input":"2021-05-22T21:35:48.810443Z","iopub.status.idle":"2021-05-22T21:35:48.833079Z","shell.execute_reply.started":"2021-05-22T21:35:48.810409Z","shell.execute_reply":"2021-05-22T21:35:48.831939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:36:09.300064Z","iopub.execute_input":"2021-05-22T21:36:09.300524Z","iopub.status.idle":"2021-05-22T21:36:09.315725Z","shell.execute_reply.started":"2021-05-22T21:36:09.300486Z","shell.execute_reply":"2021-05-22T21:36:09.31426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the submission to a csv file\nsubmit.to_csv('log_reg_baseline.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:36:25.950999Z","iopub.execute_input":"2021-05-22T21:36:25.95137Z","iopub.status.idle":"2021-05-22T21:36:26.134723Z","shell.execute_reply.started":"2021-05-22T21:36:25.951339Z","shell.execute_reply":"2021-05-22T21:36:26.133861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:37:16.163453Z","iopub.execute_input":"2021-05-22T21:37:16.163886Z","iopub.status.idle":"2021-05-22T21:37:16.171566Z","shell.execute_reply.started":"2021-05-22T21:37:16.163844Z","shell.execute_reply":"2021-05-22T21:37:16.170839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train on the training data\nrandom_forest.fit(train, train_labels)\n\n# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = random_forest.predict_proba(test)[:, 1]","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:37:26.479791Z","iopub.execute_input":"2021-05-22T21:37:26.480296Z","iopub.status.idle":"2021-05-22T21:38:34.515918Z","shell.execute_reply.started":"2021-05-22T21:37:26.480265Z","shell.execute_reply":"2021-05-22T21:38:34.514968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T21:39:14.798686Z","iopub.execute_input":"2021-05-22T21:39:14.799052Z","iopub.status.idle":"2021-05-22T21:39:14.920986Z","shell.execute_reply.started":"2021-05-22T21:39:14.79902Z","shell.execute_reply":"2021-05-22T21:39:14.920183Z"},"trusted":true},"execution_count":null,"outputs":[]}]}