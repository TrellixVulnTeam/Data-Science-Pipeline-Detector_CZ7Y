{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## Introduction\nMany people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.\n\nHome Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n\nWhile Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.\n\nThis is a standard supervised classification task.\n\nSupervised: The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features\nClassification: The label is a binary variable, 0 (will repay loan on time), 1 (will have difficulty repaying loan)\n\n## Data\nThere are 7 different sources of data:\n\napplication_train/application_test: the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the feature SK_ID_CURR. The training application data comes with the TARGET indicating 0: the loan was repaid or 1: the loan was not repaid. \n\nbureau: data concerning client's previous credits from other financial institutions. Each previous credit has its own row in bureau, but one loan in the application data can have multiple previous credits.\n\nbureau_balance: monthly data about the previous credits in bureau. Each row is one month of a previous credit, and a single previous credit can have multiple rows, one for each month of the credit length. \n\nprevious_application: previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature SK_ID_PREV. \n\nPOS_CASH_BALANCE: monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows.\n\ncredit_card_balance: monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows.\n\ninstallments_payment: payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment. \n\nThis diagram shows how all of the data is related:\n![image](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)\nMoreover, we are provided with the definitions of all the columns (in HomeCredit_columns_description.csv) and an example of the expected submission file.\n\nMetric: ROC AUC\nThe idea is relatively simple: the ROC curve shows how the recall vs precision relationship changes as we vary the threshold for identifying a positive in our model. The threshold represents the value above which a data point is considered in the positive class. If we have a model for identifying a disease, our model might output a score for each patient between 0 and 1 and we can set a threshold in this range for labeling a patient as having the disease (a positive label). By altering the threshold, we can try to achieve the right precision vs recall balance.\nAn ROC curve plots the true positive rate on the y-axis versus the false positive rate on the x-axis. The true positive rate (TPR) is the recall and the false positive rate (FPR) is the probability of a false alarm. Both of these can be calculated from the confusion matrix:\n![image.png](https://cdn-images-1.medium.com/max/1000/1*6NkN_LINs2erxgVJ9rkpUA.png)\nA typical ROC curve is shown below:\n![image](http://www.statisticshowto.com/wp-content/uploads/2016/08/ROC-curve.png)\nThe black diagonal line indicates a random classifier and the red and blue curves show two different classification models. For a given model, we can only stay on one curve, but we can move along the curve by adjusting our threshold for classifying a positive case. Generally, as we decrease the threshold, we move to the right and upwards along the curve. With a threshold of 1.0, we would be in the lower left of the graph because we identify no data points as positives leading to no true positives and no false positives (TPR = FPR = 0). As we decrease the threshold, we identify more data points as positive, leading to more true positives, but also more false positives (the TPR and FPR increase). Eventually, at a threshold of 0.0 we identify all data points as positive and find ourselves in the upper right corner of the ROC curve (TPR = FPR = 1.0).\n\nFinally, we can quantify a modelâ€™s ROC curve by calculating the total Area Under the Curve (AUC), a metric which falls between 0 and 1 with a higher number indicating better classification performance. In the graph above, the AUC for the blue curve will be greater than that for the red curve, meaning the blue model is better at achieving a blend of precision and recall. A random classifier (the black line) achieves an AUC of 0.5."},{"metadata":{"trusted":true,"_uuid":"0c722b0439c5f233c02cc226a2848da42d300dce"},"cell_type":"code","source":"# numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd \n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6790cc625c257bcaa18de38bf463f821d221525e"},"cell_type":"markdown","source":"## Read in Data\nFirst, we can list all the available data files. There are a total of 9 files: 1 main file for training (with target) 1 main file for testing (without the target), 1 example submission file, and 6 other files containing additional information about each loan."},{"metadata":{"trusted":true,"_uuid":"1769a9bbf2c250076da907521ae71d0cd9b6c78c"},"cell_type":"code","source":"# List files available\nprint(os.listdir(\"../input/\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecc30cb4cb9089fa55982d5c65bb8315371ffafa"},"cell_type":"code","source":"# Training data\napp_train = pd.read_csv('../input/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c8a19303307ecbad769dc5dae568f1b8e0df265"},"cell_type":"code","source":"# Test data.\napp_test = pd.read_csv('../input/application_test.csv')\nprint('Test data shape: ', app_test.shape)\napp_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1289a13eaa74e8fe0003f162f0a2afb00a297c7f"},"cell_type":"markdown","source":"## Exploratory Data Analysi\nExploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data. The goal of EDA is to learn what our data can tell us. It generally starts out with a high level overview, then narrows in to specific areas as we find intriguing areas of the data. The findings may be interesting in their own right, or they can be used to inform our modeling choices, such as by helping us decide which features to use.\n\n## Examine the Distribution of the Target Column\nThe target is what we asked to predict: either a 0 for the loan was repaid on time, or a 1 indicating the client had payment difficulties. "},{"metadata":{"trusted":true,"_uuid":"6b87004a866419beef76ffd2bff2ad3dbb21cf43"},"cell_type":"code","source":"app_train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ced4126d5745d634d81ebdc95f8f21ed4536cdc4"},"cell_type":"code","source":"app_train['TARGET'].astype(int).plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f9287417e63430edaafff6de25afedbc1842d51"},"cell_type":"markdown","source":"We see this is an imbalanced class problem. There are far more loans that were repaid on time than loans that were not repaid. Once we get into more sophisticated machine learning models, we can weight the classes by their representation in the data to reflect this imbalance.\n\n## Examine Missing Values\nNext we look at the number and percentage of missing values in each column."},{"metadata":{"trusted":true,"_uuid":"26cebe71d7f54f7d174dc8dcf5afe7c2625dbe21"},"cell_type":"code","source":"# Function to calculate missing values by column #\ndef missing_value_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * mis_val / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0: 'Missing Values', 1: '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] !=0].sort_values(\n        '% of Total Values', ascending=False).round(2)\n        \n        # Print some summary information\n        print(\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"\n             \"There are \" + str(mis_val_table_ren_columns.shape[0]) + \n             \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a171505c37318d89f02b95a667a9bbc1e18846e"},"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_value_table(app_train)\nmissing_values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"945269d5c3121ed0374db1892eaff377a94a58b8"},"cell_type":"markdown","source":"When it comes time to build our machine learning models, we will have to fill in these missing values (known as imputation). In later work, we will use models such as XGBoost that can handle missing values with no need for imputation. Another option would be to drop columns with a high percentage of missing values, although it is impossible to know ahead of time if these columns will be helpful to our model. Therefore, we will keep all of the columns for now.\n\n## Column Types\nLet's look at the number of columns of each data type. int64 and float64 are numeric variables (which can be either discrete or continuous). Object columns contain strings and are categorical features. "},{"metadata":{"trusted":true,"_uuid":"c46460e725800d992b7043da79c8f5e25aaefb43"},"cell_type":"code","source":"# Number of each type of column\napp_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6e4dcc12f76f58cfc30b10c409c7b67a74ac87c"},"cell_type":"markdown","source":"Now let's look at the number of unique entries in each of the object (categorical) columns"},{"metadata":{"trusted":true,"_uuid":"3cae04d5dd6871f45024d1d7b989f6e9353d94ba"},"cell_type":"code","source":"# Number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72b0248ecfcafc60f6ed77b0d0303f14b3541837"},"cell_type":"markdown","source":"## Encoding Categorical Variables\nA machine learing model unfortunately cannot deal with categorical variables ( Except for some models such as LightGBM).  Therefore, we have to find a way to encode these variables as numbers before handing them to the model. There are two main ways to carry out this process:\n\n* Label Encoding: assign each unique category in a categorical variable with an integer. No new columns are created. An example is shown below \n![image](https://raw.githubusercontent.com/WillKoehrsen/Machine-Learning-Projects/master/label_encoding.png)\n\n* One-hot encoding: create a new column for each unique category in a categorical variable. Each observation receives a 1 in the column for its corresponding category and a 0 in all other new columns. \n![image](https://raw.githubusercontent.com/WillKoehrsen/Machine-Learning-Projects/master/one_hot_encoding.png)\n\nThe problem with Label Encoding is that it gives the categories an arbitary order. The value assigned to each of the cagegories is random and does not reflect any inherent aspect of the category. In the example above, programmer receives a 4 and data scientist receives a 1, but if we did the same process again, the labels could be reversed or completely different. The actual assignment of the integers is arbitary. Therefore, when we perform label encoding, the model might use the relative value of the feature to assign weights which is not what we want. If we only have two unique values for a categorical variable (such as Male/Female), then label encoding is fine, but for more than 2 unique categories, on-hot encoding is the safe option. \n\nThere is some debate about the relative merits of these approaches, and some models can deal with label encoded categorical variables with no issues. [Here is a good Stack Overflow discussion](https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor). The only downside to one-hot encoding is that the number of features (dimensions of the data) can explode with categorical variables with many categories. To deal with this, we can perform one-hot encoding followed by [PCA](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf) or other [dimensionality reduction methods](https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/) to reduce the number of dimensions (while still trying to preserve information). \n\nIn this notebook, we will use Label Encoding for any categorical variables with only 2 categories and One-Hot Encoding for any categorical variables with more than 2 categories. This process may need to change as we get further into the project, but for now, we will see where this gets us. (We will also not use any dimensionality reduction in this notebook but will explore in future iterations).\n\n## Label Encoding and One-Hot Encoding\n\nLet's implement the policy described above: for any categorical variable (`dtype == object`) with 2 unique categories, we will use label encoding, and for any categorical variable with more than 2 unique categories, we will use one-hot encoding. \n\nFor label encoding, we use the Scikit-Learn `LabelEncoder` and for one-hot encoding, the pandas `get_dummies(df)` function.\n"},{"metadata":{"trusted":true,"_uuid":"14007d571967c0d2806b4425bd498024b12bf2b0"},"cell_type":"code","source":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <=2:\n            # Train on the training data\n            le.fit(app_train[col])\n            # Transform both training and test data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col]  = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint ('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ede1b83685e3cf50843a220c832f96b7315ed67"},"cell_type":"code","source":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test  = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Test Feature shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56021dd29317af154ae388307ad3dd41fff6025c"},"cell_type":"markdown","source":"## Aligning Training and Testing Data\n\nThere need to be the same features (columns) in both the training and test data. One-hot encoding has created more columns in the training data because there were some categorical variables with categories not represented in the test data. To remove the columns in the training data that are not in the test data, we need to `align` the dataframes. First we extract the target column from the training data (because this is not in the test data but we need to keep this information). When we do the align, we must make sure to set `axis = 1` to align the dataframes based on the columns and not on the rows!"},{"metadata":{"trusted":true,"_uuid":"591861e1facb3ab0fac4ec5fc723b676e9e19746"},"cell_type":"code","source":"train_labels = app_train['TARGET']\n\n# Align the training and test data, keep only columns present in both data\napp_train, app_test = app_train.align(app_test, join ='inner', axis=1)\n\n# Add the target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Test Feature shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35791ffa6ca5852bb6b6b2028f64883ddeda2528"},"cell_type":"markdown","source":"## Back to Exploratory Data Analysis\n\n### Anomalies\n\nOne problem we always want to be on the lookout for when doing EDA is anomalies within the data. These may be due to mis-typed numbers, errors in measuring equipment, or they could be valid but extreme measurements. One way to support anomalies quantitatively is by looking at the statistics of a column using the `describe` method. The numbers in the `DAYS_BIRTH` column are negative because they are recorded relative to the current loan application. To see these stats in years, we can mutliple by -1 and divide by the number of days in a year:"},{"metadata":{"trusted":true,"_uuid":"394764504ff75545b8fe486bebd587387e649412"},"cell_type":"code","source":"(app_train['DAYS_BIRTH'] / -365).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2435b9deab472851a19ab80fd4f45fd963b0684a"},"cell_type":"code","source":"(app_train['DAYS_BIRTH'] / -365).plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aee87e859d1d6e0d08e137647b740458322ff77d"},"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5321f2f03341be232c5e0201118276e56d3bb16d"},"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].plot.hist(title ='Days Employment Histogram')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47b9bb1d40baeb9c06ba5afeec74d83aacb7b525"},"cell_type":"code","source":"anom = app_train[app_train['DAYS_EMPLOYED'] > 50000]\nanom['DAYS_EMPLOYED'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aff265497df0e510b46ddf048b9ca77f3cfc485d"},"cell_type":"code","source":"non_anom = app_train[app_train['DAYS_EMPLOYED'] < 50000]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6a71aa45c5597a36fc0ac682e3fda8c760c4ed1"},"cell_type":"markdown","source":"Well that is extremely interesting! It turns out that the anomalies have a lower rate of default.\n\nHandling the anomalies depends on the exact situation, with no set rules. One of the safest approaches is just to set the anomalies to a missing value and then have them filled in (using Imputation) before machine learning. In this case, since all the anomalies have the exact same value, we want to fill them in with the same value in case all of these loans share something in common. The anomalous values seem to have some importance, so we want to tell the machine learning model if we did in fact fill in these values. As a solution, we will fill in the anomalous values with not a number (np.nan) and then create a new boolean column indicating whether or not the value was anomalous."},{"metadata":{"trusted":true,"_uuid":"baabb0ad30feed2f90adeac8911c4fbf30c1eabb"},"cell_type":"code","source":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train['DYAS_EMPLOYED'] == 365243\n\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\napp_train['DAYS_EMPLOYED'].plot.his(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb2cd3c0c180fc77abe5e27d80e8308706c1607c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}