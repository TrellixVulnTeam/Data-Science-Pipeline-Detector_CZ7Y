{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\n# Modeling\nimport lightgbm as lgb\n\n# Splitting data\nfrom sklearn.model_selection import train_test_split\n# Performance\nfrom sklearn.metrics import roc_auc_score\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n# Dimensionality reduction\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.decomposition import NMF\nfrom sklearn.preprocessing import OneHotEncoder\n\nN_FOLDS = 5\nMAX_EVALS = 5\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3396faa14c5d4618e4fe33262aa4d6176572084c"},"cell_type":"code","source":"# One-hot encoding for categorical columns with get_dummies\ndef one_hot_encoder(df, nan_as_category = True):\n    original_columns = list(df.columns)\n    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n    new_columns = [c for c in df.columns if c not in original_columns]\n    return df, new_columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f102213f6b5ccdabcd657f2706db5f82f871ac15"},"cell_type":"code","source":"# Preprocess application_train.csv and application_test.csv\ndef application_train_test(num_rows = None, nan_as_category = False):\n    # Read data and merge\n    df = pd.read_csv('../input/application_train.csv', nrows= num_rows)\n    test_df = pd.read_csv('../input/application_test.csv', nrows= num_rows)\n    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n    df = df.append(test_df).reset_index()\n    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n    df = df[df['CODE_GENDER'] != 'XNA']\n    \n    # Categorical features with Binary encode (0 or 1; two categories)\n    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n    # Categorical features with One-Hot encode\n    df, cat_cols = one_hot_encoder(df, nan_as_category)\n    \n    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n    # Some simple new features (percentages)\n    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n    del test_df\n    gc.collect()\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3804dd97b5c03b3d5f79a7981b2b1b20f5453cb0"},"cell_type":"code","source":"# Preprocess bureau.csv and bureau_balance.csv\ndef bureau_and_balance(num_rows = None, nan_as_category = True):\n    bureau = pd.read_csv('../input/bureau.csv', nrows = num_rows)\n    bb = pd.read_csv('../input/bureau_balance.csv', nrows = num_rows)\n    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n    \n    # Bureau balance: Perform aggregations and merge with bureau.csv\n    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n    for col in bb_cat:\n        bb_aggregations[col] = ['mean']\n    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n    del bb, bb_agg\n    gc.collect()\n    \n    # Bureau and bureau_balance numeric features\n    num_aggregations = {\n        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n        'DAYS_CREDIT_UPDATE': ['mean'],\n        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n        'AMT_ANNUITY': ['max', 'mean'],\n        'CNT_CREDIT_PROLONG': ['sum'],\n        'MONTHS_BALANCE_MIN': ['min'],\n        'MONTHS_BALANCE_MAX': ['max'],\n        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n    }\n    # Bureau and bureau_balance categorical features\n    cat_aggregations = {}\n    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n    \n    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n    # Bureau: Active credits - using only numerical aggregations\n    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n    del active, active_agg\n    gc.collect()\n    # Bureau: Closed credits - using only numerical aggregations\n    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n    del closed, closed_agg, bureau\n    gc.collect()\n    return bureau_agg\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18f9062f3ccb8d4e7546d9cddcb7e7c61ffb8bbf","collapsed":true},"cell_type":"code","source":"def load_data(feature_reduction,numeric_feature):\n    train_features = pd.read_csv('../input/application_train.csv')\n    target_features = pd.read_csv('../input/application_test.csv')\n    bureau=bureau_and_balance()\n    # Sample 16000 rows (10000 for training, 6000 for testing)\n    if feature_reduction==True:\n         train_features = train_features.sample(n = 16000, random_state = 42)\n    \n    # Only numeric features\n    if numeric_feature==True:\n        train_features = train_features.select_dtypes('number')\n        target_features= target_features.select_dtypes('number')\n    #Histogram analysis of training data\n    data_analysis(train_features)\n    # Extract the labels\n    train_y_labels = np.array(train_features['TARGET'].astype(np.int32)).reshape((-1, ))\n    # Extract the Target ids\n    target_ids = target_features['SK_ID_CURR']\n    # Combine Datasets\n    df=combine_datasets(train_features,target_features)\n    print(\"Bureau df shape:\", bureau.shape)\n    df = df.join(bureau, how='left', on='SK_ID_CURR')\n    # Drop\n    train_features = train_features.drop(columns = ['TARGET', 'SK_ID_CURR'])\n    target_features= target_features.drop(columns = ['SK_ID_CURR'])\n    # Encoding Features\n    train_sz=train_features.shape\n    train_features,target_features=data_encoding(df,train_sz)\n    # Feature reduction to most relevant features\n    train_red,target_red=feature_reduction_pca(train_features,train_y_labels,target_features,2,10)\n    # Split into training and testing data\n    train_X, test_X, train_y, test_y = train_test_split(train_red, train_y_labels, test_size = 0.2, random_state = 50)\n    \n    print(\"Training features shape train_X.shape,train_red.shape: \\n\", train_X.shape,train_red.shape)\n    print(\"Testing features shape target_features.shape, target_red.shape: \\n \", target_features.shape, target_red.shape)\n    return train_X,test_X,train_y,test_y,target_red,target_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"05fb46051770931f352c4f61eab0c8e1353c056a"},"cell_type":"code","source":"def combine_datasets(train_app,target_app):\n    # Combine Datasets\n    df = train_app.append(target_app).reset_index()\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e39c72c0f5b89bc1f4126996c0b0cd22b9870706"},"cell_type":"code","source":"def data_analysis(data):\n    # Histogram of target\n    data['TARGET'].astype(int).plot.hist()\n    # Number of each type of column\n    print(\"Number of each type of column # \",data.dtypes.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c996f64e7bbc8b565d2af78f5cbfb26adfb42ee","collapsed":true},"cell_type":"code","source":"def data_encoding(df,train_sz):\n    # Create a label encoder object\n    le = LabelEncoder()\n    le_count = 0\n    # Combine Datasets\n    #df = train.append(target).reset_index()\n    # Iterate through the columns\n    for col in df:\n        if df[col].dtype == 'object':\n            # If 2 or fewer unique categories\n            if len(list(df[col].unique())) <= 2:\n                # Train on the training data\n                le.fit(df[col])\n                # Transform both training and testing data\n                df[col] = le.transform(df[col])\n                #target[col] = le.transform(target[col])\n            \n                # Keep track of how many columns were label encoded\n                le_count += 1\n            \n    print('%d columns were label encoded.' % le_count)\n    # one-hot encoding of categorical variables\n    df = pd.get_dummies(df)\n    # Preprocessing\n    df=preprocessing(df)\n    #target = pd.get_dummies(target)\n    #train_sz=train.shape\n    print(\"Train Size during encoding : \\n \",train_sz[0],train_sz[1])\n    train=df.iloc[:train_sz[0]]\n    target=df.iloc[train_sz[0]:]\n    #target = target.drop(columns = ['TARGET'])\n    \n    return train,target\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ef6b354f877bd53f958070d3ffb88b5c15630f4","collapsed":true},"cell_type":"code","source":"def feature_reduction_pca(train_X,train_y,target_X,red_tech,n_c):\n    if red_tech==1:\n        reducer=PCA(n_components=n_c)\n    elif red_tech==2:\n        reducer=LDA(n_components=10)\n    \n    \n    reducer.fit(train_X, train_y)\n    train_reduced_samples = pd.DataFrame(reducer.transform(train_X))\n    target_reduced_samples = pd.DataFrame(reducer.transform(target_X))\n    \n    train_X_reduced=train_reduced_samples.values\n    target_X_reduced=target_reduced_samples.values\n    print(\"Train Head after feature reduction : \\n\",train_reduced_samples.head())\n    \n    return train_X_reduced,target_X_reduced","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2e3bc1cff67706db09c3a798888d4d6a6ece5d60"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\ndef preprocessing(df):\n    all_features = list(df.columns)\n   # all_features.remove('TARGET')\n   # all_features.remove('SK_ID_CURR')\n    scaler=RobustScaler()\n    # fill na\n    for feature in all_features:\n        df[feature] = df[feature].fillna(df[feature].mean())\n    \n    # scaling\n    df[all_features] = pd.DataFrame(scaler.fit_transform(df[all_features]))\n    \n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b35b92003de5be96419e840dd2a33b9e900c14a5"},"cell_type":"code","source":"def lgb_preprocessing(train_X,test_X,train_y,test_y):\n    # Create a training and testing dataset\n    train_set = lgb.Dataset(data = train_X, label = train_y)\n    test_set = lgb.Dataset(data = test_X, label = test_y)\n    return train_set,test_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8cf8bb7a6bf6dfad884e0f7cac83d8576cc3427d"},"cell_type":"code","source":"def lgb_crossvalidation(train_set):\n    # Get default hyperparameters\n    #model = lgb.LGBMClassifier()\n    model = lgb.LGBMClassifier(\n            boosting_type='gbdt',\n            objective='binary',\n            n_estimators=1000,\n            learning_rate=0.1,\n            num_leaves=31,\n            feature_fraction=0.8,\n            subsample=0.8,\n            max_depth=8,\n            reg_alpha=1,\n            reg_lambda=1,\n            min_child_weight=40,\n            random_state=2018,\n            nthread=-1)\n    \n    default_params = model.get_params()\n\n    # Remove the number of estimators because we set this to 10000 in the cv call\n    del default_params['n_estimators']\n\n    # Cross validation with early stopping\n    cv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, \n                    metrics = 'auc', nfold = N_FOLDS, seed = 42)\n    # Printing results\n    print('The maximum validation ROC AUC was: {:.5f} with a standard deviation of {:.5f}.'.format(cv_results['auc-mean'][-1], cv_results['auc-stdv'][-1]))\n    print('The optimal number of boosting rounds (estimators) was {}.'.format(len(cv_results['auc-mean'])))\n    \n    #Training using optimal parameters\n    model.n_estimators = len(cv_results['auc-mean'])\n    # Train and make predicions with model\n    model.fit(train_X, train_y)\n    \n    return model,cv_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4ac7f35fbd9edfde2fb64d7c9c0fb66ecae87482"},"cell_type":"code","source":"def lgb_testing(model,test_X,test_y):\n    preds = model.predict_proba(test_X)[:, 1]\n    baseline_auc = roc_auc_score(test_y, preds)\n\n    print('The baseline model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))\n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a078625cc7ada9eb7e1d6243e7cbf87b75bd9326"},"cell_type":"code","source":"def result_submission(target_ids,preds):\n    submission = pd.DataFrame({'SK_ID_CURR': target_ids, 'TARGET': preds})\n    submission.to_csv('submission_simple_features_random.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_X,test_X,train_y,test_y,target,target_ids=load_data(False,False)\ntrain_set,test_set=lgb_preprocessing(train_X,test_X,train_y,test_y)\nmodel,cv_results=lgb_crossvalidation(train_set)\ntest_preds=lgb_testing(model,test_X,test_y)\napplication_train_test()\n# Predictions on the target data\npreds = model.predict_proba(target)[:, 1]\nresult_submission(target_ids,preds)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}