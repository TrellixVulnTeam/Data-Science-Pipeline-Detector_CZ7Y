{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-27T20:07:14.333101Z","iopub.execute_input":"2021-08-27T20:07:14.333466Z","iopub.status.idle":"2021-08-27T20:07:14.342724Z","shell.execute_reply.started":"2021-08-27T20:07:14.333431Z","shell.execute_reply":"2021-08-27T20:07:14.341899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install creditHome","metadata":{"execution":{"iopub.status.busy":"2021-08-27T20:07:00.470171Z","iopub.execute_input":"2021-08-27T20:07:00.470616Z","iopub.status.idle":"2021-08-27T20:07:11.239918Z","shell.execute_reply.started":"2021-08-27T20:07:00.470516Z","shell.execute_reply":"2021-08-27T20:07:11.238643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from creditHome.eda1 import *","metadata":{"execution":{"iopub.status.busy":"2021-08-27T20:07:18.57188Z","iopub.execute_input":"2021-08-27T20:07:18.572319Z","iopub.status.idle":"2021-08-27T20:07:19.612633Z","shell.execute_reply.started":"2021-08-27T20:07:18.572283Z","shell.execute_reply":"2021-08-27T20:07:19.61167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def grab_col_names1(dataframe, cat_th=10, car_th=20):\n    \"\"\"\n\n    Veri setindeki kategorik, numerik ve kategorik fakat kardinal değişkenlerin isimlerini verir.\n    Not: Kategorik değişkenlerin içerisine numerik görünümlü kategorik değişkenler de dahildir.\n\n    Parameters\n    ------\n        dataframe: dataframe\n                Değişken isimleri alınmak istenilen dataframe\n        cat_th: int, optional\n                numerik fakat kategorik olan değişkenler için sınıf eşik değeri\n        car_th: int, optinal\n                kategorik fakat kardinal değişkenler için sınıf eşik değeri\n\n    Returns\n    ------\n        cat_cols: list\n                Kategorik değişken listesi\n        num_cols: list\n                Numerik değişken listesi\n        cat_but_car: list\n                Kategorik görünümlü kardinal değişken listesi\n\n    Examples\n    ------\n        import seaborn as sns\n        df = sns.load_dataset(\"iris\")\n        print(grab_col_names(df))\n\n\n    Notes\n    ------\n        cat_cols + num_cols + cat_but_car = toplam değişken sayısı\n        num_but_cat cat_cols'un içerisinde.\n        Return olan 3 liste toplamı toplam değişken sayısına eşittir: cat_cols + num_cols + cat_but_car = değişken sayısı\n\n    \"\"\"\n\n\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car","metadata":{"execution":{"iopub.status.busy":"2021-08-27T20:07:24.098093Z","iopub.execute_input":"2021-08-27T20:07:24.098495Z","iopub.status.idle":"2021-08-27T20:07:24.109683Z","shell.execute_reply.started":"2021-08-27T20:07:24.098462Z","shell.execute_reply":"2021-08-27T20:07:24.108517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, cross_validate,RandomizedSearchCV\nfrom lightgbm import LGBMClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.width', 170)\n\n\n\ndf1 = pd.read_csv(\"/kaggle/input/home-credit-default-risk/bureau_balance.csv\")\ndf2 = pd.read_csv(\"/kaggle/input/home-credit-default-risk/bureau.csv\")\ntrain = pd.read_csv(\"/kaggle/input/home-credit-default-risk/application_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/home-credit-default-risk/application_test.csv\")\n\ndff = train.append(test).reset_index(drop=True)\ndff.head()\n\n\n\ncheck_df(df1)\n\n# Are there any missing values in the data? (bureau_balance)\ndf1.isnull().sum()\n\n\n\n\ncat_cols, num_cols, cat_but_car = grab_col_names1(df1)\n\nfor col in cat_cols:\n    cat_summary(df1, col)\n\nfor col in num_cols:\n    num_summary(df1, col)\n\n\n\n\n# One-Hot Encoder\nbb, bb_cat = one_hot_encoder(df1, nan_as_category=False)\n\n# Bureau balance: Perform aggregations and merge with bureau.csv\nbb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n\nfor col in bb_cat:\n    bb_aggregations[col] = ['mean']\n\nbb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\nbb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n\nbb_agg[\"STATUS_C0_MEAN_SUM\"] = bb_agg[[\"STATUS_C_MEAN\", \"STATUS_0_MEAN\"]].sum(axis = 1)\nbb_agg[\"STATUS_12_MEAN_SUM\"] = bb_agg[[\"STATUS_1_MEAN\", \"STATUS_2_MEAN\"]].sum(axis = 1)\nbb_agg[\"STATUS_345_MEAN_SUM\"] = bb_agg[[\"STATUS_3_MEAN\", \"STATUS_4_MEAN\", \"STATUS_5_MEAN\"]].sum(axis = 1)\nbb_agg[\"STATUS_12345_MEAN_SUM\"] = bb_agg[[\"STATUS_1_MEAN\", \"STATUS_2_MEAN\", \"STATUS_3_MEAN\", \"STATUS_4_MEAN\", \"STATUS_5_MEAN\"]].sum(axis = 1)\n\nclosed = df1[df1.STATUS == \"C\"]\nclosed = closed.groupby(\"SK_ID_BUREAU\").MONTHS_BALANCE.min().reset_index().rename({\"MONTHS_BALANCE\":\"MONTHS_BALANCE_FIRST_C\"}, axis = 1)\nclosed[\"MONTHS_BALANCE_FIRST_C\"] = np.abs(closed[\"MONTHS_BALANCE_FIRST_C\"])\nbb_agg = pd.merge(bb_agg, closed, how = \"left\", on = \"SK_ID_BUREAU\")\nbb_agg[\"MONTHS_BALANCE_CLOSED_DIF\"] = np.abs(bb_agg.MONTHS_BALANCE_MIN) - bb_agg.MONTHS_BALANCE_FIRST_C\n\ndel closed, bb_aggregations, bb_cat\n\nbb_agg.shape\n\ndf2.head()\ndf2.shape\n\ncheck_df(df2)\n\ndf2[\"CREDIT_TYPE\"].head(20)\ndf2[\"AMT_CREDIT_MAX_OVERDUE\"].head(20)\n\ndf = pd.merge(df2, bb_agg, how = \"left\", on = \"SK_ID_BUREAU\")\ndel bb_agg\n\ndf.head()\n\ndf.shape\n\n# Correlation\ncorr_plot(df, remove=['SK_ID_CURR','SK_ID_BUREAU'], corr_coef = \"spearman\")\n\nhigh_correlation(df, remove=['SK_ID_CURR','SK_ID_BUREAU'], corr_coef = \"spearman\", corr_value = 0.7)\n\n#daha önce kaç kez kredi aldığı bilgisi\nprevious_loan_counts = df.groupby('SK_ID_CURR', as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU': 'PREVIOUS_LOAN_COUNTS'})\n\ndff = dff.merge(previous_loan_counts, on = 'SK_ID_CURR', how = 'left')\ndff.head()\n\ndff.loc[dff[\"PREVIOUS_LOAN_COUNTS\"].isnull(),\"PREVIOUS_LOAN_COUNTS\"]=0\n\ndf.loc[df[\"CNT_CREDIT_PROLONG\"]!=0]\n\n\nA= df.groupby('SK_ID_CURR', as_index=False)['CNT_CREDIT_PROLONG'].sum().rename(columns = {'CNT_CREDIT_PROLONG': 'PRE_CNT_CREDIT_PROLONG_SUM'})\n\ndff = dff.merge(A, on = 'SK_ID_CURR', how = 'left')\ndff.head(10)\n\ndff[\"CREDIT_RATE\"]=dff[\"PRE_CNT_CREDIT_PROLONG_SUM\"]/dff[\"PREVIOUS_LOAN_COUNTS\"]\n#toplam kredi zamanı (gün)\ndf[\"CREDIT_DAY_SUM\"] = df[\"DAYS_CREDIT\"]-df[\"DAYS_CREDIT_ENDDATE\"]\n\nA= df.groupby('SK_ID_CURR', as_index=False)[\"CREDIT_DAY_SUM\"].sum()\ndff = dff.merge(A, on = 'SK_ID_CURR', how = 'left')\n\ndff.loc[dff[\"CREDIT_DAY_SUM\"].isnull(),\"CREDIT_DAY_SUM\"]=0\ndff[\"CREDIT_DAY_SUM\"]=dff[\"CREDIT_DAY_SUM\"]*(-1)\ndff.head()\n\ndf[\"ACTIVE_CREDIT_COUNT\"]=[0 if i>0 else 1 for i in df[\"DAYS_CREDIT_ENDDATE\"]]\n\nA= df.groupby('SK_ID_CURR', as_index=False)[\"ACTIVE_CREDIT_COUNT\"].sum()\ndff = dff.merge(A, on = 'SK_ID_CURR', how = 'left')\ndff.head()\n\ndff[\"CREDIT_ACTIVE_RATE\"]=dff[\"ACTIVE_CREDIT_COUNT\"] / dff[\"PREVIOUS_LOAN_COUNTS\"]\n\ncat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(df, car_th=10)\nnum_cols\nnum_cols=[col for col in num_cols if col not in [\"SK_ID_CURR\" ,\"SK_ID_BUREAU\"]]\n\n\nnew_sorting(df,\"CREDIT_ACTIVE\",['Closed', 'Active', 'Sold', 'Bad debt'],[0,1,2,3])\ndf.head()\n\n\nrare_analyser(df, \"NEW_CREDIT_ACTIVE\", cat_cols)\n\ndf=df.drop(\"CREDIT_CURRENCY\",axis=1)\ndf.head()\n\ndf=df.drop(\"CREDIT_ACTIVE\",axis=1)\ndf.head()\n\ncat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(df, car_th=10)\n\n\n\ndf=rare_encoder(df, 0.001, cat_cols)\n\ndf = df.drop(\"CREDIT_DAY_SUM\",axis=1)\ndf = df.drop(\"ACTIVE_CREDIT_COUNT\",axis=1)\n\n\ncol=[i for i in df.columns if i not in [\"SK_ID_CURR\",\"SK_ID_BUREAU\"]]\ndf=df.groupby('SK_ID_CURR', as_index=False)[col].sum()\ndff = dff.merge(df,on='SK_ID_CURR', how=\"left\")\n\n# buraya kadar dilara ve sevval\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-27T20:07:32.805733Z","iopub.execute_input":"2021-08-27T20:07:32.806178Z","iopub.status.idle":"2021-08-27T20:11:43.887439Z","shell.execute_reply.started":"2021-08-27T20:07:32.806131Z","shell.execute_reply":"2021-08-27T20:11:43.886363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from creditHome.eda2 import *","metadata":{"execution":{"iopub.status.busy":"2021-08-27T20:12:10.740133Z","iopub.execute_input":"2021-08-27T20:12:10.740487Z","iopub.status.idle":"2021-08-27T20:12:10.74549Z","shell.execute_reply.started":"2021-08-27T20:12:10.740458Z","shell.execute_reply":"2021-08-27T20:12:10.744769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = dff.copy()\ndf.head()\n\n\n\nprint(\"Train veri seti boyutları:\",train.shape)\nprint(\"Test veri seti boyutları:\",test.shape)\n\n\nrate_null_columns=control_df(df,train)\nuseless_columns=[col for col in rate_null_columns if rate_null_columns[col] > 0.15 and col != \"TARGET\"]\n\ncat_cols,num_cols=grab_cat_num(df)\n\nfor col in num_cols:\n    num_analysis(df,col)\n    \nfor col in cat_cols:\n    cat_summary(df,col,True)\n\n\ngg(df[cat_cols],\"TARGET\")\n\n\n\n\n# cinsiyet problemi\ndf = df[~(df.CODE_GENDER.str.contains(\"XNA\"))]\n# aile durumu bilinmeyenleri drop\ndf = df[df.NAME_FAMILY_STATUS != \"Unknown\"]\n# kac tane dokuman getirmis\nflag = [x for x in df.columns if 'FLAG_DOC' in x]\ndf['DOCUMENT_COUNT'] = df[flag].sum(axis=1)\n# dis kaynakli verilerden olusturulan degisken\ndf['EXT_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n# kredi miktarinin odeme ve mallara orani\ndf['CREDIT_TO_ANNUITY_RATIO'] = df['AMT_CREDIT'] / df[\"AMT_ANNUITY_x\"]\ndf['CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n# gelire gore kredi odemesi. toplam kredi miktari orani.\ndf['ANNUITY_TO_INCOME_RATIO'] = df[\"AMT_ANNUITY_x\"] / df['AMT_INCOME_TOTAL']\ndf['CREDIT_TO_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n# gunluk kac para kazaniyor. kac gun calismis. gelirine gore orani.\ndf['INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_EMPLOYED']\ndf['INCOME_TO_BIRTH_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_BIRTH']\n# issizligi az mi cok mu\ndf['EMPLOYED_TO_BIRTH_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n# arabayi ne zaman degismis orani\ndf['CAR_TO_EMPLOYED_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n# telefonu ne zaman degismis orani\ndf['PHONE_TO_EMPLOYED_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_EMPLOYED']\n# aile bireyi basina dusen gelir miktari\ndf['CNT_FAM_INCOME_RATIO'] = df['AMT_INCOME_TOTAL']/df['CNT_FAM_MEMBERS']\n\n# DROP\ncols = [\"NAME_HOUSING_TYPE\", \"WEEKDAY_APPR_PROCESS_START\", \"FONDKAPREMONT_MODE\", \"WALLSMATERIAL_MODE\", \"HOUSETYPE_MODE\",\"EMERGENCYSTATE_MODE\",\"FLAG_MOBIL\", \"FLAG_EMP_PHONE\",\"FLAG_WORK_PHONE\", \"FLAG_CONT_MOBILE\", \"FLAG_PHONE\", \"FLAG_EMAIL\"]\ndf.drop(cols, axis = 1, inplace = True)\n\n# REGION\ncols = [\"REG_REGION_NOT_LIVE_REGION\",\"REG_REGION_NOT_WORK_REGION\", \"LIVE_REGION_NOT_WORK_REGION\", \"REG_CITY_NOT_LIVE_CITY\",\"REG_CITY_NOT_WORK_CITY\",\"LIVE_CITY_NOT_WORK_CITY\"]\ndf[\"REGION\"] = df[cols].sum(axis = 1)\ndf.drop(cols, axis = 1, inplace = True)\n\n# Drop FLAG_DOCUMENT\ndf.drop(df.columns[df.columns.str.contains(\"FLAG_DOCUMENT\")], axis = 1, inplace = True)\n\n\ndf[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.str.contains(\"Business Entity\"), \"Business Entity\", df.ORGANIZATION_TYPE)\ndf[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.str.contains(\"Industry\"), \"Industry\", df.ORGANIZATION_TYPE)\ndf[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.str.contains(\"Trade\"), \"Trade\", df.ORGANIZATION_TYPE)\ndf[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.str.contains(\"Transport\"), \"Transport\", df.ORGANIZATION_TYPE)\ndf[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"School\", \"Kindergarten\", \"University\"]), \"Education\", df.ORGANIZATION_TYPE)\ndf[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"Emergency\",\"Police\", \"Medicine\",\"Goverment\", \"Postal\", \"Military\", \"Security Ministries\", \"Legal Services\"]), \"Public\", df.ORGANIZATION_TYPE)\ndf[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"Bank\", \"Insurance\"]), \"Finance\", df.ORGANIZATION_TYPE)\ndf[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"Realtor\", \"Housing\"]), \"House\", df.ORGANIZATION_TYPE)\ndf[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"Hotel\", \"Restaurant\"]), \"HotelRestaurant\", df.ORGANIZATION_TYPE)\ndf[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"Cleaning\",\"Electricity\", \"Telecom\", \"Mobile\", \"Advertising\", \"Religion\", \"Culture\"]), \"Other\", df.ORGANIZATION_TYPE)\n\n# mumkun degil bu deger\ndf['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n\n# musteri teli en son ne zaman degismis?\ndf['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True)\n\n\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\ncat_cols = [x for x in cat_cols if x !='TARGET']\n\ndf = one_hot_encoder(df,cat_cols,True)\n\n\ndf.columns = list(map(lambda x: str(x).replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"_/_\", \"_\").upper(), df.columns))\nimport re\ndf = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\ntrain = df[df.TARGET.isnull() == False]\ntest = df[df.TARGET.isnull()]\nx_train = train.drop([\"TARGET\", \"SK_ID_CURR\"], axis = 1)\nx_test = test.drop([\"TARGET\", \"SK_ID_CURR\"], axis = 1)\ny_train = train.TARGET\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-27T20:12:14.326625Z","iopub.execute_input":"2021-08-27T20:12:14.326977Z","iopub.status.idle":"2021-08-27T20:15:37.676352Z","shell.execute_reply.started":"2021-08-27T20:12:14.326948Z","shell.execute_reply":"2021-08-27T20:15:37.675141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n\nlgbm_model = LGBMClassifier(\n                n_estimators=4000,\n                learning_rate=0.01,\n                max_depth = 11,\n                num_leaves=58,\n                colsample_bytree=0.613,\n                subsample=0.708,\n                max_bin=407,\n                reg_alpha=3.564,\n                reg_lambda=4.930,\n                min_child_weight= 6,\n                min_child_samples=165,\n                silent=-1,\n                verbose=-1,\n                ).fit(x_train, y_train, eval_set=[(x_train, y_train)],\n        eval_metric='auc', verbose=200)\n\n\n#cv 10 katli normalde\ncv_results_1 = cross_validate(lgbm_model, x_train, y_train, cv=10, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])\ncv_results_1['test_accuracy'].mean()\ncv_results_1['test_f1'].mean()\ncv_results_1['test_roc_auc'].mean()\n\ndef plot_importance(model, features, num=len(x_train), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')\n\n\nplot_importance(lgbm_model,x_train)\nplot_importance(lgbm_model,x_train,num=40)\n\nfeature_importances = pd.DataFrame({'feature':x_train.columns,\n                                    'importance':lgbm_model.feature_importances_})\n#  onemli\nimportant_features = feature_importances[feature_importances['importance'] > 0]\n\n#  onemsiz\nunimportant_features = feature_importances[feature_importances['importance'] < 1]\n\nimportant = important_features.sort_values(ascending=False,by='importance')\n\nimportant.to_excel('important_features.xlsx')\nunimportant_features.to_excel('unimportant_features.xlsx')\n\n\n# unimportant featurelari dustukten sonra bi degisim olmadi. dustukten sonra hiperparemetre\n# opt. yapilabilir.\ndf_2 = df.drop(unimportant_features['feature'],inplace = True,axis=1)\n\ntrain_2 = df[df.TARGET.isnull() == False]\ntest_2 = df[df.TARGET.isnull()]\nx_train_2 = train.drop([\"TARGET\", \"SK_ID_CURR\"], axis = 1)\nx_test_2 = test.drop([\"TARGET\", \"SK_ID_CURR\"], axis = 1)\ny_train_2 = train.TARGET\n\nlgbm_model_2 = LGBMClassifier(\n                n_estimators=4000,\n                learning_rate=0.01,\n                max_depth = 11,\n                num_leaves=58,\n                colsample_bytree=0.613,\n                subsample=0.708,\n                max_bin=407,\n                reg_alpha=3.564,\n                reg_lambda=4.930,\n                min_child_weight= 6,\n                min_child_samples=165,\n                silent=-1,\n                verbose=-1,\n                ).fit(x_train_2, y_train_2, eval_set=[(x_train_2, y_train_2)],\n        eval_metric='auc', verbose=200)\n\ncv_results_2 = cross_validate(lgbm_model_2, x_train_2, y_train_2, cv=3, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])\ncv_results_2['test_accuracy'].mean()\ncv_results_2['test_f1'].mean()\ncv_results_2['test_roc_auc'].mean()\n\n\nlgbm = LGBMClassifier()\nlgbm_params = {\"learning_rate\": [0.01, 0.1, 0.001,0.05,1,0.005],\n               \"n_estimators\": [100, 300, 500, 1000],\n               \"colsample_bytree\": [0.5, 0.7, 1,1.5],\n               'num_leaves' : [31,10,15,45],\n               'max_depth':[-1,1,3,5,10],\n               'min_split_gain':[0.,0.1,0.5,1,0.01,0.05]}\n\n# random search\nrandom_lgb = RandomizedSearchCV(estimator=lgbm,\n                               param_distributions=lgbm_params,\n                               n_iter=100,  # denenecek parametre sayısı\n                               cv=3,\n                               verbose=True,\n                               random_state=42,\n                               n_jobs=-1).fit(x_train,y_train)\nrandom_lgb.best_score_\nrandom_lgb.best_params_\n\n# grid search icin. randomdan gelen degerleri ekleyerek calistir.\nlgbm_params2 = {\"learning_rate\": [0.01, 0.1, 0.001,0.005],\n               \"n_estimators\": [2000,4000],\n               \"colsample_bytree\": [0.5, 0.7, 1],\n               'num_leaves' : [30,15,45,60],\n               'max_depth':[-1,3,10],\n               'min_split_gain':[0.,0.1,0.5,1]}\n\n\n# grid search\nlgbm = LGBMClassifier()\nlgm_grid = GridSearchCV(lgbm, lgbm_params2, cv=3, n_jobs=-1, verbose=True).fit(x_train, y_train)\nlgm_grid.best_params_\n\n\n\n# creating final submission\nsubmission = pd.DataFrame({\n    \"SK_ID_CURR\": test.SK_ID_CURR,\n    \"TARGET\": lgbm_model.predict_proba(x_test)[:,1]\n})\n\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]}]}