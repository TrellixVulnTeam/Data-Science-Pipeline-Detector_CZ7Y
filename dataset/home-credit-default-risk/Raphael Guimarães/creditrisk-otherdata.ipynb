{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install feature_engine","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom feature_engine.outliers import Winsorizer, OutlierTrimmer\nfrom category_encoders import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler,PolynomialFeatures\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nimport lightgbm as lgb\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train = pd.read_csv(\"/kaggle/input/home-credit-default-risk/application_train.csv\")\napp_test = pd.read_csv(\"/kaggle/input/home-credit-default-risk/application_test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**BUREAU**","metadata":{}},{"cell_type":"code","source":"# Read in bureau\nbureau = pd.read_csv('/kaggle/input/home-credit-default-risk/bureau.csv')\nbureau.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Bureau Balance**","metadata":{}},{"cell_type":"code","source":"bureau_balance = pd.read_csv('/kaggle/input/home-credit-default-risk/bureau_balance.csv')\nbureau_balance.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def agg_numeric(df, parent_var, df_name):\n    \"\"\"\n    Groups and aggregates the numeric values in a child dataframe\n    by the parent variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the child dataframe to calculate the statistics on\n        parent_var (string): \n            the parent variable used for grouping and aggregating\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated by the `parent_var` for \n            all numeric columns. Each observation of the parent variable will have \n            one row in the dataframe with the parent variable as the index. \n            The columns are also renamed using the `df_name`. Columns with all duplicate\n            values are removed. \n    \n    \"\"\"\n    \n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != parent_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    # Only want the numeric variables\n    parent_ids = df[parent_var].copy()\n    numeric_df = df.select_dtypes('number').copy()\n    numeric_df[parent_var] = parent_ids\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(parent_var).agg(['count', 'mean', 'max', 'min', 'sum'])\n\n    # Need to create new column names\n    columns = []\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        if var != parent_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n    \n    agg.columns = columns\n    \n    # Remove the columns with all redundant values\n    _, idx = np.unique(agg, axis = 1, return_index=True)\n    agg = agg.iloc[:, idx]\n    \n    return agg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def agg_categorical(df, parent_var, df_name):\n    \"\"\"\n    Aggregates the categorical features in a child dataframe\n    for each observation of the parent variable.\n    \n    Parameters\n    --------\n    df : dataframe \n        The dataframe to calculate the value counts for.\n        \n    parent_var : string\n        The variable by which to group and aggregate the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    df_name : string\n        Variable added to the front of column names to keep track of columns\n\n    \n    Return\n    --------\n    categorical : dataframe\n        A dataframe with aggregated statistics for each observation of the parent_var\n        The columns are also renamed and columns with duplicate values are removed.\n        \n    \"\"\"\n    \n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('object'))\n     \n\n    # Make sure to put the identifying id on the column\n    categorical[parent_var] = df[parent_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(parent_var).agg(['sum', 'count', 'mean',\"min\",\"max\"])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['sum', 'count', 'mean',\"min\",\"max\"]:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    # Remove duplicate columns by values\n    _, idx = np.unique(categorical, axis = 1, return_index = True)\n    categorical = categorical.iloc[:, idx]\n    \n    return categorical","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Bureau_balance aggregation**","metadata":{}},{"cell_type":"code","source":"bureau_balance_agg_numeric = agg_numeric(bureau_balance,\"SK_ID_BUREAU\",'bureau_balance')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bureau_balance_agg_numeric.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bureau_balance_agg_categorical = agg_categorical(bureau_balance,\"SK_ID_BUREAU\",'bureau_balance')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bureau_balance_agg_categorical.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MERGE WITH BUREAU**","metadata":{}},{"cell_type":"code","source":"bureau = bureau.merge(bureau_balance_agg_numeric, on = 'SK_ID_BUREAU', how = 'left')\nbureau = bureau.merge(bureau_balance_agg_categorical, on = 'SK_ID_BUREAU', how = 'left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Bureau Aggregation**","metadata":{}},{"cell_type":"code","source":"bureau_agg_numeric = agg_numeric(bureau,\"SK_ID_CURR\",'bureau')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bureau_agg_categorical = agg_categorical(bureau,\"SK_ID_CURR\",'bureau')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Domain Features**","metadata":{}},{"cell_type":"code","source":"app_train['CREDIT_INCOME_PERCENT'] = app_train['AMT_CREDIT'] / app_train['AMT_INCOME_TOTAL']\napp_train['ANNUITY_INCOME_PERCENT'] = app_train['AMT_ANNUITY'] / app_train['AMT_INCOME_TOTAL']\napp_train['CREDIT_TERM'] = app_train['AMT_ANNUITY'] / app_train['AMT_CREDIT']\napp_train['DAYS_EMPLOYED_PERCENT'] = app_train['DAYS_EMPLOYED'] / app_train['DAYS_BIRTH']\n\napp_test['CREDIT_INCOME_PERCENT'] = app_test['AMT_CREDIT'] / app_test['AMT_INCOME_TOTAL']\napp_test['ANNUITY_INCOME_PERCENT'] = app_test['AMT_ANNUITY'] / app_test['AMT_INCOME_TOTAL']\napp_test['CREDIT_TERM'] = app_test['AMT_ANNUITY'] / app_test['AMT_CREDIT']\napp_test['DAYS_EMPLOYED_PERCENT'] = app_test['DAYS_EMPLOYED'] / app_test['DAYS_BIRTH']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MERGE WITH APP**","metadata":{}},{"cell_type":"code","source":"app_train = app_train.merge(bureau_agg_numeric, on = 'SK_ID_CURR', how = 'left')\napp_train = app_train.merge(bureau_agg_categorical, on = 'SK_ID_CURR', how = 'left') \n\napp_test = app_test.merge(bureau_agg_numeric, on = 'SK_ID_CURR', how = 'left')\napp_test = app_test.merge(bureau_agg_categorical, on = 'SK_ID_CURR', how = 'left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = app_train.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr[\"TARGET\"].sort_values(ascending=False)[:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr[\"TARGET\"].sort_values(ascending=True)[:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure = plt.figure(figsize = (15, 5))\nfigure.suptitle('DAYS_CREDIT', fontsize=16)\nplt.grid(linestyle='-')\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'bureau_DAYS_CREDIT_mean'], label = 'target == 0')\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'bureau_DAYS_CREDIT_mean'], label = 'target == 1')\nfigure.legend(loc=\"center\",prop={'size': 15}) \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure = plt.figure(figsize = (15, 5))\nfigure.suptitle('MONTHS_BALANCE min_y_mean', fontsize=16)\nplt.grid(linestyle='-')\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'bureau_bureau_balance_MONTHS_BALANCE_min_mean'], label = 'target == 0')\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'bureau_bureau_balance_MONTHS_BALANCE_min_mean'], label = 'target == 1')\nfigure.legend(loc=\"center\",prop={'size': 15}) \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Pipelines**","metadata":{}},{"cell_type":"code","source":"label_encoder_vars = [col for col in app_train.select_dtypes(\"object\").columns if len(app_train[col].unique()) <= 2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dummies_vars = [col for col in app_train.select_dtypes(\"object\").columns if len(app_train[col].unique()) > 2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_vars = [col for col in app_train.select_dtypes(\"number\").columns if col not in [\"SK_ID_CURR\", \"TARGET\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_treino = app_train.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1)\ny_treino = app_train[\"TARGET\"] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dummies\ndummies_pipe = Pipeline(steps=[         \n  ('one_hot_encoder', OneHotEncoder())\n])\n\n# ordinal encoder\nordinal_encoder_pipe = Pipeline(steps=[ \n  (\"label_encoder\", OrdinalEncoder())\n])\n\n# standard scaler\nnumerical_pipe = Pipeline(steps=[ \n  (\"standard_scaler\", StandardScaler())\n])\n\n# Polynomial Features\npolynomial_pipe = Pipeline(steps=[ \n    (\"imputer_median\", SimpleImputer(strategy = 'median')),\n    (\"polynomial_pipe\", PolynomialFeatures(degree = 3))\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poly_colums = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']\n# column transformer\ncolumn_transformer = ColumnTransformer(transformers=[    \n    (\"cat_label_encoder\", ordinal_encoder_pipe, label_encoder_vars),\n    (\"cat_dummies\", dummies_pipe, dummies_vars),\n    (\"numerical\", numerical_pipe, numerical_vars),    \n    (\"polynomial\",polynomial_pipe,poly_colums),\n])\n\nX_treino_transf = column_transformer.fit_transform(X_treino)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_treino_transf, y_treino, test_size=0.33, random_state=42)\n# Create the model\nmodel = lgb.LGBMClassifier(n_estimators=2000, objective = 'binary', \n                           class_weight = 'balanced', learning_rate = 0.05, \n                           reg_alpha = 0.1, reg_lambda = 0.1, \n                           subsample = 0.8, n_jobs = -1, random_state = 50)\n\n# Train the model\nmodel.fit(X_treino_transf, y_treino, eval_metric = 'auc',\n          eval_set = [(X_train, y_train), (X_test, y_test)],\n          eval_names = ['train', 'valid'],\n          early_stopping_rounds = 100, verbose = 200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nprob = model.predict_proba(X_treino_transf)[:,1]\nprint(\"ROC_AUC_SCORE\",roc_auc_score(y_treino, prob))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nimport sklearn\ndef get_feature_names(column_transformer):\n    \"\"\"Get feature names from all transformers.\n    Returns\n    -------\n    feature_names : list of strings\n        Names of the features produced by transform.\n    \"\"\"\n    # Remove the internal helper function\n    #check_is_fitted(column_transformer)\n    \n    # Turn loopkup into function for better handling with pipeline later\n    def get_names(trans):\n        # >> Original get_feature_names() method\n        if trans == 'drop' or (\n                hasattr(column, '__len__') and not len(column)):\n            return []\n        if trans == 'passthrough':\n            if hasattr(column_transformer, '_df_columns'):\n                if ((not isinstance(column, slice))\n                        and all(isinstance(col, str) for col in column)):\n                    return column\n                else:\n                    return column_transformer._df_columns[column]\n            else:\n                indices = np.arange(column_transformer._n_features)\n                return ['x%d' % i for i in indices[column]]\n        if not hasattr(trans, 'get_feature_names'):\n        # >>> Change: Return input column names if no method avaiable\n            # Turn error into a warning\n            warnings.warn(\"Transformer %s (type %s) does not \"\n                                 \"provide get_feature_names. \"\n                                 \"Will return input column names if available\"\n                                 % (str(name), type(trans).__name__))\n            # For transformers without a get_features_names method, use the input\n            # names to the column transformer\n            if column is None:\n                return []\n            else:\n                return [name + \"__\" + f for f in column]\n\n        return [name + \"__\" + f for f in trans.get_feature_names()]\n    \n    ### Start of processing\n    feature_names = []\n    \n    # Allow transformers to be pipelines. Pipeline steps are named differently, so preprocessing is needed\n    if type(column_transformer) == Pipeline:\n        l_transformers = [(name, trans, None, None) for step, name, trans in column_transformer._iter()]\n    else:\n        # For column transformers, follow the original method\n        l_transformers = list(column_transformer._iter(fitted=True))\n    \n    \n    for name, trans, column, _ in l_transformers: \n        if type(trans) == Pipeline:\n            # Recursive call on pipeline\n            _names = get_feature_names(trans)\n            # if pipeline has no transformer that returns names\n            if len(_names)==0:\n                _names = [name + \"__\" + f for f in column]\n            feature_names.extend(_names)\n        else:\n            feature_names.extend(get_names(trans))\n    \n    return feature_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#feature_importance\nfeature_name_transf = get_feature_names(column_transformer)\nf_importance = [{'key': feature_name_transf[idx],'importance':importance} for idx,importance in enumerate(model.feature_importances_)]\nf_importance.sort(key = lambda x:x['importance'],reverse=True)\nimportance_df = pd.DataFrame(f_importance)\nfigure = plt.figure(figsize=(10,10)) \nsns.barplot(x=\"importance\", y=\"key\", data=importance_df[:20])\nfigure.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}