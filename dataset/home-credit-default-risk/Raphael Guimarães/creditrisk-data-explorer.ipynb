{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\napp_train = pd.read_csv(\"/kaggle/input/home-credit-default-risk/application_train.csv\")\napp_test = pd.read_csv(\"/kaggle/input/home-credit-default-risk/application_test.csv\")\n\ndata_frames = [app_train,app_test]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Correlações**\n<br>Coeficiente de correlação de Pearson:\n* 0.9 para mais ou para menos indica uma correlação muito forte.\\n\n* 0.7 a 0.9 positivo ou negativo indica uma correlação forte.\\n\n* 0.5 a 0.7 positivo ou negativo indica uma correlação moderada.\n* 0.3 a 0.5 positivo ou negativo indica uma correlação fraca.\n* 0 a 0.3 positivo ou negativo indica uma correlação desprezível.","metadata":{}},{"cell_type":"code","source":"correlations = app_train.corr()['TARGET']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlations.sort_values(ascending=False)[:15]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlations.sort_values()[:15]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Critério utilizado - Selecionar as 4 maiores correlações para feature engeneering**","metadata":{}},{"cell_type":"markdown","source":"**DAYS_BIRTH**","metadata":{}},{"cell_type":"code","source":"app_train['DAYS_BIRTH'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure = plt.figure(figsize = (20, 10))\nplt.grid(linestyle='-')\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'], label = 'target == 0')\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'], label = 'target == 1')\nfigure.legend(loc=\"center\",prop={'size': 15})\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_days(df):\n    df['DAYS_BIRTH']=abs(df['DAYS_BIRTH'])\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_frames = [transform_days(df) for df in data_frames]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**External Sources**","metadata":{}},{"cell_type":"code","source":"figure = plt.figure(figsize = (20, 10))\nfigure.suptitle('EXT_SOURCE_1', fontsize=16)\nplt.grid(linestyle='-')\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'EXT_SOURCE_1'], label = 'target == 0')\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'EXT_SOURCE_1'], label = 'target == 1')\nfigure.legend(loc=\"center\",prop={'size': 15}) \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure = plt.figure(figsize = (20, 10))\nfigure.suptitle('EXT_SOURCE_2', fontsize=16)\nplt.grid(linestyle='-')\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'EXT_SOURCE_2'], label = 'target == 0')\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'EXT_SOURCE_2'], label = 'target == 1')\nfigure.legend(loc=\"center\",prop={'size': 15}) \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure = plt.figure(figsize = (20, 10))\nfigure.suptitle('EXT_SOURCE_3', fontsize=16)\nplt.grid(linestyle='-')\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'EXT_SOURCE_3'], label = 'target == 0')\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'EXT_SOURCE_3'], label = 'target == 1')\nfigure.legend(loc=\"center\",prop={'size': 15}) \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poly_colums = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\nsns.heatmap(ext_data_corrs,annot=True,cmap='RdYlGn',linewidths=0.2) \nfig=plt.gcf()\nfig.set_size_inches(8,8)\nplt.xticks(rotation = 75); \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Pipeline**","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom category_encoders import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler,PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split \n\ndef get_collumn_transformer(df):\n\n    label_encoder_vars = [col for col in df.select_dtypes(\"object\").columns if len(df[col].unique()) <= 2]\n    dummies_vars = [col for col in df.select_dtypes(\"object\").columns if len(df[col].unique()) > 2]\n    numerical_vars = [col for col in df.select_dtypes(\"number\").columns if col not in [\"SK_ID_CURR\", \"TARGET\"]]\n\n\n    # dummies\n    dummies_pipe = Pipeline(steps=[  \n        ('one_hot_encoder', OneHotEncoder()),\n        ('median', SimpleImputer(strategy = 'most_frequent'))\n    ])\n    # ordinal encoder\n    ordinal_encoder_pipe = Pipeline(steps=[\n      (\"label_encoder\", OrdinalEncoder()),\n      ('median', SimpleImputer(strategy = 'most_frequent'))\n    ])\n    # standard scaler\n    numerical_pipe = Pipeline(steps=[\n        (\"standard_scaler\", StandardScaler()),\n        ('median', SimpleImputer(strategy = 'median')),\n    ])\n\n    polynomial_pipe = Pipeline(steps=[ \n        (\"imputer_median\", SimpleImputer(strategy = 'median')),\n        (\"polynomial_pipe\", PolynomialFeatures(degree = 3))\n    ])\n\n    column_transformer = ColumnTransformer(transformers=[    \n        (\"cat_label_encoder\", ordinal_encoder_pipe, label_encoder_vars),\n        (\"cat_dummies\", dummies_pipe, dummies_vars),\n        (\"numerical\", numerical_pipe, numerical_vars),   \n        (\"polynomial\",polynomial_pipe,poly_colums),\n    ])\n    return column_transformer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model():\n    X_treino = app_train.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1)\n    y_treino  = app_train[\"TARGET\"]\n    column_transformer = get_collumn_transformer(app_train)\n    X_treino_transf = column_transformer.fit_transform(X_treino)\n    X_train, X_test, y_train, y_test = train_test_split(X_treino_transf, y_treino, test_size=0.33, random_state=42) \n\n    # Create the model\n    import lightgbm as lgb\n    from sklearn.metrics import roc_auc_score\n    model = lgb.LGBMClassifier(n_estimators=2000, objective = 'binary', \n                               class_weight = 'balanced', learning_rate = 0.05, \n                               reg_alpha = 0.1, reg_lambda = 0.1, \n                               subsample = 0.8, n_jobs = -1, random_state = 50)\n\n    # Train the model\n    model.fit(X_treino_transf, y_treino, eval_metric = 'auc',\n              eval_set = [(X_train, y_train), (X_test, y_test)],\n              eval_names = ['train', 'valid'],\n              early_stopping_rounds = 100, verbose = False)\n\n\n\n    X_test_transf = column_transformer.transform(app_test.drop([\"SK_ID_CURR\",\"SK_ID_CURR\"],axis=1))\n    prob = model.predict_proba(X_test)[:,1]\n    print(\"ROC_AUC_SCORE\",roc_auc_score(y_test, prob))\n    return model,column_transformer\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nimport sklearn\ndef get_feature_names(column_transformer):\n    \"\"\"Get feature names from all transformers.\n    Returns\n    -------\n    feature_names : list of strings\n        Names of the features produced by transform.\n    \"\"\"\n    # Remove the internal helper function\n    #check_is_fitted(column_transformer)\n    \n    # Turn loopkup into function for better handling with pipeline later\n    def get_names(trans):\n        # >> Original get_feature_names() method\n        if trans == 'drop' or (\n                hasattr(column, '__len__') and not len(column)):\n            return []\n        if trans == 'passthrough':\n            if hasattr(column_transformer, '_df_columns'):\n                if ((not isinstance(column, slice))\n                        and all(isinstance(col, str) for col in column)):\n                    return column\n                else:\n                    return column_transformer._df_columns[column]\n            else:\n                indices = np.arange(column_transformer._n_features)\n                return ['x%d' % i for i in indices[column]]\n        if not hasattr(trans, 'get_feature_names'):\n        # >>> Change: Return input column names if no method avaiable\n            # Turn error into a warning\n            warnings.warn(\"Transformer %s (type %s) does not \"\n                                 \"provide get_feature_names. \"\n                                 \"Will return input column names if available\"\n                                 % (str(name), type(trans).__name__))\n            # For transformers without a get_features_names method, use the input\n            # names to the column transformer\n            if column is None:\n                return []\n            else:\n                return [name + \"__\" + f for f in column]\n\n        return [name + \"__\" + f for f in trans.get_feature_names()]\n    \n    ### Start of processing\n    feature_names = []\n    \n    # Allow transformers to be pipelines. Pipeline steps are named differently, so preprocessing is needed\n    if type(column_transformer) == Pipeline:\n        l_transformers = [(name, trans, None, None) for step, name, trans in column_transformer._iter()]\n    else:\n        # For column transformers, follow the original method\n        l_transformers = list(column_transformer._iter(fitted=True))\n    \n    \n    for name, trans, column, _ in l_transformers: \n        if type(trans) == Pipeline:\n            # Recursive call on pipeline\n            _names = get_feature_names(trans)\n            # if pipeline has no transformer that returns names\n            if len(_names)==0:\n                _names = [name + \"__\" + f for f in column]\n            feature_names.extend(_names)\n        else:\n            feature_names.extend(get_names(trans))\n    \n    return feature_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train model\nmodel,column_transformer = train_model()\n\n#feature_importance\nfeature_name_transf = get_feature_names(column_transformer)\nf_importance = [{'key': feature_name_transf[idx],'importance':importance} for idx,importance in enumerate(model.feature_importances_)]\nf_importance.sort(key = lambda x:x['importance'],reverse=True)\nimportance_df = pd.DataFrame(f_importance)\nfigure = plt.figure(figsize=(10,10)) \nsns.barplot(x=\"importance\", y=\"key\", data=importance_df[:20])\nfigure.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------\n# Table - application_{train|test}.csv\n# Col - AMT_INCOME_TOTAL\n# Des -  Income of the client\n# ----------\n# Table - application_{train|test}.csv\n# Col - AMT_CREDIT\n# Des -  Credit amount of the loan\n# ----------\n# Table - application_{train|test}.csv\n# Col - AMT_ANNUITY\n# Des -  Loan annuity\n# ----------\n\ndef domain_featuring(df):\n    df['CREDIT_INCOME_PERCENT'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n    df['ANNUITY_INCOME_PERCENT'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n    df['CREDIT_TERM'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n    df['DAYS_EMPLOYED_PERCENT'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_frames = [domain_featuring(df) for df in data_frames]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train model\nmodel,column_transformer = train_model()\n\n#feature_importance\nfeature_name_transf = get_feature_names(column_transformer)\nf_importance = [{'key': feature_name_transf[idx],'importance':importance} for idx,importance in enumerate(model.feature_importances_)]\nf_importance.sort(key = lambda x:x['importance'],reverse=True)\nimportance_df = pd.DataFrame(f_importance)\nfigure = plt.figure(figsize=(10,10)) \nsns.barplot(x=\"importance\", y=\"key\", data=importance_df[:20])\nfigure.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}