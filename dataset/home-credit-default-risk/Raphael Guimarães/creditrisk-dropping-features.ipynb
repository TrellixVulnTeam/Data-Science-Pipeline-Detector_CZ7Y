{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-05-25T01:47:43.384869Z","iopub.status.idle":"2021-05-25T01:47:43.385372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.pipeline import Pipeline \nfrom category_encoders import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler,PolynomialFeatures\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nimport lightgbm as lgb\nfrom sklearn.impute import SimpleImputer\nimport numpy as np \n\n\n\napp_train = pd.read_csv(\"/kaggle/input/home-credit-default-risk/application_train.csv\")\napp_test = pd.read_csv(\"/kaggle/input/home-credit-default-risk/application_test.csv\")\n\napp_train['CREDIT_INCOME_PERCENT'] = app_train['AMT_CREDIT'] / app_train['AMT_INCOME_TOTAL']\napp_train['ANNUITY_INCOME_PERCENT'] = app_train['AMT_ANNUITY'] / app_train['AMT_INCOME_TOTAL']\napp_train['CREDIT_TERM'] = app_train['AMT_ANNUITY'] / app_train['AMT_CREDIT']\napp_train['DAYS_EMPLOYED_PERCENT'] = app_train['DAYS_EMPLOYED'] / app_train['DAYS_BIRTH']\napp_test['CREDIT_INCOME_PERCENT'] = app_test['AMT_CREDIT'] / app_test['AMT_INCOME_TOTAL']\napp_test['ANNUITY_INCOME_PERCENT'] = app_test['AMT_ANNUITY'] / app_test['AMT_INCOME_TOTAL']\napp_test['CREDIT_TERM'] = app_test['AMT_ANNUITY'] / app_test['AMT_CREDIT']\napp_test['DAYS_EMPLOYED_PERCENT'] = app_test['DAYS_EMPLOYED'] / app_test['DAYS_BIRTH']\n\n# Read in bureau\nbureau = pd.read_csv('/kaggle/input/home-credit-default-risk/bureau.csv')\nbureau.head()\n\n#Read in bureau_balance\nbureau_balance = pd.read_csv('/kaggle/input/home-credit-default-risk/bureau_balance.csv')\nbureau_balance.head()\n\ndef agg_numeric(df, parent_var, df_name):\n    \"\"\"\n    Groups and aggregates the numeric values in a child dataframe\n    by the parent variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the child dataframe to calculate the statistics on\n        parent_var (string): \n            the parent variable used for grouping and aggregating\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated by the `parent_var` for \n            all numeric columns. Each observation of the parent variable will have \n            one row in the dataframe with the parent variable as the index. \n            The columns are also renamed using the `df_name`. Columns with all duplicate\n            values are removed. \n    \n    \"\"\"\n    \n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != parent_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    # Only want the numeric variables\n    parent_ids = df[parent_var].copy()\n    numeric_df = df.select_dtypes('number').copy()\n    numeric_df[parent_var] = parent_ids\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(parent_var).agg(['count', 'mean', 'max', 'min', 'sum'])\n\n    # Need to create new column names\n    columns = []\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        if var != parent_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n    \n    agg.columns = columns\n    \n    # Remove the columns with all redundant values\n    _, idx = np.unique(agg, axis = 1, return_index=True)\n    agg = agg.iloc[:, idx]\n    \n    return agg\n\ndef agg_categorical(df, parent_var, df_name):\n    \"\"\"\n    Aggregates the categorical features in a child dataframe\n    for each observation of the parent variable.\n    \n    Parameters\n    --------\n    df : dataframe \n        The dataframe to calculate the value counts for.\n        \n    parent_var : string\n        The variable by which to group and aggregate the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    df_name : string\n        Variable added to the front of column names to keep track of columns\n\n    \n    Return\n    --------\n    categorical : dataframe\n        A dataframe with aggregated statistics for each observation of the parent_var\n        The columns are also renamed and columns with duplicate values are removed.\n        \n    \"\"\"\n    \n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('object'))\n     \n\n    # Make sure to put the identifying id on the column\n    categorical[parent_var] = df[parent_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(parent_var).agg(['sum', 'count', 'mean',\"min\",\"max\"])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['sum', 'count', 'mean',\"min\",\"max\"]:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    # Remove duplicate columns by values\n    _, idx = np.unique(categorical, axis = 1, return_index = True)\n    categorical = categorical.iloc[:, idx]\n    \n    return categorical\n\nbureau_balance_agg_numeric = agg_numeric(bureau_balance,\"SK_ID_BUREAU\",'bureau_balance')\n\nbureau_balance_agg_categorical = agg_categorical(bureau_balance,\"SK_ID_BUREAU\",'bureau_balance')\n\n#merge with bureau\nbureau = bureau.merge(bureau_balance_agg_numeric, on = 'SK_ID_BUREAU', how = 'left')\nbureau = bureau.merge(bureau_balance_agg_categorical, on = 'SK_ID_BUREAU', how = 'left')\n\nbureau_agg_numeric = agg_numeric(bureau,\"SK_ID_CURR\",'bureau')\nbureau_agg_categorical = agg_categorical(bureau,\"SK_ID_CURR\",'bureau')\n\n#merge with app\napp_train = app_train.merge(bureau_agg_numeric, on = 'SK_ID_CURR', how = 'left')\napp_train = app_train.merge(bureau_agg_categorical, on = 'SK_ID_CURR', how = 'left') \n\napp_test = app_test.merge(bureau_agg_numeric, on = 'SK_ID_CURR', how = 'left')\napp_test = app_test.merge(bureau_agg_categorical, on = 'SK_ID_CURR', how = 'left')\n\n# **Domain Features**\n\napp_train['CREDIT_INCOME_PERCENT'] = app_train['AMT_CREDIT'] / app_train['AMT_INCOME_TOTAL']\napp_train['ANNUITY_INCOME_PERCENT'] = app_train['AMT_ANNUITY'] / app_train['AMT_INCOME_TOTAL']\napp_train['CREDIT_TERM'] = app_train['AMT_ANNUITY'] / app_train['AMT_CREDIT']\napp_train['DAYS_EMPLOYED_PERCENT'] = app_train['DAYS_EMPLOYED'] / app_train['DAYS_BIRTH']\n\napp_test['CREDIT_INCOME_PERCENT'] = app_test['AMT_CREDIT'] / app_test['AMT_INCOME_TOTAL']\napp_test['ANNUITY_INCOME_PERCENT'] = app_test['AMT_ANNUITY'] / app_test['AMT_INCOME_TOTAL']\napp_test['CREDIT_TERM'] = app_test['AMT_ANNUITY'] / app_test['AMT_CREDIT']\napp_test['DAYS_EMPLOYED_PERCENT'] = app_test['DAYS_EMPLOYED'] / app_test['DAYS_BIRTH']\n\n# **Pipelines**\n\nlabel_encoder_vars = [col for col in app_train.select_dtypes(\"object\").columns if len(app_train[col].unique()) <= 2]\ndummies_vars = [col for col in app_train.select_dtypes(\"object\").columns if len(app_train[col].unique()) > 2]\nnumerical_vars = [col for col in app_train.select_dtypes(\"number\").columns if col not in [\"SK_ID_CURR\", \"TARGET\"]]\npoly_colums = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']\n\n# dummies\ndummies_pipe = Pipeline(steps=[    \n  ('one_hot_encoder', OneHotEncoder())\n])\n\n# ordinal encoder\nordinal_encoder_pipe = Pipeline(steps=[\n  (\"label_encoder\", OrdinalEncoder())\n])\n\n# standard scaler\nnumerical_pipe = Pipeline(steps=[\n  (\"standard_scaler\", StandardScaler())\n])\n\n# Polynomial Features\npolynomial_pipe = Pipeline(steps=[ \n    (\"imputer_median\", SimpleImputer(strategy = 'median')),\n    (\"polynomial_pipe\", PolynomialFeatures(degree = 3))\n])\n\n# column transformer\ncolumn_transformer = ColumnTransformer(transformers=[    \n    (\"cat_label_encoder\", ordinal_encoder_pipe, label_encoder_vars),\n    (\"cat_dummies\", dummies_pipe, dummies_vars),\n    (\"numerical\", numerical_pipe, numerical_vars),    \n    (\"polynomial\",polynomial_pipe,poly_colums),\n])\n\nX_treino = app_train.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1)\ny_treino = app_train[\"TARGET\"] \n\ncolumn_transformer.fit(X_treino)\n\nimport warnings\nimport sklearn\ndef get_feature_names(column_transformer):\n    \"\"\"Get feature names from all transformers.\n    Returns\n    -------\n    feature_names : list of strings\n        Names of the features produced by transform.\n    \"\"\"\n    # Remove the internal helper function\n    #check_is_fitted(column_transformer)\n    \n    # Turn loopkup into function for better handling with pipeline later\n    def get_names(trans):\n        # >> Original get_feature_names() method\n        if trans == 'drop' or (\n                hasattr(column, '__len__') and not len(column)):\n            return []\n        if trans == 'passthrough':\n            if hasattr(column_transformer, '_df_columns'):\n                if ((not isinstance(column, slice))\n                        and all(isinstance(col, str) for col in column)):\n                    return column\n                else:\n                    return column_transformer._df_columns[column]\n            else:\n                indices = np.arange(column_transformer._n_features)\n                return ['x%d' % i for i in indices[column]]\n        if not hasattr(trans, 'get_feature_names'):\n        # >>> Change: Return input column names if no method avaiable\n            # Turn error into a warning\n            warnings.warn(\"Transformer %s (type %s) does not \"\n                                 \"provide get_feature_names. \"\n                                 \"Will return input column names if available\"\n                                 % (str(name), type(trans).__name__))\n            # For transformers without a get_features_names method, use the input\n            # names to the column transformer\n            if column is None:\n                return []\n            else:\n                return [name + \"__\" + f for f in column]\n\n        return [name + \"__\" + f for f in trans.get_feature_names()]\n    \n    ### Start of processing\n    feature_names = []\n    \n    # Allow transformers to be pipelines. Pipeline steps are named differently, so preprocessing is needed\n    if type(column_transformer) == Pipeline:\n        l_transformers = [(name, trans, None, None) for step, name, trans in column_transformer._iter()]\n    else:\n        # For column transformers, follow the original method\n        l_transformers = list(column_transformer._iter(fitted=True))\n    \n    \n    for name, trans, column, _ in l_transformers: \n        if type(trans) == Pipeline:\n            # Recursive call on pipeline\n            _names = get_feature_names(trans)\n            # if pipeline has no transformer that returns names\n            if len(_names)==0:\n                _names = [name + \"__\" + f for f in column]\n            feature_names.extend(_names)\n        else:\n            feature_names.extend(get_names(trans))\n    \n    return feature_names\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-25T01:47:43.488224Z","iopub.execute_input":"2021-05-25T01:47:43.488572Z","iopub.status.idle":"2021-05-25T01:49:41.998508Z","shell.execute_reply.started":"2021-05-25T01:47:43.488528Z","shell.execute_reply":"2021-05-25T01:49:41.997208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_treino_transf = column_transformer.fit_transform(X_treino)\nfeature_name_transf = get_feature_names(column_transformer)\nX_treino_transf = pd.DataFrame(X_treino_transf, \n             columns=feature_name_transf) ","metadata":{"execution":{"iopub.status.busy":"2021-05-25T01:49:42.000027Z","iopub.execute_input":"2021-05-25T01:49:42.000391Z","iopub.status.idle":"2021-05-25T01:49:57.696964Z","shell.execute_reply.started":"2021-05-25T01:49:42.000359Z","shell.execute_reply":"2021-05-25T01:49:57.695481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_app_test = app_test.drop([\"SK_ID_CURR\"], axis=1)\n\nX_app_test_transf = column_transformer.transform(X_app_test)\nX_app_test_transf = pd.DataFrame(X_treino_transf, \n             columns=feature_name_transf)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T01:49:57.698284Z","iopub.execute_input":"2021-05-25T01:49:57.698577Z","iopub.status.idle":"2021-05-25T01:49:58.81526Z","shell.execute_reply.started":"2021-05-25T01:49:57.698551Z","shell.execute_reply":"2021-05-25T01:49:58.81402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_treino_transf.to_csv(\"app_train_transf.csv\")\nX_app_test_transf.to_csv(\"app_test_transf.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-25T01:49:58.816448Z","iopub.execute_input":"2021-05-25T01:49:58.816756Z","iopub.status.idle":"2021-05-25T01:57:39.539324Z","shell.execute_reply.started":"2021-05-25T01:49:58.816727Z","shell.execute_reply":"2021-05-25T01:57:39.537995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_treino_transf.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T01:57:39.701033Z","iopub.execute_input":"2021-05-25T01:57:39.701812Z","iopub.status.idle":"2021-05-25T01:58:02.827929Z","shell.execute_reply.started":"2021-05-25T01:57:39.701763Z","shell.execute_reply":"2021-05-25T01:58:02.827089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the model\nmodel_all = lgb.LGBMClassifier(n_estimators=2000, objective = 'binary', \n                           class_weight = 'balanced',  learning_rate=0.05, \n                           reg_alpha = 0.1, reg_lambda = 0.1, \n                           subsample = 0.8, n_jobs = -1, random_state = 50)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T03:58:48.929312Z","iopub.execute_input":"2021-05-25T03:58:48.929846Z","iopub.status.idle":"2021-05-25T03:58:48.936669Z","shell.execute_reply.started":"2021-05-25T03:58:48.929813Z","shell.execute_reply":"2021-05-25T03:58:48.935224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nX_train, X_test, y_train, y_test = train_test_split(X_treino_transf, y_treino, test_size=0.33, random_state=42)\nmodel_all.fit(X_treino_transf, y_treino, eval_metric = 'auc',\n          eval_set = [(X_train, y_train), (X_test, y_test)],\n          eval_names = ['train', 'valid'],\n          early_stopping_rounds = 100, verbose = 200)\nprob = model_all.predict_proba(X_test)[:,1]\nprint(\"ROC_AUC_SCORE\",roc_auc_score(y_test, prob))","metadata":{"execution":{"iopub.status.busy":"2021-05-25T03:58:51.140255Z","iopub.execute_input":"2021-05-25T03:58:51.140655Z","iopub.status.idle":"2021-05-25T04:06:58.411579Z","shell.execute_reply.started":"2021-05-25T03:58:51.140622Z","shell.execute_reply":"2021-05-25T04:06:58.409337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance_all = [{'key': feature_name_transf[idx],'importance':importance} for idx,importance in enumerate(model_all.feature_importances_)]\nimportance_all.sort(key = lambda x:x['importance'],reverse=True)\nimportance_all_df = pd.DataFrame(importance_all)\nimportance_all_df","metadata":{"execution":{"iopub.status.busy":"2021-05-25T04:16:19.726502Z","iopub.execute_input":"2021-05-25T04:16:19.727131Z","iopub.status.idle":"2021-05-25T04:16:19.751864Z","shell.execute_reply.started":"2021-05-25T04:16:19.727076Z","shell.execute_reply":"2021-05-25T04:16:19.750676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model,X,y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n    model.fit(X, y, eval_metric = 'auc',\n          eval_set = [(X_train, y_train), (X_test, y_test)],\n          eval_names = ['train', 'valid'],\n          early_stopping_rounds = 100, verbose = False)\n    prob = model.predict_proba(X_test)[:,1]\n    return roc_auc_score(y_test,prob)\n\ndef dropping_features(importance):\n    \n    actual_error = 0\n    for i in range(1,len(importance)):\n        print(\"Número de Features Utilizadas\",i )\n        drop_features =  [f['key'] for f in importance[i:]]\n        roc_auc = train_model(model,X_treino_transf.drop(drop_features,axis=1), y_treino)\n        importance[i-1]['error'] = roc_auc\n        print(\"roc_auc_score \",roc_auc)\n        importance[i]['score'] = roc_auc\n        if abs(actual_error - roc_auc) < 1/10000:            \n            break\n        actual_error = roc_auc\n    return importance\n        \nimportance = dropping_features(importance_all)    ","metadata":{"execution":{"iopub.status.busy":"2021-05-25T02:13:34.613005Z","iopub.execute_input":"2021-05-25T02:13:34.613413Z","iopub.status.idle":"2021-05-25T03:30:20.659087Z","shell.execute_reply.started":"2021-05-25T02:13:34.613381Z","shell.execute_reply":"2021-05-25T03:30:20.657747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Features Utilizadas no modelo final\") \n[f['key'] for f in importance_all[:43] if 'importance' in f ]","metadata":{"execution":{"iopub.status.busy":"2021-05-25T04:35:30.398725Z","iopub.execute_input":"2021-05-25T04:35:30.399126Z","iopub.status.idle":"2021-05-25T04:35:30.407636Z","shell.execute_reply.started":"2021-05-25T04:35:30.399094Z","shell.execute_reply":"2021-05-25T04:35:30.406741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_43_features_A = X_treino_transf.drop([f['key'] for f in importance_all[43:] if 'importance' in f ],axis=1)\nX_43_features_A.columns","metadata":{"execution":{"iopub.status.busy":"2021-05-25T04:34:35.820917Z","iopub.execute_input":"2021-05-25T04:34:35.821328Z","iopub.status.idle":"2021-05-25T04:34:35.953194Z","shell.execute_reply.started":"2021-05-25T04:34:35.821295Z","shell.execute_reply":"2021-05-25T04:34:35.95228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nimportance_43 = [{'key':X_43_features_A.columns[idx], 'importance':importance} for idx,importance in enumerate(model.feature_importances_)]\nimportance_43.sort(key = lambda x:x['importance'],reverse=True)\n\nfigure = plt.figure(figsize=(10,10)) \nsns.barplot(x=\"importance\", y=\"key\", data=pd.DataFrame(importance_43))\nfigure.tight_layout()\nfigure.savefig(\"43importance.png\")","metadata":{"execution":{"iopub.status.busy":"2021-05-25T04:40:13.14276Z","iopub.execute_input":"2021-05-25T04:40:13.143226Z","iopub.status.idle":"2021-05-25T04:40:14.673334Z","shell.execute_reply.started":"2021-05-25T04:40:13.143174Z","shell.execute_reply":"2021-05-25T04:40:14.67165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfigure = plt.figure(figsize=(10,10)) \nsns.barplot(x=\"importance\", y=\"key\", data=pd.DataFrame(importance_all[:20]))\nfigure.tight_layout()\nfigure.savefig(\"importance_all_graph.png\")","metadata":{"execution":{"iopub.status.busy":"2021-05-25T04:45:36.70724Z","iopub.execute_input":"2021-05-25T04:45:36.707593Z","iopub.status.idle":"2021-05-25T04:45:37.255561Z","shell.execute_reply.started":"2021-05-25T04:45:36.707564Z","shell.execute_reply":"2021-05-25T04:45:37.254223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nimportance_df = pd.DataFrame([f for f in importance if 'score' in f ])\nfigure = plt.figure(figsize=(20,10)) \n\nsns.lineplot(x=\"key\", y=\"score\", data=importance_df)\nplt.xticks(rotation=90)\nfigure.tight_layout()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-19T23:43:18.591146Z","iopub.execute_input":"2021-05-19T23:43:18.591418Z","iopub.status.idle":"2021-05-19T23:43:19.399517Z","shell.execute_reply.started":"2021-05-19T23:43:18.591392Z","shell.execute_reply":"2021-05-19T23:43:19.398431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_features =  [f['key'] for f in importance_all[43:] if 'importance' in f ]\nX = X_treino_transf.drop(drop_features,axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y_treino, test_size=0.33, random_state=42)\n# model.fit(X, y_treino, eval_metric = 'auc',\n#           eval_set = [(X_train, y_train), (X_test, y_test)],\n#           eval_names = ['train', 'valid'],\n#           early_stopping_rounds = 100, verbose = False)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T04:49:11.420303Z","iopub.execute_input":"2021-05-25T04:49:11.420796Z","iopub.status.idle":"2021-05-25T04:49:11.572939Z","shell.execute_reply.started":"2021-05-25T04:49:11.420766Z","shell.execute_reply":"2021-05-25T04:49:11.572128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle","metadata":{"execution":{"iopub.status.busy":"2021-05-19T23:45:17.946868Z","iopub.execute_input":"2021-05-19T23:45:17.947211Z","iopub.status.idle":"2021-05-19T23:45:17.95206Z","shell.execute_reply.started":"2021-05-19T23:45:17.94718Z","shell.execute_reply":"2021-05-19T23:45:17.95118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_app_test_transf.drop( [f['key'] for f in importance[43:]],axis=1)\nX.to_csv(\"test_features_43.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T04:46:58.022059Z","iopub.execute_input":"2021-05-25T04:46:58.022423Z","iopub.status.idle":"2021-05-25T04:46:58.076629Z","shell.execute_reply.started":"2021-05-25T04:46:58.022395Z","shell.execute_reply":"2021-05-25T04:46:58.074687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.to_csv(\"features_43.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-19T23:45:42.330087Z","iopub.execute_input":"2021-05-19T23:45:42.330516Z","iopub.status.idle":"2021-05-19T23:46:06.512978Z","shell.execute_reply.started":"2021-05-19T23:45:42.330463Z","shell.execute_reply":"2021-05-19T23:46:06.512063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n# save the model to disk\nfilename = \"model_LGBMClassifier_43_features.pkl\"  \n\nwith open(filename, 'wb') as file:  \n    pickle.dump(model, file)\n\nwith open(\"features_list\",\"wb\") as file:\n    pickle.dump(drop_features, file)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-19T23:46:06.514448Z","iopub.execute_input":"2021-05-19T23:46:06.515082Z","iopub.status.idle":"2021-05-19T23:46:06.855495Z","shell.execute_reply.started":"2021-05-19T23:46:06.515038Z","shell.execute_reply":"2021-05-19T23:46:06.854404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shap  # package used to calculate Shap values\nshap.initjs()\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(model)\nNROWS = 100\n# Calculate Shap values\nshap_values = explainer.shap_values(X.iloc[:NROWS])","metadata":{"execution":{"iopub.status.busy":"2021-05-25T04:49:21.522958Z","iopub.execute_input":"2021-05-25T04:49:21.523478Z","iopub.status.idle":"2021-05-25T04:49:26.021039Z","shell.execute_reply.started":"2021-05-25T04:49:21.523446Z","shell.execute_reply":"2021-05-25T04:49:26.020247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.force_plot(explainer.expected_value[1], shap_values[1], X.iloc[:NROWS])","metadata":{"execution":{"iopub.status.busy":"2021-05-25T04:49:26.076635Z","iopub.execute_input":"2021-05-25T04:49:26.077179Z","iopub.status.idle":"2021-05-25T04:49:26.170772Z","shell.execute_reply.started":"2021-05-25T04:49:26.077135Z","shell.execute_reply":"2021-05-25T04:49:26.169906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.force_plot(explainer.expected_value[1], shap_values[1][0], X.iloc[0])","metadata":{"execution":{"iopub.status.busy":"2021-05-25T04:49:30.705643Z","iopub.execute_input":"2021-05-25T04:49:30.706151Z","iopub.status.idle":"2021-05-25T04:49:30.715484Z","shell.execute_reply.started":"2021-05-25T04:49:30.70612Z","shell.execute_reply":"2021-05-25T04:49:30.714209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model, random_state=1).fit(X.iloc[:NROWS], y_treino[:NROWS])\neli5.show_weights(perm, feature_names = X.columns.tolist())\n","metadata":{"execution":{"iopub.status.busy":"2021-05-25T04:49:33.66991Z","iopub.execute_input":"2021-05-25T04:49:33.670337Z","iopub.status.idle":"2021-05-25T04:49:37.280946Z","shell.execute_reply.started":"2021-05-25T04:49:33.670302Z","shell.execute_reply":"2021-05-25T04:49:37.279536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}