{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install dtreeviz -q\n! pip install dalex -q","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's use a decision tree as a baseline, then use Random Forest for the main model to have a decent performance without a lot of hyperparameter tuning. GBM variants usually take longer to get to optimal performance, so I'd stick with a Random Forest for good performance with less time.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport dalex as dx\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nfrom dtreeviz.trees import dtreeviz\nfrom functools import partial\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.inspection import permutation_importance\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/home-credit-default-risk/application_train.csv\")\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train[:200000]\nX_val = train[200000:250000]\nX_test = train[250000:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_data(df):\n    return df.drop(\"TARGET\", axis=1), df[\"TARGET\"]\nX_train, y_train = split_data(X_train)\nX_val, y_val = split_data(X_val)\nX_test, y_test = split_data(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's start by preprocessing the data we have by:\n1. Creating is_null columns\n2. Filling in nulls in the column with the median\n3. Converting strings to categories","metadata":{}},{"cell_type":"code","source":"def fill_in_nulls_train(df):\n    null_cols = set(df.columns[df.isna().any()].tolist())\n    numeric_cols = set(df.select_dtypes(include= np.number).columns.tolist())\n    cols_to_fill = null_cols.intersection(numeric_cols)\n    null_dict = {}\n    for col in cols_to_fill:\n        df[\"{}_na\".format(col)] = df[col].isna()\n        median = df[col].median()\n        df[col] = df[col].fillna(median)\n        null_dict[col] = median\n    return df, null_dict\nX_train, nas = fill_in_nulls_train(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fill_in_nulls_test(df, nas):\n    for col, val in nas.items():\n        df[\"{}_na\".format(col)] = df[col].isna()\n        df[col] = df[col].fillna(val)\n    return df\nX_val = fill_in_nulls_test(X_val, nas)\nX_test = fill_in_nulls_test(X_test, nas)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_cats_train(df):\n    cat_codes = {}\n    for col in df.select_dtypes(\"object\").columns:\n        df[col] = df[col].astype(\"category\")\n        cat_codes[col] = {v: k for k, v in dict( enumerate(df[col].cat.categories ) ).items()}\n        df[col] = df[col].cat.codes\n    return df, cat_codes\nX_train, cats = convert_to_cats_train(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_cats_test(df, cats):\n    for col, mapping in cats.items():\n        df[col] = df[col].map(mapping).fillna(-1)\n    return df\nX_val = convert_to_cats_test(X_val, cats)\nX_test = convert_to_cats_test(X_test, cats)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have gotten the basic preprocessing out of the way, let's start by looking at a depth 3 decision tree to have a baseline","metadata":{}},{"cell_type":"code","source":"clf = tree.DecisionTreeClassifier(max_depth=3, class_weight=\"balanced\")  # limit depth of tree\nclf.fit(X_train, y_train)\n\nviz = dtreeviz(clf, \n               X_train, \n               y_train,\n               target_name='target',\n               feature_names=X_train.columns)  \n              \nviz","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see that the most indicative features are unsurprisingly external credit scores, though we see that the best credit scores are 2 and 3.\n\nNow, let's see how good the ROC/AUC is with this decision tree.","metadata":{}},{"cell_type":"code","source":"def metric_checker(clf, metric_fn, X_train, y_train, X_val, y_val, proba=True):\n    if proba:\n        preds_train = clf.predict_proba(X_train)[:, 1]\n        preds_val = clf.predict_proba(X_val)[:, 1]\n    else:\n        preds_train = clf.predict(X_train)\n        preds_val = clf.predict(X_val)\n    return metric_fn(y_train, preds_train), metric_fn(y_val, preds_val)\nroc_auc_checker = partial(metric_checker, metric_fn=roc_auc_score,  X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val)\nroc_auc_checker(clf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def breakeven_interest(default_rate):\n    return (1/(1-default_rate))-1\n\ndef print_extended_classification_report(clf, X, y):\n    print(classification_report(y, clf.predict(X)))\n    tn, fp, fn, tp = confusion_matrix(y, clf.predict(X)).ravel()\n    orig_default = y.sum()/y.count()\n    new_default = fn/(tn + fn)\n    diff = abs(new_default - orig_default)/orig_default\n    print(\"original default rate: {:.2%}\".format(orig_default) )\n    print(\"new default rate: {:.2%}\".format(new_default))\n    print(\"difference in default: {:.2%}\".format(diff) )\n    print(\"decrease in eligibility: {:.2%}\".format((tp+fp)/y.shape[0]))\n    print(\"decrease in default/decrease in eligibility: {:.5}\".format(diff/((tp+fp)/y.shape[0])))\n    print(\"old breakeven interest: {:.2%}\".format(breakeven_interest(orig_default)))\n    print(\"new breakeven interest: {:.2%}\".format(breakeven_interest(new_default)))\n\nprint_extended_classification_report(clf, X_val, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With a quick baseline, we can see that we can have a model that reduces the default rate from 8.51% to 4.51% making the breakeven interest rate 4.72%. However, we decrease the eligible customers by 36.88% for a decrease in default of 44.27%, which is a good tradeoff. The good news here is that lower interest rates can attract more customers, but this can backfire if people keep getting rejected when applying for a loan.\n\nNow that we have a decent baseline, let's start by seeing what we can do with a Random Forest model.","metadata":{}},{"cell_type":"code","source":"clf = RandomForestClassifier(n_jobs=-1, max_samples=50000, class_weight=\"balanced\", min_samples_leaf=25)\nclf.fit(X_train, y_train)\nroc_auc_checker(clf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's set up gridsearch to find a good set of parameters given \"rule of thumb\" values","metadata":{}},{"cell_type":"code","source":"# res = []\n# for leaves in [1, 3, 5, 10, 25, 100]:\n#      for feats in [None, \"sqrt\", \"log2\", 0.5]:\n#             for max_samples in [0.25, 0.5, 0.623, None]:\n#                 clf = RandomForestClassifier(class_weight=\"balanced\", n_jobs=-1, random_state=42, n_estimators=50,\n#                                              min_samples_leaf=leaves, max_features=feats, max_samples=max_samples)\n#                 clf.fit(X_train, y_train)\n#                 train_auc, val_auc = roc_auc_checker(clf)\n#                 res.append({\n#                     \"min_samples_leaf\": leaves,\n#                     \"max_features\": feats,\n#                     \"max_samples\": max_samples,\n#                     \"train\": train_auc,\n#                     \"val\": val_auc\n#                 })\n#                 print(val_auc)\n# res_df = pd.DataFrame.from_records(res)\n# res_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# res_df.sort_values(\"val\", ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best performing model appears to be min_samples_leaf = 100, max_features = sqrt and no max_samples. The code is commented out for now to orevent rerunning. Now let's use that in our model and understand the features better.","metadata":{}},{"cell_type":"code","source":"clf = RandomForestClassifier(class_weight=\"balanced\", n_jobs=-1, random_state=0, n_estimators=100,\n                                             min_samples_leaf=100, max_features='sqrt')\nclf.fit(X_train, y_train)\nroc_auc_checker(clf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_extended_classification_report(clf, X_val, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it becomes quite interesting. We have a model that performs better based on the classification report with a higher AUC, but the breakeven interest is now ~6%. However, we also see that we reduce eligibility by ~15%, which is a better outcome. Compared to decreasing default by roughly 1.2% for every percentage point we lose in eligibility for the decision tree, this new model can decrease default by ~2% for every percentage point we lose in eligibility. \n\nNext, let's see if we can further improve the model by understanding the features and trimming ones that can be potential sources of leakage","metadata":{}},{"cell_type":"code","source":"# df_tv = pd.concat([X_train.copy().assign(target=0), X_val.copy().assign(target=1)])\n# X_tv = df_tv.drop(\"target\", axis=1)\n# y_tv = df_tv[\"target\"]\n# X_train_tv, X_test_tv, y_train_tv, y_test_tv = train_test_split(X_tv, y_tv, test_size=0.33, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clf_tv = RandomForestClassifier(class_weight=\"balanced\", n_jobs=-1, random_state=0, n_estimators=100,\n#                                              min_samples_leaf=100, max_features='sqrt')\n\n# clf_tv.fit(X_train_tv, y_train_tv)\n# roc_auc_score(y_test_tv, clf_tv.predict_proba(X_test_tv)[:,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importances_df = pd.DataFrame({\"features\": X_train_tv.columns, \"importances\": clf_tv.feature_importances_}).sort_values(\"importances\", ascending=False)\n# cols_to_drop = importances_df.head(10).features.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clf_tv = RandomForestClassifier(class_weight=\"balanced\", n_jobs=-1, random_state=0, n_estimators=100,\n#                                              min_samples_leaf=100, max_features='sqrt')\n\n# clf_tv.fit(X_train_tv.drop(cols_to_drop, axis=1), y_train_tv)\n# roc_auc_score(y_test_tv, clf_tv.predict_proba(X_test_tv.drop(cols_to_drop, axis=1))[:,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importances_df = pd.DataFrame({\"features\": X_train_tv.drop(cols_to_drop, axis=1).columns, \"importances\": clf_tv.feature_importances_}).sort_values(\"importances\", ascending=False)\n# importances_df.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cols_to_drop +=  importances_df.head(3).features.tolist()\n# clf_tv = RandomForestClassifier(class_weight=\"balanced\", n_jobs=-1, random_state=0, n_estimators=100,\n#                                              min_samples_leaf=100, max_features='sqrt')\n\n# clf_tv.fit(X_train_tv.drop(cols_to_drop, axis=1), y_train_tv)\n# roc_auc_score(y_test_tv, clf_tv.predict_proba(X_test_tv.drop(cols_to_drop, axis=1))[:,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importances_df = pd.DataFrame({\"features\": X_train_tv.drop(cols_to_drop, axis=1).columns, \"importances\": clf_tv.feature_importances_}).sort_values(\"importances\", ascending=False)\n# importances_df.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cols_to_drop +=  importances_df.head(4).features.tolist()\n# clf_tv = RandomForestClassifier(class_weight=\"balanced\", n_jobs=-1, random_state=0, n_estimators=100,\n#                                              min_samples_leaf=100, max_features='sqrt')\n\n# clf_tv.fit(X_train_tv.drop(cols_to_drop, axis=1), y_train_tv)\n# roc_auc_score(y_test_tv, clf_tv.predict_proba(X_test_tv.drop(cols_to_drop, axis=1))[:,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright we've now taken out the features that are most likely going to ruin the model through leakage, let's see how the model performs now.","metadata":{}},{"cell_type":"code","source":"cols_to_drop = ['NAME_INCOME_TYPE',\n 'FLAG_OWN_CAR',\n 'FLAG_OWN_REALTY',\n 'NAME_HOUSING_TYPE',\n 'CODE_GENDER',\n 'ORGANIZATION_TYPE',\n 'NAME_FAMILY_STATUS',\n 'NAME_CONTRACT_TYPE',\n 'SK_ID_CURR',\n 'WEEKDAY_APPR_PROCESS_START',\n 'NAME_TYPE_SUITE',\n 'NAME_EDUCATION_TYPE',\n 'OCCUPATION_TYPE',\n 'EMERGENCYSTATE_MODE',\n 'HOUSETYPE_MODE',\n 'WALLSMATERIAL_MODE',\n 'FONDKAPREMONT_MODE']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = RandomForestClassifier(class_weight=\"balanced\", n_jobs=-1, random_state=0, n_estimators=100,\n                                             min_samples_leaf=100, max_features='sqrt')\n\nclf.fit(X_train.drop(cols_to_drop, axis=1), y_train)\nroc_auc_score(y_val, clf.predict_proba(X_val.drop(cols_to_drop, axis=1))[:,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_extended_classification_report(clf, X_val.drop(cols_to_drop, axis=1), y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For a slight drop in AUC, we should now be safe against leakage. It does look like the tradeoff doesn't look as good, but this can be managed by changing the cutoffs later on.","metadata":{}},{"cell_type":"code","source":"importances_df = pd.DataFrame({\"features\": X_train.drop(cols_to_drop, axis=1).columns, \"importances\": clf.feature_importances_}).sort_values(\"importances\", ascending=False)\nimportances_df.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems like our most important features are the external credit scores(not surprising), but let's visit improvements later on by exploring their interactions for now, let's take a look at how big our potential market is as well as test set performance","metadata":{}},{"cell_type":"code","source":"roc_auc_score(y_test, clf.predict_proba(X_test.drop(cols_to_drop, axis=1))[:,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_extended_classification_report(clf, X_test.drop(cols_to_drop, axis=1), y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/home-credit-default-risk/application_test.csv\")\n\ntest = fill_in_nulls_test(test, nas)\ntest = convert_to_cats_test(test, cats)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds = clf.predict(test.drop(cols_to_drop, axis=1))\neligible_users = np.abs((test_preds - 1).sum())\neligible_pct = eligible_users/test_preds.shape[0]\nprint(\"We will have {0} customers the test set or {1:.2%} of the test set will be eligible\".format(eligible_users, eligible_pct))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While we can stop here in the meantime, we can also try to find the tradeoff in eligibility for different interest rates. Let's assume that a reasonable breakeven interest rate would be between 3-5%, can we find the cutoffs and the resulting eligible base?\n\nFor this exercise, we will be inverting the scale by making 1 a good payer and 0 a bad payer.","metadata":{}},{"cell_type":"code","source":"X_test_preds_df = pd.DataFrame({\"prediction\": 1 - clf.predict_proba(X_test.drop(cols_to_drop, axis=1))[:,1], \n                                \"target\": y_test}).sort_values(\"prediction\", ascending=False).reset_index(drop=True)\nX_test_preds_df[\"target_cumsum\"] = X_test_preds_df[\"target\"].cumsum()\nX_test_preds_df[\"default_rate\"] = X_test_preds_df[\"target_cumsum\"]/(X_test_preds_df.index+1)\nX_test_preds_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for interest_rate in [0.03, 0.035, 0.04, 0.045, 0.05]:\n    cutoff = X_test_preds_df.loc[X_test_preds_df[\"default_rate\"] <= interest_rate, \"prediction\"].min()\n    test_preds = 1 - clf.predict_proba(test.drop(cols_to_drop, axis=1))[:,1]\n    eligible_users = (test_preds > cutoff).sum()\n    eligible_pct = eligible_users/test_preds.shape[0]\n    print(\"With a cutoff of {0:.2}, we will have {1} customers the test set or {2:.2%} of the test set will be eligible for a breakeven interest rate of {3:.1%}\".format(\n        cutoff, eligible_users, eligible_pct, interest_rate))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's look into the fairness of the model. For now, we will assume a model is fair if it does not discriminate on the basis of race, gender or religion. Since we only have gender in this data set, let's take a look if the model discriminates on the basis of gender using the 4/5 rule.","metadata":{}},{"cell_type":"code","source":"cats[\"CODE_GENDER\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp = dx.Explainer(clf, X_test.drop(cols_to_drop, axis=1), y_test)\nprotected = X_test[\"CODE_GENDER\"]\nprivileged = 1\nfobject = exp.model_fairness(protected = protected, privileged=privileged)\nfobject.fairness_check(epsilon = 0.8) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Oh no, looks like we have features that are likely causing bias. Let's find those features and try to take them out.","metadata":{}},{"cell_type":"code","source":"X_f = X_train[X_train[\"CODE_GENDER\"].isin([0, 1])].drop(cols_to_drop, axis=1)\ny_f = X_train.loc[X_train[\"CODE_GENDER\"].isin([0, 1]), 'CODE_GENDER']\nX_train_f, X_test_f, y_train_f, y_test_f = train_test_split(X_f, y_f, test_size=0.33, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_f = RandomForestClassifier(class_weight=\"balanced\", n_jobs=-1, random_state=0, n_estimators=100,\n                                             min_samples_leaf=100, max_features='sqrt')\n\nclf_f.fit(X_train_f, y_train_f)\nroc_auc_score(y_test_f, clf_f.predict_proba(X_test_f)[:,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like some features are indicative of whether a user is male or female, so let's try to weed them out.","metadata":{}},{"cell_type":"code","source":"importances_df = pd.DataFrame({\"features\": X_train_f.columns, \"importances\": clf_f.feature_importances_}).sort_values(\"importances\", ascending=False)\nimportances_df.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first 5 features look significantly higher than the rest. Let's take them out for now.","metadata":{}},{"cell_type":"code","source":"cols_to_drop_f = ['OWN_CAR_AGE_na',\n 'OWN_CAR_AGE',\n 'AMT_INCOME_TOTAL',\n 'FLAG_DOCUMENT_8',\n 'EXT_SOURCE_1']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_f = RandomForestClassifier(class_weight=\"balanced\", n_jobs=-1, random_state=0, n_estimators=100,\n                                             min_samples_leaf=100, max_features='sqrt')\n\nclf_f.fit(X_train_f.drop(cols_to_drop_f, axis=1), y_train_f)\nroc_auc_score(y_test_f, clf_f.predict_proba(X_test_f.drop(cols_to_drop_f, axis=1))[:,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = RandomForestClassifier(class_weight=\"balanced\", n_jobs=-1, random_state=0, n_estimators=100,\n                                             min_samples_leaf=100, max_features='sqrt')\n\nclf.fit(X_train.drop(cols_to_drop + cols_to_drop_f, axis=1), y_train)\nroc_auc_score(y_val, clf.predict_proba(X_val.drop(cols_to_drop + cols_to_drop_f, axis=1))[:,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_extended_classification_report(clf, X_val.drop(cols_to_drop + cols_to_drop_f, axis=1), y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp = dx.Explainer(clf, X_test.drop(cols_to_drop + cols_to_drop_f, axis=1), y_test)\nprotected = X_test[\"CODE_GENDER\"]\nprivileged = 1\nfobject = exp.model_fairness(protected = protected, privileged=privileged)\nfobject.fairness_check(epsilon = 0.8) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now have a fair model that doesn't discriminate based on gender, however since EXT_SOURCE_1 is one of the top features that increases bias, this external model likely has biases inside the model. \n\nFor now, let's check what the test set has to say about our finals model.","metadata":{}},{"cell_type":"code","source":"print(roc_auc_score(y_test, clf.predict_proba(X_test.drop(cols_to_drop + cols_to_drop_f, axis=1))[:,1]))\nprint_extended_classification_report(clf, X_test.drop(cols_to_drop + cols_to_drop_f, axis=1), y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds = clf.predict(test.drop(cols_to_drop + cols_to_drop_f, axis=1))\neligible_users = np.abs((test_preds - 1).sum())\neligible_pct = eligible_users/test_preds.shape[0]\nprint(\"We will have {0} customers the test set or {1:.2%} of the test set will be eligible\".format(eligible_users, eligible_pct))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_preds_df = pd.DataFrame({\"prediction\": 1 - clf.predict_proba(X_test.drop(cols_to_drop + cols_to_drop_f, axis=1))[:,1], \n                                \"target\": y_test}).sort_values(\"prediction\", ascending=False).reset_index(drop=True)\nX_test_preds_df[\"target_cumsum\"] = X_test_preds_df[\"target\"].cumsum()\nX_test_preds_df[\"default_rate\"] = X_test_preds_df[\"target_cumsum\"]/(X_test_preds_df.index+1)\nX_test_preds_df\nfor interest_rate in [0.03, 0.035, 0.04, 0.045, 0.05]:\n    cutoff = X_test_preds_df.loc[X_test_preds_df[\"default_rate\"] <= interest_rate, \"prediction\"].min()\n    test_preds = 1 - clf.predict_proba(test.drop(cols_to_drop + cols_to_drop_f, axis=1))[:,1]\n    eligible_users = (test_preds > cutoff).sum()\n    eligible_pct = eligible_users/test_preds.shape[0]\n    print(\"With a cutoff of {0:.2}, we will have {1} customers the test set or {2:.2%} of the test set will be eligible for a breakeven interest rate of {3:.1%}\".format(\n        cutoff, eligible_users, eligible_pct, interest_rate))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like we lose roughly 1-2k users by reducing the bias in our model, but at least we're fairly confident that we're treating customers fairly. Now let's understand how our features work.","metadata":{}},{"cell_type":"code","source":"import shap  # package used to calculate Shap values\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(clf)\n\n# calculate shap values. This is what we will plot.\n# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\n# shap_values = explainer.shap_values(X_val[:10000])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap_values = explainer.shap_values(X_val.drop(cols_to_drop + cols_to_drop_f, axis=1), approximate=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.summary_plot(shap_values[0], X_val.drop(cols_to_drop + cols_to_drop_f, axis=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No surprise, but the top features for the decision tree also appear to be our top features for the random forest. Looks like leveraging external credit scores is a good way to have a good model without exerting too much effort.","metadata":{}},{"cell_type":"code","source":"# shap.plots.beeswarm(shap_values[0], max_display=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shap.summary_plot(shap_values, X_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# result = permutation_importance(clf, X_train, y_train, n_repeats=10,\n#                                 random_state=42)\n# perm_sorted_idx = result.importances_mean.argsort()\n\n# tree_importance_sorted_idx = np.argsort(clf.feature_importances_)\n# tree_indices = np.arange(0, len(clf.feature_importances_)) + 0.5\n\n# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n# ax1.barh(tree_indices,\n#          clf.feature_importances_[tree_importance_sorted_idx], height=0.7)\n# ax1.set_yticks(tree_indices)\n# ax1.set_yticklabels(data.feature_names[tree_importance_sorted_idx])\n# ax1.set_ylim((0, len(clf.feature_importances_)))\n# ax2.boxplot(result.importances[perm_sorted_idx].T, vert=False,\n#             labels=data.feature_names[perm_sorted_idx])\n# fig.tight_layout()\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}