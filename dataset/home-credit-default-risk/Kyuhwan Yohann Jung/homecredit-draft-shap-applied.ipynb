{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_columns = 999\ntrain = pd.read_csv('/kaggle/input/home-credit-default-risk/application_train.csv')\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/home-credit-default-risk/application_test.csv')\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_cash_balance = pd.read_csv('/kaggle/input/home-credit-default-risk/POS_CASH_balance.csv')\npos_cash_balance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"previous_app = pd.read_csv('/kaggle/input/home-credit-default-risk/previous_application.csv')\n\n# pos_cash_balance 의 'SK_ID_PREV' 와 bureau 의 ANNUITY 와 곂침\n\nprevious_app = previous_app.rename(columns={'SK_ID_PREV': 'SK_ID_PREV_pa', 'AMT_ANNUITY':'AMT_ANNUITY_pa'})\nprevious_app","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='1'>간접적인 정보 추가방법</a>\n* <a id='1'> 1. 통계량 정보 추가 (mean, avg, max, min...)</a>\n* <a id='1'> 2. 카테고리형에서 nunique 정보 추가</a>\n* <a id='1'> 3. 각 ID 별 count 량 추가 --> 여기선 통계량 정보 추가가 (sum 같은) 비슷한 역할을 할수있음</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_stats = pos_cash_balance.groupby('SK_ID_CURR').agg(['mean', 'std', 'median', 'max', 'min', 'sum'])\npos_stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau = pd.read_csv('/kaggle/input/home-credit-default-risk/bureau.csv')\nbureau","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_stats = bureau.groupby('SK_ID_CURR').agg(['mean', 'std', 'median','max', 'min', 'sum'])\nbureau_stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"previous_stats = previous_app.groupby('SK_ID_CURR').agg(['mean', 'std', 'median','max', 'min', 'sum'])\n\n# to.prefix 같은걸 써서 중복되는 컬럼이름을 바꿔준다\n\nprevious_stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='1'>직접적인 정보 추가방법</a>\n* <a id='1'> 카테고리형 칼럼추가</a>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"previous_nct_direct = previous_app.groupby(['SK_ID_CURR', 'NAME_CONTRACT_TYPE'])['SK_ID_PREV_pa'].agg(len).unstack()\nprevious_nct_direct","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"previous_ngc_direct = previous_app.groupby(['SK_ID_CURR', 'NAME_GOODS_CATEGORY'])['SK_ID_PREV_pa'].agg(len).unstack()\nprevious_ngc_direct","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='1'>칼럼의 갯수를 줄여 차원수 줄이고, RAM 소모량 줄이기 위해 PCA / SVD 사용</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components = 3)\n\nprevious_ngc_pca = pca.fit_transform(previous_ngc_direct.fillna(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"previous_ngc_pca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_previous = pd.DataFrame(previous_ngc_pca, index = previous_ngc_direct.index).add_suffix('_NGC')\npca_previous","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"previous_nsi_direct = previous_app.groupby(['SK_ID_CURR', 'NAME_SELLER_INDUSTRY'])['SK_ID_PREV_pa'].agg(len).unstack()\nprevious_nsi_direct","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"previous_nsi_pca = pca.fit_transform(previous_nsi_direct.fillna(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_previous_nsi = pd.DataFrame(previous_nsi_pca, index = previous_nsi_direct.index).add_suffix('_NSI')\npca_previous_nsi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_columns = 999\nalldata = pd.concat([train, test])\n\n# 위 bureau_stats 와 pos_stats, previous_stats 정보 추가\n\nalldata = alldata.join(pos_stats, on = 'SK_ID_CURR')\nalldata = alldata.join(bureau_stats, on = 'SK_ID_CURR')\nalldata = alldata.join(previous_stats, on = 'SK_ID_CURR')\nalldata = alldata.join(previous_nct_direct, on = 'SK_ID_CURR')\nalldata = alldata.join(pca_previous_nsi, on = 'SK_ID_CURR')\nalldata = alldata.join(pca_previous, on = 'SK_ID_CURR')\n\nalldata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata_corr = alldata.corr()['TARGET'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('Most Positive Correlations:\\n', alldata_corr.tail(15))\nprint('\\nMost Negative Correlations:\\n', alldata_corr.head(15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DAYS_BIRTH의 절대값과 TARGET변수와의 상관계수\nalldata['DAYS_BIRTH']=abs(alldata['DAYS_BIRTH'])\n#alldata2['DAYS_BIRTH'].corr(alldata2['TARGET'])\nalldata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata2 = alldata.drop(columns = ['TARGET', 'SK_ID_CURR'])\nalldata2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train 분포 "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['TARGET'].astype(int).plot.hist();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_values_table(df):\n    # 전체 결측치 개수 확인\n    mis_val=df.isnull().sum()\n    \n    # 결측치 비중 확인\n    mis_val_percent=100*df.isnull().sum()/len(df)\n    \n    # 결측치 개수 , 결측치 비중 테이블 만들기\n    mis_val_table=pd.concat([mis_val, mis_val_percent],axis=1)\n    \n    # 컬럼 이름바꾸기\n    mis_val_table_ren_columns=mis_val_table.rename(columns={0:'Missing Values',1:'% of Total Values'})\n\n    # 결측치 0인 컬럼은 제외하고 정렬\n    mis_val_table_ren_columns=mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1]!=0].sort_values('% of Total Values',ascending=False).round(1)\n\n    # 요약 결과 print\n    print(\"app_train의 전체 컬럼 개수는 \"+str(df.shape[1])+\"개 이다.\\n\"\n         \"그 중에서 결측치가 있는 컬럼 개수는 \"+str(mis_val_table_ren_columns.shape[0])+'개 이다.')\n    \n    return mis_val_table_ren_columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values=missing_values_table(train)\nmissing_values.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Label encoding vs One Hot encoding\n\nThe problem with label encoding is that it gives the categories an arbitrary ordering. The value assigned to each of the categories is random and does not reflect any inherent aspect of the category. In the example above, programmer recieves a 4 and data scientist a 1, but if we did the same process again, the labels could be reversed or completely different. The actual assignment of the integers is arbitrary. Therefore, when we perform label encoding, the model might use the relative value of the feature (for example programmer = 4 and data scientist = 1) to assign weights which is not what we want. If we only have two unique values for a categorical variable (such as Male/Female), then label encoding is fine, but for more than 2 unique categories, one-hot encoding is the safe option.\n\nThere is some debate about the relative merits of these approaches, and some models can deal with label encoded categorical variables with no issues. Here is a good Stack Overflow discussion. I think (and this is just a personal opinion) for categorical variables with many classes, one-hot encoding is the safest approach because it does not impose arbitrary values to categories. The only downside to one-hot encoding is that the number of features (dimensions of the data) can explode with categorical variables with many categories. To deal with this, we can perform one-hot encoding followed by PCA or other dimensionality reduction methods to reduce the number of dimensions (while still trying to preserve information).\n\nIn this notebook, we will use Label Encoding for any categorical variables with only 2 categories and One-Hot Encoding for any categorical variables with more than 2 categories. This process may need to change as we get further into the project, but for now, we will see where this gets us. (We will also not use any dimensionality reduction in this notebook but will explore in future iterations)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.preprocessing import LabelEncoder\n\n# le=LabelEncoder()\n\n# c = alldata.columns[alldata.dtypes == object]\n\n# for i in c:\n#     alldata[i] = le.fit_transform(list(alldata[i]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # One Hot Encoding\n\n# alldata = pd.get_dummies(alldata)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pd.options.display.max_columns = 999\n\n# alldata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nc = alldata2.columns[alldata2.dtypes == object]\n\nfor i in c :\n    alldata2[i] = le.fit_transform(alldata2[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\n# # 결측치를 median값으로 처리\n# imputer = SimpleImputer(strategy='median')\n\n# '''\n# 각 Feature의 값을 일정한 범위 또는 규칙에 따르게 하기 위해서 스케일링을 사용\n# '''\n# # 각각의 변수를 0~1 사이의 값으로 만들어주는 MinMaxScaler 사용\n# ## MinMaxScaler 클래스의 인스턴스를 만들어준다\nscaler=MinMaxScaler(feature_range=(0,1))\n\n# # training 데이터에 fit\n# imputer.fit(alldata2)\n\n# # training데이터와 testing데이터에 둘다 transform\n# ## imputer 처리 하고나면 DataFrame에서 array형태로 바뀜\n# alldata=imputer.transform(alldata2)\n\n# Scaling\nscaler.fit(alldata2)\nalldata=scaler.transform(alldata2)\n\nprint('Alldata shape: ', alldata2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2 = alldata2[:len(train)]\ntest2 = alldata2[len(train):]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='1'> RandomForest 대신 Catboost 사용 --> 카테고리형 컬럼이 많기때문에 훨씬 좋은 성능을 보임</a>\n# <a id='1'> + 교차검증 사용</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom catboost import CatBoostClassifier\n\ncbc = CatBoostClassifier(verbose = 50, task_type = 'GPU', iterations = 10000, learning_rate = 0.1, eval_metric = 'AUC')\n\nskf = StratifiedKFold(n_splits = 5, shuffle= True, random_state = 42)\n\nresult = 0\nbest_score = 0\n\nfor train_index, valid_index in skf.split(train2, train['TARGET']):\n    x_train, x_valid = train2.iloc[train_index], train2.iloc[valid_index]\n    y_train, y_valid = train['TARGET'].iloc[train_index], train['TARGET'].iloc[valid_index]\n    cbc.fit(x_train, y_train, eval_set = (x_valid, y_valid), early_stopping_rounds = 25)\n    best_score += cbc.best_score_['validation']['AUC'] / 5\n    result += cbc.predict_proba(test2) /5\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_score\n\n# Imputer, MinMaxScaler 사용 + direct + indirect info added = 0.7778919041156769\n# 위에서 Imputer 제거 --> 0.7779257774353028\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='1'> SHAP 적용</a>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\nshap.initjs()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_sampled = x_train.sample(1000, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explainer = shap.TreeExplainer(cbc)\nshap_values = explainer.shap_values(X_sampled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value, shap_values[0,:], X_sampled.iloc[0,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value, shap_values, x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, X_sampled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, X_sampled, plot_type = 'bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n\n# rf=RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rf.fit(train2, train['TARGET'])\n\n# #result = 1- (rf.predict_proba(test2))\n\n\n\n# result = rf.predict_proba(test2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = result[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/home-credit-default-risk/sample_submission.csv')\nsub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['TARGET'] = result\nsub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.tail(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('sub1.csv', index= 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}