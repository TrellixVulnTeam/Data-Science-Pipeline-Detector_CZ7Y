{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport seaborn as sns ; sns.set_theme()\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n![](https://www.economist.com/img/b/1280/720/90/sites/default/files/images/print-edition/20190706_IRD001_0.jpg)\n\nMany people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.\n\nSo can we use the power of advanced data analysis techniques accompanied with modern machine learning algorithms to predict their clients repayment abilities\n\nI think it's application of the concept \"AI for good\" , if we have a good model to predict client's repayment abilities , we may have decreased the number of people who take the loan and can't repay it so it will end with them to the jail , also we have directed the money of loans to people who deserves it"},{"metadata":{},"cell_type":"markdown","source":"# Development Cycle\n\nwe will follow the data science agile development life cycle along our problem\n\n![](https://i.pinimg.com/originals/8c/d7/5f/8cd75ffb7c2524686bb9c342ff8490c7.png)"},{"metadata":{},"cell_type":"markdown","source":"# 1- Business understanding\n\nCost of risk is one of the biggest components in banks’ cost structure. Thus, even a slight improvement in credit risk modelling can translate in huge savings. That’s why Machine Learning is often implemented in this area"},{"metadata":{},"cell_type":"markdown","source":"# 2- Data Mining\n\nin a typical industry scenario data mining includes for example fetching customer's databases , web sracbing ... etc\nfortunately in our problem , data is already collected so let's move to the next step"},{"metadata":{},"cell_type":"markdown","source":"# 3- Data Cleaning and Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data\ndftrain = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\nprint('Training data shape: ', dftrain.shape)\ndftrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tech Insight**\n\nwe think that the number of columns (features) may be very large (122 features) , that may slow down EDA process and also may lead to model overfitting while training , so we may need in feature engineering stage to lower the dimentions of the data , before doing EDA and training"},{"metadata":{},"cell_type":"markdown","source":"## 3-1 Searching for missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot missing values\nfig, ax = plt.subplots(figsize=(30,10))\nsns.heatmap(dftrain.isnull(), cbar=False, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### well ! , there are too much nan vlaues in our data\n#### How to handle missing data ?!\n#### 1 - naive methods like drop rows with missing values will shrink a lot our data corpus and we do not want to lose data\n#### 2 - dropping features with high nan values still introducing issues , as dropped features may have had high impact on model perfomance\n#### 3- our approach will be try to fit an advanced tree based classifier like xgboost which has his own strategy to handle missing values , we just wanted to see how important the features with high nan rate , if they are very important , we will try to use advanced imputation analysis "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 3-2 Focus on important features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# categorical feature encoding\n# using one hot encoding\ndftrainenc = pd.get_dummies(dftrain)\ndftrainenc.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit xgboost on train data to rank features with respect to importance\nfrom xgboost import XGBClassifier\n# fit model no training data\nX=dftrainenc.iloc[:,2:]\ny=dftrainenc['TARGET']\nmodel = XGBClassifier()\nmodel.fit(X, y)\n# feature importance\nprint(model.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot top 50 most important features \nfeat_imp = pd.Series(model.feature_importances_, index=X.columns)\nfeat_imp.nlargest(50).plot(kind='barh', figsize=(8,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### fortunately the features with most nans are not from the most important features except external source feature , so our approach (in feature engineering section) is to drop heavy nan filled features except external source features because xgboost will impute them automatically during training"},{"metadata":{},"cell_type":"markdown","source":"## 4- Data Exploration (EDA)"},{"metadata":{},"cell_type":"markdown","source":"### 4-1 what is the relation between target and applicant number of childrens"},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\nchtarget = pd.DataFrame(dftrainenc.groupby(['CNT_CHILDREN'])['TARGET'].count())\nfig = go.Figure([go.Bar(x=chtarget.index, y=chtarget['TARGET'])])\nfig.update_layout(title_text='relation between target and applicant number of childrens',\n                 xaxis_title=\"Applicant children\",\n                 yaxis_title=\"Number of applications\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### insight : single people or married with no children , dominates the applications pool , this insight may help marketing teams that they need to target this sector of customers"},{"metadata":{},"cell_type":"markdown","source":"### 4-2 what is the relation between target and applicant Income"},{"metadata":{"trusted":true},"cell_type":"code","source":"inctarget = pd.DataFrame(dftrainenc.groupby(['TARGET'])['AMT_INCOME_TOTAL'].mean())\nfig = go.Figure([go.Bar(x=inctarget.index, y=inctarget['AMT_INCOME_TOTAL'])])\nfig.update_layout(title_text='relation between target and applicant income',\n                 xaxis_title=\"Target\",\n                 yaxis_title=\"applicants mean income per category in target\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### insight : this graph show how critical our problem is , notice that ration between target 1 and 0 is nearly 1/11 so for every 1 applicant have diffculties repay the loan 11 applicant repayed the loan , the graph also show huge income gap as average income of minority nearly reaches average majority income , also the graph clearly guide admins to not to trust much people with high income "},{"metadata":{},"cell_type":"markdown","source":"### 4-3 what is the relation between target and applicant Income type"},{"metadata":{"trusted":true},"cell_type":"code","source":"inctype = pd.crosstab(dftrain.NAME_INCOME_TYPE,dftrain.TARGET)\nfig = go.Figure(data=[\n    go.Bar(name='1 : has issues', x=inctype.index, y=inctype[1]),\n    go.Bar(name='0 : no issues', x=inctype.index, y=inctype[0])\n])\n# Change the bar mode\nfig.update_layout(barmode='stack',title_text='relation between target and applicant Income type')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"note : unemployed , student and maternity leave are not zero , you need to zoom the graph to find their numbers"},{"metadata":{},"cell_type":"markdown","source":"### 4-4 what is the relation between target and applicant occupation"},{"metadata":{"trusted":true},"cell_type":"code","source":"occtype = pd.crosstab(dftrain.OCCUPATION_TYPE,dftrain.TARGET)\nfig = go.Figure(data=[\n    go.Bar(name='1 : has issues', x=occtype.index, y=occtype[1]),\n    go.Bar(name='0 : no issues', x=occtype.index, y=occtype[0])\n])\n# Change the bar mode\nfig.update_layout(barmode='stack',title_text='relation between target and applicant Income type')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### insight : it is logical that Laborers higst number of applications may be due to their unstable working environment , but a good insight they have very high potential of repaying the loan"},{"metadata":{},"cell_type":"markdown","source":"## 5- Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### 5-1 feature selection\n\nin previous section we used introduced the concept of feature importance to try to rank important features , in this section as experiment we will choose top 50 importna t features , then try to remove noise samples or outliers "},{"metadata":{"trusted":true},"cell_type":"code","source":"unwanted = []\nfor col in dftrainenc.columns:\n    if col not in feat_imp.nlargest(50):\n        unwanted.append(col)\ndftrainenc.drop(columns=unwanted,inplace=True)\ndftrainenc.sample(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"finaldf = pd.concat([dftrainenc,dftrain['TARGET']],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5-2 trying tackle imbalanced classes problem"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nX=finaldf.iloc[:,0:50]\ny=finaldf['TARGET']\nxtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=0.30)\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nxtrain=imputer.fit_transform(xtrain)\nxtrain=pd.DataFrame(xtrain,columns=X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\n# define pipeline\n# using SMOTE algorithm to synthically oversample minor samples\nover = SMOTE(sampling_strategy=0.1)\nunder = RandomUnderSampler(sampling_strategy=0.5)\nsteps = [('o', over), ('u', under)]\npipeline = Pipeline(steps=steps)\n# transform the dataset\nxtrain, ytrain = pipeline.fit_resample(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6- Predective Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nmodel = XGBClassifier(n_estimators=400)\nmodel.fit(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7- Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\npredictions = model.predict(xtest) # test model against test set\npreds_train = model.predict(xtrain)\nprint(\"Model Acurracy in testing = {}\".format(accuracy_score(ytest, predictions))) # print test accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### accuracy score shows good performance on training and test sets , so we think it's a good fit model with no overfitting or underfitting , we think automated hyperparameters search may help boost accuracy"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}