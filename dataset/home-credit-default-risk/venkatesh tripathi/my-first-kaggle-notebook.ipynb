{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing the required modules","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport pandas_profiling\nimport numpy as np\nimport missingno as mngo\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler\nfrom category_encoders.target_encoder import TargetEncoder\nfrom sklearn.impute import SimpleImputer,KNNImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif,mutual_info_classif,chi2,f_regression\nimport seaborn as sns\nfrom scipy.stats.mstats import winsorize\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.model_selection import KFold,cross_val_score,StratifiedKFold,RepeatedStratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom joblib import Parallel, delayed\n!pip install imblearn\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Importing the training and test dataset\n## Indicating the dependent variable\n## forming a cols list containing the continuous numeric variables\n## This list will be used in future codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.listdir('/kaggle/input/')\ndata=pd.read_csv(\"../input/home-credit-default-risk/application_train.csv\")\ntarget_y=data['TARGET']\ntarget_y\ncols=['EXT_SOURCE_1','EXT_SOURCE_2', 'EXT_SOURCE_3','AMT_CREDIT','AMT_INCOME_TOTAL','AMT_GOODS_PRICE','AMT_ANNUITY', 'DAYS_REGISTRATION','DAYS_EMPLOYED','DAYS_BIRTH','DAYS_LAST_PHONE_CHANGE']\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test=pd.read_csv(\"../input/home-credit-default-risk/application_train.csv\")\ndata_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Creating a function for removing columns with more than 50 percent of missing values \n## Except EXT_SOURCE_1 which is an important variable for us","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_values(xdata):\n    b=pd.DataFrame(xdata.isna().sum())\n    b.rename(columns={0:'no. of missing values'},inplace=True)\n    b['percent of missing values']=b['no. of missing values']/len(xdata)\n    b=b.sort_values(by='no. of missing values',ascending=False)\n    b.reset_index(level=0,inplace=True)\n    b.rename(columns={'index':'feature_name'},inplace=True)\n    plt.figure(figsize=(16,12))\n    plt.plot(b['feature_name'].head(30),b['percent of missing values'].head(30))\n    plt.xticks(rotation=90)\n    plt.show()\n    c=b[(b['feature_name']!='EXT_SOURCE_1') & (b['percent of missing values']<=0.50)]\n    list1=list(c['feature_name'])\n    list1.append('EXT_SOURCE_1')\n    \n    data_removing_missing=xdata.loc[:,list1]\n    return data_removing_missing\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Applying the function on test and train datasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_removing_missing=missing_values(data)\ntrain_data_removing_missing\ntest_data_removing_missing=missing_values(data_test)\ntest_data_removing_missing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Shape of test and train datasets after applying the above function","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_data_removing_missing.shape)\nprint(train_data_removing_missing.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## A function for creating a distribution for all the continuous numeric variables\n## We can see the graphs to see whether the variables and normally distributed and\n## if there are any outliers in the variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def continuous_variable_distributions(xdata,plot_type):\n    \n    for cols in xdata:\n        \n        if plot_type=='displot':\n            sns.displot(data=xdata,x=cols,bins=25)\n            plt.xticks(rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## We can see that some variables are not normally distributed and \n## particularly in the days employed variable we can see aan outlier\n## value with a considerable frequency. We shall investigate later","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"continuous_variable_distributions(train_data_removing_missing[cols],'displot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## The pandas profile report gives a detailed analysis\n## of the variables denoting the missing values in each variable,\n## the correlation and interaction between different variables\n## along with a univariate plot of the variables\n## We can see significant number of missing values in variables\n## like EXT_SOURCE_1,EXT_SOURCE_2. We will treat them.","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train_data_removing_missing[cols].profile_report()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## This plot using missingno module shows the relationship\n## between the missing values of different variables. As we can\n## see the white lines represent missing values and they are largely \n## present in ext_source_1 and ext_source3. This plot can help us \n## to see if the missing values in ext_source_1 and ext_source3 are \n## correlated. We have to see whether missing values are random or \n## their appearance can be explained or can not be explained.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_numerical=train_data_removing_missing[cols]\nmngo.matrix(train_data_numerical)\nsorted1=train_data_numerical.sort_values(by='EXT_SOURCE_1')\nmngo.dendrogram(train_data_numerical)\nmngo.heatmap(train_data_numerical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Creating two flag variables to see which rows \n## are missing in ext_source_1 and ext_source_3.\n## As we impute the missing values and these variables\n## have significant proportion of missing values we \n## might to save this information as a flag column and \n## see if it can be an important feature later.\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_removing_missing['missing_ext_source_1'] = train_data_removing_missing['EXT_SOURCE_1'].apply(lambda x: 0 if pd.isnull(x)==True else 1)\ntrain_data_removing_missing['missing_ext_source_3'] = train_data_removing_missing['EXT_SOURCE_3'].apply(lambda x: 0 if pd.isnull(x)==True else 1)\ntest_data_removing_missing['missing_ext_source_1'] = test_data_removing_missing['EXT_SOURCE_1'].apply(lambda x: 0 if pd.isnull(x)==True else 1)\ntest_data_removing_missing['missing_ext_source_3'] = test_data_removing_missing['EXT_SOURCE_3'].apply(lambda x: 0 if pd.isnull(x)==True else 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Imputing all the missing values in continuous variables\n## in both training and test dataset with median of the columns \n## in the training dataset so that there is no data leakage.\n## Median is robust to outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=['EXT_SOURCE_1','EXT_SOURCE_3','AMT_CREDIT','AMT_INCOME_TOTAL','AMT_GOODS_PRICE','AMT_ANNUITY', 'DAYS_REGISTRATION','DAYS_BIRTH','DAYS_EMPLOYED','DAYS_LAST_PHONE_CHANGE']\n\nimp=SimpleImputer(missing_values=np.nan,strategy='median')\na=train_data_removing_missing[cols]\nb=test_data_removing_missing[cols]\nimp.fit(a)\ndata=pd.DataFrame(imp.transform(a))\ndata.rename(columns=dict(zip(data.columns,cols)),inplace=True)\nx=train_data_removing_missing.drop(cols,axis=1)\ntrain_data_removing_missing=pd.concat([data,x],axis=1)\ndata2=pd.DataFrame(imp.transform(b))\ndata2.rename(columns=dict(zip(data2.columns,cols)),inplace=True)\ny=test_data_removing_missing.drop(cols,axis=1)\ntest_data_removing_missing=pd.concat([data2,y],axis=1)\nprint(test_data_removing_missing.shape)\nprint(train_data_removing_missing.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Checking whether the continuous variables have missing \n## values now after doing missing value imputation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print((train_data_removing_missing[cols]).isna().sum())\nprint((test_data_removing_missing[cols]).isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Now creating a function to detect the proportion of outliers\n## in the continuous variables. Using two criteria to find the proportion\n## First criteria:- twenty fifth percentile -1.5*IQR,seventy fifth percentile +1.5*IQR\n## Second Criteria:-twenty fifth percentile -3*IQR,seventy fifth percentile +3*IQR\n## Outliers according to first criteria we call light outliers and outliers according\n## to second criteria we call heavy outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def outlier_detection(cols):\n    \n    dict_extreme={}\n    dict_light={}\n    \n    for col in cols:\n        q1=train_data_removing_missing[col].quantile(0.25)\n        q3=train_data_removing_missing[col].quantile(0.75)\n        left_boundary_extreme=q1-3*(q3-q1)\n        right_boundary_extreme=q3+3*(q3-q1)\n        left_boundary_light=q1-1.5*(q3-q1)\n        right_boundary_light=q3+1.5*(q3-q1)\n        outlier_list_extreme=[]\n        outlier_list_light=[]\n        for index,x in enumerate(train_data_removing_missing[col]):\n            if (x<left_boundary_extreme) or (x>right_boundary_extreme):\n                outlier_list_extreme.append(index)\n            if (x<left_boundary_light) or(x>right_boundary_light):\n                outlier_list_light.append(index)\n        dict_extreme[col]=len(outlier_list_extreme)/len(train_data_removing_missing)\n        dict_light[col]=len(outlier_list_light)/len(train_data_removing_missing)\n    return dict_extreme,dict_light\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Converting all the continuous day variables into years.\n## We convert days into positive and divide by 365 to get years","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_removing_missing['YEARS_EMPLOYED']=-1*(train_data_removing_missing['DAYS_EMPLOYED'])/365\ntrain_data_removing_missing['YEARS_REGISTRATION']=-1*(train_data_removing_missing['DAYS_REGISTRATION'])/365\ntrain_data_removing_missing['YEARS_BIRTH']=-1*(train_data_removing_missing['DAYS_BIRTH'])/365\ntrain_data_removing_missing['YEARS_LAST_PHONE_CHANGE']=-1*(train_data_removing_missing['DAYS_LAST_PHONE_CHANGE'])/365\ntest_data_removing_missing['YEARS_EMPLOYED']=-1*(test_data_removing_missing['DAYS_EMPLOYED'])/365\ntest_data_removing_missing['YEARS_REGISTRATION']=-1*(test_data_removing_missing['DAYS_REGISTRATION'])/365\ntest_data_removing_missing['YEARS_BIRTH']=-1*(test_data_removing_missing['DAYS_BIRTH'])/365\ntest_data_removing_missing['YEARS_LAST_PHONE_CHANGE']=-1*(test_data_removing_missing['DAYS_LAST_PHONE_CHANGE'])/365","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Applying outlier detection columns on the continuos variables\n## we see that days employed has significant proportion of outliers\n## which we saw in the graph. ext_source_1 has high proportion but the \n## range is (0,1) so we can ignore this variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=['EXT_SOURCE_1','EXT_SOURCE_2', 'EXT_SOURCE_3','AMT_CREDIT','AMT_INCOME_TOTAL','AMT_GOODS_PRICE','AMT_ANNUITY', 'DAYS_REGISTRATION','DAYS_EMPLOYED','DAYS_BIRTH','DAYS_LAST_PHONE_CHANGE']\n\noutlier_detection(cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot to show proportion of heavy and light outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=['EXT_SOURCE_1','EXT_SOURCE_2', 'EXT_SOURCE_3','AMT_CREDIT','AMT_INCOME_TOTAL','AMT_GOODS_PRICE','AMT_ANNUITY', 'DAYS_REGISTRATION','DAYS_EMPLOYED','DAYS_BIRTH','DAYS_LAST_PHONE_CHANGE']\n\ndict_extreme,dict_light=outlier_detection(cols)\nprint(dict_extreme)\nprint(dict_light)\nplt.plot(dict_extreme.keys(),dict_extreme.values())\nplt.xticks(rotation=90)\nplt.plot(dict_light.keys(),dict_light.values())\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creating a dataframe to see the range of values\n## from 90 to 99.9 percentile for all continuous variables\n## This can be used in outlier imputation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def quantile_length(data,cols):\n    quantile_list={}\n    \n    outlier_list=[]\n    for col in cols:\n        for quantile in [0.90,0.925,0.95,0.975,0.99,0.999]:\n        \n            outlier_list.append(data[col].quantile(quantile))\n        outlier_list.append(data[col].max())\n        outlier_list.append(data[col].min())\n        outlier_list.append(data[col].mean())\n        quantile_list[col]=outlier_list\n        outlier_list=[]\n            \n    return quantile_list   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=['EXT_SOURCE_1','EXT_SOURCE_2', 'EXT_SOURCE_3','AMT_CREDIT','AMT_INCOME_TOTAL','AMT_GOODS_PRICE','AMT_ANNUITY', 'DAYS_REGISTRATION','DAYS_EMPLOYED','DAYS_BIRTH','DAYS_LAST_PHONE_CHANGE']\nquantile_list=quantile_length(train_data_removing_missing,cols)\nquantile_list_dataframe=pd.DataFrame(quantile_list,index=['90 percentile','92.5 percentile','95 percentile','97.5 percentile','99 percentile','99.9 percentile','max','min','mean'])\nprint(quantile_list_dataframe)\n#print(train_data_removing_missing[cols].describe())\n#for key,values in quantile_list.items():\n #   print(key,values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creating an outlier variable for years employed\n## we will impute the outliers in the variable with its mean\n## so we create a flag to see which rows are outliers so maybe\n## this feature can be useful in modelling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_removing_missing['years_employed_outlier']=train_data_removing_missing['YEARS_EMPLOYED'].apply(lambda x:1 if x>=1000.66 else 0)\ntrain_data_removing_missing                                                                                         ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_removing_missing['years_employed_outlier']=test_data_removing_missing['YEARS_EMPLOYED'].apply(lambda x:1 if x>=1000.66 else 0)\ntest_data_removing_missing                                                                                         ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## We create an outlier replacement function.\n## the values less than fifth percentile will\n## be replaced by fifth percentile and values greater\n## than ninety fifth percentile replaced by ninety fifth\n## percentile. This range (0.05,0.95) is normally used","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def outlier_replacement(cols):\n    \n    for col in cols:\n        left_boundary=train_data_removing_missing[col].quantile(0.05)\n        right_boundary=train_data_removing_missing[col].quantile(0.95)\n\n        train_data_removing_missing[col]=np.where(train_data_removing_missing[col]<left_boundary,left_boundary,train_data_removing_missing[col])\n        train_data_removing_missing[col]=np.where(train_data_removing_missing[col]>right_boundary,right_boundary,train_data_removing_missing[col])\n        \n        test_data_removing_missing[col]=np.where(test_data_removing_missing[col]<left_boundary,left_boundary,test_data_removing_missing[col])\n        test_data_removing_missing[col]=np.where(test_data_removing_missing[col]>right_boundary,right_boundary,test_data_removing_missing[col])\n\n\n    return(train_data_removing_missing,test_data_removing_missing) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## For days employed because the values are same\n## starting from ninety fifth percentile and above so any value greater than ninety\n## fifth percentile will be replaced by median.\n## For years employed because the values are same\n## starting from fifth percentile and below so any value greater less than\n## fifth percentile will be replaced by median.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"left_boundary=train_data_removing_missing['DAYS_EMPLOYED'].quantile(0.05)\nright_boundary=train_data_removing_missing['DAYS_EMPLOYED'].quantile(0.95)\nmedian=train_data_removing_missing['DAYS_EMPLOYED'].median()\nleft_boundary2=train_data_removing_missing['YEARS_EMPLOYED'].quantile(0.05)\nright_boundary2=train_data_removing_missing['YEARS_EMPLOYED'].quantile(0.95)\nmedian2=train_data_removing_missing['YEARS_EMPLOYED'].median()\n\ntrain_data_removing_missing['DAYS_EMPLOYED']=np.where(train_data_removing_missing['DAYS_EMPLOYED']<=left_boundary,left_boundary,train_data_removing_missing['DAYS_EMPLOYED'])\ntrain_data_removing_missing['DAYS_EMPLOYED']=np.where(train_data_removing_missing['DAYS_EMPLOYED']>=right_boundary,median,train_data_removing_missing['DAYS_EMPLOYED'])\ntrain_data_removing_missing['DAYS_EMPLOYED'].describe()\ntest_data_removing_missing['DAYS_EMPLOYED']=np.where(test_data_removing_missing['DAYS_EMPLOYED']<=left_boundary,left_boundary,test_data_removing_missing['DAYS_EMPLOYED'])\ntest_data_removing_missing['DAYS_EMPLOYED']=np.where(test_data_removing_missing['DAYS_EMPLOYED']>=right_boundary,median,test_data_removing_missing['DAYS_EMPLOYED'])\ntest_data_removing_missing['DAYS_EMPLOYED'].describe()\n\ntrain_data_removing_missing['YEARS_EMPLOYED']=np.where(train_data_removing_missing['YEARS_EMPLOYED']<=left_boundary2,median2,train_data_removing_missing['YEARS_EMPLOYED'])\ntrain_data_removing_missing['YEARS_EMPLOYED']=np.where(train_data_removing_missing['YEARS_EMPLOYED']>=right_boundary2,right_boundary2,train_data_removing_missing['YEARS_EMPLOYED'])\ntrain_data_removing_missing['YEARS_EMPLOYED'].describe()\ntest_data_removing_missing['YEARS_EMPLOYED']=np.where(test_data_removing_missing['YEARS_EMPLOYED']<=left_boundary2,median2,test_data_removing_missing['YEARS_EMPLOYED'])\ntest_data_removing_missing['YEARS_EMPLOYED']=np.where(test_data_removing_missing['YEARS_EMPLOYED']>=right_boundary2,right_boundary2,test_data_removing_missing['YEARS_EMPLOYED'])\ntest_data_removing_missing['YEARS_EMPLOYED'].describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Checking outlier proportion after applying the function\n## and we see considerable drop in outliers in all continuous \n## variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_removing_missing,test_data_removing_missing=outlier_replacement(cols)\noutlier_detection(cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Outlier detection in years column and we see reduction \n##in outliers in the years column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_new=['YEARS_REGISTRATION','YEARS_EMPLOYED','YEARS_BIRTH','YEARS_LAST_PHONE_CHANGE']\ntrain_data_removing_missing,test_data_removing_missing=outlier_replacement(col_new)\noutlier_detection(col_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Now we see the distributions of the continuous variables\n## after outlier repalcement and we see that some variables have improved\n## to normal distribution. Some still have skewed distributions but in days\n## employed and years employed the outliers and gone and the distribution\n## is better now. We have also applied the min max scaler so as to bring\n## all the variables on a consistent scale so that the model does not give \n## weights based on difference in scales","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=['AMT_CREDIT','AMT_INCOME_TOTAL','AMT_GOODS_PRICE','AMT_ANNUITY', 'DAYS_REGISTRATION','DAYS_BIRTH','DAYS_LAST_PHONE_CHANGE'\n     ,'DAYS_EMPLOYED','YEARS_REGISTRATION','YEARS_BIRTH','YEARS_LAST_PHONE_CHANGE','YEARS_EMPLOYED']\n\ntransformer=MinMaxScaler()\ntransformer.fit_transform(train_data_removing_missing[cols])\ntransformer.transform(test_data_removing_missing[cols])\ncontinuous_variable_distributions(train_data_removing_missing[cols],'displot')\na=train_data_removing_missing[cols]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creating the density plots for all the continous\n## variables grouped by the target variable. We want\n## to see if the distribution is different for both the\n## target labels which can signify that this continuous variable\n## may be significant in detecting the target variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=a, x='AMT_INCOME_TOTAL',hue=target_y,fill=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=a, x='AMT_GOODS_PRICE',hue=target_y,fill=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=a, x='AMT_ANNUITY',hue=target_y,fill=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=a, x='DAYS_BIRTH',hue=target_y,fill=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=a, x='DAYS_REGISTRATION',hue=target_y,fill=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=a, x='DAYS_LAST_PHONE_CHANGE',hue=target_y,fill=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=a, x='DAYS_EMPLOYED',hue=target_y,fill=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=a, x='YEARS_REGISTRATION',hue=target_y,fill=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=a, x='YEARS_BIRTH',hue=target_y,fill=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=a, x='YEARS_LAST_PHONE_CHANGE',hue=target_y,fill=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=a, x='YEARS_EMPLOYED',hue=target_y,fill=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##The density plots reveal not a lot so we will use \n## other methods to select the features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Now for the remaining categorical variables we will \n## do target encoding. which means we replace all the \n##classes with their respective means. This converts this\n##into a continuous variable which may be more helpful\n##than a label or one hot encoding because this now contains\n##significant information from the target labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"te=TargetEncoder()\ntrain_data_removing_missing=te.fit_transform(train_data_removing_missing,target_y)\ntrain_data_removing_missing.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Now seeing the distribution of target encoded variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_encoded_columns=[ 'EMERGENCYSTATE_MODE',\n       'OCCUPATION_TYPE', 'NAME_TYPE_SUITE', 'CODE_GENDER', 'FLAG_OWN_CAR',\n       'FLAG_OWN_REALTY', 'ORGANIZATION_TYPE', 'NAME_CONTRACT_TYPE',\n       'WEEKDAY_APPR_PROCESS_START', 'NAME_HOUSING_TYPE', 'NAME_FAMILY_STATUS',\n       'NAME_EDUCATION_TYPE', 'NAME_INCOME_TYPE']\ncontinuous_variable_distributions(train_data_removing_missing[target_encoded_columns],'displot')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Now seeing the density plots of the target encoded variables\n##with the target variable so as to see which target encoded variable\n## is important","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=train_data_removing_missing, x='NAME_TYPE_SUITE',hue=target_y,fill=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=train_data_removing_missing, x='CODE_GENDER',hue=target_y,fill=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=train_data_removing_missing, x='FLAG_OWN_CAR',hue=target_y,fill=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=train_data_removing_missing, x='FLAG_OWN_REALTY',hue=target_y,fill=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=train_data_removing_missing, x='ORGANIZATION_TYPE',hue=target_y,fill=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=train_data_removing_missing, x='NAME_CONTRACT_TYPE',hue=target_y,fill=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=train_data_removing_missing, x='WEEKDAY_APPR_PROCESS_START',hue=target_y,fill=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=train_data_removing_missing, x='NAME_HOUSING_TYPE',hue=target_y,fill=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=train_data_removing_missing, x='NAME_FAMILY_STATUS',hue=target_y,fill=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=train_data_removing_missing, x='NAME_EDUCATION_TYPE',hue=target_y,fill=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=train_data_removing_missing, x='NAME_INCOME_TYPE',hue=target_y,fill=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##The density plots reveal not a lot so we will use \n## other methods to select the features     \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## We now take all the continuous variables\n##along with the newly target encoded variables and \n##pass it through an f_classif function which a sort of an anova\n## test to see which continuous variable is important for target\n##variable prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"['EXT_SOURCE_1',\n 'EXT_SOURCE_3',\n 'AMT_CREDIT',\n 'AMT_INCOME_TOTAL',\n 'AMT_GOODS_PRICE',\n 'AMT_ANNUITY',\n 'DAYS_REGISTRATION',\n 'DAYS_BIRTH',\n 'DAYS_EMPLOYED',\n 'DAYS_LAST_PHONE_CHANGE',\n 'FLOORSMAX_MODE',\n 'FLOORSMAX_MEDI',\n 'FLOORSMAX_AVG',\n 'YEARS_BEGINEXPLUATATION_MODE',\n 'YEARS_BEGINEXPLUATATION_MEDI',\n 'YEARS_BEGINEXPLUATATION_AVG',\n 'TOTALAREA_MODE',\n 'EMERGENCYSTATE_MODE',\n 'OCCUPATION_TYPE',\n 'AMT_REQ_CREDIT_BUREAU_HOUR',\n 'AMT_REQ_CREDIT_BUREAU_DAY',\n 'AMT_REQ_CREDIT_BUREAU_WEEK',\n 'AMT_REQ_CREDIT_BUREAU_MON',\n 'AMT_REQ_CREDIT_BUREAU_QRT',\n 'AMT_REQ_CREDIT_BUREAU_YEAR',\n 'NAME_TYPE_SUITE',\n 'OBS_30_CNT_SOCIAL_CIRCLE',\n 'DEF_30_CNT_SOCIAL_CIRCLE',\n 'OBS_60_CNT_SOCIAL_CIRCLE',\n 'DEF_60_CNT_SOCIAL_CIRCLE',\n 'EXT_SOURCE_2',\n 'CNT_FAM_MEMBERS',\n 'NAME_CONTRACT_TYPE',\n 'CODE_GENDER',\n 'FLAG_OWN_CAR',\n 'FLAG_OWN_REALTY',\n 'WEEKDAY_APPR_PROCESS_START',\n 'ORGANIZATION_TYPE',\n 'REGION_POPULATION_RELATIVE',\n 'NAME_HOUSING_TYPE',\n 'NAME_FAMILY_STATUS',\n 'NAME_EDUCATION_TYPE',\n 'NAME_INCOME_TYPE',\n 'YEARS_EMPLOYED',\n 'YEARS_REGISTRATION',\n 'YEARS_BIRTH',\n 'YEARS_LAST_PHONE_CHANGE']\n\nfs_cat_an = SelectKBest(score_func=f_classif, k=5)\nm=train_data_removing_missing[cols]\nfs_cat_an.fit(m, target_y)\nX_train_fs_cat = fs_cat_an.transform(m)\ncols=fs_cat_an.get_support()\nselected_feature_anova= []\nfor bool,feature in zip(cols,m.columns.to_list()):\n    if bool:\n        selected_feature_anova.append(feature)\n    \nselected_feature_anova\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##From the anova test our important variables are:-['DAYS_BIRTH','DAYS_EMPLOYED','YEARS_BIRTH','YEARS_LAST_PHONE_CHANGE','YEARS_EMPLOYED']\n## We take years employed and drop days_employed because days employed is used in creating the years employed variable and similarly\n## we do it for yera birth\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## We take the features selected from anova as well as ext_source_1 and ext_source_3 because these two are \n##important variables. We selected some domain based features like amt_annuity, amt_credit, amt_income_total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfitted_data1=train_data_removing_missing[['EXT_SOURCE_1','EXT_SOURCE_3','YEARS_BIRTH','AMT_ANNUITY','AMT_CREDIT','AMT_INCOME_TOTAL','YEARS_EMPLOYED','YEARS_LAST_PHONE_CHANGE']] \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Now trying different models starting with gradient boost \n##First we use grid search cross validation to find optimal number of \n##estimators. Our performance metric or scoring metric is roc_auc\n## Let us tune some models and find the optimal hyperparameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_test1={'n_estimators': range(20,100,10)}\ngsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,max_depth=8,max_features='sqrt',subsample=0.8,random_state=10), \nparam_grid = param_test1, scoring='roc_auc',n_jobs=4, cv=5)\ngsearch1.fit(fitted_data1,target_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Optimal number of estimators hyperparameter is 90","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gsearch1.best_estimator_,gsearch1.best_score_,gsearch1.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Now we use grid search to find optimal min_samples_split hyperparameter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_test2={'min_samples_split':range(1000,2100,200)}\ngsearch2 = GridSearchCV(estimator = GradientBoostingClassifier(n_estimators=90,learning_rate=0.1,min_samples_leaf=50,max_features='sqrt',subsample=0.8,random_state=10), \nparam_grid = param_test2, scoring='roc_auc',n_jobs=4, cv=5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## we get min_samples_split as 1800","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gsearch2.fit(fitted_data1,target_y)\ngsearch2.best_estimator_,gsearch2.best_score_,gsearch2.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Now we use grid search cv to find optimal min_samples_leaf hyperparameter\n## which comes out to be 30","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_test3={ 'min_samples_leaf':range(30,100,10)}\ngsearch3 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, max_depth=8,min_samples_split=1000,max_features='sqrt',subsample=0.8,random_state=10), \nparam_grid = param_test3, scoring='roc_auc',n_jobs=-1,cv=5)\ngsearch3.fit(fitted_data1,target_y)\ngsearch3.best_estimator_,gsearch3.best_score_,gsearch3.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Now we find the optimal subsample hyperparameter\n## which is 0.9","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_test4={ 'subsample':[0.5,0.6,0.7,0.75,0.8,0.85,0.9]}\ngsearch4 = GridSearchCV(estimator = GradientBoostingClassifier(n_estimators=90,learning_rate=0.1, max_depth=8,min_samples_split=1000,max_features=5,min_samples_leaf=80,random_state=10), \nparam_grid = param_test4, scoring='roc_auc',n_jobs=-1, cv=5)\ngsearch4.fit(fitted_data1,target_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gsearch4.best_estimator_,gsearch4.best_score_,gsearch4.best_params_\n#scores=cross_val_score(gsearch5.best_estimator_,fitted_data1,target_y,cv=5,scoring='roc_auc')\n#np.mean(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Now trying LGBM,Xgboost, Random Forest and Logistic regression \n##without any model tuning. We see that logistic model has the lowest\n## auc and lgbm and xgboost have considerably high auc. But our tuned\n## gradient boost model above has the highest auc compared to these \n## four models below. So we proceed with this model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=LGBMClassifier()\nkfold=StratifiedKFold(n_splits=10,shuffle=True,random_state=7)\nmodel.fit(fitted_data1,target_y)\nscores=cross_val_score(model,fitted_data1,target_y,cv=kfold,scoring='roc_auc')\nnp.mean(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=RandomForestClassifier()\n\nmodel.fit(fitted_data1,target_y)\nscores=cross_val_score(model,fitted_data1,target_y,cv=5,scoring='roc_auc')\nnp.mean(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=XGBClassifier()\nkfold=StratifiedKFold(n_splits=10,shuffle=True,random_state=7)\nmodel.fit(fitted_data1,target_y)\nscores=cross_val_score(model,fitted_data1,target_y,cv=kfold,scoring='roc_auc')\nnp.mean(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=LogisticRegression()\nkfold=StratifiedKFold(n_splits=10,shuffle=True,random_state=7)\nmodel.fit(fitted_data1,target_y)\nscores=cross_val_score(model,fitted_data1,target_y,cv=kfold,scoring='roc_auc')\nnp.mean(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##I did not optimise max_depth any where and it may help\n##The max features above I took as sqrt which is default but\n## I took five here as the max_features to chose to see the difference \n##in model performance. There is not much difference between our previously\n##trained model with max features parameter as sqrt and then the model \n##with five.We will go with five as of now","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold=StratifiedKFold(n_splits=10,shuffle=True,random_state=7)\nmodel=GradientBoostingClassifier(max_depth=8, max_features=5, min_samples_leaf=80,\n                           min_samples_split=1000, n_estimators=90,\n                           random_state=10, subsample=0.9)\nscores=cross_val_score(model,fitted_data1,target_y,cv=kfold,scoring='roc_auc',n_jobs=-1)\nnp.mean(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##We see from this we can see there is heavy imbalance between the classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum(target_y==0)/len(target_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## To mitigate the effects of class balance we use random undersampling\n##and random oversampling which combines the effect of duplicating minority\n##classes and removing majority classes.This must be done before cross validation\n##Also for class imbalance we use stratified k fold which ensures the proportion \n##of classes is the same in the folds as in the entire training set. We see that \n##there is no increase in auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"over=RandomOverSampler(sampling_strategy=0.1)\nunder=RandomUnderSampler(sampling_strategy=0.5)\nmodel=GradientBoostingClassifier(max_depth=8, max_features=5, min_samples_leaf=80,\n                           min_samples_split=1000, n_estimators=90,\n                           random_state=10, subsample=0.9)\nsteps=[('o',over),('u',under),('m',model)]\npipeline=Pipeline(steps=steps)\nkfold=StratifiedKFold(n_splits=10,shuffle=True,random_state=7)\nscores=cross_val_score(pipeline,fitted_data1,target_y,cv=kfold,scoring='roc_auc')\nnp.mean(scores)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## With random oversampling and random undersampling applied \n##along with repeated stratified kfold we see no increase in auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"over=RandomOverSampler(sampling_strategy=0.1)\nunder=RandomUnderSampler(sampling_strategy=0.5)\nmodel=GradientBoostingClassifier(max_depth=8, max_features=5, min_samples_leaf=80,\n                           min_samples_split=1000, n_estimators=90,\n                           random_state=10, subsample=0.9)\nsteps=[('o',over),('u',under),('m',model)]\npipeline=Pipeline(steps=steps)\nkfold=RepeatedStratifiedKFold(n_splits=10,random_state=7,n_repeats=3)\nscores=cross_val_score(pipeline,fitted_data1,target_y,cv=kfold,scoring='roc_auc')\nnp.mean(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Trying random over sampling and random under sampling\n## with repeated stratified kfold on lgbm and xgboost models\n## We again see these two models are lower in auc performance\n## than our gradient boost model we picked above.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"over=RandomOverSampler(sampling_strategy=0.1)\nunder=RandomUnderSampler(sampling_strategy=0.5)\nmodel=LGBMClassifier()\nsteps=[('o',over),('u',under),('m',model)]\npipeline=Pipeline(steps=steps)\nkfold=RepeatedStratifiedKFold(n_splits=10,random_state=7,n_repeats=3)\nscores=cross_val_score(pipeline,fitted_data1,target_y,cv=kfold,scoring='roc_auc')\nnp.mean(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"over=RandomOverSampler(sampling_strategy=0.1)\nunder=RandomUnderSampler(sampling_strategy=0.5)\nmodel=XGBClassifier()\nsteps=[('o',over),('u',under),('m',model)]\npipeline=Pipeline(steps=steps)\nkfold=RepeatedStratifiedKFold(n_splits=10,random_state=7,n_repeats=3)\nscores=cross_val_score(pipeline,fitted_data1,target_y,cv=kfold,scoring='roc_auc')\nnp.mean(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## We try to optimise max_depth in xgboost classifier\n## using grid search cv and this is the final model we try \n## Again this model is lower in auc performance with our gradient\n## boost model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_params = {\n 'max_depth':[4,6,8,10,12]\n}\nmodel=XGBClassifier()\n\nmodel = GridSearchCV(estimator = model,param_grid = test_params,scoring='roc_auc')\nmodel.fit(fitted_data1,target_y)\nmodel.best_params_,model.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## So the final model selected has auc score of 0.71559\n## Our gradient boost model has the following parameters:-\n## max_depth=8, max_features=5, min_samples_leaf=30,min_samples_split=1800, n_estimators=90,subsample=0.9\n## We do stratified k fold cross vaidation without any random oversampling or undersampling ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##I took major inspiration from this notebook:\n## https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Please comment on what can be done to improve the model performance\n## This is my first notebook so any advise on how to improve is welcomed","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}