{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import LabelEncoder\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c54e1559611512ebd447ac24f2226c2fffd61dcd","_cell_guid":"2cdca894-e637-43a9-8f80-5791c2bb9041","trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Basic LightGBM - First Step"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nMAX_EVALS = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=pd.read_csv('../input/application_train.csv')\nfeatures=features.sample(n=16000,random_state=42)\nprint(features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = features.select_dtypes('number')\nlabels = np.array(features['TARGET'])\nfeatures = features.drop(columns = ['TARGET', 'SK_ID_CURR'])\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 6000, random_state = 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training features shape: \", train_features.shape)\nprint(\"Testing features shape: \", test_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = lgb.Dataset(data = train_features, label = train_labels)\ntest_set = lgb.Dataset(data = test_features, label = test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb.LGBMClassifier()\ndefault_params = model.get_params()\n\n# Remove the number of estimators because we set this to 10000 in the cv call\ndel default_params['n_estimators']\n\n# Cross validation with early stopping\ncv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, \n                    metrics = 'auc', nfold = 5, seed = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(max(cv_results['auc-mean']))\nprint(len(cv_results['auc-mean']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.n_estimators = len(cv_results['auc-mean'])\n# Train and make predicions with model\nmodel.fit(train_features, train_labels)\npreds = model.predict_proba(test_features)[:, 1]\nbaseline_auc = roc_auc_score(test_labels, preds)\nprint('The model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(hyperparameters, iteration):\n    \n    if 'n_estimators' in hyperparameters.keys():\n        del hyperparameters['n_estimators']   \n\n    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold =5, \n                        early_stopping_rounds = 100, metrics = 'auc', seed = 42)\n\n    score = cv_results['auc-mean'][-1]\n    estimators = len(cv_results['auc-mean'])\n    hyperparameters['n_estimators'] = estimators     \n    return [score, hyperparameters, iteration]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score, params, iteration = objective(default_params, 1)\n\nprint('The cross-validation ROC AUC was {:.5f}.'.format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb.LGBMModel()\nmodel.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n    'boosting_type': ['gbdt', 'goss', 'dart'],\n    'num_leaves': list(range(20, 150)),\n    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n    'subsample_for_bin': list(range(20000, 300000, 20000)),\n    'min_child_samples': list(range(20, 500, 5)),\n    'reg_alpha': list(np.linspace(0, 1)),\n    'reg_lambda': list(np.linspace(0, 1)),\n    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n    'subsample': list(np.linspace(0.5, 1, 100)),\n    'is_unbalance': [True, False]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\nrandom.seed(50)\n\n# Randomly sample a boosting type\nboosting_type = random.sample(param_grid['boosting_type'], 1)[0]\n\n# Set subsample depending on boosting type\nsubsample = 1.0 if boosting_type == 'goss' else random.sample(param_grid['subsample'], 1)[0]\n\nprint('Boosting type: ', boosting_type)\nprint('Subsample ratio: ', subsample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))\n\ngrid_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\n\ndef grid_search(param_grid, max_evals = MAX_EVALS):\n    \"\"\"Grid search algorithm (with limit on max evals)\"\"\"\n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))\n    keys, values = zip(*param_grid.items())    \n    i = 0\n    for v in itertools.product(*values):\n        hyperparameters = dict(zip(keys, v))\n        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n        eval_results = objective(hyperparameters, i)       \n        results.loc[i, :] = eval_results\n        i += 1\n        if i > MAX_EVALS:\n            break\n    results.sort_values('score', ascending = False, inplace = True)\n    results.reset_index(inplace = True)    \n    return results    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_results = grid_search(param_grid)\nprint('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))\nprint('\\nThe best hyperparameters were:')\nimport pprint\npprint.pprint(grid_results.loc[0, 'params'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search_params = grid_results.loc[0, 'params']\nmodel = lgb.LGBMClassifier(**grid_search_params, random_state=42)\nmodel.fit(train_features, train_labels)\n\npreds = model.predict_proba(test_features)[:, 1]\n\nprint('The best model from grid search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(50)\n\n# Randomly sample from dictionary\nrandom_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n# Deal with subsample ratio\nrandom_params['subsample'] = 1.0 if random_params['boosting_type'] == 'goss' else random_params['subsample']\n\nrandom_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_search(param_grid, max_evals = MAX_EVALS):\n    \"\"\"Random search for hyperparameter optimization\"\"\"\n    \n    # Dataframe for results\n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                                  index = list(range(MAX_EVALS)))\n    \n    # Keep searching until reach max evaluations\n    for i in range(MAX_EVALS):\n        \n        # Choose random hyperparameters\n        hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n\n        # Evaluate randomly selected hyperparameters\n        eval_results = objective(hyperparameters, i)\n        \n        results.loc[i, :] = eval_results\n    \n    # Sort with best score on top\n    results.sort_values('score', ascending = False, inplace = True)\n    results.reset_index(inplace = True)\n    return results ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_results = random_search(param_grid)\n\nprint('The best validation score was {:.5f}'.format(random_results.loc[0, 'score']))\nprint('\\nThe best hyperparameters were:')\n\nimport pprint\npprint.pprint(random_results.loc[0, 'params'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search_params = random_results.loc[0, 'params']\n\n# Create, train, test model\nmodel = lgb.LGBMClassifier(**random_search_params, random_state = 42)\nmodel.fit(train_features, train_labels)\n\npreds = model.predict_proba(test_features)[:, 1]\n\nprint('The best model from random search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/application_train.csv')\ntest = pd.read_csv('../input/application_test.csv')\n\n# Extract the test ids and train labels\ntest_ids = test['SK_ID_CURR']\ntrain_labels = np.array(train['TARGET'].astype(np.int32)).reshape((-1, ))\n\ntrain = train.drop(columns = ['SK_ID_CURR', 'TARGET'])\ntest = test.drop(columns = ['SK_ID_CURR'])\n\nprint('Training shape: ', train.shape)\nprint('Testing shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nle_count = 0\nfor col in train:\n    if train[col].dtype == 'object':\n        if len(list(train[col].unique())) <= 2:\n            le.fit(train[col])\n            train[col] = le.transform(train[col])\n            test[col] = le.transform(test[col])\n            le_count += 1\nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.get_dummies(train)\ntest=pd.get_dummies(test)\nprint(train.shape,test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train.align(test, join = 'inner', axis = 1)\n\n# Add the target back in\n#train['TARGET'] = train_labels\n\nprint('Training Features shape: ', train.shape)\nprint('Testing Features shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = lgb.Dataset(train, label = train_labels)\n\nhyperparameters = dict(**random_results.loc[0, 'params'])\ndel hyperparameters['n_estimators']\n\n# Cross validation with n_folds and early stopping\ncv_results = lgb.cv(hyperparameters, train_set,\n                    num_boost_round = 10000, early_stopping_rounds = 100, \n                    metrics = 'auc', nfold = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb.LGBMClassifier(n_estimators = len(cv_results['auc-mean']), **hyperparameters)\nmodel.fit(train, train_labels)\n                        \n# Predictions on the test data\npreds = model.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': preds})\nsubmission.to_csv('submission_simple_features_random.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM - Second Step"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"app_train = pd.read_csv('../input/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbd1c4111df6f07bc0d479b51f50895e728b717a","_cell_guid":"d077aee0-5271-440e-bc07-6087eab40b74","trusted":true},"cell_type":"code","source":"app_test = pd.read_csv('../input/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2163ca09678b53dbe88388ccbc7d0e0f7d6c6230","_cell_guid":"5fb6ab16-1b38-4ecf-8123-e48c7c061773","trusted":true},"cell_type":"code","source":"app_train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b2611fb3cf392023c3f40fd2f7b96f56f5dee7d","_cell_guid":"0e93c1e2-f6b8-4a0b-82b6-7dad8df56048","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.set(style=\"whitegrid\",font_scale=1)\ng=sns.distplot(app_train['TARGET'],kde=False,hist_kws={\"alpha\": 1, \"color\": \"#DA1A32\"})\nplt.title('Distribution of target (1:default, 0:no default)',size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## About missing values"},{"metadata":{"_uuid":"7a2f5c72c45fa04d9fa95e8051ae595be806e9a2","_cell_guid":"fc4c675f-e4a1-4e4f-9ece-3c59e5c8f7fd","trusted":true},"cell_type":"code","source":"def missing_values_table(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98b0a82a3009b8f6d0bc718a2e1eaba779b4ace9","_cell_guid":"786881f0-235e-441c-8319-f715a3b7d920","trusted":true},"cell_type":"code","source":"missing_values = missing_values_table(app_train)\nmissing_values.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0672e40c3ab75a7901c0de35d248b322a227dc7f"},"cell_type":"markdown","source":"## Column Types"},{"metadata":{"trusted":true,"_uuid":"a03caadd76fa32f4b193e52467d4f39f2145d7b6"},"cell_type":"code","source":"# Number of each type of column\napp_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d021eda10939a19b141292d34491b357acd201a"},"cell_type":"code","source":"# Number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddfaae5c3dcc7ec6bb47a2dffc10d364e8d25355","_cell_guid":"70641d4d-1075-4837-8972-e58d70d8f242","trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nle_count = 0\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        if len(list(app_train[col].unique())) <= 2:\n            le.fit(app_train[col])\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            le_count += 1\nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6796c6dc793a08e162b6e20c6f185ef37bdf51f3","_cell_guid":"0851773b-39fd-4cf0-9a66-e30adeef3e57","trusted":true},"cell_type":"code","source":"app_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels_last=app_train['TARGET']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0d12a13cb95521c19b10d8829e8abe2b1118396","_cell_guid":"d99ca215-e893-490c-a6a4-83f3e8a067b3","trusted":true},"cell_type":"code","source":"train_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n\n# Add the target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a60be93c2d7d63855e6d65c1109f408ad85da134"},"cell_type":"code","source":"(app_train['DAYS_BIRTH'] / -365).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"600c59dd5d970d3ccfea3a6af0036d85958adc91"},"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(app_train['DAYS_EMPLOYED'],color=\"#DA1A32\")\nplt.title('Ditribution of employed days')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**350000 days of employment seems wrong.**"},{"metadata":{"trusted":true,"_uuid":"67ea87d9ef6974b1780a7db1eefd13f90f81b5be"},"cell_type":"code","source":"anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Replace it with nan.**"},{"metadata":{"trusted":true,"_uuid":"e23ec3cb89428f3dd994b572f718cc729740cfab"},"cell_type":"code","source":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Ditribution of employed days',color=\"#DA1A32\")\nplt.xlabel('Days')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0d7c77b2adecaa878f39cf86ffddcfbbe51a190"},"cell_type":"code","source":"app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlations"},{"metadata":{"_uuid":"d39d15d64db1f2c9015c6f542911ef9a9cac119e","_cell_guid":"02acdb8d-d95f-41b9-8ad1-e2b6cb26f398","trusted":true},"cell_type":"code","source":"# Find correlations with the target and sort\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6197819149feaff75176e64e54c65ea6be3864fe","_cell_guid":"e2ab3b7f-3a53-4495-a1de-31ad287f032a","trusted":true},"cell_type":"code","source":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20b21a6b4e15a726c29596abeb01346dc416729c","_cell_guid":"0479863d-cfa9-47ab-83e6-7d7877e3e939","trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 6))\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature engineering"},{"metadata":{},"cell_type":"markdown","source":"<font size=\"4\">Manual features. </font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"manual_features=app_train[['SK_ID_CURR','AMT_CREDIT','AMT_ANNUITY','AMT_INCOME_TOTAL',\n                           'DAYS_BIRTH','DAYS_EMPLOYED']]\nmanual_features_test=app_test[['SK_ID_CURR','AMT_CREDIT','AMT_ANNUITY','AMT_INCOME_TOTAL',\n                           'DAYS_BIRTH','DAYS_EMPLOYED']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=list(manual_features.columns)\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'median')\nmanual_features = imputer.fit_transform(manual_features)\nmanual_features_test = imputer.transform(manual_features_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"manual_features=pd.DataFrame(manual_features,columns=cols)\nmanual_features_test=pd.DataFrame(manual_features_test,columns=cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"manual_features['CREDIT_INCOME_PERCENT'] = manual_features['AMT_CREDIT'] /manual_features['AMT_INCOME_TOTAL']\nmanual_features['ANNUITY_INCOME_PERCENT'] = manual_features['AMT_ANNUITY'] / manual_features['AMT_INCOME_TOTAL']\nmanual_features['CREDIT_TERM'] = manual_features['AMT_ANNUITY'] / manual_features['AMT_CREDIT']\nmanual_features['DAYS_EMPLOYED_PERCENT'] = manual_features['DAYS_EMPLOYED'] / manual_features['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"manual_features_test['CREDIT_INCOME_PERCENT'] = manual_features_test['AMT_CREDIT'] / manual_features_test['AMT_INCOME_TOTAL']\nmanual_features_test['ANNUITY_INCOME_PERCENT'] = manual_features_test['AMT_ANNUITY'] / manual_features_test['AMT_INCOME_TOTAL']\nmanual_features_test['CREDIT_TERM'] = manual_features_test['AMT_ANNUITY'] / manual_features_test['AMT_CREDIT']\nmanual_features_test['DAYS_EMPLOYED_PERCENT'] = manual_features_test['DAYS_EMPLOYED'] / manual_features_test['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"4\">Polynomial features & Imputation of missing values. </font>"},{"metadata":{"_uuid":"a63d53dcac14c4ac2e31ea9c5e16b5d161c2415b","_cell_guid":"e5b0efd9-67ac-4aa0-91e9-2141a87a6a8a","trusted":true},"cell_type":"code","source":"poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'median')\n\npoly_target = poly_features['TARGET']\npoly_features = poly_features.drop(columns = ['TARGET'])\n\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n                                \npoly_transformer = PolynomialFeatures(degree = 3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72c5ecaae9c6ff038d16cbd9208f1abb69912631","_cell_guid":"2be7c1ab-d1e5-40f2-b8e7-e2b2ce1e2f9a","trusted":true},"cell_type":"code","source":"poly_transformer.fit(poly_features)\n\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"121f98d2ec9c81c5dabb911dc68562d0b2b6d737","_cell_guid":"7465d1e6-d360-4029-afa7-67cb34f60249","trusted":true},"cell_type":"code","source":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e712923de757457bb87a35ecaccd27007b351e6c","_cell_guid":"95725a63-f8f2-4680-8f7a-4252f04e7f7f","trusted":true},"cell_type":"code","source":"poly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\npoly_features['TARGET'] = poly_target\n\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed758ed436a86f92a8ee574999aa91089242ca7a"},"cell_type":"code","source":"poly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly_1 = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\napp_train_poly_3=app_train_poly_1.merge(manual_features,on = 'SK_ID_CURR', how = 'left')\n\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly_2 = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\napp_test_poly_4=app_test_poly_2.merge(manual_features_test,on = 'SK_ID_CURR', how = 'left')\n\napp_train_poly, app_test_poly = app_train_poly_3.align(app_test_poly_4, join = 'inner', axis = 1)\n\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train_poly=app_train_poly_3.T.drop_duplicates().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train_poly.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train_poly.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_2=list(app_train_poly.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(app_train_poly.shape,app_test_poly.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import Imputer\n\nif 'TARGET' in app_train_poly:\n    train = app_train_poly.drop(columns = ['TARGET'])\nelse:\n    train = app_train_poly.copy()\n\nfeatures = list(train.columns)\n\ntest = app_test_poly.copy()\n\nimputer = Imputer(strategy = 'median')\n\nimputer.fit(train)\n\ntrain = imputer.transform(train)\ntest = imputer.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.DataFrame(train,columns=col_2)\ntest=pd.DataFrame(test,columns=col_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['TARGET']=app_train['TARGET']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_last=train.copy()\n#test_last=test.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nMAX_EVALS = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=train.sample(n=16000,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = np.array(features['TARGET'])\nfeatures = features.drop(columns = ['TARGET', 'SK_ID_CURR'])\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 6000, random_state = 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training features shape: \", train_features.shape)\nprint(\"Testing features shape: \", test_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = lgb.Dataset(data = train_features, label = train_labels)\ntest_set = lgb.Dataset(data = test_features, label = test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb.LGBMClassifier()\ndefault_params = model.get_params()\n\n# Remove the number of estimators because we set this to 10000 in the cv call\ndel default_params['n_estimators']\n\n# Cross validation with early stopping\ncv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, \n                    metrics = 'auc', nfold = 5, seed = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(max(cv_results['auc-mean']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(cv_results['auc-mean']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.n_estimators = len(cv_results['auc-mean'])\n# Train and make predicions with model\nmodel.fit(train_features, train_labels)\npreds = model.predict_proba(test_features)[:, 1]\nbaseline_auc = roc_auc_score(test_labels, preds)\nprint('The baseline model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(hyperparameters, iteration):\n    \"\"\"Objective function for grid and random search. Returns\n       the cross validation score from a set of hyperparameters.\"\"\"\n    \n    # Number of estimators will be found using early stopping\n    if 'n_estimators' in hyperparameters.keys():\n        del hyperparameters['n_estimators']   \n     # Perform n_folds cross validation\n    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold =5, \n                        early_stopping_rounds = 100, metrics = 'auc', seed = 42)\n    # results to retun\n    score = cv_results['auc-mean'][-1]\n    estimators = len(cv_results['auc-mean'])\n    hyperparameters['n_estimators'] = estimators     \n    return [score, hyperparameters, iteration]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score, params, iteration = objective(default_params, 1)\n\nprint('The cross-validation ROC AUC was {:.5f}.'.format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb.LGBMModel()\nmodel.get_params()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid search"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n    'boosting_type': ['gbdt', 'goss', 'dart'],\n    'num_leaves': list(range(20, 150)),\n    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n    'subsample_for_bin': list(range(20000, 300000, 20000)),\n    'min_child_samples': list(range(20, 500, 5)),\n    'reg_alpha': list(np.linspace(0, 1)),\n    'reg_lambda': list(np.linspace(0, 1)),\n    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n    'subsample': list(np.linspace(0.5, 1, 100)),\n    'is_unbalance': [True, False]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\nrandom.seed(50)\n\n# Randomly sample a boosting type\nboosting_type = random.sample(param_grid['boosting_type'], 1)[0]\n\n# Set subsample depending on boosting type\nsubsample = 1.0 if boosting_type == 'goss' else random.sample(param_grid['subsample'], 1)[0]\n\nprint('Boosting type: ', boosting_type)\nprint('Subsample ratio: ', subsample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))\n\ngrid_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\n\ndef grid_search(param_grid, max_evals = MAX_EVALS):\n    \"\"\"Grid search algorithm (with limit on max evals)\"\"\"\n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))\n    keys, values = zip(*param_grid.items())    \n    i = 0\n    for v in itertools.product(*values):\n        hyperparameters = dict(zip(keys, v))\n        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n        eval_results = objective(hyperparameters, i)       \n        results.loc[i, :] = eval_results\n        i += 1\n        if i > MAX_EVALS:\n            break\n    results.sort_values('score', ascending = False, inplace = True)\n    results.reset_index(inplace = True)    \n    return results    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_results = grid_search(param_grid)\nprint('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))\nprint('\\nThe best hyperparameters were:')\nimport pprint\npprint.pprint(grid_results.loc[0, 'params'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search_params = grid_results.loc[0, 'params']\nmodel = lgb.LGBMClassifier(**grid_search_params, random_state=42)\nmodel.fit(train_features, train_labels)\n\npreds = model.predict_proba(test_features)[:, 1]\n\nprint('The best model from grid search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random search"},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(50)\nrandom_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\nrandom_params['subsample'] = 1.0 if random_params['boosting_type'] == 'goss' else random_params['subsample']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_search(param_grid, max_evals = MAX_EVALS):\n   \n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                                 index = list(range(MAX_EVALS)))\n    for i in range(MAX_EVALS):\n        hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n        eval_results = objective(hyperparameters, i)\n        results.loc[i, :] = eval_results\n        \n    results.sort_values('score', ascending = False, inplace = True)\n    results.reset_index(inplace = True)\n    return results ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_results = random_search(param_grid)\n\nprint('The best validation score was {:.5f}'.format(random_results.loc[0, 'score']))\nprint('\\nThe best hyperparameters were:')\n\nimport pprint\npprint.pprint(random_results.loc[0, 'params'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search_params = random_results.loc[0, 'params']\n\nmodel = lgb.LGBMClassifier(**random_search_params, random_state = 42)\nmodel.fit(train_features, train_labels)\n\npreds = model.predict_proba(test_features)[:, 1]\n\nprint('The best model from random search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train = pd.read_csv('../input/application_train.csv')\n#test = pd.read_csv('../input/application_test.csv')\n\n# Extract the test ids and train labels\n#test_ids = test['SK_ID_CURR']\n#train_labels = np.array(train['TARGET'].astype(np.int32)).reshape((-1, ))\n\n#train = train.drop(columns = ['SK_ID_CURR', 'TARGET'])\n#test = test.drop(columns = ['SK_ID_CURR'])\n\n#print('Training shape: ', train.shape)\n#print('Testing shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#le = LabelEncoder()\n#le_count = 0\n#for col in train:\n#    if train[col].dtype == 'object':\n#        if len(list(train[col].unique())) <= 2:\n#            le.fit(train[col])\n#            train[col] = le.transform(train[col])\n#            test[col] = le.transform(test[col])\n#            le_count += 1\n#print('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train = pd.get_dummies(train)\n#test=pd.get_dummies(test)\n#print(train.shape,test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Align the training and testing data, keep only columns present in both dataframes\n#train, test = train.align(test, join = 'inner', axis = 1)\n\n# Add the target back in\n#train['TARGET'] = train_labels\n\n#print('Training Features shape: ', train.shape)\n#print('Testing Features shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train=train.select_dtypes('number')\n#test=test.select_dtypes('number')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = lgb.Dataset(train, label = train_labels_last)\n\nhyperparameters = dict(**random_results.loc[0, 'params'])\ndel hyperparameters['n_estimators']\n\n# Cross validation with n_folds and early stopping\ncv_results = lgb.cv(hyperparameters, train_set,\n                    num_boost_round = 10000, early_stopping_rounds = 100, \n                    metrics = 'auc', nfold = 5)\nprint(max(cv_results['auc-mean']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['SK_ID_CURR'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ids = test['SK_ID_CURR']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.drop(columns=['TARGET','SK_ID_CURR'])\ntest=test.drop(columns='SK_ID_CURR')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape,test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb.LGBMClassifier(n_estimators = len(cv_results['auc-mean']), **hyperparameters)\nmodel.fit(train, train_labels_last)\n                        \n# Predictions on the test data\npreds = model.predict_proba(test)[:, 1]\n#auc1=roc_auc_score(y_test, preds)\n#print(auc1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': preds})\n#submission['SK_ID_CURR']=submission['SK_ID_CURR'].astype('int32')\n#submission.to_csv('submission_simple_features_random.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}