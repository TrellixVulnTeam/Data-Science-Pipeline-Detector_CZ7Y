{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Description**"},{"metadata":{},"cell_type":"markdown","source":"Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.\n\nHome Credit Group\n\n![](https://storage.googleapis.com/kaggle-media/competitions/home-credit/about-us-home-credit.jpg)\n\nHome Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n\nWhile Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful."},{"metadata":{},"cell_type":"markdown","source":"**Evaluation**"},{"metadata":{},"cell_type":"markdown","source":"For each SK_ID_CURR in the test set, you must predict a probability for the TARGET variable. \n* => 과거 대출 데이타를 가지고, 대출요청자가 대출을 갚을 수 있는지 없는지 예상.\n* => Standard Supervised Classification Task.\n* => Supervised: The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features ( 라벨을 예상 )\n* => Classification: The label is a binary variable, 0 (will repay loan on time), 1 (will have difficulty repaying loan) ( 라벨이 binary varaible임. 0은 제때 갚을 듯. 1은 갚기 어려울 듯 )"},{"metadata":{},"cell_type":"markdown","source":"**Data**"},{"metadata":{},"cell_type":"markdown","source":"![](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)"},{"metadata":{},"cell_type":"markdown","source":"* application_{train|test}.csv\n\nThis is the main table, broken into two files for Train (with TARGET) and Test (without TARGET).\n\n=> TARGET : 0: 대출 상환함, 1:대출 상환되지 않았음.\n\n=> 각 대출 신청 정보가 포함됨. 모든 대출은 고유한 행을 가지고, SK_ID_CURR 로 식별. \n\nStatic data for all applications. One row represents one loan in our data sample. => 한 행이 한 대출.\n\n* bureau.csv\n\nAll client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample). \n\n=> 다른 금융 기관에 있었던 이전 대출, 이전 대출은 한 행. 그러나, 대출 한 건은 여러 개 이전 대출 신청(?)을 가질 수 있음\n\nFor every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date. => 크레딧 수(?) 만큼 행이 있다. 대출 신청 전, 대출했던 모든 이력(?)\n\n* bureau_balance.csv\n\nMonthly balances of previous credits in Credit Bureau. => 이전 대출(Credit?) 월간 데이타. 각 행은 이전 크레딧(?대출?) 한 달이고, 단일 이전 크레딧은 크레딧 길이(대출 기간?) 각각 월별로 여러 행 가질 수 있음.(?)\n\nThis table has one row for each month of history of every previous credit reported to Credit Bureau – i.e the table has (#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows. => 한달에 한 행. \n\n* POS_CASH_balance.csv\n\nMonthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit. \n\n=> 이전 판매 시점 혹은 현금 대출에 대한 월별 데이타\n\n=> 각 행은 이전 판매 시점 또한 현금 대출 한 달 의미하고, 이전 대출은 여러 행 가질 수 있음.\n\nThis table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows. \n\n* credit_card_balance.csv\n\nMonthly balance snapshots of previous credit cards that the applicant has with Home Credit.\n\n=> 이전 신용 카드 월별 데이타. 각 행은 신용 카드 잔고 한 달 기준이고, 한 신용 카드는 여러 행을 가질 수 있음.\n\nThis table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has (#loans in sample * # of relative previous credit cards * # of months where we have some history observable for the previous credit card) rows. \n\n* previous_application.csv\n\nAll previous applications for Home Credit loans of clients who have loans in our sample.\n\nThere is one row for each previous application related to loans in our data sample.\n\n=> 대출을 가진 이전 신청자들 \n\n=> 각 현재 대출은 여러 개 이전 대출을 가질 수 있음.\n\n=> 이전 각 지원서는 한 행이고, SK_ID_PREV로 식별.\n\n* installments_payments.csv\n\nRepayment history for the previously disbursed credits in Home Credit related to the loans in our sample.\n=> 이전 반환 내역\n\n=> 이전 대출에 대한 지불 내역, 모든 결제는 한 행이고, 누락된 결재도 한 행임.\n\nThere is a) one row for every payment that was made plus b) one row each for missed payment.\n=> 모든 결제(대출하고 상환?) 는 한 행\n=> 상환하지 못한 건은 한 행\n\nOne row is equivalent to one payment of one installment OR one installment corresponding to one payment of one previous Home Credit credit related to loans in our sample.\n\n* HomeCredit_columns_description.csv\n\nThis file contains descriptions for the columns in the various data files.\n\n=> 모든 컬럼 정의."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Metric : ROC AUC**"},{"metadata":{},"cell_type":"markdown","source":"* 0/1을 판단할 때, ROC 모델을 사용함. Receiver Operating Characteristic Area Under the Curve ( ROC AUC, also sometimes called AUROC )\n* ( https://www.youtube.com/watch?v=3llmZMHHL_8 모델평가, binary classification, precision recall, ROC )"},{"metadata":{},"cell_type":"markdown","source":"* common classification metric : ROC AUC ( AUROC ), Receiver Operationg Characteristic Area Under the Curve\n\n* 양분된 결과를 예측하는 테스트의 정확도를 평가하기 위하여 흔히 두 가지 지표, 민감도(sensitivity)와 특이도(specificity)를 사용\n\nex) 어떤 건강 상태를 가지고 있는 경우와 그렇지 않은 경우를 얼마나 잘 구분할 수 있는지를 의미 \n\n민감도(sensitivity) \n\n- 1인 케이스에 대해 1이라고 예측한 것 \n\n특이도(specificity) \n\n- 0인 케이스에 대해 0이라고 예측한 것 \n\n양성율(True Positive Rate; TPR) \n\n- TPR = 민감도 = 1 - 위음성율, true accept rate \n\n- 1인 케이스에 대해 1로 맞게 예측한 비율 \n\n- ex) 암환자를 진찰해서 암이라고 진단 함 \n\n위양성율(False Positive Rate; FPR) \n\n- FPR = 1 - 특이도, false accept rate \n\n- 0인 케이스에 대해 1로 잘못 예측한 비율 \n\n- ex) 암환자가 아닌데 암이라고 진단 함 \n\n\nTPR과 FPR은 서로 반비례적인 관계에 있음 \n\n- 암환자를 진단할 때, 성급한 의사는 아주 조금의 징후만 보여도 암인 것 같다고 할 것이다. \n\n- 이 경우 TPR은 1에 가까워지지만 FPR은 반대로 매우 낮아져버린다. (정상인 사람도 다 암이라고 하니까)\n\n- 반대로 돌팔이 의사라서 암환자를 알아내지 못한다면, 모든 환자에 대해 암이 아니라고 할 것이다. \n\n- 이 경우 TPR은 매우 낮아져 0에 가까워지지만 반대로 FPR은 급격히 높아져 1에 가까워질 것이다.(암환자라는 진단 자체를 안하므로, 암환자라고 잘못 진단 하는 경우가 없음) \n\n이처럼 TPR과 FPR은 둘다, 어떤 기준(언제 1이라고 예측 할 지)을 연속적으로 바꾸면서 측정 해야한다. \n\n결국 TPR과 FPR의 여러가지 상황을 고려해서 성능을 판단해야 하는데, 이것을 한눈에 볼 수 있게 한 것이 바로 ROC 커브이다. \n\n그래서 ROC커브는 이것들을 그래프로 표현하여, 어떤 지점을 기준으로 잡을 지 결정하기 쉽게 시각화 한 것이다. \n\nROC커브의 밑면적(the Area Under a ROC Curve; AUC; AUROC) \n\n- ROC 커브의 X,Y축은 [0,1]의 범위며, (0,0) 에서 (1,1)을 잇는 곡선 \n\n- ROC 커브의 밑 면적이 1에 가까울수록(즉, 왼쪽 상단 꼭지점에 다가갈수록) 좋은 성능 \n\n- 이때의 면적(AUC)은 0.5~1의 범위를 가짐(0.5면 성능이 전혀 없음. 1이면 최고의 성능) \n\nAUC 해석 \n\n- 쉽게 1로 예측하는 경우 민감도는 높아지지만 모든 경우를 1이라고 하므로 특이도가 낮아진다. \n\n- 그러므로 이 민감도와 특이도 모두 1에 가까워야 의미가 있음 \n\n- 따라서ROC커브를 그릴때 특이도를 1-특이도를 X축에 놓고, Y축에 민감도를 놓는다. \n\n- 그러면 x=0일때 y=1인 경우 최적의 성능이고, 점점 우측 아래로 갈수록, 즉 특이도가 감소하는 속도에비해 얼마나 빠르게 민감도가 \n\n증가하는지를 나타냄. \n\n- AUC값은 전체적인 민감도와 특이도의 상관 관계를 보여줄 수 있어 매우 편리한 성능 측정 기준임 \n\nAUC = 0.5인 경우 \n\n- 특이도가 감소하는만큼 민감도가 증가하므로 민감도와 특이도를 동시에 높일 수 있는 지점이 없음 \n\n- 특이도가 1일때 민감도는 0, 특이도가 0일때 민감도는 1이되는 비율이 정확하게 trade off관계로, 두 값의 합이 항상 1임  \n"},{"metadata":{},"cell_type":"markdown","source":"** 이 문제 해결을 위해서 다음 내용을 아는게 좋다**\n* Manual Feature Engineering Part One ( https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering )\n* Manual Feature Engineering Part Two ( https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2 )\n* Introduction to Automated Feature Engineering ( https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics )\n* Advanced Automated Feature Engineering ( https://www.kaggle.com/willkoehrsen/tuning-automated-feature-engineering-exploratory )\n* Feature Selection ( https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection )\n* Intro to Model Tuning: Grid and Random Search ( https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search )\n* Automated Model Tuning ( https://www.kaggle.com/willkoehrsen/automated-model-tuning )\n* Model Tuning Results ( https://www.kaggle.com/willkoehrsen/model-tuning-results-random-vs-bayesian-opt/notebook )"},{"metadata":{},"cell_type":"markdown","source":"* Import"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport os\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Read in Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"/kaggle/input/home-credit-default-risk/\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Data\napp_train = pd.read_csv('/kaggle/input/home-credit-default-risk/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => 122개 feature가 있다. ( TARGET 포함 )"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing data features\napp_test = pd.read_csv('/kaggle/input/home-credit-default-risk/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => TARGET 컬럼이 없고, 총 121 feature임."},{"metadata":{},"cell_type":"markdown","source":"**Exploratory Data Analysis**"},{"metadata":{},"cell_type":"markdown","source":"* Examine the Distribution of the Target Column\n* => TARGET(0) : for the loan was repaid on time\n* => TARGET(1) : the client had payment difficulties"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['TARGET'].astype(int).plot.hist();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => 이 차트를 보고, imbalanced class problem 이라고 알 수 있다. "},{"metadata":{},"cell_type":"markdown","source":"* Examine Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 전체 data는 ? 307,511 \nprint(len(app_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_values_table(df):\n    # Total missing values\n    mis_val = df.isnull().sum()\n    \n    # Percentage of missing values\n    mis_val_percent = 100 * df.isnull().sum() / len(df)\n    \n    # Make a table with the results\n    # axis=1 ; app_train + mis_val 컬럼 + mis_val_percent 컬럼 식으로 추가\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    \n    # Rename the columns\n    mis_val_table_ren_columns = mis_val_table.rename( columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n    \n    # Sort the table by percentage of missing descending\n    mis_val_table_ren_columns = mis_val_table_ren_columns[ mis_val_table_ren_columns.iloc[:,1] != 0 ].sort_values('% of Total Values', ascending=False).round(1)\n    \n    #print(\"##\")\n    #print(mis_val_table_ren_columns.iloc[:,1] )\n    #print(\"##\")\n    #print(\"##\")\n    #print(mis_val_table_ren_columns.iloc[:,1] != 0)\n    #print(\"##\")\n\n\n#    mis_val_table_ren_columns = mis_val_table_ren_columns[ mis_val_table_ren_columns.iloc[:,1] != 0 ].sort_values(by='% of Total Values', ascending=False).round(1)\n#    mis_val_table_ren_columns = mis_val_table_ren_columns.sort_values(by='% of Total Values', ascending=False).round(1)\n    \n    # Print some summary information\n    print(\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\" \n         \"There are \" + str(mis_val_table_ren_columns.shape[0]) + \n         \" columns that have missing values.\")\n    \n    # Return the datafrmae with missing information\n    return mis_val_table_ren_columns\n\n# Missing values statistics\nmissing_values = missing_values_table(app_train)\nmissing_values.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => 1) NaN -> 값 채우기\n* => 2) 값 채우지 않고, XGBoost 와 같은 모델 사용 ( https://stats.stackexchange.com/questions/235489/xgboost-can-handle-missing-data-in-the-forecasting-phase )\n* => 3) NaN 이 높은 % 는 칼럼 삭제\n* => NaN 값이 많음에도 불구하고, 이 컬럼이 도움 될지 않될지 모르기 때문에 유지."},{"metadata":{},"cell_type":"markdown","source":"* Column Types\n* => int64, float64 ; numeric variables\n* => object : strings, categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of each type of column\napp_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => 이런 categorical variables 다루기 위한 방법은 찾아야함."},{"metadata":{},"cell_type":"markdown","source":"* Encoding Categorical Variables\n* => 2 categories 이상 : One-Hot Encoding \n* => 2 categories : Label Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoding 전, 데이타 체크, 122개 column임. \napp_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in app_train:\n    if app_train[col].dtypes == 'object':\n        print(app_train[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            print(app_train[col])\n            # Train on the training data\n            le.fit(app_train[col])\n            \n            # Transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train.tail(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => Label Encoding 이라서, column이 추가되진 않음."},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => Training data경우, 122개 컬럼에서, 243개 컬럼으로 늘어남. "},{"metadata":{},"cell_type":"markdown","source":"* Aligning Training and Testing Data\n* => training과 Testing 데이타 컬럼 수가 다르기 때문에, 데이타 프레임을 column 기준으로 결함 ( axis = 1 )"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n\n# Add the target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => inner(양쪽 DataFrame 교집합) 로 데이타를 정렬함. \n* => one-hot encoding 후 original column 삭제 효과. ( TARGET 컬럼도 삭제 )\n* => Training에 'TARGET'컬럼 추가."},{"metadata":{},"cell_type":"markdown","source":"**Back to Exploratory Data Analysis**"},{"metadata":{},"cell_type":"markdown","source":"* Anomalies\n* => EDA 하면서 발생한 이상한 데이타 추적"},{"metadata":{"trusted":true},"cell_type":"code","source":"# - ( negative ) days : 현재 대출 신청 기록. 과거 대출은 - 로 표시 \napp_train['DAYS_BIRTH'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 날짜가 크기 때문에, 년 단위로 변경\n( app_train['DAYS_BIRTH'] / 365).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# positive로 표시\n( app_train['DAYS_BIRTH'] / 365 * -1).describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => 특별히 이상한 값음 없음."},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => max 365243 days / 365 days => 1,243 year?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# DAYS_EMPLOYED : How many days before the application the person started current employment, 일 하기 시작한 날?\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => 비정상적인 수치는 365,243 에 분포되어있다. \n* => 365일로 나누면, 243년? => 비정상적인 데이타라고 간주."},{"metadata":{"trusted":true},"cell_type":"code","source":"anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => 비정상적인 값을 확인했고, 채우기로 함\n* => 모든 대출이 공통 내용 공유한다면, 동일한 값으로 채울 수 있음.\n* => 새로운 bool 컬럼 생성 : np.nan이 아닌 값으로 채우는데, 이 값이 변칙적인지 여부를 파악하기 위함. ( 365243이면 True, 아니면 False )"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train['DAYS_EMPLOYED'] == 365243\n\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => distribution 잘 된것 같다, 갑자기 왜 - 값인지? 바로 이전 histogram은 0 ~ 365243이였.."},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill to app_test DataFrame\n# 변칙적인 값이 어느 정도 중요하다고 판단하는 경우, 변친 값을 판단하는 boolean 컬럼을 새로 만들어 둔다. \n# 그리고, 원래 컬럼에 변칙적인 값이 있으면 그 값은 NaN 으로 채워 둔다. \napp_test['DAYS_EMPLOYED_ANOM'] = app_test['DAYS_EMPLOYED'] == 365243\napp_test['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test['DAYS_EMPLOYED_ANOM'].sum(), len(app_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Correlations**"},{"metadata":{},"cell_type":"markdown","source":"* The correlation coefficient is not the greatest method to represent \"relevance\" of a feature.\n* but it does give us an idea of possible relationships within the data.\n* http://www.statstutor.ac.uk/resources/uploaded/pearsons.pdf 값에 대한 해석 참조해서 읽어보면 좋음.\n* .00 ~ .19 \"very week\"\n* .20 ~ .39 \"weak\"\n* .40 ~ .59 \"moderate\"\n* .60 ~ .79 \"strong\"\n* .80 ~ 1.0 \"very strong\""},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find correlations with the target and sort\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => DAYS_BIRTH feature가 가장 긍정적인 상관 관계\n* => DAYS_BIRTH : Client's age in days at the time of application ??? 대출 시점에 고객 연령(days) 실제로 이 값은 음수로 표현. 클라이언트가 나이가 들수록 대출에 대한 채무 불이행 가능성이 낮아짐. 그렇다고, - 값을 절대값을 취하면 correlation 값이 음수가 됨. 쩝"},{"metadata":{},"cell_type":"markdown","source":"**Effect of Age on Repayment**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the correlation of the positive days since birth and target\n\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => 고객이 나이가 들면, TARGET값과는 negative linear relationship\n* => 나이가 들수록, 대출금을 자주 상환하는 것 같다. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a histogra of age\n\n# Set the style of plots\nplt.style.use('fivethirtyeight')\n\n# Plot the distribution of ages in years\nplt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)\nplt.title('Age of Client')\nplt.xlabel('Age (years)')\nplt.ylabel('Count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => 나이 분포를 보면, 특이치(outlier)는 보이지 않는다. \n* => 그래서, 나이를 kernel density estimation plot(KDE, a smoothed histogram)로 찍어보자. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,8))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n\n# Labeling of plot\nplt.xlabel('Age (years)')\nplt.ylabel('Density')\nplt.title('Distribution of Ages')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => target == 1, 나이가 어린쪽으로 곡선이 기울어짐\n* => distribution이 있어 보여서 ML 모델에 사용하는게 좋다고 판단"},{"metadata":{},"cell_type":"markdown","source":"* 다른 방법으로 리뷰\n* => 연령대 별 대출 상환 평균 실패"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Age information into a separate dataframe\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n\n# Bin the age data\n# pd.cut : 실수 값 경계선 지정 function\n# bins : 카테고리를 나누는 기준 값\n# np.linspace : 20부터 70까지 균등하게 배열을 11개 생성\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group by the bin and calculate average\nage_groups = age_data.groupby('YEARS_BINNED').mean()\nage_groups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75)\nplt.xlabel('Age Group (years)')\nplt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => 나이가 어린 클라이언트 대출을 상환하지 않을 가능성이 높다."},{"metadata":{},"cell_type":"markdown","source":"**Exterior Source**"},{"metadata":{},"cell_type":"markdown","source":"* 3개 feature, EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3 : target 과 강한 negative 상관관계\n* => \"normalized score from external data source\" \n* => 많은 외부 데이타를 사용해서 누적한 신용 등급"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 숫자보다는 heatmap\nplt.figure(figsize = (8,6))\n\n# Heatmap of correlation\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heapmap')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => 모든 EXT_SOURCE feature는 target과 negative correlation 임\n* => EXT_SOURCE 가 증가하면 대출을 상환할 가능성이 높아진다. \n* => DAYS_BIRTH는 EXT_SOURCE_1 과 postive 상관 관계이고, 이 positive 상관 관계 수치 요인 중 하나가 client aget 일수 있음을 가리킨다."},{"metadata":{},"cell_type":"markdown","source":"* EXT_SOURCE feature와 TARGET : kdeplot으로 distribution 살펴보기"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 12))\n\n# iterate through the sources\nfor i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    \n    # create a new subplot for each source\n    plt.subplot(3, 1, i + 1)\n    \n    # plot repaid loans\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n    \n    # plot loans that were not repaid\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value ' % source)\n    plt.xlabel('%s' % source)\n    plt.ylabel('Density')\n    \nplt.tight_layout(h_pad = 2.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => EXT_SOURCE_3 ; the greatest difference. \n* => relationship이 강하진 않지만, 제 시간에 대출을 상환할지 말지를 예상하는 ML model에 유용할 것 같다. "},{"metadata":{},"cell_type":"markdown","source":"**Pairs Plot**"},{"metadata":{},"cell_type":"markdown","source":"* EXT_SOURCE variable과 DAYS_BIRTH variable 의 pair plot\n* 여러 변수 사이 분포와 단일 변수 분포 볼수 있음"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy the data for plotting\nplot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n\n# Add in the age of the client in years\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n\n# Drop na values and limit to first 100000 rows\nplot_data = plot_data.dropna().loc[:100000, :]\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)\n\n# Create the pairgrid object\ngrid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n                    hue = 'TARGET', \n                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.2)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => 빨간색 : 상환하지 않았던 대출\n* => 파란색 : 상환했던 대출\n* => EXT_SOURCE_1 과 DAYS_BIRTH(혹은 YEARS_BIRTH) 사이 moderate positive linear relationship => 고객(채무자) 나이 고려할 수 있음을 보여 줌 (????)"},{"metadata":{},"cell_type":"markdown","source":"**Feature Engineering**"},{"metadata":{},"cell_type":"markdown","source":"* Kaggle competition은 주로 Feature Engineering에 달림. \n* 모델 Gradient Boost 변경이 많이 우승하는 경향\n* 모델을 만들고, hyperparameter 튜닝하는 것 보다 Feature Engineering 이 더 좋은 Prediction 만들어냄 \n* Andrew Ng \"applied machine learning is basically feature engineering.\""},{"metadata":{},"cell_type":"markdown","source":"* 모델은 단지 주어진 데이타를 가지고 학습할 수 있음\n* Data Scientist : 이 데이타로 가능한 한 작업과 관련이 있는지 확인 ( 자동화된 도구 이용 )\n* 많은 feature engineering 중에서, Polynomial features 와 Domain knowledge features 사용해보자\n* => 선택한 이유가? ...."},{"metadata":{},"cell_type":"markdown","source":"**Polynomial Features**"},{"metadata":{},"cell_type":"markdown","source":"* => feature engineering 설명 중, Polynomial features 참고 : https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a new dataframe for polynomial features\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\n# imputer for handling missing values ( strategy:median 의미가???? )\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'median')  \n\npoly_target = poly_features['TARGET']\n\npoly_features = poly_features.drop(columns = ['TARGET'])\n\n# Need to impute missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Create the polynomial object with specified degree ( degree:3, 3차수 x^3 )\npoly_transformer = PolynomialFeatures(degree = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => 4개 feature만 poly_features 에 넣었는데, \n* => transform 이후는 35개 features임.\n* => 새롭게 만든 feature는 get_feature_name 으로 리스트 가능"},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => 새롭게 만든 EXT_SOURCE_1^2 등 feature 확인 가능"},{"metadata":{},"cell_type":"markdown","source":"* 이 features와 Target 상관 관계를 살펴보자"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe of the features\npoly_features = pd.DataFrame(poly_features, columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Add in the target\npoly_features['TARGET'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => 0.15 정도로 몇 가지 feature 상관 관계 값이 있지만 ( 0.0 ~ 0.19 very week )\n* => new feature를 사용할지, 기존 feature를 사용할지 결정 필요\n"},{"metadata":{},"cell_type":"markdown","source":"* 위 에서 만든 feature를 training과 testing data에 추가하고, 모델을 평가. 추가하지 않고도 평가. \n* ML은 just work and try out!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, columns = poly_transformer.get_feature_names\n                                  (['EXT_SOURCE_1', 'EXT_SOURCE_2',\n                                   'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Merge polynomial features into training dataframe\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n\n# Merge polynomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n\n# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape : ', app_train_poly.shape)\nprint('Testing data with polynomial features shape : ', app_test_poly.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => 약 48,000, 121 features => 275 features로 늘어났음 "},{"metadata":{},"cell_type":"markdown","source":"**Domain Knowledge Features**"},{"metadata":{},"cell_type":"markdown","source":"* https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features 에 따라서 5 features 를 더 들여다 보자\n* CREDIT_INCOME_PERCENT ; 고객 소득 대비 신용(credit amout) 비율(percent)\n* ANNUITY_INCOME_PERCENT ; 고객 소득 대비 대출 연금(annuity????) 비율\n* CREDIT_TERM : 월 단위 지불 기간 ( 연금이 월 지불액 이라서 )\n* DAYS_EMPLOYED_PERCENT ; 고객 나이 대비 고용된 날짜 비율 "},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\n# AMT_CREDIT : Credit amount of the loan\n# AMT_INCOME_TOTAL : Income of the client\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\n\n# AMT_ANNUITY ; Loan annuity ( 대출 연금 )\n# AMT_INCOME_TOTAL : Income of the client \napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\n\n# AMT_ANNUITY ; Loan annuity ( 대출 연금 )\n# AMT_CREDIT ; Credit amount of the loan\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\n\n# DAYS_EMPLOYED ; How many days before the application the person started current employment\n# DAYS_BIRTH ; Client's age in days at the time of application\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Visualize New Variables\n* => TARGET = 0 or 1 에 따라 test_domain feature별 kdeplot "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 20))\n\n# iterate through the new features\nfor i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n    \n    # create a new subplot for each source, 4x1, index\n    plt.subplot(4, 1, i + 1)\n    \n    # plot repaid loans\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature], label = 'target == 0')\n    \n    # plot loans that were not repaid\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % feature)\n    plt.xlabel('%s' % feature)\n    plt.ylabel('Density')\n    \n# h_pad ; 인접한 pad 높이 간격\nplt.tight_layout(h_pad = 2.5) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => distribution이 3번째, 4번째는 있지만, 이 새로운 feature가 유용하다고 단정지을 순 없다\n* => 알아보기 위해서는 이 features를 가지고 predict 해봐야 ..."},{"metadata":{},"cell_type":"markdown","source":"**Baseline**"},{"metadata":{},"cell_type":"markdown","source":"* naive baseline : testing set 에서 모든 예는 동일한 값을 추측할 것 같다. \n* This will get us a Reciever Operating Characteristic Area Under the Curve (AUC ROC) of 0.5 in the competition (random guessing on a classification task will score a 0.5). => AUC ROC가 0.5라는건 반반인데, 좀 문제가 있는 prediction 이다. \n* 그래서, baseline을 Logistic Regression을 사용해서 좀 더 정교한 모델을 사용."},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression(로지스틱 회귀) Implementation**\n* ( ref : https://ratsgo.github.io/machine%20learning/2017/04/02/logistic/ or web searching... )"},{"metadata":{},"cell_type":"markdown","source":"* Machine Learning Algorithm : http://faculty.marshall.usc.edu/gareth-james/ 과 같은 책을 보라~\n* Features : after encoding the categorical variables\n* missing values(imputation): by filling the missing values and normalizing the range of features\n* => imputation : 누락된 데이타 값을 채우는 방법\n* => Deletion : 분명하지 않은 결측 값이 있는 데이타를 제거하는 방법\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, Imputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns = ['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# Mean(평균), median(중앙값), mode(최빈값)\n# Median(중앙값) imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0,1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make prediction\n# Make sure to select the second column only\n# predict() : output이 0 or 1\n# predict_proba() : 1의 probability\nlog_reg_pred = log_reg.predict_proba(test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the submission to a csv file\nsubmit.to_csv('log_reg_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Check Data folder\n* => Output : /kaggle/working/log_reg_baseline.csv "},{"metadata":{},"cell_type":"markdown","source":"**Improved Model : Random Forest**\n* Random Forest 로 개선\n* tree가 수백개일 때, 강력함. \n* 100개 tree 사용 예정\n* 참고1 : Model  https://datascienceschool.net/view-notebook/766fe73c5c46424ca65329a9557d0918/ \n* 참고2 : https://analysis-flood.tistory.com/103"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train on the training data\nrandom_forest.fit(train, train_labels)\n\n# Extract feature importances\n# Random Forest 장점은 각 독립 변수 중요도(feature importance)를 계산할 수 있다.\nfeature_importance_values = random_forest.feature_importances_\n\n# 이 문서 거의 마지막 모델 \"Model Interpretation : Feature Importances\" 에서 사용\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = random_forest.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => Random Forest와 LogisticRegression Model score는? ( 이건 누가 매기는 거지? )\n* => Random.... : 0.678\n* => Logi... : 0.671"},{"metadata":{},"cell_type":"markdown","source":"**Make Predictions using Engineered Features**\n* Polynomial Features, Domain knowledge 사용해보자"},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_features_names = list(app_train_poly.columns)\n\n# Impute the polynomial features\nimputer = Imputer(strategy = 'median')\n\npoly_features = imputer.fit_transform(app_train_poly)\npoly_features_test = imputer.fit_transform(app_test_poly)\n\n# Scale the polynomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\npoly_features = scaler.fit_transform(poly_features)\npoly_features_test = scaler.transform(poly_features_test)\n\nrandom_forest_poly = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train on the training data\nrandom_forest_poly.fit(poly_features, train_labels)\n\n# Make predictions on the test data\npredictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_engineered.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => Random Forest와 LogisticRegression Model score, Random Forest using Engineering features는? ( 이건 누가 매기는 거지? )\n* => Random.... : 0.678\n* => Logi... : 0.671\n* => Random using Engineering features ... : 0.678\n* => 이건 모,,,, Engineering Features를 왜 한건지..."},{"metadata":{},"cell_type":"markdown","source":"**Testing Domain Features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train_domain = app_train_domain.drop(columns = 'TARGET')\n\ndomain_features_names = list(app_train_domain.columns)\n\n# Impute the domainnomial features\nimputer = Imputer(strategy = 'median')\n\ndomain_features = imputer.fit_transform(app_train_domain)\ndomain_features_test = imputer.transform(app_test_domain)\n\n# Scale the domainnomial features\nscaler = MinMaxScaler(feature_range = (0,1))\n\ndomain_features = scaler.fit_transform(domain_features)\ndomain_features_test = scaler.transform(domain_features_test)\n\nrandom_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n# Train on the training data\nrandom_forest_domain.fit(domain_features, train_labels)\n\n# Extract feature importances\nfeature_importance_values_domain = random_forest_domain.feature_importances_\nfeature_importance_domain = pd.DataFrame({'feature': domain_features_names, 'importance': feature_importance_values_domain})\n\n# Make predictions on the test data\npredictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_domain.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => testing domain features, scores : 0.679\n* => Random Forest와 LogisticRegression Model score, Random Forest using Engineering features는? ( 이건 누가 매기는 거지? )\n* => Random.... : 0.678\n* => Logi... : 0.671\n* => Random using Engineering features ... : 0.678\n* => 이건 모,,,, Engineering Features를 왜 한건지..."},{"metadata":{},"cell_type":"markdown","source":"**Model Interpretation : Feature Importances**\n* 좀 더 해보면, EXT_SOURCE와 DAYS_BIRTH가 중요한 feature라는 걸 예상해야한다. 그래서 이걸 ... 사용해보자"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the feature importances for the default features\n# feature_importances : Randome Forest 후 나온 중요도 변수를 train pandas 데이타에 추가함. \nfeature_importances_sorted = plot_feature_importances(feature_importances)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => 예상대로, EXT_SOURCE, DAYS_BIRTH가 중요해 보임."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_domain_sorted = plot_feature_importances(feature_importance_domain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => Domain Knowledge로 했을 때 중요도."},{"metadata":{},"cell_type":"markdown","source":"**Conclusions**"},{"metadata":{},"cell_type":"markdown","source":"**We followed the general outline of a machine learning project:**\n* Understand the problem and the data\n* Data cleaning and formatting (this was mostly done for us)\n* Exploratory Data Analysis\n* Baseline model\n* Improved model\n* Model interpretation (just a little)"},{"metadata":{},"cell_type":"markdown","source":"Light Gradient Boosting Machine 모델 사용한ㄴ 경우에 prediction이 좋아짐"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}