{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport gc\n\nbureau_balance = pd.read_csv('../input/bureau_balance.csv')\n\n# bb feature\nbureau_balance['STATUS_mod'] = bureau_balance.STATUS.map({'0':0, '1':1, '2':2, '3':3, '4':4, '5':5, 'X':np.nan, 'C':0}).map(lambda x: 0 if x=='C' else x).interpolate(method = 'linear')\nbureau_balance['write_off'] = bureau_balance.STATUS.map(lambda x: 1 if x=='5' else 0)\nbureau_balance['adj_score'] = (bureau_balance.MONTHS_BALANCE-bureau_balance.MONTHS_BALANCE.min()+1)*bureau_balance.STATUS_mod\n\nbb_month_count = bureau_balance.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].count()\nbb_dpd_sum = bureau_balance.groupby('SK_ID_BUREAU')['STATUS_mod'].sum()\nbb_write_off = bureau_balance.groupby('SK_ID_BUREAU')['write_off'].sum()\nbb_dpd_sum_2_year = bureau_balance.loc[bureau_balance.MONTHS_BALANCE>=-24].groupby('SK_ID_BUREAU')['STATUS_mod'].sum()\nbb_write_off_2_year = bureau_balance.loc[bureau_balance.MONTHS_BALANCE>=-24].groupby('SK_ID_BUREAU')['write_off'].sum()\nbb_adj_score = bureau_balance.groupby('SK_ID_BUREAU')['adj_score'].sum()\n\nbb_feature = pd.DataFrame({'bb_month_count':bb_month_count, 'bb_dpd_sum':bb_dpd_sum, 'bb_write_off':bb_write_off, 'bb_dpd_sum_2_year':bb_dpd_sum_2_year, \n                          'bb_write_off_2_year':bb_write_off_2_year, 'bb_adj_score': bb_adj_score}).reset_index().fillna(0)\ndel bb_month_count, bb_dpd_sum, bb_write_off, bb_dpd_sum_2_year, bb_write_off_2_year, bb_adj_score, bureau_balance\ngc.collect()\n\n# bureau\nbureau = pd.read_csv('../input/bureau.csv')\nbureau = bureau.sort_values(['SK_ID_CURR', 'DAYS_CREDIT'])\n\nbureau['ADJ_DAYS'] = (bureau.DAYS_CREDIT - bureau.DAYS_CREDIT.min()) / (bureau.DAYS_CREDIT.max() - bureau.DAYS_CREDIT.min()) + 0.5 # more recent, more effecitve\n# application count\nbur_ncount = bureau.groupby('SK_ID_CURR')['SK_ID_BUREAU'].count()\nbur_act_count = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['SK_ID_BUREAU'].count() # fillna: 0\nbur_bad_count = bureau.loc[bureau.CREDIT_ACTIVE=='Bad debt'].groupby('SK_ID_CURR')['SK_ID_BUREAU'].count() # fillna: 0\nbur_sold_count = bureau.loc[bureau.CREDIT_ACTIVE=='Sold out'].groupby('SK_ID_CURR')['SK_ID_BUREAU'].count() # fillna: 0\n# application date\nbur_recent_application = -bureau.groupby('SK_ID_CURR')['DAYS_CREDIT'].max()\nbur_eariliest_application = -bureau.groupby('SK_ID_CURR')['DAYS_CREDIT'].min()\nbur_max_enddate = -bureau.groupby('SK_ID_CURR')['DAYS_CREDIT_ENDDATE'].max()\n# application itervel\nbureau['application_interval'] = bureau.groupby('SK_ID_CURR')['DAYS_CREDIT'].diff(-1)\nmissing_iter = iter(bureau.groupby('SK_ID_CURR')['DAYS_CREDIT'].max())\nbureau.application_interval = bureau.application_interval.map(lambda x: -next(missing_iter) if np.isnan(x) else -x)\nbur_avg_intervel = bureau.groupby('SK_ID_CURR')['application_interval'].mean()\nbur_sd_intervel = bureau.groupby('SK_ID_CURR')['application_interval'].agg('std').fillna(0)\n# overdue days\nbur_max_overdue_days = bureau.groupby('SK_ID_CURR')['CREDIT_DAY_OVERDUE'].max()\nbur_active_total_overdue_days = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['CREDIT_DAY_OVERDUE'].sum()\nbur_active_max_overdue_days = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['CREDIT_DAY_OVERDUE'].max()\nbureau['DAYS_CREDIT_ISPAST'] = bureau.DAYS_CREDIT_ENDDATE.map(lambda x: 1 if x < 0 else 0)\nbur_avg_remaining_days = bureau.loc[bureau.DAYS_CREDIT_ENDDATE>0].groupby('SK_ID_CURR')['DAYS_CREDIT_ENDDATE'].mean()\n# overdue amount\nbureau['ADJ_AMT_CREDIT_MAX_OVERDUE'] = bureau.ADJ_DAYS * bureau.AMT_CREDIT_MAX_OVERDUE\nbur_total_max_overdue_adj = bureau.groupby('SK_ID_CURR')['ADJ_AMT_CREDIT_MAX_OVERDUE'].sum() # use adj days\nbur_avg_max_overdue = bureau.groupby('SK_ID_CURR')['AMT_CREDIT_MAX_OVERDUE'].mean()\nbur_overall_max_overdue = bureau.groupby('SK_ID_CURR')['AMT_CREDIT_MAX_OVERDUE'].max()\n# adj prelong\nbureau['ADJ_CNT_CREDIT_PROLONG'] = bureau.ADJ_DAYS * bureau.CNT_CREDIT_PROLONG\nbur_avg_prelonged = bureau.groupby('SK_ID_CURR')['ADJ_CNT_CREDIT_PROLONG'].mean().fillna(0) # use adj days\nbur_max_prelonged = bureau.groupby('SK_ID_CURR')['CNT_CREDIT_PROLONG'].max().fillna(0)\nbur_total_prelonged_adj = bureau.groupby('SK_ID_CURR')['ADJ_CNT_CREDIT_PROLONG'].sum().fillna(0)\n# historical amount\nbureau['ADJ_AMT_CREDIT_SUM_DEBT'] = bureau.ADJ_DAYS * bureau.AMT_CREDIT_SUM\nbur_total_amount_adj = bureau.groupby('SK_ID_CURR')['ADJ_AMT_CREDIT_SUM_DEBT'].sum()\nbur_avg_amount = bureau.groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].mean()\n# current amount\nbur_active_total_amount = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].sum() # fillna 0\nbur_active_avg_amount = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].mean() # fillna 0\nbur_active_total_debt = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['AMT_CREDIT_SUM_DEBT'].sum() # fillna 0\nbur_active_avg_debt = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['AMT_CREDIT_SUM_DEBT'].mean() # fillna 0\nbur_active_total_limit  = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['AMT_CREDIT_SUM_LIMIT'].sum() # fillna 0\nbur_active_avg_limit  = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['AMT_CREDIT_SUM_LIMIT'].mean() # fillna 0\nbur_active_total_overdue = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['AMT_CREDIT_SUM_OVERDUE'].sum() # fillna 0\nbur_active_avg_overdue = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['AMT_CREDIT_SUM_OVERDUE'].mean() # fillna 0\nbur_active_ratio_debt_credit = (bur_active_total_debt / bur_active_total_amount.map(lambda x: x+0.1)) # fillna 0\nbur_active_ratio_overdue_debt = (bur_active_total_overdue / bur_active_total_debt.map(lambda x: x+0.1)) # fillna 0\n# credit update\nbur_avg_update = bureau.groupby('SK_ID_CURR')['DAYS_CREDIT_UPDATE'].mean()\nbur_recent_update = bureau.groupby('SK_ID_CURR')['DAYS_CREDIT_UPDATE'].max()\n# annuity\nbur_avg_annuity = bureau.groupby('SK_ID_CURR')['AMT_ANNUITY'].mean() # can't fillna 0\nbur_total_annuity = bureau.groupby('SK_ID_CURR')['AMT_ANNUITY'].sum() # can't fillna 0\nbur_active_total_annuity = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['AMT_ANNUITY'].sum()\nbureau['term'] = bureau.AMT_CREDIT_SUM / bureau.AMT_ANNUITY\nbur_avg_term = bureau.loc[bureau.term < float('inf')].groupby('SK_ID_CURR')['term'].mean() # can't fillna 0\n\nbureau_num_feature = pd.DataFrame({'bur_ncount':bur_ncount, 'bur_act_count':bur_act_count, 'bur_bad_count':bur_bad_count, 'bur_sold_count':bur_sold_count,\n            'bur_recent_application':bur_recent_application,'bur_eariliest_application':bur_eariliest_application, 'bur_max_enddate':bur_max_enddate,\n            'bur_avg_intervel':bur_avg_intervel, 'bur_sd_intervel':bur_sd_intervel,\n            'bur_max_overdue_days':bur_max_overdue_days, 'bur_active_total_overdue_days':bur_active_total_overdue_days, 'bur_active_max_overdue_days':bur_active_max_overdue_days,\n            'bur_avg_remaining_days':bur_avg_remaining_days, 'bur_total_max_overdue_adj':bur_total_max_overdue_adj, 'bur_avg_max_overdue':bur_avg_max_overdue, 'bur_overall_max_overdue':bur_overall_max_overdue, \n            'bur_avg_prelonged':bur_avg_prelonged, 'bur_max_prelonged':bur_max_prelonged, 'bur_total_prelonged_adj':bur_total_prelonged_adj,\n            'bur_total_amount_adj':bur_total_amount_adj, 'bur_avg_amount':bur_avg_amount,\n            'bur_active_total_amount':bur_active_total_amount, 'bur_active_avg_amount':bur_active_avg_amount, 'bur_active_total_debt':bur_active_total_debt, 'bur_active_avg_debt':bur_active_avg_debt, \n            'bur_active_total_limit':bur_active_total_limit, 'bur_active_avg_limit':bur_active_avg_limit, 'bur_active_total_overdue':bur_active_total_overdue, 'bur_active_avg_overdue':bur_active_avg_overdue,\n            'bur_active_ratio_debt_credit':bur_active_ratio_debt_credit, 'bur_active_ratio_overdue_debt':bur_active_ratio_overdue_debt,\n            'bur_avg_update':bur_avg_update, 'bur_recent_update':bur_recent_update,\n            'bur_avg_annuity':bur_avg_annuity, 'bur_total_annuity':bur_total_annuity, 'bur_active_total_annuity':bur_active_total_annuity, 'bur_avg_term':bur_avg_term}).reset_index()\nfill0_list = ['bur_act_count', 'bur_bad_count', 'bur_sold_count', 'bur_active_total_overdue_days', 'bur_active_max_overdue_days', 'bur_active_total_amount', 'bur_active_avg_amount',\n             'bur_active_total_debt', 'bur_active_avg_debt', 'bur_active_total_limit', 'bur_active_avg_limit', 'bur_active_total_overdue', 'bur_active_avg_overdue', \n              'bur_active_ratio_debt_credit', 'bur_active_ratio_overdue_debt', 'bur_active_total_annuity']\nbureau_num_feature[fill0_list] = bureau_num_feature[fill0_list] .fillna(0)\n\nbureau_cat = pd.get_dummies(bureau[['SK_ID_CURR','CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'CREDIT_TYPE']], prefix='bur')\nbureau_cat_feature = bureau_cat.groupby('SK_ID_CURR').mean().reset_index()\ndel bureau_cat\ngc.collect()\n\nbureau_bb = bureau[['SK_ID_CURR','SK_ID_BUREAU']].merge(bb_feature, on='SK_ID_BUREAU', how='left')\n\nbb_avg_month = bureau_bb.groupby('SK_ID_CURR')['bb_month_count'].mean()\nbb_total_overdue_month = bureau_bb.groupby('SK_ID_CURR')['bb_dpd_sum'].sum()\nbb_total_writeoff = bureau_bb.groupby('SK_ID_CURR')['bb_write_off'].sum()\nbb_max_overdue_month = bureau_bb.groupby('SK_ID_CURR')['bb_dpd_sum'].max()\nbb_max_writeoff = bureau_bb.groupby('SK_ID_CURR')['bb_write_off'].max()\n\nbb_total_overdue_month_2year = bureau_bb.groupby('SK_ID_CURR')['bb_dpd_sum_2_year'].sum()\nbb_max_overdue_month_2year= bureau_bb.groupby('SK_ID_CURR')['bb_dpd_sum_2_year'].max()\nbb_total_writeoff_2year = bureau_bb.groupby('SK_ID_CURR')['bb_write_off_2_year'].sum()\nbb_max_writeoff_2year = bureau_bb.groupby('SK_ID_CURR')['bb_write_off_2_year'].max()\n\nbb_max_score = bureau_bb.groupby('SK_ID_CURR')['bb_adj_score'].max()\nbb_total_score = bureau_bb.groupby('SK_ID_CURR')['bb_adj_score'].sum()\nbb_avg_score = bureau_bb.groupby('SK_ID_CURR')['bb_adj_score'].mean()\n\nbureau_bb_feature = pd.DataFrame({'bb_avg_month':bb_avg_month, \n            'bb_total_overdue_month':bb_total_overdue_month, 'bb_total_writeoff':bb_total_writeoff, 'bur_sold_count':bb_max_overdue_month,'bb_max_writeoff':bb_max_writeoff,\n            'bb_total_overdue_month_2year':bb_total_overdue_month_2year, 'bb_total_writeoff_2year':bb_total_writeoff_2year, \n            'bb_max_overdue_month_2year':bb_max_overdue_month_2year,'bb_max_writeoff_2year':bb_max_writeoff_2year, \n            'bb_max_score':bb_max_score, 'bb_total_score':bb_total_score, 'bb_avg_score':bb_avg_score}).reset_index()\n\t\t\t\nbureau_feature = bureau_num_feature.merge(bureau_cat_feature, on='SK_ID_CURR').merge(bureau_bb_feature, on='SK_ID_CURR', how='left')\nprint(bureau_feature.shape)\nbureau_feature.to_csv('bureau_feature.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\napplication_train = pd.read_csv('../input/application_train.csv')\ncredit_card_balance = pd.read_csv('../input/credit_card_balance.csv')\ncredit_card_balance = credit_card_balance.sort_values(['SK_ID_CURR', 'SK_ID_PREV', 'MONTHS_BALANCE'])\n\n# count cards\nccb_prev_count = credit_card_balance.groupby('SK_ID_CURR')['SK_ID_PREV'].nunique()\n# INSTALMENTS\ncredit_card_balance['PAY_MONTH'] = credit_card_balance.CNT_INSTALMENT_MATURE_CUM.map(lambda x: 1 if x > 0 else 0)\nccb_temp = credit_card_balance.groupby(['SK_ID_CURR','SK_ID_PREV'])['PAY_MONTH'].sum().reset_index()\nccb_avg_inst_card = ccb_temp.groupby('SK_ID_CURR')['PAY_MONTH'].mean()\nccb_total_inst_card = ccb_temp.groupby('SK_ID_CURR')['PAY_MONTH'].sum()\n# limit\nccb_temp = credit_card_balance.groupby(['SK_ID_CURR','SK_ID_PREV'])['AMT_CREDIT_LIMIT_ACTUAL'].mean().reset_index()\nccb_avg_limit_card = ccb_temp.groupby('SK_ID_CURR')['AMT_CREDIT_LIMIT_ACTUAL'].mean()\nccb_max_limit_card = credit_card_balance.groupby('SK_ID_CURR')['AMT_CREDIT_LIMIT_ACTUAL'].max()\nccb_total_limit_card = ccb_temp.groupby('SK_ID_CURR')['AMT_CREDIT_LIMIT_ACTUAL'].sum()\n# avg drawing amount\nccb_temp = credit_card_balance.loc[credit_card_balance.CNT_DRAWINGS_CURRENT>0].groupby(['SK_ID_CURR','MONTHS_BALANCE'])['AMT_DRAWINGS_CURRENT', 'CNT_DRAWINGS_CURRENT'].sum().reset_index()\nccb_temp['avg_drawing_amount'] = (ccb_temp.AMT_DRAWINGS_CURRENT / ccb_temp.CNT_DRAWINGS_CURRENT).fillna(0)\nccb_avg_drawing_amount = ccb_temp.groupby('SK_ID_CURR')['avg_drawing_amount'].mean().fillna(0)\n# count Refused\nccb_count_rej = credit_card_balance.groupby(['SK_ID_CURR'])['NAME_CONTRACT_STATUS'].agg(lambda x: np.sum(x=='Refused'))\nlast_month_credit = credit_card_balance.groupby(['SK_ID_CURR','SK_ID_PREV'])['MONTHS_BALANCE'].max().reset_index()\nlast_month_credit = last_month_credit.merge(credit_card_balance, on=['SK_ID_CURR','SK_ID_PREV', 'MONTHS_BALANCE'])\n# current credit card situation\nccb_cur_total_receivable = last_month_credit.groupby('SK_ID_CURR')['AMT_TOTAL_RECEIVABLE'].sum()\nccb_cur_total_limit = last_month_credit.loc[last_month_credit.NAME_CONTRACT_STATUS == 'Active'].groupby('SK_ID_CURR')['AMT_CREDIT_LIMIT_ACTUAL'].sum() # fillna: 0\nccb_cur_total_payment = last_month_credit.groupby('SK_ID_CURR')['AMT_INST_MIN_REGULARITY'].sum()\nccb_cur_total_balance = last_month_credit.groupby('SK_ID_CURR')['AMT_BALANCE'].sum()\n# drawing in 1y\nccb_temp = credit_card_balance.loc[credit_card_balance.MONTHS_BALANCE>=-12]\nccb_drawing_amount_1y = ccb_temp.groupby('SK_ID_CURR')['AMT_DRAWINGS_CURRENT'].sum()\nccb_drawing_times_1y = ccb_temp.groupby('SK_ID_CURR')['CNT_DRAWINGS_CURRENT'].sum()\n# drawing in 6m\nccb_temp = credit_card_balance.loc[credit_card_balance.MONTHS_BALANCE>=-6]\nccb_drawing_amount_6m = ccb_temp.groupby('SK_ID_CURR')['AMT_DRAWINGS_CURRENT'].sum()\nccb_drawing_times_6m = ccb_temp.groupby('SK_ID_CURR')['CNT_DRAWINGS_CURRENT'].sum()\n# DPD\nccb_temp = credit_card_balance[['SK_ID_CURR', 'SK_ID_PREV', 'SK_DPD', 'SK_DPD_DEF']].groupby(['SK_ID_CURR','SK_ID_PREV'])['SK_DPD','SK_DPD_DEF'].max().reset_index()\nccb_max_dpd_days = ccb_temp.groupby('SK_ID_CURR')['SK_DPD'].max()\nccb_total_dpd_days = ccb_temp.groupby('SK_ID_CURR')['SK_DPD'].sum()\nccb_max_largedpd_days = ccb_temp.groupby('SK_ID_CURR')['SK_DPD_DEF'].max()\nccb_total_largedpd_days = ccb_temp.groupby('SK_ID_CURR')['SK_DPD_DEF'].sum()\n\nccb_feature = pd.DataFrame({'ccb_prev_count':ccb_prev_count, 'ccb_avg_inst_card':ccb_avg_inst_card, 'ccb_avg_limit_card':ccb_avg_limit_card, 'ccb_total_inst_card':ccb_total_inst_card, 'ccb_count_rej': ccb_count_rej,\n                        'ccb_avg_limit_card':ccb_avg_limit_card, 'ccb_max_limit_card':ccb_max_limit_card, 'ccb_total_limit_card':ccb_total_limit_card, 'ccb_avg_drawing_amount':ccb_avg_drawing_amount,\n                        'ccb_cur_total_receivable':ccb_cur_total_receivable, 'ccb_cur_total_limit':ccb_cur_total_limit, 'ccb_cur_total_payment':ccb_cur_total_payment, 'ccb_cur_total_balance':ccb_cur_total_balance,\n                        'ccb_drawing_amount_1y':ccb_drawing_amount_1y, 'ccb_drawing_times_1y':ccb_drawing_times_1y, 'ccb_drawing_amount_6m':ccb_drawing_amount_6m, 'ccb_drawing_times_6m':ccb_drawing_times_6m,\n                        'ccb_max_dpd_days':ccb_max_dpd_days, 'ccb_total_dpd_days':ccb_total_dpd_days, 'ccb_max_largedpd_days':ccb_max_largedpd_days, 'ccb_total_largedpd_days':ccb_total_largedpd_days}).reset_index()\nccb_feature.to_csv('ccb_feature.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ninstallments_payments = pd.read_csv('../input/installments_payments.csv')\ninstallments_payments = installments_payments.sort_values(['SK_ID_CURR', 'SK_ID_PREV', 'NUM_INSTALMENT_NUMBER'])\nprevious_application = pd.read_csv(\"../input/previous_application.csv\")\nprevious_application = previous_application.sort_values(['SK_ID_CURR', 'DAYS_DECISION'])\n\n# recent application\nrecent_record = installments_payments.merge(installments_payments.groupby('SK_ID_CURR')['SK_ID_PREV'].max().reset_index().drop('SK_ID_CURR', axis=1), on='SK_ID_PREV')\nip_recent_term = recent_record.groupby('SK_ID_CURR')['NUM_INSTALMENT_NUMBER'].max()\nip_recent_total_actual_payment = recent_record.groupby('SK_ID_CURR')['AMT_PAYMENT'].sum()\nip_recent_total_required_payment = recent_record.groupby('SK_ID_CURR')['AMT_INSTALMENT'].sum()\n# recent late & less time \nrecent_record['is_late'] = (recent_record.DAYS_INSTALMENT < recent_record.DAYS_ENTRY_PAYMENT).map(lambda x: 1 if x==True else 0).fillna(0)\nrecent_record['is_less'] = (recent_record.AMT_INSTALMENT > recent_record.AMT_PAYMENT).map(lambda x: 1 if x==True else 0).fillna(0)\nip_recent_total_late_times = recent_record.groupby('SK_ID_CURR')['is_late'].sum()\nip_recent_total_less_times = recent_record.groupby('SK_ID_CURR')['is_less'].sum()\n# recent late & less amount\nip_temp1 = recent_record.loc[recent_record.is_late==1]\nip_temp2 = recent_record.loc[recent_record.is_less==1]\nip_temp1['total_late'] = ip_temp1.DAYS_ENTRY_PAYMENT - ip_temp1.DAYS_INSTALMENT\nip_temp2['total_less'] = ip_temp2.AMT_INSTALMENT - ip_temp2.AMT_PAYMENT\nip_recent_total_late_days = ip_temp1.groupby('SK_ID_CURR')['total_late'].sum()\nip_recent_total_less_amount = ip_temp2.groupby('SK_ID_CURR')['total_less'].sum()\ndel ip_temp1, ip_temp2\ngc.collect()\n\n# previous application times\nip_prev_count = installments_payments.groupby('SK_ID_CURR')['SK_ID_PREV'].nunique()\nip_payment_count = installments_payments.groupby('SK_ID_CURR')['SK_ID_PREV'].count()\n# credit card\ninstallments_payments['IS_CREDIT'] = installments_payments.NUM_INSTALMENT_VERSION.map(lambda x: 1 if x==0 else 0)\nip_creditcard_user = installments_payments.groupby('SK_ID_CURR')['IS_CREDIT'].sum().map(lambda x: 1 if x>0 else 0)\nip_creditcard_count = installments_payments.groupby(['SK_ID_CURR','SK_ID_PREV'])['IS_CREDIT'].sum().map(lambda x: 1 if x>0 else 0).reset_index().groupby('SK_ID_CURR')['IS_CREDIT'].sum()\n# change times\nip_temp = (installments_payments.groupby(['SK_ID_CURR','SK_ID_PREV'])['NUM_INSTALMENT_VERSION'].nunique()-1).reset_index()\nip_total_change_times = ip_temp.groupby('SK_ID_CURR')['NUM_INSTALMENT_VERSION'].sum()\nip_avg_change_times = ip_temp.groupby('SK_ID_CURR')['NUM_INSTALMENT_VERSION'].mean()\n# avg instl\nip_temp = installments_payments.groupby(['SK_ID_CURR', 'SK_ID_PREV'])['NUM_INSTALMENT_NUMBER'].max().reset_index()\nip_avg_instl = ip_temp.groupby('SK_ID_CURR')['NUM_INSTALMENT_NUMBER'].mean()\nip_max_instl = ip_temp.groupby('SK_ID_CURR')['NUM_INSTALMENT_NUMBER'].max()\n# total late & less time \ninstallments_payments['is_late'] = (installments_payments.DAYS_INSTALMENT < installments_payments.DAYS_ENTRY_PAYMENT).map(lambda x: 1 if x==True else 0).fillna(0)\ninstallments_payments['is_less'] = (installments_payments.AMT_INSTALMENT > installments_payments.AMT_PAYMENT).map(lambda x: 1 if x==True else 0).fillna(0)\nip_total_late_times = installments_payments.groupby('SK_ID_CURR')['is_late'].sum()\nip_total_less_times = installments_payments.groupby('SK_ID_CURR')['is_less'].sum()\n# total late & less amount\nip_temp1 = installments_payments.loc[installments_payments.is_late==1]\nip_temp2 = installments_payments.loc[installments_payments.is_less==1]\nip_temp1['total_late'] = ip_temp1.DAYS_ENTRY_PAYMENT - ip_temp1.DAYS_INSTALMENT\nip_temp2['total_less'] = ip_temp2.AMT_INSTALMENT - ip_temp2.AMT_PAYMENT\nip_total_late_days = ip_temp1.groupby('SK_ID_CURR')['total_late'].sum()\nip_total_less_amount = ip_temp2.groupby('SK_ID_CURR')['total_less'].sum()\ndel ip_temp1, ip_temp2\ngc.collect()\n# total payment\nip_total_actual_payment = installments_payments.groupby('SK_ID_CURR')['AMT_PAYMENT'].sum()\nip_total_required_payment = installments_payments.groupby('SK_ID_CURR')['AMT_INSTALMENT'].sum()\n\n# total late & less time in recent 1 year\nip_1y = installments_payments.loc[installments_payments.DAYS_ENTRY_PAYMENT>-365]\n# payment 1 year\nip_payment_count_1y = ip_1y.groupby('SK_ID_CURR')['SK_ID_PREV'].count()\nip_creditcard_count_1y = ip_1y.groupby(['SK_ID_CURR','SK_ID_PREV'])['IS_CREDIT'].sum().map(lambda x: 1 if x>0 else 0).reset_index().groupby('SK_ID_CURR')['IS_CREDIT'].sum()\nip_total_late_times_1y = ip_1y.groupby('SK_ID_CURR')['is_late'].sum()\nip_total_less_times_1y = ip_1y.groupby('SK_ID_CURR')['is_less'].sum()\n# avg instl\nip_temp = ip_1y.groupby(['SK_ID_CURR', 'SK_ID_PREV'])['NUM_INSTALMENT_NUMBER'].max().reset_index()\nip_avg_instl_1y = ip_temp.groupby('SK_ID_CURR')['NUM_INSTALMENT_NUMBER'].mean()\n# total late & less amount\nip_temp1 = ip_1y.loc[ip_1y.is_late==1]\nip_temp2 = ip_1y.loc[ip_1y.is_less==1]\nip_temp1['total_late'] = ip_temp1.DAYS_ENTRY_PAYMENT - ip_temp1.DAYS_INSTALMENT\nip_temp2['total_less'] = ip_temp2.AMT_INSTALMENT - ip_temp2.AMT_PAYMENT\nip_total_late_days_1y = ip_temp1.groupby('SK_ID_CURR')['total_late'].sum()\nip_total_less_amount_1y = ip_temp2.groupby('SK_ID_CURR')['total_less'].sum()\ndel ip_temp1, ip_temp2\ngc.collect()\n# total payment\nip_total_actual_payment_1y = ip_1y.groupby('SK_ID_CURR')['AMT_PAYMENT'].sum()\nip_total_required_payment_1y = ip_1y.groupby('SK_ID_CURR')['AMT_INSTALMENT'].sum()\n\n# total late & less time in recent 6 months\nip_6m = installments_payments.loc[installments_payments.DAYS_ENTRY_PAYMENT>-180]\nip_payment_count_6m = ip_6m.groupby('SK_ID_CURR')['SK_ID_PREV'].count()\nip_creditcard_count_6m = ip_6m.groupby(['SK_ID_CURR','SK_ID_PREV'])['IS_CREDIT'].sum().map(lambda x: 1 if x>0 else 0).reset_index().groupby('SK_ID_CURR')['IS_CREDIT'].sum()\nip_total_late_times_6m = ip_6m.groupby('SK_ID_CURR')['is_late'].sum()\nip_total_less_times_6m = ip_6m.groupby('SK_ID_CURR')['is_less'].sum()\n# avg instl\nip_temp = ip_6m.groupby(['SK_ID_CURR', 'SK_ID_PREV'])['NUM_INSTALMENT_NUMBER'].max().reset_index()\nip_avg_instl_6m = ip_temp.groupby('SK_ID_CURR')['NUM_INSTALMENT_NUMBER'].mean()\n# total late & less amount\nip_temp1 = ip_6m.loc[ip_6m.is_late==1]\nip_temp2 = ip_6m.loc[ip_6m.is_less==1]\nip_temp1['total_late'] = ip_temp1.DAYS_ENTRY_PAYMENT - ip_temp1.DAYS_INSTALMENT\nip_temp2['total_less'] = ip_temp2.AMT_INSTALMENT - ip_temp2.AMT_PAYMENT\nip_total_late_days_6m = ip_temp1.groupby('SK_ID_CURR')['total_late'].sum()\nip_total_less_amount_6m = ip_temp2.groupby('SK_ID_CURR')['total_less'].sum()\ndel ip_temp1, ip_temp2\ngc.collect()\n# total payment\nip_total_actual_payment_6m = ip_6m.groupby('SK_ID_CURR')['AMT_PAYMENT'].sum()\nip_total_required_payment_6m = ip_6m.groupby('SK_ID_CURR')['AMT_INSTALMENT'].sum()\n\nip_feature = pd.DataFrame({'ip_prev_count':ip_prev_count, 'ip_payment_count':ip_payment_count, 'ip_creditcard_user':ip_creditcard_user, 'ip_creditcard_count':ip_creditcard_count,\n                           'ip_total_change_times':ip_total_change_times, 'ip_avg_change_times':ip_avg_change_times, 'ip_avg_instl':ip_avg_instl, 'ip_max_instl':ip_max_instl,\n                           'ip_total_late_times':ip_total_late_times, 'ip_total_less_times':ip_total_less_times, 'ip_total_late_days':ip_total_late_days, 'ip_total_less_amount':ip_total_less_amount,\n                           'ip_total_actual_payment':ip_total_actual_payment, 'ip_total_required_payment':ip_total_required_payment,\n                           'ip_recent_term':ip_recent_term, 'ip_recent_total_actual_payment':ip_recent_total_actual_payment, 'ip_recent_total_required_payment':ip_recent_total_required_payment,\n                           'ip_recent_total_late_times':ip_recent_total_late_times, 'ip_recent_total_less_times':ip_recent_total_less_times, \n                           'ip_recent_total_late_days':ip_recent_total_late_days, 'ip_recent_total_less_amount':ip_recent_total_less_amount}).fillna(0)\nip_1y = pd.DataFrame({'ip_payment_count_1y':ip_payment_count_1y, 'ip_creditcard_count_1y': ip_creditcard_count_1y, 'ip_avg_instl_1y':ip_avg_instl_1y,\n                      'ip_total_late_times_1y':ip_total_late_times_1y, 'ip_total_less_times_1y':ip_total_less_times_1y, 'ip_total_late_days_1y':ip_total_late_days_1y,\n                      'ip_total_less_amount_1y':ip_total_less_amount_1y, 'ip_total_actual_payment_1y':ip_total_actual_payment_1y, 'ip_total_required_payment_1y':ip_total_required_payment_1y}).reset_index().fillna(0)\nip_6m = pd.DataFrame({'ip_payment_count_6m':ip_payment_count_6m, 'ip_creditcard_count_6m': ip_creditcard_count_6m, 'ip_avg_instl_6m':ip_avg_instl_6m,\n                      'ip_total_late_times_6m':ip_total_late_times_6m, 'ip_total_less_times_6m':ip_total_less_times_6m, 'ip_total_late_days_6m':ip_total_late_days_6m,\n                      'ip_total_less_amount_6m':ip_total_less_amount_6m, 'ip_total_actual_payment_6m':ip_total_actual_payment_6m, 'ip_total_required_payment_6m':ip_total_required_payment_6m}).reset_index().fillna(0)\nip_feature = ip_feature.merge(ip_1y, on='SK_ID_CURR', how='left').merge(ip_6m, on='SK_ID_CURR', how='left')\nip_feature['ip_active_1y'] = ip_feature.ip_total_late_times_1y.notnull().map(lambda x: 1 if x==True else 0)\nip_feature['ip_active_6m'] = ip_feature.ip_total_late_times_6m.notnull().map(lambda x: 1 if x==True else 0)\n\n# active account\nACCOUNT_1Y = installments_payments.groupby('SK_ID_PREV')['DAYS_ENTRY_PAYMENT'].max().map(lambda x: 1 if x>=-365 else 0)\nACCOUNT_6M = installments_payments.groupby('SK_ID_PREV')['DAYS_ENTRY_PAYMENT'].max().map(lambda x: 1 if x>=-180 else 0)\nACTIVE_ACCOUNT = pd.DataFrame({'ACCOUNT_1Y':ACCOUNT_1Y, 'ACCOUNT_6M':ACCOUNT_6M}).reset_index()\npa_ip = previous_application[['SK_ID_CURR', 'SK_ID_PREV', 'NAME_CONTRACT_TYPE']].merge(ACTIVE_ACCOUNT, on='SK_ID_PREV', how='left')\nCOUNT_1Y = pd.get_dummies(pa_ip.loc[pa_ip.ACCOUNT_1Y==1, ['SK_ID_CURR', 'NAME_CONTRACT_TYPE']], prefix='ip_count_1y_').groupby('SK_ID_CURR').sum().reset_index()\nCOUNT_6M = pd.get_dummies(pa_ip.loc[pa_ip.ACCOUNT_6M==1, ['SK_ID_CURR', 'NAME_CONTRACT_TYPE']], prefix='ip_count_6m_').groupby('SK_ID_CURR').sum().reset_index()\nip_feature = ip_feature.merge(COUNT_1Y, on='SK_ID_CURR', how='left').merge(COUNT_6M, on='SK_ID_CURR', how='left')\nfor i in range(-6, 0):\n    ip_feature.iloc[:, i] = ip_feature.iloc[:, i].fillna(0)\nip_feature.shape\n\nip_feature.to_csv('ip_feature.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\napplication_train = pd.read_csv('../input/application_train.csv')\nPOS_CASH_balance = pd.read_csv('../input/POS_CASH_balance.csv')\nPOS_CASH_balance = POS_CASH_balance.sort_values(['SK_ID_CURR', 'SK_ID_PREV', 'MONTHS_BALANCE'])\n\npcb_prev_count = POS_CASH_balance.groupby('SK_ID_CURR')['SK_ID_PREV'].nunique()\npcb_avg_month = POS_CASH_balance.groupby('SK_ID_CURR')['MONTHS_BALANCE'].count()/pcb_prev_count\npcb_recent_active = POS_CASH_balance.groupby('SK_ID_CURR')['MONTHS_BALANCE'].max()\n\n# times of INSTALMENT change\npcb_temp_inst_change_time = POS_CASH_balance[['SK_ID_PREV', 'CNT_INSTALMENT']].groupby('SK_ID_PREV')['CNT_INSTALMENT'].nunique().map(lambda x: x - 1).reset_index().rename(columns={'CNT_INSTALMENT':'pcb_prev_inst_change_time'})\npcb_temp = POS_CASH_balance.groupby(['SK_ID_CURR', 'SK_ID_PREV'])['MONTHS_BALANCE'].count().reset_index()\npcb_temp = pcb_temp.merge(pcb_temp_inst_change_time, on='SK_ID_PREV')\npcb_inst_change_time = pcb_temp.groupby('SK_ID_CURR')['pcb_prev_inst_change_time'].sum()\n\n# avg INSTALMENT\npcb_temp = POS_CASH_balance.groupby(['SK_ID_CURR', 'SK_ID_PREV'])['CNT_INSTALMENT'].mean().reset_index()\npcb_avg_inst = pcb_temp.groupby(['SK_ID_CURR'])['CNT_INSTALMENT'].mean()\n\n# active INSTALMENT\npcb_temp = POS_CASH_balance.groupby(['SK_ID_CURR', 'SK_ID_PREV'])['MONTHS_BALANCE'].max().reset_index()\npcb_temp = pcb_temp.merge(POS_CASH_balance[['SK_ID_CURR', 'SK_ID_PREV', 'MONTHS_BALANCE', 'NAME_CONTRACT_STATUS']], on=['SK_ID_CURR', 'SK_ID_PREV', 'MONTHS_BALANCE'])\npcb_temp['active_1'] = pcb_temp.MONTHS_BALANCE.map(lambda x: 1 if x>=-4 else 0)\npcb_temp['active_2'] = pcb_temp.NAME_CONTRACT_STATUS.map(lambda x: 1 if x=='Active' else 0)\npcb_temp['active'] = pcb_temp.active_1 * pcb_temp.active_2\npcb_active_inst = pcb_temp.groupby('SK_ID_CURR')['active'].count()\n\n# DPD\npcb_temp = POS_CASH_balance[['SK_ID_CURR', 'SK_ID_PREV', 'SK_DPD', 'SK_DPD_DEF']].groupby(['SK_ID_CURR','SK_ID_PREV'])['SK_DPD','SK_DPD_DEF'].max().reset_index()\npcb_max_dpd_days = pcb_temp.groupby('SK_ID_CURR')['SK_DPD'].max()\npcb_total_dpd_days = pcb_temp.groupby('SK_ID_CURR')['SK_DPD'].sum()\npcb_max_largedpd_days = pcb_temp.groupby('SK_ID_CURR')['SK_DPD_DEF'].max()\npcb_total_largedpd_days = pcb_temp.groupby('SK_ID_CURR')['SK_DPD_DEF'].sum()\n\npcb_temp = POS_CASH_balance.loc[POS_CASH_balance.MONTHS_BALANCE>=-12, ['SK_ID_CURR', 'SK_ID_PREV', 'SK_DPD', 'SK_DPD_DEF']].groupby(['SK_ID_CURR','SK_ID_PREV'])['SK_DPD','SK_DPD_DEF'].max().reset_index()\npcb_max_dpd_days_1y = pcb_temp.groupby('SK_ID_CURR')['SK_DPD'].max()\npcb_total_dpd_days_1y = pcb_temp.groupby('SK_ID_CURR')['SK_DPD'].sum()\npcb_max_largedpd_days_1y = pcb_temp.groupby('SK_ID_CURR')['SK_DPD_DEF'].max()\npcb_total_largedpd_days_1y = pcb_temp.groupby('SK_ID_CURR')['SK_DPD_DEF'].sum()\n\npcb_temp = POS_CASH_balance.loc[POS_CASH_balance.MONTHS_BALANCE>=-24, ['SK_ID_CURR', 'SK_ID_PREV', 'SK_DPD', 'SK_DPD_DEF']].groupby(['SK_ID_CURR','SK_ID_PREV'])['SK_DPD','SK_DPD_DEF'].max().reset_index()\npcb_max_dpd_days_2y = pcb_temp.groupby('SK_ID_CURR')['SK_DPD'].max()\npcb_total_dpd_days_2y = pcb_temp.groupby('SK_ID_CURR')['SK_DPD'].sum()\npcb_max_largedpd_days_2y = pcb_temp.groupby('SK_ID_CURR')['SK_DPD_DEF'].max()\npcb_total_largedpd_days_2y = pcb_temp.groupby('SK_ID_CURR')['SK_DPD_DEF'].sum()\n\npcb_num = pd.DataFrame({'pcb_prev_count':pcb_prev_count, 'pcb_avg_month':pcb_avg_month, 'pcb_recent_active':pcb_recent_active, 'pcb_inst_change_time':pcb_inst_change_time,\n                       'pcb_avg_inst':pcb_avg_inst, 'pcb_active_inst':pcb_active_inst, \n                       'pcb_max_dpd_days':pcb_max_dpd_days, 'pcb_total_dpd_days':pcb_total_dpd_days, 'pcb_max_largedpd_days':pcb_max_largedpd_days, 'pcb_total_largedpd_days':pcb_total_largedpd_days,\n                       'pcb_max_dpd_days_1y':pcb_max_dpd_days_1y, 'pcb_total_dpd_days_1y':pcb_total_dpd_days_1y, 'pcb_max_largedpd_days_1y':pcb_max_largedpd_days_1y, \n                       'pcb_total_largedpd_days_1y':pcb_total_largedpd_days_1y, 'pcb_max_dpd_days_2y':pcb_max_dpd_days_2y, 'pcb_total_dpd_days_2y':pcb_total_dpd_days_2y, \n                       'pcb_max_largedpd_days_2y':pcb_max_largedpd_days_2y, 'pcb_total_largedpd_days_2y':pcb_total_largedpd_days_2y,}).reset_index()\n\t\t\t\t\t   \n# INSTALMENT: ratio of end status of each type\npcb_temp = POS_CASH_balance.groupby(['SK_ID_CURR', 'SK_ID_PREV'])['MONTHS_BALANCE'].max().reset_index()\npcb_temp = pcb_temp.merge(POS_CASH_balance[['SK_ID_CURR', 'SK_ID_PREV', 'MONTHS_BALANCE', 'NAME_CONTRACT_STATUS']], on=['SK_ID_CURR', 'SK_ID_PREV', 'MONTHS_BALANCE'])\npcb_temp_dummy = pd.get_dummies(pcb_temp, prefix='pcb_end_as')\npcb_end_as_dummy = pcb_temp_dummy.loc[pcb_temp_dummy.pcb_end_as_Active!=1].drop(['SK_ID_PREV','MONTHS_BALANCE', 'pcb_end_as_Active'], axis=1).groupby('SK_ID_CURR').sum().reset_index()\ndel pcb_temp_dummy\ngc.collect()\n\npcb_feature = pcb_num.merge(pcb_end_as_dummy, on='SK_ID_CURR', how='left')\n\npcb_feature['pcb_no_dpd'] = pcb_feature.pcb_total_dpd_days.map(lambda x: 0 if x== 0 else 1)\npcb_feature['pcb_no_largedpd'] = pcb_feature.pcb_total_largedpd_days.map(lambda x: 0 if x== 0 else 1)\npcb_feature['pcb_no_dpd_1y'] = pcb_feature.pcb_total_dpd_days_1y.map(lambda x: 0 if x== 0 else 1)\npcb_feature['pcb_no_largedpd_1y'] = pcb_feature.pcb_total_largedpd_days_1y.map(lambda x: 0 if x== 0 else 1)\npcb_feature['pcb_no_dpd_2y'] = pcb_feature.pcb_total_dpd_days_2y.map(lambda x: 0 if x== 0 else 1)\npcb_feature['pcb_no_largedpd_2y'] = pcb_feature.pcb_total_largedpd_days_2y.map(lambda x: 0 if x== 0 else 1)\npcb_feature.to_csv('pcb_feature.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprevious_application = pd.read_csv(\"../input/previous_application.csv\")\nprevious_application = previous_application.sort_values(['SK_ID_CURR', 'DAYS_DECISION'])\n\n# cat\ncat_col = []\nfor i in range(len(previous_application.columns)):\n    if previous_application.iloc[:, i].dtype == 'object':\n        cat_col.append(i)\ncat_pa = previous_application.iloc[:, cat_col]\ncat_pa = pd.concat([previous_application[['HOUR_APPR_PROCESS_START', 'NFLAG_LAST_APPL_IN_DAY', 'NFLAG_INSURED_ON_APPROVAL']],cat_pa], axis=1).fillna('XNA')\ncat_pa.HOUR_APPR_PROCESS_START = cat_pa.HOUR_APPR_PROCESS_START.astype('object')\ncat_pa.NFLAG_LAST_APPL_IN_DAY = cat_pa.NFLAG_LAST_APPL_IN_DAY.astype('object')\ncat_pa.NFLAG_INSURED_ON_APPROVAL = cat_pa.NFLAG_INSURED_ON_APPROVAL.astype('object')\n# selected version\ncat_NAME_CONTRACT_TYPE = pd.get_dummies(cat_pa.NAME_CONTRACT_TYPE, prefix='pa_NAME_CONTRACT_TYPE').drop('pa_NAME_CONTRACT_TYPE_XNA', axis=1)\ncat_NAME_CASH_LOAN_PURPOSE = pd.get_dummies(cat_pa.NAME_CASH_LOAN_PURPOSE, prefix='pa_NAME_CASH_LOAN_PURPOSE').drop('pa_NAME_CASH_LOAN_PURPOSE_XNA', axis=1)\ncat_NAME_CONTRACT_STATUS = pd.get_dummies(cat_pa.NAME_CONTRACT_STATUS, prefix='pa_NAME_CONTRACT_STATUS')\ncat_NAME_PAYMENT_TYPE = pd.get_dummies(cat_pa.NAME_PAYMENT_TYPE, prefix='pa_NAME_PAYMENT_TYPE').drop('pa_NAME_PAYMENT_TYPE_XNA', axis=1)\ncat_CODE_REJECT_REASON = pd.get_dummies(cat_pa.CODE_REJECT_REASON, prefix='pa_CODE_REJECT_REASON').drop('pa_CODE_REJECT_REASON_XNA', axis=1)\ncat_NAME_CLIENT_TYPE = pd.get_dummies(cat_pa.NAME_CLIENT_TYPE, prefix='pa_NAME_CLIENT_TYPE').drop('pa_NAME_CLIENT_TYPE_XNA', axis=1)\ncat_NAME_PORTFOLIO = pd.get_dummies(cat_pa.NAME_PORTFOLIO, prefix='pa_NAME_PORTFOLIO').drop('pa_NAME_PORTFOLIO_XNA', axis=1)\ncat_NAME_PRODUCT_TYPE = pd.get_dummies(cat_pa.NAME_PRODUCT_TYPE, prefix='pa_NAME_PRODUCT_TYPE').drop('pa_NAME_PRODUCT_TYPE_XNA', axis=1)\ncat_NAME_YIELD_GROUP = pd.get_dummies(cat_pa.NAME_YIELD_GROUP, prefix='pa_NAME_YIELD_GROUP').drop('pa_NAME_YIELD_GROUP_XNA', axis=1)\ncat_PRODUCT_COMBINATION = pd.get_dummies(cat_pa.PRODUCT_COMBINATION, prefix='pa_PRODUCT_COMBINATION').drop('pa_PRODUCT_COMBINATION_XNA', axis=1)\ncat_NFLAG_INSURED_ON_APPROVAL = pd.get_dummies(cat_pa.NFLAG_INSURED_ON_APPROVAL, prefix='pa_NFLAG_INSURED_ON_APPROVAL').drop('pa_NFLAG_INSURED_ON_APPROVAL_XNA', axis=1)\ncat_pa_dummy = pd.concat([previous_application[['SK_ID_PREV', 'SK_ID_CURR']], cat_NAME_CONTRACT_TYPE, cat_NAME_CASH_LOAN_PURPOSE, cat_NAME_CONTRACT_STATUS, cat_NAME_PAYMENT_TYPE, cat_CODE_REJECT_REASON, \n                          cat_NAME_CLIENT_TYPE, cat_NAME_PORTFOLIO, cat_NAME_PRODUCT_TYPE, cat_NAME_YIELD_GROUP, cat_PRODUCT_COMBINATION, cat_NFLAG_INSURED_ON_APPROVAL], axis=1)\ndel cat_NAME_CONTRACT_TYPE, cat_NAME_CASH_LOAN_PURPOSE, cat_NAME_CONTRACT_STATUS, cat_NAME_PAYMENT_TYPE, cat_CODE_REJECT_REASON, cat_NAME_CLIENT_TYPE, \ncat_NAME_PORTFOLIO, cat_NAME_PRODUCT_TYPE, cat_NAME_YIELD_GROUP, cat_PRODUCT_COMBINATION, cat_NFLAG_INSURED_ON_APPROVAL, cat_pa\ngc.collect()\npa_cat = cat_pa_dummy.drop('SK_ID_PREV', axis=1).groupby('SK_ID_CURR').mean().reset_index()\n\n# num\nnum_col = []\nfor i in range(len(previous_application.columns)):\n    if (previous_application.iloc[:, i].dtype == 'int64') or (previous_application.iloc[:, i].dtype == 'float64'):\n        num_col.append(i)\nnum_pa = previous_application.iloc[:, num_col].drop(['HOUR_APPR_PROCESS_START', 'NFLAG_LAST_APPL_IN_DAY', 'NFLAG_INSURED_ON_APPROVAL'], axis=1)\nfor i in range(-5, 0):\n    num_pa.iloc[:, i] = num_pa.iloc[:, i].map(lambda x: np.nan if x==365243.0 else x)\n# create new variables\nnum_pa['app_vs_actual_less'] = (num_pa.AMT_APPLICATION < num_pa.AMT_CREDIT).map(lambda x: 1 if x==True else 0)\nnum_pa['app_vs_actual_more'] = (num_pa.AMT_APPLICATION > num_pa.AMT_CREDIT).map(lambda x: 1 if x==True else 0)\na = num_pa.DAYS_DECISION - num_pa.DAYS_DECISION.min() + 1\nnum_pa['ADJ_SCORE'] = (a-a.min()) / (a.max()-a.min()) + 0.5\nnum_pa['ADJ_AMT_ANNUITY'] = num_pa['ADJ_SCORE']*num_pa['AMT_ANNUITY']\nnum_pa['ADJ_AMT_APPLICATION'] = num_pa['ADJ_SCORE']*num_pa['AMT_APPLICATION']\nnum_pa['ADJ_AMT_CREDIT'] = num_pa['ADJ_SCORE']*num_pa['AMT_CREDIT']\nnum_pa['FREQ'] = (num_pa['DAYS_LAST_DUE']-num_pa['DAYS_FIRST_DUE'])/num_pa['CNT_PAYMENT']\npa_prev_count = num_pa.groupby('SK_ID_CURR')['SK_ID_PREV'].count()\npa_avg_annuity = num_pa.groupby('SK_ID_CURR')['AMT_ANNUITY'].mean()\npa_avg_application = num_pa.groupby('SK_ID_CURR')['AMT_APPLICATION'].mean()\npa_avg_actual_credit = num_pa.groupby('SK_ID_CURR')['AMT_CREDIT'].mean()\npa_max_application = num_pa.groupby('SK_ID_CURR')['AMT_APPLICATION'].max()\npa_max_actual_credit = num_pa.groupby('SK_ID_CURR')['AMT_CREDIT'].max()\npa_total_annuity = num_pa.groupby('SK_ID_CURR')['AMT_ANNUITY'].sum()\npa_total_application = num_pa.groupby('SK_ID_CURR')['AMT_APPLICATION'].sum()\npa_total_actual_credit = num_pa.groupby('SK_ID_CURR')['AMT_CREDIT'].sum()\n# compare application and actual credit\npa_not_full_credit_times = num_pa.groupby('SK_ID_CURR')['app_vs_actual_less'].sum()\npa_not_full_credit_rate = num_pa.groupby('SK_ID_CURR')['app_vs_actual_less'].mean()\npa_get_more_credit_times = num_pa.groupby('SK_ID_CURR')['app_vs_actual_more'].sum()\npa_get_more_credit_rate = num_pa.groupby('SK_ID_CURR')['app_vs_actual_more'].mean()\n# adjusted: more recent, more important\npa_total_annuity_adj = num_pa.groupby('SK_ID_CURR')['ADJ_AMT_ANNUITY'].sum()\npa_total_application_adj = num_pa.groupby('SK_ID_CURR')['ADJ_AMT_APPLICATION'].sum()\npa_total_actual_credit_adj = num_pa.groupby('SK_ID_CURR')['ADJ_AMT_CREDIT'].sum()\n# down payment\npa_avg_down_payment = num_pa.groupby('SK_ID_CURR')['AMT_DOWN_PAYMENT'].mean()\npa_max_down_payment = num_pa.groupby('SK_ID_CURR')['AMT_DOWN_PAYMENT'].max()\npa_total_down_payment = num_pa.groupby('SK_ID_CURR')['AMT_DOWN_PAYMENT'].sum()\n# goods price\npa_avg_goods_price = num_pa.groupby('SK_ID_CURR')['AMT_GOODS_PRICE'].mean()\npa_max_goods_price = num_pa.groupby('SK_ID_CURR')['AMT_GOODS_PRICE'].max()\npa_total_goods_price = num_pa.groupby('SK_ID_CURR')['AMT_GOODS_PRICE'].sum()\n# down payment rate\npa_avg_down_payment_rate = num_pa.groupby('SK_ID_CURR')['RATE_DOWN_PAYMENT'].mean()\npa_max_down_payment_rate = num_pa.groupby('SK_ID_CURR')['RATE_DOWN_PAYMENT'].max()\npa_total_down_payment_rate = num_pa.groupby('SK_ID_CURR')['RATE_DOWN_PAYMENT'].sum()\n# selling area\npa_avg_selling_area = num_pa.groupby('SK_ID_CURR')['SELLERPLACE_AREA'].mean()\npa_max_selling_area = num_pa.groupby('SK_ID_CURR')['SELLERPLACE_AREA'].max()\npa_total_selling_area = num_pa.groupby('SK_ID_CURR')['SELLERPLACE_AREA'].sum()\n# term\npa_avg_term = num_pa.groupby('SK_ID_CURR')['CNT_PAYMENT'].mean()\npa_max_term = num_pa.groupby('SK_ID_CURR')['CNT_PAYMENT'].max()\npa_total_term = num_pa.groupby('SK_ID_CURR')['CNT_PAYMENT'].sum()\npa_most_frequent_term = num_pa.groupby('SK_ID_CURR')['CNT_PAYMENT'].agg(pd.Series.mode).map(lambda x: np.mean(x))\n# recent decision\npa_recent_decision_day = num_pa.groupby('SK_ID_CURR')['DAYS_DECISION'].max()\npa_earliest_decision_day = num_pa.groupby('SK_ID_CURR')['DAYS_DECISION'].min()\npa_usage_length = pa_recent_decision_day - pa_earliest_decision_day\n# application interval\nnum_pa['application_interval'] = num_pa.groupby('SK_ID_CURR')['DAYS_DECISION'].diff(-1)\nmissing_iter = iter(num_pa.groupby('SK_ID_CURR')['DAYS_DECISION'].max())\nnum_pa.application_interval = num_pa.application_interval.map(lambda x: -next(missing_iter) if np.isnan(x) else -x)\npa_avg_intervel = num_pa.groupby('SK_ID_CURR')['application_interval'].mean()\npa_sd_intervel = num_pa.groupby('SK_ID_CURR')['application_interval'].agg('std').fillna(0)\n# days\npa_first_due_day = num_pa.groupby('SK_ID_CURR')['DAYS_FIRST_DUE'].min()\npa_last_due_day = num_pa.groupby('SK_ID_CURR')['DAYS_LAST_DUE'].max()\npa_last_termination_day = num_pa.groupby('SK_ID_CURR')['DAYS_TERMINATION'].max()\npa_lastdue_termination_range = pa_last_termination_day - pa_last_due_day\npa_avg_freq = num_pa.groupby('SK_ID_CURR')['FREQ'].mean()\npa_min_freq = num_pa.groupby('SK_ID_CURR')['FREQ'].min()\npa_num = pd.DataFrame({'pa_prev_count':pa_prev_count, 'pa_avg_annuity':pa_avg_annuity, 'pa_avg_application':pa_avg_application, 'pa_avg_actual_credit':pa_avg_actual_credit, \n'pa_max_application':pa_max_application, 'pa_max_actual_credit':pa_max_actual_credit, 'pa_total_annuity':pa_total_annuity, 'pa_total_application':pa_total_application, 'pa_total_actual_credit':pa_total_actual_credit,\n'pa_not_full_credit_times':pa_not_full_credit_times, 'pa_not_full_credit_rate':pa_not_full_credit_rate, 'pa_get_more_credit_times':pa_get_more_credit_times, 'pa_get_more_credit_rate':pa_get_more_credit_rate,\n'pa_total_annuity_adj':pa_total_annuity_adj, 'pa_total_application_adj':pa_total_application_adj, 'pa_total_actual_credit_adj':pa_total_actual_credit_adj,\n'pa_avg_down_payment':pa_avg_down_payment, 'pa_max_down_payment':pa_max_down_payment, 'pa_total_down_payment':pa_total_down_payment,\n'pa_avg_goods_price':pa_avg_goods_price, 'pa_max_goods_price':pa_max_goods_price, 'pa_total_goods_price':pa_total_goods_price,\n'pa_avg_down_payment_rate':pa_avg_down_payment_rate, 'pa_max_down_payment_rate':pa_max_down_payment_rate, 'pa_total_down_payment_rate':pa_total_down_payment_rate,\n'pa_avg_selling_area':pa_avg_selling_area, 'pa_max_selling_area':pa_max_selling_area, 'pa_total_selling_area':pa_total_selling_area,\n'pa_avg_term':pa_avg_term, 'pa_max_term':pa_max_term, 'pa_total_term':pa_total_term, 'pa_most_frequent_term':pa_most_frequent_term,\n'pa_recent_decision_day':pa_recent_decision_day, 'pa_earliest_decision_day':pa_earliest_decision_day, 'pa_usage_length': pa_usage_length,\n'pa_avg_intervel':pa_avg_intervel, 'pa_sd_intervel':pa_sd_intervel, 'pa_first_due_day':pa_first_due_day, 'pa_last_due_day':pa_last_due_day, \n'pa_last_termination_day':pa_last_termination_day, 'pa_lastdue_termination_range':pa_lastdue_termination_range, 'pa_avg_freq':pa_avg_freq, 'pa_min_freq':pa_min_freq}).reset_index()\npa_feature = pa_num.merge(pa_cat, on='SK_ID_CURR')\ndel pa_num, pa_cat\ngc.collect()\npa_feature.to_csv('pa_feature.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\n\nbureau = pd.read_csv('../input/bureau.csv')\nbureau_balance = pd.read_csv('../input/bureau_balance.csv')\nmed = bureau.AMT_CREDIT_SUM.median()\nbureau.AMT_CREDIT_SUM = bureau.AMT_CREDIT_SUM.fillna(med)\nmed = bureau.AMT_CREDIT_SUM_DEBT.median()\nbureau.AMT_CREDIT_SUM_DEBT = bureau.AMT_CREDIT_SUM_DEBT.fillna(med)\nbureau['OVERDUE_DEBT_RATIO'] = bureau.AMT_CREDIT_SUM_OVERDUE/(bureau.AMT_CREDIT_SUM_DEBT+1)\nbureau['DEBT_TOTAL_RATIO'] = bureau.AMT_CREDIT_SUM_DEBT/(bureau.AMT_CREDIT_SUM+1)\nbureau_balance['INT_STATUS'] = bureau_balance.STATUS.replace('X', 0.1).replace('C', 0).astype('int64')\nbur_max_bad_level = bureau_balance.groupby('SK_ID_BUREAU')['INT_STATUS'].max().reset_index()\ncluter_bur = bureau[['SK_ID_BUREAU', 'CREDIT_DAY_OVERDUE', 'OVERDUE_DEBT_RATIO', 'DEBT_TOTAL_RATIO', 'CNT_CREDIT_PROLONG']].merge(bur_max_bad_level, on='SK_ID_BUREAU', how='left').fillna(0)\n\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.preprocessing import Normalizer\nX = cluter_bur.drop(['SK_ID_BUREAU'], axis=1)\nX = Normalizer().fit_transform(X)\ngmm = GaussianMixture(n_components=2, verbose=5, max_iter=100, init_params='kmeans')\ngmm.fit(X)\ngroup_prob = gmm.predict_proba(X)\ngroup_prob = np.round(group_prob, decimals=2)\nbur_cluster = pd.concat([bureau[['SK_ID_CURR', 'SK_ID_BUREAU']], pd.DataFrame({'cluster':group_prob[:, 0]})], axis=1)\nbur_cluster = bur_cluster.groupby('SK_ID_CURR')['cluster'].mean().reset_index()\nbur_cluster.to_csv('bur_cluster.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Derivatives","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport featuretools as ft\nimport gc\nimport warnings\nimport time\na = time.time()\nrow1=None\nrow2=None\nrow3=None\napp_train = pd.read_csv('../input/application_train.csv', nrows=row1).sort_values('SK_ID_CURR')\napp_test = pd.read_csv('../input/application_test.csv', nrows=row1).sort_values('SK_ID_CURR')\nbureau = pd.read_csv('../input/bureau.csv', nrows=row2).sort_values(['SK_ID_CURR', 'SK_ID_BUREAU'])\nbureau_balance = pd.read_csv('../input/bureau_balance.csv', nrows=row3).sort_values(['SK_ID_BUREAU', 'MONTHS_BALANCE'])\nprevious = pd.read_csv('../input/previous_application.csv', nrows=row3).sort_values(['SK_ID_CURR', 'SK_ID_PREV'])\ncash = pd.read_csv('../input/POS_CASH_balance.csv', nrows=row3).sort_values(['SK_ID_CURR', 'SK_ID_PREV', 'MONTHS_BALANCE'])\ncredit = pd.read_csv('../input/credit_card_balance.csv', nrows=row3).sort_values(['SK_ID_CURR', 'SK_ID_PREV', 'MONTHS_BALANCE'])\ninstallments = pd.read_csv('../input/installments_payments.csv', nrows=row3).sort_values(['SK_ID_CURR', 'SK_ID_PREV'])\n\n# data manipulation\napp_train = app_train[['SK_ID_CURR']]\napp_test = app_test[['SK_ID_CURR']]\nbureau = bureau[['SK_ID_CURR', 'SK_ID_BUREAU', 'DAYS_CREDIT', 'AMT_CREDIT_MAX_OVERDUE', 'CNT_CREDIT_PROLONG', \n                 'AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM_LIMIT', 'AMT_CREDIT_SUM_OVERDUE']]\nbureau_balance.STATUS = bureau_balance.STATUS.map({'C':0, 'X':0.1, '1':1, '2':2, '3':3, '4':4, '5':5})\nprevious = previous[['SK_ID_CURR', 'SK_ID_PREV', 'AMT_ANNUITY', 'AMT_APPLICATION', 'AMT_CREDIT', 'AMT_GOODS_PRICE', 'NAME_CONTRACT_STATUS', 'DAYS_DECISION', \n                     'CNT_PAYMENT', 'NAME_YIELD_GROUP', 'NFLAG_INSURED_ON_APPROVAL','SELLERPLACE_AREA']]\nprevious.NAME_CONTRACT_STATUS = previous.NAME_CONTRACT_STATUS.map(lambda x: 1 if x=='Refused' else 0)\nprevious.NAME_YIELD_GROUP = previous.NAME_YIELD_GROUP.map({'XNA':0, 'low_noraml':1, 'low_action':1, 'middle':2, 'high':3})\ncash = cash[['SK_ID_PREV', 'MONTHS_BALANCE', 'SK_DPD', 'SK_DPD_DEF']]\ncredit = credit[['SK_ID_PREV', 'MONTHS_BALANCE', 'SK_DPD', 'SK_DPD_DEF', 'AMT_BALANCE', 'AMT_CREDIT_LIMIT_ACTUAL', 'AMT_INST_MIN_REGULARITY', 'AMT_TOTAL_RECEIVABLE']]\ninstallments['date_diff'] = installments.DAYS_INSTALMENT - installments.DAYS_ENTRY_PAYMENT\ninstallments['amount_diff'] = installments.AMT_INSTALMENT - installments.AMT_PAYMENT\ninstallments = installments[['SK_ID_PREV', 'DAYS_INSTALMENT', 'amount_diff', 'date_diff']]\n\n# time\nimport re\n\ndef replace_day_outliers(df):\n    \"\"\"Replace 365243 with np.nan in any columns with DAYS\"\"\"\n    for col in df.columns:\n        if \"DAYS\" in col:\n            df[col] = df[col].replace({365243: np.nan})\n\n    return df\n\n# Replace all the day outliers\napp_train = replace_day_outliers(app_train)\napp_test = replace_day_outliers(app_test)\nbureau = replace_day_outliers(bureau)\nbureau_balance = replace_day_outliers(bureau_balance)\nprevious = replace_day_outliers(previous)\ncash = replace_day_outliers(cash)\ncredit = replace_day_outliers(credit)\ninstallments = replace_day_outliers(installments)\nstart_date = pd.Timestamp(\"2018-01-01\")\n# bureau\nfor col in ['DAYS_CREDIT']:\n    bureau[col] = pd.to_timedelta(bureau[col], 'D')\n# Create the date columns\nbureau['bureau_credit_application_date'] = start_date + bureau['DAYS_CREDIT']\n# balance\nbureau_balance['MONTHS_BALANCE'] = pd.to_timedelta(bureau_balance['MONTHS_BALANCE'], 'M')\n# Make a date column\nbureau_balance['bureau_balance_date'] = start_date + bureau_balance['MONTHS_BALANCE']\nbureau = bureau_balance.drop(columns = ['DAYS_CREDIT'])\nbureau_balance = bureau_balance.drop(columns = ['MONTHS_BALANCE'])\n# Convert to timedeltas in days\nfor col in ['DAYS_DECISION']:\n    previous[col] = pd.to_timedelta(previous[col], 'D')\n    \n# Make date columns\nprevious['previous_decision_date'] = start_date + previous['DAYS_DECISION']\n\n# Drop the time offset columns\nprevious = previous.drop(columns = ['DAYS_DECISION'])\n\n# # cash\ncash['MONTHS_BALANCE'] = pd.to_timedelta(cash['MONTHS_BALANCE'], 'M')\ncash['cash_balance_date'] = start_date + cash['MONTHS_BALANCE']\ncash = cash.drop(columns = ['MONTHS_BALANCE'])\n\n# # credit\ncredit['MONTHS_BALANCE'] = pd.to_timedelta(credit['MONTHS_BALANCE'], 'M')\ncredit['credit_balance_date'] = start_date + credit['MONTHS_BALANCE']\ncredit = credit.drop(columns = ['MONTHS_BALANCE'])\n\n# installment\ninstallments['DAYS_INSTALMENT'] = pd.to_timedelta(installments['DAYS_INSTALMENT'], 'D')\ninstallments['installments_due_date'] = start_date + installments['DAYS_INSTALMENT']\ninstallments = installments.drop(columns = ['DAYS_INSTALMENT'])\n# Make an entityset\nes = ft.EntitySet(id = 'clients')\n\nes = es.entity_from_dataframe(entity_id = 'app_train', dataframe = app_train, \n                              index = 'SK_ID_CURR')\n\nes = es.entity_from_dataframe(entity_id = 'app_test', dataframe = app_test, \n                              index = 'SK_ID_CURR')\n\n# part 1\nes = es.entity_from_dataframe(entity_id = 'bureau', dataframe = bureau, \n                              index = 'SK_ID_BUREAU', time_index='bureau_credit_application_date')\n\nes = es.entity_from_dataframe(entity_id = 'bureau_balance', dataframe = bureau_balance, \n                              make_index = True, index = 'bb_index',\n                              time_index = 'bureau_balance_date')\n\n# part 2\nes = es.entity_from_dataframe(entity_id = 'previous', dataframe = previous, \n                              index = 'SK_ID_PREV', time_index = 'previous_decision_date')\n\n\nes = es.entity_from_dataframe(entity_id = 'cash', dataframe = cash, \n                              make_index = True, index = 'cash_index',\n                              time_index = 'cash_balance_date')\n\nes = es.entity_from_dataframe(entity_id = 'installments', dataframe = installments,\n                              make_index = True, index = 'installments_index',\n                              time_index = 'installments_due_date')\n\nes = es.entity_from_dataframe(entity_id = 'credit', dataframe = credit,\n                              make_index = True, index = 'credit_index',\n                              time_index = 'credit_balance_date')\n# Relationship between app and bureau\nr_app_bureau = ft.Relationship(es['app_train']['SK_ID_CURR'], es['bureau']['SK_ID_CURR'])\n\n# Test Relationship between app and bureau\nr_test_app_bureau = ft.Relationship(es['app_test']['SK_ID_CURR'], es['bureau']['SK_ID_CURR'])\n\n# Relationship between bureau and bureau balance\nr_bureau_balance = ft.Relationship(es['bureau']['SK_ID_BUREAU'], es['bureau_balance']['SK_ID_BUREAU'])\n\n# Relationship between current app and previous apps\nr_app_previous = ft.Relationship(es['app_train']['SK_ID_CURR'], es['previous']['SK_ID_CURR'])\n\n# Test Relationship between current app and previous apps\nr_test_app_previous = ft.Relationship(es['app_test']['SK_ID_CURR'], es['previous']['SK_ID_CURR'])\n\n# Relationships between previous apps and cash, installments, and credit\nr_previous_cash = ft.Relationship(es['previous']['SK_ID_PREV'], es['cash']['SK_ID_PREV'])\nr_previous_installments = ft.Relationship(es['previous']['SK_ID_PREV'], es['installments']['SK_ID_PREV'])\nr_previous_credit = ft.Relationship(es['previous']['SK_ID_PREV'], es['credit']['SK_ID_PREV'])\n\n# Add in the defined relationships\nes = es.add_relationships([r_app_bureau, r_test_app_bureau, r_bureau_balance, \n\t\t\t\t\t\t   r_app_previous, r_test_app_previous, r_previous_cash,\n\t\t\t\t\t\t   r_previous_installments, r_previous_credit])\n\t\t\t\t\t\t   \n\ndel app_train, app_test, bureau, bureau_balance, cash, credit, previous, installments\ngc.collect()\nprint('prepare time:', str(time.time()-a))\n\n# train features\na = time.time()\ntime_features, time_feature_names = ft.dfs(entityset = es, target_entity = 'app_train', \n                                           #trans_primitives = ['cum_sum'], \n                                           max_depth = 2,\n                                           agg_primitives = ['trend'],\n                                           features_only = False, verbose = True,\n                                           chunk_size = 30000,\n                                           ignore_entities = ['app_test'])\nprint('feature time:,', str(time.time()-a))\ntime_features.reset_index().to_csv('trend3_train.csv', index=False)\n# test features\ntime_features_test, time_feature_names = ft.dfs(entityset = es, target_entity = 'app_test', \n                                           #trans_primitives = ['cum_sum', 'time_since_previous'], \n                                           max_depth = 2,\n                                           agg_primitives = ['trend'],\n                                           features_only = False, verbose = True,\n                                           chunk_size = 25000,\n                                           ignore_entities = ['app_train'])\ntime_features_test.reset_index().to_csv('trend4_test.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merge","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nfull_df = pd.read_csv(\"../input/home-credit-default-risk/application_train.csv\")\ntest_df = pd.read_csv(\"../input/home-credit-default-risk/application_test.csv\")\n\n# bureau_feature_df = pd.read_csv(\"../input/features/bureau_feature.csv\")\n# ccb_feature_df = pd.read_csv(\"../input/features/ccb_feature.csv\")\n# ip_feature_df = pd.read_csv(\"../input/features/ip_feature.csv\")\n# pcb_feature_df = pd.read_csv(\"../input/features/pcb_feature.csv\")\n# pa_feature_df = pd.read_csv(\"../input/features/pa_feature.csv\")\n# bur_cluster_df = pd.read_csv(\"../input/features/bur_cluster.csv\")\n\n# full_df = full_df.merge(bureau_feature_df, on = 'SK_ID_CURR', how = 'left')\n# full_df = full_df.merge(ccb_feature_df, on = 'SK_ID_CURR', how = 'left')\n# full_df = full_df.merge(ip_feature_df, on = 'SK_ID_CURR', how = 'left')\n# full_df = full_df.merge(pcb_feature_df, on = 'SK_ID_CURR', how = 'left')\n# full_df = full_df.merge(pa_feature_df, on = 'SK_ID_CURR', how = 'left')\n# full_df = full_df.merge(bur_cluster_df, on = 'SK_ID_CURR', how = 'left')\n\n# test_df = test_df.merge(bureau_feature_df, on = 'SK_ID_CURR', how = 'left')\n# test_df = test_df.merge(ccb_feature_df, on = 'SK_ID_CURR', how = 'left')\n# test_df = test_df.merge(ip_feature_df, on = 'SK_ID_CURR', how = 'left')\n# test_df = test_df.merge(pcb_feature_df, on = 'SK_ID_CURR', how = 'left')\n# test_df = test_df.merge(pa_feature_df, on = 'SK_ID_CURR', how = 'left')\n# test_df = test_df.merge(bur_cluster_df, on = 'SK_ID_CURR', how = 'left')\n\ntest_df\n\n# del bureau_feature_df\n# del ccb_feature_df\n# del ip_feature_df\n# del pcb_feature_df \n# del pa_feature_df \n# del bur_cluster_df \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# One-Hot\n","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# one-hot encoding of categorical variables\nfull_df = pd.get_dummies(full_df)\ntest_df = pd.get_dummies(test_df)\n\nfull_df, test_df = full_df.align(test_df, join = 'inner', axis = 1)\n\ntest_df.head(10)\ntest_df.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Impute\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# list(full_df.drop(columns=['TARGET']).columns)\nfull_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import numpy as np\n\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.impute import SimpleImputer\n\n\n# full_df = full_df.replace([np.inf, -np.inf], np.nan)\n# test_df = test_df.replace([np.inf, -np.inf], np.nan)\n\n# test_df.head(10)\n\n# # Feature names\n# features = list(full_df.columns)\n\n# # Median imputation of missing values\n# imputer = SimpleImputer(strategy = 'median')\n\n# # Scale each feature to 0-1\n# scaler = StandardScaler()\n\n# # Fit on the training data\n# imputer.fit(full_df)\n\n# # Transform both training and testing data\n# train = imputer.transform(full_df)\n# test = imputer.transform(test_df)\n\n# # Repeat with the scaler\n# scaler.fit(train)\n# train = scaler.transform(train)\n# test = scaler.transform(test)\n\n# print('Training data shape: ', train.shape)\n# print('Testing data shape: ', test.shape)\n\n\n# full_df = pd.DataFrame(data=train, columns=full_df.columns)\n# test_df = pd.DataFrame(data=test, columns=test_df.columns)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport gc\n\ndef model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=500, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.5, \n                                   reg_alpha = 0.7, reg_lambda = 0.5, \n                                   subsample = 0.5, n_jobs = -1, random_state = 30)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 25, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nsubmission, fi, metrics = model(full_df, test_df)\nprint('Baseline metrics')\nprint(metrics)\nsubmission.to_csv('baseline_lgb.csv', index = False)\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}