{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Home Credit Default Risk Competition (Introduction KOR)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# 전산학프로젝트 : Machine Leaning 을 통한 리스크 관리 프로젝트\n\n### 본 프로젝트의 목적\n - 은행에서 사용가능한 연체 예측 프로젝트를 진행하는 것이 그 목적이나, 실제 은행의 데이터를 가져오는 것에 어려움이 있어 본 프로젝트는 캐글 내 Home Credit Default Risk 데이터를 기반으로 진행하기로 함.\n - 기본적인 분석 방향은 캐글 내 가장 추천 수가 많은 https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction 를 참고하여 진행함.\n - 머신 러닝을 위한 기본적인 지식과 방법들을 우선 정리하고 추후 이를 활용하여 더 나은 결과와 기능을 만들어내기 위해 지속적으로 수정 및 보완 예정임.","metadata":{}},{"cell_type":"markdown","source":"### Import package","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nfrom lightgbm import LGBMClassifier\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')\n\nfrom contextlib import contextmanager\nimport multiprocessing as mp\nfrom functools import partial\nfrom scipy.stats import kurtosis, iqr, skew","metadata":{"execution":{"iopub.status.busy":"2021-11-05T00:22:40.612792Z","iopub.execute_input":"2021-11-05T00:22:40.613106Z","iopub.status.idle":"2021-11-05T00:22:42.952052Z","shell.execute_reply.started":"2021-11-05T00:22:40.613067Z","shell.execute_reply":"2021-11-05T00:22:42.951212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 대표적인 패키지\n\n### - numpy \nNumerical Python을 의미하는 넘파이는 파이썬에서 선형대수 기반의 프로그램을 쉽게 만들 수 있도록 지원하는 대표적인 패키지로 루프를 사용하지 않고 대량 데이터의 배열 연산을 가능하게 하므로 빠른 배열 연산 속도를 보장함.\n\n### - pandas\n판다스는 데이터 처리를 위해 존재하는 가장 인기 있는 라이브러리로 행과 열로 이뤄진 2차원 데이터를 효율적으로 가공/처리할 수 있는 기능을 제공함.\n\n### - sklearn\n사이킷런은 파이썬 머신러닝 라이브러리 중 가장 많이 사용되는 라이브러리로 머신러닝을 위한 다양한 알고리즘과 개발을 위한 편리한 프레임워크와 API를 제공함.\n \n### - matplotlib\nmatplotlib.pyplot 모듈은 MATLAB과 비슷하게 명령어 스타일로 동작하는 함수의 모음으로 matplotlib.pyplot 모듈의 각각의 함수를 사용해서 간편하게 그래프를 만들고 변화를 줄 수 있음.\n\n### - seaborn\nSeaborn은 matplotlib 기반의 시각화 라이브러리로 유익한 통계 그래픽을 그리기 위한 고급 인터페이스를 제공함.\n\n### - lightgbm\nXGBoost와 함께 가장 각광을 받고 있는 부스팅 계열 알고리즘으로 학습 시간과 메모리 사용량이 상대적으로 적은 편이며 리프 중심 트리분할 방식을 사용함.\n단, 적은 데이터 세트에 사용할 경우 과적합이 발생하기 쉬움(10,000건 이하의 데이터 세트).","metadata":{}},{"cell_type":"markdown","source":"### 데이터 분석\n우선 분석할 데이터는 캐글 내 Home Credit Default Risk 데이터로 각 파일이 의미하는 바는 다음과 같음.\n - application : 대출 신청 시 작성한 내용.\n - Previous application : 과거 대출 기록.\n - bureau : 개인신용평가기관에 기록된 신청자의 과거 타금융기관과 신용거래 내역(국내의 NICE / KCB).\n - application_train.csv : 학습 메인 테이블.\n - application_test.csv : 테스트 메인 테이블.\n - bureau.csv : 신용평가기관에서 제공한 신용도 정보.\n - bureau_balance.csv : 이전 신용거래 월 잔액 정보.\n - POS_CASH_balance.csv : 신용 거래 정보.\n - credit_card_balance.csv : 신용카드 월 잔액 정보.\n - previous_application.csv : 이전 가계신용대출 정보.\n - installments_payments.csv : 대출 상환 내역 정보.\n - HomeCredit_columns_description.csv : 파일의 열에 대한 정보.","metadata":{}},{"cell_type":"code","source":"# List files available\nprint(os.listdir(\"../input/home-credit-default-risk/\"))","metadata":{"execution":{"iopub.status.busy":"2021-11-05T00:22:51.239385Z","iopub.execute_input":"2021-11-05T00:22:51.239763Z","iopub.status.idle":"2021-11-05T00:22:51.246428Z","shell.execute_reply.started":"2021-11-05T00:22:51.239726Z","shell.execute_reply":"2021-11-05T00:22:51.245214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 학습 데이터(train)\n- 각 칼럼에 관련한 사항은 https://chocoffee20.tistory.com/6 참고함.","metadata":{}},{"cell_type":"code","source":"# Training data\napp_train = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T00:49:01.35422Z","iopub.execute_input":"2021-11-05T00:49:01.354559Z","iopub.status.idle":"2021-11-05T00:49:07.871132Z","shell.execute_reply.started":"2021-11-05T00:49:01.354523Z","shell.execute_reply":"2021-11-05T00:49:07.870265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 테스트 데이터(test)\n- 학습 데이터와의 차이 : TARGET 열이 없음\n- [TARGET] : 0 과 1(default)로 구성된 연체 정보로 테스트 데이터에서는 이를 예측하여야 하기 때문에 열이 존재하지 않음.","metadata":{}},{"cell_type":"code","source":"# Testing data features : row 48744 / columns : 121\napp_test = pd.read_csv('../input/home-credit-default-risk/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T00:50:30.326028Z","iopub.execute_input":"2021-11-05T00:50:30.326364Z","iopub.status.idle":"2021-11-05T00:50:31.354578Z","shell.execute_reply.started":"2021-11-05T00:50:30.326329Z","shell.execute_reply":"2021-11-05T00:50:31.353502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train['TARGET'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T00:52:54.059827Z","iopub.execute_input":"2021-11-05T00:52:54.060202Z","iopub.status.idle":"2021-11-05T00:52:54.078957Z","shell.execute_reply.started":"2021-11-05T00:52:54.060159Z","shell.execute_reply":"2021-11-05T00:52:54.077876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train['TARGET'].astype(int).plot.hist();","metadata":{"execution":{"iopub.status.busy":"2021-11-05T00:52:56.143094Z","iopub.execute_input":"2021-11-05T00:52:56.143426Z","iopub.status.idle":"2021-11-05T00:52:56.446038Z","shell.execute_reply.started":"2021-11-05T00:52:56.143375Z","shell.execute_reply":"2021-11-05T00:52:56.445116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이 결과를 보았을 때 제대로 상환되지 않은 대출보다 상환된 대출의 비율이 훨씬 높은 것을 확인할 수 있으며(0값 : 대출 상환, 1값 : 대출 체납)\n- imbalanced class problem\n문제가 발생할 수 있음을 예상할 수 있음.\n- 이러한 문제를 해결하기 위하여 https://www.kaggle.com/kaanboke/catboost-lightgbm-xgboost-explained-by-shap 를 참고하여 추가적으로 작업 진행 예정임(Deals With Imbalanced Data).\n\nimbalanced class problem 이란?\n- 다수 클래스의 수가 소수 클래스의 수보다 월등히 많은 학습상황을 의미.\n- 분류 성능이 저하되는 문제가 발생.\n- https://blog.naver.com/tjdtjdgus99/222208515494 참고","metadata":{}},{"cell_type":"markdown","source":"### 데이터 타입 분석","metadata":{}},{"cell_type":"code","source":"# Number of each type of column\napp_train.dtypes.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T00:54:56.363141Z","iopub.execute_input":"2021-11-05T00:54:56.364425Z","iopub.status.idle":"2021-11-05T00:54:56.374787Z","shell.execute_reply.started":"2021-11-05T00:54:56.364363Z","shell.execute_reply":"2021-11-05T00:54:56.373672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 오브젝트 타입 분석","metadata":{}},{"cell_type":"code","source":"# Number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T00:55:30.403522Z","iopub.execute_input":"2021-11-05T00:55:30.404375Z","iopub.status.idle":"2021-11-05T00:55:30.928442Z","shell.execute_reply.started":"2021-11-05T00:55:30.404309Z","shell.execute_reply":"2021-11-05T00:55:30.92776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 범주형 변수들을 어떻게 처리할 것인가?\n\n### Label encoding\n- 문자열 값들을 숫자형으로 변경.\n- 함수 : labelencoder_counter(df)\n- df 에 읽은 파일을 변수로 입력 : app_train = pd.read_csv('./input/application_train.csv')","metadata":{}},{"cell_type":"code","source":"def labelencoder_counter(df) :\n    # Create a label encoder object\n    le = LabelEncoder()\n    le_count = 0\n\n    # Iterate through the columns\n    for col in df:\n        if app_train[col].dtype == 'object':\n            # If 2 or fewer unique categories\n            if len(list(app_train[col].unique())) <= 2:\n                # Train on the training data\n                le.fit(app_train[col])\n                # Transform both training and testing data\n                app_train[col] = le.transform(app_train[col])\n                app_test[col] = le.transform(app_test[col])\n\n                # Keep track of how many columns were label encoded\n                le_count += 1\n\n    return print('%d columns were label encoded.' % le_count)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T00:56:59.517848Z","iopub.execute_input":"2021-11-05T00:56:59.51815Z","iopub.status.idle":"2021-11-05T00:56:59.52626Z","shell.execute_reply.started":"2021-11-05T00:56:59.518119Z","shell.execute_reply":"2021-11-05T00:56:59.525194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelencoder_counter(app_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T00:57:00.321801Z","iopub.execute_input":"2021-11-05T00:57:00.322917Z","iopub.status.idle":"2021-11-05T00:57:01.208031Z","shell.execute_reply.started":"2021-11-05T00:57:00.32286Z","shell.execute_reply":"2021-11-05T00:57:01.206853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### One-hot encoding\n- 고유값에 해당하는 칼럼에는 1, 나머지에는 0을 표시.\n- 학습(train) / 시험(test) 데이터에 모두 동일한 열이 필요함.\n- 열을 기준으로 정렬 : axis = 1","metadata":{}},{"cell_type":"code","source":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T00:57:18.281183Z","iopub.execute_input":"2021-11-05T00:57:18.282085Z","iopub.status.idle":"2021-11-05T00:57:19.448082Z","shell.execute_reply.started":"2021-11-05T00:57:18.282033Z","shell.execute_reply":"2021-11-05T00:57:19.447216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - 학습(train) / 시험(test) 데이터에 모두 동일한 열이 필요함.\n - 열을 기준으로 정렬 : axis = 1","metadata":{}},{"cell_type":"code","source":"train_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n\n# Add the target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T00:57:25.653232Z","iopub.execute_input":"2021-11-05T00:57:25.653564Z","iopub.status.idle":"2021-11-05T00:57:26.168017Z","shell.execute_reply.started":"2021-11-05T00:57:25.653532Z","shell.execute_reply":"2021-11-05T00:57:26.166944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 상관관계 분석\n - 함수 : correlations(number)\n - number 에 파악하고자 하는 숫자를 입력.","metadata":{}},{"cell_type":"code","source":"def correlations(number) :\n    # Find correlations with the target and sort\n    correlations = app_train.corr()['TARGET'].sort_values()\n\n    # Display correlations\n    print('Most Positive Correlations:\\n', correlations.tail(20))\n    print('\\nMost Negative Correlations:\\n', correlations.head(20))","metadata":{"execution":{"iopub.status.busy":"2021-11-05T01:07:43.664294Z","iopub.execute_input":"2021-11-05T01:07:43.66463Z","iopub.status.idle":"2021-11-05T01:07:43.67193Z","shell.execute_reply.started":"2021-11-05T01:07:43.664598Z","shell.execute_reply":"2021-11-05T01:07:43.670603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlations(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T01:07:44.591932Z","iopub.execute_input":"2021-11-05T01:07:44.592254Z","iopub.status.idle":"2021-11-05T01:08:36.0991Z","shell.execute_reply.started":"2021-11-05T01:07:44.592222Z","shell.execute_reply":"2021-11-05T01:08:36.098173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 상관계수\n- 일반적인 해석\n    - .00-.19 : very weak\n    - .20-.39 : weak\n    - .40-.59 : moderate\n    - .60-.79 : strong\n    - .80-1.0 :very strong\n- TARGET 과 DAYS_BIRTH 의 상관관계가 가장 높은 것으로 확인됨.","metadata":{}},{"cell_type":"markdown","source":"### 결측치(Missing Value) 확인\n - 함수 : missing_values_table(df)\n - df 에 읽은 파일을 변수로 입력 : app_train = pd.read_csv('../input/application_train.csv')\n - 누락 데이터를 처리하는 방법 : https://dining-developer.tistory.com/19 참고.","metadata":{}},{"cell_type":"code","source":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","metadata":{"execution":{"iopub.status.busy":"2021-11-05T01:13:52.425598Z","iopub.execute_input":"2021-11-05T01:13:52.42592Z","iopub.status.idle":"2021-11-05T01:13:52.433853Z","shell.execute_reply.started":"2021-11-05T01:13:52.425875Z","shell.execute_reply":"2021-11-05T01:13:52.432937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_values_table(app_train)\nmissing_values.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T01:13:53.894397Z","iopub.execute_input":"2021-11-05T01:13:53.894818Z","iopub.status.idle":"2021-11-05T01:13:54.175129Z","shell.execute_reply.started":"2021-11-05T01:13:53.894784Z","shell.execute_reply":"2021-11-05T01:13:54.174493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 머신러닝에서는 이러한 결측값들을 채워 넣어야 함.\n- 이 후 과정에서 결측치의 비율이 높은 칼럼의 경우 이 열을 사용할 것인지 사용하지 않을 것인지 정할 예정임.","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:31:36.228573Z","iopub.execute_input":"2021-10-21T08:31:36.229012Z","iopub.status.idle":"2021-10-21T08:31:36.235748Z","shell.execute_reply.started":"2021-10-21T08:31:36.22898Z","shell.execute_reply":"2021-10-21T08:31:36.234751Z"}}},{"cell_type":"markdown","source":"### 이상치 확인\n- 상관관계가 가장 Column을 먼저 분석함.\n- DAYS_BIRTH : 현재 대출과 비교하여 기록되기 때문에 음수이므로 나누기를 음수로 하여 더 쉽게 나이를 확인할 수 있도록 함.\n- mean : 평균\n- min : 최솟값\n- max : 최댓값\n- std : 표준편차","metadata":{}},{"cell_type":"code","source":"(app_train['DAYS_BIRTH'] / -365).describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-27T01:51:31.702363Z","iopub.execute_input":"2021-10-27T01:51:31.702627Z","iopub.status.idle":"2021-10-27T01:51:31.721547Z","shell.execute_reply.started":"2021-10-27T01:51:31.702599Z","shell.execute_reply":"2021-10-27T01:51:31.720513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- DAYS_EMPLOYED : 이상치 발견.\n- 이상치 : 최소값 NAME_INCOME_TYPE 열에서 Pensioner 로 연금수급자임을 확인함.\n- 이러한 값들의 처리에 대해 현재 고민중이며 추후 데이터를 처리하여 수정, 보완 예정임.","metadata":{}},{"cell_type":"code","source":"(app_train['DAYS_EMPLOYED'] / -365).describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-27T01:51:36.783574Z","iopub.execute_input":"2021-10-27T01:51:36.783828Z","iopub.status.idle":"2021-10-27T01:51:36.804085Z","shell.execute_reply.started":"2021-10-27T01:51:36.783805Z","shell.execute_reply":"2021-10-27T01:51:36.803286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 커널 밀도 추정\n- 연령이 대상에 미치는 영향을 시각화(seaborn kdeplot 사용).","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:45:18.517989Z","iopub.execute_input":"2021-10-21T08:45:18.519183Z","iopub.status.idle":"2021-10-21T08:45:18.523262Z","shell.execute_reply.started":"2021-10-21T08:45:18.519125Z","shell.execute_reply":"2021-10-21T08:45:18.522584Z"}}},{"cell_type":"code","source":"plt.figure(figsize = (10, 8))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n\n# Labeling of plot\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:57:08.515859Z","iopub.execute_input":"2021-10-21T08:57:08.516513Z","iopub.status.idle":"2021-10-21T08:57:10.429283Z","shell.execute_reply.started":"2021-10-21T08:57:08.51648Z","shell.execute_reply":"2021-10-21T08:57:10.42828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 나이를 5년 단위로 분리하여 데이터 분석\n- 나이 변수를 절대값으로 변환하여 확인이 쉽도록 변경함.","metadata":{}},{"cell_type":"code","source":"# Find the correlation of the positive days since birth and target\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:57:10.430572Z","iopub.execute_input":"2021-10-21T08:57:10.430819Z","iopub.status.idle":"2021-10-21T08:57:10.436723Z","shell.execute_reply.started":"2021-10-21T08:57:10.43079Z","shell.execute_reply":"2021-10-21T08:57:10.435999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Age information into a separate dataframe\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:57:10.437704Z","iopub.execute_input":"2021-10-21T08:57:10.438328Z","iopub.status.idle":"2021-10-21T08:57:10.561072Z","shell.execute_reply.started":"2021-10-21T08:57:10.438253Z","shell.execute_reply":"2021-10-21T08:57:10.560266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group by the bin and calculate averages\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:57:10.562259Z","iopub.execute_input":"2021-10-21T08:57:10.56274Z","iopub.status.idle":"2021-10-21T08:57:10.585508Z","shell.execute_reply.started":"2021-10-21T08:57:10.562699Z","shell.execute_reply":"2021-10-21T08:57:10.584249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 나이가 어릴수록 default 비율이 더 높은 것을 알 수 있음.","metadata":{}},{"cell_type":"markdown","source":"### EXT_SOURCE \n- Target과 가장 음의 상관계수를 가지는 변수들.\n- 값이 클수록 대출 상환 비율이 증가함.","metadata":{}},{"cell_type":"code","source":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:57:10.58864Z","iopub.execute_input":"2021-10-21T08:57:10.589236Z","iopub.status.idle":"2021-10-21T08:57:10.644439Z","shell.execute_reply.started":"2021-10-21T08:57:10.589196Z","shell.execute_reply":"2021-10-21T08:57:10.643317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10, 12))\n\n# iterate through the sources\nfor i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    \n    # create a new subplot for each source\n    plt.subplot(3, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:57:10.645675Z","iopub.execute_input":"2021-10-21T08:57:10.645909Z","iopub.status.idle":"2021-10-21T08:57:15.167088Z","shell.execute_reply.started":"2021-10-21T08:57:10.64588Z","shell.execute_reply":"2021-10-21T08:57:15.166421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 전체적으로 Target과 변수들의 상관관계가 높은 편은 아니지만 이러한 변수들이 결과를 예측하는데에는 도움이 될 수 있음.","metadata":{}},{"cell_type":"markdown","source":"### Polynomial Features\n- interaction terms : 변수를 결합하여 대상과의 관계를 확인.","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Make a new dataframe for polynomial features\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\n# imputer for handling missing values\nSimpleImputer = SimpleImputer(strategy = 'median')\n\npoly_target = poly_features['TARGET']\n\npoly_features = poly_features.drop(columns = ['TARGET'])\n\n# Need to impute missing values\npoly_features = SimpleImputer.fit_transform(poly_features)\npoly_features_test =SimpleImputer.transform(poly_features_test)\n                                  \n# Create the polynomial object with specified degree\npoly_transformer = PolynomialFeatures(degree = 3)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:57:42.168847Z","iopub.execute_input":"2021-10-21T08:57:42.169525Z","iopub.status.idle":"2021-10-21T08:57:42.425373Z","shell.execute_reply.started":"2021-10-21T08:57:42.169471Z","shell.execute_reply":"2021-10-21T08:57:42.424625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:57:15.194256Z","iopub.status.idle":"2021-10-21T08:57:15.194995Z","shell.execute_reply.started":"2021-10-21T08:57:15.194776Z","shell.execute_reply":"2021-10-21T08:57:15.194802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:57:15.196189Z","iopub.status.idle":"2021-10-21T08:57:15.196565Z","shell.execute_reply.started":"2021-10-21T08:57:15.196353Z","shell.execute_reply":"2021-10-21T08:57:15.196376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe of the features \npoly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Add in the target\npoly_features['TARGET'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:57:15.198111Z","iopub.status.idle":"2021-10-21T08:57:15.199083Z","shell.execute_reply.started":"2021-10-21T08:57:15.198861Z","shell.execute_reply":"2021-10-21T08:57:15.198887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Merge polynomial features into training dataframe\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n\n# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:57:15.200661Z","iopub.status.idle":"2021-10-21T08:57:15.201465Z","shell.execute_reply.started":"2021-10-21T08:57:15.201177Z","shell.execute_reply":"2021-10-21T08:57:15.201202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 새롭게 생성한 변수 중 일부는 원래 변수보다 목표와의 상관관계가 더 크다는 점을 알 수 있음.\n- 변수들 간의 관계를 고려하여 더 상관관계가 높은 변수들의 조합을 찾아볼 예정임.","metadata":{}},{"cell_type":"markdown","source":"### Baseline\n- Logistic Regression\n- 범주형 변수 인코딩.\n- 결측값을 채우고 데이터 전처리(strategy = 'median').","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns = ['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# Median imputation of missing values\nSimpleImputer = SimpleImputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nSimpleImputer.fit(train)\n\n# Transform both training and testing data\ntrain = SimpleImputer.transform(train)\ntest = SimpleImputer.transform(app_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:57:15.202918Z","iopub.status.idle":"2021-10-21T08:57:15.203255Z","shell.execute_reply.started":"2021-10-21T08:57:15.203082Z","shell.execute_reply":"2021-10-21T08:57:15.203103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:57:15.205259Z","iopub.status.idle":"2021-10-21T08:57:15.205717Z","shell.execute_reply.started":"2021-10-21T08:57:15.20551Z","shell.execute_reply":"2021-10-21T08:57:15.205538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions\n# Make sure to select the second column only\nlog_reg_pred = log_reg.predict_proba(test)[:, 1]","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:57:15.207622Z","iopub.status.idle":"2021-10-21T08:57:15.208039Z","shell.execute_reply.started":"2021-10-21T08:57:15.20784Z","shell.execute_reply":"2021-10-21T08:57:15.207868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:57:15.209546Z","iopub.status.idle":"2021-10-21T08:57:15.209966Z","shell.execute_reply.started":"2021-10-21T08:57:15.209749Z","shell.execute_reply":"2021-10-21T08:57:15.209786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:57:15.211723Z","iopub.status.idle":"2021-10-21T08:57:15.212119Z","shell.execute_reply.started":"2021-10-21T08:57:15.21193Z","shell.execute_reply":"2021-10-21T08:57:15.211956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the submission to a csv file\nsubmit.to_csv('log_reg_baseline.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:57:15.258522Z","iopub.execute_input":"2021-10-21T08:57:15.258792Z","iopub.status.idle":"2021-10-21T08:57:15.275805Z","shell.execute_reply.started":"2021-10-21T08:57:15.258762Z","shell.execute_reply":"2021-10-21T08:57:15.274882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:58:16.10017Z","iopub.execute_input":"2021-10-21T08:58:16.100898Z","iopub.status.idle":"2021-10-21T08:58:16.167519Z","shell.execute_reply.started":"2021-10-21T08:58:16.100826Z","shell.execute_reply":"2021-10-21T08:58:16.166583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train on the training data\nrandom_forest.fit(train, train_labels)\n\n# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = random_forest.predict_proba(test)[:, 1]","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:58:21.343906Z","iopub.execute_input":"2021-10-21T08:58:21.344216Z","iopub.status.idle":"2021-10-21T08:59:41.080786Z","shell.execute_reply.started":"2021-10-21T08:58:21.344186Z","shell.execute_reply":"2021-10-21T08:59:41.078269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:59:41.082113Z","iopub.status.idle":"2021-10-21T08:59:41.082494Z","shell.execute_reply.started":"2021-10-21T08:59:41.082293Z","shell.execute_reply":"2021-10-21T08:59:41.082315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\npoly_features_names = list(app_train_poly.columns)\n\n# Impute the polynomial features\nSimpleImputer = SimpleImputer(strategy = 'median')\n\npoly_features = SimpleImputer.fit_transform(app_train_poly)\npoly_features_test = SimpleImputer.transform(app_test_poly)\n\n# Scale the polynomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\npoly_features = scaler.fit_transform(poly_features)\npoly_features_test = scaler.transform(poly_features_test)\n\nrandom_forest_poly = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:59:41.083589Z","iopub.status.idle":"2021-10-21T08:59:41.083905Z","shell.execute_reply.started":"2021-10-21T08:59:41.083739Z","shell.execute_reply":"2021-10-21T08:59:41.083762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train on the training data\nrandom_forest_poly.fit(poly_features, train_labels)\n\n# Make predictions on the test data\npredictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:59:41.085018Z","iopub.status.idle":"2021-10-21T08:59:41.085344Z","shell.execute_reply.started":"2021-10-21T08:59:41.085168Z","shell.execute_reply":"2021-10-21T08:59:41.085191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_engineered.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:59:41.087182Z","iopub.status.idle":"2021-10-21T08:59:41.087548Z","shell.execute_reply.started":"2021-10-21T08:59:41.087349Z","shell.execute_reply":"2021-10-21T08:59:41.08737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Random Forest 의 기능이 성능 향상에 크게 도움이 되지 않음을 확인함.\n- Gradient Boosting Model 에는 적용가능함.","metadata":{}},{"cell_type":"markdown","source":"### Feature Importances\n- 어떤 변수가 관련성이 높은지 확인.","metadata":{}},{"cell_type":"code","source":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:59:45.505373Z","iopub.execute_input":"2021-10-21T08:59:45.505707Z","iopub.status.idle":"2021-10-21T08:59:45.514382Z","shell.execute_reply.started":"2021-10-21T08:59:45.505674Z","shell.execute_reply":"2021-10-21T08:59:45.513246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show the feature importances for the default features\nfeature_importances_sorted = plot_feature_importances(feature_importances)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T08:59:49.476743Z","iopub.execute_input":"2021-10-21T08:59:49.47728Z","iopub.status.idle":"2021-10-21T08:59:49.70032Z","shell.execute_reply.started":"2021-10-21T08:59:49.477241Z","shell.execute_reply":"2021-10-21T08:59:49.699448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Most Important Features\n- EXT_SOURCE\n- DAYS_BIRTH","metadata":{}},{"cell_type":"markdown","source":"### Light Gradient Boosting Machine","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport gc\n\ndef model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n\n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n\n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n\n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:00:53.796531Z","iopub.execute_input":"2021-10-21T09:00:53.797592Z","iopub.status.idle":"2021-10-21T09:00:53.819027Z","shell.execute_reply.started":"2021-10-21T09:00:53.797547Z","shell.execute_reply":"2021-10-21T09:00:53.817953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission, fi, metrics = model(app_train, app_test)\nprint('Baseline metrics')\nprint(metrics)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:00:58.23975Z","iopub.execute_input":"2021-10-21T09:00:58.240084Z","iopub.status.idle":"2021-10-21T09:03:26.846612Z","shell.execute_reply.started":"2021-10-21T09:00:58.240049Z","shell.execute_reply":"2021-10-21T09:03:26.845443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fi_sorted = plot_feature_importances(fi)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:03:26.847917Z","iopub.execute_input":"2021-10-21T09:03:26.848134Z","iopub.status.idle":"2021-10-21T09:03:27.089606Z","shell.execute_reply.started":"2021-10-21T09:03:26.848106Z","shell.execute_reply":"2021-10-21T09:03:27.088614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('baseline_lgb.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:03:27.091128Z","iopub.execute_input":"2021-10-21T09:03:27.091525Z","iopub.status.idle":"2021-10-21T09:03:27.215007Z","shell.execute_reply.started":"2021-10-21T09:03:27.091483Z","shell.execute_reply":"2021-10-21T09:03:27.213934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 추가사항\n- https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features?scriptVersionId=6025993 를 참고하여 아래와 같은 변수들이 결과값들과 유의미한 상관관계가 있음을 확인함.\n    - CREDIT_INCOME_PERCENT: the percentage of the credit amount relative to a client's income\n    - ANNUITY_INCOME_PERCENT: the percentage of the loan annuity relative to a client's income\n    - CREDIT_TERM: the length of the payment in months (since the annuity is the monthly amount due\n    - DAYS_EMPLOYED_PERCENT: the percentage of the days employed relative to the client's age","metadata":{}},{"cell_type":"code","source":"app_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:03:27.216723Z","iopub.execute_input":"2021-10-21T09:03:27.216991Z","iopub.status.idle":"2021-10-21T09:03:27.359878Z","shell.execute_reply.started":"2021-10-21T09:03:27.216961Z","shell.execute_reply":"2021-10-21T09:03:27.358954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:03:27.361308Z","iopub.execute_input":"2021-10-21T09:03:27.361652Z","iopub.status.idle":"2021-10-21T09:03:27.374806Z","shell.execute_reply.started":"2021-10-21T09:03:27.361611Z","shell.execute_reply":"2021-10-21T09:03:27.373593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train_domain['TARGET'] = train_labels\n\n# Test the domain knolwedge features\nsubmission_domain, fi_domain, metrics_domain = model(app_train_domain, app_test_domain)\nprint('Baseline with domain knowledge features metrics')\nprint(metrics_domain)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:03:27.376129Z","iopub.execute_input":"2021-10-21T09:03:27.376458Z","iopub.status.idle":"2021-10-21T09:06:15.494321Z","shell.execute_reply.started":"2021-10-21T09:03:27.376377Z","shell.execute_reply":"2021-10-21T09:06:15.493483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fi_sorted = plot_feature_importances(fi_domain)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:06:15.495528Z","iopub.execute_input":"2021-10-21T09:06:15.49576Z","iopub.status.idle":"2021-10-21T09:06:15.720155Z","shell.execute_reply.started":"2021-10-21T09:06:15.495732Z","shell.execute_reply":"2021-10-21T09:06:15.719352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 진행예정 사항\n- 데이터 전처리 : 제공된 데이터의 항목 뿐만 아니라 이를 결합하여 새로운 변수를 만들고 이를 토대로 분석 진행.\n- 사용자 편의성 : 시각화 및 분석 모델을 함수화 하여 사용자가 더 쉽게 프로그램을 사용할 수 있도록 프로그래밍 진행.\n- 정확도 : 더 높은 정확도를 얻기 위해 여러 자료들 참고하여 수정 보완.\n- https://www.kaggle.com/c/kaggle-survey-2021/discussion/279327#1553191 에 나온 팁을 참고하여 계속적으로 새로운 모델 시도.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}