{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"Description as per kaggle https://www.kaggle.com/c/home-credit-default-risk/overview :\nMany people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.\n\nHome Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n\nWhile Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful."},{"metadata":{},"cell_type":"markdown","source":"Also the page https://www.kaggle.com/c/home-credit-default-risk/data defines various data-set for this competation.\n\nThis is a standard supervised classification task:\nSupervised: The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features\nClassification: The label is a binary variable, 0 (will repay loan on time), 1 (will have difficulty repaying loan).\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n\nWill start with just the application train and test data-set."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# numpy and pandas for data manipulation\n# already imported as a default by the Kaggel Kernels.\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the training data\ntrain_data = pd.read_csv('/kaggle/input/home-credit-default-risk/application_train.csv')\nprint('Training data shape: ', train_data.shape)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation : The training data has 307511 observations and 122 features including the TARGET (the label we want to predict)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if there are any duplicate application.\ntrain_data[train_data.duplicated(['SK_ID_CURR'])]\n#train_data[train_data.duplicated(['SK_ID_CURR'])].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if there are any duplicate rows.\ntrain_data[train_data.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation : awesome.. No duplicate rows.. and no duplicate application id 'SK_ID_CURR'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the test data\ntest_data = pd.read_csv('/kaggle/input/home-credit-default-risk/application_test.csv')\nprint('Test data shape: ', test_data.shape)\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if there are any duplicate rows.\ntest_data[test_data.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature and Target Distribution"},{"metadata":{},"cell_type":"markdown","source":"I read the article from one of the Kaggle winner, he mentioned that we have to check the distribution of TARGET column. \nReason : To check the class balance in our dataset.\nWe can first examine the number of loans falling into each category."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data['TARGET'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['TARGET'].astype(int).plot.hist();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation : This seems to be a class imbalance problem.\nRefer to http://www.chioka.in/class-imbalance-problem/ to get more details.\nThere are far more loans that were repaid on time than loans that were not repaid."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of each type of column\ntrain_data.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get list of categorical variables\ns = (train_data.dtypes == 'object')\ntrain_data_cat_var = list(s[s].index)\ntrain_data_cat_var","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get list of Numerical variables\ntrain_data_num_var = list(train_data.select_dtypes(exclude=['object']).columns)\ntrain_data_num_var","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_data_num_var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_num_var[1:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of each feature\n#pd.options.display.mpl_style = 'default'\n\nimport matplotlib\nmatplotlib.style.use('ggplot')\n\nplt.figure(figsize=(20,5))\ntrain_data.boxplot(column=train_data_num_var[1:10])\n\nplt.show(block=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\ntrain_data.hist(column=train_data_num_var[1:5])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking for Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets get the % of each null values.\ntotal = train_data.isnull().sum().sort_values(ascending=False)\npercent_1 = train_data.isnull().sum()/train_data.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'], sort=False)\nmissing_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total columns that have missing values :\", str(len(missing_data[(missing_data['%']>0) ]) ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total columns that have missing values (More than 50%):\", str(len(missing_data[(missing_data['%']>50) ]) ))\nprint(\"Total columns that have missing values (Less than 50%):\", str(len(missing_data[(missing_data['%']<50) & (missing_data['%']>0)] ) ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation : Seems we have 64 columns with NULL value. Now we are left with 3 options as below.\n\n* Option 1: fill-in these missing values, as all the models work with non-NaN values.\n* Option 2: drop columns having more than 50% of missing values. ie we will drop 41 columns.\n* Option 3: Use XGBoost model.. as this is the only model as of now i am aware that can work with NaN or missing values."},{"metadata":{},"cell_type":"markdown","source":"# Correlation of features with target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[train_data_num_var[1:5]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Pearson Correlation\n\nplt.figure(figsize=(20,10))\n\ncor = train_data[train_data_num_var[1:20]].corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation with output variable\ncor_target = abs(cor[\"TARGET\"])\n\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.05]\nrelevant_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems none of the numeric feature have much correlation with our target variable.\n\nCorrelation coefficients whose magnitude are between 0.5 and 0.7 indicate variables which can be considered moderately correlated. Correlation coefficients whose magnitude are between 0.3 and 0.5 indicate variables which have a low correlation."},{"metadata":{},"cell_type":"markdown","source":"# Uniqueness of data in each column"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of unique values in each categorical column\ntrain_data.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe(include=[np.object])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_dict = dict()\nfor column in train_data:\n    if train_data[column].dtype == 'object':\n        if len(list(train_data[column].unique())) <= 2:\n            cat_dict[column] = len(list(train_data[column].unique()))\n#             print(cat_dict)\n        else :\n            cat_dict[column] = len(list(train_data[column].unique()))\n\nprint(cat_dict)\n            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above result of we have 16 Categorical features. Out of which.\n* NAME_CONTRACT_TYPE, FLAG_OWN_CAR, FLAG_OWN_REALTY  : binary Categorical values.\n* Remaining 13 are having more multi-value categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# write a function to get the distinct value in each categorical value\ndef get_Unique_Values(list_cat_var) :\n    cat_dict = dict()\n    for i in list_cat_var:\n        cat_dict[i] = list(train_data[i].unique())\n    return cat_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(get_Unique_Values(['NAME_CONTRACT_TYPE', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(get_Unique_Values(['CODE_GENDER', 'EMERGENCYSTATE_MODE', 'HOUSETYPE_MODE', 'NAME_EDUCATION_TYPE', 'FONDKAPREMONT_MODE']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding techniques\nFor encoding we do have following approach.. either we can go with Label Encoding or One-Hot Encoding or hard-code the values manually.\nBut seemd there are some NaN in EMERGENCYSTATE_MODE; HOUSETYPE_MODE; FONDKAPREMONT_MODE.. will leave them as is, and encode them as well for now."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Binary encoding\ntrain_data['NAME_CONTRACT_TYPE'] = [0 if x == 'Cash loans' else 1 for x in train_data['NAME_CONTRACT_TYPE']]\ntrain_data['FLAG_OWN_CAR'] = [0 if x == 'N' else 1 for x in train_data['FLAG_OWN_CAR']]\ntrain_data['FLAG_OWN_REALTY'] = [0 if x == 'N' else 1 for x in train_data['FLAG_OWN_REALTY']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}