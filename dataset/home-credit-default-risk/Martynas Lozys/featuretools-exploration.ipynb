{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# featuretools for automated feature engineering\n# !pip install featuretools --upgrade\nimport featuretools as ft\n\n# matplotlit and seaborn for visualizations\nimport matplotlib.pyplot as plt\nimport pylab\npylab.rcParams['figure.figsize'] = (15, 8)\npylab.rcParams['font.size'] = 10\nimport seaborn as sns\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This notebook explores [featuretools](https://featuretools.alteryx.com/en/stable/#) library for automated feature engineering.  \nI will use Kaggle's [home credit default risk](https://www.kaggle.com/c/home-credit-default-risk/) dataset, which has just the right structure for this task.  \nThanks to [Will Koehrsen](https://www.kaggle.com/willkoehrsen) for awesome walkthroughs and explanations of this package."},{"metadata":{},"cell_type":"markdown","source":"# Data overview\n\nThis dataset has application, previous loan payment and credit data. Since the aim is to understand featuretools, I will not spend too much time digging deep into the data and just pick `application`, `bureau` and `bureau_balance` files for this."},{"metadata":{"_uuid":"677e7f162883b005cae0c75ea10b54cc024dd007"},"cell_type":"markdown","source":"![image](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)"},{"metadata":{},"cell_type":"markdown","source":"## Application data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Utility to quickly inspect data\ndef inspect_df(df, target_col=None):\n    print(f'df shape: {df.shape}')\n    print('_____________________')\n    print(f'datatypes: {df.dtypes.value_counts()}')\n    print('_____________________')\n    print(f'Num null vals: {df.isnull().sum().sum()}')\n    print('_____________________')\n    if target_col is not None:\n        print(f'{target_col} classes: \\n{df[target_col].value_counts()}')\n    print('_____________________')\n    return df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Application data is of considerable size both in terms of rows and columns, has mixed data types, some missing values and inbalanced target classes.  \nSince automated feature generation is quite resource intensive process, we need to downsample, preprocess and reduce dimensionality before creating more features."},{"metadata":{"trusted":true},"cell_type":"code","source":"app_df = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\ninspect_df(app_df, target_col='TARGET')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Balance target classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Downsample majority class to have same number of rows as minority class\ndef balance_target(df, target_col, positive_class=1, random_state=42):\n    positive_idx = df[df[target_col] == positive_class].index\n    negative_idx = (df.loc[~df.index.isin(positive_idx)]\n                    .sample(len(positive_idx), replace=False, random_state=random_state)).index\n    return df.loc[positive_idx.union(negative_idx)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reduction from 300k rows to 50k should speed up exploration and since classes are balanced, shouldn't affect the accuracy too much.\napp_df_sample = balance_target(app_df, target_col='TARGET')\napp_df_sample.TARGET.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitter into features and target\ndef split_x_y(df, target_col):\n    return df.drop(target_col, axis=1), df[target_col]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a data preparator that:\n* separates id columns\n* splits features into categorical and numerical\n* fills in missing values\n* factorizes categories (numerical encoding)\n* reduces float precision of numericals for faster processing\n* adds random features as benchmark for feature selection later"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataPreparator:\n    \n    def __init__(self, id_cols, add_rand_cols=False):\n        self.id_cols = id_cols\n        self.add_rand_cols = add_rand_cols\n        np.random.seed(42)\n    \n    def prepare_data(self,\n                     X,\n                     cat_fill_val='none', \n                     cat_trans_func=lambda x: pd.factorize(x)[0],\n                     cat_rand_func=lambda x: np.random.choice([0, 1], x.shape[0]),\n                     num_fill_val=0,\n                     num_trans_func=lambda x: x.astype('float32'),\n                     num_rand_func=lambda x: np.random.rand(x.shape[0])):\n        ids, X = X[self.id_cols], X.drop(self.id_cols, axis=1)\n        X_cat = self._preprocess(X, 'object', cat_fill_val, cat_trans_func, cat_rand_func)\n        X_num = self._preprocess(X, 'number', num_fill_val, num_trans_func, num_rand_func)\n        return pd.concat([ids, X_cat, X_num], axis=1)\n        \n    def _preprocess(self, X, dtypes, fill_val, trans_func, rand_func):\n        X_proc = (X.select_dtypes(include=dtypes)\n                    .fillna(fill_val)\n                    .apply(trans_func))\n        if X_proc.shape[0] > 0:\n            return X_proc.assign(**{f'rand_{dtypes}': rand_func}) if self.add_rand_cols else X_proc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_df_feat, y = split_x_y(app_df_sample, target_col='TARGET')\napp_df_proc = DataPreparator(id_cols=['SK_ID_CURR'], add_rand_cols=True).prepare_data(app_df_feat)\ninspect_df(app_df_proc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inspection shows that no rows are lost, all missing values are filled and all columns are numeric"},{"metadata":{},"cell_type":"markdown","source":"## Reduce number of features\n\nThe idea here is to fit simple model with in-built feature importances (Random Forest) and drop all columns that have lower significance than random columns added in preparation step."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclass FeatureSelector:\n    \n    def __init__(self, X, y, id_cols, rand_cols):\n        self.X = X\n        self.y = y\n        self.id_cols = id_cols\n        self.rand_cols = rand_cols\n\n    def select_important_features(self):\n        rf = RandomForestClassifier(n_estimators=100, oob_score=True, n_jobs=-1, random_state=42)\n        rf.fit(self.X, self.y)\n        print(f'Model score with full feature set {rf.oob_score_}')\n        important_cols = self.get_important_cols(rf, self.X.columns)\n        rf.fit(self.X[important_cols], self.y)\n        print(f'Model score with reduced feature set {rf.oob_score_}')\n        return self.X[self.id_cols + important_cols]\n\n    def get_important_cols(self, model, column_names):\n        importances = pd.Series(model.feature_importances_, index=column_names)\n        rand_importance = np.max(importances.loc[importances.index.isin(self.rand_cols)])\n        important_cols = importances[importances > rand_importance].index.tolist()\n        print(f'Number of features with greater than random column importance {len(important_cols)}')\n        importances.sort_values().plot(title='feature importance')\n        return important_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_df_reduced = FeatureSelector(app_df_proc, y, id_cols=['SK_ID_CURR'], rand_cols=['rand_object', 'rand_number']).select_important_features()\ninspect_df(app_df_reduced)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Neat! We reduced number of features from 122 down to 9 (not counting ID), while model score changed only slightly."},{"metadata":{},"cell_type":"markdown","source":"# Bureau data\n\nWill need to perform downsampling and preprocessing for other dataframes involved as well. The key is to select only rows that link main dataframe on SK_ID_CURR column."},{"metadata":{"trusted":true},"cell_type":"code","source":"def sample_from_parent_df(parent_df, id_col, child_df):\n    sample_ids = parent_df.set_index(id_col).index\n    child_df = (child_df.set_index(id_col)\n                .apply(lambda x: x.loc[x.index.isin(sample_ids)])\n                .reset_index())\n    print(f'Num ids in parent df: {len(sample_ids)}, '\n          f'num ids in child df: {child_df[id_col].nunique()}')\n    return child_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_df = sample_from_parent_df(parent_df=app_df_reduced, id_col='SK_ID_CURR', \n                                  child_df=pd.read_csv('../input/home-credit-default-risk/bureau.csv'))\ninspect_df(bureau_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since this table has higher granularity, we cannot perform feature reduction in the same way like we did with application data and it has just 17 columns anyway, also there's no need to add random features when processing."},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_df_proc = DataPreparator(id_cols=['SK_ID_CURR', 'SK_ID_BUREAU']).prepare_data(bureau_df)\ninspect_df(bureau_df_proc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bureau balance data"},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_bal_df = sample_from_parent_df(parent_df=bureau_df_proc, id_col='SK_ID_BUREAU', \n                                      child_df=pd.read_csv('../input/home-credit-default-risk/bureau_balance.csv'))\ninspect_df(bureau_bal_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_bal_df_proc = DataPreparator(id_cols=['SK_ID_BUREAU']).prepare_data(bureau_bal_df)\ninspect_df(bureau_bal_df_proc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering\n\nOk, now that data is ready, let's see what featuretools have to offer."},{"metadata":{},"cell_type":"markdown","source":"## Create entity set\n\nFirst we need to create entity set - list all dataframes and their ids. We'll create an index for bureau balance data because it doesn't have one and it is required by featuretools.\nThere's an inbuilt plotting function to check entity set before proceeding."},{"metadata":{"trusted":true},"cell_type":"code","source":"es = ft.EntitySet(id='credit_data')\nes = es.entity_from_dataframe(entity_id='applications',\n                              dataframe=app_df_reduced,\n                              index='SK_ID_CURR')\nes = es.entity_from_dataframe(entity_id='bureau',\n                              dataframe=bureau_df_proc,\n                              index='SK_ID_BUREAU')\nes = es.entity_from_dataframe(entity_id='bureau_balance',\n                              dataframe=bureau_bal_df_proc,\n                              index='SK_ID_BUREAU_BAL',\n                              make_index=True)\nes.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create relationships\n\nOnce we have our entity set, we need to establish relationships between entities (tables/dataframes).  As it was shown (see diagram in the beginning of the notebook) application data is the main table, which links to bureau data via `SK_ID_CURR` column. Each `SK_ID_CURR` can have multiple records in bureau table, which has `SK_ID_BUREAU` unique identifier that subsequently links to records in bureau_balance table."},{"metadata":{"trusted":true},"cell_type":"code","source":"rel_app_bureau = ft.Relationship(parent_variable=es['applications']['SK_ID_CURR'], \n                                 child_variable=es['bureau']['SK_ID_CURR'])\nrel_bureau_bal = ft.Relationship(parent_variable=es['bureau']['SK_ID_BUREAU'], \n                                 child_variable=es['bureau_balance']['SK_ID_BUREAU'])\nes = es.add_relationships([rel_app_bureau, rel_bureau_bal])\nes.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create features\n\nAutomating feature creation is as simple as calling a on-liner with established entity set and pointing to a dataframe, where features should be added. Depending on data size, entity set complexity, chosen primitives, transforms and depth (see more on https://featuretools.alteryx.com/en/stable/getting_started/afe.html) this might take a while to run."},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_mat, feat_def = ft.dfs(entityset=es, target_entity='applications', n_jobs=-1, max_depth=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inspect_df(feat_mat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Featuretools created more than 200 features out of our entity set with default primitive and transform configurations, of course not all of them make sense or add signal, so we have to perform selection again."},{"metadata":{},"cell_type":"markdown","source":"# Modeling\n\nJust to see if this tool added some benefit, we'll run already introduced feature selection and see if we improved the score of benchmark model."},{"metadata":{},"cell_type":"markdown","source":" ## Benchmark model"},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_mat.head().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_mat_proc = DataPreparator(id_cols=['SK_ID_CURR'], add_rand_cols=True).prepare_data(feat_mat.reset_index())\ninspect_df(feat_mat_proc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_mat_imp = FeatureSelector(feat_mat_proc, y, id_cols=['SK_ID_CURR'], rand_cols=['rand_object', 'rand_number']).select_important_features()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nAdded features didn't seem to improve the score pre and post selection by importance. This of course does not mean that the tool is useless, because most of the time feature engineering is just adding the same basic primitives and transforms - counts, sums, means, etc. Remember, we have not considered all tables available in the data, perhaps they contain more signal. Also, featuretools were run with default presets, which could be tinkered with, so definitely looks like something to add in the toolset, to inrease productivity, especially when building PoCs."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}