{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Load and inspect data\n\nThe purpose of this notebook is not to get to the leaderboard, but rather explore various categorical preprocessing techniques, while choosing some heuristics for other modelling decisions.  \n\nI will examine the following categorical encoding techniques:\n* one hot\n* ordinal\n* target"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/home-credit-default-risk/application_train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pylab\nimport matplotlib.pyplot as plt\n\ndef plot_missing_values(df):\n    \"\"\" For each column with missing values plot proportion that is missing.\"\"\"\n    data = [(col, df[col].isnull().sum() / len(df)) \n            for col in df.columns if df[col].isnull().sum() > 0]\n    col_names = ['column', 'percent_missing']\n    missing_df = pd.DataFrame(data, columns=col_names).sort_values('percent_missing')\n    pylab.rcParams['figure.figsize'] = (15, 8)\n    missing_df.plot(kind='barh', x='column', y='percent_missing'); \n    plt.title('Percent of missing values in colummns');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_missing_values(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f'Total missing values: {df.isnull().sum().sum()}'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.TARGET.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, so it's not a terribly large dataset, with mixed variable types, considerable amount of missing values and imbalanced target classes.  "},{"metadata":{},"cell_type":"markdown","source":"# Sampling\n\nFirst of all, I'd like to downsample dominant target class to reduce imbalance.\nTypically, we don't need 1:1 ratio of positive to negative class, because that will most likely result in considerable information loss, however, I'll use it here to speed up development."},{"metadata":{"trusted":true},"cell_type":"code","source":"def downsample_df(df, target_col):\n    positive_class = df[df[target_col] == 1]\n    negative_class = (df[df[target_col] == 0]\n                      .sample(n=positive_class.shape[0],\n                              random_state=42))\n    return pd.concat([positive_class, negative_class], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sampled_df = downsample_df(df, 'TARGET')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sampled_df.shape)\nsampled_df.TARGET.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split train and validation sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = sampled_df.TARGET\nX = sampled_df.drop('TARGET', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.2, random_state=42)\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Establish baseline\n\nAny fruitful experimentation must begin by first setting a reasonable baseline result and then trying to beat it.  \nSo let's contruct a basic pipeline that provides us the benchmark, while also simplifying experimentation."},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing\n\nBefore we can get our first score on the validation set, we need to make sure that the data is in the right format to be ingested by the model, i.e. all columns should be numericalized and have no missing values.\n\nFor dealing with missing values let's just simply fill categorical NAs with 'none' value and numericals with median.\nMore elaborate schemes for missing value imputation are not in the scope of this notebook and can be found here: https://marloz.github.io/projects/sklearn/pipeline/missing/preprocessing/2020/03/20/sklearn-pipelines-missing-values.html  \n\nFor conversion of categoricals into numerics, I'll use OneHotEncoder first, as it has in-built functionality of handling unseen classes, when applying pipeline to out-of-sample data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_categorical_columns(X):\n    return [col for col in X.columns if X[col].dtypes == 'O']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\n\ncat_imputer = SimpleImputer(strategy='constant', fill_value='none')\nohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\ncat_processor = Pipeline([\n    ('imputer', cat_imputer),\n    ('encoder', ohe)\n])\n\nnum_imputer = SimpleImputer(strategy='median')\n\npreprocessor = ColumnTransformer([('categoricals', cat_processor, get_categorical_columns)],\n                                 remainder=num_imputer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = pd.DataFrame(preprocessor.fit_transform(X_train))\nprint(f'Number of missing values after imputatation {_.isnull().sum().sum()}')\nprint(f'All data types are numeric: {sum(_.dtypes == float) == _.shape[1]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model\n\nFor modelling let's bring out yer good 'ol RandomForestClassifier, as it's very widely used, requires little preprocessing and performs well with very little tuning, which is just perfect for such exploration."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(random_state=42, n_jobs=-1)\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', rf)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.fit(X_train, y_train)\npipeline.score(X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay, so this gives us a plain vanilla pipeline score, let's see if we can squeeze out some improvement!"},{"metadata":{},"cell_type":"markdown","source":"* # Combine rare categories\n\nOne of the simpler techniques that we can start with is combining rare classes, to avoid creating a bunch of sparse OHE columns, which doesn't help the model (for more see here https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769)"},{"metadata":{},"cell_type":"markdown","source":"Let's first check how many variables have rare categories as this may not be issue at all in this case.\nThere are only few variables that have categories with counts < 100, so probably this technique will not add much."},{"metadata":{"trusted":true},"cell_type":"code","source":"min_count_categories = [(col, X_train[col].value_counts().min()) for col in get_categorical_columns(X_train)]\npd.DataFrame(min_count_categories).set_index(0)[1].sort_values().plot(kind='bar')\nplt.yscale('log')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also it's worth doing cardinality check, as noted in the beginning of the section, categorical variables with many levels can cause issues for tree based algorithms, especially when One Hot Encoding is applied.\n    There's only a single variable, which has close to 60 categories, which is not too bad, so One Hot Encoding might actually be quite suitable in this case."},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_counts = [(col, X_train[col].nunique()) for col in get_categorical_columns(X_train)]\npd.DataFrame(unique_counts).set_index(0)[1].sort_values().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's still apply rare category combiner, just to see how it works and have it in the toolbox."},{"metadata":{"trusted":true},"cell_type":"code","source":"def combine_low_count_classes(X, threshold=100):\n    \"\"\" This is a transformer function inteded to be used on categorical columns without any missing values.\n    It loops through variables and checks for categories that have related counts lower than specified threshold.\n    Then it combines all these low count categories into single 'other' category,\n    along with first category above the threshold\"\"\"\n    X = pd.DataFrame(X)\n    for column in X.columns:\n        frequencies = X[column].value_counts()\n        if frequencies.min() < threshold:\n            lower_bound = frequencies[frequencies >= threshold].min()\n            mask = frequencies[frequencies <= lower_bound].index\n            replace_dict = dict.fromkeys(mask, 'other')\n            X[column] = X[column].replace(replace_dict)\n    return X.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import FunctionTransformer\n\ncombiner = FunctionTransformer(combine_low_count_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_processor = Pipeline([\n    ('imputer', cat_imputer),\n    ('combiner', combiner),\n    ('encoder', ohe)\n])\npipeline.set_params(**{'preprocessor__categoricals': cat_processor});","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.fit(X_train, y_train)\npipeline.score(X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, this actually made the score slightly worse. As argued above, rare categories and high cardinality is probably not the main concern for this dataset, so this technique removes more information than noise."},{"metadata":{},"cell_type":"markdown","source":"# Ordinal encoding\n\nOrdinal encoder doesn't handle unseen data, so the pipeline will fail on validation set. To accomodate for this we need to replace new categories in validation set with some value and append this replaced value to the known classes of fitted encoder (inspired by https://stackoverflow.com/questions/40321232/handling-unknown-values-for-label-encoding)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import OrdinalEncoder\n\nclass CustomLabelEncoder(BaseEstimator, TransformerMixin):\n    \"\"\" Fits sklearn's OrdinalEncoder (OE) to categorical columns without missing values.\n    Loops through columns and checks if each category is in within fitted OE categories.\n    If not, new category is assigned 'new_category' value and appended to OE categories,\n    such that OE can be applied to unseen data.\"\"\"\n    \n    def fit(self, X, y=None):\n        self.oe = OrdinalEncoder()\n        self.oe.fit(X)\n        return self\n    \n    def transform(self, X):\n        for col_idx in range(X.shape[1]):\n            X[:, col_idx] = self.replace_new_categories(X[:, col_idx], self.oe.categories_[col_idx])\n            self.oe.categories_[col_idx] = np.append(self.oe.categories_[col_idx], 'new_category')\n        return self.oe.transform(X)\n    \n    def replace_new_categories(self, col, categories):\n        return pd.Series(col).map(lambda current_category: 'new_category' \n                                  if current_category not in categories \n                                  else current_category).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_processor = Pipeline([\n    ('imputer', cat_imputer),\n    ('encoder', CustomLabelEncoder())\n])\npipeline.set_params(**{'preprocessor__categoricals': cat_processor});","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.fit(X_train, y_train)\npipeline.score(X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No improvement from the benchmark, but scores are pretty close."},{"metadata":{},"cell_type":"markdown","source":"# Target encoding\nAnother popular way of dealing with categoricals is Target Encoding, which basically includes information about the mean of the dependent variable as it relates to particular category. We have to be careful though not to leak data from validation set and also consider the number of values per category as some small groups can introduce noise (a concept akin to combining rare classes discussed before). See more here (https://medium.com/@venkatasai.katuru/target-encoding-done-the-right-way-b6391e66c19f)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TargetEncoder(BaseEstimator, TransformerMixin):\n    \"\"\" Calculates global mean on train set, then proceeds with creating encoding dictionary\n    for each column, which contains smoothed target variable mean for each category.\n    The smoothing parameter can be changed upon initiation of the class and represents\n    number of values per category, i.e. for rare categories global mean has more weight.\n    \n    Transform part uses same trick as customized Ordinal Encoder used above - \n    checking for new categories in data and assigning them global mean.\n    \"\"\"\n    \n    def __init__(self, smooth_weight=100):\n        self.smooth_weight = smooth_weight\n    \n    def fit(self, X, y):\n        self.global_mean = np.mean(y)\n        self.enc_dict = self.create_encoding_dict(X, y)\n        return self\n    \n    def create_encoding_dict(self, X, y):\n        enc_dict = {}\n        for col_idx in range(X.shape[1]):\n            enc_dict[col_idx] = self.get_smooth_means_for_col(X[:, col_idx], y)\n        return enc_dict\n    \n    def get_smooth_means_for_col(self, col, y):\n        smooth_mean_agg = (lambda x: (x['count'] * x['mean'] + self.smooth_weight * self.global_mean) \n                           / (x['count'] + self.smooth_weight))\n        col_y_concat = pd.concat([pd.Series(col, name='col'), pd.Series(y, name='target')], axis=1)\n        return (col_y_concat.groupby('col')['target'].agg(['count', 'mean'])\n               .assign(smooth_mean=smooth_mean_agg))['smooth_mean'].to_dict()\n    \n    def transform(self, X):\n        for col_idx in range(X.shape[1]):\n            X[:, col_idx] = self.replace_new_categories(X[:, col_idx], self.enc_dict[col_idx].keys())\n            X[:, col_idx] = (pd.Series(X[:, col_idx]).map(self.enc_dict[col_idx])\n                             .fillna(self.global_mean).values)\n        return X\n    \n    def replace_new_categories(self, col, categories):\n        return pd.Series(col).map(lambda current_category: 'new_category' \n                                  if current_category not in categories \n                                  else current_category).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_processor = Pipeline([\n    ('imputer', cat_imputer),\n    ('encoder', TargetEncoder())\n])\npipeline.set_params(**{'preprocessor__categoricals': cat_processor});","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.fit(X_train, y_train)\npipeline.score(X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Slight improvement from benchmark, which is always nice.  \n\nThanks for reading, comments are appreciated!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}