{"cells":[{"metadata":{"trusted":true,"_uuid":"948adf685851541faa56bc539e4d0c0caa0cabee"},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"../input\"))\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\n\n# other libraries and functions\nfrom sklearn.preprocessing import MinMaxScaler, Imputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e989b6ccbe6d6a85dcf490a062adde1c722395fe"},"cell_type":"markdown","source":"## Read in Data"},{"metadata":{"trusted":true,"_uuid":"c48029371a462361b74efa964abdce116ec56525"},"cell_type":"code","source":"df = pd.read_csv(\"../input/application_train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a372698d101136f316644251e7a061c9f7fd20b0"},"cell_type":"code","source":"train, test = train_test_split(df, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a984f7fc9598e28be33ac124ed1ee7e347b0973"},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ab830ed64a9b434ed56594eacbb45e1d408b018"},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a6d943244b4a1e6a146f8ed98f506894b56500b"},"cell_type":"markdown","source":"## Peak at Data"},{"metadata":{"trusted":true,"_uuid":"b4c7258053d7f47934a412a63851b098582247f1"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e936ac6c5be0b9652c228c91f4b8b5586d45f20e"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e81b61819226741d60a6fd3e005ce5330c1eb9e"},"cell_type":"markdown","source":"## Check Missing Values"},{"metadata":{"_uuid":"ede20885f8b7f5545fdab10baf9edb70eedb2fec"},"cell_type":"markdown","source":"First, define a function to build a nice table of missing value percentages.  This will make things easier given the large number of initial features."},{"metadata":{"trusted":true,"_uuid":"d059d821173877240947a9eafcea8e816f6903a8"},"cell_type":"code","source":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"172d98abdc5ae62666494f846c103c03aea47e44"},"cell_type":"code","source":"# Missing values statistics\nmissing_train = missing_values_table(train)\nmissing_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3540698552c57a3a0a15cdf7a1f994004dcbaaa1"},"cell_type":"markdown","source":"# Data Types"},{"metadata":{"trusted":true,"_uuid":"f3f264288d6dc62ac1450b8e268d8a94abb13048"},"cell_type":"code","source":"# Number of each type of column\ntrain.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a74b4e0162ac527b72dc58ab5fff424f4b3733c"},"cell_type":"code","source":"# Number of unique classes in each object column\ntrain.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d945338cde438117209992c8a005a01012b0548"},"cell_type":"markdown","source":"# Descriptive Statistics (To detect outliers and anomolies in continuous variables)"},{"metadata":{"_uuid":"c33b1edbfaabf1ee8369ca3e90ecf525526a8c46"},"cell_type":"markdown","source":"## Overview of all variables"},{"metadata":{"trusted":true,"_uuid":"400d1dd64184019cc82fbc7f482e8e44af7190d5"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bae13a03ac378d16f75def3abbcd37c06391c344"},"cell_type":"markdown","source":"## Age"},{"metadata":{"trusted":true,"_uuid":"23dda3539d4fda0a371cad1f0594b16f190a54d7"},"cell_type":"code","source":"(train['DAYS_BIRTH'] / -365).describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"054f28b008236f01eb095ca8ee8156af823d660b"},"cell_type":"markdown","source":"## Amount of time at current job"},{"metadata":{"trusted":true,"_uuid":"f9dd1ad9ed7d67affa51eff7e3ad94fa0a477b35"},"cell_type":"code","source":"train['DAYS_EMPLOYED'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"581ed2973e6252c31292f1e91046164de04485d6"},"cell_type":"markdown","source":"## Number of Children"},{"metadata":{"trusted":true,"_uuid":"8dabe2890b6d091f4ecb300e0222d40865422be6"},"cell_type":"code","source":"train['CNT_CHILDREN'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5dc9ec5eb12bccddacf3d5cac02fe88e4749d3a2"},"cell_type":"markdown","source":"## Income"},{"metadata":{"trusted":true,"_uuid":"7c0d4b4464b866172a7eb193a00fb6012e71d32f"},"cell_type":"code","source":"train['AMT_INCOME_TOTAL'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61cfc3cd0d43c6a6e1478996e5714ca32069fb52"},"cell_type":"markdown","source":"## Number of Days before application that client changed his/her registration"},{"metadata":{"trusted":true,"_uuid":"988a1b8d6f17a35fc42f2c71154c17208096447f"},"cell_type":"code","source":"train['DAYS_REGISTRATION'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bea994b109e82a74bd04377abeba7423bd93950"},"cell_type":"markdown","source":"## Preprocessing Notes\n- The max value for the DAYS_EMPLOYED variable appears to be an error.  We should replace this with a NAN value in our preprocessing step.\n- We should replace the DAYS_BIRTH variable with an absolute value, and divide by 365, so it is in years.\n- There are many categorical variables that we need to perform one-hot encoding on in the preprocessing step.\n- We need to perform min-max scaling before feeding the data into machine learning algorithms\n- 67 columns have missing data.  We will need to develop a strategy for dropping columns, rows, or imputing values during the preprocessing step."},{"metadata":{"_uuid":"0b9213065b8fb5a16ffaadec0782cb647592ff9a"},"cell_type":"markdown","source":"# Distributions of Important Features"},{"metadata":{"_uuid":"110a259147d8a92ec2bc3b58aefe9beb4306a15c"},"cell_type":"markdown","source":"## Target (repaid loan or not)"},{"metadata":{"trusted":true,"_uuid":"06a083c944e655de4daf3e6af07a5821a920ff9d"},"cell_type":"code","source":"# TARGET value 0 means loan is repayed, value 1 means loan is not repayed.\nplt.figure(figsize=(15,5))\nsns.countplot(train.TARGET)\nplt.xlabel('Target (0 = repaid, 1 = not repaid)'); plt.ylabel('C'); plt.title('Distribution of Loan Repayment');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e560e56f3064a007402153aa542d06d9107d6a4c"},"cell_type":"markdown","source":"## Contract Type"},{"metadata":{"trusted":true,"_uuid":"ef5b52db9c24ad3c35993cbfb1e86d3bd48e3ee5"},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(train.NAME_CONTRACT_TYPE.values,data=train)\nplt.xlabel('Contract Type'); plt.ylabel('Count'); plt.title('Distribution of Contract Types');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d11c3d41f87e4fda011a1cc80e8aaaa74b7bbde5"},"cell_type":"markdown","source":"## Gender"},{"metadata":{"trusted":true,"_uuid":"a842f5d5226bf67945f120b9837961a9b12709d6"},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(train.CODE_GENDER.values,data=train)\nplt.xlabel('Gender'); plt.ylabel('Number of Clients'); plt.title('Distribution of Gender');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5730001af93f961ded4f0dd14e7690b5dc1b8d8"},"cell_type":"markdown","source":"## Education Type/Level"},{"metadata":{"trusted":true,"_uuid":"58df020192018bac40c18ec017a33ff574e5a724"},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(train.NAME_EDUCATION_TYPE.values,data=train)\nplt.xlabel('Education Type/Level'); plt.ylabel('Number of Clients'); plt.title('Distribution of Education Type/Level');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1357209d87aec0cbdfc4e18bc38a72f3b5f37ed"},"cell_type":"markdown","source":"## Car Ownership"},{"metadata":{"trusted":true,"_uuid":"817b605952db56a0ecd441fb9c6b4059c20d60ab"},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(train.FLAG_OWN_CAR.values,data=train)\nplt.xlabel('Car Ownership (Y = Yes, N = No)'); plt.ylabel('Number of Clients'); plt.title('Distribution of Car Ownership');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb18407767143ac44923197e8295e4a06bb7d326"},"cell_type":"markdown","source":"## Home Ownership"},{"metadata":{"trusted":true,"_uuid":"2d7e248e5674555a514f4dc54198821e0e08421f"},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(train.FLAG_OWN_REALTY.values,data=train)\nplt.xlabel('Home Ownership (Y = Yes, N = No)'); plt.ylabel('Number of Clients'); plt.title('Distribution of Home Ownership');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4b25733e0b37b9298e0864899835fc607abe17f"},"cell_type":"markdown","source":"## Number of Children"},{"metadata":{"trusted":true,"_uuid":"b2068f190bb3fb8c4809646ab7ea2292d3754f96"},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(train.CNT_CHILDREN.values,data=train)\nplt.xlabel('Number of Children'); plt.ylabel('Number of Clients'); plt.title('Distribution of Children Per Client');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4cead685057d9dcb9114eef1718435c40a390ca5"},"cell_type":"markdown","source":"## Family Status"},{"metadata":{"trusted":true,"_uuid":"5b07621edc9e4c989c14c1261d37cf853b1e4a9d"},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(train.NAME_FAMILY_STATUS.values,data=train)\nplt.xlabel('Family Status'); plt.ylabel('Number of Clients'); plt.title('Family Status Distribution');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cf7f4511db668ac2466f0a50662a6f316cdf1d4"},"cell_type":"markdown","source":"## Housing Type"},{"metadata":{"trusted":true,"_uuid":"39814129f5f836ebdd7e499b8af26239e91f64d8"},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(train.NAME_HOUSING_TYPE.values,data=train)\nplt.xlabel('Housing Type'); plt.ylabel('Number of Clients'); plt.title('Housing Type Distribution');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b21e75479dfff0e00834e60c8ab3d3785d0af6ab"},"cell_type":"markdown","source":"## Age of Client"},{"metadata":{"trusted":true,"_uuid":"218bbc572dff1850d78eabaf0cafdbe924e65e19"},"cell_type":"code","source":"train['DAYS_BIRTH'] = abs(train['DAYS_BIRTH'])\n\nplt.figure(figsize=(15,5))\nsns.distplot(train['DAYS_BIRTH'] / 365,bins=5)\nplt.xlabel('Age (Years)'); plt.ylabel('Density'); plt.title('Age Distribution');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7634f279954c67c7239f8a7b31b154ff75b6af2"},"cell_type":"markdown","source":"## Notable Visualization"},{"metadata":{"trusted":true,"_uuid":"b5131dd912d0fbb51da4900669c70a6c359ad422"},"cell_type":"code","source":"# Age information into a separate dataframe\nage_data = train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b2a0363f5419813f6c861f86fb45f884fb49405"},"cell_type":"code","source":"# Group by the bin and calculate averages\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73332ef2852962362242394592556bb9cec16d34"},"cell_type":"code","source":"plt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97b8ec65bd7f6ad7e074fa17076837d73be75c21"},"cell_type":"markdown","source":"# Correlations with the Target"},{"metadata":{"trusted":true,"_uuid":"e6987a5f24f3048f4a3e226a392a6c4cf575d171"},"cell_type":"code","source":"# Find correlations with the target and sort\ncorrelations = train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ded13e82297401451c169decbf2ec24c92ff07d0"},"cell_type":"markdown","source":"# Preprocessing Notes\n- The max value for the DAYS_EMPLOYED variable appears to be an error.  We should replace this with a NAN value in our preprocessing step.\n- We should replace the DAYS_BIRTH variable with an absolute value, and divide by 365, so it is in years.\n- There are many categorical variables that we need to perform one-hot encoding on in the preprocessing step.\n- We need to perform min-max scaling before feeding the data into machine learning algorithms\n- 67 columns have missing data.  We will need to develop a strategy for dropping columns, rows, or imputing values during the preprocessing step.\n- Many features are highly-correlated with one another.  We will need to remove the unnecessary features."},{"metadata":{"trusted":true,"_uuid":"d36419142833c4dea03a8a3057ec2590aa4f568e"},"cell_type":"markdown","source":"# Benchmark Model"},{"metadata":{"trusted":true,"_uuid":"499844948736df8bab9dbf22ce40b39d4f4b1b50"},"cell_type":"markdown","source":"We will use the logistic regression model to establish our benchmark results.  First, we will copy the training and testing sets, so we can perform some basic preprocessing on the data.  Specifically, we will scale the features between 0 and 1, and fill-in all missing values with the median value of the columns, and perform one-hot encoding on categorical variables."},{"metadata":{"trusted":true,"_uuid":"5c17d8a255ccdddd2b3082a9c002742477605b83"},"cell_type":"code","source":"# Copy data into a different dataframe to preserve the original\nbench_train = train.copy()\nbench_test = test.copy()\n\n# one-hot encoding of categorical variables\nbench_train = pd.get_dummies(bench_train)\nbench_test = pd.get_dummies(bench_test)\n\n# capture the labels\nbench_train_labels = bench_train['TARGET']\nbench_test_labels = bench_test['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\nbench_train, bench_test = bench_train.align(bench_test, join = 'inner', axis = 1)\n\n# Drop the target from the training and testing data\nbench_train = bench_train.drop(columns = ['TARGET'])\nbench_test = bench_test.drop(columns = ['TARGET'])\n\n# Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(bench_train)\nimputer.fit(bench_test)\n\n# Transform both training and testing data\nbench_train = imputer.transform(bench_train)\nbench_test = imputer.transform(bench_test)\n\n# Repeat with the scaler\nscaler.fit(bench_train)\nscaler.fit(bench_test)\nbench_train = scaler.transform(bench_train)\nbench_test = scaler.transform(bench_test)\n\nprint('Training data shape: ', bench_train.shape)\nprint('Testing data shape: ', bench_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df195a719f070aca98621035a3de51dd72dd4dee"},"cell_type":"code","source":"# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\nshuffle = KFold(n_splits=5, shuffle=True)\nscores = cross_val_score(log_reg, bench_train, bench_train_labels, cv=shuffle, scoring='roc_auc')\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"315b37988ac8a8bf1af52e88e2f48308ece5a25a"},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{"_uuid":"a23d968cfe39b6348cc8c090289364e9a1c3f006"},"cell_type":"markdown","source":"## Steps\n- Replace the max value for the DAYS_EMPLOYED variable with a NAN value.\n- Replace the DAYS_BIRTH variable with an absolute value.\n- Remove any columns that are missing more than 50 percent of their values.\n- Perform one-hot encoding on categorical variables.\n- Perform min-max scaling before feeding the data into machine learning algorithms.\n- Remove colinear features.  If any columns have a correlation over 0.9, only keep one, and remove the others.\n- Use the feature_importances attribute of the lightGBM model to remove features that have very little importance.\n- Impute any additional missing rows with median values.\n"},{"metadata":{"_uuid":"b15c644597fd172345ef999670302c83e7cd8e6d"},"cell_type":"markdown","source":"## Copy Data"},{"metadata":{"trusted":true,"_uuid":"270e4d01b0b927c1ea71da6170c3087181bed2e8"},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4f64e4f39d7f7d8b380be76485fd6f8acf4cc1c"},"cell_type":"code","source":"# Copy data into a different dataframe to preserve the original\nmain_train = train.copy()\nmain_test = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df30082a10779f1d6e281fee70d38cc88e216428"},"cell_type":"markdown","source":"## Handle Outliers and Transformations"},{"metadata":{"trusted":true,"_uuid":"142761d0cc0d2c2332a3cefa5d31bfcc21411b37"},"cell_type":"code","source":"main_train['DAYS_BIRTH'] = abs(main_train['DAYS_BIRTH'])\nmain_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f0b711fb89b9848c050d85b2ea2b8afb0978b22"},"cell_type":"markdown","source":"## One-Hot Encode Categorical Features"},{"metadata":{"trusted":true,"_uuid":"14b336e9ee8b72577133c5f4cbc09eb98faeaa72"},"cell_type":"code","source":"# one-hot encoding of categorical variables\nmain_train = pd.get_dummies(main_train)\nmain_test = pd.get_dummies(main_test)\n\n# Align the training and testing data, keep only columns present in both dataframes\nmain_train, main_test = main_train.align(main_test, join = 'inner', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdceb7280b38be4f88b807486477e6bb5c84dcd3"},"cell_type":"code","source":"print(main_train.shape)\nprint(main_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0866559fe1a2c6bebbe6052dbff82d17e471ebab"},"cell_type":"markdown","source":"## Remove Collinear Features Above Threshold"},{"metadata":{"trusted":true,"_uuid":"f61391b72680f481cfa73412dbd14b882afd4488"},"cell_type":"code","source":"# Threshold for removing correlated variables\nthreshold = 0.8\n\n# Absolute value correlation matrix\ncorr_matrix = main_train.corr().abs()\ncorr_matrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27243dec7920dc17f16fc26dea03c23147e48626"},"cell_type":"code","source":"upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68eefda9e0e256ea2b3b2dc015e9ba62a6c90d4a"},"cell_type":"code","source":"# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint('There are %d columns to remove.' % (len(to_drop)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c12a28d84f229c3ae76d009398b91ca1d2acdc9"},"cell_type":"code","source":"main_train = main_train.drop(columns = to_drop)\nmain_test = main_test.drop(columns = to_drop)\n\nprint('Training shape: ', main_train.shape)\nprint('Testing shape: ', main_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"960adab2cc1391b1a1c84b6f7ea2975a631aceff"},"cell_type":"markdown","source":"Extra features that have a correlation above 0.8 have now been dropped."},{"metadata":{"_uuid":"a8e3dc6d27a5ae8c25c1e0eaca1370e17f9f385f"},"cell_type":"markdown","source":"## Remove Features with Missing Values Above Threshold"},{"metadata":{"trusted":true,"_uuid":"b6a0ebd5fa2841eb2148dc705304cdde2011a8f6"},"cell_type":"code","source":"# Train missing values (in percent)\ntrain_missing = (main_train.isnull().sum() / len(main_train)).sort_values(ascending = False)\ntrain_missing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75a3f7750d61c938bb97693020a405e1183d42af"},"cell_type":"code","source":"# Test missing values (in percent)\ntest_missing = (main_test.isnull().sum() / len(main_test)).sort_values(ascending = False)\ntest_missing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de632e664679c4faeeb6f432b07df1479fcd2e2b"},"cell_type":"code","source":"# Identify missing values above threshold\ntrain_missing = train_missing.index[train_missing > 0.50]\ntest_missing = test_missing.index[test_missing > 0.50]\n\nall_missing = list(set(set(train_missing) | set(test_missing)))\nprint('There are %d columns with more than 50%% missing values' % len(all_missing))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f768f57b32de1810ef2a97c89c76c5c0307da73"},"cell_type":"code","source":"# Need to save the labels because aligning will remove this column\nmain_train_labels = main_train[\"TARGET\"]\nmain_train_ids = main_train['SK_ID_CURR']\nmain_test_ids = main_test['SK_ID_CURR']\n\nmain_train = pd.get_dummies(main_train.drop(columns = all_missing))\nmain_test = pd.get_dummies(main_test.drop(columns = all_missing))\n\nmain_train, main_test = main_train.align(main_test, join = 'inner', axis = 1)\n\nprint('Training set full shape: ', main_train.shape)\nprint('Testing set full shape: ' , main_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdb937208ed5be2e3c02fdcf5d5e440325f83031"},"cell_type":"markdown","source":"Columns with greater than 50 percent of observations missing have been removed."},{"metadata":{"trusted":true,"_uuid":"d916d78a2bb9360b249268d9f77d186dc7af7dd8"},"cell_type":"code","source":"main_train = main_train.drop(columns = ['SK_ID_CURR'])\nmain_test = main_test.drop(columns = ['SK_ID_CURR'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e7140c4405a9a8d0b8b2197eae7fc7fa3f9f6a5"},"cell_type":"code","source":"print('Training set full shape: ', main_train.shape)\nprint('Testing set full shape: ' , main_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d762b86081f6db98014f7a967f43595a61a6ebcf"},"cell_type":"markdown","source":"ID column is now dropped.  We don't need this extra info when we feed the data to our models."},{"metadata":{"_uuid":"a0a3f634e80341235887442e4e05b1ddcb7f1a4f"},"cell_type":"markdown","source":"## Impute and Scale Features"},{"metadata":{"trusted":true,"_uuid":"6bd41e01be731de7897c31c59b1f7785ce3d0999"},"cell_type":"code","source":"# capture the labels\nmain_train_labels = main_train['TARGET']\nmain_test_labels = main_test['TARGET']\n\n# Drop the target from the training and testing data\nmain_train = main_train.drop(columns = ['TARGET'])\nmain_test = main_test.drop(columns = ['TARGET'])\n\n# impute median values\nimputer = Imputer(strategy = 'median')\nimputer.fit(main_train)\nimputer.fit(main_test)\nmain_train = imputer.transform(main_train)\nmain_test = imputer.transform(main_test)\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\nscaler.fit(main_train)\nscaler.fit(main_test)\nmain_train = scaler.transform(main_train)\nmain_test = scaler.transform(main_test)\n\nprint('Training data shape: ', main_train.shape)\nprint('Testing data shape: ', main_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc785702258f11de6c201aae6d5e7b5b7cfca73e"},"cell_type":"markdown","source":"# Models\n- Logistic Regression\n- K-Nearest Neighbors\n- Naive Bayes\n- SVM Classifier\n- Random Forest\n- Decision Trees\n- AdaBoost\n- XgBoost\n- LightGBM"},{"metadata":{"_uuid":"70684c79aa1962cf2ea7aa4953e3f7dd229f2f74"},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"160ce7b8a53bb46f9d869c221383d49c264dcad4"},"cell_type":"code","source":"log_reg = LogisticRegression(C = 0.0001)\n\nshuffle = KFold(n_splits=5, shuffle=True)\nscores = cross_val_score(log_reg, main_train, main_train_labels, cv=shuffle, scoring='roc_auc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88114022b3b7561c97ddfa167b303bbc410e3269"},"cell_type":"code","source":"print(\"All Scores:\")\nprint(scores)\nprint(\"Average Score:\")\nprint(round((sum(scores) / len(scores)), 4)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6267a868f33e184c8a6ecf18e87d14b00817a30f"},"cell_type":"markdown","source":"## K Nearest Neighbors"},{"metadata":{"trusted":true,"_uuid":"1a6119939222074ba09bb86b2a99c5d8e8eaab21"},"cell_type":"code","source":"knn = KNeighborsClassifier()\n\nshuffle = KFold(n_splits=5, shuffle=True)\nscores = cross_val_score(knn, main_train, main_train_labels, cv=shuffle, scoring='roc_auc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf84f95448e41f764922354b0d687f98366e2dd2"},"cell_type":"code","source":"print(\"All Scores:\")\nprint(scores)\nprint(\"Average Score:\")\nprint(round((sum(scores) / len(scores)), 4)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0803302762b20e47f23f9a7e88f1874746d734d1"},"cell_type":"markdown","source":"## Naive Bayes"},{"metadata":{"trusted":true,"_uuid":"433e56a8064b1d22727bdfe535ce8e4b905023c3"},"cell_type":"code","source":"nb = GaussianNB()\n\nshuffle = KFold(n_splits=5, shuffle=True)\nscores = cross_val_score(nb, main_train, main_train_labels, cv=shuffle, scoring='roc_auc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cad39d056e100a8850c51afb23da2d42591f41a"},"cell_type":"code","source":"print(\"All Scores:\")\nprint(scores)\nprint(\"Average Score:\")\nprint(round((sum(scores) / len(scores)), 4)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"589163b5f74b4647b70078fa31b7853f13c26c36"},"cell_type":"markdown","source":"## Support Vector Machine"},{"metadata":{"trusted":true,"_uuid":"2f41467fc39ca29eeae2d38dd0fd65d5aaac0878"},"cell_type":"code","source":"svm = SVC()\n\nshuffle = KFold(n_splits=5, shuffle=True)\nscores = cross_val_score(svm, main_train, main_train_labels, cv=shuffle, scoring='roc_auc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4af684bdeb382b0610c74fce285a2d08174de38f"},"cell_type":"code","source":"print(\"All Scores:\")\nprint(scores)\nprint(\"Average Score:\")\nprint(round((sum(scores) / len(scores)), 4)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f6cfb9b54672f4024828f64b151ef8e9955638b"},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{"trusted":true,"_uuid":"a4ae2bd33e0475499e40adb7ebbcbe9cdf3c88e6"},"cell_type":"code","source":"dtc = DecisionTreeClassifier(random_state=0)\n\nshuffle = KFold(n_splits=5, shuffle=True)\nscores = cross_val_score(dtc, main_train, main_train_labels, cv=shuffle, scoring='roc_auc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acc579ef4dd8f9cb6c08d29fe37a9a76faa624c3"},"cell_type":"code","source":"print(\"All Scores:\")\nprint(scores)\nprint(\"Average Score:\")\nprint(round((sum(scores) / len(scores)), 4)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"150850d95768eb4a20f9d09d40e8357940af50e5"},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true,"_uuid":"e92371a72956e498447b6368585e04b14bfe0ae9"},"cell_type":"code","source":"rfc = RandomForestClassifier()\n\nshuffle = KFold(n_splits=5, shuffle=True)\nscores = cross_val_score(rfc, main_train, main_train_labels, cv=shuffle, scoring='roc_auc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db84bad2ebf660873e96dd831dcb7b27b18bac0c"},"cell_type":"code","source":"print(\"All Scores:\")\nprint(scores)\nprint(\"Average Score:\")\nprint(round((sum(scores) / len(scores)), 4)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"897fd6bc263e9af89f9961711bf8666dba2898cf"},"cell_type":"markdown","source":"## AdaBoost Classifier"},{"metadata":{"trusted":true,"_uuid":"d71abb86fe1e5e8c8a1a4d2620009a0ed6192c02"},"cell_type":"code","source":"abc = AdaBoostClassifier(DecisionTreeClassifier())\n\nshuffle = KFold(n_splits=5, shuffle=True)\nscores = cross_val_score(abc, main_train, main_train_labels, cv=shuffle, scoring='roc_auc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27e35618692c7997aac4d611d36ea3a59610b5e5"},"cell_type":"code","source":"print(\"All Scores:\")\nprint(scores)\nprint(\"Average Score:\")\nprint(round((sum(scores) / len(scores)), 4)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f086c3f4bd20966cb1c14953b0d91a97517fae78"},"cell_type":"markdown","source":"## XGBoost Classifer"},{"metadata":{"trusted":true,"_uuid":"fcbb7ce0512d0614e8588d9b5484cc027f7bd6b0"},"cell_type":"code","source":"xgb = XGBClassifier()\n\nshuffle = KFold(n_splits=5, shuffle=True)\nscores = cross_val_score(xgb, main_train, main_train_labels, cv=shuffle, scoring='roc_auc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2ff39bdff0daaedd0702797e5df96a8165907e3"},"cell_type":"code","source":"print(\"All Scores:\")\nprint(scores)\nprint(\"Average Score:\")\nprint(round((sum(scores) / len(scores)), 4)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56fa8de2b8978cac44931c4710f6a679b5433b12"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"854d06373babb53a700164f497c0c9d571de062f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d9e427ab4e2847962e3671c452147eef15b6533"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2036611e8d5592222c6f8126c7151e51f2aa7f9f"},"cell_type":"markdown","source":"# Refinement"},{"metadata":{"trusted":true,"_uuid":"b71102368955ae5ece09578ce9798367b17ad46c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}