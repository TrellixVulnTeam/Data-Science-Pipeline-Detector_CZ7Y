{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport lightgbm as lgb\nimport gc\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b661fd6185d4392f69caf19c4dac25b3c13dd02"},"cell_type":"code","source":"# Training data\napp_train = pd.read_csv('../input/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c4f155ff2d08549a2ab4422003f4ddbc83aabab"},"cell_type":"code","source":"# Testing data features\napp_test = pd.read_csv('../input/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4ce3bf05a3d68b9b070ce21385b2fda31f21a95"},"cell_type":"code","source":"app_train['TARGET'].value_counts()\nprint('The proportion of label 1 is %.2f' % (sum(app_train['TARGET']==1)/app_train.shape[0]*100), '%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7be83dabec69b5c7d1ac616347d411384f11c3c","collapsed":true},"cell_type":"code","source":"# 该函数确定数据集每一列的缺失值个数及占比\ndef missing_values_table(df):\n    #Total missing values\n    mis_val = df.isnull().sum()\n    \n    #Percentages of missing values\n    mis_val_percent = df.isnull().sum() * 100 / df.shape[0]\n    \n    # Make a table with the results\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis = 1)\n    \n    # Rename the columns\n    mis_val_table_rename_columns = mis_val_table.rename(columns = {0: 'Missing Values', 1: 'Percentage'})\n    \n    #Sort the table\n    mis_val_table_rename_columns = mis_val_table_rename_columns[\n        mis_val_table_rename_columns.iloc[:, 1]!=0].sort_values('Percentage', ascending=False).round(1) #round(1) to keep only one decimal\n    \n    #Print information\n    print('The total dataframe has ' + str(df.shape[1]) + ' columns')\n    print('There are ' + str(mis_val_table_rename_columns.shape[0]) + ' columns')\n    \n    return mis_val_table_rename_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"92a90e69e75150a6fdb632a1525e24c904d06e40"},"cell_type":"code","source":"missing_values = missing_values_table(app_train)\nmissing_values.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f002861edd1cd3dbcc098928d84cdb3a10c02e3"},"cell_type":"code","source":"app_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81bd6f63a744f5dfd8aea2e490fa88a01bef7168"},"cell_type":"code","source":"app_train.select_dtypes('object').apply(pd.Series.nunique, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c35bbe4c0025ce6ae9b4722345c3449e8bec3fb4"},"cell_type":"code","source":"app_test.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69f4bcdf152e874f621d04f7768b90072908099a"},"cell_type":"code","source":"# 该部分将'object'类型的columns转换为数值型，One-hot\n\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7083b69b2232379a3ec7318371e4b818490e49c2"},"cell_type":"code","source":"# 对齐train和test的数据，丢掉无用的列\ntrain_labels = app_train['TARGET']\n\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4a416d38fb553c20258b86daee06e13f457c8e5"},"cell_type":"code","source":"# 删除部分列的异常值\napp_train['DAYS_EMPLOYED_ANOM'] = app_train['DAYS_EMPLOYED'] == 365243\n\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\napp_test['DAYS_EMPLOYED_ANOM'] = app_test['DAYS_EMPLOYED'] == 365243\n\napp_test['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b8a5e7cbc45385587dd4bcff02b1e920fd1525d"},"cell_type":"code","source":"# 查看数据的Correlations\n\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n# 输出最相关的30个特征\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d29b930c1e6730d3b3a1144374a12765c16152fc"},"cell_type":"code","source":"#该部分加入专业知识特征\n\napp_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']\n\napp_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']\n\nprint('Domain Training Features shape: ', app_train_domain.shape)\nprint('Domain Testing Features shape: ', app_test_domain.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cadc824f045eaee632e7ccf0663cc6317bd19278"},"cell_type":"markdown","source":"** Manual Feature Engineering (Part 1) : bureau.csv**\n\n分为四步：\n1. 将SK_ID_CURR的次数统计出来\n2. 将bureau的数值类型特征抽取出来\n3. 将bureau的字符类型特征抽取出来\n4. 将上述特征加入train和test"},{"metadata":{"trusted":true,"_uuid":"0b0024365551196563a8adc8f8e2ed7311278ea9"},"cell_type":"code","source":"# Read in bureau\nbureau = pd.read_csv('../input/bureau.csv')\nbureau.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bf2010edb2236c4fc7b364ae4e558e0523604ce"},"cell_type":"code","source":"# Step 1: 将每个'SK_ID_CURR'出现的次数统计出来\nprevious_loan_counts = bureau.groupby('SK_ID_CURR', as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU': 'previous_loan_counts'})\nprevious_loan_counts.head()\n\n## 特别注意：将次数加入到train和test时候需要将设为0，即没有出现过","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ad5da4aea92189e53c47c2218600227c86772c6"},"cell_type":"code","source":"# Step 2: 将数值特征的抽取出来\n\ndef agg_numeric(df, group_var, df_name):\n    \"\"\"Aggregates the numeric values in a dataframe. This can\n    be used to create features for each instance of the grouping variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the dataframe to calculate the statistics on\n        group_var (string): \n            the variable by which to group df\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated for \n            all numeric columns. Each instance of the grouping variable will have \n            the statistics (mean, min, max, sum; currently supported) calculated. \n            The columns are also renamed to keep track of features created.\n    \n    \"\"\"\n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != group_var and 'SK_ID' in col:  #去掉'SK_ID_CURR'以外的带有'SK_ID'的特征，即其它文件的id\n            df = df.drop(columns = col)\n            \n    group_ids = df[group_var]\n    numeric_df = df.select_dtypes('number')\n    numeric_df[group_var] = group_ids        #将数值型特征和TARGET组成一个新的DF\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n\n    # Need to create new column names\n    columns = [group_var]\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        # Skip the grouping variable\n        if var != group_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1][:-1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n\n    agg.columns = columns\n    return agg\n\nbureau_agg = agg_numeric(bureau.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5846fcda1635fae5649393c488cbca806b8c3719"},"cell_type":"code","source":"# 第3步：将字符型特征的抽取出来\n\ndef count_categorical(df, group_var, df_name):\n    \"\"\"Computes counts and normalized counts for each observation\n    of `group_var` of each unique category in every categorical variable\n    \n    Parameters\n    --------\n    df : dataframe \n        The dataframe to calculate the value counts for.\n        \n    group_var : string\n        The variable by which to group the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    df_name : string\n        Variable added to the front of column names to keep track of columns\n\n    \n    Return\n    --------\n    categorical : dataframe\n        A dataframe with counts and normalized counts of each unique category in every categorical variable\n        with one row for every unique value of the `group_var`.\n        \n    \"\"\"\n    \n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('object'))\n\n    # Make sure to put the identifying id on the column\n    categorical[group_var] = df[group_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['count', 'count_norm']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    return categorical\n\nbureau_counts = count_categorical(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8f2b01876f28741c760e004b7a31afb94579c7f8"},"cell_type":"code","source":"# Step 4: 将上述特征加入到train和test中\n\n# 1. 将previous_loan_counts加入到train和test中\n\ntrain = app_train_domain.merge(previous_loan_counts, on = 'SK_ID_CURR', how = 'left')\ntrain['previous_loan_counts'] = train['previous_loan_counts'].fillna(0)\ntest = app_test_domain.merge(previous_loan_counts, on = 'SK_ID_CURR', how = 'left')\ntest['previous_loan_counts'] = test['previous_loan_counts'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e401a1089b6603318913451ad23561e81aff1504"},"cell_type":"code","source":"# 2. 将bureau_agg加入到train和test中\ntrain = train.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\ntest = test.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"23d2fb788956973998698ed780b120ed3b4d477c"},"cell_type":"code","source":"# 3. 将bureau_counts加入到train和test中\ntrain = train.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')\ntest = test.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b25bc5b6b8d141797b23d986dc4f64c99253b36f"},"cell_type":"code","source":"print('Before align train.shape: ', train.shape)\nprint('Before align test.shape: ', test.shape)\n\ntrain_labels = train['TARGET']\n\n# Align the dataframes, this will remove the 'TARGET' column\ntrain, test = train.align(test, join = 'inner', axis = 1)\n\ntrain['TARGET'] = train_labels\n\nprint('After align train.shape: ', train.shape)\nprint('After align test.shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7345ea44adf43e675c9f02d52edf0a0551d8a766","collapsed":true},"cell_type":"code","source":"def train_with_cv(train_data, test_data, n_folds, seed_varying):\n    train_ids = train_data['SK_ID_CURR']\n    test_ids = test_data['SK_ID_CURR']\n    \n    train_labels = train_data['TARGET']\n    \n    train_features = train_data.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_data.drop(columns = ['SK_ID_CURR'])\n    \n    feature_names = list(train_features.columns)\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    train_features = np.array(train_features)\n    test_features = np.array(test_features)\n    \n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50+seed_varying)\n    \n    test_pred = np.zeros(test_features.shape[0])\n    out_of_fold = np.zeros(train_features.shape[0])\n    \n    valid_scores = []\n    train_scores = []\n    \n    for train_indices, valid_indices in k_fold.split(train_features):\n        x_train, y_train = train_features[train_indices], train_labels[train_indices]\n        x_valid, y_valid = train_features[valid_indices], train_labels[valid_indices]\n        \n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50+seed_varying)\n        \n        model.fit(x_train, y_train, eval_metric = 'auc',\n                  eval_set = [(x_valid, y_valid), (x_train, y_train)],\n                  eval_names = ['valid', 'train'], categorical_feature = 'auto',\n                  early_stopping_rounds = 100, verbose = -1)\n        \n        best_iteration = model.best_iteration_\n        \n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        test_pred += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        out_of_fold[valid_indices] = model.predict_proba(x_valid, num_iteration = best_iteration)[:, 1]\n        \n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        gc.enable()\n        del model, x_train, y_train, x_valid, y_valid\n        gc.collect()\n    \n    pred_score = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_pred})\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    valid_auc = roc_auc_score(train_labels, out_of_fold)\n    \n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return pred_score, feature_importances, metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"025e0e70ad76bd5cee1d923eb49263c50a47aa6c"},"cell_type":"code","source":"train_times = 3\nn_folds = 5\n\ni = 0\nmetrics_all = np.zeros((train_times, 2))\nfor seed_varying in range(train_times):\n    print('\\n=======================================================')\n    print('The ', seed_varying, ' time of train')\n    print('\\n=======================================================')\n    sub, fi, metrics = train_with_cv(train, test, n_folds, seed_varying)      #注意修改train和test\n    if i==0:\n        submission = sub\n        feat_import = fi\n    else:\n        submission['TARGET'] += sub['TARGET']\n        feat_import['importance'] += fi['importance']\n    \n    metrics_all[i, :] = metrics.iloc[-1, 1:3]\n    i += 1\n\nmetrics_all_average = metrics_all.mean(axis = 0)\nmetrics_all = np.row_stack([metrics_all, metrics_all_average])\ntrain_time_names = list(range(train_times))\ntrain_time_names.append('Average')\nmetrics_final = pd.DataFrame({'train_time': train_time_names,\n                            'train': metrics_all[:,0],\n                            'valid': metrics_all[:,1]}) \n\nsubmission['TARGET'] = submission['TARGET'] / train_times\nfeat_import['importance'] = feat_import['importance'] / train_times\n\nsubmission.to_csv('lightgbm_version_9.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1a81bd61c548d236c3b9ead8c5fb80e5ddb2a82","collapsed":true},"cell_type":"code","source":"metrics_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1ff87e44d329f4642b1d9e660d34db77be749d7","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d7b93934619fd569a0bbf912a9b79f210e07824","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21fc044e3b68b880260e9f821889e0a5e86da99f","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e142a903656a71018bdd6395160a22f8e4fedca5","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58e6822ac6a4d98c5f032be77e22f4d1a6c53ae7","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2975baf3538c10be41bb4b16218836275f87ca07"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}