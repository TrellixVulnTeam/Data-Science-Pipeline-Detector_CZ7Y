{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-23T09:45:24.892553Z","iopub.execute_input":"2021-07-23T09:45:24.892943Z","iopub.status.idle":"2021-07-23T09:45:24.9073Z","shell.execute_reply.started":"2021-07-23T09:45:24.892865Z","shell.execute_reply":"2021-07-23T09:45:24.906556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 목표: 대출을 받은 고객이 상환능력이 있는지 없는지를 분류하는 에측 모델을 만드는 것","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:24.922735Z","iopub.execute_input":"2021-07-23T09:45:24.923289Z","iopub.status.idle":"2021-07-23T09:45:24.926597Z","shell.execute_reply.started":"2021-07-23T09:45:24.923258Z","shell.execute_reply":"2021-07-23T09:45:24.9258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Imports\n\n#### We are using a typical data science stack: numpy, pandas, sklearn, matplotlib","metadata":{}},{"cell_type":"code","source":"# numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd\n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system management\nimport os\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:24.950629Z","iopub.execute_input":"2021-07-23T09:45:24.951091Z","iopub.status.idle":"2021-07-23T09:45:26.000955Z","shell.execute_reply.started":"2021-07-23T09:45:24.951062Z","shell.execute_reply":"2021-07-23T09:45:25.999916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read in Data","metadata":{}},{"cell_type":"code","source":"# List files available\nprint(os.listdir(\"../input/\"))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:26.002608Z","iopub.execute_input":"2021-07-23T09:45:26.002912Z","iopub.status.idle":"2021-07-23T09:45:26.007556Z","shell.execute_reply.started":"2021-07-23T09:45:26.002882Z","shell.execute_reply":"2021-07-23T09:45:26.006629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training data\napp_train = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:26.008986Z","iopub.execute_input":"2021-07-23T09:45:26.009487Z","iopub.status.idle":"2021-07-23T09:45:31.769457Z","shell.execute_reply.started":"2021-07-23T09:45:26.00946Z","shell.execute_reply":"2021-07-23T09:45:31.768393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing data\napp_test = pd.read_csv('../input/home-credit-default-risk/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:31.771352Z","iopub.execute_input":"2021-07-23T09:45:31.771782Z","iopub.status.idle":"2021-07-23T09:45:32.669442Z","shell.execute_reply.started":"2021-07-23T09:45:31.771741Z","shell.execute_reply":"2021-07-23T09:45:32.668386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## The test set is considerably smaller and lacks a Target column","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:32.671046Z","iopub.execute_input":"2021-07-23T09:45:32.671471Z","iopub.status.idle":"2021-07-23T09:45:32.675878Z","shell.execute_reply.started":"2021-07-23T09:45:32.671429Z","shell.execute_reply":"2021-07-23T09:45:32.674882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"#### Examine the Distribution of the Target Column","metadata":{}},{"cell_type":"code","source":"app_train['TARGET'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:32.677449Z","iopub.execute_input":"2021-07-23T09:45:32.677903Z","iopub.status.idle":"2021-07-23T09:45:32.697567Z","shell.execute_reply.started":"2021-07-23T09:45:32.677858Z","shell.execute_reply":"2021-07-23T09:45:32.696339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train['TARGET'].astype(int).plot.hist();","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:32.699025Z","iopub.execute_input":"2021-07-23T09:45:32.69946Z","iopub.status.idle":"2021-07-23T09:45:32.93801Z","shell.execute_reply.started":"2021-07-23T09:45:32.699416Z","shell.execute_reply":"2021-07-23T09:45:32.937047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### 이 정보에서 우리는 이것이 불균형 클래스 문제임을 알 수 있습니다.\n### 갚지 않은 대출보다 제때 갚은 대출이 훨씬 더 많다. \n### 좀 더 정교한 기계 학습 모델에 들어가면 이러한 불균형을 반영하기 위해 데이터의 표현에 따라 클래스에 가중치를 부여할 수 있습니다.","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:32.941102Z","iopub.execute_input":"2021-07-23T09:45:32.941527Z","iopub.status.idle":"2021-07-23T09:45:32.946351Z","shell.execute_reply.started":"2021-07-23T09:45:32.941493Z","shell.execute_reply":"2021-07-23T09:45:32.945374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate missing values by column Funct\n\ndef missing_values_table(df):\n    # Total missing values\n    mis_val = df.isnull().sum()\n    \n    # Percentage of missing values\n    mis_val_percent = 100 * df.isnull().sum()/len(df)\n    \n    # Make a table with the results\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    \n    # Rename the columns\n    mis_val_table_ren_columns = mis_val_table.rename(columns = {0: 'Missing Values', 1: '% of Total Values'})\n    \n    # Sort the table by percentage of missing descending\n    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1]!=0].sort_values(\n    '% of Total Values', ascending=False).round(1)\n    \n    #Print some summary information\n    print(\"Your selected dataframe has\" + str(df.shape[1])+\"columns.\\n\"\n         \"There are\" + str(mis_val_table_ren_columns.shape[0])+\n         \"columns that have missing values.\")\n    \n    # Return the dataframe with missing information\n    return mis_val_table_ren_columns\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:32.948582Z","iopub.execute_input":"2021-07-23T09:45:32.94901Z","iopub.status.idle":"2021-07-23T09:45:32.961732Z","shell.execute_reply.started":"2021-07-23T09:45:32.948967Z","shell.execute_reply":"2021-07-23T09:45:32.960809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_values_table(app_train)\nmissing_values.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:32.963019Z","iopub.execute_input":"2021-07-23T09:45:32.963382Z","iopub.status.idle":"2021-07-23T09:45:33.946245Z","shell.execute_reply.started":"2021-07-23T09:45:32.963338Z","shell.execute_reply":"2021-07-23T09:45:33.945198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### 기계 학습 모델을 구축할 때가 되면 이러한 결측값을 채워야 합니다\n### 이후 작업에서 우리는 대치할 필요 없이 결측값을 처리할 수 있는 XGBoost와 같은 모델을 사용할 것입니다. \n### 또 다른 옵션은 누락된 값의 비율이 높은 열을 삭제하는 것이지만 이러한 열이 우리 모델에 도움이 될지 미리 알 수는 없습니다. 따라서 지금은 모든 열을 유지합니다.","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:33.947541Z","iopub.execute_input":"2021-07-23T09:45:33.947828Z","iopub.status.idle":"2021-07-23T09:45:33.951787Z","shell.execute_reply.started":"2021-07-23T09:45:33.9478Z","shell.execute_reply":"2021-07-23T09:45:33.950908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Column Types","metadata":{}},{"cell_type":"code","source":"## Number of each type of column\napp_train.dtypes.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:33.952967Z","iopub.execute_input":"2021-07-23T09:45:33.953336Z","iopub.status.idle":"2021-07-23T09:45:33.968951Z","shell.execute_reply.started":"2021-07-23T09:45:33.953305Z","shell.execute_reply":"2021-07-23T09:45:33.967908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:33.970368Z","iopub.execute_input":"2021-07-23T09:45:33.97087Z","iopub.status.idle":"2021-07-23T09:45:34.812955Z","shell.execute_reply.started":"2021-07-23T09:45:33.970838Z","shell.execute_reply":"2021-07-23T09:45:34.812093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Encoding Categorical Variables\n\n# 1.Label encoding: 범주형 변수의 각 고유 범주를 정수로 할당합니다. 새 열이 생성되지 않습니다.\n\n# 2. One-hot encoding: 범주형 변수의 각 고유 범주에 대해 새 열을 만듭니다. 각 관찰은 해당 범주에 대해 열에 1을 받고 다른 모든 새 열에 0을 받습니다.\n\n## 레이블 인코딩의 문제는 범주에 임의의 순서를 부여한다는 것입니다. \n## 각 범주에 할당된 값은 무작위이며 범주의 고유한 측면을 반영하지 않습니다.\n## 따라서 레이블 인코딩을 수행할 때 모델은 기능의 상대 값(예: 프로그래머 = 4 및 데이터 과학자 = 1)을 사용하여 우리가 원하는 것이 아닌 가중치를 할당할 수 있습니다. \n## 범주형 변수(예: 남성/여성)에 대해 고유한 값이 두 개뿐인 경우 레이블 인코딩은 괜찮지만 고유 범주가 2개 이상인 경우 원 핫 인코딩이 안전한 옵션입니다.\n## 클래스가 많은 범주형 변수의 경우 원-핫 인코딩이 범주에 임의의 값을 부과하지 않기 때문에 가장 안전한 접근 방식이라고 생각합니다. \n## 원-핫 인코딩의 유일한 단점은 기능의 수(데이터 차원)가 많은 범주의 범주형 변수로 폭발할 수 있다는 것입니다. \n## 이를 처리하기 위해 원-핫 인코딩을 수행한 후 PCA 또는 기타 차원 축소 방법을 수행하여 차원 수를 줄일 수 있습니다.","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:34.814117Z","iopub.execute_input":"2021-07-23T09:45:34.81442Z","iopub.status.idle":"2021-07-23T09:45:34.818499Z","shell.execute_reply.started":"2021-07-23T09:45:34.814394Z","shell.execute_reply":"2021-07-23T09:45:34.817635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Label Encoding and One-Hot Encoding\n\n# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # IF 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(app_train[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            #Keep track of how many columns were label encoded\n            le_count += 1\n            \n        print('%d columns were label encoded.'%le_count)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:34.820081Z","iopub.execute_input":"2021-07-23T09:45:34.820631Z","iopub.status.idle":"2021-07-23T09:45:35.671731Z","shell.execute_reply.started":"2021-07-23T09:45:34.820592Z","shell.execute_reply":"2021-07-23T09:45:35.670654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Feature shape: ', app_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:35.673099Z","iopub.execute_input":"2021-07-23T09:45:35.673495Z","iopub.status.idle":"2021-07-23T09:45:36.722848Z","shell.execute_reply.started":"2021-07-23T09:45:35.673454Z","shell.execute_reply":"2021-07-23T09:45:36.721978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Aligning Training and Testing Data\n\ntrain_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join='inner', axis=1)\n## 두 객체 둘 다 있는 인덱스를 교차\n\n# Add the target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Feature shape: ', app_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:36.723985Z","iopub.execute_input":"2021-07-23T09:45:36.724303Z","iopub.status.idle":"2021-07-23T09:45:37.102164Z","shell.execute_reply.started":"2021-07-23T09:45:36.724227Z","shell.execute_reply":"2021-07-23T09:45:37.101277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Back to Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# DAYS_BIRTH 열의 숫자는 현재 대출 신청을 기준으로 기록되기 때문에 음수입니다. 이러한 통계를 연도 단위로 보려면 -1을 곱하고 1년의 일수로 나눌 수 있습니다.\n# 이상치는 없는 것 같\n\n(app_train['DAYS_BIRTH']/-365).describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:37.103301Z","iopub.execute_input":"2021-07-23T09:45:37.103606Z","iopub.status.idle":"2021-07-23T09:45:37.129571Z","shell.execute_reply.started":"2021-07-23T09:45:37.103578Z","shell.execute_reply":"2021-07-23T09:45:37.128562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:37.130711Z","iopub.execute_input":"2021-07-23T09:45:37.131022Z","iopub.status.idle":"2021-07-23T09:45:37.150164Z","shell.execute_reply.started":"2021-07-23T09:45:37.130993Z","shell.execute_reply":"2021-07-23T09:45:37.149431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:37.151151Z","iopub.execute_input":"2021-07-23T09:45:37.151576Z","iopub.status.idle":"2021-07-23T09:45:37.378806Z","shell.execute_reply.started":"2021-07-23T09:45:37.151546Z","shell.execute_reply":"2021-07-23T09:45:37.378158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anom = app_train[app_train['DAYS_EMPLOYED']==365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED']!=365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100*non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100*anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment'% len(anom))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:37.379721Z","iopub.execute_input":"2021-07-23T09:45:37.380083Z","iopub.status.idle":"2021-07-23T09:45:37.811288Z","shell.execute_reply.started":"2021-07-23T09:45:37.380058Z","shell.execute_reply":"2021-07-23T09:45:37.810552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### 예외 처리는 정해진 규칙 없이 정확한 상황에 따라 다릅니다. \n### 가장 안전한 접근 방식 중 하나는 예외를 결측값으로 설정한 다음 기계 학습 전에 (대치를 사용하여) 채우는 것입니다. \n### 이 경우 모든 변칙의 값이 정확히 같으므로 이러한 모든 대출이 공통점을 공유할 경우를 대비하여 동일한 값으로 채우고자 합니다. \n### 비정상적인 값은 어느 정도 중요한 것 같으므로 실제로 이 값을 채웠는지 머신 러닝 모델에 알리고 싶습니다. \n### 해결책으로 비정상적인 값을 숫자(np.nan)가 아닌 값으로 채운 다음 값이 비정상적인지 여부를 나타내는 새 부울 열을 만듭니다.","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:37.812197Z","iopub.execute_input":"2021-07-23T09:45:37.812551Z","iopub.status.idle":"2021-07-23T09:45:37.815683Z","shell.execute_reply.started":"2021-07-23T09:45:37.812525Z","shell.execute_reply":"2021-07-23T09:45:37.814988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train['DAYS_EMPLOYED'] == 365243\n\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243:np.nan}, inplace=True)\n\napp_train['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram');\nplt.xlabel('Days Employment');","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:37.818461Z","iopub.execute_input":"2021-07-23T09:45:37.818838Z","iopub.status.idle":"2021-07-23T09:45:38.107909Z","shell.execute_reply.started":"2021-07-23T09:45:37.818811Z","shell.execute_reply":"2021-07-23T09:45:38.106832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 훈련 데이터에 대해 수행하는 모든 작업은 테스트 데이터에도 수행해야 합니다. \n## 테스트 데이터에서 새 열을 만들고 기존 열을 np.nan으로 채우도록 합시다.","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:38.109549Z","iopub.execute_input":"2021-07-23T09:45:38.109844Z","iopub.status.idle":"2021-07-23T09:45:38.114337Z","shell.execute_reply.started":"2021-07-23T09:45:38.109817Z","shell.execute_reply":"2021-07-23T09:45:38.11318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_test['DAYS_EMPLOYED_ANOM'] = app_test['DAYS_EMPLOYED']==365243\napp_test['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace=True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test['DAYS_EMPLOYED_ANOM'].sum(), len(app_test)))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:38.115685Z","iopub.execute_input":"2021-07-23T09:45:38.116119Z","iopub.status.idle":"2021-07-23T09:45:38.13572Z","shell.execute_reply.started":"2021-07-23T09:45:38.116086Z","shell.execute_reply":"2021-07-23T09:45:38.134798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find correlations with the target and sort\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:45:38.136857Z","iopub.execute_input":"2021-07-23T09:45:38.137134Z","iopub.status.idle":"2021-07-23T09:46:28.344419Z","shell.execute_reply.started":"2021-07-23T09:45:38.137108Z","shell.execute_reply":"2021-07-23T09:46:28.343433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### 설명서를 보면 DAYS_BIRTH는 대출 당시 클라이언트의 나이로 음수일(이유가 무엇이든!)입니다. \n### 상관 관계는 양수이지만 이 기능의 값은 실제로 음수입니다. \n### 즉, 고객이 나이가 들수록 대출 불이행 가능성이 줄어듭니다(즉, 목표 == 0). \n### 이것은 약간 혼란스럽기 때문에 특성의 절대값을 취하면 상관 관계가 음수가 됩니다.","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:46:28.345789Z","iopub.execute_input":"2021-07-23T09:46:28.346168Z","iopub.status.idle":"2021-07-23T09:46:28.350696Z","shell.execute_reply.started":"2021-07-23T09:46:28.34613Z","shell.execute_reply":"2021-07-23T09:46:28.349448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Effect of Age on Repayment","metadata":{}},{"cell_type":"code","source":"# Find the correlation of the positive days since birth and target\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:46:28.352117Z","iopub.execute_input":"2021-07-23T09:46:28.352454Z","iopub.status.idle":"2021-07-23T09:46:28.374165Z","shell.execute_reply.started":"2021-07-23T09:46:28.352425Z","shell.execute_reply":"2021-07-23T09:46:28.373306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the style of plots\nplt.style.use('fivethirtyeight')\n\n# Plot the distribution of ages in years\nplt.hist(app_train['DAYS_BIRTH']/365, edgecolor = 'k', bins=25)\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:46:28.37529Z","iopub.execute_input":"2021-07-23T09:46:28.375555Z","iopub.status.idle":"2021-07-23T09:46:28.59208Z","shell.execute_reply.started":"2021-07-23T09:46:28.37553Z","shell.execute_reply":"2021-07-23T09:46:28.591143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10,8))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET']==0, 'DAYS_BIRTH']/365, label='target==0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET']==1, 'DAYS_BIRTH']/365, label='target==1')\n\n# Labeling of plot\nplt.xlabel('Age(years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:46:28.594192Z","iopub.execute_input":"2021-07-23T09:46:28.594637Z","iopub.status.idle":"2021-07-23T09:46:30.659513Z","shell.execute_reply.started":"2021-07-23T09:46:28.594596Z","shell.execute_reply":"2021-07-23T09:46:30.658521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Age information into a separate dataframe\nage_data = app_train[['TARGET','DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH']/365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins=np.linspace(20,70,num=11))\nage_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:46:30.660877Z","iopub.execute_input":"2021-07-23T09:46:30.661372Z","iopub.status.idle":"2021-07-23T09:46:30.822184Z","shell.execute_reply.started":"2021-07-23T09:46:30.661333Z","shell.execute_reply":"2021-07-23T09:46:30.821162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group by the bin and calculate averages\nage_groups = age_data.groupby('YEARS_BINNED').mean()\nage_groups","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:46:30.823329Z","iopub.execute_input":"2021-07-23T09:46:30.823628Z","iopub.status.idle":"2021-07-23T09:46:30.846231Z","shell.execute_reply.started":"2021-07-23T09:46:30.823601Z","shell.execute_reply":"2021-07-23T09:46:30.845337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100*age_groups['TARGET'])\n\n# plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay(%)')\nplt.title('Failure to Repay by Age Group');","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:46:30.847335Z","iopub.execute_input":"2021-07-23T09:46:30.847587Z","iopub.status.idle":"2021-07-23T09:46:31.073675Z","shell.execute_reply.started":"2021-07-23T09:46:30.847564Z","shell.execute_reply":"2021-07-23T09:46:31.072642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### 분명한 추세가 있습니다.\n### 젊은 지원자는 대출을 상환하지 않을 가능성이 더 높습니다\n### 연체율은 최연소 3세 10% 이상, 고령 5% 미만이다.","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:46:31.075169Z","iopub.execute_input":"2021-07-23T09:46:31.075579Z","iopub.status.idle":"2021-07-23T09:46:31.079859Z","shell.execute_reply.started":"2021-07-23T09:46:31.075537Z","shell.execute_reply":"2021-07-23T09:46:31.078848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2',\n                     'EXT_SOURCE_3','DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:46:31.081475Z","iopub.execute_input":"2021-07-23T09:46:31.081864Z","iopub.status.idle":"2021-07-23T09:46:31.139433Z","shell.execute_reply.started":"2021-07-23T09:46:31.081825Z","shell.execute_reply":"2021-07-23T09:46:31.138401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\n\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap=plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax=0.6)\nplt.title('Correlation Heatmap');","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:46:31.140722Z","iopub.execute_input":"2021-07-23T09:46:31.141011Z","iopub.status.idle":"2021-07-23T09:46:31.520929Z","shell.execute_reply.started":"2021-07-23T09:46:31.140984Z","shell.execute_reply":"2021-07-23T09:46:31.520284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10,12))\n\n# iterate through the sources\nfor i,source in enumerate(['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']):\n    \n    #create a new subplot for each source\n    plt.subplot(3,1,i+1)\n    \n    #plot repaid loans\n    sns.kdeplot(app_train.loc[app_train['TARGET']==0, source], label='target==0')\n    \n    #plot loans that were not repaid\n    sns.kdeplot(app_train.loc[app_train['TARGET']==1, source],label='target==1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n\n    plt.tight_layout(h_pad = 2.5)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:46:31.521807Z","iopub.execute_input":"2021-07-23T09:46:31.522044Z","iopub.status.idle":"2021-07-23T09:46:35.770554Z","shell.execute_reply.started":"2021-07-23T09:46:31.522021Z","shell.execute_reply":"2021-07-23T09:46:35.769432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Pairs Plot은 단일 변수의 분포뿐만 아니라 여러 쌍의 변수 간의 관계를 볼 수 있게 해줌","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Copy the data for plotting\nplot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n\n# Add in the age of the client in years\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n\n# Drop na values and limit to first 100000 rows\nplot_data = plot_data.dropna().loc[:100000,:]\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x,y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r={:.2f}\".format(r),\n               xy=(.2, .8), xycoords=ax.transAxes,\n               size = 20)\n    \n# Create the pairgrid object\ngrid = sns.PairGrid(data=plot_data, size=3, diag_sharey=False,\n                   hue = 'TARGET',\n                   vars = [x for x in list(plot_data.columns)\n                          if x != 'TARGET'])\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.2)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap=plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', size=32, y=1.05);","metadata":{"execution":{"iopub.status.busy":"2021-07-23T10:02:26.273037Z","iopub.execute_input":"2021-07-23T10:02:26.273391Z","iopub.status.idle":"2021-07-23T10:05:38.158144Z","shell.execute_reply.started":"2021-07-23T10:02:26.273359Z","shell.execute_reply":"2021-07-23T10:05:38.157178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Engineering","metadata":{}},{"cell_type":"code","source":"### 피쳐 엔지니어링은 일반적인 프로세스를 말하며 피쳐 구성(기존 데이터에서 새 피쳐 추가)과 피쳐 선택(가장 중요한 피쳐만 선택하거나 다른 차원 축소 방법)을 모두 포함할 수 있습니다. \n### 피처를 생성하고 피처를 선택하는 데 사용할 수 있는 많은 기술이 있습니다.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Polynomial Features","metadata":{}},{"cell_type":"code","source":"## 다항 회귀란, 데이터들 간의 형태가 비선형일 때 데이터에 각 특성의 제곱을 추가해 주어서 특성이 추가된 비선형 데이터를 선형 회귀 모델로 훈련시키는 방법\n\n## Scikit-Learn에는 지정된 정도까지 다항식과 상호 작용 항을 생성하는 PolynomialFeatures라는 유용한 클래스가 있습니다. \n## 결과를 보기 위해 차수 3을 사용할 수 있습니다(다항식 특징을 생성할 때 특징의 수가 차수에 따라 기하급수적으로 확장되고 문제가 발생할 수 있기 때문에 너무 높은 차수를 사용하는 것을 피하고 싶습니다. 과적합)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T10:14:51.74982Z","iopub.execute_input":"2021-07-23T10:14:51.750148Z","iopub.status.idle":"2021-07-23T10:14:51.753275Z","shell.execute_reply.started":"2021-07-23T10:14:51.750125Z","shell.execute_reply":"2021-07-23T10:14:51.752521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a new dataframe for polynomial features\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH','TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']]\n\n# imputer for handling missing values\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy = 'median')\n\npoly_target = poly_features['TARGET']\n\npoly_features = poly_features.drop(columns=['TARGET'])\n\n# Need to impute missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Create the polynomail object with specified degree\npoly_transformer = PolynomialFeatures(degree=3)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T10:26:02.458241Z","iopub.execute_input":"2021-07-23T10:26:02.458768Z","iopub.status.idle":"2021-07-23T10:26:02.67602Z","shell.execute_reply.started":"2021-07-23T10:26:02.458729Z","shell.execute_reply":"2021-07-23T10:26:02.675323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\n\nprint('Polynomial Features shape: ', poly_features.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T10:29:22.207192Z","iopub.execute_input":"2021-07-23T10:29:22.207605Z","iopub.status.idle":"2021-07-23T10:29:22.376035Z","shell.execute_reply.started":"2021-07-23T10:29:22.207572Z","shell.execute_reply":"2021-07-23T10:29:22.374978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 다항식 기능\n\npoly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH'])[:15]","metadata":{"execution":{"iopub.status.busy":"2021-07-23T10:31:11.096732Z","iopub.execute_input":"2021-07-23T10:31:11.097328Z","iopub.status.idle":"2021-07-23T10:31:11.106043Z","shell.execute_reply.started":"2021-07-23T10:31:11.097288Z","shell.execute_reply":"2021-07-23T10:31:11.105304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe of the features\npoly_features = pd.DataFrame(poly_features,\n                            columns = poly_transformer.get_feature_names(['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']))\n\n# Add in the target\npoly_features['TARGET'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T10:35:53.917586Z","iopub.execute_input":"2021-07-23T10:35:53.918056Z","iopub.status.idle":"2021-07-23T10:35:55.202049Z","shell.execute_reply.started":"2021-07-23T10:35:53.918004Z","shell.execute_reply":"2021-07-23T10:35:55.201119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, columns=poly_transformer.get_feature_names(['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']))\n\n# Merge polynomial features into training dataframe\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how='left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how='left')\n\n# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join='inner', axis=1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape: ', app_test_poly.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T10:49:22.020289Z","iopub.execute_input":"2021-07-23T10:49:22.020745Z","iopub.status.idle":"2021-07-23T10:49:24.953068Z","shell.execute_reply.started":"2021-07-23T10:49:22.020717Z","shell.execute_reply":"2021-07-23T10:49:24.952046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT']/app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY']/app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY']/app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED']/app_train_domain['DAYS_BIRTH']","metadata":{"execution":{"iopub.status.busy":"2021-07-23T10:54:46.495134Z","iopub.execute_input":"2021-07-23T10:54:46.495534Z","iopub.status.idle":"2021-07-23T10:54:46.69575Z","shell.execute_reply.started":"2021-07-23T10:54:46.495501Z","shell.execute_reply":"2021-07-23T10:54:46.694943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:02:14.253924Z","iopub.execute_input":"2021-07-23T11:02:14.25432Z","iopub.status.idle":"2021-07-23T11:02:14.26516Z","shell.execute_reply.started":"2021-07-23T11:02:14.254289Z","shell.execute_reply":"2021-07-23T11:02:14.264294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12,20))\n\n# iterate through the new features\nfor i, feature in enumerate(['CREDIT_INCOME_PERCENT','ANNUITY_INCOME_PERCENT','CREDIT_TERM','DAYS_EMPLOYED_PERCENT']):\n    \n    # create a new subplot for each source\n    plt.subplot(4, 1, i+1)\n    \n    # plot repaid loans\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET']==0, feature], label='target==0')\n    \n    # plot loans that were not repaid\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET']==1, feature], label='target==1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % feature)\n    plt.xlabel('%s' % feature); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:08:37.053273Z","iopub.execute_input":"2021-07-23T11:08:37.053696Z","iopub.status.idle":"2021-07-23T11:08:43.521398Z","shell.execute_reply.started":"2021-07-23T11:08:37.05366Z","shell.execute_reply":"2021-07-23T11:08:43.520412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression Implementation","metadata":{}},{"cell_type":"code","source":"### 기준선을 얻기 위해 범주형 변수를 인코딩한 후 모든 기능을 사용합니다. \n### 누락된 값을 채우고(대치) 기능의 범위를 정규화(기능 스케일링)하여 데이터를 전처리합니다. ","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:10:02.964003Z","iopub.execute_input":"2021-07-23T11:10:02.964431Z","iopub.status.idle":"2021-07-23T11:10:02.968619Z","shell.execute_reply.started":"2021-07-23T11:10:02.964394Z","shell.execute_reply":"2021-07-23T11:10:02.967533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns=['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# Median imputation of missing values\nimputer = SimpleImputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0,1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:18:13.880009Z","iopub.execute_input":"2021-07-23T11:18:13.88043Z","iopub.status.idle":"2021-07-23T11:18:52.117193Z","shell.execute_reply.started":"2021-07-23T11:18:13.880393Z","shell.execute_reply":"2021-07-23T11:18:52.116005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 첫 번째 모델에 대해 Scikit-Learn의 LogisticRegression을 사용합니다. \n# 기본 모델 설정에서 변경할 수 있는 유일한 변경 사항은 과적합의 양을 제어하는 정규화 매개변수 C를 낮추는 것입니다(낮은 값은 과적합을 줄여야 함). \n# 이렇게 하면 기본 LogisticRegression보다 약간 더 나은 결과를 얻을 수 있지만 향후 모델에 대해서는 여전히 낮은 기준을 설정합니다.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:23:58.690874Z","iopub.execute_input":"2021-07-23T11:23:58.691231Z","iopub.status.idle":"2021-07-23T11:24:02.010119Z","shell.execute_reply.started":"2021-07-23T11:23:58.691187Z","shell.execute_reply":"2021-07-23T11:24:02.009055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 첫번째 열은 대상이 0일 확률이고 두번째 열은 대상이 1일 확률\n## 단일 행의 경우, 두 열의 합이 1이 되어야 함\n## 우리는 대출이 상환되지 않을 확률을 원하므로 두 번째 열을 선택","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:30:18.015028Z","iopub.execute_input":"2021-07-23T11:30:18.015402Z","iopub.status.idle":"2021-07-23T11:30:18.019582Z","shell.execute_reply.started":"2021-07-23T11:30:18.015374Z","shell.execute_reply":"2021-07-23T11:30:18.018439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions\n# Make sure to select the second column only\nlog_reg_pred = log_reg.predict_proba(test)[:,1]","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:31:20.996384Z","iopub.execute_input":"2021-07-23T11:31:20.99672Z","iopub.status.idle":"2021-07-23T11:31:21.019958Z","shell.execute_reply.started":"2021-07-23T11:31:20.996693Z","shell.execute_reply":"2021-07-23T11:31:21.01873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:32:23.78873Z","iopub.execute_input":"2021-07-23T11:32:23.789089Z","iopub.status.idle":"2021-07-23T11:32:23.802761Z","shell.execute_reply.started":"2021-07-23T11:32:23.789061Z","shell.execute_reply":"2021-07-23T11:32:23.801936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the submission to a csv file\nsubmit.to_csv('log_reg_baseline.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:34:07.472676Z","iopub.execute_input":"2021-07-23T11:34:07.473038Z","iopub.status.idle":"2021-07-23T11:34:07.654251Z","shell.execute_reply.started":"2021-07-23T11:34:07.47301Z","shell.execute_reply":"2021-07-23T11:34:07.653343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Improved Model: Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose=1, n_jobs=-1)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:36:22.469385Z","iopub.execute_input":"2021-07-23T11:36:22.469763Z","iopub.status.idle":"2021-07-23T11:36:22.529307Z","shell.execute_reply.started":"2021-07-23T11:36:22.469734Z","shell.execute_reply":"2021-07-23T11:36:22.528411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train on the training data\nrandom_forest.fit(train, train_labels)\n\n# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature':features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = random_forest.predict_proba(test)[:, 1]","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:41:31.505917Z","iopub.execute_input":"2021-07-23T11:41:31.506329Z","iopub.status.idle":"2021-07-23T11:42:50.757975Z","shell.execute_reply.started":"2021-07-23T11:41:31.506296Z","shell.execute_reply":"2021-07-23T11:42:50.756969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:43:17.721236Z","iopub.execute_input":"2021-07-23T11:43:17.721608Z","iopub.status.idle":"2021-07-23T11:43:17.858533Z","shell.execute_reply.started":"2021-07-23T11:43:17.721579Z","shell.execute_reply":"2021-07-23T11:43:17.857208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make Predictions using Engineered Features","metadata":{}},{"cell_type":"code","source":"poly_features_names = list(app_train_poly.columns)\n\n# Impute the polynomial features\nimputer = SimpleImputer(strategy = 'median')\n\npoly_features = imputer.fit_transform(app_train_poly)\npoly_features_test = imputer.transform(app_test_poly)\n\n# Scale the polynomial features\nscaler = MinMaxScaler(feature_range = (0,1))\n\npoly_features = scaler.fit_transform(poly_features)\npoly_features_test = scaler.transform(poly_features_test)\n\nrandom_forest_poly = RandomForestClassifier(n_estimators=100,\n                                           random_state = 50, verbose = 1, n_jobs = -1)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:49:22.058792Z","iopub.execute_input":"2021-07-23T11:49:22.0592Z","iopub.status.idle":"2021-07-23T11:50:07.896555Z","shell.execute_reply.started":"2021-07-23T11:49:22.059167Z","shell.execute_reply":"2021-07-23T11:50:07.895579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train on the training data\nrandom_forest_poly.fit(poly_features, train_labels)\n\n# Make predictions on the test data\npredictions = random_forest_poly.predict_proba(poly_features_test)[:,1]","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:53:14.435098Z","iopub.execute_input":"2021-07-23T11:53:14.435433Z","iopub.status.idle":"2021-07-23T11:55:13.852074Z","shell.execute_reply.started":"2021-07-23T11:53:14.435402Z","shell.execute_reply":"2021-07-23T11:55:13.851085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_engineered.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T11:55:13.853648Z","iopub.execute_input":"2021-07-23T11:55:13.85392Z","iopub.status.idle":"2021-07-23T11:55:13.991159Z","shell.execute_reply.started":"2021-07-23T11:55:13.853893Z","shell.execute_reply":"2021-07-23T11:55:13.99034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Testing Domain Features\n\napp_train_domain = app_train_domain.drop(columns = 'TARGET')\n\ndomain_features_names = list(app_train_domain.columns)\n\n# Impute the domainnomial features\nimputer = SimpleImputer(strategy = 'median')\n\ndomain_features = imputer.fit_transform(app_train_domain)\ndomain_features_test = imputer.transform(app_test_domain)\n\n# Scale the domainnomial features\nscaler = MinMaxScaler(feature_range = (0,1))\n\ndomain_features = scaler.fit_transform(domain_features)\ndomain_features_test = scaler.transform(domain_features_test)\n\nrandom_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose=1, n_jobs = -1)\n\n# Train on the training data\nrandom_forest_domain.fit(domain_features, train_labels)\n\n# Extract feature importances\nfeature_importance_values_domain = random_forest_domain.feature_importances_\nfeature_importance_domain = pd.DataFrame({'feature':domain_features_names,'importance':feature_importance_values_domain})\n\n# Make predictions on the test data\npredictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]","metadata":{"execution":{"iopub.status.busy":"2021-07-23T12:04:05.875129Z","iopub.execute_input":"2021-07-23T12:04:05.875494Z","iopub.status.idle":"2021-07-23T12:06:03.464076Z","shell.execute_reply.started":"2021-07-23T12:04:05.87546Z","shell.execute_reply":"2021-07-23T12:06:03.462184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_domain.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T12:06:03.467369Z","iopub.execute_input":"2021-07-23T12:06:03.467652Z","iopub.status.idle":"2021-07-23T12:06:03.599681Z","shell.execute_reply.started":"2021-07-23T12:06:03.467624Z","shell.execute_reply":"2021-07-23T12:06:03.59884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-07-23T12:06:50.225144Z","iopub.execute_input":"2021-07-23T12:06:50.225527Z","iopub.status.idle":"2021-07-23T12:06:50.233163Z","shell.execute_reply.started":"2021-07-23T12:06:50.225495Z","shell.execute_reply":"2021-07-23T12:06:50.232243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show the feature importances for the default features\nfeature_importances_sorted = plot_feature_importances(feature_importances)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T12:07:38.131106Z","iopub.execute_input":"2021-07-23T12:07:38.131681Z","iopub.status.idle":"2021-07-23T12:07:38.373325Z","shell.execute_reply.started":"2021-07-23T12:07:38.131645Z","shell.execute_reply":"2021-07-23T12:07:38.37243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusions\n\n#### We followed the general outline of a machine learning project:\n\n#### 1. Understand the problem and the data\n#### 2. Data cleaning and formatting (this was mostly done for us)\n#### 3. Exploratory Data Analysis\n#### 4. Baseline model\n#### 5. Improved model\n#### 6. Model interpretation (just a little)","metadata":{}},{"cell_type":"markdown","source":"### Just for Fun: Light Gradient Boosting Machine","metadata":{}},{"cell_type":"code","source":"### LightGBM은 현재 구조화된 데이터 세트(특히 Kaggle에서) 학습을 위한 최고의 모델","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport gc\n\ndef model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            # 오브젝트이면\n            if features[col].dtype == 'object':\n                \n                #라벨 인코더를 진행\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object \n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n    \n    # Empty array for feature importances\n    # 특성 중요도 배열\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    # 테스트 예측했을 때 넣을 배열\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    # validation할 때 나오는 예측값들 넣는 배열\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # 한 번 kfold를 돌고 모델을 훈련시킨 뒤 최고 성적을 가져오고\n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        # 한 번 kfold 돌고 모델을 훈련시킨 뒤 그 훈련할 때 중요하게 본 특성을 가져와서 feature_importance_values에 넣어준다.\n        # kfold만큼 도니까 나눠준다\n        # 여기서는 특성 중요도를 kfold할 때마다 적용시킨 것이니까 나눠줘야 한다\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        # 예측을 넣어준다. 여기서도 kfold만큼 도니깐 나눠준다.\n        # 여기서는 test니까 test 데이터 전체를 넣어줌. \n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        # validation에 대한 값을 넣어주고\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        # 최고 좋은 성적을 뽑아냄\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    # test에 대한 예측값을 넣어주고\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","metadata":{"execution":{"iopub.status.busy":"2021-07-23T12:45:05.514299Z","iopub.execute_input":"2021-07-23T12:45:05.514697Z","iopub.status.idle":"2021-07-23T12:45:06.776977Z","shell.execute_reply.started":"2021-07-23T12:45:05.514666Z","shell.execute_reply":"2021-07-23T12:45:06.776022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Out of Fold(OOF) 방법으로 모델 평가\n### 모델의 성능을 평가하는 방법으로서, 실무보다는 Kaggle, Dacon과 같은 예측 알고리즘 대회에서 자주 사용되는 방식\n### K-fold를 이용한 것이 OOF라고 할 수 있으며, OOF안에는 K-fold가 속한다고 볼 수 있다\n\n### 참고 링크: https://techblog-history-younghunjo1.tistory.com/142","metadata":{"execution":{"iopub.status.busy":"2021-07-23T12:44:45.513574Z","iopub.execute_input":"2021-07-23T12:44:45.514032Z","iopub.status.idle":"2021-07-23T12:44:45.517618Z","shell.execute_reply.started":"2021-07-23T12:44:45.513997Z","shell.execute_reply":"2021-07-23T12:44:45.516829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission, fi, metrics = model(app_train, app_test)\nprint('Baseline metrics')\nprint(metrics)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T12:45:47.562249Z","iopub.execute_input":"2021-07-23T12:45:47.56259Z","iopub.status.idle":"2021-07-23T12:49:26.156156Z","shell.execute_reply.started":"2021-07-23T12:45:47.562562Z","shell.execute_reply":"2021-07-23T12:49:26.155081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fi_sorted = plot_feature_importances(fi)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T12:49:26.157656Z","iopub.execute_input":"2021-07-23T12:49:26.157937Z","iopub.status.idle":"2021-07-23T12:49:26.393766Z","shell.execute_reply.started":"2021-07-23T12:49:26.15791Z","shell.execute_reply":"2021-07-23T12:49:26.393157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('baseline_lgb.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train_domain['TARGET'] = train_labels\n\n# Test the domain knowledge features\nsubmission_domain, fi_domain, metrics_domain = model(app_train_domain, app_test_domain)\n\nprint('Baseline with domain knowledge features metrics')\nprint(metrics_domain)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T12:49:26.39496Z","iopub.execute_input":"2021-07-23T12:49:26.395301Z","iopub.status.idle":"2021-07-23T12:53:02.933128Z","shell.execute_reply.started":"2021-07-23T12:49:26.395276Z","shell.execute_reply":"2021-07-23T12:53:02.932135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fi_sorted = plot_feature_importances(fi_domain)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T12:53:02.934505Z","iopub.execute_input":"2021-07-23T12:53:02.934754Z","iopub.status.idle":"2021-07-23T12:53:03.16077Z","shell.execute_reply.started":"2021-07-23T12:53:02.93473Z","shell.execute_reply":"2021-07-23T12:53:03.16004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_domain.to_csv('baseline_lgb_domain_features.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T12:53:34.277162Z","iopub.execute_input":"2021-07-23T12:53:34.277659Z","iopub.status.idle":"2021-07-23T12:53:34.452478Z","shell.execute_reply.started":"2021-07-23T12:53:34.277626Z","shell.execute_reply":"2021-07-23T12:53:34.45169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}