{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1) What Is Feature Engineering ?\n\n__transformation__ of raw data into features suitable for modeling and improve the accuracy of the algorithm.\n\n\n# 2) Why Is Feature Engineering?\nIn practice, data rarely comes in the form of ready-to-use matrices. That's why every task begins with feature engineering.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom functools import reduce \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# for regression problems\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\n# for classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n# to split and standarize the datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# to evaluate regression models\nfrom sklearn.metrics import mean_squared_error\n\n# to evaluate classification models\nfrom sklearn.metrics import roc_auc_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text\nText is a type of data that can come in different formats; there are so many text processing methods that cannot fit in a single article. Nevertheless, we will review the most popular ones.\n\n<img src=https://habrastorage.org/webt/r7/sq/my/r7sqmyj1nmqmzltaftt40zi7-gw.png width=50%>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = [['i', 'have', 'a', 'cat'], \n        ['he', 'have', 'a', 'dog'], \n        ['he', 'and', 'i', 'have', 'a', 'cat', 'and', 'a', 'dog']]\n\ndictionary = list(enumerate(set(list(reduce(lambda x, y: x + y, texts)))))\n\nprint (dictionary)\n\ndef vectorize(text): \n    vector = np.zeros(len(dictionary)) \n    for i, word in dictionary: \n        num = 0 \n        for w in text: \n            if w == word: \n                num += 1 \n        if num: \n            vector[i] = num \n    return vector\n\nfor t in texts: \n    print(vectorize(t))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tabular\nTabular data is data that is structured into rows, each of which contains information about some thing. Each row contains the same number of cells (although some of these cells may be empty), which provide values of properties of the thing described by the row. In tabular data, cells within the same column provide values for the same property of the things described by each row.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3) How to Engineer Features\n\n#### 1. Imputation\nThe act of replacing missing data with statistical estimates of the missing values. The goal of any imputation technique is to produce a **complete dataset** that can then be then used for machine learning.\n#### 2. Encoding categorical variables \nTransform the strings of categorical variables into numbers, so that we can feed these variables in machine learning algorithms.\n#### Normalisation; Engineering mixed variables, rare values; Remove outliers .... ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4) Missing values\nMissing data occur when __no data__ / __no value__ is stored for a certain observation within a variable. \n\n## Why is data missing?\nThere are 3 mechanisms that lead to missing data, 2 of them involve missing data randomly or almost-randomly, and the third one involves a systematic loss of data.\n\n### Missing Completely at Random, MCAR:\n\nA variable is missing completely at random (MCAR) if the probability of being missing is the same for all the observations. \nWhen data is MCAR, there is absolutely no relationship between the data missing and any other values, observed or missing, within the dataset. In other words, those missing data points are a random subset of the data. There is nothing systematic going on that makes some data more likely to be missing than other.\n\nIf values for observations are missing completely at random, then disregarding those cases would not bias the inferences made.\n\n\n### Missing at Random, MAR: \n\nMAR occurs when there is a systematic relationship between the propensity of missing values and the observed data. In other words, the probability an observation being missing depends only on available information (other variables in the dataset). For example, if men are more likely to disclose their weight than women, weight is MAR. The weight information will be missing at random for those men and women that decided not to disclose their weight, but as men are more prone to disclose it, there will be more missing values for women than for men.\n\nIn a situation like the above, if we decide to proceed with the variable with missing values (in this case weight), we might benefit from including gender to control the bias in weight for the missing observations.\n\n### Missing Not at Random, MNAR: \n\nMissing of values is not at random (MNAR) if their being missing depends on information not recorded in the dataset. In other words, there is a mechanism or a reason why missing values are introduced in the dataset.\n\nExamples:\n\nWhen a financial company asks for bank and identity documents from customers in order to prevent identity fraud, typically, fraudsters impersonating someone else will not upload documents, because they don't have them, precisely because they are fraudsters. Therefore, there is a systematic relationship between the missing documents and the target we want to predict: fraud.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Real Life example: \n\n### Predicting Survival on the Titanic: understanding society behaviour and beliefs\n\nPerhaps one of the most infamous shipwrecks in history, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 people on board. Interestingly, by analysing the probability of survival based on few attributes like gender, age, and social status, we can make very accurate predictions on which passengers would survive. Some groups of people were more likely to survive than others, such as women, children, and the upper-class. Therefore, we can learn about the society priorities and privileges at the time.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# you can determine the total number of missing values using\n# the isnull method plus the sum method on the dataframe\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# alternatively, you can call the mean method after isnull\n# to visualise the percentage of the dataset that \n# contains missing values for each variable\n\ndata.isnull().mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are missing data in the variables Age, Cabin (in which the passenger was travelling) and Embarked, which is the port from which the passenger got into the Titanic.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Missing data Not At Random (MNAR): Systematic missing values\nIn this dataset, both the missing values of the variables Cabin and Age, were introduced systematically. For many of the people who did not survive, the age they had or the cabin they were staying in, could not be established. The people who survived could be asked for that information.\n\nCan we infer this by looking at the data?\n\nIn a situation like this, we could expect a greater number of missing values for people who did not survive.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# we create a dummy variable that indicates whether the value\n# of the variable cabin is missing\n\ndata['AMT_REQ_CREDIT_BUREAU_WEEK_null'] = np.where(data.AMT_REQ_CREDIT_BUREAU_WEEK.isnull(), 1, 0)\n\n# find percentage of null values\ndata.AMT_REQ_CREDIT_BUREAU_WEEK.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and then we evaluate the mean of the missing values in\n# cabin for the people who survived vs the non-survivors.\n\n# group data by Survived vs Non-Survived\n# and find nulls for cabin\ndata.groupby(['TARGET'])['AMT_REQ_CREDIT_BUREAU_WEEK_null'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that the percentage of missing values is higher for people who did not survive (0.87), respect to people that survived (0.60).\nThis finding is aligned with our hypothesis that the data is missing because after the people died, the information could not be retrieved.\n\nHaving said this, to truly underpin whether the data is missing not at random, we would need to get extremely familiar with the way data was collected. Analysing datasets, can only point us in the right direction or help us build assumptions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# we repeat the exercise for the variable age:\n# First we create a dummy variable that indicates\n# whether the value of the variable Age is missing\n\ndata['AMT_REQ_CREDIT_BUREAU_WEEK_null'] = np.where(data.AMT_REQ_CREDIT_BUREAU_WEEK.isnull(), 1, 0)\n\n# and then look at the mean in the different survival groups:\n# there are more NaN for the people who did not survive\ndata.groupby(['TARGET'])['AMT_REQ_CREDIT_BUREAU_WEEK_null'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, we observe an increase in missing data for the people who did not survive the tragedy. The analysis therefore suggests: \n\n**There is a systematic loss of data: people who did not survive tend to have more information missing. Presumably, the method chosen to gather the information, contributes to the generation of these missing data.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Missing data Completely At Random (MCAR)\n\nIn the titanic dataset, there were also missing values for the variable Embarked, let's have a look.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# slice the dataframe to show only those observations\n# with missing value for Embarked\n\ndata[data.OWN_CAR_AGE.isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These 2 women were travelling together, Miss Icard was the maid of Mrs Stone.\n\nA priori, there does not seem to be an indication that the missing information in the variable Embarked is depending on any other variable, and the fact that these women survived, means that they could have been asked for this information.\n\nVery likely this missingness was generated at the time of building the dataset and therefore we could assume that it is completely random. We can assume that the probability of data being missing for these 2 women is the same as the probability for this variable to be missing for any other person. Of course this will be hard, if possible at all, to prove.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 5) Imputation Methods","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Mean and median imputation\nMean/median imputation consists of replacing all occurrences of missing values (NA) within a variable by the mean (if the variable has a Gaussian distribution) or median (if the variable has a skewed distribution).\n\n### Assumptions\n\nMean/median imputation has the assumption that the data are missing completely at random (MCAR). If this is the case, we can think of replacing the NA with the  most frequent occurrence of the variable, which is the mean if the variable has a Gaussian distribution, or the median otherwise.\n\nThe rationale is to replace the population of missing values with the most frequent value, since this is the most likely occurrence.\n\n### Advantages\n\n- Easy to implement\n- Fast way of obtaining complete datasets\n\n### Limitations\n\n- Distortion of original variance\n- Distortion of covariance with remaining variables within the dataset\n\nWhen replacing NA with the mean or median, the variance of the variable will be distorted if the number of NA is big respect to the total number of observations (since the imputed values do not differ from the mean or from each other). Therefore leading to underestimation of the variance.\n\nIn addition, estimates of covariance and correlations with other variables in the dataset may also be affected.  This is because we may be destroying intrinsic correlations since the mean/median that now replace NA will not preserve the relation with the remaining variables.\n\n### Final note\nReplacement of NA with mean/median is widely used in the data science community and in various data science competitions. If the data was missing completely at random, this would be contemplated by the mean imputation, and if it wasn't this would be captured by the additional variable.\n\nIn addition, both methods are extremely straight forward to implement, and therefore are a top choice in data science competitions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at the percentage of NA\n\ndata.isnull().mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Imputation important\n\nImputation should be done over the training set, and then propagated to the test set. This means that the mean/median to be used to fill missing values both in train and test set, should be extracted from the train set only. And this is to avoid overfitting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's separate into training and testing set\n\nX_train, X_test, y_train, y_test = train_test_split(data, data.TARGET, test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.AMT_REQ_CREDIT_BUREAU_YEAR.median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's make a function to create 2 variables from Age:\n# one filling NA with median, and another one filling NA with zeroes\n\ndef impute_na(df, variable, median):\n    df[variable+'_median'] = df[variable].fillna(median)\n    df[variable+'_zero'] = df[variable].fillna(0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"impute_na(X_train, 'AMT_REQ_CREDIT_BUREAU_YEAR', X_train.AMT_REQ_CREDIT_BUREAU_YEAR.median())\nX_train.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"impute_na(X_test, 'AMT_REQ_CREDIT_BUREAU_YEAR', X_train.AMT_REQ_CREDIT_BUREAU_YEAR.median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Mean/median imputation alters the variance of the original distribution of the variable\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can see that the distribution has changed slightly with now more values accumulating towards the median\nfig = plt.figure()\nax = fig.add_subplot(111)\nX_train['AMT_REQ_CREDIT_BUREAU_YEAR'].plot(kind='kde', ax=ax)\nX_train.AMT_REQ_CREDIT_BUREAU_YEAR_median.plot(kind='kde', ax=ax, color='red')\nlines, labels = ax.get_legend_handles_labels()\nax.legend(lines, labels, loc='best')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As mentioned above, the median imputation distorts the original distribution of the variable Age. The transformed variable shows more values around the median value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# filling NA with zeroes creates a peak of population around 0, as expected\nfig = plt.figure()\nax = fig.add_subplot(111)\nX_train['AMT_REQ_CREDIT_BUREAU_YEAR'].plot(kind='kde', ax=ax)\nX_train.AMT_REQ_CREDIT_BUREAU_YEAR_zero.plot(kind='kde', ax=ax, color='red')\nlines, labels = ax.get_legend_handles_labels()\nax.legend(lines, labels, loc='best')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nFilling NA with 0s also distorts the distribution of the original variable, generating an accumulation of values around 0. We will see in the next lecture a method of NA imputation that preserves variable distribution.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Machine learning model performance on different imputation methods\n\n#### Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's compare the performance of Logistic Regression using Age filled with zeros or alternatively the median\n\n# model on NA imputed with zeroes\nlogit = RandomForestClassifier(random_state=44) # c big to avoid regularization\nlogit.fit(X_train[['AMT_REQ_CREDIT_BUREAU_YEAR_zero']], y_train)\nprint('Train set zero imputation')\npred = logit.predict_proba(X_train[['AMT_REQ_CREDIT_BUREAU_YEAR_zero']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\nprint('Test set zero imputation')\npred = logit.predict_proba(X_test[['AMT_REQ_CREDIT_BUREAU_YEAR_zero']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\nprint()\n\n# model on NA imputed with median\nlogit = RandomForestClassifier(random_state=44) # c big to avoid regularization\nlogit.fit(X_train[['AMT_REQ_CREDIT_BUREAU_YEAR_median']], y_train)\nprint('Train set median imputation')\npred = logit.predict_proba(X_train[['AMT_REQ_CREDIT_BUREAU_YEAR_median']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\nprint('Test set median imputation')\npred = logit.predict_proba(X_test[['AMT_REQ_CREDIT_BUREAU_YEAR_median']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that median imputation leads to better performance of the logistic regression. Why?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Children were more likely to survive the catastrophe (0.57 for children vs 0.38 for the entire Titanic). Thus, smaller values of Age are a good indicator of survival.\n\nWhen we replace NA with zeroes, we are masking the predictive power of Age. After zero imputation it looks like children did not have a greater chance of survival, and therefore the model loses predictive power.\n\nOn the other hand, replacing NA with the median, preserves the predictive power of the variable Age, as smaller Age values will favour survival.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Random sample imputation\n\nRandom sampling imputation is in principle similar to mean/median imputation, in the sense that it aims to preserve the statistical parameters of the original variable, for which data is missing.\n\nRandom sampling consist of taking a random observation from the pool of available observations of the variable, and using that randomly extracted value to fill the NA. In Random Sampling one takes as many random observations as missing values are present in the variable.\n\nBy random sampling observations of the variable for those instances where data is available, we guarantee that the mean and standard deviation of the variable are preserved.\n\n\n### Assumptions\n\nRandom sample imputation assumes that the data are missing completely at random (MCAR). If this is the case, it makes sense to substitute the missing values, by values extracted from the original variable distribution. \n\nFrom a probabilistic  point of view, values that are more frequent (like the mean or the median) will be selected more often (because there are more of them to select from), but other less frequent values will be selected as well. Thus, the variance of the variable is preserved. \n\nThe rationale is to replace the population of missing values with a population of values with the same distribution of the variable.\n\n\n### Advantages\n\n- Easy to implement\n- Fast way of obtaining complete datasets\n- Preserves the variance of the variable\n\n### Limitations\n\n- Randomness\n\nRandomness may not seem much of a concern when replacing missing values for data competitions, where the whole batch of missing values is replaced once and then the dataset is scored and that is the end of the problem. However, in business scenarios the situation is very different. \n\nImagine for example the scenario of Mercedes-Benz, where they are trying to predict how long a certain car will be in the garage before it passes all the security tests. Today, they receive a car with missing data in some of the variables, they run the machine learning model to predict how long this car will stay in the garage, the model replaces missing values by a random sample of the variable and then produces an estimate of time. Tomorrow, when they run the same model on the same car, the model will randomly assign values to the missing data, that may or may not be the same as the ones it selected today, therefore, the final estimation of time in the garage, may or may not be the same as the one obtained the day before.\n\nIn addition, imagine also that Mercedes-Benz evaluates 2 different cars that have exactly the same values for all of the variables, and missing values in exactly the same subset of variables. They run the machine learning model for each car, and because the missing data is randomly filled with values, the 2 cars, that are exactly the same, may end up with different estimates of time in the garage. \n\nThis may sound completely trivial and unimportant, however, businesses must follow a variety of regulations, and some of them require that the same treatment be provided to the same situation. So if instead of cars, these were people applying for a loan, or people seeking some disease treatment, the machine learning model would end up providing different solutions to candidates that are otherwise in the same conditions. And this is not fair or acceptable.\n\nIt is still possible to replace missing data by random sample, but these randomness needs to be controlled, so that individuals in the same situation end up with the same scores and therefore solutions.\n\nFinally, another potential limitation of random sampling, similarly to replacing with the mean and median, is that estimates of covariance and correlations with other variables in the dataset may also be washed off by the randomness.\n\n### Final note\n\nReplacement of missing values by random sample, although similar in concept to replacement by the median or mean, is not as widely used in the data science community as the mean/median imputation, presumably because of the element of randomness.\n\nHowever, it is a valid approach, with advantages over mean/median imputation as it preserves the distribution of the variable. And if you are mindful of the element of randomness and account for it somehow, this may as well be your method of choice.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# load the Titanic Dataset with a few variables for demonstration\n\ndata = pd.read_csv('titanic.csv', usecols = ['Age', 'Fare', 'Survived'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's separate into training and testing set\n\nX_train, X_test, y_train, y_test = train_test_split(data, data.TARGET, test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def impute_na(df, variable, median):\n    df[variable+'_median'] = df[variable].fillna(median)\n    df[variable+'_zero'] = df[variable].fillna(0)\n    \n    # random sampling\n    df[variable+'_random'] = df[variable]\n    # extract the random sample to fill the na\n    random_sample = X_train[variable].dropna().sample(df[variable].isnull().sum(), random_state=0)\n    # pandas needs to have the same index in order to merge datasets\n    random_sample.index = df[df[variable].isnull()].index\n    df.loc[df[variable].isnull(), variable+'_random'] = random_sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"impute_na(X_train, 'AMT_REQ_CREDIT_BUREAU_YEAR', X_train.AMT_REQ_CREDIT_BUREAU_YEAR.median())\nX_train.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"impute_na(X_test, 'AMT_REQ_CREDIT_BUREAU_YEAR', X_train.AMT_REQ_CREDIT_BUREAU_YEAR.median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Random sampling preserves the original distribution of the variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can see that the distribution of the variable after filling NA is exactly the same as that one before filling NA\nfig = plt.figure()\nax = fig.add_subplot(111)\nX_train['AMT_REQ_CREDIT_BUREAU_YEAR'].plot(kind='kde', ax=ax)\nX_train.AMT_REQ_CREDIT_BUREAU_YEAR_random.plot(kind='kde', ax=ax, color='red')\nlines, labels = ax.get_legend_handles_labels()\nax.legend(lines, labels, loc='best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's compare the performance of logistic regression on Age NA imputed by zeroes, or median or random sampling\n\nlogit = RandomForestClassifier(random_state=44) # c big to avoid regularization\nlogit.fit(X_train[['AMT_REQ_CREDIT_BUREAU_YEAR_zero']], y_train)\nprint('Train set zero imputation')\npred = logit.predict_proba(X_train[['AMT_REQ_CREDIT_BUREAU_YEAR_zero']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\nprint('Test set zero imputation')\npred = logit.predict_proba(X_test[['AMT_REQ_CREDIT_BUREAU_YEAR_zero']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\nprint()\nlogit = RandomForestClassifier(random_state=44) # c big to avoid regularization\nlogit.fit(X_train[['AMT_REQ_CREDIT_BUREAU_YEAR_median']], y_train)\nprint('Train set median imputation')\npred = logit.predict_proba(X_train[['AMT_REQ_CREDIT_BUREAU_YEAR_median']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\nprint('Test set median imputation')\npred = logit.predict_proba(X_test[['AMT_REQ_CREDIT_BUREAU_YEAR_median']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\nprint()\nlogit = RandomForestClassifier(random_state=44) # c big to avoid regularization\nlogit.fit(X_train[['AMT_REQ_CREDIT_BUREAU_YEAR_random']], y_train)\nprint('Train set random sample imputation')\npred = logit.predict_proba(X_train[['AMT_REQ_CREDIT_BUREAU_YEAR_random']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\nprint('Test set random sample imputation')\npred = logit.predict_proba(X_test[['AMT_REQ_CREDIT_BUREAU_YEAR_random']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that replacing the NA with a random sample of the dataset, does not perform as well as when replacing with the median. However, this is entirely due to randomness. I invite you to change the seed (random_sate) in the impute_na function, then recreate the X_train and X_test, and you will see how the performance of logistic regression varies. In some cases, the performance will be better.\n\nSo if the performance of median imputation vs random sample imputation are similar, which method should I use?\n\nChoosing which imputation method to use, will depend on various things:\n- are NA missing completely at random?\n- do you want to preserve the distribution of the variable?\n- are you willing to accept an element of randomness in your imputation method?\n- are you aiming to win a data competition? or to make business driven decisions?\n\nThere is no 'correct' answer to which imputation method you can use, it rather depends on what you are trying to achieve.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Adding a variable to capture NA\n\nIn previous lectures we studied how to replace missing values by mean/median imputation or by extracting a random sample of the variable for those instances where data is available, and using those values to replace the missing values. We also discussed that these 2 methods assume that the missing data are missing completely at random (MCAR).\n\nSo what if the data are not missing completely at random? By using this procedure, we would be missing important, predictive information.\n\nHow can we prevent that?\n\nWe can capture the importance of missingness by creating an additional variable indicating whether the data was missing for that observation (1) or not (0). The additional variable is a binary variable: it takes only the values 0 and 1, 0 indicating that a value was present for that observation, and 1 indicating that the value was missing for that observation.\n\n\n### Advantages\n\n- Easy to implement\n- Captures the importance of missingess if there is one\n\n### Disadvantages\n\n- Expands the feature space\n\nThis method of imputation will add 1 variable per variable in the dataset with missing values. So if a dataset contains 10 features, and all of them have missing values, we will end up with a dataset with 20 features. The original features where we replaced the missing values by the mean/median (or random sampling), and additional 10 features, indicating for each of the variables, whether the value was missing or not.\n\nThis may not be a problem in datasets with tens to a few hundreds of variables, but if your original dataset contains thousands of variables, by creating an additional variable to indicate NA, you will end up with very big datasets.\n\nIn addition, data tends to be missing for the same observation on multiple variables, so it may also be the case, that many of your added variables will be actually similar to each other.\n\n\n### Final note\n\nTypically, mean/median imputation is done together with adding a variable to capture those observations where the data was missing (see lecture \"Replacing NA with the median/mean\"), thus covering 2 angles: if the data was missing completely at random, this would be contemplated by the mean imputation, and if it wasn't this would be captured by the additional variable.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the Titanic Dataset with a few variables for demonstration\n\ndata = pd.read_csv('../input/home-credit-default-risk/application_train.csv', usecols = ['OWN_CAR_AGE','TARGET'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's separate into training and testing set\n\nX_train, X_test, y_train, y_test = train_test_split(data, data.TARGET, test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create variable indicating missingness\n\nX_train['OWN_CAR_AGE_NA'] = np.where(X_train['OWN_CAR_AGE'].isnull(), 1, 0)\nX_test['OWN_CAR_AGE_NA'] = np.where(X_test['OWN_CAR_AGE'].isnull(), 1, 0)\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's replace the NA with the median value in the training set\nX_train['OWN_CAR_AGE'].fillna(X_train.OWN_CAR_AGE.median(), inplace=True)\nX_test['OWN_CAR_AGE'].fillna(X_train.OWN_CAR_AGE.median(), inplace=True)\n\nX_train.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_train = pd.DataFrame(scaler.fit_transform(X_train))\nX_test = pd.DataFrame(scaler.transform(X_test))\n\nX_train.columns = ['TARGET','OWN_CAR_AGE','OWN_CAR_AGE_NA']\nX_test.columns = ['TARGET','OWN_CAR_AGE','OWN_CAR_AGE_NA']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we compare the models built using Age filled with median, vs Age filled with median + additional\n# variable indicating missingness\n\nlogit = RandomForestClassifier(random_state=44) # c big to avoid regularization\nlogit.fit(X_train[['OWN_CAR_AGE']], y_train)\nprint('Train set')\npred = logit.predict_proba(X_train[['OWN_CAR_AGE']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\nprint('Test set')\npred = logit.predict_proba(X_test[['OWN_CAR_AGE']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\n\nlogit = LogisticRegression(random_state=44, C=1000) # c big to avoid regularization\nlogit.fit(X_train[['OWN_CAR_AGE','OWN_CAR_AGE_NA']], y_train)\nprint('Train set')\npred = logit.predict_proba(X_train[['OWN_CAR_AGE','OWN_CAR_AGE_NA']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\nprint('Test set')\npred = logit.predict_proba(X_test[['OWN_CAR_AGE','OWN_CAR_AGE_NA']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## End of the distribution imputation\n\nOn occasions, one has reasons to suspect that missing values are not missing at random. And if the value is missing, there has to be a reason for it. Therefore, we would like to capture this information.\n\nAdding an additional variable indicating missingness may help with this task (as we discussed in the previous lecture). However, the values are still missing in the original variable, and they need to be replaced if we plan to use the variable in machine learning.\n\nSometimes, we may also not want to increase the feature space by adding a variable to capture missingness.\n\nSo what can we do instead?\n\nWe can replace the NA, by values that are at the far end of the distribution of the variable.\n\nThe rationale is that if the value is missing, it has to be for a reason, therefore, we would not like to replace missing values for the mean and make that observation look like the majority of our observations. Instead, we want to flag that observation as different, and therefore we assign a value that is at the tail of the distribution, where observations are rarely represented in the population.\n\n### Advantages\n\n- Easy to implement\n- Captures the importance of missingess if there is one\n\n### Disadvantages\n\n- Distorts the original distribution of the variable\n- If missingess is not important, it may mask the predictive power of the original variable by distorting its distribution\n- If the number of NA is big, it will mask true outliers in the distribution\n- If the number of NA  is small, the replaced NA may be considered an outlier and pre-processed in a subsequent step of feature engineering\n\n\n### Final note\n\nThis method is used in finance companies. When capturing the financial history of customers, if some of the variables are missing, the company does not like to assume that missingness is random. Therefore, a different treatment is provided to replace them, by placing them at the end of the distribution.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# load the Titanic Dataset with a few variables for demonstration\n\ndata = pd.read_csv('titanic.csv', usecols = ['Age', 'Fare','Survived'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's separate into training and testing set\n\nX_train, X_test, y_train, y_test = train_test_split(data, data.TARGET, test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.OWN_CAR_AGE.hist(bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# far end of the distribution\nX_train.OWN_CAR_AGE.mean()+3*X_train.OWN_CAR_AGE.std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we see that there are a few outliers for Age, according to its distribution\n# these outliers will be masked when we replace NA by values at the far end \n# see below\n\nsns.boxplot('OWN_CAR_AGE', data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def impute_na(df, variable, median, extreme):\n    df[variable+'_far_end'] = df[variable].fillna(extreme)\n    df[variable].fillna(median, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's replace the NA with the median value in the training and testing sets\nimpute_na(X_train, 'OWN_CAR_AGE', X_train.OWN_CAR_AGE.median(), X_train.OWN_CAR_AGE.mean()+3*X_train.OWN_CAR_AGE.std())\nimpute_na(X_test, 'OWN_CAR_AGE', X_train.OWN_CAR_AGE.median(), X_train.OWN_CAR_AGE.mean()+3*X_train.OWN_CAR_AGE.std())\n\nX_train.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we see an accumulation of values around the median for the median imputation\nX_train.OWN_CAR_AGE.hist(bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we see an accumulation of values at the far end for the far end imputation\n\nX_train.OWN_CAR_AGE_far_end.hist(bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# indeed, far end imputation now indicates that there are no outliers in the variable\nsns.boxplot('OWN_CAR_AGE_far_end', data=X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# on the other hand, replacing values by the median, now generates the impression of a higher\n# amount of outliers\n\nsns.boxplot('OWN_CAR_AGE', data=X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we compare the models built using Age filled with median, vs Age filled with values at the far end of the distribution\n# variable indicating missingness\n\nlogit = RandomForestClassifier(random_state=44) # c big to avoid regularization\nlogit.fit(X_train[['OWN_CAR_AGE']], y_train)\nprint('Train set')\npred = logit.predict_proba(X_train[['OWN_CAR_AGE']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\nprint('Test set')\npred = logit.predict_proba(X_test[['OWN_CAR_AGE']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\n\nlogit = RandomForestClassifier(random_state=44) # c big to avoid regularization\nlogit.fit(X_train[['OWN_CAR_AGE_far_end']], y_train)\nprint('Train set')\npred = logit.predict_proba(X_train[['OWN_CAR_AGE_far_end']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\nprint('Test set')\npred = logit.predict_proba(X_test[['OWN_CAR_AGE_far_end']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Arbitrary value imputation\n\nReplacing the NA by artitrary values should be used when there are reasons to believe that the NA are not missing at random. In situations like this, we would not like to replace with the median or the mean, and therefore make the NA look like the majority of our observations.\n\nInstead, we want to flag them. We want to capture the missingness somehow.\n\nIn previous lectures we saw 2 methods to do this:\n\n1) adding an additional binary variable to indicate whether the value is missing (1) or not (0)\n\n2) replacing the NA by a value at a far end of the distribution\n\nHere, I suggest an alternative to option 2, which I have seen in several Kaggle competitions. It consists of replacing the NA by an arbitrary value. Any of your creation, but ideally different from the median/mean/mode, and not within the normal values of the variable.\n\nThe problem consists in deciding which arbitrary value to choose.\n\n### Advantages\n\n- Easy to implement\n- Captures the importance of missingess if there is one\n\n### Disadvantages\n\n- Distorts the original distribution of the variable\n- If missingess is not important, it may mask the predictive power of the original variable by distorting its distribution\n- Hard to decide which value to use\n If the value is outside the distribution it may mask or create outliers\n\n### Final note\n\nWhen variables are captured by third parties, like credit agencies, they place arbitrary numbers already to signal this missingness. So if not common practice in data competitions, it is common practice in real life data collections.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# load the Titanic Dataset with a few variables for demonstration\n\ndata = pd.read_csv('titanic.csv', usecols = ['Age', 'Fare','Survived'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's separate into training and testing set\n\nX_train, X_test, y_train, y_test = train_test_split(data, data.TARGET, test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def impute_na(df, variable):\n    df[variable+'_zero'] = df[variable].fillna(0)\n    df[variable+'_hundred']= df[variable].fillna(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's replace the NA with the median value in the training set\nimpute_na(X_train, 'OWN_CAR_AGE')\nimpute_na(X_test, 'OWN_CAR_AGE')\n\nX_train.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we compare the models built using Age filled with zero, vs Age filled with 100\n\nlogit = RandomForestClassifier(random_state=44) # c big to avoid regularization\nlogit.fit(X_train[['OWN_CAR_AGE_zero']], y_train)\nprint('Train set')\npred = logit.predict_proba(X_train[['OWN_CAR_AGE_zero']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\nprint('Test set')\npred = logit.predict_proba(X_test[['OWN_CAR_AGE_zero']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\n\nlogit = RandomForestClassifier(random_state=44) # c big to avoid regularization\nlogit.fit(X_train[['OWN_CAR_AGE_hundred']], y_train)\nprint('Train set')\npred = logit.predict_proba(X_train[['OWN_CAR_AGE_hundred']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\nprint('Test set')\npred = logit.predict_proba(X_test[['OWN_CAR_AGE_hundred']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that replacing NA with 100 makes the models perform better than replacing NA with 0. This is, if you remember from the lecture \"Replacing NA by mean or median\" because children were more likely to survive than adults. Then filling NA with zeroes, distorts this relation and makes the models loose predictive power. See below for a re-cap.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Final notes\nThe arbitrary value has to be determined for each variable specifically. For example, for this dataset, the choice of replacing NA in age by 0 or 100 are valid, because none of those values are frequent in the original distribution of the variable, and they lie at the tails of the distribution.\n\nHowever, if we were to replace NA in fare, those values are not good any more, because we can see that fare can take values of up to 500. So we might want to consider using 500 or 1000 to replace NA instead of 100.\n\nAs you can see this is totally arbitrary. And yet, it is used in the industry.\n\nTypical values chose by companies are -9999 or 9999, or similar.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 6) Engineer labels of categorical variables\n\nIn this section, I will describe a variety of methods to transform the strings of categorical variables into numbers, so that we can feed these variables in machine learning algorithms.\n\n## One Hot Encoding\n\nOne hot encoding, consists of replacing the categorical variable by different boolean variables, which take value 0 or 1, to indicate whether or not a certain category / label of the variable was present for that observation.\n\nEach one of the boolean variables are also known as **dummy variables** or binary variables.\n\nFor example, from the categorical variable \"Gender\", with labels 'female' and 'male', we can generate the boolean variable \"female\", which takes 1 if the person is female or 0 otherwise. We can also generate the variable male, which takes 1 if the person is \"male\" and 0 otherwise. \n\n### Advantages\n\n- Straightforward to implement\n- Makes no assumption\n- Keeps all the information of the categorical variable\n\n### Disadvantages\n\n- Does not add any information that may make the variable more predictive\n- If the variable has loads of categories, then OHE increases the feature space dramatically\n\n### Notes\n\nIf our datasets have a few multi-label variables, we will end up very soon with datasets with thousands of columns or more. And this may make training of our algorithms slow.\n\nIn addition, many of these dummy variables may be similar to each other, since it is not unusual for 2 or more variables to share the same combinations of 1 and 0s.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndata = pd.read_csv('../input/home-credit-default-risk/application_train.csv', usecols=['CODE_GENDER'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encoding\n\npd.get_dummies(data).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for better visualisation\npd.concat([data, pd.get_dummies(data)], axis=1).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's make a copy of the dataset, in which we encode the categorical variables using OHE\n\ndata_OHE = pd.concat([data[['TARGET', 'OWN_CAR_AGE', 'CNT_CHILDREN']], # numerical variables \n                      pd.get_dummies(data.CODE_GENDER),   # binary categorical variable\n                      pd.get_dummies(data.FLAG_OWN_REALTY)],  # k categories in categorical\n                    axis=1)\n\ndata_OHE.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and now let's separate into train and test set\n\nX_train, X_test, y_train, y_test = train_test_split(data_OHE[['OWN_CAR_AGE', 'CNT_CHILDREN', 'F', 'M', 'XNA', 'N', 'Y']].fillna(0),\n                                                    data_OHE.TARGET,\n                                                    test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def impute_na(df, variable, extreme):\n    df[variable].fillna(extreme, inplace=True)\n    \nimpute_na(X_train, 'OWN_CAR_AGE', X_train.OWN_CAR_AGE.mean()+3*X_train.OWN_CAR_AGE.std())\nimpute_na(X_test, 'OWN_CAR_AGE', X_train.OWN_CAR_AGE.mean()+3*X_train.OWN_CAR_AGE.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and finally a logistic regression\n\nlogit = RandomForestClassifier(random_state=44)\nlogit.fit(X_train, y_train)\nprint('Train set')\npred = logit.predict_proba(X_train)\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\nprint('Test set')\npred = logit.predict_proba(X_test)\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Count or frequency encoding\n\nAnother way to refer to variables that have a multitude of categories, is to call them variables with **high cardinality**.\n\nWe observed in the previous lecture, that if a categorical variable contains multiple labels, then by re-encoding them using one hot encoding, we will expand the feature space dramatically.\n\nOne approach that is heavily used in Kaggle competitions, is to replace each label of the categorical variable by the count, this is the amount of times each label appears in the dataset. Or the frequency, this is the percentage of observations within that category. The 2 are equivalent.\n\nThere is not any rationale behind this transformation, other than its simplicity.\n\n### Advantages\n\n- Simple\n- Does not expand the feature space\n\n### Disadvantages\n\n-  If 2 labels appear the same amount of times in the dataset, that is, contain the same number of observations, they will be merged: may loose valuable information\n- Adds somewhat arbitrary numbers, and therefore weights to the different labels, that may not be related to their predictive power","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Important\n\nWhen doing count transformation of categorical variables, it is important to calculate the count (or frequency = count/total observations) **over the training set**, and then use those numbers to replace the labels in the test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data[['OWN_CAR_AGE', 'CNT_CHILDREN']].fillna(0),\n                                                    data.TARGET,\n                                                    test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# And now let's replace each label in X2 by its count\n\n# first we make a dictionary that maps each label to the counts\nX_frequency_map = X_train.OWN_CAR_AGE.value_counts().to_dict()\n\n# and now we replace X2 labels both in train and test set with the same map\nX_train.Sex = X_train.OWN_CAR_AGE.map(X_frequency_map)\nX_test.Sex = X_test.OWN_CAR_AGE.map(X_frequency_map)\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# And now let's replace each label in X2 by its count\n\n# first we make a dictionary that maps each label to the counts\nX_frequency_map = X_train.CNT_CHILDREN.value_counts().to_dict()\n\n# and now we replace X2 labels both in train and test set with the same map\nX_train.Embarked = X_train.CNT_CHILDREN.map(X_frequency_map)\nX_test.Embarked = X_test.CNT_CHILDREN.map(X_frequency_map)\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and finally a logistic regression\n\nlogit = RandomForestClassifier(random_state=44)\nlogit.fit(X_train, y_train)\nprint('Train set')\npred = logit.predict_proba(X_train)\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\nprint('Test set')\npred = logit.predict_proba(X_test)\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Note\n\nI want you to keep in mind something important:\n\nIf a category is present in the test set, that was not present in the train set, this method will generate missing data in the test set. This is why it is extremely important to handle rare categories.\n\nThen we can combine rare label replacement plus categorical encoding with counts like this: we may choose to replace the 10 most frequent labels by their count, and then group all the other labels under one label (for example \"Rare\"), and replace \"Rare\" by its count.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Target guided ordinal encoding\n\nIn the previous lectures in this section on how to engineer the labels of categorical variables, we learnt how to convert a label into a number, by using one hot encoding or replacing by frequency or counts. These methods are simple, make no assumptions and work generally well in different scenarios.\n\nThere are however methods that allow us to capture information while pre-processing the labels of categorical variables. These methods include:\n\n- Ordering the labels according to the target\n- Replacing labels by the risk (of the target)\n- Replacing the labels by the joint probability of the target being 1 or 0\n- Weight of evidence.\n\n### Advantages\n\n- Capture information within the label, therefore rendering more predictive features\n- Create a monotonic relationship between the variable and the target\n- Do not expand the feature space\n\n### Disadvantage\n\n- Prone to cause over-fitting\n\n\n### Ordering  labels according to the target\n\nOrdering the labels according to the target means assigning a number to the label, but this numbering, this ordering, is informed by the mean of the target within the label.\n\nBriefly, we calculate the mean of the target for each label/category, then we order the labels according to these mean from smallest to biggest, and we number them accordingly.\n\nSee the example below:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's load again the titanic dataset\n\ndata = pd.read_csv('../input/home-credit-default-risk/application_train.csv', usecols=['OWN_CAR_AGE', 'TARGET'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's first fill NA values with an additional label\n\ndata.OWN_CAR_AGE.fillna('OWN_CAR_AGE', inplace=True)\ndata['OWN_CAR_AGE'] = data['OWN_CAR_AGE'].astype(str).str[0]\ndata.OWN_CAR_AGE.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's separate into train and test set\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data[['OWN_CAR_AGE', 'TARGET']], data.TARGET, test_size=0.3, random_state=0)\n\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we order the labels according to the mean target value\n\nX_train.groupby(['OWN_CAR_AGE'])['TARGET'].mean().sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and now we create a dictionary that maps each label to the number\nordered_labels = X_train.groupby(['OWN_CAR_AGE'])['TARGET'].mean().sort_values().index\nordinal_label = {k:i for i, k in enumerate(ordered_labels, 0)} \nordinal_label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This method assigned the number 0 to T, the category with the lowest target mean, and 8 to B, the category with the highest target mean.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace the labels with the ordered numbers\n# both in train and test set (note that we created the dictionary only using the training set)\n\nX_train['OWN_CAR_AGE_ordered'] = X_train.OWN_CAR_AGE.map(ordinal_label)\nX_test['OWN_CAR_AGE_ordered'] = X_test.OWN_CAR_AGE.map(ordinal_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the results\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's inspect the newly created monotonic relationship with the target\n\n#first we plot the original variable for comparison, there is no monotonic relationship\n\nfig = plt.figure()\nfig = X_train.groupby(['OWN_CAR_AGE'])['TARGET'].mean().plot()\nfig.set_title('Normal relationship between variable and target')\nfig.set_ylabel('TARGET')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the transformed result: the monotonic variable\n\nfig = plt.figure()\nfig = X_train.groupby(['OWN_CAR_AGE_ordered'])['TARGET'].mean().plot()\nfig.set_title('Monotonic relationship between variable and target')\nfig.set_ylabel('Survived')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's load again the titanic dataset\n\ndata = pd.read_csv('../input/home-credit-default-risk/application_train.csv', usecols=['OWN_CAR_AGE', 'TARGET'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's first fill NA values with an additional label\n\ndata.OWN_CAR_AGE.fillna('Missing', inplace=True)\ndata['OWN_CAR_AGE'] = data['OWN_CAR_AGE'].astype(str).str[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's separate into training and testing set\n\nX_train, X_test, y_train, y_test = train_test_split(data[['OWN_CAR_AGE', 'TARGET']], data.TARGET, test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's calculate the target frequency for each label\n\nX_train.groupby(['OWN_CAR_AGE'])['TARGET'].mean()\nordered_labels = X_train.groupby(['OWN_CAR_AGE'])['TARGET'].mean().to_dict()\nX_train['OWN_CAR_AGE_ordered'] = X_train.OWN_CAR_AGE.map(ordered_labels)\nX_test['OWN_CAR_AGE_ordered'] = X_test.OWN_CAR_AGE.map(ordered_labels)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the original variable\n\nfig = plt.figure()\nfig = X_train.groupby(['OWN_CAR_AGE'])['TARGET'].mean().plot()\nfig.set_title('Normal relationship between variable and target')\nfig.set_ylabel('Survived')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the transformed result: the monotonic variable\n\nfig = plt.figure()\nfig = X_train.groupby(['OWN_CAR_AGE_ordered'])['TARGET'].mean().plot()\nfig.set_title('Monotonic relationship between variable and target')\nfig.set_ylabel('Survived')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Probability ratio encoding\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# let's load again the titanic dataset\n\ndata = pd.read_csv('../input/home-credit-default-risk/application_train.csv', usecols=['OWN_CAR_AGE', 'TARGET'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's first fill NA values with an additional label\n\ndata.OWN_CAR_AGE.fillna('Missing', inplace=True)\ndata['OWN_CAR_AGE'] = data['OWN_CAR_AGE'].astype(str).str[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's separate into training and testing set\n\nX_train, X_test, y_train, y_test = train_test_split(data[['OWN_CAR_AGE', 'TARGET']],\n                                                    data.TARGET, test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now let's  calculate the probability of target = 0 (people who did not survive)\nprob_df = X_train.groupby(['OWN_CAR_AGE'])['TARGET'].mean()\nprob_df = pd.DataFrame(prob_df)\nprob_df['Difficult'] = 1-prob_df.TARGET\nprob_df['ratio'] = prob_df.TARGET/prob_df.Difficult\nordered_labels = prob_df['ratio'].to_dict()\nX_train['OWN_CAR_AGE_ordered'] = X_train.OWN_CAR_AGE.map(ordered_labels)\nX_test['OWN_CAR_AGE_ordered'] = X_test.OWN_CAR_AGE.map(ordered_labels)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the original variable\n\nfig = plt.figure()\nfig = X_train.groupby(['OWN_CAR_AGE'])['TARGET'].mean().plot()\nfig.set_title('Normal relationship between variable and target')\nfig.set_ylabel('TARGET')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the transformed result: the monotonic variable\n\nfig = plt.figure()\nfig = X_train.groupby(['OWN_CAR_AGE_ordered'])['TARGET'].mean().plot()\nfig.set_title('Monotonic relationship between variable and target')\nfig.set_ylabel('TARGET')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Weight  of evidence\n\nWeight of Evidence (WoE) was developed primarily for the credit and financial industries to help build more predictive models to evaluate the risk of loan default. That is, to predict how likely the money lent to a person or institution is to be lost. Thus, Weight of Evidence is a measure of the \"strength” of a grouping technique to separate good and bad risk (default). \n\nIt is computed from the basic odds ratio: ln( (Proportion of Good Credit Outcomes) / (Proportion of Bad Credit Outcomes))\n\nWoE will be 0 if the P(Goods) / P(Bads) = 1. That is, if the outcome is random for that group. If P(Bads) > P(Goods) the odds ratio will be < 1 and the WoE will be < 0; if, on the other hand, P(Goods) > P(Bads) in a group, then WoE > 0.\n\nWoE is well suited for Logistic Regression, because the Logit transformation is simply the log of the odds, i.e., ln(P(Goods)/P(Bads)). Therefore, by using WoE-coded predictors in logistic regression, the predictors are all prepared and coded to the same scale, and the parameters in the linear logistic regression equation can be directly compared.\n\nThe WoE transformation has three advantages:\n\n- It establishes a monotonic relationship to the dependent variable.\n- It orders the categories on a \"logistic\" scale which is natural for logistic regression\n- The transformed variables, can then be compared because they are on the same scale. Therefore, it is possible to determine which one is more predictive.\n\nThe WoE also has three drawbacks:\n\n- May incur in loss of information (variation) due to binning to few categories (we will discuss this further in the discretisation section)\n- It does not take into account correlation between independent variables\n- Prone to cause over-fitting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's load again the titanic dataset\n\ndata = pd.read_csv('../input/home-credit-default-risk/application_train.csv', usecols=['OWN_CAR_AGE', 'TARGET'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's first fill NA values with an additional label\n\ndata.OWN_CAR_AGE.fillna('Missing', inplace=True)\ndata['OWN_CAR_AGE'] = data['OWN_CAR_AGE'].astype(str).str[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's divide into train and test set\n\nX_train, X_test, y_train, y_test = train_test_split(data[['OWN_CAR_AGE', 'TARGET']], data.TARGET, test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and now the probability of target = 0 \n# and we add it to the dataframe\n\nprob_df = X_train.groupby(['OWN_CAR_AGE'])['TARGET'].mean()\nprob_df = pd.DataFrame(prob_df)\nprob_df['Difficult'] = 1-prob_df.TARGET\n# since the log of zero is not defined, let's set this number to something small and non-zero\n\nprob_df.loc[prob_df.TARGET == 0, 'TARGET'] = 0.00001\nprob_df['WoE'] = np.log(prob_df.TARGET/prob_df.Difficult)\nordered_labels = prob_df['WoE'].to_dict()\n\nX_train['OWN_CAR_AGE_ordered'] = X_train.OWN_CAR_AGE.map(ordered_labels)\nX_test['OWN_CAR_AGE_ordered'] = X_test.OWN_CAR_AGE.map(ordered_labels)\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the original variable\n\nfig = plt.figure()\nfig = X_train.groupby(['OWN_CAR_AGE'])['TARGET'].mean().plot()\nfig.set_title('Normal relationship between variable and target')\nfig.set_ylabel('Survived')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the transformed result: the monotonic variable\n\nfig = plt.figure()\nfig = X_train.groupby(['OWN_CAR_AGE_ordered'])['TARGET'].mean().plot()\nfig.set_title('Monotonic relationship between variable and target')\nfig.set_ylabel('Survived')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}