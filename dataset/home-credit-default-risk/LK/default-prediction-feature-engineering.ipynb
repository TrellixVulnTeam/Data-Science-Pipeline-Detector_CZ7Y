{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n# Memory management\nimport gc \n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as plt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f6561f9053942e9f340998ffc90fe8064858e81","collapsed":true},"cell_type":"code","source":"dataset= pd.read_csv('../input/application_train.csv')\nPath='../input/' \n\n##Initial data understanding \n\n##test dataset which is to be predicted\ntest=pd.read_csv(\"../input/application_test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47b59392a8c521ef2342768dceb58da29cbe6794"},"cell_type":"code","source":"#Checking type of the variables REGION_RATING_CLIENT REGION_RATING_CLIENT_W_CITY\ncheck =dataset[[\"REGION_RATING_CLIENT\",\"REGION_RATING_CLIENT_W_CITY\"]]\nfor col in check:\n    if check[col].dtype != 'object':\n        print (col)\ndataset.REGION_RATING_CLIENT.dtype   \n\ndataset[\"REGION_RATING_CLIENT\"] = dataset[\"REGION_RATING_CLIENT\"].astype('object')\ndataset[\"REGION_RATING_CLIENT_W_CITY\"] = dataset[\"REGION_RATING_CLIENT_W_CITY\"].astype('object')\n\ntest[\"REGION_RATING_CLIENT\"] = test[\"REGION_RATING_CLIENT\"].astype('object')\ntest[\"REGION_RATING_CLIENT_W_CITY\"] = test[\"REGION_RATING_CLIENT_W_CITY\"].astype('object')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b9a20c183b841942a37e343eeb5bac2923ce760"},"cell_type":"code","source":"dataset.groupby(['REGION_RATING_CLIENT']).TARGET.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99bf10bb12c05247a1fcdef9fbd13849a4a77cdb","collapsed":true},"cell_type":"code","source":"dataset.groupby(['REGION_RATING_CLIENT_W_CITY']).TARGET.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b17b98cb5e1741cf08b7abb6ced16d18997ddaf2"},"cell_type":"code","source":"#reducing the unique values in occupation by grouping by skill level. This grouping can differ based on more information about each occupation\ndataset['NAME_TYPE_SUITE'].replace({'Children':'Family',\n                                    'Group of people':'Other',\n                                    'Other_A':'Other',\n                                    'Other_B':'Other',\n                                    'Spouse, partner':'Family'},inplace=True)\n\ntest['NAME_TYPE_SUITE'].replace({'Children':'Family',\n                                    'Group of people':'Other',\n                                    'Other_A':'Other',\n                                    'Other_B':'Other',\n                                    'Spouse, partner':'Family'},inplace=True)\n \ndataset['NAME_EDUCATION_TYPE'].replace({'Academic degree':'Higher education '},inplace=True)\ntest['NAME_EDUCATION_TYPE'].replace({'Academic degree':'Higher education '},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ecf05f8e4f362563c578c01596d8e0617a7aa2eb"},"cell_type":"code","source":"#reducing the unique values in occupation by grouping by skill level. This grouping can differ based on more information about each occupation\ndataset['OCCUPATION_TYPE'].replace({'High skill tech staff':'High_Skill',\n                                    'Managers':'High_Skill',\n                                    'Accountants':'High_Med_Skill',\n                                    'HR staff':'High_Med_Skill',\n                                    'Core staff':'Med_Skill',\n                                   'Cooking staff':'Med_Skill',\n                                    'Realty agents':'Med_Skill',\n                                    'Sales staff':'Med_Skill',\n                                    'IT staff':'High_Med_Skill',\n                                    'Medicine staff':'High_Med_Skill',\n                                    'Secretaries':'Med_Skill',\n                                    'Security staff':'Med_Skill',\n                                    'Cleaning staff':'Low_Skill',\n                                      'Laborers':'Low_Skill',\n                                      'Low-skill Laborers':'Low_Skill',\n                                      'Cleaning staff':'Low_Skill',\n                                    'Waiters/barmen staff':'Low_Skill',\n                                    'Private service staff':'Low_Skill',\n                                    'Drivers':'Med_Skill'\n                                   },inplace=True)\ntest['OCCUPATION_TYPE'].replace({'High skill tech staff':'High_Skill',\n                                    'Managers':'High_Skill',\n                                    'Accountants':'High_Med_Skill',\n                                    'HR staff':'High_Med_Skill',\n                                    'Core staff':'Med_Skill',\n                                   'Cooking staff':'Med_Skill',\n                                    'Realty agents':'Med_Skill',\n                                    'Sales staff':'Med_Skill',\n                                    'IT staff':'High_Med_Skill',\n                                    'Medicine staff':'High_Med_Skill',\n                                    'Secretaries':'Med_Skill',\n                                    'Security staff':'Med_Skill',\n                                    'Cleaning staff':'Low_Skill',\n                                      'Laborers':'Low_Skill',\n                                      'Low-skill Laborers':'Low_Skill',\n                                      'Cleaning staff':'Low_Skill',\n                                    'Waiters/barmen staff':'Low_Skill',\n                                    'Private service staff':'Low_Skill',\n                                    'Drivers':'Med_Skill'\n                                   },inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"efa37560f04cc38fc70888764a48f0fd020aa07c"},"cell_type":"code","source":"#grouping\ndataset['NAME_INCOME_TYPE'].replace({'Businessman':'Other','Student':'Other','Maternity leave':'Other'},inplace=True)\ntest['NAME_INCOME_TYPE'].replace({'Businessman':'Other','Student':'Other','Maternity leave':'Other'},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16c1839284022a6b03175c18a2cf77d55cad23b2"},"cell_type":"code","source":"app_train=dataset.copy()\napp_test=test.copy()\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in dataset:\n    if dataset[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(dataset[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(dataset[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(dataset[col])\n            app_test[col] = le.transform(test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7bdb06852905bce24600a824943045b8a1764ec"},"cell_type":"code","source":"gc.enable()\ndel dataset, test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c85a36b3475e16904f3d03f56f0be1330139305","collapsed":true},"cell_type":"code","source":"app_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dd1a5b1c0fd49fdeca3cff5be66e3f8efb841c51"},"cell_type":"code","source":"app_train= pd.get_dummies(app_train)\napp_test= pd.get_dummies(app_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dff248833102639f9e97944d5eba4816c29cd46","collapsed":true},"cell_type":"code","source":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n#app_train['DAYS_EMPLOYED'].replace({0: 1}, inplace = True)\napp_train.fillna(app_train.median(),inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb5ac41e5e15551f4118415cbe6623c3456b5bfb","collapsed":true},"cell_type":"code","source":"app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\n# Replace the anomalous values with nan\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n#app_test['DAYS_EMPLOYED'].replace({0: 1}, inplace = True)\napp_test.fillna(app_test.median(),inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c31f2417d43c014dfa70a72266af9f029906ad96","collapsed":true},"cell_type":"code","source":"app_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95c454b2afcdfb331c5e5c76bf20420e7aefd7dc","collapsed":true},"cell_type":"code","source":"# Create an id change flag column\napp_train['ID_CHANGE_LST3M'] = app_train[\"DAYS_ID_PUBLISH\"] <=-90\n# Create an reg change flag column\napp_train['REG_CHANGE_LST3M'] = app_train[\"DAYS_REGISTRATION\"] <=-90\n\n# Create an reg change flag column\napp_test['ID_CHANGE_LST3M'] = app_test[\"DAYS_ID_PUBLISH\"] <=-90\napp_test['REG_CHANGE_LST3M'] = app_test[\"DAYS_REGISTRATION\"] <=-90\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3857dc139c88e9f466463730a19eff0e088320e"},"cell_type":"code","source":"#replace all  NaN in the var_list with zero\nVar_List=('OBS_30_CNT_SOCIAL_CIRCLE','OBS_30_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE','DEF_60_CNT_SOCIAL_CIRCLE',\n        'AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK',\n         'AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_QRT','AMT_REQ_CREDIT_BUREAU_YEAR')\ndef missing_val_replace(data,Var_List):\n    for col in data:\n        for i in Var_List:\n            if col==i:\n                data[col].fillna(0)\n                print (col)\n    return data\napp_train=missing_val_replace(app_train,Var_List) \n#replace all other NaN with median values\n\napp_test=missing_val_replace(app_test,Var_List) \n#replace all other NaN with median values\napp_train=app_train.fillna(app_train.median)\napp_test=app_test.fillna(app_test.median)\n\napp_train.dtypes.value_counts()\napp_test.dtypes.value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f26d4a99dceef5d52126bacd00637b809867e0c4"},"cell_type":"code","source":"app_train['CREDIT_INCOME_PERCENT'] = app_train['AMT_CREDIT'] / app_train['AMT_INCOME_TOTAL']\napp_train['ANNUITY_INCOME_PERCENT'] = app_train['AMT_ANNUITY'] / app_train['AMT_INCOME_TOTAL']\napp_train['CREDIT_TERM'] = app_train['AMT_ANNUITY'] / app_train['AMT_CREDIT']\napp_train['DAYS_EMPLOYED_PERCENT'] = app_train['DAYS_EMPLOYED'] / app_train['DAYS_BIRTH']\napp_train['INCOME_AGE_RATIO'] = app_train['AMT_INCOME_TOTAL'] / app_train['DAYS_BIRTH']\n\napp_train['AMT_CR_AGE_RATIO'] = app_train['AMT_CREDIT'] / app_train['DAYS_BIRTH']\napp_train['CREDIT_AMT_GDS_PERCENT'] = app_train['AMT_CREDIT'] / app_train['AMT_GOODS_PRICE']\napp_train['AMT_GDS_INCOME_PERCENT'] = app_train['AMT_GOODS_PRICE'] / app_train['AMT_INCOME_TOTAL']\n#app_train['AMT_CR_EMP_RATIO'] = app_train['AMT_CREDIT'] / app_train['DAYS_EMPLOYED']\n\napp_test['CREDIT_INCOME_PERCENT'] = app_test['AMT_CREDIT'] / app_test['AMT_INCOME_TOTAL']\napp_test['ANNUITY_INCOME_PERCENT'] = app_test['AMT_ANNUITY'] / app_test['AMT_INCOME_TOTAL']\napp_test['CREDIT_TERM'] = app_test['AMT_ANNUITY'] / app_test['AMT_CREDIT']\napp_test['DAYS_EMPLOYED_PERCENT'] = app_test['DAYS_EMPLOYED'] / app_test['DAYS_BIRTH']\n\napp_test['INCOME_AGE_RATIO'] = app_test['AMT_INCOME_TOTAL'] / app_test['DAYS_BIRTH']\napp_test['AMT_CR_AGE_RATIO'] = app_test['AMT_CREDIT'] / app_test['DAYS_BIRTH']\napp_test['CREDIT_AMT_GDS_PERCENT'] = app_test['AMT_CREDIT'] / app_test['AMT_GOODS_PRICE']\napp_test['AMT_GDS_INCOME_PERCENT'] = app_test['AMT_GOODS_PRICE'] / app_test['AMT_INCOME_TOTAL']\n#app_test['AMT_CR_EMP_RATIO'] = app_test['AMT_CREDIT'] / app_test['DAYS_EMPLOYED']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b95947e803f5eca4fd98eecdb04ff52246ae8f5","collapsed":true},"cell_type":"markdown","source":"**Bureau data******"},{"metadata":{"trusted":true,"_uuid":"92a5c966dad085ddd6915088c0ed8f6b24f29401"},"cell_type":"code","source":"#bureau.csv\npath=\"../input/\"\nbureau=pd.read_csv(\"../input/bureau.csv\")\nbureau.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d079e2f534663c8137c4859c11974f68a58e0042"},"cell_type":"markdown","source":"**Function to summarize categorical variable******"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"952b7ea1f551186a8f34bffdc98f76b1b211ce61"},"cell_type":"code","source":"def count_categorical(df, group_var, df_name):\n    \"\"\"Computes counts and normalized counts for each observation\n    of `group_var` of each unique category in every categorical variable\n    \n    Parameters\n    --------\n    df : dataframe \n        The dataframe to calculate the value counts for.\n        \n    group_var : string\n        The variable by which to group the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    df_name : string\n        Variable added to the front of column names to keep track of columns\n\n    \n    Return\n    --------\n    categorical : dataframe\n        A dataframe with counts and normalized counts of each unique category in every categorical variable\n        with one row for every unique value of the `group_var`.\n        \n    \"\"\"\n    \n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes(include=['object']))\n\n    # Make sure to put the identifying id on the column\n    categorical[group_var] = df[group_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['count', 'count_norm']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    return categorical","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4f1344b062fbf92b642be653dd89571e600a5a4"},"cell_type":"markdown","source":"**Function to summarize numeric variables**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1194291c439ab3f87e1f528fd271a945ec7378f2"},"cell_type":"code","source":"def agg_numeric(df, group_var, df_name):\n    \"\"\"Aggregates the numeric values in a dataframe. This can\n    be used to create features for each instance of the grouping variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the dataframe to calculate the statistics on\n        group_var (string): \n            the variable by which to group df\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated for \n            all numeric columns. Each instance of the grouping variable will have \n            the statistics (mean, min, max, sum; currently supported) calculated. \n            The columns are also renamed to keep track of features created.\n    \n    \"\"\"\n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != group_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    group_ids = df[group_var]\n    numeric_df = df.select_dtypes(include=['number'])\n    numeric_df[group_var] = group_ids\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n\n    # Need to create new column names\n    columns = [group_var]\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        # Skip the grouping variable\n        if var != group_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1][:-1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n\n    agg.columns = columns\n    return agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff7043966396a48902bd7193fef03bab7133fb36"},"cell_type":"code","source":"# Counts of each type of caterigical variable bureau\nbureau_counts = count_categorical(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbcec5754b98bd8d8aa08c3c1a99713c8ac65aaf"},"cell_type":"code","source":"bureau_agg = agg_numeric(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bd4e92e2f262cf5dc26bd54951bf59c7f1dc47b","collapsed":true},"cell_type":"code","source":"bureau_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dcad0838ed65283cb49e60b164bbacfbdb18c675"},"cell_type":"markdown","source":"**Applying Operations to another dataframe bureau_balance**\nWe will now turn to the bureau balance dataframe. This dataframe has monthly information about each client's previous loan(s) with other financial institutions. Instead of grouping this dataframe by the SK_ID_CURR which is the client id, we will first group the dataframe by the SK_ID_BUREAU which is the id of the previous loan. This will give us one row of the dataframe for each loan. Then, we can group by the SK_ID_CURR and calculate the aggregations across the loans of each client. The final result will be a dataframe with one row for each client, with stats calculated for their loans."},{"metadata":{"trusted":true,"_uuid":"09473ee8dd52b787e988a946f9968dc3bb5b36c7"},"cell_type":"code","source":"\n# Read in bureau balance\nbureau_balance = pd.read_csv('../input/bureau_balance.csv')\nbureau_balance.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"702ac4b54aa95d2303da440c23f9d89d24895952"},"cell_type":"code","source":"# Calculate summary statistics for each `SK_ID_BUREAU` \nbureau_balance_agg = agg_numeric(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d2afeae9704b72b2612fc60b96ab9064468ebed"},"cell_type":"markdown","source":"The above dataframes have the calculations done on each loan. Now we need to aggregate these for each client. We can do this by merging the dataframes together first and then since all the variables are numeric, we just need to aggregate the statistics again, this time grouping by the SK_ID_CURR."},{"metadata":{"trusted":true,"_uuid":"283cad38e30213170a3710606ead67abb474dc07"},"cell_type":"code","source":"# Counts of each type of status for each previous loan\nbureau_balance_counts = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3f7c578e6a13b5d157d6541818577678e9bd589","collapsed":true},"cell_type":"code","source":"app_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf1eda770f1584ca70109c19610b2004edea9e7d"},"cell_type":"code","source":"# Dataframe grouped by the loan\nbureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n\n# Merge to include the SK_ID_CURR\nbureau_by_loan = bureau_by_loan.merge(bureau[['SK_ID_BUREAU', 'SK_ID_CURR']], on = 'SK_ID_BUREAU', how = 'left')\n\nbureau_by_loan.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4c3e721b2d1a90440de5e0be47c39ae2c339bc8"},"cell_type":"code","source":"#summary by cust id\nbureau_balance_by_client = agg_numeric(bureau_by_loan.drop(['SK_ID_BUREAU'],1), group_var = 'SK_ID_CURR', df_name = 'client')\nbureau_balance_by_client.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eec0206489c472a1ca583d8dc84cf1f1b5cba7f3"},"cell_type":"code","source":"   # Clean up memory\ngc.enable()\ndel bureau_by_loan, bureau_balance, bureau\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d8b031623074a6af0c2be85aa5f20e190a19cb3"},"cell_type":"markdown","source":"**previous_application data**"},{"metadata":{"trusted":true,"_uuid":"05b094bc6c66c0e7aecdf29398d6e325883b4cb1"},"cell_type":"code","source":"# Read in previous_application \nprevious_application = pd.read_csv('../input/previous_application.csv')\nprevious_application.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d0e3c30aca95ebbe2d378a485e6be897e975203"},"cell_type":"code","source":"#histogram of price by condition and brand\n# Histogram\n# bins = number of bar in figure\nprevious_application[['DAYS_LAST_DUE','DAYS_TERMINATION','DAYS_LAST_DUE_1ST_VERSION','DAYS_FIRST_DUE','DAYS_FIRST_DRAWING']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1a0ace651bd9c8fea27ce9ae5c079a8bb44bb516"},"cell_type":"code","source":"# Create an anomalous flag column\nprevious_application['DAYS_LAST_DUE_ANOMALY_FLG'] = previous_application[\"DAYS_LAST_DUE\"] == 365243\nprevious_application['DAYS_TERMINATION_ANOMALY_FLG'] = previous_application[\"DAYS_TERMINATION\"] == 365243\nprevious_application['DAYS_LAST_DUE_1ST_VERSION_ANOMALY_FLG'] = previous_application[\"DAYS_LAST_DUE_1ST_VERSION\"] == 365243\nprevious_application['DAYS_FIRST_DUE_ANOMALY_FLG'] = previous_application[\"DAYS_FIRST_DUE\"] == 365243\nprevious_application['DAYS_FIRST_DRAWING_ANOMALY_FLG'] = previous_application[\"DAYS_FIRST_DRAWING\"] == 365243\n# Replace the anomalous values with nan\nprevious_application['DAYS_LAST_DUE'].replace({365243: np.nan}, inplace = True)\nprevious_application['DAYS_TERMINATION'].replace({365243: np.nan}, inplace = True)\nprevious_application['DAYS_LAST_DUE_1ST_VERSION'].replace({365243: np.nan}, inplace = True)\nprevious_application['DAYS_FIRST_DUE'].replace({365243: np.nan}, inplace = True)\nprevious_application['DAYS_FIRST_DRAWING'].replace({365243: np.nan}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8109bda726ec73a4e68ade5d980d17600059e1be"},"cell_type":"code","source":"\n##reducing dimenstions in categorical variable\nprevious_application['PRODUCT_COMBINATION'].replace({'Cash Street: high':'Cash_Street',\n                                                     'Cash Street: low':'Cash_Street',\n                                                     'Cash Street: middle':'Cash_Street',\n                                                       'Cash X-Sell: high':'Cash_XSell',\n                                                      'Cash X-Sell: middle':'Cash_XSell',\n                                                      'Cash X-Sell: low':'Cash_XSell',\n                                                     'POS household with interest':'POS_Interest',                                                       \n'POS household without interest':'POS_No_Interest',     \n'POS industry with interest': 'POS_Interest'   ,    \n'POS industry without interest': 'POS_No_Interest' ,    \n'POS mobile with interest':  'POS_Interest',        \n'POS mobile without interest':   'POS_No_Interest',    \n'POS other with interest':    'POS_Interest'    ,   \n'POS others without interest':  'POS_No_Interest'      \n                                                     \n                                    },inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8115d285f28934c39f440dbda238998dee1ee3f"},"cell_type":"code","source":"previous_application['NAME_CONTRACT_TYPE'].replace({'XNA':'Unknown'},inplace=True)\nprevious_application.groupby(['NAME_CONTRACT_STATUS']).SK_ID_PREV.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dde333fa4c28fc424df0295ed52389e8a21f15d2"},"cell_type":"code","source":"previous_application['NAME_PORTFOLIO'].replace({'Cars':'Cards'\n                                    },inplace=True)\n\nprevious_application['CHANNEL_TYPE'].replace({'Car dealer':'Other',\n                                    'Channel of corporate sales':'Other'\n                            },inplace=True)\nprevious_application.groupby(['CHANNEL_TYPE']).SK_ID_PREV.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3f471910cb8092d9a5707b34249e33b99cbe9e7"},"cell_type":"code","source":"previous_application['CODE_REJECT_REASON'].replace({'SYSTEM':'Other',\n                                    'VERIF':'Other',\n                                    'XNA':'Other',\n                                    'VERIF':'Other'\n                            },inplace=True)\n\nprevious_application['NAME_SELLER_INDUSTRY'].replace({'Auto technology':'Other',\n                                    'Jewelry':'Other',\n                                    'MLM partners':'Other',\n                                    'Tourism':'Other'\n                            },inplace=True)\nprevious_application.groupby(['NAME_SELLER_INDUSTRY']).SK_ID_PREV.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c1a20f6675e395b8f31f29bf001377cea6fc23b"},"cell_type":"code","source":"previous_application['NAME_CASH_LOAN_PURPOSE'].replace({'Building a house or an annex':'House',\n                                                    'Buying a garage':'House',\n                                                    'Buying a home':'House',\n                                                    'Buying a holiday home / land':'House',\n                                                    'Repairs':'House',\n                                                    'Buying a new car':'LifeStyle',\n                                                   'Buying a used car':'LifeStyle',\n                                                   'Car repairs':'LifeStyle',\n                                                   'Furniture':'LifeStyle',\n                                                   'Hobby':'LifeStyle',\n                                                   'Journey':'LifeStyle',\n                                                   'Wedding / gift / holiday':'LifeStyle',\n                                                       'Purchase of electronic equipment':'LifeStyle',\n                                                        'Urgent needs':'Needs',\n                                                        'Refusal to name the goal':'Needs',\n                                                        'Money for a third person':'Needs',\n                                                        'Gasification / water supply':'Needs',\n                                                        'Medicine':'Needs',\n                                                        'Payments on other loans':'Needs',\n                                                        'Everyday expenses':'Needs',\n                                                        'Business development':'Needs'                                                       \n                                                       },inplace=True)\nprevious_application.groupby(['NAME_CASH_LOAN_PURPOSE']).SK_ID_PREV.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2a3bd2e5bc5ddd2279b262cdc9ba3e346b92c8d"},"cell_type":"code","source":"previous_application['NAME_TYPE_SUITE'].replace({'Children':'Family',\n                                    'Group of people':'Other',\n                                    'Other_A':'Other',\n                                    'Other_B':'Other',\n                                    'Spouse, partner':'Family'},inplace=True)\nprevious_application.groupby(['NAME_TYPE_SUITE']).SK_ID_PREV.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fb6e7cbefe8e9c6349302ce983bcad98c88d79b7"},"cell_type":"code","source":"#drop categorical variables which are captured by other variables and does not makes sense to keep it \n#drop name goods category as it is broadly captured by Industry variable\nprevious_application=previous_application.drop(['NAME_GOODS_CATEGORY','WEEKDAY_APPR_PROCESS_START','HOUR_APPR_PROCESS_START'],1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1207e564d9fc5f4a4c6217aff0d1ba44e5eff174"},"cell_type":"code","source":"# Calculate value counts for each categorical column\nprevious_counts = count_categorical(previous_application, group_var = 'SK_ID_CURR', df_name = 'previous_loans')\nprevious_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc6febeb79ba5d2d2e20a5a1ffef46450ce71779"},"cell_type":"code","source":"# Calculate aggregate statistics for each numeric column\nprevious_agg = agg_numeric(previous_application.drop( ['SK_ID_PREV'],1), group_var = 'SK_ID_CURR', df_name = 'previous_loans')\nprevious_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70c4f1994e82fdc50a58a8f8bc8cf6a9eac98124"},"cell_type":"code","source":"# Remove variables to free memory\n\ngc.enable()\ndel previous_application\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a96dfa00c8b4a80cb85acf5ab274e5dec7fa2888"},"cell_type":"markdown","source":"**Create feasures based on domain**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"72c481cbbd7773c69deef0e5b6473c36d96a2eee"},"cell_type":"code","source":"previous_agg['previous_loans_CREDIT_TERM'] = previous_agg['previous_loans_AMT_ANNUITY_sum'] / previous_agg[ 'previous_loans_AMT_CREDIT_sum']\n\nprevious_agg['previous_loans_CREDIT_AMT_GDS_PERCENT'] = previous_agg['previous_loans_AMT_GOODS_PRICE_sum'] / previous_agg['previous_loans_AMT_CREDIT_sum']\n\nprevious_agg['previous_loans_AMT_APPLICATION_PERCENT'] = previous_agg['previous_loans_AMT_APPLICATION_sum'] / previous_agg['previous_loans_AMT_CREDIT_sum']\n\nprevious_agg['previous_loans_AMT_DOWN_PAYMENT_PERCENT'] = previous_agg['previous_loans_AMT_DOWN_PAYMENT_sum'] / previous_agg['previous_loans_AMT_CREDIT_sum']\n\nprevious_agg['previous_loans_AMT_APPLICATION_range'] = previous_agg['previous_loans_AMT_APPLICATION_max']-previous_agg['previous_loans_AMT_APPLICATION_min']\n\nprevious_agg['previous_loans_AMT_GOODS_PRICE_range'] = previous_agg['previous_loans_AMT_GOODS_PRICE_max']-previous_agg['previous_loans_AMT_GOODS_PRICE_min']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca159f798d9b4e3ea3e3acd494c97fbd203a314c"},"cell_type":"code","source":"previous_agg[['previous_loans_DAYS_LAST_DUE_mean','previous_loans_DAYS_LAST_DUE_min','previous_loans_DAYS_LAST_DUE_max']].describe()\n#'DAYS_TERMINATION','DAYS_LAST_DUE_1ST_VERSION','DAYS_FIRST_DUE','DAYS_FIRST_DRAWING']].","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2dfbd94de61d5afc1c75726684a5a0075cd5be15"},"cell_type":"code","source":"def agg_numeric(df, parent_var, df_name):\n    \"\"\"\n    Groups and aggregates the numeric values in a child dataframe\n    by the parent variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the child dataframe to calculate the statistics on\n        parent_var (string): \n            the parent variable used for grouping and aggregating\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated by the `parent_var` for \n            all numeric columns. Each observation of the parent variable will have \n            one row in the dataframe with the parent variable as the index. \n            The columns are also renamed using the `df_name`. Columns with all duplicate\n            values are removed. \n    \n    \"\"\"\n    \n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != parent_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    # Only want the numeric variables\n    parent_ids = df[parent_var].copy()\n    numeric_df = df.select_dtypes('number').copy()\n    numeric_df[parent_var] = parent_ids\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(parent_var).agg(['count', 'mean', 'max', 'min', 'sum'])\n\n    # Need to create new column names\n    columns = []\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        if var != parent_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n    \n    agg.columns = columns\n    \n    # Remove the columns with all redundant values\n    _, idx = np.unique(agg, axis = 1, return_index=True)\n    agg = agg.iloc[:, idx]\n    \n    return agg\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2674493a0bfd3e1f55a59b5795739b346329066e"},"cell_type":"code","source":"def agg_categorical(df, parent_var, df_name):\n    \"\"\"\n    Aggregates the categorical features in a child dataframe\n    for each observation of the parent variable.\n    \n    Parameters\n    --------\n    df : dataframe \n        The dataframe to calculate the value counts for.\n        \n    parent_var : string\n        The variable by which to group and aggregate the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    df_name : string\n        Variable added to the front of column names to keep track of columns\n\n    \n    Return\n    --------\n    categorical : dataframe\n        A dataframe with aggregated statistics for each observation of the parent_var\n        The columns are also renamed and columns with duplicate values are removed.\n        \n    \"\"\"\n    \n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('category'))\n\n    # Make sure to put the identifying id on the column\n    categorical[parent_var] = df[parent_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(parent_var).agg(['sum', 'count', 'mean'])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['sum', 'count', 'mean']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    # Remove duplicate columns by values\n    _, idx = np.unique(categorical, axis = 1, return_index = True)\n    categorical = categorical.iloc[:, idx]\n    \n    return categorical\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c0f3fb93dff2440a1697dbf6ba198ed0608dc3f"},"cell_type":"markdown","source":"**Function to Aggregate Stats at the Client Level**"},{"metadata":{"trusted":true,"_uuid":"96683385148e903d5dcaf1bc9aab5476c78dd65e","collapsed":true},"cell_type":"code","source":"def aggregate_client(df, group_vars, df_names):\n    \"\"\"Aggregate a dataframe with data at the loan level \n    at the client level\n    \n    Args:\n        df (dataframe): data at the loan level\n        group_vars (list of two strings): grouping variables for the loan \n        and then the client (example ['SK_ID_PREV', 'SK_ID_CURR'])\n        names (list of two strings): names to call the resulting columns\n        (example ['cash', 'client'])\n        \n    Returns:\n        df_client (dataframe): aggregated numeric stats at the client level. \n        Each client will have a single row with all the numeric data aggregated\n    \"\"\"\n    \n    # Aggregate the numeric columns\n    df_agg = agg_numeric(df, parent_var = group_vars[0], df_name = df_names[0])\n    \n    # If there are categorical variables\n    if any(df.dtypes == 'category'):\n    \n        # Count the categorical columns\n        df_counts = agg_categorical(df,group_vars[0], df_name = df_names[0])\n\n        # Merge the numeric and categorical\n        df_by_loan = df_counts.merge(df_agg, on = group_vars[0], how = 'outer')\n\n        gc.enable()\n        del df_agg, df_counts\n        gc.collect()\n\n        # Merge to get the client id in dataframe\n        df_by_loan = df_by_loan.merge(df[[group_vars[0], group_vars[1]]], on = group_vars[0], how = 'left')\n\n        # Remove the loan id\n        df_by_loan = df_by_loan.drop(columns = [group_vars[0]])\n\n        # Aggregate numeric stats by column\n        df_by_client = agg_numeric(df_by_loan, parent_var = group_vars[1], df_name = df_names[1])\n\n        \n    # No categorical variables\n    else:\n        # Merge to get the client id in dataframe\n        df_by_loan = df_agg.merge(df[[group_vars[0], group_vars[1]]], on = group_vars[0], how = 'left')\n        \n        gc.enable()\n        del df_agg\n        gc.collect()\n        \n        # Remove the loan id\n        df_by_loan = df_by_loan.drop(columns = [group_vars[0]])\n        \n        # Aggregate numeric stats by column\n        df_by_client = agg_numeric(df_by_loan, parent_var = group_vars[1], df_name = df_names[1])\n        \n    # Memory management\n    gc.enable()\n    del df, df_by_loan\n    gc.collect()\n\n    return df_by_client","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f6ef75452a33804dc91f3f64b9d254deb2e11af5"},"cell_type":"code","source":"import sys\n\ndef return_size(df):\n    \"\"\"Return size of dataframe in gigabytes\"\"\"\n    return round(sys.getsizeof(df) / 1e9, 2)\n\ndef convert_types(df, print_info = False):\n    \n    original_memory = df.memory_usage().sum()\n    \n    # Iterate through each column\n    for c in df:\n        \n        # Convert ids and booleans to integers\n        if ('SK_ID' in c):\n            df[c] = df[c].fillna(0).astype(np.int32)\n            \n        # Convert objects to category\n        elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n            df[c] = df[c].astype('category')\n        \n        # Booleans mapped to integers\n        elif list(df[c].unique()) == [1, 0]:\n            df[c] = df[c].astype(bool)\n        \n        # Float64 to float32\n        elif df[c].dtype == float:\n            df[c] = df[c].astype(np.float32)\n            \n        # Int64 to int32\n        elif df[c].dtype == int:\n            df[c] = df[c].astype(np.int32)\n        \n    new_memory = df.memory_usage().sum()\n    \n    if print_info:\n        print(f'Original Memory Usage: {round(original_memory / 1e9, 2)} gb.')\n        print(f'New Memory Usage: {round(new_memory / 1e9, 2)} gb.')\n        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9032d6e1455a5c6969de5ab5192f67cf86df0d79"},"cell_type":"code","source":"cash = pd.read_csv('../input/POS_CASH_balance.csv')\ncash = convert_types(cash, print_info=True)\ncash.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1136416a725b66caac4d90bcf50e8f186ec6a3c7"},"cell_type":"code","source":"cash_by_client = aggregate_client(cash, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['cash', 'client'])\ncash_by_client.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7788968877d86b4b3082ad07b6cdd22c46b0f12b"},"cell_type":"code","source":"# Clean up memory\ngc.enable()\ndel cash\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfd95660cc153356eb66f4b6b0a230d17a143b0d"},"cell_type":"markdown","source":"**Monthly Credit Data**"},{"metadata":{"trusted":true,"_uuid":"ca213ea91f024ca22baee61b97ec33187138738a"},"cell_type":"code","source":"credit = pd.read_csv('../input/credit_card_balance.csv')\ncredit = convert_types(credit, print_info = True)\ncredit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b71d6fab1c9fe501d8b4be020ad02d61af373642"},"cell_type":"code","source":"credit_by_client = aggregate_client(credit, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['credit', 'client'])\ncredit_by_client.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0b06b0fbf5b732381783e966d1b870939ae2c13"},"cell_type":"code","source":"# Clean up memory\ngc.enable()\ndel credit\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa668ccf443a3b3affeb342b918dd833d630e93f"},"cell_type":"markdown","source":"**Installment Payments**"},{"metadata":{"trusted":true,"_uuid":"7136d772d78980044c29491234382fb32c4aef77"},"cell_type":"code","source":"installments = pd.read_csv('../input/installments_payments.csv')\ninstallments = convert_types(installments, print_info = True)\ninstallments.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97c6585acd8d4f871ca2d4b864cc3a1fab6e096c"},"cell_type":"code","source":"installments_by_client = aggregate_client(installments, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['installments', 'client'])\ninstallments_by_client.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"515f6ad556f0070464c78fe87269ad6e69997ac2"},"cell_type":"code","source":"# Clean up memory\ngc.enable()\ndel installments\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"902679f6dec107546ed479258553f36a5f321ccf"},"cell_type":"code","source":"\napp_train = app_train.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\napp_test = app_test.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\napp_train = app_train.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')\napp_test = app_test.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n\n# Clean up memory\ngc.enable()\ndel bureau_agg,bureau_counts\ngc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9b300bc376e256842ec6cef8e5409d6260d04dda"},"cell_type":"code","source":"\napp_train = app_train.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')\napp_test = app_test.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"062bf92253f524c221b1cb71bcb7306db9c5644a"},"cell_type":"code","source":"\napp_train = app_train.merge(previous_counts, on = 'SK_ID_CURR', how = 'left')\napp_test = app_test.merge(previous_counts, on = 'SK_ID_CURR', how = 'left')\n\n\n\napp_train = app_train.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\napp_test = app_test.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n\n\n\n\napp_train = app_train.merge(cash_by_client, on = 'SK_ID_CURR', how = 'left')\napp_test = app_test.merge(cash_by_client, on = 'SK_ID_CURR', how = 'left')\n\napp_train = app_train.merge(credit_by_client, on = 'SK_ID_CURR', how = 'left')\napp_test = app_test.merge(credit_by_client, on = 'SK_ID_CURR', how = 'left')\n\napp_train = app_train.merge(installments_by_client, on = 'SK_ID_CURR', how = 'left')\napp_test = app_test.merge(installments_by_client, on = 'SK_ID_CURR', how = 'left')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b234e7b05d1a6bd7515bc8c1fcc114443f5812b4"},"cell_type":"code","source":"# Clean up memory\ngc.enable()\ndel previous_counts,bureau_balance_by_client,previous_agg,previous_counts,cash_by_client,credit_by_client,installments_by_client\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f272296ae9652fdd37ef53b47c2b32da92fc1997"},"cell_type":"code","source":"print('Installments by client shape: ', app_train.shape)\ngc.enable()\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cde93fa73d2bedf5583c6348c99dedff1d46436"},"cell_type":"markdown","source":"**Modeling**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a592a4dde28964d8d91c58b7c531a9d2df9028d4"},"cell_type":"code","source":"def model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8d67e1fce61badd3855cbad95e63f3690a7a571f"},"cell_type":"code","source":"submission, fi, metrics = model(app_train,app_test)\nprint('Baseline metrics')\nprint(metrics)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}