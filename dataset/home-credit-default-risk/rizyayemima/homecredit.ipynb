{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler, Imputer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/home-credit-default-risk/application_train.csv', sep=',', header=0)\ntest = pd.read_csv('../input/home-credit-default-risk/application_test.csv', sep=',', header=0)\nbureau = pd.read_csv('../input/home-credit-default-risk/bureau.csv', sep=',', header=0)\nprev = pd.read_csv('../input/home-credit-default-risk/previous_application.csv', sep=',', header=0)\n\ny_train = train['TARGET']\n\nprint('Train shape ',train.shape)\nprint('Test shape ',test.shape)\nprint('Bureau shape ',bureau.shape)\nprint('Previous application shape ',prev.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cek sebaran target\ntrain['TARGET'].astype(int).plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cek null\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ambil fitur lainnya dari Bureau"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dari bureau ambil jumlah pinjaman sebelumnya per id\ntbureau = bureau.groupby('SK_ID_CURR', as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU': 'COUNT_PREV'})\n\ntrain = train.join(tbureau.set_index('SK_ID_CURR'), how='left', on='SK_ID_CURR')\ntest = test.join(tbureau.set_index('SK_ID_CURR'), how='left', on='SK_ID_CURR')\n\nprint('Train shape',train.shape)\nprint('Test shape', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group by sk id curr, hitung agg\nbureau_agg = bureau.drop(columns=['SK_ID_BUREAU'])\nbureau_agg = bureau_agg.groupby('SK_ID_CURR', as_index = False).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n\ncolumns = ['SK_ID_CURR']\nfor col in bureau_agg.columns.levels[0]:\n    if col != 'SK_ID_CURR':\n        for agg in bureau_agg.columns.levels[1][:-1]:\n            columns.append('BUREAU_%s_%s' % (col, agg))\n            \nbureau_agg.columns = columns\nbureau_agg.head()\n\ntrain = train.join(bureau_agg.set_index('SK_ID_CURR'), how='left', on='SK_ID_CURR')\ntest = test.join(bureau_agg.set_index('SK_ID_CURR'), how='left', on='SK_ID_CURR')\n\nprint('Train shape',train.shape)\nprint('Test shape', test.shape)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_cat = pd.get_dummies(bureau.select_dtypes('object'))\nbureau_cat['SK_ID_CURR'] = bureau['SK_ID_CURR']\n\nbureau_cat = bureau_cat.groupby('SK_ID_CURR', as_index = False).agg(['count', 'mean']).reset_index()\n\ncolumns = ['SK_ID_CURR']\nfor col in bureau_cat.columns.levels[0]:\n    if col != 'SK_ID_CURR':\n        for agg in bureau_cat.columns.levels[1][:-1]:\n            columns.append('BUREAU_%s_%s' % (col, agg))\n            \nbureau_cat.columns = columns\n\ntrain = train.join(bureau_cat.set_index('SK_ID_CURR'), how='left', on='SK_ID_CURR')\ntest = test.join(bureau_cat.set_index('SK_ID_CURR'), how='left', on='SK_ID_CURR')\n\nprint('Train shape',train.shape)\nprint('Test shape', test.shape)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ambil fitur lainnya dari previous application"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dari previous application ambil jumlah pinjaman sebelumnya per id\ntprev = prev.groupby('SK_ID_CURR', as_index=False)['SK_ID_PREV'].count().rename(columns = {'SK_ID_PREV': 'COUNT_PREVS'})\n\ntrain = train.join(tprev.set_index('SK_ID_CURR'), how='left', on='SK_ID_CURR')\ntest = test.join(tprev.set_index('SK_ID_CURR'), how='left', on='SK_ID_CURR')\n\nprint('Train shape',train.shape)\nprint('Test shape', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group by sk id curr, hitung agg\nprev_agg = prev.drop(columns=['SK_ID_PREV'])\nprev_agg = prev_agg.groupby('SK_ID_CURR', as_index = False).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n\ncolumns = ['SK_ID_CURR']\nfor col in prev_agg.columns.levels[0]:\n    if col != 'SK_ID_CURR':\n        for agg in prev_agg.columns.levels[1][:-1]:\n            columns.append('PREV_%s_%s' % (col, agg))\n            \nprev_agg.columns = columns\nprev_agg.head()\n\ntrain = train.join(prev_agg.set_index('SK_ID_CURR'), how='left', on='SK_ID_CURR')\ntest = test.join(prev_agg.set_index('SK_ID_CURR'), how='left', on='SK_ID_CURR')\n\nprint('Train shape',train.shape)\nprint('Test shape', test.shape)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prev_cat = pd.get_dummies(prev.select_dtypes('object'))\nprev_cat['SK_ID_CURR'] = prev['SK_ID_CURR']\n\nprev_cat = prev_cat.groupby('SK_ID_CURR', as_index = False).agg(['count', 'mean']).reset_index()\n\ncolumns = ['SK_ID_CURR']\nfor col in prev_cat.columns.levels[0]:\n    if col != 'SK_ID_CURR':\n        for agg in prev_cat.columns.levels[1][:-1]:\n            columns.append('BUREAU_%s_%s' % (col, agg))\n            \nprev_cat.columns = columns\n\ntrain = train.join(prev_cat.set_index('SK_ID_CURR'), how='left', on='SK_ID_CURR')\ntest = test.join(prev_cat.set_index('SK_ID_CURR'), how='left', on='SK_ID_CURR')\n\nprint('Train shape',train.shape)\nprint('Test shape', test.shape)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One hot encoding - for categorical columns main table"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.get_dummies(train)\ntest = pd.get_dummies(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitur tambahan (persentase)\n\ntrain['CREDIT_INCOME_PERCENT'] = train['AMT_CREDIT'] / train['AMT_INCOME_TOTAL']\ntrain['ANNUITY_INCOME_PERCENT'] = train['AMT_ANNUITY'] / train['AMT_INCOME_TOTAL']\ntrain['CREDIT_TERM'] = train['AMT_ANNUITY'] / train['AMT_CREDIT']\ntrain['DAYS_EMPLOYED_PERCENT'] = train['DAYS_EMPLOYED'] / train['DAYS_BIRTH']\ntrain['INCOME_PER_PERSON'] = train['AMT_INCOME_TOTAL'] / train['CNT_FAM_MEMBERS']\n\ntest['CREDIT_INCOME_PERCENT'] = test['AMT_CREDIT'] / test['AMT_INCOME_TOTAL']\ntest['ANNUITY_INCOME_PERCENT'] = test['AMT_ANNUITY'] / test['AMT_INCOME_TOTAL']\ntest['CREDIT_TERM'] = test['AMT_ANNUITY'] / test['AMT_CREDIT']\ntest['DAYS_EMPLOYED_PERCENT'] = test['DAYS_EMPLOYED'] / test['DAYS_BIRTH']\ntest['INCOME_PER_PERSON'] = test['AMT_INCOME_TOTAL'] / test['CNT_FAM_MEMBERS']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Match dataframe train and test\ntrain, test = train.align(test, join = 'inner', axis = 1)\ntrain['TARGET'] = y_train\ntrain.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check missing value / null columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"def checknull(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : '% Missing Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table = mis_val_table[\n            mis_val_table.iloc[:,1] != 0].sort_values(\n        '% Missing Values', ascending=False).round(1)\n        \n        # Return the dataframe with missing information\n        return mis_val_table\n\nchecknull(train).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checknull(train)['% Missing Values'].astype(int).plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_col = list(checknull(train).index[checknull(train)['% Missing Values'] > 50])\ntrain = train.drop(columns=missing_col)\ntest = test.drop(columns=missing_col)\n\nprint('Remove %d columns with more than 30%% missing values' % len(missing_col))\nprint('Train shape', train.shape)\nprint('Test shape', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs = []\n\nfor col in train.columns:\n    if col != 'TARGET' and col != 'SK_ID_CURR':\n        corr = train['TARGET'].corr(train[col])\n        corrs.append((col,corr))\n\ncorrs = pd.DataFrame(corrs)\ncorrs = corrs.rename(columns = {0 : 'Columns', 1 : 'Correlation'})\ncorrs = corrs[\n            corrs.iloc[:,1] != 0].sort_values(\n        'Correlation', ascending=False)\ncorrs.set_index('Columns',inplace=True)\n\ncorrs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs['Correlation'].plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lowcorr_col = list(corrs.index[abs(corrs['Correlation']) < 0.01])\n\ntrain = train.drop(columns=lowcorr_col)\ntest = test.drop(columns=lowcorr_col)\n\nprint('Remove %d columns with <0.01 correlation to target' % len(lowcorr_col))\nprint('Train shape', train.shape)\nprint('Test shape', test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(columns=['TARGET'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer = Imputer(strategy='median')\nimputer.fit(train)\n\nntrain = imputer.transform(train)\nntest = imputer.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler(feature_range=(0,1))\nscaler.fit(ntrain)\n\nntrain = scaler.transform(ntrain)\nntest = scaler.transform(ntest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(C=1)\nlogreg.fit(ntrain,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = logreg.predict_proba(ntest)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = test[['SK_ID_CURR']]\nsubmit['TARGET']=pred\n\nsubmit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.to_csv('submission.csv',sep=',',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}