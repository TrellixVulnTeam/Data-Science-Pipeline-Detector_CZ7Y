{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#样本文件路径\nSAMPLE_FILE_PATH = \"/kaggle/input/challenges-in-representation-learning-facial-expression-recognition-challenge/icml_face_data.csv\"\n#分类数量\nNUM_CLASSES = 7\n#训练、校验、测试数据集HDF5文件的输出路径\nTRAIN_HDF5 = \"/kaggle/working/train.hdf5\"\nVAL_HDF5 = \"/kaggle/working/VAL.hdf5\"\nTEST_HDF5 = \"/kaggle/working/test.hdf5\"\n\n#每批次样本数量\nBATCH_SIZE = 128\n\n#项目输出文件保存目录\nOUTPUT_PATH = \"/kaggle/working/\"\n\n#数据集样本RGB平均值存储位置及文件名称\nDATASET_MEAN_FILE = OUTPUT_PATH + \"rgb_mean.json\"\n\n#模型保存位置及文件名称\nMODEL_FILE = OUTPUT_PATH + \"model.h5\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n# os.remove(\"./train.hdf5\")\n# os.remove(\"./test.hdf5\")\n# os.remove(\"./VAL.hdf5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport os\nimport numpy as np\nfrom keras.callbacks import BaseLogger\nimport matplotlib.pyplot as plt\n\n\n\nclass TrainingMonitor(BaseLogger):\n    def __init__(self, fig_path, json_path=None, start_at=0):\n        super(TrainingMonitor, self).__init__()\n        self.fig_path = fig_path\n        self.json_path = json_path\n        self.start_at = start_at\n\n    def on_train_begin(self, logs={}):\n        self.history = {}\n\n\n        if self.json_path is not None:\n            if os.path.exists(self.json_path):\n                self.history = json.loads(open(self.json_path).read())\n\n                if self.start_at > 0:\n\n                    for k in self.history.keys():\n                        self.history[k] = self.history[k][:self.start_at]\n\n    def on_epoch_end(self, epoch, logs={}):\n\n        for (k, v) in logs.items():\n            log = self.history.get(k, [])\n            log.append(float(v))\n            self.history[k] = log\n\n        if self.json_path is not None:\n            f = open(self.json_path, 'w')\n            f.write(json.dumps(self.history, skipkeys=True))\n\n            f.close()\n\n\n        if len(self.history[\"loss\"]) > 1:\n\n            N = np.arange(0, len(self.history[\"loss\"]))\n            plt.style.use(\"ggplot\")\n            plt.figure()\n            plt.plot(N, self.history[\"loss\"], label=\"train_loss\")\n            plt.plot(N, self.history[\"val_loss\"], label=\"val_loss\")\n\n            plt.plot(N, self.history[\"accuracy\"], label=\"train_acc\")\n\n            plt.plot(N, self.history[\"val_accuracy\"], label=\"val_acc\")\n            epochs = len(self.history[\"loss\"])\n            plt.title(\"Training Loss & Accuracy [Epoch {}]\".format(epochs))\n            plt.xlabel(\"Epoch #\")\n            plt.ylabel(\"Loss/Accuracy\")\n            plt.legend()\n            plt.savefig(self.fig_path)\n            plt.close()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import img_to_array\n\n\nclass ImageToArrayPreprocessor:\n    def __init__(self, data_format=None):\n\n        self.data_format = data_format\n\n    def preprocess(self, image):\n\n\n\n        return img_to_array(image, data_format=self.data_format)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nimport h5py\n\n\n# HDF5数据集生成器\n\n\nclass HDF5DatasetWriter:\n\n\n\n    def __init__(self, dims, output_path, data_key=\"images\", buf_size=1000):\n\n\n        if os.path.exists(output_path):\n            raise ValueError(\"您提供的输出文件{}已经存在，请先手工删除！\".format(output_path))\n\n        self.db = h5py.File(output_path, \"w\")\n        self.data = self.db.create_dataset(data_key, dims, dtype=\"float\")\n        self.labels = self.db.create_dataset(\"labels\", (dims[0],), dtype=\"int\")\n\n\n        self.buf_size = buf_size  # 1000\n        self.buffer = {\"data\": [], \"labels\": []}\n        self.idx = 0\n\n    def add(self, raw, label):\n\n        self.buffer[\"data\"].extend(raw)\n        self.buffer[\"labels\"].extend(label)\n        if len(self.buffer[\"data\"]) >= self.buf_size:\n            self.flush()\n\n    def flush(self):\n\n        i = self.idx + len(self.buffer[\"data\"])\n        self.data[self.idx:i] = self.buffer[\"data\"]\n        self.labels[self.idx:i] = self.buffer[\"labels\"]\n        self.idx = i\n        self.buffer = {\"data\": [], \"labels\": []}\n\n    def store_class_labels(self, class_labels):\n\n        dt = h5py.special_dtype(vlen=str)\n        label_dim = (len(class_labels),)\n        label_set = self.db.create_dataset(\"label_names\", label_dim, dtype=dt)\n        label_set[:] = class_labels\n\n    def close(self):\n\n        if len(self.buffer[\"data\"]) > 0:\n            self.flush()\n\n        self.db.close()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import h5py\nimport numpy as np\n# from tensorflow.keras.utils.np_utils import to_categorical\nfrom keras.utils import to_categorical\n\n\n\nclass HDF5DatasetGenerator:\n    def __init__(self, db_file, batch_size, preprocessors=None, aug=None, binarize=True, classes=2):\n\n        self.batchSize = batch_size\n\n\n        self.preprocessors = preprocessors\n\n\n        self.aug = aug\n\n\n        self.binarize = binarize\n\n\n        self.classes = classes\n\n\n        self.db = h5py.File(db_file, mode='r')\n\n\n        self.numImages = self.db[\"labels\"].shape[0]\n\n    def generator(self, passes=np.inf):\n\n        epochs = 0\n\n        while epochs < passes:\n\n            for i in np.arange(0, self.numImages, self.batchSize):\n\n\n                images = self.db[\"images\"][i:i + self.batchSize]\n                labels = self.db[\"labels\"][i:i + self.batchSize]\n\n\n                if self.binarize:\n                    labels = to_categorical(labels, self.classes)\n\n\n                if self.preprocessors is not None:\n\n                    processed_images = []\n\n\n                    for image in images:\n                        for p in self.preprocessors:\n                            image = p.preprocess(image)\n\n\n                        processed_images.append(image)\n\n                    images = np.array(processed_images)\n\n                if self.aug is not None:\n                    (images, labels) = next(self.aug.flow(images, labels, batch_size=self.batchSize))\n\n                yield images, labels\n\n            epochs += 1\n\n    def close(self):\n\n        self.db.close()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import Callback\nimport os\n\n\nclass EpochCheckpoint(Callback):\n    def __init__(self, output_path, every=5, start_at=0):\n        super(Callback, self).__init__()\n        self.output_path = output_path\n        self.every = every\n        self.start_epoch = start_at\n\n    def on_epoch_end(self, epoch, logs={}):\n       \n        self.start_epoch += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nprint('加载csv格式数据集文件...')\n\nfile=open(SAMPLE_FILE_PATH)\n# 跳过表头\nfile.__next__()\n(train_images,train_labels)=([],[])\n(val_images,val_labels)=([],[])\n(test_images,test_labels)=([],[])\n# 存储数据集样本类别分布的字典\n\ncount_by_label_train={}\ncount_by_label_val={}\ncount_by_label_test={}\n\n# 遍历一遍文件的每一行\nfor row in file:\n    (label,usage,image)=row.strip().split(\",\")\n    # 标签转化整数\n    label=int(label)\n    image=np.array(image.split(' '),dtype='uint8')\n    image=image.reshape((48,48))\n    #用途为train，添加训练样本数据集\n    if usage=='Training':\n        train_images.append(image)\n        train_labels.append(label)\n        count=count_by_label_train.get(label,0)\n        count_by_label_train[label]=count+1\n    # 用 途PublicTest，添加到校验样本集\n    elif usage=='PublicTest':\n        val_images.append(image)\n        val_labels.append(label)\n        count=count_by_label_val.get(label,0)\n        count_by_label_val[label]=count+1\n    # 用 PrivateTest，添加到测试样本集\n    elif usage=='PrivateTest':\n        test_images.append(image)\n        test_labels.append(label)\n        count=count_by_label_test.get(label,0)\n        count_by_label_test[label]=count+1\nfile.close()\n\nprint(\"训练样本数量:{}\".format(len(train_images)))\nprint(\"校验样本数量:{}\".format(len(val_images)))\nprint(\"测试样本数量:{}\".format(len(test_images)))\n# 输出训练、校验、测试样本分布\nprint(count_by_label_train)\nprint('校验样本分布')\nprint(count_by_label_val)\nprint(\"测试样本分布\")\nprint(count_by_label_test)\n# 构建一个训练、校验、测试数据集列表\n# 每个元素由数据集类型名称、全部样本文件名称、全部样本整形标签、HDF5输出文件构成\ndatasets=[(train_images,train_labels,TRAIN_HDF5),\n          (val_images,val_labels,VAL_HDF5),\n          (test_images,test_labels,TEST_HDF5)]\n\n# 遍历数据集元祖\nfor (images,labels,outputPath) in datasets:\n    #     创建HDF5写入器\n    print(\"构建 {}...\".format(outputPath))\n    writer=HDF5DatasetWriter((len(images),48,48),outputPath)\n    #遍历每个图像，将其加入数据集\n    for (image,label) in zip(images,labels):\n        writer.add([image],[label])\n\n    #关闭HDF5写入器\n    writer.close()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.regularizers import l2,l1\nfrom tensorflow.keras import backend\nclass MiniVGG11:\n    @staticmethod\n    def build(width,height,channel,classes,reg=0.0002):\n        \"\"\"\n        Args:\n            width:输入样本的宽度\n            height: 输入样本的长度\n            channel: 输入样本的通道\n            classes: 分类数量\n            reg: 正则化因子\n\n        Returns:  MiniVGG11 网络模型对象\n\n        \"\"\"\n\n        model=Sequential(name='MiniVGG11')\n        shape=(height,width,channel)\n        channel_dimension=-1\n        \n        if backend.image_data_format()==\"channels_first\":\n            shape = (channels,height,width)\n            channel_dimension = 1\n\n        # 如果输入格式为通道前置\n        #重新设置输入格式和通道位置指示\n        if backend.image_data_format()=='channels_first':\n            shape=(channel,height,width)\n            channel_dimension=1\n        # 第一卷积快\n        model.add(Conv2D(64, (3, 3), input_shape=shape, padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n        # 第二卷积快\n        model.add(Conv2D(128, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n        # 第三卷积快\n        model.add(Conv2D(256, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Conv2D(256, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n        # 第四卷积块\n        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n        # 第五卷积快\n        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\", strides=(1, 1)))\n        # 第一全连接\n        model.add(Flatten())\n        model.add(Dense(256, kernel_regularizer=l2(reg)))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Dropout(0.5))\n        # 第二全练接\n        model.add(Dense(128, kernel_regularizer=l2(reg)))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Dropout(0.5))\n        # 第三全连接\n        model.add(Dense(classes, kernel_regularizer=l1(reg)))\n        model.add(Activation(\"softmax\"))\n\n        return model\n    \nif __name__ == \"__main__\":\n    model = MiniVGG11.build(width=48,height=48,channel=1,classes=7,reg=0.0002)\n    print(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import load_model\nimport tensorflow.keras.backend as K\n\nimport argparse\nimport json\nimport os\n\nmatplotlib.use(\"Agg\")\n\nap = argparse.ArgumentParser()\nap.add_argument(\"-c\", \"--checkpoints\", help=\"检查点输出目录\")\nap.add_argument(\"-m\", \"--model\", type=str, help=\"要加载的检查点模型文件\")\nap.add_argument(\"-s\", \"--start-epoch\", type=int, default=0, help=\"重新训练的趟数期起点\")\nargs = vars(ap.parse_args(args=[]))\n\ntrain_aug = ImageDataGenerator(rotation_range=10,\n                               zoom_range=0.1,\n                               horizontal_flip=True,\n                               rescale=1 / 255.0,\n                               fill_mode=\"nearest\")\n\nval_aug = ImageDataGenerator(rescale=1 /255.0)\n\niap = ImageToArrayPreprocessor()\n\ntrain_gen = HDF5DatasetGenerator(TRAIN_HDF5,\n                                 BATCH_SIZE,\n                                 aug=train_aug,\n                                 preprocessors=[iap],\n                                 classes=NUM_CLASSES)\n\nval_gen = HDF5DatasetGenerator(VAL_HDF5,\n                                 BATCH_SIZE,\n                                 aug=val_aug,\n                                 preprocessors=[iap],\n                                 classes=NUM_CLASSES)\n\nif args[\"model\"] is None:\n    print(\"[信息] 编译模型。。。\")\n    opt = Adam(lr=1e-3)\n    model = MiniVGG11.build(width=48, height=48, channel=1, classes=NUM_CLASSES)\n    model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics = [\"accuracy\"])\nelse:\n    print(\"[信息] 加载{}。。。\".format(args[\"model\"]))\n    model = load_model(args[\"model\"])\n    print(\"[信息] 原学习率：{}。。。\".format(K.get_value(model.optimizer.lr)))\n    K.set_value(model.optimizer.lr, 1e-3)\n    print(\"[信息] 现学习率：{}。。。\".format(K.get_value(model.optimizer.lr)))\n\nfig_path = os.path.sep.join([OUTPUT_PATH, \"VGG11_320203303.png\"])\njson_path = os.path.sep.join([OUTPUT_PATH, \"VGG11._320203303json\"])\ncallbacks = [EpochCheckpoint(args[\"checkpoints\"],every=5, start_at=args[\"start_epoch\"]),\n             TrainingMonitor(fig_path=fig_path, json_path=json_path, start_at=args[\"start_epoch\"])\n]\n\nmodel.fit_generator(train_gen.generator(),\n                    steps_per_epoch=train_gen.numImages//BATCH_SIZE,\n                    validation_data = val_gen.generator(),\n                    validation_steps= val_gen.numImages//BATCH_SIZE,\n                    epochs=30,\n                    max_queue_size= BATCH_SIZE *2,\n                    callbacks=callbacks,\n                    verbose=1)\n\nprint(\"[信息] 保存模型。。。\")\nmodel.save(MODEL_FILE, overwrite=True)\n\ntrain_gen.close()\nval_gen.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import load_model\n\nteat_AUG = ImageDataGenerator(rescale=1 / 255.0)\niap = ImageToArrayPreprocessor()\n\ntestGen = HDF5DatasetGenerator(TEST_HDF5,\n                               BATCH_SIZE,\n                               aug=teat_AUG,\n                               preprocessors=[iap],\n                               classes=NUM_CLASSES)\n\nprint(\"[信息]加载网络模型....\")\nmodel = load_model(MODEL_FILE)\n\n(loss, acc) = model.evaluate_generator(testGen.generator(),\n                                       steps=testGen.numImages // BATCH_SIZE,\n                                       max_queue_size=BATCH_SIZE * 2)\n\n\nprint(\"[信息]测试集准确率: {:.2f}%\".format(acc * 100))\n\ntestGen.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}