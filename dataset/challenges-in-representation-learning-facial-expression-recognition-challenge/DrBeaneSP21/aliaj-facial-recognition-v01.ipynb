{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1.Load Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import *\nfrom keras_preprocessing.image import ImageDataGenerator\n\nimport zipfile \n\nimport cv2\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfrom keras import models\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.utils import to_categorical\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Load the Data","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/challenges-in-representation-learning-facial-expression-recognition-challenge/'\nos.listdir(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(path+'icml_face_data.csv')\ndata.columns = ['emotion', 'Usage', 'pixels']      # Added by rb\ntrain = pd.read_csv(path+'train.csv')\ntest = pd.read_csv(path+'test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Usage'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we see that data is the sum of all the Train set, test set (private test and public test). ","metadata":{}},{"cell_type":"markdown","source":"> **Helping functions**","metadata":{}},{"cell_type":"code","source":"def prepare_data(data):\n    image_array = np.zeros(shape=(len(data), 48, 48, 1))\n    image_label = np.array(list(map(int, data['emotion'])))\n\n    for i, row in enumerate(data.index):\n        image = np.fromstring(data.loc[row, 'pixels'], dtype=int, sep=' ')\n        image = np.reshape(image, (48, 48)) \n        image_array[i, :, :, 0] = image / 255\n\n    return image_array, image_label\n\ndef vis_training(hlist, start=1):\n    \n    loss = np.concatenate([h.history['loss'] for h in hlist])\n    val_loss = np.concatenate([h.history['val_loss'] for h in hlist])\n    acc = np.concatenate([h.history['accuracy'] for h in hlist])\n    val_acc = np.concatenate([h.history['val_accuracy'] for h in hlist])\n    \n    epoch_range = range(1,len(loss)+1)\n\n    plt.figure(figsize=[12,6])\n    plt.subplot(1,2,1)\n    plt.plot(epoch_range[start-1:], loss[start-1:], label='Training Loss')\n    plt.plot(epoch_range[start-1:], val_loss[start-1:], label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.legend()\n\n    plt.subplot(1,2,2)\n    plt.plot(epoch_range[start-1:], acc[start-1:], label='Training Accuracy')\n    plt.plot(epoch_range[start-1:], val_acc[start-1:], label='Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend()\n\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Prepare the data","metadata":{}},{"cell_type":"code","source":"emotions = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Defining train and test data. ","metadata":{}},{"cell_type":"code","source":"full_train_images, full_train_labels = prepare_data(data[data['Usage']=='Training'])\ntest_images, test_labels = prepare_data(data[data['Usage']!='Training'])\n\nprint(full_train_images.shape)\nprint(full_train_labels.shape)\nprint(test_images.shape)\nprint(test_labels.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images, valid_images, train_labels, valid_labels =\\\n    train_test_split(full_train_images, full_train_labels, test_size=0.2, random_state=1)\n\nprint(train_images.shape)\nprint(valid_images.shape)\nprint(train_labels.shape)\nprint(valid_labels.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 Display Sample of Images ","metadata":{}},{"cell_type":"code","source":"N_train = train_labels.shape[0]\n\nsel = np.random.choice(range(N_train), replace=False, size=16)\n\nX_sel = train_images[sel, :, :, :]\ny_sel = train_labels[sel]\n\nplt.figure(figsize=[12,12])\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.imshow(X_sel[i,:,:,0], cmap='binary_r')\n    plt.title(emotions[y_sel[i]])\n    plt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.CNN Model","metadata":{}},{"cell_type":"code","source":"%%time \n\ncnn = Sequential()\n\ncnn.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(48,48,1)))\ncnn.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\ncnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\ncnn.add(Dropout(0.25))\ncnn.add(BatchNormalization())\n\ncnn.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\ncnn.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\ncnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\ncnn.add(Dropout(0.25))\ncnn.add(BatchNormalization())\n\ncnn.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'))\ncnn.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'))\ncnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\ncnn.add(Dropout(0.5))\ncnn.add(BatchNormalization())\n\ncnn.add(Flatten())\n\ncnn.add(Dense(512, activation='relu'))\ncnn.add(Dropout(0.5))\ncnn.add(Dense(512, activation='relu'))\ncnn.add(Dropout(0.5))\ncnn.add(Dense(512, activation='relu'))\ncnn.add(Dropout(0.5))\n\ncnn.add(Dense(7, activation='softmax'))\n\ncnn.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nopt = keras.optimizers.Adam(lr=0.001)\ncnn.compile(loss='sparse_categorical_crossentropy',\n                  optimizer=opt, metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1 Training Run 1","metadata":{}},{"cell_type":"code","source":"%%time \n\nh1 = cnn.fit(train_images, train_labels, batch_size=256, epochs=30, verbose=1, \n                   validation_data =(valid_images, valid_labels)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vis_training([h1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Training Run 2","metadata":{}},{"cell_type":"code","source":"%%time \nkeras.backend.set_value(cnn.optimizer.learning_rate, 0.0001)\n\nh2 = cnn.fit(train_images, train_labels, batch_size=256, epochs=30, verbose=1, \n                   validation_data =(valid_images, valid_labels)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vis_training([h1, h2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Save Model","metadata":{}},{"cell_type":"code","source":"cnn.save('first_model_57.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Evaluating the model ","metadata":{}},{"cell_type":"markdown","source":"## 4.1 - Generate Test Predictions and Calculating Accuracy","metadata":{}},{"cell_type":"code","source":"test_prob = cnn.predict(test_images)\ntest_pred = np.argmax(test_prob, axis=1)\ntest_accuracy = np.mean(test_pred == test_labels)\n\nprint(test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Confusion matrix","metadata":{}},{"cell_type":"code","source":"conf_mat = confusion_matrix(test_labels, test_pred)\n\npd.DataFrame(conf_mat, columns=emotions.values(), index=emotions.values())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plot_confusion_matrix(conf_mat=conf_mat,\n                                show_normed=True,\n                                show_absolute=False,\n                                class_names=emotions.values(),\n                                figsize=(8, 8))\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Classification report ","metadata":{}},{"cell_type":"code","source":"print(classification_report(test_labels, test_pred, target_names=emotions.values()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Trying to save the model ","metadata":{}},{"cell_type":"markdown","source":"## 4.3 Class Activation Maps","metadata":{}},{"cell_type":"code","source":"class GradCAM:\n    def __init__(self, model, classIdx, layerName=None):\n        self.model = model\n        self.classIdx = classIdx\n        self.layerName = layerName\n        if self.layerName is None:\n            self.layerName = self.find_target_layer()\n            \n    def find_target_layer(self):\n        for layer in reversed(self.model.layers):\n            if len(layer.output_shape) == 4:\n                return layer.name\n        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n        \n    def compute_heatmap(self, image, eps=1e-8):\n        gradModel = Model(\n            inputs=[self.model.inputs],\n            outputs=[self.model.get_layer(self.layerName).output,self.model.output]\n       )\n           \n        with tf.GradientTape() as tape:\n            inputs = tf.cast(image, tf.float32)\n            (convOutputs, predictions) = gradModel(inputs)\n            loss = predictions[:, self.classIdx]\n            grads = tape.gradient(loss, convOutputs)\n\n            castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n            castGrads = tf.cast(grads > 0, \"float32\")\n            guidedGrads = castConvOutputs * castGrads * grads\n            convOutputs = convOutputs[0]\n            guidedGrads = guidedGrads[0]\n\n            weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n            cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n\n            (w, h) = (image.shape[2], image.shape[1])\n            heatmap = cv2.resize(cam.numpy(), (w, h))\n            numer = heatmap - np.min(heatmap)\n            denom = (heatmap.max() - heatmap.min()) + eps\n            heatmap = numer / denom\n            heatmap = (heatmap * 255).astype(\"uint8\")\n        return heatmap\n\n    def overlay_heatmap(self, heatmap, image, alpha=0.5,\n        colormap = cv2.COLORMAP_VIRIDIS):\n        heatmap = cv2.applyColorMap(heatmap, colormap)\n        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n        return (heatmap, output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=[16,16])\nfor i in range(36):\n    img = test_images[i,:,:,0]\n    p_dist = cnn.predict(img.reshape(1,48,48,1))\n    k = np.argmax(p_dist)\n    p = np.max(p_dist)\n\n    cam = GradCAM(cnn, k)\n    heatmap = cam.compute_heatmap(img.reshape(1,48,48,1))\n\n    plt.subplot(6,6,i+1)\n    plt.imshow(img, cmap='binary_r')\n    plt.imshow(heatmap, alpha=0.5, cmap='coolwarm')\n    plt.title(f'{emotions[test_labels[i]]} - ({emotions[k]} - {p:.4f})')\n    plt.axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}