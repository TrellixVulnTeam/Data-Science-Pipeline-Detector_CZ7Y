{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Face Expression Recognizer: Use Cases- Market Research, Gaming Industry, Behaviour Testing  \nEmotions: seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral)  \n","metadata":{"id":"pZHYyRq09tGk"}},{"cell_type":"markdown","source":"## Import Data and Dependencies","metadata":{"id":"hAaUGjWdDiom"}},{"cell_type":"code","source":"!wget https://www.dropbox.com/s/si11cws2pyho1bp/archive.zip","metadata":{"id":"R8gF9ZcFFKub","outputId":"b246a20b-8f03-4f1d-8704-d1aafcf606f6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip -q \"/content/archive.zip\"","metadata":{"id":"mkziAgJNIjHf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imports required for this project\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\ntf.random.set_seed(4)","metadata":{"id":"Y26WoMcNJXns"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the Pathlib PATH objects\ntrain_path = Path(\"/content/train\")\ntest_path = Path(\"/content/test\")","metadata":{"id":"gDa2l6EpInFd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting Image paths \ntrain_image_paths = list(train_path.glob(\"*/*\"))\ntrain_image_paths = list(map(lambda x : str(x) , train_image_paths))\n\ntrain_image_paths[:10]","metadata":{"id":"ImXim_mMJVRa","outputId":"e3f7cb65-5808-49b8-8671-8d486f1f7ecf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting their respective labels \n\ndef get_label(image_path):\n    return image_path.split(\"/\")[-2]\n\ntrain_image_labels = list(map(lambda x : get_label(x) , train_image_paths))\ntrain_image_labels[:10]","metadata":{"id":"PvsVVLJtJa70","outputId":"84da5fbc-b760-4603-da09-153656525a4c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder \n\nLe = LabelEncoder()\ntrain_image_labels = Le.fit_transform(train_image_labels)\n\ntrain_image_labels[:10]","metadata":{"id":"J2PyisAQJdnk","outputId":"e57a5823-c59f-4e69-e1c9-7d88fbc89fe0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_image_labels = tf.keras.utils.to_categorical(train_image_labels)\n\ntrain_image_labels[:10]","metadata":{"id":"teXPoVwzJgD0","outputId":"2225921e-4c91-4335-da01-0a4e7577ba6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split \n\nX_train , X_val , y_train , y_val = train_test_split(train_image_paths , train_image_labels , test_size = 0.25)","metadata":{"id":"nWtX-CgzJimk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute class weights \n\nclassTotals = y_train.sum(axis=0)\nclassWeight = classTotals.max() / classTotals\n\nclass_weight = {e : weight for e , weight in enumerate(classWeight)}\nprint(class_weight)","metadata":{"id":"ZQ8RnggIJpRU","outputId":"98d54efe-e015-4c67-ac5c-71dd534338e5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Loading and Augmentation","metadata":{"id":"8M0NsCetNh8V"}},{"cell_type":"code","source":"# Function used for Transformation\n\ndef load(image , label):\n    image = tf.io.read_file(image)\n    image = tf.io.decode_jpeg(image , channels = 3)\n    return image , label","metadata":{"id":"nPNxtPADJxPF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define IMAGE SIZE and BATCH SIZE \nIMG_SIZE = 96 \nBATCH_SIZE = 32\n\n# Basic Transformation\nresize = tf.keras.Sequential([\n    tf.keras.layers.experimental.preprocessing.Resizing(IMG_SIZE, IMG_SIZE)          \n])\n\n# Data Augmentation\ndata_augmentation = tf.keras.Sequential([\n    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n    tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n    tf.keras.layers.experimental.preprocessing.RandomZoom(height_factor = (-0.1, -0.05))\n])","metadata":{"id":"rl2Hs7NkJ1K8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function used to Create a Tensorflow Data Object\nAUTOTUNE = tf.data.experimental.AUTOTUNE\ndef get_dataset(paths , labels , train = True):\n    # convert paths and labels to tensor\n    image_paths = tf.convert_to_tensor(paths)\n    labels = tf.convert_to_tensor(labels)\n\n    # create dataset objects for images and labels\n    image_dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n    label_dataset = tf.data.Dataset.from_tensor_slices(labels)\n\n    # zip them to be able to iterate on both at once\n    dataset = tf.data.Dataset.zip((image_dataset , label_dataset))\n\n    # apply load and resize on dataset\n    dataset = dataset.map(lambda image , label : load(image , label))\n    dataset = dataset.map(lambda image, label: (resize(image), label) , num_parallel_calls=AUTOTUNE)\n\n    # shuffle and batch the dataset\n    dataset = dataset.shuffle(1000)\n    dataset = dataset.batch(BATCH_SIZE)\n\n    # if train = True apply data augmentation\n    if train:\n        dataset = dataset.map(lambda image, label: (data_augmentation(image), label) , num_parallel_calls=AUTOTUNE)\n    \n    # if not training repeat over the dataset and return\n    dataset = dataset.repeat()\n    return dataset","metadata":{"id":"E5u4FAn3J3PE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating Train Dataset object and Verifying it\n%time train_dataset = get_dataset(X_train , y_train)\n\nimage , label = next(iter(train_dataset))\nprint(image.shape)\nprint(label.shape)","metadata":{"id":"RVIWOL_ZJ7j8","outputId":"841550d5-83d7-4da6-82f9-92b00931fb24"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View a sample Training Image\nprint(Le.inverse_transform(np.argmax(label , axis = 1))[0])\nplt.imshow((image[0].numpy()/255).reshape(96 , 96 , 3))","metadata":{"id":"VmMB-2JEKBY8","outputId":"329f3d0b-fcdf-4a5c-8c81-955c76d71f8b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time val_dataset = get_dataset(X_val , y_val , train=False)\n\nimage , label = next(iter(val_dataset))\nprint(image.shape)\nprint(label.shape)","metadata":{"id":"KSfzF5mdKD_E","outputId":"98e48bcc-c081-4308-d363-582e831927bc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View a sample Validation Image\nprint(Le.inverse_transform(np.argmax(label , axis = 1))[0])\nplt.imshow((image[0].numpy()/255).reshape(96 , 96 , 3))","metadata":{"id":"RViYQM4yKKu8","outputId":"77bef04f-9cbd-4be7-da83-558a9907b35b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{"id":"3EKdPBGpNp9R"}},{"cell_type":"code","source":"# Building EfficientNet model\nfrom tensorflow.keras.applications import EfficientNetB2\n\nbackbone = EfficientNetB2(\n    input_shape=(96, 96, 3),\n    include_top=False  # not including a final layers of EfficientNetB2\n)\n\nmodel = tf.keras.Sequential([\n    backbone,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(7, activation='softmax')\n])\n\nmodel.summary()","metadata":{"id":"FAqXI9w-KMoE","outputId":"4f2f216a-5d4f-43e3-9da4-127091cc3331"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compiling your model by providing the Optimizer , Loss and Metrics\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07),\n    loss = 'categorical_crossentropy',\n    metrics=['accuracy' , tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')]\n)","metadata":{"id":"lnNHDcr5KQys"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    train_dataset,\n    steps_per_epoch=len(X_train)//BATCH_SIZE,\n    epochs=12,\n    validation_data=val_dataset,\n    validation_steps = len(X_val)//BATCH_SIZE,\n    class_weight=class_weight\n)","metadata":{"id":"PJOhdO8mKU-d","outputId":"3d125e07-3d84-46cf-ae3a-574d9f2b60d5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we expect our model to just learn patterns in current data.\n# then we freeze all the layers and train only on last layer where we do classification, \n# this gives more importance to the classification layer, thus making it more accurate","metadata":{"id":"H7g5HiRoM0tl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# turnoff the backbone after tuning weights - to ensure the lesser chance of overfitting\nmodel.layers[0].trainable = False","metadata":{"id":"LrOKhPcuKmkH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining our callbacks \ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\"best_weights.h5\",\n                                                verbose=1,\n                                                save_best_only=True,\n                                                save_weights_only = True)\n\nearly_stop = tf.keras.callbacks.EarlyStopping(patience=4)","metadata":{"id":"pkfGV8V5MKHG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"id":"jeB-a6WjMKEU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    train_dataset,\n    steps_per_epoch=len(X_train)//BATCH_SIZE,\n    epochs=8,\n    callbacks=[checkpoint , early_stop],\n    validation_data=val_dataset,\n    validation_steps = len(X_val)//BATCH_SIZE,\n    class_weight=class_weight\n)","metadata":{"id":"CsKMveVtMJ_8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating the model","metadata":{"id":"DOhXVI5VOA84"}},{"cell_type":"code","source":"from tensorflow.keras.applications import EfficientNetB2\n\nbackbone = EfficientNetB2(\n    input_shape=(96, 96, 3),\n    include_top=False\n)\n\nmodel = tf.keras.Sequential([\n    backbone,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(7, activation='softmax')\n])\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07),\n    loss = 'categorical_crossentropy',\n    metrics=['accuracy' , tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')]\n)","metadata":{"id":"h6j7RWwxMJ9X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights(\"best_weights.h5\")","metadata":{"id":"zVGSlY1kMJ4l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a Dataset Object for 'Testing' Set just the way we did for Training and Validation\ntest_image_paths = list(test_path.glob(\"*/*\"))\ntest_image_paths = list(map(lambda x : str(x) , test_image_paths))\ntest_labels = list(map(lambda x : get_label(x) , test_image_paths))\n\ntest_labels = Le.transform(test_labels)\ntest_labels = tf.keras.utils.to_categorical(test_labels)\n\ntest_image_paths = tf.convert_to_tensor(test_image_paths)\ntest_labels = tf.convert_to_tensor(test_labels)\n\ndef decode_image(image , label):\n    image = tf.io.read_file(image)\n    image = tf.io.decode_jpeg(image , channels = 3)\n    image = tf.image.resize(image , [96 , 96] , method=\"bilinear\")\n    return image , label\n\ntest_dataset = (\n     tf.data.Dataset\n    .from_tensor_slices((test_image_paths, test_labels))\n    .map(decode_image)\n    .batch(BATCH_SIZE)\n)","metadata":{"id":"wPh1e8R8MJ11"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verify Test Dataset Object\nimage , label = next(iter(test_dataset))\nprint(image.shape)\nprint(label.shape)","metadata":{"id":"EMpPO9cNNQaf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View a sample Validation Image\nprint(Le.inverse_transform(np.argmax(label , axis = 1))[0])\nplt.imshow((image[0].numpy()/255).reshape(96 , 96 , 3))","metadata":{"id":"g2xcLkWRNRSf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluating the loaded model\nloss, acc, prec, rec = model.evaluate(test_dataset)\n\nprint(\" Testing Acc : \" , acc)\nprint(\" Testing Precision \" , prec)\nprint(\" Testing Recall \" , rec)","metadata":{"id":"2pA9c7WfNRPh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save Model\nmodel.save(\"FacialExpressionModel.h5\")","metadata":{"id":"8kmjEfT0NRJ9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save Label Encoder \nimport pickle\n\ndef save_object(obj , name):\n    pickle_obj = open(f\"{name}.pck\",\"wb\")\n    pickle.dump(obj, pickle_obj)\n    pickle_obj.close()","metadata":{"id":"LuPVzwx5NWGm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_object(Le, \"LabelEncoder\")","metadata":{"id":"4nyyr8wBNWD0"},"execution_count":null,"outputs":[]}]}