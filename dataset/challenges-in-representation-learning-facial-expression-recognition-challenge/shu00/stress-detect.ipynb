{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n'''\n0: -4593 images- Angry\n1: -547 images- Disgust\n2: -5121 images- Fear\n3: -8989 images- Happy\n4: -6077 images- Sad\n5: -4002 images- Surprise\n6: -6198 images- Neutral'''\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-26T17:13:06.333783Z","iopub.execute_input":"2021-05-26T17:13:06.334068Z","iopub.status.idle":"2021-05-26T17:13:06.867285Z","shell.execute_reply.started":"2021-05-26T17:13:06.334023Z","shell.execute_reply":"2021-05-26T17:13:06.866631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/challenges-in-representation-learning-facial-expression-recognition-challenge/icml_face_data.csv'\ndata = pd.read_csv(path)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-05-26T17:13:09.854652Z","iopub.execute_input":"2021-05-26T17:13:09.854959Z","iopub.status.idle":"2021-05-26T17:13:16.055158Z","shell.execute_reply.started":"2021-05-26T17:13:09.854903Z","shell.execute_reply":"2021-05-26T17:13:16.054466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data[data[\"emotion\"] != 1]\ndata.head() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"emotion\"].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"width, height = 48, 48\ndatapoints = data[' pixels'].tolist()\n\nX = []\nfor xseq in datapoints:\n    xx = [int(xp) for xp in xseq.split(' ')]\n    xx = np.asarray(xx).reshape(width, height)\n    X.append(xx.astype('float32'))\n\nX = np.asarray(X)\nX = np.expand_dims(X, -1)\nX\n#getting labels for training\ny = pd.get_dummies(data['emotion']).as_matrix()\n\n#storing them using numpy\n#np.save('fdataX', X)\n#np.save('flabels', y)\n\n#print(\"Preprocessing Done\")\n#print(\"Number of Features: \"+str(len(X[0])))\n#print(\"Number of Labels: \"+ str(len(y[0])))\n#print(\"Number of examples in dataset:\"+str(len(X)))\n#print(\"X,y stored in fdataX.npy and flabels.npy respectively\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys, os\nimport pickle\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\nprint(X,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.models import model_from_json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_features = 64\nnum_labels = 7\nbatch_size = 64\nepochs = 20\nwidth, height = 48, 48\n\n#x = np.load('./fdataX.npy')\n#y = np.load('./flabels.npy')\n\nX -= np.mean(X, axis=0)\nX /= np.std(X, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#splitting into training, validation and testing data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=41)\n\n#saving the test samples to be used later\n#np.save('modXtest', X_test)\n#np.save('modytest', y_test)\n\n#desinging the CNN\nmodel = Sequential()\n\nmodel.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu', input_shape=(width, height, 1), data_format='channels_last', kernel_regularizer=l2(0.01)))\nmodel.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(2*2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(2*2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(2*2*2*num_features, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(2*2*num_features, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(2*num_features, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(num_labels, activation='softmax'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=categorical_crossentropy,\n              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7),\n              metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(np.array(X_train), np.array(y_train),\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(np.array(X_valid), np.array(y_valid)),\n          shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history.history.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#saving the  model to be used later\nmodel.save('./')\nfer_json = model.to_json()\nwith open(\"fer.json\", \"w\") as json_file:\n    json_file.write(fer_json)\nmodel.save_weights(\"fer.h5\")\nprint(\"Saved model to disk\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}