{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras import models, layers\nimport tqdm\nfrom PIL import Image\n\nimport os\nfrom sklearn.metrics import multilabel_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom keras.applications import Xception\nfrom keras.applications import VGG19\nfrom keras.applications import ResNet50\nfrom keras.applications import InceptionResNetV2\n\nfrom keras.applications import MobileNet\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Dense, GlobalAveragePooling2D\n\nfrom keras.models import Model\n\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = \"/kaggle/input/challenges-in-representation-learning-facial-expression-recognition-challenge\"\n\ndata = pd.read_csv(os.path.join(PATH, \"icml_face_data.csv\"))\n\nemotions = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = (71,71,1)\nbatch_size = 32\ntarget_size = (71,71)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to parse data into right format\n# Output: Image in right shaped and normalized + labels\ndef parse_data(data):\n    image_array = np.zeros(shape=(len(data), 48, 48, 1))\n    image_label = np.array(list(map(int, data['emotion'])))\n    \n    for i, row in enumerate(data.index):\n        image = np.fromstring(data.loc[row, ' pixels'], dtype=int, sep=' ')\n        image = np.reshape(image, (48, 48, 1))\n        image_array[i] = image\n        \n    return image_array, image_label\n\n# Splitting the data into train, validation and testing set thanks to Usage column\ntrain_imgs, train_lbls = parse_data(data[data[\" Usage\"] == \"Training\"])\nval_imgs, val_lbls = parse_data(data[data[\" Usage\"] == \"PrivateTest\"])\ntest_imgs, test_lbls = parse_data(data[data[\" Usage\"] == \"PublicTest\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_xception = Xception(weights=None, include_top=False, input_shape=input_shape)\nmodel_vgg19 = VGG19(weights=None, include_top=False, input_shape=input_shape)\nmodel_resnet = ResNet50(weights=None, include_top=False, input_shape=input_shape)\nmodel_mobilenet = MobileNet(weights=None, include_top=False, input_shape=input_shape)\nmodel_xception.trainable = False\nmodel_vgg19.trainable = False\nmodel_resnet.trainable = False\nmodel_mobilenet.trainable = False\nmodel_vgg19.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"train shape\", np.shape(train_imgs))\nprint(\"validation shape\", np.shape(val_imgs))\nprint(\"validatio shape\", np.shape(val_imgs))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flat1 = model_vgg19.layers[-1].output\nflat1 = GlobalAveragePooling2D()(flat1)\nclass1 = layers.Dense(512, activation='selu')(flat1)\nclass1 = layers.Dense(256, activation='selu')(class1)\nclass1 = layers.Dense(512, activation='relu')(class1)\nclass1 = layers.Dense(256, activation='relu')(class1)\nclass1 = layers.Dense(512, activation='elu')(class1)\nclass1 = layers.Dense(256, activation='elu')(class1)\noutput = layers.Dense(7, activation='softmax')(class1)\n# define new model\nmodel = Model(inputs=model_vgg19.inputs, outputs=output)\n# summarize\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_addons as tfa\n\nopt = tfa.optimizers.RectifiedAdam(\n                 lr=0.00001,\n                 total_steps=10000,\n                 warmup_proportion=0.1,\n                 min_lr=1e-5)\nranger = tfa.optimizers.Lookahead(opt, sync_period=6, slow_step_size=0.5)\n\nmodel.compile(\n  loss = \"binary_crossentropy\",\n  optimizer=ranger,\n  metrics = [\"acc\"]\n)\n\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the model, and validating\nhistory = model.fit(train_imgs, train_lbls, \n          epochs=10, batch_size=256, \n          validation_data=(val_imgs, val_lbls), verbose=1, callbacks=[es])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train accuracy and validation accuracy vs epoch graph\nimport matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, label='Training acc')\nplt.plot(epochs, val_acc, label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, label='Training loss')\nplt.plot(epochs, val_loss, label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_prob = model.predict(test_imgs)\ntest_pred = np.argmax(test_prob, axis=1)\ntest_accuracy = np.mean(test_pred == test_lbls)\n\nprint(test_accuracy)\n\nconf_mat = confusion_matrix(test_lbls, test_pred)\n\npd.DataFrame(conf_mat, columns=emotions.values(), index=emotions.values())","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}