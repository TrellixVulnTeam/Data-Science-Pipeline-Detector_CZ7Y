{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n#https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import train and test data and view data format\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\ndata = pd.read_csv('../input/challenges-in-representation-learning-facial-expression-recognition-challenge/train.csv')\ndata.head()\n\ntest_data = pd.read_csv('../input/challenges-in-representation-learning-facial-expression-recognition-challenge/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(data['pixels']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create 3 channel numpy array of [num_examples, 3, 48, 48] \n#type for np.array dim 3 should be uint8 for conversion to pil image\ntrain_imgs = []\ntest_imgs = []\n\nfor image in data['pixels']:\n    img = image.split()\n    img = np.array([float(i) for i in img], dtype=np.uint8)\n    img = np.reshape(img, (48,48))\n    img = np.array([img]*3)\n    train_imgs.append(img)\ntrain_imgs = np.array(train_imgs)\ntrain_labels = [np.array([i]) for i in data['emotion'].to_numpy()]\n\nfor image in test_data['pixels']:\n    img = image.split()\n    img = np.array([float(i) for i in img], dtype=np.uint8)\n    img = np.reshape(img, (48,48))\n    img = np.array([img]*3)\n    test_imgs.append(img)\ntest_imgs = np.array(test_imgs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FaceDataset(Dataset):\n    \"\"\"Custom data set for faces and labels input as numpy array\"\"\"\n    def __init__(self, samples, labels=None, transform=None):\n\n        self.samples = samples\n        self.labels = labels\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        if self.labels == None:\n            sample = {'image': torch.from_numpy(self.samples[idx])}\n        else:\n            sample = {'image': torch.from_numpy(self.samples[idx]), 'label': torch.from_numpy(self.labels[idx])}\n\n        if self.transform:\n            sample['image'] = self.transform(sample['image'])\n\n        return sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#compose transforms and datasets\n\ntransform = transforms.Compose([transforms.ToPILImage(), \n                                transforms.Resize((224, 224)),\n                                transforms.RandomHorizontalFlip(p=0.5),\n                                transforms.RandomRotation(20),\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n\ntransform1 = transforms.Compose([transforms.ToPILImage(),\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n\ntemp_dataset = FaceDataset(train_imgs, train_labels, transform=transform)\n\nsplit = int(np.round(len(temp_dataset)*0.8))\n\ntrain_dataset, val_dataset = torch.utils.data.random_split(temp_dataset, [split, len(temp_dataset) - split])\ntest_dataset = FaceDataset(test_imgs, transform=transform1)\n\ntrain_loader = DataLoader(train_dataset, batch_size = 24, shuffle = True)\nval_loader = DataLoader(val_dataset, batch_size = 24, shuffle = True)\ntest_loader = DataLoader(test_dataset, batch_size = 24, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(next(iter(val_loader))['imag'].squeeze())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nexample = next(iter(train_dataset))\n\nplt.imshow(example['image'])\nprint(example['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nmodel = models.resnet18(pretrained=True)\n#print(resnet18)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = False\n\nmodel.fc = nn.Sequential(nn.Linear(512, 256), \n                        nn.ReLU(), \n                        nn.Linear(256, 100), \n                        nn.ReLU(),\n                        nn.Linear(100, 7))\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n\n#print(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training loop\nepochs = 20\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Training on Device: {device}\")\nfor epoch in range(epochs):\n    running_loss = []\n    \n    model.train()\n    model.to(device)\n    n=0\n    for item in train_loader:\n        n+=1\n        optimizer.zero_grad()\n        \n        image = item['image'].to(device)\n        label = item['label'].to(device)\n        label = label.squeeze()\n        output = model(image)\n        loss = criterion(output, label)\n        \n        loss.backward()\n        if n%200 == 0:\n            _, pred = torch.max(output.data, 1)\n            #print(f\"Output: {output}\")\n            #print(f\"Label: {label}\")\n            #print(f\"Pred: {pred}\")\n            \n        running_loss.append(loss.item())\n        \n        optimizer.step()\n        \n        #validation loop\n    with torch.no_grad():\n        model.eval()\n        total = 0\n        correct = 0\n        for item in val_loader:\n            image = item['image'].to(device)\n            label = item['label'].to(device)\n            label = label.squeeze()\n            output = model(image)\n            _, pred = torch.max(output.data, 1)\n            \n            total+= label.size(0)\n            correct += (pred == label).sum().item()\n            \n        print(f\"Epoch: {epoch}\")\n        print(f\"Training Loss: {sum(running_loss)/len(running_loss)}\")\n        print(f\"Validation Accuracy: {float(correct)/total}\")\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_loader) + len(val_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}