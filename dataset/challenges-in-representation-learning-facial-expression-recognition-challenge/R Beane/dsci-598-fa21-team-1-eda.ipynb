{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Facial Expression Recognition**\n## **Exploratory Data Analysis**\n### Alejandro Alemany, Sara Manrriquez, and Ben Zaretzky\n<br/>\nIn this notebook we explore the facial expression recognition dataset.","metadata":{}},{"cell_type":"markdown","source":"## Import Packages\nWe begin by importing the packages needed for this notebook.","metadata":{}},{"cell_type":"code","source":"# Import packages for EDA notebook\nimport os\nos.environ[\"KMP_SETTINGS\"] = \"false\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import load_model","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:12:03.27085Z","iopub.execute_input":"2021-12-08T03:12:03.271228Z","iopub.status.idle":"2021-12-08T03:12:09.467217Z","shell.execute_reply.started":"2021-12-08T03:12:03.271132Z","shell.execute_reply":"2021-12-08T03:12:09.466436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Training Data\nWe will now load the training data which consists of 28,709 facial images. The pixel values for each image are given in an string.","metadata":{}},{"cell_type":"code","source":"# Load the training data and then view the shape\ntrain = pd.read_csv('/kaggle/input/challenges-in-representation-learning-facial-expression-recognition-challenge/train.csv')\nprint(train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:12:09.468707Z","iopub.execute_input":"2021-12-08T03:12:09.468936Z","iopub.status.idle":"2021-12-08T03:12:14.505819Z","shell.execute_reply.started":"2021-12-08T03:12:09.468909Z","shell.execute_reply":"2021-12-08T03:12:14.504996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View the first five rows of the training data\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:12:15.415305Z","iopub.execute_input":"2021-12-08T03:12:15.415583Z","iopub.status.idle":"2021-12-08T03:12:15.435952Z","shell.execute_reply.started":"2021-12-08T03:12:15.415554Z","shell.execute_reply":"2021-12-08T03:12:15.435425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess Data\nThe pixels values for each image are converted from a string to an array. Then, the numerical labels are replaced with the corresponding emotion for increased interpretability.","metadata":{}},{"cell_type":"code","source":"# Convert the pixels values from a string to a numpy array\ntrain['pixels'] = [np.fromstring(x, dtype=int, sep=' ').reshape(-1,48,48,1) for x in train['pixels']]","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:12:17.261927Z","iopub.execute_input":"2021-12-08T03:12:17.262657Z","iopub.status.idle":"2021-12-08T03:12:20.679254Z","shell.execute_reply.started":"2021-12-08T03:12:17.262614Z","shell.execute_reply":"2021-12-08T03:12:20.678334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assign the emotions to the corresponding number and apply them to the DataFrame\nemotion_cat = {0:'Anger', 1:'Disgust', 2:'Fear', 3:'Happiness', 4: 'Sadness', 5: 'Surprise', 6: 'Neutral'}\ntrain['emotion'] = train['emotion'].apply(lambda x: emotion_cat[x])","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:12:20.680681Z","iopub.execute_input":"2021-12-08T03:12:20.680906Z","iopub.status.idle":"2021-12-08T03:12:20.695542Z","shell.execute_reply.started":"2021-12-08T03:12:20.680882Z","shell.execute_reply":"2021-12-08T03:12:20.694412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create variables for pixels and labels\npixels = np.concatenate(train['pixels'])\nlabels = train.emotion.values","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:12:20.696931Z","iopub.execute_input":"2021-12-08T03:12:20.697185Z","iopub.status.idle":"2021-12-08T03:12:21.512411Z","shell.execute_reply.started":"2021-12-08T03:12:20.697154Z","shell.execute_reply":"2021-12-08T03:12:21.511531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Label Distribution\nThe distribution of the labels are provided below. They are first presented in a table format and then in a bar chart.","metadata":{}},{"cell_type":"code","source":"# Calculate the proportions for each emotion\nemotion_prop = (train.emotion.value_counts() / len(train)).to_frame().sort_index(ascending=True)\nemotion_prop","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:12:21.514134Z","iopub.execute_input":"2021-12-08T03:12:21.514408Z","iopub.status.idle":"2021-12-08T03:12:21.542022Z","shell.execute_reply.started":"2021-12-08T03:12:21.514377Z","shell.execute_reply":"2021-12-08T03:12:21.541006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a bar chart for the labels\npalette = ['orchid', 'lightcoral', 'orange', 'gold', 'lightgreen', 'deepskyblue', 'cornflowerblue']\n\nplt.figure(figsize=[12,6])\n\nplt.bar(x=emotion_prop.index, height=emotion_prop['emotion'], color=palette, edgecolor='black')\n    \nplt.xlabel('Emotion')\nplt.ylabel('Proportion')\nplt.title('Emotion Label Proportions')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:12:21.54347Z","iopub.execute_input":"2021-12-08T03:12:21.543684Z","iopub.status.idle":"2021-12-08T03:12:21.743109Z","shell.execute_reply.started":"2021-12-08T03:12:21.543659Z","shell.execute_reply":"2021-12-08T03:12:21.742284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the table and bar chart, we can see that the proportion of images labeled as disgust is significantly lower than the other labels. Also, the proportion of images lableled as happy is signififcantly higher than the other labels. Both of these observations lead to the conclusion that we are not dealing with balanced classes. Imbalanced classes may lead to poor performances from CNN models, especially for the disgust label. Our intuition suggests that images displaying disgust will be confused with anger the most. A confusion matrix for our final model will show if the performance for the disgust class is worse than the other classes. ","metadata":{}},{"cell_type":"markdown","source":"## View Sample of Images\nIn this section, we will view five sample images from each emotion.","metadata":{}},{"cell_type":"code","source":"plt.close()\nplt.rcParams[\"figure.figsize\"] = [16,16]\n\nrow = 0\nfor emotion in np.unique(labels):\n\n    all_emotion_images = train[train['emotion'] == emotion]\n    for i in range(5):\n        \n        img = all_emotion_images.iloc[i,].pixels.reshape(48,48)\n        lab = emotion\n\n        plt.subplot(7,5,row+i+1)\n        plt.imshow(img, cmap='binary_r')\n        plt.text(-30, 5, s = str(lab), fontsize=10, color='b')\n        plt.axis('off')\n    row += 5\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:12:22.076335Z","iopub.execute_input":"2021-12-08T03:12:22.076603Z","iopub.status.idle":"2021-12-08T03:12:24.255614Z","shell.execute_reply.started":"2021-12-08T03:12:22.076575Z","shell.execute_reply":"2021-12-08T03:12:24.25468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use these sample images to get an idea of the distinguishable features for each emotion. The goal of our CNN model is to indentify these features through training and use them to correctly predict the emotion of images the model has not seen. We supsect the area around the mouth will play an important role in differentiating the emotions. For example, a wide circular mouth could indicate fear or surprise. Also, faces with lips close together could indicate sadness or neutral emotions. However, there will not be single feature that can distinguish one emotion from the other. There will be a combination of features that aid in the classification of each emotion. Class activation maps can help highlight key areas in an image that help determine the emotion. These will be discussed in further detail in another notebook.","metadata":{}},{"cell_type":"markdown","source":"# Image Augmentation\n\nImage augemenatiton is a technique used to artifically create data from a preexisting data set. The data is created by most commonly by flipping, rotating, shifting, zooming, and blurring. The newly created images are combined with the orginal training set with the hope of increasing the robustness and performance of a deep learning model.\n\nWe considered and experimented with image augmentation as a method of reducing the overfitting of previously created models. While this was not the only technique used to address overfitting, image augementation approaches it from the root of the problem, the training set. Below is a sample of the image augmentation used in building our final CNN model. We allowed images to be flipped, shifted, rotated, and zoomed in or out. ","metadata":{}},{"cell_type":"code","source":"# Create image generator\ntrain_datagen = ImageDataGenerator(\n    rotation_range = 30,\n    width_shift_range = 0.2, \n    height_shift_range = 0.2, \n    zoom_range = 0.2, \n    horizontal_flip = True, \n    fill_mode = 'nearest'\n)\n\n# Use only one image for illustrative example\nim_aug_example = train_datagen.flow(train['pixels'][0], batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:12:27.604757Z","iopub.execute_input":"2021-12-08T03:12:27.605035Z","iopub.status.idle":"2021-12-08T03:12:27.611926Z","shell.execute_reply.started":"2021-12-08T03:12:27.605007Z","shell.execute_reply":"2021-12-08T03:12:27.61131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create plot for image augmentation\nplt.close()\nplt.rcParams[\"figure.figsize\"] = [16,16]\n\nfor i in range(12):\n    # preparing the subplot\n    plt.subplot(3,4,i+1)\n    \n    # generating images in batches\n    batch = im_aug_example.next()\n    \n    # Remember to convert these images to unsigned integers for viewing \n    image = batch[0].astype('uint8')\n    \n    # Plotting the data\n    plt.imshow(image,cmap='binary_r')\n    \n# Displaying the figure\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:12:29.737407Z","iopub.execute_input":"2021-12-08T03:12:29.737915Z","iopub.status.idle":"2021-12-08T03:12:30.997709Z","shell.execute_reply.started":"2021-12-08T03:12:29.737861Z","shell.execute_reply":"2021-12-08T03:12:30.997109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see from the plot above that all twelve images contain the same face and therefore the same emotion. However, the images are all slightly different due to the augmentation. This helps inhibit our model's ability to memorize training instances. It also increases the size of our training set, which is helpful for smaller data sets.","metadata":{}},{"cell_type":"markdown","source":"# Filter Visualization\nFilter visualziation can give us an idea about how each filter in our model works. The following functions produce one image for each filter in our model. The filter is meant to be highly responsive to that image. Below we generate images for the first two convolutional layers in our final model.","metadata":{}},{"cell_type":"code","source":"# Load CNN model for filter visualization\ncnn = load_model('../input/dsci-598-fa21/team_01_model_05.h5')\ncnn.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:12:43.243027Z","iopub.execute_input":"2021-12-08T03:12:43.243717Z","iopub.status.idle":"2021-12-08T03:12:47.764581Z","shell.execute_reply.started":"2021-12-08T03:12:43.243676Z","shell.execute_reply":"2021-12-08T03:12:47.763649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create functions for filter visualization\ndef compute_loss(input_image, layer, filter_index):\n    feature_extractor = tf.keras.Model(inputs=cnn.inputs, outputs=layer.output)\n    activation = feature_extractor(input_image)\n    # We avoid border artifacts by only involving non-border pixels in the loss.\n    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n    return tf.reduce_mean(filter_activation)\n\ndef gradient_ascent_step(img, layer, filter_index, learning_rate):\n    with tf.GradientTape() as tape:\n        tape.watch(img)\n        loss = compute_loss(img, layer, filter_index)\n    # Compute gradients.\n    grads = tape.gradient(loss, img)\n    # Normalize gradients.\n    grads = tf.math.l2_normalize(grads)\n    img += learning_rate * grads\n    return loss, img\n\ndef initialize_image():\n    img = tf.random.uniform((1, 48, 48, 1))\n    # ResNet50V2 expects inputs in the range [-1, +1].\n    # Here we scale our random inputs to [-0.125, +0.125]\n    return (img - 0.5) * 0.25\n\n\ndef visualize_filter(layer, filter_index, steps, learning_rate):\n    img = initialize_image()\n    for iteration in range(steps):\n        loss, img = gradient_ascent_step(img, layer, filter_index, learning_rate)\n\n    # Decode the resulting input image\n    img = deprocess_image(img[0].numpy())\n    return loss, img\n\n\ndef deprocess_image(img):\n    # Normalize array: center on 0., ensure variance is 0.15\n    img -= img.mean()\n    img /= img.std() + 1e-5\n    img *= 0.15\n\n    # Clip to [0, 1]\n    img += 0.5\n    img = np.clip(img, 0, 1)\n\n    # Convert to RGB array\n    img *= 255\n    img = np.clip(img, 0, 255).astype(\"uint8\")\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:12:53.59799Z","iopub.execute_input":"2021-12-08T03:12:53.598642Z","iopub.status.idle":"2021-12-08T03:12:53.611156Z","shell.execute_reply.started":"2021-12-08T03:12:53.598605Z","shell.execute_reply":"2021-12-08T03:12:53.610179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_layer_filters(layer_name, steps=60, learning_rate=1):\n    layer = cnn.get_layer(name=layer_name)\n            \n    n_filters = layer.filters\n    n_cols = 8\n    n_rows = n_filters // n_cols\n    \n    print(f'{layer_name} - {n_filters} filters')\n    \n    plt.figure(figsize=[2*n_cols, 2*n_rows])\n    for i in range(n_filters):\n        plt.subplot(n_rows, n_cols, i+1)\n        loss, img = visualize_filter(layer, i, steps, learning_rate)\n        plt.imshow(img, cmap='binary_r')\n        plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:12:54.358421Z","iopub.execute_input":"2021-12-08T03:12:54.358698Z","iopub.status.idle":"2021-12-08T03:12:54.367976Z","shell.execute_reply.started":"2021-12-08T03:12:54.358669Z","shell.execute_reply":"2021-12-08T03:12:54.367029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_layer_filters('conv2d', steps=200)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:15:24.09175Z","iopub.execute_input":"2021-12-08T03:15:24.092065Z","iopub.status.idle":"2021-12-08T03:15:54.696748Z","shell.execute_reply.started":"2021-12-08T03:15:24.092033Z","shell.execute_reply":"2021-12-08T03:15:54.695844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These filter visualiztion can be hard to understand sometimes. We notice that some of the images contain vertical and horizontal lines of either a light or dark color at the edges. These could indicate that these filters attempt to find some pattern around the edge of an image. ","metadata":{}},{"cell_type":"code","source":"display_layer_filters('conv2d_1', steps=200)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:13:22.96868Z","iopub.execute_input":"2021-12-08T03:13:22.969406Z","iopub.status.idle":"2021-12-08T03:15:14.672187Z","shell.execute_reply.started":"2021-12-08T03:13:22.969364Z","shell.execute_reply":"2021-12-08T03:15:14.671284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compared to the first group of images, these images for the second convolution layer are more complex. Some of the images are almost filled with the same color, while others contain a zebra like pattern.","metadata":{}},{"cell_type":"markdown","source":"# Visualize Filter Activations\nInstead of creating an image that each filter is responsive to, we can visualize how each filter works on an image in our training set. We run the sample through our network and record where each filter is activated on that image and how strong it is. This allows us to get a sense of where our filters are looking for patterns. Our intuition sugguest that the mouth and eyes should play a vital role in determining emotion. Below we select two random images and view the activations of the first and second convolutional layers. ","metadata":{}},{"cell_type":"code","source":"# Create function to view filter acitvations\ndef display_layer(layer_index, activations, cmap):\n    layer_activations = activations[layer_index]\n    n_filters = layer_activations.shape[-1]\n       \n    n_cols = 8\n    n_rows = n_filters // n_cols\n    \n    print(f'{cnn.layers[layer_index].name} - {n_filters} Filters')\n    plt.figure(figsize=[2*n_cols, 2*n_rows])\n    \n    for i in range(n_filters):\n        img = layer_activations[0,:,:,i]\n        plt.subplot(n_rows, n_cols, i+1)\n        plt.imshow(img, cmap=cmap)\n        plt.axis('off')\n    plt.show() \n\n\ndef display_activations(img_tensor, layer_indices=[], cmap='viridis'):\n    layer_outputs = [layer.output for layer in cnn.layers]\n    activation_model = tf.keras.models.Model(inputs=cnn.inputs, outputs=layer_outputs)\n    activations = activation_model(img_tensor)\n    \n    for i in layer_indices:\n        display_layer(i, activations, cmap)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:16:02.134541Z","iopub.execute_input":"2021-12-08T03:16:02.135112Z","iopub.status.idle":"2021-12-08T03:16:02.14433Z","shell.execute_reply.started":"2021-12-08T03:16:02.135073Z","shell.execute_reply":"2021-12-08T03:16:02.143517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select two images and plot them\nrow0 = train.iloc[1000,:]\nimg0 = all_emotion_images.iloc[1000,].pixels.reshape(48,48)    \n\nrow1 = train.iloc[100,:]\nimg1 = all_emotion_images.iloc[100,].pixels.reshape(48,48)    \n\nplt.subplot(1,2,1)\nplt.imshow(img0, cmap='binary_r')\nplt.text(0, -2, row0[0], color='k')\nplt.axis('off')\n\nplt.subplot(1,2,2)\nplt.imshow(img1, cmap='binary_r')\nplt.text(0, -2, row1[0], color='k')\nplt.axis('off')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:16:02.73139Z","iopub.execute_input":"2021-12-08T03:16:02.731663Z","iopub.status.idle":"2021-12-08T03:16:02.94615Z","shell.execute_reply.started":"2021-12-08T03:16:02.731635Z","shell.execute_reply":"2021-12-08T03:16:02.945551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tensor0 = img0.reshape(1,48,48,1)/255\ntensor1 = img1.reshape(1,48,48,1)/255","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:16:05.652362Z","iopub.execute_input":"2021-12-08T03:16:05.652782Z","iopub.status.idle":"2021-12-08T03:16:05.6565Z","shell.execute_reply.started":"2021-12-08T03:16:05.652753Z","shell.execute_reply":"2021-12-08T03:16:05.655952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Filter Activations for the First Image","metadata":{}},{"cell_type":"code","source":"display_activations(tensor0, [1,4], cmap='viridis')","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:16:08.5698Z","iopub.execute_input":"2021-12-08T03:16:08.57025Z","iopub.status.idle":"2021-12-08T03:16:13.162507Z","shell.execute_reply.started":"2021-12-08T03:16:08.570208Z","shell.execute_reply":"2021-12-08T03:16:13.161705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Filter Activations for the Second Image","metadata":{}},{"cell_type":"code","source":"display_activations(tensor1, [1,4], cmap='viridis')","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:16:13.164148Z","iopub.execute_input":"2021-12-08T03:16:13.164361Z","iopub.status.idle":"2021-12-08T03:16:17.595125Z","shell.execute_reply.started":"2021-12-08T03:16:13.164334Z","shell.execute_reply":"2021-12-08T03:16:17.594107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see for some of the filters, the eyes and mouth are important areas. While other filters seem to focus on the outline of the face or the background. This is not a complete overview of how these filter works, but it does provide some evidence that our model is recognizing these facial features as factors that are helpful in determining emotion.","metadata":{}}]}