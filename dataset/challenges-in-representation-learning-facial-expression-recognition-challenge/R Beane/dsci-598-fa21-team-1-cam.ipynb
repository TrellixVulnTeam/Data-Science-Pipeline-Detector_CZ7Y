{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Facial Expression Recognition**\n## **Class Activation Maps**\n### Alejandro Alemany, Sara Manrriquez, and Benjamin Zaretzky","metadata":{}},{"cell_type":"markdown","source":"Class activation maps are used to visualize which pixels of an image contribute more to the output of a model. In this notebook we explore the effects facial features have on image classification using class activation maps. Specifically, we will use gradient-weighted class activation mapping, as known as Grad-CAM. Grad-CAM is class-specific and creates a heatmap based off input image, the trained CNN, and the class of interest.","metadata":{}},{"cell_type":"markdown","source":"## Import Packages","metadata":{}},{"cell_type":"markdown","source":"We import all necessary packages.","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KMP_SETTINGS\"] = \"false\"\nimport cv2\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom scipy import stats\nfrom sklearn.ensemble import VotingClassifier\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tqdm import tqdm ","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:23:05.089146Z","iopub.execute_input":"2021-12-10T20:23:05.089863Z","iopub.status.idle":"2021-12-10T20:23:07.087533Z","shell.execute_reply.started":"2021-12-10T20:23:05.089787Z","shell.execute_reply":"2021-12-10T20:23:07.086742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"markdown","source":"We load the full data set and isolate the test set. ","metadata":{}},{"cell_type":"code","source":"# Read in full data set\ndata = pd.read_csv('../input/challenges-in-representation-learning-facial-expression-recognition-challenge/icml_face_data.csv')\ndata.columns = ['emotion', 'Usage', 'pixels']\nprint(data.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:23:07.093053Z","iopub.execute_input":"2021-12-10T20:23:07.093531Z","iopub.status.idle":"2021-12-10T20:23:09.763819Z","shell.execute_reply.started":"2021-12-10T20:23:07.093492Z","shell.execute_reply":"2021-12-10T20:23:09.763008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View first five rows\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:23:09.765279Z","iopub.execute_input":"2021-12-10T20:23:09.765545Z","iopub.status.idle":"2021-12-10T20:23:09.779517Z","shell.execute_reply.started":"2021-12-10T20:23:09.76551Z","shell.execute_reply":"2021-12-10T20:23:09.778699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select only rows that are in the public or private test set\ntest = data.loc[data[\"Usage\"] != 'Training',['emotion','pixels']]\n#test.drop(columns='Usage', inplace=True)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:23:09.782039Z","iopub.execute_input":"2021-12-10T20:23:09.782509Z","iopub.status.idle":"2021-12-10T20:23:09.801279Z","shell.execute_reply.started":"2021-12-10T20:23:09.782471Z","shell.execute_reply":"2021-12-10T20:23:09.800608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess Data","metadata":{}},{"cell_type":"markdown","source":"The images for this training set are stored as a string. In order to visualize the images we need to process these strings into a 4D array of pixel values.","metadata":{}},{"cell_type":"code","source":"# Reshape the pixels\ntest['pixels'] = [np.fromstring(x, dtype=int, sep=' ').reshape(-1,48,48,1) for x in test['pixels']]","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:23:09.802577Z","iopub.execute_input":"2021-12-10T20:23:09.802848Z","iopub.status.idle":"2021-12-10T20:23:10.498878Z","shell.execute_reply.started":"2021-12-10T20:23:09.802815Z","shell.execute_reply":"2021-12-10T20:23:10.498105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine pixels into single array\npixels = np.concatenate(test['pixels'].values)\nprint(pixels.shape)\n# Separate emotion values\nlabels = test.emotion.values\nprint(labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:23:10.50005Z","iopub.execute_input":"2021-12-10T20:23:10.500926Z","iopub.status.idle":"2021-12-10T20:23:10.55132Z","shell.execute_reply.started":"2021-12-10T20:23:10.500888Z","shell.execute_reply":"2021-12-10T20:23:10.550434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardize the pixels values between 0 and 1\npixels = pixels / 255","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:23:10.552651Z","iopub.execute_input":"2021-12-10T20:23:10.552987Z","iopub.status.idle":"2021-12-10T20:23:10.608532Z","shell.execute_reply.started":"2021-12-10T20:23:10.552949Z","shell.execute_reply":"2021-12-10T20:23:10.607678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Model","metadata":{}},{"cell_type":"markdown","source":"We load our pretrained model from week 3. ","metadata":{}},{"cell_type":"code","source":"# Week 3 model\nwk3_model1 = load_model('../input/dsci-598-fa21/team_01_model_05.h5')","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:23:10.609967Z","iopub.execute_input":"2021-12-10T20:23:10.610316Z","iopub.status.idle":"2021-12-10T20:23:15.870955Z","shell.execute_reply.started":"2021-12-10T20:23:10.610276Z","shell.execute_reply":"2021-12-10T20:23:15.869081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Week 3 Model Predictions","metadata":{}},{"cell_type":"markdown","source":"We predict the labels for the test set.","metadata":{}},{"cell_type":"code","source":"# Compute the probabilities\nwk3_test_probs1 = wk3_model1.predict(pixels)\n# Compute the prediction\nwk3_test_pred1 = np.argmax(wk3_test_probs1, axis=1)\n\n# Add the prediction to the data\ntest['wk3_predictions'] = wk3_test_pred1\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:23:15.876202Z","iopub.execute_input":"2021-12-10T20:23:15.878219Z","iopub.status.idle":"2021-12-10T20:23:21.349185Z","shell.execute_reply.started":"2021-12-10T20:23:15.87818Z","shell.execute_reply":"2021-12-10T20:23:21.34638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Heatmap Functions","metadata":{}},{"cell_type":"markdown","source":"We create two functions to display the heatmaps. The first function creates a \"gradient model\" from our CNN. The second function creates a numpy array of the heatmap for an image. ","metadata":{}},{"cell_type":"code","source":"def create_grad_model(model):\n    for layer in reversed(model.layers):\n        if len(layer.output_shape) == 4:\n            last_conv_layer = layer.name\n            break\n\n    grad_model = tf.keras.models.Model(\n        inputs=[model.inputs],\n        outputs=[model.get_layer(last_conv_layer).output, model.output])\n    \n    return grad_model \n\ndef compute_heatmap(image, class_ix, grad_model):\n\n    with tf.GradientTape() as tape:\n        inputs = tf.cast(image, tf.float32)\n        (conv_outputs, predictions) = grad_model(inputs)\n        loss = predictions[:, class_ix]\n    grads = tape.gradient(loss, conv_outputs)\n\n    cast_conv_outputs = tf.cast(conv_outputs > 0, \"float32\")\n    cast_grads = tf.cast(grads > 0, \"float32\")\n    guided_grads = cast_conv_outputs * cast_grads * grads\n\n    conv_outputs = conv_outputs[0]\n    guided_grads = guided_grads[0]\n\n    weights = tf.reduce_mean(guided_grads, axis=(0, 1))\n\n    cam = tf.reduce_sum(tf.multiply(weights, conv_outputs), axis=-1)\n\n    (w, h) = (image.shape[2], image.shape[1])\n    heatmap = cv2.resize(cam.numpy(), (w, h))\n        \n    return heatmap","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:23:21.354438Z","iopub.execute_input":"2021-12-10T20:23:21.356041Z","iopub.status.idle":"2021-12-10T20:23:21.374687Z","shell.execute_reply.started":"2021-12-10T20:23:21.355993Z","shell.execute_reply":"2021-12-10T20:23:21.373586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sample of Images","metadata":{}},{"cell_type":"markdown","source":"We view a sample of the images within the test set. ","metadata":{}},{"cell_type":"code","source":"# Label values\nemotions = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:23:21.37848Z","iopub.execute_input":"2021-12-10T20:23:21.378789Z","iopub.status.idle":"2021-12-10T20:23:21.386787Z","shell.execute_reply.started":"2021-12-10T20:23:21.37874Z","shell.execute_reply":"2021-12-10T20:23:21.385979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot sample images of each emotion\nplt.close()\nplt.rcParams[\"figure.figsize\"] = [16,16]\n\nrow = 0\nfor emotion in np.unique(labels):\n\n    all_emotion_images = test[test['emotion'] == emotion]\n    for i in range(5):\n        \n        img = all_emotion_images.iloc[i,].pixels.reshape(48,48)\n        lab = emotions[emotion]\n\n        plt.subplot(7,5,row+i+1)\n        plt.imshow(img, cmap='binary_r')\n        plt.text(-30, 5, s = str(lab), fontsize=10, color='b')\n        plt.axis('off')\n    row += 5\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:23:21.388422Z","iopub.execute_input":"2021-12-10T20:23:21.38904Z","iopub.status.idle":"2021-12-10T20:23:23.661987Z","shell.execute_reply.started":"2021-12-10T20:23:21.388995Z","shell.execute_reply":"2021-12-10T20:23:23.660776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Week 3 Model with Grad-CAM","metadata":{}},{"cell_type":"markdown","source":"### Heatmap of Predictions","metadata":{}},{"cell_type":"markdown","source":"We plot the sample of images with heatmaps and predicted labels.","metadata":{}},{"cell_type":"code","source":"# Plot images with heatmap and predicted label\ngm = create_grad_model(wk3_model1)\n\nplt.figure(figsize=[16,16])\n\nfor i in range(36):\n    img = pixels[i,:,:,0]\n    p_dist = wk3_model1.predict(img.reshape(1, 48, 48, 1))\n    k = np.argmax(p_dist)\n    p = np.max(p_dist)\n\n    heatmap = compute_heatmap((img.reshape(1, 48, 48, 1)), 1, gm)\n\n    plt.subplot(6, 6, i+1)\n    plt.imshow(img, alpha=0.8, cmap='binary_r')\n    plt.imshow(heatmap, alpha=0.6, cmap='coolwarm')\n    plt.title(f'{emotions[k]} - ({emotions[k]} - {p:.4f})')\n    plt.axis('off')\n    \nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:23:23.664576Z","iopub.execute_input":"2021-12-10T20:23:23.664939Z","iopub.status.idle":"2021-12-10T20:23:31.193063Z","shell.execute_reply.started":"2021-12-10T20:23:23.664907Z","shell.execute_reply":"2021-12-10T20:23:31.191671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the display of heatmaps above, we can see the eye, cheek, and mouth regions play an important role in predicting facial expressions. ","metadata":{}},{"cell_type":"markdown","source":"### Heatmap with Distribution of Predictions","metadata":{}},{"cell_type":"markdown","source":"We plot the an image for each class with a heatmap distribution of prediction probabilities. ","metadata":{}},{"cell_type":"code","source":"# Create a list of images\nsel_imgs = []\nfor i in range(0,7):\n    index = labels.tolist().index(i)\n    sel_imgs.append(index)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:23:31.194555Z","iopub.execute_input":"2021-12-10T20:23:31.195033Z","iopub.status.idle":"2021-12-10T20:23:31.200868Z","shell.execute_reply.started":"2021-12-10T20:23:31.194998Z","shell.execute_reply":"2021-12-10T20:23:31.200197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot each image label with prediction probabilities\nfor n in sel_imgs:\n    img = pixels[n,:,:,0]\n    p_dist = wk3_model1.predict(img.reshape(1, 48, 48, 1))\n    k = np.argmax(p_dist)\n    \n    plt.figure(figsize=[10,3])\n    plt.subplot(1, 3, 1)\n    plt.imshow(img, cmap='binary_r')\n    plt.title(f'True Label: {emotions[labels[n]]}')\n    plt.axis('off')\n    \n    heatmap = compute_heatmap((img.reshape(1, 48, 48, 1)), 1, gm)\n    \n    plt.subplot(1, 3, 2)\n    plt.imshow(img, alpha=0.8, cmap='binary_r')\n    plt.imshow(heatmap, alpha=0.6, cmap='coolwarm')\n    plt.title(f'Predicted Label: {emotions[k]}')\n    plt.axis('off')\n    \n    plt.subplot(1, 3, 3)\n    plt.bar(emotions.values(), wk3_test_probs1[n, :], color='dodgerblue', edgecolor='k')\n    plt.xticks(rotation=45)\n    plt.ylim([0,1])\n    plt.title('Distribution of Predictions')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:23:31.202335Z","iopub.execute_input":"2021-12-10T20:23:31.202854Z","iopub.status.idle":"2021-12-10T20:23:34.114786Z","shell.execute_reply.started":"2021-12-10T20:23:31.202817Z","shell.execute_reply":"2021-12-10T20:23:34.114057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plots above, we can see that this model performs well at predicting the emotion happy; however, the model is not as confident predicting other emotions. For example, the model struggles to distringush between angry and sad. ","metadata":{}},{"cell_type":"markdown","source":"## Resources","metadata":{}},{"cell_type":"markdown","source":"[Class Activation Maps in Deep Learning](https://valentinaalto.medium.com/class-activation-maps-in-deep-learning-14101e2ec7e1)<br/>\n[Grad-CAM: Visual Explanations from Deep Networks](https://glassboxmedicine.com/2020/05/29/grad-cam-visual-explanations-from-deep-networks/)<br/>\n[Class Activation Maps (Cactus)](https://www.kaggle.com/drbeane/class-activation-maps-cactus#Heatmap-Functions)<br/>\n[Facial Expression Recognition with CNN & Grad-CAM](https://www.kaggle.com/baotramduong/facial-expression-recognition-with-cnn-grad-cam)","metadata":{}}]}