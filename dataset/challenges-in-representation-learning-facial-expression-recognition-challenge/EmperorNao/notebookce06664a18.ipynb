{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Deep_Emotion(nn.Module):\n    def __init__(self):\n        '''\n        Deep_Emotion class contains the network architecture.\n        '''\n        super(Deep_Emotion,self).__init__()\n        self.conv1 = nn.Conv2d(1,10,3)\n        self.conv2 = nn.Conv2d(10,10,3)\n        self.pool2 = nn.MaxPool2d(2,2)\n\n        self.conv3 = nn.Conv2d(10,10,3)\n        self.conv4 = nn.Conv2d(10,10,3)\n        self.pool4 = nn.MaxPool2d(2,2)\n\n        self.norm = nn.BatchNorm2d(10)\n\n        self.fc1 = nn.Linear(810,50)\n        self.fc2 = nn.Linear(50,8)\n\n        self.localization = nn.Sequential(\n            nn.Conv2d(1, 8, kernel_size=7),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True),\n            nn.Conv2d(8, 10, kernel_size=5),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True)\n        )\n\n        self.fc_loc = nn.Sequential(\n            nn.Linear(640, 32),\n            nn.ReLU(True),\n            nn.Linear(32, 3 * 2)\n        )\n        self.fc_loc[2].weight.data.zero_()\n        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n\n    def stn(self, x):\n        xs = self.localization(x)\n        xs = xs.view(-1, 640)\n        theta = self.fc_loc(xs)\n        theta = theta.view(-1, 2, 3)\n\n        grid = F.affine_grid(theta, x.size())\n        x = F.grid_sample(x, grid)\n        return x\n\n    def forward(self,input):\n        out = self.stn(input)\n\n        out = F.relu(self.conv1(out))\n        out = self.conv2(out)\n        out = F.relu(self.pool2(out))\n\n        out = F.relu(self.conv3(out))\n        out = self.norm(self.conv4(out))\n        out = F.relu(self.pool4(out))\n\n        out = F.dropout(out)\n        out = out.view(-1, 810)\n        out = F.relu(self.fc1(out))\n        out = self.fc2(out)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-06-25T07:57:54.761136Z","iopub.execute_input":"2022-06-25T07:57:54.761539Z","iopub.status.idle":"2022-06-25T07:57:54.805872Z","shell.execute_reply.started":"2022-06-25T07:57:54.761477Z","shell.execute_reply":"2022-06-25T07:57:54.802991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nclass Plain_Dataset(Dataset):\n    def __init__(self,csv_file,img_dir,datatype,transform):\n        '''\n        Pytorch Dataset class\n        params:-\n                 csv_file : the path of the csv file    (train, validation, test)\n                 img_dir  : the directory of the images (train, validation, test)\n                 datatype : string for searching along the image_dir (train, val, test)\n                 transform: pytorch transformation over the data\n        return :-\n                 image, labels\n        '''\n        self.csv_file = pd.read_csv(csv_file)\n        self.lables = self.csv_file['emotion']\n        self.img_dir = img_dir\n        self.transform = transform\n        self.datatype = datatype\n\n    def __len__(self):\n        return len(self.csv_file)\n\n    def __getitem__(self,idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        img = Image.open(self.img_dir+self.datatype+str(idx)+'.jpg')\n        lables = np.array(self.lables[idx])\n        lables = torch.from_numpy(lables).long()\n\n        if self.transform :\n            img = self.transform(img)\n        return img,lables\n\n#Helper function\ndef eval_data_dataloader(csv_file,img_dir,datatype,sample_number,transform= None):\n    '''\n    Helper function used to evaluate the Dataset class\n    params:-\n            csv_file : the path of the csv file    (train, validation, test)\n            img_dir  : the directory of the images (train, validation, test)\n            datatype : string for searching along the image_dir (train, val, test)\n            sample_number : any number from the data to be shown\n    '''\n    if transform is None :\n        transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n    dataset = Plain_Dataset(csv_file=csv_file,img_dir = img_dir,datatype = datatype,transform = transform)\n\n    label = dataset.__getitem__(sample_number)[1]\n    print(label)\n    imgg = dataset.__getitem__(sample_number)[0]\n    imgnumpy = imgg.numpy()\n    imgt = imgnumpy.squeeze()\n    plt.imshow(imgt)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T07:57:54.880798Z","iopub.execute_input":"2022-06-25T07:57:54.881177Z","iopub.status.idle":"2022-06-25T07:57:54.903642Z","shell.execute_reply.started":"2022-06-25T07:57:54.881141Z","shell.execute_reply":"2022-06-25T07:57:54.902524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\n\n\nclass Generate_data():\n    def __init__(self, datapath):\n        \"\"\"\n        Generate_data class\n        Two methods to be used\n        1-split_test\n        2-save_images\n        [Note] that you have to split the public and private from fer2013 file\n        \"\"\"\n        self.data_path = datapath\n\n    def split_test(self):\n        \"\"\"\n        Helper function to split the validation and train data from general train file.\n            params:-\n                data_path = path to the folder that contains the train data file\n        \"\"\"\n        print(self.data_path)\n        train_csv_path = os.path.join(self.data_path, 'train.csv')\n        print(train_csv_path)\n        train = pd.read_csv(train_csv_path)\n        validation_data = pd.DataFrame(train.iloc[:3589,:])\n        train_data = pd.DataFrame(train.iloc[3589:,:])\n        train_data.to_csv(\"train.csv\")\n        validation_data.to_csv( \"val.csv\")\n        print(\"Done splitting the test file into validation & final test file\")\n\n    def str_to_image(self, str_img = ' '):\n        '''\n        Convert string pixels from the csv file into image object\n            params:- take an image string\n            return :- return PIL image object\n        '''\n        imgarray_str = str_img.split(' ')\n        imgarray = np.asarray(imgarray_str,dtype=np.uint8).reshape(48,48)\n        return Image.fromarray(imgarray)\n\n    def save_images(self, datatype='train'):\n        '''\n        save_images is a function responsible for saving images from data files e.g(train, test) in a desired folder\n            params:-\n            datatype= str e.g (train, val, test)\n        '''\n        foldername= datatype\n        csvfile_path= datatype+'.csv'\n        if not os.path.exists(foldername):\n            os.mkdir(foldername)\n\n        data = pd.read_csv(csvfile_path)\n        images = data['pixels'] #dataframe to series pandas\n        numberofimages = images.shape[0]\n        for index in tqdm(range(numberofimages)):\n            img = self.str_to_image(images[index])\n            img.save(os.path.join(foldername,'{}{}.jpg'.format(datatype,index)),'JPEG')\n        print('Done saving {} data'.format((foldername)))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T07:57:54.983384Z","iopub.execute_input":"2022-06-25T07:57:54.983871Z","iopub.status.idle":"2022-06-25T07:57:55.005694Z","shell.execute_reply.started":"2022-06-25T07:57:54.983832Z","shell.execute_reply":"2022-06-25T07:57:55.00455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\nimport argparse\nimport numpy  as np\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torchvision import transforms\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef Train(epochs,train_loader,val_loader,criterion,optmizer,device):\n    '''\n    Training Loop\n    '''\n    print(\"===================================Start Training===================================\")\n    \n    best_val_ac = 0\n    for e in range(epochs):\n        train_loss = 0\n        validation_loss = 0\n        train_correct = 0\n        val_correct = 0\n        # Train the model  #\n        net.train()\n        \n        print(\"Train\")\n        for data, labels in tqdm(train_loader):\n            data, labels = data.to(device), labels.to(device)\n            optmizer.zero_grad()\n            outputs = net(data)\n            loss = criterion(outputs,labels)\n            loss.backward()\n            optmizer.step()\n            train_loss += loss.item()\n            _, preds = torch.max(outputs,1)\n            train_correct += torch.sum(preds == labels.data)\n\n        print(\"Eval\")\n        #validate the model#\n        net.eval()\n        for data,labels in tqdm(val_loader):\n            data, labels = data.to(device), labels.to(device)\n            val_outputs = net(data)\n            val_loss = criterion(val_outputs, labels)\n            validation_loss += val_loss.item()\n            _, val_preds = torch.max(val_outputs,1)\n            val_correct += torch.sum(val_preds == labels.data)\n\n        train_loss = train_loss/len(train_dataset)\n        train_acc = train_correct.double() / len(train_dataset)\n        validation_loss =  validation_loss / len(validation_dataset)\n        val_acc = val_correct.double() / len(validation_dataset)\n        print('Epoch: {} \\tTraining Loss: {:.8f} \\tValidation Loss {:.8f} \\tTraining Acuuarcy {:.3f}% \\tValidation Acuuarcy {:.3f}%'\n                                                           .format(e+1, train_loss,validation_loss,train_acc * 100, val_acc*100))\n        if val_acc > best_val_ac:\n            print(\"SAVING MODEL with val acc\", val_acc)\n            best_val_ac = val_acc\n            torch.save(net.state_dict(),'/kaggle/working/weights')\n    print(\"===================================Training Finished===================================\")","metadata":{"execution":{"iopub.status.busy":"2022-06-25T07:57:55.092597Z","iopub.execute_input":"2022-06-25T07:57:55.093273Z","iopub.status.idle":"2022-06-25T07:57:55.112454Z","shell.execute_reply.started":"2022-06-25T07:57:55.093215Z","shell.execute_reply":"2022-06-25T07:57:55.11142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir tmp","metadata":{"execution":{"iopub.status.busy":"2022-06-25T07:57:55.139276Z","iopub.execute_input":"2022-06-25T07:57:55.139558Z","iopub.status.idle":"2022-06-25T07:57:55.821178Z","shell.execute_reply.started":"2022-06-25T07:57:55.13953Z","shell.execute_reply":"2022-06-25T07:57:55.819936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nnoface = pd.read_csv(\"/kaggle/input/noface-data/tmp.csv\")\nnoface = pd.read_csv(\"/kaggle/input/noface-data/tmp.csv\").drop(columns=noface.columns[0])\nalldata = pd.read_csv(\"/kaggle/input/challenges-in-representation-learning-facial-expression-recognition-challenge/train.csv\")\n\nfull_data = alldata.append(noface)\nfull_data.to_csv(\"tmp/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-25T07:57:55.823736Z","iopub.execute_input":"2022-06-25T07:57:55.824137Z","iopub.status.idle":"2022-06-25T07:58:09.159692Z","shell.execute_reply.started":"2022-06-25T07:57:55.824096Z","shell.execute_reply":"2022-06-25T07:58:09.158734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_data['emotion'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T07:58:09.161063Z","iopub.execute_input":"2022-06-25T07:58:09.162101Z","iopub.status.idle":"2022-06-25T07:58:09.176033Z","shell.execute_reply.started":"2022-06-25T07:58:09.162058Z","shell.execute_reply":"2022-06-25T07:58:09.175145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndata = r\"tmp\"\n\n\ngenerate_dataset = Generate_data(data)\ngenerate_dataset.split_test()\ngenerate_dataset.save_images('train')\ngenerate_dataset.save_images('val')\n","metadata":{"execution":{"iopub.status.busy":"2022-06-25T07:58:09.178203Z","iopub.execute_input":"2022-06-25T07:58:09.178607Z","iopub.status.idle":"2022-06-25T07:58:47.042835Z","shell.execute_reply.started":"2022-06-25T07:58:09.178574Z","shell.execute_reply":"2022-06-25T07:58:47.041583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 50\nlr = 0.0005\nbatchsize = 256","metadata":{"execution":{"iopub.status.busy":"2022-06-25T07:58:47.044467Z","iopub.execute_input":"2022-06-25T07:58:47.04484Z","iopub.status.idle":"2022-06-25T07:58:47.049737Z","shell.execute_reply.started":"2022-06-25T07:58:47.044803Z","shell.execute_reply":"2022-06-25T07:58:47.048622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = Deep_Emotion()\nnet.to(device)\ntraincsv_file = 'train.csv'\nvalidationcsv_file = 'val.csv'\ntrain_img_dir = 'train/'\nvalidation_img_dir = 'val/'\n\ntransformation= transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\ntrain_dataset= Plain_Dataset(csv_file=traincsv_file, img_dir = train_img_dir, datatype = 'train', transform = transformation)\nvalidation_dataset= Plain_Dataset(csv_file=validationcsv_file, img_dir = validation_img_dir, datatype = 'val', transform = transformation)\ntrain_loader= DataLoader(train_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\nval_loader=   DataLoader(validation_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\ncweights = [8, 8, 8, 2, 8, 8, 3, 8]\nclass_weights = torch.FloatTensor(cweights).to(device)\ncriterion= nn.CrossEntropyLoss()\noptmizer= optim.Adam(net.parameters(),lr= lr)\nTrain(epochs, train_loader, val_loader, criterion, optmizer, device)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T07:58:47.051215Z","iopub.execute_input":"2022-06-25T07:58:47.051596Z","iopub.status.idle":"2022-06-25T08:12:24.977838Z","shell.execute_reply.started":"2022-06-25T07:58:47.051561Z","shell.execute_reply":"2022-06-25T08:12:24.976605Z"},"trusted":true},"execution_count":null,"outputs":[]}]}