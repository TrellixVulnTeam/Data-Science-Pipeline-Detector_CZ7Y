{"cells":[{"metadata":{"_uuid":"798121cb-abc3-4c32-be99-92266ecabe19","_cell_guid":"4944f663-ceb4-44f7-bcb7-04a2e08b4007","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom torch.utils import data\nfrom torch import nn\nimport time\nfrom IPython import display\nimport seaborn as sns\nimport tensorflow as tf\nimport numpy as np\nimport random\nfrom torch.nn import functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport lime\nfrom lime import lime_image\nfrom skimage.segmentation import mark_boundaries\n\n\n# load and preprocess data\nsource = pd.read_csv(\"../input/challenges-in-representation-learning-facial-expression-recognition-challenge/train.csv\")\nprocessed_data = []\nfor i in range(0, len(source)):\n    item = source.iloc[i, 1]\n    temp1 = [float(j) for j in item.split(\" \")]\n    if not any(temp1):\n        continue\n    temp2 = []\n    for j in range(0, 48):\n        temp2.append(temp1[48 * j: 48 * j + 48])\n    \n    if sum(temp2, []).count(np.argmax(np.bincount(sum(temp2, [])))) >= 48 * 48 / 2:\n        continue\n    temp3 = [torch.tensor([temp2]), source.iloc[i, 0]]\n    processed_data.append(temp3)\n\ntrain_lr, tune_lr, train_epochs, tune_epochs, batch_size = 0.001, 0.0001, 50, 50, 512\nrandom.shuffle(processed_data)\ntrain_iter = data.DataLoader(processed_data[:int(len(source) * 0.9)], batch_size, shuffle=True)\nvalid_iter = data.DataLoader(processed_data[int(len(source) * 0.9):], batch_size, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd59570b-0ed6-4c5c-9316-eb81b02c4bee","_cell_guid":"e54b253b-e42c-45fa-85a6-ac884b98b68b","trusted":true},"cell_type":"code","source":"# construct net\nstage1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(64),\n                       nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(64),\n                       nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(64),\n                       nn.MaxPool2d(kernel_size=2), nn.Dropout(p=0.5))\n\nstage2 = nn.Sequential(nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(128),\n                       nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(128),\n                       nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(128),\n                       nn.MaxPool2d(kernel_size=2), nn.Dropout(p=0.5))\n\nstage3 = nn.Sequential(nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(128),\n                       nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(128),\n                       nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(128),\n                       nn.MaxPool2d(kernel_size=2), nn.Dropout(p=0.5))\n\nstage4 = nn.Sequential(nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(128),\n                       nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(128),\n                       nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(128),\n                       nn.MaxPool2d(kernel_size=2), nn.Dropout(p=0.5))\n\nstage5 = nn.Sequential(nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(256),\n                       nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(256),\n                       nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(256),\n                       nn.MaxPool2d(kernel_size=2), nn.Dropout(p=0.5))\n\nnet = nn.Sequential(stage1, stage2, stage3, stage4, stage5, nn.Flatten(), nn.Linear(256, 1024), nn.ReLU(), nn.Dropout(p=0.5), nn.Linear(1024, 1024), nn.ReLU(), nn.Dropout(p=0.5), nn.Linear(1024, 7))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"655d421c-5a49-4b2c-9e3b-28a9917b16e7","_cell_guid":"4e18849e-7177-48ef-93ce-46efe76f3a1d","trusted":true},"cell_type":"code","source":"# train\nargmax = lambda x, *args, **kwargs: x.argmax(*args, **kwargs)\nastype = lambda x, *args, **kwargs: x.type(*args, **kwargs)\nreduce_sum = lambda x, *args, **kwargs: x.sum(*args, **kwargs)\nsize = lambda x, *args, **kwargs: x.numel(*args, **kwargs)\n\n\nclass Timer:\n    def __init__(self):\n        self.times = []\n        self.start()\n\n    def start(self):\n        self.tik = time.time()\n\n    def stop(self):\n        self.times.append(time.time() - self.tik)\n        return self.times[-1]\n\n    def sum(self):\n        return sum(self.times)\n\n\nclass Accumulator:\n    def __init__(self, n):\n        self.data = [0.0] * n\n\n    def add(self, *args):\n        self.data = [a + float(b) for a, b in zip(self.data, args)]\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n\ndef set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n    axes.set_xlabel(xlabel)\n    axes.set_ylabel(ylabel)\n    axes.set_xscale(xscale)\n    axes.set_yscale(yscale)\n    axes.set_xlim(xlim)\n    axes.set_ylim(ylim)\n    if legend:\n        axes.legend(legend)\n    axes.grid()\n\n\nclass Animator:\n    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n                 ylim=None, xscale='linear', yscale='linear',\n                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n                 figsize=(3.5, 2.5)):\n        if legend is None:\n            legend = []\n        display.set_matplotlib_formats('svg')\n        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n        if nrows * ncols == 1:\n            self.axes = [self.axes, ]\n        self.config_axes = lambda: set_axes(\n            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n        self.X, self.Y, self.fmts = None, None, fmts\n\n    def add(self, x, y):\n        if not hasattr(y, \"__len__\"):\n            y = [y]\n        n = len(y)\n        if not hasattr(x, \"__len__\"):\n            x = [x] * n\n        if not self.X:\n            self.X = [[] for _ in range(n)]\n        if not self.Y:\n            self.Y = [[] for _ in range(n)]\n        for i, (a, b) in enumerate(zip(x, y)):\n            if a is not None and b is not None:\n                self.X[i].append(a)\n                self.Y[i].append(b)\n        self.axes[0].cla()\n        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n            self.axes[0].plot(x, y, fmt)\n        self.config_axes()\n        display.clear_output(wait=True)\n\n\ndef try_gpu(i=0):\n    if torch.cuda.device_count() >= i + 1:\n        return torch.device(f'cuda:{i}')\n    return torch.device('cpu')\n\n\ndef accuracy(y_hat, y):\n    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n        y_hat = argmax(y_hat, axis=1)\n    cmp = astype(y_hat, y.dtype) == y\n    return float(reduce_sum(astype(cmp, y.dtype)))\n\n\ndef evaluate_accuracy_gpu(net, data_iter, device=None):\n    net.eval()\n    if not device:\n        device = next(iter(net.parameters())).device\n    metric = Accumulator(2)\n    for X, y in data_iter:\n        X, y = X.to(device), y.to(device)\n        metric.add(accuracy(net(X), y), size(y))\n    return metric[0] / metric[1]\n\n\ndef train_plot(net, train_iter, valid_iter, num_epochs, opt, tune, \n               device=try_gpu()):\n    def init_weights(m):\n        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n            torch.nn.init.xavier_uniform_(m.weight)\n    if tune == False:\n        net.apply(init_weights)\n    print('training on', device)\n    net.to(device)\n    optimizer = opt\n    loss = nn.CrossEntropyLoss()\n    animator = Animator(xlabel='epoch', xlim=[0, num_epochs],\n                        legend=['loss', 'train acc', 'valid acc'])\n    timer = Timer()\n    best_acc = 0\n    \n    for epoch in range(num_epochs):\n        metric = Accumulator(3)\n        for i, (X, y) in enumerate(train_iter):\n            timer.start()\n            net.train()\n            optimizer.zero_grad()\n            X, y = X.to(device), y.to(device)\n            y_hat = net(X)\n            l = loss(y_hat, y)\n            l.backward()\n            optimizer.step()\n            with torch.no_grad():\n                metric.add(l * X.shape[0], accuracy(y_hat, y), X.shape[0])\n            timer.stop()\n            train_loss = metric[0] / metric[2]\n            train_acc = metric[1] / metric[2]\n\n            if (i + 1) % 50 == 0:\n                animator.add(epoch + i / len(train_iter),\n                             (train_loss, train_acc, None))\n        valid_acc = evaluate_accuracy_gpu(net, valid_iter)\n        if valid_acc > best_acc:\n            print(\"New best accuracy: {}\".format(valid_acc))\n            best_acc = valid_acc\n            if tune == False:\n                torch.save(net.state_dict(), \"Net.param\")\n            else:\n                torch.save(net.state_dict(), \"Net (fine-tune).param\")\n        animator.add(epoch + 1, (None, None, valid_acc))\n        print(\"Epoch: {}/{}, train accuray: {}, valid accuracy: {}\".format(epoch, num_epochs, train_acc, valid_acc))\n    print(f'loss {train_loss:.3f}, train acc {train_acc:.3f}, '\n          f'valid acc {valid_acc:.3f}')\n    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n          f'on {str(device)}')\n    if tune == False:\n        plt.savefig(\"Result.jpg\")\n    else:\n        plt.savefig(\"Result (fine-tune).jpg\")\n    plt.show()\n\n\ntrain_plot(net, train_iter, valid_iter, train_epoches, torch.optim.Adam(net.parameters(), lr=train_lr), False)\nplt.savefig(\"Result.jpg\")\nplt.show()\ntorch.save(net.state_dict(), \"VGG.param\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c79fb04-6628-45c4-b850-dd9aa6701777","_cell_guid":"c3a0cd66-fbfc-43fc-abd5-5881972bb086","trusted":true},"cell_type":"code","source":"# fine-tune\nnet.load_state_dict(torch.load(\"./Net.param\"))\ntrain_plot(net, train_iter, valid_iter, tune_epochs, torch.optim.SGD(net.parameters(), lr=tune_lr), True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1680e3a7-5cc4-4e90-80f0-9b97514d75ea","_cell_guid":"0b3cfc4b-7b7e-479b-8f75-441b570b71ba","trusted":true},"cell_type":"code","source":"# predict\nnet.load_state_dict(torch.load(\"./Net (fine-tune).param\"))\nnet.eval()\n\ntest = pd.read_csv(\"../input/challenges-in-representation-learning-facial-expression-recognition-challenge/test.csv\")\n\nresult = []\nfor i in range(0, len(test)):\n    map = torch.tensor([float(j) for j in test.iloc[i, 0].split(\" \")]).reshape(48 ,48).unsqueeze(0).unsqueeze(0).to(try_gpu())\n    arr = net(map)\n    result.append(int((torch.max(arr, dim=-1)).indices.cpu()))\n\nindex = [i for i in range(1, len(test) + 1)]\noutput = pd.DataFrame({'ID': index, 'emotion': result})\noutput.to_csv('Answer.csv',index = None,encoding = 'utf8')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00793296-9e0f-45fe-a695-b03f17a0a85e","_cell_guid":"1fd590c1-0e3d-4e00-a4bf-063cc2d50034","trusted":true},"cell_type":"code","source":"# saliency map\nemotion_dict = {0: \"Angry\", 1: \"Disgust\", 2: \"Fear\", 3: \"Happy\", 4: \"Sad\", 5: \"Surprise\", 6: \"Neutral\"}\n\n\ndef compute_saliency_map(series, model):\n    fig = torch.unsqueeze(processed_data[series][0], dim=0)\n    model.eval()\n    X_var = torch.autograd.Variable(fig, requires_grad=True)\n    scores = model(X_var.to(try_gpu()))[0]\n    scores.backward(torch.FloatTensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]).to(try_gpu()))\n    saliency_map = X_var.grad.data\n    saliency_map = saliency_map.abs()\n    saliency_map, i = torch.max(saliency_map, dim=1)\n    saliency_map = saliency_map.squeeze()\n    \n    return saliency_map\n\n\ndef plot_saliency_maps(true_type, predict_type, num=5):\n    for i in range(0, num):\n        trial = random.randint(0, len(processed_data))\n        pred = net(processed_data[trial][0].unsqueeze(0).to(try_gpu()))\n        pred = int((torch.max(pred, dim=-1)).indices.cpu())\n        while (pred != predict_type or processed_data[trial][1] != true_type):\n            trial = random.randint(0, len(processed_data))\n            pred = net(processed_data[trial][0].unsqueeze(0).to(try_gpu()))\n            pred = int((torch.max(pred, dim=-1)).indices.cpu())\n        plt.subplot(2, num, i + 1)\n        plt.imshow(processed_data[trial][0][0], cmap=\"gray\")\n        plt.axis('off')\n        plt.subplot(2, num, num + i + 1)\n        saliency = compute_saliency_map(trial, net)\n        plt.imshow(saliency)\n        plt.axis('off')\n        plt.gcf().set_size_inches(12, 5)\n    plt.savefig(emotion_dict[true_type] + \"(\" + emotion_dict[predict_type] + \")\")\n    plt.show()\n\n\nfor i in range(0, 7):\n    plot_saliency_maps(i, i)\nplot_saliency_maps(1, 0)\nplot_saliency_maps(2, 4)\nplot_saliency_maps(2, 5)\nplot_saliency_maps(6, 4)\nplot_saliency_maps(4, 6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"365b2ada-95eb-4458-bd52-6b12e781b4fb","_cell_guid":"d6fbb772-4ca3-4dee-a58f-4b71b90f205c","trusted":true},"cell_type":"code","source":"# gradient ascent\ndef part_calculate(net, stage, layer, channel, matrix):\n    ret = matrix\n    stage = stage - 1\n    for i in range(0, stage):\n        ret = net[i](ret)\n    if layer >= 1:\n        ret = net[stage][0](ret)\n    if layer >= 2:\n        ret = net[stage][1](ret)\n        ret = net[stage][2](ret)\n        ret = net[stage][3](ret)\n    if layer >= 3:\n        ret = net[stage][4](ret)\n        ret = net[stage][5](ret)\n        ret = net[stage][6](ret)\n    if layer >= 4:\n        ret = net[stage][7](ret)\n        ret = net[stage][8](ret)\n        ret = net[stage][9](ret)\n    return ret[0][channel-1]\n\n\ndef gradient_ascent(net, pic, lr, decay, epochs, stage, layer, channel):\n    for round in range(0, epochs):\n        loss = part_calculate(net, stage, layer, channel, pic).sum() * (-1)\n        loss.backward()\n        with torch.no_grad():\n            pic.data.sub_(lr * pic.grad)\n            for i in range(0, 48):\n                for j in range(0, 48):\n                    if pic[0][0][i][j] < 0:\n                        pic[0][0][i][j] = 0\n                    elif pic[0][0][i][j] > 255:\n                        pic[0][0][i][j] = 255\n        pic.grad.zero_()\n        lr = lr * decay\n        # print(\"Epoch: {}, sum: {}\".format(round, -loss))\n    return pic\n\n\ndef find_best_activation(net, stage, layer, channel):\n    X = torch.rand(size=(1, 1, 48, 48)).to(device='cuda') * 255\n    X.requires_grad_(True)\n    best = gradient_ascent(net, X, 100, 0.99, 300, stage, layer, channel)\n    plt.imshow(best.cpu().detach().numpy()[0][0], cmap=\"gray\")\n    plt.savefig(\"stage\" + str(stage) + \" layer\" + str(layer) + \" channel\" + str(channel) + \".jpg\")\n    plt.show()\n\nfor i in range(1, 6):\n    for j in range(1, 5):\n        find_best_activation(net, i, j, 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74600bab-2326-42cf-b5a8-11fc9e99df13","_cell_guid":"87e00954-3518-4fc5-8e7e-959cbadb8680","trusted":true},"cell_type":"code","source":"# filter output\ndef plot_intermediate_layer(net, stage, layer, channel):\n    img = part_calculate(net, stage, layer, channel, processed_data[9][0].unsqueeze(0).cuda())\n    plt.imshow(img.cpu().detach().numpy(), cmap=\"gray\")\n    plt.savefig(\"stage\" + str(stage) + \" layer\" + str(layer) + \" channel\" + str(channel) + \".jpg\")\n    plt.show()\n\nfor i in range(1, 6):\n    for j in range(1, 5):\n        plot_intermediate_layer(net, i, j, 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0574f945-c5d0-4372-b0ec-311203254c67","_cell_guid":"033d6445-b52c-4e6b-87b1-75e26e690992","trusted":true},"cell_type":"code","source":"# confusion matrix\ny_pred = []\ny_true = []\nfor X, y in valid_iter:\n    X = X.to(try_gpu())\n    ans = net(X)\n    for i in range(0, len(ans)):\n        temp = list(ans[i])\n        y_pred.append(temp.index(max(temp)))\n        y_true.append(int(y[i]))\n\nconfusion_matrix = tf.math.confusion_matrix(labels=y_true, predictions=y_pred).numpy()\nconfusion_matrix = np.around(confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis], decimals=2)\nconfusion_matrix = pd.DataFrame(confusion_matrix, index=emotion_dict.values(), columns=emotion_dict.values())\n\nfigure = plt.figure(figsize=(8, 8))\nsns.heatmap(confusion_matrix, annot=True, cmap=plt.cm.Blues)\nplt.tight_layout()\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.savefig(\"Confusion matrix.jpg\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lime\ndef predict(images):\n    trans = transforms.Compose([transforms.ToTensor()])\n    batch = torch.stack(tuple(trans(i) for i in images), dim=0)\n    gray_pic = torch.zeros(size=(batch.size()[0], 1, 48, 48), device='cuda')\n    for i in range(0, batch.size()[0]):\n        gray_pic[i] = batch[i][0].unsqueeze(0)\n    gray_pic.cuda()\n    logits = net(gray_pic)\n    probs = F.softmax(logits, dim=1)\n    return probs.detach().cpu().numpy()\n\n\ndef analyse(series):\n    explainer = lime_image.LimeImageExplainer()\n    explanation = explainer.explain_instance(processed_data[series][0][0], predict, top_labels=7, num_samples=1000)\n    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, hide_rest=False)\n    img_boundry = mark_boundaries(temp/255, mask)\n    return img_boundry\n\n\ndef plot_lime_maps(true_type, predict_type, num=5):\n    for i in range(0, num):\n        trial = random.randint(0, len(processed_data))\n        pred = net(processed_data[trial][0].unsqueeze(0).to(device='cuda'))\n        pred = int((torch.max(pred, dim=-1)).indices.cpu())\n        while (pred != predict_type or processed_data[trial][1] != true_type):\n            trial = random.randint(0, len(processed_data))\n            pred = net(processed_data[trial][0].unsqueeze(0).to(device='cuda'))\n            pred = int((torch.max(pred, dim=-1)).indices.cpu())\n        plt.subplot(2, num, i + 1)\n        plt.imshow(processed_data[trial][0][0], cmap=\"gray\")\n        plt.axis('off')\n        plt.subplot(2, num, num + i + 1)\n        boundary = analyse(trial)\n        plt.imshow(boundary)\n        plt.axis('off')\n        plt.gcf().set_size_inches(12, 5)\n    plt.savefig(emotion_dict[true_type] + \"(\" + emotion_dict[predict_type] + \")\")\n    plt.show()\n\n\nfor i in range(0, 7):\n    plot_lime_maps(i, i)\nplot_lime_maps(1, 0)\nplot_lime_maps(2, 4)\nplot_lime_maps(2, 5)\nplot_lime_maps(6, 4)\nplot_lime_maps(4, 6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filter analyse\ndef filter_analyse(series, num=5):\n    image = processed_data[series][0][0].unsqueeze(0).unsqueeze(0).cuda()\n    for i in range(1, 6):\n        for j in range(1, 5):\n            for k in range(0, num):\n                sample = random.randint(1, 64)\n                output = part_calculate(net, i, j, sample, image)\n                plt.subplot(20, num, 20 * i + 5 * j + k - 24)\n                plt.imshow(output.cpu().detach().numpy(), cmap=\"gray\")\n                plt.axis('off')\n                plt.gcf().set_size_inches(10, 50)\n                plt.savefig('Filter analyse')\n    plt.show()\n\nfilter_analyse(16)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}