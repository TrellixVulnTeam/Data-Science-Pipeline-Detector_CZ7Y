{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Final Report Notebook","metadata":{}},{"cell_type":"markdown","source":"## Team Members:\n\n#### Alejandro Alemany, Benjamin Zaretzky, Sara Manrriquez","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents\n\n* [Project Description](#Project_Description)\n* [Preparing Data](#Preparing_Data)\n* [Visualizations](#Visualizations)\n* [Technical Challenges](#Technical_Challenges)\n* [Weekly Progress](#Week_Progress)\n    - [Weekly #1](#Week_1)\n    - [Weekly #2](#Week_2)\n    - [Weekly #3](#Week_3)\n    - [Weekly #4](#Week_4)\n* [Summary of Model Architectures](#Summary_of_Model_Architectures)\n    - [VGG Architecture](#VGG_Architecture)\n    - [ResNet50 Architecture](#ResNet50_Architecture)\n    - [Xception Architecture](#Xception_Architecture)\n* [Summary of final results](#Summary_of_final_results)\n    - [Classification Report](#CR)\n    - [Confussion Matrix](#Confussion_Matrix)\n    - [Top K Accuracy score](#Top_K_Accuracy_score)\n    ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Project_Description\"></a>\n# Project Description:\n\nArtificial intelligence has been increasingly implicated in any scope of the life of human beings. Technologies adapt to human needs, and artificial intelligence makes this adaptation possible between technology and human beings. \n\nThere are AI techniques very present in algorithms for the recognition of human expressions. When human beings try to communicate with other people, a very high percentage is represented by communication, not verbal. Many studies show that facial expressions have a link with human emotions. The capacity of human beings to detect and identify these emotions makes it possible for us to understand each other. The main objective of this part of artificial intelligence is to use learning techniques so that the machine can identify these emotions.\n\nFacial expression recognition is a part of intelligence whose main objective is to recognize primary affective expression forms on people's faces. But can we offer Machine learning and Deep learning techniques to identify these efficient enough facial expressions? \n\nIn this project, we will apply the mechanisms these technologies offer us to recognize feelings or emotions in people. We will first process and obtain the information we want from a database to do this. This database is a series of pixels from images of people showing different feelings. ","metadata":{}},{"cell_type":"markdown","source":"#### Before moving forward in the project, we will explore and get familiar with the dataset.","metadata":{}},{"cell_type":"code","source":"%%capture\n\n!pip install --upgrade scikit-learn","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:12:00.926536Z","iopub.execute_input":"2021-12-13T01:12:00.926882Z","iopub.status.idle":"2021-12-13T01:12:09.501465Z","shell.execute_reply.started":"2021-12-13T01:12:00.92684Z","shell.execute_reply":"2021-12-13T01:12:09.500016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading libraries needed in the project\n\n%config Completer.use_jedi = False\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pickle\nimport scikitplot\n\nfrom sklearn.model_selection import train_test_split \nfrom tensorflow.keras.models import load_model\nfrom sklearn.metrics import classification_report\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import Xception\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import top_k_accuracy_score","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:12:09.506661Z","iopub.execute_input":"2021-12-13T01:12:09.50697Z","iopub.status.idle":"2021-12-13T01:12:09.527726Z","shell.execute_reply.started":"2021-12-13T01:12:09.506938Z","shell.execute_reply":"2021-12-13T01:12:09.52667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Preparing_Data\"></a>\n# Preparing Data\n\nThis project provides a training set and a test set. We will be using both sets in this notebook for demonstration purposes. ","metadata":{}},{"cell_type":"code","source":"# Load the training data\ntrain = pd.read_csv('../input/challenges-in-representation-learning-facial-expression-recognition-challenge/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:12:09.529318Z","iopub.execute_input":"2021-12-13T01:12:09.529564Z","iopub.status.idle":"2021-12-13T01:12:12.116504Z","shell.execute_reply.started":"2021-12-13T01:12:09.529535Z","shell.execute_reply":"2021-12-13T01:12:12.115241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View the first few rows of the data set\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:12:12.119577Z","iopub.execute_input":"2021-12-13T01:12:12.119968Z","iopub.status.idle":"2021-12-13T01:12:12.133594Z","shell.execute_reply.started":"2021-12-13T01:12:12.119923Z","shell.execute_reply":"2021-12-13T01:12:12.13245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After exploring the dataset, we found the first challenge. We do not have pictures to train our model. Instead, we have the pixels from each image. We need to convert those pixels to a format to visualize and prepare our algorithms. ","metadata":{}},{"cell_type":"code","source":"# Convert the pixels values from a string to a numpy array\ntrain['pixels'] = [np.fromstring(x, dtype=int, sep=' ').reshape(-1,48,48,1) for x in train['pixels']]","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:12:12.135134Z","iopub.execute_input":"2021-12-13T01:12:12.135399Z","iopub.status.idle":"2021-12-13T01:12:14.468802Z","shell.execute_reply.started":"2021-12-13T01:12:12.13537Z","shell.execute_reply":"2021-12-13T01:12:14.467718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Visualizations\"></a>\n# Visualizations\n\nAfter reviewing the labels distributions, we can see that the distributions are similar for most labels. However, \"disgust\" only accounts for a small percentage. ","metadata":{}},{"cell_type":"code","source":"# Assign the emotions to the corresponding number and apply them to the DataFrame\nemotion_cat = {0:'Anger', 1:'Disgust', 2:'Fear', 3:'Happiness', 4: 'Sadness', 5: 'Surprise', 6: 'Neutral'}\ntrain['emotion'] = train['emotion'].apply(lambda x: emotion_cat[x])\n\n# Create variables for pixels and labels\npixels = np.concatenate(train['pixels'])\nlabels = train.emotion.values\n\nemotion_prop = (train.emotion.value_counts() / len(train)).to_frame().sort_index(ascending=True)\n\n# Create a bar chart for the labels\npalette = ['orchid', 'lightcoral', 'orange', 'gold', 'lightgreen', 'deepskyblue', 'cornflowerblue']\n\nplt.figure(figsize=[12,6])\n\nplt.bar(x=emotion_prop.index, height=emotion_prop['emotion'], color=palette, edgecolor='black')\n    \nplt.xlabel('Emotion')\nplt.ylabel('Proportion')\nplt.title('Emotion Label Proportions')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:12:14.470463Z","iopub.execute_input":"2021-12-13T01:12:14.470725Z","iopub.status.idle":"2021-12-13T01:12:15.026358Z","shell.execute_reply.started":"2021-12-13T01:12:14.470695Z","shell.execute_reply":"2021-12-13T01:12:15.025659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View sample images for each emotion label\nplt.close()\nplt.rcParams[\"figure.figsize\"] = [16,16]\n\nrow = 0\nfor emotion in np.unique(labels):\n\n    all_emotion_images = train[train['emotion'] == emotion]\n    for i in range(5):\n        \n        img = all_emotion_images.iloc[i,].pixels.reshape(48,48)\n        lab = emotion\n\n        plt.subplot(7,5,row+i+1)\n        plt.imshow(img, cmap='binary_r')\n        plt.text(-30, 5, s = str(lab), fontsize=10, color='b')\n        plt.axis('off')\n    row += 5\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:12:15.027739Z","iopub.execute_input":"2021-12-13T01:12:15.02854Z","iopub.status.idle":"2021-12-13T01:12:16.96324Z","shell.execute_reply.started":"2021-12-13T01:12:15.028493Z","shell.execute_reply":"2021-12-13T01:12:16.962482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For more visualizations and analysis go to our EDA notebook:\n\nhttps://www.kaggle.com/bnzaretzky/aa-sm-bz-fer-eda-v02","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Technical_Challenges\"></a>\n# Technical Challenges\n\nAs stated above, the first technical challenge we encountered was the pixels format of our data. We had to extract the values from a column in the dataset and convert the string into a vector. \nThe application of transfer learning in our project was crucial to increase the performance of our model. However, when we implemented the TensorFlow library for the Xception architecture, we found that the model could not work with images smaller than 71x71. We tried to rescale the images without success. We found the source code and implemented the architecture from scratch as a last resource. ( https://colab.research.google.com/github/mavenzer/Autism-Detection-Using_YOLO/blob/master/Tutorial_implementing_Xception_in_TensorFlow_2_0_using_the_Functional_API.ipynb#scrollTo=uy3q-iLm3VV2)","metadata":{}},{"cell_type":"markdown","source":"The model architecture is shown below. For more info, refer to the training notebook:  https://www.kaggle.com/bnzaretzky/bz-facial-recognition-v05","metadata":{}},{"cell_type":"code","source":"# Xception Model Architecture\n\ndef entry_flow(inputs):\n\n  x = layers.Conv2D(32, 3, strides=2, padding='same')(inputs)\n  x = layers.BatchNormalization()(x)\n  x = layers.Activation('relu')(x)\n\n  x = layers.Conv2D(64, 3, padding='same')(x)\n  x = layers.BatchNormalization()(x)\n  x = layers.Activation('relu')(x)\n\n  previous_block_activation = x  # Set aside residual\n  \n  # Blocks 1, 2, 3 are identical apart from the feature depth.\n  for size in [128, 256, 728]:\n    x = layers.Activation('relu')(x)\n    x = layers.SeparableConv2D(size, 3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Activation('relu')(x)\n    x = layers.SeparableConv2D(size, 3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n    \n    residual = layers.Conv2D(  # Project residual\n        size, 1, strides=2, padding='same')(previous_block_activation)           \n    x = layers.add([x, residual])  # Add back residual\n    previous_block_activation = x  # Set aside next residual\n\n  return x\n\n\ndef middle_flow(x, num_blocks=8):\n  \n  previous_block_activation = x\n\n  for _ in range(num_blocks):\n    x = layers.Activation('relu')(x)\n    x = layers.SeparableConv2D(728, 3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Activation('relu')(x)\n    x = layers.SeparableConv2D(728, 3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    \n    x = layers.Activation('relu')(x)\n    x = layers.SeparableConv2D(728, 3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.add([x, previous_block_activation])  # Add back residual\n    previous_block_activation = x  # Set aside next residual\n    \n  return x\n\n\ndef exit_flow(x, num_classes=7):\n  \n  previous_block_activation = x\n\n  x = layers.Activation('relu')(x)\n  x = layers.SeparableConv2D(728, 3, padding='same')(x)\n  x = layers.BatchNormalization()(x)\n\n  x = layers.Activation('relu')(x)\n  x = layers.SeparableConv2D(1024, 3, padding='same')(x)\n  x = layers.BatchNormalization()(x)\n  \n  x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n\n  residual = layers.Conv2D(  # Project residual\n      1024, 1, strides=2, padding='same')(previous_block_activation)\n  x = layers.add([x, residual])  # Add back residual\n  \n  x = layers.SeparableConv2D(1536, 3, padding='same')(x)\n  x = layers.BatchNormalization()(x)\n  x = layers.Activation('relu')(x)\n  \n  x = layers.SeparableConv2D(2048, 3, padding='same')(x)\n  x = layers.BatchNormalization()(x)\n  x = layers.Activation('relu')(x)\n  \n  x = layers.GlobalAveragePooling2D()(x)\n  if num_classes == 1:\n    activation = 'sigmoid'\n  else:\n    activation = 'softmax'\n  return layers.Dense(num_classes, activation=activation)(x)\n\ninputs = keras.Input(shape=(48, 48, 1))  # Variable-size image inputs.\noutputs = exit_flow(middle_flow(entry_flow(inputs)))\nxception = keras.Model(inputs, outputs)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:12:16.964915Z","iopub.execute_input":"2021-12-13T01:12:16.965383Z","iopub.status.idle":"2021-12-13T01:12:18.108037Z","shell.execute_reply.started":"2021-12-13T01:12:16.96534Z","shell.execute_reply":"2021-12-13T01:12:18.106618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Week_Progress\"></a>\n# Weekly Progress\n\nEvery week, we made progress in working with the dataset. We will explain the most significant aspect of each week. For more info, visit the link to each weekly notebook. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Week_1\"></a>\n### Week 1 \n\nWe did not use image augmentation or any other tool to explore the dataset during week one. Instead, we implemented a convolutional neural network to see how well it worked on the set. The CNN can be seen below. Our accuracy score for the first week was ~60%.\n\nhttps://www.kaggle.com/saramanrriquez/aa-sm-bz-fer-week-1-training-v01","metadata":{}},{"cell_type":"code","source":"np.random.seed(1)\ntf.random.set_seed(1)\n\ncnn = Sequential([\n    Conv2D(64, (3,3), activation = 'relu', padding = 'same', input_shape=(48,48,1)),\n    Conv2D(64, (5,5), activation = 'relu', padding = 'same'),\n    MaxPooling2D(2,2),\n    Dropout(0.5),\n    BatchNormalization(),\n    \n    Conv2D(128, (3,3), activation = 'relu', padding = 'same'),\n    Conv2D(128, (3,3), activation = 'relu', padding = 'same'),\n    MaxPooling2D(2,2),\n    Dropout(0.5),\n    BatchNormalization(),\n\n    Flatten(),\n    \n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(256, activation='relu'),\n    Dropout(0.5),\n    BatchNormalization(),\n    Dense(7, activation='softmax')\n])\n\ncnn.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:12:18.109881Z","iopub.execute_input":"2021-12-13T01:12:18.110226Z","iopub.status.idle":"2021-12-13T01:12:18.301232Z","shell.execute_reply.started":"2021-12-13T01:12:18.110181Z","shell.execute_reply":"2021-12-13T01:12:18.300259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Week_2\"></a>\n### Week 2 \n\nWe continued testing CNN to improve our score. We improved the architecture and gained points of accuracy.\n\nhttps://www.kaggle.com/alexkali/aa-facial-expression-recognition-v03?scriptVersionId=79744611","metadata":{}},{"cell_type":"code","source":"cnn = Sequential([\n    Conv2D(64, (3,3), padding='same', input_shape=(48,48,1)),\n    BatchNormalization(),\n    Activation('relu'),\n    MaxPooling2D(pool_size=(2,2)),\n    Dropout(0.25),\n\n    Conv2D(64, (5,5), padding='same'),\n    BatchNormalization(),\n    Activation('relu'),\n    MaxPooling2D(pool_size=(2,2)),\n    Dropout(0.25),\n\n    Conv2D(128, (3,3), padding='same'),\n    BatchNormalization(),\n    Activation('relu'),\n    MaxPooling2D(pool_size=(2,2)),\n    Dropout(0.25),\n\n    Conv2D(128, (3,3), padding='same'),\n    BatchNormalization(),\n    Activation('relu'),\n    MaxPooling2D(pool_size=(2,2)),\n    Dropout(0.25),\n\n    Flatten(),\n\n    Dense(128),\n    BatchNormalization(),\n    Activation('relu'),\n    Dropout(0.25),\n\n    Dense(256),\n    BatchNormalization(),\n    Activation('relu'),\n    Dropout(0.25),\n    Dense(7, activation='softmax')\n])\n\ncnn.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:12:18.303902Z","iopub.execute_input":"2021-12-13T01:12:18.304146Z","iopub.status.idle":"2021-12-13T01:12:18.566363Z","shell.execute_reply.started":"2021-12-13T01:12:18.304117Z","shell.execute_reply":"2021-12-13T01:12:18.565292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Week_3\"></a>\n### Week 3 \n\nWe concluded that CNN would not provide improvements, so we started implementing new techniques. We used image augmentation and transfer learning. To explore our transfer learning architecture check the links below: \n\n- [VGG Architecture](#VGG_Architecture)\n- [ResNet50 Architecture](#ResNet50_Architecture)\n- [Xception Architecture](#Xception_Architecture)","metadata":{}},{"cell_type":"markdown","source":"#### Image Augmentation used in our project. A link to the notebook will be found in the Transfer Learning section. ","metadata":{}},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n    rotation_range = 30,\n    width_shift_range = 0.2, \n    height_shift_range = 0.2, \n    zoom_range = 0.2, \n    horizontal_flip = True, \n    fill_mode = 'nearest'\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:12:18.567937Z","iopub.execute_input":"2021-12-13T01:12:18.568169Z","iopub.status.idle":"2021-12-13T01:12:18.57382Z","shell.execute_reply.started":"2021-12-13T01:12:18.568141Z","shell.execute_reply":"2021-12-13T01:12:18.572705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Week_4\"></a>\n### Week 4 \n\nIn our last week of experimentation, we decided to implement ensemble learning. The accuracy improved by one point on the dataset. To explore the ensemble learning design visit the following link:\n\nhttps://www.kaggle.com/bnzaretzky/bz-facial-recognition-submission-ensemble","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Summary_of_Model_Architectures\"></a>\n# Summary of Model Architectures","metadata":{}},{"cell_type":"markdown","source":"<a id=\"VGG_Architecture\"></a>\n### VGG Architecture\n\nThis architecture has a poor performance in the dataset. However, our convolutional neural network did perform better than this transfer learning architecture. The best score we got was 36%. The model can be reviewed here: https://www.kaggle.com/alexkali/aa-facial-expression-recognition-v03","metadata":{}},{"cell_type":"code","source":"np.random.seed(1)\ntf.random.set_seed(1)\n\nbase_model = tf.keras.applications.VGG16(input_shape=(48,48,1),include_top=False, weights=None)\n\nbase_model.trainable = False\ntf.keras.utils.plot_model(base_model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:12:18.57533Z","iopub.execute_input":"2021-12-13T01:12:18.575672Z","iopub.status.idle":"2021-12-13T01:12:19.170959Z","shell.execute_reply.started":"2021-12-13T01:12:18.575592Z","shell.execute_reply":"2021-12-13T01:12:19.169587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ResNet50_Architecture\"></a>\n### ResNet50 Architecture\n\nThe ResNet50 architecture did perform better than VGG; however, the overall performance was poor compared to our convolutional neural network. The highest accuracy we accomplished with this model was ~44%. The model can be reviewed here: https://www.kaggle.com/saramanrriquez/sm-facial-expression-recognition-v06","metadata":{}},{"cell_type":"code","source":"base_model = tf.keras.applications.resnet.ResNet50(include_top=False, weights=None, input_shape=(48,48,1))\n\nbase_model.trainable = False\ntf.keras.utils.plot_model(base_model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:12:19.174413Z","iopub.execute_input":"2021-12-13T01:12:19.175486Z","iopub.status.idle":"2021-12-13T01:12:23.277885Z","shell.execute_reply.started":"2021-12-13T01:12:19.175392Z","shell.execute_reply":"2021-12-13T01:12:23.276533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Xception_Architecture\"></a>\n### Xception Architecture\n\nThis architecture was our rock star. The higher accuracy that we accomplished with our CNN was 62%, but we achieved a 66% accuracy when implementing this Architecture. To review the model check the following notebook: https://www.kaggle.com/bnzaretzky/bz-facial-recognition-v05 ","metadata":{}},{"cell_type":"code","source":"np.random.seed(1)\ntf.random.set_seed(1)\n\ncnn = xception\ncnn.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:12:23.279511Z","iopub.execute_input":"2021-12-13T01:12:23.280373Z","iopub.status.idle":"2021-12-13T01:12:23.366846Z","shell.execute_reply.started":"2021-12-13T01:12:23.280338Z","shell.execute_reply":"2021-12-13T01:12:23.366199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Summary_of_final_results\"></a>\n# Summary of Final Results\n\nIn the end, we selected the CNN model using the Xception architecture. We achieved a 66% accuracy on the test set. Those results can be observed in the following classification reports:","metadata":{}},{"cell_type":"code","source":"# Read in full data set\ndata = pd.read_csv('../input/challenges-in-representation-learning-facial-expression-recognition-challenge/icml_face_data.csv')\ndata.columns = ['emotion', 'Usage', 'pixels']\nprint(data.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:12:23.367777Z","iopub.execute_input":"2021-12-13T01:12:23.368119Z","iopub.status.idle":"2021-12-13T01:12:26.418314Z","shell.execute_reply.started":"2021-12-13T01:12:23.368074Z","shell.execute_reply":"2021-12-13T01:12:26.41695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select only rows that are in the public or private test set\ntest = data.loc[data[\"Usage\"] != 'Training',['emotion','pixels']]\n#test.drop(columns='Usage', inplace=True)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:12:26.419948Z","iopub.execute_input":"2021-12-13T01:12:26.420866Z","iopub.status.idle":"2021-12-13T01:12:26.441368Z","shell.execute_reply.started":"2021-12-13T01:12:26.420806Z","shell.execute_reply":"2021-12-13T01:12:26.440146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reshape the pixels\ntest['pixels'] = [np.fromstring(x, dtype=int, sep=' ').reshape(-1,48,48,1) for x in test['pixels']]","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:12:26.443688Z","iopub.execute_input":"2021-12-13T01:12:26.44394Z","iopub.status.idle":"2021-12-13T01:12:26.985538Z","shell.execute_reply.started":"2021-12-13T01:12:26.443912Z","shell.execute_reply":"2021-12-13T01:12:26.984574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine pixels into single array\npixels = np.concatenate(test['pixels'].values)\n\nprint(pixels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:12:26.987038Z","iopub.execute_input":"2021-12-13T01:12:26.987246Z","iopub.status.idle":"2021-12-13T01:12:27.055282Z","shell.execute_reply.started":"2021-12-13T01:12:26.987221Z","shell.execute_reply":"2021-12-13T01:12:27.054445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardize the pixels values between 0 and 1\npixels = pixels / 255","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:12:27.056526Z","iopub.execute_input":"2021-12-13T01:12:27.056734Z","iopub.status.idle":"2021-12-13T01:12:27.113758Z","shell.execute_reply.started":"2021-12-13T01:12:27.056708Z","shell.execute_reply":"2021-12-13T01:12:27.112652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load model and generate predictions\nmodel = load_model('../input/models/Facial Recognition Models/fer_v05_BZ.h5')\ntest_probs = model.predict(pixels)\ntest_pred = np.argmax(test_probs, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:12:27.115635Z","iopub.execute_input":"2021-12-13T01:12:27.115902Z","iopub.status.idle":"2021-12-13T01:13:14.564363Z","shell.execute_reply.started":"2021-12-13T01:12:27.115875Z","shell.execute_reply":"2021-12-13T01:13:14.562679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['predictions'] = test_pred\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:13:14.566097Z","iopub.execute_input":"2021-12-13T01:13:14.566418Z","iopub.status.idle":"2021-12-13T01:13:15.581098Z","shell.execute_reply.started":"2021-12-13T01:13:14.566381Z","shell.execute_reply":"2021-12-13T01:13:15.580194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"CR\"></a>\n### Classification Report","metadata":{}},{"cell_type":"code","source":"# Assign labels to each value\nemotion_cat = {0:'Anger', 1:'Disgust', 2:'Fear', 3:'Happiness', 4: 'Sadness', 5: 'Surprise', 6: 'Neutral'}\ntest['emotion'] = test['emotion'].apply(lambda x: emotion_cat[x])\ntest['predictions'] = test['predictions'].apply(lambda x: emotion_cat[x])","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:13:15.582419Z","iopub.execute_input":"2021-12-13T01:13:15.582662Z","iopub.status.idle":"2021-12-13T01:13:15.595029Z","shell.execute_reply.started":"2021-12-13T01:13:15.582633Z","shell.execute_reply":"2021-12-13T01:13:15.593704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate the classification report\nmy_classification_report = classification_report(test['emotion'], test['predictions'])\nprint(my_classification_report)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:13:15.596703Z","iopub.execute_input":"2021-12-13T01:13:15.596981Z","iopub.status.idle":"2021-12-13T01:13:15.802338Z","shell.execute_reply.started":"2021-12-13T01:13:15.59695Z","shell.execute_reply":"2021-12-13T01:13:15.800798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confussion_Matrix\n<a id=\"Confussion_Matrix\"></a>","metadata":{}},{"cell_type":"code","source":"# Total number of incorrect predictions\nprint('Total Wrong Predictions:', np.sum(test['emotion'] != test['predictions']))","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:13:15.804179Z","iopub.execute_input":"2021-12-13T01:13:15.804513Z","iopub.status.idle":"2021-12-13T01:13:15.812159Z","shell.execute_reply.started":"2021-12-13T01:13:15.80447Z","shell.execute_reply":"2021-12-13T01:13:15.810892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrix\nscikitplot.metrics.plot_confusion_matrix(test['emotion'], test['predictions'], figsize=(7,7))    ","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:13:15.814005Z","iopub.execute_input":"2021-12-13T01:13:15.814349Z","iopub.status.idle":"2021-12-13T01:13:16.438972Z","shell.execute_reply.started":"2021-12-13T01:13:15.814305Z","shell.execute_reply":"2021-12-13T01:13:16.437693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Top_K_Accuracy_score\"></a>\n### Top K Accuracy Score","metadata":{}},{"cell_type":"code","source":"# View shape of probabilites\ntest_probs.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:13:16.440854Z","iopub.execute_input":"2021-12-13T01:13:16.441097Z","iopub.status.idle":"2021-12-13T01:13:16.448355Z","shell.execute_reply.started":"2021-12-13T01:13:16.441069Z","shell.execute_reply":"2021-12-13T01:13:16.447345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute Top-K accuracy for each class\nfor k in range(0, 7):\n    print(f\"{emotion_cat[k]} top accuracy: {round(top_k_accuracy_score(test['emotion'], test_probs, k=k), 2)}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-13T01:13:16.449717Z","iopub.execute_input":"2021-12-13T01:13:16.449957Z","iopub.status.idle":"2021-12-13T01:13:16.510412Z","shell.execute_reply.started":"2021-12-13T01:13:16.449926Z","shell.execute_reply":"2021-12-13T01:13:16.509352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Links to notebooks used in the project and report.\n\n* EDA Notebook - https://www.kaggle.com/bnzaretzky/aa-sm-bz-fer-eda-v02\n* Final Model Training Notebook -  https://www.kaggle.com/bnzaretzky/bz-facial-recognition-v05\n* Final Model Evaluation Notebook - https://www.kaggle.com/bnzaretzky/aa-sm-bz-final-model-evaluation\n* Class Activation Maps Notebook - https://www.kaggle.com/saramanrriquez/aa-sm-bz-fer-class-activation-maps-v01\n* Transfer Learning Notebook - https://www.kaggle.com/bnzaretzky/bz-facial-recognition-v05","metadata":{}}]}