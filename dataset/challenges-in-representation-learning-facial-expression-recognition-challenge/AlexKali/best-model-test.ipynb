{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import glob\nimport shutil\nimport os\nimport random\nimport cv2\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport scikitplot\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\nfrom keras.utils import np_utils\n\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, ZeroPadding2D, Convolution2D\nfrom tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU, Activation\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/challenges-in-representation-learning-facial-expression-recognition-challenge/train.csv')\n\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion_cat = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 4: 'sadness', 5: 'surprise', 6: 'neutral'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = data['pixels'].apply(lambda x: np.array(x.split(' ')).reshape(48, 48, 1).astype('float32'))\nimg = np.stack(img, axis=0)\n\nle = LabelEncoder()\nlabel = le.fit_transform(data['emotion'])\nlabel = np_utils.to_categorical(label)\n\nprint(f'Image Shape: {img.shape}')\nprint(f'Label Shape: {label.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(img, label, shuffle=True, stratify=label, test_size=0.2, random_state=42)\n\nprint(f'X training shape: {X_train.shape}')\nprint(f'X validation shape: {X_valid.shape}')\nprint(f'Y training shape: {y_train.shape}')\nprint(f'Y validation shape: {y_valid.shape}')\n\nvalid_data = (X_valid, y_valid) \ntrain_data = (X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n                                    rescale = 1. / 255,\n                                    rotation_range=25,\n                                    zoom_range=0.15,\n                                    width_shift_range=0.2,\n                                    height_shift_range=0.2,\n                                    shear_range=0.15,\n                                    horizontal_flip=True,\n                                    fill_mode=\"nearest\")\n    \nvalid_datagen = ImageDataGenerator(\n                                    rescale = 1. / 255,\n                                    rotation_range=25,\n                                    zoom_range=0.15,\n                                    width_shift_range=0.2,\n                                    height_shift_range=0.2,\n                                    shear_range=0.15,\n                                    horizontal_flip=True,\n                                    fill_mode=\"nearest\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 200\nEPOCHS = 100 \n\ntrain_loader = train_datagen.flow(\n        X_train, y_train,\n        shuffle=True,\n        batch_size=32\n    )\n\nvalid_loader = train_datagen.flow(\n        X_valid, y_valid,\n        shuffle=True,\n        batch_size=32\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TR_STEPS = len(train_loader)\nVA_STEPS = len(valid_loader)\n\nprint(TR_STEPS)\nprint(VA_STEPS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(1)\ntf.random.set_seed(1)\n\nmodel = Sequential()\n\nmodel.add(Conv2D(128, (3,3), activation = 'relu', padding = 'same', input_shape=(48,48,1)))\nmodel.add(Conv2D(128, (3,3), activation = 'relu', padding = 'same'))\nmodel.add(MaxPooling2D(2,2))\nmodel.add(Dropout(0.30))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(128, (3,3), activation = 'relu', padding = 'same'))\nmodel.add(Conv2D(128, (3,3), activation = 'relu', padding = 'same'))\nmodel.add(MaxPooling2D(2,2))\nmodel.add(Dropout(0.30))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(256, (3,3), activation = 'relu', padding = 'same'))\nmodel.add(Conv2D(256, (3,3), activation = 'relu', padding = 'same'))\nmodel.add(MaxPooling2D(2,2))\nmodel.add(Dropout(0.30))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(1024, (3,3), activation = 'relu', padding = 'same'))\nmodel.add(Conv2D(1024, (3,3), activation = 'relu', padding = 'same'))\nmodel.add(MaxPooling2D(2,2))\nmodel.add(Dropout(0.30))\nmodel.add(BatchNormalization())\n\nmodel.add(Flatten())\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.35))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.35))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(7, activation='softmax'))\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(learning_rate=0.001)\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time \n\nh1 = model.fit(\n        X_train, y_train,\n        steps_per_epoch=len(X_train) // BATCH_SIZE,\n        validation_data=(X_valid, y_valid),\n        validation_steps=len(X_valid) // BATCH_SIZE,\n        epochs=EPOCHS\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"[INFO] Evaluation phase...\")\n\npredictions = model.predict(valid_data[0]).argmax(axis=1)\n\nmy_classification_report = classification_report(np.argmax(valid_data[1], axis=1), predictions)\n\nprint(\"[INFO] Classification report : \")\nprint(my_classification_report)\n\nprint(\"[INFO] Total wrong validation predictions : \")\nprint(np.sum(np.argmax(valid_data[1], axis=1) != predictions))\n\nprint(\"[INFO] Confusion matrix : \")\nscikitplot.metrics.plot_confusion_matrix(np.argmax(valid_data[1], axis=1), predictions, figsize=(7,7))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = h1.history\nn_epochs = len(history['loss'])\n\nplt.figure(figsize=[10,4])\nplt.subplot(1,2,1)\nplt.plot(range(1, n_epochs+1), history['loss'], label='Training')\nplt.plot(range(1, n_epochs+1), history['val_loss'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')\nplt.legend()\nplt.subplot(1,2,2)\nplt.plot(range(1, n_epochs+1), history['accuracy'], label='Training')\nplt.plot(range(1, n_epochs+1), history['val_accuracy'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.set_value(model.optimizer.learning_rate, 0.00001)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time \n\nh2 = model.fit(\n        X_train, y_train,\n        steps_per_epoch=len(X_train) // BATCH_SIZE,\n        validation_data=(X_valid, y_valid),\n        validation_steps=len(X_valid) // BATCH_SIZE,\n        epochs=EPOCHS\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"[INFO] Evaluation phase...\")\n\npredictions = model.predict(valid_data[0]).argmax(axis=1)\n\nmy_classification_report = classification_report(np.argmax(valid_data[1], axis=1), predictions)\n\nprint(\"[INFO] Classification report : \")\nprint(my_classification_report)\n\nprint(\"[INFO] Total wrong validation predictions : \")\nprint(np.sum(np.argmax(valid_data[1], axis=1) != predictions))\n\nprint(\"[INFO] Confusion matrix : \")\nscikitplot.metrics.plot_confusion_matrix(np.argmax(valid_data[1], axis=1), predictions, figsize=(7,7))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in history.keys():\n    history[k] += h2.history[k]\n\nepoch_range = range(1, len(history['loss'])+1)\n\nplt.figure(figsize=[14,4])\nplt.subplot(1,2,1)\nplt.plot(epoch_range, history['loss'], label='Training')\nplt.plot(epoch_range, history['val_loss'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')\nplt.legend()\nplt.subplot(1,2,2)\nplt.plot(epoch_range, history['accuracy'], label='Training')\nplt.plot(epoch_range, history['val_accuracy'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.set_value(model.optimizer.learning_rate, 0.0000001)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time \n\nh3 = model.fit(\n        X_train, y_train,\n        steps_per_epoch=len(X_train) // BATCH_SIZE,\n        validation_data=(X_valid, y_valid),\n        validation_steps=len(X_valid) // BATCH_SIZE,\n        epochs=EPOCHS\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"[INFO] Evaluation phase...\")\n\npredictions = model.predict(valid_data[0]).argmax(axis=1)\n\nmy_classification_report = classification_report(np.argmax(valid_data[1], axis=1), predictions)\n\nprint(\"[INFO] Classification report : \")\nprint(my_classification_report)\n\nprint(\"[INFO] Total wrong validation predictions : \")\nprint(np.sum(np.argmax(valid_data[1], axis=1) != predictions))\n\nprint(\"[INFO] Confusion matrix : \")\nscikitplot.metrics.plot_confusion_matrix(np.argmax(valid_data[1], axis=1), predictions, figsize=(7,7))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in history.keys():\n    history[k] += h3.history[k]\n\nepoch_range = range(1, len(history['loss'])+1)\n\nplt.figure(figsize=[14,4])\nplt.subplot(1,2,1)\nplt.plot(epoch_range, history['loss'], label='Training')\nplt.plot(epoch_range, history['val_loss'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')\nplt.legend()\nplt.subplot(1,2,2)\nplt.plot(epoch_range, history['accuracy'], label='Training')\nplt.plot(epoch_range, history['val_accuracy'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}