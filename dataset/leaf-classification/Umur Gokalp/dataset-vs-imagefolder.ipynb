{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"../input\"))\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\nfrom tqdm import tqdm_notebook as tnote\ntorch.manual_seed(42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from zipfile import ZipFile\nwith ZipFile('../input/images.zip', 'r') as zipObj: # Unzipping images\n   # Extract all the contents of zip file in current directory\n   zipObj.extractall()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset Method"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets take a quick look at how the data is structured."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_path = \"../input/train.csv.zip\"\ndata = pd.read_csv(train_path)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The only two columns we are interested are id and species columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[:,['id','species']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. To load data into Pytorch we are going to define a class which inherits from the `data.Dataset` class.\n2. By creating this class we will be able to use the `DataLoader` which simplifies the training/validatation process. \n3. But first we need to implement `__len__` and `__getitem__` methods for the `LeafLoader` object"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LeafLoader(Dataset):\n    \"\"\"Loads the Leaf Classification dataset.\"\"\"\n\n    def __init__(self, csv_file, transform=None):\n        self.data = pd.read_csv(csv_file)\n        self.transform = transform\n\n        # First 2 columns contains the id for the image and the class of the image\n        self.dict = self.data.iloc[:,:2].to_dict()\n        # When we index we want to get the id\n        self.ids = self.dict[\"id\"]\n        \n\n        self.classes = self.data[\"species\"].unique() # List of unique class names\n        self.class_to_idx = {j: i for i, j in enumerate(self.classes)} \n        # Assigns number to every class in the order which it appears in the data\n        self.species = self.dict[\"species\"]\n        # Use this go back to class name from index of the class name\n        self.path_leaf = \"images\" # Where the images are stored\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n\n        if torch.is_tensor(idx):\n            idx = idx.item()\n            assert isinstance(idx, int)\n\n        num = self.ids[idx] # Id of the indexed item\n        loc = f\"/{num}.jpg\"\n        label = self.dict[\"species\"][idx] # Find the label/class of the image at given index\n        label = self.class_to_idx[label] # Convert it to int\n        image = Image.open(self.path_leaf + loc)\n        if self.transform:\n            image = self.transform(image)\n\n        return (image, label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can then apply our standard transformations and load data into `DataLoader`"},{"metadata":{"trusted":true},"cell_type":"code","source":"image_size = (28,28)\nnormalize = ((0.5), (0.5))\n\ntransform = transforms.Compose([transforms.Resize(image_size),transforms.ToTensor(), transforms.Normalize(*normalize)])\ndataset = LeafLoader(train_path,transform)\n\ntrain_size = int(0.8 * len(dataset)) # 80% of the data to be used for training\ntest_size = len(dataset) - train_size # The remainder for testing\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n# Function above takes dataset, and lengths of train,test as input that's what we a supplying here\n\nbatch_size = 16\ntrainloader_dataset = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntestloader_dataset = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can plot a batch of images to check if it's working properly"},{"metadata":{"trusted":true},"cell_type":"code","source":"def subplot_random():\n    im, lab = next(iter(trainloader))\n    fig=plt.figure(figsize=(15, 15))\n\n    for idx,(i,j) in enumerate(zip(im,lab)):\n        idx +=1\n        ax = fig.add_subplot(4,4,idx)\n        ax.imshow(i.squeeze().numpy())\n        ax.set_title(dataset.idx_to_class[j.item()])\n    plt.show()\n\nsubplot_random() # We plot a batch using this helper function","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Folder Method"},{"metadata":{},"cell_type":"markdown","source":"Image Folder Method is expecially useful when your data is structed in the following way.\n- root/Acer_Capillipes/1196.jpg\n- root/Acer_Capillipes/227.jpg\n- root/Acer_Capillipes/990.jpg\n- .\n- .\n- .\n- root/Zelkova_Serrata/1410.jpg\n- root/Zelkova_Serrata/718.jpg\n- root/Zelkova_Serrata/1136.jpg\n\n\nHowever, in this example the data is not structured in that way so we will get it in that form."},{"metadata":{"trusted":true},"cell_type":"code","source":"import shutil # To copy files from one directory to another","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a list of species to iterate on\nlabels = data.species.values.tolist() # Labels are the species of the leafs\ndef make_folders(verbose=False):\n    folder_count = 0\n    root = 'Data/'\n    print('Total labels = ',len(set(labels)))\n    for i in set(labels):\n        os.makedirs(f'{root}{i}') # Make directories similar to Data/class_name\n        folder_count += 1\n    print(\"Total folders = \", folder_count )\n    print(f\"Root is {root}\")\nmake_folders()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since we know that we have 10 images for each class we can define a function that splits  \n# the list once it reaches a length of 10\ndef create_chunks(list_name, n):\n    for i in range(0, len(list_name), n):\n        yield list_name[i:i + n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"species_list = data.sort_values('species').species.unique().tolist() # Unique species\nid_list = list(create_chunks(data.sort_values('species').id.values.tolist(),10)) # list of lists with sublist length of 10\ndict_train = dict(zip(species_list,id_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checks if the data is correct\nfor key,val in dict_train.items():\n    assert sorted(data[data.species == key].id.tolist()) == sorted(val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for item,key in dict_train.items():\n    for i in range(10):\n        path1 = f'images/{str(dict_train.get(item)[i])}.jpg'\n        path2 = f'Data/{item}'\n        shutil.copyfile(path1,path2+'/'+str(dict_train.get(item)[i])+'.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"root = 'Data/'\ntransform = transforms.Compose([transforms.Resize(image_size),\n                               transforms.Grayscale(num_output_channels=1),\n                               transforms.ToTensor(),\n                               transforms.Normalize(*normalize)\n                               ])\ndataset_fold = ImageFolder(root, transform= transform)\ntrain,valid = random_split(dataset,[train_size,test_size])\n\n# To load our data in batches\ntrain_loader_folder = DataLoader(train, batch_size=16, shuffle=True)\nvalid_loader_folder = DataLoader(valid, batch_size=16, shuffle=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert len(trainloader_dataset) == len(train_loader_folder)\nassert len(testloader_dataset) == len(valid_loader_folder)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shutil.rmtree(\"Data\")\nshutil.rmtree(\"images\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# References:\n`create_chunks(list_name, n):` from [DataCamp](https://www.datacamp.com/community/tutorials/lists-n-sized-chunks)\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}