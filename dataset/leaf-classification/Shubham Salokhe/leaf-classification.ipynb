{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-07T07:06:51.113075Z","iopub.execute_input":"2021-10-07T07:06:51.113467Z","iopub.status.idle":"2021-10-07T07:06:51.149883Z","shell.execute_reply.started":"2021-10-07T07:06:51.113373Z","shell.execute_reply":"2021-10-07T07:06:51.148762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train= pd.read_csv('/kaggle/input/leaf-classification/train.csv.zip')\ndf_test = pd.read_csv('/kaggle/input/leaf-classification/test.csv.zip')","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:51.151397Z","iopub.execute_input":"2021-10-07T07:06:51.151625Z","iopub.status.idle":"2021-10-07T07:06:51.303839Z","shell.execute_reply.started":"2021-10-07T07:06:51.151598Z","shell.execute_reply":"2021-10-07T07:06:51.302811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:51.305245Z","iopub.execute_input":"2021-10-07T07:06:51.305463Z","iopub.status.idle":"2021-10-07T07:06:51.339851Z","shell.execute_reply.started":"2021-10-07T07:06:51.305437Z","shell.execute_reply":"2021-10-07T07:06:51.338841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:51.341962Z","iopub.execute_input":"2021-10-07T07:06:51.342258Z","iopub.status.idle":"2021-10-07T07:06:51.366785Z","shell.execute_reply.started":"2021-10-07T07:06:51.342228Z","shell.execute_reply":"2021-10-07T07:06:51.365683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:51.368453Z","iopub.execute_input":"2021-10-07T07:06:51.368811Z","iopub.status.idle":"2021-10-07T07:06:51.412283Z","shell.execute_reply.started":"2021-10-07T07:06:51.368763Z","shell.execute_reply":"2021-10-07T07:06:51.411174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:51.413694Z","iopub.execute_input":"2021-10-07T07:06:51.413975Z","iopub.status.idle":"2021-10-07T07:06:51.802688Z","shell.execute_reply.started":"2021-10-07T07:06:51.413943Z","shell.execute_reply":"2021-10-07T07:06:51.80187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:51.804086Z","iopub.execute_input":"2021-10-07T07:06:51.804702Z","iopub.status.idle":"2021-10-07T07:06:52.168482Z","shell.execute_reply.started":"2021-10-07T07:06:51.804655Z","shell.execute_reply":"2021-10-07T07:06:52.167414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:52.171685Z","iopub.execute_input":"2021-10-07T07:06:52.172109Z","iopub.status.idle":"2021-10-07T07:06:52.18383Z","shell.execute_reply.started":"2021-10-07T07:06:52.172077Z","shell.execute_reply":"2021-10-07T07:06:52.182664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:52.185567Z","iopub.execute_input":"2021-10-07T07:06:52.186117Z","iopub.status.idle":"2021-10-07T07:06:52.195182Z","shell.execute_reply.started":"2021-10-07T07:06:52.186075Z","shell.execute_reply":"2021-10-07T07:06:52.194605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train.shape)\nprint(df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:52.196466Z","iopub.execute_input":"2021-10-07T07:06:52.196815Z","iopub.status.idle":"2021-10-07T07:06:52.209903Z","shell.execute_reply.started":"2021-10-07T07:06:52.196788Z","shell.execute_reply":"2021-10-07T07:06:52.208937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:52.21184Z","iopub.execute_input":"2021-10-07T07:06:52.212274Z","iopub.status.idle":"2021-10-07T07:06:52.25512Z","shell.execute_reply.started":"2021-10-07T07:06:52.212155Z","shell.execute_reply":"2021-10-07T07:06:52.25391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['species'].nunique()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:52.256661Z","iopub.execute_input":"2021-10-07T07:06:52.257024Z","iopub.status.idle":"2021-10-07T07:06:52.265216Z","shell.execute_reply.started":"2021-10-07T07:06:52.25698Z","shell.execute_reply":"2021-10-07T07:06:52.264177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.columns.values","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:52.267219Z","iopub.execute_input":"2021-10-07T07:06:52.267543Z","iopub.status.idle":"2021-10-07T07:06:52.279285Z","shell.execute_reply.started":"2021-10-07T07:06:52.267502Z","shell.execute_reply":"2021-10-07T07:06:52.278536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"code Reffered from other notebook for learning perpose(thanks )","metadata":{}},{"cell_type":"markdown","source":"## Data Preparation ","metadata":{}},{"cell_type":"markdown","source":"Here we use swiss army knife function to orgnize the data\n<br>Also we use label encoding ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedShuffleSplit # we will know about that package while using it","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:52.280837Z","iopub.execute_input":"2021-10-07T07:06:52.281404Z","iopub.status.idle":"2021-10-07T07:06:53.226114Z","shell.execute_reply.started":"2021-10-07T07:06:52.281368Z","shell.execute_reply":"2021-10-07T07:06:53.225465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label Encoding\ndef encode(df_train,df_test):\n    le = LabelEncoder().fit(df_train.species)\n    labels = le.transform(df_train.species) # Species are in stings \n    \n    classes = list(le.classes_) #creating list of column names for submission\n    \n    test_ids = df_test.id # creating variable for IDs\n    \n    df_train = df_train.drop(['species','id'],axis = 1) #droping columns \n    df_test = df_test.drop(['id'],axis = 1)\n\n    return df_train, labels, classes, test_ids, df_test\n\ndf_train, labels, classes, test_ids, df_test = encode(df_train,df_test)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:53.227232Z","iopub.execute_input":"2021-10-07T07:06:53.227999Z","iopub.status.idle":"2021-10-07T07:06:53.240053Z","shell.execute_reply.started":"2021-10-07T07:06:53.227948Z","shell.execute_reply":"2021-10-07T07:06:53.239206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:53.241519Z","iopub.execute_input":"2021-10-07T07:06:53.241755Z","iopub.status.idle":"2021-10-07T07:06:53.281356Z","shell.execute_reply.started":"2021-10-07T07:06:53.241729Z","shell.execute_reply":"2021-10-07T07:06:53.280768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:53.284057Z","iopub.execute_input":"2021-10-07T07:06:53.284675Z","iopub.status.idle":"2021-10-07T07:06:53.293832Z","shell.execute_reply.started":"2021-10-07T07:06:53.284641Z","shell.execute_reply":"2021-10-07T07:06:53.293218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stratified TrainTest Split\n### why stratified train test split?\nHere are relatively large no of classes(columns) available (192 classes/columns for 990 samples/rows).This will ensure we have all classes represented in both the train and test indices.","metadata":{}},{"cell_type":"code","source":"# labels is our traget columns which is we created by transforming of species colums(LabelEncoding)\nlabels","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:53.29494Z","iopub.execute_input":"2021-10-07T07:06:53.295623Z","iopub.status.idle":"2021-10-07T07:06:53.311264Z","shell.execute_reply.started":"2021-10-07T07:06:53.295589Z","shell.execute_reply":"2021-10-07T07:06:53.31036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_train.values\ny = labels","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:53.312256Z","iopub.execute_input":"2021-10-07T07:06:53.313573Z","iopub.status.idle":"2021-10-07T07:06:53.324895Z","shell.execute_reply.started":"2021-10-07T07:06:53.313539Z","shell.execute_reply":"2021-10-07T07:06:53.324057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sss = StratifiedShuffleSplit(n_splits=10, test_size=0.25, random_state=5)\n>>> sss.get_n_splits(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:53.325882Z","iopub.execute_input":"2021-10-07T07:06:53.326604Z","iopub.status.idle":"2021-10-07T07:06:53.338527Z","shell.execute_reply.started":"2021-10-07T07:06:53.326575Z","shell.execute_reply":"2021-10-07T07:06:53.337795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sss","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:53.339743Z","iopub.execute_input":"2021-10-07T07:06:53.340448Z","iopub.status.idle":"2021-10-07T07:06:53.352711Z","shell.execute_reply.started":"2021-10-07T07:06:53.34041Z","shell.execute_reply":"2021-10-07T07:06:53.351675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for train_index, test_index in sss.split(X, y):\n#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:53.354271Z","iopub.execute_input":"2021-10-07T07:06:53.354758Z","iopub.status.idle":"2021-10-07T07:06:53.398311Z","shell.execute_reply.started":"2021-10-07T07:06:53.354715Z","shell.execute_reply":"2021-10-07T07:06:53.397252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classification\nwe will use 10 classification technique and printing their results. These will perform much better after tuning their Hyperparameters, this gives you a decent ballpark idea.\n1. KNeighborsClassifier\n2. SVC(Support Vector Classifier)\n3. NuSVC(Nu-Support Vector Classification)Similar to SVC but uses a parameter to control the number of support vectors. The implementation is based on libsvm.\n4. DecisionTreeClassifier\n5. RandomForestClassifier\n6. AdaBoostClassifier\n7. GradientBoostingClassifier\n8. GaussianNB\n9. LinearDiscriminantAnalysis\n10. QuadraticDiscriminantAnalysis","metadata":{}},{"cell_type":"code","source":"# importing libraries\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC,LinearSVC,NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import log_loss\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel='rbf',C = 0.025, probability= True),\n    NuSVC(probability = True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis()\n]","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:53.400012Z","iopub.execute_input":"2021-10-07T07:06:53.400384Z","iopub.status.idle":"2021-10-07T07:06:53.709355Z","shell.execute_reply.started":"2021-10-07T07:06:53.400335Z","shell.execute_reply":"2021-10-07T07:06:53.70752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating interation for these 10 algorithm and printing result then we select higher algorithm which having high accuracy and less log loss","metadata":{}},{"cell_type":"code","source":"# Logging for Visual Comparison\nlog_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"] # created list \nlog = pd.DataFrame(columns=log_cols)\n\nfor clf in classifiers:\n    clf.fit(X_train, y_train) # fit method for applying algorithm \n    name = clf.__class__.__name__\n    \n    print(\"=\"*30) ### printing = in 30 times\n    print(name) ### printing name of algorithm\n    \n    print('****Results****') \n    train_predictions = clf.predict(X_test)   ### predicting y_predict\n    acc = accuracy_score(y_test, train_predictions) # now compairing y predict(train_prediction) with y_test\n    print(\"Accuracy: {:.4%}\".format(acc)) ### printing accuracy  with 4 decimal with pecentage mark \n    \n    train_predictions = clf.predict_proba(X_test) ### Probability estimates y_predict_probability(train_predictions)\n    ll = log_loss(y_test, train_predictions) ### appllying log loss on y_test and y_predict_probability(train_predictions)\n    print(\"Log Loss: {}\".format(ll)) ### printing log loss  \n    \n    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols) ## creating new dataframe with \n                                                                        ## name of log_entry considering name ,\n                                                                          ## accuracy score(this score not in pecentage form hence it multiply by 100)\n                                                                            ##logloss stored each iteratin value in log_cols\n    log = log.append(log_entry) ### append those entry in log \n    \nprint(\"=\"*30)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-07T07:06:53.711012Z","iopub.execute_input":"2021-10-07T07:06:53.711977Z","iopub.status.idle":"2021-10-07T07:11:40.846266Z","shell.execute_reply.started":"2021-10-07T07:06:53.711903Z","shell.execute_reply":"2021-10-07T07:11:40.844916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_color_codes(\"muted\")\nsns.barplot(y='Classifier', x='Accuracy', data=log, color=\"b\")\n\nplt.xlabel('Accuracy %')\nplt.title('Classifier Accuracy')\nplt.show()\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Log Loss', y='Classifier', data=log, color=\"g\")\n\nplt.xlabel('Log Loss')\nplt.title('Classifier Log Loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T08:00:47.158394Z","iopub.execute_input":"2021-10-07T08:00:47.158743Z","iopub.status.idle":"2021-10-07T08:00:48.111152Z","shell.execute_reply.started":"2021-10-07T08:00:47.158708Z","shell.execute_reply":"2021-10-07T08:00:48.109979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T08:05:06.467403Z","iopub.execute_input":"2021-10-07T08:05:06.468253Z","iopub.status.idle":"2021-10-07T08:05:06.484586Z","shell.execute_reply.started":"2021-10-07T08:05:06.468202Z","shell.execute_reply":"2021-10-07T08:05:06.483414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random Forest Classifier has the highest accuracy  also it has less log loss so we choosing Random forest Classifier as our model ","metadata":{}},{"cell_type":"code","source":"# now predicting test values \nchosen_clf = RandomForestClassifier()\nchosen_clf.fit(X_train,y_train)\ntest_predictions = chosen_clf.predict_proba(df_test)\n\n# Format DataFrame\nsubmission = pd.DataFrame(test_predictions, columns=classes)\nsubmission.insert(0, 'id', test_ids)\nsubmission.reset_index()\n\n# saving submission\nsubmission.to_csv('submission.csv', index = False)\nsubmission.tail()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T08:15:38.662204Z","iopub.execute_input":"2021-10-07T08:15:38.662543Z","iopub.status.idle":"2021-10-07T08:15:40.506148Z","shell.execute_reply.started":"2021-10-07T08:15:38.662512Z","shell.execute_reply":"2021-10-07T08:15:40.505232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}