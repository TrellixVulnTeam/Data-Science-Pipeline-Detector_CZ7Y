{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d7769ae-9a98-749d-cb9b-931a26a4de05"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0aa787bb-987d-22bf-ddb6-9b7f72cf4932"},"outputs":[],"source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\ntrain.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d8c87924-2d25-8f86-145b-d90b0287e206"},"outputs":[],"source":"import tensorflow as tf\n\nprint(\"TODO: update input layer size\")\nx = tf.placeholder(tf.float32, [None, 784])\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\n\ny = tf.nn.softmax(tf.matmul(x, W) + b)\ny_ = tf.placeholder(tf.float32, [None, 99])\n\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n\ninit = tf.initialize_all_variables()\nsess = tf.Session()\nsess.run(init)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fc3f102d-b9ac-04cf-4f06-8f6b886a0c2d"},"outputs":[],"source":"for i in range(1000):\n    batch_xs, batch_ys = mnist.train.next_batch(100)\n    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n\ncorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nprint(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2f4f930e-d0a4-2c77-19e5-53ad7e367b38"},"outputs":[],"source":"print(check_output([\"ls\", \"../input/images\"]).decode(\"utf8\"))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd7c0c55-855e-f5b3-26f2-0f9b720a403a"},"outputs":[],"source":"filename_queue = tf.train.string_input_producer(['/Users/HANEL/Desktop/tf.png'])\n\nreader = tf.WholeFileReader()\nkey, value = reader.read(filename_queue)\n\nmy_img = tf.image.decode_png(value) # use png or jpg decoder based on your files.\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27bf3133-8885-4acc-7e05-38039f0e46d5"},"outputs":[],"source":"import logging\nlogging.getLogger().setLevel(logging.INFO)\n\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n\n#read data set\ntrain_file = \"../input/train.csv\"\ntest_file = \"../input/test.csv\"\ntrain = pd.read_csv(train_file)\ntest = pd.read_csv(test_file)\n\n\nx_train = train.drop(['species', 'id'], axis=1).values\nle = LabelEncoder().fit(train['species'])\ny_train = le.transform(train['species'])\n\nx_test = test.drop(['id'], axis=1).values\n\nscaler = StandardScaler().fit(x_train)\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)\n\n\n# Build 3 layer DNN with 1024, 512, 256 units respectively.\nclassifier = tf.contrib.learn.DNNClassifier(hidden_units=[1024,512,256],\nn_classes=99)\n\n# Fit model.\nclassifier.fit(x=x_train, y=y_train, steps = 2000)\n\n# Make prediction for test data\ny = classifier.predict(x_test)\ny_prob = classifier.predict_proba(x_test)\n\n# prepare csv for submission\ntest_ids = test.pop('id')\nsubmission = pd.DataFrame(y_prob, index=test_ids, columns=le.classes_)\nsubmission.to_csv('submission_log_reg.csv')\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"76ca85a3-c6c0-48e2-bedb-6efdc4835860"},"outputs":[],"source":"submission.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"546e7e63-fbab-ef11-a70a-240b287fa1e5"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}