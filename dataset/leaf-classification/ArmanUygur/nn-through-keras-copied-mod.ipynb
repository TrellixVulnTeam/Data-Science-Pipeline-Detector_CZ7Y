{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"e9e2b0da-44de-722d-5850-3b408fa380b0"},"source":"## Using Neural Networks through Keras"},{"cell_type":"markdown","metadata":{"_cell_guid":"faa42311-1ae2-1e94-cbae-f67fb8f8262f"},"source":"Copied from Kaggle itself - see if I make it better !"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e2295c62-b360-c048-ab1c-a19e91218aad"},"outputs":[],"source":"## Importing standard libraries\n\n%pylab inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"889c7da9-1760-d487-2a51-c599979ca93e"},"outputs":[],"source":"## Importing sklearn libraries\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import LabelEncoder"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"17ff0a6b-735f-a0b1-2e7f-368c16d1a5bf"},"outputs":[],"source":"## Keras Libraries for Neural Networks\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Activation\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import EarlyStopping"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3883e40b-48fb-03e4-d365-d01af7505e84"},"outputs":[],"source":"## Read data from the CSV file\n\ndata = pd.read_csv('../input/train.csv')\nparent_data = data.copy()    ## Always a good idea to keep a copy of original data\nID = data.pop('id')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7ad8b1ad-c973-acff-9a77-080e0aef557e"},"outputs":[],"source":"data.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dacc91e2-d384-5979-0207-66dd145004f9"},"outputs":[],"source":"## Since the labels are textual, so we encode them categorically\n\ny = data.pop('species')\ny = LabelEncoder().fit(y).transform(y)\nprint(y.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"262251c4-2ca1-5a1b-f67d-cf47b7763976"},"outputs":[],"source":"## Most of the learning algorithms are prone to feature scaling\n## Standardising the data to give zero mean =)\n\nX = StandardScaler().fit(data).transform(data)\nprint(X.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a6041431-1433-762e-d0e0-91aadb8c3370"},"outputs":[],"source":"## We will be working with categorical crossentropy function\n## It is required to further convert the labels into \"one-hot\" representation\n\ny_cat = to_categorical(y)\nprint(y_cat.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4be105f5-77a6-8c46-182f-42b7241cf150"},"outputs":[],"source":"2## Developing a layered model for Neural Networks\n## Input dimensions should be equal to the number of features\n## We used softmax layer to predict a uniform probabilistic distribution of outcomes\n\nmodel = Sequential()\nmodel.add(Dense(600,input_dim=192,  init='uniform', activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(300, activation='sigmoid'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(99, activation='softmax'))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3135f21f-367c-77b7-2fd9-6190296820bd"},"outputs":[],"source":"## Error is measured as categorical crossentropy or multiclass logloss\n## Adagrad, rmsprop, SGD, Adadelta, Adam, Adamax, Nadam\nmodel.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics = [\"accuracy\"])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"52ed1c8d-6dc0-7a91-9828-849ed28c45d0"},"outputs":[],"source":"## Fitting the model on the whole training data\n## the validation is literally just the last x% of samples in the input you passed.\n## the split is deeply flawed because it should split proportionally for multi-class problems\n## this is one of the reasons of low performance of this model\nearly_stopping = EarlyStopping(monitor='val_loss', patience=280)\nhistory = model.fit(X,y_cat,batch_size=192,\n                    nb_epoch=800 ,verbose=0, validation_split=0.1, callbacks=[early_stopping])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b475d5fc-2fd6-0319-3bb7-daee98aceec4"},"outputs":[],"source":"## we need to consider the loss for final submission to leaderboard\n## print(history.history.keys())\nprint('val_acc: ',max(history.history['val_acc']))\nprint('val_loss: ',min(history.history['val_loss']))\nprint('train_acc: ',max(history.history['acc']))\nprint('train_loss: ',min(history.history['loss']))\n\nprint()\nprint(\"train/val loss ratio: \", min(history.history['loss'])/min(history.history['val_loss']))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7ddaba51-92f9-a333-f495-1d3ec4a8f3fd"},"outputs":[],"source":"# summarize history for loss\n## Plotting the loss with the number of iterations\n\nplt.semilogy(history.history['loss'])\nplt.semilogy(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8749d835-76e2-d7fb-8d1d-596af7efd91a"},"outputs":[],"source":"## Plotting the error with the number of iterations\n## With each iteration the error reduces smoothly\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"531c8811-67c9-1ca6-19fd-f92fa0c76817"},"outputs":[],"source":"## read test file\ntest = pd.read_csv('../input/test.csv')\nindex = test.pop('id')\ntest = StandardScaler().fit(test).transform(test)\nyPred = model.predict_proba(test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5004cb45-6612-b245-3bf1-30e28a0a77d0"},"outputs":[],"source":"## Converting the test predictions in a dataframe as depicted by sample submission\nyPred = pd.DataFrame(yPred,index=index,columns=sort(parent_data.species.unique()))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0754597d-f556-9bb2-df3b-6b94dd60d9d7"},"outputs":[],"source":"fp = open('submission_nn_kernel.csv','w')\nfp.write(yPred.to_csv())"},{"cell_type":"markdown","metadata":{"_cell_guid":"8bbde5ef-340e-9b2b-84d1-390e813a38f8"},"source":"---------\n\nEarlier` we used a 4 layer network but the result came out to be overfitting the test set. We dropped the count of neurones in the network and also restricted the number of layers to 3 so as to keep it simple.\nInstead of submitting each test sample as a one hot vector we submitted each samples as a probabilistic distribution over all the possible outcomes. This \"may\" help reduce the penalty being exercised by the multiclass logloss thus producing low error on the leaderboard! ;)\nAny suggestions are welcome!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3737fdb1-4a75-f2fe-c6cb-5e20ee336616"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}