{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"6c2f8ee8-29fa-7a68-6c7b-4fbba1c11ffc"},"source":"## Build your own, high-granurality features\n\n![Feature comparison image][1]\n\n\n  [1]: https://raw.githubusercontent.com/lorinc/kaggle-notebooks/master/leaf_feature_comparison.png"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1073064-2763-7a9c-29d5-29e2a6edf849"},"outputs":[],"source":"import numpy as np\n\nimport scipy as sp\nimport scipy.ndimage as ndi\nfrom scipy.signal import argrelextrema\n\nimport pandas as pd\n\nfrom skimage import measure\nfrom sklearn import metrics\n\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\n%matplotlib inline\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (6, 6)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d4b7c3d2-b4ec-83f6-560d-34ae30db159b"},"source":"This part contains the already polished functionality."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c3da753c-82d4-2de1-c11b-6904b7742ee0"},"outputs":[],"source":"# ----------------------------------------------------- I/O ---\n\ndef read_img(img_no):\n    \"\"\"reads image from disk\"\"\"\n    return mpimg.imread('../input/images/' + str(img_no) + '.jpg')\n\n\ndef get_imgs(num):\n    \"\"\"convenience function, yields random sample from leaves\"\"\"\n    if type(num) == int:\n        imgs = range(1, 1584)\n        num = np.random.choice(imgs, size=num, replace=False)\n        \n    for img_no in num:\n        yield img_no, preprocess(read_img(img_no))\n\n\n# ----------------------------------------------------- preprocessing ---\n\ndef threshold(img, threshold=250):\n    \"\"\"splits img to 0 and 255 values at threshold\"\"\"\n    return ((img > threshold) * 255).astype(img.dtype)\n\n\ndef portrait(img):\n    \"\"\"makes all leaves stand straight\"\"\"\n    y, x = np.shape(img)\n    return img.transpose() if x > y else img\n    \n\ndef resample(img, size):\n    \"\"\"resamples img to size without distorsion\"\"\"\n    ratio = size / max(np.shape(img))\n    return sp.misc.imresize(img, ratio, mode='L', interp='nearest')\n\n    \ndef fill(img, size=500, tolerance=0.95):\n    \"\"\"extends the image if it is signifficantly smaller than size\"\"\"\n    y, x = np.shape(img)\n\n    if x <= size * tolerance:\n        pad = np.zeros((y, int((size - x) / 2)), dtype=int)\n        img = np.concatenate((pad, img, pad), axis=1)\n\n    if y <= size * tolerance:\n        pad = np.zeros((int((size - y) / 2), x), dtype=int)\n        img = np.concatenate((pad, img, pad), axis=0) \n    \n    return img\n\n\n# ----------------------------------------------------- postprocessing ---\n\ndef standardize(arr1d):\n    \"\"\"move mean to zero, 1st SD to -1/+1\"\"\"\n    return (arr1d - arr1d.mean()) / arr1d.std()\n\n\ndef coords_to_cols(coords):\n    \"\"\"from x,y pairs to feature columns\"\"\"\n    return coords[::,1], coords[::,0]\n\n\ndef get_contour(img):\n    \"\"\"returns the coords of the longest contour\"\"\"\n    return max(measure.find_contours(img, .8), key=len)\n\n\ndef get_center(img):\n    \"\"\"so that I do not have to remember the function ;)\"\"\"\n    return ndi.measurements.center_of_mass(img)\n\n\n# ----------------------------------------------------- feature engineering ---\n\ndef extract_shape(img):\n    \"\"\"\n    Expects prepared image, returns leaf shape in img format.\n    The strength of smoothing had to be dynamically set\n    in order to get consistent results for different sizes.\n    \"\"\"\n    size = int(np.count_nonzero(img)/1000)\n    brush = int(5 * size/size**.75)\n    return ndi.gaussian_filter(img, sigma=brush, mode='nearest') > 200\n\n\ndef near0_ix(timeseries_1d, radius=5):\n    \"\"\"finds near-zero values in time-series\"\"\"\n    return np.where(timeseries_1d < radius)[0]\n\n\ndef dist_line_line(src_arr, tgt_arr):\n    \"\"\"\n    returns 2 tgt_arr length arrays, \n    1st is distances, 2nd is src_arr indices\n    \"\"\"\n    return np.array(sp.spatial.cKDTree(src_arr).query(tgt_arr))\n\n\ndef dist_line_point(src_arr, point):\n    \"\"\"returns 1d array with distances from point\"\"\"\n    point1d = [[point[0], point[1]]] * len(src_arr)\n    return metrics.pairwise.paired_distances(src_arr, point1d)\n\n\ndef index_diff(kdt_output_1):\n    \"\"\"\n    Shows pairwise distance between all n and n+1 elements.\n    Useful to see, how the dist_line_line maps the two lines.\n    \"\"\"\n    return np.diff(kdt_output_1)\n\n\n# ----------------------------------------------------- wrapping functions ---\n\n# wrapper function for all preprocessing tasks    \ndef preprocess(img, do_portrait=True, do_resample=500, \n               do_fill=True, do_threshold=250):\n    \"\"\" prepares image for processing\"\"\"\n    if do_portrait:\n        img = portrait(img)\n    if do_resample:\n        img = resample(img, size=do_resample)\n    if do_fill:\n        img = fill(img, size=do_resample)\n    if do_threshold:\n        img = threshold(img, threshold=do_threshold)\n        \n    return img"},{"cell_type":"markdown","metadata":{"_cell_guid":"ea58bef0-e4ae-25d2-28fd-59715fac4abe"},"source":"This is the exploratory part."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9577ad1d-9858-6518-22ab-8f1f01435c1b"},"outputs":[],"source":"# exploring solution before building it as function\n\n# img, shape\ntitle, img = list(get_imgs([194]))[0]  #709\nblur = extract_shape(img)\n\n# img contour, shape contour  \nblade = get_contour(img)\nshape = get_contour(blur)\n\n# matplotlib needs them in columnar format\nblade_x, blade_y = coords_to_cols(blade)\nshape_x, shape_y = coords_to_cols(shape)\n\n# flagging blade points that fall inside the shape contour\nblade_inv_ix = blur[blade_y.astype(int), blade_x.astype(int)]\n\n# img distance, shape distance (for time series plotting)\nshape_cy, shape_cx = get_center(blur)\nblade_dist = dist_line_line(shape, blade)\nshape_dist = dist_line_point(shape, [shape_cx, shape_cy])\n\n# fixing false + signs in the blade time series\nblade_dist[0, blade_inv_ix] = blade_dist[0, blade_inv_ix] * -1\n\n# loading kaggle features\nkaggle_shape = pd.read_csv('../input/train.csv').iloc[107,66:130]\nkaggle_blade = pd.read_csv('../input/train.csv').iloc[107,2:66]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"987b9033-eda4-5df5-6625-91d984860b27"},"outputs":[],"source":"# visualization of the two set of features\nrcParams['figure.figsize'] = (9,6)\n\nax1 = plt.subplot2grid((4,3), (0,0), rowspan=4)\nax1.set_title('Image #' + str(title))\nax1.set_xticks([])\nax1.set_yticks([])\nax1.plot(shape_x, shape_y, c='g')\nax1.plot(blade_x, blade_y, c='b')\nax1.scatter(shape_cx, shape_cy, marker='x')\n\nax2 = plt.subplot2grid((4,3), (0,1), colspan=2)\nax2.text(1710, 170, 'Img Shape', rotation=270)\nax2.set_xticks([])\nax2.set_yticks([])\nax2.set_title('Extracted vs. provided features ('+ \n              str(len(shape_dist)) +' vs. 64 points)')\nax2.plot(range(len(shape_dist)), shape_dist, c='g')\n\nax3 = plt.subplot2grid((4,3), (1,1), colspan=2)\nax3.text(2460, 30, 'Img Edge', rotation=270)\nax3.set_xticks([])\nax3.set_yticks([])\nax3.plot(range(len(blade_dist[0])), blade_dist[0], c='b')\n\nax4 = plt.subplot2grid((4,3), (2,1), colspan=2)\nax4.text(63.7, .00032, 'Kaggle Shape', rotation=270)\nax4.set_xticks([])\nax4.set_yticks([])\nax4.plot(range(len(kaggle_shape)), kaggle_shape, c='g')\n\nax5 = plt.subplot2grid((4,3), (3,1), colspan=2)\nax5.text(63.7, .06, 'Kaggle Edge', rotation=270)\nax5.set_xticks([])\nax5.set_yticks([])\nax5.plot(range(len(kaggle_blade)), kaggle_blade, c='b')\n\nplt.show()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}