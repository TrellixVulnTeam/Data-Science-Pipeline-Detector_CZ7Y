{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n['images', 'sample_submission.csv', 'train.csv', 'test.csv']\n#CNN for classifier\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom sklearn.model_selection import train_test_split\n\nclass Data_Clean(object):\n    def __init__(self):\n        self.numerical_data, self.num_test_data = self.read_numerical_data()\n        self.id, self.species, self.num_train, self.test_id, self.test_num = self.split_numerical_data()\n        self.image_data = self.read_image_data()\n        self.image_test_data = self.read_image_test_data()\n\n    def split_numerical_data(self):\n        #将numerical data划分为id, species,和训练集\n        id = self.numerical_data.pop('id')\n        species = self.numerical_data.pop('species')\n        species = LabelEncoder().fit(species).transform(species)\n        species = to_categorical(species, num_classes=99)\n        self.numerical_data = StandardScaler().fit(self.numerical_data).transform(self.numerical_data)\n\n        test_id = self.num_test_data.pop('id')\n        num_test_data = StandardScaler().fit(self.num_test_data).transform(self.num_test_data)\n        return id, species, self.numerical_data, test_id, num_test_data\n\n    def read_numerical_data(self):\n        root = \"../input\"\n        data = pd.read_csv('%s/train.csv' %root)\n        test_data = pd.read_csv('%s/test.csv' %(root))\n        return data, test_data\n\n    def read_image_test_data(self):\n        #读取图片，将图片等比例缩放到96*96，最长边为96， 图片放置到中心位置\n        max_dim = 96\n        #设定一个初始的output,全白背景\n        X = np.empty((len(self.test_id), max_dim, max_dim, 1))\n        #开始读图\n        root = \"../input\"\n        for index, id in enumerate(self.test_id):\n            x = self.resize_img(load_img(os.path.join(root, 'images', str(id) + '.jpg'), grayscale=True), max_dim = max_dim)\n            #将图片格式转化为矩阵\n            x = img_to_array(x)\n            #获得缩放后图片的长和宽\n            length = x.shape[0]\n            width = x.shape[1]\n            #将图片放置到X的中心位置\n            h1 = int((max_dim - length) / 2)\n            h2 = h1 + length\n            w1 = int((max_dim - width) / 2)\n            w2 = w1 + width\n            #放置到X中\n            X[index, h1:h2, w1:w2, 0:1] = x\n        return np.around(X / 255.0)\n\n    def read_image_data(self):\n        #读取图片，将图片等比例缩放到96*96，最长边为96， 图片放置到中心位置\n        max_dim = 96\n        #设定一个初始的output,全白背景\n        X = np.empty((len(self.id), max_dim, max_dim, 1))\n        #开始读图\n        root = \"../input\"\n        for index, id in enumerate(self.id):\n            x = self.resize_img(load_img(os.path.join(root, 'images', str(id) + '.jpg'), grayscale=True), max_dim = max_dim)\n            #将图片格式转化为矩阵\n            x = img_to_array(x)\n            #获得缩放后图片的长和宽\n            length = x.shape[0]\n            width = x.shape[1]\n            #将图片放置到X的中心位置\n            h1 = int((max_dim - length) / 2)\n            h2 = h1 + length\n            w1 = int((max_dim - width) / 2)\n            w2 = w1 + width\n            #放置到X中\n            X[index, h1:h2, w1:w2, 0:1] = x\n        return np.around(X / 255.0)\n\n    def resize_img(self, image, max_dim):\n        #对图片进行缩放\n        max_length = max(image.size[0], image.size[1])\n        #确定缩放比例\n        scale = max_dim / max_length\n        return image.resize((int(image.size[0] * scale), int(image.size[1] * scale)))\n\n    def run(self):\n        ##Check the data\n        # print('id:', self.id.loc[0])\n        # print('species:', self.species[0])\n        # print('feature:', self.num_train[0])\n        # plt.imshow(self.image_data[0].reshape(960, 960), cmap='gray')\n        # plt.show()\n        return self.species, self.num_train, self.image_data, self.test_num, self.image_test_data, self.test_id\n\nfrom keras.models import Model\nfrom keras.layers import Convolution2D, LeakyReLU, BatchNormalization, Dense, MaxPool2D, Flatten, Dropout, Input, merge\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import ReduceLROnPlateau\n\nclass CNN(object):\n    def __init__(self, image_train, image_vali, num_train, num_vali, species_train, species_vali, num_test, image_test, test_id):\n        self.image_train = image_train\n        self.image_vali = image_vali\n        self.num_train = num_train\n        self.num_vali = num_vali\n        self.species_train = species_train\n        self.species_vali = species_vali\n        self.num_test = num_test\n        self.image_test = image_test\n        self.test_id = test_id\n\n    def define_CNN(self):\n        image = Input(shape=(96, 96, 1), name='image')\n        x = Convolution2D(filters=32, kernel_size=(3,3), padding='same', use_bias=False)(image)\n        x = LeakyReLU(alpha=0.1)(x)\n        x = BatchNormalization()(x)\n\n        x = Convolution2D(filters=32, kernel_size=(3,3), padding='same', use_bias=False)(x)\n        x = LeakyReLU(alpha=0.1)(x)\n        x = BatchNormalization()(x)\n        x = MaxPool2D(pool_size=(2,2))(x)\n\n        x = Convolution2D(filters=64, kernel_size=(3, 3), padding='same', use_bias=False)(x)\n        x = LeakyReLU(alpha=0.1)(x)\n        x = BatchNormalization()(x)\n\n        x = Convolution2D(filters=64, kernel_size=(3, 3), padding='same', use_bias=False)(x)\n        x = LeakyReLU(alpha=0.1)(x)\n        x = BatchNormalization()(x)\n        x = MaxPool2D(pool_size=(2, 2))(x)\n\n        x = Convolution2D(filters=96, kernel_size=(3, 3), padding='same', use_bias=False)(x)\n        x = LeakyReLU(alpha=0.1)(x)\n        x = BatchNormalization()(x)\n\n        x = Convolution2D(filters=96, kernel_size=(3, 3), padding='same', use_bias=False)(x)\n        x = LeakyReLU(alpha=0.1)(x)\n        x = BatchNormalization()(x)\n        x = MaxPool2D(pool_size=(2, 2))(x)\n\n        x = Convolution2D(filters=128, kernel_size=(3, 3), padding='same', use_bias=False)(x)\n        x = LeakyReLU(alpha=0.1)(x)\n        x = BatchNormalization()(x)\n\n        x = Convolution2D(filters=128, kernel_size=(3, 3), padding='same', use_bias=False)(x)\n        x = LeakyReLU(alpha=0.1)(x)\n        x = BatchNormalization()(x)\n        x = MaxPool2D(pool_size=(2, 2))(x)\n\n        x = Convolution2D(filters=256, kernel_size=(3, 3), padding='same', use_bias=False)(x)\n        x = LeakyReLU(alpha=0.1)(x)\n        x = BatchNormalization()(x)\n\n        x = Convolution2D(filters=256, kernel_size=(3, 3), padding='same', use_bias=False)(x)\n        x = LeakyReLU(alpha=0.1)(x)\n        x = BatchNormalization()(x)\n        x = MaxPool2D(pool_size=(2, 2))(x)\n\n        x = Convolution2D(filters=512, kernel_size=(3, 3), padding='same', use_bias=False)(x)\n        x = LeakyReLU(alpha=0.1)(x)\n        x = BatchNormalization()(x)\n\n        x = Convolution2D(filters=512, kernel_size=(3, 3), padding='same', use_bias=False)(x)\n        x = LeakyReLU(alpha=0.1)(x)\n        x = BatchNormalization()(x)\n        x = MaxPool2D(pool_size=(2, 2))(x)\n\n        x = Flatten()(x)\n\n        numerical = Input(shape=(192,), name='numerical')\n        concatenated = merge.concatenate([x, numerical])\n\n        x = Dense(512, activation='relu')(concatenated)\n        x = Dropout(0.1)(x)\n        x = Dense(99, activation='softmax')(x)\n        self.model = Model(inputs=[image, numerical], outputs=x)\n\n    def RMSprop(self, batch_size, epoch):\n        optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-8, decay=0.0)\n        self.model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics=['accuracy'])\n        learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001)\n        self.model.fit([self.image_train, self.num_train], self.species_train, batch_size=batch_size, epochs=epoch,\n                       validation_data=([self.image_vali, self.num_vali], self.species_vali), verbose=0, callbacks=[learning_rate_reduction])\n\n    def make_predict(self):\n        ypred_prob = self.model.predict([self.image_test, self.num_test])\n        root = \"../input\"\n        labels = sorted(pd.read_csv(os.path.join(root, 'train.csv')).species.unique())\n        ypred = pd.DataFrame(ypred_prob, index = self.test_id, columns=labels)\n        fp = open('submit.csv', 'w')\n        fp.write(ypred.to_csv())\n        print('Done!')\n\n    def run(self):\n        self.define_CNN()\n        self.RMSprop(batch_size = 128, epoch=1600)\n        self.make_predict()\n\n\nif __name__ == '__main__':\n    data_clean = Data_Clean()\n    species, num, image, num_test, image_test, test_id = data_clean.run()\n    #划分训练集和测试集\n    species_train, species_vali, num_train, num_vali, image_train, image_vali = train_test_split(species, num, image, test_size=0.15, random_state=93)\n\n    #搭建CNN模型，看看CNN对图像的分类情况\n    cnn = CNN(image_train, image_vali, num_train, num_vali,species_train, species_vali, num_test, image_test, test_id)\n    cnn.run()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}