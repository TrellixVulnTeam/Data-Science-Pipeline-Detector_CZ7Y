{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"4b43dabc-3dad-ac97-dc5a-1658eb40c169"},"source":"# leaf classification problem\n\n### The training set contains 99 species and 10 samples for each"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3e91873f-868b-b0ed-0c4c-c38b2d81d85a"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f0887d92-985d-af80-90de-a673f0529c5b"},"outputs":[],"source":"train_df = pd.read_csv('../input/train.csv')\ntrain_df.info()\ncopy_df = train_df.copy()"},{"cell_type":"markdown","metadata":{"_cell_guid":"d48cf74d-61a7-22df-39a4-bdbc966ae818"},"source":"## Now we're done preperation. Time to do some data wrangling (mapping species into indices)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"90804b14-4e95-fe96-2dbb-5367064981c5"},"outputs":[],"source":"species = train_df['species'].unique()\nspecies.sort()\nspe_dict = dict(enumerate(species))\ninv_spe_dict = {v: k for k,v in spe_dict.items()}\ntrain_df['species_index'] = train_df['species'].map(inv_spe_dict).astype(int)\ntrain_df = train_df.drop(['species','id'],axis = 1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"8eacdc32-a26c-be3f-9f34-604ce59c21f5"},"source":"## Done data wrangling. Start training CLF"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd369de0-2a4c-48a8-436b-2db40b0cdc91"},"outputs":[],"source":"train_data = train_df.values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ffd80508-e215-a60d-6329-596d4c1c9618"},"outputs":[],"source":"#Essntially, 'lbfgs' methond alreday did a great job on data set smaller than 1,000(so, a bigger training set seems\n#like a overkill). Over 1,000, we'd like to use 'adam' but 'adam' works so badly.\n\n#You can do it over and over again in order to get a larger training set.\na = np.random.permutation(train_data)\nb = np.random.permutation(train_data)\ntrain_data = np.vstack((a,b))\ntrain_data.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e30fca64-c9e8-0e8e-fc06-4acf47fa7bdc"},"outputs":[],"source":"#since we have dropped the 'specie'&'id', the features starts at col 0. The last col is the labels.\nX,y = train_data[:,:-1],train_data[:,-1]\nscaler = StandardScaler().fit(X)\nX = scaler.transform(X)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7d82fcbf-c1f4-0b53-029f-fd42a7cd87d2"},"outputs":[],"source":"model = MLPClassifier(hidden_layer_sizes=(150,),activation='logistic',solver='lbfgs',alpha=0.001\n                      ,max_iter=200,early_stopping=True,validation_fraction=0.2,\n                      learning_rate='adaptive',tol=1e-8,random_state=1).fit(X,y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0201af8f-f1cf-ab7d-35c3-15e72f101846"},"outputs":[],"source":"def cv(a,b,model):\n    cv_X = train_data[a:b,:-1]\n    cv_X = scaler.transform(cv_X)\n    cv_y = train_data[a:b,-1]\n    cv_p1_y = model.predict(cv_X)\n    cv_p2_y = model.predict_log_proba(cv_X)\n    print(accuracy_score(cv_y,cv_p1_y))\n    print(cv_y)\n    print(cv_p1_y)\n#print(cv_p2_y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4fca503f-c022-dce7-6bd9-a97996f4bc46"},"outputs":[],"source":"cv(200,220,model)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6244408a-79a0-365b-c72f-6dad039ccd34"},"outputs":[],"source":"#Testing overfitting/underfitting of our origional training set\nd = np.random.permutation(train_df.values)\nXp = d[:,:-1]\nXp = scaler.transform(Xp)\nyp = d[:,-1]\nypp = model.predict(Xp)\n#cv_p2_y = model.predict_log_proba(cv_X)\nprint(accuracy_score(yp,ypp))\n#print(cv_y)\n#print(cv_p1_y)\n#print(cv_p2_y)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a419e53a-97f8-58c4-c0ec-e8087e64ba6e"},"source":"## Let's go predict, shall we?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"084303b2-7787-6510-5eb2-82b9a0dfb016"},"outputs":[],"source":"test_df = pd.read_csv('../input/test.csv')\nindex = test_df.pop('id')\ntest_data = test_df.values\n\ntest_X = test_data\ntest_X = scaler.transform(test_X)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"de8f6d91-5f54-7c27-6a42-4df575ef0375"},"outputs":[],"source":"predict = model.predict(test_X)\npredict_proba = model.predict_proba(test_X)\npredict_proba"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"13a3f317-0801-f23d-8b77-249c9b726856"},"outputs":[],"source":"#print(species.tolist())\nspecies_list = species.tolist()\nresult = pd.DataFrame(predict_proba,index = index, columns = species_list)\n# It seems like this competition scores higher for these probability results(well, unless you can figure\n# out the true answers completely ¯\\_(ツ) _/¯)\nresult"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"876ea75b-c99d-7a8f-46d3-2198ea63fcf7"},"outputs":[],"source":"fp = open('submission.csv','w')\nfp.write(result.to_csv())"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}