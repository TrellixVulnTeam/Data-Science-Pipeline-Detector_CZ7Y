{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"75bd814e-9fa4-070c-aa71-bc4b18106296"},"source":"## Using Neural Networks through Keras"},{"cell_type":"markdown","metadata":{"_cell_guid":"224f9801-741c-7d4a-8946-5034d310583b"},"source":"Copied from Kaggle itself - see if I make it better !"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93768274-a451-f111-440c-cf929716d679"},"outputs":[],"source":"## Measure execution time, becaus Kaggle cloud fluctuates  \nimport time\nstart = time.time()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"42859ad0-e281-758d-dc97-1453c170df10"},"outputs":[],"source":"## Importing standard libraries\n%pylab inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"472dc525-28ef-8909-6daf-14db89d02736"},"outputs":[],"source":"## Importing sklearn libraries\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedShuffleSplit"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1a31d09-8fb8-1b9a-e2d5-3af7f1b9a395"},"outputs":[],"source":"## Keras Libraries for Neural Networks\n\nfrom keras.models import Sequential\nfrom keras.layers import Merge\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import EarlyStopping"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4cda25b4-3b84-99b1-2535-bbf258413407"},"outputs":[],"source":"## Read data from the CSV file\ndata = pd.read_csv('../input/train.csv')\nparent_data = data.copy()    ## Always a good idea to keep a copy of original data\nID = data.pop('id')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"260f0d07-9d76-942b-4c3b-cd68e6607581"},"outputs":[],"source":"data.shape\ndata.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"70a39b8b-c802-0b1c-ec80-31f2fca5b113"},"outputs":[],"source":"## Since the labels are textual, so we encode them categorically\n\ny = data.pop('species')\ny = LabelEncoder().fit(y).transform(y)\nprint(y.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4d6decaa-a33e-ccd3-a31d-e9a25b7f41a8"},"outputs":[],"source":"## Most of the learning algorithms are prone to feature scaling\n## Standardising the data to give zero mean =)\nfrom sklearn import preprocessing\nX = preprocessing.MinMaxScaler().fit(data).transform(data)\nX = StandardScaler().fit(data).transform(data)\n## normalizing does not help here; l1 and l2 allowed\n## X = preprocessing.normalize(data, norm='l1')\nprint(X.shape)\nX"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2f5c24c6-ca68-f5d8-5e1e-277455b2bb13"},"outputs":[],"source":"## We will be working with categorical crossentropy function\n## It is required to further convert the labels into \"one-hot\" representation\n\ny_cat = to_categorical(y)\nprint(y_cat.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cbd61c0d-b225-2a84-21c5-f31e2f6e02c8"},"outputs":[],"source":"## retain class balances\nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.2,random_state=12345)\ntrain_index, val_index = next(iter(sss.split(X, y)))\nx_train, x_val = X[train_index], X[val_index]\ny_train, y_val = y_cat[train_index], y_cat[val_index]\nprint(\"x_train dim: \",x_train.shape)\nprint(\"x_val dim:   \",x_val.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"820fb857-a7db-2279-a228-c799dddbf110"},"outputs":[],"source":"## Developing a layered model for Neural Networks\n## Input dimensions should be equal to the number of features\n## We used softmax layer to predict a uniform probabilistic distribution of outcomes\n## https://keras.io/initializations/ ;glorot_uniform, glorot_normal, lecun_uniform, orthogonal,he_normal\n\nmodel = Sequential()\nmodel.add(Dense(800,input_dim=192,  init='glorot_normal', activation='relu'))\nmodel.add(Dropout(0.1))\n\nmodel.add(Dense(400, activation='sigmoid'))\nmodel.add(Dropout(0.1))\n\nmodel.add(Dense(99, activation='softmax'))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8cd5713-e6b6-7248-2ac9-34e8c4b13be2"},"outputs":[],"source":"## Error is measured as categorical crossentropy or multiclass logloss\n## Adagrad, rmsprop, SGD, Adadelta, Adam, Adamax, Nadam\n\nmodel.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics = [\"accuracy\"])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"05e4916c-17cc-693d-2784-e913b103d063"},"outputs":[],"source":"## Fitting the model on the whole training data\n## the validation is literally just the last x% of samples in the input you passed.\n## the split is deeply flawed because it should split proportionally for multi-class problems\n## this is one of the reasons of low performance of this model\nearly_stopping = EarlyStopping(monitor='val_loss', patience=300)\n\n# history = model.fit(X,y_cat,batch_size=192,\n#                    nb_epoch=800 ,verbose=0, validation_split=0.1, callbacks=[early_stopping])\n\nhistory = model.fit(x_train, y_train,batch_size=192,nb_epoch=2500 ,verbose=0,\n                    validation_data=(x_val, y_val),callbacks=[early_stopping])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"991bf2ab-a9af-baaa-3f47-805839e13ee3"},"outputs":[],"source":"## we need to consider the loss for final submission to leaderboard\n## print(history.history.keys())\nprint('val_acc: ',max(history.history['val_acc']))\nprint('val_loss: ',min(history.history['val_loss']))\nprint('train_acc: ',max(history.history['acc']))\nprint('train_loss: ',min(history.history['loss']))\n\nprint()\nprint(\"train/val loss ratio: \", min(history.history['loss'])/min(history.history['val_loss']))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b48ee871-13de-0b1e-438d-d6e2937e17f8"},"outputs":[],"source":"## summarize history for loss\n## Plotting the loss with the number of iterations\nplt.semilogy(history.history['loss'])\nplt.semilogy(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"08739222-9ca7-05ea-eabc-743002cc7327"},"outputs":[],"source":"## Plotting the error with the number of iterations\n## With each iteration the error reduces smoothly\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6060cdf0-cba4-0bd9-b0ee-d75dd15f0462"},"outputs":[],"source":"## read test file\ntest = pd.read_csv('../input/test.csv')\nindex = test.pop('id')\n\n## we need to perform the same transformations from the training set to the test set\ntest = preprocessing.MinMaxScaler().fit(test).transform(test)\ntest = StandardScaler().fit(test).transform(test)\nyPred = model.predict_proba(test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c52f1c2-ecfa-f3a3-f3b3-5fa0197995d0"},"outputs":[],"source":"## Converting the test predictions in a dataframe as depicted by sample submission\nyPred = pd.DataFrame(yPred,index=index,columns=sort(parent_data.species.unique()))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe58a1d0-7e94-24df-ceb8-0ef876379677"},"outputs":[],"source":"## write submission to file\nfp = open('submission_nn_kernel.csv','w')\nfp.write(yPred.to_csv())\n\n## print run time\nend = time.time()\nprint(round((end-start),2), \"seconds\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"10bf679e-a4e5-72ac-e7a6-f0652a4a3891"},"source":"---------\n\nEarlier` we used a 4 layer network but the result came out to be overfitting the test set. We dropped the count of neurones in the network and also restricted the number of layers to 3 so as to keep it simple.\nInstead of submitting each test sample as a one hot vector we submitted each samples as a probabilistic distribution over all the possible outcomes. This \"may\" help reduce the penalty being exercised by the multiclass logloss thus producing low error on the leaderboard! ;)\nAny suggestions are welcome!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dfe70334-1f60-e1c6-6ca8-90ebe626a5af"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}