{"nbformat_minor":1,"cells":[{"execution_count":null,"source":"%pylab inline\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom keras.models import Sequential\nfrom keras.layers import Merge\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import EarlyStopping\n\n## Read data from the CSV file\ndata = pd.read_csv('../input/train.csv')\nparent_data = data.copy()    ## Always a good idea to keep a copy of original data\n__id__ = data.pop('id')\n\ndata.shape\ndata.describe()\n\n## Need to encode labels as they're strings\ny = data.pop('species')\ny = LabelEncoder().fit(y).transform(y)\nprint(y.shape)\n\n## Normalizing data, zero mean\nX = preprocessing.MinMaxScaler().fit(data).transform(data)\nX = StandardScaler().fit(data).transform(data)\nprint(X.shape)\nX\n\n## We will be working with categorical crossentropy function\n## It is required to further convert the labels into \"one-hot\" representation\ny_cat = to_categorical(y)\nprint(y_cat.shape)\n\n## retain class balances \nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.1,random_state=12345)\ntrain_id, value_id = next(iter(sss.split(X, y)))\nx_train, x_val = X[train_id], X[value_id]\ny_train, y_val = y_cat[train_id], y_cat[value_id]\nprint(\"x_train dim: \",x_train.shape)\nprint(\"x_val dim:   \",x_val.shape)\nprint()\n# ----------------\n## Developing a layered model for Neural Networks No/4/\n## Input dimensions should be equal to the number of features\n## We used softmax layer to predict a uniform probabilistic distribution of outcomes\nModel = Sequential()\nModel.add(Dense(900,input_dim=192,  init='uniform', activation='relu'))\nModel.add(Dropout(0.25))\nModel.add(Dense(450, activation='sigmoid'))\nModel.add(Dropout(0.25))\nModel.add(Dense(99, activation='softmax'))\n\n\nModel.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics = [\"accuracy\"])\nearly_stopping = EarlyStopping(monitor='val_loss', patience=600)\nhistory = Model.fit(x_train, y_train,batch_size=192,nb_epoch=2500 ,verbose=0,validation_data=(x_val, y_val),callbacks=[early_stopping])\n                    \nprint('val_acc: ',max(history.history['val_acc']))\nprint('val_loss: ',min(history.history['val_loss']))\nprint('train_acc: ',max(history.history['acc']))\nprint('train_loss: ',min(history.history['loss']))\nprint(\"train/val loss ratio: \", min(history.history['loss'])/min(history.history['val_loss']))\n\n\n\n\n## read test file\ntest = pd.read_csv('../input/test.csv')\nindex = test.pop('id')\n\n## we need to perform the same transformations from the training set to the test set\ntest = preprocessing.MinMaxScaler().fit(test).transform(test)\ntest = StandardScaler().fit(test).transform(test)\n\n\nyPred =  Model.predict_proba(test)\n\nyPred = pd.DataFrame(yPred,index=index,columns=sort(parent_data.species.unique()))\n\n\n\nfp = open('submission_nn_kernel.csv','w')\nfp.write(yPred.to_csv())\n\n","cell_type":"code","metadata":{"_uuid":"017b05d747517b59240d14c07db73474240816a6","_cell_guid":"ae537d1a-e285-4836-a9cf-f7317234de28"},"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.4","file_extension":".py","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3"}},"nbformat":4}