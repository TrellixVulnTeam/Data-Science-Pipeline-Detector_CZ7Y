{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"dc756b03-2f5a-b49e-59c1-2a9e2fe6d40f"},"source":"Effects of Features combination using 10 different classifier\n=============================================================\n\nThis is just an extension of @Jeff Delaney's notebook, [10 Classifier Showdown in Scikit-Learn][1].\nNothing really usefull here, since all the classifiers are not fine-tuned for this task. Anyway, I've just wanted to see the effects of the combination of different features over the accuracy.\n\n  [1]: https://www.kaggle.com/jeffd23/leaf-classification/10-classifier-showdown-in-scikit-learn/notebook"},{"cell_type":"markdown","metadata":{"_cell_guid":"421934d0-ff36-7d3e-d95e-7334e5229c44"},"source":"In this code block, I've created two arrays: *train_set* and *test_set*. Each element of these arrays is a dataframe, filtered from the initial *train/test* dataframes in order to separate the different features.\n7 different features combination are going to be computed: Shape, Margin, Texture, Shape-Margin, Shape-Texture, Margin-Texture, Shape-Margin-Texture."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff9360a0-cd57-c83a-7382-e8c0aaee4658"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cross_validation import StratifiedShuffleSplit\n\ntrain = pd.read_csv('../input/../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# Swiss army knife function to organize the data\ndef encode(train, test):\n    le = LabelEncoder().fit(train.species) \n    labels = le.transform(train.species)           # encode species strings\n    classes = list(le.classes_)                    # save column names for submission\n    test_ids = test.id                             # save test ids for submission\n    \n    train = train.drop(['species', 'id'], axis=1)  \n    test = test.drop(['id'], axis=1)\n    \n    cols_s = [c for c in train.columns if 'shape' in c]\n    cols_t = [c for c in train.columns if 'texture' in c]\n    cols_m = [c for c in train.columns if 'margin' in c]\n    cols_st = [c for c in train.columns if ('shape' in c or 'texture' in c)]\n    cols_mt = [c for c in train.columns if ('margin' in c or 'texture' in c)]\n    cols_sm = [c for c in train.columns if ('margin' in c or 'shape' in c)]\n\n    train_set = [train[cols_s], train[cols_m], train[cols_t], train[cols_st], train[cols_sm], train[cols_mt], train]\n    test_set = [test[cols_s], test[cols_m], test[cols_t], test[cols_st], test[cols_sm], test[cols_mt], test]\n    \n    return train, labels, test, test_ids, classes, train_set, test_set\n\ntrain, labels, test, test_ids, classes, train_set, test_set = encode(train, test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1b4c0d1d-d08e-b99e-1ee9-65f6a27c95c1"},"source":"Preparation of the classifiers\n------------------------------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"087a6408-f491-72a9-2043-93873b7a4864"},"outputs":[],"source":"# Simply looping through 10 out-of-the box classifiers and printing the results.\n# Obviously, these will perform much better after tuning their hyperparameters, \n# but this gives you a decent ballpark idea\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    NuSVC(probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis()]"},{"cell_type":"markdown","metadata":{"_cell_guid":"00f80e7a-d031-b2fc-9d8d-17491dc9679b"},"source":"Elaboration\n===========\n\nFor each classifier, compute its accuracy and log-loss and store these values in the log."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f7fba794-c7b5-efff-5aa0-c96aeba97cb2"},"outputs":[],"source":"# Logging for Visual Comparison\nlog_cols=[\"Classifier\", \"Acc Sh\", \"Acc Ma\", \"Acc Te\", \"Acc Sh-Te\", \"Acc Sh-Ma\", \"Acc Ma-Te\", \"Acc Sh-Ma-Te\", \"LL Sh\", \"LL Ma\", \"LL Te\", \"LL Sh-Te\", \"LL Sh-Ma\", \"LL Ma-Te\", \"LL Sh-Ma-Te\"]\nlog = pd.DataFrame(columns=log_cols)\nfeat = [\"Shape\", \"Margin\", \"Texture\", \"Shape-Texture\", \"Shape-Margin\", \"Margin-Texture\", \"Shape-Margin-Texture\"]\n\nfor clf in classifiers:\n    name = clf.__class__.__name__\n    print(\"=\"*30)\n    print(name)\n    i = 0\n    acc_col = []\n    ll_col = []    \n    \n    for tr, te in zip(train_set, test_set):\n        print(\"\\t\" + feat[i])\n        i += 1\n        # Stratification is necessary for this dataset because there is a relatively \n        # large number of classes (100 classes for 990 samples). This will ensure we \n        # have all classes represented in both the train and test indices\n        sss = StratifiedShuffleSplit(labels, 10, test_size=0.2, random_state=23)\n        \n        for train_index, test_index in sss:\n            X_train, X_test = tr.values[train_index], tr.values[test_index]\n            y_train, y_test = labels[train_index], labels[test_index]\n        \n        # train the classifier\n        clf.fit(X_train, y_train)\n        \n        train_predictions = clf.predict(X_test)\n        acc = accuracy_score(y_test, train_predictions)\n        acc_col.append(acc)\n        \n        train_predictions = clf.predict_proba(X_test)\n        ll = log_loss(y_test, train_predictions)\n        ll_col.append(ll)\n        \n    log_entry = pd.DataFrame([[name]+acc_col+ll_col], columns=log_cols)\n    log = log.append(log_entry)"},{"cell_type":"markdown","metadata":{"_cell_guid":"24c58bbe-211f-135e-d953-16edfca35f2f"},"source":"Plot the result\n==============="},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ed958d9a-1438-09a9-e2bc-1ff962aecee3"},"outputs":[],"source":"cols = [c for c in log.columns if c[0:2] != 'LL']\ndf1 = log[cols]\ncols = [c for c in log.columns if c[0:2] != 'Ac']\ndf2 = log[cols]\ndf1.plot(kind='barh', x='Classifier', title=\"Accuracy\", figsize=(10,20), width=0.8, colormap=\"jet\")\ndf2.plot(kind='barh', x='Classifier', title=\"Log Loss\", figsize=(10,20), width=0.8, colormap=\"jet\")"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}