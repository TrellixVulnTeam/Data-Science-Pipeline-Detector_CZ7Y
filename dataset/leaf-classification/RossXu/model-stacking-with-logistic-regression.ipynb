{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"fd982c8a-5d29-067c-889c-fdae69b46e21"},"source":"# Model Stacking with Logistic Regression"},{"cell_type":"markdown","metadata":{"_cell_guid":"983af63c-f2cc-a135-2c2e-4de7f1eac86c"},"source":"In this notebook, I tried to blend results of several models by a logistic regression, which generates improved logloss in comparison to the 1st level models. In this dataset, the blened result is not better than using logistic regression directly (LB: 0.040). However, this method definitely has more potential with a bigger and more complex dataset.\n\n1st level models - parameters have been tuned in one of my earlier published notebook.\n\nAda Boosting best CV score: -2.27116019719\nGradient Boosting best CV score: -2.12612708664\nRandom Forest best CV score: -0.826567251939\nKNN best CV score: -0.173115044296\nSVC best CV score: -2.41476782204\n\n2nd leve Logistic Regression best LB score: 0.042\n\nMy acknowledgement goes to Tilli's resply to my question in another competition, and also thanks to authors of the following codes:\nhttps://github.com/emanuele/kaggle_pbr/blob/master/blend.py"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"105dc18f-a7ed-6924-56b5-07579f5af57f"},"outputs":[],"source":"import numpy as np\nimport scipy as sp\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef warn(*args, **kwargs): pass\nimport warnings\nwarnings.warn = warn\n\n%matplotlib inline\n\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.cross_validation import StratifiedShuffleSplit\n\ntrain = pd.read_csv('../input/train.csv').drop('id',axis=1)\ntest = pd.read_csv('../input/test.csv')\ntest_ids = test['id']\ntest.drop('id',axis=1,inplace=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6079b316-e3eb-a5af-4ac2-aa00d700b351"},"source":"There is no null value in train and test data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"74c2eeef-d821-2538-4bf1-6712f650ab13"},"outputs":[],"source":"print(train.isnull().any().any())\nprint(test.isnull().any().any())"},{"cell_type":"markdown","metadata":{"_cell_guid":"4d21cde6-f4a0-2a04-70a0-9042f0eaccc2"},"source":"# Label Encoding the Label"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7bd7e2bb-26ec-34a5-e69f-feb27e022992"},"outputs":[],"source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ffc4dd13-3163-adf1-81ec-908bd67eaad1"},"outputs":[],"source":"species = train['species']\ntrain.drop('species',axis=1,inplace=True)\ny_train = le.fit_transform(species)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c0b940fb-d0e7-5242-3e34-593781f9bfc4"},"source":"# Normalize the Sparse Features"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e5bcbae0-d109-c392-ca13-f7cb919c895a"},"outputs":[],"source":"from sklearn.preprocessing import MaxAbsScaler"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b533b01-139f-46df-6815-508f18682b13"},"outputs":[],"source":"x_data = np.vstack([train,test])\nmas = MaxAbsScaler()\nn_x_data = mas.fit_transform(x_data)\nprint(n_x_data.shape)\nn_x_data"},{"cell_type":"markdown","metadata":{"_cell_guid":"b96c3350-6f8c-3eb8-91b2-437e2fbe843b"},"source":"# Split the dataset - raw features"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55fc716a-8c24-5956-9d3a-e99490a28f08"},"outputs":[],"source":"x_test = n_x_data[len(species):,:]\nx_train = n_x_data[0:len(species),:]"},{"cell_type":"markdown","metadata":{"_cell_guid":"d2f96381-cb13-5b35-4846-c246cf6d3d88"},"source":"# Setting up models and grid search"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3600a4ae-1c85-b58d-4344-88b4ce5199f6"},"outputs":[],"source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import accuracy_score, log_loss"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"17043611-06fd-8b53-e5ca-843b077ac83e"},"outputs":[],"source":"seed=1\nmodels = [\n            'ADB',\n            'GBC',\n            'RFC',\n            'KNC',\n            'SVC'\n         ]\nclfs = [\n        AdaBoostClassifier(random_state=seed,n_estimators = 150, learning_rate = 0.01), # best score -2.27\n        GradientBoostingClassifier(random_state=seed,min_samples_split=2, n_estimators=100, learning_rate=0.01, \n                                   max_depth=3, min_samples_leaf=4), # best score -2.13\n        RandomForestClassifier(random_state=seed,n_jobs=-1,min_samples_split=2,n_estimators=100,\n                               criterion='gini',min_samples_leaf=1),\n        KNeighborsClassifier(n_jobs=-1,n_neighbors=5, weights='distance', leaf_size=15),\n        SVC(random_state=seed,probability=True,kernel='sigmoid',C=100, tol=0.005)\n        ]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"828aa8e0-b3fe-c79b-977c-7d32f792d17b"},"outputs":[],"source":"pred_train_models = []\npred_test_models = []"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c7f9f50-f75c-3948-d7dd-aad04b28ca2f"},"outputs":[],"source":"from sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import StratifiedKFold\n\nkfold = 3 # use a bigger number\n\nsss = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=seed)\ncvfolds = list(sss.split(x_train,y_train))\n\nfor j,clf in enumerate(clfs):\n    print(j,clf)\n    dataset_test_j = 0 \n    dataset_train_j = np.zeros((x_train.shape[0],len(np.unique(y_train))))\n    for i,(train_index, test_index) in enumerate(cvfolds):\n        n_x_train, n_x_val = x_train[train_index], x_train[test_index]\n        n_y_train, n_y_val = y_train[train_index], y_train[test_index]\n        print('fold ' + str(i))        \n        clf.fit(n_x_train,n_y_train)\n        dataset_train_j[test_index,:] = clf.predict_proba(n_x_val)\n        dataset_test_j += clf.predict_proba(x_test)\n    pred_train_models.append(dataset_train_j)\n    pred_test_models.append(dataset_test_j/float(kfold))\n    \npred_blend_train = np.hstack(pred_train_models)\npred_blend_test = np.hstack(pred_test_models)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"108ceb6a-6ec1-868e-1815-c565057fbaa2"},"outputs":[],"source":"print('\\Blending results with a Logistic Regression ... ')\n\nblendParams = {'C':[1000],'tol':[0.01]} # test more values in your local machine\nclf = GridSearchCV(LogisticRegression(solver='newton-cg', multi_class='multinomial'), blendParams, scoring='log_loss',\n                   refit='True', n_jobs=-1, cv=5)\nclf.fit(pred_blend_train,y_train)\nprint('The Best parameters of the blending model\\n{}'.format(clf.best_params_))\nprint('The best score:{}'.format(clf.best_score_))\n\nestimates = clf.predict_proba(pred_blend_test)\nsubmission = pd.DataFrame(estimates, index=test_ids, columns=le.classes_)\nsubmission.to_csv('./blendedEnsembles.csv')"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}