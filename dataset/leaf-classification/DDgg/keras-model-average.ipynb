{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"57a257b8-f5f9-ddc8-2c84-0968cbd90315"},"source":"Average several keras models to obtain better scores"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e1a3c8fd-06df-aeb3-1519-5fceda7fa470"},"outputs":[],"source":"## Measure execution time, becaus Kaggle cloud fluctuates  \nimport time\nstart = time.time()\n\n## Importing standard libraries\n%pylab inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n## Importing sklearn libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n## Keras Libraries for Neural Networks\nfrom keras.models import Sequential\nfrom keras.layers import Merge\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import EarlyStopping\n\n## Read data from the CSV file\ndata = pd.read_csv('../input/train.csv')\nparent_data = data.copy()    ## Always a good idea to keep a copy of original data\nID = data.pop('id')\n\ndata.shape\ndata.describe()\n\n## Since the labels are textual, so we encode them categorically\ny = data.pop('species')\ny = LabelEncoder().fit(y).transform(y)\nprint(y.shape)\n\n## Most of the learning algorithms are prone to feature scaling\n## Standardising the data to give zero mean =)\nfrom sklearn import preprocessing\nX = preprocessing.MinMaxScaler().fit(data).transform(data)\nX = StandardScaler().fit(data).transform(data)\n## normalizing does not help here; l1 and l2 allowed\n## X = preprocessing.normalize(data, norm='l1')\nprint(X.shape)\nX\n\n## We will be working with categorical crossentropy function\n## It is required to further convert the labels into \"one-hot\" representation\ny_cat = to_categorical(y)\nprint(y_cat.shape)\n\n## retain class balances \nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.1,random_state=12345)\ntrain_index, val_index = next(iter(sss.split(X, y)))\nx_train, x_val = X[train_index], X[val_index]\ny_train, y_val = y_cat[train_index], y_cat[val_index]\nprint(\"x_train dim: \",x_train.shape)\nprint(\"x_val dim:   \",x_val.shape)\nprint()\n\n## --------------------------------------------------\n## Developing a layered model for Neural Networks No/1/\n## Input dimensions should be equal to the number of features\n## We used softmax layer to predict a uniform probabilistic distribution of outcomes\nmodel1 = Sequential()\nmodel1.add(Dense(600,input_dim=192,  init='uniform', activation='relu'))\nmodel1.add(Dropout(0.3))\nmodel1.add(Dense(600, activation='sigmoid'))\nmodel1.add(Dropout(0.3))\nmodel1.add(Dense(99, activation='softmax'))\n\n## Error is measured as categorical crossentropy or multiclass logloss\n## Adagrad, rmsprop, SGD, Adadelta, Adam, Adamax, Nadam\nmodel1.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics = [\"accuracy\"])\n\n## Fitting the model on the whole training data\nearly_stopping = EarlyStopping(monitor='val_loss', patience=300)\nhistory = model1.fit(x_train, y_train,batch_size=192,nb_epoch=2500 ,verbose=0,\n                    validation_data=(x_val, y_val),callbacks=[early_stopping])\n                    \n## we need to consider the loss for final submission to leaderboard\nprint('val_acc: ',max(history.history['val_acc']))\nprint('val_loss: ',min(history.history['val_loss']))\nprint('train_acc: ',max(history.history['acc']))\nprint('train_loss: ',min(history.history['loss']))\nprint(\"train/val loss ratio: \", min(history.history['loss'])/min(history.history['val_loss']))\n\n## summarize history for loss\n## Plotting the loss with the number of iterations\nplt.semilogy(history.history['loss'])\nplt.semilogy(history.history['val_loss'])\nplt.title('model1 loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n## Plotting the error with the number of iterations\n## With each iteration the error reduces smoothly\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model1 accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n## --------------------------------------------------\n## Developing a layered model for Neural Networks No/2/\n## Input dimensions should be equal to the number of features\n## We used softmax layer to predict a uniform probabilistic distribution of outcomes\nmodel2 = Sequential()\nmodel2.add(Dense(1024,input_dim=192,  init='glorot_normal', activation='relu'))\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(512, activation='sigmoid'))\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(99, activation='softmax'))\n\n## Error is measured as categorical crossentropy or multiclass logloss\n## Adagrad, rmsprop, SGD, Adadelta, Adam, Adamax, Nadam\nmodel2.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics = [\"accuracy\"])\n\n## Fitting the model on the whole training data\nearly_stopping = EarlyStopping(monitor='val_loss', patience=300)\nhistory = model2.fit(x_train, y_train,batch_size=192,nb_epoch=2500 ,verbose=0,\n                    validation_data=(x_val, y_val),callbacks=[early_stopping])\n                    \n## we need to consider the loss for final submission to leaderboard\nprint('val_acc: ',max(history.history['val_acc']))\nprint('val_loss: ',min(history.history['val_loss']))\nprint('train_acc: ',max(history.history['acc']))\nprint('train_loss: ',min(history.history['loss']))\nprint(\"train/val loss ratio: \", min(history.history['loss'])/min(history.history['val_loss']))\n\n## summarize history for loss\n## Plotting the loss with the number of iterations\nplt.semilogy(history.history['loss'])\nplt.semilogy(history.history['val_loss'])\nplt.title('model2 loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n## Plotting the error with the number of iterations\n## With each iteration the error reduces smoothly\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model2 accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n## --------------------------------------------------\n## Developing a layered model for Neural Networks No/3/\n## Input dimensions should be equal to the number of features\n## We used softmax layer to predict a uniform probabilistic distribution of outcomes\nmodel3 = Sequential()\nmodel3.add(Dense(600,input_dim=192,  init='uniform', activation='relu'))\nmodel3.add(Dropout(0.3))\nmodel3.add(Dense(600, activation='sigmoid'))\nmodel3.add(Dropout(0.3))\nmodel3.add(Dense(99, activation='softmax'))\n\n## Error is measured as categorical crossentropy or multiclass logloss\n## Adagrad, rmsprop, SGD, Adadelta, Adam, Adamax, Nadam\nmodel3.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics = [\"accuracy\"])\n\n## Fitting the model on the whole training data\nearly_stopping = EarlyStopping(monitor='val_loss', patience=300)\nhistory = model3.fit(x_train, y_train,batch_size=192,nb_epoch=2500 ,verbose=0,\n                    validation_data=(x_val, y_val),callbacks=[early_stopping])\n                    \n## we need to consider the loss for final submission to leaderboard\nprint('val_acc: ',max(history.history['val_acc']))\nprint('val_loss: ',min(history.history['val_loss']))\nprint('train_acc: ',max(history.history['acc']))\nprint('train_loss: ',min(history.history['loss']))\nprint(\"train/val loss ratio: \", min(history.history['loss'])/min(history.history['val_loss']))\n\n## summarize history for loss\n## Plotting the loss with the number of iterations\nplt.semilogy(history.history['loss'])\nplt.semilogy(history.history['val_loss'])\nplt.title('model3 loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n## Plotting the error with the number of iterations\n## With each iteration the error reduces smoothly\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model3 accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n## --------------------------------------------------\n## Developing a layered model for Neural Networks No/4/\n## Input dimensions should be equal to the number of features\n## We used softmax layer to predict a uniform probabilistic distribution of outcomes\nmodel4 = Sequential()\nmodel4.add(Dense(768,input_dim=192,  init='uniform', activation='relu'))\nmodel4.add(Dropout(0.3))\nmodel3.add(Dense(384, activation='sigmoid'))\nmodel3.add(Dropout(0.3))\nmodel4.add(Dense(99, activation='softmax'))\n\n## Error is measured as categorical crossentropy or multiclass logloss\n## Adagrad, rmsprop, SGD, Adadelta, Adam, Adamax, Nadam\nmodel4.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics = [\"accuracy\"])\n\n## Fitting the model on the whole training data\nearly_stopping = EarlyStopping(monitor='val_loss', patience=600)\nhistory = model4.fit(x_train, y_train,batch_size=192,nb_epoch=2500 ,verbose=0,\n                    validation_data=(x_val, y_val),callbacks=[early_stopping])\n                    \n## we need to consider the loss for final submission to leaderboard\nprint('val_acc: ',max(history.history['val_acc']))\nprint('val_loss: ',min(history.history['val_loss']))\nprint('train_acc: ',max(history.history['acc']))\nprint('train_loss: ',min(history.history['loss']))\nprint(\"train/val loss ratio: \", min(history.history['loss'])/min(history.history['val_loss']))\n\n## summarize history for loss\n## Plotting the loss with the number of iterations\nplt.semilogy(history.history['loss'])\nplt.semilogy(history.history['val_loss'])\nplt.title('model4 loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n## Plotting the error with the number of iterations\n## With each iteration the error reduces smoothly\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model4 accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n#----------------------------------------------\n#----------------------------------------------\n## read test file\ntest = pd.read_csv('../input/test.csv')\nindex = test.pop('id')\n\n## we need to perform the same transformations from the training set to the test set\ntest = preprocessing.MinMaxScaler().fit(test).transform(test)\ntest = StandardScaler().fit(test).transform(test)\nyPred1 = model1.predict_proba(test)\nyPred2 = model2.predict_proba(test)\nyPred3 = model3.predict_proba(test)\nyPred4 = model4.predict_proba(test)\n\n## average all models\nyPred = (yPred1 + yPred2 + yPred3 + yPred4 ) / 4.0\n\n## Converting the test predictions in a dataframe as depicted by sample submission\nyPred = pd.DataFrame(yPred,index=index,columns=sort(parent_data.species.unique()))\n\n# show data frame\nyPred\n\n## write submission to file\nfp = open('submission_nn_kernel.csv','w')\nfp.write(yPred.to_csv())\n\n## print run time\nend = time.time()\nprint()\nprint(round((end-start),2), \"seconds\")"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}