{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transaction Value Prediction\nIn this notebook, we will tackle the challenge presented by Santander Group in terms of predicting \"the value of transactions for each potential customer\". In other words, the rgoup would provide an an anonymized dataset (for security purpose) to attain this objective. In essence, this presents a regression problem which we shall resolve using different machine learning techniques.\n\n# Table of Contents\n1. Environment set-up\n    * Importing Libraries\n    * Loading the data\n2. Initial Diagnostics\n    * Glimpse\n    * Descriptive Statitics\n    * Target Variable Analysis\n3. Data Cleaning\n    * Missing Values\n    * Outliers\n    * Duplicate Observations\n4. Correlation Analysis\n5. Machine Learning set-up\n    * Feature Scaling\n    * Train-test split\n6. Machine Learning - Ensemble Methods\n    * Random Forest Regressor\n    * XGBoostRegressor\n7. Machine Learning - Neural Networks\n8. Final Submission","metadata":{}},{"cell_type":"markdown","source":"# 1. Environment set-up","metadata":{}},{"cell_type":"code","source":"## Importing Libraries\n\n#Set seed\nimport random\nrandom.seed(69)\n\n# Manipulating & Visualizing Data\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(12, 8)})\n\n# Machine Learning set-up\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, KFold\n\n# Cross Validation\nfrom sklearn.model_selection import  KFold\n\n# Ensemble Learning\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\n# Neural Networks\nfrom sklearn.neural_network import MLPRegressor\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# Performance metrics\nimport sklearn.metrics as skm\n\n# System file\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-04T02:31:11.801084Z","iopub.execute_input":"2022-03-04T02:31:11.8019Z","iopub.status.idle":"2022-03-04T02:31:19.048689Z","shell.execute_reply.started":"2022-03-04T02:31:11.801807Z","shell.execute_reply":"2022-03-04T02:31:19.047982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Loading the dataset\ndf = pd.read_csv(\"/kaggle/input/santander-value-prediction-challenge/train.csv\")\n\n#Removing the unnecessary ID column\ndf.drop(\"ID\", axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T02:31:19.04986Z","iopub.execute_input":"2022-03-04T02:31:19.050253Z","iopub.status.idle":"2022-03-04T02:31:24.787706Z","shell.execute_reply.started":"2022-03-04T02:31:19.050223Z","shell.execute_reply":"2022-03-04T02:31:24.786946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Initial Diagnostics","metadata":{}},{"cell_type":"code","source":"## Glimpse of the data\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T02:31:24.788997Z","iopub.execute_input":"2022-03-04T02:31:24.7893Z","iopub.status.idle":"2022-03-04T02:31:24.825202Z","shell.execute_reply.started":"2022-03-04T02:31:24.789259Z","shell.execute_reply":"2022-03-04T02:31:24.82459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Structure of dataframe\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T02:31:24.827828Z","iopub.execute_input":"2022-03-04T02:31:24.82875Z","iopub.status.idle":"2022-03-04T02:31:25.251651Z","shell.execute_reply.started":"2022-03-04T02:31:24.828711Z","shell.execute_reply":"2022-03-04T02:31:25.250799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note:** The histogram below shows a right-skewed distribution indicating that most transactions are on the lower end. It remains consistent given that banking transaction for x person (on average) is relatively modest, with some expectations on the upper tail as shown with the outliers on the boxplot below.  ","metadata":{}},{"cell_type":"code","source":"# Target Variable Analysis\nsns.histplot(data=df, x='target', stat='frequency')\nplt.title(\"Transaction Value Distribution\")\nplt.xlabel(\"Value ($)\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T02:31:25.252754Z","iopub.execute_input":"2022-03-04T02:31:25.252959Z","iopub.status.idle":"2022-03-04T02:31:25.670867Z","shell.execute_reply.started":"2022-03-04T02:31:25.252934Z","shell.execute_reply":"2022-03-04T02:31:25.670066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note:** As noted in the introduction, we have an anonymized dataset with all predictors standardized. Thus, we would have minimal insights in exploring them individually. ","metadata":{}},{"cell_type":"markdown","source":"# 3. Data Cleaning","metadata":{}},{"cell_type":"code","source":"# Missing Values\ndf.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T02:31:25.67205Z","iopub.execute_input":"2022-03-04T02:31:25.672281Z","iopub.status.idle":"2022-03-04T02:31:25.733934Z","shell.execute_reply.started":"2022-03-04T02:31:25.672254Z","shell.execute_reply":"2022-03-04T02:31:25.733335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Takeaway:** With the data already formatted, we would indeed expect no null values.","metadata":{}},{"cell_type":"code","source":"## Outliers\nsns.boxplot(data=df, x='target')\nplt.title(\"Transaction Value Boxplot\")\nplt.xlabel(\"Value ($)\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T02:31:25.734749Z","iopub.execute_input":"2022-03-04T02:31:25.734952Z","iopub.status.idle":"2022-03-04T02:31:25.980066Z","shell.execute_reply.started":"2022-03-04T02:31:25.734927Z","shell.execute_reply":"2022-03-04T02:31:25.979416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Takeaway:** From the box plot above, We have already determined some outliers. However, we will not exclude them but use other techniques instead to ensure robust results from the machine learning predictions.","metadata":{}},{"cell_type":"code","source":"# Duplicate Observations\ndf.duplicated().value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T02:31:25.980946Z","iopub.execute_input":"2022-03-04T02:31:25.981246Z","iopub.status.idle":"2022-03-04T02:31:26.780949Z","shell.execute_reply.started":"2022-03-04T02:31:25.98122Z","shell.execute_reply":"2022-03-04T02:31:26.780128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Takeaway:**  With no duplicate values, we may proceed without making any further changes.","metadata":{}},{"cell_type":"markdown","source":"# 4. Correlation Analysis","metadata":{}},{"cell_type":"markdown","source":"**Takeaway:** Given that we have close to 5000 columns, it would be quite computationally intensive to display the entire correlation matrix and visualize it for user-friendly analysis. ","metadata":{}},{"cell_type":"markdown","source":"# 5. Machine Learning set-up\n\nUnder this section, we will explain the procedure of two main splitting approach to estimate our models' performance. ","metadata":{}},{"cell_type":"code","source":"# Splitting features & target variable\nX = df.drop(['target'], axis=1).values\ny = df['target'].values","metadata":{"execution":{"iopub.status.busy":"2022-03-04T02:31:26.782057Z","iopub.execute_input":"2022-03-04T02:31:26.782281Z","iopub.status.idle":"2022-03-04T02:31:26.917524Z","shell.execute_reply.started":"2022-03-04T02:31:26.782253Z","shell.execute_reply":"2022-03-04T02:31:26.916568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Definition:** As we may imagine, variables come in different measurements like height in inches, weight in pounds, or dollar amount. To ensure a smooth run of our models, we will proceed with feature scaling by representing every value in the column based on a given reference. In our instance, we will use **StandardScaler** by subtracting from the mean and dividing by the standard deviation.","metadata":{}},{"cell_type":"code","source":"# Feature Scaling\n# scaler = preprocessing.StandardScaler()\n# X = scaler.fit_transform(X)\n# X","metadata":{"execution":{"iopub.status.busy":"2022-03-04T02:31:26.918934Z","iopub.execute_input":"2022-03-04T02:31:26.919198Z","iopub.status.idle":"2022-03-04T02:31:26.922335Z","shell.execute_reply.started":"2022-03-04T02:31:26.919155Z","shell.execute_reply":"2022-03-04T02:31:26.921533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Definition:** Often denoted as the most popular by its simplicity, the train-test split is a sampling technique dividing the dataset between training and testing sets. In doing so, the goal would be to have enough (but not too much) in our training set used for the machine learning model to predict the observations in the testing set as accurately as possible. Most would opt for a 70/30 training-testing split, respectively, others 80/20, 60/40, or whichever else works best for the case scenario. Further information [here](https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/). ","metadata":{}},{"cell_type":"code","source":"## Training Testing Split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                    test_size=0.3, \n                                                    random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-04T02:31:26.923753Z","iopub.execute_input":"2022-03-04T02:31:26.924035Z","iopub.status.idle":"2022-03-04T02:31:27.204665Z","shell.execute_reply.started":"2022-03-04T02:31:26.923996Z","shell.execute_reply":"2022-03-04T02:31:27.203756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Machine Learning - Neural Networks","metadata":{}},{"cell_type":"markdown","source":"**Definition:** MLPRegressor","metadata":{}},{"cell_type":"code","source":"mlpr = MLPRegressor(hidden_layer_sizes= (5,5,5,5),\n                           max_iter=500, alpha=0.05, solver='sgd',\n                           learning_rate='adaptive', activation='tanh')\nmlpr.fit(X_train, y_train)\npred = mlpr.predict(X_train)\nnonneg_pred = np.clip(pred, df['target'].min(), df['target'].max())\nrmse = skm.mean_squared_error(y_train, nonneg_pred, squared=False) \nprint('MLPRegressor - RMSE: {:.5f}' .format(rmse))","metadata":{"execution":{"iopub.status.busy":"2022-03-04T02:44:17.492465Z","iopub.execute_input":"2022-03-04T02:44:17.492766Z","iopub.status.idle":"2022-03-04T02:44:28.796906Z","shell.execute_reply.started":"2022-03-04T02:44:17.492737Z","shell.execute_reply":"2022-03-04T02:44:28.795928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Final Submission","metadata":{}},{"cell_type":"code","source":"# Loading the dataset\ntest_df = pd.read_csv(\"/kaggle/input/santander-value-prediction-challenge/test.csv\")\nids = test_df['ID']\ntest_df.drop(['ID'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T02:31:27.540314Z","iopub.status.idle":"2022-03-04T02:31:27.540658Z","shell.execute_reply.started":"2022-03-04T02:31:27.540471Z","shell.execute_reply":"2022-03-04T02:31:27.540496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining our orignal model\nfinal_model = MLPRegressor(hidden_layer_sizes= (5,5,5,5),\n                           max_iter=500, alpha=0.05, solver='sgd',\n                           learning_rate='adaptive', activation='tanh')\nfinal_model.fit(X, y)\npred = final_model.predict(test_df)\nnonneg_pred = np.clip(pred, df['target'].min(), df['target'].max())\nsub = pd.DataFrame({'ID': ids, 'target': nonneg_pred})\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T02:31:27.542075Z","iopub.status.idle":"2022-03-04T02:31:27.542386Z","shell.execute_reply.started":"2022-03-04T02:31:27.542224Z","shell.execute_reply":"2022-03-04T02:31:27.54224Z"},"trusted":true},"execution_count":null,"outputs":[]}]}