{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import VarianceThreshold\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import VarianceThreshold\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's give the dataset a small EDA , before we proceed to building a model and submitting the solution "},{"metadata":{},"cell_type":"markdown","source":"### Usually I tend to take the following steps\n\n1) Finding the  n/p ratio and deciding how to move ahead , \n         n -> no of observations\n         p -> no of predictors\n         \n2) Seeing the distribution of Data / Detecting Outliers \n\n3) Mean / Meadin / Mode of Target Variable\n\n4) Handling Missing Values\n\n5) Correlation"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/santander-value-prediction-challenge/train.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# As we have a large number of columns , i.e 4993 .. Let's go forward and check the entire datashape , so that we get an understanding of what is our n/p ratio"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.set_index(\"ID\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The rows = 4459  (N)\n# The columns = 4993 (P)\n\nThe N/P < 1 .. so from my understanding we can't fit a very flexible(complex) model to the data , as it would go forward and overfit by great margins , as the provided data is very very small.\n\nOr we need to find a way to drop a large number or columns (predictors) before throwing a fancy algorithm at it."},{"metadata":{},"cell_type":"markdown","source":"### Usually I tend to take the following steps\n\n1) Finding the  n/p ratio and deciding how to move ahead , \n         n -> no of observations\n         p -> no of predictors\n         \n      1.1) Removing rows with all 0's\n      1.2) Removing duplicate columns\n      1.3) Removing columns with constant value\n      1.4) Removing correlated variables\n         \n         \n2) Seeing the distribution of Data / Detecting Outliers \n\n\n3) Mean / Meadin / Mode of Target Variable\n\n4) Handling Missing Values\n\n5) Correlation"},{"metadata":{},"cell_type":"markdown","source":"### Task 1.1 ) Removing columns with less than no values as non - zero"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[(df.T != 0).any()]\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no such columns , with all zeroes"},{"metadata":{},"cell_type":"markdown","source":"### Task 1.2 ) Removing duplicate columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.loc[:,~df.columns.duplicated()]\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no such columns , with all duplicate values"},{"metadata":{},"cell_type":"markdown","source":"### Task 1.3 ) Removing constant value columns , or its better to call them features with 0 variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features, test_features, train_labels, test_labels=train_test_split(df.drop(labels=['target'], axis=1),df['target'],test_size=0.2,\n    random_state=41)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"constant_filter = VarianceThreshold(threshold=0)\nconstant_filter.fit(train_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_features.columns[constant_filter.get_support()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = constant_filter.transform(train_features)\ntest_features = constant_filter.transform(test_features)\n\ntrain_features.shape, test_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# THE TEST DATA WITH 49K VALUES ALSO HAS TO GO THROUGH THE SAME PROCESS , SO I WILL DEAL WITH IT HERE AND NOW"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/santander-value-prediction-challenge/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.shape\ntest_df = constant_filter.transform(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.shape()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We removed many columns , precisely (256) columns were removed . But still the no or predictors are more than the number of rows or the number of observations "},{"metadata":{},"cell_type":"markdown","source":"### Task 1.4 ) Removing variable which have correlation to other predictor variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(columns = \"target\")\ny = df['target']\nnum_colums = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_columns = list(X.select_dtypes(include=num_colums).columns)\nX = X[numerical_columns]\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlated_features = set()\ncorrelation_matrix = X.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(correlation_matrix .columns)):\n    for j in range(i):\n        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n            colname = correlation_matrix.columns[i]\n            correlated_features.add(colname)\nX.drop(labels=correlated_features, axis=1, inplace=True)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TASK 2 - Studying Data Distribution & Outliers - \n\nThanks to an amazing kernal ->https://www.kaggle.com/sudalairajkumar/simple-exploration-baseline-santander-value ,  navigating this section was really easy"},{"metadata":{},"cell_type":"markdown","source":"Task 2.1) Detecting Outliers\n\nTask 2.2) Studying Distribution Of the Data\n\nTask 2.3) Fixing the target data\n\nTask 2.4) Studying Mean Median Mode of new target"},{"metadata":{},"cell_type":"markdown","source":"\n### Task 2.1) Detecting Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.lineplot(data=df ,x=range(df['target'].shape[0]), y=np.sort(df[\"target\"].values))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no visible outliers"},{"metadata":{},"cell_type":"markdown","source":"### Task 2.2) Studying Distribution of Target Variable "},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.distplot(df['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df['target']).skew()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.distplot(np.log1p(df['target']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(np.log1p(df['target']).skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The skew went down from 2 to -0.4"},{"metadata":{},"cell_type":"markdown","source":"### More exploration of the target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.log1p(df['target']).value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['target'] = np.log1p(df['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y =  (df['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TASK 5 -> reducing variables using correlation"},{"metadata":{},"cell_type":"markdown","source":"# There will be various feature selection processes , I would experiment with in the task-5 Section , I'll also train a default Gradient Boosting Regression Model for checking the quality of each method's progess . Creating good quality function is key here ."},{"metadata":{},"cell_type":"markdown","source":" * 1) Univariate Selection\n * 2) Feature Importance\n * 3) Correlation Heatmap\n * 4) PCA"},{"metadata":{},"cell_type":"markdown","source":"# Univariate Selection\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#apply SelectKBest class to extract top 10 best features\n#bestfeatures = SelectKBest(score_func=chi2, k=10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit = bestfeatures.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n# Make an instance of the Model\npca = PCA(.75)\npca.fit(X)\ndf_pca_dropped = pca.transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pca_dropped.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pca_dropped = pd.DataFrame(df_pca_dropped)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = y\nX = df_pca_dropped\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = GradientBoostingRegressor()\nmodel.fit(X_train, y_train)\nmodel.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nregr = RandomForestRegressor(max_depth=2, random_state=0)\nregr.fit(X_train, y_train)\ny_pred = regr.predict(X_test)\nprint(r2_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dropping columns using PCA gave us about r sqaure of 0.20 (GBM) AND 0.18(Random Forest) looks like PCA is dropping out on a lot of information:("},{"metadata":{},"cell_type":"markdown","source":"# There are some algorithms like lasso , decision trees which themselves choose b/w the large amount of variables and use the most suitable ones , let's go forward and train them directly on the initial full data"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}