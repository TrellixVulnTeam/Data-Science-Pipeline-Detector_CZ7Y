{"cells":[{"metadata":{"_uuid":"7dac78b57d3a00174c55da99d42d7524200e7def"},"cell_type":"markdown","source":"Since the dataset of this competition is sparse data, so I regarded each row as a document and each column as a word for applying Topic Model.\nUnfortunately, since this idea was not applied since the leaked case was discovered,  this idea was not included in my final submission.\nIn this Kernel, I introduce this Topic Modeling idea which is effective to gain the score.\nFirst, applying LightGBM model to only the plane features to check original score of RMSLE, and then applying the topic model features to verify whether the score becomes better or not.\n  \n今回のデータはスパースなデータだったため、各行を文書とみなし、各列を単語とみなしたトピックモデル が適用できるのではと考え、適用してみた。残念ながらリークの件が発覚して以降、このアイディアを適用していなかったため、最終サブミットにはこのアイディアを盛り込んでいなかったが、このKernelでは、素のデータにトピックモデル の特徴量だけ追加した時に精度が上がるかどうかを検証する。"},{"metadata":{"_uuid":"8f62525813aae689323b50c6191a39dba251d746"},"cell_type":"markdown","source":"[Idea]  \nConverting the values of dataset as ratio with max values of each column,(i.e applying MinMaxScaler for each column). In that case, with multiplying 100 and cast with `int`, the values can be interpretating as a percent compared with max values. Then I regards the values as a word count.(Bag of words) So I can apply Topic Model for this dataset.\n\n[アイディア]\nデータセットの各値をその列の最大値で割ることによって、最大値から比較した比率にすることができる。これに100をかけてintでキャストすることによって整数にする。この値を各列を単語とみなしてその出現回数とみなすことができる。各行は文書に見立てて各列に対応する単語がその文書に何回出現したかと考えBag of wordsであるとして取り扱える。このようなデータセットにトピックモデル を適用してみた。"},{"metadata":{"_uuid":"9991f42a5377a2265655f02c686ab550bef85094"},"cell_type":"markdown","source":"[RESULT]  \n* RMSLE with plane features:             **1.4181**\n* RMSLE with topic model features:  **1.3967**  \nThe score becomes better!!!\n\n[Number of Topic]  \nBasically, the more number of topics, the better score. However, there is trade off between training speed and good score."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import math, sys, functools, os, codecs, gc, time\nimport importlib\nfrom pathlib import Path\nimport numpy as np\nimport numpy.random as rd\nimport pandas as pd\nfrom datetime import  datetime as dt\nfrom collections import Counter\nimport traceback\n\nimport lightgbm as lgb\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.decomposition import LatentDirichletAllocation\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1994705945bf98be3e14315bddc36fc9497475e"},"cell_type":"code","source":"\ndef current_time():\n    return dt.strftime(dt.now(),'%Y-%m-%d %H:%M:%S')\n\n\ndef pred(training, testing, y, lgbm_params, submit_file_name):\n    # Init predictions\n    oof_preds = np.zeros(training.shape[0])\n    sub_preds = np.zeros(testing.shape[0])\n\n    # Run KFold\n    clf_list = []\n    auc_list = []\n    feature_importance_df = pd.DataFrame()\n\n    folds = KFold(n_splits=FOLD_NUM, shuffle=True, random_state=SEED)\n    fold_iter = folds.split(training)\n\n    for n_fold, (trn_idx, val_idx) in enumerate(fold_iter):\n        print(f\"============ start {n_fold+1} fold training ============\")\n\n        X_train = training.iloc[trn_idx]\n        X_valid = training.iloc[val_idx]\n        y_train = np.log1p(y.iloc[trn_idx])\n        y_valid = np.log1p(y.iloc[val_idx])\n        \n        # Lgbm Dataset\n        lgtrain = lgb.Dataset(X_train, y_train,\n                          feature_name=training.columns.tolist())\n        lgvalid = lgb.Dataset(X_valid, y_valid,\n                          feature_name=training.columns.tolist())\n        # Start fitting\n        gbdt_reg = lgb.train(\n            lgbm_params,\n            lgtrain,\n            num_boost_round=20000,\n            early_stopping_rounds=100,\n            verbose_eval=100,\n            valid_sets=[lgtrain, lgvalid],\n            valid_names=['train', 'valid']\n        )\n\n        # evaluation\n        preds_train = gbdt_reg.predict(X_train)\n        rmse_train = np.sqrt(mean_squared_error(y_train, preds_train))\n        print(\"RMSE train cv{}:\".format(n_fold+1), rmse_train)\n\n        preds_valid = gbdt_reg.predict(X_valid)\n        rmse_valid = np.sqrt(mean_squared_error(y_valid, preds_valid))\n        print(\"RMSE valid cv{}:\".format(n_fold+1), rmse_valid)\n        oof_preds[val_idx] = np.expm1(preds_valid)\n\n        preds_test = gbdt_reg.predict(testing)\n        sub_preds += np.expm1(preds_test) / FOLD_NUM\n\n        del lgtrain\n        del lgvalid\n        gc.collect()\n    print(\"=+\"*30)\n    rmse_valid = np.sqrt(mean_squared_error(np.log1p(y), np.log1p(oof_preds)))\n    print(\"RMSE valid FULL cv{}:\".format(n_fold+1), rmse_valid)\n    \n    submit = pd.DataFrame(preds_test, columns=[\"target\"], index=testdex)\n    submit.to_csv(submit_file_name, index=True, header=True)\n    \n    return rmse_valid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e9486b608e6807bc6aefa7486dd5545c6f5931f"},"cell_type":"code","source":"# Parameter\nDATA_PATH = Path('../input/')\nSEED = 71\nTOPIC_COMP = 20 # number of topics\nFOLD_NUM = 5\n#===================\n# Data Loading\nprint(\"data loading\")\nnrows = None #100 #None\n# parse_dates=[\"activation_date\"],\ntraining = pd.read_csv(str(DATA_PATH/'train.csv'), index_col=\"ID\",  nrows=nrows)\ntraindex = training.index\ntesting = pd.read_csv(str(DATA_PATH/'test.csv'), index_col=\"ID\",  nrows=nrows)\ntestdex = testing.index\nprint(\"loading finished.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f982287a36650deeb75903df4df6c13f41ed48b"},"cell_type":"code","source":"# target\ny = training.target.copy()\ndel training['target']\ny_log = np.log1p(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a6c5a00fa1a2b73d4981175d6ecfbba5dddc134"},"cell_type":"code","source":"# removing duplicated columns, highly correlated columns\nduplicate_cols = [\"d60ddde1b\", \"912836770\", \"acc5b709d\", \"f8d75792f\", \"f333a5f60\"]\nhigh_corr_cols = [\n\"04e06920e\", \"e90ed19da\", \"7d72d6787\", \"4c256f2f9\", \"871617f50\",\n\"4a3248e89\", \"15bba6b9e\", \"3c29aec1e\", \"4647da55a\", \"083640132\",\n\"c4ed18259\", \"8966b4eee\", \"45713ba5f\", \"9a3f53be7\", \"1d0affea2\",\n\"2306bf286\", \"62d2a813b\", \"acd155589\", \"5d26f4d92\", \"28b21c1d2\",\n\"6dcac05e7\", \"bfde2aa61\", \"34d3974de\", \"598ae7ea9\", \"e851264a5\",\n\"5619c1297\", \"0c5eaf8a7\", \"bacadce94\", \"22b3e64c8\", \"224a28832\",\n\"07cfb1624\", \"8c1e20670\", \"49131c9e6\", \"1de1fda2c\", \"a04f3e320\",\n\"dcc181073\", \"2e648ce4b\", \"3c556d78f\", \"869a169f9\", \"99258443a\"]\n\nprint(\"training.shape\", training.shape)\nprint(\"testing.shape\", testing.shape)\n\ncols = [c for c in training.columns if c not in duplicate_cols + high_corr_cols]\ntraining = training[cols]\ntesting = testing[cols]\n\nprint(\"training.shape\", training.shape)\nprint(\"testing.shape\", testing.shape)\n\n############################################################################\nprint(\"data preprocessing\")\n\n# remove constant cols\nprint(\"training.shape\", training.shape)\nprint(\"testing.shape\", testing.shape)\nnuniq = training.nunique()\nconstant_col = nuniq.index[nuniq==1].values\ntraining.drop(constant_col, axis=1, inplace=True)\ntesting.drop(constant_col, axis=1, inplace=True)\nprint(\"removed constant columns: {}\".format(len(constant_col)))\n\nprint(\"training.shape\", training.shape)\nprint(\"testing.shape\", testing.shape)\ndf_cols = training.columns.tolist()\nprint(\"data preprocessing finished\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a501e243719c7b22d6892dc919377cb6a23249d2"},"cell_type":"markdown","source":"# Predict with LGBM"},{"metadata":{"trusted":true,"_uuid":"3df3d7d85ba3118c9da949e7c563430db78a1711"},"cell_type":"code","source":"lgbm_params = { \n        'objective': 'regression',\n        'num_leaves': 60,\n        'subsample': 0.61,\n        'colsample_bytree': 0.64,\n        'min_split_gain': 0.00259,\n        'reg_alpha': 0.00514,\n        'reg_lambda': 57.148,\n        'min_child_weight': 0.7117,\n        'verbose': -1,\n        'seed': 3,\n        'boosting_type': 'gbdt',\n        'max_depth': -1,\n        'learning_rate': 0.05,\n        'metric': 'rmse',\n    }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6780db03b7192fddbdd0acb947f93d6e9ab7d1c4"},"cell_type":"markdown","source":"# No Topic Modela"},{"metadata":{"trusted":true,"_uuid":"e0985264dc41b78651836b50a369e88e0b8dd9d0"},"cell_type":"code","source":"valid_full_rmse = pred(training, testing, y, lgbm_params, \"submit_plane_pred.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"e00e0bb5aa96aef099494232296184278c1e8c2b"},"cell_type":"code","source":"print(f\"RMSLE with only plane features: {valid_full_rmse}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"148abaeac4ccb2f8eb4b57ff696a33f31add5beb"},"cell_type":"markdown","source":"RMSE is about **1.4181** with only plane features.   \nトピックモデル を適用しなかった時のValidation setのRMSLEは約 **1.4181**\n"},{"metadata":{"_uuid":"0c1167e6880ced3ce3edc016f893dbad7bed927a"},"cell_type":"markdown","source":"# WithTopic Model\n### making topic model features"},{"metadata":{"trusted":true,"_uuid":"2833b835476823c232fa0cb8e3c26516e549f6d8"},"cell_type":"code","source":"# Data concatenation\nprint(\"Data concatenation.\")\ndf = pd.concat([training, testing], axis=0)\n\n# Converting values to ratio of max value of each cols\nprint(\"convert df to percentage value\")\ndf_max = df.max(axis=0)\ndf_ratio = pd.DataFrame(np.divide(df.values, df_max[np.newaxis, :]))\ndf_ratio.index = df.index\ndf_ratio.columns = df.columns\n\n# topic model features\nprint(\"start topic modeling\")\n# assuming occurence count of a word(columns) for each column valuees. (i.e. \"5%\"\" means 5 times occuring on a document)\n# 各項目maxに対する比率を出現回数とみなしてLDAの対象データを算出(ex: maxに対し5%の値は5回出現したとみなす)\ndf_ratio_100 = (df_ratio.fillna(0)*100).astype(int)\n\n# Run LDA\nprint(current_time(), 'Run LDA')\nlda = LatentDirichletAllocation(n_components=TOPIC_COMP, max_iter=10, learning_method='online',\n                                learning_offset=50.,random_state=SEED).fit(df_ratio_100)\ntopic_result = lda.transform(df_ratio_100)\n\ndf_topic_result = pd.DataFrame(topic_result, columns=[\"{0}_{1:02d}\".format('tp', i) for i in range(TOPIC_COMP)])\ndf_topic_result.index = df_ratio_100.index\nprint(current_time(), \"finished topic modeling\")\n\ndf = df.join(df_topic_result, on=\"ID\", how='left')\n\n# split train and test dataset\ntraining = df.loc[traindex]\ntesting = df.loc[testdex]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d6cd3502e1b2d1d1c773f04f3117cb874511f99a"},"cell_type":"code","source":"valid_full_rmse_with_tp = pred(training, testing, y, lgbm_params, \"submit_topic_pred.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36744658b06f30f81e844a204d33321762b6605d"},"cell_type":"code","source":"print(f\"RMSLE with topic features: {valid_full_rmse_with_tp}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"019e5e169a78e2519383b99db851480e415ccad8"},"cell_type":"code","source":"print(f\"ratio of RMSLE with topic feature vs plane{valid_full_rmse_with_tp/valid_full_rmse}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f6cfd18c7aa1b643239516de6818d376d146bfc"},"cell_type":"markdown","source":"RMSE is about **1.3967** with Topic Model features. The score is improving compared with only plane features!  \nトピックモデルを適用した時のValidation setのRMSLEは約**1.3967**で、素の特徴量のみの時よりスコアが改善している！"},{"metadata":{"trusted":true,"_uuid":"1ce6accf49753c7eb40441d7c4271b32e8d01e4e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30f45137a16e0471438f0c906672acb9454bc1d9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc617c3612c80480e1f2dce4c181b83fc57005db"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f66128ab1026814a6873dc2048da1ee79ca88a5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}