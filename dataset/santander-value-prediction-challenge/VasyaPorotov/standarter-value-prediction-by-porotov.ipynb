{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"### Import required libraries\n\nimport numpy as np\nimport pandas as pd\nfrom math import sqrt\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.base import clone\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\nfrom IPython.display import display # Allows the use of display() for DataFrames\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load Train and Test Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Использованный код https://www.kaggle.com/samratp/beginner-guide-to-stacking\n# Read train and test files\ntrain_df = pd.read_csv('/kaggle/input/santander-value-prediction-challenge/train.csv')\ntest_df = pd.read_csv('/kaggle/input/santander-value-prediction-challenge/test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df = train_df[:10000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Check if there are any NULL values in Train Data\nprint(\"Total Train Features with NaN Values = \" + str(train_df.columns[train_df.isnull().sum() != 0].size))\nif (train_df.columns[train_df.isnull().sum() != 0].size):\n    print(\"Features with NaN => {}\".format(list(train_df.columns[train_df.isnull().sum() != 0])))\n    train_df[train_df.columns[train_df.isnull().sum() != 0]].isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Check if there are any NULL values in Test Data\nprint(\"Total Test Features with NaN Values = \" + str(test_df.columns[test_df.isnull().sum() != 0].size))\nif (test_df.columns[test_df.isnull().sum() != 0].size):\n    print(\"Features with NaN => {}\".format(list(test_df.columns[test_df.isnull().sum() != 0])))\n    test_df[test_df.columns[test_df.isnull().sum() != 0]].isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stacking Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def transformer(y, func=None):\n    \"\"\"Transforms target variable and prediction\"\"\"\n    if func is None:\n        return y\n    else:\n        return func(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(predictions, targets):\n    return np.sqrt(((predictions - targets) ** 2).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stacking_regression(models, meta_model, X_train, y_train, X_test,\n             transform_target=None, transform_pred=None,\n             metric=None, n_folds=5, average_fold=True,\n             shuffle=False, random_state=42, verbose=1):\n\n    # Specify default metric for cross-validation\n    if metric is None:\n        metric = rmse\n\n    # Print metric\n    if verbose > 0:\n        print('metric: [%s]\\n' % metric.__name__)\n\n    # Split indices to get folds\n    kf = KFold(n_splits = n_folds, shuffle = shuffle, random_state = random_state)\n\n    if X_train.__class__.__name__ == \"DataFrame\":\n        X_train = X_train.values\n        X_test = X_test.values\n\n    # Create empty numpy arrays for stacking features\n    S_train = np.zeros((X_train.shape[0], len(models)))\n    S_test = np.zeros((X_test.shape[0], len(models)))\n\n    # Loop across models\n    for model_counter, model in enumerate(models):\n        if verbose > 0:\n            print('model %d: [%s]' % (model_counter, model.__class__.__name__))\n\n        # Create empty numpy array, which will contain temporary predictions for test set made in each fold\n        S_test_temp = np.zeros((X_test.shape[0], n_folds))\n        # Loop across folds\n        for fold_counter, (tr_index, te_index) in enumerate(kf.split(X_train, y_train)):\n            X_tr = X_train[tr_index]\n            y_tr = y_train[tr_index]\n            X_te = X_train[te_index]\n            y_te = y_train[te_index]\n            # Clone the model because fit will mutate the model.\n            instance = clone(model)\n            # Fit 1-st level model\n            instance.fit(X_tr, transformer(y_tr, func = transform_target))\n            # Predict out-of-fold part of train set\n            S_train[te_index, model_counter] = transformer(instance.predict(X_te), func = transform_pred)\n            # Predict full test set\n            S_test_temp[:, fold_counter] = transformer(instance.predict(X_test), func = transform_pred)\n\n            # Delete temperatory model\n            del instance\n\n            if verbose > 1:\n                print('    fold %d: [%.8f]' % (fold_counter, metric(y_te, S_train[te_index, model_counter])))\n\n        # Compute mean or mode of predictions for test set\n        if average_fold:\n            S_test[:, model_counter] = np.mean(S_test_temp, axis = 1)\n        else:\n            model.fit(X_train, transformer(y_train, func = transform_target))\n            S_test[:, model_counter] = transformer(model.predict(X_test), func = transform_pred)\n\n        if verbose > 0:\n            print('    ----')\n            print('    MEAN RMSE:   [%.8f]\\n' % np.sqrt((metric(y_train, S_train[:, model_counter]))))\n\n    # Fit our second layer meta model\n    meta_model.fit(S_train, transformer(y_train, func = transform_target))\n    # Make our final prediction\n    stacking_prediction = transformer(meta_model.predict(S_test), func = transform_pred)\n\n    return stacking_prediction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stacking Base Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df= train_df[:3000]\nX_train = train_df.drop([\"ID\", \"target\"], axis=1)\ny_train = np.log1p(train_df[\"target\"].values)\n\nX_test = test_df.drop([\"ID\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elastic_net = ElasticNet(alpha = 0.02, l1_ratio = 0.15, random_state = 42)\nelastic_net.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_tree = RandomForestRegressor(n_estimators = 1000,\n                                max_features = \"sqrt\",\n                                max_depth = 15,\n                                min_samples_split = 20,\n                                min_samples_leaf = 5,\n                                bootstrap = True,\n                                random_state = 42)\nrf_tree.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_tree = GradientBoostingRegressor(max_depth = 5, \n                                    learning_rate = 0.01, \n                                    n_estimators = 1000,\n                                    min_samples_split = 15,\n                                    max_features = \"sqrt\",\n                                    min_samples_leaf = 3,\n                                    random_state=42)\ngb_tree.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Очень долго\nX_train=X_train[:1000]\ny_train=y_train[:1000]\nxgb_tree = XGBRegressor(max_depth = 10, \n                        learning_rate = 0.01, \n                        n_estimators = 1000,\n                        min_child_weight = 5,\n                        reg_alpha = 0.03, \n                        random_state=42)\nxgb_tree.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_tree = lgb.LGBMRegressor(learning_rate = 0.01, \n                             num_leaves = 40,\n                             n_estimators = 1000,\n                             bagging_fraction = 0.6,\n                             feature_fraction = 0.5,\n                             random_state=42)\nlgb_tree.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stacked Modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [rf_tree, gb_tree, xgb_tree, lgb_tree]\n#models =  xgb_tree\nmeta_model = elastic_net","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny_predicted = stacking_regression(models, meta_model, X_train, y_train, X_test,\n             metric=None, n_folds=5, average_fold=True,\n             shuffle=True, random_state=42, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = np.expm1(y_predicted)\nsub = pd.read_csv('/kaggle/input/santander-value-prediction-challenge/sample_submission.csv')\nsub[\"target\"] = y_pred\n\nprint(sub.head())\nsub.to_csv('sub_stacking.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#1.53468","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}