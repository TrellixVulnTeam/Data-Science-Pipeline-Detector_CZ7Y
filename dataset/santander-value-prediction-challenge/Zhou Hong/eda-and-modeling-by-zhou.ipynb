{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"from IPython.core.display import display, HTML\nfrom IPython.display import Image\ndisplay(HTML(\"<style>.container { width:80% !important; }</style>\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Name: Zhou Hong\nStudent ID: 19025779"},{"metadata":{},"cell_type":"markdown","source":"# Introduction\nThis kernel is build for the competetion  “Santander Value Prediction Challenge” from [https://www.kaggle.com/c/santander-value-prediction-challenge](http://). In this competition, we have a dataset with customers' transation in their bank accounts, and our aim is to predict what the customers need in order to provide personalized service."},{"metadata":{},"cell_type":"markdown","source":"### Importing library"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output./","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\nimport lightgbm as lgb\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport time\nimport seaborn as sns\nfrom pylab import rcParams\n%matplotlib inline\n#sklearn library","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading data "},{"metadata":{"trusted":false},"cell_type":"code","source":"train = pd.read_csv(\"../input/santander-value-prediction-challenge/train.csv\")\ntest = pd.read_csv(\"../input/santander-value-prediction-challenge/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print (\"In train dataset, the number of Records is {}\".format(train.shape[0])+\", and number of Features is {}\".format(train.shape[1]-2)) #not counting ID and target\nprint (\"In test dataset, the number of Records is {}\".format(test.shape[0])+\", and number of Features is {}\".format(test.shape[1]-1))#not counting ID ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"test dataset has the same number of features with train dataset, but contains much more records."},{"metadata":{"trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.iloc[:,2:].info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1844 columns of train is float type and 3147 columns is int type."},{"metadata":{"trusted":false},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test.iloc[:,1:].info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All data in test dataset are float type, different to the train dataset."},{"metadata":{"trusted":false},"cell_type":"code","source":"train.iloc[:,2:]=train.iloc[:,2:].astype(float)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now, our train dataset is as the same type as test without changing any value."},{"metadata":{"trusted":false},"cell_type":"code","source":"train.iloc[:,2:].info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"## Target"},{"metadata":{"trusted":false},"cell_type":"code","source":"train.target.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First look at our target. It has 1413 unique values, I think it is numerical data rather than categorical data."},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.hist(train.target, bins=50)\nplt.title('target Histogram ')\nplt.xlabel('Target')\nplt.ylabel('Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like a skewed distribution, just see the log transform distribution."},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.hist(np.log1p(train.target), bins=50) # equal to \"np.log(x+1)\"   add 1 to avoid log(0)\nplt.title('log target Histogram ')\nplt.xlabel('Target')\nplt.ylabel('Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That looks like a normal distribution. We may need to use log transformation when using target and in predition."},{"metadata":{"trusted":false},"cell_type":"code","source":"train.target.value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"np.log(train.target.median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features"},{"metadata":{},"cell_type":"markdown","source":"### missing value"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(train.isnull().values.any())\nprint(test.isnull().values.any())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great, we don't have missing value to deal with this time."},{"metadata":{},"cell_type":"markdown","source":"###  all zero columns"},{"metadata":{},"cell_type":"markdown","source":"first look at the columns that only have zero value"},{"metadata":{"trusted":false},"cell_type":"code","source":"all_zero_columns=[i for i in train.columns if train[i].nunique()==1]\nprint (\"There are {}\".format(len(all_zero_columns))+\" all zero columns in train dataset\")\nprint(\"There is {}\".format(len([i for i in test.columns if test[i].nunique()==1]))+\" all zero column in test dataset\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is not a normal case, the columns that provide no information,has values in the tset dataset. The reason for this may be  the test dataset has ten times as many records as train dataset has. However, no matter what the reason is, we just need to drop them here, it is useless for trainning."},{"metadata":{},"cell_type":"markdown","source":"### duplicate columns"},{"metadata":{},"cell_type":"markdown","source":"Let's check whether there are duplicate columns"},{"metadata":{"trusted":false},"cell_type":"code","source":"def find_duplicate_columns(df):\n    duplicate_columns=[]\n    for i in range(len(df.columns)):\n        this=df.iloc[:,i]\n        for j in range(i+1,len(df.columns)):\n            compare=df.iloc[:,j]\n            if this.equals(compare):\n                duplicate_columns.append(train.columns[j])\n    return duplicate_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#a=find_duplicate_columns(train)\n#a=['d60ddde1b', 'acc5b709d', '912836770', 'f8d75792f', 'f333a5f60'] it did take an hour.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"a=getDuplicateColumns(train)\n\na=['d60ddde1b', 'acc5b709d', '912836770', 'f8d75792f', 'f333a5f60'] \n\nit did take near an hour to run,so I just simply copy the array every next time."},{"metadata":{},"cell_type":"markdown","source":"## Features analysis"},{"metadata":{},"cell_type":"markdown","source":"### Features importance"},{"metadata":{"trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#use lgbm's parameters I tuned in other kernel ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"clf_lgb=lgb.LGBMRegressor(bagging_fraction=0.5, boosting_type='gbdt', class_weight=None,\n              colsample_bytree=1.0, feature_fraction=0.5,\n              importance_type='split', learning_rate=0.01, max_depth=-1,\n              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n              n_estimators=500, n_jobs=-1, num_leaves=130,\n              objective='regression', random_state=42, reg_alpha=0.0,\n              reg_lambda=0.0, silent=True, subsample=1.0,\n              subsample_for_bin=200000, subsample_freq=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"clf_lgb.fit(np.log1p(train.iloc[:,2:]),np.log1p(train.iloc[:,1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14,10))\nlgb.plot_importance(clf_lgb, max_num_features=50, height=0.8,color=\"tomato\",ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As demonstrated above, column '58e2e026' provides most information and most of the features are useless for predition(In fact, I can also get 1.5 for the final score with 200 most important features.)"},{"metadata":{"trusted":false},"cell_type":"code","source":"#store the features importance\nfeat_importances = pd.Series(clf_lgb.booster_.feature_importance(),clf_lgb.booster_.feature_name())\ntop30=[i for i in feat_importances.nlargest(30).index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"top30.insert(0,'target')\ntop30.insert(0,'ID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# build a dataset for rich features\nrichdf=train[[i for i in top30]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### feature distribution"},{"metadata":{"trusted":false},"cell_type":"code","source":"top30_to_plot =richdf.iloc[:,2:10] .melt(var_name='columns')\ng = sns.FacetGrid(top30_to_plot, col='columns')\ng = (g.map(sns.distplot, 'value'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"richdf.iloc[:,1:]=np.log1p(richdf.iloc[:,1:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the log transform of features"},{"metadata":{"trusted":false},"cell_type":"code","source":"top30_to_plot =richdf.iloc[:,2:10] .melt(var_name='columns')\ng = sns.FacetGrid(top30_to_plot, col='columns')\ng = (g.map(sns.distplot, 'value'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"top30_to_plot1 =richdf.iloc[:,10:18] .melt(var_name='columns')\ng = sns.FacetGrid(top30_to_plot, col='columns')\ng = (g.map(sns.distplot, 'value'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that, the features'value seem to follow skewed distribution, but the log transform of it not follow normal distribution. We can see that the number of values around 0 is  comparatively large. Let drop the 0."},{"metadata":{"trusted":false},"cell_type":"code","source":"top30_to_plot['value'] = top30_to_plot['value'].replace(0.0,np.nan)\ng = sns.FacetGrid(top30_to_plot.dropna(), col='columns')\ng = (g.map(sns.distplot, 'value'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"top30_to_plot1['value'] = top30_to_plot['value'].replace(0.0,np.nan)\ng = sns.FacetGrid(top30_to_plot1.dropna(), col='columns')\ng = (g.map(sns.distplot, 'value'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great, perfet normal distribution! We got to use log transform in modeling."},{"metadata":{},"cell_type":"markdown","source":"### correlation"},{"metadata":{},"cell_type":"markdown","source":"Check the correlation between features and target."},{"metadata":{"trusted":false},"cell_type":"code","source":"corr=richdf.iloc[:,1:].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#forked from https://www.kaggle.com/samratp/beginner-guide-to-eda-and-modeling\n#I have other heatmap but this one is so beautiful!\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nf, ax = plt.subplots(figsize=(16,16))\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.5, center=0,square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.title(\"Correlation HeatMap\", fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see some columns are highly relative to all other columns and some are not that relative."},{"metadata":{"trusted":false},"cell_type":"code","source":"del richdf,corr,top30_to_plot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features engineering"},{"metadata":{},"cell_type":"markdown","source":"As shown above,there no missing value in this dataset,some columns are constant,and some are duplicate.These need to be dropped at first.\nSecondly,the target and features follow skewed distribution, we need to use log transform to get a normal distribution data."},{"metadata":{"trusted":false},"cell_type":"code","source":"# constant columns\nall_zero_columns=[i for i in train.columns if train[i].nunique()==1]\ntrain=train[[i for i in train.columns if i not in all_zero_columns]]\ntest=test[[i for i in test.columns if i not in all_zero_columns]]\n# duplicate columns\nduplicte_columns=['d60ddde1b', 'acc5b709d', '912836770', 'f8d75792f', 'f333a5f60']\ntrain=train[[i for i in train.columns if i not in duplicte_columns]]\ntest=test[[i for i in test.columns if i not in duplicte_columns]]\n# log transform\nX = np.log1p(train.drop([\"ID\", \"target\"], axis=1))\ny = np.log1p(train[\"target\"].values)\ntest = np.log1p(test.drop([\"ID\"], axis=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling"},{"metadata":{},"cell_type":"markdown","source":"I use many strategies to build the model, first is Random forest regression.\n\nhttps://www.kaggle.com/daphnetree/ramdom-forest?scriptVersionId=20810061\n\nThis is the final model and score："},{"metadata":{"trusted":false},"cell_type":"code","source":"rf=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n                      max_features='auto', max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=0.1, min_samples_split=0.3,\n                      min_weight_fraction_leaf=0.0, n_estimators=300,\n                      n_jobs=None, oob_score=True, random_state=None, verbose=0,\n                      warm_start=False)\nImage(\"../input/imageforscore/rf.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Models_score={}\nModels_score['Random Forest']=1.69","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then I spent a long time on tuning the parameters for lightgbm and xgboost algorithm.\nThe process of tuning can be view in my kaggle.\nI use gridsearch to find the best value for each parameters, sometimes I run it with 2 values at the same time.\neg.GridSearchCV(clf,parameter,cv = 5,scoring = 'neg_mean_squared_error',verbose=5)\n\nhttps://www.kaggle.com/daphnetree/xbg-model#Final-model-and-prediction \n\nhttps://www.kaggle.com/daphnetree/lgb-model (tuning process)\n\nhttps://www.kaggle.com/zhouhong0/lgbbest (tuning lambda and alpha and get the best result)\n\nMy best models is as follow:"},{"metadata":{"trusted":false},"cell_type":"code","source":"lgbbest=lgb.LGBMRegressor(bagging_fraction=0.5, boosting_type='gbdt', class_weight=None,\n              colsample_bytree=1.0, feature_fraction=0.5,\n              importance_type='split', learning_rate=0.01, max_depth=-1,\n              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n              n_estimators=500, n_jobs=-1, num_leaves=130,\n              objective='regression', random_state=42, reg_alpha=0.0,\n              reg_lambda=1, silent=True, subsample=1.0,\n              subsample_for_bin=200000, subsample_freq=0)\nxgbbest=xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.9, gamma=0.3,\n             importance_type='gain', learning_rate=0.02, max_delta_step=0,\n             max_depth=5, min_child_weight=5, missing=0, n_estimators=500,\n             n_job=4, n_jobs=1, nthread=None, objective='reg:squarederror',\n             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n             seed=0, silent=None, subsample=0.7, verbosity=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"../input/imageforscore/lgb.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"../input/imageforscore/xgb.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Models_score['Lightgbm']=1.40192\nModels_score['xgboost']=1.43379","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I also tried two ensamble methods , stacking and soft voting, to boost the final result.\n\nFirst I tried stacking with random forest,lightgbm and xgboost. But the result is merely better than random forest's result, far away from lgb and xgbosst.\n\nIn soft voting, the result is much better than stacking, but still worse than lgb's result. "},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"../input/imageforscore/stacking_lgb_xgb_rf.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"../input/imageforscore/softvoting_lgb_xgb_rf.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Models_score['softvoting_lgb_xgb_rf']=1.62068\nModels_score['stacking_lgb_xgb_rf']=1.45082","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I thought it was the random forest model brought the score down. So I removed it in the ensamble modeling.\n\nAnd the softvoting model was significant boosted ,reaching 1.39.\n\nHowever,stacking still has a bad performance.\nhttps://www.kaggle.com/daphnetree/stacking?scriptVersionId=20842799"},{"metadata":{"trusted":true},"cell_type":"code","source":"Models_score['softvoting_lgb_xgb']=1.39769\nImage(\"../input/imageforscore/softvoting_lgb_xgb.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Models_score['stacking_lgb_xgb']=1.5950\nImage(\"../input/imageforscore/stacking_lgb_xgb.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modeldf=pd.DataFrame(list(Models_score.items()), columns=['Model', 'RMSE'])\nmodeldf=modeldf.sort_values('RMSE',ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rcParams['figure.figsize'] = 25, 10\nrcParams['font.size'] = 15\nax = sns.barplot(x=\"Model\", y=\"RMSE\", data=modeldf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nIn this assignment,I make a roughly analysis of the dataset, exploring the property of features.\n\nHowever, according to the discussion in kaggle, they found the leak of the features,which seems like time series and made the competition becoming a leak seeking game. \n\nThere are many zero value in the dataset, I think it means nan or nothing happened. The target and most features follow skewed distribution and their log transform follow normal distribution.\n\nIt also contains constant columns and duplicate columns."},{"metadata":{},"cell_type":"markdown","source":"In features engineering, I dropped the constant columns and duplicate columns and use their log transform for the machine learning. "},{"metadata":{},"cell_type":"markdown","source":"I learn a lot while tuning parameters for lightgbm and xgboost model, and I found the lightgbm run faster and has a better result.\n\nI use gridsearch with cross validation to tun the parameters. It was a hard work.\n\nMaybe I pay too less time in random forest, it's result is disappointing.\n\nI'm not familiar with stacking, I think that's the reason of it's bad performance, it's parameters may need to be tuned in other way.\n\nThe soft voting performs best thanks to the good performance of xgb and lgbm, it averages the two predictions to form the final prediction."},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":1}