{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Santander Value Prediction Challenge \n\n## Understanding the Dataset.\n\nPlease refer to the link for more details https://www.kaggle.com/c/santander-value-prediction-challenge/ \n"},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load DataSet \nFocusing only on train dataset for now."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/santander-value-prediction-challenge/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* So we have 4459 rows in train dataset. \n* We also have 4993 columns in total including the target and id column.\n* First time i am seeing where the number of columns are more than the data points (data rows). So need to be careful with feature selection / engineering.\n* We are provided with an anonymized dataset containing numeric feature variables, the numeric target column, and a string ID column. so we do not know what they mean.\n* The task is to predict the value of target column in the test set.\n* There are many zero values present in the data, this is just by looking top and last 5 records."},{"metadata":{},"cell_type":"markdown","source":"# Checking Data Type for each columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Due to huge count of columns, we could not use the info() method.\n\nLets have our own Data Frame to get the counf of different Data Types."},{"metadata":{"trusted":true},"cell_type":"code","source":"dtype_df = train.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(train.select_dtypes(['object']).columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`ID` is of object type and we will not use it for our prediction, so no need to worry about it."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"int_f = list(train.select_dtypes(['int']).columns)\nfloat_f = list(train.select_dtypes(['float']).columns)\n\nlen(int_f), len(float_f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Majority of the columns are of integer type and the rest are float type. \n* There is only one string column which is nothing but 'ID' column."},{"metadata":{},"cell_type":"markdown","source":"# Converting the float64 to float32"},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in float_f:\n    train[f] = train[f].astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtype_df = train.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check for any Missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As column list is huge, lets take a DataFrame and capture the missing columns and their count. "},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_df = train.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df[missing_df['missing_count']>0]\nmissing_df = missing_df.sort_values(by='missing_count')\nmissing_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool... no missing values in the dataset :)\n\nBut we have seen lots of zeros.. not sure if it represent somethig specific or its a missing data. Considering Zero represent some logical value."},{"metadata":{},"cell_type":"markdown","source":"# Check for Uniqueness of data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# randomly took one columns\ntrain['d5308d8bc'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems for randomly selected one of the column 'd5308d8bc' has only single value in the entire data-set. For such we can consider them as a Constant and easily drop such columns as they do not contribute any thing in ML model.\n\nLets find out more of such.\n\nTo achieve this first we have to get the Unique value count for each column, for this let me create another data frame and have the list of all columns with their respective count of unique values."},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_df = train.nunique().reset_index()\nunique_df.columns = [\"col_name\", \"unique_count\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have the details.. lets check how may columns are there which has single value for entire data-set."},{"metadata":{"trusted":true},"cell_type":"code","source":"constant_df = unique_df[unique_df[\"unique_count\"]==1]\nconstant_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow.. lot of constant columns. We should be good to drop these 256 while training the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# train['9fc776466'].nunique()\ntrain['9fc776466'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# constant_df.col_name.tolist()\nprint('Original Shape of Train Dataset {}'.format(train.shape))\ntrain.drop(constant_df.col_name.tolist(), axis = 1, inplace = True)\nprint('Shape after dropping Constant Columns from Train Dataset {}'.format(train.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We do not need ID field, so we will drop that as well.. but will have it in different DataFrame."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ids_df = train['ID']\ntrain_ids_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop('ID', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation\nAs the count of columns are huge.. unable to work on Correlation"},{"metadata":{},"cell_type":"markdown","source":"https://stackoverflow.com/questions/64003981/how-to-get-the-correlation-of-variables-with-thousands-of-feature-in-a-dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# k = 15 # Number of variables for heatmap.\n# target = 'target'\n\n# cols = train[int_f].corr().nlargest(k, target)[target].index\n\n# cm = train[cols].corr()\n\n# plt.figure(figsize = (10, 6))\n\n# sns.heatmap(cm, annot = True, cmap = 'viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection\n\n## Using SelectKBest (Univariate Feature Selection)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, chi2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"select_feature = SelectKBest(score_func=chi2, k = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop('target', axis = 1)\ny = train['target']\n\nX.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# select_feature.fit_transform(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting some problem.. will visit this later."},{"metadata":{},"cell_type":"markdown","source":"# Target Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.scatter(range(train.shape[0]), np.sort(train['target'].values))\nplt.xlabel('Index --> ', fontsize=12)\nplt.ylabel('Target --> ', fontsize=12)\nplt.title(\"Target Distribution\", fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems range is very high, and no visible outliers in the data.\n\nWe can now do a histogram plot of the target variable, lets see our findings."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.distplot(train[\"target\"].values, bins=50, kde=True)\nplt.xlabel('Target --> ', fontsize=12)\nplt.title(\"Target Histogram\", fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From Histogram we could see that the majority of the data points are having low value."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taking log of target variable and re-check the same...\nplt.figure(figsize=(12,8))\nsns.distplot( np.log1p(train[\"target\"].values), bins=50, kde=True)\nplt.xlabel('Target --> ', fontsize=12)\nplt.title(\"Log of Target Histogram\", fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<!-- # Correlation\nCopied the code from SRK (sudalairajkumar), thanks for the code. -->"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from scipy.stats import spearmanr\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n\n# labels = []\n# values = []\n# for col in train.columns:\n#     if col not in [\"ID\", \"target\"]:\n#         labels.append(col)\n#         values.append(spearmanr(train[col].values, train[\"target\"].values)[0])\n# corr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\n# corr_df = corr_df.sort_values(by='corr_values')\n \n# corr_df = corr_df[(corr_df['corr_values']>0.1) | (corr_df['corr_values']<-0.1)]\n# ind = np.arange(corr_df.shape[0])\n# width = 0.9\n# fig, ax = plt.subplots(figsize=(12,30))\n# rects = ax.barh(ind, np.array(corr_df.corr_values.values), color='b')\n# ax.set_yticks(ind)\n# ax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\n# ax.set_xlabel(\"Correlation coefficient\")\n# ax.set_title(\"Correlation coefficient of the variables\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<!-- There are quite a few variables with absolute correlation greater than 0.1 -->"},{"metadata":{"trusted":true},"cell_type":"code","source":"# cols_to_use = corr_df[(corr_df['corr_values']>0.11) | (corr_df['corr_values']<-0.11)].col_labels.tolist()\n\n# temp_df = train[cols_to_use]\n# corrmat = temp_df.corr(method='spearman')\n# f, ax = plt.subplots(figsize=(20, 20))\n\n# # Draw the heatmap using seaborn\n# sns.heatmap(corrmat, vmax=1., square=True, cmap=\"YlGnBu\", annot=True)\n# plt.title(\"Important variables correlation map\", fontsize=15)\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<!-- Seems like none of the selected variables have spearman correlation more than 0.7 with each other.\n\nThe above plots helped us in identifying the important individual variables which are correlated with target. However we generally build many non-linear models in Kaggle competitions. So let us build some non-linear models and get variable importance from them.\n\nIn this notebook, we will build two models to get the feature importances - Extra trees and Light GBM. It could also help us to see if the important features coming out from both of them are consistent. Let us first start with ET model. -->"},{"metadata":{},"cell_type":"markdown","source":"<!-- # Feature Importance - Extra trees model\n\nOur Evaluation metric for the competition is RMSLE. So let us use log of the target variable to build our models. Also please note that we are removing those variables with constant values (that we identified earlier). -->"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ### Get the X and y variables for building model ###\n# train_X = train.drop([\"ID\", \"target\"], axis=1)\n# train_y = np.log1p(train[\"target\"].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn import ensemble\n# model = ensemble.ExtraTreesRegressor(n_estimators=200, max_depth=20, max_features=0.5, n_jobs=-1, random_state=0)\n# model.fit(train_X, train_y)\n\n# ## plot the importances ##\n# feat_names = train_X.columns.values\n# importances = model.feature_importances_\n# std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n# indices = np.argsort(importances)[::-1][:20]\n\n# plt.figure(figsize=(12,12))\n# plt.title(\"Feature importances\")\n# plt.bar(range(len(indices)), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n# plt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\n# plt.xlim([-1, len(indices)])\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<!-- * 'f190486d6' seems to be the important variable followed by '58e2e02e6'. -->"},{"metadata":{},"cell_type":"markdown","source":"# Splitting the dataset into the Training set and Validation set.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split \n  \nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_valid.shape, y_train.shape, y_valid.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Scaling\n\nDoing the pre-processing part on training and validation data set such as fitting the Standard scale. For testing dataset will do it later."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler \nsc = StandardScaler() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = sc.fit_transform(X_train) \nX_valid = sc.transform(X_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying PCA function\nApplying the PCA function into training and validation set for analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA \n  \npca = PCA(n_components = 10) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_pca = pca.fit_transform(X_train) \nX_valid_pca = pca.transform(X_valid) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explained_variance = pca.explained_variance_ratio_ ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explained_variance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_pca.shape, X_valid_pca.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(pca.components_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_comp = pd.DataFrame(pca.components_, X.columns) \n  \n# plt.figure(figsize =(14, 6)) \n  \n# # plotting heatmap \n# sns.heatmap(df_comp) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling - using XGBClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_xgb = XGBRegressor()\nclf_xgb.fit(X_train_pca, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = abs(clf_xgb.predict(X_valid_pca))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RMSLE Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import math\n\n# #A function to calculate Root Mean Squared Logarithmic Error (RMSLE)\n# def rmsle(y, y_pred):\n#     assert len(y) == len(y_pred)\n#     terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n#     return (sum(terms_to_sum) * (1.0/len(y))) ** 0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rmsle_value = rmsle(y_valid, y_pred)\n# rmsle_value","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the SKLEarn"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error\nnp.sqrt(mean_squared_log_error( y_valid, y_pred ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_valid.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min(y_pred), max(y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Working on Test DataSet"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/santander-value-prediction-challenge/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"float_f = list(test_df.select_dtypes(['float']).columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in float_f:\n    test_df[f] = test_df[f].astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtype_test_df = train.dtypes.reset_index()\ndtype_test_df.columns = [\"Count\", \"Column Type\"]\ndtype_test_df.groupby(\"Column Type\").aggregate('count').reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Will drop the features.. which we dropped during the training phase."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.drop(constant_df.col_name.tolist(), axis = 1, inplace = True)\ntest_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ids_df = test_df['ID']\ntest_ids_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop ID feature as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.drop('ID', axis = 1, inplace = True)\ntest_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scale the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = sc.transform(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transform PCA on to test_df"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_pca = pca.transform(test_df) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_pca.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_pca[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_pca_5 = test_df_pca[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_full = abs(clf_xgb.predict(test_df_pca_5))\npred_test_full","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_full = abs(clf_xgb.predict(test_df_pca))\n# pred_test_full","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(pred_test_full)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> XGBClassifier is executed for 1+ hrs and yet did not resulted any predictions.. so i have forecfull stopped it. Will try other Algoriths."},{"metadata":{},"cell_type":"markdown","source":"Will try with going into loops."},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred_test_full = []\n\n# pr = clf_xgb.predict(test_df_pca_5)\n# # print(len(pr))\n\n# pred_test_full.append(pr.tolist())\n\n# # pr\n# # print(len(pred_test_full))\n# # pred_test_full\n\n# pr = clf_xgb.predict(test_df_pca[5:10])\n# pred_test_full.append(pr.tolist())\n\n# # print(len(pred_test_full))\n\n# pred_test_full","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# flat_list = [item for sublist in pred_test_full for item in sublist]\n# flat_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pr.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_df_pca[0]\n# np.reshape(test_df_pca[0],(1, test_df_pca[0].size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred_test_full = []\n\n# # for i in range(0,50) :\n# for i in range(0,test_df_pca.shape[0]) :\n#     pr = clf_xgb.predict(np.reshape(test_df_pca[i],(1, test_df_pca[i].size)))\n#     pred_test_full.append(pr.tolist())\n    \n# flat_list = [item for sublist in pred_test_full for item in sublist]\n# len(flat_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# l = train.shape[0]\n# c = l\n\n# pred_test_full = []\n\n# for i in range(0, len(l), 20):\n#     pred_test_full.append(test_df_pca[i:c])\n#     c += l","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# flat_list = [item for sublist in pred_test_full for item in sublist]\n# len(flat_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling using LightGBM; XGB; CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import lightgbm as lgb\n# import xgboost as xgb\n# from catboost import CatBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 40,\n        \"learning_rate\" : 0.004,\n        \"bagging_fraction\" : 0.6,\n        \"feature_fraction\" : 0.6,\n        \"bagging_frequency\" : 6,\n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1,\n        \"seed\": 42\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 5000, \n                      valid_sets=[lgtrain, lgval], \n                      early_stopping_rounds=100, \n                      verbose_eval=150, \n                      evals_result=evals_result)\n    \n    pred_test_y = np.expm1(model.predict(test_X, num_iteration=model.best_iteration))\n    return pred_test_y, model, evals_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Training LGB\n# pred_lgb, model, evals_result = run_lgb(X_train_pca, y_train, X_valid_pca, y_valid, test_df_pca)\n# print(\"LightGBM Training Completed...\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # feature importance\n# print(\"Features Importance...\")\n# gain = model.feature_importance('gain')\n# featureimp = pd.DataFrame({'feature':model.feature_name(), \n#                    'split':model.feature_importance('split'), \n#                    'gain':100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n# print(featureimp[:50])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGB "},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_xgb(train_X, train_y, val_X, val_y, test_X):\n    params = {'objective': 'reg:linear', \n          'eval_metric': 'rmse',\n          'eta': 0.001,\n          'max_depth': 10, \n          'subsample': 0.6, \n          'colsample_bytree': 0.6,\n          'alpha':0.001,\n          'random_state': 42, \n          'silent': True}\n    \n    tr_data = xgb.DMatrix(train_X, train_y)\n    va_data = xgb.DMatrix(val_X, val_y)\n    \n    watchlist = [(tr_data, 'train'), (va_data, 'valid')]\n    \n    model_xgb = xgb.train(params, tr_data, 2000, watchlist, maximize=False, early_stopping_rounds = 100, verbose_eval=100)\n    \n    dtest = xgb.DMatrix(test_X)\n    xgb_pred_y = np.expm1(model_xgb.predict(dtest, ntree_limit=model_xgb.best_ntree_limit))\n    \n    return xgb_pred_y, model_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Training XGB\n# pred_test_xgb, model_xgb = run_xgb(X_train_pca, y_train, X_valid_pca, y_valid, test_df_pca)\n# print(\"XGB Training Completed...\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Catboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# cb_model = CatBoostRegressor(iterations=500,\n#                              learning_rate=0.05,\n#                              depth=10,\n#                              eval_metric='RMSE',\n#                              random_seed = 42,\n#                              bagging_temperature = 0.2,\n#                              od_type='Iter',\n#                              metric_period = 50,\n#                              od_wait=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cb_model.fit(X_train_pca, y_train,\n#              eval_set=(X_valid_pca, y_valid),\n#              use_best_model=True,\n#              verbose=50)\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred_test_cat = np.expm1(cb_model.predict(test_df_pca))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate - RMSLE "},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.metrics import mean_squared_log_error\n# np.sqrt(mean_squared_log_error( y_valid, y_pred ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{},"cell_type":"markdown","source":"Lets see the expected submission format."},{"metadata":{"trusted":true},"cell_type":"code","source":"subm_sample = pd.read_csv('../input/santander-value-prediction-challenge/sample_submission.csv')\nsubm_sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_ids_df.head()\ntest_ids_df[:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm_df = pd.DataFrame({\"ID\":test_ids_df[:]})\nsubm_df[\"target\"] = pred_test_full\nsubm_df.to_csv(\"XGBReg_v1.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm_df.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}