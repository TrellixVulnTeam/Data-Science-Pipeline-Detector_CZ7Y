{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_csv = pd.read_csv('/kaggle/input/santander-value-prediction-challenge/train.csv')\ntest_csv = pd.read_csv('/kaggle/input/santander-value-prediction-challenge/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_csv.shape, test_csv.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see, the number of columns is 10 time as the number of the rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are many zero. It's sparse matrix.\n* The name of columns is anonymous."},{"metadata":{},"cell_type":"markdown","source":"### Target Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nplt.scatter(range(train_csv.shape[0]), np.sort(train_csv.target.values))\nplt.grid()\nplt.xlabel('index', fontsize=12)\nplt.ylabel('Target', fontsize=12)\nplt.title(\"Target Distribution\", fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.hist(train_csv.target.values, bins=50)\nplt.xlabel('Target', fontsize=12)\nplt.title(\"Target Histogram\", fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.hist(np.log1p(train_csv.target.values), bins=50)\nplt.xlabel('Target', fontsize=12)\nplt.title(\"Target Histogram\", fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_nunique = train_csv.nunique().reset_index()\nnum_nunique.columns = ['Col_name', 'Value_columns']\nnum_nunique","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"only1_nuique = num_nunique[num_nunique.Value_columns == 1]\nonly1_nuique.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import spearmanr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 256 columns are only 1 value."},{"metadata":{},"cell_type":"markdown","source":"### Correlation Coefficient"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm, tqdm_notebook\nfrom scipy.stats import spearmanr, pearsonr\nimport warnings\nwarnings.filterwarnings('ignore')\ncol_names = []\ncor_value = []\nfor col in tqdm(train_csv.columns, ncols=100 , leave= True, desc=\"Spearman r : \"):\n    if col not in ['ID','target']:\n        col_names.append(col)\n        cor_value.append(spearmanr(train_csv[col].values, train_csv.target.values)[0])\ncorrs = pd.DataFrame({'Feature_Name':col_names,'Corr_value':cor_value})\ncorrs = corrs.sort_values(by = 'Corr_value')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_df = corrs[(corrs.Corr_value > 0.1) | (corrs.Corr_value < -0.1)].reset_index()\ncorr_df.drop('index', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_df = corr_df.set_index('Feature_Name')\ncorr_df.plot(kind='barh', figsize = (12,15), title='Correlation of variables')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_df = corrs[(corrs.Corr_value > 0.11) | (corrs.Corr_value < -0.11)].reset_index()\ncorr_df.drop('index', axis = 1, inplace = True)\ncorr_df = corr_df.set_index('Feature_Name')\ncorr_df.plot(kind='barh', figsize = (12,15), title='Correlation of variables')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Spearman did not show much more correlation between the variables.\\\nWhat about pearson?"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = []\nvalues = []\nfor col in tqdm(train_csv.columns, ncols = 100, desc = 'Pearson r :'):\n    if col not in ['ID', 'target']:\n        corr_value = pearsonr(train_csv[col].values, train_csv['target'].values)\n        values.append(corr_value)\n        labels.append(col)\ncorr_df = pd.DataFrame({'Feature_Name' : labels, 'Feature_Value' : values})\ncorr_df = corr_df.sort_values(by = 'Feature_Value')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train_csv.target\ntrain_csv.drop(list(only1_nuique.Col_name) + ['ID', 'target'], axis = 1, inplace=True)\ntest_csv.drop(list(only1_nuique.Col_name), axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id = test_csv.ID\ntest_csv.drop('ID', axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = np.log1p(target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import ensemble\nmodel = ensemble.ExtraTreesRegressor(n_estimators=200, max_depth=20, max_features=0.5,\n                                     n_jobs=-1, random_state=50)\nmodel.fit(train_csv, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = pd.DataFrame({'Feature_Name' : train_csv.columns, 'feature_importance' : model.feature_importances_})\nfeature_importances = feature_importances.set_index('Feature_Name')\n\n# sort by importances\nfeature_importances = feature_importances.sort_values(by = 'feature_importance')\nstd_feat_importances = np.std([tree.feature_importances_ for tree in model.estimators_],axis=0)\nfeature_importances.iloc[-50:].plot(kind='barh',figsize = (10,20),color = 'g',\n                         xerr=std_feat_importances[-50:], align='center',\n                                   title='top 50 Feature Importances')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variable f190486d6 is an important elemant."},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn import  model_selection, metrics\nparams = {\n    \"objective\" : \"regression\",\n    \"metric\" : \"rmse\",\n    \"num_leaves\" : 30,\n    \"learning_rate\" : 0.01,\n    \"bagging_seed\" : 1884,\n    \"device\" : \"gpu\",\n    \"gpu_platform_id\" : 0,\n    \"gpu_device_id\" : 0,\n    \"num_thread\" : 8\n}\nval_data = train_csv[0:500]\nval_label = target[0:500]\nval_set = lgb.Dataset(val_data, val_label)\ntrain_set = lgb.Dataset(train_csv, label=target)\nmodel = lgb.train(params, train_set ,5000, valid_sets = [val_set],  early_stopping_rounds = 100, verbose_eval = 200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb.plot_importance(model, max_num_features=50, height=0.8, figsize=(12, 20))\nplt.title(\"LightGBM - Feature Importance top 50\", fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = model.predict(test_csv, num_iteration=model.best_iteration)\nresult = np.expm1(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = pd.DataFrame({'ID':test_id, 'target':result})\n\nsubmit.to_csv('submit3_3.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Score : 1.53"},{"metadata":{},"cell_type":"markdown","source":"************"},{"metadata":{},"cell_type":"markdown","source":"### Inspired by This solution\nhttps://www.kaggle.com/alexpengxiao/preprocessing-model-averaging-by-xgb-lgb-1-39\n* 1. the feature importance is found by random forest\n* 2. he drop many columns and remain 1000 to train\n* 3. he use  Kolmogorov-Smirnov test to test whether the columns of the train and test are generated from the same distribution.\n* 4. And he add some new feature.\n* 5. AveragingModels"},{"metadata":{},"cell_type":"markdown","source":"We choose top 1500 columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"importance = model.feature_importance(importance_type='split')\nfeature_name = model.feature_name()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = pd.DataFrame({'Feature_Name' : feature_name, 'Feature_Important' : importance}).sort_values(by=['Feature_Important'], ascending=[False])[:1000]\ntrain = train_csv[list(features.Feature_Name)]\ntest = test_csv[list(features.Feature_Name)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add new features, add some additional statistical features to the original features"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ntrain = train.shape[0]\nntest = test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weight = ((train != 0).sum()/len(train)).values\ntmp_train = train[train!=0]\ntmp_test = test[test!=0]\ntrain[\"weight_count\"] = (tmp_train*weight).sum(axis=1)\ntest[\"weight_count\"] = (tmp_test*weight).sum(axis=1)\ntrain[\"count_not0\"] = (train != 0).sum(axis=1)\ntest[\"count_not0\"] = (test != 0).sum(axis=1)\ntrain[\"sum\"] = train.sum(axis=1)\ntest[\"sum\"] = test.sum(axis=1)\ntrain[\"var\"] = tmp_train.var(axis=1)\ntest[\"var\"] = tmp_test.var(axis=1)\ntrain[\"median\"] = tmp_train.median(axis=1)\ntest[\"median\"] = tmp_test.median(axis=1)\ntrain[\"mean\"] = tmp_train.mean(axis=1)\ntest[\"mean\"] = tmp_test.mean(axis=1)\ntrain[\"std\"] = tmp_train.std(axis=1)\ntest[\"std\"] = tmp_test.std(axis=1)\ntrain[\"max\"] = tmp_train.max(axis=1)\ntest[\"max\"] = tmp_test.max(axis=1)\ntrain[\"min\"] = tmp_train.min(axis=1)\ntest[\"min\"] = tmp_test.min(axis=1)\ntrain[\"skew\"] = tmp_train.skew(axis=1)\ntest[\"skew\"] = tmp_test.skew(axis=1)\ntrain[\"kurtosis\"] = tmp_train.kurtosis(axis=1)\ntest[\"kurtosis\"] = tmp_test.kurtosis(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"weight_count\"] = train[\"weight_count\"].fillna(0)\ntrain[\"count_not0\"] = train[\"count_not0\"].fillna(0)\ntrain[\"sum\"] = train[\"sum\"].fillna(0)\ntrain[\"var\"] = train[\"var\"].fillna(0)\ntrain[\"median\"] = train[\"median\"].fillna(0)\ntrain[\"mean\"] = train[\"mean\"].fillna(0)\ntrain[\"std\"] = train[\"std\"].fillna(0)\ntrain[\"max\"] = train[\"max\"].fillna(0)\ntrain[\"min\"] = train[\"min\"].fillna(0)\ntrain[\"skew\"] = train[\"skew\"].fillna(0)\ntrain[\"kurtosis\"] = train[\"kurtosis\"].fillna(0)\n\ntest[\"weight_count\"] = test[\"weight_count\"].fillna(0)\ntest[\"count_not0\"] = test[\"count_not0\"].fillna(0)\ntest[\"sum\"] = test[\"sum\"].fillna(0)\ntest[\"var\"] = test[\"var\"].fillna(0)\ntest[\"median\"] = test[\"median\"].fillna(0)\ntest[\"mean\"] = test[\"mean\"].fillna(0)\ntest[\"std\"] = test[\"std\"].fillna(0)\ntest[\"max\"] = test[\"max\"].fillna(0)\ntest[\"min\"] = test[\"min\"].fillna(0)\ntest[\"skew\"] = test[\"skew\"].fillna(0)\ntest[\"kurtosis\"] = test[\"kurtosis\"].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import random_projection\nNUM_OF_COM = 100\ntmp = pd.concat([train,test])\ntransformer = random_projection.SparseRandomProjection(n_components = NUM_OF_COM)\nRP = transformer.fit_transform(tmp)\nrp = pd.DataFrame(RP)\ncolumns = [\"RandomProjection{}\".format(i) for i in range(NUM_OF_COM)]\nrp.columns = columns\nrp_train = rp[:ntrain]\nrp_test = rp[ntrain:]\nrp_test.index = test.index\n\n#concat RandomProjection and raw data\ntrain = pd.concat([train,rp_train],axis=1)\ntest = pd.concat([test,rp_test],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import ensemble\nmodel = ensemble.ExtraTreesRegressor(n_estimators=200, max_depth=20, max_features=0.5,\n                                     n_jobs=-1, random_state=50)\nmodel.fit(train, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = model.predict(test)\nresult = np.expm1(result)\n\nsubmit = pd.DataFrame({'ID':test_id, 'target':result})\n\nsubmit.to_csv('submit3_4_extra.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nimport lightgbm as lgb\n\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.055, colsample_bylevel =0.5, \n                             gamma=1.5, learning_rate=0.02, max_depth=32, \n                             objective='reg:linear',booster='gbtree',\n                             min_child_weight=57, n_estimators=1000, reg_alpha=0, \n                             reg_lambda = 0,eval_metric = 'rmse', subsample=0.7, \n                             silent=1, n_jobs = -1, early_stopping_rounds = 14,\n                             random_state =7, nthread = -1)\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=144,\n                              learning_rate=0.005, n_estimators=720, max_depth=13,\n                              metric='rmse',is_training_metric=True,\n                              max_bin = 55, bagging_fraction = 0.8,verbose=-1,\n                              bagging_freq = 5, feature_fraction = 0.9) \n\nmodel_xgb.fit(train, target)\nmodel_lgb.fit(train, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = model_xgb.predict(test)\nresult = np.expm1(result)\n\nsubmit = pd.DataFrame({'ID':test_id, 'target':result})\n\nsubmit.to_csv('submit3_4_xgb.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = model_lgb.predict(test)\nresult = np.expm1(result)\n\nsubmit = pd.DataFrame({'ID':test_id, 'target':result})\n\nsubmit.to_csv('submit3_4_lgb.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Score : 1.41"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}