{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/santander-value-prediction-challenge/train.csv\")\ndb=pd.read_csv(\"/kaggle/input/santander-value-prediction-challenge/sample_submission.csv\")\ndc=pd.read_csv(\"/kaggle/input/santander-value-prediction-challenge/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#Использованный код https://www.kaggle.com/samratp/lightgbm-xgboost-catboost\n#Здесь идет очистка столбцов, содержащих константы (типо каждая строка в столбце = 0)\ncolsToRemove = []\nfor col in df.columns:\n    if col != 'ID' and col != 'target':\n        if df[col].std() == 0: \n            colsToRemove.append(col)\n        \n# remove constant columns in the training set\ndf.drop(colsToRemove, axis=1, inplace=True)\n\n# remove constant columns in the test set\ndc.drop(colsToRemove, axis=1, inplace=True) \n\nprint(\"Removed `{}` Constant Columns\\n\".format(len(colsToRemove)))\nprint(colsToRemove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Функция поиска... повторяющихся столбцов? Долго работает, поэтому я просто скопирую (и удалю) названия удаленных дубликатов (которые автор уже нашел)\ndef duplicate_columns(frame):\n    groups = frame.columns.to_series().groupby(frame.dtypes).groups\n    dups = []\n\n    for t, v in groups.items():\n\n        cs = frame[v].columns\n        vs = frame[v]\n        lcs = len(cs)\n\n        for i in range(lcs):\n            ia = vs.iloc[:,i].values\n            for j in range(i+1, lcs):\n                ja = vs.iloc[:,j].values\n                if np.array_equal(ia, ja):\n                    dups.append(cs[i])\n                    break\n\n    return dups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colsToRemove = ['34ceb0081', '8d57e2749', '168b3e5bc', 'a765da8bc', 'acc5b709d']\nprint(colsToRemove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove duplicate columns in the training set\ndf.drop(colsToRemove, axis=1, inplace=True) \n\n# remove duplicate columns in the testing set\ndc.drop(colsToRemove, axis=1, inplace=True)\n\nprint(\"Removed `{}` Duplicate Columns\\n\".format(len(colsToRemove)))\nprint(colsToRemove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Функция очищения столбцов которые видимо... \ndef drop_sparse(train, test):\n    flist = [x for x in train.columns if not x in ['ID','target']]\n    for f in flist:\n        if len(np.unique(train[f]))<2:\n            train.drop(f, axis=1, inplace=True)\n            test.drop(f, axis=1, inplace=True)\n            #print(train[f])\n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_df, test_df = drop_sparse(df, dc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train set size: {}\".format(df.shape))\nprint(\"Test set size: {}\".format(dc.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df.drop([\"ID\", \"target\"], axis=1)\ny_train = np.log1p(df[\"target\"].values)\n\nX_test = dc.drop([\"ID\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndev_X, val_X, dev_y, val_y = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_xgb(train_X, train_y, val_X, val_y, test_X):\n    params = {'objective': 'reg:linear', \n          'eval_metric': 'rmse',\n          'eta': 0.001,\n          'max_depth': 5, \n          'subsample': 0.6, \n          'colsample_bytree': 0.6,\n          'alpha':0.001,\n          'random_state': 42, \n          'silent': True}\n    \n    tr_data = xgb.DMatrix(train_X, train_y)\n    va_data = xgb.DMatrix(val_X, val_y)\n    \n    watchlist = [(tr_data, 'train'), (va_data, 'valid')]\n    \n    model_xgb = xgb.train(params, tr_data, 2000, watchlist, maximize=False, early_stopping_rounds = 100, verbose_eval=100)\n    \n    dtest = xgb.DMatrix(test_X)\n    xgb_pred_y = np.expm1(model_xgb.predict(dtest, ntree_limit=model_xgb.best_ntree_limit))\n    \n    return xgb_pred_y, model_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\npred_test_xgb, model_xgb = run_xgb(dev_X, dev_y, val_X, val_y, X_test)\nprint(\"XGB Training Completed...\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"db['target'] = pred_test_xgb ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(db.head())\ndb.to_csv('sub_lgb_xgb_cat.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}