{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_columns', 10)\npd.set_option('display.max_rows', 50)\n\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_set = pd.read_csv('../input/train.csv')\n# test_set = pd.read_csv('../input/test.csv')\nprint(train_set)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49b2562799b1fe67bcb80550fb3dcd5751c35142"},"cell_type":"markdown","source":"Firstly,  let us simply explore the dateset to have to basic expression of our data"},{"metadata":{"trusted":true,"_uuid":"5778ed516b18b708f9a5c9c2553e6b02cab06002"},"cell_type":"code","source":"train_set.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f7911e5c463312001e4dc82c110d4c3dc73cdca"},"cell_type":"markdown","source":"There are 4459 samples in our data, each sample has 4993 columns, in which 4991 columns can be regarded as features , ID has no special meaning and target is the value wo are going to predict.\nThen,, let's see if there are missing values in this dataset"},{"metadata":{"trusted":true,"_uuid":"20000307c9bd7d8718457ca4c7f2a6162110b4c7"},"cell_type":"code","source":"missing_set = train_set.isnull().sum(axis=0).reset_index()\nmissing_set.columns = ['column', 'count']\nmissing_set.sort_values(by=[\"count\"], inplace=True, ascending=False)\nmissing_set","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15c559f4cbd3230bd1e2e682ede8a1cc022dacd1"},"cell_type":"markdown","source":"Now we can say there are no missing values in this dataset, so, wo do not need to do precessing for missing values in the following analysis. \nNext, let's dive deeper into our data."},{"metadata":{"trusted":true,"_uuid":"f7e13d735674babb672bdef1e9e38fa9e2fe233c"},"cell_type":"code","source":"desc = train_set.describe().T\nconstant_index = desc[desc['std'] == 0].index\nconstant_index","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d9ad019099b13645f7926347e944882c4d20ee7"},"cell_type":"markdown","source":"We have 256 columns whose standard deviation equal to 0, meaning that both these columns have a constant value and they have nothing to do with out target. So, in the coming analysis, we will remove these columns to make our data less scalable."},{"metadata":{"trusted":true,"_uuid":"7f57b857b906b145c86acbf128b0d621285ff9a4","collapsed":true},"cell_type":"code","source":"train_set = train_set.drop(columns=list(constant_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9225d0d1b0455f7fa06f042fd41ea5cc0656e312"},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\nsns.distplot(train_set[\"target\"], ax=axes[0,0])\nsns.distplot(np.log(train_set[\"target\"]), ax=axes[0, 1])\nsns.boxplot(data=train_set[\"target\"], ax=axes[1, 0])\nsns.boxplot(data=np.log(train_set[\"target\"]), ax=axes[1, 1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b057e3f9e1474bb31d218c4298deb99620ead80"},"cell_type":"markdown","source":"Apparently,  out target is right skew distributed, if we display it in hist and box graph, we can see that it has a long tail which can even be regarded as outliers, most of its value are concentrated among a narrow area。 But after log transfermation, our target has a much better distribution, very close to normal distribution."},{"metadata":{"_uuid":"9fdf55ae438797b0f6d9e406f21341f6d067e0dd","trusted":true,"collapsed":true},"cell_type":"code","source":"corr = abs(train_set.corr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4cfdc23d8441a5aa0dcd487f72caef5ab5c571e"},"cell_type":"code","source":"count = corr == 1\ncount = count.sum(axis=0).reset_index()\ncount.columns = ['column', 'count']\ncount.sort_values(by=[\"count\"], inplace=True, ascending=False)\ncount","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10283e3c5ea6745817efa1bb48bfde45a978454e"},"cell_type":"code","source":"lg_1 = count['count'] > 1\nlg_1.sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2da23c9175befa1c979117f43523b30453c948b5"},"cell_type":"markdown","source":"We can see that in the correlation matrix, not only the value in the digonal equal to 1, but also there are some values equal to 1 in other position. We can summarize that many columns in our data must be the same or linear dependent to each other. So, we have to remove those duplicated columns, as we have a 4736*4736 correlation coefficient matrix, to find those value equal to 1 and then remove the column in our data one by one can be a tough work. So PCA can be apply to our data to remove redundant features，before that， we will calculate  the 1000 largest correlation coefficient and decide how many principal component to reserve in our new data."},{"metadata":{"trusted":true,"_uuid":"642627d60e42a86af187e8e93da8fc85b93c9c90"},"cell_type":"code","source":"n_largest = corr.nlargest(n=1000, columns='target')\nn_largest","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0559947c2311c759e3a6e8e72cd49f5a04c599b5"},"cell_type":"markdown","source":"It shows that more than 3000 features  has a correlation  coefficient less than 0.036, which means they nearly has nothing to do with our target, so in our PCA analysis, 500 components may be enough to be reserved."},{"metadata":{"trusted":true,"_uuid":"d97566184cf78b00ce0db044a2ee5685ca4b8ab2"},"cell_type":"code","source":"heat = corr.nlargest(n=10, columns='target')\ncolumns = list(heat.index)\nheat = heat[heat.index]\nf, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(heat, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=heat.columns, xticklabels=heat.index, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd4d1079db773abd0bd356a499600f3bd4fc8038"},"cell_type":"markdown","source":"We draw a hear map to show the ten largest features correlated to our target. And now, we will apply PCA to our data."},{"metadata":{"trusted":true,"_uuid":"b861a9f2f4d9fe8d4f45d7ff2f63b3b4e0e92b18"},"cell_type":"code","source":"from sklearn.decomposition import PCA\ntrain_set = train_set.drop(columns=\"ID\")\ntarget = train_set['target']\ntrain_set = train_set.drop(columns=\"target\")\ntrain_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b5066b348e2bac969cecac4b3f89237c441b818"},"cell_type":"code","source":"pca = PCA(n_components=500)\nnew_set = pca.fit_transform(train_set)\nnew_set","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9231fa96c1e6d61a15f1be2557e7469b45f60fca"},"cell_type":"markdown","source":"PCA is applied to our data and a new data set  which has only 500 features has been created, next, we will use LR and LGB model to predict the target value according to this new data set."},{"metadata":{"trusted":true,"_uuid":"cc6a9f344a2352696fa914b7dfecabb5b6146ecb"},"cell_type":"code","source":"from sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LinearRegression\nnew_target = np.log(target)\nlinear_model = LinearRegression()\nlinear_scores = cross_validate(linear_model, new_set, new_target, cv=5, return_train_score=True)\nlinear_scores\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fa8ae801c94ce359025e9a4797184c3d5ea9b19"},"cell_type":"code","source":"import lightgbm as lgb \nfrom sklearn.model_selection import train_test_split\nlgb_train, lgb_test, lgb_target_train, lgb_target_test = train_test_split(new_set, new_target, test_size=0.20, shuffle = True, random_state = 42)\nparam =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'auc',\n    'num_leaves': 32,\n    'learning_rate': 0.02,\n    'verbose': 0,\n    'lambda_l1': 1,\n    'scale_pos_weight': 8 \n} \nlgtrain = lgb.Dataset(lgb_train, lgb_target_train)\nlgvalid = lgb.Dataset(lgb_test, lgb_target_test)\nlgb_clf = lgb.train(\n    param,\n    lgtrain,\n    num_boost_round=10000,\n    valid_sets=[lgtrain, lgvalid],\n    valid_names=['train','valid'],\n    early_stopping_rounds=100,\n    verbose_eval=100\n    )\n# lgb.cv(param, lgb_train, 5, nfold=5)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}