{"cells":[{"metadata":{"_uuid":"a301e87b1cd65e17318b93460d2972266ef0e5f8"},"cell_type":"markdown","source":"This kernel was created based on Samrat P's Kernel.\nI appreciate Samrat P's wonderful Kernel"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/santander-value-prediction-challenge\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\n\nimport lightgbm as lgb\n\nclass SklearnWrapper(object):\n    def __init__(self, clf, seed=0, params=None, seed_bool = True):\n        if(seed_bool == True):\n            params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n            \ndef get_oof(clf, x_train, y, x_test):\n    #trainの一部を抜いて時の予測の平均を特徴量に加える。\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf):\n        print('\\nFold {}'.format(i))\n        x_tr = x_train.iloc[train_index]\n        y_tr = y[train_index]\n        x_te = x_train.iloc[test_index]\n\n        clf.train(x_tr, y_tr)\n        \n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"029712033a812952c1c39971f3a168e3a57ab407","collapsed":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/santander-value-prediction-challenge/train.csv')\ntest_df = pd.read_csv('../input/santander-value-prediction-challenge/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1c69d72557c48ea638a6cb977bcf2b4b9551fa85"},"cell_type":"code","source":"X_train = train_df.drop([\"ID\", \"target\"], axis=1)\n\ny_train = np.log1p(train_df[\"target\"].values)\n\nX_test = test_df.drop([\"ID\"], axis=1)\n\nntrain = len(X_train)\nntest = len(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c5b3966616a199b79c25694651cb753514a8d868"},"cell_type":"code","source":"colsToRemove = []\nfor col in X_train.columns:\n    if X_train[col].std() == 0: \n        colsToRemove.append(col)\n        \n# remove constant columns in the training set\nX_train.drop(colsToRemove, axis=1, inplace=True)\n\n# remove constant columns in the test set\nX_test.drop(colsToRemove, axis=1, inplace=True) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3e56a632d73cf720b3a03419921c1e4967c1937a"},"cell_type":"code","source":"colsToRemove = []\ncolsScaned = []\ndupList = {}\n\ncolumns = X_train.columns\n\nfor i in range(len(columns)-1):\n    v = X_train[columns[i]].values\n    dupCols = []\n    for j in range(i+1,len(columns)):\n        if np.array_equal(v, X_train[columns[j]].values):\n            colsToRemove.append(columns[j])\n            if columns[j] not in colsScaned:\n                dupCols.append(columns[j]) \n                colsScaned.append(columns[j])\n                dupList[columns[i]] = dupCols\n                \n# remove duplicate columns in the training set\nX_train.drop(colsToRemove, axis=1, inplace=True) \n\n# remove duplicate columns in the testing set\nX_test.drop(colsToRemove, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0d04a335cc4b6b8dc612a2dbad4c6bb402505ef2"},"cell_type":"code","source":"weight = ((X_train != 0).sum()/len(X_train)).values\n\ntmp_train = X_train[X_train!=0].fillna(0)\ntmp_test = X_test[X_test!=0].fillna(0)\n\nX_train[\"weight_count\"] = (tmp_train*weight).sum(axis=1)\nX_test[\"weight_count\"] = (tmp_test*weight).sum(axis=1)\n\nX_train[\"count_not0\"] = (X_train != 0).sum(axis=1)\nX_test[\"count_not0\"] = (X_test != 0).sum(axis=1)\n\nX_train[\"sum\"] = X_train.sum(axis=1)\nX_test[\"sum\"] = X_test.sum(axis=1)\n\nX_train[\"var\"] = X_train.var(axis=1)\nX_test[\"var\"] = X_test.var(axis=1)\n\nX_train[\"mean\"] = X_train.mean(axis=1)\nX_test[\"mean\"] = X_test.mean(axis=1)\n\nX_train[\"std\"] = X_train.std(axis=1)\nX_test[\"std\"] = X_test.std(axis=1)\n\nX_train[\"max\"] = X_train.max(axis=1)\nX_test[\"max\"] = X_test.max(axis=1)\n\nX_train[\"min\"] = X_train.min(axis=1)\nX_test[\"min\"] = X_test.min(axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e76e03e1b689016848136863d9b4fd38e478f4a1","collapsed":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn.cross_validation import KFold\n\n\nSEED = 5\nNFOLDS = 5\n\nkf = KFold(ntrain, n_folds=NFOLDS, shuffle=True, random_state=SEED)\nridge_params = {'alpha':30.0, 'fit_intercept':True, 'normalize':False, 'copy_X':True,\n                'max_iter':None, 'tol':0.001, 'solver':'auto', 'random_state':SEED}\n\n#Ridge oof method from Faron's kernel\n#I was using this to analyze my vectorization, but figured it would be interesting to add the results back into the dataset\n#It doesn't really add much to the score, but it does help lightgbm converge faster\nridge = SklearnWrapper(clf=Ridge, seed = SEED, params = ridge_params)\nridge_oof_train, ridge_oof_test = get_oof(ridge, X_train, y_train, X_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bca90d3208279b2a733039f16b39e721d5f774cf","collapsed":true},"cell_type":"code","source":"rms = sqrt(mean_squared_error(y_train, ridge_oof_train))\nprint('Ridge OOF RMSE: {}'.format(rms))\n\nprint(\"Modeling Stage\")\n\nX_train[\"ridge_preds\"] = ridge_oof_train\nX_test[\"rigde_preds\"] =  ridge_oof_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9659675dbf3eb786cccbb82770e16ce355bd2b93"},"cell_type":"code","source":"dev_X, val_X, dev_y, val_y = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"57ae22866ebb116ecc320a6d1f54938bc3eba38b"},"cell_type":"code","source":"def run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 40,\n        \"learning_rate\" : 0.005,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.5,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1,\n        \"seed\": 42\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 5000, \n                      valid_sets=[lgval], \n                      early_stopping_rounds=100, \n                      verbose_eval=50, \n                      evals_result=evals_result)\n    \n    pred_test_y = np.expm1(model.predict(test_X, num_iteration=model.best_iteration))\n    return pred_test_y, model, evals_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc2c3197e987e88bab26fbaaf93cfb657f055d34","collapsed":true},"cell_type":"code","source":"pred_test, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, X_test)\nprint(\"LightGBM Training Completed...\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70f3150372a9031f1692def1f85958ef068524e2","collapsed":true},"cell_type":"code","source":"sub = pd.read_csv('../input/santander-value-prediction-challenge/sample_submission.csv')\nsub[\"target\"] = pred_test\nprint(sub.head())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a4ec9e754f20466083f64c18e0c54d43cc3a4768"},"cell_type":"code","source":"sub.to_csv('sub_lgbm_ridge_agg.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3ad3669bc63f29a32a32f77a001b488c99ddb9d9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}