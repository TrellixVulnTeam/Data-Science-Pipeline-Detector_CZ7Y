{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Sources:**\n\nhttps://www.kaggle.com/sudalairajkumar/simple-exploration-baseline-santander-value\n\nhttps://www.kaggle.com/c/santander-value-prediction-challenge/discussion/59128\n\nhttps://en.wikipedia.org/wiki/Ordinal_data\n\nhttps://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/a-comparison-of-the-pearson-and-spearman-correlation-methods/#:~:text=The%20Pearson%20correlation%20evaluates%20the%20linear%20relationship%20between%20two%20continuous%20variables.&text=The%20Spearman%20correlation%20coefficient%20is,evaluate%20relationships%20involving%20ordinal%20variables.\n\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.nunique.html\n\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html\n\nhttps://seaborn.pydata.org/generated/seaborn.heatmap.html\n\nhttps://matplotlib.org/3.1.0/tutorials/colors/colormaps.html\n\nhttps://www.geeksforgeeks.org/matplotlib-pyplot-xlim-in-python/#:~:text=The%20xlim()%20function%20in,limits%20of%20the%20current%20axes.&text=Parameters%3A%20This%20method%20accept%20the,set%20the%20xlim%20to%20right.\n\nhttps://numpy.org/doc/stable/reference/generated/numpy.expm1.html","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.options.display.max_columns = 9999 #Making sure all columns appear\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FILEPATH_TRAIN = '/kaggle/input/santander-value-prediction-challenge/train.csv'\nFILEPATH_TEST = '/kaggle/input/santander-value-prediction-challenge/test.csv'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(FILEPATH_TRAIN, sep=',', engine='c') #Specify sep when using C engine\ntest = pd.read_csv(FILEPATH_TEST, sep=',', engine='c')\nprint(\"Shape of train:\",train.shape, '\\n',\"Shape of test:\",test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random values are the column names which mean the columns are anonymous**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting of the target variable**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9, 7))\nplt.scatter(range(train.shape[0]), np.sort(train['target'].values))\nplt.xlabel('Index', fontsize=14)\nplt.ylabel('Target', fontsize=14)\nplt.title(\"Target Distribution\", fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.distplot(train['target'].values, bins=50, kde=False)\nplt.xlabel('Target', fontsize=11)\nplt.title('Target Histogram', fontsize=11)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Best way to display a right skewed distribution is to use a log scale**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.distplot(np.log1p(train['target'].values), bins=50, kde=False)\nplt.xlabel('Target', fontsize=11)\nplt.title('Target Histogram', fontsize=11)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Much better**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**No null values. Always a good sign**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_vals = train.nunique().reset_index() #This drops NaN values by default\nunique_vals.columns = [\"Name\", \"Uniqueness\"]\nconst_d = unique_vals[unique_vals[\"Uniqueness\"]==1]\nconst_d.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"256 Unique columns are present","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Print the anonymised columns**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"str(const_d.Name.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Spearman correlation is better to use due to the fact that it is computed based on ranks and this data is not linear where we can use Pearson correlation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ignore any warnings that arise\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom scipy.stats import spearmanr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = []\nvalues = []\n\nfor col in train.columns:\n    if col not in [\"ID\", \"target\"]:\n        labels.append(col)\n        values.append(spearmanr(train[col].values, train['target'].values)[0])\n\ncorrelation_df = pd.DataFrame({'column_label':labels, 'correlation_val':values})        \ncorrelation_df = correlation_df.sort_values(by='correlation_val')\n\ncorrelation_df = correlation_df[(correlation_df['correlation_val']>0.1) | (correlation_df['correlation_val']<-0.1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = np.arange(correlation_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(10,25))\nrec = ax.barh(index, np.array(correlation_df.correlation_val.values), color='r')\nax.set_yticks(index) #Set Y to index value of the df\nax.set_yticklabels(correlation_df.column_label.values, rotation='horizontal') #Define horizontal bar graph\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient of the variables\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The Heatmap of Correlation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\ncolumns = correlation_df[(correlation_df['correlation_val']>0.11) | (correlation_df['correlation_val']<-0.11)].column_label.tolist()\n\ntmp = train[columns]\ncomat = tmp.corr(method='spearman') #Since we used spearman coefficient\nfig, ax = plt.subplots(figsize=(30,30))\n\nsns.heatmap(comat, square=True, cmap=\"RdYlGn\", annot=True)\nplt.title(\"Correlation Heatmap\", fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_x = train.drop(const_d.Name.tolist()+ [\"ID\", \"target\"], axis=1)\nte_x = test.drop(const_d.Name.tolist()+[\"ID\"], axis=1)\ntr_y = np.log1p(train['target'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import ensemble\nmodel = ensemble.ExtraTreesRegressor(n_estimators=200, max_depth=20, max_features=0.5, n_jobs=-1, random_state=0)\nmodel.fit(tr_x, tr_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot Importance factor\nfeatures = tr_x.columns.values\nimportance = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importance)[::-1][:20]\n\nplt.figure(figsize=(14,14))\nplt.title(\"Feature Importances\")\nplt.bar(range(len(indices)), importance[indices], color=\"b\", yerr=std[indices])\nplt.xticks(range(len(indices)), features[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Baseline Light GBM** (TODO:Tune it)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_lgb(train_x, train_y, val_x, val_y, test_x):\n    parameters = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'num_leaves': 30,\n        'learning_rate': 0.01,\n        'bagging_fraction': 0.7,\n        'feature_fraction': 0.7,\n        'bagging_frequency': 5,\n        'bagging_seed': 2018,\n        'verbosity': -1\n    }\n    \n    lgtrain = lgb.Dataset(train_x, label=train_y)\n    lgval = lgb.Dataset(val_x, label=val_y)\n    evals_result = {}\n    model = lgb.train(parameters, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=200, evals_result=evals_result)\n    \n    pred_test_y = model.predict(test_x, num_iteration=model.best_iteration)\n    \n    return pred_test_y, model, evals_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**K fold cross validation for predictions in test set**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_fold = KFold(n_splits=5, shuffle=True, random_state=2020)\npred_test_final = 0\n\nfor d_ind, v_ind in k_fold.split(tr_x):\n    \n    d_x, v_x = tr_x.loc[d_ind, :], tr_x.loc[v_ind, :]\n    d_y, v_y = tr_y[d_ind], tr_y[v_ind]\n    pred_test, model, evals_result = run_lgb(d_x, d_y, v_x, v_y, te_x)\n    pred_test_final += pred_test\n    \npred_test_final /= 5\npred_test_final = np.expm1(pred_test_final)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating Submission file**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df = pd.DataFrame({\"ID\":test[\"ID\"].values, \"target\":pred_test_final})\nfinal_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature importance for LightGBM\nfig, ax = plt.subplots(figsize=(14,20))\nlgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n#ax.grid(False)\nplt.title(\"LightGBM - Feature Importance\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TODO: Hyperparameter Tuning","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}