{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-14T17:10:36.110365Z","iopub.execute_input":"2021-07-14T17:10:36.110894Z","iopub.status.idle":"2021-07-14T17:10:36.123467Z","shell.execute_reply.started":"2021-07-14T17:10:36.110759Z","shell.execute_reply":"2021-07-14T17:10:36.122545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.layers as Layer\n\nBASE_PATH = '../input/g2net-gravitational-wave-detection/'","metadata":{"execution":{"iopub.status.busy":"2021-07-14T17:10:36.124663Z","iopub.execute_input":"2021-07-14T17:10:36.125177Z","iopub.status.idle":"2021-07-14T17:10:43.374196Z","shell.execute_reply.started":"2021-07-14T17:10:36.12514Z","shell.execute_reply":"2021-07-14T17:10:43.373307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q nnAudio","metadata":{"execution":{"iopub.status.busy":"2021-07-14T03:45:43.663086Z","iopub.execute_input":"2021-07-14T03:45:43.663341Z","iopub.status.idle":"2021-07-14T03:45:51.307343Z","shell.execute_reply.started":"2021-07-14T03:45:43.663316Z","shell.execute_reply":"2021-07-14T03:45:51.306281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def npy_header_offset(npy_path):\n    with open(str(npy_path), 'rb') as f:\n        if f.read(6) != b'\\x93NUMPY':\n            raise ValueError('Invalid NPY file.')\n        version_major, version_minor = f.read(2)\n        if version_major == 1:\n            header_len_size = 2\n        elif version_major == 2:\n            header_len_size = 4\n        else:\n            raise ValueError('Unknown NPY file version {}.{}.'.format(version_major, version_minor))\n        header_len = sum(b << (8 * i) for i, b in enumerate(f.read(header_len_size)))\n        header = f.read(header_len)\n        if not header.endswith(b'\\n'):\n            raise ValueError('Invalid NPY file.')\n        return f.tell()\n\ndef apply_raw_path(row, is_train=True): \n    file_name = row[0]\n    if is_train:\n        return os.path.join(\n            BASE_PATH, 'train',\n            file_name[0],\n            file_name[1],\n            file_name[2],\n            file_name + \".npy\")\n    else:\n        return os.path.join(\n            BASE_PATH, 'test',\n            file_name[0],\n            file_name[1],\n            file_name[2],\n            file_name + \".npy\")","metadata":{"execution":{"iopub.status.busy":"2021-07-14T03:45:51.311665Z","iopub.execute_input":"2021-07-14T03:45:51.311975Z","iopub.status.idle":"2021-07-14T03:45:51.322423Z","shell.execute_reply.started":"2021-07-14T03:45:51.311942Z","shell.execute_reply":"2021-07-14T03:45:51.321504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom nnAudio.Spectrogram import CQT1992v2\n\ntransform=CQT1992v2(sr=2048, fmin=22, fmax=512, hop_length=64)\n\ndef cqt_2_rgb(data): # in order to use efficientnet we need 3 dimension images\n    data = data.numpy()\n    \n    wave0 = np.concatenate((data[0], data[0]))\n    wave1 = np.concatenate((data[1], data[1]))\n    wave2 = np.concatenate((data[2], data[2]))\n    \n    wave0 = transform(torch.from_numpy(wave0))\n    wave1 = transform(torch.from_numpy(wave1))\n    wave2 = transform(torch.from_numpy(wave2))\n    image = tf.convert_to_tensor([np.array(wave0)[0], np.array(wave1)[0], np.array(wave2)[0]])\n    image = tf.transpose(image,(1,2,0))\n    return image\n\ndef wave_transform(data):\n    data = tf.reshape(tf.io.decode_raw(data, tf.float64), (3,4096))\n    data = tf.cast(data, tf.float32)\n\n    data0 = data[0]/data[0][tf.argmax(data[0])]\n    data1 = data[1]/data[1][tf.argmax(data[1])]\n    data2 = data[2]/data[2][tf.argmax(data[2])]\n    data = tf.stack([data0, data1, data2])\n    \n    data = tf.py_function(\n            cqt_2_rgb,\n            [data],\n            [tf.float32])\n    \n    data = tf.convert_to_tensor(data[0])\n    data = tf.math.minimum(data, .99)\n\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-07-14T03:45:51.32458Z","iopub.execute_input":"2021-07-14T03:45:51.324966Z","iopub.status.idle":"2021-07-14T03:45:53.377789Z","shell.execute_reply.started":"2021-07-14T03:45:51.324927Z","shell.execute_reply":"2021-07-14T03:45:53.376906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = pd.read_csv(os.path.join(BASE_PATH, 'training_labels.csv'))\nsample_submission = pd.read_csv('../input/g2net-gravitational-wave-detection/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-14T03:45:53.378994Z","iopub.execute_input":"2021-07-14T03:45:53.379375Z","iopub.status.idle":"2021-07-14T03:45:53.557853Z","shell.execute_reply.started":"2021-07-14T03:45:53.379334Z","shell.execute_reply":"2021-07-14T03:45:53.557015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.efficientnet import *\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\nmodel = tf.keras.models.load_model('/kaggle/input/full-b5-dec95-4e4/full_b5_dec95_4e4')\n","metadata":{"execution":{"iopub.status.busy":"2021-07-14T17:10:43.375497Z","iopub.execute_input":"2021-07-14T17:10:43.37587Z","iopub.status.idle":"2021-07-14T17:11:26.71851Z","shell.execute_reply.started":"2021-07-14T17:10:43.37584Z","shell.execute_reply":"2021-07-14T17:11:26.717606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df['file_path'] = df.apply(apply_raw_path, args=(True,), axis=1)\n# df['target'] = df['target'].astype('float32')","metadata":{"execution":{"iopub.status.busy":"2021-07-14T03:46:37.436219Z","iopub.execute_input":"2021-07-14T03:46:37.436569Z","iopub.status.idle":"2021-07-14T03:46:37.442384Z","shell.execute_reply.started":"2021-07-14T03:46:37.436517Z","shell.execute_reply":"2021-07-14T03:46:37.441655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 64\n\ndf_test = sample_submission\ndf_test['file_path'] = df_test.apply(apply_raw_path, args=(False,), axis=1)\n\nds_test = tf.data.FixedLengthRecordDataset(\n    df_test['file_path'],\n    98304,\n    header_bytes=128,\n    num_parallel_reads=4).map(wave_transform)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-14T03:46:37.444582Z","iopub.execute_input":"2021-07-14T03:46:37.444967Z","iopub.status.idle":"2021-07-14T03:46:40.510795Z","shell.execute_reply.started":"2021-07-14T03:46:37.444931Z","shell.execute_reply":"2021-07-14T03:46:40.509968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_data = ds_test.take(5000).batch(BATCH_SIZE).prefetch(buffer_size=16)\n# preds = model.predict(test_data)\n# submission = pd.DataFrame({'id':sample_submission['id'][5000],'target':preds.reshape(-1)})","metadata":{"execution":{"iopub.status.busy":"2021-07-14T03:46:40.512254Z","iopub.execute_input":"2021-07-14T03:46:40.512604Z","iopub.status.idle":"2021-07-14T03:48:11.014628Z","shell.execute_reply.started":"2021-07-14T03:46:40.512567Z","shell.execute_reply":"2021-07-14T03:48:11.013382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = ds_test.batch(BATCH_SIZE).prefetch(buffer_size=16)\npreds = model.predict(test_data)\nsubmission = pd.DataFrame({'id':sample_submission['id'],'target':preds.reshape(-1)})","metadata":{"execution":{"iopub.status.busy":"2021-07-14T03:39:33.616426Z","iopub.execute_input":"2021-07-14T03:39:33.616728Z","iopub.status.idle":"2021-07-14T03:42:44.85941Z","shell.execute_reply.started":"2021-07-14T03:39:33.616701Z","shell.execute_reply":"2021-07-14T03:42:44.857281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T19:51:21.414064Z","iopub.execute_input":"2021-07-13T19:51:21.414346Z","iopub.status.idle":"2021-07-13T19:51:21.452757Z","shell.execute_reply.started":"2021-07-13T19:51:21.414319Z","shell.execute_reply":"2021-07-13T19:51:21.451688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[0:20]","metadata":{"execution":{"iopub.status.busy":"2021-07-13T19:52:58.957737Z","iopub.execute_input":"2021-07-13T19:52:58.958327Z","iopub.status.idle":"2021-07-13T19:52:58.97073Z","shell.execute_reply.started":"2021-07-13T19:52:58.958293Z","shell.execute_reply":"2021-07-13T19:52:58.96979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}