{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport math\nimport json\nfrom sklearn.model_selection import train_test_split\nimport tensorflow.keras as keras\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils import shuffle\nimport pandas as pd\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2022-06-22T03:21:11.228326Z","iopub.execute_input":"2022-06-22T03:21:11.228704Z","iopub.status.idle":"2022-06-22T03:21:11.235194Z","shell.execute_reply.started":"2022-06-22T03:21:11.228673Z","shell.execute_reply":"2022-06-22T03:21:11.234091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(data_path):\n    \"\"\"Loads training dataset from json file.\n        :param data_path (str): Path to json file containing data\n        :return X (ndarray): Inputs\n        :return y (ndarray): Targets\n    \"\"\"\n\n    with open(data_path, \"r\") as fp:\n        data = json.load(fp)\n\n    X = np.array(data[\"mfcc\"])\n    y = np.array(data[\"labels\"])\n    return X, y\n\n\ndef plot_history(history):\n    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n        :param history: Training history of model\n        :return:\n    \"\"\"\n\n    fig, axs = plt.subplots(2)\n\n    # create accuracy sublpot\n    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n    axs[0].plot(history.history[\"val_accuracy\"], label=\"validation accuracy\")\n    axs[0].set_ylabel(\"Accuracy\")\n    axs[0].legend(loc=\"lower right\")\n    axs[0].set_title(\"Accuracy eval\")\n\n    # create error sublpot\n    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n    axs[1].plot(history.history[\"val_loss\"], label=\"validation error\")\n    axs[1].set_ylabel(\"Error\")\n    axs[1].set_xlabel(\"Epoch\")\n    axs[1].legend(loc=\"upper right\")\n    #axs[1].set_title(\"Error eval\")\n\n    plt.show()\n\ndef build_model_lstm(input_shape):\n    \"\"\"Generates RNN-LSTM model\n    :param input_shape (tuple): Shape of input set\n    :return model: RNN-LSTM model\n    \"\"\"\n\n    # build network topology\n    model = keras.Sequential()\n\n    # 2 LSTM layers\n    model.add(keras.layers.LSTM(64, input_shape=input_shape, return_sequences=True))\n\n\n    #LSTM\n    model.add(keras.layers.LSTM(64,  return_sequences=True))\n    model.add(keras.layers.Dropout(0.3))\n    model.add(keras.layers.LSTM(64))\n    model.add(keras.layers.Dropout(0.3))\n\n    # dense layer\n    model.add(keras.layers.Dense(256, activation='relu'))\n    model.add(keras.layers.Dropout(0.3))\n\n\n\n    # output layer\n    model.add(keras.layers.Dense(152, activation='softmax'))\n\n    return model\n\ndef build_model_cnn(input_shape):\n\n    # Let's design the model architecture.\n    model = keras.models.Sequential([\n        \n        keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n        keras.layers.MaxPooling2D((2,2), padding='same'),\n        keras.layers.BatchNormalization(),\n\n        keras.layers.Conv2D(64, (3,3), activation='relu'),\n        keras.layers.MaxPooling2D((3,3), strides=(2,2), padding='same'),\n        keras.layers.BatchNormalization(),\n\n        keras.layers.Conv2D(32, (2,2), activation='relu'),\n        keras.layers.MaxPooling2D((3,3), strides=(2,2), padding='same'),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dropout(0.3),\n        \n        keras.layers.Flatten(),\n        keras.layers.Dense(64, activation='relu'), \n        keras.layers.Dense(152, activation='softmax')\n    ])\n\n    return model\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-22T03:16:23.892831Z","iopub.execute_input":"2022-06-22T03:16:23.893298Z","iopub.status.idle":"2022-06-22T03:16:23.911803Z","shell.execute_reply.started":"2022-06-22T03:16:23.893263Z","shell.execute_reply":"2022-06-22T03:16:23.90925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mfcc_path = '../input/feijai-mfcc-3'\n","metadata":{"execution":{"iopub.status.busy":"2022-06-22T03:16:31.118287Z","iopub.execute_input":"2022-06-22T03:16:31.118657Z","iopub.status.idle":"2022-06-22T03:16:31.123368Z","shell.execute_reply.started":"2022-06-22T03:16:31.118627Z","shell.execute_reply":"2022-06-22T03:16:31.122364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=[]\ny=[]\nmap=[]\ndf=pd.DataFrame()\nfor dirpath, dirnames, filenames in os.walk(mfcc_path):\n    for i, file in enumerate(filenames):\n        mfcc = np.load(dirpath+'/'+file)\n        label=file.split('.')[0]\n\n        if i==0:\n            X=mfcc\n        else:\n            X=np.concatenate((X,mfcc))\n        \n        tags=[i]*mfcc.shape[0]\n        #print(mfcc.shape[0])\n        y=np.concatenate((y, tags))\n        labels=[label]*mfcc.shape[0]\n        map=np.concatenate((map, labels))\n\ny = [int(y) for y in y]\ny = np.array(y)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T03:16:33.426521Z","iopub.execute_input":"2022-06-22T03:16:33.426945Z","iopub.status.idle":"2022-06-22T03:18:00.71809Z","shell.execute_reply.started":"2022-06-22T03:16:33.42691Z","shell.execute_reply":"2022-06-22T03:18:00.717068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CALL_BACK = EarlyStopping('val_loss',patience=3)\ntrain_X, train_y = X, y\n\n#shuffle避免validation的bias過大\ntrain_X, train_y=shuffle(train_X, train_y,random_state=0)\n\n\"\"\"\nkf = StratifiedKFold(n_splits=5,                      \n            random_state=10,\n            shuffle=True)\n            \nkf.get_n_splits(train_X,train_y)\n\"\"\"\nT_train_acc = []\nT_test_acc = []\nT_train_loss = []\nT_test_loss = []\n\n#for train_index, test_index in kf.split(train_X,train_y):\n    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    #X_train, X_test = train_X[train_index], train_X[test_index]\n    #y_train, y_test = train_y[train_index], train_y[test_index]\n\nX_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.2, random_state=10)   \n\n# LSTM network\ninput_shape = (X_train.shape[1],X_train.shape[2]) # X, 13\nmodel = build_model_lstm(input_shape)\n\n\"\"\"\n# CNN network\nshape_1 = X_train.shape[1]\nshape_2 = X_train.shape[2]\n\nX_train = X_train.reshape(X_train.shape[0], shape_1, shape_2, 1)\nX_test = X_test.reshape(X_test.shape[0], shape_1, shape_2, 1)\n\ninput_shape = (shape_1,shape_2,1)\nmodel = build_model_cnn(input_shape)\n\"\"\"\n    \n\n# compile model\noptimiser = keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(optimizer=optimiser,\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\nif (len(T_train_acc)) < 1:\n    model.summary()\n\n\n\n# train model\n\n\nhistory = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_split=0.2) #, callbacks=[CALL_BACK]\nT_train_acc.append(history.history['val_accuracy'][-1])\nT_train_loss.append(history.history['val_loss'][-1])\nprint('\\n\\n5-fold cross',len(T_train_acc))\nprint('\\nValidation accuracy:', history.history['val_accuracy'][-1])\nprint('\\nValidation loss:', history.history['val_loss'][-1])\n\n# plot accuracy/error for training and validation\nplot_history(history)\n\n# evaluate model on test set\ntest_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\nT_test_loss.append(test_loss)\nT_test_acc.append(test_acc)\nprint('\\nTest accuracy:', test_acc)\nprint('\\nTest loss:', test_loss)\n\nprint('\\n\\nMean of validation accuracy:')\nprint(np.mean(T_train_acc))\nprint('Mean of validation loss:')\nprint(np.mean(T_train_loss))\nprint('Mean of test accuracy:')\nprint(np.mean(T_test_acc))\nprint('Mean of test loss:')\nprint(np.mean(T_test_loss))","metadata":{"execution":{"iopub.status.busy":"2022-06-22T03:22:34.149775Z","iopub.execute_input":"2022-06-22T03:22:34.150163Z","iopub.status.idle":"2022-06-22T03:57:32.885841Z","shell.execute_reply.started":"2022-06-22T03:22:34.150133Z","shell.execute_reply":"2022-06-22T03:57:32.884914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('../BS_LSTM_100epoch_3.h5')","metadata":{"execution":{"iopub.status.busy":"2022-06-22T03:58:11.889269Z","iopub.execute_input":"2022-06-22T03:58:11.889822Z","iopub.status.idle":"2022-06-22T03:58:11.937963Z","shell.execute_reply.started":"2022-06-22T03:58:11.889779Z","shell.execute_reply":"2022-06-22T03:58:11.936929Z"},"trusted":true},"execution_count":null,"outputs":[]}]}