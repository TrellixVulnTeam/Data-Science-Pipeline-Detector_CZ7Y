{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nInf\n</b></h1> \n\n### **[FE](Melspec PCEN melspec**1.5)**","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nExp Info\n</b></h1> ","metadata":{}},{"cell_type":"markdown","source":"```\nD-300-TRAIN-EF_MELSPEC_PCEN-tf_efficientnet_b0_ns-SizeExp4-f0..4\nIMAGE_RESIZE_RATE=2.0\n\n\n0.731797545 ---low fold0\n0.737641303\n0.73400335\n0.731386122 ---low fold4\n0.73463218\n\nOOF:0.7338921\n```\n\n```\nZA-320-TRAIN-PCEN-tf_efficientnetv2_s_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f0\n0.742723319  ---low fold0\n0.75465313\n0.75177305\n0.745695364  ---low fold3\n0.764919222\n0.747076512  ---low fold5\n0.754109359\n0.753623188\n0.738317757  ---low fold8\n0.759451322\nOOF:0.751234222\n```\n\n```\nZA-500-TRAIN-PCEN-tf_efficientnetv2_l_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f0\n0.754087421\n0.755778894\n0.75276012   ---low fold2\n0.761329305\n0.75862069\n0.761050183\n0.769940672\n0.759133965\n0.750921891  ---low fold8\n0.752230611  ---low fold9\nOOF:0.757585375\n```\n\n```\nZA-300-TRAIN-PCEN-tf_efficientnet_b0_ns-Split10-Resizex20-BGM-DaExp0-AllFold-f0\n0.738659445  ---low fold0\n0.752422319\n0.748        ---low fold2\n0.732488326  ---low fold3\n0.755449591\n0.750830565\n0.755584757\n0.74775673\n0.741903172  ---low fold8\n0.75889859\nOOF:0.748199349\n```","metadata":{}},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nExternal Lib\n</b></h2> ","metadata":{}},{"cell_type":"code","source":"!pip install ../input/torchlibrosa/torchlibrosa-0.0.5-py3-none-any.whl > /dev/null","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:20:28.255665Z","iopub.execute_input":"2022-05-24T14:20:28.255963Z","iopub.status.idle":"2022-05-24T14:20:57.498975Z","shell.execute_reply.started":"2022-05-24T14:20:28.25593Z","shell.execute_reply":"2022-05-24T14:20:57.497901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport audioread\nimport logging\nimport os\nimport sys\nsys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\nimport random\nimport time\nimport warnings\n\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport soundfile as sf\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as torchdata\n\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom typing import List\nfrom typing import Optional\nfrom sklearn import metrics\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold\n\nfrom albumentations.core.transforms_interface import ImageOnlyTransform\nfrom torchlibrosa.stft import LogmelFilterBank, Spectrogram\nfrom torchlibrosa.augmentation import SpecAugmentation\nfrom tqdm import tqdm\n\nimport albumentations as A\nimport albumentations.pytorch.transforms as T\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:20:57.501585Z","iopub.execute_input":"2022-05-24T14:20:57.502287Z","iopub.status.idle":"2022-05-24T14:20:57.514403Z","shell.execute_reply.started":"2022-05-24T14:20:57.502236Z","shell.execute_reply":"2022-05-24T14:20:57.513358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = True  # type: ignore\n    torch.backends.cudnn.benchmark = True  # type: ignore\n    \n    \ndef get_logger(out_file=None):\n    logger = logging.getLogger()\n    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n    logger.handlers = []\n    logger.setLevel(logging.INFO)\n\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    handler.setLevel(logging.INFO)\n    logger.addHandler(handler)\n\n    if out_file is not None:\n        fh = logging.FileHandler(out_file)\n        fh.setFormatter(formatter)\n        fh.setLevel(logging.INFO)\n        logger.addHandler(fh)\n    logger.info(\"logger set up\")\n    return logger\n    \n    \n@contextmanager\ndef timer(name: str, logger: Optional[logging.Logger] = None):\n    t0 = time.time()\n    msg = f\"[{name}] start\"\n    if logger is None:\n        print(msg)\n    else:\n        logger.info(msg)\n    yield\n\n    msg = f\"[{name}] done in {time.time() - t0:.2f} s\"\n    if logger is None:\n        print(msg)\n    else:\n        logger.info(msg)\n\n\ndef get_device() -> torch.device:\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndevice = get_device()\nlogger = get_logger(\"main.log\")\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:20:57.51713Z","iopub.execute_input":"2022-05-24T14:20:57.517528Z","iopub.status.idle":"2022-05-24T14:20:57.539193Z","shell.execute_reply.started":"2022-05-24T14:20:57.517442Z","shell.execute_reply":"2022-05-24T14:20:57.538197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_layer(layer):\n    nn.init.xavier_uniform_(layer.weight)\n\n    if hasattr(layer, \"bias\"):\n        if layer.bias is not None:\n            layer.bias.data.fill_(0.)\n\n\ndef init_bn(bn):\n    bn.bias.data.fill_(0.)\n    bn.weight.data.fill_(1.0)\n\n\ndef init_weights(model):\n    classname = model.__class__.__name__\n    if classname.find(\"Conv2d\") != -1:\n        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n        model.bias.data.fill_(0)\n    elif classname.find(\"BatchNorm\") != -1:\n        model.weight.data.normal_(1.0, 0.02)\n        model.bias.data.fill_(0)\n    elif classname.find(\"GRU\") != -1:\n        for weight in model.parameters():\n            if len(weight.size()) > 1:\n                nn.init.orghogonal_(weight.data)\n    elif classname.find(\"Linear\") != -1:\n        model.weight.data.normal_(0, 0.01)\n        model.bias.data.zero_()\n\n\ndef interpolate(x: torch.Tensor, ratio: int):\n    \"\"\"Interpolate data in time domain. This is used to compensate the\n    resolution reduction in downsampling of a CNN.\n    Args:\n      x: (batch_size, time_steps, classes_num)\n      ratio: int, ratio to interpolate\n    Returns:\n      upsampled: (batch_size, time_steps * ratio, classes_num)\n    \"\"\"\n    (batch_size, time_steps, classes_num) = x.shape\n    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n    return upsampled\n\n\ndef pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n    is the same as the value of the last frame.\n    Args:\n      framewise_output: (batch_size, frames_num, classes_num)\n      frames_num: int, number of frames to pad\n    Outputs:\n      output: (batch_size, frames_num, classes_num)\n    \"\"\"\n    output = F.interpolate(\n        framewise_output.unsqueeze(1),\n        size=(frames_num, framewise_output.size(2)),\n        align_corners=True,\n        mode=\"bilinear\").squeeze(1)\n\n    return output\n\n\n\nclass AttBlockV2(nn.Module):\n    def __init__(self,\n                 in_features: int,\n                 out_features: int,\n                 activation=\"linear\"):\n        super().__init__()\n\n        self.activation = activation\n        self.att = nn.Conv1d(\n            in_channels=in_features,\n            out_channels=out_features,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True)\n        self.cla = nn.Conv1d(\n            in_channels=in_features,\n            out_channels=out_features,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True)\n\n        self.init_weights()\n\n    def init_weights(self):\n        init_layer(self.att)\n        init_layer(self.cla)\n\n    def forward(self, x):\n        # x: (n_samples, n_in, n_time)\n        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n        cla = self.nonlinear_transform(self.cla(x))\n        x = torch.sum(norm_att * cla, dim=2)\n        return x, norm_att, cla\n\n    def nonlinear_transform(self, x):\n        if self.activation == 'linear':\n            return x\n        elif self.activation == 'sigmoid':\n            return torch.sigmoid(x)\n\n\nclass TimmSED(nn.Module):\n    def __init__(self, base_model_name: str, pretrained=False, num_classes=24, in_channels=1):\n        super().__init__()\n\n        self.spec_augmenter = SpecAugmentation(time_drop_width=64//2, time_stripes_num=2,\n                                               freq_drop_width=8//2, freq_stripes_num=2)\n\n        self.bn0 = nn.BatchNorm2d(int(CFG.n_mels*CFG.IMAGE_RESIZE_RATE))\n\n        base_model = timm.create_model(\n            base_model_name, pretrained=pretrained, in_chans=in_channels)\n        layers = list(base_model.children())[:-2]\n        self.encoder = nn.Sequential(*layers)\n\n        if hasattr(base_model, \"fc\"):\n            in_features = base_model.fc.in_features\n        else:\n            in_features = base_model.classifier.in_features\n\n        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n        self.att_block = AttBlockV2(\n            in_features, num_classes, activation=\"sigmoid\")\n\n        self.init_weight()\n\n    def init_weight(self):\n        init_bn(self.bn0)\n        init_layer(self.fc1)\n        \n\n    def forward(self, input_data):\n        x = input_data # (batch_size, 3, time_steps, mel_bins)\n\n        frames_num = x.shape[2]\n\n        x = x.transpose(1, 3)\n        x = self.bn0(x)\n        x = x.transpose(1, 3)\n\n        if self.training:\n            if random.random() < 0.25:\n                x = self.spec_augmenter(x)\n\n        x = x.transpose(2, 3)\n\n        x = self.encoder(x)\n        \n        # Aggregate in frequency axis\n        x = torch.mean(x, dim=3)\n\n        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n        x = x1 + x2\n\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = x.transpose(1, 2)\n        x = F.relu_(self.fc1(x))\n        x = x.transpose(1, 2)\n        x = F.dropout(x, p=0.5, training=self.training)\n\n        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n        segmentwise_output = segmentwise_output.transpose(1, 2)\n\n        interpolate_ratio = frames_num // segmentwise_output.size(1)\n\n        # Get framewise output\n        framewise_output = interpolate(segmentwise_output,\n                                       interpolate_ratio)\n        framewise_output = pad_framewise_output(framewise_output, frames_num)\n\n        framewise_logit = interpolate(segmentwise_logit, interpolate_ratio)\n        framewise_logit = pad_framewise_output(framewise_logit, frames_num)\n\n        output_dict = {\n            'framewise_output': framewise_output,\n            'clipwise_output': clipwise_output,\n            'logit': logit,\n            'framewise_logit': framewise_logit,\n        }\n\n        return output_dict","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:20:57.543364Z","iopub.execute_input":"2022-05-24T14:20:57.543672Z","iopub.status.idle":"2022-05-24T14:20:57.585855Z","shell.execute_reply.started":"2022-05-24T14:20:57.543628Z","shell.execute_reply":"2022-05-24T14:20:57.584744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nTTA\n</b></h2> ","metadata":{}},{"cell_type":"code","source":"mean = (0.485, 0.456, 0.406) # RGB\nstd = (0.229, 0.224, 0.225) # RGB\n\nalbu_transforms = {\n    'train' : A.Compose([\n            A.HorizontalFlip(p=0.5),\n            A.OneOf([\n                A.Cutout(max_h_size=5, max_w_size=16),\n                A.CoarseDropout(max_holes=4),\n            ], p=0.5),\n            A.Normalize(mean, std),\n    ]),\n    'valid' : A.Compose([\n            A.Normalize(mean, std),\n    ]),\n}","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:20:57.587587Z","iopub.execute_input":"2022-05-24T14:20:57.588242Z","iopub.status.idle":"2022-05-24T14:20:57.602567Z","shell.execute_reply.started":"2022-05-24T14:20:57.588196Z","shell.execute_reply":"2022-05-24T14:20:57.601285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nCFG\n</b></h2> ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    IMAGE_RESIZE_RATE=2.0\n    EXP_ID = '018' \n\n    ######################\n    # Globals #\n    ######################\n    seed = 42\n    epochs = 5\n    train = True\n    folds = [0, 1, 2, 3, 4]\n    img_size = 224\n    main_metric = \"epoch_f1_at_03\"\n    minimize_metric = False\n\n    ######################\n    # Dataset #\n    ######################\n    transforms = {\n        \"train\": [{\"name\": \"Normalize\"}],\n        \"valid\": [{\"name\": \"Normalize\"}]\n    }\n    period = 5\n    n_mels = 224\n    fmin = 20\n    fmax = 16000\n    n_fft = 2048\n    hop_length = 512\n    sample_rate = 32000\n    melspectrogram_parameters = {\n        \"n_mels\": 224,\n        \"fmin\": 20,\n        \"fmax\": 16000\n    }\n\n    target_columns = 'afrsil1 akekee akepa1 akiapo akikik amewig aniani apapan arcter \\\n                      barpet bcnher belkin1 bkbplo bknsti bkwpet blkfra blknod bongul \\\n                      brant brnboo brnnod brnowl brtcur bubsan buffle bulpet burpar buwtea \\\n                      cacgoo1 calqua cangoo canvas caster1 categr chbsan chemun chukar cintea \\\n                      comgal1 commyn compea comsan comwax coopet crehon dunlin elepai ercfra eurwig \\\n                      fragul gadwal gamqua glwgul gnwtea golphe grbher3 grefri gresca gryfra gwfgoo \\\n                      hawama hawcoo hawcre hawgoo hawhaw hawpet1 hoomer houfin houspa hudgod iiwi incter1 \\\n                      jabwar japqua kalphe kauama laugul layalb lcspet leasan leater1 lessca lesyel lobdow lotjae \\\n                      madpet magpet1 mallar3 masboo mauala maupar merlin mitpar moudov norcar norhar2 normoc norpin \\\n                      norsho nutman oahama omao osprey pagplo palila parjae pecsan peflov perfal pibgre pomjae puaioh \\\n                      reccar redava redjun redpha1 refboo rempar rettro ribgul rinduc rinphe rocpig rorpar rudtur ruff \\\n                      saffin sander semplo sheowl shtsan skylar snogoo sooshe sooter1 sopsku1 sora spodov sposan \\\n                      towsol wantat1 warwhe1 wesmea wessan wetshe whfibi whiter whttro wiltur yebcar yefcan zebdov'.split()\n\n    ######################\n    # Loaders #\n    ######################\n    loader_params = {\n        \"train\": {\n            \"batch_size\": 32, \n            \"num_workers\": 0,\n            \"shuffle\": True\n        },\n        \"valid\": {\n            \"batch_size\": 64,\n            \"num_workers\": 0,\n            \"shuffle\": False\n        }\n    }\n\n    ######################\n    # Model #\n    ######################\n#     base_model_name = \"tf_efficientnet_b0_ns\"\n    pooling = \"max\"\n    pretrained = False\n    num_classes = 152\n    in_channels = 3\n\n    N_FOLDS = 5\n    LR = 1e-3\n    T_max=10\n    min_lr=1e-6","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:20:57.604615Z","iopub.execute_input":"2022-05-24T14:20:57.605246Z","iopub.status.idle":"2022-05-24T14:20:57.619984Z","shell.execute_reply.started":"2022-05-24T14:20:57.605197Z","shell.execute_reply":"2022-05-24T14:20:57.618832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUDIO_PATH = '../input/birdclef-2022/train_audio'\nCLASSES = sorted(os.listdir(AUDIO_PATH))\nNUM_CLASSES = len(CLASSES)\nclass AudioParams:\n    \"\"\"\n    Parameters used for the audio data\n    \"\"\"\n    sr = 32000\n    duration = 5\n    # Melspectrogram\n    n_mels = 224\n    fmin = 20\n    fmax = 16000","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:20:57.622676Z","iopub.execute_input":"2022-05-24T14:20:57.623711Z","iopub.status.idle":"2022-05-24T14:20:57.637945Z","shell.execute_reply.started":"2022-05-24T14:20:57.62359Z","shell.execute_reply":"2022-05-24T14:20:57.636981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalize_melspec(X: np.ndarray):\n    eps = 1e-6\n    mean = X.mean()\n    X = X - mean\n    std = X.std()\n    Xstd = X / (std + eps)\n    norm_min, norm_max = Xstd.min(), Xstd.max()\n    if (norm_max - norm_min) > eps:\n        V = Xstd\n        V[V < norm_min] = norm_min\n        V[V > norm_max] = norm_max\n        V = 255 * (V - norm_min) / (norm_max - norm_min)\n        V = V.astype(np.uint8)\n    else:\n        # Just zero\n        V = np.zeros_like(Xstd, dtype=np.uint8)\n    return V\n\n\n\"\"\"\n    Feature Engineering\n    * ch1:Melspec\n    * ch2:PCEN\n    * ch3:Melspec ** 1.5\n\"\"\"\ndef compute_melspec_pcen(y, params):\n    melspec = librosa.feature.melspectrogram(\n        y=y, sr=params.sr, n_mels=params.n_mels, fmin=params.fmin, fmax=params.fmax,\n    )\n    \n    pcen = librosa.pcen(melspec,\n                        sr=CFG.sample_rate,\n                        time_constant=0.06,\n                        eps=1e-6,\n                        gain=0.8,\n                        power=0.25,\n                        bias=10,\n                        hop_length=CFG.hop_length\n                       )\n    clean_mel = librosa.power_to_db(melspec ** 1.5)\n    \n    # Feature Normalize Ch3\n    melspec = librosa.power_to_db(melspec)\n    norm_melspec = normalize_melspec(melspec)\n    norm_pcen = normalize_melspec(pcen)\n    norm_clean_mel = normalize_melspec(clean_mel)\n    \n    # Convert Image\n    image = np.stack([norm_melspec, norm_pcen, norm_clean_mel], axis=-1)\n    height, width, _ = image.shape\n    image = cv2.resize(image, \n                   (int(int(width * 224 / height)*CFG.IMAGE_RESIZE_RATE), int(224*CFG.IMAGE_RESIZE_RATE))\n                  )\n    return image","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:20:57.64134Z","iopub.execute_input":"2022-05-24T14:20:57.642516Z","iopub.status.idle":"2022-05-24T14:20:57.6577Z","shell.execute_reply.started":"2022-05-24T14:20:57.642411Z","shell.execute_reply":"2022-05-24T14:20:57.656452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_melspec(y, params):\n    \"\"\"\n    Computes a mel-spectrogram and puts it at decibel scale\n    Arguments:\n        y {np array} -- signal\n        params {AudioParams} -- Parameters to use for the spectrogram. Expected to have the attributes sr, n_mels, f_min, f_max\n    Returns:\n        np array -- Mel-spectrogram\n    \"\"\"\n    melspec = librosa.feature.melspectrogram(\n        y=y, sr=params.sr, n_mels=params.n_mels, fmin=params.fmin, fmax=params.fmax,\n    )\n\n    melspec = librosa.power_to_db(melspec).astype(np.float32)\n    return melspec\n\n\ndef mono_to_color(X, eps=1e-6, mean=None, std=None):\n    \"\"\"\n    Converts a one channel array to a 3 channel one in [0, 255]\n    Arguments:\n        X {numpy array [H x W]} -- 2D array to convert\n    Keyword Arguments:\n        eps {float} -- To avoid dividing by 0 (default: {1e-6})\n        mean {None or np array} -- Mean for normalization (default: {None})\n        std {None or np array} -- Std for normalization (default: {None})\n    Returns:\n        numpy array [3 x H x W] -- RGB numpy array\n    \"\"\"\n    X = np.stack([X, X, X], axis=-1)\n\n    # Standardize\n    mean = mean or X.mean()\n    std = std or X.std()\n    X = (X - mean) / (std + eps)\n\n    # Normalize to [0, 255]\n    _min, _max = X.min(), X.max()\n\n    if (_max - _min) > eps:\n        V = np.clip(X, _min, _max)\n        V = 255 * (V - _min) / (_max - _min)\n        V = V.astype(np.uint8)\n    else:\n        V = np.zeros_like(X, dtype=np.uint8)\n\n    return V","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:20:57.65939Z","iopub.execute_input":"2022-05-24T14:20:57.660451Z","iopub.status.idle":"2022-05-24T14:20:57.673835Z","shell.execute_reply.started":"2022-05-24T14:20:57.660405Z","shell.execute_reply":"2022-05-24T14:20:57.672807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestDataset(torchdata.Dataset):\n    def __init__(self, df: pd.DataFrame, clip: np.ndarray):\n        self.df = df\n        # self.clip = clip\n        self.clip = np.concatenate([clip, clip, clip])\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx: int):\n        SR = 32000\n        sample = self.df.loc[idx, :]\n        row_id = sample.row_id\n\n        end_seconds = int(sample.seconds)\n        start_seconds = int(end_seconds - 5)\n        \n        # end_index = int(SR * (end_seconds + (self.train_period - 5) / 2) + len(self.clip) // 3)\n        # start_index = int(SR * (start_seconds - (self.train_period - 5) / 2) + len(self.clip) // 3)\n        \n        # y = self.clip[start_index:end_index].astype(np.float32)\n        image = self.clip[SR*start_seconds:SR*end_seconds].astype(np.float32)\n        image = np.nan_to_num(image)\n        \n#         image = compute_melspec(image, AudioParams)\n#         image = mono_to_color(image)\n#         image = image.astype(np.uint8)\n        image = compute_melspec_pcen(image, AudioParams)\n        image = image.astype(np.float32)\n\n\n        image = albu_transforms['valid'](image=image)['image'].T\n            \n        return {\n            \"image\": image,\n            \"row_id\": row_id,\n        }","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:20:57.679152Z","iopub.execute_input":"2022-05-24T14:20:57.680067Z","iopub.status.idle":"2022-05-24T14:20:57.692324Z","shell.execute_reply.started":"2022-05-24T14:20:57.680018Z","shell.execute_reply":"2022-05-24T14:20:57.691277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=#cbb></a>\n<h2 style=\"color: #6cb4e4; background: #dfefff;  box-shadow: 0px 0px 0px 5px #dfefff;  border: dashed 4px white;  padding: 0.2em 0.5em;\">\n<b>\nInf\n</b></h2> ","metadata":{}},{"cell_type":"code","source":"model_paths = [\n               '../input/birdclef2022-dc-d-300-20220504004758/D-300-TRAIN-EF_MELSPEC_PCEN-tf_efficientnet_b0_ns-SizeExp4-f0/fold-0.bin',\n#                '../input/birdclef2022-dc-d-300-20220504004758/D-300-TRAIN-EF_MELSPEC_PCEN-tf_efficientnet_b0_ns-SizeExp4-f1/fold-1.bin',\n#                '../input/birdclef2022-dc-d-300-20220504004758/D-300-TRAIN-EF_MELSPEC_PCEN-tf_efficientnet_b0_ns-SizeExp4-f2/fold-2.bin',\n#                '../input/birdclef2022-dc-d-300-20220504004758/D-300-TRAIN-EF_MELSPEC_PCEN-tf_efficientnet_b0_ns-SizeExp4-f3/fold-3.bin',\n               '../input/birdclef2022-dc-d-300-20220504004758/D-300-TRAIN-EF_MELSPEC_PCEN-tf_efficientnet_b0_ns-SizeExp4-f4/fold-4.bin',\n#                '../input/birdclef2022-dc-za-500-20220524030848/ZA-500-TRAIN-PCEN-tf_efficientnetv2_l_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f0/fold-0.bin',\n#                '../input/birdclef2022-dc-za-500-20220524030848/ZA-500-TRAIN-PCEN-tf_efficientnetv2_l_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f1/fold-1.bin',\n               '../input/birdclef2022-dc-za-500-20220524030848/ZA-500-TRAIN-PCEN-tf_efficientnetv2_l_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f2/fold-2.bin',\n#                '../input/birdclef2022-dc-za-500-20220524030848/ZA-500-TRAIN-PCEN-tf_efficientnetv2_l_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f3/fold-3.bin',\n#                '../input/birdclef2022-dc-za-500-20220524030848/ZA-500-TRAIN-PCEN-tf_efficientnetv2_l_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f4/fold-4.bin',\n#                '../input/birdclef2022-dc-za-500-20220524030848/ZA-500-TRAIN-PCEN-tf_efficientnetv2_l_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f5/fold-5.bin',\n#                '../input/birdclef2022-dc-za-500-20220524030848/ZA-500-TRAIN-PCEN-tf_efficientnetv2_l_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f6/fold-6.bin',\n#                '../input/birdclef2022-dc-za-500-20220524030848/ZA-500-TRAIN-PCEN-tf_efficientnetv2_l_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f7/fold-7.bin',\n               '../input/birdclef2022-dc-za-500-20220524030848/ZA-500-TRAIN-PCEN-tf_efficientnetv2_l_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f8/fold-8.bin',\n               '../input/birdclef2022-dc-za-500-20220524030848/ZA-500-TRAIN-PCEN-tf_efficientnetv2_l_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f9/fold-9.bin',\n               '../input/birdclef2022-dc-za-320-20220518154439/ZA-320-TRAIN-PCEN-tf_efficientnetv2_s_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f0/fold-0.bin',\n#                '../input/birdclef2022-dc-za-320-20220518154439/ZA-320-TRAIN-PCEN-tf_efficientnetv2_s_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f1/fold-1.bin',\n#                '../input/birdclef2022-dc-za-320-20220518154439/ZA-320-TRAIN-PCEN-tf_efficientnetv2_s_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f2/fold-2.bin',\n               '../input/birdclef2022-dc-za-320-20220518154439/ZA-320-TRAIN-PCEN-tf_efficientnetv2_s_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f3/fold-3.bin',\n#                '../input/birdclef2022-dc-za-320-20220518154439/ZA-320-TRAIN-PCEN-tf_efficientnetv2_s_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f4/fold-4.bin',\n               '../input/birdclef2022-dc-za-320-20220518154439/ZA-320-TRAIN-PCEN-tf_efficientnetv2_s_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f5/fold-5.bin',\n#                '../input/birdclef2022-dc-za-320-20220518154439/ZA-320-TRAIN-PCEN-tf_efficientnetv2_s_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f6/fold-6.bin',\n#                '../input/birdclef2022-dc-za-320-20220518154439/ZA-320-TRAIN-PCEN-tf_efficientnetv2_s_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f7/fold-7.bin',\n               '../input/birdclef2022-dc-za-320-20220518154439/ZA-320-TRAIN-PCEN-tf_efficientnetv2_s_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f8/fold-8.bin',\n#                '../input/birdclef2022-dc-za-320-20220518154439/ZA-320-TRAIN-PCEN-tf_efficientnetv2_s_in21ft1k-Split10-Resizex20-BGM-DaExp0-AllFold-f9/fold-9.bin',\n               '../input/birdclef2022-dc-za-300-20220517141400/ZA-300-TRAIN-PCEN-tf_efficientnet_b0_ns-Split10-Resizex20-BGM-DaExp0-AllFold-f0/fold-0.bin',\n#                '../input/birdclef2022-dc-za-300-20220517141400/ZA-300-TRAIN-PCEN-tf_efficientnet_b0_ns-Split10-Resizex20-BGM-DaExp0-AllFold-f1/fold-1.bin',\n               '../input/birdclef2022-dc-za-300-20220517141400/ZA-300-TRAIN-PCEN-tf_efficientnet_b0_ns-Split10-Resizex20-BGM-DaExp0-AllFold-f2/fold-2.bin',\n               '../input/birdclef2022-dc-za-300-20220517141400/ZA-300-TRAIN-PCEN-tf_efficientnet_b0_ns-Split10-Resizex20-BGM-DaExp0-AllFold-f3/fold-3.bin',\n#                '../input/birdclef2022-dc-za-300-20220517141400/ZA-300-TRAIN-PCEN-tf_efficientnet_b0_ns-Split10-Resizex20-BGM-DaExp0-AllFold-f4/fold-4.bin',\n#                '../input/birdclef2022-dc-za-300-20220517141400/ZA-300-TRAIN-PCEN-tf_efficientnet_b0_ns-Split10-Resizex20-BGM-DaExp0-AllFold-f5/fold-5.bin',\n#                '../input/birdclef2022-dc-za-300-20220517141400/ZA-300-TRAIN-PCEN-tf_efficientnet_b0_ns-Split10-Resizex20-BGM-DaExp0-AllFold-f6/fold-6.bin',\n#                '../input/birdclef2022-dc-za-300-20220517141400/ZA-300-TRAIN-PCEN-tf_efficientnet_b0_ns-Split10-Resizex20-BGM-DaExp0-AllFold-f7/fold-7.bin',\n               '../input/birdclef2022-dc-za-300-20220517141400/ZA-300-TRAIN-PCEN-tf_efficientnet_b0_ns-Split10-Resizex20-BGM-DaExp0-AllFold-f8/fold-8.bin',\n#                '../input/birdclef2022-dc-za-300-20220517141400/ZA-300-TRAIN-PCEN-tf_efficientnet_b0_ns-Split10-Resizex20-BGM-DaExp0-AllFold-f9/fold-9.bin',\n]\nmodel_arc = [\n             'tf_efficientnet_b0_ns',\n             'tf_efficientnet_b0_ns',\n#              'tf_efficientnetv2_l_in21ft1k',\n#              'tf_efficientnetv2_l_in21ft1k',\n#              'tf_efficientnetv2_l_in21ft1k',\n#              'tf_efficientnetv2_l_in21ft1k',\n#              'tf_efficientnetv2_l_in21ft1k',\n#              'tf_efficientnetv2_l_in21ft1k',\n#              'tf_efficientnetv2_l_in21ft1k',\n             'tf_efficientnetv2_l_in21ft1k',\n             'tf_efficientnetv2_l_in21ft1k',\n             'tf_efficientnetv2_l_in21ft1k',\n             'tf_efficientnetv2_s_in21ft1k',\n             'tf_efficientnetv2_s_in21ft1k',\n             'tf_efficientnetv2_s_in21ft1k',\n             'tf_efficientnetv2_s_in21ft1k',\n#              'tf_efficientnetv2_s_in21ft1k',\n#              'tf_efficientnetv2_s_in21ft1k',\n#              'tf_efficientnetv2_s_in21ft1k',\n#              'tf_efficientnetv2_s_in21ft1k',\n#              'tf_efficientnetv2_s_in21ft1k',\n#              'tf_efficientnetv2_s_in21ft1k',\n             'tf_efficientnet_b0_ns',\n             'tf_efficientnet_b0_ns',\n             'tf_efficientnet_b0_ns',\n             'tf_efficientnet_b0_ns',\n#              'tf_efficientnet_b0_ns',\n#              'tf_efficientnet_b0_ns',\n#              'tf_efficientnet_b0_ns',\n#              'tf_efficientnet_b0_ns',\n#              'tf_efficientnet_b0_ns',\n#              'tf_efficientnet_b0_ns',\n]","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:20:57.693822Z","iopub.execute_input":"2022-05-24T14:20:57.694422Z","iopub.status.idle":"2022-05-24T14:20:57.709906Z","shell.execute_reply.started":"2022-05-24T14:20:57.694375Z","shell.execute_reply":"2022-05-24T14:20:57.708712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = []\nfor _model_arc , p in zip(model_arc, model_paths):\n    model = TimmSED(\n        base_model_name=_model_arc,\n        pretrained=CFG.pretrained,\n        num_classes=CFG.num_classes,\n        in_channels=CFG.in_channels)\n    \n    model.to(device)\n    model.load_state_dict(torch.load(p,map_location='cuda:0'))\n    model.eval()\n    models.append(model)\n    \nprint()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:20:57.711846Z","iopub.execute_input":"2022-05-24T14:20:57.712574Z","iopub.status.idle":"2022-05-24T14:21:14.959991Z","shell.execute_reply.started":"2022-05-24T14:20:57.71253Z","shell.execute_reply":"2022-05-24T14:21:14.958801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TARGET_SR = 32000\nDATADIR = Path(\"../input/birdclef-2022/test_soundscapes/\")","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:21:14.965356Z","iopub.execute_input":"2022-05-24T14:21:14.968669Z","iopub.status.idle":"2022-05-24T14:21:14.97417Z","shell.execute_reply.started":"2022-05-24T14:21:14.968621Z","shell.execute_reply":"2022-05-24T14:21:14.972823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_audios = list(DATADIR.glob(\"*.ogg\"))\nsample_submission = pd.read_csv('../input/birdclef-2022/sample_submission.csv')\nsample_submission\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:21:14.976517Z","iopub.execute_input":"2022-05-24T14:21:14.977223Z","iopub.status.idle":"2022-05-24T14:21:15.016646Z","shell.execute_reply.started":"2022-05-24T14:21:14.977176Z","shell.execute_reply":"2022-05-24T14:21:15.015138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IS_SCORED_BIRD_W = 1.0\nISNOT_SCORED_BIRD_W = 1.0\n\nBIRD_WEIGHT=[\n    ISNOT_SCORED_BIRD_W, #'afrsil1'\t0\n    ISNOT_SCORED_BIRD_W, #'akekee'\t1\n    ISNOT_SCORED_BIRD_W, #'akepa1'\t2\n    IS_SCORED_BIRD_W, #'akiapo'\t3  ★Scored_bird\n    ISNOT_SCORED_BIRD_W, #'akikik'\t4\n    ISNOT_SCORED_BIRD_W, #'amewig'\t5\n    IS_SCORED_BIRD_W, #'aniani'\t6  ★Scored_bird\n    IS_SCORED_BIRD_W, #'apapan'\t7  ★Scored_bird\n    ISNOT_SCORED_BIRD_W, #'arcter'\t8\n    IS_SCORED_BIRD_W, #'barpet'\t9  ★Scored_bird\n    ISNOT_SCORED_BIRD_W, #'bcnher'\t10\n    ISNOT_SCORED_BIRD_W, #'belkin1'\t11\n    ISNOT_SCORED_BIRD_W, #'bkbplo'\t12\n    ISNOT_SCORED_BIRD_W, #'bknsti'\t13\n    ISNOT_SCORED_BIRD_W, #'bkwpet'\t14\n    ISNOT_SCORED_BIRD_W, #'blkfra'\t15\n    ISNOT_SCORED_BIRD_W, #'blknod'\t16\n    ISNOT_SCORED_BIRD_W, #'bongul'\t17\n    ISNOT_SCORED_BIRD_W, #'brant'\t18\n    ISNOT_SCORED_BIRD_W, #'brnboo'\t19\n    ISNOT_SCORED_BIRD_W, #'brnnod'\t20\n    ISNOT_SCORED_BIRD_W, #'brnowl'\t21\n    ISNOT_SCORED_BIRD_W, #'brtcur'\t22\n    ISNOT_SCORED_BIRD_W, #'bubsan'\t23\n    ISNOT_SCORED_BIRD_W, #'buffle'\t24\n    ISNOT_SCORED_BIRD_W, #'bulpet'\t25\n    ISNOT_SCORED_BIRD_W, #'burpar'\t26\n    ISNOT_SCORED_BIRD_W, #'buwtea'\t27\n    ISNOT_SCORED_BIRD_W, #'cacgoo1'\t28\n    ISNOT_SCORED_BIRD_W, #'calqua'\t29\n    ISNOT_SCORED_BIRD_W, #'cangoo'\t30\n    ISNOT_SCORED_BIRD_W, #'canvas'\t31\n    ISNOT_SCORED_BIRD_W, #'caster1'\t32\n    ISNOT_SCORED_BIRD_W, #'categr'\t33\n    ISNOT_SCORED_BIRD_W, #'chbsan'\t34\n    ISNOT_SCORED_BIRD_W, #'chemun'\t35\n    ISNOT_SCORED_BIRD_W, #'chukar'\t36\n    ISNOT_SCORED_BIRD_W, #'cintea'\t37\n    ISNOT_SCORED_BIRD_W, #'comgal1'\t38\n    ISNOT_SCORED_BIRD_W, #'commyn'\t39\n    ISNOT_SCORED_BIRD_W, #'compea'\t40\n    ISNOT_SCORED_BIRD_W, #'comsan'\t41\n    ISNOT_SCORED_BIRD_W, #'comwax'\t42\n    ISNOT_SCORED_BIRD_W, #'coopet'\t43\n    IS_SCORED_BIRD_W, #'crehon'\t44 ★Scored_bird\n    ISNOT_SCORED_BIRD_W, #'dunlin'\t45\n    IS_SCORED_BIRD_W, #'elepai'\t46 ★Scored_bird\n    IS_SCORED_BIRD_W, #'ercfra'\t47 ★Scored_bird\n    ISNOT_SCORED_BIRD_W, #'eurwig'\t48\n    ISNOT_SCORED_BIRD_W, #'fragul'\t49\n    ISNOT_SCORED_BIRD_W, #'gadwal'\t50\n    ISNOT_SCORED_BIRD_W, #'gamqua'\t51\n    ISNOT_SCORED_BIRD_W, #'glwgul'\t52\n    ISNOT_SCORED_BIRD_W, #'gnwtea'\t53\n    ISNOT_SCORED_BIRD_W, #'golphe'\t54\n    ISNOT_SCORED_BIRD_W, #'grbher3'\t55\n    ISNOT_SCORED_BIRD_W, #'grefri'\t56\n    ISNOT_SCORED_BIRD_W, #'gresca'\t57\n    ISNOT_SCORED_BIRD_W, #'gryfra'\t58\n    ISNOT_SCORED_BIRD_W, #'gwfgoo'\t59\n    IS_SCORED_BIRD_W, #'hawama'\t60 ★Scored_bird\n    ISNOT_SCORED_BIRD_W, #'hawcoo'\t61\n    IS_SCORED_BIRD_W, #'hawcre'\t62 ★Scored_bird\n    IS_SCORED_BIRD_W, #'hawgoo'\t63 ★Scored_bird\n    IS_SCORED_BIRD_W, #'hawhaw'\t64 ★Scored_bird\n    IS_SCORED_BIRD_W, #'hawpet1'\t65 ★Scored_bird\n    ISNOT_SCORED_BIRD_W, #'hoomer'\t66\n    IS_SCORED_BIRD_W, #'houfin'\t67 ★Scored_bird\n    ISNOT_SCORED_BIRD_W, #'houspa'\t68\n    ISNOT_SCORED_BIRD_W, #'hudgod'\t69\n    IS_SCORED_BIRD_W, #'iiwi'\t70 ★Scored_bird\n    ISNOT_SCORED_BIRD_W, #'incter1'\t71\n    IS_SCORED_BIRD_W, #'jabwar'\t72 ★Scored_bird\n    ISNOT_SCORED_BIRD_W, #'japqua'\t73\n    ISNOT_SCORED_BIRD_W, #'kalphe'\t74\n    ISNOT_SCORED_BIRD_W, #'kauama'\t75\n    ISNOT_SCORED_BIRD_W, #'laugul'\t76\n    ISNOT_SCORED_BIRD_W, #'layalb'\t77\n    ISNOT_SCORED_BIRD_W, #'lcspet'\t78\n    ISNOT_SCORED_BIRD_W, #'leasan'\t79\n    ISNOT_SCORED_BIRD_W, #'leater1'\t80\n    ISNOT_SCORED_BIRD_W, #'lessca'\t81\n    ISNOT_SCORED_BIRD_W, #'lesyel'\t82\n    ISNOT_SCORED_BIRD_W, #'lobdow'\t83\n    ISNOT_SCORED_BIRD_W, #'lotjae'\t84\n    ISNOT_SCORED_BIRD_W, #'madpet'\t85\n    ISNOT_SCORED_BIRD_W, #'magpet1'\t86\n    ISNOT_SCORED_BIRD_W, #'mallar3'\t87\n    ISNOT_SCORED_BIRD_W, #'masboo'\t88\n    ISNOT_SCORED_BIRD_W, #'mauala'\t89\n    IS_SCORED_BIRD_W, #'maupar'\t90 ★Scored_bird\n    ISNOT_SCORED_BIRD_W, #'merlin'\t91\n    ISNOT_SCORED_BIRD_W, #'mitpar'\t92\n    ISNOT_SCORED_BIRD_W, #'moudov'\t93\n    ISNOT_SCORED_BIRD_W, #'norcar'\t94\n    ISNOT_SCORED_BIRD_W, #'norhar2'\t95\n    ISNOT_SCORED_BIRD_W, #'normoc'\t96\n    ISNOT_SCORED_BIRD_W, #'norpin'\t97\n    ISNOT_SCORED_BIRD_W, #'norsho'\t98\n    ISNOT_SCORED_BIRD_W, #'nutman'\t99\n    ISNOT_SCORED_BIRD_W, #'oahama'\t100\n    IS_SCORED_BIRD_W, #'omao'\t101 ★Scored_bird\n    ISNOT_SCORED_BIRD_W, #'osprey'\t102\n    ISNOT_SCORED_BIRD_W, #'pagplo'\t103\n    ISNOT_SCORED_BIRD_W, #'palila'\t104\n    ISNOT_SCORED_BIRD_W, #'parjae'\t105\n    ISNOT_SCORED_BIRD_W, #'pecsan'\t106\n    ISNOT_SCORED_BIRD_W, #'peflov'\t107\n    ISNOT_SCORED_BIRD_W, #'perfal'\t108\n    ISNOT_SCORED_BIRD_W, #'pibgre'\t109\n    ISNOT_SCORED_BIRD_W, #'pomjae'\t110\n    IS_SCORED_BIRD_W, #'puaioh'\t111 ★Scored_bird\n    ISNOT_SCORED_BIRD_W, #'reccar'\t112\n    ISNOT_SCORED_BIRD_W, #'redava'\t113\n    ISNOT_SCORED_BIRD_W, #'redjun'\t114\n    ISNOT_SCORED_BIRD_W, #'redpha1'\t115\n    ISNOT_SCORED_BIRD_W, #'refboo'\t116\n    ISNOT_SCORED_BIRD_W, #'rempar'\t117\n    ISNOT_SCORED_BIRD_W, #'rettro'\t118\n    ISNOT_SCORED_BIRD_W, #'ribgul'\t119\n    ISNOT_SCORED_BIRD_W, #'rinduc'\t120\n    ISNOT_SCORED_BIRD_W, #'rinphe'\t121\n    ISNOT_SCORED_BIRD_W, #'rocpig'\t122\n    ISNOT_SCORED_BIRD_W, #'rorpar'\t123\n    ISNOT_SCORED_BIRD_W, #'rudtur'\t124\n    ISNOT_SCORED_BIRD_W, #'ruff'\t125\n    ISNOT_SCORED_BIRD_W, #'saffin'\t126\n    ISNOT_SCORED_BIRD_W, #'sander'\t127\n    ISNOT_SCORED_BIRD_W, #'semplo'\t128\n    ISNOT_SCORED_BIRD_W, #'sheowl'\t129\n    ISNOT_SCORED_BIRD_W, #'shtsan'\t130\n    IS_SCORED_BIRD_W, #'skylar'\t131 ★Scored_bird\n    ISNOT_SCORED_BIRD_W, #'snogoo'\t132\n    ISNOT_SCORED_BIRD_W, #'sooshe'\t133\n    ISNOT_SCORED_BIRD_W, #'sooter1'\t134\n    ISNOT_SCORED_BIRD_W, #'sopsku1'\t135\n    ISNOT_SCORED_BIRD_W, #'sora'\t136\n    ISNOT_SCORED_BIRD_W, #'spodov'\t137\n    ISNOT_SCORED_BIRD_W, #'sposan'\t138\n    ISNOT_SCORED_BIRD_W, #'towsol'\t139\n    ISNOT_SCORED_BIRD_W, #'wantat1'\t140\n    IS_SCORED_BIRD_W, #'warwhe1'\t141 ★Scored_bird\n    ISNOT_SCORED_BIRD_W, #'wesmea'\t142\n    ISNOT_SCORED_BIRD_W, #'wessan'\t143\n    ISNOT_SCORED_BIRD_W, #'wetshe'\t144\n    ISNOT_SCORED_BIRD_W, #'whfibi'\t145\n    ISNOT_SCORED_BIRD_W, #'whiter'\t146\n    ISNOT_SCORED_BIRD_W, #'whttro'\t147\n    ISNOT_SCORED_BIRD_W, #'wiltur'\t148\n    ISNOT_SCORED_BIRD_W, #'yebcar'\t149\n    IS_SCORED_BIRD_W, #'yefcan'\t150 ★Scored_bird\n    ISNOT_SCORED_BIRD_W, #'zebdov'\t151\n]","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:21:15.021892Z","iopub.execute_input":"2022-05-24T14:21:15.023129Z","iopub.status.idle":"2022-05-24T14:21:15.068455Z","shell.execute_reply.started":"2022-05-24T14:21:15.023082Z","shell.execute_reply":"2022-05-24T14:21:15.067159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(BIRD_WEIGHT)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:21:15.071523Z","iopub.execute_input":"2022-05-24T14:21:15.072553Z","iopub.status.idle":"2022-05-24T14:21:15.082845Z","shell.execute_reply.started":"2022-05-24T14:21:15.072502Z","shell.execute_reply":"2022-05-24T14:21:15.081264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INFERENCE_BS=32","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:21:15.086776Z","iopub.execute_input":"2022-05-24T14:21:15.087962Z","iopub.status.idle":"2022-05-24T14:21:15.098659Z","shell.execute_reply.started":"2022-05-24T14:21:15.087913Z","shell.execute_reply":"2022-05-24T14:21:15.096962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prediction_for_clip(test_df: pd.DataFrame, \n                        clip: np.ndarray, \n                        models, \n                        threshold=0.05, \n                        threshold_long=None):\n\n    \"\"\"\n        Init dataset\n    \"\"\"\n    dataset = TestDataset(df=test_df,clip=clip,)\n    loader = torchdata.DataLoader(dataset, batch_size=INFERENCE_BS, shuffle=False)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    \"\"\"\n        Init proba result.\n    \"\"\"\n    model_proba_dict = {}\n    for _a, model in enumerate(models):\n        model_proba_dict[str(_a)] = []\n    \n    # --------------------------------------------------- #\n    # Prediction looop\n    # * on dataset\n    # --------------------------------------------------- #\n    prediction_dict = {}\n    for data in tqdm(loader):\n        image = data['image'].to(device)\n\n        # --------------------------------------------------- #\n        # Ph.Prediction loop all models.\n        # * {<Model>:[<Proba>[<Proba>]]}\n        # --------------------------------------------------- #\n        with torch.no_grad():\n            probas = []\n            probas_long = []\n            for _i, model in enumerate(models):\n                print(\"# --------------------------------------------------------- #\")\n                print(\"[INFO]Prediction Model:\",_i)\n                print(\"# --------------------------------------------------------- #\")\n                with torch.cuda.amp.autocast():\n                    output = model(image)\n                probas.append(output['clipwise_output'].detach().cpu().numpy().reshape(-1))\n                model_proba_dict[str(_i)] = model_proba_dict[str(_i)]+output['clipwise_output'].detach().cpu().numpy().tolist()\n\n    # --------------------------------------------------- #\n    # Ph.Blending looop\n    # --------------------------------------------------- #\n    for index, row in test_df.iterrows():\n        row_id = row['row_id']\n        probas = []\n        \n        \"\"\"\n            bird weight\n        \"\"\"\n        for k in model_proba_dict.keys():\n            probas.append([x * y for (x, y) in zip(model_proba_dict[k][index], BIRD_WEIGHT)])\n        \n        \"\"\"\n            thresholding\n        \"\"\"\n        probas = np.array(probas)\n        if threshold_long is None:\n            events = probas.mean(0) >= threshold\n        else:\n            events = ((probas.mean(0) >= threshold).astype(int) \\\n                      + (probas_long.mean(0) >= threshold_long).astype(int)) >= 2\n        \n        \"\"\"\n            convert labels\n        \"\"\"\n        labels = np.argwhere(events).reshape(-1).tolist()\n        if len(labels) == 0:\n            prediction_dict[str(row_id)] = \"nocall\"\n        else:\n            labels_str_list = list(map(lambda x: CFG.target_columns[x], labels))\n            label_string = \" \".join(labels_str_list)\n            prediction_dict[str(row_id)] = label_string\n    return prediction_dict","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:21:15.101628Z","iopub.execute_input":"2022-05-24T14:21:15.102479Z","iopub.status.idle":"2022-05-24T14:21:15.14155Z","shell.execute_reply.started":"2022-05-24T14:21:15.102429Z","shell.execute_reply":"2022-05-24T14:21:15.139921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prediction(test_audios,\n               threshold=0.05, \n               threshold_long=None):\n    \n    # models = [model]\n    warnings.filterwarnings(\"ignore\")\n    prediction_dicts = {}\n    for audio_path in test_audios:\n        with timer(f\"Loading {str(audio_path)}\", logger):\n            clip, _ = sf.read(audio_path, always_2d=True)\n            clip = np.mean(clip, 1)\n            \n        seconds = []\n        row_ids = []\n        for second in range(5, 65, 5):\n            row_id = \"_\".join(audio_path.name.split(\".\")[:-1]) + f\"_{second}\"\n            seconds.append(second)\n            row_ids.append(row_id)\n        print(row_ids)\n        test_df = pd.DataFrame({\n            \"row_id\": row_ids,\n            \"seconds\": seconds\n        })\n        with timer(f\"Prediction on {audio_path}\", logger):\n            prediction_dict = prediction_for_clip(test_df,\n                                                  clip=clip,\n                                                  models=models,\n                                                  threshold=threshold, threshold_long=threshold_long)\n        prediction_dicts.update(prediction_dict)\n    return prediction_dicts","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:21:15.144139Z","iopub.execute_input":"2022-05-24T14:21:15.150159Z","iopub.status.idle":"2022-05-24T14:21:15.168328Z","shell.execute_reply.started":"2022-05-24T14:21:15.150113Z","shell.execute_reply":"2022-05-24T14:21:15.167289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold = 0.04\nthreshold_long = None # 0.05\n\nprediction_dicts = prediction(test_audios=all_audios,\n           threshold=threshold, \n           threshold_long=threshold_long)\n# print(prediction_dicts)\n\nfor i in range(len(sample_submission)):\n    sample = sample_submission.row_id[i]\n    key = sample.split(\"_\")[0] + \"_\" + sample.split(\"_\")[1] + \"_\" + sample.split(\"_\")[3]\n    target_bird = sample.split(\"_\")[2]\n#     print(key, target_bird)\n    if key in prediction_dicts:\n        sample_submission.iat[i, 1] = (target_bird in prediction_dicts[key])\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:21:15.175679Z","iopub.execute_input":"2022-05-24T14:21:15.179147Z","iopub.status.idle":"2022-05-24T14:21:18.393071Z","shell.execute_reply.started":"2022-05-24T14:21:15.179093Z","shell.execute_reply":"2022-05-24T14:21:18.392062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:21:18.394739Z","iopub.execute_input":"2022-05-24T14:21:18.395169Z","iopub.status.idle":"2022-05-24T14:21:18.406202Z","shell.execute_reply.started":"2022-05-24T14:21:18.395118Z","shell.execute_reply":"2022-05-24T14:21:18.404885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nEOF\n</b></h1> ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}