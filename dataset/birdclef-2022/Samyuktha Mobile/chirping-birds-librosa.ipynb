{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# /kaggle/input/birdclef-2022/sample_submission.csv\n# /kaggle/input/birdclef-2022/scored_birds.json\n# /kaggle/input/birdclef-2022/eBird_Taxonomy_v2021.csv\n# /kaggle/input/birdclef-2022/test.csv\n# /kaggle/input/birdclef-2022/train_metadata.csv\n# /kaggle/input/birdclef-2022/train_audio/bongul/XC516224.ogg","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-25T15:43:39.639516Z","iopub.execute_input":"2022-03-25T15:43:39.639804Z","iopub.status.idle":"2022-03-25T15:43:39.645073Z","shell.execute_reply.started":"2022-03-25T15:43:39.639771Z","shell.execute_reply":"2022-03-25T15:43:39.643798Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è üíÅüèª‚Äç‚ôÄÔ∏èüíÅüèª‚Äç‚ôÄÔ∏èüíÅüèª‚Äç‚ôÄÔ∏èüë©üèª‚Äçüî¨\n\n## **If you find this notebook helpful. please upvote!** \n\n","metadata":{}},{"cell_type":"markdown","source":"# BirdCLEF 2022 Audio Detection\n\n\n## About the competition:\n\nAs the ‚Äúextinction capital of the world,‚Äù Hawai'i has lost 68% of its bird species, the consequences of which can harm entire food chains. Researchers use population monitoring to understand how native birds react to changes in the environment and conservation efforts. But many of the remaining birds across the islands are isolated in difficult-to-access, high-elevation habitats. With physical monitoring difficult, scientists have turned to sound recordings. Known as bioacoustic monitoring, this approach could provide a passive, low labor, and cost-effective strategy for studying endangered bird populations.\n\n![Image](https://storage.googleapis.com/kaggle-competitions/kaggle/33246/logos/header.png?t=2022-02-08-17-06-27)\n\nCurrent methods for processing large bioacoustic datasets involve manual annotation of each recording. This requires specialized training and prohibitively large amounts of time. Thankfully, recent advances in machine learning have made it possible to automatically identify bird songs for common species with ample training data. However, it remains challenging to develop such tools for rare and endangered species, such as those in Hawai'i.\n\nThe Cornell Lab of Ornithology's K. Lisa Yang Center for Conservation Bioacoustics (KLY-CCB) develops and applies innovative conservation technologies across multiple ecological scales to inspire and inform the conservation of wildlife and habitats. KLY-CCB does this by collecting and interpreting sounds in nature and they've joined forces with Google Bioacoustics Group, LifeCLEF, Listening Observatory for Hawaiian Ecosystems (LOHE) Bioacoustics Lab at the University of Hawai'i at Hilo, and Xeno-Canto for this competition.\n\nIn this competition, you‚Äôll use your machine learning skills to identify bird species by sound. Specifically, you'll develop a model that can process continuous audio data and then acoustically recognize the species. The best entries will be able to train reliable classifiers with limited training data.\n\nIf successful, you'll help advance the science of bioacoustics and support ongoing research to protect endangered Hawaiian birds. Thanks to your innovations, it will be easier for researchers and conservation practitioners to accurately survey population trends. They'll be able to regularly and more effectively evaluate threats and adjust their conservation actions.\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Data Description\n\nYour challenge in this competition is to identify which birds are calling in long recordings given quite limited training data. This is the exact challenge faced by scientists trying to monitor rare birds in Hawaii. For example, there are only a few thousand individual Nene geese left in the world, which makes it difficult to acquire recordings of their calls.\n\nThis competition uses a hidden test. When your submitted notebook is scored, the actual test data (including a sample submission) will be availabe to your notebook.\n\n![Bird](https://storage.googleapis.com/kaggle-media/competitions/Birdsong/Screen%20Shot%202022-02-08%20at%202.04.09%20PM.png)\n\n## Files\n\n* train_metadata.csv - A wide range of metadata is provided for the training data. The most directly relevant fields are:\n\n        - primary_label - a code for the bird species. You can review detailed information about the bird codes by appending the code to https://ebird.org/species/, such as https://ebird.org/species/amecro for the American Crow.\n        - secondary_labels: Background species as annotated by the recordist. An empty list does not mean that no background birds are audible.\n        - author - the eBird user who provided the recording.\n        - filename: the associated audio file.\n        - rating: Float value between 0.0 and 5.0 as an indicator of the quality rating on Xeno-canto and the number of background species, where 5.0 is the highest and 1.0 is the lowest. 0.0 means that this recording has no user rating yet.\n        \n \n* train_audio/ - The bulk of the training data consists of short recordings of individual bird calls generously uploaded by users of xenocanto.org. These files have been downsampled to 32 kHz where applicable to match the test set audio and converted to the ogg format.\n\n* test_soundscapes/ - When you submit a notebook, the test_soundscapes directory will be populated with approximately 5,500 recordings to be used for scoring. These are each within a few milliseconds of 1 minute long and in the ogg audio format. Only one soundscape is available for download.\n\n* test.csv - Metadata for the test set. Only the first three rows are available for download; the full test.csv is provided in the hidden test set.\n\n        - row_id - A unique identifier for the row.\n        - file_id - A unique identifier for the audio file.\n        - bird - The ebird code for the row. There is one row for each of the scored species per 5 second window per audio file.\n        - end_time - The last second of the 5 second time window (5, 10, 15, etc).\n        \n\n* sample_submission.csv - A valid sample submission. Only the first three rows are available for download; the full submission.csv is provided in the hidden test set.\n\n        - row_id - A unique identifier for the row.\n        - target - True/False for whether or not the bird in question called during the 5 second window.\n        \n        \n* scored_birds.json - The subset of the species in the dataset that are scored.\n\n* eBird_Taxonomy_v2021.csv - Data on the relationships between different species.","metadata":{}},{"cell_type":"markdown","source":"## Load the data ","metadata":{}},{"cell_type":"code","source":"main_dir = '../input/birdclef-2022/'\ntrain = pd.read_csv(main_dir + 'train_metadata.csv')\ntrain.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T15:43:39.681606Z","iopub.execute_input":"2022-03-25T15:43:39.68187Z","iopub.status.idle":"2022-03-25T15:43:39.812959Z","shell.execute_reply.started":"2022-03-25T15:43:39.681844Z","shell.execute_reply":"2022-03-25T15:43:39.812169Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(f):\n    t = pd.read_csv(main_dir+str(f))\n    return t","metadata":{"execution":{"iopub.status.busy":"2022-03-25T15:43:39.814403Z","iopub.execute_input":"2022-03-25T15:43:39.814631Z","iopub.status.idle":"2022-03-25T15:43:39.819446Z","shell.execute_reply.started":"2022-03-25T15:43:39.814604Z","shell.execute_reply":"2022-03-25T15:43:39.818688Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = load_data('train_metadata.csv')\ntrain_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T15:43:39.821068Z","iopub.execute_input":"2022-03-25T15:43:39.821349Z","iopub.status.idle":"2022-03-25T15:43:39.902304Z","shell.execute_reply.started":"2022-03-25T15:43:39.821311Z","shell.execute_reply":"2022-03-25T15:43:39.901462Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T15:43:39.904298Z","iopub.execute_input":"2022-03-25T15:43:39.904912Z","iopub.status.idle":"2022-03-25T15:43:39.936016Z","shell.execute_reply.started":"2022-03-25T15:43:39.904871Z","shell.execute_reply":"2022-03-25T15:43:39.93504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss = load_data('sample_submission.csv')\nss.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T15:43:39.937304Z","iopub.execute_input":"2022-03-25T15:43:39.937898Z","iopub.status.idle":"2022-03-25T15:43:39.953781Z","shell.execute_reply.started":"2022-03-25T15:43:39.93785Z","shell.execute_reply":"2022-03-25T15:43:39.952098Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = load_data('test.csv')\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T15:43:39.95486Z","iopub.execute_input":"2022-03-25T15:43:39.955576Z","iopub.status.idle":"2022-03-25T15:43:39.972039Z","shell.execute_reply.started":"2022-03-25T15:43:39.955542Z","shell.execute_reply":"2022-03-25T15:43:39.971206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Librosa : Audio data handling \n\nlibrosa is a python package for music and audio analysis.Librosa is basically used when we work with audio data like in music generation(using LSTM's), Automatic Speech Recognition. It provides the building blocks necessary to create the music information retrieval systems.librosa uses soundfile and audioread to load audio files. Note that soundfile does not currently support MP3, which will cause librosa to fall back on the audioread library.\n\nMore about audio data hands-on: \n\n* [Hands-On Guide To Librosa For Handling Audio Files](https://analyticsindiamag.com/hands-on-guide-to-librosa-for-handling-audio-files/)\n* [Visualizing Sounds Using Librosa Machine Learning Library!](https://analyticsvidhya.com/blog/2021/06/visualizing-sounds-librosa/)","metadata":{}},{"cell_type":"markdown","source":"*Let's load data from audio folders*","metadata":{}},{"cell_type":"code","source":"train_audio_path = main_dir+'train_audio/'\ntest_audio_path = main_dir+'test_soundscapes/'\nsample_audio_path = '/kaggle/input/birdclef-2022/train_audio/bongul/XC516224.ogg'","metadata":{"execution":{"iopub.status.busy":"2022-03-25T15:43:39.973202Z","iopub.execute_input":"2022-03-25T15:43:39.973791Z","iopub.status.idle":"2022-03-25T15:43:39.978427Z","shell.execute_reply.started":"2022-03-25T15:43:39.973734Z","shell.execute_reply":"2022-03-25T15:43:39.97762Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import librosa\n# audio_data = y, sample_rate = sr\ny, sr = librosa.load(sample_audio_path)\nprint('audio_data',y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T15:44:05.333881Z","iopub.execute_input":"2022-03-25T15:44:05.334149Z","iopub.status.idle":"2022-03-25T15:44:07.459948Z","shell.execute_reply.started":"2022-03-25T15:44:05.334119Z","shell.execute_reply":"2022-03-25T15:44:07.459093Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What does ‚Äústft‚Äù mean in the code?\nStft is a short form for Short-Time Fourier Transform. As mentioned in Librosa official documentation, ‚ÄúThe STFT represents a signal in the time-frequency domain by computing discrete Fourier transforms (DFT) over short overlapping windows‚Äù (Librosa Development Team, 2021)","metadata":{}},{"cell_type":"code","source":"D = librosa.stft(y)\nD","metadata":{"execution":{"iopub.status.busy":"2022-03-25T15:44:07.461793Z","iopub.execute_input":"2022-03-25T15:44:07.462023Z","iopub.status.idle":"2022-03-25T15:44:07.527018Z","shell.execute_reply.started":"2022-03-25T15:44:07.461995Z","shell.execute_reply":"2022-03-25T15:44:07.526325Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What does ‚ÄúChroma‚Äù mean?\n\nChroma is a type of transformation of sounds into numerical values. The majority of the time, Chroma can become a vector data type. A synopsis of Chroma history includes the process of feature extraction and can\nbecome a vital part of data engineering.","metadata":{}},{"cell_type":"code","source":"s = np.abs(librosa.stft(y)**2) # Get magnitude of stft\nchroma = librosa.feature.chroma_stft(S = s, sr = sr)\nprint(chroma)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T15:44:07.528173Z","iopub.execute_input":"2022-03-25T15:44:07.528372Z","iopub.status.idle":"2022-03-25T15:44:07.725694Z","shell.execute_reply.started":"2022-03-25T15:44:07.528348Z","shell.execute_reply":"2022-03-25T15:44:07.724825Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##cumulative sum function involves adding values of a specific axis.\nchroma = np.cumsum(chroma)\n\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-chroma, chroma)\nplt.plot(x , np.sin(x))\nplt.xlabel('Angle [rad]')\nplt.ylabel('sin(x)')\nplt.axis('tight')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T15:44:07.727568Z","iopub.execute_input":"2022-03-25T15:44:07.728229Z","iopub.status.idle":"2022-03-25T15:44:55.420687Z","shell.execute_reply.started":"2022-03-25T15:44:07.728181Z","shell.execute_reply":"2022-03-25T15:44:55.419799Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If the audio file inside this code were to become replaced with another file, shapes, and movements inside the graph would be different and vary as features of each element of sound can change. ","metadata":{}},{"cell_type":"code","source":"import librosa.display\nchroma_orig = librosa.feature.chroma_cqt(y,sr)\n\nidx = tuple([slice(None),slice(*list(librosa.time_to_frames([45,60])))])\nC = np.abs(librosa.cqt(y=y, sr=sr, bins_per_octave=12*3, n_bins=7*12*3))\nfig, ax = plt.subplots(nrows=2, sharex=True)\nimg1 = librosa.display.specshow(librosa.amplitude_to_db(C, ref=np.max)[idx],\n                                y_axis='cqt_note', x_axis='time', bins_per_octave=12*3,\n                                ax=ax[0])\nfig.colorbar(img1, ax=[ax[0]], format=\"%+2.f dB\")\nax[0].label_outer()\n\nimg2 = librosa.display.specshow(chroma_orig[idx], y_axis='chroma', x_axis='time', ax=ax[1])\nfig.colorbar(img2, ax=[ax[1]])\nax[1].set(ylabel='Default chroma')","metadata":{"execution":{"iopub.status.busy":"2022-03-25T15:45:30.721699Z","iopub.execute_input":"2022-03-25T15:45:30.722151Z","iopub.status.idle":"2022-03-25T15:45:33.547842Z","shell.execute_reply.started":"2022-03-25T15:45:30.722115Z","shell.execute_reply":"2022-03-25T15:45:33.5472Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can do better by isolating the harmonic component of the audio signal. We‚Äôll use a large margin for separating harmonics from percussives:\n\n","metadata":{}},{"cell_type":"code","source":"y_harm = librosa.effects.harmonic(y=y, margin=8)\nchroma_harm = librosa.feature.chroma_cqt(y=y_harm, sr=sr)\n\n\nfig, ax = plt.subplots(nrows=2, sharex=True, sharey=True)\nlibrosa.display.specshow(chroma_orig[idx], y_axis='chroma', x_axis='time', ax=ax[0])\nax[0].set(ylabel='Default chroma')\nax[0].label_outer()\n\nlibrosa.display.specshow(chroma_harm[idx], y_axis='chroma', x_axis='time', ax=ax[1])\nax[1].set(ylabel='Harmonic')","metadata":{"execution":{"iopub.status.busy":"2022-03-25T15:45:33.549029Z","iopub.execute_input":"2022-03-25T15:45:33.549539Z","iopub.status.idle":"2022-03-25T15:45:39.532841Z","shell.execute_reply.started":"2022-03-25T15:45:33.549504Z","shell.execute_reply":"2022-03-25T15:45:39.531802Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There‚Äôs still some noise in there though. We can clean it up using non-local filtering. This effectively removes any sparse additive noise from the features.","metadata":{}},{"cell_type":"code","source":"chroma_filter = np.minimum(chroma_harm,\n                           librosa.decompose.nn_filter(chroma_harm,\n                                                       aggregate=np.median,\n                                                       metric='cosine'))\n\n\nfig, ax = plt.subplots(nrows=2, sharex=True, sharey=True)\nlibrosa.display.specshow(chroma_harm[idx], y_axis='chroma', x_axis='time', ax=ax[0])\nax[0].set(ylabel='Harmonic')\nax[0].label_outer()\n\nlibrosa.display.specshow(chroma_filter[idx], y_axis='chroma', x_axis='time', ax=ax[1])\nax[1].set(ylabel='Non-local')","metadata":{"execution":{"iopub.status.busy":"2022-03-25T15:46:35.338222Z","iopub.execute_input":"2022-03-25T15:46:35.338693Z","iopub.status.idle":"2022-03-25T15:46:37.561327Z","shell.execute_reply.started":"2022-03-25T15:46:35.338644Z","shell.execute_reply":"2022-03-25T15:46:37.56049Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Local discontinuities and transients can be suppressed by using a horizontal median filter.","metadata":{}},{"cell_type":"code","source":"import scipy\nchroma_smooth = scipy.ndimage.median_filter(chroma_filter, size=(1, 9))\n\n\nfig, ax = plt.subplots(nrows=2, sharex=True, sharey=True)\nlibrosa.display.specshow(chroma_filter[idx], y_axis='chroma', x_axis='time', ax=ax[0])\nax[0].set(ylabel='Non-local')\nax[0].label_outer()\n\nlibrosa.display.specshow(chroma_smooth[idx], y_axis='chroma', x_axis='time', ax=ax[1])\nax[1].set(ylabel='Median-filtered')","metadata":{"execution":{"iopub.status.busy":"2022-03-25T15:47:24.829616Z","iopub.execute_input":"2022-03-25T15:47:24.829938Z","iopub.status.idle":"2022-03-25T15:47:25.269358Z","shell.execute_reply.started":"2022-03-25T15:47:24.829903Z","shell.execute_reply":"2022-03-25T15:47:25.268717Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A final comparison between the CQT, original chromagram and the result of our filtering.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=3, sharex=True)\nlibrosa.display.specshow(librosa.amplitude_to_db(C, ref=np.max)[idx],\n                         y_axis='cqt_note', x_axis='time',\n                         bins_per_octave=12*3, ax=ax[0])\nax[0].set(ylabel='CQT')\nax[0].label_outer()\nlibrosa.display.specshow(chroma_orig[idx], y_axis='chroma', x_axis='time', ax=ax[1])\nax[1].set(ylabel='Default chroma')\nax[1].label_outer()\nlibrosa.display.specshow(chroma_smooth[idx], y_axis='chroma', x_axis='time', ax=ax[2])\nax[2].set(ylabel='Processed')","metadata":{"execution":{"iopub.status.busy":"2022-03-25T15:47:51.813877Z","iopub.execute_input":"2022-03-25T15:47:51.814503Z","iopub.status.idle":"2022-03-25T15:47:52.475556Z","shell.execute_reply.started":"2022-03-25T15:47:51.814463Z","shell.execute_reply":"2022-03-25T15:47:52.474791Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Chroma variants\n\nThere are three chroma variants implemented in librosa: chroma_stft, chroma_cqt, and chroma_cens. chroma_stft and chroma_cqt are two alternative ways of plotting chroma. chroma_stft performs short-time fourier transform of an audio input and maps each STFT bin to chroma, while chroma_cqt uses constant-Q transform and maps each cq-bin to chroma.\n\nA comparison between the STFT and the CQT methods for chromagram.\n\n","metadata":{}},{"cell_type":"code","source":"chromagram_stft = librosa.feature.chroma_stft(y=y, sr=sr)\nchromagram_cqt = librosa.feature.chroma_cqt(y=y, sr=sr)\n\n\nfig, ax = plt.subplots(nrows=2, sharex=True, sharey=True)\nlibrosa.display.specshow(chromagram_stft[idx], y_axis='chroma', x_axis='time', ax=ax[0])\nax[0].set(ylabel='STFT')\nax[0].label_outer()\n\nlibrosa.display.specshow(chromagram_cqt[idx], y_axis='chroma', x_axis='time', ax=ax[1])\nax[1].set(ylabel='CQT')","metadata":{"execution":{"iopub.status.busy":"2022-03-25T15:49:02.858215Z","iopub.execute_input":"2022-03-25T15:49:02.858868Z","iopub.status.idle":"2022-03-25T15:49:04.721198Z","shell.execute_reply.started":"2022-03-25T15:49:02.858822Z","shell.execute_reply":"2022-03-25T15:49:04.720255Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CENS features (chroma_cens) are variants of chroma features introduced in M√ºller and Ewart, 2011, in which additional post processing steps are performed on the constant-Q chromagram to obtain features that are invariant to dynamics and timbre.\n\nThus, the CENS features are useful for applications, such as audio matching and retrieval.\n\n**Following steps are additional processing done on the chromagram, and are implemented in chroma_cens:**\n\n    1. L1-Normalization across each chroma vector\n\n    2. Quantization of the amplitudes based on ‚Äúlog-like‚Äù amplitude thresholds\n\n    3. Smoothing with sliding window (optional parameter)\n\n    4. Downsampling (not implemented)\n\nA comparison between the original constant-Q chromagram and the CENS features.\n\n","metadata":{}},{"cell_type":"code","source":"chromagram_cens = librosa.feature.chroma_cens(y=y, sr=sr)\n\n\nfig, ax = plt.subplots(nrows=2, sharex=True, sharey=True)\nlibrosa.display.specshow(chromagram_cqt[idx], y_axis='chroma', x_axis='time', ax=ax[0])\nax[0].set(ylabel='Orig')\n\nlibrosa.display.specshow(chromagram_cens[idx], y_axis='chroma', x_axis='time', ax=ax[1])\nax[1].set(ylabel='CENS')","metadata":{"execution":{"iopub.status.busy":"2022-03-25T15:50:08.920792Z","iopub.execute_input":"2022-03-25T15:50:08.921052Z","iopub.status.idle":"2022-03-25T15:50:10.517403Z","shell.execute_reply.started":"2022-03-25T15:50:08.921024Z","shell.execute_reply":"2022-03-25T15:50:10.516446Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Takeaways:\n\n    * Audio files can translate to visuals without the creation of data tables.\n    \n    * Librosa can generate many views of audio files and become interpreted accordingly.\n    \n    * Trigonometry and general math are appropriate for sound analytics.\n    \n    * Depending on the demographics of listeners who intentionally or unintentionally listen to audio files of this nature, opinions, and interpretations can vary from each individual.","metadata":{}},{"cell_type":"markdown","source":"![Chirping birds](https://tf-cmsv2-smithsonianmag-media.s3.amazonaws.com/filer/52/a7/52a72deb-8158-40cf-ac7e-75f794124a78/sparrowsongcovidstoryimage.jpg)\n\n## ‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è üíÅüèª‚Äç‚ôÄÔ∏èüíÅüèª‚Äç‚ôÄÔ∏èüíÅüèª‚Äç‚ôÄÔ∏èüë©üèª‚Äçüî¨\n\n## **If you find this note book helpful. please upvote!** \n\n## ‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è","metadata":{}},{"cell_type":"markdown","source":"### Work in Progress!","metadata":{}}]}