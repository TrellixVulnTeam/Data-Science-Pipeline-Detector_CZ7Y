{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch as tc\nimport os,math,glob,tqdm,ujson,librosa,random,time\nimport torch.nn as nn\nimport pandas as pd\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\nclass my_dic:\n    def __init__(self) -> None:\n        self.data=[]\n        self.dict={}\n    \n    def load_data(self):\n        self.dict=dict(zip(self.data,range(len(self.data))))\n        self.undict=dict(zip(range(len(self.data)),self.data))\n\n    def ck(self,char):\n        if char not in self.data:\n            self.data.append(char)\n    \n    def transform(self,str:list) ->tc.Tensor:\n        if len(self.dict)!=len(self.data):\n            self.load_data()\n        \n        tenser=tc.zeros(len(str),len(self.dict))\n        for x in range(tenser.shape[0]):\n            str[x]\n            tenser[x,self.dict[str[x]]]=1\n        return tenser\n    \n    def untransform(self,ts:tc.Tensor):\n        data=[]\n        tc=ts==1\n        data=np.array(self.data)\n        data=data[tc]\n        return data\n\n    def save(self,filepath):\n        with open(filepath+\"dc\",\"w\")as file:\n            ujson.dump(self.data,file)\n    \n    def load(self,filepath):\n        with open(filepath+\"dc\",\"r\")as file:\n            self.data=ujson.load(file)\n            self.load_data()     \n   \nclass Module(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.optimizer_list=[]\n    \n    def init(self,pack:list,optimize=tc.optim.AdamW):\n        self.Model_pack=pack\n        self.optimizer_list=[optimize(x.parameters()) for x in pack]\n    \n    def load_checkpoint(self,checkpoint):\n        if not os.path.exists(checkpoint):\n            print(\"load error checkpoint\")\n            return None\n        if checkpoint != 'No':\n            print(\"loading checkpoint...\")\n            model_dict = self.state_dict()\n            modelCheckpoint = tc.load(checkpoint)\n            pretrained_dict = modelCheckpoint['state_dict']\n            try:\n                for x in range(len(self.optimizer_list)):\n                    self.optimizer_list[x].load_state_dict(modelCheckpoint['optimizer'][x])\n            except:\n                print('not loaded optimizer')\n            # 过滤操作\n            new_dict={}\n            for k,v in pretrained_dict.items():\n                if k in model_dict.keys() and model_dict[k].shape==v.shape:\n                    v=v.to(model_dict[k].device)\n                    new_dict[k]=v\n                else :\n                    try:\n                        print(f\"pass: {k} data_v: {v.shape} model_v: {model_dict[k].shape}\")\n                    except:\n                        print(f\"pass: {k} data_v: {v.shape} model_v: None\")\n                    input(\"enter?\")\n                    v:tc.Tensor\n                    #cache=v.clone()\n                    if k not in model_dict.keys():\n                        print(\"key error\")\n                        continue\n                    model_dict[k]*=0\n                    cache=model_dict[k].shape\n                    try:\n                        v=v.to(model_dict[k].device)\n                        cache=v.shape\n                        if v.ndim==4:\n                            model_dict[k][:cache[0],:cache[1],:cache[2],:cache[3]]+=v\n                        elif v.ndim==3:\n                            model_dict[k][:cache[0],:cache[1],:cache[2]]+=v\n                        elif v.ndim==2:\n                            model_dict[k][:cache[0],:cache[1]]+=v\n                        elif v.ndim==1:\n                            model_dict[k][:cache[0]]+=v\n                        else:\n                            break\n                        print(\"load finish\")\n                    except:\n                        #v=cache\n                        print(\"load error\")\n                    \n            #new_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict.keys()}\n            model_dict.update(new_dict)\n            # 打印出来，更新了多少的参数\n            print('Total : {}, update: {}'.format(len(pretrained_dict), len(new_dict)))\n            self.load_state_dict(model_dict)\n            print(\"loaded finished!\")\n            # 如果不需要更新优化器那么设置为false\n            \n        else:\n            print('No checkpoint is included')\n        print(\"load checkpoint Succeeded\")\n        self.save()\n\n    def save(self,path=None,loss=None):\n        try:\n            cache=self.save_x\n        except:\n            self.save_x=0 \n        if len(self.optimizer_list):\n            optimizer_data=[x.state_dict() for x in self.optimizer_list]\n        else :\n            optimizer_data=[]\n        if path:\n            tc.save({\"state_dict\":self.state_dict(),\"optimizer\":optimizer_data},path+\"_%2f.pth\"%loss)\n        else:\n            if \"model_data\" not in glob.glob(\"*\"):\n                os.system(\"mkdir model_data\")\n            if loss==None:\n                tc.save({\"data\":self.save_x,\"state_dict\":self.state_dict(),\n                    \"optimizer\":optimizer_data\n                         },\"model_data/\"+str(self.save_x)+\".pth\")\n            else :\n                tc.save({\"data\":self.save_x,\"state_dict\":self.state_dict(),\n                        \"optimizer\":optimizer_data\n                         },\"model_data/\"+str(self.save_x)+\"_%2f.pth\"%loss)\n                \n            self.save_x+=1\n\ndef preprocess_voice(voice_path):\n    audio, _ = librosa.core.load(voice_path, sr=None, mono=True)\n\n    melspec = librosa.feature.melspectrogram(\n        audio,\n        sr=32_000,\n        n_fft=800,\n        hop_length=320,\n        n_mels=128,\n        fmin=20,\n        fmax=14_000,\n        power=1,\n    )\n    x_train=librosa.pcen(\n        melspec * (2 ** 31),\n        time_constant=0.06,\n        eps=1e-6,\n        gain=0.8,\n        power=0.25,\n        bias=10,\n        sr=32_000,\n        hop_length=320,\n    )\n    return tc.from_numpy(x_train.T)\n\n##############################################################################################################\ndef conv_bn_relu(in_channels:int,out_channels:int,kernel_size,stride=1,padding=0,groups:int=1,relu=True,conv=nn.Conv2d):\n    if relu:\n        return nn.Sequential(conv(in_channels,out_channels,kernel_size,stride=stride,padding=padding,groups=groups,),\n                            nn.BatchNorm2d(out_channels,affine=True),\n                            nn.LeakyReLU(1e-4))\n    else: \n        return nn.Sequential(conv(in_channels,out_channels,kernel_size,stride=stride,padding=padding,groups=groups,),\n                            nn.BatchNorm2d(out_channels,affine=True))\n\ndef Linear_dp_relu(input_size,output_size,dropout_rate=0,relu=True):\n    cache=[nn.Linear(input_size,output_size)]\n    if relu:\n        cache.append(nn.LeakyReLU(1e-4))\n    if dropout_rate:\n        cache.append(nn.Dropout(dropout_rate))\n    return nn.Sequential(*cache)\n\nclass Bottleneck1(nn.Module):#Bottleneck1\n    def __init__(self,in_channels:int,out_channels:int,kernel_size,stride=1,padding=1,groups:int=1,s=False):\n        super().__init__()\n        self.cnn1=conv_bn_relu(in_channels,in_channels,1,stride,0,groups)\n        self.cnn2=conv_bn_relu(in_channels,in_channels,kernel_size,stride,padding,groups)\n        self.cnn3=conv_bn_relu(in_channels,out_channels,1,stride,0,groups,relu=False)\n        self.cnn4=conv_bn_relu(in_channels,out_channels,1,stride,0,groups,relu=False)\n        #self.a=tc.nn.Parameter(-tc.ones(1))\n        self.s=s\n    def forward(self,x:tc.Tensor):\n        x1=self.cnn1.forward(x)\n        x2=self.cnn4.forward(x1)\n        if self.s:\n            x1=tc.max_pool2d(x1,self.s)\n            x2=tc.max_pool2d(x2,self.s)\n        x1=self.cnn2.forward(x1)\n        x1=self.cnn3.forward(x1)\n        \n        #a=tc.sigmoid(self.a)\n        x=x1+x2\n        #x=x1*a+x2*(1-a)\n        return F.leaky_relu(x,1e-4)\n\nclass Bottleneck2(nn.Module):#Bottleneck2\n    def __init__(self,in_channels:int,hid_channels:int,kernel_size,stride=1,padding=1,groups:int=1,s=False):\n        super().__init__()\n        self.cnn1=conv_bn_relu(in_channels,hid_channels,1,stride,0,groups)\n        self.cnn2=conv_bn_relu(hid_channels,hid_channels,kernel_size,stride,padding,groups)\n        self.cnn3=conv_bn_relu(hid_channels,in_channels,1,stride,0,groups,relu=False)\n        #self.a=tc.nn.Parameter(-tc.ones(1))\n        self.s=s\n    def forward(self,x:tc.Tensor):\n        x1=self.cnn1.forward(x)\n        x1=self.cnn2.forward(x1)\n        x1=self.cnn3.forward(x1)\n        \n        #a=tc.sigmoid(self.a)\n        x=x1+x\n        #x=x1*a+x*(1-a)\n        return F.leaky_relu(x,1e-4)\n        \nclass Generate(nn.Module):\n    def __init__(self,dropout_rate=0):\n        super().__init__()\n        #input=(batch,152+1024=1176)\n        #outpu=(batch,500,128)\n        self.dp=nn.Dropout(dropout_rate)\n                \n        class D_Linear(nn.Module):\n            def __init__(self,input_size:tuple=(64,64),output_size:tuple=(128,128),dropout=True):\n                super().__init__()\n                self.size=output_size\n                self.dnn1=Linear_dp_relu(input_size[0],output_size[0],dropout_rate=dropout_rate*dropout)\n                self.dnn2=Linear_dp_relu(input_size[1],output_size[1],dropout_rate=dropout_rate*dropout)\n            def forward(self,x:tc.Tensor):\n                #(batch,c,h,w)\n                shape=x.shape\n                x=x.reshape(shape[0]*shape[1],shape[2],shape[3])\n                x=self.dnn1.forward(x)\n                #(batch,c,h,w1)\n                x=x.transpose(-1,-2)\n                #(batch,c,w1,h)\n                x=self.dnn2.forward(x)\n                #(batch,c,w1,h1)\n                x=x.reshape(shape[0],shape[1],self.size[1],self.size[0])\n                return x\n            \n        self.dnn1=Linear_dp_relu(1176,2048,dropout_rate=dropout_rate)\n        self.dnn2=Linear_dp_relu(32,64,dropout_rate=dropout_rate)\n        self.cnn1=nn.Sequential(Bottleneck1(1,16,3),Bottleneck2(16,8,3))\n        self.dnn3=D_Linear((64,64),(128,128))\n        self.cnn2=nn.Sequential(Bottleneck1(16,64,3),Bottleneck2(64,16,3),Bottleneck2(64,16,3))\n        self.dnn4=D_Linear((128,128),(256,128))\n        self.cnn3=nn.Sequential(Bottleneck2(64,16,3),Bottleneck2(64,16,3),Bottleneck1(64,16,3))#shape=(batch,64,256,128)\n        self.dnn5=D_Linear((256,128),(500,120),False)\n        self.cnn4=nn.Conv2d(16,1,1)\n        #self.dnn_test=D_Linear((64,64),(500,120),False)\n        \n    def forward(self,x:tc.Tensor):\n        #input=(batch,152)\n        x=tc.cat((x,tc.ones(x.shape[0],1024,device=x.device)),dim=1)\n        #x=tc.cat((x,tc.randn(x.shape[0],1024,device=x.device)),dim=1)\n        x=self.dnn1.forward(x)\n        x=x.reshape(x.shape[0],1,64,32)\n        x=self.dnn2.forward(x)#(batch,1,64,64)\n        #x=self.dnn_test.forward(x)[:,0].transpose(-1,-2)\n        #return tc.sigmoid(x)\n        x=self.cnn1.forward(x)#(batch,16,64,64)\n        x=self.dnn3.forward(x)#(batch,16,128,128)\n        x=self.cnn2.forward(x)#(batch,64,128,128)\n        x=self.dnn4.forward(x)#(batch,64,256,128)\n        x=self.cnn3.forward(x)#(batch,64,256,128)\n        x=self.dnn5.forward(x)#(batch,64,500,120)\n        x=self.cnn4.forward(x)[:,0].transpose(-1,-2)#(batch,1,500,120)\n        max_=x.max(dim=2)[0].max(dim=1)[0][:,None,None]\n        min_=x.min(dim=2)[0].min(dim=1)[0][:,None,None]\n        return tc.sigmoid(x)\n        #return (min_-x)/(min_-max_)\n        \nclass Model_p(nn.Module):\n    def __init__(self,dropout_rate=0):\n        super().__init__()\n        #input=(batch,500,128)\n        number=[1,64,256,512,1024,2048]#cnn_number\n\n        self.cnn1=conv_bn_relu(number[0],number[1],7,stride=1,padding=5)\n        self.cnn2=conv_bn_relu(number[1],number[1],3,stride=1,padding=2)\n        #torch.Size([10, 64, 128, 64])\n        stage1=[Bottleneck1(number[1],number[2],kernel_size=3),\n                Bottleneck2(number[2],int(number[2]/4),kernel_size=3),\n                Bottleneck2(number[2],int(number[2]/4),kernel_size=3,)]\n        self.stage1=nn.Sequential(*stage1)\n        #torch.Size([10, 256, 128, 64])\n        stage2=[Bottleneck1(number[2],number[3],kernel_size=3,s=2),\n                Bottleneck2(number[3],int(number[3]/4),kernel_size=3),\n                Bottleneck2(number[3],int(number[3]/4),kernel_size=3),\n                Bottleneck2(number[3],int(number[3]/4),kernel_size=3)]\n        self.stage2=nn.Sequential(*stage2)\n        #torch.Size([10, 512, 64, 32])\n        stage3=[Bottleneck1(number[3],number[4],kernel_size=3,s=2),\n                Bottleneck2(number[4],int(number[4]/4),kernel_size=3),\n                Bottleneck2(number[4],int(number[4]/4),kernel_size=3),\n                Bottleneck2(number[4],int(number[4]/4),kernel_size=3),\n                Bottleneck2(number[4],int(number[4]/4),kernel_size=3),\n                Bottleneck2(number[4],int(number[4]/4),kernel_size=3)]\n        self.stage3=nn.Sequential(*stage3)\n        #torch.Size([10, 1024, 16, 16])\n        stage4=[Bottleneck1(number[4],number[5],kernel_size=3,s=2),\n                Bottleneck2(number[5],int(number[5]/4),kernel_size=3),\n                Bottleneck2(number[5],int(number[5]/4),kernel_size=3)]\n        self.stage4=nn.Sequential(*stage4)\n        #torch.Size([10, 2048, 8, 8])\n        #torch.Size([10,64,2048])\n        class dnn(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.dnn1=Linear_dp_relu(2048,512,dropout_rate=dropout_rate)\n                #torch.Size([10,64,512])\n                self.dnn2=Linear_dp_relu(512,128,dropout_rate=dropout_rate)\n                #torch.Size([10,64,128])\n                #torch.Size([10,2,4096])\n                self.dnn3=Linear_dp_relu(4096,2048,dropout_rate=dropout_rate)\n                #torch.Size([10,2,2048])\n                #torch.Size([10,4096])\n                self.dnn4=Linear_dp_relu(4096,1024,dropout_rate=dropout_rate)\n                #torch.Size([10,1024])\n            \n            def forward(self,x:tc.Tensor):\n                #torch.Size([10,64,2048])\n                x=self.dnn1(x)\n                #torch.Size([10,64,512])\n                x=self.dnn2(x)\n                #torch.Size([10,64,128])\n                x=x.reshape(x.shape[0],2,-1)\n                #torch.Size([10,2,4096])\n                x=self.dnn3(x)\n                #torch.Size([10,2,2048])\n                x=x.reshape(x.shape[0],-1)\n                #torch.Size([10,4096])\n                x=self.dnn4(x)\n                #torch.Size([10,1024])\n                return x\n\n        dnn_set=[dnn() for x in range(4)]\n        self.dnn_set=nn.ModuleList(dnn_set)\n        \n        dnn_output1=Linear_dp_relu(4096,1024,dropout_rate=dropout_rate)\n        dnn_output2=Linear_dp_relu(1024,152,dropout_rate=0,relu=False)\n        self.dnn_final=nn.Sequential(dnn_output1,dnn_output2)\n        #self.dnn_output=self.dnn_final\n        class discriminate(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.dnn1=Linear_dp_relu(4096,1024,dropout_rate=dropout_rate)\n                self.dnn2=Linear_dp_relu(1024,128,dropout_rate=dropout_rate)\n                self.dnn3=Linear_dp_relu(128,1,dropout_rate=0,relu=False)\n            def forward(self,x:tc.Tensor):\n                #x.shape=(batch,4096)\n                x=self.dnn1.forward(x)\n                x=self.dnn2.forward(x)\n                x=self.dnn3.forward(x)\n                return x\n            \n        self.discriminate=discriminate()\n        \n    def referee_data(self):\n        return self.discriminate.parameters()\n    \n    def feature_data(self):\n        for x in [self.cnn1.parameters(),self.cnn2.parameters(),\n                  self.stage1.parameters(),self.stage2.parameters(),\n                  self.stage3.parameters(),self.stage4.parameters(),\n                  self.dnn_set.parameters()]:\n            for y in x:\n                yield y\n    \n    def feature_forward(self,x:tc.Tensor)->tc.Tensor:\n        x=x[:,None]\n        \n        x=self.cnn1(x)\n        x=tc.max_pool2d(x,(4,2),padding=(2,0))\n        \n        x=self.cnn2(x)\n        x=tc.max_pool2d(x,(2,1))\n\n        x=self.stage1.forward(x)\n        x=self.stage2.forward(x)\n        x=self.stage3.forward(x)\n        x=self.stage4.forward(x)\n        \n        x=x.reshape(x.shape[0],2048,64).transpose(1,2)\n        cache=[]\n        for i in range(len(self.dnn_set)):\n            #x.shape=[batch,64,2048]\n            cache.append(self.dnn_set[i].forward(x))\n            #output.shape=[batch,1024]\n        x=tc.cat(cache,dim=-1)\n        #x.shape=[batch,4096]\n        return x\n    \n    def forward(self,x:tc.Tensor,discriminate=False,feature_no_grad=False):\n        \"\"\"if discriminate false ->feature_no_grad=falses\"\"\"\n        if discriminate:\n            if feature_no_grad:\n                with tc.no_grad():\n                    x=self.feature_forward(x)\n            else :\n                x=self.feature_forward(x)\n            output=self.dnn_final.forward(x)\n            d=self.discriminate(x)\n            return tc.sigmoid(tc.cat((output,d),dim=1))\n            #output.shape=[batch,152+1]\n        else :\n            x=self.feature_forward(x)\n            x=self.dnn_final.forward(x)\n            return tc.sigmoid(x)\n            #output.shape=[batch,152]\n\nclass Model(Module):\n    def __init__(self,dropout_rate=0.1,\n                 optimizer=tc.optim.AdamW,lr=1e-4,betas=(0.9,0.9),weight_decay=0.1,\n                 loss_fc:nn.BCELoss=nn.BCELoss(reduction=\"sum\"),\n                 generate:Generate=Generate(0),generate_rate=0.2):\n        super().__init__()\n        \n        self.cnn=Model_p(dropout_rate)\n        cache=dict(lr=lr,betas=betas,weight_decay=weight_decay)\n        self.loss_fc=loss_fc\n        \n        #self.generate=generate\n        #self.generate_rate=generate_rate\n        #self.generate_optimizer=optimizer(self.generate.parameters(),**cache)\n        self.optim=optimizer\n        self.optim_cache=cache\n        self.init_optimizer()\n        \n        #self.optimizer_list=[self.generate_optimizer,self.cnn_optimizer]\n    \n    def init_optimizer(self):\n        self.cnn_optimizer=self.optim(self.cnn.parameters(),**self.optim_cache)\n        self.optimizer_list=[self.cnn_optimizer]\n\n    def forward(self,x:tc.Tensor,discriminate=False):\n        #index is record the park of batch \n        #index.shape=(batch,n)\n        #x.shape=(batch,2000,257)\n        #model output shape=(batch*4,157)\n        #use index to reshape to (batch,157)\n        #output=tc.nan_to_num(output,0.0)\n        #output=(output-output.min())/(output.max()-output.min())\n        cache=x.reshape(x.shape[0],4,500,-1)\n        max_=cache.max(dim=3)[0].max(dim=2)[0][:,:,None,None]\n        min_=cache.min(dim=3)[0].min(dim=2)[0][:,:,None,None]\n        cache=(min_-cache)/(min_-max_)\n        output=self.cnn(cache.reshape(x.shape[0]*4,500,-1),discriminate)\n        del cache\n        output=output.reshape(x.shape[0],4,output.shape[-1])\n        \n        output=output.max(dim=1)[0]\n        return output\n    \n    def train_cnn(self,x:tc.Tensor,y:tc.Tensor,mask=None):\n        self.cnn_optimizer.zero_grad()\n\n        if x.shape[1]==500:\n            output=self.cnn(x)\n        else :\n            output=self.forward(x)\n        if not (mask is None):\n            output=output[:,mask]\n            \n        loss=self.loss_fc.forward(output,y)/output.shape[0]\n        loss.backward()\n        self.cnn_optimizer.step()\n        return {\"output\":output,\"loss\":tc.tensor([loss])}\n    \n    def train_all(self,x:tc.Tensor,y:tc.Tensor,mask=None):\n        #https://pabebezz.github.io/article/2d45afe4/  领域自适应方法\n        #http://www.atyun.com/18356.html 对抗网络\n        self.cnn_optimizer.zero_grad()\n        self.generate_optimizer.zero_grad()\n\n        generate_time=int(x.shape[0]*self.generate_rate)\n        x_batch=x.shape[0]\n        if x.shape[1]==500:\n            a=tc.zeros(generate_time,152,dtype=tc.bool,device=x.device)\n        else :\n            a=tc.zeros(generate_time*4,152,dtype=tc.bool,device=x.device)\n            \n        for cache in a:\n            for i in range(np.random.randint(0,5)):\n                cache[np.random.randint(0,152)]=True\n        more_train=self.generate.forward(a)\n        \n        y=tc.cat((y,a),dim=0)\n        a=tc.zeros(y.shape[0],1,dtype=tc.float32,device=x.device)\n        a[x_batch:,0]=1\n        y=tc.cat((y,a),dim=1)\n        \n        if x.shape[1]==500:\n            x=tc.cat((x,more_train),dim=0)\n            output=self.cnn(x,True)\n        else :\n            x=tc.cat((x,more_train.reshape(x.shape[0],2000,-1)),dim=0)\n            output=self.forward(x,True)\n        if not (mask is None):\n            output=output[:,mask]\n        loss_sum=[]\n        loss=self.loss_fc.forward(output[x_batch:,:152],y[x_batch:,:152])/generate_time#referee\n        loss.backward(retain_graph=True)\n        loss_sum.append(loss)\n        self.cnn_optimizer.zero_grad()#清空梯度\n        #referee=0 feature=0 output=0 generate_time=dloss/dx #x=data\n        #print(output[:,-1],y[:,-1])\n        #input()\n        loss=self.loss_fc.forward(output[:,-1],y[:,-1])/output.shape[0]\n        loss_sum.append(loss)\n        \n        #referee=dr/dx feature=dr/dx  generate_time=dloss1/dx+dr/dx #r=referee x=data\n        \"\"\"\n        loss.backward(retain_graph=True)\n        for cache in self.cnn.feature_data():\n            cache:tc.Tensor\n            cache.grad*=-tc.ones(1,device=x.device)\"\"\"\n        #referee=dr/dx feature=-dr/dx  generate_time=dloss1/dx+dr/dx #r=referee x=data\n        self.generate_optimizer.step()\n        #referee=dr/dx feature=-dr/dx  generate_time=0 #r=referee x=data\n        \"\"\"loss=self.loss_fc.forward(output[:x_batch,:152],y[:x_batch,:152])/x_batch#referee\n        loss.backward(retain_graph=False,inputs=list(self.cnn.parameters()))\n        #referee=dr/dx feature=-dr/dx+dloss1/dx generate_time=0\n        loss_sum.append(loss)\n        \n        self.cnn_optimizer.step()\n        self.cnn_optimizer.zero_grad()\"\"\"\n        #self.generate_optimizer.zero_grad()\n        #referee=0 feature=0 generate_time=0\n        \n        \n        return {\"output\":output[:x_batch,:152],\"loss\":tc.stack(loss_sum)}\n    \n    def train_generate(self,time):\n        a=tc.zeros(time,152,dtype=tc.bool,device=data[\"device\"])\n        for cache in a:\n            for i in range(np.random.randint(1,5)):\n                cache[np.random.randint(0,152)]=True\n        self.generate_optimizer.zero_grad()\n        more_train=self.generate.forward(a.to(tc.float32))\n        output=self.cnn(more_train,False)#main_point\n\n        loss=self.loss_fc.forward(output,a.to(tc.float32))/time#referee\n        loss.backward()\n        self.generate_optimizer.step()\n        self.cnn_optimizer.zero_grad()\n                \n        return {\"output\":output,\"loss\":tc.tensor([loss])}\n\n    def test(self,x:tc.Tensor):\n        #x.shape=(batch,500,257)\n        output=self.cnn(x)\n        output=output>=0\n        return output\n       \n    def test_error(self,x:tc.Tensor):\n        if x.shape[-2]!=2000:\n            print(x.shape)\n            print(\"test_error\")\n            exit()\n        cache=x.reshape(x.shape[0],4,500,-1)\n        max_=cache.max(dim=3)[0].max(dim=2)[0][:,:,None,None]\n        min_=cache.min(dim=3)[0].min(dim=2)[0][:,:,None,None]\n        cache=(min_-cache)/(min_-max_)\n        output=self.cnn.forward(cache.reshape(x.shape[0]*4,500,-1))\n        #del cache\n        output1=output.reshape(x.shape[0],4,output.shape[-1])\n        output=output1.max(dim=1)[0]\n        #output=tc.sigmoid(output)\n\n        return output,output1,cache.reshape(x.shape[0],500*4,-1)\n\nclass testDataset(tc.utils.data.Dataset):\n    \"\"\"Some Information about MyDataset\"\"\"\n    def __init__(self,paths:list,bird_index:tc.Tensor,filepath:str,end_time:list,fit_randn=0.01):\n        super().__init__()\n        if not isinstance(paths,np.ndarray):\n            paths=np.array(paths,dtype=np.object0)\n        self.filepath=filepath\n        self.paths=paths\n        self.end_time=end_time\n        self.randn=fit_randn\n        self.bird_index=bird_index\n\n    \n\n    def __getitem__(self, index):\n        def get(index):\n            return self.paths[index]\n        \n        path=get(index)\n        path=self.filepath+path+\".ogg\"\n        if os.path.isfile(path):\n            voice_data=preprocess_voice(path)\n        else: \n            cache=tc.zeros(self.bird_index.shape[1],dtype=tc.bool)\n            cache[0]=1\n            return tc.randn(500,120,dtype=tc.float32)*0.01,cache\n\n        time_=int(self.end_time[index])#s\n        \n        if voice_data.shape[0]<time_*100:\n            #print(\"add error\")\n            voice_data=voice_data[-500:]\n        else :\n            voice_data=voice_data[time_*100-500:time_*100]\n        \n        \"\"\"more Data_Augmentation\"\"\"\n        #voice_data.shape=(500,128)\n        cache=np.random.randint(1,7)\n        voice_data=voice_data[:,cache:-8+cache]#voice_data.shape=(500,120)#-(8-cache)\n        #d1=voice_data.max()*0.8\n        #voice_data=tc.cat((tc.rand(500,cache)*d1,voice_data,tc.rand(500,8-cache)*d1),dim=1)\n        #voice_data.shape=(500,n+120+8-n)=(500,120)\n        #randn=tc.randint(1,1000,[1])/1000*self.randn\n        \n        voice_data=(voice_data-voice_data.min())/(voice_data.max()-voice_data.min())\n        d1=tc.var(voice_data)\n        cache=tc.randn(voice_data.shape)*d1**0.5*self.randn#0.01\n        voice_data+=cache#0.01\n        voice_data=(voice_data-voice_data.min())/(voice_data.max()-voice_data.min())\n        return voice_data.to(tc.float32),self.bird_index[index].to(tc.bool)\n    \n    def __len__(self):\n        return len(self.paths)\n\n########################################################\nif __name__ ==\"__main__\":\n    tc.backends.cudnn.benchmark = True###new\n\n    data={\"batch\":64,\"skip\":2,\"device\":\"cuda:0\",\"num_workers\":2}\n    data[\"device\"] = \"cuda:0\" if tc.cuda.is_available() else \"cpu\"\n\n    data_path=[\"../input/data123/\",\"../input/data123/sAs500_loss4p0_i124_4p8_p70_2p0_p82.pth\",\"../input/birdclef-2022/\"]\n    #data_path=[\"D:/python_data/voice/birdclef_voice/voice_data/\",\"model_data/main.pth\",\"D:/python_data/voice/birdclef_voice/\"]\n\n\n    dc=my_dic() \n    dc.load(data_path[0])\n    #print(dc.data)\n    with open(data_path[2]+\"scored_birds.json\",\"r\")as file:\n        target=ujson.load(file)\n    target=dc.transform(target).sum(0).to(tc.bool)\n    dc.data=np.array(dc.data)[target]\n    #print(dc.data)\n    #exit()\n    data[\"scored\"]=target\n\n    model = Model()\n    model.load_checkpoint(data_path[1])\n    model=model.to(data[\"device\"]).train(False)\n\n    data_path=data_path[2]\n\n    data[\"i\"]=tc.arange(124,device=data[\"device\"])[None]+tc.arange(4,device=data[\"device\"])[:,None]\n    def run(x,index):\n        #x.shape=(batch,500,128)\n        batch=x.shape[0]\n        x=x[...,data[\"i\"]].transpose(1,2).reshape(batch*4,500,124)\n        #if x.shape[-1]==128:\n        #    x=x[...,2:-2]\n        with tc.no_grad():\n            sigmoid_O=model.cnn.forward(x.to(data[\"device\"]))[:,data[\"scored\"]]\n            #sigmoid_O=sigmoid_O>0.135\n        sigmoid_O=sigmoid_O.reshape(batch,4,-1).mean(dim=1)\n        return sigmoid_O.cpu()#[index.to(tc.bool)]\n    \"\"\"def run(x,index):\n        if x.shape[-1]==128:\n            x=x[...,2:-2]\n        with tc.no_grad():\n            sigmoid_O=model.cnn.forward(x.to(data[\"device\"]))[:,data[\"scored\"]]\n        return sigmoid_O.cpu()#[index.to(tc.bool)]\"\"\"\n    ##############################################################################################################################\n    \"\"\"with tc.no_grad():\n        a=model.cnn.forward(tc.randn((data[\"batch\"],500,128),device=data[\"device\"]))\n        del a\"\"\"\n\n    ##############################################################################################################################\n\n    test=pd.read_csv(data_path+\"test.csv\",encoding=\"UTF-8\")\n\n    voice_error=0\n\n    #in test error\n    start=time.time()\n    target=[]\n    row_id=test[\"row_id\"]\n    file_id=test[\"file_id\"]\n    end_time=test[\"end_time\"]\n    hs_voice_path=\"\"\n    voice_set=[]\n    bird=test[\"bird\"]\n    bird_index=[]\n    for x in range(len(file_id)):\n        if hs_voice_path!=file_id[x]:\n            #print(file_id[x])\n            if os.path.isfile(data_path+\"test_soundscapes/\"+file_id[x]+\".ogg\"):\n                voices=preprocess_voice(data_path+\"test_soundscapes/\"+file_id[x]+\".ogg\").to(tc.float32)\n                voice_error=0\n            else:\n                voice_error=1\n\n            hs_voice_path=file_id[x]\n\n        if voice_error:\n            #print(\"voice_error\")\n            voice=tc.randn(500,128,dtype=tc.float32)*0.1\n        else:\n            time_=int(end_time[x])#s\n            if voices.shape[0]<time_*100:\n                #print(\"add error\")\n                voice=voices[-500:]\n            else :\n                voice=voices[time_*100-500:time_*100]\n\n        voice=(voice-voice.min())/(voice.max()-voice.min())\n        d1=tc.var(voice)\n        voice=voice+tc.randn(voice.shape)*d1**0.5*0.#0.01\n        voice=(voice-voice.min())/(voice.max()-voice.min())\n        voice_set.append(voice)\n\n        bird_index.append(bird[x])\n\n        if x%data[\"batch\"]==0 and len(voice_set)>1:\n            voice_set=tc.stack(voice_set)\n\n            #bird_index=dc.transform(bird_index)\n            a=run(voice_set,bird_index)\n            voice_set=[]\n            #bird_index=[]\n            if isinstance(target,list):\n                target=a\n            else:\n                target=tc.cat((target,a),dim=0)\n    if len(voice_set)>=1:\n        if len(voice_set)>1:\n            voice_set=tc.stack(voice_set)\n        else: \n            voice_set=voice_set[0][None]\n\n        #bird_index=dc.transform(bird_index)\n        a=run(voice_set,bird_index)\n        if isinstance(target,list):\n            target=a\n        else:\n            target=tc.cat((target,a),dim=0)\n        \n    end = time.time()\n    print(end-start)\n    \n    plt.imshow(target[:10])\n    plt.show()\n    \n    percentage=0.35#x in target#0.02 i think is good number :1/21/2......\n\n    \"\"\"with open(\"../input/data123/\"+\"score_bird_rate.json\",\"r\") as file:\n        t=ujson.load(file).values()\n        t=list(t)\"\"\"\n    cache=target.sort(dim=0)\n    \n    \n    for x in range(target.shape[1]):\n        #n=math.ceil(t[x]*target.shape[0])\n        n=math.ceil(percentage*target.shape[0])\n        target[:,x]=False\n        target[cache[1][-n:,x],x]=True\n    \n    plt.imshow(cache[0][:10])\n    plt.show()\n    plt.imshow(target[:10])\n    plt.show()\n    bird_index=dc.transform(bird_index)\n    target=target[bird_index.to(tc.bool)].to(tc.bool)\n    \n    output=pd.DataFrame({\"row_id\":row_id,\"target\":target})\n    output=output.set_index('row_id')\n    output.to_csv(\"submission.csv\")\n    print(\"fanish\")\n    print(output[:3])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-29T13:01:00.105964Z","iopub.execute_input":"2022-05-29T13:01:00.106241Z","iopub.status.idle":"2022-05-29T13:01:05.951559Z","shell.execute_reply.started":"2022-05-29T13:01:00.10621Z","shell.execute_reply":"2022-05-29T13:01:05.95091Z"},"trusted":true},"execution_count":null,"outputs":[]}]}