{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-24T11:23:29.173489Z","iopub.execute_input":"2022-05-24T11:23:29.174134Z","iopub.status.idle":"2022-05-24T11:23:29.934256Z","shell.execute_reply.started":"2022-05-24T11:23:29.174015Z","shell.execute_reply":"2022-05-24T11:23:29.933315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os,sys,re,glob,random\nimport pandas as pd\nimport librosa as lb\nimport torchaudio\nimport IPython.display as ipd\nimport soundfile as sf\nimport numpy as np\nimport cv2\nimport ast\nfrom pathlib import Path\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport librosa.display\nfrom sklearn import preprocessing\n\n#Deep learning from pytorch\nimport torch\nimport torchvision\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom albumentations.core.transforms_interface import ImageOnlyTransform\nfrom tqdm import tqdm\nimport joblib","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:23:29.936484Z","iopub.execute_input":"2022-05-24T11:23:29.936822Z","iopub.status.idle":"2022-05-24T11:23:34.034872Z","shell.execute_reply.started":"2022-05-24T11:23:29.936779Z","shell.execute_reply":"2022-05-24T11:23:34.033906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#timmのdirpathを設定\ntimm_path = \"../input/timm-pytorch-image-models/pytorch-image-models-master\"\n\nimport sys\nsys.path.append(timm_path)\nimport timm\n\ndevice = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:23:34.036371Z","iopub.execute_input":"2022-05-24T11:23:34.03665Z","iopub.status.idle":"2022-05-24T11:23:35.481258Z","shell.execute_reply.started":"2022-05-24T11:23:34.036612Z","shell.execute_reply":"2022-05-24T11:23:35.480369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class cfg:\n    CLASS_NUM = 21\n    period = 5\n    sr = 32000\n    n_mels = 128\n    fmin = 20\n    fmax = 16000\n    n_fft = 32000//10\n    hop_len = 3200//4\n    power = 2\n    top_db = 80\n    \n    #timm model name\n    model_names = [\n        \"eca_nfnet_l0\",\n        \"resnest50d\",\n        \"convnext_tiny\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",    \n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\", \n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\", \n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\", \n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\", \n        \"convnext_tiny\",\n        \"convnext_tiny\",\n        \"convnext_tiny\",\n        \"convnext_tiny\",\n        \"convnext_tiny\",\n    ]\n    \n    #weight path\n    paths = [\n        \"../input/bridclef2022dataset/eca_nfnet_l0_p7CV0.881.bin\",\n        \"../input/bridclef2022dataset/resnest50d_CV0.852.bin\",     \n        \"../input/bridclef2022dataset/convnext_tiny_CV0.83.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0_0501/fold0.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0_0501/fold1.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0_0501/fold2.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0_0501/fold3.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0_0501/fold4.bin\",\n        \"../input/birdclef20225fold/ecanfnet0504_reductbias/fold0.bin\",\n        \"../input/birdclef20225fold/ecanfnet0504_reductbias/fold1.bin\",\n        \"../input/birdclef20225fold/ecanfnet0504_reductbias/fold2.bin\",\n        \"../input/birdclef20225fold/ecanfnet0504_reductbias/fold3.bin\",\n        \"../input/birdclef20225fold/ecanfnet0504_reductbias/fold4.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0_CV0.80/fold0.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0_CV0.80/fold1.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0_CV0.80/fold2.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0_CV0.80/fold3.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0_CV0.80/fold4.bin\",\n        \"../input/birdclef20225fold/efl0img_0515_fulllabeling/fold0.bin\",\n        \"../input/birdclef20225fold/efl0img_0515_fulllabeling/fold1.bin\",\n        \"../input/birdclef20225fold/efl0img_0515_fulllabeling/fold2.bin\",\n        \"../input/birdclef20225fold/efl0img_0515_fulllabeling/fold3.bin\",\n        \"../input/birdclef20225fold/efl0img_0515_fulllabeling/fold4.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0502_mixup2.0/fold0.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0502_mixup2.0/fold1.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0502_mixup2.0/fold2.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0502_mixup2.0/fold3.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0502_mixup2.0/fold4.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0_0431_v2/fold0.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0_0431_v2/fold1.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0_0431_v2/fold2.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0_0431_v2/fold3.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0_0431_v2/fold4.bin\",\n        \"../input/birdclef20225fold/convnext_0502/fold0.bin\",\n        \"../input/birdclef20225fold/convnext_0502/fold1.bin\",\n        \"../input/birdclef20225fold/convnext_0502/fold2.bin\",\n        \"../input/birdclef20225fold/convnext_0502/fold3.bin\",\n        \"../input/birdclef20225fold/convnext_0502/fold4.bin\",\n    ]\n    \n    region_paths = [\n        \"../input/birdclef20225fold/ecanfnetl0_0511_21+region/model_0_0.886.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0_0511_21+region/model_1_0.873.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0_0511_21+region/model_2_0.849.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0_0511_21+region/model_3_0.872.bin\",\n        \"../input/birdclef20225fold/ecanfnetl0_0511_21+region/model_4_0.824.bin\"\n    ]\n    \n    region_model_names = [\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n        \"eca_nfnet_l0\",\n    ]\n        \n    \n    scored_birds = [\n        \"akiapo\",\n        \"aniani\", \n        \"apapan\", \n        \"barpet\", \n        \"crehon\",#5 ラベル少ないNo.2\n        \"elepai\", \n        \"ercfra\",#7 ラベル少ないNo.6\n        \"hawama\", \n        \"hawcre\", \n        \"hawgoo\",#10 ラベル少ないNo.7\n        \"hawhaw\",#11 ラベル少ないNo.4\n        \"hawpet1\",#12 ラベルすくないNo.5\n        \"houfin\",#13 ラベルトップ No.2 \n        \"iiwi\", \n        \"jabwar\", \n        \"maupar\",#16 絶滅危惧種No.1\n        \"omao\", \n        \"puaioh\",#18 絶滅危惧種No.3\n        \"skylar\",#13 ラベルトップ No.1\n        \"warwhe1\", \n        \"yefcan\"\n    ]\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:23:35.484018Z","iopub.execute_input":"2022-05-24T11:23:35.484372Z","iopub.status.idle":"2022-05-24T11:23:35.501594Z","shell.execute_reply.started":"2022-05-24T11:23:35.48433Z","shell.execute_reply":"2022-05-24T11:23:35.500395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WaveformDataset:\n    def __init__(self,\n                 df: pd.DataFrame,\n                 period,\n                 sr, \n                 n_mels, \n                 fmin, \n                 fmax,\n                 ):\n        \n        self.df = df\n        \n        #make Melspectrum\n        self.sr = sr\n        self.period = period\n        self.n_mels = n_mels\n        self.fmin = fmin\n        self.fmax = fmax\n        kwargs = {}\n        kwargs[\"n_fft\"] = kwargs.get(\"n_fft\", self.sr//10)\n        kwargs[\"hop_length\"] = kwargs.get(\"hop_length\", self.sr//(10*4))\n        self.kwargs = kwargs\n    \n    def make_melspec(self, y):\n\n        melspec = librosa.feature.melspectrogram(\n            y=y, \n            sr=self.sr, \n            n_mels=self.n_mels, \n            fmin=self.fmin, \n            fmax=self.fmax, \n            **self.kwargs,\n        )\n\n        melspec = librosa.power_to_db(melspec).astype(np.float32)\n        return melspec\n    \n    def mono_to_color(self, X, eps=1e-6, mean=None, std=None):\n        mean = mean or X.mean()\n        std = std or X.std()\n        X = (X - mean) / (std + eps)\n\n        _min, _max = X.min(), X.max()\n\n        if (_max - _min) > eps:\n            V = np.clip(X, _min, _max)\n            V = 255 * (V - _min) / (_max - _min)\n            V = V.astype(np.uint8)\n        else:\n            V = np.zeros_like(X, dtype=np.uint8)\n\n        return V\n\n    def crop_or_pad(self, y, length, is_train=False, start=None):\n        if len(y) < length:\n            y = np.concatenate([y, np.zeros(length - len(y))])\n\n            n_repeats = length // len(y)\n            epsilon = length % len(y)\n\n            y = np.concatenate([y]*n_repeats + [y[:epsilon]])\n\n        elif len(y) > length:\n            if not is_train:\n                start = start or 0\n            else:\n                start = start or np.random.randint(len(y) - length)\n\n            y = y[start:start + length]\n\n        return y\n    \n    def audio_to_image(self, audio):\n        melspec = self.make_melspec(audio) \n        image = self.mono_to_color(melspec)\n        return image\n\n    def __call__(self, path):        \n        #データ読み込み\n        data, sr = librosa.load(path, sr=self.sr)\n        \n        #test datasetの最大長\n        max_sec = 60\n        \n        #予測フレーム\n        pred_sec = 7\n        \n        #データを5秒間隔でかつ7秒幅を取って区切る\n        datas = [data[int(max(0, i-1) * sr):int(min(max_sec, i+6) * sr)] for i in range(0, max_sec, self.period)]\n        \n        #端は1秒短くなるので埋める\n        datas[0] = self.crop_or_pad(datas[0] , length=sr*pred_sec)\n        datas[-1] = self.crop_or_pad(datas[-1] , length=sr*pred_sec)\n        \n        #データをメル周波数によって画像化\n        images = [self.audio_to_image(audio) for audio in datas]\n        images = np.stack(images)\n        \n        #保存\n        filename = path.split(\"/\")[-1]\n        path = f\"/kaggle/audio_images/{filename}.npy\"\n        np.save(path, images)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:23:35.50601Z","iopub.execute_input":"2022-05-24T11:23:35.506296Z","iopub.status.idle":"2022-05-24T11:23:35.529053Z","shell.execute_reply.started":"2022-05-24T11:23:35.506241Z","shell.execute_reply":"2022-05-24T11:23:35.528385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_audios_as_images(paths):\n    pool = joblib.Parallel(2)\n    \n    converter = WaveformDataset(paths,\n                             period=cfg.period,\n                             sr=cfg.sr,\n                             n_mels=cfg.n_mels,\n                             fmin = cfg.fmin,\n                             fmax = cfg.fmax,\n                            )\n    mapper = joblib.delayed(converter)\n    tasks = [mapper(path) for path in tqdm(paths)]\n    pool(tqdm(tasks))","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:23:35.530364Z","iopub.execute_input":"2022-05-24T11:23:35.53075Z","iopub.status.idle":"2022-05-24T11:23:35.5373Z","shell.execute_reply.started":"2022-05-24T11:23:35.530714Z","shell.execute_reply":"2022-05-24T11:23:35.53662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/audio_images\nall_audio_paths = glob.glob(\"../input/birdclef-2022/test_soundscapes/*.ogg\")\nget_audios_as_images(all_audio_paths)\n!ls /kaggle/audio_images/","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:23:35.538795Z","iopub.execute_input":"2022-05-24T11:23:35.539266Z","iopub.status.idle":"2022-05-24T11:23:38.978523Z","shell.execute_reply.started":"2022-05-24T11:23:35.539232Z","shell.execute_reply":"2022-05-24T11:23:38.977618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WaveformDataset(Dataset):\n    def __init__(self,\n                 paths):\n        self.paths = paths\n        \n    @staticmethod\n    def normalize(image):\n        image = image[:,None,:,:].astype(\"float32\", copy=False) / 255.0\n        return image\n\n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, idx):\n        path = self.paths[idx]\n        image = np.load(path)\n        image = self.normalize(image)\n        file_id = path.split(\"/\")[-1].replace(\".ogg.npy\",\"\")\n        \n        return image, file_id","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:23:38.981814Z","iopub.execute_input":"2022-05-24T11:23:38.985348Z","iopub.status.idle":"2022-05-24T11:23:38.995556Z","shell.execute_reply.started":"2022-05-24T11:23:38.985307Z","shell.execute_reply":"2022-05-24T11:23:38.994765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self,name,pretrained=False):\n        super(Model, self).__init__()\n        self.model = timm.create_model(name,pretrained=pretrained, in_chans=1)\n        self.model.reset_classifier(num_classes=0) \n        in_features = self.model.num_features\n        self.fc = nn.Linear(in_features, cfg.CLASS_NUM)\n\n    def forward(self, x):\n        x = self.model(x)\n        x = self.fc(x)\n        return x\n    \nclass regionModel(nn.Module):\n    def __init__(self,name,pretrained=False):\n        super(regionModel, self).__init__()\n        self.model = timm.create_model(name,pretrained=pretrained, in_chans=1)\n        self.model.reset_classifier(num_classes=0) \n        in_features = self.model.num_features\n        self.fc = nn.Linear(in_features, cfg.CLASS_NUM+1)\n        \n    def forward(self, x):\n        x = self.model(x)\n        x = self.fc(x)\n        return x[:,:21], x[:,21]","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:23:39.003647Z","iopub.execute_input":"2022-05-24T11:23:39.004112Z","iopub.status.idle":"2022-05-24T11:23:39.015705Z","shell.execute_reply.started":"2022-05-24T11:23:39.004074Z","shell.execute_reply":"2022-05-24T11:23:39.014851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = []\nfor name, path in zip(cfg.model_names,cfg.paths):\n    print(path)\n    model = Model(name=name).to(device)\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    models.append(model)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:24:55.418245Z","iopub.execute_input":"2022-05-24T11:24:55.418814Z","iopub.status.idle":"2022-05-24T11:25:14.929153Z","shell.execute_reply.started":"2022-05-24T11:24:55.418773Z","shell.execute_reply":"2022-05-24T11:25:14.928416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nregion_model = []\nfor name, path in zip(cfg.region_model_names,cfg.region_paths):\n    model = regionModel(name=name).to(device)\n    model.load_state_dict(torch.load(path),strict=False)\n    model.eval()\n    region_model.append(model)\n\"\"\"    \npaths = glob.glob(\"/kaggle/audio_images/*.npy\")\npaths","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:24:31.131342Z","iopub.execute_input":"2022-05-24T11:24:31.131636Z","iopub.status.idle":"2022-05-24T11:24:37.058891Z","shell.execute_reply.started":"2022-05-24T11:24:31.131598Z","shell.execute_reply":"2022-05-24T11:24:37.058214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = [\n    7,7,7,#最初の3つは大きめ\n    1,1,1,1,1,#ecanfnetl0_0501\n    0.8,0.8,0.8,0.8,0.8,#ecanfnet0504_reductbias\n    1,1,1,1,1,#ecanfnetl0_CV0.80\n    0.3,0.3,0.3,0.3,0.3,#efl0img_0515_fulllabeling\n    0.8,0.8,0.8,0.8,0.8,#ecanfnetl0502_mixup2.0\n    0.7,0.7,0.7,0.7,0.7,#ecanfnetl0_0431_v2\n    0.5,0.5,0.5,0.5,0.5,#convnext_0502\n]","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:24:37.060192Z","iopub.execute_input":"2022-05-24T11:24:37.06101Z","iopub.status.idle":"2022-05-24T11:24:37.066911Z","shell.execute_reply.started":"2022-05-24T11:24:37.060969Z","shell.execute_reply":"2022-05-24T11:24:37.06598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = WaveformDataset(paths)\nproba_arr = []\nregion_arr = []\nfile_ids = []\n\npred_list = [None for _ in range(len(models))]\n\nfor n, (images, file_id) in tqdm(enumerate(dataset), total=len(dataset)):\n    with torch.no_grad():\n        images = torch.tensor(images, dtype=torch.float).to(device)\n        pred_l = None\n        \n        for i,model in enumerate(models):\n            if i <= 2:\n                pred = model(images).sigmoid().detach().cpu().numpy()\n            else:\n                pred1 = model(images[:,:,:,0:201]).sigmoid().detach().cpu().numpy()\n                pred2 = model(images[:,:,:,40:241]).sigmoid().detach().cpu().numpy()\n                pred3 = model(images[:,:,:,80:281]).sigmoid().detach().cpu().numpy()\n                pred = 0.25*pred1 + 0.5*pred2 + 0.25*pred3\n            if i==0:\n                preds =  pred* weights[i]/sum(weights)\n                \n            else:\n                preds += pred* weights[i]/sum(weights)\n                \n            if n==0:\n                pred_list[i] = pred\n            else:\n                pred_list[i] = np.concatenate([pred_list[i], pred])\n        \n        proba_arr.append(preds)\n            \n        \"\"\"\n        for i,model in enumerate(region_model):\n            #pred1,  = model(images[:,:,:,0:201]).sigmoid().detach().cpu().numpy()\n            region = model(images[:,:,:,40:241])[1].sigmoid().detach().cpu().numpy()\n            #pred3, _ = model(images[:,:,:,80:281]).sigmoid().detach().cpu().numpy()\n            #pred = 0.25*pred1 + 0.5*pred2 + 0.25*pred3\n            if i==0:\n                preds = region/len(region_model)\n            else:\n                preds += region/len(region_model)\n        \n        region_arr.append(preds)\n        \"\"\"\n        file_ids.append(file_id)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:24:37.069206Z","iopub.execute_input":"2022-05-24T11:24:37.069796Z","iopub.status.idle":"2022-05-24T11:24:45.342605Z","shell.execute_reply.started":"2022-05-24T11:24:37.069711Z","shell.execute_reply":"2022-05-24T11:24:45.341974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Voting Re-scoring Post-Processing","metadata":{}},{"cell_type":"code","source":"judge_percentile1 = 0.90\njudge_percentile2 = 0.80\njudge_percentile3 = 0.7\n\njudge1_birds = [ \n        \"apapan\", \n        \"barpet\", \n        \"hawcre\", \n        \"houfin\",#13 ラベルトップ No.2 \n        \"iiwi\", \n        \"jabwar\", \n        \"skylar\",#13 ラベルトップ No.1\n    ]\n\njudge2_birds = [\n        \"akiapo\",\n        \"aniani\", \n        \"elepai\", \n        \"hawama\", \n        \"omao\", \n        \"warwhe1\", \n        \"yefcan\"\n    ]\n\njudge3_birds = [ \n        \"crehon\",#5 ラベル少ないNo.2\n        \"ercfra\",#7 ラベル少ないNo.6\n        \"hawgoo\",#10 ラベル少ないNo.7\n        \"hawhaw\",#11 ラベル少ないNo.4\n        \"hawpet1\",#12 ラベルすくないNo.5  \n        \"maupar\",#16 絶滅危惧種No.1\n        \"puaioh\",#18 絶滅危惧種No.3\n    ]","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:24:45.346331Z","iopub.execute_input":"2022-05-24T11:24:45.348331Z","iopub.status.idle":"2022-05-24T11:24:45.356622Z","shell.execute_reply.started":"2022-05-24T11:24:45.348291Z","shell.execute_reply":"2022-05-24T11:24:45.35599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_df = np.zeros(pred_list[0].shape)\nfor pred in pred_list:\n    for i,specie in enumerate(cfg.scored_birds):\n        pred_per_s = pred[:,i]\n        if specie in judge1_birds:\n            thre = np.percentile(pred_per_s, judge_percentile1)\n        elif specie in judge2_birds:\n            thre = np.percentile(pred_per_s, judge_percentile2)\n        elif specie in judge3_birds:\n            thre = np.percentile(pred_per_s, judge_percentile3)\n        score = np.where(pred_per_s>thre, 1, 0)\n        score_df[:,i] += score","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:24:45.360342Z","iopub.execute_input":"2022-05-24T11:24:45.362253Z","iopub.status.idle":"2022-05-24T11:24:45.482967Z","shell.execute_reply.started":"2022-05-24T11:24:45.362218Z","shell.execute_reply":"2022-05-24T11:24:45.482307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"アンサンブルするモデルが指定した閾値を超えているかどうかをvotingする。\nそのvotingに従って、予測確率に対する重みを最大2倍まで高める","metadata":{}},{"cell_type":"code","source":"weight_df = score_df.copy()\nweight_df = weight_df/len(models)+1\nweight_df","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:24:45.48395Z","iopub.execute_input":"2022-05-24T11:24:45.48417Z","iopub.status.idle":"2022-05-24T11:24:45.493451Z","shell.execute_reply.started":"2022-05-24T11:24:45.484139Z","shell.execute_reply":"2022-05-24T11:24:45.492651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#weight_df = np.concatenate([weight_df, weight_df, weight_df[:10]])\n#proba_arr = proba_arr + proba_arr + [proba_arr[0][:10]]","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:24:45.494975Z","iopub.execute_input":"2022-05-24T11:24:45.495466Z","iopub.status.idle":"2022-05-24T11:24:45.500284Z","shell.execute_reply.started":"2022-05-24T11:24:45.495428Z","shell.execute_reply":"2022-05-24T11:24:45.499374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num = 0\nfor i, prob in enumerate(proba_arr):\n    extract_len = len(prob)\n    weight = weight_df[num:num+extract_len]\n    proba_arr[i] = prob * weight\n    num += extract_len","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:24:45.501877Z","iopub.execute_input":"2022-05-24T11:24:45.502351Z","iopub.status.idle":"2022-05-24T11:24:45.509205Z","shell.execute_reply.started":"2022-05-24T11:24:45.502246Z","shell.execute_reply":"2022-05-24T11:24:45.508543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Submission","metadata":{}},{"cell_type":"code","source":"#judge_percentile1 = 85\njudge_percentile2 = 70\n#judge_percentile2 = 50\n\n\n\"\"\"judge1_birds = [ \n        \"apapan\", \n        \"aniani\",\n        \"hawama\", \n        \"hawcre\", \n        \"houfin\",#13 ラベルトップ No.2 \n        \"jabwar\", \n        \"omao\",\n        \"skylar\",#13 ラベルトップ No.1\n    ]\njudge1_class = [i for i,s in enumerate(cfg.scored_birds) if s in judge1_birds]\nother_class = [i for i in range(len(cfg.scored_birds)) if not i in judge1_class]\"\"\"\njudge2_birds = [\n        \"barpet\", \n        \"crehon\",#5 ラベル少ないNo.2\n        \"elepai\", \n        \"ercfra\",#7 ラベル少ないNo.6\n        \"hawhaw\",#11 ラベル少ないNo.4\n        \"hawpet1\",#12 ラベルすくないNo.5\n        \"maupar\",#16 絶滅危惧種No.1\n    ]\njudge2_class = [i for i,s in enumerate(cfg.scored_birds) if s in judge2_birds]\nother_class = [i for i in range(len(cfg.scored_birds)) if not i in judge2_birds]","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:24:45.510456Z","iopub.execute_input":"2022-05-24T11:24:45.51084Z","iopub.status.idle":"2022-05-24T11:24:45.517874Z","shell.execute_reply.started":"2022-05-24T11:24:45.510803Z","shell.execute_reply":"2022-05-24T11:24:45.517181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#LBPBdataの予測平均をthreshouldとして予測\nthresholds = np.array([np.array(preds.mean(0)) for preds in proba_arr])\nthreshold80 = np.percentile(thresholds,80,axis=0)\nconcat_proba_arr = np.concatenate(proba_arr)\n#threshold_j1 = np.percentile(concat_proba_arr,judge_percentile1,axis=0)\nthreshold_j2 = np.percentile(concat_proba_arr,judge_percentile2,axis=0)\nprint(threshold80)\nthreshold = np.zeros(threshold80.shape)\nthreshold[judge2_class] = threshold_j2[judge2_class]\nthreshold[other_class] = threshold80[other_class]\nprint(threshold)\n#threshold = thresholds.mean(0)\n\n#レアラベルの場合は半分FP許容\naudio_level_pred_thre = np.percentile(thresholds,70,axis=0)\n\n#中間層以外はaudio_levelはしない\naudio_level_pred_thre[[0,1,3,4,5,6,9,10,11,12,15,17,18]] = 1.0\n\ndisplay(threshold,audio_level_pred_thre)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:24:45.519222Z","iopub.execute_input":"2022-05-24T11:24:45.519841Z","iopub.status.idle":"2022-05-24T11:24:45.537368Z","shell.execute_reply.started":"2022-05-24T11:24:45.519805Z","shell.execute_reply":"2022-05-24T11:24:45.536719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_dict = {}\nfor preds, file_id in zip(proba_arr,file_ids): \n    \n    #１度鳴いてる鳥は全部鳴いてる確率が高い\n    index = np.argwhere(preds.mean(0) > np.array(audio_level_pred_thre))\n    if len(index) != 0:\n        preds[:,index] = 1.0\n        \n    for idx in range(12):\n        last_time = int((idx+1)*5)\n        \n        #どうやら60秒未満のものがあるらしい[index 3000あたり？]\n        try:\n            probas = preds[idx]\n \n        except:\n            probas = probas\n        \n        row_id = file_id + \"_\" + str(last_time)   \n        labels = np.argwhere(probas >= threshold).reshape(-1).tolist()\n\n        if len(labels) == 0:\n            prediction_dict[str(row_id)] = \"nocall\"\n        else:\n            labels_str_list = list(map(lambda x: cfg.scored_birds[x], labels))\n            label_string = \" \".join(labels_str_list)\n            prediction_dict[str(row_id)] = label_string","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:24:45.538374Z","iopub.execute_input":"2022-05-24T11:24:45.538747Z","iopub.status.idle":"2022-05-24T11:24:45.548319Z","shell.execute_reply.started":"2022-05-24T11:24:45.538711Z","shell.execute_reply":"2022-05-24T11:24:45.547513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv('../input/birdclef-2022/sample_submission.csv')\nsample_submission[\"target\"] = True\nsample_submission","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:24:45.549681Z","iopub.execute_input":"2022-05-24T11:24:45.550041Z","iopub.status.idle":"2022-05-24T11:24:45.579931Z","shell.execute_reply.started":"2022-05-24T11:24:45.550007Z","shell.execute_reply":"2022-05-24T11:24:45.579259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(sample_submission)):\n    #soundscape_1000170626_akiapo_5 (sample)\n    sample = sample_submission.row_id[i]\n    #soundscape_1000170626_5 (key)\n    key = sample.split(\"_\")[0] + \"_\" + sample.split(\"_\")[1] + \"_\" + sample.split(\"_\")[3]\n    #akiapo (target_bird)\n    target_bird = sample.split(\"_\")[2]\n    print(key, target_bird)\n    # soundscape_1000170626_5 in {soundscape_1000170626_5:\"nocall\",'soundscape_453028782_5': 'nocall',....}\n    if key in prediction_dict:\n        #\"nocall\" (prediction_dicts[key]) → False\n        sample_submission.iat[i, 1] = (target_bird in prediction_dict[key])\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:24:45.582397Z","iopub.execute_input":"2022-05-24T11:24:45.582592Z","iopub.status.idle":"2022-05-24T11:24:45.595679Z","shell.execute_reply.started":"2022-05-24T11:24:45.58257Z","shell.execute_reply":"2022-05-24T11:24:45.594423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:24:45.596862Z","iopub.execute_input":"2022-05-24T11:24:45.597108Z","iopub.status.idle":"2022-05-24T11:24:45.605717Z","shell.execute_reply.started":"2022-05-24T11:24:45.597074Z","shell.execute_reply":"2022-05-24T11:24:45.604928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}