{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ../input/torchlibrosa/torchlibrosa-0.0.5-py3-none-any.whl > /dev/null","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport audioread\nimport logging\nimport gc\nimport os\nimport sys\nsys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\nimport random\nimport time\nimport warnings\n\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport soundfile as sf\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as torchdata\n\nfrom contextlib import contextmanager\nfrom joblib import Parallel, delayed\nfrom pathlib import Path\nfrom typing import Optional\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold\n\nfrom albumentations.core.transforms_interface import ImageOnlyTransform\nfrom torchlibrosa.stft import LogmelFilterBank, Spectrogram\nfrom torchlibrosa.augmentation import SpecAugmentation\nfrom tqdm import tqdm\n\nimport albumentations as A\nimport albumentations.pytorch.transforms as T\n\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SR = 32000\nUSE_SEC = 20\n\n\ndef Audio_to_Array(path):\n    y, sr = sf.read(path, always_2d=True)\n    y = np.mean(y, 1) # there is (X, 2) array\n    if len(y) > SR:\n        y = y[SR:-SR]\n    if len(y) < SR * USE_SEC:\n        return [y, y]\n    chunks = int(len(y)//(SR*USE_SEC))\n    Ys = []\n    for i in range(chunks):\n        Ys.append(y[i*SR*USE_SEC : (i+1)*SR*USE_SEC])\n    Ys.append(y[-SR*USE_SEC :])         \n    return Ys\n\n\nAUDIO_PATH = '../input/birdclef-2022/train_audio'\n\ntrain = pd.read_csv('../input/birdies/train_H.csv')\ntrain[\"file_path\"] = AUDIO_PATH + '/' + train['filename']\npaths = train[\"file_path\"].values","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_WORKERS = 4\nCLASSES = [\"akiapo\", \"aniani\", \"apapan\", \"barpet\", \"crehon\", \"elepai\", \"ercfra\", \"hawama\", \n           \"hawcre\", \"hawgoo\", \"hawhaw\", \"hawpet1\", \"houfin\", \"iiwi\", \"jabwar\", \"maupar\", \n           \"omao\", \"puaioh\", \"skylar\", \"warwhe1\", \"yefcan\"]\n#print(CLASSES)\nlens = []\n\nfor dir_ in CLASSES:\n    #print(dir_)\n    _ = os.makedirs('train_np/' + dir_, exist_ok=True)\n    \nfor apath in paths:\n    Yy = Audio_to_Array(apath)\n    chs = len(Yy)\n    #print(apath, '....', chs)\n    lens.append(chs)\n    for i in range(chs):\n        save_path = \"train_np/\" + \"/\".join(apath.split('/')[-2:])+'_'+str(i+1)\n        np.save(save_path, Yy[i])        \n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['chunks'] = lens\ntrain.to_csv('train_chunks.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}