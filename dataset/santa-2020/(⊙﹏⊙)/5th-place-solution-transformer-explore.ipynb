{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nimport math\nimport random\nimport pickle\nimport json\nimport copy\nimport os\nimport numpy as np\nimport bisect\nimport time\n# device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\ndevice = 'cpu'\n\nMaxSeq=120\nMaxNum=16\ndata_path = '../data/'\ncache_path = '../cache/'\nmodel_path = '/kaggle_simulations/agent/'\n# model_path = './'\n\ninit_prob = 0.5 \n\ndef get_mask(seq_len):\n    return torch.from_numpy( np.triu(np.ones((seq_len ,seq_len)), k=1).astype('bool')).to(device)\n\ndef get_pos(seq_len):\n    return torch.arange( seq_len ).unsqueeze(0).to(device)\n\n\ndef cnt_cut(x):\n    if x<=0:\n        return 0\n    else:\n        return x\n    \nclass EncModel(nn.Module):\n    def __init__(self, MaxNum, embed_dim=128, nlayers1=2, nlayers2=2, nheads=8, dropout=0):\n        super(EncModel, self).__init__()\n        self.MaxNum = MaxNum\n        self.embed_dim = embed_dim\n        self.pos_embd1 = nn.Embedding(MaxSeq, embed_dim)\n        self.pos_embd2 = nn.Embedding(self.MaxNum, embed_dim)\n        self.seq_embd = nn.Embedding(9, embed_dim)\n        self.type_embd = nn.Embedding(self.MaxNum+2, embed_dim)\n        self.cnt_embd = nn.Embedding(2002, embed_dim)\n\n        self.layer_normal1 = nn.LayerNorm(embed_dim)\n        encoder_layers1 = nn.TransformerEncoderLayer(embed_dim, nheads, embed_dim, dropout)\n        self.transformer_encoder1 = nn.TransformerEncoder(encoder_layers1, nlayers1)\n\n        self.layer_normal2 = nn.LayerNorm(embed_dim)\n        encoder_layers2 = nn.TransformerEncoderLayer(embed_dim, nheads, embed_dim, dropout)\n        self.transformer_encoder2 = nn.TransformerEncoder(encoder_layers2, nlayers2)\n        \n        self.pos1 = get_pos(MaxSeq)\n        self.pos2 = get_pos(self.MaxNum)\n        self.fc1 = nn.Linear(embed_dim, 1)\n\n    def forward(self,x,x_type,x_cnt):\n        sizeB,sizeN = x.shape[0],x.shape[1]\n#         x_ori = x\n        x = self.seq_embd(x) # x.shape  B * N * S\n        pos1 = self.pos_embd1(self.pos1)\n        x = x + pos1\n\n        x = x.reshape(sizeB*sizeN,MaxSeq,-1)\n        x = x.permute(1, 0, 2)\n        x = self.layer_normal1(x)\n        x = self.transformer_encoder1(x)\n        x = x[-1,:,:].reshape(sizeB,sizeN,-1)\n        \n        pos2 = self.pos_embd2(self.pos2)\n        x_type = self.type_embd(x_type)\n        x_cnt = self.cnt_embd(x_cnt)\n        x = x + pos2 + x_type #+ x_cnt\n        \n        x = x.permute(1, 0, 2) # x.shape   N * B * D\n        x = self.layer_normal2(x)\n        x = self.transformer_encoder2(x)\n        x = x.permute(1, 0, 2) # x.shape   B * N * D\n\n        out = self.fc1(x).squeeze(-1) # x.shape   B * N\n        return out\n    \nmodel = EncModel(MaxNum=16,\n                 embed_dim=128,\n                 nlayers1=2,\n                 nlayers2=3,\n                 nheads=4,\n            )\nmodel.load_state_dict(torch.load(model_path+'santa1_16000_{}.pt'.format(MaxNum),map_location=device))\nmodel.to(device)\n\ndecay = {}\nfor pos in range(4000):\n    if pos < 200: \n        decay[pos] = 0.97**pos\n    else:\n        decay[pos] = 0\n\n\n# (-2,-2):0, (-2,-1):1, (0,-2):2, (1,-2):3, (0,-1):4, (1,-1):5\ndef update(sample,m,rtn,rtn_action,i):\n    m = m.copy()\n    rtn = rtn.copy()\n    rtn_action = rtn_action.copy()\n    \n    my_action = sample[1][i]\n    opp_action = sample[2][i]\n    my_reward = sample[3][i]\n\n    if i>0: # 若上一次的机器这次没人选，则占一个0\n        last_my_action = sample[1][i-1]\n        last_opp_action = sample[2][i-1]\n\n        if last_my_action != my_action and last_my_action != opp_action and m[last_my_action][0] == -1:\n            m[last_my_action] = np.append(m[last_my_action][1:],0)\n        if last_my_action != last_opp_action and last_opp_action != my_action and last_opp_action != opp_action and m[last_opp_action][0] == -1:\n            m[last_opp_action] = np.append(m[last_opp_action][1:],0)\n\n    if my_action == opp_action and m[my_action][0] == -1:\n        if my_reward:\n            m[my_action] = np.append(m[my_action][1:], 5)\n        else:\n            m[my_action] = np.append(m[my_action][1:], 4)\n    else:\n        if m[opp_action][0] == -1:\n            m[opp_action] = np.append(m[opp_action][1:], 1)\n        \n        if m[my_action][0] == -1:\n            if my_reward:\n                m[my_action] = np.append(m[my_action][1:], 3)\n            else:\n                m[my_action] = np.append(m[my_action][1:], 2)\n\n    rtn[:-1,:] = rtn[1:,:]\n    rtn[-1,:] = m[opp_action].copy()\n    \n    rtn_action[:-1] = rtn_action[1:].copy()\n    rtn_action[-1] = opp_action\n\n    return m,rtn,rtn_action\n\n\ndef get_action(step):\n    m_score = [0 for i in range(100)]\n    \n    t_prob = copy.deepcopy(m_prob)\n    for i in range(100):\n        t_prob[i] = m_prob[i]*decay[m_cnt[i]]\n        \n        if m_cnt[i] == 1 and m_cnt_my[i] == 0:\n            t_prob[i] = min(t_prob[i],init_prob*decay[m_cnt[i]])\n        \n    if step>1950 or step<20:\n        return np.argmax(t_prob + np.random.random(100)/1e7).item() # 每次就选最大的\n    \n    thre = np.percentile(t_prob,90)\n    op_machine_max = -1\n    for action in op_machine:\n        op_machine_max = max(op_machine_max,t_prob[action])\n        m_score[action] = t_prob[action]\n\n    rate = (1 - m_cnt_my[op_action_list[-1]]/m_cnt[op_action_list[-1]])*(1-m_cnt_my[op_action_list[-1]]/20)\n    rate = min(0.05,max(0,rate/5))\n    if m_cnt[op_action_list[-1]] == 1 and m_cnt_my[op_action_list[-1]] == 0:\n        rate = 0\n    if t_prob[op_action_list[-1]]*(1+rate) > thre:\n        return op_action_list[-1]\n\n    for action in range(100):\n        if t_prob[action] >= thre and action not in my_action_list[-5:]: #可以进行探索\n            m_score[action] = t_prob[action]+10000-m_cnt[action] # 哪个数量少选哪个  数量一样选概率最大的\n        else:\n            m_score[action] = t_prob[action]\n\n    return np.argmax(m_score + np.array(range(100))/1e7).item()\n    \n\ntime_list = []\n\ntotal_reward = 0\nlabel = []\nmy_action_list = []\nop_action_list = []\nmy_reward_list = []\nop_machine = set()\n\nmachine = [np.zeros(MaxSeq,dtype='int')-1 for _ in range(100)]\nrtn = np.zeros([MaxNum,MaxSeq],dtype='int8')-1\nrtn_action = np.zeros(MaxNum,dtype='int8') - 1\nm_cnt = [0 for _ in range(100)]\nm_cnt_op = [0 for _ in range(100)]\nm_cnt_my = [0 for _ in range(100)]\n\nm_prob = [init_prob for _ in range(100)]\n\nm_prob_list = []\nm_cnt_list = []\ndef agent(observation, configuration):\n    s_time = time.time()\n    global total_reward,my_action_list,op_action_list,my_reward_list,machine,rtn,rtn_action,m_prob,time_list\n    if observation['step'] == 0:\n        my_pull = random.randrange(100)\n        total_reward = 0\n    else:\n        last_reward = observation['reward'] - total_reward\n        total_reward = observation['reward']\n        \n        my_idx = observation['agentIndex']\n        my_last_action = observation['lastActions'][my_idx]\n        op_last_action = observation['lastActions'][1-my_idx]\n        m_cnt[my_last_action]+=1\n        m_cnt[op_last_action]+=1\n        m_cnt_op[op_last_action]+=1\n        m_cnt_my[my_last_action]+=1\n        op_machine.add(op_last_action)\n        \n        my_action_list.append(my_last_action)\n        op_action_list.append(op_last_action)\n        my_reward_list.append(last_reward)\n        \n        sample = (label,my_action_list,op_action_list,my_reward_list)# 仿照训练时的数据格式\n        \n        machine, rtn, rtn_action = update(sample,machine,rtn,rtn_action,len(sample[1])-1)\n        \n        # 机器序列\n        x = np.zeros([MaxNum,MaxSeq],dtype='int8')-1\n        x[-1,:] = machine[my_last_action].copy()\n        x[:-1,:] = rtn[1:,:].copy()\n        x = x+1\n        x = torch.tensor(x).to(device).long().unsqueeze(0)\n        \n        # 对方动作\n        x_rtn_action = np.zeros(MaxNum,dtype='int8')-1\n        x_rtn_action[:-1] = rtn_action[1:].copy()\n        x_rtn_action[-1] = my_last_action\n        \n        \n        # 机器类型\n        dic_tmp = {}\n        x_rtn_type = np.zeros(MaxNum,dtype='int8')\n        for i,action in enumerate(x_rtn_action[::-1]):\n            if action == -1:\n                break\n            if action not in dic_tmp:\n                dic_tmp[action] = len(dic_tmp)+1\n                \n            x_rtn_type[MaxNum-1-i] = dic_tmp[action]\n        x_rtn_type = torch.tensor(x_rtn_type).to(device).long().unsqueeze(0)\n            \n        # 当前次数\n        pos =  len(my_action_list)-1\n        x_rtn_cnt = []\n        for i in range(MaxNum):\n            if i == MaxNum - 1:\n                x_rtn_cnt.append(cnt_cut(pos+1))\n            else:\n                x_rtn_cnt.append(cnt_cut(pos-MaxNum+i+3))\n        x_rtn_cnt = np.array(x_rtn_cnt)\n        x_rtn_cnt = torch.tensor(x_rtn_cnt).to(device).long().unsqueeze(0)\n        \n        output = model(x,x_rtn_type,x_rtn_cnt)\n        for i,action in enumerate(x_rtn_action):\n            if action == -1:\n                continue\n            m_prob[action] = output[0][i].item()/100\n            \n#         print('rtn_action',rtn_action)\n#         print('machice[my_last_action]',machine[my_last_action])\n#         print('machice[op_last_action]',machine[op_last_action])\n#         print(m_prob)\n#         print('m_prob',m_prob)\n#         print('tttt',len(m_prob))\n#         print()\n        m_prob_list.append(m_prob.copy())\n        m_cnt_list.append(m_cnt.copy())\n        my_pull = get_action(observation['step'])\n    \n        print('op_last_action:',m_prob[op_last_action]*decay[m_cnt[op_last_action]])\n    time_list.append(time.time()-s_time)\n\n    return my_pull\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}