{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook shows how to use multi-armed bandit.\n\nMulti-armed bandit is a widely used RL-algorithm because it is very balanced in terms of exploitation/exploration.\n\nAlgorithm logic:\n\n- At each step for each bandit generate a random number from B(a+1, b+1). B - beta-distribution, a - decay adjusted total reward from this bandit, b - number of this bandits's historical losses.\n- Select the bandit with the largest generated number and use it to generate the next step.\n- Repeat"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install kaggle-environments --upgrade","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%writefile submission.py\n\nimport json\nimport numpy as np\nimport pandas as pd\n\nbandit_state = None\ntotal_reward = 0\nlast_step = None\n    \ndef multi_armed_bandit_agent (observation, configuration):\n    global history, history_bandit\n\n    no_reward_step = 0.3\n    decay_rate = 0.97 # how much do we decay the win count after each call\n    \n    global bandit_state,total_reward,last_step\n        \n    if observation.step == 0:\n        # initial bandit state\n        bandit_state = [[1,1] for i in range(configuration[\"banditCount\"])]\n    else:       \n        # updating bandit_state using the result of the previous step\n        last_reward = observation[\"reward\"] - total_reward\n        total_reward = observation[\"reward\"]\n        \n        # we need to understand who we are Player 1 or 2\n        player = int(last_step == observation.lastActions[1])\n        \n        if last_reward > 0:\n            bandit_state[observation.lastActions[player]][0] += last_reward\n        else:\n            bandit_state[observation.lastActions[player]][1] += no_reward_step\n        \n        bandit_state[observation.lastActions[0]][0] = (bandit_state[observation.lastActions[0]][0] - 1) * decay_rate + 1\n        bandit_state[observation.lastActions[1]][0] = (bandit_state[observation.lastActions[1]][0] - 1) * decay_rate + 1\n\n#     generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in range(configuration[\"banditCount\"]):\n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n        \n    last_step = best_agent\n    return best_agent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile random_agent.py\n\nimport random\n\ndef random_agent(observation, configuration):\n    return random.randrange(configuration.banditCount)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"mab\", debug=True)\n\nenv.reset()\nenv.run([\"random_agent.py\", \"submission.py\"])\nenv.render(mode=\"ipython\", width=800, height=700)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.reset()\nenv.run([\"submission.py\", \"submission.py\"])\nenv.render(mode=\"ipython\", width=800, height=700)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}