{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1> A Beginner's Approach to Performance Ananlysis</h1>"},{"metadata":{},"cell_type":"markdown","source":"Welcome to my first notebook! <br><br> As a beginner, most of the discussions and the math went right over my head. I found [this wonderful notebook](https://www.kaggle.com/sirishks/pull-vegas-slot-machines) by [sirishks](https://www.kaggle.com/sirishks), with a fairly easy-to-understand strategy. I made a few agents based off of this, but wanted a way to properly compare. <br> <br> <br> Keeping in mind that the main goal of our agent is to pick the vending machine with the highest chances of giving us a candy, I tried to graph the chances chosen by our agents with the maximum chance possible at that step. <br><br>\n**Changes in the V3**:\n\n* Now we have a subplot comparing the accuracy of the thresholds expected by the agents.\n* We run a match against an agent returning only 0, so we have a better idea on how accurately our agents can predict thresholds without an opponent interfering."},{"metadata":{"_kg_hide-output":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"!pip install kaggle-environments --upgrade -q\n\nfrom kaggle_environments import make, evaluate\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nenv = make(\"mab\", debug=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def divide_steps(game, ind):\n  red_steps, green_steps, blue_steps = [], [], []\n  for i in range(2000):\n    pres_prob = game[i][0].observation.thresholds[game[i][ind].action]\n    sorted_thres = np.sort(game[i][0].observation.thresholds)\n    if pres_prob >= sorted_thres[-30]:\n      green_steps.append(i)\n    elif pres_prob >= sorted_thres[-60]:\n      blue_steps.append(i)\n    else:\n      red_steps.append(i)\n  return red_steps, blue_steps, green_steps","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style='background:#FBE338; border:0; color:black'><center>Random Agent<center><h2>\n\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%writefile random.py\n\nimport random\n\ndef agent(obs, conf):\n    return random.randrange(conf.banditCount)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style='background:#FBE338; border:0; color:black'><center>Return 0 Agent<center><h2>\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile return0.py\n\nimport random\n\ndef agent(obs, conf):\n    return 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style='background:#FBE338; border:0; color:black'><center>Vegas Slot Pulls Agent<center></h2>\n    \nCopied from [here.](https://www.kaggle.com/sirishks/pull-vegas-slot-machines)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%writefile vegas_pull.py\n\nimport numpy as np\nimport pandas as pd\nimport random, os, datetime\n\ntotal_reward = 0\nbandit_dict = {}\ndf_probs = []\ndf_steps = []\n\ndef set_seed(my_seed=42):\n    os.environ['PYTHONHASHSEED'] = str(my_seed)\n    random.seed(my_seed)\n    np.random.seed(my_seed)\n\ndef get_next_bandit(step):\n    global df_steps, df_probs\n    best_bandit = 0\n    best_bandit_expected = 0\n    for bnd in bandit_dict:\n        expect = (bandit_dict[bnd]['win'] - bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'] - (bandit_dict[bnd]['opp']>0)*1.5) \\\n                 / (bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'])\n        if expect > best_bandit_expected:\n            best_bandit_expected = expect\n            best_bandit = bnd\n    df_steps.append(step)\n    df_probs.append(best_bandit_expected*100)\n    return best_bandit\n\ndef multi_armed_probabilities(observation, configuration):\n    global total_reward, bandit_dict\n    global df_steps, df_probs\n    my_pull = random.randrange(configuration['banditCount'])\n    if 0 == observation['step']:\n        set_seed()\n        total_reward = 0\n        bandit_dict = {}\n        for i in range(configuration['banditCount']):\n            bandit_dict[i] = {'win': 1, 'loss': 0, 'opp': 0}\n    else:\n        last_reward = observation['reward'] - total_reward\n        total_reward = observation['reward']\n        \n        my_idx = observation['agentIndex']\n        if 0 < last_reward:\n            bandit_dict[observation['lastActions'][my_idx]]['win'] = bandit_dict[observation['lastActions'][my_idx]]['win'] +1\n        else:\n            bandit_dict[observation['lastActions'][my_idx]]['loss'] = bandit_dict[observation['lastActions'][my_idx]]['loss'] +1\n        bandit_dict[observation['lastActions'][1-my_idx]]['opp'] = bandit_dict[observation['lastActions'][1-my_idx]]['opp'] +1\n        my_pull = get_next_bandit(observation.step)\n    if observation.step > 1995:\n        df = pd.DataFrame(df_probs, index=df_steps, columns=['probs'])\n        df.to_csv(\"vegas_pull.csv\")\n    return my_pull","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"agent = \"vegas_pull\"\nmygame = env.run([\"random.py\", f\"{agent}.py\"])\n\n\nfig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(2, 2, figsize=(16,12))\n\nred_steps, blue_steps, green_steps = divide_steps(mygame, 1)\nax1.scatter(green_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in green_steps], color='green', label='Chosen bandit ranks 1-30th thresold-wise')\nax1.scatter(blue_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in blue_steps], color='blue', label='Chosen bandit ranks 31-60th thresold-wise')\nax1.scatter(red_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in red_steps], color='red', label='Chosen bandit ranks 61-100th threshold-wise')\nax1.plot([x for x in range(2000)], [max(mygame[i][0].observation.thresholds) for i in range(2000)], 'C1', label='Maximum chances of getting a candy at that step')\nax1.set_title(\"vs Random agent: Chances of getting a candy\")\nax1.legend(loc='upper right')\n\ndf = pd.read_csv(f\"{agent}.csv\", index_col=[0])\nred_steps, blue_steps, green_steps = [i for i in red_steps if i in df.index], [i for i in blue_steps if i in df.index], [i for i in green_steps if i in df.index]\nax2.scatter(green_steps, df.loc[green_steps, 'probs'], color='green', label='Actual threshold ranks 1-30th')\nax2.scatter(blue_steps, df.loc[blue_steps, 'probs'], color='blue', label='Actual threshold ranks 31-60th')\nax2.scatter(red_steps, df.loc[red_steps, 'probs'], color='red', label='Actual threshold ranks 61-100th')\nax2.set_title(\"Thresholds as expected by the agent\")\nax2.legend(loc='upper right')\n\nmygame = env.run([\"return0.py\", f\"{agent}.py\"])\n\nred_steps, blue_steps, green_steps = divide_steps(mygame, 1)\nax3.scatter(green_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in green_steps], color='green', label='Chosen bandit ranks 1-30th thresold-wise')\nax3.scatter(blue_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in blue_steps], color='blue', label='Chosen bandit ranks 31-60th thresold-wise')\nax3.scatter(red_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in red_steps], color='red', label='Chosen bandit ranks 61-100th threshold-wise')\nax3.plot([x for x in range(2000)], [max(mygame[i][0].observation.thresholds) for i in range(2000)], 'C1', label='Maximum chances of getting a candy at that step')\nax3.set_title(\"vs Return0 agent: Chances of getting a candy\")\nax3.legend(loc='upper right')\n\ndf = pd.read_csv(f\"{agent}.csv\", index_col=[0])\nred_steps, blue_steps, green_steps = [i for i in red_steps if i in df.index], [i for i in blue_steps if i in df.index], [i for i in green_steps if i in df.index]\nax4.scatter(green_steps, df.loc[green_steps, 'probs'], color='green', label='Actual threshold ranks 1-30th')\nax4.scatter(blue_steps, df.loc[blue_steps, 'probs'], color='blue', label='Actual threshold ranks 31-60th')\nax4.scatter(red_steps, df.loc[red_steps, 'probs'], color='red', label='Actual threshold ranks 61-100th')\nax4.set_title(\"Thresholds as expected by the agent\")\nax4.legend(loc='upper right')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, Vegas Pull doesn't do quite well if the opponent picks a lot of wrong machines because of:\n\n>expect = (bandit_dict[bnd]['win'] - bandit_dict[bnd]['loss'] **+ bandit_dict[bnd]['opp'] - (bandit_dict[bnd]['opp']>0)*1.5)** /(bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'])\n\nBut it won't quite matter in real matches I think, if your opponent is making such bad choices, vegas_pull.py is gonna win anyway.\n\nAs for normal matches:\nIn the steps 100-500~, this agent has one of the best intersections between the expected and the actual thresholds. And keep in mind, first few hundred steps are the time you want to suck off the big bandits, and this agent does the best job at that, no wonder it's on the top of the leaderboard."},{"metadata":{},"cell_type":"markdown","source":"<h2 style='background:#FBE338; border:0; color:black'><center>UCB With Decay<center></h2>\n    \nCopied from [here](https://www.kaggle.com/xhlulu/santa-2020-ucb-and-bayesian-ucb-starter)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%writefile ucb_decay.py\n\nimport numpy as np\nimport pandas as pd\n\ndecay = 0.97\ntotal_reward = 0\nbandit = None\ndf_steps = []\ndf_probs = []\n\ndef agent(observation, configuration):\n    global reward_sums, n_selections, total_reward, bandit\n    global df_steps, df_probs\n    n_bandits = configuration.banditCount\n\n    if observation.step == 0:\n        n_selections, reward_sums = np.full((2, n_bandits), 1e-32)\n    else:\n        reward_sums[bandit] += decay * (observation.reward - total_reward)\n        total_reward = observation.reward\n\n    avg_reward = reward_sums / n_selections    \n    delta_i = np.sqrt(2 * np.log(observation.step + 1) / n_selections)\n    bandit = int(np.argmax(avg_reward + delta_i))\n    \n    best_avg_reward = 1\n    best_delta_i = np.sqrt(2 * np.log(observation.step + 1) / n_selections[bandit])\n    best_chance = best_avg_reward + best_delta_i\n    # I'm NOT sure of the above math, corrections if any, would be very welcome.\n    # What I understood and learnt is, we are creating an 'interval'\n    # as to how unlucky/lucky the agent could be, as:\n    # [rewards(n)/num_called(n) - root:c*log(n)/num_called(n), rewards(n)/num_called(n) + root:c*log(n)/num_called(n)]\n    # as n increases, delta further decreases, and the interval gets smaller,\n    # hence further increasing our 'confidence' on the agent.\n    # To convert it into a % , I divided the chosen bandits upper confidence level with the\n    # best possible at that step for that bandit...\n    df_steps.append(observation.step)\n    df_probs.append(100 * np.max(avg_reward + delta_i) / best_chance)\n    \n    if observation.step > 1995:\n        df = pd.DataFrame(df_probs, index=df_steps, columns=['probs'])\n        df.to_csv(\"ucb_decay.csv\")\n\n    n_selections[bandit] += 1\n\n    return bandit","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"agent = \"ucb_decay\"\nmygame = env.run([\"random.py\", f\"{agent}.py\"])\n\n\nfig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(2, 2, figsize=(16,12))\n\nred_steps, blue_steps, green_steps = divide_steps(mygame, 1)\nax1.scatter(green_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in green_steps], color='green', label='Chosen bandit ranks 1-30th thresold-wise')\nax1.scatter(blue_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in blue_steps], color='blue', label='Chosen bandit ranks 31-60th thresold-wise')\nax1.scatter(red_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in red_steps], color='red', label='Chosen bandit ranks 61-100th threshold-wise')\nax1.plot([x for x in range(2000)], [max(mygame[i][0].observation.thresholds) for i in range(2000)], 'C1', label='Maximum chances of getting a candy at that step')\nax1.set_title(\"vs Random agent: Chances of getting a candy\")\nax1.legend(loc='upper right')\n\ndf = pd.read_csv(f\"{agent}.csv\", index_col=[0])\nred_steps, blue_steps, green_steps = [i for i in red_steps if i in df.index], [i for i in blue_steps if i in df.index], [i for i in green_steps if i in df.index]\nax2.scatter(green_steps, df.loc[green_steps, 'probs'], color='green', label='Actual threshold ranks 1-30th')\nax2.scatter(blue_steps, df.loc[blue_steps, 'probs'], color='blue', label='Actual threshold ranks 31-60th')\nax2.scatter(red_steps, df.loc[red_steps, 'probs'], color='red', label='Actual threshold ranks 61-100th')\nax2.set_title(\"Thresholds as expected by the agent\")\nax2.legend(loc='upper right')\n\nmygame = env.run([\"return0.py\", f\"{agent}.py\"])\n\nred_steps, blue_steps, green_steps = divide_steps(mygame, 1)\nax3.scatter(green_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in green_steps], color='green', label='Chosen bandit ranks 1-30th thresold-wise')\nax3.scatter(blue_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in blue_steps], color='blue', label='Chosen bandit ranks 31-60th thresold-wise')\nax3.scatter(red_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in red_steps], color='red', label='Chosen bandit ranks 61-100th threshold-wise')\nax3.plot([x for x in range(2000)], [max(mygame[i][0].observation.thresholds) for i in range(2000)], 'C1', label='Maximum chances of getting a candy at that step')\nax3.set_title(\"vs Return0 agent: Chances of getting a candy\")\nax3.legend(loc='upper right')\n\ndf = pd.read_csv(f\"{agent}.csv\", index_col=[0])\nred_steps, blue_steps, green_steps = [i for i in red_steps if i in df.index], [i for i in blue_steps if i in df.index], [i for i in green_steps if i in df.index]\nax4.scatter(green_steps, df.loc[green_steps, 'probs'], color='green', label='Actual threshold ranks 1-30th')\nax4.scatter(blue_steps, df.loc[blue_steps, 'probs'], color='blue', label='Actual threshold ranks 31-60th')\nax4.scatter(red_steps, df.loc[red_steps, 'probs'], color='red', label='Actual threshold ranks 61-100th')\nax4.set_title(\"Thresholds as expected by the agent\")\nax4.legend(loc='upper right')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The UCB decay agent seems to overestimate the thresholds by a lot, but at the same time, no other agent so regularly makes as many decisions so close to the best-threshold line. This agent could perform magnitudes better if the number of decisions below the 50% are further decreased, which I am very interested in working towards."},{"metadata":{},"cell_type":"markdown","source":"<h2 style='background:#FBE338; border:0; color:black'><center>Bayesian UCB<center></h2>\n    \nCopied from [here](https://www.kaggle.com/xhlulu/santa-2020-ucb-and-bayesian-ucb-starter)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%writefile bayesian_ucb.py\n\nimport numpy as np\nfrom scipy.stats import beta\nimport pandas as pd\n\npost_a, post_b, bandit = [None] * 3\ntotal_reward = 0\nc = 3\ndf_steps = []\ndf_probs = []\n\ndef agent(observation, configuration):\n    global total_reward, bandit, post_a, post_b, c\n    global df_steps, df_probs\n    if observation.step == 0:\n        post_a, post_b = np.ones((2, configuration.banditCount))\n    else:\n        r = observation.reward - total_reward\n        total_reward = observation.reward\n        # Update Gaussian posterior\n        post_a[bandit] += r\n        post_b[bandit] += 1 - r\n    \n    bound = post_a / (post_a + post_b) + beta.std(post_a, post_b) * c\n    bandit = int(np.argmax(bound))\n    \n    # Again, like with the ucb-decay, this method creates an interval, and chooses\n    # a bandit which could be the luckiest. For this,they have used population standard deviation for deciding the interval width.\n    # We are going to do the same as before i.e, create an ideal bandit and compare the Upper bounds for the expected %.\n    best_a, best_b = np.array(post_a), np.array(post_b)\n    best_a[bandit] += best_b[bandit] - 1\n    best_b[bandit] = 1 #created an ideal agent.\n    best_bound = best_a / (best_a + best_b) + beta.std(best_a, best_b) * c\n    \n    df_probs.append(100 * np.max(bound) / best_bound[bandit])\n    df_steps.append(observation.step)\n    \n    if observation.step > 1995:\n        df = pd.DataFrame(df_probs, index=df_steps, columns=['probs'])\n        df.to_csv(\"bayesian_ucb.csv\")\n    return bandit","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"agent = \"bayesian_ucb\"\nmygame = env.run([\"random.py\", f\"{agent}.py\"])\n\n\nfig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(2, 2, figsize=(16,12))\n\nred_steps, blue_steps, green_steps = divide_steps(mygame, 1)\nax1.scatter(green_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in green_steps], color='green', label='Chosen bandit ranks 1-30th thresold-wise')\nax1.scatter(blue_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in blue_steps], color='blue', label='Chosen bandit ranks 31-60th thresold-wise')\nax1.scatter(red_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in red_steps], color='red', label='Chosen bandit ranks 61-100th threshold-wise')\nax1.plot([x for x in range(2000)], [max(mygame[i][0].observation.thresholds) for i in range(2000)], 'C1', label='Maximum chances of getting a candy at that step')\nax1.set_title(\"vs Random agent: Chances of getting a candy\")\nax1.legend(loc='upper right')\n\ndf = pd.read_csv(f\"{agent}.csv\", index_col=[0])\nred_steps, blue_steps, green_steps = [i for i in red_steps if i in df.index], [i for i in blue_steps if i in df.index], [i for i in green_steps if i in df.index]\nax2.scatter(green_steps, df.loc[green_steps, 'probs'], color='green', label='Actual threshold ranks 1-30th')\nax2.scatter(blue_steps, df.loc[blue_steps, 'probs'], color='blue', label='Actual threshold ranks 31-60th')\nax2.scatter(red_steps, df.loc[red_steps, 'probs'], color='red', label='Actual threshold ranks 61-100th')\nax2.set_title(\"Thresholds as expected by the agent\")\nax2.legend(loc='upper right')\n\nmygame = env.run([\"return0.py\", f\"{agent}.py\"])\n\nred_steps, blue_steps, green_steps = divide_steps(mygame, 1)\nax3.scatter(green_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in green_steps], color='green', label='Chosen bandit ranks 1-30th thresold-wise')\nax3.scatter(blue_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in blue_steps], color='blue', label='Chosen bandit ranks 31-60th thresold-wise')\nax3.scatter(red_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in red_steps], color='red', label='Chosen bandit ranks 61-100th threshold-wise')\nax3.plot([x for x in range(2000)], [max(mygame[i][0].observation.thresholds) for i in range(2000)], 'C1', label='Maximum chances of getting a candy at that step')\nax3.set_title(\"vs Return0 agent: Chances of getting a candy\")\nax3.legend(loc='upper right')\n\ndf = pd.read_csv(f\"{agent}.csv\", index_col=[0])\nred_steps, blue_steps, green_steps = [i for i in red_steps if i in df.index], [i for i in blue_steps if i in df.index], [i for i in green_steps if i in df.index]\nax4.scatter(green_steps, df.loc[green_steps, 'probs'], color='green', label='Actual threshold ranks 1-30th')\nax4.scatter(blue_steps, df.loc[blue_steps, 'probs'], color='blue', label='Actual threshold ranks 31-60th')\nax4.scatter(red_steps, df.loc[red_steps, 'probs'], color='red', label='Actual threshold ranks 61-100th')\nax4.set_title(\"Thresholds as expected by the agent\")\nax4.legend(loc='upper right')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The agent seems to heavily overestimate the thresholds, and I'm very much doubting my math and logic on this. But at the same time, there seem to be similarities in the graphs, and the percentages aren't really breaking any rules, so I'm confused. Feedback would really be appreciated. As of yet, the best agent in terms of keeping it's decisions near and above the 50% threshold mark. Fills exactly where the ucb_decay lacked, maybe a hybrid agent would do the job? :P"},{"metadata":{},"cell_type":"markdown","source":"<h2 style='background:#FBE338; border:0; color:black'><center>Simple Multi-armed Bandit<center></h2>\n    \nCopied from [here](https://www.kaggle.com/ilialar/simple-multi-armed-bandit)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%writefile multi_armed_bandit.py\n\nimport json\nimport numpy as np\nimport pandas as pd\n\nbandit_state = None\ntotal_reward = 0\nlast_step = None\ndf_steps = []\ndf_probs = []\n\ndef multi_armed_bandit_agent (observation, configuration):\n    global history, history_bandit\n    global df_steps, df_probs\n    step = 1.0 #you can regulate exploration / exploitation balacne using this param\n    decay_rate = 0.97 # how much do we decay the win count after each call\n    \n    global bandit_state,total_reward,last_step\n        \n    if observation.step == 0:\n        # initial bandit state\n        bandit_state = [[1,1] for i in range(configuration[\"banditCount\"])]\n    else:       \n        # updating bandit_state using the result of the previous step\n        last_reward = observation[\"reward\"] - total_reward\n        total_reward = observation[\"reward\"]\n        \n        # we need to understand who we are Player 1 or 2\n        player = int(last_step == observation.lastActions[1])\n        \n        if last_reward > 0:\n            bandit_state[observation.lastActions[player]][0] += last_reward * step\n        else:\n            bandit_state[observation.lastActions[player]][1] += step\n        \n        bandit_state[observation.lastActions[0]][0] = (bandit_state[observation.lastActions[0]][0] - 1) * decay_rate + 1\n        bandit_state[observation.lastActions[1]][0] = (bandit_state[observation.lastActions[1]][0] - 1) * decay_rate + 1\n\n#     generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in range(configuration[\"banditCount\"]):\n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n    \n    df_steps.append(observation.step)\n    df_probs.append(best_proba * 100)\n    if observation.step > 1995:\n        df = pd.DataFrame(df_probs, index=df_steps, columns=['probs'])\n        df.to_csv(\"multi_armed_bandit.csv\")\n    last_step = best_agent\n    return best_agent","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nagent = \"multi_armed_bandit\"\nmygame = env.run([\"random.py\", f\"{agent}.py\"])\n\n\nfig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(2, 2, figsize=(16,12))\n\nred_steps, blue_steps, green_steps = divide_steps(mygame, 1)\nax1.scatter(green_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in green_steps], color='green', label='Chosen bandit ranks 1-30th thresold-wise')\nax1.scatter(blue_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in blue_steps], color='blue', label='Chosen bandit ranks 31-60th thresold-wise')\nax1.scatter(red_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in red_steps], color='red', label='Chosen bandit ranks 61-100th threshold-wise')\nax1.plot([x for x in range(2000)], [max(mygame[i][0].observation.thresholds) for i in range(2000)], 'C1', label='Maximum chances of getting a candy at that step')\nax1.set_title(\"vs Random agent: Chances of getting a candy\")\nax1.legend(loc='upper right')\n\ndf = pd.read_csv(f\"{agent}.csv\", index_col=[0])\nred_steps, blue_steps, green_steps = [i for i in red_steps if i in df.index], [i for i in blue_steps if i in df.index], [i for i in green_steps if i in df.index]\nax2.scatter(green_steps, df.loc[green_steps, 'probs'], color='green', label='Actual threshold ranks 1-30th')\nax2.scatter(blue_steps, df.loc[blue_steps, 'probs'], color='blue', label='Actual threshold ranks 31-60th')\nax2.scatter(red_steps, df.loc[red_steps, 'probs'], color='red', label='Actual threshold ranks 61-100th')\nax2.set_title(\"Thresholds as expected by the agent\")\nax2.legend(loc='upper right')\n\nmygame = env.run([\"return0.py\", f\"{agent}.py\"])\n\nred_steps, blue_steps, green_steps = divide_steps(mygame, 1)\nax3.scatter(green_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in green_steps], color='green', label='Chosen bandit ranks 1-30th thresold-wise')\nax3.scatter(blue_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in blue_steps], color='blue', label='Chosen bandit ranks 31-60th thresold-wise')\nax3.scatter(red_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in red_steps], color='red', label='Chosen bandit ranks 61-100th threshold-wise')\nax3.plot([x for x in range(2000)], [max(mygame[i][0].observation.thresholds) for i in range(2000)], 'C1', label='Maximum chances of getting a candy at that step')\nax3.set_title(\"vs Return0 agent: Chances of getting a candy\")\nax3.legend(loc='upper right')\n\ndf = pd.read_csv(f\"{agent}.csv\", index_col=[0])\nred_steps, blue_steps, green_steps = [i for i in red_steps if i in df.index], [i for i in blue_steps if i in df.index], [i for i in green_steps if i in df.index]\nax4.scatter(green_steps, df.loc[green_steps, 'probs'], color='green', label='Actual threshold ranks 1-30th')\nax4.scatter(blue_steps, df.loc[blue_steps, 'probs'], color='blue', label='Actual threshold ranks 31-60th')\nax4.scatter(red_steps, df.loc[red_steps, 'probs'], color='red', label='Actual threshold ranks 61-100th')\nax4.set_title(\"Thresholds as expected by the agent\")\nax4.legend(loc='upper right')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I feel bad for saying the previous agents were overestimating the probabilities, looks like they all do. It's understandable that there are only a few(20~) steps per bandit, but considering these agents target only the best bandits, I was expecting they atleast come close in the last few hundred steps, but doesn't seem to be the case."},{"metadata":{},"cell_type":"markdown","source":"<h2 style='background:#FBE338; border:0; color:black'><center>Thompson Sampling<center></h2>\n    \nCopied from [here](https://www.kaggle.com/ilialar/simple-multi-armed-bandit)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%writefile thompson.py\n\nimport numpy as np\nimport pandas as pd\n\npost_a = None\npost_b = None\nbandit = None\ntotal_reward = 0\nc = 3\ndf_steps = []\ndf_probs = []\n\ndef agent(observation, configuration):\n    global reward_sums, total_reward, bandit, post_a, post_b, c\n    global df_steps, df_probs\n    \n    n_bandits = configuration.banditCount\n\n    if observation.step == 0:\n        post_a = np.ones(n_bandits)\n        post_b = np.ones(n_bandits)\n    else:\n        r = observation.reward - total_reward\n        total_reward = observation.reward\n\n        # Update Gaussian posterior\n        post_a[bandit] += r\n        post_b[bandit] += (1 - r)\n\n    samples = np.random.beta(post_a, post_b)\n    bandit = int(np.argmax(samples))\n    \n    df_steps.append(observation.step)\n    df_probs.append(samples[bandit] * 100)\n    if observation.step > 1995:\n        df = pd.DataFrame(df_probs, index=df_steps, columns=['probs'])\n        df.to_csv(\"thompson.csv\")\n        \n    return bandit\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nagent = \"thompson\"\nmygame = env.run([\"random.py\", f\"{agent}.py\"])\n\n\nfig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(2, 2, figsize=(16,12))\n\nred_steps, blue_steps, green_steps = divide_steps(mygame, 1)\nax1.scatter(green_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in green_steps], color='green', label='Chosen bandit ranks 1-30th thresold-wise')\nax1.scatter(blue_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in blue_steps], color='blue', label='Chosen bandit ranks 31-60th thresold-wise')\nax1.scatter(red_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in red_steps], color='red', label='Chosen bandit ranks 61-100th threshold-wise')\nax1.plot([x for x in range(2000)], [max(mygame[i][0].observation.thresholds) for i in range(2000)], 'C1', label='Maximum chances of getting a candy at that step')\nax1.set_title(\"vs Random agent: Chances of getting a candy\")\nax1.legend(loc='upper right')\n\ndf = pd.read_csv(f\"{agent}.csv\", index_col=[0])\nred_steps, blue_steps, green_steps = [i for i in red_steps if i in df.index], [i for i in blue_steps if i in df.index], [i for i in green_steps if i in df.index]\nax2.scatter(green_steps, df.loc[green_steps, 'probs'], color='green', label='Actual threshold ranks 1-30th')\nax2.scatter(blue_steps, df.loc[blue_steps, 'probs'], color='blue', label='Actual threshold ranks 31-60th')\nax2.scatter(red_steps, df.loc[red_steps, 'probs'], color='red', label='Actual threshold ranks 61-100th')\nax2.set_title(\"Thresholds as expected by the agent\")\nax2.legend(loc='upper right')\n\nmygame = env.run([\"return0.py\", f\"{agent}.py\"])\n\nred_steps, blue_steps, green_steps = divide_steps(mygame, 1)\nax3.scatter(green_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in green_steps], color='green', label='Chosen bandit ranks 1-30th thresold-wise')\nax3.scatter(blue_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in blue_steps], color='blue', label='Chosen bandit ranks 31-60th thresold-wise')\nax3.scatter(red_steps, [mygame[i][0].observation.thresholds[mygame[i][1].action] for i in red_steps], color='red', label='Chosen bandit ranks 61-100th threshold-wise')\nax3.plot([x for x in range(2000)], [max(mygame[i][0].observation.thresholds) for i in range(2000)], 'C1', label='Maximum chances of getting a candy at that step')\nax3.set_title(\"vs Return0 agent: Chances of getting a candy\")\nax3.legend(loc='upper right')\n\ndf = pd.read_csv(f\"{agent}.csv\", index_col=[0])\nred_steps, blue_steps, green_steps = [i for i in red_steps if i in df.index], [i for i in blue_steps if i in df.index], [i for i in green_steps if i in df.index]\nax4.scatter(green_steps, df.loc[green_steps, 'probs'], color='green', label='Actual threshold ranks 1-30th')\nax4.scatter(blue_steps, df.loc[blue_steps, 'probs'], color='blue', label='Actual threshold ranks 31-60th')\nax4.scatter(red_steps, df.loc[red_steps, 'probs'], color='red', label='Actual threshold ranks 61-100th')\nax4.set_title(\"Thresholds as expected by the agent\")\nax4.legend(loc='upper right')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Much like the multi-armed-bandit, which is understandable, because well their math is the same, just that the multi_armed_bandit decays it's rewards. More agents to be added in the next version!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}