{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Santa 2020 - Agents Comparison\n\nThis notebook attempts to compare the various public notebooks for this competition.\n\nIf you have a public notebook not included here, please leave a comment and a link for inclusion, else fork and simply add your own private notebooks as a dataset and rerun.\n\nInspired by:\n- https://www.kaggle.com/ihelon/rock-paper-scissors-agents-comparison\n- https://www.kaggle.com/naokimaeda/local-evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install kaggle-environments --upgrade -q","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport math\nimport glob\nimport re\nimport os\nimport itertools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nfrom collections import defaultdict\nfrom joblib import Parallel, delayed\nfrom kaggle_environments import evaluate, make, utils","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Agents"},{"metadata":{"trusted":true},"cell_type":"code","source":"excludes = [ \"random_agent.py\" ]  # + [ \"always_first_agent.py\" ]\nagent_files = glob.glob('../input/**/*.py')\nagent_files = [ name for name in agent_files if not any( exclude in name for exclude in excludes ) ]\nagent_names = map(lambda name: re.sub('\\.\\./input/|/submission.py','', name), agent_files)\nagent_names = map(lambda name: re.sub('^((rock|paper|candy|cane|santa|2020|get-started-to)[.-]?)+|-agent|\\.py$','', name), agent_names)\nagent_names = map(lambda name: re.sub('/', ' \\n ', name), agent_names)\nagents = { k: v for k, v in zip(agent_files, agent_names) }\nagents","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf ./agents\n!rsync -r ../input ./agents\n!find ./agents/ -type f -not -name '*.py' -delete\n!find ./agents/ -type d -name '__results___files' -delete\n# !find ./agents","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef evaluate_mab(i1, i2, agent1, agent2):\n    print(i1, i2, agent1, agent2)\n    try:\n        result = evaluate(\"mab\", [ agent1, agent2 ])\n        result = np.array(result).flatten()\n    except:\n        result = np.array([0,0])\n    return (i1, i2, result)\n    \n\n    \nif os.environ.get('KAGGLE_KERNEL_RUN_TYPE','') == 'Interactive':\n    agents = { key: agents[key] for key in sorted(agents.keys())[-4:] + [\"../input/santa-2020-starter/always_first_agent.py\"] } # for debugging\n    display(agents)\n\nsafety_time  = 3*60*60\nmax_notebook = 9*60*60\ntime_end     = time.perf_counter() + max_notebook - safety_time\nresults      = []\nwhile time.perf_counter() < time_end:\n    results += Parallel(-1)( \n        delayed(evaluate_mab)(i1, i2, agent1, agent2) \n        for i1, agent1 in enumerate(agents.keys())\n        for i2, agent2 in enumerate(agents.keys())\n    )\n    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE','') == 'Interactive': break\n    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE','') == 'Batch':       continue\n\n# results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def df_half(df, ratio=0.5):\n    rows = int(len(df) * ratio)\n    cols = df.columns[:rows]\n    return df[:rows][cols]\n\ndef df_sort(df, by=None, ascending=False):\n    by = df if by is None else by\n    for axis in [0,1]:\n        df = df.reindex( by.mean().sort_values(ascending=ascending).index, axis=axis)\n    return df\n\ndef winrate_score(score1, score2):\n    try:\n        if score1 == score2: return  0\n        if score1 is None:   return -1\n        if score2 is None:   return  1\n        if score1 >  score2: return  1\n        if score1 <  score2: return -1\n    except: pass\n    return 0\n    \n\nscores_agent = defaultdict(list)\nscores_total = np.zeros(( len(agents), len(agents) ), dtype=np.int)\nscores_diff  = np.zeros(( len(agents), len(agents) ), dtype=np.float)\nwinrates     = np.zeros(( len(agents), len(agents) ), dtype=np.int)\n\nfor (i1, i2, result) in results:\n    scores_total[i1,i2] += (result[0] or 0)\n    scores_total[i2,i1] += (result[1] or 0)\n    scores_diff[i1,i2]  += (result[0] or 0) - (result[1] or 0) \n    scores_diff[i2,i1]  += (result[1] or 0) - (result[0] or 0)\n    winrates[i1,i2]     += winrate_score(result[0], result[1])\n    winrates[i2,i1]     += winrate_score(result[1], result[0])\n    scores_agent[ list(agents.values())[i1] ].append( result[0] )\n    scores_agent[ list(agents.values())[i2] ].append( result[1] )\n    \ndf_scores_total = pd.DataFrame(\n    scores_total, \n    index   = list(agents.values()), \n    columns = list(agents.values()),\n)\ndf_scores_diff = pd.DataFrame(\n    scores_diff, \n    index   = list(agents.values()), \n    columns = list(agents.values()),\n)\ndf_winrates = pd.DataFrame(\n    winrates, \n    index   = list(agents.values()), \n    columns = list(agents.values()),\n)\ndf_scores_agent = pd.DataFrame(scores_agent)\n\n# Sort by mean score\ndf_scores_total = df_sort(df_scores_total, by=df_scores_agent)\ndf_scores_diff  = df_sort(df_scores_diff,  by=df_scores_agent)\ndf_winrates     = df_sort(df_winrates,     by=df_scores_agent)\ndf_scores_agent = df_scores_agent.reindex( df_scores_agent.mean().sort_values(ascending=False).index, axis=1)\n\ndisplay(df_scores_agent.T)\ndisplay(df_scores_agent.T.mean(axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_df_heatmap(df, title, doublelabel=False, **kwargs):\n    plt.figure(figsize=(df.shape[1], df.shape[0]))\n    plt.title(title)\n    sns.heatmap(\n        df, annot=True, cbar=False, \n        cmap='coolwarm', linewidths=1, \n        linecolor='black', \n        fmt='.0f',\n        **kwargs\n    )\n    plt.tick_params(labeltop=doublelabel, labelright=doublelabel)\n    plt.xticks(rotation=90, fontsize=max(10,df.shape[0]//1.5))\n    plt.yticks(rotation=0,  fontsize=max(10,df.shape[0]//1.5))\n    print(title)\n    print(df.mean(axis=1).sort_values(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scores vs Always First Agent\n\nA good baseline metric is to compare scores against the Always First Agent. \n\nThis shows how well an agent can exploit its envronment in the absence of any competition.\n\nThe score for the Always First Agent also gives an indication as the average maximum payout from a single bandit. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scores_vs_always_first = df_scores_total[[\"starter \\n always_first_agent\"]]\ndf_scores_vs_always_first.T.mean().sort_values(ascending=False).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Total Scores\n\nThis shows the total score of each agent, showing how many resources it can extract before the opponent."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_df_heatmap(df_scores_total, 'Total Scores')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_df_heatmap(df_sort(df_half(df_scores_total), ascending=True), 'Total Scores', doublelabel=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Relative Scores\n\nWinning this game doesn't actually depend on getting a high total score, simply getting a higher score than your opponent."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_df_heatmap(df_scores_diff, 'Relative Scores')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_df_heatmap(df_sort(df_half(df_scores_diff), ascending=True), 'Relative Scores', doublelabel=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Winrates\n\nUltimately you only need to need to score one point higher than your opponent, so this plot shows how the leaderboard would actually respond to agents."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_df_heatmap(df_sort(df_winrates, ascending=True), 'Winrates')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_df_heatmap(df_sort(df_half(df_winrates), ascending=True), 'Winrates', doublelabel=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Boxplots"},{"metadata":{"trusted":true},"cell_type":"code","source":"def batch(iterable, n=1):\n    l = len(iterable)\n    for ndx in range(0, l, n):\n        yield iterable[ndx:min(ndx + n, l)]\n\ndef plot_df_boxplot(df, title, columns=8, boxplot_args={}, stripplot_args={}):\n    df_orig = df\n    n_rows    = math.ceil( len(df.columns) / columns )\n    n_columns = math.ceil( len(df.columns) / n_rows  )\n    for cols in batch(df.columns, n_columns):\n        df = df_orig[cols]\n        plt.figure(figsize=(n_columns*2, 5))\n        plt.title(title, loc=\"center\")\n\n        stripplot_args = { \"facecolor\": 'white', **boxplot_args }\n        ax = sns.boxplot(data=df, **boxplot_args)\n        plt.setp(ax.artists, edgecolor='grey', facecolor='w')\n        plt.setp(ax.lines, color='grey')\n\n        stripplot_args = { \"jitter\": 0.25, \"size\": 5, **stripplot_args }\n        ax = sns.stripplot(data=df, **stripplot_args)\n\n        # ax = sns.swarmplot(data=df_scores_agent)\n        plt.xticks(rotation=90, fontsize=15)\n        plt.yticks(rotation=0,  fontsize=15)\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_df_boxplot(df_scores_agent, \"All Matchmaking Scores\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}