{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Table of contents\n1. [Purpose](#purpose)\n2. [Update log](#update)\n3. [Formula](#formula)\n4. [Code](#code)\n5. [Verifications](#verification)\n6. [Sampling](#sampling)\n7. [Information loss](#time_random)\n\n\n![](https://i.ibb.co/VwzVbgh/pexels-nadi-lindsay-3521963.jpg)\n\npic hyperlink (free license): https://www.pexels.com/photo/gingerbread-man-near-coffee-mug-3521963/"},{"metadata":{},"cell_type":"markdown","source":"# Purpose <a id='purpose'></a>\n\nThe purpose of this notebook is to provide sloopy baseline estimation of $\\mathbb{P}(p_i|D)$. Many ideas here were tested, __none of them__ are silver bullets to this competition. Let's collaborate and have some candies together during this holiday season. "},{"metadata":{},"cell_type":"markdown","source":"# Update log: <a id='update'></a>\n\n12/14/2020: Formula typo fix.\n\n12/15/2020: 1. Sampling from custom distribution provided. 2. Change example from 100 -> 20. 3.bug fix. (introduced when reducing the trial number) 4. add DF_t in sampling example.\n\n12/16/2020: Shows examples of information loss. Explain that it may come from sampling generated by strategy not random."},{"metadata":{},"cell_type":"markdown","source":"\n\n\n\n\n# Formula <a id='formula'></a>\n\nPosterior calculation\n\nFor i-th slot machine, estimate $\\mathbb{P}(t=0, i)=p_{i}$\n\n\nSuppose you pull the machine several times what you observe are $(d_k, r_k)$, here $d_{k}$ is the discounting factor, $r_{k}$ is the reward.\n\n\nPrior is trivial, $\\mathbb{P}(D_{k}|D_{k-1}, p_i)=(d_k p_{i})^{r_{k}}(1 - d_k p_{i})^{1 - r_{k}}$, we have:\n\n$$\\mathbb{P}(p_i|D) \\sim \\prod_{k}\\left[(d_k p_{i})^{r_{k}}(1 - d_k p_{i})^{1 - r_{k}}\\right].$$\n\n\n\n**Step 1:** Normalization constant:\n$$N = \\int_{0}^1 \\prod_{k}\\left[(d_k p_{i})^{r_{k}}(1 - d_k p_{i})^{1 - r_{k}}\\right] dp_i.$$\n\n\n**Step 2:** Compute expected mean and std:\n$$\\mathbb{E}(p_i^k|D) = \\int_{0}^1 p_i^k\\mathbb{P}(p_i|D) dp_i=\\int_{0}^1 p_i^k \\frac{\\prod_{k}\\left[(d_k p_{i})^{r_{k}}(1 - d_k p_{i})^{1 - r_{k}}\\right]}{N} dp_i.$$"},{"metadata":{},"cell_type":"markdown","source":"# Code <a id='code'></a>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from scipy.stats import beta\nfrom scipy import integrate\n\ndef pdf(p, weights, rewards, normalization=1):\n    s = 1\n    for weight, reward in zip(weights, rewards):\n        if reward == 1:\n            s *= (weight * p)\n        else:\n            s *= (1 - weight * p)\n    return s / normalization\n\ndef get_expected_mean_std(weights, rewards):\n    normalization = integrate.quad(\n        lambda x: pdf(x, weights, rewards), 0, 1\n    )[0]\n    first_order = integrate.quad(\n        lambda x: x * pdf(x, weights, rewards, normalization), 0, 1\n    )[0]\n    second_order = integrate.quad(\n        lambda x: x * x * pdf(x, weights, rewards, normalization), 0, 1\n    )[0]\n    return first_order, (second_order - first_order ** 2) ** 0.5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Matching $\\beta$ distribution when no discounting factor <a id='verification'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ntrials = 20\ndiscountings = [1 for _ in range(trials)]\nrewards = [int(np.random.choice([0, 1], p=(0.3, 0.7))) for _ in range(trials)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_expected_mean_std(discountings, rewards)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(beta.mean(1 + sum(rewards), 21 - sum(rewards)),\n beta.std(1 + sum(rewards), 21 - sum(rewards)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks Daniel for your post. \n\nfor it in range(trials):\n    discountings[it] = 0.97**it\n    p_k = 0.7 * discountings[it]\n    rewards[it] = int(np.random.choice([0, 1], p=(1-p_k, p_k)))\n\n# Use the with-weightings formula\nprint(\"Weighted approach gives  mean, std:\")\nprint(get_expected_mean_std(discountings, rewards))\n\n# Do the equivalent calculations using beta function:\nprint(\"The beta function gives  mean, std:\")\nprint(\n    (\n        beta.mean(1 + sum(rewards), trials+1 - sum(rewards)),\n        beta.std(1 + sum(rewards), trials+1 - sum(rewards))\n    )\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inverse Transform Sampling <a id='sampling'></a>\n\nPeople may want to sample from this distribution, for example see a nice post here https://www.kaggle.com/ilialar/simple-multi-armed-bandit. Here we have a function to sample from p_i|D. More infor about inverse transform sampling, https://en.wikipedia.org/wiki/Inverse_transform_sampling. (The function below has no performance optimization ... at all.. It taks ~500ms to sample 1000. -.-|||)"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport numpy as np\nfrom scipy.optimize import newton\nimport matplotlib.pyplot as plt\ndef sample_one(weights, rewards):\n    normalization = integrate.quad(\n        lambda x: pdf(x, weights, rewards), 0, 1\n    )[0]\n    cdf = lambda x: integrate.quad(\n        lambda p: pdf(p, weights, rewards, normalization), 0, x\n    )[0]\n    x = float(np.random.uniform())\n    return newton(lambda p: cdf(p) - x, x0=0.5)\n\n\nDF_t = 0.5\nsamples = [sample_one([1, 1, 1], [0, 1, 0]) * DF_t for _ in range(1000)]\nplt.hist(samples)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Information loss <a id='time_random'></a>\n\nWhat people observe is not random sampling, but samples from their strategy & enemy moves.\n\nNow consider one over simplified case:\n* Your opponent pulled several times before you.\n* Start from 5 exploration steps.\n* Stop until getting 1 zeros at time $\\tau$."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n\ndef sample_oversimplified_strategy(p):\n    sample = []\n    for i in range(5):\n        sample.append(int(np.random.uniform() < p))\n    while np.random.uniform() < p:\n        sample.append(1)\n    sample.append(0)\n    return sample\n        \n\ndef generate_oversimplified_strategy_estimation(random_arrival_time=True):\n\n    initial_probability = np.linspace(0, 0.95, 200)\n    if random_arrival_time:\n        priority = [\n            float(np.random.uniform(0.4, 0.5)) for _ in range(200)\n        ]\n    else:\n        priority = [\n            1 for _ in range(200)\n        ]\n\n    observations = [\n        sample_oversimplified_strategy(\n            priority[i]* p\n        ) for i, p in enumerate(initial_probability)\n    ]\n\n    discountings = [\n        [\n            priority[i] * 0.97 ** i for i in range(len(ob))\n        ] for i, ob in enumerate(observations)\n    ]\n\n\n    return [\n        get_expected_mean_std(df, ob)[0] for df, ob in zip(\n            discountings, observations\n        )\n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(generate_oversimplified_strategy_estimation(random_arrival_time=True), label=\"random arrival time\")\nplt.plot(generate_oversimplified_strategy_estimation(random_arrival_time=False), label=\"no opponent\")\nplt.legend()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}