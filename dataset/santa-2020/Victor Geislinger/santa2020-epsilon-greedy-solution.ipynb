{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# [Santa 2020 - The Candy Cane Contest:](https://www.kaggle.com/c/santa-2020/) Epsilon-Greedy Solution"},{"metadata":{},"cell_type":"markdown","source":"[This competion](https://www.kaggle.com/c/santa-2020/) is a twist on the [multi-armed bandit problem](https://en.wikipedia.org/wiki/Multi-armed_bandit) where we compete with another agent to get the most candy canes from 100 vending machines that produce a candy cane by some unknown distiribution. To add icing to the cake, the vending machine reduces the likelihood of producing a candy cane every time an elf (agent) tests that machine."},{"metadata":{},"cell_type":"markdown","source":"# Epsilon-Greedy ($\\epsilon$-greedy)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"There are some intesting strategies to this problem but here we'll try the simple epsilon-greedy solution where we randomly try out different vending machines (explore) $\\epsilon$ portion of the times and the rest of the time pick the most optimal machine tried so far."},{"metadata":{},"cell_type":"markdown","source":"## Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install kaggle-environments --upgrade","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile agent-epsilon_greedy.py\n\nimport random\nrandom.seed(72)\n\nclass Agent:\n    \n    def __init__(self,eps=7e-1,eps_decay=0.99999):\n        self.best_machine = None\n        self.total_reward = 0\n        self.rewards = None \n\n        self.bandit_chosen = None\n        self.rewards = None\n        self.selections = None\n        self.epsilon = eps\n        self.decay = eps_decay\n        \n    def decay_eps(self):\n        self.epsilon = self.epsilon * self.decay\n        \n\nmyagent = Agent(eps=9e-1)\n\ndef agent(observation, configuration):\n    '''\n    '''\n    # Need to preserve this information across runs\n#     global best_machine, total_reward, rewards, bandit_chosen, rewards, selections, possible_machines\n#     global DEFAULT_EPS\n    \n    # Determine the reward-score of selection\n    def get_reward_score(bandit):\n        '''\n        '''\n        # No reward for no bandit\n        if bandit is None:\n            score = 0\n        # Average of rewards\n        else:\n            score = myagent.rewards[bandit]/myagent.selections[bandit] \n        return score\n\n    \n    # Record reward from last round\n    if myagent.bandit_chosen is not None:\n        # Difference from last reward and ttoal reward\n        last_reward = observation.reward - myagent.total_reward\n        myagent.rewards[myagent.bandit_chosen] += last_reward\n        myagent.total_reward += last_reward\n        # Check if this is now the best solution\n        # On the first two runs, the solution is undefined or the last chosen\n        if (myagent.best_machine is None): \n            myagent.best_machine = myagent.bandit_chosen\n        # Use the average rewards to determine the \"best\"\n        elif get_reward_score(myagent.bandit_chosen) >= get_reward_score(myagent.best_machine):\n            myagent.best_machine = myagent.bandit_chosen\n            \n    \n    \n    # Number of vending machines to select & rewards so far from selections\n    if observation.step == 0:\n        myagent.selections = [0] * configuration.banditCount\n        myagent.possible_machines = list(range(configuration.banditCount))\n        myagent.rewards = [0] * configuration.banditCount\n\n\n    # Determine if explore or exploit\n    eps = myagent.epsilon\n    beta = random.random()\n    # Explore only eps portion of the time\n    explore = (beta < eps) or (myagent.best_machine is None)\n        \n    # Pick randomly if exploring\n    if explore:\n        myagent.bandit_chosen = random.choice(myagent.possible_machines)\n    # Find the best vending machine if exploiting\n    else:\n        myagent.bandit_chosen = myagent.best_machine\n    \n    # Record selection\n    myagent.selections[myagent.bandit_chosen] += 1\n    \n    # Decay (if any)\n    myagent.decay_eps()\n\n    return myagent.bandit_chosen","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make(\"mab\", debug=True)\n\nenv.run([\"agent-epsilon_greedy.py\", \"agent-epsilon_greedy.py\"])\nenv.render(mode=\"ipython\", width=800, height=300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Try Different Exploring Options"},{"metadata":{},"cell_type":"markdown","source":"Here we're including the visualizer I made (see this notebook: https://www.kaggle.com/mrgeislinger/visualizing-reward-outcomes) "},{"metadata":{"trusted":true},"cell_type":"code","source":"# import module we'll need to import our custom module\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src=\"../input/visualizing-reward-outcomes/SimulationExplorer.py\", \n         dst= \"../working/SimulationExplorer.py\")\n\n# import all our functions\nimport SimulationExplorer as Explorer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nrandom.seed(27)\n\nclass Agent:\n    \n    def __init__(self,eps=5e-1, eps_decay=1.0):\n        self.best_machine = None\n        self.total_reward = 0\n        self.rewards = None \n\n        self.bandit_chosen = None\n        self.rewards = None\n        self.selections = None\n        self.epsilon = eps\n        self.decay = eps_decay\n        \n    def decay_eps(self):\n        self.epsilon = self.epsilon * self.decay","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def agent(observation, configuration):\n    '''\n    '''\n    \n    # Determine the reward-score of selection\n    def get_reward_score(bandit):\n        '''\n        '''\n        # No reward for no bandit\n        if bandit is None:\n            score = 0\n        # Average of rewards\n        else:\n            score = myagent.rewards[bandit]/myagent.selections[bandit] \n        return score\n\n    \n    # Record reward from last round\n    if myagent.bandit_chosen is not None:\n        # Difference from last reward and ttoal reward\n        last_reward = observation.reward - myagent.total_reward\n        myagent.rewards[myagent.bandit_chosen] += last_reward\n        myagent.total_reward += last_reward\n        # Check if this is now the best solution\n        # On the first two runs, the solution is undefined or the last chosen\n        if (myagent.best_machine is None): \n            myagent.best_machine = myagent.bandit_chosen\n        # Use the average rewards to determine the \"best\"\n        elif get_reward_score(myagent.bandit_chosen) >= get_reward_score(myagent.best_machine):\n            myagent.best_machine = myagent.bandit_chosen\n            \n    \n    \n    # Number of vending machines to select & rewards so far from selections\n    if observation.step == 0:\n        print(f'Eps: {myagent.epsilon}')\n        myagent.selections = [0] * configuration.banditCount\n        myagent.possible_machines = list(range(configuration.banditCount))\n        myagent.rewards = [0] * configuration.banditCount\n\n\n    # Determine if explore or exploit\n    eps = myagent.epsilon\n    beta = random.random()\n    # Explore only eps portion of the time\n    explore = (beta < eps) or (myagent.best_machine is None)\n        \n    # Pick randomly if exploring\n    if explore:\n        myagent.bandit_chosen = random.choice(myagent.possible_machines)\n    # Find the best vending machine if exploiting\n    else:\n        myagent.bandit_chosen = myagent.best_machine\n    \n    # Record selection\n    myagent.selections[myagent.bandit_chosen] += 1\n    \n    # Decay (if any)\n    myagent.decay_eps()\n\n    return myagent.bandit_chosen","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rand_agent(obs,conf):\n    import numpy as np\n    return int(np.random.choice(np.arange(conf.banditCount)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_eps = [1e-1, 5e-1, 7e-1, 9e-1, 9.5e-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\n\nsims = {}\nfor e in all_eps[1:]:\n\n    for trial in range(2):\n        start_time = time.time()\n        decay = 0.99999 if trial % 2 else 1.0\n        myagent = Agent(e,decay)\n        env = make(\"mab\", debug=True)\n        env.run([agent, rand_agent])\n\n        #\n        name = f'egreed_{e}d{decay} v rand: #{trial}'\n        sims[name] = env\n        print(f'\\t{time.time()-start_time}')\n    \ntest = Explorer.SimViz(sims)\ntest.plot_total_reward()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}