{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Happy New Year!"},{"metadata":{},"cell_type":"markdown","source":"![](https://t1.daumcdn.net/thumb/R1280x0.fjpg/?fname=http://t1.daumcdn.net/brunch/service/user/aS4g/image/7uHg3miGUTx7NCwcfrptfTJ1psA.JPG)\n[ELLA](https://www.instagram.com/ella_lina_joe/?igshid=1n4kwuk0jqh0j)"},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nIn 2020, the morale of Santa Village was greatly reduced due to the corona. To boost morale, I decided to supply candy cane and hold this competition. You need to use the vending machine in the lounge to supply candy cane. There are 100 vending machines and only 2 elves can fit in the lounge by social distance. 100 vending machines can only pull one lever at a time. Pulling the lever will give you a candy cane and you will get a 1 or 0 reward. Each time the point of consideration pulls the lever, the unknown reward activity decreases at a rate of 0.97. The goal is to develop a strategy to maximize rewards by pulling the lever 2000 times.\n\n2020년 산타마을의 사기가 코로나로 인해 많이 저하 되었습니다. 사기를 올리기 위해 사탕 지팡이를 공급하기로 결정하고 이번 대회를 개최하게 되었습니다. 사탕 지팡이를 공급하기 위해서는 휴게실의 자판기를 이용해야 합니다. 자판기는 100대가 있으며 휴게실에는 사회적거리두기로 2명의 엘프만이 들어갈수 있습니다. 100대의 자판기는 한번에 한개의 레버만 당길수 있으습니다. 레버를 당기면 사탕 지팡이가 지급대며 1 또는 0의 보상을 받게 됩니다. 고려할점이 레버를 당길 때마다 알려지지 않은 보상 활률은 0.97의 비율로 감소합니다. 목표는 2000번의 레버를 당김으로써 보상을 극대화하는 전략을 마련하는것 입니다."},{"metadata":{},"cell_type":"markdown","source":"The above optimization problem is a classic multi-armed bandit (MAB) problem. It is well explained in the first step of reinforcement learning, so I have extracted it as below.\n\nThe simplest form of reinforcement learning is a bandit with n-handles (=arm), or multi-armed bandit. It's easy to think of a bandit as a slot machine with n handles. Each handle provides compensation with a different probability. The goal of the agent is to maximize return compensation over time by finding the handle that offers the highest compensation and always selecting it.\n...\nThe reason why slot machines with n's are a good starting point when first coming down on reinforcement learning is that there is no need to worry about the dependence of time and the dependence of state. All you have to consider in a slot machine with n-handles is what rewards are associated with which actions and to ensure that we choose the best action. (정책=Policy)\nA policy describes a set of actions in which an agent is commissioned in a given environment. The policy in which the agent obtains maximum rewards within a given environment is considered the optimal policy.\n\n\n위 최적화 문제는 고전적인 MAB (multi-armed bandit) 문제 입니다. 강화학습 첫걸음에 잘 설명되어있어 아래와 같이 발췌해 왔습니다.\n\n강화학습에서 가장 단순한 형태의 문제는 n개의 손잡이(=arm)가 달린 밴딧(bandit), 즉 멀티암드 밴딧(multi-armed bandit) 입니다. 여기서 밴딧은 '손잡이가 n개인 슬롯머신'이라고 생각하면 쉽습니다. 각각의 손잡이는 다른 확률로 보상을 제공합니다. 에이전트의 목표는 시간의 흐름에 따라 가장 높은 보상을 제공하는 손잡이를 찾아 내고 항상 이 손잡이를 선택함으로써 돌아오는 보상을 최대화하는 것입니다.\n...\n처음 강화학습에 대해 하습할 때 n개인 슬롯머신이 좋은 시작점이 되는 이유는 시간의존성, 상태의존성에 대한 고민할 필요가 없기 때문입니다. 손잡이가 n개인 슬롯머신에서 고려해야 하는 것은, 어떤 보상이 어떤 액션과 연관되어 있는지, 그리고 우리가 최적의 액션을 선택하도록 보장하는 것이 전부입니다. (정책=Policy)\n정책은 주어진 환경의 어떤 상황에서 어떤 에이전트가 취학 되는 일련의 액션을 기술합니다. 에이전트가 주어진 환경 내에서 최대의 보상을 얻는 정책을 최적의 정책으로 간주하게 됩니다.\n[출처. 강화학습 첫걸음 - 2장 벤딧 문제 중]"},{"metadata":{},"cell_type":"markdown","source":"\"Exploration vs. Exploitation Dilemma\"의 Principles은 아래와 같습니다. \n\n\n## Principles\n\n* Naive(or Ramdom) Exploration  : Add noise to greedy policy (e.g. Epsilon-greedy, SoftMax) \n* Optimistic Initialisation : Assume the best until proven otherwise \n* Optimism in the Face of Uncertainty : Prefer actions with uncertain values (e.g. UCB)\n* Probability Matching : Select actions according to probability they are best (e.g. Thompson sampling)\n* Information State Search : Lookahead search incorporating value of information\n"},{"metadata":{},"cell_type":"markdown","source":"## a simple bandit algorithm\nMAB(multi-armed bandit)는 Stationary or Nonstationary MAB로 구분됩니다. Let's start with a Stationary MAB where the probability distribution of the reward value does not change over time. To simplify the problem and make it easy to approach one by one step, let's implement \"a simple bandit algorithm\" and even submit it to the catalog.\n\n\"a simple bandit algorithm\" is shown below.\n\n\nMAB(multi-armed bandit)는 Stationary or Nonstationary MAB로 구분됩니다. 보상값의 확률 분포가 시간이 지나도 변화지 않는 Stationary MAB로 시작하겠습니다. 문제를 단순화 하고 단계적으로 하나하나 쉽게 접근하기 위해 \"a simple bandit algorithm\"을 구현해보고 캐글에 제출까지 해보겠습니다.\n\n\"a simple bandit algorithm\"은 아래와 같습니다.\n\n\nReinforcement Learning An Introduction - Chapter 2: Multi-armed Bandits (2.4)\n![Reinforcement Learning An Introduction - Chapter 2: Multi-armed Bandits (2.4)](https://t1.daumcdn.net/thumb/R1280x0.fpng/?fname=http://t1.daumcdn.net/brunch/service/user/aS4g/image/gJO0vssuT_UypLVVoJwIcnp9i-o.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install kaggle-environments --upgrade","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%writefile simple_bandit_agent.py\n\nimport random\nimport numpy as np\nfrom kaggle_environments import make\n\nN = None\nQ = None\nR_total = 0\nepsilon = 0.15\n\ndef simple_bandit_agent(observation, configuration):\n    global N, Q, A, R_total, epsilon\n\n    banditCount = configuration.banditCount\n\n    if observation.step == 0:\n        # Initialize, for a=1 to k:\n        N = np.zeros(banditCount)\n        Q = np.zeros(banditCount)\n    else: # observation.step > 0:\n        # R = bandit(A)\n        R = observation.reward - R_total\n        R_total = observation.reward\n        # ...\n        N[A] += 1\n        Q[A] += (R - Q[A]) / N[A]\n\n    if np.random.binomial(1, epsilon, 1):\n        A = np.random.randint(0, banditCount)\n    else:\n        A = np.argmax(Q)\n    return int(A)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile random_agent.py\n\nimport random\n\ndef random_agent(observation, configuration):\n    return random.randrange(configuration.banditCount)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"mab\", debug=True)\nenv.reset()\nenv.run([\"simple_bandit_agent.py\", \"random_agent.py\"])\nenv.render(mode=\"ipython\", width=800, height=700)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## submissions\nWhen I submitted it, I got 479 points.\nIf it helped, please recommend(=vote) it.\n\n제출해보니 479점을 받았습니다.\n\n도음이 되셨다면 up vote 부탁드립니다.\n조금더 자세한 정보는 브런치를 참고 부탁드립니다 - https://brunch.co.kr/@hansungdev/23"},{"metadata":{},"cell_type":"markdown","source":"Epsilon-greedy, Thompson sampling 로 구현 후 제출한 점수하고는 차이가 있네요.\n\n![](https://t1.daumcdn.net/thumb/R1280x0.fpng/?fname=http://t1.daumcdn.net/brunch/service/user/aS4g/image/_dQz6VrnWTOu4D__CWOz0d21fn0.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}