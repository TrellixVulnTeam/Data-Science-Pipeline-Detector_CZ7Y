{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install kaggle-environments --upgrade -q","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%writefile random_agent.py\nimport random\n\ndef random_agent(observation, configuration):\n    return random.randrange(configuration.banditCount)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile submission.py\n\nimport math\n\nlast_bandit = -1\ntotal_reward = 0\n\nsums_of_reward = None\nnumbers_of_selections = None\n\ndef ucb_agent(observation, configuration):    \n    global sums_of_reward, numbers_of_selections, last_bandit, total_reward\n\n    if observation.step == 0:\n        numbers_of_selections = [0] * configuration[\"banditCount\"]\n        sums_of_reward = [0] * configuration[\"banditCount\"]\n\n    if last_bandit > -1:\n        reward = observation.reward - total_reward\n        sums_of_reward[last_bandit] += reward\n        total_reward += reward\n\n    bandit = 0\n    max_upper_bound = 0\n    for i in range(0, configuration.banditCount):\n        if (numbers_of_selections[i] > 0):\n            average_reward = sums_of_reward[i] / numbers_of_selections[i]\n            delta_i = math.sqrt(2 * math.log10(observation.step+1) / numbers_of_selections[i])\n            upper_bound = average_reward + delta_i\n        else:\n            upper_bound = 1e400\n        if upper_bound > max_upper_bound and last_bandit != i:\n            max_upper_bound = upper_bound\n            bandit = i\n            last_bandit = bandit\n\n    numbers_of_selections[bandit] += 1\n\n    if bandit is None:\n        bandit = 0\n\n    return bandit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"mab\", debug=True)\nenv.reset()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.run([\"random_agent.py\",\"submission.py\"])\nenv.render(mode=\"ipython\", width=800, height=800)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# env.reset()\n# env.run([\"submission.py\", \"submission.py\"])\n# env.render(mode=\"ipython\", width=800, height=800)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check(file1, file2):\n    env = make(\"mab\", debug=True)\n\n    for i in range(5):\n        env.run([file1, file2])\n        p1_score = env.steps[-1][0]['reward']\n        p2_score = env.steps[-1][1]['reward']\n        env.reset()\n        print(f\"Round {i+1}: {p1_score} - {p2_score}\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Default vs epsilon-greedy')\ncheck(\"random_agent.py\",\"submission.py\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile epsilon_greedy.py\n\nimport math\nimport random\n\nepsilon = 0.3\n\nlast_bandit = -1\ntotal_reward = 0\n\nsums_of_reward = None\nnumbers_of_selections = None\nrandom.seed(50)\n\ndef agent(observation, configuration):    \n    global sums_of_reward, numbers_of_selections, last_bandit, total_reward\n\n    if observation.step == 0:\n        numbers_of_selections = [0] * configuration.banditCount\n        sums_of_reward = [0] * configuration.banditCount\n\n    if last_bandit > -1:\n        reward = observation.reward - total_reward\n        sums_of_reward[last_bandit] += reward\n        total_reward += reward\n\n    if random.random() < epsilon:\n        bandit = random.randint(0, configuration.banditCount-1)\n        last_bandit = bandit\n    else:\n        bandit = 0\n        max_upper_bound = 0\n\n        for i in range(0, configuration.banditCount):\n            if numbers_of_selections[i] > 0:\n                upper_bound = sums_of_reward[i] / numbers_of_selections[i]\n            else:\n                upper_bound = 1e400\n            if upper_bound > max_upper_bound and last_bandit != i:\n                max_upper_bound = upper_bound\n                bandit = i\n                last_bandit = bandit\n\n    numbers_of_selections[bandit] += 1\n\n    if bandit is None:\n        bandit = 0\n\n    return bandit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.run([\"random_agent.py\",\"epsilon_greedy.py\"])\nenv.render(mode=\"ipython\", width=800, height=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Default vs epsilon-greedy')\ncheck(\"random_agent.py\",\"epsilon_greedy.py\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile epsilon_greedy_decay.py\n\nimport math\nimport random\n\nepsilon = 0.3\n\nlast_bandit = -1\ntotal_reward = 0\n\nsums_of_reward = None\nnumbers_of_selections = None\nrandom.seed(50)\n\ndef agent(observation, configuration):    \n    global sums_of_reward, numbers_of_selections, last_bandit, total_reward\n\n    if observation.step == 0:\n        numbers_of_selections = [0] * configuration.banditCount\n        sums_of_reward = [0] * configuration.banditCount\n\n    if last_bandit > -1:\n        reward = observation.reward - total_reward\n        sums_of_reward[last_bandit] += reward\n        total_reward += reward\n\n    if random.random() < epsilon:\n        bandit = random.randint(0, configuration.banditCount-1)\n        last_bandit = bandit\n    else:\n        bandit = 0\n        max_upper_bound = 0\n\n        for i in range(0, configuration.banditCount):\n            if numbers_of_selections[i] > 0:\n                decay=0.97**numbers_of_selections[i]\n                upper_bound = decay*(sums_of_reward[i] / numbers_of_selections[i])\n            else:\n                upper_bound = 1e400\n            if upper_bound > max_upper_bound and last_bandit != i:\n                max_upper_bound = upper_bound\n                bandit = i\n                last_bandit = bandit\n\n    numbers_of_selections[bandit] += 1\n\n    if bandit is None:\n        bandit = 0\n\n    return bandit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Default vs epsilon-greedy_decay')\ncheck(\"random_agent.py\",\"epsilon_greedy_decay.py\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile ucb_and_epsilon_greedy_decay.py\n\nimport math\nimport random\n\nepsilon = 0.1\n\nlast_bandit = -1\ntotal_reward = 0\n\nsums_of_reward = None\nnumbers_of_selections = None\nrandom.seed(50)\n\ndef agent(observation, configuration):    \n    global sums_of_reward, numbers_of_selections, last_bandit, total_reward\n\n    if observation.step == 0:\n        numbers_of_selections = [0] * configuration.banditCount\n        sums_of_reward = [0] * configuration.banditCount\n\n    if last_bandit > -1:\n        reward = observation.reward - total_reward\n        sums_of_reward[last_bandit] += reward\n        total_reward += reward\n\n    if random.random() < epsilon:\n        bandit = random.randint(0, configuration.banditCount-1)\n        last_bandit = bandit\n    else:\n        bandit = 0\n        max_upper_bound = 0\n\n        for i in range(0, configuration.banditCount):\n            if numbers_of_selections[i] > 0:\n                decay=0.97**numbers_of_selections[i]\n                delta_i = math.sqrt(2 * math.log10(observation.step+1) / numbers_of_selections[i])\n                upper_bound = decay*(sums_of_reward[i] / numbers_of_selections[i])+ delta_i\n           \n            else:\n                upper_bound = 1e400\n            if upper_bound > max_upper_bound and last_bandit != i:\n                max_upper_bound = upper_bound\n                bandit = i\n                last_bandit = bandit\n\n    numbers_of_selections[bandit] += 1\n\n    if bandit is None:\n        bandit = 0\n\n    return bandit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Default vs ucb_and_epsilon-greedy_decay')\ncheck(\"random_agent.py\",\"ucb_and_epsilon_greedy_decay.py\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile ucb_and_epsilon_greedy_decay2.py\n\nimport math\nimport random\n\nepsilon = 0.2\n\nlast_bandit = -1\ntotal_reward = 0\n\nsums_of_reward = None\nnumbers_of_selections = None\nrandom.seed(50)\n\ndef agent(observation, configuration):    \n    global sums_of_reward, numbers_of_selections, last_bandit, total_reward\n\n    if observation.step == 0:\n        numbers_of_selections = [0] * configuration.banditCount\n        sums_of_reward = [0] * configuration.banditCount\n\n    if last_bandit > -1:\n        reward = observation.reward - total_reward\n        sums_of_reward[last_bandit] += reward\n        total_reward += reward\n\n    if random.random() < epsilon:\n        bandit = random.randint(0, configuration.banditCount-1)\n        last_bandit = bandit\n    else:\n        bandit = 0\n        max_upper_bound = 0\n\n        for i in range(0, configuration.banditCount):\n            if numbers_of_selections[i] > 0:\n                decay=0.97**numbers_of_selections[i]\n                delta_i = math.sqrt(2 * math.log10(observation.step+1) / numbers_of_selections[i])\n                upper_bound = max(decay*(sums_of_reward[i] / numbers_of_selections[i]),\n                                  sums_of_reward[i] / numbers_of_selections[i] + delta_i)\n           \n            else:\n                upper_bound = 1e400\n            if upper_bound > max_upper_bound and last_bandit != i:\n                max_upper_bound = upper_bound\n                bandit = i\n                last_bandit = bandit\n\n    numbers_of_selections[bandit] += 1\n\n    if bandit is None:\n        bandit = 0\n\n    return bandit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Default vs ucb_and_epsilon-greedy_decay2')\ncheck(\"random_agent.py\",\"ucb_and_epsilon_greedy_decay2.py\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile ucb_bayesian.py\n\nimport numpy as np\nfrom scipy.stats import beta\n\nepsilon = 0.1\ndecay=0.97\nbandit=0\ntotal_reward = 0\na,b,bound=[None]*3\nc=2\n\ndef agent(observation, configuration):    \n    global total_reward,a,b,bandit\n\n    if observation.step == 0:\n        a=[1]*configuration.banditCount\n        b=[1]*configuration.banditCount\n    else:\n        r = decay*(observation.reward - total_reward)\n        total_reward = observation.reward \n    \n        bandit = max(\n            range(configuration.banditCount),\n            key=lambda x: a[x] / float(a[x] + b[x]) + beta.std(\n                a[x], b[x]) * c\n        )\n  \n        a[bandit] += r\n        b[bandit] += 1 - r\n        \n\n    if bandit is None:\n        bandit = 0\n\n    return bandit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Default vs ucb_bayesian')\ncheck(\"random_agent.py\",\"ucb_bayesian.py\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile thompson.py\n\nimport numpy as np\nfrom scipy.stats import beta\n\nepsilon = 0.1\ndecay=0.97\nbandit=0\ntotal_reward = 0\na,b,bound=[None]*3\nc=4\n\ndef agent(observation, configuration):    \n    global total_reward,a,b,bandit\n\n    if observation.step == 0:\n        a=[1]*configuration.banditCount\n        b=[1]*configuration.banditCount\n    else:\n        r = decay*(observation.reward - total_reward)\n        total_reward = observation.reward \n    \n        bandit = max(\n            range(configuration.banditCount),\n            key=lambda x: a[x] / float(a[x] + b[x]) + beta.std(\n                a[x], b[x]) * c\n        )\n  \n        a[bandit] += r\n        b[bandit] += 1 - r\n        \n\n    if bandit is None:\n        bandit = 0\n\n    return bandit","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}