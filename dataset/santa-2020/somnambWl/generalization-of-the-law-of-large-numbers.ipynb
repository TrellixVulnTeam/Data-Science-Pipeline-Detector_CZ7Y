{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Standard library\nimport random\n\n# Basic data libraries\nimport numpy as np\nimport pandas as pd\n\n# Graphs\nfrom bokeh.io import show, output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.models import Panel, Tabs, HoverTool, ColumnDataSource\noutput_notebook()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generalization of the law of large numbers\n\n## Introduction\n\nIn December 2020, Kaggle launched its annual [Santa's Christmas competition](https://www.kaggle.com/c/santa-2020/overview/environment-rules).\nThis year participants were trying to solve multi-armed bandit problem - \nhaving a set of 100 one-armed bandits (slot machines), each with a random initial probability\nof a reward which is not known to participants. \nEach machine can output a reward according to its probability of a reward when pulled.\nThe goal is to get the highest possible reward, while the number of pulls (trials) is limited.\nThere were two twists to the classic problem, your agent plays\nagainst another agent and the likelihood of reward decreases by 3 % after\neach machine pull (i.e. probability of a reward after $N$-th pull is $P_{N} = P_0 \\cdot 0.97^N$).\n\nIn order to do well in the competition, one needs to estimate the initial likelihood of reward $P_0$ \nof a machine as precisely as possible. \nThis is not possible to do with the original law of large numbers\nas it is stated only for the case where random variables (results of trials) are independent\nand identically distributed. In our case, they are not identically distributed as the\ndistribution changes after each trial. It is not a simple Bernoulli trial, because\nthe probability of success changes. We need to restate the law of large numbers\nin such a way that the condition of the probability of success not changing is lifted.\n\n## Generalization using history\n\nThe original law of large numbers works only with counts.\nBe it for one-armed bandits, where we need to know only the number\nof times when we did or didn't get a reward, or for a simple dice rolling,\nwhere we count occurrences of all values.\nIt does not work with history because it does not matter\nwhether one-armed bandit produces history [0, 0, 1, 1] or [1, 0, 0, 1].\nThe count of values is still the same and we can assume that probability to get\na reward is 50 % as the average converges to the expected value (in this case, the probability of a reward).\nThe information about history is redundant.\nThis is the consequence of the fact that the probability of a reward does not change in time, i.e. $P_N = P_0$.\n\nIf the likelihood of reward changes in time, we need to take into account history.\nThe probability of a reward depends only on the initial probability and the number of trials,\nafter $N$-th trial it is $P_N = f(P_0, N)$. The history is composed out of trial outcomes $E_i$\nand has length $N$.\nWhile $f(P_0, N)$ is set, for each outcome $E_i$ in history \nand every possible initial probability $P_0$ we can compute\nthe probability that $P_0$ produces the outcome, $p_i = P(E_i | f, P_0)$.\nThese individual probabilities $p_i$ multiplied together give the probability $P_H$ of the initial probability\nto produce the history of outcomes when function $f$ is applied after every trial, \n$P_H (f, P_0) = \\prod_i p_i$. \nThe true initial probability is then most likely the one with the highest $P_H$.\n\n> **The bandit's initial probability of a reward is most likely the one \n> which yields the history of outcomes with the highest probability.** \n\nRestated this way, the law works the same way as before in the case that $P_0$ does not change, \nbecause $f$ can be just $f(P_0, N) = P_0$.\nHowever, it works now even in the case that the probability of a reward changes between trials.\nThe longer is the history of outcomes, the more precise is the estimation.\n\n## Example\n\nThe example is from the Kaggle competition, where machines have 3% probability\ndecay.\nA machine is pulled three times.\nFor the first and the third time we get a reward, on the second pull we get nothing.\nThe machine produced history [1, 0, 1].\nWhat initial probability is the most likely to produce this history?\n\nIf the initial probability was 1, then after the first pull, \nthe probability of success would be 0.97 and after the second pull it would be 0.9409.\nThe probability that the machine produces history [1, 0, 1] is:\n\n$$\n\\underbrace{1}_{\\substack{\\text{Probability of success} \\\\ \\text{on the first pull}}} \\cdotÂ \\underbrace{(1-0.97)}_{\\substack{\\text{Probability of loss} \\\\ \\text{on the second pull}}} \\cdot \\underbrace{0.9409}_{\\substack{\\text{Probability of success} \\\\ \\text{on the third pull}}} = 0.0282\n$$\n\nWith the initial probability 0, the machine can not produce this history at all. \n\n$$ \n0 \\cdot (1 - 0) \\cdot 0 = 0\n$$\n\nNow, consider the middle, initial probability 0.5, the probability of producing the history is:\n\n$$ \n0.5 \\cdot (1 - 0.485) \\cdot 0.4705 = 0.1211\n$$\n\nWe see that it is more likely that the initial probability of the machine was 0.5 than 1 \nand the initial probability 0 is completely out of question. \n\nThe Kaggle problem is a bit easier than a general case as the\ninitial probability was always integer between 0 and 100 (in percents) and not a float.\nThe function to find out the most likely probability can be easily vectorized,\ncomputing the probability of producing the history for all possible initial probabilities at once.\nSpecifically for history [1, 0, 1] we can find out that the most likely\ninitial probability with 3% decay is 0.71 and the probability of producing the history\nis $P_H = 0.1481$.\n\n## Limitation\n\nWhen $f(P_0, N)$ is set in such a way that $P_N$ grows or decays, similarly as in the Kaggle case,\nthere is a possibility that the assumed $P_0$ will not converge to the true $P_0$.\nSimilarly as in the original law of large numbers, the estimate of $P_0$ is the more\nprecise, the more observations we have.\nHowever, unlike with the original law of large numbers, \nwe can eventually reach a state where no further observations will improve the estimate.\nAn extreme example of this maybe be when $f(P_0, N) = \\text{max}(0, P_0-1 \\cdot N)$, i.e.\nafter the first pull onwards, the probability of a reward is zero.\nWhatever the initial probability is, the assumed original probability is\ndependent solely on the first trial. \nIf a reward is gained, assumed probability will be $P_0 = 1$,\nin the other case, it will be $P_0 = 0$.\nNo further trials can change the assumed $P_0$.\n\n---\n\nCite as:\n\n```\n@article{wagner2021generalization,\n  title   = \"Generalization of the law of large numbers\",\n  author  = \"Wagner, Jakub\",\n  journal = \"somnambwl.netlify.app\",\n  year    = \"2021\",\n  url     = \"https://somnambwl.netlify.app/Science/LawOfLargeNumbers/\"\n}\n```"},{"metadata":{},"cell_type":"markdown","source":"## Python code\n\nThe following part contains the Python code which defines a class for a single machine. Code used in the competition, where there is a need to count with opponent pulls is available in [another Kaggle notebook](https://www.kaggle.com/somnambwl/santa-2020)."},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def probability_change(initial_probability, N):\n    \"\"\"\n    Computes probability to gain a reward after `N`-th pull.\n    \n    Parameters\n    ==========\n    initial_probability: np.array\n        Possible initial probabilities to gain a reward when a machine is pulled.\n    N: int\n        Number of pulls\n        \n    Returns\n    =======\n    float\n        Probability to gain a reward after `N`-th pull.\n    \"\"\"\n    return initial_probability * 0.97**N\n\nclass VendingMachine(object):\n    \"\"\"\n    A class describing a single machine.\n    \n    Attributes\n    ==========\n    pulls: int\n        How many times was this machine pulled?\n    rewards: int\n        How many times a reward came out of the machine after a pull?\n    unknown: int\n        How many times we don't know if there was a reward or not?\n    losses: int\n        How many times the machine was pulled and did not output a reward?\n    history: list of str\n        \"S\" is for success, \"F\" for failure, \"?\" for unknown result\n        (when e.g. the opponent pulls the machine).\n    probability_change: func\n        Function that computes the probability to gain a reward out of the machine\n        after the N-th pull.\n    aop_history: list of float\n        History of assumed original probabilities, i.e. what we thought\n        that was the initial probability of a reward of the machine at a certain step.\n        For example if it is [100, 50], after the first pull we estimated the initial\n        probability to be 100 %, while after the second step we estimated it to 50 %.\n    acp_history: list of float\n        History of assumed current probabilities, i.e. what we thought\n        that was the current probability of a reward at a certain step.\n    \"\"\"\n    \n    def __init__(self, probability_change):\n        self.rewards = 0\n        self.unknown = 0\n        self.losses = 0\n        self.pulls = 0\n        self.history = []\n        self.markers = []\n        self.probability_change = probability_change\n        self.aop_history = []\n        self.acp_history = []\n        \n    def pulled(self, result):\n        \"\"\"\n        Record my pull.\n        \n        Parameters\n        ==========\n        result: str\n            \"S\" for success, \"F\" for loss and \"?\" for an unknown outcome..\n        \"\"\"\n        self.pulls += 1\n        result = str(result)\n        if result == \"S\":\n            self.rewards += 1\n            self.markers.append(\"^\")\n        elif result == \"?\":\n            self.unknowns +=1\n            self.markers.append(\"s\")\n        elif result == \"F\":\n            self.losses += 1\n            self.markers.append(\"v\")\n        self.history.append(result)\n        self.aop_history.append(self.assumed_original_probability)\n        self.acp_history.append(self.assumed_current_probability)\n        \n    def assume_probability_from_history(self):\n        \"\"\"\n        Assumes original probability of reward.\n        \n        As we have a history on a vending machine, we can compute\n        how likely is the current probability. We basically go through\n        all possible starting probabilities (for cycle hidden\n        in linspace) and the probability that is most likely to \n        give the current history of pulls is the most certainly\n        the original probability.\n        \"\"\"\n        base = np.linspace(0, 1, 101)\n        probabilities = np.ones(101)\n        for N_pulls, event in enumerate(self.history):\n            probability_of_success = self.probability_change(base, N_pulls)\n            if event == \"S\":\n                probabilities *= (probability_of_success)\n            elif event == \"F\":\n                probabilities *= (1-probability_of_success)\n            elif event == \"?\":\n                pass\n        return np.argmax(probabilities) * 0.01\n            \n    @property\n    def assumed_original_probability(self):\n        \"\"\"\n        Returns the assumed original probability.\n        \"\"\"\n        return self.assume_probability_from_history()\n    \n    @property\n    def assumed_current_probability(self):\n        \"\"\"\n        Return the assumed current probability of reward.\n        \n        As we can compute the original probability, we just apply\n        the decay and that is the current probability of a reward.\n        \"\"\"\n        return self.probability_change(self.assumed_original_probability, self.pulls)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"class TrainingMachine(VendingMachine):\n    \"\"\"\n    Extension for a single machine.\n    \n    In the competition, we select one of 100 machines and the environment\n    returns an observation, whether we got a reward. We are now interested\n    only in a single machine and estimating its initial probability of a reward,\n    so this extension for a single machine adds an additional function that\n    simulates a single pull.\n    \"\"\"\n    \n    def __init__(self, initial_probability, probability_change):\n        super().__init__(probability_change)\n        self.initial_probability = initial_probability\n        self.probability = initial_probability\n        self.probability_history = [initial_probability]\n    \n    def pull(self, me=True):\n        \"\"\"\n        Simulate a single pull.\n        \n        Parameters\n        ==========\n        me: bool (True)\n            If True, then we know the result - whether we got a reward or not.\n            False is used if an opponent is present and we do not get an\n            information about their rewards.\n        \"\"\"\n        got_reward = random.random() < self.probability\n        reward_string = \"S\" if got_reward else \"F\"\n        if me:\n            self.pulled(reward_string)\n        else:\n            self.pulled(\"?\")\n        self.probability = self.probability_change(self.initial_probability, self.pulls)\n        self.probability_history.append(self.probability)\n        \n    def set_history(self, history):\n        \"\"\"\n        Set a custom history.\n        \n        Useful for answering questions like\n        'How much would assumed probability deviate from the true probability if,\n        e.g. machine returned reward five times in a row?'\n        \n        Parameters\n        ==========\n        history: list of str\n            \"S\" is for success, \"F\" for failure, \"?\" for unknown result\n        \"\"\"\n        for reward in history:\n            self.pulled(reward)\n            self.probability = self.probability_change(self.initial_probability, self.pulls)\n            self.probability_history.append(self.probability)        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Playground\n\nIn the following cell, it is possible to try out different reward-changing functions, set different initial probability and the number of pulls."},{"metadata":{"trusted":true},"cell_type":"code","source":"def probability_change(initial_probability, N):\n    \"\"\"\n    Computes probability to gain a reward after `N`-th pull.\n    \n    Parameters\n    ==========\n    initial_probability: np.array\n        Possible initial probabilities to gain a reward when a machine is pulled.\n    N: int\n        Number of pulls\n        \n    Returns\n    =======\n    float\n        Probability to gain a reward after `N`-th pull.\n    \"\"\"\n    return initial_probability * 0.97**N\n\ntm = TrainingMachine(0.5, probability_change)\n# tm.set_history([\"S\", \"S\", \"S\", \"S\", \"S\"])   # Set a custom history\nfor i in range(100):   # Simulate pulls\n    tm.pull()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"green = \"#3FA719\"\nred = \"#B33F3F\"\nblue = \"#7AB8E8\"\npurple = \"#855CBF\"\ngray = \"#A7848E\"\n\n\n# First tab with original probability\n\ny = np.array(tm.aop_history)\ny2 = np.ones(len(y)+1) * tm.initial_probability\nx = np.array(list(range(len(y))))+1\nx2 = list(range(len(y)+1))\ndf = pd.DataFrame.from_records(zip(x, y), columns=[\"N_trials\", \"assumed_original_probability\"])\nsource = ColumnDataSource(df)\nm = np.array(tm.markers)\n\np1 = figure(plot_width=700, plot_height=500, y_range=(-0.1, 1.1))\nl1 = p1.line(x=\"N_trials\", y=\"assumed_original_probability\", source=source, line_width=3, color=purple, alpha=0.7, legend_label=\"Assumed\")\np1.line(x2, y2, line_width=3, color=gray, alpha=0.7, legend_label=\"True\")\nfor marker_tag, marker_type, color in zip([\"^\", \"v\", \"s\"], [\"triangle\", \"inverted_triangle\", \"square\"], [green, red, blue]):\n    p1.scatter(x[m==marker_tag], y[m==marker_tag], marker=marker_type, color=color, size=8, alpha=0.3)\np1.xaxis.axis_label = \"Number of trials\"\np1.yaxis.axis_label = \"Assumed original probability\"\nht1 = HoverTool(\n    tooltips=[\n        (\"Assumed probability\", \"@assumed_original_probability\" ),],\n    mode='vline',\n    renderers=[l1])\np1.add_tools(ht1)\ntab1 = Panel(child=p1, title=\"Assumed initial probability\")\n\n\n# Second tab with current probability\n\ny1 = np.array([None]+tm.acp_history)\ny2 = np.array(tm.probability_history)\nx1 = np.array(list(range(len(y1))))\nx2 = np.array(list(range(len(y2))))\nm = np.array(tm.markers)\ndf = pd.DataFrame.from_records(zip(x1, y1, y2), columns=[\"N_trials\", \"assumed_current_probability\", \"true_current_probability\"])\nsource = ColumnDataSource(df)\n\n\np2 = figure(plot_width=700, plot_height=500, y_range=(-0.1, 1.1))\nr1 = p2.line(x=\"N_trials\", y=\"true_current_probability\", source=source, line_width=3, color=gray, alpha=0.8, legend_label=\"True\")\np2.line(x1, y1, line_width=3, color=purple, alpha=0.7, legend_label=\"Assumed\")\nfor marker_tag, marker_type, color in zip([\"^\", \"v\", \"s\"], [\"triangle\", \"inverted_triangle\", \"square\"], [green, red, blue]):\n    mask = np.array([False]+list(m==marker_tag))\n    p2.scatter(x1[mask], y1[mask], marker=marker_type, color=color, size=8, alpha=0.4)\np2.xaxis.axis_label = \"Number of trials\"\np2.yaxis.axis_label = \"Probability\"\nht2 = HoverTool(\n    tooltips=[\n        (\"True probability\", \"@true_current_probability\"),\n        (\"Assumed probability\", \"@assumed_current_probability\" ),],\n    mode='vline',\n    renderers=[r1])\np2.add_tools(ht2)\ntab2 = Panel(child=p2, title=\"Assumed current probability\")\n\n\nshow(Tabs(tabs=[tab1, tab2]), notebook_handle=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}