{"cells":[{"metadata":{"_uuid":"6b63ad26c7cf91dfbb3ab3163d7494843f568cea"},"cell_type":"markdown","source":"元の偉大なカーネル（ https://www.kaggle.com/christofhenkel/extract-image-features-from-pretrained-nn ）を日本語化しました。\nあとは説明用のコードを多少追加しました。\n\n- Take only profile picture (if existing else black)\n- pad to square aspect ratio\n- resize to 256\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import cv2\nimport pandas as pd\nimport numpy as np\nimport os\nfrom tqdm import tqdm, tqdm_notebook\n\ntrain_df = pd.read_csv('../input/train/train.csv')\nimg_size = 256\nbatch_size = 16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8a7d78a79a1ef646f5bf73fb6b4059ef5fc0771"},"cell_type":"code","source":"pet_ids = train_df['PetID'].values\nn_batches = len(pet_ids) // batch_size + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 元画像はこーんな感じ"},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 20,10\n\ndef load_raw_image(path, pet_id):\n    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n    return image\n\nfor i in range(1,10):\n    plt.subplot(330+i),plt.imshow(load_raw_image(\"../input/train_images/\", pet_ids[i]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 前処理をします"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.densenet import preprocess_input, DenseNet121\n\ndef resize_img(im):\n    old_size = im.shape[:2] # old_size is in (height, width) format\n    ratio = float(img_size)/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    # new_size should be in (width, height) format\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = img_size - new_size[1]\n    delta_h = img_size - new_size[0]\n    top, bottom = delta_h//2, delta_h-(delta_h//2)\n    left, right = delta_w//2, delta_w-(delta_w//2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n    return im\n\ndef resize_to_square(im):\n    old_size = im.shape[:2] # old_size is in (height, width) format\n    ratio = float(img_size)/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    # new_size should be in (width, height) format\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = img_size - new_size[1]\n    delta_h = img_size - new_size[0]\n    top, bottom = delta_h//2, delta_h-(delta_h//2)\n    left, right = delta_w//2, delta_w-(delta_w//2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n    return new_im","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## resizeの処理"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_image(path, pet_id):\n    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n    new_image = resize_to_square(image) # resize\n    return new_image\n\nfor i in range(1,10):\n    plt.subplot(330+i),plt.imshow(load_image(\"../input/train_images/\", pet_ids[i]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## preprocess\n- 転移学習するときは「preprocess_input」を入れる\n- 元々imagenetで画像を学習したときに合わせた前処理がなされるらしい"},{"metadata":{"trusted":true,"_uuid":"fe31dbcf0c682ade04a66e2dd19c39fd53cb8591"},"cell_type":"code","source":"def load_preprocess_image(path, pet_id):\n    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n    new_image = resize_to_square(image)\n    new_image = preprocess_input(new_image) # preprocess_input\n    return new_image\n\nfor i in range(1,10):\n    plt.subplot(330+i),plt.imshow(load_preprocess_image(\"../input/train_images/\", pet_ids[i]))\nprint('なんか人の目ではわかりづらくなったように見えるが、これでよいらしい')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49a2e974500ebdef071054d74a5cc96baf79e0cd"},"cell_type":"markdown","source":"## ここから特徴量抽出\n- このkernelではdensenet121の重みを使っている\n- 他にもいろいろな重みが使えるよ https://keras.io/applications/\n- include_top=Falseにすることで、モデルの全結合層（特徴を分類する層）を入れない状態のネットワークを定義できる\n- 出力は1024次元だが、1/4にするようプーリングして出力"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\nimport keras.backend as K\ninp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor = inp, include_top = False)\nx = backbone.output # この時点で 8 * 8 * 1024channel\nx = GlobalAveragePooling2D()(x) # 各channelの画素平均 このままでも1024列の特徴量として出力される\nx = Lambda(lambda x: K.expand_dims(x,axis = -1))(x) # 256列にするために配列末尾にdimを挿入 [None,1024] → [None,1024,1]\nx = AveragePooling1D(4)(x) # 1/4にするようにプーリング\nout = Lambda(lambda x: x[:,:,0])(x) # [None,256,1] → [None,256]\nm = Model(inp,out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## petの画像を読み込んで上記ネットワークで予測。重みはimagenetで既に訓練されたもの"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = {}\nbatch_pets = pet_ids[0:9]\nbatch_images = np.zeros((len(batch_pets),img_size,img_size,3))\nfor i,pet_id in enumerate(batch_pets):\n    try:\n        batch_images[i] = load_preprocess_image(\"../input/train_images/\", pet_id)\n    except:\n        pass\nbatch_preds = m.predict(batch_images)\nfor i,pet_id in enumerate(batch_pets):\n    features[pet_id] = batch_preds[i]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 無事に画像から特徴が抽出できました！\n- あとは煮るなり焼くなりお好きにどうぞ\n- note:今回のコンペは学習済みのimagenetのデータセット( http://starpentagon.net/analytics/ilsvrc2012_class_image/ )に犬や猫の画像が大量に入っているので特徴をちゃんと捉えた特徴量になっているのでは、と推測します。もし、imagenetで全然使用されていないカテゴリの画像から特徴量を抽出したい場合は、別途そのカテゴリの画像を用意してfine tuningしたモデルを新たに作った方が良い特徴量が抽出できると思います。"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feats = pd.DataFrame.from_dict(features, orient='index')\ntrain_feats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extra\n\nで、画像の特徴量入れるとどう精度が変わるの？\n\nまず、抽出した特徴量を入れずに、属性情報+sentiment+text、lgbでモデリングしたkernel(private:0.382）\nhttps://www.kaggle.com/wrosinski/baselinemodeling\n\n上記に抽出した特徴量（+αで画像のタテヨコとかの特徴量とか）も加えてxgbでモデリングしたkernel\nhttps://www.kaggle.com/ranjoranjan/single-xgboost-model (private:0.453）\n\nかなり精度が違う！"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}