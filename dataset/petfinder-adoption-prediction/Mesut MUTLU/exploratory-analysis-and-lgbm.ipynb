{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n#this kernel is based on personal work and other kernels found on kaggle \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy as sp\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nfrom datetime import datetime\nimport json\nimport math\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n#print(os.listdir(\"../input/petfinder-adoption-prediction/\"))\nprint(os.listdir(\"../input/train\"))\nprint(os.listdir(\"../input/test\"))\n\n# Any results you write to the current directory are saved as output.\n\ndef plog(msg):\n    print(datetime.now(), msg)\n\ntrain_df = pd.read_csv(\"../input/train/train.csv\")\ntest_df = pd.read_csv(\"../input/test/test.csv\")\nbreed_labels = pd.read_csv(\"../input/breed_labels.csv\")\nstate_labels = pd.read_csv(\"../input/state_labels.csv\")\ncolor_labels = pd.read_csv(\"../input/color_labels.csv\")\n#with open('../input/breed_attributes.json', 'r') as f:\n    #breed_attributes = json.load(f)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from enum import Enum\n\nclass Columns(Enum):\n    rescuer_id = [\"RescuerID\"]\n    ind_cont_columns = [\"Age\", \"Fee\", \"VideoAmt\", \"PhotoAmt\",\"Quantity\",\n                        \"DescScore\", \"DescMagnitude\", \"DescLength\", \"SentMagnitude\", \"SentMagnitute_Mean\", \n                                   \"SentScore\", \"SentScore_Mean\", \"EntSalience\", \"EntSalience_Mean\",\n                        \"NameLength\",\"Pet_Maturity\", \"Color_Type\",\"Fee_Per_Pet\"]\n    ind_num_cat_columns = [\"Type\", \"Breed1\", \"Breed2\", \"Gender\", \"Color1\", \"Color2\", \"Color3\",\n                           \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"Health\", \"State\", \"MaturitySize\",\n                           \"FurLength\", \"Pet_Breed\", \"Breed_Merge\", \"Pet_Purity\", \"Overall_Status\"]\n    ind_text_columns = [\"Name\", \"Description\", \"Entities\"]\n    iden_columns = [\"PetID\"]\n    dep_columns = [\"AdoptionSpeed\"]\n    n_desc_svd_comp = 120\n    desc_svd_cols = [\"desc_svd_\"+str(i) for i in range(n_desc_svd_comp)]\n    n_desc_nmf_comp = 5\n    desc_nmf_cols = [\"desc_nmf_\"+str(i) for i in range(n_desc_nmf_comp)]\n    img_num_cols_all = [\"P_RGB\", \"P_Dom_Px_Frac\", \"P_Dom_Px_Frac_Mean\", \"P_Dom_Score\", \"P_Dom_Score_Mean\", \"P_Vertex_X\", \"P_Vertex_Y\",\n                 \"P_Bound_Conf\", \"P_Bound_Conf_Mean\", \"P_Bound_Imp_Frac\", \"P_Bound_Imp_Frac_Mean\", \"P_Label_Score\", \"P_Label_Score_Mean\"]\n    img_num_cols_1 = [\"Vertex_X_1\", \"Vertex_Y_1\", \"Bound_Conf_1\", \"Bound_Imp_Frac_1\",\n                      \"Dom_Blue_1\", \"Dom_Green_1\", \"Dom_Red_1\",\n                \"RGBint_1\", \"Dom_Px_Fr_1\", \"Dom_Scr_1\", \"Lbl_Scr_1\",]\n    img_num_cols_2 = [\"Vertex_X_2\", \"Vertex_Y_2\", \"Bound_Conf_2\", \"Bound_Imp_Frac_2\",\n                      \"Dom_Blue_2\", \"Dom_Green_2\", \"Dom_Red_2\",\n                \"RGBint_2\", \"Dom_Px_Fr_2\", \"Dom_Scr_2\", \"Lbl_Scr_2\"]\n    img_num_cols_3 = [\"Vertex_X_3\", \"Vertex_Y_3\", \"Bound_Conf_3\", \"Bound_Imp_Frac_3\",\n                      \"Dom_Blue_3\", \"Dom_Green_3\", \"Dom_Red_3\",\n                \"RGBint_3\", \"Dom_Px_Fr_3\", \"Dom_Scr_3\", \"Lbl_Scr_3\"]\n    img_lbl_cols_1 = [\"Lbl_Img_1\"]\n    img_lbl_cols_2 = [\"Lbl_Img_2\"]\n    img_lbl_cols_3 = [\"Lbl_Img_3\"]\n    img_lbl_col = [\"Lbl_Dsc\"]\n    n_iann_svd_comp = 5\n    iann_svd_cols = [\"iann_svd_\" + str(i) for i in range(n_iann_svd_comp)]\n    n_iann_nmf_comp = 5\n    iann_nmf_cols = [\"iann_nmf_\" + str(i) for i in range(n_iann_nmf_comp)]\n    n_entities_svd_comp = 5\n    entities_svd_cols = [\"entities_svd_\" + str(i) for i in range(n_entities_svd_comp)]\n    n_entities_nmf_comp = 5\n    entities_nmf_cols = [\"entities_nmf_\" + str(i) for i in range(n_entities_nmf_comp)]\n    item_cnt_incols = [\"RescuerID\", \"Breed1\", \"Breed2\", \"Breed_Merge\", \"Age\"]#, \"Breed1\",\"Breed2\", \"Color1\", \"Color2\", \"Color3\", \"State\"]\n    item_cnt_cols =  [c + \"_Cnt\" for c in item_cnt_incols]\n    item_cnt_mtype_cols =  [c + \"_Cnt_MType\" for c in item_cnt_incols]\n    item_type_incols = [\"RescuerID_Cnt\", \"Breed1_Cnt\", \"Breed2_Cnt\", \"Breed_Merge_Cnt\", \"Age_Cnt\"]#, \"Breed1\",\"Breed2\", \"Color1\", \"Color2\", \"Color3\", \"State\"]\n    item_type_cols =  [c + \"_StdType\" for c in item_type_incols]\n    kbin_incols = [\"Age\", \"Fee\", \"Fee_Per_Pet\"] + item_cnt_cols  #, \"Breed1\",\"Breed2\", \"Color1\", \"Color2\", \"Color3\", \"State\"]\n    kbin_cols =  [c + \"_Kbin\" for c in kbin_incols]\n    item_adp_incols = item_type_cols + item_cnt_cols + kbin_cols + item_cnt_mtype_cols#[\"RescuerID_Cnt_StdType\", \"Breed_Merge_Cnt_StdType\"]#, \"Breed1\",\"Breed2\", \"Color1\", \"Color2\", \"Color3\", \"State\"]\n    item_adp_cols =  [c + \"_Adp\" for c in item_adp_incols]\n    fee_mean_incols = [\"Breed1\", \"Breed2\", \"Age\", \"Breed_Merge\", \"State\"] + item_cnt_cols + item_cnt_mtype_cols + item_type_cols\n    fee_mean_cols = [\"Fee_Per_Pet_\" + c for c in fee_mean_incols]\n    loo_incols = [\"Pet_Breed\", \"State\"]\n    loo_cols = [c + \"_Loo\" for c in loo_incols]\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    scaling_cols = [\"Age\", \"Fee\", \"RescuerID_Cnt\", \"Breed1_Cnt\", \"Breed2_Cnt\", \"Breed_Merge_Cnt\", \"Age_Cnt\", \"Fee_Per_Pet\"]  + fee_mean_cols\n    \n    ft_cat_cols = [\"Breed1\", \"Breed2\", \"Breed_Merge\", \"Overall_Status\"] + item_cnt_cols + item_type_cols + kbin_cols\n    ft_new_cols = [\"Age\", \"Fee\", \"Quantity\", \"VideoAmt\", \"PhotoAmt\"]\n    agg_calc = [\"std\", \"skew\", \"mean\", \"max\", \"min\"]\n    \n    def feature_cols(fcc, fnc, agg_calc):\n        tmp_ft_cols = []\n        for cc in fcc:\n            # print(cc)\n            for a in agg_calc:\n                # print(a)\n                if a != \"COUNT\":\n                    for x in fnc:\n                        # print(x)\n                        tmp_ft_cols.append(cc + \"_\" + a + \"(Pets.\" + x + \")\")\n                else:\n                    tmp_ft_cols.append(cc + \"_\" + a + \"(Pets)\")\n        return tmp_ft_cols\n\n    ft_cols = feature_cols(ft_cat_cols, ft_new_cols, agg_calc)\n    #\"Type\", \"Gender\", \"Color1\", \"Color2\", \"Color3\",\"Vaccinated\", \"Dewormed\", \"Sterilized\", \"FurLength\",\n    \n    \n    barplot_cols = ind_num_cat_columns + item_type_cols + kbin_cols + item_cnt_mtype_cols \n    boxplot_cols = ind_cont_columns + desc_svd_cols + desc_nmf_cols + img_num_cols_all + img_num_cols_1 + img_num_cols_2 + img_num_cols_3 + img_lbl_cols_1 + img_lbl_cols_2 + img_lbl_cols_3 \\\n                    + iann_svd_cols + iann_nmf_cols + entities_svd_cols + entities_nmf_cols + ft_cols + item_cnt_cols  + item_adp_cols + fee_mean_cols \n    \nplog(\"Done\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e7b5656a3bc870ce5b85e49612515d36a4729df"},"cell_type":"code","source":"def leaveoneout(X, y):\n    \n    ret = pd.DataFrame(columns=Columns.loo_cols.value, index=None)\n    R = 10\n    sums = np.zeros([len(X), 1])\n    lens = np.zeros([len(X), 1])\n    randomness =1+np.random.normal(scale = 0.03, size=len(sums)).reshape(len(sums), 1)\n    for c in X.columns.values:\n        for uval in X[c].unique():\n            indexes = X[X[c] == uval].index\n            s_tot = y.loc[indexes].sum()\n            l = len(indexes)-1+R\n            lens[indexes, 0] = 1/l\n            #print(uval, indexes, s_tot)\n            for i in indexes:\n                s_ind = s_tot - int(y.loc[i])\n                sums[i,0] = s_ind\n        ret[c+\"_Loo\"] = (sums*lens*randomness).ravel().tolist()\n                \n    return ret\nif 1 == 0:\n    for c in Columns.loo_cols.value:\n        if c in train_df.columns.values.tolist():\n            plog(\"Dropping \"+c +\" in train dataset\")\n            train_df.drop(c, axis=1, inplace=True)\n            plog(\"Dropped \"+c +\" in train dataset\")\n\n    df_loo = leaveoneout(train_df[Columns.loo_incols.value], train_df[\"AdoptionSpeed\"])\n    train_df = pd.concat([train_df,df_loo], axis=1)\n    #print(train_df.head())\n\n\n    for c in Columns.loo_cols.value:\n        if c in test_df.columns.values.tolist():\n            plog(\"Dropping \"+c +\" in test dataset\")\n            test_df.drop(c, axis=1, inplace=True)\n            plog(\"Dropped \"+c +\" in test dataset\")\n\n    for c in Columns.loo_incols.value:\n        df_loo = train_df[[c,\"AdoptionSpeed\"]].groupby(c).agg({'AdoptionSpeed':['sum', 'count']})\n        df_loo.columns = df_loo.columns.droplevel(0)\n        df_loo[c+\"_Loo\"] = df_loo[\"sum\"] / (df_loo[\"count\"]+10)\n        df_loo.drop([\"sum\",\"count\"], axis=1, inplace=True)\n        test_df = test_df.set_index(c).join(df_loo).reset_index()\n\n    for c in Columns.loo_incols.value:\n        if train_df[c].isna().any():\n            print(\"null\", c)\n            #train_df[c].fillna(train_df[c].mean(), inplace=True)\nplog(\"Done\")\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bf21aaf8b4c71b221985db8524738f8d22e47c2"},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nimport numpy.core.defchararray as np_f\nfrom category_encoders.leave_one_out import LeaveOneOutEncoder\n\ndef onehotencoding(df):\n    enc = OneHotEncoder(categories=\"auto\")\n    encodedcols = []\n    for c in [\"Type\", \"Breed1\", \"Breed2\", \"Gender\"]: #Columns.ind_num_cat_columns.value:\n        y = enc.fit_transform(df[[c]])\n        col_name = enc.get_feature_names()\n        col_name = np_f.replace(col_name.astype(str), 'x0', c)\n        dfo = pd.DataFrame(columns=col_name, data=y.toarray())       \n        df = pd.concat([df, dfo], axis=1)\n        for ec in col_name:\n            encodedcols.append(ec)\n    return df, encodedcols\n\nif 1 == 0:\n    plog(\"One hot encoding started for train set\")\n    train_df, train_encodedcols = onehotencoding(train_df)\n    plog(\"One hot encoding ended for train set\")\n    plog(\"One hot encoding started for test set\")\n    test_df, test_encodedcols = onehotencoding(test_df)\n    plog(\"One hot encoding ended for test set\")\n    if train_encodedcols == test_encodedcols:\n        plog(\"Columns are same on test and train sets for one hot encoded cols\")\n        ohc_cols = train_encodedcols\nplog(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed400d96e4598bda97eb00d59e90e43574aa4327"},"cell_type":"code","source":"if 1 == 0:\n    breed_id = {}\n    for id,name in zip(breed_labels.BreedID,breed_labels.BreedName):\n        breed_id[id] = name\n\n    cat_atts = breed_attributes['cat_breeds']\n    dog_atts = breed_attributes['dog_breeds']\n\n    cat_names = [i for i in cat_atts.keys()]\n    #print(cat_names)\n    dog_names = [i for i in dog_atts.keys()]\n    #print(dog_names)\n\n    def get_breed_atts(df, breed):\n        i=1\n        df_breed_ids = df[breed].unique()#(breed)[\"PetID\"].count().reset_index()\n        print(i, len(df_breed_ids))\n        for id in df_breed_ids:\n            #print(i, datetime.now())\n            if id in breed_id.keys(): \n                name = breed_id[id] \n                if name in cat_names:\n                    #print(cat_ratings[name])\n                    for key in cat_atts[name].keys():\n                        k = breed+\"_\"+key.strip().replace(\" \", \"_\")\n                        #if k in df.columns.values.tolist():\n                        #    df.drop(k, axis=1, inplace=True)\n                        if k not in df.columns.values.tolist():\n                            df[k] = np.NAN\n                        indexes = df.loc[:,breed] == id\n                        df.loc[indexes, k] = cat_atts[name][key]\n                        #print(df[df[breed]==id].head())\n                        #df[k].head()\n                if name in dog_names:\n                    #print(dog_ratings[name])\n                    for key in dog_atts[name].keys():\n                        k = breed+\"_\"+key.strip().replace(\" \", \"_\")\n                        #if k in df.columns.values.tolist():\n                        #    df.drop(k, axis=1, inplace=True)\n                        if k not in df.columns.values.tolist():\n                            df[k] = np.NAN\n                        #print(len(df[df[breed]==id]), name, key, dog_atts[name][key])\n                        indexes = df.loc[:,breed] == id\n                        df.loc[indexes, k] = dog_atts[name][key]\n                        #print(df[df[breed]==id].head())\n                        #print(df[k].head())\n            i += 1\n        return df\n\n    plog(\"Getting Breed1 attributes for train\")\n    train_df = get_breed_atts(train_df, \"Breed1\")\n    plog(\"Getted Breed1 attributes for train\")\n    plog(\"Getting Breed2 attributes for train\")\n    train_df = get_breed_atts(train_df, \"Breed2\")\n    plog(\"Getted Breed2 attributes for train\")\n    print(train_df.info())\n    import sys\n    sys.exit()\n    plog(\"Getting Breed1 attributes for test\")\n    test_df = get_breed_atts(test_df, \"Breed1\")\n    plog(\"Getted Breed1 attributes for test\")\n    plog(\"Getting Breed2 attributes for test\")\n    test_df = get_breed_atts(test_df, \"Breed2\")\n    plog(\"Getted Breed2 attributes for test\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dbab1ceb53f685ab43dbc2b137e1d79ed4859e0"},"cell_type":"code","source":"import json\n#getting description sentiment analyses\n    \ndef get_desc_anly(type, recalc):\n    if recalc == 1:\n        if type == \"train\":\n            path = \"../input/train_sentiment/\"#../input/train_sentiment/\n        elif type == \"test\":\n            path = \"../input/test_sentiment/\"#../input/test_sentiment/\n        plog(\"Getting description sentiment analysis for \"+type+\"_sentiment.csv\")\n        files = [f for f in os.listdir(path) if (f.endswith('.json') & os.path.isfile(path+f))]\n\n        df = pd.DataFrame(columns=[\"PetID\", \"DescScore\", \"DescMagnitude\"])\n        i = 0\n        for f in files:\n            #print(path + f)\n            with open(path+f, encoding=\"utf8\") as json_data:\n                data = json.load(json_data)\n            #print(data)\n            #pf = pd.DataFrame.from_dict(data, orient='index').T.set_index('index')\n\n            #print(data[\"documentSentiment\"][\"score\"],data[\"documentSentiment\"][\"magnitude\"])\n            df.loc[i]= [f[:-5],data[\"documentSentiment\"][\"score\"],data[\"documentSentiment\"][\"magnitude\"]]\n            i = i+1\n        #df.to_csv(type+\"_sentiment.csv\", index=False)\n    elif recalc == 0:\n        df = pd.read_csv(type+\"_sentiment.csv\")\n    plog(\"Got description sentiment analysis for \"+type+\"_sentiment.csv\")\n    return df\n\nrc = 1\nif 1 == 0:\n    train_snt = get_desc_anly(\"train\",rc)\n    test_snt = get_desc_anly(\"test\", rc)\n\n    train_df = train_df.set_index(\"PetID\").join(train_snt.set_index(\"PetID\")).reset_index()\n    test_df = test_df.set_index(\"PetID\").join(test_snt.set_index(\"PetID\")).reset_index()\n    #train_df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n    #test_df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n\n\n    #train_df[\"DescScore\"].fillna(0, inplace=True)\n    #train_df[\"DescMagnitude\"].fillna(0, inplace=True)\n    train_df[\"Description\"].fillna(\"\", inplace=True)\n    train_df[\"Name\"].fillna(\"\", inplace=True)\n    #test_df[\"DescScore\"].fillna(0, inplace=True)\n    #test_df[\"DescMagnitude\"].fillna(0, inplace=True)\n    test_df[\"Description\"].fillna(\"\", inplace=True)\n    test_df[\"Name\"].fillna(\"\", inplace=True)\n    train_df[\"DescLength\"] = train_df[\"Description\"].str.len()\n    test_df[\"DescLength\"] = test_df[\"Description\"].str.len()\n    train_df[\"NameLength\"] = train_df[\"Name\"].str.len()\n    test_df[\"NameLength\"] = test_df[\"Name\"].str.len()\n\n    #print(train_df.describe(include=\"all\"))\n    #print(test_df.describe(include=\"all\"))\nplog(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6fbf994bde22bb851e7673950c534eb8d4c66c8"},"cell_type":"code","source":"import json\n#getting description sentiment analyses\n    \ndef get_desc_anly_v2(type, recalc):\n    if recalc == 1:\n        if type == \"train\":\n            path = \"../input/train_sentiment/\"#../input/train_sentiment/\n        elif type == \"test\":\n            path = \"../input/test_sentiment/\"#../input/test_sentiment/\n        plog(\"Getting description sentiment analysis for \"+type+\"_sentiment.csv\")\n        files = [f for f in os.listdir(path) if (f.endswith('.json') & os.path.isfile(path+f))]\n\n        df = pd.DataFrame(columns=[\"PetID\", \"DescScore\", \"DescMagnitude\", \"SentMagnitude\", \"SentMagnitute_Mean\", \n                                   \"SentScore\", \"SentScore_Mean\", \"EntSalience\", \"EntSalience_Mean\", \"Entities\"])\n        i = 0\n        for f in files:\n            #print(path + f)\n            with open(path+f, encoding=\"utf8\") as json_data:\n                data = json.load(json_data)\n            #print(data)\n            #pf = pd.DataFrame.from_dict(data, orient='index').T.set_index('index')\n            s_magnitude = 0\n            m_magnitude = 0\n            s_score = 0\n            m_score = 0\n            num_sentences = 0\n            for sentence in data.get(\"sentences\"):\n                s_magnitude += sentence.get(\"sentiment\").get(\"magnitude\",0)\n                s_score += sentence.get(\"sentiment\").get(\"score\",0)\n                num_sentences += 1\n            if num_sentences > 0:\n                m_magnitude = s_magnitude / num_sentences\n                m_score = s_score / num_sentences\n                \n            s_salience = 0\n            m_salience = 0\n            s_entities = \"\"\n            num_entities = 0\n            for entity in data.get(\"entities\"):\n                s_salience += entity.get(\"salience\",0)\n                s_entities = s_entities + \" \" + entity.get(\"name\", \"\")\n                num_entities += 1\n            if num_entities > 0:\n                m_salience = s_salience / num_entities\n            \n            d_score = data[\"documentSentiment\"][\"score\"]\n            d_magnitude = data[\"documentSentiment\"][\"magnitude\"]\n            #print(data[\"documentSentiment\"][\"score\"],data[\"documentSentiment\"][\"magnitude\"])\n            df.loc[i]= [f[:-5],d_score,  d_magnitude, s_magnitude, m_magnitude, s_score, m_score, s_salience, m_salience, s_entities]\n            i = i+1\n        #df.to_csv(type+\"_sentiment.csv\", index=False)\n    elif recalc == 0:\n        df = pd.read_csv(type+\"_sentiment.csv\")\n    plog(\"Got description sentiment analysis for \"+type+\"_sentiment.csv\")\n    return df\n\nrc = 1\nif 1 == 1:\n    \n    for c in [ \"DescScore\", \"DescMagnitude\" , \"SentMagnitude\", \"SentMagnitute_Mean\", \n                                   \"SentScore\", \"SentScore_Mean\", \"EntSalience\", \"EntSalience_Mean\", \"Entities\"]:\n    \n        if c in train_df.columns.values.tolist():\n            train_df.drop([c], axis=1, inplace=True)\n        if c in test_df.columns.values.tolist():\n            test_df.drop([c], axis=1, inplace=True)\n        \n    \n    train_snt = get_desc_anly_v2(\"train\",rc)\n    test_snt = get_desc_anly_v2(\"test\", rc)\n\n    train_df = train_df.set_index(\"PetID\").join(train_snt.set_index(\"PetID\")).reset_index()\n    test_df = test_df.set_index(\"PetID\").join(test_snt.set_index(\"PetID\")).reset_index()\n    #train_df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n    #test_df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n\n\n    for c in [ \"DescScore\", \"DescMagnitude\" , \"SentMagnitude\", \"SentMagnitute_Mean\", \n                                   \"SentScore\", \"SentScore_Mean\", \"EntSalience\", \"EntSalience_Mean\"]:\n        #train_df[c].fillna(0, inplace=True)\n        #test_df[c].fillna(0, inplace=True)\n        pass\n    for c in [\"Description\", \"Name\", \"Entities\"]:\n        train_df[c].fillna(\"\", inplace=True)\n        test_df[c].fillna(\"\", inplace=True)\n        \n    train_df[\"DescLength\"] = train_df[\"Description\"].str.len()\n    test_df[\"DescLength\"] = test_df[\"Description\"].str.len()\n    train_df[\"NameLength\"] = train_df[\"Name\"].str.len()\n    test_df[\"NameLength\"] = test_df[\"Name\"].str.len()\n\n    #print(train_df.describe(include=\"all\"))\n    #print(test_df.describe(include=\"all\"))\nplog(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fa40f2a4a7d9a12a3ae986ff28cc38d660183f7"},"cell_type":"code","source":"def get_img_meta(type, img_num, recalc):\n    #getting image analyse metadata\n    if recalc == 1:\n        if type == \"train\":\n            path = \"../input/train_metadata/\"  \n        else:\n            path = \"../input/test_metadata/\" \n            \n        if img_num == \"1\":\n            cols = Columns.iden_columns.value + Columns.img_num_cols_1.value + Columns.img_lbl_cols_1.value\n            df_imgs = pd.DataFrame(columns=cols)\n        elif img_num == \"2\":\n            cols = Columns.iden_columns.value + Columns.img_num_cols_2.value + Columns.img_lbl_cols_2.value\n            df_imgs = pd.DataFrame(columns=cols)\n        elif img_num == \"3\":\n            cols = Columns.iden_columns.value + Columns.img_num_cols_3.value + Columns.img_lbl_cols_3.value\n            df_imgs = pd.DataFrame(columns=cols)\n        else:\n            plog(\"This function supports images until 3rd, so img_num should be <= 3\")\n            return False\n            \n        plog(\"Getting image analyse metadata for \" +type + str(img_num)+\" files\")\n\n        images = [f for f in sorted(os.listdir(path)) if (f.endswith(\"-\"+img_num+\".json\") & os.path.isfile(path + f))]\n\n        i = 0\n        for img in images:\n            PetID = img[:-7]\n            #print(i, PetID,k, img, (img[-6:-5]), l_petid)\n\n            with open(path + img, encoding=\"utf8\") as json_data:\n                data = json.load(json_data)\n            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2].get('x',-1)\n            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2].get('y',-1)\n            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0].get('confidence',-1)\n            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('blue', 255)\n            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('green', 255)\n            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('red', 255)\n            RGBint = (dominant_red << 16) + (dominant_green << 8) + dominant_blue\n            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0].get('pixelFraction', -1)\n            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0].get('score', -1)\n\n            if data.get('labelAnnotations'):\n                label_description = \"\"\n                label_score = 0\n                j = 1\n                for ann in data.get('labelAnnotations'):\n                    if ann.get('score', 0) >= 0.00:\n                        label_score = (ann.get('score', 0) + label_score) / j\n                        label_description = label_description + \" \" + ann.get(\"description\", \"nothing\")\n                        j += 1\n            else:\n                label_description = 'nothing'\n                label_score = -1\n\n            df_imgs.loc[i, cols] = [PetID,vertex_x, vertex_y, bounding_confidence,bounding_importance_frac, \n                                    RGBint, dominant_blue, dominant_green, dominant_red, \n                            dominant_pixel_frac, dominant_score, label_score, label_description]\n\n            i += 1\n\n        #print(df_imgs.head())\n        #df_imgs.to_csv(type+\"_metadata-\"+img_num+\".csv\", index=False)\n    elif recalc == 0:\n        df_imgs = pd.read_csv(type+\"_metadata-\"+img_num+\".csv\")\n    plog(\"Got image analyse metadata for \" +type + str(img_num)+\" files\")\n    return df_imgs\n\nrc = 1\nif 1 == 0:\n    train_metadata_1 = get_img_meta(\"train\", \"1\", rc)\n    train_metadata_2 = get_img_meta(\"train\", \"2\", rc)\n    train_metadata_3 = get_img_meta(\"train\", \"3\", rc)\n    test_metadata_1 = get_img_meta(\"test\", \"1\", rc)\n    test_metadata_2 = get_img_meta(\"test\", \"2\", rc)\n    test_metadata_3 = get_img_meta(\"test\", \"3\", rc)\n\n    train_metadata = train_metadata_1.set_index(\"PetID\").join(train_metadata_2.set_index(\"PetID\")).join(train_metadata_3.set_index(\"PetID\")).reset_index()\n    test_metadata = test_metadata_1.set_index(\"PetID\").join(test_metadata_2.set_index(\"PetID\")).join(test_metadata_3.set_index(\"PetID\")).reset_index()\n\n    #train_metadata[\"Lbl_Dsc\"] = train_metadata[\"Lbl_Img_1\"] + train_metadata[\"Lbl_Img_2\"] + train_metadata[\"Lbl_Img_3\"]\n    #test_metadata[\"Lbl_Dsc\"] = test_metadata[\"Lbl_Img_1\"] + test_metadata[\"Lbl_Img_2\"] + test_metadata[\"Lbl_Img_3\"]\n\n    #train_metadata[\"Lbl_Dsc\"].fillna(\"\", inplace=True)\n    #test_metadata[\"Lbl_Dsc\"].fillna(\"\", inplace=True)\n\n    train_metadata.drop([\"Lbl_Img_1\", \"Lbl_Img_2\", \"Lbl_Img_3\"], axis=1, inplace = True)\n    test_metadata.drop([\"Lbl_Img_1\", \"Lbl_Img_2\", \"Lbl_Img_3\"], axis=1, inplace = True)\n\n    #train_metadata.fillna(0, inplace=True)\n    #test_metadata.fillna(0, inplace=True)\n\n    train_df = train_df.set_index(\"PetID\").join(train_metadata.set_index(\"PetID\")).reset_index()\n    test_df = test_df.set_index(\"PetID\").join(test_metadata.set_index(\"PetID\")).reset_index()\n\n    #train_df[\"Lbl_Dsc\"].fillna(\"\", inplace=True)\n    #test_df[\"Lbl_Dsc\"].fillna(\"\", inplace=True)\n\n    for col in Columns.img_num_cols_1.value + Columns.img_num_cols_2.value + Columns.img_num_cols_3.value:\n        pass\n        #train_df[col].fillna(train_df[col].mean(), inplace=True)\n        #test_df[col].fillna(test_df[col].mean(), inplace=True)    \n\n    print(train_df.columns.values)\n    print(test_df.columns.values)\n    plog(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ea59440f219e07b4fd9da7cef1e335590814193"},"cell_type":"code","source":"def get_all_img_meta(type, recalc):\n    # getting image analyse metadata\n    if recalc == 1:\n        if type == \"train\":\n            path = \"../input/train_metadata/\"\n        else:\n            path = \"../input/test_metadata/\"\n\n        all_images = [f for f in sorted(os.listdir(path)) if\n                  (f.endswith(\".json\") & os.path.isfile(path + f))]\n        len_images = len(all_images)\n\n        pets = list(set([f[:f.find(\"-\")] for f in sorted(os.listdir(path)) if\n                  (f.endswith(\".json\") & os.path.isfile(path + f))]))\n\n        np_pets = np.asarray(pets).reshape(len(pets), 1)\n        df_cols = Columns.img_num_cols_all.value + [\"Lbl_Dsc\"]\n        #print(df_cols)\n        np_data = np.zeros((len(pets),len(df_cols)))\n        data = np.concatenate((np_pets,np_data), axis=1 )\n        df_pet_img_meta = pd.DataFrame(columns=[\"PetID\"]+df_cols, data=data)\n        df_pet_img_meta[\"Lbl_Dsc\"] = \"\"\n        df_pet_img_meta.set_index(\"PetID\", inplace=True)\n        #print(df_pet_img_meta.head())\n\n        h = 1\n        for pet in pets:\n            images = [k for k in all_images if pet in k]\n\n            p_rgb = 0\n            p_dominant_pixel_frac = 0\n            p_dominant_score = 0\n            p_vertex_x = 0\n            p_vertex_y = 0\n            p_bounding_confidence = 0\n            p_bounding_importance_frac = 0\n            p_label_score = 0\n            p_label_description = \"\"\n            num_of_images = len(images)\n\n            for img in images:\n                imgnum = img.split(\"-\", 1)[1].strip(\".json\")\n                with open(path + img, encoding=\"utf8\") as json_data:\n                    image = json.load(json_data)\n\n                #print(img, h, len_images)\n                h = h+1\n\n                i_rgb = 0\n                i2_rgb = 0\n                i_dominant_pixel_frac = 0\n                i_dominant_score = 0\n                i_label_score = 0\n                i_label_description = \"\"\n                i_num_colors = 0\n                i_num_label_annotations = 0\n\n                i_vertex_x = image['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2].get('x', 0)\n                i_vertex_y = image['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2].get('y', 0)\n                i_bounding_confidence = image['cropHintsAnnotation']['cropHints'][0].get('confidence', 0)\n                i_bounding_importance_frac = image['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', 0)\n\n                if image.get('imagePropertiesAnnotation').get(\"dominantColors\").get(\"colors\"):\n                    i_num_colors = 0 #len(image.get('imagePropertiesAnnotation').get(\"dominantColors\").get(\"colors\"))\n                    t_rgb = 0\n                    for color in image.get('imagePropertiesAnnotation').get(\"dominantColors\").get(\"colors\"):\n                        #print(color, color.get(\"color\").get(\"red\"))\n                        if color.get('score', 0) >= 0.00:\n                            r = color.get(\"color\").get('red', 255)\n                            g = color.get(\"color\").get('green', 255)\n                            b = color.get(\"color\").get('blue', 255)\n                            #rgbint = ((r << 16) + (g << 8) + b)\n                            m_rgb = (r**2 + g**2 + b**2)/3\n                            t_rgb = t_rgb + m_rgb\n                            i_dominant_pixel_frac = i_dominant_pixel_frac + color.get('pixelFraction', 0)\n                            i_dominant_score = i_dominant_score + color.get('score', 0)\n                            i_num_colors = i_num_colors + 1\n\n                    if i_num_colors>0:\n                        if t_rgb > 0:\n                            i_rgb = np.sqrt(t_rgb/i_num_colors)\n                        if i_dominant_score > 0:\n                            i_dominant_score = i_dominant_score/i_num_colors\n                    #print(i_rgb, i_dominant_pixel_frac, i_dominant_score)\n                if image.get('labelAnnotations'):\n                    i_num_label_annotations = 0 #len(image.get('labelAnnotations'))\n                    for ann in image.get('labelAnnotations'):\n                        if ann.get('score', 0) >= 0.00:\n                            i_label_score = ann.get('score', 0) + i_label_score\n                            i_num_label_annotations = i_num_label_annotations + 1\n                            if ann.get(\"description\"):\n                                i_label_description = i_label_description + \" \" + ann.get(\"description\", \"\")\n\n                    if i_num_label_annotations>0:\n                        if i_label_score > 0:\n                            i_label_score = i_label_score/i_num_label_annotations\n\n                    #print(i_label_score, i_label_description)\n\n                p_rgb = p_rgb + i_rgb**2\n                p_dominant_pixel_frac = p_dominant_pixel_frac + i_dominant_pixel_frac\n                p_dominant_score = p_dominant_score + i_dominant_score\n                p_vertex_x = p_vertex_x + i_vertex_x\n                p_vertex_y = p_vertex_y + i_vertex_y\n                p_bounding_confidence = p_bounding_confidence + i_bounding_confidence\n                p_bounding_importance_frac = p_bounding_importance_frac + i_bounding_importance_frac\n                p_label_score = p_label_score + i_label_score\n                p_label_description = p_label_description + \" \" + i_label_description\n\n            p_rgb = np.sqrt(p_rgb/num_of_images)\n            pm_dominant_pixel_frac = p_dominant_pixel_frac/num_of_images\n            pm_dominant_score = p_dominant_score/num_of_images\n            p_vertex_x = p_vertex_x/num_of_images\n            p_vertex_y = p_vertex_y/num_of_images\n            pm_bounding_confidence = p_bounding_confidence/num_of_images\n            pm_bounding_importance_frac = p_bounding_importance_frac/num_of_images\n            pm_label_score = p_label_score/num_of_images\n\n            df_pet_img_meta.loc[pet, df_cols] = [p_rgb, p_dominant_pixel_frac, pm_dominant_pixel_frac, p_dominant_score, pm_dominant_score, \n                                                 p_vertex_x, p_vertex_y, p_bounding_confidence, pm_bounding_confidence, \n                                                 p_bounding_importance_frac, pm_bounding_importance_frac, p_label_score, pm_label_score,\n                                                 p_label_description]\n            #print(df_pet_img_meta.head())\n        #print(df_pet_img_meta.head())\n        #df_pet_img_meta.reset_index().to_csv(type + \"_metadata_all.csv\", index=False)\n    elif recalc == 0:\n        df_pet_img_meta = pd.read_csv(type + \"_metadata_all.csv\")\n\n    return df_pet_img_meta\n\nrc = 1\nif 1 == 1:\n    plog(\"Starting all images metadata collection on train\")\n    train_metadata = get_all_img_meta(\"train\", rc)\n    plog(\"Ended all images metadata collection on train\")\n    plog(\"Starting all images metadata collection on test\")\n    test_metadata = get_all_img_meta(\"test\", rc)\n    plog(\"Ended all images metadata collection on test\")\n\n\n    train_df = train_df.set_index(\"PetID\").join(train_metadata).reset_index()\n    test_df = test_df.set_index(\"PetID\").join(test_metadata).reset_index()\n    \n    train_df[\"Lbl_Dsc\"].fillna(\"\", inplace=True)\n    test_df[\"Lbl_Dsc\"].fillna(\"\", inplace=True)\n\n    for col in Columns.img_num_cols_all.value:\n        pass\n        #train_df[col].fillna(train_df[col].mean(), inplace=True)\n        #test_df[col].fillna(test_df[col].mean(), inplace=True)    \n\n    #print(train_df.columns.values)\n    #print(test_df.columns.values)\nplog(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40776579d35954d8e72a761f48949a0a7ed0777e"},"cell_type":"code","source":"#tfidf and tsvd implemendation on text columns description and first three image labels\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import LabelEncoder\nfrom nltk.corpus import stopwords\n\ndef tfidf_2(train, n_comp, out_cols):\n    stop_words = set(stopwords.words('english'))\n\n    tfv = TfidfVectorizer(min_df=3, max_features=10000,\n                          strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n                          ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words = stop_words)\n\n    # Fit TFIDF\n    tfv.fit(list(train))\n    X = tfv.transform(train)\n\n    svd = TruncatedSVD(n_components=n_comp, random_state=1337)\n    svd.fit(X)\n    plog(\"explained_variance_ratio_.sum() \"+str(svd.explained_variance_ratio_.sum()))\n    #print(svd.explained_variance_ratio_)\n    X = svd.transform(X)\n    df_svd = pd.DataFrame(X, columns=out_cols)\n    \n    return df_svd\n\nplog(\"TSVD for train description started\")\nfor c in Columns.desc_svd_cols.value:\n    if c in train_df.columns.values:\n        train_df.drop([c], axis=1, inplace=True)\nsvd_train_desc = tfidf_2(train_df.Description, Columns.n_desc_svd_comp.value, Columns.desc_svd_cols.value)\nplog(\"TSVD for train description ended\")\nplog(\"TSVD for test description started\")\nfor c in Columns.desc_svd_cols.value:\n    if c in test_df.columns.values:\n        test_df.drop([c], axis=1, inplace=True)\nsvd_test_desc = tfidf_2(test_df.Description, Columns.n_desc_svd_comp.value, Columns.desc_svd_cols.value)\nplog(\"TFIDF for test description ended\")\n\nplog(\"TSVD for train image label started\")\nfor c in Columns.iann_svd_cols.value:\n    if c in train_df.columns.values:\n        train_df.drop([c], axis=1, inplace=True)\nsvd_train_lbldsc = tfidf_2(train_df.Lbl_Dsc, Columns.n_iann_svd_comp.value, Columns.iann_svd_cols.value)\nplog(\"TSVD for train image label ended\")\nplog(\"TSVD for test image label started\")\nfor c in Columns.iann_svd_cols.value:\n    if c in test_df.columns.values:\n        test_df.drop([c], axis=1, inplace=True)\nsvd_test_lbldsc = tfidf_2(test_df.Lbl_Dsc, Columns.n_iann_svd_comp.value, Columns.iann_svd_cols.value)\nplog(\"TSVD for test image label ended\")\n\nplog(\"TSVD for train desc entities started\")\nfor c in Columns.entities_svd_cols.value:\n    if c in train_df.columns.values:\n        train_df.drop([c], axis=1, inplace=True)\nsvd_train_entities = tfidf_2(train_df.Entities, Columns.n_entities_svd_comp.value, Columns.entities_svd_cols.value)\nplog(\"TSVD for train desc entities ended\")\nplog(\"TSVD for test desc entities started\")\nfor c in Columns.entities_svd_cols.value:\n    if c in test_df.columns.values:\n        test_df.drop([c], axis=1, inplace=True)\nsvd_test_entities = tfidf_2(test_df.Entities, Columns.n_entities_svd_comp.value, Columns.entities_svd_cols.value)\nplog(\"TSVD for test desc entities ended\")\n\ntrain_df = pd.concat([train_df,svd_train_desc, svd_train_lbldsc, svd_train_entities], axis=1)\n#train_df = pd.concat([train_df,svd_train_desc, svd_train_lbldsc], axis=1)\ntest_df = pd.concat([test_df,svd_test_desc, svd_test_lbldsc, svd_test_entities], axis=1)\n#test_df = pd.concat([test_df,svd_test_desc, svd_test_lbldsc], axis=1)\n\nprint(train_df.shape)\n#print(train_df.columns.values)\nprint(test_df.shape)\n#print(test_df.columns.values)\nprint(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4e939bd95242efbedc0aae0fd179c5cf2eaf01a"},"cell_type":"code","source":"from sklearn.decomposition import NMF\nfrom nltk.corpus import stopwords\n\ndef nmf_d(train, n_comp, out_cols):\n    stop_words = set(stopwords.words('english'))\n    tfv = TfidfVectorizer(min_df=3, max_features=10000,\n                          strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n                          ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words = stop_words)\n\n    # Fit TFIDF\n    tfv.fit(list(train))\n    X = tfv.transform(train)\n\n    nmf = NMF(n_components=n_comp, random_state=1337)\n    nmf.fit(X)\n    #plog(\"nmf components/dictionary\")\n    #print(nmf.components_ )\n    #print(svd.explained_variance_ratio_)\n    X = nmf.transform(X)\n    df_nmf = pd.DataFrame(X, columns=out_cols)\n    \n    return df_nmf\n\nplog(\"NMF for train description started\")\nfor c in Columns.desc_nmf_cols.value:\n    if c in train_df.columns.values:\n        train_df.drop([c], axis=1, inplace=True)\nnmf_train_desc = nmf_d(train_df.Description, Columns.n_desc_nmf_comp.value, Columns.desc_nmf_cols.value)\nplog(\"NMF for train description ended\")\nplog(\"NMF for test description started\")\nfor c in Columns.desc_nmf_cols.value:\n    if c in test_df.columns.values:\n        test_df.drop([c], axis=1, inplace=True)\nnmf_test_desc = nmf_d(test_df.Description, Columns.n_desc_nmf_comp.value, Columns.desc_nmf_cols.value)\nplog(\"NMF for test description ended\")\n\nplog(\"NMF for train image label started\")\nfor c in Columns.iann_nmf_cols.value:\n    if c in train_df.columns.values:\n        train_df.drop([c], axis=1, inplace=True)\nnmf_train_lbldsc = nmf_d(train_df.Lbl_Dsc, Columns.n_iann_nmf_comp.value, Columns.iann_nmf_cols.value)\nplog(\"NMF for train image label ended\")\nplog(\"NMF for test image label started\")\nfor c in Columns.iann_nmf_cols.value:\n    if c in test_df.columns.values:\n        test_df.drop([c], axis=1, inplace=True)\nnmf_test_lbldsc = tfidf_2(test_df.Lbl_Dsc, Columns.n_iann_nmf_comp.value, Columns.iann_nmf_cols.value)\nplog(\"NMF for test image label ended\")\n\nplog(\"NMF for train desc entities started\")\nfor c in Columns.entities_nmf_cols.value:\n    if c in train_df.columns.values:\n        train_df.drop([c], axis=1, inplace=True)\nnmf_train_entities = nmf_d(train_df.Entities, Columns.n_entities_nmf_comp.value, Columns.entities_nmf_cols.value)\nplog(\"NMF for train desc entities ended\")\nplog(\"NMF for test desc entities started\")\nfor c in Columns.entities_nmf_cols.value:\n    if c in test_df.columns.values:\n        test_df.drop([c], axis=1, inplace=True)\nnmf_test_entities = nmf_d(test_df.Entities, Columns.n_entities_nmf_comp.value, Columns.entities_nmf_cols.value)\nplog(\"NMF for test desc entities ended\")\n\ntrain_df = pd.concat([train_df,nmf_train_desc, nmf_train_lbldsc, nmf_train_entities], axis=1)\n#train_df = pd.concat([train_df,svd_train_desc, svd_train_lbldsc], axis=1)\ntest_df = pd.concat([test_df,nmf_test_desc, nmf_test_lbldsc, nmf_test_entities], axis=1)\n#test_df = pd.concat([test_df,svd_test_desc, svd_test_lbldsc], axis=1)\n\ntrain_df.drop([\"Description\", \"Lbl_Dsc\", \"Entities\", \"Name\"], axis=1, inplace=True)\ntest_df.drop([\"Description\", \"Lbl_Dsc\", \"Entities\", \"Name\"], axis=1, inplace=True)\n\n\nfor c in Columns.entities_nmf_cols.value + Columns.iann_nmf_cols.value + Columns.desc_nmf_cols.value:\n    if c in train_df.columns.values:\n        #train_df[c].fillna(0, inplace=True)\n        pass\n    if c in test_df.columns.values:\n        #test_df[c].fillna(0, inplace=True)\n        pass\n\nprint(train_df.shape)\n#print(train_df.columns.values)\nprint(test_df.shape)\n#print(test_df.columns.values)\nprint(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74bb9ec02005c52690a17803747246d5edca7c13"},"cell_type":"code","source":"#data augmentation\n#this section needs to be refactored because of return samples of smoteenc, don't know if original dataset is included in returned samples\n\n\nif 1 == 0:\n    for c in train_df.drop([\"AdoptionSpeed\", \"PetID\", \"RescuerID\"], axis=1).columns.values.tolist():\n        if train_df[c].isna().any():\n            train_df[c].fillna(0, inplace=True)\n\n    c_features = [train_df.columns.get_loc(c) for c in train_df.columns if c in Columns.ind_num_cat_columns.value + \\\n                      [\"Age\", \"Fee\", \"VideoAmt\", \"PhotoAmt\",\"Quantity\"]]\n    \n    import random\n    import string\n    from imblearn.over_sampling import RandomOverSampler, SMOTENC\n    print(len(train_df))\n    plog(\"Generating mock data with smote\")\n    ros = SMOTENC(random_state=0, sampling_strategy='minority', categorical_features=c_features)\n    train_df_c = train_df.copy()\n    x_train_df_c = train_df_c.drop([\"AdoptionSpeed\", \"PetID\", \"RescuerID\"], axis=1)\n    y_train_df_c = train_df_c[\"AdoptionSpeed\"]\n    x_res, y_res = ros.fit_resample(x_train_df_c, y_train_df_c)\n    \n    #filter on newly generated columns\n    x_res = pd.DataFrame(data=x_res[len(x_train_df_c)+1:], columns=x_train_df_c.columns.values.tolist())\n    y_res = pd.DataFrame(data=y_res[len(y_train_df_c)+1:], columns=[\"AdoptionSpeed\"])\n    \n    plog(str(len(x_res)) + \" lines of mock data has been generated\")\n    rnd_resc_arr = []\n\n    for i in range (int(len(x_res)/4)):\n        rnd_RescuerID = ''.join([random.choice(string.ascii_letters + string.digits) for n in range(32)])\n        rnd_resc_arr.append(rnd_RescuerID)\n    x_res[\"RescuerID\"] = x_res.apply(lambda x : random.choice(rnd_resc_arr), axis=1)\n\n    rnd_petID_arr = []\n    for i in range (len(x_res)):\n        rnd_petID = ''.join([random.choice(string.ascii_letters + string.digits) for n in range(9)])\n        rnd_petID_arr.append(rnd_petID)\n    x_res[\"PetID\"] = np.asarray(rnd_petID_arr).reshape(-1,1)\n\n    train_df_smote = pd.concat([x_res, y_res], axis=1, sort=False)\n    print(len(train_df_smote))\n\n    #print(x_res.shape, y_res.reshape(-1,1).shape)\n    train_df = pd.concat([train_df, train_df_smote], axis=0 ,sort=False)\n    print(len(train_df))\nplog(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1809d520975a2f7539bc33a68430d02a809715a"},"cell_type":"code","source":"\nplog(\"Sorting breed1 and breed2 cols before concatenation\")\nbm_df_tr = pd.DataFrame(np.sort(train_df[[\"Breed1\", \"Breed2\"]], axis=1), train_df[[\"Breed1\", \"Breed2\"]].index, train_df[[\"Breed1\", \"Breed2\"]].columns)\nbm_df_te = pd.DataFrame(np.sort(test_df[[\"Breed1\", \"Breed2\"]], axis=1), test_df[[\"Breed1\", \"Breed2\"]].index, test_df[[\"Breed1\", \"Breed2\"]].columns)\nplog(\"Sorted breed1 and breed2 cols before concatenation\")\n\n\nplog(\"Merging Breed1 and Breed2 on train\")\ntrain_df[\"Breed_Merge\"] = bm_df_tr.apply(lambda x: \"_\".join([str(x['Breed1']), str(x['Breed2'])]), axis=1 )\nplog(\"Merged Breed1 and Breed2 on train\")\nplog(\"Merging Breed1 and Breed2 on test\")\ntest_df[\"Breed_Merge\"] = bm_df_te.apply(lambda x: \"_\".join([str(x['Breed1']), str(x['Breed2'])]), axis=1 )\nplog(\"Merged Breed1 and Breed2 on test\")\n\n#new feature listing purity of breed type\ndef set_pet_breed(b1, b2):\n    #print(b1, b2)\n    if (b1 in  (0, 307)) & (b2 in  (0, 307)):\n        return 4\n    elif (b1 ==  307) & (b2 not in  (0, 307)):\n        return 3\n    elif (b2 ==  307) & (b1 not in  (0, 307)):\n        return 3\n    elif (b1 not in  (0, 307)) & (b2 not in  (0, 307)) & (b1 != b2):\n        return 2\n    elif (b1 == 0) & (b2 not in  (0, 307)):\n        return 1\n    elif (b2 == 0) & (b1 not in  (0, 307)):\n        return 1\n    elif (b1 not in  (0, 307)) & (b2 not in  (0, 307)) & (b1 == b2):\n        return 0\n    else: \n        return 3\nplog(\"Setting Pet Breed for test dataset\")\ntest_df[\"Pet_Breed\"] = test_df.apply(lambda x: set_pet_breed(x['Breed1'], x['Breed2']), axis=1)                \nplog(\"Setted Pet Breed for test dataset\")\nplog(\"Setting Pet Breed for train dataset\")\ntrain_df[\"Pet_Breed\"] = train_df.apply(lambda x: set_pet_breed(x['Breed1'], x['Breed2']), axis=1)\nplog(\"Setted Pet Breed for train dataset\")\n\ndef chVal(val, col):\n    if (col == \"Health\") & (val == 0):\n        return 4\n    else:\n        return val\n\n\nplog(\"Setting Pet Breed for test dataset\")\ntest_df[\"Pet_Purity\"] = test_df.apply(lambda x: 1 if x[\"Pet_Breed\"] in (0, 1) else  0, axis=1)                \nplog(\"Setted Pet Breed for test dataset\")\nplog(\"Setting Pet Purity for train dataset\")\ntrain_df[\"Pet_Purity\"] = train_df.apply(lambda x: 1 if x[\"Pet_Breed\"] in (0, 1) else  0, axis=1)  \nplog(\"Setted Pet Purity for train dataset\")\n\nplog(\"Setting Pet Breed for test dataset\")\ntest_df[\"Overall_Status\"] = test_df.apply(lambda x: math.sqrt(x[\"Sterilized\"]) + \\\n                                                            math.sqrt(x[\"Dewormed\"]) + \\\n                                                            math.sqrt(x[\"Vaccinated\"]) + \\\n                                                            math.sqrt(chVal(x[\"Health\"], \"Health\")), axis=1)\nplog(\"Setted Pet Breed for test dataset\")\nplog(\"Setting Pet Purity for train dataset\")\ntrain_df[\"Overall_Status\"] = train_df.apply(lambda x: math.sqrt(x[\"Sterilized\"]) + \\\n                                                            math.sqrt(x[\"Dewormed\"]) + \\\n                                                            math.sqrt(x[\"Vaccinated\"]) + \\\n                                                            math.sqrt(chVal(x[\"Health\"], \"Health\")), axis=1)  \nplog(\"Setted Pet Purity for train dataset\")\n\n\n\n#print(train_df.columns.values)\n#print(test_df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e289069acbc8cf8b47fed2a5b08b547582f089f"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ndef label_encoder(train, test, col):\n    le = LabelEncoder()\n    #print(pd.unique(train[col]))\n    #print(pd.unique(test[col]))\n    val_train = pd.unique(train[col])\n    val_test = pd.unique(test[col])\n    #print(val_train)\n    #print(val_test)\n    if np.array_equal(val_train, val_test):\n        #print(1)\n        vals = val_train\n    else :\n        #print(2)\n        vals = np.unique(np.concatenate((val_train, val_test), axis=None))\n    #print(vals)\n    le.fit(vals)\n    train[col] = le.transform(train[col])\n    test[col] = le.transform(test[col])\n    return train, test\n\nif 1 == 1 :\n    \n    for c in [\"State\", \"Breed_Merge\"]:\n        plog(\"Started \" + c + \" encoding\")\n        train_df, test_df = label_encoder(train_df, test_df, c)\n        plog(\"Ended \" + c + \" encoding\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3071604d697ab53e07acbb4e5201c751d82d1b8c"},"cell_type":"code","source":"for c in Columns.barplot_cols.value:\n    if c in train_df.columns.values.tolist():\n        train_df[c].astype(\"int32\")\n    if c in test_df.columns.values.tolist():\n        test_df[c].astype(\"int32\")\n\nfor c in Columns.boxplot_cols.value:\n    if c in train_df.columns.values.tolist():\n        train_df[c].astype(\"float32\")\n    if c in test_df.columns.values.tolist():\n        test_df[c].astype(\"float32\")\n\nplog(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48ac5cdf73b28ed96133b021c28e3fafc525883d"},"cell_type":"code","source":"plog(\"Creating Fee_Per_Pet on train dataset\")\ntrain_df[\"Fee_Per_Pet\"] = train_df[\"Fee\"]/train_df[\"Quantity\"]\nplog(\"Created Fee_Per_Pet on train dataset\")\nplog(\"Creating Fee_Per_Pet on test dataset\")\ntest_df[\"Fee_Per_Pet\"] = test_df[\"Fee\"]/test_df[\"Quantity\"]\nplog(\"Created Fee_Per_Pet on test dataset\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7660b5596214575bfd4f69013b174042ae08a177"},"cell_type":"code","source":"#new feature listing purity of breed type\ndef set_pet_maturity(typ, age):\n    if typ == 1:\n        if age <= 6:\n            return 0\n        elif age <= 12:\n            return 1\n        elif age <= 3*12:\n            return 2\n        elif age <= 6*12:\n            return 3\n        elif age <=10*12:\n            return 4\n        else:\n            return 5\n    if typ == 2 :\n        if age <= 6:\n            return 0\n        elif age <= 12:\n            return 1\n        elif age <= 3*12:\n            return 2\n        elif age <= 6*12:\n            return 3\n        elif age <=10*12:\n            return 4\n        else:\n            return 5\n\nplog(\"Setting Pet_Maturity for test dataset\")\ntest_df[\"Pet_Maturity\"] = test_df.apply(lambda x: set_pet_maturity(x['Type'], x['Age']), axis=1)    \nplog(\"Setted Pet_Maturitye for test dataset\")\nplog(\"Setting Pet_Maturity for train dataset\")\ntrain_df[\"Pet_Maturity\"] = train_df.apply(lambda x: set_pet_maturity(x['Type'], x['Age']), axis=1)\nplog(\"Setted Pet_Maturity for train dataset\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"527de73ecabf303157a0e7a63d4fbcf76147de4e"},"cell_type":"code","source":"#new feature listing purity of breed type\ndef set_color_type(c1, c2, c3):\n    if c1 != 0:\n        c1 = 1\n    if c2 != 0:\n        c2 = 1\n    if c3 != 0:\n        c3 = 1\n    return c1 + c2 + c3\n\nplog(\"Setting Color Type for test dataset\")\ntest_df[\"Color_Type\"] = test_df.apply(lambda x: set_color_type(x['Color1'], x['Color2'], x['Color3']), axis=1) \nplog(\"Setted Color Type for test dataset\")\nplog(\"Setting Color Type for train dataset\")\ntrain_df[\"Color_Type\"] = train_df.apply(lambda x: set_color_type(x['Color1'], x['Color2'], x['Color3']), axis=1)\nplog(\"Setted Color Type for train dataset\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55ec2d1877d30bc8e740820605c33f3c3321965a"},"cell_type":"code","source":"#feature engineering for each categorical variable based on number of items in each category \ndef setItemCnt(val, c, agg):\n    if agg == 1:\n        if c in [\"Color1\", \"Color2\", \"Color3\"]:\n            v1, v2, v3, v4, v5, v6, v7 = 100, 200, 350, 500, 1000, 1000, 1000\n        elif c in [\"RescuerID\"]:\n            v1, v2, v3, v4, v5, v6, v7 = 1, 5, 10, 20, 50, 100, 200\n        elif c in [\"Breed1\", \"Breed2\", \"Breed_Merge\"]:\n            v1, v2, v3, v4, v5, v6, v7 = 1, 10, 50, 100, 200, 500, 1000\n        elif c in [\"Pet_Breed\"]:\n            v1, v2, v3, v4, v5, v6, v7  = 1, 10, 50, 250, 1000, 1000, 1000\n        else:\n            v1, v2, v3, v4, v5, v6, v7  = 1, 5, 10, 20, 50, 100, 200\n        if val > v7:\n            return 7\n        elif val > v6:\n            return 6\n        elif val > v5:\n            return 5\n        elif val > v4:\n            return 4\n        elif val > v3:\n            return 3\n        elif val > v2:\n            return 2\n        elif val > v1:\n            return 1\n        else:\n            return 0\n    else:\n        return val\n\ndef itemCnt(df, col):\n    train_r = df.groupby(col)[\"PetID\"].count().reset_index()\n    if col == \"RescuerID\":\n        pass#print(train_r)\n    train_r[col+\"_Cnt\"] = train_r.apply(lambda x: setItemCnt(x['PetID'], col, 0), axis=1)\n    train_r[col+\"_Cnt_MType\"] = train_r.apply(lambda x: setItemCnt(x['PetID'], col, 1), axis=1)\n    \n    #train_r[col+\"_Type\"] = train_r['PetID']\n    return train_r[[col, col+\"_Cnt\", col+\"_Cnt_MType\"]]\n\nfor c in Columns.item_cnt_cols.value:\n    if c in train_df.columns.values:\n        train_df.drop(c, axis=1, inplace=True)\n    if c in test_df.columns.values:\n        test_df.drop(c, axis=1, inplace=True)\n\nfor c in Columns.item_cnt_mtype_cols.value:\n    if c in train_df.columns.values:\n        train_df.drop(c, axis=1, inplace=True)\n    if c in test_df.columns.values:\n        test_df.drop(c, axis=1, inplace=True)\n\nfor c in Columns.item_cnt_incols.value:\n    #if c in train_df.columns.values:\n    #    plog(\"Deleting \"+c)\n    #    train_df.drop(c, axis=1, inplace=True)\n    plog(\"Creating \"+c+\"_Cnt for train on \"+ c)\n    df_itr = itemCnt(train_df, c)\n    train_df = train_df.set_index(c).join(df_itr.set_index(c)).reset_index()\n    plog(\"Created \"+c+\"_Cnt for train on \" + c)\n    plog(\"Creating \"+c+\"_Cnt for test on \"+ c)\n    df_its = itemCnt(test_df, c)\n    test_df = test_df.set_index(c).join(df_its.set_index(c)).reset_index()\n    plog(\"Created \"+c+\"_Cnt for test on \" + c)\n\ndef stdType(min, max, mean, std, value):\n    if min <= value < mean - 5 * std:\n        return 0\n    elif  mean -5*std <= value < mean - 4*std:\n        return 1\n    elif mean -4*std <= value < mean - 3*std:\n        return 2\n    elif mean - 3*std <= value < mean - 2*std:\n        return 3\n    elif mean - 2*std <= value < mean - std:\n        return 4\n    elif mean - std <= value < mean:\n        return 5\n    elif mean <= value < mean + std:\n        return 6\n    elif mean + std <= value < mean + 2*std:\n        return 7\n    elif mean + 2*std <= value < mean + 3*std:\n        return 8\n    elif mean + 3*std <= value < mean + 4*std:\n        return 9\n    elif mean + 4*std <= value < mean + 5*std:\n        return 10\n    elif mean + 5*std <= value <= max:\n        return 11\n    \ndef setItemByStdType(df, col):\n    mean = df[col].mean()\n    std = df[col].std()\n    min = df[col].min()\n    max = df[col].max()\n    df[col + \"_StdType\"] = df.apply(lambda x: stdType(min, max, mean, std, x[col]), axis=1)\n    return df\n\n\nfor c in Columns.item_type_cols.value:\n    if c in train_df.columns.values:\n        train_df.drop(c, axis=1, inplace=True)\n    if c in test_df.columns.values:\n        test_df.drop(c, axis=1, inplace=True)\n\nfor c in Columns.item_type_incols.value:\n    #if c in train_df.columns.values:\n    #    plog(\"Deleting \"+c)\n    #    train_df.drop(c, axis=1, inplace=True)\n    plog(\"Creating \"+c+\"_StdType for train on \"+ c)\n    train_df = setItemByStdType(train_df, c)\n    plog(\"Created \"+c+\"_StdType for train on \" + c)\n    plog(\"Creating \"+c+\"_StdType for test on \"+ c)\n    test_df = setItemByStdType(test_df, c)\n    plog(\"Created \"+c+\"_StdType for test on \" + c)\n    \n#print(train_df.columns.values)\n#print(test_df.columns.values)\n\nplog(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6b9bacf3708ac80e203d5d8f709f9d19699458d"},"cell_type":"code","source":"\ndef set_mean_fee(df, col):\n    dfm = df[[col, \"Fee\", \"Quantity\"]].groupby(col).agg({\"Fee\":\"sum\", \"Quantity\":\"sum\"}).reset_index()\n    dfm[\"Fee_\" + c] = dfm[\"Fee\"] / dfm[\"Quantity\"]\n    dfm.drop([\"Quantity\",\"Fee\"], axis = 1, inplace=True)\n    df = df.set_index(col).join(dfm.set_index(col)).reset_index()\n    return df\n\nfor c in Columns.fee_mean_incols.value:\n    if \"Fee_\" + c in train_df.columns.values.tolist():\n        plog(\"Dropping \"+\"Fee_Per_Pet_\" + c + \" on train dataset\")\n        train_df.drop(\"Fee_Per_Pet_\" + c, axis=1, inplace=True)\n    plog(\"Creating Fee_Per_Pet_\" + c + \" on train dataset\")\n    train_df = set_mean_fee(train_df, c)\n    plog(\"Created Fee_Per_Pet_\" + c +  \" on train dataset\")\n    if \"Fee_\" + c in test_df.columns.values.tolist():\n        plog(\"Dropping \"+\"Fee_Per_Pet_\" + c + \" on test dataset\")\n        test_df.drop(\"Fee_Per_Pet_\" + c, axis=1, inplace=True)\n    plog(\"Creating Fee_Per_Pet_\" + c + \" on test dataset\")\n    test_df = set_mean_fee(test_df, c)\n    plog(\"Created Fee_Per_Pet_\" + c +  \" on test dataset\")\n\n#print(train_df.columns.values)\n#print(test_df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import KBinsDiscretizer\n\nfor c in Columns.kbin_cols.value:\n    if c in train_df.columns.values.tolist():\n        train_df.drop(c, axis=1, inplace = True)\n    if c in test_df.columns.values.tolist():\n        test_df.drop(c, axis=1, inplace = True)\n\nfor c in Columns.kbin_incols.value:\n    if c in train_df.columns.values.tolist():\n        kbin_est = KBinsDiscretizer(n_bins=12, encode='ordinal', strategy='quantile')\n        kbin_est.fit(train_df[c].values.reshape(-1, 1))  \n        kbin_train = kbin_est.transform(train_df[c].values.reshape(-1, 1))\n        pd_kbin_train = pd.DataFrame(data=kbin_train, columns=[c+\"_Kbin\"])\n        train_df = pd.concat([train_df,pd_kbin_train], axis=1)\n        kbin_test = kbin_est.transform(test_df[c].values.reshape(-1, 1))\n        pd_kbin_test = pd.DataFrame(data=kbin_test, columns=[c+\"_Kbin\"])\n        test_df = pd.concat([test_df,pd_kbin_test], axis=1)\nplog(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fe93c9bce5c1496d74fe12964022803604c8cd3"},"cell_type":"code","source":"#feature engineering for each categorical variable based on number mean adoption speed\ndef setItemAdp(val):\n    if val > 3.5:\n        return 4\n    elif val > 2.5:\n        return 3\n    elif val > 1.5:\n        return 2\n    elif val > 0.5:\n        return 1\n    else:\n        return 0\n\ndef itemAdp(df, col):\n    train_r = df.groupby(col)[\"AdoptionSpeed\"].mean().reset_index()\n    train_r[col+\"_Adp\"] = train_r.apply(lambda x: setItemAdp(x['AdoptionSpeed']), axis=1)\n    #train_r[col+\"_Adp\"] = train_r['AdoptionSpeed']\n    return train_r[[col, col+\"_Adp\"]]\n\nif 1 == 1:\n    for c in Columns.item_adp_incols.value:\n        if c+\"_Adp\" in train_df.columns.values:\n            plog(\"Deleting train \"+c+\"_Adp\")\n            train_df.drop([c+\"_Adp\"], axis=1, inplace=True)\n        plog(\"Creating \"+c+\"_Adp for train on \"+ c)\n        df_itr = itemAdp(train_df, c)\n        train_df = train_df.set_index(c).join(df_itr.set_index(c)).reset_index()\n        plog(\"Created \"+c+\"_Adp for train on \" + c)\n        if c+\"_Adp\" in test_df.columns.values:\n            plog(\"Deleting test \"+c+\"_Adp\")\n            test_df.drop([c+\"_Adp\"], axis=1, inplace=True)\n        plog(\"Creating \"+c+\"_Adp for test on \"+ c)\n        #df_its = itemAdp(test_df, c)\n        #print(df_itr)\n        test_df = test_df.set_index(c).join(df_itr.set_index(c)).reset_index()\n        import math\n        #test_df[c+\"_Adp\"].fillna(test_df[c+\"_Adp\"].mean(), inplace=True)\n        plog(\"Created \"+c+\"_Adp for test on \" + c)\n\n    #print(train_df.columns.values)\n    #print(train_df.head())\n    #print(test_df.columns.values)\n    #print(test_df.head())\n\nplog(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92fcf022e4ef0e5c5ce6b89874609b93088b5894"},"cell_type":"code","source":"#drop rescuer id as no more needed\ntrain_df.drop([\"RescuerID\"], axis=1, inplace=True)\ntest_df.drop([\"RescuerID\"], axis=1, inplace=True)\nplog(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9afbcb0be34bda9ef43189480437c2dafbd543aa"},"cell_type":"code","source":"#featureengineering using featuretools\n\nimport featuretools as ft\nprint(train_df.columns.values)\nprint(\"------------------------------------\")\nprint(test_df.columns.values)\nprint(\"------------------------------------\")\ndef auto_features(df, cols, entities):\n    df_c = df[cols]\n    es = ft.EntitySet(id='petfinder')\n    es.entity_from_dataframe(entity_id=\"Pets\", dataframe=df_c, index=\"PetID\")\n    ignored_variable =  {}\n    ignored_variable.update({'Pets': entities})\n    for e in entities:\n        plog(e)\n\n        es.normalize_entity(base_entity_id='Pets', new_entity_id=e, index=e)\n        feature_matrix, feature_names = ft.dfs(entityset=es,\n                                               target_entity=e,\n                                               max_depth=2,\n                                               verbose=1,\n                                               #n_jobs=3,\n                                               ignore_variables=ignored_variable,\n                                               agg_primitives =Columns.agg_calc.value)\n        fm = feature_matrix.add_prefix(e+\"_\")\n        #print(fm.head())\n        #print(\"--------\", e)\n        #print(df[e].head())\n        df = df.set_index(e).join(fm).reset_index()\n\n    return df\n\nif 1 == 0:\n    plog(\"creating new features on train using featuretools\")\n    for c in Columns.ft_cols.value:\n        if c in train_df.columns.values:\n            train_df.drop([c], axis=1, inplace=True)\n    train_df = auto_features(train_df, \n                             Columns.iden_columns.value + Columns.ft_cat_cols.value + Columns.ft_new_cols.value, \n                             Columns.ft_cat_cols.value)\n    plog(\"created new features on train using featuretools\")\n    for c in Columns.ft_cols.value:\n        if c in test_df.columns.values:\n            test_df.drop([c], axis=1, inplace=True)\n    plog(\"creating new features on test using featuretools\")\n    test_df = auto_features(test_df, \n                            Columns.iden_columns.value + Columns.ft_cat_cols.value + Columns.ft_new_cols.value, \n                            Columns.ft_cat_cols.value)\n    plog(\"created new features on test using featuretools\")\n\n    print(train_df.shape)\n    #print(train_df.columns.values)\n    print(test_df.shape)\n    #print(test_df.columns.values)\nplog(\"Done\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aba2de41ddc28b1ec005364b4b8c9b57f0681acd"},"cell_type":"code","source":"\ndef auto_adp_features(train, test, cols, entities):\n\n    df_c = train[cols]\n    es = ft.EntitySet(id='petfinder')\n    es.entity_from_dataframe(entity_id=\"Pets\", dataframe=df_c, index=\"PetID\")\n    ignored_variable =  {}\n    ignored_variable.update({'Pets': entities})\n    for e in entities:\n        plog(e)\n\n        es.normalize_entity(base_entity_id='Pets', new_entity_id=e, index=e)\n        feature_matrix, feature_names = ft.dfs(entityset=es,\n                                               target_entity=e,\n                                               max_depth=2,\n                                               verbose=1,\n                                               #n_jobs=3,\n                                               ignore_variables=ignored_variable)\n        fm = feature_matrix.add_prefix(e+\"_\")\n        #print(feature_names)\n        fm.drop([e+\"_COUNT(Pets)\"], axis = 1, inplace=True)\n        train = train.set_index(e).join(fm).reset_index()\n        test = test.set_index(e).join(fm).reset_index()\n\n    return train, test\n\nif 1 == 0:\n    plog(\"creating adoptionspeed based new features on train and test using featuretools\")\n    train_df, test_df = auto_adp_features(train_df, test_df, \n                                          Columns.iden_columns.value + Columns.ft_cat_cols.value + Columns.item_cnt_mtype_cols.value +\n                                          Columns.item_cnt_cols.value + Columns.item_type_cols.value + [\"AdoptionSpeed\"], \n                                          Columns.item_cnt_mtype_cols.value + Columns.item_cnt_cols.value + Columns.item_type_cols.value + Columns.ft_cat_cols.value)\n    plog(\"Created adoptionspeed based new features on train and test using featuretools\")\n\n    print(train_df.shape)\n    #print(train_df.columns.values)\n    print(test_df.shape)\n    #print(test_df.columns.values)\nplog(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d2c6619a144e48552392d6edca5a2d7b2c30dc5"},"cell_type":"code","source":"#rermoving features with low variances\nif 1 == 0:\n    from sklearn.feature_selection import VarianceThreshold\n    if \"PetID\" in train_df.columns.values:\n        train_pet_id = train_df[\"PetID\"]\n        train_adoption_speed = train_df[\"AdoptionSpeed\"]\n        train_df.drop([\"PetID\", \"AdoptionSpeed\"], axis=1, inplace=True)\n\n    if \"PetID\" in test_df.columns.values:\n        test_pet_id = test_df[\"PetID\"]\n        test_df.drop([\"PetID\"], axis=1, inplace=True)\n\n    def filter_by_varth(train, test, threshold):\n        vt = VarianceThreshold(threshold=threshold)\n        vt.fit(train)\n        indices = vt.get_support(indices=True)\n        return(train.iloc[:,indices], test.iloc[:,indices])\n\n    for c in train_df.columns.values:\n        if train_df[c].isna().any():\n            print(\"null\", c)\n            #train_df[c].fillna(0, inplace=True)\n\n    plog(\"lists similarity check before from removal by variance\")\n    print(train_df.columns.values == train_df.columns.values)\n\n    plog(\"train - test diff\")\n    print(list(set(train_df.columns.values.tolist()) - set(test_df.columns.values.tolist())))\n    #print(train_df.columns.values.tolist())\n    plog(\"test - train diff\")\n    list(set(test_df.columns.values.tolist()) - set(train_df.columns.values.tolist()))\n    #print(test_df.columns.values.tolist())\n\n    train_t, test_t = filter_by_varth(train_df, test_df, 0.20)\n    plog(\"lists similarity check after from removal by variance\")\n    print(train_t.columns.values == test_t.columns.values)\n    print(train_t.shape)\n    print(test_t.shape)\n\n    train_df = pd.concat([train_t, train_pet_id, train_adoption_speed], axis=1)\n    test_df = pd.concat([test_t, test_pet_id], axis=1)\n    print(train_df.shape)\n    #print(train_df.columns.values)\n    print(test_df.shape)\n    #print(test_df.columns.values)\nplog(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"705cee7ec6dd484b83539b0b93a92d83244c2a54"},"cell_type":"code","source":"def group_x_by_y(df, x, y, type):\n    df_g = df.groupby(y).agg({x:type})\n    df_g.rename({x: x+\"_\"+type+\"_by_\"+y}, axis='columns', inplace=True)\n    df = df.set_index(y).join(df_g).reset_index()\n    return df\n\nplog(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7845e4741aab56d25ffcabcaf90caaebb13f05ef"},"cell_type":"code","source":"#check skewness of variable and make a logtransform\n\nfrom scipy.stats import skewtest, normaltest\nfrom sklearn.preprocessing import QuantileTransformer\n\nrng = np.random.RandomState(304)\nqt = QuantileTransformer(output_distribution='normal', random_state=rng)\nif 1 == 0:\n    for c in Columns.scaling_cols.value:\n        if c in train_df.columns.values.tolist():\n            p_val = normaltest(train_df[c])[1]\n\n            if p_val < 0.05:\n                plog(\"Transforming \"+ c + \" on train and test datasets with pval of normaltest:\" + str(p_val))\n                qt.fit(x_train[c].reshape(-1, 1)) \n                qt.transform(x_train[c].reshape(-1, 1))\n                qt.transform(x_test[c].reshape(-1, 1))\n            else:\n                plog(\"Not transforming \"+ c + \" on train and test datasets with pval of normaltest:\" + str(p_val))\n\nplog(\"Done\")\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12d5c86260eb76c467d48af6595328850f04a9fa"},"cell_type":"code","source":"if 1 == 0:\n    for c in Columns.scaling_cols.value:\n        if c in train_df.columns.values.tolist():\n            p_val = normaltest(train_df[c])[1]\n\n            if p_val < 0.05:\n                plog(\"Transforming \"+ c + \" on train and test datasets with pval of normaltest:\" + str(p_val))\n                #qt.fit(x_train[c].reshape(-1, 1)) \n                train_df[c] = np.log(train_df[c]+1)#qt.transform(x_train[c].reshape(-1, 1))\n                test_df[c] = np.log(test_df[c]+1)#qt.transform(x_test[c].reshape(-1, 1))\n            else:\n                plog(\"Not transforming \"+ c + \" on train and test datasets with pval of normaltest:\" + str(p_val))\n\nplog(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eefac17cd27e6b13511884eb80e650695ebfae45"},"cell_type":"code","source":"def normalize(train, test, col):\n    mean = train[col].mean(axis=0)\n    std = train[col].std(axis=0)\n    train[col] = (train[col]-mean)/std\n    test[col] = (test[col]-mean)/std\n    return train, test\n\nif 1 == 0:\n    for c in Columns.scaling_cols.value:\n        if c in train_df.columns.values.tolist():\n            plog(\"Starting normalizing for \" + c)\n            train_df, test_df = scale_num_var(train_df, test_df, c)\n            #print(train_df[col].head())\n            #print(est_df[col].head())\n            plog(\"Normalizing Ended for \" + c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5def7d53dbcfcac9bc2db0bc0ddd914414cfdb28"},"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\n\nif 1 == 0:\n    scaler = RobustScaler()\n    plog(\"Starting RobustScaler for train\")\n    scaler.fit(train_df[Columns.scaling_cols.value])\n    train_df[Columns.scaling_cols.value] = scaler.transform(train_df[Columns.scaling_cols.value])\n    plog(\"Ended RobustScaler for train\")\n    \n    scaler = RobustScaler()\n    plog(\"Starting RobustScaler for test\")\n    test_df[Columns.scaling_cols.value] = scaler.transform(test_df[Columns.scaling_cols.value])\n    plog(\"Ended RobustScaler for test\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df485fe89b69ea4dbce68b29c4f13791a587f8f6"},"cell_type":"code","source":"#outlier detection\nfrom sklearn import svm\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom collections import Counter\ndef detect_outliers(f_train):\n    #id = train[\"PetID\"]\n    #f_train = train.drop([\"PetID\"], axis=1)\n    n_samples = len(f_train)\n    outliers_fraction = 0.10\n    n_outliers = int(outliers_fraction * n_samples)\n    n_inliers = n_samples - n_outliers\n\n    # define outlier/anomaly detection methods to be compared\n    anomaly_algorithms = [\n        # (\"Robust covariance\", EllipticEnvelope(contamination=outliers_fraction)),\n        (\"One-Class SVM\", svm.OneClassSVM(nu=outliers_fraction, kernel=\"rbf\",\n                                          gamma=0.1)),\n        (\"Isolation Forest\", IsolationForest(behaviour='new',\n                                             contamination=outliers_fraction,\n                                             random_state=42)),\n        (\"Local Outlier Factor\", LocalOutlierFactor(\n            n_neighbors=35, contamination=outliers_fraction))]\n\n    # Define datasets\n    #print(f_train.head())\n    df_pred = pd.DataFrame(columns=[\"One-Class SVM\",\"Isolation Forest\",\"Local Outlier Factor\"])\n    if 1 == 1 :\n        for name, algorithm in anomaly_algorithms:\n            #t0 = time.time()\n            # algorithm.fit(f_train)\n            plog(name)\n            #t1 = time.time()\n\n            # fit the data and tag outliers\n            if name == \"Local Outlier Factor\":\n                y_pred = algorithm.fit_predict(f_train)\n            else:\n                y_pred = algorithm.fit(f_train).predict(f_train)\n\n            df_pred[name] = y_pred\n            #print(df_pred)\n\n    df_pred[\"outlier\"] = df_pred.apply(lambda x: Counter([x['One-Class SVM'], x['Isolation Forest'], x[\"Local Outlier Factor\"]]).most_common(1)[0][0], axis=1)\n    #print(df_pred.shape)\n    #prediction_df = pd.concat([id,df_pred], axis=1)\n\n    # create submission file print(prediction_df)\n    #df_pred.to_csv(\"outliers.csv\")\n    return df_pred[\"outlier\"]\n\n#print(train_df.head())\no_cols = train_df.columns.values.tolist().copy()\n\nlow_imp_features = []\n\nfor c in [\"PetID\", \"RescuerID\"]+low_imp_features: #Columns.ind_num_cat_columns.value + [\"AdoptionSpeed\",\"PetID\"] + low_imp_features :\n     if c in o_cols:\n        o_cols.remove(c)\n#print(train_df[o_cols].shape)\n\nfor c in o_cols:\n    if train_df[c].isna().any():\n        plog(\"train null value exist for column \" + c)\n        train_df[c].fillna(0, inplace=True)#train_df[c].mean(), inplace=True)\n    if c != \"AdoptionSpeed\":\n        if test_df[c].isna().any():\n            plog(\"test null value exist for column \" + c)\n            test_df[c].fillna(0, inplace=True)#test_df[c].mean(), inplace=True)\n\n#train_df[o_cols] = train_df[o_cols].apply(lambda x: x.fillna(0),axis=0)\n#print(train_df[o_cols].shape)\n#test_df[o_cols] = test_df[o_cols].apply(lambda x: x.fillna(0),axis=0)\n\n#train_df[col].fillna(train_df[col].mean(), inplace=True)\n#test_df[col].fillna(test_df[col].mean(), inplace=True)\n    \nprint(o_cols)\n\nplog(\"Outlier detection started\")\n\ncall=1\nif call ==1:\n    df_o = detect_outliers(train_df[o_cols])\n    print(\"df_o\", df_o.shape, \"train_df\", train_df.shape)\n    train_df = pd.concat([train_df, df_o], axis=1)\nelse:\n    plog(\"For Dogs\")\n    train_df_dogs=train_df[train_df[\"Type\"]==1].reset_index().drop([\"index\"] ,axis=1)\n    df_od = detect_outliers(train_df_dogs[o_cols+[\"AdoptionSpeed\"]])\n    print(\"df_od\", df_od.shape, \"train_df_dogs\", train_df_dogs.shape)\n    train_df_dogs = pd.concat([train_df_dogs,df_od], axis=1)\n    print(\"train_df_dogs\", train_df_dogs.shape)\n    plog(\"For Cats\")\n    train_df_cats=train_df[train_df[\"Type\"]==2].reset_index().drop([\"index\"] ,axis=1)\n    df_oc = detect_outliers(train_df_cats[o_cols+[\"AdoptionSpeed\"]])\n    print(\"df_oc\", df_oc.shape, \"train_df_cats\", train_df_cats.shape)\n    train_df_cats = pd.concat([train_df_cats, df_oc], axis=1)\n    print(\"train_df_cats\", train_df_cats.shape)\n    plog(\"Concatenating dogs and cats datasets\")\n    train_df=pd.concat([train_df_dogs,train_df_cats])\nplog(\"Outlier detection ended\")\n#train_df = train_df[train_df[\"outlier\"]==1]\n#train_df[\"outlier\"].fillna(1, inplace=True)\nprint(\"train_df\", train_df.shape)\n#print(train_df.columns.values)\nprint(\"test_df\", test_df.shape)\n#print(test_df.columns.values)\nplog(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9d02b8bbdaefd6327ca84d76f51063e2a40af74"},"cell_type":"code","source":"#preparing final datasets by excluding outliers\ndef prepare_data(train_df, test_df):\n    \n    #train_x = train_df[Columns.ind_cont_columns.value + Columns.ind_num_cat_columns.value \n    #                  + Columns.desc_svd_cols.value + Columns.img_num_cols_1.value \n    #                  + Columns.img_num_cols_2.value + Columns.img_num_cols_3.value + Columns.iann_svd_cols.value \n    #                  + Columns.ft_cols.value +  Columns.item_cnt_cols.value]\n    train_x = train_df.drop([\"AdoptionSpeed\", \"outlier\", \"PetID\"], axis=1)\n    train_y = train_df[Columns.dep_columns.value]\n    #test_x = test_df[Columns.ind_cont_columns.value + Columns.ind_num_cat_columns.value \n    #                  + Columns.desc_svd_cols.value + Columns.img_num_cols_1.value \n    #                  + Columns.img_num_cols_2.value + Columns.img_num_cols_3.value + Columns.iann_svd_cols.value\n    #                  + Columns.ft_cols.value + Columns.item_cnt_cols.value]\n    test_x = test_df.drop([\"PetID\"], axis=1)\n    test_id = test_df[Columns.iden_columns.value]\n    \n    return train_x, train_y, test_x, test_id\n\n# -1 means with outliers 1 means without outliers\noutlier = 1\n#test without columns having text values (name and description) and rescuer_id\nx_train, y_train, x_test, id_test = prepare_data(train_df[train_df[\"outlier\"]>=outlier], test_df)\nx_train_dogs, y_train_dogs, x_test_dogs, id_test_dogs = prepare_data(train_df[(train_df[\"Type\"]==1)&(train_df[\"outlier\"]>=outlier)], \n                                                                     test_df[test_df[\"Type\"]==1])\n\nx_train_cats, y_train_cats, x_test_cats, id_test_cats = prepare_data(train_df[(train_df[\"Type\"]==2)&(train_df[\"outlier\"]>=outlier)], \n                                                                     test_df[test_df[\"Type\"]==2])\n\n#tr_idx_dogs = x_train.index[x_train[\"Type\"] == 1].tolist()\n#tr_idx_cats = x_train.index[x_train[\"Type\"] == 2].tolist()\n#x_train_dogs = x_train.iloc[tr_idx_dogs].reset_index()\n#y_train_dogs = y_train.iloc[tr_idx_dogs].reset_index()\n#x_train_cats = x_train.iloc[tr_idx_cats].reset_index()\n#y_train_cats = y_train.iloc[tr_idx_cats].reset_index()\n\n#te_idx_dogs = x_test.index[x_test[\"Type\"] == 1].tolist()\n#te_idx_cats = x_test.index[x_test[\"Type\"] == 2].tolist()\n#x_test_dogs = x_test.iloc[te_idx_dogs].reset_index()\n#id_test_dogs = id_test.iloc[te_idx_dogs].reset_index()\n#x_test_cats = x_test.iloc[te_idx_cats].reset_index()\n#id_test_cats = id_test.iloc[te_idx_cats].reset_index()\nplog(\"x_train information\")\nprint(x_train.shape)\nprint(x_train.columns.values)\nplog(\"y_train information\")\nprint(y_train.shape)\nprint(y_train.columns.values)\nplog(\"x_test information\")\nprint(x_test.shape)\nprint(x_test.columns.values)\nplog(\"id_test information\")\nprint(id_test.shape)\nprint(id_test.columns.values)\npd.concat([x_train, y_train], axis=1).to_csv(\"train.csv\", index=False)\npd.concat([id_test, x_test], axis=1).to_csv(\"test.csv\", index=False)\n\nplog(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c9f58085dd9b436ba90aaafce0c00c737ee1339"},"cell_type":"code","source":"if 1 == 0:\n    print(x_train.head())\n    print(\"---------------------\")\n    print(x_test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a37f697cc8ad7e1c320743e8bb13d1b5b7f17629"},"cell_type":"code","source":"import matplotlib as mpl\n\ndef check_cat_perc(arr, cols):\n    for col in cols:\n        if col != \"RescuerID\":\n            fig, axes = plt.subplots(2, 1, figsize=[16, 16])\n            # axes[0,0].set_title(\"Perc. of \" + col + \" dist.\")\n            # axes[0,1].set_title(\"Perc. of \" + col + \" dist. by AdopSp.\")\n            # axes[1,0].set_title(\"Perc. of \" + col + \" dist. by AdopSp. for Dogs\")\n            # axes[1,1].set_title(\"Perc. of \" + col + \" dist. by AdopSp. for Cats\")\n            df1 = arr[col].value_counts(normalize=True).rename(\"percentage\").mul(100).reset_index()  # .sort_values(col)\n            df1.rename(columns={\"index\": col}, inplace=True)\n            ax1 = sns.catplot(x=col, y=\"percentage\", data=df1, kind=\"bar\", ax=axes[0])\n            ax1.set_axis_labels(\"All \"+col, \"Percentage\")\n            ax1.set_xticklabels(rotation=90)\n            df2 = arr.groupby([col])[\"AdoptionSpeed\"].value_counts(normalize=True).rename('percentage').mul(100).reset_index()\n            ax2 = sns.catplot(x=col, y=\"percentage\", data=df2, hue=\"AdoptionSpeed\", kind=\"bar\", ax=axes[1])\n            ax2.set_axis_labels(\"All \"+col, \"Percentage\")\n            ax2.set_xticklabels(rotation=90)\n            plt.close(2)\n            plt.close(3)\n            plt.show()\nif 1 == 0:\n    cols = []\n\n    for c in Columns.barplot_cols.value:\n        if c in x_train.columns.values.tolist():\n            cols.append(c)\n\n    check_cat_perc(pd.concat([x_train, y_train], axis=1, sort=False), cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdd9a69e6db4bd8c6378421bf1c19304eb41ef6d"},"cell_type":"code","source":"def check_num_dist(arr, cols):\n    for col in cols:\n        print(col)\n        mpl.rcParams['axes.labelsize'] = 10\n        mpl.rcParams['xtick.labelsize'] = 10\n        mpl.rcParams['ytick.labelsize'] = 10\n        f, axes = plt.subplots(3, 2, figsize=[25, 25])\n        f.suptitle(col + ' histogram')\n        sns.distplot(arr[col], ax=axes[0, 0], axlabel=\"All\")\n        sns.distplot(arr[arr[\"AdoptionSpeed\"] == 0][col], ax=axes[0, 1], axlabel=\"All for Adoption Speed 0\")\n        sns.distplot(arr[arr[\"AdoptionSpeed\"] == 1][col], ax=axes[1, 0], axlabel=\"All for Adoption Speed 1\")\n        sns.distplot(arr[arr[\"AdoptionSpeed\"] == 2][col], ax=axes[1, 1], axlabel=\"All for Adoption Speed 2\")\n        sns.distplot(arr[arr[\"AdoptionSpeed\"] == 3][col], ax=axes[2, 0], axlabel=\"All for Adoption Speed 3\")\n        sns.distplot(arr[arr[\"AdoptionSpeed\"] == 4][col], ax=axes[2, 1], axlabel=\"All for Adoption Speed 4\")\n\n        for i in range(2, 6):\n            plt.close(i)\n\n        plt.show()\n\nif 1 == 0:\n    #print(Columns.ind_cont_columns.value)\n    cols = []\n\n    for c in Columns.barplot_cols.value + Columns.boxplot_cols.value:\n        if c in x_train.columns.values.tolist():\n            cols.append(c)\n\n    check_num_dist(pd.concat([x_train, y_train], axis = 1), cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c4c576cffc16c8c92b6a8f968a85c54bf3f7e2c"},"cell_type":"code","source":"from pylab import rcParams\ndef num_boxp(arr, cols):\n    #print(arr.describe())\n    for col in cols:\n        print(col)\n        #mpl.rcParams['axes.labelsize'] = 10\n        #mpl.rcParams['xtick.labelsize'] = 10\n        #mpl.rcParams['ytick.labelsize'] = 10\n         #f, axes = plt.subplots(1, 3, figsize=[20, 9])\n         #f.suptitle(col +' by AdopSp.')\n         #ax1 = sns.boxplot(y=col, x=\"AdoptionSpeed\", data=arr, ax=axes[0])\n         #ax2 = sns.boxplot(y=col, x=\"AdoptionSpeed\", data=arr[arr[\"Type\"] == 1], ax=axes[1])\n         #ax3 = sns.boxplot(y=col, x=\"AdoptionSpeed\", data=arr[arr[\"Type\"] == 2], ax=axes[2])\n         #axes[0].set_title(\"For All\")\n         #axes[1].set_title(\"For Dogs\")\n         #axes[2].set_title(\"For Cats\")\n        a_0 = arr[arr[\"AdoptionSpeed\"] == 0][col]\n        a_1 = arr[arr[\"AdoptionSpeed\"] == 1][col]\n        a_2 = arr[arr[\"AdoptionSpeed\"] == 2][col]\n        a_3 = arr[arr[\"AdoptionSpeed\"] == 3][col]\n        a_4 = arr[arr[\"AdoptionSpeed\"] == 4][col]\n        d_0 = arr[(arr[\"Type\"] == 1) & (arr[\"AdoptionSpeed\"] == 0)][col]\n        d_1 = arr[(arr[\"Type\"] == 1) & (arr[\"AdoptionSpeed\"] == 1)][col]\n        d_2 = arr[(arr[\"Type\"] == 1) & (arr[\"AdoptionSpeed\"] == 2)][col]\n        d_3 = arr[(arr[\"Type\"] == 1) & (arr[\"AdoptionSpeed\"] == 3)][col]\n        d_4 = arr[(arr[\"Type\"] == 1) & (arr[\"AdoptionSpeed\"] == 4)][col]\n        c_0 = arr[(arr[\"Type\"] == 2) & (arr[\"AdoptionSpeed\"] == 0)][col]\n        c_1 = arr[(arr[\"Type\"] == 2) & (arr[\"AdoptionSpeed\"] == 1)][col]\n        c_2 = arr[(arr[\"Type\"] == 2) & (arr[\"AdoptionSpeed\"] == 2)][col]\n        c_3 = arr[(arr[\"Type\"] == 2) & (arr[\"AdoptionSpeed\"] == 3)][col]\n        c_4 = arr[(arr[\"Type\"] == 2) & (arr[\"AdoptionSpeed\"] == 4)][col]\n        #data = [arr[col], arr[arr[\"Type\"] == 1][col], arr[arr[\"Type\"] == 2][col]]\n        data = [a_0, a_1, a_2, a_3, a_4, d_0, d_1, d_2, d_3, d_4, c_0, c_1, d_2, c_3, c_4]\n        rcParams['figure.figsize'] = [20, 10]\n        fig7, ax7 = plt.subplots()\n        \n        ax7.set_title('Boxplot of ' + col + ' of All and by Type')\n        #sns.boxplot(hue=\"AdoptionSpeed\", y=col, x=\"Type\", data=arr )\n        ax7.boxplot(data)\n        \n        plt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], \n                   ['All 0', 'All 1', 'All 2', 'All 3', 'All 4', 'Dogs 0', 'Dogs 1', 'Dogs 2', 'Dogs 3', 'Dogs 4', \n                    'Cats 0', 'Cats 1', 'Cats 2', 'Cats 3', 'Cats 4'])\n        #for i in range(2,5):\n        #    plt.close(i)\n        plt.show()\nif 1 == 0:\n    cols = []\n\n    for c in Columns.boxplot_cols.value:\n        if c in x_train.columns.values.tolist():\n            cols.append(c)\n\n    num_boxp(pd.concat([x_train, y_train], axis = 1), cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d0e23241c501a06489848c43cc5092cea3d2a7a"},"cell_type":"code","source":"# This function tests the dependence between two categorical variables\nfrom collections import Counter\nimport math\nfrom scipy.stats import entropy\n\ndef conditional_entropy(x,y):\n    # for categorical correlation\n    # entropy of x given y\n    y_counter = Counter(y)\n    xy_counter = Counter(list(zip(x,y)))\n    total_occurrences = sum(y_counter.values())\n    entropy = 0\n    for xy in xy_counter.keys():\n        p_xy = xy_counter[xy] / total_occurrences\n        p_y = y_counter[xy[1]] / total_occurrences\n        entropy += p_xy * math.log(p_y/p_xy)\n    return entropy\n\ndef theils_u(x, y):\n    # for categorical correlation\n    s_xy = conditional_entropy(x,y)\n    x_counter = Counter(x)\n    total_occurrences = sum(x_counter.values())\n    p_x = list(map(lambda n: n/total_occurrences, x_counter.values()))\n    s_x = entropy(p_x)\n    if s_x == 0:\n        return 1\n    else:\n        return (s_x - s_xy) / s_x\n\n\ndef by_theilsu(train,cols):\n    # for categorical correlation\n    theilu = pd.DataFrame(index=cols, columns=cols)\n    i = 0\n    for c1 in cols:\n        for c2 in cols:\n            #u = theils_u(train[c1].tolist(), train[c2].tolist())\n            u = theils_u(train[c1], train[c2])\n            theilu.loc[c1, c2] = u\n        i = i+1\n    theilu.fillna(value=np.nan, inplace=True)\n    #plt.figure(figsize=(20, 1))\n    return theilu\n\n#print(Columns.ind_num_cat_columns.value)\ncols = []\n            \nfor c in Columns.ind_num_cat_columns.value:\n    if c in x_train.columns.values.tolist():\n        cols.append(c)\ndf_a = pd.concat([x_train[cols],y_train],axis=1)\n#print(df_a.describe(include=\"all\"))\nprint(\"Done\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ce7ed4ba5a49cbe142df748d4e299b22c3fe596"},"cell_type":"code","source":"if 1 == 0:\n    print(\"Effect of x on y by theils, ex: Effect of Breed1 on Type is 0.99\")\n    theils_a = by_theilsu(df_a,cols+ Columns.dep_columns.value)\n    theilu_d = by_theilsu(df_a[df_a[\"Type\"]==1],cols+Columns.dep_columns.value)\n    theilu_c = by_theilsu(df_a[df_a[\"Type\"]==2],cols+Columns.dep_columns.value)\n    theils_all = pd.concat([theils_a.loc[\"AdoptionSpeed\",:], theilu_d.loc[\"AdoptionSpeed\",:], theilu_c.loc[\"AdoptionSpeed\",:]], axis=1)\n    #print(theils_all)\n    theils_all = pd.DataFrame(index=([\"All\", \"Dogs\", \"Cats\"]), data=(theils_all.values.T), columns=theils_a.columns.values)\n    plt.rcParams[\"figure.figsize\"] = [20,5]\n    ax = plt.axes()\n    sns.heatmap(theils_all, annot=True, fmt='.4f', ax=ax)\n    ax.set_title(\"Entrophy based categorical to categorical dependent correlation for all types\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ff52b2fcaeca964b9a4d9df82cd79f11d372b49"},"cell_type":"code","source":"if 1 == 0:\n    print(\"Effect of x on y by theils, ex: Effect of Breed1 on Type is 0.99\")\n    theils_a = by_theilsu(df_a,cols+ Columns.dep_columns.value)\n    plt.rcParams[\"figure.figsize\"] = [14,14]\n    ax = plt.axes()\n    sns.heatmap(theils_a, annot=True, fmt='.2f', ax=ax)\n    ax.set_title(\"Entrophy based categorical to categorical dependent correlation for all types\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04725e84575f504baf2cc5776c0f4e4ad2d626b8"},"cell_type":"code","source":"if 1 == 0:\n    theilu_d = by_theilsu(df_a[df_a[\"Type\"]==1],cols+Columns.dep_columns.value)\n    plt.rcParams[\"figure.figsize\"] = [14,14]\n    ax = plt.axes()\n    sns.heatmap(theilu_d, annot=True, fmt='.2f', ax=ax)\n    ax.set_title(\"Entrophy based categorical independent to categorical dependent correlation for dogs\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb00bea9284cacfaf7d6104a40a8234ebd68eed6"},"cell_type":"code","source":"if 1 == 0:\n    theilu_c = by_theilsu(df_a[df_a[\"Type\"]==2],cols+Columns.dep_columns.value)\n    plt.rcParams[\"figure.figsize\"] = [14,14]\n    ax = plt.axes()\n    sns.heatmap(theilu_c, annot=True, fmt='.2f', ax=ax)\n    ax.set_title(\"Entrophy based categorical to categorical dependent correlation for cats\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"daa263340a124a6ad1b640093648ba2bc4313227"},"cell_type":"code","source":"# this function tests the dependency between  independent categorical variables  and dependent continous/ordinal variable\ndef correlation_ratio(categories, measurements):\n    fcat, _ = pd.factorize(categories)\n    cat_num = np.max(fcat)+1\n    #print(cat_num)\n    y_avg_array = np.zeros(cat_num)\n    n_array = np.zeros(cat_num)\n    for i in range(0,cat_num):\n        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n        n_array[i] = len(cat_measures)\n        y_avg_array[i] = np.average(cat_measures)\n    y_total_avg = np.sum(np.multiply(y_avg_array,n_array))/np.sum(n_array)\n    numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))\n    denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))\n    if numerator == 0:\n        eta = 0.0\n    else:\n        eta = numerator/denominator\n    return eta\n\n\ndef by_correlation_ratio(df, dep_col, cols):\n    # for numerical and categorical mix correlation\n    cr = pd.DataFrame(columns=cols)\n    i = 0\n    for c in cols:\n        eta = correlation_ratio(df[dep_col], df[c])\n        #print(i,c)\n        cr.loc[i, c] = eta\n    cr.fillna(value=np.nan, inplace=True)\n    # plt.figure(figsize=(20, 1))\n    return cr\n\nprint(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27aa9f61804f2d680605c1780e7333cc3256be86","_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"# tests the dependency between  independent categorical variables  and dependent continous/ordinal variable\nif 1 == 0:\n    cols = []\n\n    for c in Columns.ind_num_cat_columns.value:\n        if c in x_train.columns.values.tolist():\n            cols.append(c)\n\n    cr = by_correlation_ratio(pd.concat([x_train, y_train] ,axis=1).reset_index(), Columns.dep_columns.value[0], cols)\n    cr_dogs = by_correlation_ratio(pd.concat([x_train_dogs, y_train_dogs] ,axis=1).reset_index(), Columns.dep_columns.value[0], cols)\n    cr_cats = by_correlation_ratio(pd.concat([x_train_cats, y_train_cats] ,axis=1).reset_index(), Columns.dep_columns.value[0], cols)\n\n    cr_all = pd.concat([cr, cr_dogs, cr_cats])\n    df_cr_all = pd.DataFrame(index=([\"All\", \"Dogs\", \"Cats\"]), data=(cr_all.values), columns=cr_all.columns.values)\n    plt.rcParams[\"figure.figsize\"] = [20, 5]\n    ax = plt.axes()\n\n    sns.heatmap(df_cr_all, annot=True, fmt='.4f', ax=ax)\n    ax.set_title('Correlations between independent categorical and dependant ordinal/continous variable')\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b66d9243f5c18b06b5c2be4ba3860502293cd44"},"cell_type":"code","source":"#pandas corr with spearman to test continous/ordinal independent and ordinal dependent value correlation\ncols = []\n            \n    \nfor c in Columns.boxplot_cols.value + Columns.item_type_cols.value + Columns.kbin_cols.value + Columns.item_cnt_mtype_cols.value:\n    if c in x_train.columns.values.tolist():\n        cols.append(c)\n\ncorr = pd.concat([x_train[cols], y_train], axis=1)\n#print(corr)\ncorr_dogs = pd.concat([x_train_dogs[cols], y_train_dogs], axis=1)\ncorr_cats = pd.concat([x_train_cats[cols], y_train_cats], axis=1)\ncorr_all = pd.concat([corr.corr('spearman').loc[\"AdoptionSpeed\",:], corr_dogs.corr('spearman').loc[\"AdoptionSpeed\",:], corr_cats.corr('spearman').loc[\"AdoptionSpeed\",:]], axis=1)\ndf_corr_all = pd.DataFrame(columns=([\"All\", \"Dogs\", \"Cats\"]), data=(corr_all.values), index=corr_all.index.values)\nplt.rcParams[\"figure.figsize\"] = [10, 40]\nax = plt.axes()\nsns.heatmap(df_corr_all.sort_values(by=['All']), annot=True, fmt='.4f', ax=ax)\nax.set_title('Dataset correlation between independent continous/ordinal and dependent ordinal variables')\nplt.show()\n\ndf_corr_all.sort_values(by=[\"All\"], ascending=False).to_csv(\"spearman_for_continous.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22bba8652ca88dfd8e7d26f721c478cb9bd880c3"},"cell_type":"code","source":"from scipy.stats import kruskal\nimport sys\ndef by_kruskal(arr, dep_col):\n#check adoption speed median for different categories, each category value should have at least 5 measurements\n#The Kruskal-Wallis H-test tests the null hypothesis that the population median of all of the groups are equal.\n    cols = arr.columns.values.tolist()\n    cols.remove(dep_col)\n    df =  pd.DataFrame(columns=cols)\n    \n    for c in cols:\n        #print(\"-------Kruskal Wallis H-test test for :--------\", c, \"on\", dep_col)\n        arg = []\n        for v in arr[c].unique():\n            #print(\"number of measurements of value\", v, \"on column\", c, \"is\", len(arr[arr[c] == v]))\n            if (len(arr[arr[c] == v]) >= 5):\n                #print(arr[arr[dep_col] == v][c].head())\n                arg.append(arr[arr[c] == v][dep_col])\n        #print(\"number of categorical groups having more than 5 ameasurements found is \", len(arg), \"/\",len(arr[c].unique()) )\n        if len(arg)>=2:\n            #try:\n            H, pval = kruskal(*arg)\n            #print(c, \"H-statistic:\", H, \"P-Value:\", pval)\n\n            if pval <= 0.01:\n                #plog(\"Reject NULL hypothesis - Significant differences exist between groups.\")\n                df.loc[0, c] = pval\n            if pval > 0.01:\n                plog(\"Accept NULL hypothesis - No significant difference between groups for \"+ c + \" within 99% of significance level with p-value \" + str(pval))\n                df.loc[0, c] = pval\n\n            #except:\n                #plog(\"Test Error\")\n                #print(c, sys.exc_info())\n                #df.loc[0, c] = 0\n\n        else:\n            plog(\"Not Tested Because of lack of at least 2 groupes having minimum 5 measurements\")\n            df.loc[0, c] = 0\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"658e80f19756c954ae38e965809589c0451c2075"},"cell_type":"code","source":"#Exucuting kruskal test by constucting adoption speed samples over categorical variable values \ncols = []\nfor c in Columns.barplot_cols.value + Columns.dep_columns.value:\n    if c in x_train.columns.values.tolist():\n        cols.append(c)\n        \ndf = by_kruskal(pd.concat([x_train, y_train], axis=1)[cols+Columns.dep_columns.value], Columns.dep_columns.value[0])\nif 1 == 1:\n    plt.rcParams[\"figure.figsize\"] = [15, 2]\n    sns.heatmap(df[df.columns].astype(float), annot=True)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"950c8f5a201d58b7ac2dae3991161ccd3cbab065"},"cell_type":"code","source":"#Exucuting kruskal test by constucting numerical columns samples using adoption speed as categories. This time executing kruskal on inverse\ncols = []\nfor c in Columns.boxplot_cols.value + Columns.item_type_cols.value + Columns.kbin_cols.value + Columns.item_cnt_mtype_cols.value:\n    if c in x_train.columns.values.tolist():\n        cols.append(c)\n\ndf_a = pd.DataFrame(columns=[\"AdoptionSpeed\"])\nfor c in cols :\n    print(c)\n    df = by_kruskal(pd.concat([x_train, y_train], axis=1)[[c] + Columns.dep_columns.value], c)\n    df[\"Variable\"] = c\n    df_a = pd.concat([df_a, df])\ndf_a = df_a.reset_index().drop(\"index\", axis=1)\ndf_a.set_index(\"Variable\", inplace=True)\nprint(df_a[df_a.AdoptionSpeed.isnull()])\n#df_a[\"AdoptionSpeed\"].fillna(2, inplace=True)\nplt.rcParams[\"figure.figsize\"] = [15, 100]\nsns.heatmap(df_a.astype(float).sort_values(by=['AdoptionSpeed']), annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f0abb216bcffd87a3e98348eb87274b60d85065"},"cell_type":"code","source":"#dropping columns not having any statistical difference\nif 1 == 0:\n    df_a2 = df_a.reset_index()\n    d_cols=\"\"\n    for c in df_a2[df_a2[\"AdoptionSpeed\"]==0][\"Variable\"].values.tolist():\n        x_train.drop(c, axis=1, inplace=True)\n        x_test.drop(c, axis=1, inplace=True)\n        d_cols = c + \", \" + d_cols\n\n    plog(\"Dropped \" + d_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"277ccf5a149faa01b81b491d5e5050503eb97085"},"cell_type":"code","source":"if 1 == 0:\n    #check correlation for continous independent variables and dependent ordinal variable\n    cols = []\n\n    for c in Columns.boxplot_cols.value + Columns.item_type_cols.value + Columns.kbin_cols.value + Columns.item_cnt_mtype_cols.value:\n        if c in x_train.columns.values.tolist():\n            cols.append(c)\n\n    cr = by_correlation_ratio(pd.concat([x_train, y_train] ,axis=1).reset_index(), Columns.dep_columns.value[0], cols)\n    cr = cr.T\n    cr_dogs = by_correlation_ratio(pd.concat([x_train_dogs, y_train_dogs] ,axis=1).reset_index(), Columns.dep_columns.value[0], cols)\n    cr_dogs = cr_dogs.T\n    cr_cats = by_correlation_ratio(pd.concat([x_train_cats, y_train_cats] ,axis=1).reset_index(), Columns.dep_columns.value[0], cols)\n    cr_cats = cr_cats.T\n\n    cr_all = pd.concat([cr, cr_dogs, cr_cats], axis=1)\n    df_cr_all = pd.DataFrame(data=(cr_all.values), index=cr_all.index)\n    df_cr_all.columns = [\"All\", \"Dogs\", \"Cats\"]\n    ax = plt.axes()\n    plt.rcParams[\"figure.figsize\"] = [5, 100]\n    sns.heatmap(df_cr_all, annot=True, fmt='.4f', ax=ax)\n    ax.set_title('Correlations between independent continous variables and dependant ordinal variable(supposed as continous)')\n    plt.show()\n\n    if 1 == 0:\n        #listing cols where corr <= 0.0003\n        cols = []\n        for index, row in df_cr_all.iterrows():\n            if row[\"All\"] <= 0.0003:\n                if index in Columns.boxplot_cols.value + Columns.item_type_cols.value + Columns.kbin_cols.value + Columns.item_cnt_mtype_cols.value:\n                    if index in x_train.columns.values.tolist():\n                        cols.append(index)\n        print(cols)\n\n        #dropping cols where corr <= 0.01\n\n        x_train.drop(cols, axis=1, inplace=True)\n        x_test.drop(cols, axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85d1dfc20d65304a92f327aee75ca2a6865d599b"},"cell_type":"code","source":"\nimport traceback\nfrom sklearn.feature_selection import mutual_info_classif, chi2\nfrom sklearn.feature_selection import SelectKBest, SelectFdr, SelectFpr, SelectFwe\n\n\nresult = chi2(x_train[Columns.ind_num_cat_columns.value], y_train[\"AdoptionSpeed\"].values.reshape(-1, 1))\n\nchi2_pvals = []\nfor x in result[1]:\n    chi2_pvals.append(x)\ndf_chi2_pvals = pd.DataFrame(index=Columns.ind_num_cat_columns.value, columns=[\"AdoptionSpeed\"], data = chi2_pvals).sort_values(by=[\"AdoptionSpeed\"], ascending=False)\ndf_chi2_pvals.to_csv(\"chitest_for_categorical.csv\")\n    \nif 1 == 1:    \n    x_train_fs = x_train.copy()\n    \n    mic = mutual_info_classif(x_train_fs.astype(\"float\"), y_train, random_state=42)\n    df_mic = pd.DataFrame(data=mic, index=x_train.columns.values.tolist(), columns=[\"importance\"])\n    print(df_mic)\n    print(df_mic.sort_values(by=[\"importance\"]))\n    df_mic.sort_values(by=[\"importance\"]).to_csv(\"df_mic.csv\")\n    \n    try:\n        x_fpr = SelectFpr(mutual_info_classif, alpha=0.03).fit_transform(x_train_fs.astype(\"float\"), y_train)\n        df_x_fpr = pd.DataFrame(data=x_fpr, index=x_train.columns.values.tolist(), columns=[\"importance\"])\n        print(df_x_fpr)\n        print(df_x_fpr.sort_values(by=[\"importance\"]))\n        df_x_fpr.sort_values(by=[\"importance\"]).to_csv(\"x_fpr.csv\")\n    except Exception as e:\n        print(e)\n        traceback.print_exc()\n        \n    try:\n        x_fdr = SelectFdr(mutual_info_classif, alpha=0.03).fit_transform(x_train_fs.astype(\"float\"), y_train)\n        df_x_fdr = pd.DataFrame(data=x_fdr, index=x_train.columns.values.tolist(), columns=[\"importance\"])\n        print(df_x_fdr)\n        print(df_x_fdr.sort_values(by=[\"importance\"]))\n        df_x_fdr.sort_values(by=[\"importance\"]).to_csv(\"x_fdr.csv\")\n    except Exception as e:\n        print(e)\n        traceback.print_exc()\n    \n    try:\n        x_fwe = SelectFwe(mutual_info_classif, alpha=0.03).fit_transform(x_train_fs.astype(\"float\"), y_train)\n        print(x_fwe)\n        df_x_fwe = pd.DataFrame(data=x_fwe, index=x_train.columns.values.tolist(), columns=[\"importance\"])\n        print(df_x_fwe)\n        print(df_x_fwe.sort_values(by=[\"importance\"]))\n        df_x_fwe.sort_values(by=[\"importance\"]).to_csv(\"x_fwe.csv\")\n    except Exception as e:\n        print(e)\n        traceback.print_exc()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5d33556f4a39fbdd4af902e20f39c4234345b4e"},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import RidgeClassifierCV\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import ComplementNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import RadiusNeighborsClassifier\nfrom sklearn.neighbors import NearestCentroid\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import ExtraTreeClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import NuSVC\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom time import time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import cohen_kappa_score, make_scorer\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n\ndef report(df, alg, best_est, perf, est, results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            #print(\"Model with rank: {0}\".format(i))\n            #print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n            #      results['mean_test_score'][candidate],\n            #      results['std_test_score'][candidate]))\n            #print(\"Parameters: {0}\".format(results['params'][candidate]))\n            #print(\"\")\n            df.loc[len(df)] = [alg, best_est, perf, est, format(i), results['mean_test_score'][candidate],results['std_test_score'][candidate],results['params'][candidate]]\n    return df\n\nlayer_sizes = []\nfor i in range(0,100):\n    t = (np.random.random_integers(25,300), np.random.random_integers(25,300), np.random.random_integers(25,300))\n    layer_sizes.append(t)\n\ndef iterate_by_randomsearch(train_x, train_y):\n    classifiers = [\n        (AdaBoostClassifier(), {\"n_estimators\": sp.stats.randint(25, 100),\n                                 'learning_rate': sp.stats.uniform(0.0001, 1)}),\n        # (BaggingClassifier(), {\"n_estimators\": sp.stats.randint(25, 100),\n        #                        \"max_features\": sp.stats.randint(1, 7),\n        #                        \"bootstrap\": [True, False],\n        #                        \"bootstrap_features\": [True, False],\n        #                        }),\n        # (ExtraTreesClassifier(), {\"n_estimators\": sp.stats.randint(25, 100),\n        #                           \"max_depth\": sp.stats.randint(3, 30),\n        #                           \"max_features\": sp.stats.randint(1, 7),\n        #                           \"min_samples_split\": sp.stats.randint(2, 11),\n        #                           \"bootstrap\": [True, False],\n        #                           \"criterion\": [\"gini\", \"entropy\"]}),\n        # (RandomForestClassifier(), {\"n_estimators\": sp.stats.randint(25, 100),\n        #                             \"max_depth\": sp.stats.randint(3, 30),\n        #                             \"max_features\": sp.stats.randint(1, 7),\n        #                             \"min_samples_split\": sp.stats.randint(2, 11),\n        #                             \"bootstrap\": [True, False],\n        #                             \"criterion\": [\"gini\", \"entropy\"]}),\n        # (PassiveAggressiveClassifier(), {\"max_iter\": sp.stats.randint(0, 1230),\n        #                                  \"tol\": sp.stats.uniform(0.0001, 0.05)}),\n        # (RidgeClassifier(), {\"max_iter\": sp.stats.randint(0, 2000),\n        #                      \"tol\": sp.stats.uniform(0.0001, 0.05),\n        #                      \"solver\": [\"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"]}),\n        # (SGDClassifier(), {\"max_iter\": sp.stats.randint(0, 2000),\n        #                    \"tol\": sp.stats.uniform(0.0001, 0.05),\n        #                    \"loss\": [\"hinge\", \"log\", \"modified_huber\", \"squared_hinge\", \"perceptron\"],\n        #                    \"penalty\": [\"none\", \"l2\", \"l1\", \"elasticnet\"]}),\n        # (KNeighborsClassifier(), {\"n_neighbors\": sp.stats.randint(1, 50),\n        #                           \"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"],\n        #                           \"leaf_size\": sp.stats.randint(20, 100),\n        #                           \"p\": [1, 2]}),\n        # (DecisionTreeClassifier(), {\"max_depth\": sp.stats.randint(3, 10),\n        #                             \"max_features\": sp.stats.randint(1, 7),\n        #                             \"min_samples_split\": sp.stats.randint(2, 11),\n        #                             \"criterion\": [\"gini\", \"entropy\"]}),\n        # (QuadraticDiscriminantAnalysis(), {\"tol\": sp.stats.uniform(1e-5, 1e-2)}),\n        # (LogisticRegression(), {\"multi_class\":[\"ovr\", \"multinomial\", \"auto\"],\n        #                        \"solver\":[\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n        #                        \"penalty\":[\"l2\"],\n        #                        \"class_weight\":[\"balanced\"],\n        #                        \"max_iter\": sp.stats.randint(100,300),\n        #                        \"n_jobs\":[4]}),\n        #(MLPClassifier(), {\"hidden_layer_sizes\":[(np.random.random_integers(25, 200), np.random.random_integers(25, 200), np.random.random_integers(25, 200))]}),\n        # (xgb.XGBClassifier(), {\"n_estimators\": sp.stats.randint(25, 200),\n        #                             \"max_depth\": sp.stats.randint(3, 30)}),\n         (lgb.LGBMClassifier(), {'num_leaves': sp.stats.randint(25, 330),\n                                 'n_estimators': sp.stats.randint(25, 150),\n                              #'bagging_fraction': sp.stats.uniform(0.4, 0.9),\n                               'learning_rate': sp.stats.uniform(0.001, 0.5),\n                              #'min_data': sp.stats.randint(50,700),\n                              #'is_unbalance': [True, False],\n                              #'max_bin': sp.stats.randint(3,25),\n                              'boosting_type' : ['gbdt', 'dart'],\n                              #'bagging_freq': sp.stats.randint(3,35),\n                               'max_depth': sp.stats.randint(3,30),\n                                'min_split_gain': sp.stats.uniform(0.001, 0.5),\n                               'objective': 'multiclass',\n                                 \"n_jobs\":[4]} )\n    ]\n    print(len(train_x), len(train_y))\n    df = pd.DataFrame(columns=['alg', 'best_estimator', 'perf', 'est','rank','mean','std', 'parameters'])\n    \n    for clf in classifiers:\n        n_iter=10\n        kappa_scorer = make_scorer(cohen_kappa_score, weights=\"quadratic\")\n        kfold = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42)\n        cvs = kfold.split(train_x, train_y)\n        print(cvs)\n        random_search = RandomizedSearchCV(clf[0], param_distributions=clf[1], verbose=8,\n                                            cv=cvs, n_iter=10, scoring=kappa_scorer, n_jobs = 4)\n        \n        print(type(clf[0]).__name__, \"started at\", datetime.now())\n        start = time()\n        random_search.fit(train_x, train_y)\n        df = report(df, type(clf[0]).__name__, random_search.best_estimator_ ,  time() - start, n_iter, random_search.cv_results_)\n        print(type(clf[0]).__name__, \"ended at\", datetime.now())\n    best = df['mean'].idxmax()\n\n    est = df.loc[best, [\"best_estimator\", \"mean\"]]\n    i = 1\n    clfs = []\n    means = []\n    clf_p=\"\"\n    dt=np.dtype('str,float')\n    for idx, clf in df.sort_values(by=\"mean\", ascending=False).iterrows():\n        #if (clf_p != type(clf[\"best_estimator\"]).__name__) & (i<6):\n        if i<6:\n            clfs.append((str(i)+type(clf[\"best_estimator\"]).__name__, clf[\"best_estimator\"]))\n            means.append(clf[\"mean\"])\n            clf_p = type(clf[\"best_estimator\"]).__name__ \n        i = i+1\n        #clf_p = type(clf[\"best_estimator\"]).__name__\n    return clfs, means\n\nprint(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6670dbce7d80a9c24bd357e52f5ef6f8f52bccc2"},"cell_type":"code","source":"def voting_predict(clfs, means, train_x, train_y, test_x, test_id):\n\n    clf = VotingClassifier(estimators=clfs, voting = 'soft')\n    clf.fit(train_x, train_y.values.ravel())\n    pred = clf.predict(test_x)\n    #print(test_id.shape, pred.shape)\n    prediction_df = pd.DataFrame({'PetID': test_id.values.ravel(),\n                                  'AdoptionSpeed': pred})\n\n    # create submission file print(prediction_df)\n    return prediction_df\n\ndef voting_predict_with_weights(clfs, means, train_x, train_y, test_x, test_id):\n\n    clf = VotingClassifier(estimators=clfs, weights=means, voting = 'soft')\n    clf.fit(train_x, train_y.values.ravel())\n    \n    pred = clf.predict(test_x)\n    #print(test_id.shape, pred.shape)\n    prediction_df = pd.DataFrame({'PetID': test_id.values.ravel(),\n                                  'AdoptionSpeed': pred})\n\n    # create submission file print(prediction_df)\n    return prediction_df\nprint(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd4218e59afa448f94f2510e622de3b26351114e"},"cell_type":"code","source":"if 1 == 0:\n    \n    clfs, means = iterate_by_randomsearch(x_train, y_train.values.ravel())\n    print(\"--------clfs--------\")\n    print(clfs)\n    print(\"--------means--------\")\n    print(means)\n    pred = voting_predict(clfs, means, x_train, y_train,  x_test, id_test)\n    #clfs = iterate_by_randomsearch(x_train, y_train.values.ravel())\n    #voting_predict(clfs, x_train, y_train,  x_test, id_test)\n    #pred.to_csv(\"submission.csv\",index=False)\n    class_sub = pred\nprint(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c51fc1755974c2b1d93e6fd44ab7f3abcd63eea"},"cell_type":"code","source":"if 1 == 0:\n    \n    x_train_a =  x_train\n    x_test_a = x_test\n    clfs, means = iterate_by_randomsearch(x_train_a, y_train.values.ravel())\n    print(clfs)\n    print(means)\n    pred = voting_predict(clfs, means, x_train_a, y_train,  x_test_a, id_test)\n    #clfs = iterate_by_randomsearch(x_train, y_train.values.ravel())\n    #voting_predict(clfs, x_train, y_train,  x_test, id_test)\n    pred.to_csv(\"submission.csv\",index=False)\nprint(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2ba3fc457abd8696a493034e5710924c8d136e3"},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\nfrom functools import partial\nimport lightgbm\nfrom math import sqrt\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert (len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n\n    for a, b in zip(rater_a, rater_b):\n        x = a - min_rating\n        y = b - min_rating\n        if not np.isscalar(x):\n            x=x[0]\n        if not np.isscalar(y):\n            y=y[0]\n        #print(x,y)\n        conf_mat[x][y] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        x = r - min_rating\n        if not np.isscalar(x):\n            x = x[0]\n        hist_ratings[x] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating = None\n    max_rating = None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert (len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              / num_scored_items)\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n\n    return (1.0 - numerator / denominator)\n\n\nclass OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n\n        ll = quadratic_weighted_kappa(y, X_p) #cohen_kappa_score(y, X_p, labels=[0,1,2,3,4], weights=\"quadratic\")\n        return -ll\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        #print(5)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        #print(initial_coef)\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n        print(self.coef_)\n\n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']\n\ndef rmse(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted))\n\ndef run_cv_model(train, test, target, model_fn, cat_features, params={}, eval_fn=None, label='model', n_splits=5, n_repeats=2):\n    kf = RepeatedStratifiedKFold(n_splits=n_splits, random_state=42, n_repeats = n_repeats)\n    fold_splits = kf.split(train, target)\n    cv_scores = []\n    qwk_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros((train.shape[0], n_splits*n_repeats))\n    all_coefficients = np.zeros((n_splits*n_repeats, 4))\n    feature_importance_df = pd.DataFrame()\n    i = 1\n    for dev_index, val_index in fold_splits:\n        #print('Started ' + label + ' fold ' + str(i) + '/'+str(n_splits*n_repeats))\n        if isinstance(train, pd.DataFrame):\n            dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n        else:\n            dev_X, val_X = train[dev_index], train[val_index]\n        if isinstance(target, pd.DataFrame):\n            dev_y, val_y = target.iloc[dev_index], target.iloc[val_index]\n        else:\n            dev_y, val_y = target[dev_index], target[val_index]\n        params2 = params.copy()\n        pred_val_y, pred_test_y, importances, coefficients, qwk = model_fn(dev_X, dev_y, val_X, val_y, test, params2, cat_features)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index] = pred_val_y\n        all_coefficients[i-1, :] = coefficients\n        if eval_fn is not None:\n            cv_score = eval_fn(val_y, pred_val_y)\n            cv_scores.append(cv_score)\n            qwk_scores.append(qwk)\n            #print(label + ' cv score {}: RMSE {} QWK {}'.format(i, cv_score, qwk))\n        fold_importance_df = pd.DataFrame()\n        if isinstance(train, pd.DataFrame):\n            fold_importance_df['feature'] = train.columns.values\n        else:\n            fold_importance_df['feature'] = [\"pca_\"+ str(i) for i in range(train.shape[1])]\n        fold_importance_df['importance'] = importances\n        fold_importance_df['fold'] = i\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        i += 1\n    #print('{} cv RMSE scores : {}'.format(label, cv_scores))\n    #print('{} cv mean RMSE score : {}'.format(label, np.mean(cv_scores)))\n    #print('{} cv std RMSE score : {}'.format(label, np.mean(cv_scores)))\n    #print('{} cv QWK scores : {}'.format(label, qwk_scores))\n    #print('{} cv mean QWK score : {}'.format(label, np.mean(qwk_scores)))\n    #print('{} cv std QWK score : {}'.format(label, np.std(qwk_scores)))\n    pred_full_test = pred_full_test / (n_splits*n_repeats)\n    results = {'label': label,\n               'train': pred_train, 'test': pred_full_test,\n                'cv': cv_scores, 'qwk': qwk_scores,\n               'importance': feature_importance_df,\n               'coefficients': all_coefficients}\n    return results\n\ndef runLGB(train_X, train_y, test_X, test_y, test_X2, params, cat_features):\n    #print('Prep LGB')\n    \n    d_train = lgb.Dataset(train_X, label=train_y)\n    d_valid = lgb.Dataset(test_X, label=test_y)\n    watchlist = [d_train, d_valid]\n    #print('Train LGB')\n    num_rounds = params.pop('num_rounds')\n    verbose_eval = params.pop('verbose_eval')\n    early_stop = None\n    if params.get('early_stop'):\n        early_stop = params.pop('early_stop')\n    if len(cat_features) > 0:\n        model = lgb.train(params,\n                          categorical_feature=list(cat_features),\n                          train_set=d_train,\n                          num_boost_round=num_rounds,\n                          valid_sets=watchlist,\n                          verbose_eval=verbose_eval,\n                          early_stopping_rounds=early_stop)\n    else:\n        model = lgb.train(params,\n                          train_set=d_train,\n                          num_boost_round=num_rounds,\n                          valid_sets=watchlist,\n                          verbose_eval=verbose_eval,\n                          early_stopping_rounds=early_stop)\n    #print('Predict 1/2')\n    #plog(\"--------feature names--------\")\n    #print(model.feature_name())\n    #plog(\"--------feature importances by split--------\")\n    #print(lightgbm.plot_importance(booster=model))\n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    optR = OptimizedRounder()\n    optR.fit(pred_test_y, test_y)\n    coefficients = optR.coefficients()\n    pred_test_y_k = optR.predict(pred_test_y, coefficients)\n    #print(\"Valid Counts = \", Counter(test_y))\n    #print(\"Predicted Counts = \", Counter(pred_test_y_k))\n    #print(\"Coefficients = \", coefficients)\n    qwk = quadratic_weighted_kappa(test_y, pred_test_y_k) #cohen_kappa_score(test_y, pred_test_y_k, labels=[0,1,2,3,4], weights=\"quadratic\")\n    #print(\"QWK = \", qwk)\n    #print('Predict 2/2')\n    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), model.feature_importance(importance_type=\"gain\"), coefficients, qwk\n\nplog(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f55c9cafd4cf85990b8c64aedb0afe861e03e99"},"cell_type":"code","source":"def by_regressor_rs(train, test, y_train, runALG, metric, name, cv, i, id_test):\n    mqwk = 0\n    j = 0\n    for j in range(10):\n        params = {'application': 'regression',\n              'boosting': 'gbdt',\n              'metric': 'rmse',\n              'num_leaves': random.randint(50, 300),\n              'max_depth': random.randint(5, 25),\n              'learning_rate': random.uniform(0.001, 0.05),\n              'bagging_fraction': random.uniform(0.5, 1),\n              'feature_fraction': random.uniform(0.5, 1),\n              'min_split_gain': random.uniform(0.001, 0.05),\n              'min_child_samples': random.randint(75, 200),\n              'min_child_weight': random.uniform(0.001, 0.05),\n              'verbosity': -1,\n              'data_random_seed': 3,\n              'early_stop': 100,\n              'verbose_eval': False,\n              'n_jobs':4,\n              'lambda_l2': random.uniform(0.001, 0.1),\n              'num_rounds': 10000}\n        print(\"RS prms\", params)\n        results_t = run_cv_model(train, test, y_train, runALG, params, metric, name, cv, i)\n        print(\"RS QWK Scores\", results_t[\"qwk\"], \"rs mean qwk scores\", np.mean(results_t[\"qwk\"]), \"rs mean rmse scores\" , np.mean(results_t[\"cv\"]))\n        if np.mean(results_t[\"qwk\"]) > mqwk:\n            results = results_t\n            mqwk = np.mean(results_t[\"qwk\"])\n    optR = OptimizedRounder()\n    coefficients_ = np.mean(results['coefficients'], axis=0)\n    print(coefficients_)\n    train_predictions = [r[0] for r in results['train']]\n    train_predictions = optR.predict(train_predictions, coefficients_).astype(int)\n    Counter(train_predictions)\n\n    optR = OptimizedRounder()\n    test_predictions = [r[0] for r in results['test']]\n    test_predictions = optR.predict(test_predictions, coefficients_).astype(int)\n    Counter(test_predictions)\n\n    pd.DataFrame(sk_cmatrix(y_train, train_predictions), index=list(range(5)), columns=list(range(5)))\n    submission = pd.DataFrame({'PetID': id_test.PetID.values, 'AdoptionSpeed': test_predictions})\n    return submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea48ed92112548d77b96540f0ae346d3b06842db"},"cell_type":"code","source":"\ndef by_regressor(train, test, y_train, runALG, prms, metric, name, cv, i, id_test, pca, c_features):\n    \n    results = run_cv_model(train, test, y_train, runALG, c_features, prms, metric, name, cv, i)\n    print(\"RS QWK Scores\", results[\"qwk\"], \"rs mean qwk scores\", np.mean(results[\"qwk\"]), \"rs mean rmse scores\" , np.mean(results[\"cv\"]))\n    grp = results[\"importance\"].groupby('feature',as_index=False)['importance'].mean()\n    print(grp.head())\n    grp2 = grp.sort_values(by=[\"importance\"])\n    grp2.plot(kind=\"barh\", y=\"importance\", x=\"feature\")\n    grp2.to_csv(\"feature_importances.csv\")\n    optR = OptimizedRounder()\n    coefficients_ = np.mean(results['coefficients'], axis=0)\n    print(coefficients_)\n    train_predictions = [r[0] for r in results['train']]\n    train_predictions = optR.predict(train_predictions, coefficients_).astype(int)\n    Counter(train_predictions)\n\n    optR = OptimizedRounder()\n    test_predictions = [r[0] for r in results['test']]\n    test_predictions = optR.predict(test_predictions, coefficients_).astype(int)\n    Counter(test_predictions)\n\n    pd.DataFrame(sk_cmatrix(y_train, train_predictions), index=list(range(5)), columns=list(range(5)))\n    submission = pd.DataFrame({'PetID': id_test.PetID.values, 'AdoptionSpeed': test_predictions})\n    return submission\nplog(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e039186479335ca09c6b65adc8dd8ca781db5ca7"},"cell_type":"code","source":"if 1 == 1:\n    import warnings\n    warnings.filterwarnings(\"ignore\")\n    params = {'application': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'max_depth': 11, \n          'num_leaves': 350,\n          'learning_rate': 0.01,\n          'bagging_fraction': 0.85,\n          'feature_fraction': 0.8,\n          'min_split_gain': 0.01,          \n          'min_child_samples': 75,\n          'min_child_weight': 0.1,\n          'verbosity': -1,\n          'data_random_seed': 3,\n          'early_stop': 100,\n          'verbose_eval': False,\n          'n_jobs':4,\n          #'lambda_l2': 0.05,\n          'num_rounds': 10000, }\n    \n    pca = 0\n    if pca == 1:\n        from sklearn.decomposition import KernelPCA\n        kPCA = KernelPCA(n_components=150, kernel='rbf')\n        kPCA.fit(x_train)\n        x_train = kPCA.transform(x_train)\n        x_test = kPCA.transform(x_test)\n    \n    x_train_a = x_train.drop(low_imp_features, axis=1)\n    x_test_a = x_test.drop(low_imp_features, axis=1)\n    \n    if pca == 0:\n        cat_features = [x_train_a.columns.get_loc(c) for c in x_train_a.columns if c in Columns.ind_num_cat_columns.value]\n    else:\n        cat_features = []\n    \n    plog(\"listing categorical features\")\n    print(cat_features)\n    #print(df.iloc[cat_features].columns[(df.iloc[cat_features] < 0).any()])\n    \n    if call==1:\n        submission = by_regressor(x_train_a, x_test_a, y_train, runLGB, params, rmse, 'lgb', 5, 2, id_test, pca ,cat_features)\n        reg_sub = submission\n        submission.to_csv('submission.csv', index=False)\n    else:\n        x_train_dogs.drop([\"Type\"], axis=1, inplace=True)\n        x_test_dogs.drop([\"Type\"], axis=1, inplace=True)\n        submission_d = by_regressor(x_train_dogs, x_test_dogs, y_train_dogs, runLGB, params, rmse, 'lgb', 5, 2, id_test_dogs)\n        x_train_cats.drop([\"Type\"], axis=1, inplace=True)\n        x_test_cats.drop([\"Type\"], axis=1, inplace=True)\n        submission_c = by_regressor(x_train_cats, x_test_cats, y_train_cats, runLGB, params, rmse, 'lgb', 5, 2, id_test_cats)\n        submission = pd.concat([submission_d, submission_c])\n        submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92ecdf9ba953f0747ea4175114db9d8b81405ca4"},"cell_type":"code","source":"if 1 == 0:\n    x_train_a =  x_train.drop(low_imp_features, axis=1) \n    x_test_a = x_test.drop(low_imp_features, axis=1)\n    \n    submission = by_regressor_rs(x_train_a, x_test_a, y_train, runLGB, rmse, 'lgb', 5, 2, id_test)\n    #submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4fb7ce49fd9fe10f406f1b62d70ab49f335f7e6"},"cell_type":"code","source":"from sklearn.base import BaseEstimator, ClassifierMixin\nimport lightgbm as lgb\nimport numpy as np\nimport joblib\nimport pickle\nfrom sklearn.metrics import cohen_kappa_score\nimport pathlib\n\nclass RatioOrdinalClassfier(BaseEstimator, ClassifierMixin):\n    \"\"\"An example of classifier\"\"\"\n\n    def __init__(self, estimator=lgb.LGBMClassifier()):\n        \"\"\"\n        Called when initializing the classifier\n        \"\"\"\n        # Parameters should have same name as attributes\n        self.estimator = estimator\n        self.estimators_ = []\n\n    def encode_classes(self, y, yi):\n        if y[0] > yi:\n            return 1\n        else:\n            return 0\n\n    def fit(self, X, y):\n        \"\"\"\n        This should fit classifier. All the \"work\" should be done here.\n\n        Note: assert is not a good choice here and you should rather\n        use try/except blog with exceptions. This is just for short syntax.\n        \"\"\"\n        self.sorted_classes_ = np.unique(y)\n        #print(self.sorted_classes_)\n        self.num_classes_ = len(self.sorted_classes_)\n        self.num_instances_ = len(X)\n        self.probas_ = np.zeros((self.num_instances_, 1))\n\n        for yi in self.sorted_classes_[:-1]:\n            #print(yi)\n            yt = y.copy()\n            yt[yt <= yi] = 0\n            yt[yt > yi] = 1\n            est = self.estimator\n            est.fit(X, yt.ravel())\n            filename = \"ownestimatormodel_\"+str(yi)+\".sav\"\n            with open(filename, 'wb') as file:\n                pickle.dump(est, file)\n            with open(filename ,'rb') as f:\n                est = pickle.load(f)\n            self.estimators_.append(est)\n        return self\n\n    def predict_proba(self, X):\n        i = 0\n        for yi in self.sorted_classes_[:-1]:\n            filename = \"ownestimatormodel_\"+str(yi)+\".sav\"\n            with open(filename ,'rb') as f:\n                est = pickle.load(f)\n            #est = self.estimators_[yi]\n            yt_proba = est.predict_proba(X)[:, 1:2]\n            if i == 0:\n                # print(self.probas_.shape)\n                self.probas_ = yt_proba\n                # print(self.probas_.shape)\n            else:\n                # print(yt_proba.shape, self.probas_.shape)\n                self.probas_ = np.concatenate((self.probas_, yt_proba), axis=1)\n                # print(yt_proba.shape, self.probas_.shape)\n            i += 1\n\n    def predict(self, X):\n        self.predict_proba(X)\n        ypf = np.zeros((self.num_instances_,1))\n        try:\n            getattr(self, \"probas_\")\n            for i in range(self.num_classes_):\n                if i == 0:\n                    ypi = 1 - self.probas_[:, 0:1]\n                elif 0 < i < self.num_classes_-1:\n                    ypi = self.probas_[:, i-1:i] - self.probas_[:, i:i+1]\n                elif i == self.num_classes_-1:\n                    ypi = self.probas_[:, i-1:i]\n                if i == 0:\n                    ypf = ypi\n                else:\n                    ypf = np.concatenate((ypf,ypi), axis=1)\n            #print(ypf)\n            return np.argmax(ypf, axis=1).reshape(len(ypf), 1)\n\n        except AttributeError:\n            raise RuntimeError(\"You must train classifer before predicting data!\")\nif 1 == 0:    \n    params = {\n              'boosting_type': 'gbdt',\n              'metric': 'cohen_kappa_score',\n              'max_depth': 9,\n              'num_leaves': 70,\n              'learning_rate': 0.01,\n              'bagging_fraction': 0.85,\n              'feature_fraction': 0.8,\n              'min_split_gain': 0.01,\n              'min_child_samples': 50,\n              'min_child_weight': 0.1,\n              'verbosity': -1,\n              'data_random_seed': 3,\n              'n_jobs': 4,\n              'objective':'binary',\n              # 'lambda_l2': 0.05,\n              }\n    est = lgb.LGBMClassifier(**params)\n    clf = RatioOrdinalClassfier(estimator=est)\n    clf.fit(x_train.values,y_train.values)\n    pred_y = clf.predict(x_train.values)\n    print(cohen_kappa_score(y_train, pred_y))\n    submission_p = clf.predict(x_test.values)\n    print(type(id_test), type(id_test.values), type(submission_p))\n    submission_df = pd.DataFrame(data=submission_p, columns=[\"AdoptionSpeed\"])\n    submission = pd.concat([id_test, submission_df], axis=1)\n    submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54f37b0dd1a4c452781e48888a4c7489c7df99b3"},"cell_type":"code","source":"if 1==0:\n    import math\n    merge_sub = (class_sub[\"AdoptionSpeed\"].astype('int32') + reg_sub[\"AdoptionSpeed\"].astype('int32'))/2\n    submission_df = pd.DataFrame(data=merge_sub, columns=[\"AdoptionSpeed\"])\n    \n    submission_df[\"AdoptionSpeed\"] = submission_df.apply(lambda x: math.floor(x['AdoptionSpeed']), axis=1 )\n    submission = pd.concat([id_test, submission_df], axis=1)\n    submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c75bfc9261d8523e3fa9e51f0924a218f04c04c"},"cell_type":"code","source":"if 1 == 0 :\n    from keras import models, layers, regularizers\n    from keras.utils.np_utils import to_categorical\n\n    import numpy as np\n    import pandas as pd\n    from datetime import datetime\n    from sklearn.preprocessing import OneHotEncoder\n\n    x_train_k = x_train.copy()\n    x_test_k = x_test.copy()\n    id_test_k = id_test.copy()\n\n    #y_train_k = to_categorical(y_train_k.values)\n    \n    y_train_k = np.zeros((len(y_train), 5))\n    i = 0\n    for x in y_train[\"AdoptionSpeed\"].values:\n        if x == 1:\n            y_train_k[i,2] = 1\n        elif x == 2:\n            y_train_k[i, 1:3] = 1\n        elif x == 3:\n            y_train_k[i, 1:4] = 1\n        elif x == 4:\n            y_train_k[i, 1:5] = 1\n        i += 1\n    print(y_train_k )\n\n\n\n    train_cat = np.array([])\n    test_cat = np.array([])\n    for c in Columns.ind_num_cat_columns.value:\n        val_cats = np.unique(x_train_k[c].values.tolist() + x_test_k[c].values.tolist())\n\n        ohe = OneHotEncoder(categories=[val_cats], sparse=False)\n\n        print(datetime.now(), c)\n        print(x_train_k[c].values.shape)\n        x_train_k[c] = x_train_k[c].astype('int32')\n        t_cat = ohe.fit_transform(x_train_k[c].values.reshape(-1, 1))\n        if train_cat.size == 0:\n            train_cat = t_cat\n        else:\n            train_cat = np.concatenate((train_cat, t_cat), axis=1)\n        x_train_k.drop(c, inplace=True, axis=1)\n\n        x_test_k[c] = x_test_k[c].astype('int32')\n        te_cat = ohe.fit_transform(x_test_k[c].values.reshape(-1, 1))\n        if test_cat.size == 0:\n            test_cat = te_cat\n        else:\n            test_cat = np.concatenate((test_cat, te_cat), axis=1)\n        x_test_k.drop(c, inplace=True, axis=1)\n\n        print(train_cat.shape, test_cat.shape)\n\n    print(datetime.now(), \"x_train and x_test to ndarray\")\n    x_train_k = np.array(x_train_k.values)\n    x_test_k = np.array(x_test_k.values)\n\n    print(datetime.now(), \"concatenate x_train/x_test and train_cat/test_cat\")\n    x_train_k = np.concatenate((x_train_k, train_cat), axis=1)\n    x_test_k = np.concatenate((x_test_k, test_cat), axis=1)\n\n    print(datetime.now(), x_train_k.shape, y_train_k.shape)\n\n    from sklearn.model_selection import train_test_split\n\n    partial_x_train, x_val, partial_y_train, y_val = train_test_split(x_train_k, y_train_k, test_size=0.20, random_state=42, stratify=y_train_k)\n\n    model = models.Sequential()\n    model.add(layers.Dense(int(partial_x_train), kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(partial_x_train.shape[1],)))\n    #model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(int(85), kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n    #model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(5, activation='softmax'))\n\n    model.compile(optimizer='rmsprop',\n                      loss='categorical_crossentropy',\n                      metrics=['accuracy'])\n\n    history = model.fit(partial_x_train, partial_y_train, epochs=100,\n                                batch_size=256, validation_data=(x_val, y_val))\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(1, len(loss)+1)\n    plt.rcParams[\"figure.figsize\"] = [20, 10]\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n\n    plt.plot(epochs, acc, 'bo', label='Training acc')\n    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation acc')\n    plt.xlabel('Epochs')\n    plt.ylabel('Acc')\n    plt.legend()\n    plt.show()\n\n    pred_labels = model.predict_classes(x_val)\n    true_labels = np.argmax(y_val, axis=1)\n\n    print(cohen_kappa_score(pred_labels,true_labels ))\n\n\n    pred = model.predict_classes(x_test_k)\n    id_test_k = np.array(id_test_k.values)\n    print(id_test_k.shape, pred.shape)\n    submission = np.concatenate((id_test_k, pred.reshape(-1,1)), axis=1)\n    print(submission.shape)\n    print(submission)\n    submission_df = pd.DataFrame(columns=[\"PetID, AdoptionSpeed\"], data=submission)\n    print(submission_df.groupby(['AdoptionSpeed']).size())\n    #submission_df.to_csv('submission.csv', index=False)\n\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}