{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Kaggle Competition: Predict at which speed a pet is adopted"},{"metadata":{"trusted":true,"_uuid":"40a25b03ad9ac2fdef0778c95460ea31e4881269"},"cell_type":"code","source":"# Import Packages\n\n#Dataframe packages\nimport json\nimport glob\nimport pandas as pd\nimport numpy as np\nimport os\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport cv2\nimport numpy as np\nfrom collections import Counter\nfrom functools import partial\nimport scipy as sp\nimport torch\nfrom torchvision import datasets, models, transforms\n\n#Plot packages\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# LightGBM\nimport lightgbm as lgb\nimport scipy as sp\n\n# Load scikit's classifier library\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold,RandomizedSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder,OrdinalEncoder, StandardScaler,KBinsDiscretizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.metrics import cohen_kappa_score,mean_squared_error, accuracy_score, confusion_matrix, f1_score,classification_report\n\n# Evaluation\nfrom sklearn.metrics import cohen_kappa_score,make_scorer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\nimport xgboost as xgb\n\n\n#Oversampling\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8365e4448f25ff3509a9abb9ff97c081f44b163"},"cell_type":"markdown","source":"## Sentiment Data"},{"metadata":{"trusted":true,"_uuid":"7cdff9a926965d5e58feb64a9ed311e841e2ad35"},"cell_type":"code","source":"sentimental_analysis_train = sorted(glob.glob('../input/petfinder-adoption-prediction/train_sentiment/*.json'))\nsentimental_analysis_test = sorted(glob.glob('../input/petfinder-adoption-prediction/test_sentiment/*.json'))\n\nprint('num of train sentiment files: {}'.format(len(sentimental_analysis_train)))\nprint('num of train sentiment files: {}'.format(len(sentimental_analysis_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e579470994efdaa36e8ddd53ff0d37b2b6e606e0"},"cell_type":"code","source":"# Define Empty lists\nscore=[]\nmagnitude=[]\npetid=[]\n\nfor filename in sentimental_analysis_train:\n             with open(filename, 'r') as f:\n                sentiment_file = json.load(f)\n             file_sentiment = sentiment_file['documentSentiment']\n             file_score =  np.asarray(sentiment_file['documentSentiment']['score'])\n             file_magnitude =np.asarray(sentiment_file['documentSentiment']['magnitude'])\n\n\n             score.append(file_score)\n             magnitude.append(file_magnitude)\n\n             petid.append(filename.replace('.json','').replace('../input/petfinder-adoption-prediction/train_sentiment/', ''))\n\n # Output with sentiment data for each pet\nsentimental_analysis_train = pd.concat([ pd.DataFrame(petid, columns =['PetID']) ,pd.DataFrame(score, columns =['sentiment_document_score']),\n                                                    pd.DataFrame(magnitude, columns =['sentiment_document_magnitude'])],axis =1)\n\nscore=[]\nmagnitude=[]\npetid=[]\n\nfor filename in sentimental_analysis_test:\n             with open(filename, 'r') as f:\n                sentiment_file = json.load(f)\n             file_sentiment = sentiment_file['documentSentiment']\n             file_score =  np.asarray(sentiment_file['documentSentiment']['score'])\n             file_magnitude =np.asarray(sentiment_file['documentSentiment']['magnitude'])\n\n\n             score.append(file_score)\n             magnitude.append(file_magnitude)\n\n             petid.append(filename.replace('.json','').replace('../input/test_sentiment/', ''))\n\n # Output with sentiment data for each pet\nsentimental_analysis_test = pd.concat([ pd.DataFrame(petid, columns =['PetID']) ,pd.DataFrame(score, columns =['sentiment_document_score']),\n                                                    pd.DataFrame(magnitude, columns =['sentiment_document_magnitude'])],axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bcee8b84010d33d1911befcbb6acbeea419052f"},"cell_type":"markdown","source":"## Image Metadata"},{"metadata":{"trusted":true,"_uuid":"b42c6c2d455f94f8aa6804d202fdaa48a5475f3e"},"cell_type":"code","source":"image_metadata_train =  sorted(glob.glob('../input/petfinder-adoption-prediction/train_metadata/*.json'))\nimage_metadata_test =  sorted(glob.glob('../input/petfinder-adoption-prediction/test_metadata/*.json'))\nprint('num of train metadata: {}'.format(len(image_metadata_train)))\nprint('num of train metadata: {}'.format(len(image_metadata_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0db0c9c5970b671ddca7ad5a7bdcfc4bcf9c4d03"},"cell_type":"code","source":"description=[]\ntopicality=[]\nimageid=[]\n# Read Zip File and Export a Dataset with the Score and the ID\nfor filename in image_metadata_train:\n         with open(filename, 'r') as f:\n            d = json.load(f)\n            file_keys = list(d.keys())\n         if  'labelAnnotations' in file_keys:\n            file_annots = d['labelAnnotations']\n            file_topicality = np.asarray([x['topicality'] for x in file_annots])\n            file_description = [x['description'] for x in file_annots]\n            #Create a list of all descriptions and topicality\n            description.append(file_description)\n            topicality.append(file_topicality)\n            #Create a list with all image id name\n            imageid.append(filename.replace('.json','').replace('../input/petfinder-adoption-prediction/train_metadata/',''))\n\n# Prepare the output by renaming all variables\ndescription=pd.DataFrame(description)\ntopicality=pd.DataFrame(topicality)\n\nnew_names = [(i,'metadata_description_'+str(i)) for i in description.iloc[:, 0:].columns.values]\ndescription.rename(columns = dict(new_names), inplace=True)\n\nnew_names = [(i,'metadata_topicality_'+str(i)) for i in topicality.iloc[:, 0:].columns.values]\ntopicality.rename(columns = dict(new_names), inplace=True)\n\n# Output with sentiment data for each pet\nimage_labelannot_train = pd.concat([ pd.DataFrame(imageid, columns =['ImageId']) ,topicality,description],axis =1)\n\n# create the PetId variable\nimage_labelannot_train['PetID'] = image_labelannot_train['ImageId'].str.split('-').str[0]\n\n\n##############\n# TOPICALITY #\n##############\n\nimage_labelannot_train['metadata_topicality_mean'] = image_labelannot_train.iloc[:,1:10].mean(axis=1)\nimage_labelannot_train['metadata_topicality_mean']  = image_labelannot_train.groupby(['PetID'])['metadata_topicality_mean'].transform('mean') \n\nimage_labelannot_train['metadata_topicality_max'] = image_labelannot_train.iloc[:,1:10].max(axis=1)\nimage_labelannot_train['metadata_topicality_max'] = image_labelannot_train.groupby(['PetID'])['metadata_topicality_max'].transform(max)\n\nimage_labelannot_train['metadata_topicality_min'] = image_labelannot_train.iloc[:,1:10].min(axis=1)\nimage_labelannot_train['metadata_topicality_min'] = image_labelannot_train.groupby(['PetID'])['metadata_topicality_min'].transform(min)\n\n\nimage_labelannot_train['metadata_topicality_0_mean']  = image_labelannot_train.groupby(['PetID'])['metadata_topicality_0'].transform('mean')\nimage_labelannot_train['metadata_topicality_0_max'] = image_labelannot_train.groupby(['PetID'])['metadata_topicality_0'].transform(max)\nimage_labelannot_train['metadata_topicality_0_min'] = image_labelannot_train.groupby(['PetID'])['metadata_topicality_0'].transform(min)\n\n\n###############\n# DESCRIPTION #\n###############\n\n# Create Features from the Images\nimage_labelannot_train['L_metadata_0_cat']=image_labelannot_train['metadata_description_0'].str.contains(\"cat\").astype(int)\nimage_labelannot_train['L_metadata_0_dog'] =image_labelannot_train['metadata_description_0'].str.contains(\"dog\").astype(int)\n\nimage_labelannot_train['L_metadata_any_cat']=image_labelannot_train.apply(lambda row: row.astype(str).str.contains('cat').any(), axis=1)\nimage_labelannot_train['L_metadata_any_dog']=image_labelannot_train.apply(lambda row: row.astype(str).str.contains('dog').any(), axis=1)\n\nimage_labelannot_train['L_metadata_0_cat_sum'] = image_labelannot_train.groupby(image_labelannot_train['PetID'])['L_metadata_0_cat'].transform('sum')\nimage_labelannot_train['L_metadata_0_dog_sum'] = image_labelannot_train.groupby(image_labelannot_train['PetID'])['L_metadata_0_dog'].transform('sum')\n\nimage_labelannot_train['L_metadata_any_cat_sum'] = image_labelannot_train.groupby(image_labelannot_train['PetID'])['L_metadata_any_cat'].transform('sum')\nimage_labelannot_train['L_metadata_any_dog_sum'] = image_labelannot_train.groupby(image_labelannot_train['PetID'])['L_metadata_any_dog'].transform('sum')\n\nimage_labelannot_train = image_labelannot_train[['PetID','metadata_topicality_max','metadata_topicality_mean','metadata_topicality_min','metadata_topicality_0_mean','metadata_topicality_0_max',\n                                                 'metadata_topicality_0_min','L_metadata_0_cat_sum','L_metadata_0_dog_sum','L_metadata_any_cat_sum','L_metadata_any_dog_sum']]\nimage_labelannot_train=image_labelannot_train.drop_duplicates('PetID')\n\ndescription=[]\ntopicality=[]\nimageid=[]\n\n# Read Zip File and Export a Dataset with the Score and the ID\nfor filename in image_metadata_test:\n         with open(filename, 'r') as f:\n            d = json.load(f)\n            file_keys = list(d.keys())\n         if  'labelAnnotations' in file_keys:\n            file_annots = d['labelAnnotations']\n            file_topicality = np.asarray([x['topicality'] for x in file_annots])\n            file_description = [x['description'] for x in file_annots]\n            #Create a list of all descriptions and topicality\n            description.append(file_description)\n            topicality.append(file_topicality)\n            #Create a list with all image id name\n            imageid.append(filename.replace('.json','').replace('../input/petfinder-adoption-prediction/test_metadata/',''))\n\n# Prepare the output by renaming all variables\ndescription=pd.DataFrame(description)\ntopicality=pd.DataFrame(topicality)\n\nnew_names = [(i,'metadata_description_'+str(i)) for i in description.iloc[:, 0:].columns.values]\ndescription.rename(columns = dict(new_names), inplace=True)\n\nnew_names = [(i,'metadata_topicality_'+str(i)) for i in topicality.iloc[:, 0:].columns.values]\ntopicality.rename(columns = dict(new_names), inplace=True)\n\n# Output with sentiment data for each pet\nimage_labelannot_test = pd.concat([ pd.DataFrame(imageid, columns =['ImageId']) ,topicality,description],axis =1)\n\n# create the PetId variable\nimage_labelannot_test['PetID'] = image_labelannot_test['ImageId'].str.split('-').str[0]\n\n\n##############\n# TOPICALITY #\n##############\n\nimage_labelannot_test['metadata_topicality_mean'] = image_labelannot_test.iloc[:,1:10].mean(axis=1)\nimage_labelannot_test['metadata_topicality_mean']  = image_labelannot_test.groupby(['PetID'])['metadata_topicality_mean'].transform('mean') \n\nimage_labelannot_test['metadata_topicality_max'] = image_labelannot_test.iloc[:,1:10].max(axis=1)\nimage_labelannot_test['metadata_topicality_max'] = image_labelannot_test.groupby(['PetID'])['metadata_topicality_max'].transform(max)\n\nimage_labelannot_test['metadata_topicality_min'] = image_labelannot_test.iloc[:,1:10].min(axis=1)\nimage_labelannot_test['metadata_topicality_min'] = image_labelannot_test.groupby(['PetID'])['metadata_topicality_min'].transform(min)\n\n\nimage_labelannot_test['metadata_topicality_0_mean']  = image_labelannot_test.groupby(['PetID'])['metadata_topicality_0'].transform('mean')\nimage_labelannot_test['metadata_topicality_0_max'] = image_labelannot_test.groupby(['PetID'])['metadata_topicality_0'].transform(max)\nimage_labelannot_test['metadata_topicality_0_min'] = image_labelannot_test.groupby(['PetID'])['metadata_topicality_0'].transform(min)\n\n\n###############\n# DESCRIPTION #\n###############\n\n# Create Features from the Images\nimage_labelannot_test['L_metadata_0_cat']=image_labelannot_test['metadata_description_0'].str.contains(\"cat\").astype(int)\nimage_labelannot_test['L_metadata_0_dog'] =image_labelannot_test['metadata_description_0'].str.contains(\"dog\").astype(int)\n\nimage_labelannot_test['L_metadata_any_cat']=image_labelannot_test.apply(lambda row: row.astype(str).str.contains('cat').any(), axis=1)\nimage_labelannot_test['L_metadata_any_dog']=image_labelannot_test.apply(lambda row: row.astype(str).str.contains('dog').any(), axis=1)\n\nimage_labelannot_test['L_metadata_0_cat_sum'] = image_labelannot_test.groupby(image_labelannot_test['PetID'])['L_metadata_0_cat'].transform('sum')\nimage_labelannot_test['L_metadata_0_dog_sum'] = image_labelannot_test.groupby(image_labelannot_test['PetID'])['L_metadata_0_dog'].transform('sum')\n\nimage_labelannot_test['L_metadata_any_cat_sum'] = image_labelannot_test.groupby(image_labelannot_test['PetID'])['L_metadata_any_cat'].transform('sum')\nimage_labelannot_test['L_metadata_any_dog_sum'] = image_labelannot_test.groupby(image_labelannot_test['PetID'])['L_metadata_any_dog'].transform('sum')\n\nimage_labelannot_test = image_labelannot_test[['PetID','metadata_topicality_max','metadata_topicality_mean','metadata_topicality_min',\n                                               'metadata_topicality_0_mean','metadata_topicality_0_max','metadata_topicality_0_min','L_metadata_0_cat_sum','L_metadata_0_dog_sum',\n                                               'L_metadata_any_cat_sum','L_metadata_any_dog_sum']]\nimage_labelannot_test=image_labelannot_test.drop_duplicates('PetID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f061436186d9fb202c883aacef0565e2bfe8c438"},"cell_type":"code","source":"color_score_mean=[]\ncolor_score_min=[]\ncolor_score_max=[]\n\ncolor_pixelfrac_mean=[]\ncolor_pixelfrac_min=[]\ncolor_pixelfrac_max=[]\n\nimageid=[]\n\n# Read Zip File and Export a Dataset with the Score and the ID\nfor filename in image_metadata_train:\n         with open(filename, 'r') as f:\n              d = json.load(f)\n              file_keys = list(d.keys())\n              if  'imagePropertiesAnnotation' in file_keys:\n                  file_colors = d['imagePropertiesAnnotation']['dominantColors']['colors']\n               \n                  file_color_score_mean = np.asarray([x['score'] for x in file_colors]).mean()\n                  file_color_pixelfrac_mean = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n\n                  file_color_score_min = np.asarray([x['score'] for x in file_colors]).min()\n                  file_color_pixelfrac_min = np.asarray([x['pixelFraction'] for x in file_colors]).min()\n\n\n                  file_color_score_max = np.asarray([x['score'] for x in file_colors]).max()\n                  file_color_pixelfrac_max = np.asarray([x['pixelFraction'] for x in file_colors]).max()\n\n\n              #Create a list with all image id name\n              imageid.append(filename.replace('.json','').replace('../input/petfinder-adoption-prediction/train_metadata/', ''))\n\n              color_score_mean.append(file_color_score_mean)\n              color_score_min.append(file_color_score_min)\n              color_score_max.append(file_color_score_max)\n\n\n              color_pixelfrac_mean.append(file_color_pixelfrac_mean)\n              color_pixelfrac_min.append(file_color_pixelfrac_min)\n              color_pixelfrac_max.append(file_color_pixelfrac_max)\n\n      \nimage_properties_train = pd.concat([pd.DataFrame({'ImageId':imageid}),pd.DataFrame({'metadata_color_pixelfrac_mean':color_pixelfrac_mean}), pd.DataFrame({'metadata_color_pixelfrac_min':color_pixelfrac_min}),pd.DataFrame({'metadata_color_pixelfrac_max':color_pixelfrac_max}),pd.DataFrame({'metadata_color_score_mean':color_score_mean}),pd.DataFrame({'metadata_color_score_min':color_score_min}),pd.DataFrame({'metadata_color_score_max':color_score_max})],axis=1)\n\n\n# create the PetId variable\nimage_properties_train['PetID'] = image_properties_train['ImageId'].str.split('-').str[0]\n\n\n##############\n# COLOR INFO #\n##############\nimage_properties_train['metadata_color_pixelfrac_mean']  = image_properties_train.groupby(['PetID'])['metadata_color_pixelfrac_mean'].transform('mean') \nimage_properties_train['metadata_color_pixelfrac_min']  = image_properties_train.groupby(['PetID'])['metadata_color_pixelfrac_min'].transform(min) \nimage_properties_train['metadata_color_pixelfrac_max']  = image_properties_train.groupby(['PetID'])['metadata_color_pixelfrac_max'].transform(max) \n\nimage_properties_train['metadata_color_score_mean']  = image_properties_train.groupby(['PetID'])['metadata_color_score_mean'].transform('mean') \nimage_properties_train['metadata_color_score_min']  = image_properties_train.groupby(['PetID'])['metadata_color_score_min'].transform(min) \nimage_properties_train['metadata_color_score_max']  = image_properties_train.groupby(['PetID'])['metadata_color_score_max'].transform(max)\n\nimage_properties_train=image_properties_train.drop_duplicates('PetID')\nimage_properties_train = image_properties_train.drop(['ImageId'], 1)\n\n\ncolor_score_mean=[]\ncolor_score_min=[]\ncolor_score_max=[]\n\ncolor_pixelfrac_mean=[]\ncolor_pixelfrac_min=[]\ncolor_pixelfrac_max=[]\n\nimageid=[]\n\n# Read Zip File and Export a Dataset with the Score and the ID\nfor filename in image_metadata_test:\n         with open(filename, 'r') as f:\n              d = json.load(f)\n              file_keys = list(d.keys())\n              if  'imagePropertiesAnnotation' in file_keys:\n                  file_colors = d['imagePropertiesAnnotation']['dominantColors']['colors']\n               \n                  file_color_score_mean = np.asarray([x['score'] for x in file_colors]).mean()\n                  file_color_pixelfrac_mean = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n\n                  file_color_score_min = np.asarray([x['score'] for x in file_colors]).min()\n                  file_color_pixelfrac_min = np.asarray([x['pixelFraction'] for x in file_colors]).min()\n\n\n                  file_color_score_max = np.asarray([x['score'] for x in file_colors]).max()\n                  file_color_pixelfrac_max = np.asarray([x['pixelFraction'] for x in file_colors]).max()\n\n\n              #Create a list with all image id name\n              imageid.append(filename.replace('.json','').replace('../input/test_metadata/', ''))\n\n              color_score_mean.append(file_color_score_mean)\n              color_score_min.append(file_color_score_min)\n              color_score_max.append(file_color_score_max)\n\n\n              color_pixelfrac_mean.append(file_color_pixelfrac_mean)\n              color_pixelfrac_min.append(file_color_pixelfrac_min)\n              color_pixelfrac_max.append(file_color_pixelfrac_max)\n\n      \nimage_properties_test = pd.concat([pd.DataFrame({'ImageId':imageid}),pd.DataFrame({'metadata_color_pixelfrac_mean':color_pixelfrac_mean}), pd.DataFrame({'metadata_color_pixelfrac_min':color_pixelfrac_min}),pd.DataFrame({'metadata_color_pixelfrac_max':color_pixelfrac_max}),pd.DataFrame({'metadata_color_score_mean':color_score_mean}),pd.DataFrame({'metadata_color_score_min':color_score_min}),pd.DataFrame({'metadata_color_score_max':color_score_max})],axis=1)\n\n\n# create the PetId variable\nimage_properties_test['PetID'] = image_properties_test['ImageId'].str.split('-').str[0]\n\n\n##############\n# COLOR INFO #\n##############\nimage_properties_test['metadata_color_pixelfrac_mean']  = image_properties_test.groupby(['PetID'])['metadata_color_pixelfrac_mean'].transform('mean') \nimage_properties_test['metadata_color_pixelfrac_min']  = image_properties_test.groupby(['PetID'])['metadata_color_pixelfrac_min'].transform(min) \nimage_properties_test['metadata_color_pixelfrac_max']  = image_properties_test.groupby(['PetID'])['metadata_color_pixelfrac_max'].transform(max) \n\nimage_properties_test['metadata_color_score_mean']  = image_properties_test.groupby(['PetID'])['metadata_color_score_mean'].transform('mean') \nimage_properties_test['metadata_color_score_min']  = image_properties_test.groupby(['PetID'])['metadata_color_score_min'].transform(min) \nimage_properties_test['metadata_color_score_max']  = image_properties_test.groupby(['PetID'])['metadata_color_score_max'].transform(max)\n\nimage_properties_test=image_properties_test.drop_duplicates('PetID')\nimage_properties_test = image_properties_test.drop(['ImageId'], 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2ef53075ab7c5463233c081ee562f3d828ee7e1"},"cell_type":"markdown","source":"## Image Quality "},{"metadata":{"trusted":true,"_uuid":"f4805a49b047cb38664f04470e5f882416b43bc5"},"cell_type":"code","source":"import glob\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport cv2\nfrom tqdm import tqdm_notebook as tqdm\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision.transforms import functional as F\nfrom sklearn.decomposition import TruncatedSVD\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2ff32925511043ac441526531340c333b4a85de"},"cell_type":"code","source":"def get_profile_path(category):\n\n    data = []\n\n    for path in sorted(glob.glob('../input/petfinder-adoption-prediction/%s_images/*-1.jpg' % category)):\n\n        data.append({\n            'PetID': path.split('/')[-1].split('-')[0],\n            'path': path,\n        })\n            \n    return pd.DataFrame(data)\n\ntrain = get_profile_path('train')\ntest = get_profile_path('test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe7244003288ea4c6e757717f96112d4a9808b70"},"cell_type":"code","source":"size=224\n\ndef resize_to_square(image, size):\n    h, w, d = image.shape\n    ratio = size / max(h, w)\n    resized_image = cv2.resize(image, (int(w*ratio), int(h*ratio)), cv2.INTER_AREA)\n    return resized_image\n\ndef image_to_tensor(image, normalize=None):\n    tensor = torch.from_numpy(np.moveaxis(image / (255. if image.dtype == np.uint8 else 1), -1, 0).astype(np.float32))\n    if normalize is not None:\n        return F.normalize(tensor, **normalize)\n    return tensor\n\ndef pad(image, min_height, min_width):\n    h,w,d = image.shape\n\n    if h < min_height:\n        h_pad_top = int((min_height - h) / 2.0)\n        h_pad_bottom = min_height - h - h_pad_top\n    else:\n        h_pad_top = 0\n        h_pad_bottom = 0\n\n    if w < min_width:\n        w_pad_left = int((min_width - w) / 2.0)\n        w_pad_right = min_width - w - w_pad_left\n    else:\n        w_pad_left = 0\n        w_pad_right = 0\n\n    return cv2.copyMakeBorder(image, h_pad_top, h_pad_bottom, w_pad_left, w_pad_right, cv2.BORDER_CONSTANT, value=(0,0,0))\n\n\nclass Dataset(torch.utils.data.Dataset):\n    \n    def __init__(self, df, size):\n        self.df = df\n        self.size = size\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n\n        row = self.df.iloc[idx]\n\n        image = cv2.imread(row.path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = resize_to_square(image, self.size)\n        image = pad(image, self.size, self.size)\n        tensor = image_to_tensor(image, normalize={'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]})\n            \n        return tensor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9efc89386d2bde544d810cb30ce88936ab707b39"},"cell_type":"code","source":"def extract_features_dense(df):\n    model = models.densenet121(pretrained=False)\n    model.load_state_dict(torch.load('../input/pytorch-pretrained-image-models/densenet121.pth'))\n    model = model.cuda()\n    model.eval()\n\n    # register hook to access to features in forward pass\n    features = []\n    def hook(module, input, output):\n        N,C,H,W = output.shape\n        output = output.reshape(N,C,-1)\n        features.append(output.mean(dim=2).cpu().detach().numpy())\n        \n    handle = model._modules.get('features').register_forward_hook(hook)\n\n    dataset = Dataset(df, size)\n    loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=False, num_workers=4)\n\n    for i_batch, inputs in tqdm(enumerate(loader), total=len(loader)):\n        _ = model(inputs.cuda())\n\n    features = np.concatenate(features)\n\n    features = pd.DataFrame(features)\n    features = features.add_prefix('IMAGE_')\n    features.loc[:,'PetID'] = df['PetID']\n    \n    handle.remove()\n    del model\n\n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44cb07971c953f2417f3df35207c07039e671884"},"cell_type":"code","source":"def extract_features(df):\n    model = models.resnet50(pretrained=False)\n    model.load_state_dict(torch.load('../input/pytorch-pretrained-image-models/resnet50.pth'))\n    model = model.cuda()\n    model.eval()\n\n    # register hook to access to features in forward pass\n    features = []\n    def hook(module, input, output):\n        N,C,H,W = output.shape\n        output = output.reshape(N,C,-1)\n        features.append(output.mean(dim=2).cpu().detach().numpy())\n    handle = model._modules.get('avgpool').register_forward_hook(hook)\n\n    dataset = Dataset(df, size)\n    loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=False, num_workers=4)\n\n    for i_batch, inputs in tqdm(enumerate(loader), total=len(loader)):\n        _ = model(inputs.cuda())\n\n    features = np.concatenate(features)\n\n    features = pd.DataFrame(features)\n    features = features.add_prefix('IMAGE_')\n    features.loc[:,'PetID'] = df['PetID']\n    \n    handle.remove()\n    del model\n\n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05c5924507b59b26589d3d0724bfcb1249b68d57"},"cell_type":"code","source":"features_train = extract_features(train)\nfeatures_test = extract_features(test)\n\nfeatures_train_dense = extract_features_dense(train)\nfeatures_test_dense = extract_features_dense(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60aea58564bcd456ff1325af612b4cf467228614"},"cell_type":"code","source":"n_components = 5\npetID_train = features_train[['PetID']]\npetID_test = features_train[['PetID']]\nsvd_ = TruncatedSVD(n_components=n_components)\nsvd_col = svd_.fit_transform(features_train.drop('PetID', axis=1))\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('IMG_')\nimg_features = pd.concat([svd_col,petID_train], axis=1)\n\nsvd_col = svd_.transform(features_test.drop('PetID', axis=1))\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('IMG_')\ntest_img_features = pd.concat([svd_col,petID_test], axis=1)\n\npetID = features_train_dense[['PetID']]\nsvd_ = TruncatedSVD(n_components=n_components)\nsvd_col = svd_.fit_transform(features_train_dense.drop('PetID', axis=1))\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('IMG_DENSE_')\nimg_features_dense = pd.concat([svd_col,petID_test], axis=1)\n\nsvd_col = svd_.transform(features_test_dense.drop('PetID', axis=1))\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('IMG_DENSE_')\ntest_img_features_dense = pd.concat([svd_col,petID_train], axis=1)\n\nX_train_temp = img_features.merge(img_features_dense, left_on='PetID', right_on='PetID', how='left')\nX_test_temp = test_img_features.merge(test_img_features_dense, left_on='PetID', right_on='PetID', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"957c38c495172bba05627e1f86bd8d0df10b6e91"},"cell_type":"code","source":"image_quality_train =sorted(glob.glob('../input/petfinder-adoption-prediction/train_images/*.jpg'))\nimage_quality_test =sorted(glob.glob('../input/petfinder-adoption-prediction/test_images/*.jpg'))\n\nblur=[]\nimage_pixel=[]\nimageid =[]\n\nfor filename in image_quality_train:\n              #Blur \n              image = cv2.imread(filename)\n              gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n              result = cv2.Laplacian(gray, cv2.CV_64F).var() \n              # Pixels\n              with Image.open(filename) as pixel:\n                  width, height = pixel.size\n              \n              pixel = width*height\n              \n              #image pixel size for each image\n              \n              image_pixel.append(pixel)\n              #blur for each image\n              blur.append(result)\n              #image id\n              imageid.append(filename.replace('.jpg','').replace('../input/petfinder-adoption-prediction/train_images/', ''))\n                \n# Join Pixel, Blur and Image ID\nimage_quality_train = pd.concat([ pd.DataFrame(imageid, columns =['ImageId']) ,pd.DataFrame(blur, columns =['blur']),\n                                        pd.DataFrame(image_pixel,columns=['pixel'])],axis =1)\n\n# create the PetId variable\nimage_quality_train['PetID'] = image_quality_train['ImageId'].str.split('-').str[0]\n\n#Mean of the Mean\nimage_quality_train['pixel_mean'] = image_quality_train.groupby(['PetID'])['pixel'].transform('mean')\nimage_quality_train['blur_mean'] = image_quality_train.groupby(['PetID'])['blur'].transform('mean') \n\nimage_quality_train['pixel_min'] = image_quality_train.groupby(['PetID'])['pixel'].transform('min') \nimage_quality_train['blur_min'] = image_quality_train.groupby(['PetID'])['blur'].transform('min')\n\nimage_quality_train['pixel_max'] = image_quality_train.groupby(['PetID'])['pixel'].transform('max') \nimage_quality_train['blur_max'] = image_quality_train.groupby(['PetID'])['blur'].transform('max')\n\nimage_quality_train = image_quality_train.drop(['blur','pixel','ImageId'], 1)\nimage_quality_train=image_quality_train.drop_duplicates('PetID')\n\nblur=[]\nimage_pixel=[]\nimageid =[]\n\nfor filename in image_quality_test:\n              #Blur \n              image = cv2.imread(filename)\n              gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n              result = cv2.Laplacian(gray, cv2.CV_64F).var() \n              # Pixels\n              with Image.open(filename) as pixel:\n                  width, height = pixel.size\n              \n              pixel = width*height\n              \n              #image pixel size for each image\n              \n              image_pixel.append(pixel)\n              #blur for each image\n              blur.append(result)\n              #image id\n              imageid.append(filename.replace('.jpg','').replace('../input/petfinder-adoption-prediction/test_images/', ''))\n                \n# Join Pixel, Blur and Image ID\nimage_quality_test = pd.concat([ pd.DataFrame(imageid, columns =['ImageId']) ,pd.DataFrame(blur, columns =['blur']),\n                                        pd.DataFrame(image_pixel,columns=['pixel'])],axis =1)\n\n# create the PetId variable\nimage_quality_test['PetID'] = image_quality_test['ImageId'].str.split('-').str[0]\n\n#Mean of the Mean\nimage_quality_test['pixel_mean'] = image_quality_test.groupby(['PetID'])['pixel'].transform('mean')\nimage_quality_test['blur_mean'] = image_quality_test.groupby(['PetID'])['blur'].transform('mean') \n\nimage_quality_test['pixel_min'] = image_quality_test.groupby(['PetID'])['pixel'].transform('min') \nimage_quality_test['blur_min'] = image_quality_test.groupby(['PetID'])['blur'].transform('min')\n\nimage_quality_test['pixel_max'] = image_quality_test.groupby(['PetID'])['pixel'].transform('max') \nimage_quality_test['blur_max'] = image_quality_test.groupby(['PetID'])['blur'].transform('max')\n\nimage_quality_test = image_quality_test.drop(['blur','pixel','ImageId'], 1)\nimage_quality_test=image_quality_test.drop_duplicates('PetID')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f60df714dbbb35eabaa682db0139096de68f6364"},"cell_type":"markdown","source":"## Quality Information: HU Moments"},{"metadata":{"trusted":true,"_uuid":"d1f5f1a31bae56974363bdcaee410578e4247fe0"},"cell_type":"code","source":"from math import copysign, log10\n\nhuMoments0=[]\nhuMoments1=[]\nhuMoments2=[]\nhuMoments3=[]\nhuMoments4=[]\nhuMoments5=[]\nhuMoments6=[]\nimageid =[]\n\nimage_info_train =sorted(glob.glob('../input/petfinder-adoption-prediction/train_images/*.jpg'))\nimage_info_test =sorted(glob.glob('../input/petfinder-adoption-prediction/test_images/*.jpg'))\n\nfor filename in image_info_train:\n            if filename.endswith(\"-1.jpg\"): # Take only the moments of picture 1\n                image = cv2.imread(filename)\n                im = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  \n                # Calculate Moments\n                moments = cv2.moments(im)\n\n                # Calculate Hu Moments\n                huMoments = cv2.HuMoments(moments)\n                # Log scale hu moments\n                for i in range(0,7):\n                      huMoments[i] = round(-1* copysign(1.0, huMoments[i]) * log10(abs(huMoments[i])),2)\n\n                #image id\n                imageid.append(filename.replace('.jpg','').replace('../input/petfinder-adoption-prediction/train_images/', ''))\n                huMoments0.append(huMoments[0])\n\n                huMoments1.append(huMoments[1])\n                huMoments2.append(huMoments[2])\n                huMoments3.append(huMoments[3])\n                huMoments4.append(huMoments[4])\n                huMoments5.append(huMoments[5])\n                huMoments6.append(huMoments[6])\n\nimage_moments_train = pd.concat([pd.DataFrame({'ImageId':imageid}),pd.DataFrame({'huMoments0':np.concatenate(huMoments0,axis=0)}), \n                                     pd.DataFrame({'huMoments1':np.concatenate(huMoments1,axis=0)}),\n                                     pd.DataFrame({'huMoments2':np.concatenate(huMoments2,axis=0)}),\n                                     pd.DataFrame({'huMoments3':np.concatenate(huMoments3,axis=0)}),\n                                     pd.DataFrame({'huMoments4':np.concatenate(huMoments4,axis=0)}),\n                                     pd.DataFrame({'huMoments5':np.concatenate(huMoments5,axis=0)}),pd.DataFrame({'huMoments6':np.concatenate(huMoments6,axis=0)})],axis=1)\n            \n\n# create the PetId variable\nimage_moments_train['PetID'] = image_moments_train['ImageId'].str.split('-').str[0]\nimage_moments_train = image_moments_train[image_moments_train['ImageId'].apply(lambda x:x.endswith((\"-1\")))]\n\nhuMoments0=[]\nhuMoments1=[]\nhuMoments2=[]\nhuMoments3=[]\nhuMoments4=[]\nhuMoments5=[]\nhuMoments6=[]\nimageid =[]\nfor filename in image_info_test:\n            if filename.endswith(\"-1.jpg\"): # Take only the moments of picture 1\n                image = cv2.imread(filename)\n                im = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  \n                # Calculate Moments\n                moments = cv2.moments(im)\n\n                # Calculate Hu Moments\n                huMoments = cv2.HuMoments(moments)\n                # Log scale hu moments\n                for i in range(0,7):\n                      huMoments[i] = round(-1* copysign(1.0, huMoments[i]) * log10(abs(huMoments[i])),2)\n\n                #image id\n                imageid.append(filename.replace('.jpg','').replace('../input/petfinder-adoption-prediction/test_images/', ''))\n                huMoments0.append(huMoments[0])\n\n                huMoments1.append(huMoments[1])\n                huMoments2.append(huMoments[2])\n                huMoments3.append(huMoments[3])\n                huMoments4.append(huMoments[4])\n                huMoments5.append(huMoments[5])\n                huMoments6.append(huMoments[6])\n\nimage_moments_test= pd.concat([pd.DataFrame({'ImageId':imageid}),pd.DataFrame({'huMoments0':np.concatenate(huMoments0,axis=0)}), pd.DataFrame({'huMoments1':np.concatenate(huMoments1,axis=0)}),\n                                           pd.DataFrame({'huMoments2':np.concatenate(huMoments2,axis=0)}),pd.DataFrame({'huMoments3':np.concatenate(huMoments3,axis=0)}),pd.DataFrame({'huMoments4':np.concatenate(huMoments4,axis=0)}),\n                                           pd.DataFrame({'huMoments5':np.concatenate(huMoments5,axis=0)}),pd.DataFrame({'huMoments6':np.concatenate(huMoments6,axis=0)})],axis=1)\n            \n\n# create the PetId variable\nimage_moments_test['PetID'] = image_moments_test['ImageId'].str.split('-').str[0]\nimage_moments_test = image_moments_test[image_moments_test['ImageId'].apply(lambda x:x.endswith((\"-1\")))]\nimage_moments_test = image_moments_test.drop(['ImageId'], 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fec79e63cf7187365a6e23a9b93aa8d36015da7"},"cell_type":"code","source":"train = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\ntest = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')\nsample_submission = pd.read_csv('../input/petfinder-adoption-prediction/test/sample_submission.csv')\n\nbreed =pd.read_csv('../input/petfinder-adoption-prediction/breed_labels.csv',usecols=[\"BreedID\", \"BreedName\"]) #A pet could have multiple breed\ncolor =pd.read_csv('../input/petfinder-adoption-prediction/color_labels.csv') #A pet could have multiple colors\nstate =pd.read_csv('../input/petfinder-adoption-prediction/state_labels.csv')\n\n# Add information about color, breed, state and sentiment data\ntrain = (pd.merge(train, breed.rename(columns={\"BreedName\": \"BreedName1\"}),  how='left', left_on=['Breed1'], right_on = ['BreedID']).drop('BreedID', axis=1))\ntrain = (pd.merge(train, breed.rename(columns={\"BreedName\": \"BreedName2\"}),  how='left', left_on=['Breed2'], right_on = ['BreedID']).drop('BreedID', axis=1))\n\ntrain = (pd.merge(train, color.rename(columns={\"ColorName\": \"ColorName1\"}),  how='left', left_on=['Color1'], right_on = ['ColorID']).drop('ColorID', axis=1))\ntrain = (pd.merge(train, color.rename(columns={\"ColorName\": \"ColorName2\"}),  how='left', left_on=['Color2'], right_on = ['ColorID']).drop('ColorID', axis=1))\ntrain = (pd.merge(train, color.rename(columns={\"ColorName\": \"ColorName3\"}),  how='left', left_on=['Color3'], right_on = ['ColorID']).drop('ColorID', axis=1))\n\ntrain = (pd.merge(train, state,  how='inner', left_on=['State'], right_on = ['StateID']).drop('StateID', axis=1))\n\n# Add information about sentimental analysis\ntrain = (pd.merge(train, sentimental_analysis_train,  how='left', left_on=['PetID'], right_on = ['PetID']))\n\n# Add information about Metadata Images\ntrain = (pd.merge(train, image_properties_train,  how='left', left_on=['PetID'], right_on = ['PetID']))\ntrain = (pd.merge(train, image_labelannot_train,  how='left', left_on=['PetID'], right_on = ['PetID']))\ntrain = (pd.merge(train, image_moments_train,  how='left', left_on=['PetID'], right_on = ['PetID']))\n\n# Add information about quality Images\ntrain = (pd.merge(train, image_quality_train,  how='left', left_on=['PetID'], right_on = ['PetID']))\n\n\n# Add information about color, breed, state and sentiment data\ntest = (pd.merge(test, breed.rename(columns={\"BreedName\": \"BreedName1\"}),  how='left', left_on=['Breed1'], right_on = ['BreedID']).drop('BreedID', axis=1))\ntest = (pd.merge(test, breed.rename(columns={\"BreedName\": \"BreedName2\"}),  how='left', left_on=['Breed2'], right_on = ['BreedID']).drop('BreedID', axis=1))\n\ntest = (pd.merge(test, color.rename(columns={\"ColorName\": \"ColorName1\"}),  how='left', left_on=['Color1'], right_on = ['ColorID']).drop('ColorID', axis=1))\ntest = (pd.merge(test, color.rename(columns={\"ColorName\": \"ColorName2\"}),  how='left', left_on=['Color2'], right_on = ['ColorID']).drop('ColorID', axis=1))\ntest = (pd.merge(test, color.rename(columns={\"ColorName\": \"ColorName3\"}),  how='left', left_on=['Color3'], right_on = ['ColorID']).drop('ColorID', axis=1))\n\ntest = (pd.merge(test, state,  how='inner', left_on=['State'], right_on = ['StateID']).drop('StateID', axis=1))\n\n# Add information about sentimental analysis\ntest = (pd.merge(test, sentimental_analysis_test,  how='left', left_on=['PetID'], right_on = ['PetID']))\n\n# Add information about Metadata Images\ntest = (pd.merge(test, image_properties_test,  how='left', left_on=['PetID'], right_on = ['PetID']))\ntest = (pd.merge(test, image_labelannot_test,  how='left', left_on=['PetID'], right_on = ['PetID']))\ntest = (pd.merge(test, image_moments_test,  how='left', left_on=['PetID'], right_on = ['PetID']))\n\n# Add information about quality Images\ntest = (pd.merge(test, image_quality_test,  how='left', left_on=['PetID'], right_on = ['PetID']))\n\n# Add info from pytorch\ntrain = (pd.merge(train, X_train_temp,  how='left', left_on=['PetID'], right_on = ['PetID']))\ntest = (pd.merge(test, X_test_temp,  how='left', left_on=['PetID'], right_on = ['PetID']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67a4ef6e5c84566da939ff24926ad3ad648dec92"},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a017a32f4c94a7cb75234fc7189b5cf332fd1d6e"},"cell_type":"markdown","source":"## Open Source (State Information)"},{"metadata":{"trusted":true,"_uuid":"1d7098d5fe1b0ca427b78d4048770e9abb0173c0"},"cell_type":"code","source":"## Using the Kernel:https://www.kaggle.com/bibek777/stacking-kernels\n## Using the Kernel:https://www.kaggle.com/bibek777/stacking-kernels\n\n# state GDP: https://en.wikipedia.org/wiki/List_of_Malaysian_states_by_GDP\nstate_gdp = {\n    41336: 116.679,\n    41325: 40.596,\n    41367: 23.02,\n    41401: 190.075,\n    41415: 5.984,\n    41324: 37.274,\n    41332: 42.389,\n    41335: 52.452,\n    41330: 67.629,\n    41380: 5.642,\n    41327: 81.284,\n    41345: 80.167,\n    41342: 121.414,\n    41326: 280.698,\n    41361: 32.270\n}\n\n# state population: https://en.wikipedia.org/wiki/Malaysia\nstate_population = {\n    41336: 33.48283,\n    41325: 19.47651,\n    41367: 15.39601,\n    41401: 16.74621,\n    41415: 0.86908,\n    41324: 8.21110,\n    41332: 10.21064,\n    41335: 15.00817,\n    41330: 23.52743,\n    41380: 2.31541,\n    41327: 15.61383,\n    41345: 32.06742,\n    41342: 24.71140,\n    41326: 54.62141,\n    41361: 10.35977\n}\n\nstate_area ={\n    41336:19102,\n41325:9500,\n41367:15099,\n41401:243,\n41415:91,\n41324:1664,\n41332:6686,\n41335:36137,\n41330:21035,\n41380:821,\n41327:1048,\n41345:73631,\n41342:124450,\n41326:8104,\n41361:13035}\n\n# https://www.dosm.gov.my/\n# Unemployment Rate in 2017\nstate_unemployment ={\n    41336 : 3.6,\n41325 :2.9,\n41367: 3.8,\n41324: 0.9,\n41332 : 2.7,\n41335: 2.6,\n41330: 3.4,\n41380: 2.9,\n41327: 2.1,\n41345 : 5.4,\n41342 : 3.3,\n41326: 3.2,\n41361: 4.2,\n41415: 7.8,\n41401: 3.3\n}\n# https://www.dosm.gov.my/\n# per 1000 population in 2016\nstate_birth_rate = {\n 41336:16.3,\n41325:17.0,\n41367:21.4,\n41401:14.4,\n41415:18.1,\n41324:16.0,\n41332:16.4,\n41335:17.0,\n41330:14.4,\n41380:17.5,\n41327:12.7,\n41345:13.7,\n41342:13.9,\n41326:16.6,\n41361:23.3,     \n}\n\ntrain[\"state_gdp\"] = train.State.map(state_gdp)\ntrain[\"state_population\"] = train.State.map(state_population)\ntrain[\"state_area\"] = train.State.map(state_area)\ntrain['state_unemployment']=train.State.map(state_unemployment)\ntrain['state_birth_rate']=train.State.map(state_birth_rate)\n\ntest[\"state_gdp\"] =test.State.map(state_gdp)\ntest[\"state_population\"] = test.State.map(state_population)\ntest[\"state_area\"] = test.State.map(state_area)\ntest['state_unemployment']=test.State.map(state_unemployment)\ntest['state_birth_rate']=test.State.map(state_birth_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ace1e577bfb34f2bdf322c61cdc7e72f5b3db76"},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d5419699409c7bdac47179adeea5ed3b9057bf1"},"cell_type":"markdown","source":"## Create Features based on Statistical Analysis"},{"metadata":{"trusted":true,"_uuid":"d52dd11185de46b78609f733e1ab56572ef326cb"},"cell_type":"code","source":"# Color (Create a Flag pet has 1 color, 2 colors, 3 colors)\ntrain['L_Color1'] = (pd.isnull(train['ColorName3']) & pd.isnull(train['ColorName2']) & pd.notnull(train['ColorName1'])).astype(int)\ntrain['L_Color2'] = (pd.isnull(train['ColorName3']) & pd.notnull(train['ColorName2']) & pd.notnull(train['ColorName1'])).astype(int)\ntrain['L_Color3'] = (pd.notnull(train['ColorName3']) & pd.notnull(train['ColorName2']) & pd.notnull(train['ColorName1'])).astype(int)\n\n# Breed (create a flag if the pet has 1 breed or 2)\ntrain['L_Breed1'] = (pd.isnull(train['BreedName2']) & pd.notnull(train['BreedName1'])).astype(int)\ntrain['L_Breed2'] = (pd.notnull(train['BreedName2']) & pd.notnull(train['BreedName1'])).astype(int)\n\n#Name (create a flag if the name is missing, with less than two letters)\ntrain['Name_Length']=train['Name'].str.len()\ntrain['L_Name_missing'] =  (pd.isnull(train['Name'])).astype(int)\n\n# Breed create columns\ntrain['L_Breed1_Siamese'] =(train['BreedName1']=='Siamese').astype(int)\ntrain['L_Breed1_Persian']=(train['BreedName1']=='Persian').astype(int)\ntrain['L_Breed1_Labrador_Retriever']=(train['BreedName1']=='Labrador Retriever').astype(int)\ntrain['L_Breed1_Terrier']=(train['BreedName1']=='Terrier').astype(int)\ntrain['L_Breed1_Golden_Retriever ']=(train['BreedName1']=='Golden Retriever').astype(int)\n\n#Description \ntrain['Description_Length']=train['Description'].str.len() \n\n# Fee Amount\ntrain['L_Fee_Free'] =  (train['Fee']==0).astype(int)\n\n#Add the Number of Pets per Rescuer \npets_total = train.groupby(['RescuerID']).size().reset_index(name='N_pets_total')\ntrain= pd.merge(train, pets_total, left_on='RescuerID', right_on='RescuerID', how='inner')\ntrain.count()\n\n# No photo\ntrain['L_NoPhoto'] =  (train['PhotoAmt']==0).astype(int)\n\n#No Video\ntrain['L_NoVideo'] =  (train['VideoAmt']==0).astype(int)\n\n#Log Age \ntrain['Log_Age']= np.log(train.Age+1) \n\n#Negative Score \ntrain['L_scoreneg'] =  (train['sentiment_document_score']<0).astype(int)\n\n#Quantity Amount >5\ntrain.loc[train['Quantity'] > 5, 'Quantity'] = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32fd675e1554e440d9e39c7de64b6ae84efb7764"},"cell_type":"code","source":"# Color (Create a Flag pet has 1 color, 2 colors, 3 colors)\ntest['L_Color1'] = (pd.isnull(test['ColorName3']) & pd.isnull(test['ColorName2']) & pd.notnull(test['ColorName1'])).astype(int)\ntest['L_Color2'] = (pd.isnull(test['ColorName3']) & pd.notnull(test['ColorName2']) & pd.notnull(test['ColorName1'])).astype(int)\ntest['L_Color3'] = (pd.notnull(test['ColorName3']) & pd.notnull(test['ColorName2']) & pd.notnull(test['ColorName1'])).astype(int)\n\n# Breed (create a flag if the pet has 1 breed or 2)\ntest['L_Breed1'] = (pd.isnull(test['BreedName2']) & pd.notnull(test['BreedName1'])).astype(int)\ntest['L_Breed2'] = (pd.notnull(test['BreedName2']) & pd.notnull(test['BreedName1'])).astype(int)\n\n#Name (create a flag if the name is missing, with less than two letters)\ntest['Name_Length']=test['Name'].str.len()\ntest['L_Name_missing'] =  (pd.isnull(test['Name'])).astype(int)\n\n# Breed create columns\ntest['L_Breed1_Siamese'] =(test['BreedName1']=='Siamese').astype(int)\ntest['L_Breed1_Persian']=(test['BreedName1']=='Persian').astype(int)\ntest['L_Breed1_Labrador_Retriever']=(test['BreedName1']=='Labrador Retriever').astype(int)\ntest['L_Breed1_Terrier']=(test['BreedName1']=='Terrier').astype(int)\ntest['L_Breed1_Golden_Retriever ']=(test['BreedName1']=='Golden Retriever').astype(int)\n\n#Description \ntest['Description_Length']=test['Description'].str.len() \n\n# Fee Amount\ntest['L_Fee_Free'] =  (test['Fee']==0).astype(int)\n\n#Add the Number of Pets per Rescuer \npets_total = test.groupby(['RescuerID']).size().reset_index(name='N_pets_total')\ntest= pd.merge(test, pets_total, left_on='RescuerID', right_on='RescuerID', how='inner')\ntest.count()\n\n# No photo\ntest['L_NoPhoto'] =  (test['PhotoAmt']==0).astype(int)\n\n#No Video\ntest['L_NoVideo'] =  (test['VideoAmt']==0).astype(int)\n\n#Log Age \ntest['Log_Age']= np.log(test.Age+1) \n\n#Negative Score \ntest['L_scoreneg'] =  (test['sentiment_document_score']<0).astype(int)\n\n#Quantity Amount >5\ntest.loc[train['Quantity'] > 5, 'Quantity'] = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f68f03b8817ca533f66b2ae17fa324cd76e01b8"},"cell_type":"code","source":"cat_human_age = {1:0.5, 2:3, 3:4, 4:6, 5:8, 6:10, 7:12, 8:14, 9:15, 10:16, 11:17, 12:18, 24:24, 48:35, 72:42, 96:50, 120:60, 144:70, 168:80, 192:84 }\nsmall_dog_human_age = {1:1, 2:2, 3:2.5, 4:3.5, 5:4.3, 6:5, 7:6.3, 8:7, 9:9, 10:11, 11:13, 12:15, 24:23, 48:32, 72:40, 96:48, 120:56, 144:64, 168:72, 192:80 }\nnormal_dog_human_age = {1:1, 2:2, 3:2.5, 4:3.5, 5:4.3, 6:5, 7:6.3, 8:7, 9:9, 10:11, 11:13, 12:15, 24:24, 48:34, 72:42, 96:51, 120:60, 144:69, 168:78, 192:87 }\nbig_dog_human_age = {1:1, 2:2, 3:2.5, 4:3.5, 5:4.3, 6:5, 7:6.3, 8:7, 9:9, 10:11, 11:13, 12:14, 24:22, 48:34, 72:45, 96:55, 120:66, 144:77, 168:88, 192:99 }\n\ndef human_age(row):\n    months = row['Age']\n    if months == 0:\n        return 0\n    if row['Type'] == 2:\n        if cat_human_age.get(months) is not None:\n            return cat_human_age.get(months)\n        else:\n            if months < 25:\n                return 25\n            else:\n                return (25 + ((months/12) - 2) * 4)\n    elif row['Type'] == 1 and row['MaturitySize'] == 1:\n        if small_dog_human_age.get(months) is not None:\n            return small_dog_human_age.get(months)\n        else:\n            if months < 24:\n                return (months/12) * 11\n            else:\n                return (22 + ((months/12) - 2) * 4)\n    elif row['Type'] == 1 and row['MaturitySize'] == 3:\n        if big_dog_human_age.get(months) is not None:\n            return big_dog_human_age.get(months)\n        else:\n            if months < 24:\n                return (months/12) * 11\n            else:\n                return (22 + ((months/12) - 2) * 4)\n    if normal_dog_human_age.get(months) is not None:\n        return normal_dog_human_age.get(months)\n    else:\n        if months < 24:\n            return (months/12) * 11\n        else:\n            return (22 + ((months/12) - 2) * 4)\n\ndef lifestage(row):\n    age = row['human_age']\n    if age < 10:\n        return 'Kitten/Puppy'\n    elif age < 25:\n        return 'Junior'\n    elif age < 40:\n        return 'Prime'\n    elif age < 60:\n        return 'Mature'\n    elif age < 74:\n        return 'Senior'\n    return 'Geriatic'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bdbcc1f2188f60d686b2d1b0e8993b4b705d0af"},"cell_type":"code","source":"train['human_age'] = train.apply(human_age, axis=1).apply(np.log1p)\ntrain['lifestage'] = train.apply(lifestage, axis=1)\n\nmapper = {'Kitten/Puppy':1, 'Junior':2, 'Prime':3, 'Mature':4,'Senior':5,'Geriatic':6}\ntrain.lifestage.replace(mapper, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57b8fa230f8a1ffc61308a24ba1a04875b823e0f"},"cell_type":"code","source":"test['human_age'] = test.apply(human_age, axis=1).apply(np.log1p)\ntest['lifestage'] = test.apply(lifestage, axis=1)\n\nmapper = {'Kitten/Puppy':1, 'Junior':2, 'Prime':3, 'Mature':4,'Senior':5,'Geriatic':6}\ntest.lifestage.replace(mapper, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86a6aeab7788245197e300573f21a69112f89bb2"},"cell_type":"code","source":"# defining a function which returns a list of top names\ndef top_names(df, top_percent):\n    df_withnames = df[df.has_name != 0]\n    items = df_withnames.shape[0]\n    top_names = []\n    counter = 0\n    for i,v in df_withnames.Name.value_counts().items():\n        if (counter/items)>top_percent:\n            break\n        top_names.append(i)\n        counter = counter + v  \n    return top_names\n\ntrain['has_name'] = train['Name'].apply(lambda x: 0 if x == 'No Name' or x == 'Unnamed' else 1)\ntopnames = top_names(train, 0.2)\ntrain['has_topname'] = train['Name'].apply(lambda row: 1 if row in topnames else 0)\n\ntest['has_name'] = test['Name'].apply(lambda x: 0 if x == 'No Name' or x == 'Unnamed' else 1)\ntopnames = top_names(test, 0.2)\ntest['has_topname'] = test['Name'].apply(lambda row: 1 if row in topnames else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18423fa99cf499f424d74bf1d99000b7e086cd50"},"cell_type":"code","source":"def mixed_breed(row):\n    if row['Breed1'] == 307:\n        return 1\n    elif row['Breed2'] == 0:\n        return 0 \n    elif row['Breed2'] != row['Breed1']:\n        return 1\n    else:\n        return 0\n\ntrain['mixed_breed'] = train.apply(mixed_breed, axis=1)\ntest['mixed_breed'] = test.apply(mixed_breed, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bab536cb1cf67bd15d2bc9d1876536631f74af3c"},"cell_type":"code","source":"rescuer_count = train.groupby(['RescuerID'])['PetID'].count().reset_index()\nrescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n\ntrain = train.merge(rescuer_count, how='left', on='RescuerID')\ntrain['RescuerID_COUNT_log'] = train.RescuerID_COUNT.apply(np.log1p)\n\n# now we have a number count for each rescuer. now we create bins \nbinner = KBinsDiscretizer(n_bins=10,encode='ordinal', strategy='kmeans')\ntrain['rescuer_bin_kmeans'] = pd.DataFrame(binner.fit_transform(train[['RescuerID_COUNT_log']].copy()))\n\n\nrescuer_count = test.groupby(['RescuerID'])['PetID'].count().reset_index()\nrescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n\ntest = test.merge(rescuer_count, how='left', on='RescuerID')\ntest['RescuerID_COUNT_log'] = test.RescuerID_COUNT.apply(np.log1p)\n\n# now we have a number count for each rescuer. now we create bins \nbinner = KBinsDiscretizer(n_bins=10,encode='ordinal', strategy='kmeans')\ntest['rescuer_bin_kmeans'] = pd.DataFrame(binner.fit_transform(test[['RescuerID_COUNT_log']].copy()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d55e5b2243f956480a0b9607ebc5d80449423d3"},"cell_type":"code","source":"# from: https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b\nfrom sklearn import base\n\nclass KFoldTargetEncoderTrain(base.BaseEstimator, base.TransformerMixin):\n\n    def __init__(self, colnames,targetName,n_fold=5,verbosity=True,discardOriginal_col=False):\n\n        self.colnames = colnames\n        self.targetName = targetName\n        self.n_fold = n_fold\n        self.verbosity = verbosity\n        self.discardOriginal_col = discardOriginal_col\n\n    def fit(self, X, y=None):\n        return self\n\n\n    def transform(self,X):\n\n        assert(type(self.targetName) == str)\n        assert(type(self.colnames) == str)\n        assert(self.colnames in X.columns)\n        assert(self.targetName in X.columns)\n\n        mean_of_target = X[self.targetName].mean()\n        kf = StratifiedKFold(n_splits=self.n_fold, shuffle=True)\n        \n        col_mean_name = self.colnames + '_' + 'Kfold_Target_Enc'\n        X[col_mean_name] = np.nan\n\n        for tr_ind, val_ind in kf.split(X,X[self.targetName]):\n            X_tr, X_val = X.iloc[tr_ind], X.iloc[val_ind]\n            #print(tr_ind,val_ind)\n            X.loc[X.index[val_ind], col_mean_name] = X_val[self.colnames].map(X_tr.groupby(self.colnames)[self.targetName].mean())\n\n        X[col_mean_name].fillna(mean_of_target, inplace = True)\n\n        if self.verbosity:\n\n            encoded_feature = X[col_mean_name].values\n            print('Correlation between the new feature, {} and, {} is {}.'.format(col_mean_name,\n                                                                                      self.targetName,\n                                                                                      np.corrcoef(X[self.targetName].values, encoded_feature)[0][1]))\n        if self.discardOriginal_col:\n            X = X.drop(self.targetName, axis=1)\n            \n        return X\n    \n    \n    \nclass KFoldTargetEncoderTest(base.BaseEstimator, base.TransformerMixin):\n    def __init__(self,train,colNames,encodedName):\n        self.train = train\n        self.colNames = colNames\n        self.encodedName = encodedName\n        \n    def fit(self, X, y=None):\n        return self\n\n    def transform(self,X):\n\n        mean = self.train[[self.colNames,self.encodedName]].groupby(self.colNames).mean().reset_index() \n        \n        dd = {}\n        for index, row in mean.iterrows():\n            dd[row[self.colNames]] = row[self.encodedName]\n\n        X[self.encodedName] = X[self.colNames]\n        X[self.encodedName] = X[self.encodedName].map(dd)\n\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e18d59c462ab981c307b0ae91c613869daed69b7"},"cell_type":"code","source":"def target_encode(X_train, X_test, columns):\n    for column in columns:\n        print(column)\n        # target encode\n        targetc = KFoldTargetEncoderTrain(column,'AdoptionSpeed',n_fold=5)\n        X_train = targetc.fit_transform(X_train)\n\n        test_targetc = KFoldTargetEncoderTest(X_train,column,column + '_Kfold_Target_Enc')\n        X_test= test_targetc.fit_transform(X_test)\n        \n        X_train[column].fillna(X_train.AdoptionSpeed.mean(), inplace=True)\n        X_test[column].fillna(X_train.AdoptionSpeed.mean(), inplace=True)\n    return X_train, X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4c2915486dea4ce5cd795ec53c7884a343a62fe"},"cell_type":"code","source":"target_encoded_columns = ['Breed1', 'Breed2','rescuer_bin_kmeans', 'State', 'Color1', 'Color2', 'Color3']\ntrain, test = target_encode(train, test,target_encoded_columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fd443da961d9f8061d53a28b32176a723de8583"},"cell_type":"markdown","source":"## Features Text Mining"},{"metadata":{"trusted":true,"_uuid":"03b72a2bf64bc2467e46ca7329ef1a521b28c932"},"cell_type":"code","source":"# Normalize the Variable Description\ntrain['Description'] =train['Description'].fillna(\"<MISSING>\")\ntrain['Description'] = train['Description'].str.replace('\\d+', '')\ntrain['Description'] = train['Description'].str.lower()\ntrain[\"Description\"] = train['Description'].str.replace('[^\\w\\s]','')\n\n# Stop Words \nfrom nltk.corpus import stopwords\n\nstop = stopwords.words('english')\npat = r'\\b(?:{})\\b'.format('|'.join(stop))\ntrain['Description'] = train['Description'].str.replace(pat, '')\ntrain['Description'] = train['Description'].str.replace(r'\\s+', ' ')\n\n# Stem Words\ntrain['Description'] = train['Description'].astype(str).str.split()\n\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nporter_stemmer = PorterStemmer()\ntrain['Description']=train['Description'].apply(lambda x : [porter_stemmer.stem(y) for y in x])\n\ntrain['Description']=train['Description'].apply(lambda x : \" \".join(x))\n\ndef get_top_n_words(corpus, n=None):\n    from sklearn.feature_extraction.text import CountVectorizer\n\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    test=pd.DataFrame(words_freq[:n], columns=['words','freq']) \n    \n    sns.barplot(x='words', y='freq', data=test)\n\nget_top_n_words(train['Description'],10)\n\nfrom sklearn.decomposition import TruncatedSVD, NMF\n# Matrix Factorization for dimensionality reduction\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nsvd_ = TruncatedSVD(\n    n_components=5, random_state=1337)\nnmf_ = NMF(\n    n_components=5, random_state=1337)\n\ntfidf = TfidfVectorizer(min_df=2,  max_features=None,\n                          strip_accents='unicode', analyzer='word', token_pattern=r'(?u)\\b\\w+\\b',\n                          ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1)\n\ntfidf_col = tfidf.fit_transform(train['Description'])\nsvd_col = svd_.fit_transform(tfidf_col)\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('SVD_')\n\nnmf_col = nmf_.fit_transform(tfidf_col)\nnmf_col = pd.DataFrame(nmf_col)\nnmf_col = nmf_col.add_prefix('NMF_')\n\n# Concatenate all dataframes\ntrain = pd.concat([train,nmf_col,svd_col],axis=1)\n\n\n# Normalize the Variable Description\ntest['Description'] =test['Description'].fillna(\"<MISSING>\")\ntest['Description'] = test['Description'].str.replace('\\d+', '')\ntest['Description'] = test['Description'].str.lower()\ntest[\"Description\"] = test['Description'].str.replace('[^\\w\\s]','')\n\n# Stop Words \nfrom nltk.corpus import stopwords\n\nstop = stopwords.words('english')\npat = r'\\b(?:{})\\b'.format('|'.join(stop))\ntest['Description'] = test['Description'].str.replace(pat, '')\ntest['Description'] = test['Description'].str.replace(r'\\s+', ' ')\n\n# Stem Words\ntest['Description'] = test['Description'].astype(str).str.split()\n\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nporter_stemmer = PorterStemmer()\ntest['Description']=test['Description'].apply(lambda x : [porter_stemmer.stem(y) for y in x])\n\ntest['Description']=test['Description'].apply(lambda x : \" \".join(x))\n\ndef get_top_n_words(corpus, n=None):\n    from sklearn.feature_extraction.text import CountVectorizer\n\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    test=pd.DataFrame(words_freq[:n], columns=['words','freq']) \n    \n    sns.barplot(x='words', y='freq', data=test)\n\nget_top_n_words(train['Description'],10)\n\nfrom sklearn.decomposition import TruncatedSVD, NMF\n# Matrix Factorization for dimensionality reduction\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\ntfidf_col = tfidf.transform(test['Description'])\nsvd_col = svd_.transform(tfidf_col)\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('SVD_')\n\nnmf_col = nmf_.transform(tfidf_col)\nnmf_col = pd.DataFrame(nmf_col)\nnmf_col = nmf_col.add_prefix('NMF_')\n\n# Concatenate all dataframes\ntest = pd.concat([test,nmf_col,svd_col],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78ea55234ef2ba97fc97c276958f293acb3c6ede"},"cell_type":"markdown","source":"## Impute Missing Values"},{"metadata":{"trusted":true,"_uuid":"26cc993ade5b02398339a93ce68e6466af9ed3cb"},"cell_type":"code","source":"train.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"174d4399a395814aeef2281a2e0b2d6dab134582"},"cell_type":"code","source":"# Cannot be used for this analysis (IDs, Texts...)\ntrain_analysis = train.drop([\"Name\",\"BreedName2\",\"ColorName3\",'Name','Breed1','Breed2','RescuerID','Description',\n                            'BreedName1','Color1','ColorName1', 'Color2','ColorName2','Color3','Age','State','ImageId'],axis=1)\n\n# Cannot be used for this analysis (IDs, Texts...)\ntest_analysis = test.drop([\"Name\",\"Description\",\"BreedName2\",\"ColorName3\",'Name','Breed1','Breed2','RescuerID','Description',\n                            'BreedName1','Color1', 'ColorName1', 'Color2','ColorName2', 'Color3','Age','State'],axis=1)\n\ntrain_analysis = train_analysis.fillna(train_analysis.median())\ntest_analysis = test_analysis.fillna(train_analysis.median())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b204491c30af338d84706303f2b32684bb3c60c"},"cell_type":"markdown","source":"## Categorical Encoding"},{"metadata":{"trusted":true,"_uuid":"a2fc9872208a533dce33dd8b8d88abacd9b958c6"},"cell_type":"code","source":"#Label Encoding Breed\n#One Hot Encoding: ColorName1,ColorName2,StateName\ntrain_analysis = pd.concat([train_analysis.drop('StateName', axis=1),pd.get_dummies(train_analysis['StateName'], prefix='State')], axis=1)\n\ncol=['Health', 'Gender', 'Dewormed','Type','MaturitySize', 'Sterilized','Vaccinated','FurLength']\nfor i in col:\n    train_analysis = pd.concat([train_analysis.drop(i, axis=1),pd.get_dummies(train_analysis[i], prefix=i)], axis=1)\n    #Label Encoding Breed\n#One Hot Encoding: ColorName1,ColorName2,StateName\ntest_analysis = pd.concat([test_analysis.drop('StateName', axis=1),pd.get_dummies(test_analysis['StateName'], prefix='State')], axis=1)\n\ncol=['Health', 'Gender', 'Dewormed','Type','MaturitySize', 'Sterilized','Vaccinated','FurLength']\nfor i in col:\n    test_analysis = pd.concat([test_analysis.drop(i, axis=1),pd.get_dummies(test_analysis[i], prefix=i)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"043c70301a30f9eebb1e9805aa1a07ea75e2dec6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"caa645e61ee069fea838e9e28605d56d43d3bb6b"},"cell_type":"markdown","source":"# Modelisation"},{"metadata":{"trusted":true,"_uuid":"4526d7f70ff12a45f95884a990bafc4df1e7c3b7"},"cell_type":"code","source":"# FROM: https://www.kaggle.com/myltykritik/simple-lgbm-image-features\n\n# The following 3 functions have been taken from Ben Hamner's github repository\n# https://github.com/benhamner/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    \n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              / num_scored_items)\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n\n    return (1.0 - numerator / denominator)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6843fcf15ee3efd184b4980a1f8faf1e413b5f0f"},"cell_type":"code","source":"def to_bins(x, borders):\n    for i in range(len(borders)):\n        if x <= borders[i]:\n            return i\n    return len(borders)\n\nclass OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _loss(self, coef, X, y, idx):\n        X_p = np.array([to_bins(pred, coef) for pred in X])\n        ll = -quadratic_weighted_kappa(y, X_p)\n        return ll\n\n    def fit(self, X, y):\n        coef = [1.5, 2.0, 2.5, 3.0]\n        golden1 = 0.618\n        golden2 = 1 - golden1\n        ab_start = [(1, 2), (1.5, 2.5), (2, 3), (2.5, 3.5)]\n        for it1 in range(10):\n            for idx in range(4):\n                # golden section search\n                a, b = ab_start[idx]\n                # calc losses\n                coef[idx] = a\n                la = self._loss(coef, X, y, idx)\n                coef[idx] = b\n                lb = self._loss(coef, X, y, idx)\n                for it in range(20):\n                    # choose value\n                    if la > lb:\n                        a = b - (b - a) * golden1\n                        coef[idx] = a\n                        la = self._loss(coef, X, y, idx)\n                    else:\n                        b = b - (b - a) * golden2\n                        coef[idx] = b\n                        lb = self._loss(coef, X, y, idx)\n        self.coef_ = {'x': coef}\n\n    def predict(self, X, coef):\n        X_p = np.array([to_bins(pred, coef) for pred in X])\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x'] \n    \ndef rmse(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a3098cdca39418905b38c6a6aef4f8a43b1a833"},"cell_type":"markdown","source":"## Performance of the model"},{"metadata":{"trusted":true,"_uuid":"8e1429691d5aa771b99b201b69ca97edce72da76"},"cell_type":"code","source":"def evaluate(y_pred, y_true):\n  \n    cohen_kappa= cohen_kappa_score(y_true, y_pred)\n    accuracy=accuracy_score(y_true,y_pred)\n    f1=f1_score(y_true,y_pred,average='micro')\n    classification=classification_report(y_true,y_pred)\n    \n    #Confusion Matrix\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(20,6))\n    \n    sns.heatmap(cm, annot=True)\n    plt.title('Confusion matrix')\n    plt.figure(figsize = (5,4))\n    plt.show()\n    #Evaluation Metrics\n    print('Cohen Kappa: {:0.2f}.'.format(cohen_kappa))\n    print('Accuracy Score: {:0.2f}%.'.format(accuracy))\n    print('F1 Score: {:0.2f}%.'.format(f1))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f2d053446132fec24176c9b54a47ccfcd793bf8"},"cell_type":"code","source":"#Extracting Features and Output\nids=train_analysis[['PetID']]\ntrain_analysis=train_analysis.drop(['PetID'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e969c372aec762b4ccd1353f9454448effe1955"},"cell_type":"code","source":"X, y = train_analysis.loc[:, train_analysis.columns != 'AdoptionSpeed'], train_analysis['AdoptionSpeed']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2fc888b5d07611aa12d622c7d5b686d4bda9d7e"},"cell_type":"code","source":"model = lgb.LGBMRegressor()\nmodel.fit(X_train, y_train)\n\nfeature_imp = pd.DataFrame(sorted(zip(model.feature_importances_,X_train.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(10, 17))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\n\nfeatures_selection = SelectFromModel(model, threshold='1.25*median') # The Threshold is the median of features importance*1.25 \nfeatures_selection.fit(X_train, y_train)\n\nfeatures_selection_support = features_selection.get_support()\nfeatures_selection = X_train.loc[:,features_selection_support].columns.tolist()\nlen(features_selection)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"740da49f582d1acd650f84361245f1a9fc9a9610"},"cell_type":"code","source":"X_train =X_train.loc[:,features_selection]\nX_test = X_test.loc[:,features_selection]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b031bf4fdad5933f247bd201cc1e62f9b57f9b5b"},"cell_type":"markdown","source":"## Oversampling"},{"metadata":{"trusted":true,"_uuid":"a0823339144a3e7d42ed520eb1857181d1e48c3f"},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\ntrain_columns = X_train.columns\ntest_columns = X_test.columns\n\nmy_imputer = SimpleImputer()\nX_train = pd.DataFrame(my_imputer.fit_transform(X_train))\nX_train.columns = train_columns\n\nX_test = pd.DataFrame(my_imputer.transform(X_test))\nX_test.columns = test_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37c014d06d435cfaa5aebc51b4e4bb474ca5ca3b"},"cell_type":"code","source":"from collections import Counter\n\n#Let us try some sampling technique to remove class imbalance\nfrom imblearn.over_sampling import SMOTE\n#Over-sampling: SMOTE\n#SMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, \n#based on those that already exist. It works randomly picking a point from the minority class and computing \n#the k-nearest neighbors for this point.The synthetic points are added between the chosen point and its neighbors.\n#We'll use ratio='minority' to resample the minority class.\nsmote = SMOTE('minority')\n\nprint('Original dataset shape %s' % Counter(y_train))\nX_res, y_res = smote.fit_sample(X_train,y_train)\nprint('Resampled dataset shape %s' % Counter(y_res))\n\nX_res_df = pd.DataFrame(X_res)\nX_res_df.columns = train_columns\n\ny_res_df = pd.DataFrame(y_res)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f22bf2dcf2101de0b16a144a0d849899fffa3d90"},"cell_type":"markdown","source":"## Cross Validation"},{"metadata":{"trusted":true,"_uuid":"d63028a5ee1cdd3636a56a80c39fa757f23cc263"},"cell_type":"code","source":"def cross_val(model,X_train,y_train):\n    X = X_train\n    y = y_train\n    coeff = np.empty((1,4))\n    cv_scores=[]\n    fold=1\n    skf = StratifiedKFold(n_splits=5,shuffle=True)\n    for train_index, val_index in skf.split(X, y):\n        xtrain, xvalid = X[train_index], X[val_index]\n        ytrain, yvalid = y[train_index], y[val_index]\n\n        model.fit(\n            xtrain, ytrain,\n            eval_set=[(xvalid, yvalid)],\n            verbose=100,\n            early_stopping_rounds=100\n        )\n\n        valid_preds = model.predict(xvalid, num_iteration=model.best_iteration_)\n        yvalid = np.array(yvalid).tolist()\n        optR = OptimizedRounder()\n        optR.fit(valid_preds, yvalid)\n\n        coefficients = optR.coefficients()\n        valid_p = optR.predict(valid_preds, coefficients)\n\n        scr = quadratic_weighted_kappa(yvalid, valid_p)\n        cv_scores.append(scr)\n\n        print(\"QWK = {}. Coef = {}\".format(scr, coefficients))\n        #coefficients.reshape((4, 1))\n\n        coeff = np.vstack([coeff, coefficients])\n        fold += 1\n\n\n    coeff = np.delete(coeff, (0), axis=0)\n    global coefficient_mean\n    coefficient_mean = coeff.mean(axis=0)\n    print(\"Coef Mean ={}\".format(coefficient_mean))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b90863c6eef521b6e69c6000a3abe280b773153f"},"cell_type":"code","source":"lgb_params = {\n'boosting_type': 'gbdt',\n'objective': 'regression',\n'learning_rate': 0.005,\n'subsample': .8,\n'colsample_bytree': 0.8,\n'min_split_gain': 0.006,\n'min_child_samples': 150,\n'min_child_weight': 0.1,\n'n_estimators': 1000,\n'num_leaves': 80,\n'silent': -1,\n'verbose': -1,\n'max_depth': 11,\n'random_state': 2018\n}\n    \nlgb_model = lgb.LGBMRegressor(**lgb_params)\n\ncross_val(lgb_model,X_res,y_res)\n\n#Prediction\n#best iteration is used\ny_pred=lgb_model.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"542c99207f206f4d6b93fb843fbd5f299355c171"},"cell_type":"code","source":"qwk = quadratic_weighted_kappa(y_train, y_pred)\nprint(\"QWK = \", qwk)\n\noptR=OptimizedRounder()\npredictions = optR.predict(y_pred, coefficient_mean).astype(int)\nqwk = quadratic_weighted_kappa(y_train, predictions)\nprint(\"QWK = \", qwk)\n\n#predict test set\nids=test[['PetID']]\ntest_features=test.drop(['PetID'],axis=1)\ntest_features =test_features.loc[:,features_selection]\npred1 = lgb_model.predict(test_features.values)\npred1 = optR.predict(pred1, coefficient_mean).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9242bc2b37fb274c51dce5fe46ac0f7074da12aa"},"cell_type":"markdown","source":"## LightGBM: Optimize the boundaries"},{"metadata":{"_uuid":"56e3a05ea27496293a64df1495dd382d64300dd6"},"cell_type":"markdown","source":"## LightGBM 2"},{"metadata":{"trusted":true,"_uuid":"9fcbe90b1b8ceaafc3cc882ea00f2e9f93439566"},"cell_type":"code","source":"lgb_params = {\n'boosting_type': 'gbdt',\n'objective': 'regression',\n'learning_rate': 0.005,\n'subsample': .8,\n'colsample_bytree': 0.8,\n'min_split_gain': 0.006,\n'min_child_samples': 150,\n'min_child_weight': 0.1,\n'n_estimators': 1000,\n'num_leaves': 80,\n'silent': -1,\n'verbose': -1,\n'max_depth': 11,\n'random_state': 2001\n}\nlgb_model2 = lgb.LGBMRegressor(**lgb_params)\n\ncross_val(lgb_model2,X_res,y_res)\n\n#Prediction\ny_pred=lgb_model2.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdde36773889f1dd24cc871d37f76e3b61abcaa5"},"cell_type":"code","source":"#cohen_kappa_score(y_train, y_pred)\nqwk = quadratic_weighted_kappa(y_train, y_pred)\nprint(\"QWK = \", qwk)\n\noptR=OptimizedRounder()\npredictions = optR.predict(y_pred, coefficient_mean).astype(int)\nqwk = quadratic_weighted_kappa(y_train, predictions)\nprint(\"QWK = \", qwk)\n\n#predict test set\npred2 = lgb_model2.predict(test_features.values)\npred2 = optR.predict(pred2, coefficient_mean).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bf19c5875cca703f6409e1e0abbcbd2f2f59a96"},"cell_type":"markdown","source":"## LightGBM 3"},{"metadata":{"trusted":true,"_uuid":"78e6962e5b56e3fbf43f0787505af016a447a665"},"cell_type":"code","source":"lgb_params = {\n'boosting_type': 'gbdt',\n'objective': 'regression',\n'learning_rate': 0.005,\n'subsample': .8,\n'colsample_bytree': 0.8,\n'min_split_gain': 0.006,\n'min_child_samples': 150,\n'min_child_weight': 0.1,\n'n_estimators': 1000,\n'num_leaves': 80,\n'silent': -1,\n'verbose': -1,\n'max_depth': 11,\n'random_state': 2000\n}\n   \nlgb_model3 = lgb.LGBMRegressor(**lgb_params)\n\ncross_val(lgb_model3,X_res,y_res)\n\n#Prediction\ny_pred=lgb_model3.predict(X_train.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adb80018030993da13b531bce199eb7b1aeef1ce"},"cell_type":"code","source":"qwk = quadratic_weighted_kappa(y_train, y_pred)\nprint(\"QWK = \", qwk)\n\noptR=OptimizedRounder()\npredictions = optR.predict(y_pred, coefficient_mean).astype(int)\nqwk = quadratic_weighted_kappa(y_train, predictions)\nprint(\"QWK = \", qwk)\n\n#predict test set\npred3 = lgb_model3.predict(test_features.values)\npred3 = optR.predict(pred3, coefficient_mean).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6c482a47c3154b93421f513157c6a115f7600a1"},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\n\nxgb_params = {\n    'eval_metric': 'rmse',\n    'silent': 1,\n}\n\ndef run_xgb(params, X_train, y_train, X_test):\n    n_splits = 5\n    verbose_eval = 1000\n    num_rounds = 50000\n    early_stop = 1000\n\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n    \n    X = X_train\n    y = y_train\n    coeff = np.empty((1,4))\n    cv_scores=[]\n    fold=0\n    skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n\n    for train_index, val_index in skf.split(X, y):\n        xtrain, xvalid = X.iloc[train_index, :], X.iloc[val_index,:]\n        ytrain, yvalid = y.iloc[train_index,:], y.iloc[val_index,:]\n                \n        d_train = xgb.DMatrix(data=xtrain, label=ytrain, feature_names=X.columns)\n        d_valid = xgb.DMatrix(data=xvalid, label=yvalid, feature_names=X.columns)\n\n        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n        model = xgb.train(dtrain=d_train, num_boost_round=num_rounds, evals=watchlist,\n                         early_stopping_rounds=early_stop, verbose_eval=verbose_eval, params=params)\n\n        valid_pred = model.predict(xgb.DMatrix(xvalid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        yvalid = yvalid.values.reshape([-1])\n        optR = OptimizedRounder()                \n        optR.fit(valid_pred, yvalid)\n\n        coefficients = optR.coefficients()\n        valid_p = optR.predict(valid_pred, coefficients)\n\n        scr = quadratic_weighted_kappa(yvalid, valid_p)\n        cv_scores.append(scr)\n\n        print(\"QWK = {}. Coef = {}\".format(scr, coefficients))\n        #coefficients.reshape((4, 1))\n        \n        test_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        oof_test[:, fold] = test_pred\n        coeff = np.vstack([coeff, coefficients])\n        fold += 1\n\n    coeff = np.delete(coeff, (0), axis=0)\n    global coefficient_mean\n    coefficient_mean = coeff.mean(axis=0)\n    print(\"Coef Mean ={}\".format(coefficient_mean))\n    return oof_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ded4fe9de6087388281d4198e39f1694f6b3d8cd"},"cell_type":"code","source":"oof_test = run_xgb(xgb_params,X_res_df,y_res_df, test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b93b39fd0bbb58b7727e716ca8d399c8a2f649d0"},"cell_type":"code","source":"optR=OptimizedRounder()\npredictions = optR.predict(oof_test.mean(axis=1), coefficient_mean).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1a89edc0919cc36820f3c7f53ed0394b11fc405"},"cell_type":"code","source":"#create our on voting classifier, inputting our models\n# inspired by the voting Classfier with voting='soft', weights=[3,1,1,1])\n\n#score with mean from all 4 values: 0.35\n#score with weights 0.26\n#score with majority vote 0.29\ndef vote(row):\n    if row[1] == row[2] and row[2] == row[3]:\n        return row[1]\n    return row[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5492bb1d954dd48fc39e9a1870b22d3ce811a88"},"cell_type":"code","source":"# majority vote\nfrom collections import Counter\n\ndef majority_vote(row):\n    c = Counter(row)\n    value, count =  c.most_common()[0]\n    return value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76ac2140dc50ab312bb62b18e4227a85e66bba84"},"cell_type":"code","source":"combination = pd.concat([pd.DataFrame(predictions), pd.DataFrame(pred1),pd.DataFrame(pred2),pd.DataFrame(pred3)],axis=1,ignore_index=True)\ncombination['predn']=combination.apply(majority_vote, axis=1)\n\npredictions = combination['predn']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b86a877192e7e9a8898573e8a41c15125e48fc3"},"cell_type":"code","source":"submission = pd.DataFrame({'PetID': ids['PetID'].values, 'AdoptionSpeed': predictions.astype(np.int32)})\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission.head(20)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}