{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder,OrdinalEncoder, StandardScaler,KBinsDiscretizer\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\n# Evaluation\nfrom sklearn.metrics import cohen_kappa_score,make_scorer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.simplefilter(action=\"ignore\", category=ConvergenceWarning)\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time \n\n#let's also import the abstract base class for our callback\nfrom tensorflow.keras.callbacks import Callback\n\n#defining the callback\nclass TimerCallback(Callback):\n    \n    def __init__(self, maxExecutionTime):\n        \n# Arguments:\n#     maxExecutionTime (number): Time in minutes. The model will keep training \n#                                until shortly before this limit\n#                                (If you need safety, provide a time with a certain tolerance)\n        \n        self.maxExecutionTime = maxExecutionTime * 60\n    \n    \n    #Keras will call this when training begins\n    def on_train_begin(self, logs):\n        self.startTime = time.time()\n        self.longestTime = 0            #time taken by the longest epoch or batch\n        self.lastTime = self.startTime  #time when the last trained epoch or batch was finished\n\n    #this is our custom handler that will be used in place of the keras methods:\n        #`on_batch_end(batch,logs)` or `on_epoch_end(epoch,logs)`\n    def on_epoch_end(self, epoch, logs):\n        \n        currentTime      = time.time()                           \n        self.elapsedTime = currentTime - self.startTime    #total time taken until now\n        thisTime         = currentTime - self.lastTime     #time taken for the current epoch\n                                                               #or batch to finish\n        \n        self.lastTime = currentTime\n        \n        #verifications will be made based on the longest epoch or batch\n        if thisTime > self.longestTime:\n            self.longestTime = thisTime\n        \n        \n        #if the (assumed) time taken by the next epoch or batch is greater than the\n            #remaining time, stop training\n        remainingTime = self.maxExecutionTime - self.elapsedTime\n        if remainingTime < self.longestTime:\n            \n            self.model.stop_training = True  #this tells Keras to not continue training\n            print(\"\\n\\nTimerCallback: Finishing model training before it takes too much time. (Elapsed time: \" + str(self.elapsedTime/60.) + \" minutes )\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/petfinder-images-train-test-valid-realtest/images_train_test_valid_realtest/images_train_test_valid_realtest/train_df.csv\")\nvalid_df = pd.read_csv(\"../input/petfinder-images-train-test-valid-realtest/images_train_test_valid_realtest/images_train_test_valid_realtest/val_df.csv\")\ntrain_data_dir = '../input/petfinder-images-train-test-valid-realtest/images_train_test_valid_realtest/images_train_test_valid_realtest/train'\nvalid_data_dir = '../input/petfinder-images-train-test-valid-realtest/images_train_test_valid_realtest/images_train_test_valid_realtest/valid'\n\ntest_df = pd.read_csv(\"../input/petfinder-images-train-test-valid-realtest/images_train_test_valid_realtest/images_train_test_valid_realtest/test_df.csv\")\ntest_data_dir = '../input/petfinder-images-train-test-valid-realtest/images_train_test_valid_realtest/images_train_test_valid_realtest/test'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications import NASNetLarge\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.optimizers import Adam, Nadam\n\nnum_classes = 5\nmodel = NASNetLarge(weights='imagenet', include_top=False, pooling='avg')\n\nmy_new_model = Sequential()\nmy_new_model.add(model)\nmy_new_model.add(Dense(512, activation=\"relu\"))\nmy_new_model.add(Dropout(rate=0.25))\nmy_new_model.add(Dense(256, activation=\"relu\"))\nmy_new_model.add(Dropout(rate=0.25))                 \nmy_new_model.add(Dense(num_classes, activation='softmax'))\n\n# Say not to train first layer (ResNet) model. It is already trained\nmy_new_model.layers[0].trainable = False\n\n# optimizer\n# descent optimizer (adam lr defaut = 0.001)\nmy_new_model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n\nprint(my_new_model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications.nasnet import preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import Callback\n\nimage_size = 331\nbatch_size = 64\nnb_epochs = 100\n\n# steps_per_epoch: number of yields (batches) before a epoch is over\n# ceil(num_samples / batch_size)\n# epochs: Number of epochs to train the model. An epoch is an iteration over the entire data provided\n# class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). \n#  This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class.\n\nshift = 0.2\n\n#brightness_range=[0.8,1.2],\n#zoom_range=[0.8,1.2],\ntrain_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,zoom_range=[0.9,1.1],\n                                   width_shift_range=shift, height_shift_range=shift, \n                                   horizontal_flip=True)\n\n#liefert die trainingsdaten als iterator\n# image aug funktioniert so, dass f체r den aktuellen batch die bilder ge채ndert werden. Nicht dass es mehr Bilder gibt.\n# Somit wird f체r jede Epoche mit anderen Bildenr trainiert.\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(image_size, image_size),\n    batch_size=batch_size,\n    class_mode='categorical')\n\n# keine image augmenation f체r validierung\ntest_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n\n#liefert die Validationdaten als iterator\nvalidation_generator = test_datagen.flow_from_directory(\n    valid_data_dir,\n    target_size=(image_size, image_size),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nSTEP_SIZE_TRAIN=math.ceil(train_generator.n//train_generator.batch_size)\nSTEP_SIZE_VALID=math.ceil(validation_generator.n//validation_generator.batch_size)\n\nprint(STEP_SIZE_TRAIN)\nprint(STEP_SIZE_VALID)\n\n\n# Configure the TensorBoard callback and fit the model\ntensorboard_callback = TensorBoard(\"logs\")\n\n# Early stopping against overfit\nearlystopping_callback = EarlyStopping(patience=batch_size/10, monitor='val_acc', mode='auto', restore_best_weights=True)\n        \n# save best model\nmc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='auto', save_best_only=True)\n\nclass_weights = {0: 8.80595533,\n                1: 1.01597481,\n                2: 0.72513282,\n                3: 0.74640867,\n                4: 0.84505298}\n\n\n#import multiprocessing\n#multiprocessing.cpu_count()\n\ntimerCallback = TimerCallback(500)\n\nhistory = my_new_model.fit_generator(\n    train_generator,\n    steps_per_epoch = STEP_SIZE_TRAIN,\n    validation_data = validation_generator, \n    validation_steps = STEP_SIZE_VALID,\n    epochs = nb_epochs,\n    class_weight = class_weights,\n    callbacks=[mc,timerCallback],\n    workers = 2,\n    use_multiprocessing = False,\n    max_queue_size = 40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef plot_training(history):\n    # Plot training & validation accuracy values\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    # Plot training & validation loss values\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    plt.savefig('acc_vs_epochs.png')\n    \nplot_training(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.applications.nasnet import preprocess_input, decode_predictions\n\nsaved_model = load_model('../working/best_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import Callback\n\nimage_size = 331\nbatch_size = 64\n\ntrain_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n\n#liefert die traindaten als iterator\ntest_generator = train_datagen.flow_from_directory(\n    test_data_dir,\n    target_size=(image_size, image_size),\n    batch_size=batch_size,\n    class_mode=None,\n    shuffle=False)\n\ntest_generator.reset()\n\ntest_preds = saved_model.predict_generator(test_generator)\ntest_results=pd.DataFrame({\"Filename\":test_generator.filenames,\n                      \"Predictions\":test_preds.argmax(axis=-1)})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_results.Predictions.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_all_speed_from_predictions(df, PetID):\n    values = df[df['Filename'].str.contains(PetID)]['Predictions'].values\n    preds = values.tolist()\n    mean = values.mean()\n    if math.isnan(mean):\n        mean = 4 # if no photo is available, default is Speed 4\n    else: \n        mean = int(round(mean, 0))\n    return pd.Series([preds.count(0),preds.count(1), preds.count(2),preds.count(3),preds.count(4), mean], \n                     index =['AdaptionSpeed0', 'AdaptionSpeed1', 'AdaptionSpeed2', 'AdaptionSpeed3', 'AdaptionSpeed4', 'mean']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_result_speed=pd.DataFrame()\ntest_result_speed['PetID']=test_df['PetID']\ntest_result_speed['AdoptionSpeed']=test_df['AdoptionSpeed']\ntest_result_speed = test_result_speed.merge(test_result_speed.PetID.apply(lambda x: get_all_speed_from_predictions(test_results, x)), \n    left_index=True, right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_result_speed.AdoptionSpeed.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    #xgboost: \n    XGBClassifier(),\n    \n    CatBoostClassifier(verbose=0)\n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(data, MLA_list = MLA):\n    \n    target = data['AdoptionSpeed']\n    X_train = data.drop(['AdoptionSpeed'],axis=1)\n    \n    MLA_columns = ['MLA Name', 'MLA Parameters','MLA cohen_kappa_score','MLA Time']\n    MLA_compare = pd.DataFrame(columns = MLA_columns)\n\n    MLA_predict = data['AdoptionSpeed']\n    \n    row_index = 0\n    for alg in MLA_list:\n\n        MLA_name = alg.__class__.__name__\n        MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n        MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n        kf = StratifiedKFold(n_splits=10, shuffle=True)\n        kappa_score = make_scorer(cohen_kappa_score, weights='quadratic')\n        cv_results = model_selection.cross_validate(alg, X_train, target, cv  = kf, scoring=kappa_score )\n        \n        MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n        MLA_compare.loc[row_index, 'MLA cohen_kappa_score'] = cv_results['test_score'].mean() \n             \n        #MLA_predict[MLA_name] = alg.predict(X_train)\n        row_index+=1\n\n    MLA_compare.sort_values(by = ['MLA cohen_kappa_score'], ascending = False, inplace = True)\n    sns.barplot(x='MLA cohen_kappa_score', y = 'MLA Name', data = MLA_compare, color = 'b')\n    plt.title('Machine Learning Algorithm Accuracy Score \\n')\n    plt.xlabel('Accuracy Score (%)')\n    plt.ylabel('Algorithm')\n    \n    return MLA_compare","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictAndTargetColumns = ['AdoptionSpeed', 'AdaptionSpeed0', 'AdaptionSpeed1', 'AdaptionSpeed2', 'AdaptionSpeed3', 'AdaptionSpeed4', 'mean']\n\nclassifier_performance = train_model(test_result_speed[predictAndTargetColumns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_performance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = test_result_speed['AdoptionSpeed']\nX_test = test_result_speed['mean']\n\ncohen_kappa_score(X_test, target, weights='quadratic')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"kf = StratifiedKFold(n_splits=10, shuffle=True)\nkappa_score = make_scorer(cohen_kappa_score, weights='quadratic')\n\nclf = XGBClassifier()\n  \nparameters = {\n    \"learning_rate\"    : [0.05, 0.15, 0.25, 0.30 ] ,\n     \"max_depth\"        : [ 3,  5,  8,  12, 15],\n     \"min_child_weight\" : [  3,  7 ],\n     \"gamma\"            : [ 0.0, 0.2, 0.4 ],\n     \"colsample_bytree\" : [ 0.3,  0.5 , 0.7 ] }\n\nclf = GridSearchCV(clf, parameters, n_jobs=-1, cv  = kf, scoring=kappa_score)\n\ntarget = test_result_speed['AdoptionSpeed']\nX_train = test_result_speed[useColumns].drop(['AdoptionSpeed'],axis=1)\n    \nclf.fit(X_train, target)\nprint(clf.best_score_)\nclf.best_params_"},{"metadata":{"trusted":true},"cell_type":"code","source":"useColumns = ['AdaptionSpeed0', 'AdaptionSpeed1', 'AdaptionSpeed2', 'AdaptionSpeed3', 'AdaptionSpeed4', 'mean']\nX_test = test_result_speed[useColumns]\n\n#classifier = CatBoostClassifier(verbose=0)\n# train the model\n#classifier.fit(X_test, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"{'colsample_bytree': 0.3,\n 'gamma': 0.4,\n 'learning_rate': 0.3,\n 'max_depth': 5,\n 'min_child_weight': 3}\n\n\nclassifier =XGBClassifier().fit(X_test, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"realtest_df = pd.read_csv('../input/petfinder-images-train-test-valid-realtest/images_train_test_valid_realtest/images_train_test_valid_realtest/realtest/test.csv')\nrealtest_data_dir = '../input/petfinder-images-train-test-valid-realtest/images_train_test_valid_realtest/images_train_test_valid_realtest/realtest'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"realtest_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"realtest_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n\n#liefert die testdaten als iterator\nrealtest_generator = realtest_datagen.flow_from_directory(\n    realtest_data_dir, \n    target_size=(image_size, image_size),\n    batch_size=batch_size,\n    class_mode=None,\n    shuffle=False)\n\nrealtest_generator.reset()\n\nrealtest_preds = saved_model.predict_generator(realtest_generator)\nrealtest_results=pd.DataFrame({\"Filename\":realtest_generator.filenames,\n                      \"Predictions\":realtest_preds.argmax(axis=-1)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"realtest_result_speed=pd.DataFrame()\nrealtest_result_speed['PetID']=realtest_df['PetID']\nrealtest_result_speed = realtest_result_speed.merge(realtest_result_speed.PetID.apply(lambda x: get_all_speed_from_predictions(realtest_results, x)), \n    left_index=True, right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"useColumns = ['AdaptionSpeed0', 'AdaptionSpeed1', 'AdaptionSpeed2', 'AdaptionSpeed3', 'AdaptionSpeed4', 'mean']\n\nsubmit=pd.DataFrame()\nsubmit['PetID']=realtest_result_speed['PetID']\nsubmit['AdoptionSpeed']=classifier.predict(realtest_result_speed[useColumns])\nsubmit['AdoptionSpeed']=submit['AdoptionSpeed'].astype(int)\nsubmit.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}