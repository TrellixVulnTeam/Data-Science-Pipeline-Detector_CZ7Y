{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time \n\n#let's also import the abstract base class for our callback\nfrom tensorflow.keras.callbacks import Callback\n\n#defining the callback\nclass TimerCallback(Callback):\n    \n    def __init__(self, maxExecutionTime):\n        \n# Arguments:\n#     maxExecutionTime (number): Time in minutes. The model will keep training \n#                                until shortly before this limit\n#                                (If you need safety, provide a time with a certain tolerance)\n        \n        self.maxExecutionTime = maxExecutionTime * 60\n    \n    \n    #Keras will call this when training begins\n    def on_train_begin(self, logs):\n        self.startTime = time.time()\n        self.longestTime = 0            #time taken by the longest epoch or batch\n        self.lastTime = self.startTime  #time when the last trained epoch or batch was finished\n\n    #this is our custom handler that will be used in place of the keras methods:\n        #`on_batch_end(batch,logs)` or `on_epoch_end(epoch,logs)`\n    def on_epoch_end(self, epoch, logs):\n        \n        currentTime      = time.time()                           \n        self.elapsedTime = currentTime - self.startTime    #total time taken until now\n        thisTime         = currentTime - self.lastTime     #time taken for the current epoch\n                                                               #or batch to finish\n        \n        self.lastTime = currentTime\n        \n        #verifications will be made based on the longest epoch or batch\n        if thisTime > self.longestTime:\n            self.longestTime = thisTime\n        \n        \n        #if the (assumed) time taken by the next epoch or batch is greater than the\n            #remaining time, stop training\n        remainingTime = self.maxExecutionTime - self.elapsedTime\n        if remainingTime < self.longestTime:\n            \n            self.model.stop_training = True  #this tells Keras to not continue training\n            print(\"\\n\\nTimerCallback: Finishing model training before it takes too much time. (Elapsed time: \" + str(self.elapsedTime/60.) + \" minutes )\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from https://www.kaggle.com/christofhenkel/weighted-kappa-loss-for-keras-tensorflow\n\nimport tensorflow as tf\n\ndef kappa_loss(y_pred, y_true, y_pow=2, eps=1e-10, N=5, bsize=32, name='kappa_loss'):\n    \"\"\"A continuous differentiable approximation of discrete kappa loss.\n        Args:\n            y_pred: 2D tensor or array, [batch_size, num_classes]\n            y_true: 2D tensor or array,[batch_size, num_classes]\n            y_pow: int,  e.g. y_pow=2\n            N: typically num_classes of the model\n            bsize: batch_size of the training or validation ops\n            eps: a float, prevents divide by zero\n            name: Optional scope/name for op_scope.\n        Returns:\n            A tensor with the kappa loss.\"\"\"\n\n    with tf.name_scope(name):\n        y_true = tf.to_float(y_true)\n        repeat_op = tf.to_float(tf.tile(tf.reshape(tf.range(0, N), [N, 1]), [1, N]))\n        repeat_op_sq = tf.square((repeat_op - tf.transpose(repeat_op)))\n        weights = repeat_op_sq / tf.to_float((N - 1) ** 2)\n    \n        pred_ = y_pred ** y_pow\n        try:\n            pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [-1, 1]))\n        except Exception:\n            pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [bsize, 1]))\n    \n        hist_rater_a = tf.reduce_sum(pred_norm, 0)\n        hist_rater_b = tf.reduce_sum(y_true, 0)\n    \n        conf_mat = tf.matmul(tf.transpose(pred_norm), y_true)\n    \n        nom = tf.reduce_sum(weights * conf_mat)\n        denom = tf.reduce_sum(weights * tf.matmul(\n            tf.reshape(hist_rater_a, [N, 1]), tf.reshape(hist_rater_b, [1, N])) /\n                              tf.to_float(bsize))\n    \n        return nom / (denom + eps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def kappa_metric(y_pred, y_true, y_pow=2, eps=1e-10, N=5, bsize=32, name='kappa_metric'):\n    \"\"\"A continuous differentiable approximation of discrete kappa loss.\n        Args:\n            y_pred: 2D tensor or array, [batch_size, num_classes]\n            y_true: 2D tensor or array,[batch_size, num_classes]\n            y_pow: int,  e.g. y_pow=2\n            N: typically num_classes of the model\n            bsize: batch_size of the training or validation ops\n            eps: a float, prevents divide by zero\n            name: Optional scope/name for op_scope.\n        Returns:\n            A tensor with the kappa loss.\"\"\"\n\n    with tf.name_scope(name):\n        y_true = tf.to_float(y_true)\n        repeat_op = tf.to_float(tf.tile(tf.reshape(tf.range(0, N), [N, 1]), [1, N]))\n        repeat_op_sq = tf.square((repeat_op - tf.transpose(repeat_op)))\n        weights = repeat_op_sq / tf.to_float((N - 1) ** 2)\n    \n        pred_ = y_pred ** y_pow\n        try:\n            pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [-1, 1]))\n        except Exception:\n            pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [bsize, 1]))\n    \n        hist_rater_a = tf.reduce_sum(pred_norm, 0)\n        hist_rater_b = tf.reduce_sum(y_true, 0)\n    \n        conf_mat = tf.matmul(tf.transpose(pred_norm), y_true)\n    \n        nom = tf.reduce_sum(weights * conf_mat)\n        denom = tf.reduce_sum(weights * tf.matmul(\n            tf.reshape(hist_rater_a, [N, 1]), tf.reshape(hist_rater_b, [1, N])) /\n                              tf.to_float(bsize))\n    \n        return 1 - (nom / (denom + eps))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications import NASNetLarge\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.optimizers import Adam, Nadam\n\nnum_classes = 5\n#try pooling max\nmodel = NASNetLarge(weights='imagenet', include_top=False, pooling='avg')\n\nmy_new_model = Sequential()\nmy_new_model.add(model)\nmy_new_model.add(Dense(512, activation=\"relu\"))\nmy_new_model.add(Dropout(rate=0.30))\nmy_new_model.add(Dense(512, activation=\"relu\"))\nmy_new_model.add(Dropout(rate=0.30))                 \nmy_new_model.add(Dense(num_classes, activation='softmax'))\n\n# Say not to train first layer (ResNet) model. It is already trained\nmy_new_model.layers[0].trainable = False\n\n# descent optimizer (adam lr defaut = 0.001)\nmy_new_model.compile(optimizer=Adam(), loss=kappa_loss, metrics=[kappa_metric])\n\nprint(my_new_model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"layers = [(layer, layer.name, layer.trainable) for layer in my_new_model.layers]\npd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications.nasnet import preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import Callback\n\nimage_size = 331\nbatch_size = 16\nnb_epochs = 50\n\n# steps_per_epoch: number of yields (batches) before a epoch is over\n# ceil(num_samples / batch_size)\n# epochs: Number of epochs to train the model. An epoch is an iteration over the entire data provided\n# class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). \n#  This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class.\nshift = 0.2\n\n#brightness_range=[0.8,1.2],\n\ntrain_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n                                   zoom_range=[0.8,1.2],width_shift_range=shift, height_shift_range=shift, \n                                   horizontal_flip=True)\n\n#liefert die trainingsdaten als iterator\n# image aug funktioniert so, dass für den aktuellen batch die bilder geändert werden. Nicht dass es mehr Bilder gibt.\n# Somit wird für jede Epoche mit anderen Bildenr trainiert.\ntrain_generator = train_datagen.flow_from_directory(\n    '../input/petfinder-images/petfinder_images/images/train',\n    target_size=(image_size, image_size),\n    batch_size=batch_size,\n    class_mode='categorical')\n\n# keine image augmenation für validierung\ntest_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n\n#liefert die Validationdaten als iterator\nvalidation_generator = test_datagen.flow_from_directory(\n    '../input/petfinder-images/petfinder_images/images/validation',\n    target_size=(image_size, image_size),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nSTEP_SIZE_TRAIN=math.ceil(train_generator.n//train_generator.batch_size)\nSTEP_SIZE_VALID=math.ceil(validation_generator.n//validation_generator.batch_size)\n\nprint(STEP_SIZE_TRAIN)\nprint(STEP_SIZE_VALID)\n\n\n# Configure the TensorBoard callback and fit the model\ntensorboard_callback = TensorBoard(\"logs\")\n\n# Early stopping against overfit\nearlystopping_callback = EarlyStopping(patience=batch_size/10, monitor='val_loss', mode='auto', restore_best_weights=True)\n        \n# save best model\nmc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='auto', save_best_only=True)\n\nclass_weights = {0: 8.94366197,\n                1: 1.03206265,\n                2: 0.70208061,\n                3: 0.73782613,\n                4: 0.87751256}\n\ntimerCallback = TimerCallback(500)\n\nhistory = my_new_model.fit_generator(\n    train_generator,\n    steps_per_epoch = STEP_SIZE_TRAIN,\n    validation_data = validation_generator, \n    validation_steps = STEP_SIZE_VALID,\n    epochs = nb_epochs,\n    class_weight = None,\n    callbacks=[tensorboard_callback, mc,timerCallback],\n    workers = 2,\n    use_multiprocessing = False,\n    max_queue_size = 40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef plot_training(history):\n    # Plot training & validation accuracy values\n    plt.plot(history.history['kappa_metric'])\n    plt.plot(history.history['val_kappa_metric'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    # Plot training & validation loss values\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    plt.savefig('acc_vs_epochs.png')\n    \nplot_training(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.applications.nasnet import preprocess_input, decode_predictions\n\nsaved_model = load_model('../working/best_model.h5', custom_objects={'kappa_loss': kappa_loss, 'kappa_metric': kappa_metric})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n\ndef calculate_score_from_predictions(df, PetID):\n    mean = df[df['Filename'].str.contains(PetID)]['Predictions'].mean()\n    if math.isnan(mean):\n        return 4 # if no photo is available, default is Speed 4\n    else: \n        return int(round(mean, 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/petfinder-images/petfinder_images/images/real_test/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n\n#liefert die testdaten als iterator\ntest_generator = test_datagen.flow_from_directory(\n    '../input/petfinder-images/petfinder_images/images/real_test', # expects a single folder within\n    target_size=(image_size, image_size),\n    batch_size=batch_size,\n    class_mode=None,\n    shuffle=False)\n\ntest_generator.reset()\n\ntest_preds = saved_model.predict_generator(test_generator)\ntest_results=pd.DataFrame({\"Filename\":test_generator.filenames,\n                      \"Predictions\":test_preds.argmax(axis=-1)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_results.shape)\ntest_results.Predictions.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit=pd.DataFrame()\nsubmit['PetID']=test['PetID']\nsubmit['AdoptionSpeed']=test['PetID'].apply(lambda x: calculate_score_from_predictions(test_results, x))\nsubmit.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}