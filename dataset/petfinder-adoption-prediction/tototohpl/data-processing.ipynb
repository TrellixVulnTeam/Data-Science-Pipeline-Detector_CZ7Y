{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hoàng Phương Linh - 18020758","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nimport sklearn.ensemble as ens\nimport xgboost as xgb\nimport json\nimport os\nimport re\nfrom sklearn import preprocessing\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchvision import transforms as T\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nfrom sklearn.decomposition import TruncatedSVD\nfrom keras.applications import VGG19\nfrom keras.applications import DenseNet121\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.applications.vgg19 import preprocess_input\nfrom tensorflow.keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\nimport keras.backend as K\nfrom nltk.corpus import stopwords\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nimport lightgbm as lgb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-10T18:50:47.347581Z","iopub.execute_input":"2021-06-10T18:50:47.347936Z","iopub.status.idle":"2021-06-10T18:50:47.35624Z","shell.execute_reply.started":"2021-06-10T18:50:47.347906Z","shell.execute_reply":"2021-06-10T18:50:47.355467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Sentiment Data từ file Json","metadata":{}},{"cell_type":"code","source":"def SentimentData(data):\n    sentiment = [0,0,0,0,0,0]\n    if data['PetID']+'.json' in os.listdir('../input/petfinder-adoption-prediction/train_sentiment'):\n        f = open('../input/petfinder-adoption-prediction/train_sentiment/'+data['PetID']+'.json', 'r')\n        jsonContent = json.loads(f.read())\n        # Lẩy magnitude và score của từng câu:\n        mag = np.array([i['sentiment']['magnitude'] for i in jsonContent['sentences']])\n        sco = np.array([i['sentiment']['score'] for i in jsonContent['sentences']])\n        # Trả về magnitude và score của cả đoạn và trung bình,tổng của magnitude,score với từng câu:\n        sentiment = [np.sum(mag),np.sum(sco),np.mean(mag),np.mean(sco),\n                     jsonContent['documentSentiment']['magnitude'],jsonContent['documentSentiment']['score']]\n        print(data['PetID'],end='\\r')\n    return sentiment\n\ndef SentimentDataEntity(data):\n    sentiment = ''\n    if data['PetID']+'.json' in os.listdir('../input/petfinder-adoption-prediction/train_sentiment'):\n        f = open('../input/petfinder-adoption-prediction/train_sentiment/'+data['PetID']+'.json', 'r')\n        jsonContent = json.loads(f.read())\n        # Trả về 1 câu nối name của các đối tuợng entities của đoạn Description:\n        sentiment = ' '.join([i['name'] for i in jsonContent['entities']])\n        print(data['PetID'],end='\\r')\n    return sentiment","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:50:53.138237Z","iopub.execute_input":"2021-06-10T18:50:53.138721Z","iopub.status.idle":"2021-06-10T18:50:53.146325Z","shell.execute_reply.started":"2021-06-10T18:50:53.138677Z","shell.execute_reply":"2021-06-10T18:50:53.145255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Vấn đề:** load dữ liệu từ file json Sentiment miêu tả cảm xúc nhận được từ Description của post\n\n* **Cách làm:** tìm các PetID có file json trong thư mục bằng thư viện os. Load file json thành dict, lấy các trường\n    *  magnitude của cả đoạn văn (chỉ độ lớn cảm xúc từ 0 -> )\n    *  score của cả đoạn văn (chỉ độ tích cực cảm xúc từ -1 -> 1)\n    *  trung bình magnitude của từng câu\n    *  tổng magnitude của từng câu\n    *  trung bình score của từng câu\n    *  tổng score của từng câu\n    *  name của các đối tượng trong câu (dog, cat, house,...)\n* Những PetID không có file sentiment thì để [0,0,0,0,0,0], [''], hoặc thiếu thì để chỗ đấy = 0\n* **Kết quả:**\n    * mảng 14993 hàng chứa các thuộc tính trên cho tập train\n    * mảng 3971 hàng chứa các thuộc tính trên cho tập test\n    \n   ","metadata":{}},{"cell_type":"markdown","source":"# Load Metadata từ file Json","metadata":{}},{"cell_type":"code","source":"def Metadata(data):\n    metadata=[0,'',0,0]\n    if data['PetID']+'-1.json' in os.listdir('../input/petfinder-adoption-prediction/train_metadata'):\n        f = open('../input/petfinder-adoption-prediction/train_metadata/'+data['PetID']+'-1.json', 'r')\n        jsonContent = json.loads(f.read())\n        # trung bình score (điểm chính xác về sự miêu tả đó)\n        label_score = np.array([i['score'] for i in jsonContent['labelAnnotations']]).mean()\n        # 1 câu nối tất cả description - miêu tả bức ảnh đó\n        description_image = ' '.join([i['description'] for i in jsonContent['labelAnnotations']])\n        # trung bình score của color (điểm chính xác về sự xuất hiện màu đó)\n        color_score = np.array([i['score'] for i in jsonContent['imagePropertiesAnnotation']['dominantColors']['colors']]).mean()\n        # trung bình pixelFraction\n        pixelFraction = np.array([i['pixelFraction'] for i in jsonContent['imagePropertiesAnnotation']['dominantColors']['colors']]).mean()\n        metadate = [label_score,description_image,color_score,pixelFraction]\n        print(data['PetID'],end='\\r')\n    return metadata","metadata":{"execution":{"iopub.status.busy":"2021-06-10T14:16:49.981977Z","iopub.execute_input":"2021-06-10T14:16:49.982312Z","iopub.status.idle":"2021-06-10T14:16:49.990305Z","shell.execute_reply.started":"2021-06-10T14:16:49.982281Z","shell.execute_reply":"2021-06-10T14:16:49.989406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Vấn đề:** load dữ liệu từ file json Metadata miêu tả ảnh của các vật nuôi\n* **Cách làm:** tìm các PetID có file json trong thư mục bằng thư viện os. Load file json thành dict, lấy các trường\n    *     description - miêu tả bức ảnh đó\n    *     trung bình score (điểm chính xác về sự miêu tả đó)\n    *     trung bình score của color (điểm chính xác về sự xuất hiện màu đó)\n    *     trung bình pixelFraction\n*     Những PetID không có ảnh thì để [0,'',0,0], hoặc thiếu thì để chỗ đấy = 0. Những PetID có nhiều hơn 1 ảnh thì lâý file json của ảnh thứ nhất -1.json\n\n* **Kết quả:**\n    * mảng 14993 hàng chứa các thuộc tính trên cho tập train\n    * mảng 3971 hàng chứa các thuộc tính trên cho tập test\n","metadata":{}},{"cell_type":"markdown","source":"# Load tất cả dữ liệu","metadata":{}},{"cell_type":"code","source":"# load dữ liệu\ntrain = pd.read_csv(r'../input/petfinder-adoption-prediction/train/train.csv')\ntest = pd.read_csv(r'../input/petfinder-adoption-prediction/test/test.csv')\n\n# load sentiment đã đưọc lấy ra từ file json sentiment và save thành csv\nsentiment = pd.read_csv(r'../input/sentiment-petfinder/sentiment.csv')\nsentiment_test =  pd.read_csv(r'../input/sentiment-petfinder/sentiment_test.csv')\n\n# load sentimentEntity đã đưọc lấy ra từ file json sentiment và save thành csv\nsentimentEntity = pd.read_csv(r'../input/sentimententity-petfinder/sentimentEntity.csv').drop('Unnamed: 0', axis=1)\nsentimentEntity_test = pd.read_csv(r'../input/sentimententity-petfinder/sentimentEntity_test.csv')\n\n# load metadata đã đưọc lấy ra từ file json metadata và save thành csv\nmetadata = pd.read_csv(r'../input/metadata-petfinder/metadata.csv')\nmetadata_test = pd.read_csv(r'../input/metadata-petfinder/metadata_test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:51:05.093634Z","iopub.execute_input":"2021-06-10T18:51:05.093978Z","iopub.status.idle":"2021-06-10T18:51:05.367306Z","shell.execute_reply.started":"2021-06-10T18:51:05.093948Z","shell.execute_reply":"2021-06-10T18:51:05.36631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Vấn đề:** Load các file json khá lâu\n* **Cách làm:** Load file json rồi save thành file csv đẩy lên dataset rồi load ra mỗi khi cần\n* **Kết quả:** Có các file đã được load sẵn thành csv tương ứng mỗi file là 1 dòng","metadata":{}},{"cell_type":"code","source":"train = pd.concat([train,sentiment,sentimentEntity,metadata], axis=1)\ntest = pd.concat([test,sentiment_test,sentimentEntity_test,metadata_test], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:51:08.416453Z","iopub.execute_input":"2021-06-10T18:51:08.416785Z","iopub.status.idle":"2021-06-10T18:51:08.429573Z","shell.execute_reply.started":"2021-06-10T18:51:08.416755Z","shell.execute_reply":"2021-06-10T18:51:08.428785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preProcesser(data):\n    # Thêm 1 trường con vật đấy có thuẩn chủng hay không nếu 1 trong 2 trường Breed = 0 hoặc 2 trường Breed bằng nhau\n    data['IsPureBreed'] = (data[\"Breed1\"] == 0) | (data[\"Breed2\"] == 0) | (data[\"Breed1\"] == data[\"Breed2\"])\n    \n    # Thêm 1 trưòng con vật đấy có ảnh hay không, có nếu PhotoAmt > 0\n    data['HavePhoto'] = data['PhotoAmt']\n    data.loc[data['HavePhoto']==0, 'HavePhoto'] = False\n    data.loc[data['HavePhoto']!=0, 'HavePhoto'] = True\n    # Thêm 1 trường chuẩn hoá trường ảnh bằng cách chia cho số lượng ảnh lớn nhất\n    data['PhotoAmt_Normalize'] = data['PhotoAmt']/data['PhotoAmt'].max()\n    \n    # Thêm 1 trường độ dài của đoạn văn miêu tả bằng cách tìm độ dài của trưòng Description sau đó chuẩn hoá chia cho dữ liệu có độ dài lớn nhất\n    data['DescriptionLength'] = data['Description'].map(str).apply(len)\n    data['DescriptionLength'] = data['DescriptionLength']/data['DescriptionLength'].max()\n    \n    # Thêm 1 trưòng gán nhãn cho RescuerID, chuẩn hoá lại từ 0 -> hơn 5000\n    le = preprocessing.LabelEncoder()\n    le.fit(data['RescuerID'])\n    data['EncodeRescuerID'] = le.transform(data['RescuerID'])\n    # Thêm 1 trường đếm số lượng con vật mà từng RescuerID có\n    df = data.groupby(['RescuerID'])['PetID'].count().reset_index()\n    df['RescuerIDCount'] = df['PetID']\n    df = df.drop('PetID', axis=1)\n    data = pd.merge(data,df, on=\"RescuerID\")\n    \n    # Thêm 1 trường số lượng màu sắc của con vật đấy nếu 2 color = 0 thì con vật đó 1 màu,...\n    temp = ((data['Color1']==0) & (data['Color2']==0))|((data['Color1']==0) & (data['Color3']==0))|((data['Color3']==0) & (data['Color2']==0))\n    data['NumberColor'] = [1 if (tmp == True) else 0 for tmp in temp]\n    temp = ((data['Color1']!=0) & (data['Color2']!=0) & (data['Color3']==0))|((data['Color1']!=0) & (data['Color3']!=0)& (data['Color2']==0))|((data['Color3']!=0) & (data['Color2']!=0) & (data['Color1']==0))\n    data.loc[[tmp == True for tmp in temp], 'NumberColor'] = 2\n    temp = ((data['Color1']!=0) & (data['Color2']!=0) & (data['Color3']!=0))\n    data.loc[[tmp == True for tmp in temp], 'NumberColor'] = 3\n    \n    # Thêm 1 trường tính tuổi theo năm, vì tuổi ở đấy đang tính theo tháng nên chia cho 12\n    data['AgeYear'] = data['Age']//12\n    \n    # Thêm 1 trường xem con vật đấy có phải là con vật duy nhất trong 1 bài post không\n    data['onePet'] = (data['Quantity']==1)\n    \n    # Thêm 1 truờng con vật đấy có video không\n    data['HaveVideo'] = (data['VideoAmt']!=0)\n    \n    # Thêm 1 trưòng sentiment bằng cách nhân mức độ tích cực với mức độ cảm xúc với nhau\n    data['Sentiment'] = data.apply(sentimet, axis=1)\n    \n    # Thêm 1 trường sentiment bằng cách đặt threshold 4 mức độ Positive, Negative, Neutral, Mixed\n    data['SentimentThreshold'] = data.apply(sentimentThreshold, axis=1)\n    \n    return data\n\ndef sentimet(data):\n    # Hàm trả về tích mức độ tích cực Score với mức độ cảm xúc Magnitude\n    return data['SentimentScore']*data['SentimentMagnitude']\ndef sentimentThreshold(data):\n    threshold1 = 0.6\n    threshold2 = - threshold1\n    thOfMagnitude = 3\n    Positive = 5\n    Negative = -5\n    Neutral = 0\n    Mixed = 2\n    temp = 0\n    # Positive nếu score >= 0.6\n    if data['SentimentScore'] >= threshold1:\n        temp = Positive\n    # Mixed nếu -0.6<score<0.6 và magnitude >= 3\n    elif data['SentimentScore'] > threshold2 and data['SentimentScore'] < threshold1 and data['SentimentMagnitude'] >= thOfMagnitude:\n        temp = Mixed\n    # Neutral nếu -0.6<score<0.6 và magnitude < 3\n    elif data['SentimentScore'] > threshold2 and data['SentimentScore'] < threshold1 and data['SentimentMagnitude'] < thOfMagnitude:\n        temp = Neutral\n    # Negative nếu score <= -0.6\n    elif data['SentimentScore'] <= threshold2:\n        temp = Negative\n    # Trả về kết quả ứng với từng mức độ Positive=5, Negative=-5, Neutral=0, Mixed=2\n    return temp","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:51:09.922483Z","iopub.execute_input":"2021-06-10T18:51:09.92297Z","iopub.status.idle":"2021-06-10T18:51:09.937401Z","shell.execute_reply.started":"2021-06-10T18:51:09.922921Z","shell.execute_reply":"2021-06-10T18:51:09.936603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Vấn đề:** Cần mở rộng thêm tập dữ liệu vì có khá ít feature\n\n* **Cách làm:** Thêm các trường mới:\n    * **IsPureBreed:** trường con vật đấy có thuẩn chủng hay không nếu 1 trong 2 trường Breed = 0 hoặc 2 trường Breed bằng nhau\n    * **HavePhoto:** trưòng con vật đấy có ảnh hay không, có nếu PhotoAmt > 0\n    * **PhotoAmt_Normalize:** trường chuẩn hoá trường ảnh bằng cách chia cho số lượng ảnh lớn nhất\n    * **DescriptionLength:** trường độ dài của đoạn văn miêu tả bằng cách tìm độ dài của trưòng Description sau đó chuẩn hoá chia cho dữ liệu có độ dài lớn nhất\n    * **EncodeRescuerID:** trưòng gán nhãn cho RescuerID, chuẩn hoá lại từ 0 -> hơn 5000\n    * **RescuerIDCount:** trường đếm số lượng con vật mà từng RescuerID có\n    * **NumberColor:** trường số lượng màu sắc của con vật đấy nếu 2 color = 0 thì con vật đó 1 màu,...\n    * **AgeYear:** trường tính tuổi theo năm, vì tuổi ở đấy đang tính theo tháng nên chia cho 12\n    * **onePet:** trường xem con vật đấy có phải là con vật duy nhất trong 1 bài post không\n    * **HaveVideo:** truờng con vật đấy có video không\n    * **Sentiment:** trưòng sentiment bằng cách nhân mức độ tích cực với mức độ cảm xúc với nhau\n    * **SentimentThreshold:** trường sentiment bằng cách đặt threshold 4 mức độ Positive, Negative, Neutral, Mixed\n    \n    \n* **Kết quả:** Ta có thêm 12 trường dữ liệu nữa","metadata":{}},{"cell_type":"code","source":"train_pre = preProcesser(train)\ntest_pre = preProcesser(test)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:51:14.459503Z","iopub.execute_input":"2021-06-10T18:51:14.460006Z","iopub.status.idle":"2021-06-10T18:51:15.294231Z","shell.execute_reply.started":"2021-06-10T18:51:14.459966Z","shell.execute_reply":"2021-06-10T18:51:15.293128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preProcesserText(data):\n    # Chuyển dữ liệu đang ở dạng object sang unicode\n    a = data['Description'].values.astype('U')\n    # Sử dụng regex để loại bỏ tất cả những gì không phải chữ cái tiếng anh, số, dấu ' và - ra (bao gồm cả từ tiếng Trung)\n    for i in range(len(a)):\n        a[i] = re.sub('[^a-zA-Z0-9\\'\\-]', ' ', a[i])\n    # Áp dụng tf-idf với stopwords là English để trả về 1 tập dữ liệu được chuẩn hoá dựa vào (tf) tần số xuất hiện, (idf) giảm giá trị những từ xuất hiện ở nhiều văn bản\n    vectorizer = TfidfVectorizer(stop_words = set(stopwords.words('english')))\n    X = vectorizer.fit_transform(a)\n    # Giảm chiều dữ liệu bằng SVD thành 8 trường\n    svd = TruncatedSVD(n_components=8, random_state=31)\n    svd_col = pd.DataFrame(svd.fit_transform(X))\n    svd_col = svd_col.add_prefix('DescriptionSVD')\n    data = pd.concat([data,svd_col], axis=1)\n    \n    #Chuyển dữ liệu đang ở dạng object sang unicode\n    a = data['SentimentEntity'].values.astype('U')\n    # Sử dụng regex để loại bỏ tất cả những gì không phải chữ cái tiếng anh, số, dấu ' và - ra (bao gồm cả từ tiếng Trung)\n    for i in range(len(a)):\n        a[i] = re.sub('[^a-zA-Z0-9\\'\\-]', ' ', a[i])\n    # Áp dụng tf-idf với stopwords là English để trả về 1 tập dữ liệu được chuẩn hoá dựa vào (tf) tần số xuất hiện, (idf) giảm giá trị những từ xuất hiện ở nhiều văn bản\n    vectorizer = TfidfVectorizer(stop_words = set(stopwords.words('english')))\n    X = vectorizer.fit_transform(a)\n    # Giảm chiều dữ liệu bằng SVD thành 8 trường\n    svd = TruncatedSVD(n_components=8, random_state=31)\n    svd_col = pd.DataFrame(svd.fit_transform(X))\n    svd_col = svd_col.add_prefix('SentimentEntitynSVD')\n    data = pd.concat([data,svd_col], axis=1)\n    \n    #Chuyển dữ liệu đang ở dạng object sang unicode\n    a = data['DescriptionImage'].values.astype('U')\n    # Sử dụng regex để loại bỏ tất cả những gì không phải chữ cái tiếng anh, số, dấu ' và - ra (bao gồm cả từ tiếng Trung)\n    for i in range(len(a)):\n        a[i] = re.sub('[^a-zA-Z0-9\\'\\-]', ' ', a[i])\n    # Áp dụng tf-idf với stopwords là English để trả về 1 tập dữ liệu được chuẩn hoá dựa vào (tf) tần số xuất hiện, (idf) giảm giá trị những từ xuất hiện ở nhiều văn bản\n    vectorizer = TfidfVectorizer(stop_words = set(stopwords.words('english')))\n    X = vectorizer.fit_transform(a)\n    # Giảm chiều dữ liệu bằng SVD thành 8 trường\n    svd = TruncatedSVD(n_components=8, random_state=31)\n    svd_col = pd.DataFrame(svd.fit_transform(X))\n    svd_col = svd_col.add_prefix('DescriptionImageSVD')\n    data = pd.concat([data,svd_col], axis=1)\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:51:17.38201Z","iopub.execute_input":"2021-06-10T18:51:17.382333Z","iopub.status.idle":"2021-06-10T18:51:17.392428Z","shell.execute_reply.started":"2021-06-10T18:51:17.382305Z","shell.execute_reply":"2021-06-10T18:51:17.391554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Vấn đề:** Ta có 3 trường text Description, SentimentEntity, DescriptionImage và không thể đưa vào học được ngay\n* **Cách làm:** \n    * Chuyển dữ liệu đang ở dạng object sang unicode\n    * Sử dụng regex để loại bỏ tất cả những gì không phải chữ cái tiếng anh, số, dấu ' và - ra (bao gồm cả từ tiếng Trung)\n    * Áp dụng tf-idf với stopwords là English để trả về 1 tập dữ liệu được chuẩn hoá dựa vào (tf) tần số xuất hiện, (idf) giảm giá trị những từ xuất hiện ở nhiều văn bản\n    * Giảm chiều dữ liệu bằng SVD thành 8 trường\n    * Concat vào cuổi bảng dữ liệu\n* **Kết quả:** Ta có thêm 24 trưòng dữ liệu text được chuẩn hoá","metadata":{}},{"cell_type":"code","source":"train_text = preProcesserText(train_pre)\ntest_text = preProcesserText(test_pre)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:51:20.575487Z","iopub.execute_input":"2021-06-10T18:51:20.575811Z","iopub.status.idle":"2021-06-10T18:51:25.404201Z","shell.execute_reply.started":"2021-06-10T18:51:20.575781Z","shell.execute_reply":"2021-06-10T18:51:25.403193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load ảnh","metadata":{}},{"cell_type":"code","source":"inp = Input((256,256,3))\n#Sử dụng pretrain model với 2 model đã sử dụng qua là VGG19 và DenseNet121 (include_top=False lấy phần feature phía duới, bỏ phần Classifier tầng phía trên) để trích xuất đặc trưng của từng bức ảnh\nmodel = DenseNet121(input_tensor = inp, weights='imagenet',include_top=False)\nx = model.output\n# Output của bưóc trên là 1x7x7x256, ta cho qua 1 lần GlobalAveragePooling2D để kích thưóc còn 1x1024 và AveragePooling để có kích thưóc nhỏ hơn là 1x256\nx = GlobalAveragePooling2D()(x)\nx = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\nx = AveragePooling1D(4)(x)\nout = Lambda(lambda x: x[:,:,0])(x)\nmodel = Model(inp,out)\n\ndef append(img):\n    # Lấy đường dẫn của ảnh\n    if img['HavePhoto']:\n        return '../input/petfinder-adoption-prediction/train_images/'+img['PetID']+'-1.jpg'\n    return ''\n\ndef append_test(img):\n    # Lấy đường dẫn của ảnh\n    if img['HavePhoto']:\n        return '../input/petfinder-adoption-prediction/test_images/'+img['PetID']+'-1.jpg'\n    return ''\n\ndef loadImage(data):\n    if data['Input_image']!='':\n        # Load bức ảnh và resize kích thưóc về 256x256x3\n        img = image.load_img(data['Input_image'], target_size=(256, 256))\n        # chuyển buớc ảnh về ma trận numpy array\n        x = image.img_to_array(img)\n        # thêm 1 trưòng nữa để đưa bức ảnh thành 1x256x256x3 trước khi đưa vào model\n        x = np.expand_dims(x, axis=0)\n        # Sử dụng pretrain model với 2 model đã sử dụng qua là VGG19 và DenseNet121 (include_top=False lấy phần feature phía duới, bỏ phần Classifier tầng phía trên) để trích xuất đặc trưng của từng bức ảnh\n        a = model.predict(x)\n        print(data['PetID'], end='\\r')\n        return a[0]\n    return np.zeros(256)\n\ntrain_image = train.copy()\n# Lấy đường dẫn của ảnh\ntrain_image['Input_image'] = train_image.apply(append, axis=1)\n# Trích xuất đặc trưng\ntrain_image['Image'] = train_image.apply(loadImage, axis=1)\nimage = pd.DataFrame(train_image['Image'].to_list(), columns=[i for i in range(256)])\n# giảm chiều dữ liệu bằng SVD (hoặc NMF) còn 32 trường\nsvd=TruncatedSVD(n_components=32)\nimage=pd.DataFrame(svd.fit_transform(image))\nimage = image.add_prefix('imageSVD')\n# image = image.add_prefix('imageNMF')\n\ntest_image = test.copy()\n# Lấy đường dẫn của ảnh\ntest_image['Input_image'] = test_image.apply(append_test, axis=1)\n# Trích xuất đặc trưng\ntest_image['Image'] = test_image.apply(loadImage, axis=1)\nimage_test = pd.DataFrame(test_image['Image'].to_list(), columns=[i for i in range(256)])\n# giảm chiều dữ liệu bằng SVD (hoặc NMF) còn 32 trường\nsvd=TruncatedSVD(n_components=32)\nimage_test=pd.DataFrame(svd.fit_transform(image_test))\nimage_test = image_test.add_prefix('imageSVD')\n# image_test = image_test.add_prefix('imageNMF')\n\n# image.to_csv('imageNMFDenseNet121.csv', index=False)\n# image_test.to_csv('imageNMFDenseNet121_test.csv', index=False)\n# image.to_csv('imageSVDDenseNet121.csv', index=False)\n# image_test.to_csv('imageSVDDenseNet121_test.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Vấn đề:** Ta có 1 tập dữ liệu ảnh có nhiều kích thưóc khác nhau và khá lớn, cần chuẩn hoá về 1 số đặc trưng nhất định\n* **Cách làm:** \n    * Lấy đường dẫn của ảnh\n    * Load bức ảnh và resize kích thưóc về 256x256x3, chuyển buớc ảnh về ma trận numpy array, thêm 1 trưòng nữa để đưa bức ảnh thành 1x256x256x3 trước khi đưa vào model\n    * Sử dụng pretrain model với 2 model đã sử dụng qua là VGG19 và DenseNet121 (include_top=False lấy phần feature phía duới, bỏ phần Classifier tầng phía trên) để trích xuất đặc trưng của từng bức ảnh\n    * Output của bưóc trên là 1x7x7x256, ta cho qua 1 lần GlobalAveragePooling2D để kích thưóc còn 1x1024 và AveragePooling để có kích thưóc nhỏ hơn là 1x256\n    * Sau đó ta lại giảm chiều dữ liệu bằng SVD (hoặc NMF) còn 32 trường\n* **Kết quả:** Ta có thêm 32 trưòng đặc trưng được lấy ra từ ảnh\n","metadata":{}},{"cell_type":"code","source":"# VGG19_NMF\nimage = pd.read_csv(r'../input/imagenmf-petfinder/imageNMF.csv')\nimage_test = pd.read_csv(r'../input/imagenmf-petfinder/imageNMF_test.csv')\n# DensenNet121_NMF\nimageDenseNet121 = pd.read_csv(r'../input/imagenmf-petfinder/imageNMFDenseNet121.csv')\nimageDenseNet121_test = pd.read_csv(r'../input/imagenmf-petfinder/imageNMFDenseNet121_test.csv')\nimageDenseNet121 = imageDenseNet121.add_prefix('DenseNet121')\nimageDenseNet121_test = imageDenseNet121_test.add_prefix('DenseNet121')\n# DensenNet121_SVD\nimageDenseNet121SVD = pd.read_csv(r'../input/imagenmf-petfinder/imageSVDDenseNet121.csv')\nimageDenseNet121SVD_test = pd.read_csv(r'../input/imagenmf-petfinder/imageSVDDenseNet121_test.csv')\nimageDenseNet121SVD = imageDenseNet121SVD.add_prefix('DenseNet121')\nimageDenseNet121SVD_test = imageDenseNet121SVD_test.add_prefix('DenseNet121')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:27:27.425263Z","iopub.execute_input":"2021-06-10T15:27:27.42596Z","iopub.status.idle":"2021-06-10T15:27:28.538571Z","shell.execute_reply.started":"2021-06-10T15:27:27.425873Z","shell.execute_reply":"2021-06-10T15:27:28.537899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Vấn đề:** Trích xuất đặc trưng từ ảnh mất rất nhiều thời gian\n* **Cách làm:** Save lại vào file csv và load ra dùng mỗi khi cần\n* **Kết quả:** file csv và load ra dùng mỗi khi cần","metadata":{}},{"cell_type":"code","source":"train_image = pd.concat([train_text,image,imageDenseNet121,imageDenseNet121SVD], axis=1)\ntest_image = pd.concat([test_text,image_test,imageDenseNet121_test,imageDenseNet121SVD_test], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:27:33.163626Z","iopub.execute_input":"2021-06-10T15:27:33.164303Z","iopub.status.idle":"2021-06-10T15:27:33.197941Z","shell.execute_reply.started":"2021-06-10T15:27:33.164263Z","shell.execute_reply":"2021-06-10T15:27:33.197212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_image.to_csv('train_petfinder4.csv', index=False)\ntest_image.to_csv('test_petfinder4.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:27:44.513139Z","iopub.execute_input":"2021-06-10T15:27:44.513457Z","iopub.status.idle":"2021-06-10T15:27:47.993076Z","shell.execute_reply.started":"2021-06-10T15:27:44.513429Z","shell.execute_reply":"2021-06-10T15:27:47.991972Z"},"trusted":true},"execution_count":null,"outputs":[]}]}