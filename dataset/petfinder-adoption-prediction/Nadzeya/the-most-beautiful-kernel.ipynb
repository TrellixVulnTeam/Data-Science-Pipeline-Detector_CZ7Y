{"cells":[{"metadata":{"trusted":true,"_uuid":"c929351936eca7bbcd9dada26cabada769f16e46"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport os\nimport json\nfrom pandas.io.json import json_normalize\nfrom pprint import pprint\nfrom PIL import Image\nfrom IPython.display import display, HTML\n%matplotlib inline\nfrom PIL import Image\nfrom joblib import Parallel, delayed\nimport gc\nsplit_char = '/'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"breeds = pd.read_csv('../input/petfinder-adoption-prediction/breed_labels.csv')\ncolors = pd.read_csv('../input/petfinder-adoption-prediction/color_labels.csv')\nstates = pd.read_csv('../input/petfinder-adoption-prediction/state_labels.csv')\ntrain = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\ntest = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6be5aa90db4feaa181dd7deffbd3fee6d630a532"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6, 5))\nax.set_title(\"AdoptionSpeed count in train\")\nsns.countplot(x=\"AdoptionSpeed\", data=train, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b63fc27b698c45bdea1007b34d878e08b454b2cc"},"cell_type":"markdown","source":"Lets get a look how ordinal data is distributed"},{"metadata":{"trusted":true,"_uuid":"0f198395e36b34277c4bf406df9063369fe8cefc"},"cell_type":"code","source":"sns.distplot(train['Age'], bins = 100, kde=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85ae32b9d64e00ddd4ab6e4191836d6c86d89fc1"},"cell_type":"code","source":"train.Age.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58dd567746abc1034348f735db2dc7eaf5d3957b"},"cell_type":"code","source":"#Delete age over 20 and replace it with average mean\ntrain['Age'] = train['Age'].fillna( method='pad')\ntrain.loc[train['Age'] > 20, 'Age'] = train['Age'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f40fd3aba1ddac02e8e3610e4c468b045b564f49"},"cell_type":"code","source":"sns.distplot(train['Age'], bins = 20, kde=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96128b15b50a52532f3da02df1baaec6d036ffeb"},"cell_type":"code","source":"breed_dict = dict(zip(breeds['BreedID'], breeds['BreedName']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88769118a2228c006b03d04534dcbee54206f194"},"cell_type":"code","source":"animal_dict = {1: 'Dog', 2: 'Cat'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2aab10c70cd771401247f3f9d799eae48ddc7a7"},"cell_type":"code","source":"plt.figure(figsize=(28, 12));\nsns.barplot(x='PhotoAmt', y='AdoptionSpeed', data=train)\nplt.title('What about photos?')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3d9259f453ba5740e7eafff607ac0afb75ecf9a"},"cell_type":"markdown","source":"Take some features from this student's notebook https://www.kaggle.com/tgibbons/student-project-looking-for-feedback\n\nHi! We also students. Good luck :)"},{"metadata":{"trusted":true,"_uuid":"53dec3b477b415f083ad99f8eadb3b9feda66989"},"cell_type":"code","source":"encodedColor1 = pd.get_dummies( train['Color1'], prefix=\"color\" )\ntrain = pd.concat([train, encodedColor1], axis='columns')\n\nencodedColor2 = pd.get_dummies( test['Color1'], prefix=\"color\" )\ntest = pd.concat([test, encodedColor2], axis='columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f8a215072256b102fefb92c6206680c9d6e507d"},"cell_type":"code","source":"import cv2\nfrom tqdm import tqdm, tqdm_notebook\nfrom keras.applications.densenet import preprocess_input, DenseNet121\n\nimg_size = 256\nbatch_size = 16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23307066c18ff88041d2eabced8b8c529c49b93e"},"cell_type":"code","source":"pet_ids = train['PetID'].values\nn_batches = len(pet_ids) // batch_size + 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"973de67d3eddaf841029e4185845f4ac57ba1d1f"},"cell_type":"markdown","source":"The following functions and the rest solution have been taken from Valentina's notebook https://www.kaggle.com/myltykritik/simple-lgbm-image-features\n\nThank you <3"},{"metadata":{"trusted":true,"_uuid":"cc80269595fd047add99f49e94ee97754fb30b79"},"cell_type":"code","source":"import scipy as sp\n\nfrom collections import Counter\nfrom functools import partial\nfrom math import sqrt\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\n\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              / num_scored_items)\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n\n    return (1.0 - numerator / denominator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d81c13328f8dd66be26a80b19b210cf39c062ede"},"cell_type":"code","source":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n    \n    def _kappa_loss(self, coef, X, y):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return -cohen_kappa_score(y, preds, weights='quadratic')\n    \n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X = X, y = y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n    \n    def predict(self, X, coef):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return preds\n    \n    def coefficients(self):\n        return self.coef_['x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f1fd4a14cba5685d360bcabc3143c72827c10de"},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\nimport keras.backend as K\ninp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor = inp, \n                       weights=\"../input/densenet-keras/DenseNet-BC-121-32-no-top.h5\",\n                       include_top = False)\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\nx = AveragePooling1D(4)(x)\nout = Lambda(lambda x: x[:,:,0])(x)\n\nm = Model(inp,out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cbbcba620f05c76701deb409c5018261f684217"},"cell_type":"code","source":"features = {}\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/train_images/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44f783e2a6220e60d5b830027d487462f4d9036e"},"cell_type":"code","source":"train_feats = pd.DataFrame.from_dict(features, orient='index')\ntrain_feats.columns = ['pic_'+str(i) for i in range(train_feats.shape[1])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1b4c667573ead719d3d288729e20b3829069c42"},"cell_type":"code","source":"pet_ids = test['PetID'].values\nn_batches = len(pet_ids) // batch_size + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bd1aa7ba314b35ad3313992b239ad2e59ef1cc9"},"cell_type":"code","source":"features = {}\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/test_images/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e8409ca05401b2cb8bc3352f250580df5ffb149"},"cell_type":"code","source":"test_feats = pd.DataFrame.from_dict(features, orient='index')\ntest_feats.columns = ['pic_'+str(i) for i in range(test_feats.shape[1])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d42ccde013469128c4353f7b126cd75cdb45d004"},"cell_type":"code","source":"test_feats = test_feats.reset_index()\ntest_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n\ntrain_feats = train_feats.reset_index()\ntrain_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4764e6714cf38dee0f40cafa508a34c08edf1f00"},"cell_type":"code","source":"target = train['AdoptionSpeed']\ntrain_id = train['PetID']\ntest_id = test['PetID']\n\n\ntrain = pd.merge(train, train_feats, how='left', on='PetID')\ntest = pd.merge(test, test_feats, how='left', on='PetID')\n\n#train.drop(['AdoptionSpeed', 'PetID'], axis=1, inplace=True)\n#test.drop(['PetID'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4be4e2b937213f1b2ef503d1223588c30fc1bc5"},"cell_type":"markdown","source":"More features for the God of features. Next features we took here https://www.kaggle.com/rookzeno/ensemble-other-people-kernel"},{"metadata":{"trusted":true,"_uuid":"75b0df85b75c4767108a014ac561ea2640e2e16f"},"cell_type":"code","source":"import glob\nimport json","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b8c17d701f304dc8854a56b1f9aa58ddcf8057f"},"cell_type":"code","source":"train_image_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_images/*.jpg'))\ntrain_metadata_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_metadata/*.json'))\ntrain_sentiment_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_sentiment/*.json'))\n\nprint(f'num of train images files: {len(train_image_files)}')\nprint(f'num of train metadata files: {len(train_metadata_files)}')\nprint(f'num of train sentiment files: {len(train_sentiment_files)}')\n\n\ntest_image_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_images/*.jpg'))\ntest_metadata_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_metadata/*.json'))\ntest_sentiment_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_sentiment/*.json'))\n\nprint(f'num of test images files: {len(test_image_files)}')\nprint(f'num of test metadata files: {len(test_metadata_files)}')\nprint(f'num of test sentiment files: {len(test_sentiment_files)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4d5f9ce314ba3e6b35df8e828c2f64e135544e5"},"cell_type":"code","source":"train_df_ids = train[['PetID']]\nprint(train_df_ids.shape)\n\n# Metadata:\ntrain_df_ids = train[['PetID']]\ntrain_df_metadata = pd.DataFrame(train_metadata_files)\ntrain_df_metadata.columns = ['metadata_filename']\ntrain_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\ntrain_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\nprint(len(train_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(train_metadata_pets.unique(), train_df_ids['PetID'].unique()))\nprint(f'fraction of pets with metadata: {pets_with_metadatas / train_df_ids.shape[0]:.3f}')\n\n# Sentiment:\ntrain_df_ids = train[['PetID']]\ntrain_df_sentiment = pd.DataFrame(train_sentiment_files)\ntrain_df_sentiment.columns = ['sentiment_filename']\ntrain_sentiment_pets = train_df_sentiment['sentiment_filename'].apply(lambda x: x.split(split_char)[-1].split('.')[0])\ntrain_df_sentiment = train_df_sentiment.assign(PetID=train_sentiment_pets)\nprint(len(train_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(train_sentiment_pets.unique(), train_df_ids['PetID'].unique()))\nprint(f'fraction of pets with sentiment: {pets_with_sentiments / train_df_ids.shape[0]:.3f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"508ffe6c9263617a794890ef63f2a8f76456974e"},"cell_type":"code","source":"# Images:\ntest_df_ids = test[['PetID']]\nprint(test_df_ids.shape)\n\n# Metadata:\ntest_df_metadata = pd.DataFrame(test_metadata_files)\ntest_df_metadata.columns = ['metadata_filename']\ntest_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\ntest_df_metadata = test_df_metadata.assign(PetID=test_metadata_pets)\nprint(len(test_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(test_metadata_pets.unique(), test_df_ids['PetID'].unique()))\nprint(f'fraction of pets with metadata: {pets_with_metadatas / test_df_ids.shape[0]:.3f}')\n\n# Sentiment:\ntest_df_sentiment = pd.DataFrame(test_sentiment_files)\ntest_df_sentiment.columns = ['sentiment_filename']\ntest_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split(split_char)[-1].split('.')[0])\ntest_df_sentiment = test_df_sentiment.assign(PetID=test_sentiment_pets)\nprint(len(test_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(test_sentiment_pets.unique(), test_df_ids['PetID'].unique()))\nprint(f'fraction of pets with sentiment: {pets_with_sentiments / test_df_ids.shape[0]:.3f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"725c2fb168ae79e74100f295c11940a604c4e37f"},"cell_type":"code","source":"class PetFinderParser(object):\n    \n    def __init__(self, debug=False):\n        \n        self.debug = debug\n        self.sentence_sep = ' '\n        \n        self.extract_sentiment_text = False\n    \n    def open_json_file(self, filename):\n        with open(filename, 'r', encoding='utf-8') as f:\n            json_file = json.load(f)\n        return json_file\n        \n    def parse_sentiment_file(self, file):\n        \"\"\"\n        Parse sentiment file. Output DF with sentiment features.\n        \"\"\"\n        \n        file_sentiment = file['documentSentiment']\n        file_entities = [x['name'] for x in file['entities']]\n        file_entities = self.sentence_sep.join(file_entities)\n        \n        file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]\n        \n        file_sentences_sentiment = pd.DataFrame.from_dict(\n            file_sentences_sentiment, orient='columns')\n        file_sentences_sentiment_df = pd.DataFrame(\n            {\n                'magnitude_sum': file_sentences_sentiment['magnitude'].sum(axis=0),\n                'score_sum': file_sentences_sentiment['score'].sum(axis=0),\n                'magnitude_mean': file_sentences_sentiment['magnitude'].mean(axis=0),\n                'score_mean': file_sentences_sentiment['score'].mean(axis=0),\n                'magnitude_var': file_sentences_sentiment['magnitude'].var(axis=0),\n                'score_var': file_sentences_sentiment['score'].var(axis=0),\n            }, index=[0]\n        )\n        \n        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n        df_sentiment = pd.concat([df_sentiment, file_sentences_sentiment_df], axis=1)\n            \n        df_sentiment['entities'] = file_entities\n        df_sentiment = df_sentiment.add_prefix('sentiment_')\n        \n        return df_sentiment\n    \n    def parse_metadata_file(self, file):\n        \"\"\"\n        Parse metadata file. Output DF with metadata features.\n        \"\"\"\n        \n        file_keys = list(file.keys())\n        \n        if 'labelAnnotations' in file_keys:\n            file_annots = file['labelAnnotations']\n            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n            file_top_desc = [x['description'] for x in file_annots]\n        else:\n            file_top_score = np.nan\n            file_top_desc = ['']\n        \n        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n        file_crops = file['cropHintsAnnotation']['cropHints']\n\n        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n\n        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n        \n        if 'importanceFraction' in file_crops[0].keys():\n            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n        else:\n            file_crop_importance = np.nan\n\n        df_metadata = {\n            'annots_score': file_top_score,\n            'color_score': file_color_score,\n            'color_pixelfrac': file_color_pixelfrac,\n            'crop_conf': file_crop_conf,\n            'crop_importance': file_crop_importance,\n            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n        }\n        \n        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n        df_metadata = df_metadata.add_prefix('metadata_')\n        \n        return df_metadata\n    \n\ndef extract_additional_features(pet_id, mode='train'):\n    \n    sentiment_filename = f'../input/petfinder-adoption-prediction/{mode}_sentiment/{pet_id}.json'\n    try:\n        sentiment_file = pet_parser.open_json_file(sentiment_filename)\n        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n        df_sentiment['PetID'] = pet_id\n    except FileNotFoundError:\n        df_sentiment = []\n\n    dfs_metadata = []\n    metadata_filenames = sorted(glob.glob(f'../input/petfinder-adoption-prediction/{mode}_metadata/{pet_id}*.json'))\n    if len(metadata_filenames) > 0:\n        for f in metadata_filenames:\n            metadata_file = pet_parser.open_json_file(f)\n            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n            df_metadata['PetID'] = pet_id\n            dfs_metadata.append(df_metadata)\n        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n    dfs = [df_sentiment, dfs_metadata]\n    \n    return dfs\n\n\npet_parser = PetFinderParser()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cae3199a7debe545e13482bdf5aa3530c34359b"},"cell_type":"code","source":"debug = False\ntrain_pet_ids = train.PetID.unique()\ntest_pet_ids = test.PetID.unique()\n\nif debug:\n    train_pet_ids = train_pet_ids[:1000]\n    test_pet_ids = test_pet_ids[:500]\n\n\ndfs_train = Parallel(n_jobs=-1, verbose=1)(\n    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\n\ntrain_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\ntrain_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\n\ntrain_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\ntrain_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\n\nprint(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n\n\ndfs_test = Parallel(n_jobs=-1, verbose=1)(\n    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\n\ntest_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\ntest_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\n\ntest_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\ntest_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\n\nprint(test_dfs_sentiment.shape, test_dfs_metadata.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8953c1187e6771a07b8f82ce17c1f08b9645cebd"},"cell_type":"code","source":"aggregates = ['sum', 'mean', 'var']\nsent_agg = ['sum']\n\n\n# Train\ntrain_metadata_desc = train_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntrain_metadata_desc = train_metadata_desc.reset_index()\ntrain_metadata_desc[\n    'metadata_annots_top_desc'] = train_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntrain_metadata_gr = train_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in train_metadata_gr.columns:\n    if 'PetID' not in i:\n        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\ntrain_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\ntrain_metadata_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in train_metadata_gr.columns.tolist()])\ntrain_metadata_gr = train_metadata_gr.reset_index()\n\n\ntrain_sentiment_desc = train_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntrain_sentiment_desc = train_sentiment_desc.reset_index()\ntrain_sentiment_desc[\n    'sentiment_entities'] = train_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntrain_sentiment_gr = train_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in train_sentiment_gr.columns:\n    if 'PetID' not in i:\n        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\ntrain_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(sent_agg)\ntrain_sentiment_gr.columns = pd.Index([f'{c[0]}' for c in train_sentiment_gr.columns.tolist()])\ntrain_sentiment_gr = train_sentiment_gr.reset_index()\n\n\n# Test\ntest_metadata_desc = test_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntest_metadata_desc = test_metadata_desc.reset_index()\ntest_metadata_desc[\n    'metadata_annots_top_desc'] = test_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntest_metadata_gr = test_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in test_metadata_gr.columns:\n    if 'PetID' not in i:\n        test_metadata_gr[i] = test_metadata_gr[i].astype(float)\ntest_metadata_gr = test_metadata_gr.groupby(['PetID']).agg(aggregates)\ntest_metadata_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in test_metadata_gr.columns.tolist()])\ntest_metadata_gr = test_metadata_gr.reset_index()\n\n\ntest_sentiment_desc = test_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntest_sentiment_desc = test_sentiment_desc.reset_index()\ntest_sentiment_desc[\n    'sentiment_entities'] = test_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntest_sentiment_gr = test_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in test_sentiment_gr.columns:\n    if 'PetID' not in i:\n        test_sentiment_gr[i] = test_sentiment_gr[i].astype(float)\ntest_sentiment_gr = test_sentiment_gr.groupby(['PetID']).agg(sent_agg)\ntest_sentiment_gr.columns = pd.Index([f'{c[0]}' for c in test_sentiment_gr.columns.tolist()])\ntest_sentiment_gr = test_sentiment_gr.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aec86e41077b4236f7728ca5b688d306ff6147dd"},"cell_type":"code","source":"# Train merges:\ntrain_proc = train.copy()\ntrain_proc = train_proc.merge(\n    train_sentiment_gr, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_metadata_gr, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_metadata_desc, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_sentiment_desc, how='left', on='PetID')\n\n# Test merges:\ntest_proc = test.copy()\ntest_proc = test_proc.merge(\n    test_sentiment_gr, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_metadata_gr, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_metadata_desc, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_sentiment_desc, how='left', on='PetID')\n\nprint(train_proc.shape, test_proc.shape)\nassert train_proc.shape[0] == train.shape[0]\nassert test_proc.shape[0] == test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f614339a91b3527f9c1883b3661d9e7d0623752"},"cell_type":"code","source":"train_breed_main = train_proc[['Breed1']].merge(\n    breeds, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\n\ntrain_breed_main = train_breed_main.iloc[:, 2:]\ntrain_breed_main = train_breed_main.add_prefix('main_breed_')\n\ntrain_breed_second = train_proc[['Breed2']].merge(\n    breeds, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\n\ntrain_breed_second = train_breed_second.iloc[:, 2:]\ntrain_breed_second = train_breed_second.add_prefix('second_breed_')\n\n\ntrain_proc = pd.concat(\n    [train_proc, train_breed_main, train_breed_second], axis=1)\n\n\ntest_breed_main = test_proc[['Breed1']].merge(\n    breeds, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\n\ntest_breed_main = test_breed_main.iloc[:, 2:]\ntest_breed_main = test_breed_main.add_prefix('main_breed_')\n\ntest_breed_second = test_proc[['Breed2']].merge(\n    breeds, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\n\ntest_breed_second = test_breed_second.iloc[:, 2:]\ntest_breed_second = test_breed_second.add_prefix('second_breed_')\n\n\ntest_proc = pd.concat(\n    [test_proc, test_breed_main, test_breed_second], axis=1)\n\nprint(train_proc.shape, test_proc.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b193d073a3d42adde811ec5b9b9bbc178e2222f"},"cell_type":"code","source":"X = pd.concat([train_proc, test_proc], ignore_index=True, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2249340bc4abd2129231efb230e34052a705c63a"},"cell_type":"markdown","source":"Then cause it's really only few days to go, I use this model https://www.kaggle.com/masterscrat/crash-course-what-you-need-to-know-to-compete"},{"metadata":{"trusted":true,"_uuid":"b79769de319d037f325ec5c33da5b1f331d15060"},"cell_type":"code","source":"X_temp = X.copy()\n\nto_drop_columns = ['PetID', 'Name', 'RescuerID', 'Description', 'metadata_annots_top_desc', 'sentiment_entities', 'main_breed_BreedName', 'second_breed_BreedName']\nX_temp = X_temp.drop(to_drop_columns, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39df854bcd2287d5cf0c96cf95235fcf6b29ee11"},"cell_type":"code","source":"X_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\nX_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\nX_test = X_test.drop(['AdoptionSpeed'], axis=1)\nX_train_non_null = X_train.fillna(-1)\nX_test_non_null = X_test.fillna(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c085f3876236afdd912836cb80dd7d439bc004af"},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\n\nxgb_params = {\n    'eval_metric': 'rmse',\n    'seed': 1337,\n    'eta': 0.0123,\n    'subsample': 0.8,\n    'colsample_bytree': 0.85,\n    'tree_method': 'gpu_hist',\n    'device': 'gpu',\n    'silent': 1,\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf0d801fa8eed8d8178739dcd0e53b10986bc5d9"},"cell_type":"code","source":"def run_xgb(params, X_train, X_test):\n    n_splits = 10\n    verbose_eval = 1000\n    num_rounds = 60000\n    early_stop = 500\n\n    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1337)\n\n    oof_train = np.zeros((X_train.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n\n    i = 0\n    for train_idx, valid_idx in kf.split(X_train, X_train['AdoptionSpeed'].values):\n\n        X_tr = X_train.iloc[train_idx, :]\n        X_val = X_train.iloc[valid_idx, :]\n\n        y_tr = X_tr['AdoptionSpeed'].values\n        X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n\n        y_val = X_val['AdoptionSpeed'].values\n        X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n\n        d_train = xgb.DMatrix(data=X_tr, label=y_tr, feature_names=X_tr.columns)\n        d_valid = xgb.DMatrix(data=X_val, label=y_val, feature_names=X_val.columns)\n\n        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n        model = xgb.train(dtrain=d_train, num_boost_round=num_rounds, evals=watchlist,\n                         early_stopping_rounds=early_stop, verbose_eval=verbose_eval, params=params)\n\n        valid_pred = model.predict(xgb.DMatrix(X_val, feature_names=X_val.columns), ntree_limit=model.best_ntree_limit)\n        test_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_test.columns), ntree_limit=model.best_ntree_limit)\n\n        oof_train[valid_idx] = valid_pred\n        oof_test[:, i] = test_pred\n\n        i += 1\n    return model, oof_train, oof_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea5a221d103bc16980fdb851833ceb3e659e22e6"},"cell_type":"code","source":"model, oof_train, oof_test = run_xgb(xgb_params, X_train_non_null, X_test_non_null)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e114955f854ec45dc1dbc8c00ae7b61527014774"},"cell_type":"code","source":"optR = OptimizedRounder()\noptR.fit(oof_train, X_train['AdoptionSpeed'].values)\ncoefficients = optR.coefficients()\nvalid_pred = optR.predict(oof_train, coefficients)\nqwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, valid_pred)\nprint(\"QWK = \", qwk)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c10ed1ba8dd33843dc96777c7e59a8a309b3d57e"},"cell_type":"code","source":"test_predictions = optR.predict(oof_test.mean(axis=1), coefficients).astype(np.int8)\nsubmission = pd.DataFrame({'PetID': test['PetID'].values, 'AdoptionSpeed': test_predictions})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}