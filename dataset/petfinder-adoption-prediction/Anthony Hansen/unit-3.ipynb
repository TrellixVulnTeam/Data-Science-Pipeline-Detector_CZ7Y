{"cells":[{"metadata":{"id":"RHAUKyWlWQ9L","outputId":"2b401753-d73b-4409-e959-ff55389dc11c","trusted":true},"cell_type":"code","source":"# First, we'll import pandas and numpy, two data processing libraries\nimport pandas as pd\nimport numpy as np\n\n# We'll also import seaborn and matplot, twp Python graphing libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Import the needed sklearn libraries\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\n\n# The Keras library provides support for neural networks and deep learning\n# Use the updated Keras library from Tensorflow -- provides support for neural networks and deep learning\nimport tensorflow.keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Lambda, Flatten, LSTM\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.optimizers import Adam, RMSprop\n#from tensorflow.keras.utils import np_utils\nfrom tensorflow.keras.utils import to_categorical\n\n\n# We will turn off some warns in this notebook to make it easier to read for new students\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint (\"All libraries imported\")","execution_count":null,"outputs":[]},{"metadata":{"id":"Z_gd5XuZvScg","outputId":"723ecdeb-34e0-497d-8cf4-2064e1810c9e","trusted":true},"cell_type":"code","source":"# Read data from the actual Kaggle download files stored in a raw file in GitHub\ngithub_folder = 'https://raw.githubusercontent.com/CIS3115-Machine-Learning-Scholastica/CIS3115ML-Units7and8/master/petfinder-adoption/'\nkaggle_folder = '../input/petfinder-adoption-prediction/'\n\ndata_folder = github_folder\n# Uncomment the next line to switch from using the github files to the kaggle files for a submission\n#data_folder = kaggle_folder\n\ntrain = pd.read_csv(data_folder + 'train/train.csv')\nsubmit = pd.read_csv(data_folder + 'test/test.csv')\n\nsample_submission = pd.read_csv(data_folder + 'test/sample_submission.csv')\nlabels_breed = pd.read_csv(data_folder + 'breed_labels.csv')\nlabels_color = pd.read_csv(data_folder + 'color_labels.csv')\nlabels_state = pd.read_csv(data_folder + 'state_labels.csv')\n\nprint (\"training data shape: \" ,train.shape)\nprint (\"submission data shape: : \" ,submit.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"qvM7dhR59eSj","outputId":"6aa34cff-a6b1-40fb-acf1-dc9c66498f31","trusted":true},"cell_type":"code","source":"#from tensorflow.keras.utils import to_categorical\n\n# Select which features to use\npet_train = train[['Age','Gender','Health','MaturitySize','Dewormed','Sterilized']]\n# Everything we do to the training data we also should do the the submission data\npet_submit = submit[['Age','Gender','Health','MaturitySize','Dewormed','Sterilized']]\n\n# Convert output to one-hot encoding\npet_adopt_speed = to_categorical( train['AdoptionSpeed'] )\n\nprint (\"pet_train data shape: \" ,pet_train.shape)\nprint (\"pet_submit data shape: \" ,pet_submit.shape)\nprint (\"pet_adopt_speed data shape: \" ,pet_adopt_speed.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"N2WObN0q44FT","trusted":true},"cell_type":"code","source":"# Add any columns to the list below that you want dummy variables created\ncat_columns = ['Breed1','FurLength','Color1','Gender']\n\n# You should not need to change any code below this line\n# =======================================================\n\n# Create the dummy variables for the columns listed above\ndfTemp = pd.get_dummies( train[cat_columns], columns=cat_columns )\npet_train = pd.concat([pet_train, dfTemp], axis='columns')\n\n# Do the same to the submission data\ndfSummit = pd.get_dummies( submit[cat_columns], columns=cat_columns )\npet_submit = pd.concat([pet_submit, dfSummit], axis='columns')\n# Get missing columns in the submission data\nmissing_cols = set( pet_train.columns ) - set( pet_submit.columns )\n# Add a missing column to the submission set with default value equal to 0\nfor c in missing_cols:\n    pet_submit[c] = 0\n# Ensure the order of column in the test set is in the same order than in train set\npet_submit = pet_submit[pet_train.columns]\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"v9Ejd1uHGKuZ","outputId":"d5c49c6a-4142-4f40-f895-e8338ecb3770","trusted":true},"cell_type":"code","source":"# We should check the that the number of features is not too large and that the training and submission data still have the same number of features\n\n\n\n# print out the current data\nprint (\"Size of pet_train = \", pet_train.shape)\nprint (\"Size of pet_submit = \", pet_submit.shape)\npet_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"9OEJd-Fr9eSm","outputId":"d2184e7d-1e06-4ac6-8569-c7bc55329866","trusted":true},"cell_type":"code","source":"# Vaccinated - Pet has been vaccinated (1 = Yes, 2 = No, 3 = Not Sure)\n#encodedVaccinated = train[['Vaccinated']] \ndef fixVac( value ):\n    if value == 1: return +1\n    elif value == 2: return -1\n    else: return 0\n\n#train['encodedVaccinated'] = list(map(lambda a: 0 if (a>1) else a,train['Vaccinated']))\npet_train['encodedVaccinated'] = list(map(fixVac,train['Vaccinated']))\n# Do the same thing to the submission data\npet_submit['encodedVaccinated'] = list(map(fixVac,submit['Vaccinated']))\n\npet_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"O-7_xF5L42yG"},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"id":"yAHNOrk49eSr","outputId":"f60efba2-2ffd-49b9-9ae6-8a67975fa3e7","trusted":true},"cell_type":"code","source":"print (\"pet_train data shape: \" ,pet_train.shape)\nprint (\"pet_adopt_speed data shape: \" ,pet_adopt_speed.shape)\nprint (\"pet_submit data shape: \" ,pet_submit.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"RlQcJg04CtGO","outputId":"d77c0b59-abe6-44ef-ab9a-4e9f5b541fa9","trusted":true},"cell_type":"code","source":"# Scale the data to put large features like area_mean on the same footing as small features like smoothness_mean\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nscaler = StandardScaler()\npet_train_scaled = scaler.fit_transform(pet_train)\npet_submit_scaled = scaler.fit_transform(pet_submit)\n\npet_train_scaled","execution_count":null,"outputs":[]},{"metadata":{"id":"XbkB0OYe9eSw","outputId":"555bb58e-e39c-4d35-b212-35051e7808fc","trusted":true},"cell_type":"code","source":"# Split the data into 90% for training and 10% for testing out the models\nX_train, X_test, y_train, y_test = train_test_split(pet_train_scaled, pet_adopt_speed, test_size=0.1)\n\nprint (\"X_train training data shape of 28x28 pixels greyscale: \" ,X_train.shape)\nprint (\"X_test submission data shape of 28x28 pixels greyscale: : \" ,X_test.shape)\n\nprint (\"y_train training data shape of 28x28 pixels greyscale: \" ,y_train.shape)\nprint (\"y_test submission data shape of 28x28 pixels greyscale: : \" ,y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"M1H9GVNvvSdM","outputId":"3c8deb6f-f152-4669-9aaa-bed8bfc36964","trusted":true},"cell_type":"code","source":"# Set up the Neural Network\ninput_Size = X_test.shape[1]     # This is the number of features you selected for each pet\noutput_Size = y_train.shape[1]   # This is the number of categories for adoption speed, should be 5\n\nmodel = Sequential()\nmodel.add(Dense(50, activation='relu', input_dim=(input_Size)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(40, activation='relu'))\nmodel.add(Dense(30, activation='relu'))\nmodel.add(Dense(20, activation='relu'))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(output_Size, activation='softmax'))\n\n# Compile neural network model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nprint (\"Neural Network created\")\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"U-k7Y9H-2wUr","trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n                                            patience=5, \n                                            verbose=2, \n                                            factor=0.5,                                            \n                                            min_lr=0.000001)\n\nearly_stops = EarlyStopping(monitor='val_loss', \n                            min_delta=0, \n                            patience=20, \n                            verbose=2, \n                            mode='auto')\n\ncheckpointer = ModelCheckpoint(filepath = 'cis6115_PetFinder.{epoch:02d}-{accuracy:.6f}.hdf5',\n                               verbose=2,\n                               save_best_only=True, \n                               save_weights_only = True)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"pMeiCJoZvSd6","outputId":"b63ba079-308e-418a-bc71-558773b54cd4","trusted":true},"cell_type":"code","source":"# Fit model on training data for network with dense input layer\n\nhistory = model.fit(X_train, y_train,\n          epochs=200,\n          verbose=1,\n          validation_data=(X_test, y_test))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"J_2GLi6avSeF","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"Y965DSk5vSeJ","outputId":"b8c5cf8a-8014-42a2-af09-e7d44961c43d","trusted":true},"cell_type":"code","source":"# 10. Evaluate model on test data\nprint (\"Running final scoring on test data\")\nscore = model.evaluate(X_test, y_test, verbose=1)\nprint (\"The accuracy for this model is \", format(score[1], \",.2f\"))","execution_count":null,"outputs":[]},{"metadata":{"id":"EsT7FG9SvSeP","outputId":"57f7e3e2-fc9d-4250-908b-6dcc53a510a4","trusted":true},"cell_type":"code","source":"# We will display the loss and the accuracy of the model for each epoch\n# NOTE: this is a little fancy display than is shown in the textbook\ndef display_training_curves(training, validation, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])\ndisplay_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\ndisplay_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 212)","execution_count":null,"outputs":[]},{"metadata":{"id":"R_vn3B18-Adk","outputId":"2bab31c4-db7b-40d9-9999-51c2f951d6f2","trusted":true},"cell_type":"code","source":"print (\"pet_train data shape: \" ,pet_train.shape)\nprint (\"submit data shape: \" ,submit.shape)\nprint (\"pet_submit data shape: \" ,pet_submit_scaled.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"NKSi64z70XNv","outputId":"0ec6c759-c0f7-4987-a75b-b9b4bad8a1d3","trusted":true},"cell_type":"code","source":"predictions = model.predict_classes(pet_submit_scaled, verbose=1)\n\nsubmissions=pd.DataFrame({'PetID': submit.PetID})\nsubmissions['AdoptionSpeed'] = predictions\n\nsubmissions.to_csv(\"submission.csv\", index=False, header=True)\n\nsubmissions.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}