{"cells":[{"metadata":{"_uuid":"49d850ab52bdc567282e1eb384ce08c103e43957"},"cell_type":"markdown","source":"stratifiedとGroupcvをアンサンブルアンサンブル\nhistgram系のを抜く\nimage featuresを追加 sharpnessなど\nlgbmとxgbでweighting\nTarget Encodingする\nseed ensenbleする\nOputuna した パラメータを試す\n#  Forked from [Baseline Modeling](https://www.kaggle.com/wrosinski/baselinemodeling)"},{"metadata":{"_uuid":"bd00707e4bcbd72df5ca3adfbdd370219af8abc0"},"cell_type":"markdown","source":"## Added Image features from [Extract Image features from pretrained NN](https://www.kaggle.com/christofhenkel/extract-image-features-from-pretrained-nn)"},{"metadata":{"_uuid":"9b8a4b0572f7cad96e7ca1167b6b4f6ab3873fd9"},"cell_type":"markdown","source":"## Added Image size features from [Extract Image Features](https://www.kaggle.com/kaerunantoka/extract-image-features)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport glob\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pprint\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm, tqdm_notebook\n\n%matplotlib inline\n\nnp.random.seed(seed=1337)\nwarnings.filterwarnings('ignore')\n\nsplit_char = '/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b3e28f331a9bf5e8838c2afb123c5e1741012a8"},"cell_type":"code","source":"class Config:\n    num_seed_stratified_group = 4\n    num_seed_stratified = 4\n    num_seed_group = 4\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"os.listdir('../input')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24a6811e5b612c3d2aef6639f577dd10f2564be4"},"cell_type":"code","source":"train = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\ntest = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')\nsample_submission = pd.read_csv('../input/petfinder-adoption-prediction/test/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"052af9faacdccaa34191d06da2f13f73417dd628"},"cell_type":"markdown","source":"## Image features"},{"metadata":{"trusted":true,"_uuid":"af167755c88bb47c01b01982b71391bc39238d6d"},"cell_type":"code","source":"import cv2\nfrom PIL import Image as IMG\nimport os\nfrom keras.applications.densenet import preprocess_input, DenseNet121","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a3a8ba29edf2467ba63aed248e756397c1bb237"},"cell_type":"code","source":"img_size = 256\nbatch_size = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcfb03c60959060631f41a2bc056d56c3c85bf11"},"cell_type":"code","source":"def resize_no_pad(im):\n    old_size = im.shape[:2]\n    ratio = float(img_size)/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    return im\n\ndef load_raw_image(path, pet_id):\n    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n    new_image = resize_no_pad(image)\n    return  new_image, image.shape\n\nfrom collections import defaultdict\nimport operator\ndef color_analysis(img):\n    # obtain the color palatte of the image \n    palatte = defaultdict(int)\n    for pixel in img.getdata():\n        palatte[pixel] += 1\n    \n    # sort the colors present in the image \n    sorted_x = sorted(palatte.items(), key=operator.itemgetter(1), reverse = True)\n    light_shade, dark_shade, shade_count = 0, 0, 0\n    pixel_limit = 25\n    for i, x in enumerate(sorted_x[:pixel_limit]):\n        if all(xx <= 20 for xx in x[0][:3]): ## dull : too much darkness \n            dark_shade += x[1]\n        if all(xx >= 220 for xx in x[0][:3]): ## bright : too much whiteness \n            light_shade += x[1]\n        shade_count += x[1]\n        \n    light_percent = round((float(light_shade)/shade_count)*100, 2)\n    dark_percent = round((float(dark_shade)/shade_count)*100, 2)\n    return light_percent, dark_percent\n\ndef get_blurrness_score(image):\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    fm = cv2.Laplacian(image, cv2.CV_64F).var()\n    return fm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"954f42f4682472fb3399389f1f24a7d92c59535a"},"cell_type":"code","source":"from scipy.stats import skew\n\npet_ids = train['PetID'].values\nim_feature_dict = pd.DataFrame(index=pet_ids, columns=['light_percent', 'dark_percent', 'sharpness',\n                                                      'top1_size', 'top1_width', 'top1_height']).add_prefix('im_')\n\nfor pid in tqdm_notebook(pet_ids):\n    try:\n        im, original_shape = load_raw_image(\"../input/petfinder-adoption-prediction/train_images/\", pid)\n        im_feature_dict.loc[pid, 'im_top1_size'] = original_shape[0] * original_shape[1]\n        im_feature_dict.loc[pid, 'im_top1_width'] = original_shape[1]\n        im_feature_dict.loc[pid, 'im_top1_height'] = original_shape[0]\n        \n        im_feature_dict.loc[pid, 'im_sharpness'] = get_blurrness_score(im)\n        im = IMG.fromarray(np.uint8(im))\n        light_percent, dark_percent = color_analysis(im)\n        im_feature_dict.loc[pid, 'im_light_percent'] = light_percent\n        im_feature_dict.loc[pid, 'im_dark_percent'] = dark_percent\n    except AttributeError:\n        pass\n\n\npet_ids = test['PetID'].values\ntest_im_feature_dict = pd.DataFrame(index=pet_ids, columns=['light_percent', 'dark_percent', 'sharpness',\n                                                            'top1_size', 'top1_width', 'top1_height']).add_prefix('im_')\nfor pid in tqdm_notebook(pet_ids):\n    try:\n        im, original_shape = load_raw_image(\"../input/petfinder-adoption-prediction/test_images/\", pid)\n        test_im_feature_dict.loc[pid, 'im_top1_size'] = original_shape[0] * original_shape[1]\n        test_im_feature_dict.loc[pid, 'im_top1_width'] = original_shape[1]\n        test_im_feature_dict.loc[pid, 'im_top1_height'] = original_shape[0]\n        test_im_feature_dict.loc[pid, 'im_sharpness'] = get_blurrness_score(im)\n        im = IMG.fromarray(np.uint8(im))\n        light_percent, dark_percent = color_analysis(im)\n        test_im_feature_dict.loc[pid, 'im_light_percent'] = light_percent\n        test_im_feature_dict.loc[pid, 'im_dark_percent'] = dark_percent\n    except AttributeError:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a48d5b43bee7554f45a8b9c6eb37ee80068b06b"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,10))\nim_feature_dict.fillna(-1).hist(ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc2210e22d5a0581554393d9f861f1ae9405084c"},"cell_type":"code","source":"train = train.merge(im_feature_dict, how='left', left_on='PetID', right_index=True)\nprint(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf85815c77f78ad7d4617f63ee18e6499fa7ee0b"},"cell_type":"code","source":"test = test.merge(test_im_feature_dict, how='left', left_on='PetID', right_index=True)\nprint(test.shape)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71def76c69445cd7d42cb8450483220e63438dee"},"cell_type":"code","source":"def resize_to_square(im):\n    old_size = im.shape[:2]\n    ratio = float(img_size)/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = img_size - new_size[1]\n    delta_h = img_size - new_size[0]\n    top, bottom = delta_h//2, delta_h-(delta_h//2)\n    left, right = delta_w//2, delta_w-(delta_w//2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n    return new_im\n\ndef load_image(path, pet_id):\n    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n    new_image = resize_to_square(image)\n    new_image = preprocess_input(new_image)\n    return new_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8505c12860cf4a4ca4f74a04b9fe0c8db8b2222e"},"cell_type":"code","source":"img_size = 256\nbatch_size = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73b44c6c68421de8fb39f91342660c57e6ef4c2c"},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\nimport keras.backend as K\ninp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor = inp, \n                       weights=\"../input/densenet-keras/DenseNet-BC-121-32-no-top.h5\",\n                       include_top = False)\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\nx = AveragePooling1D(4)(x)\nout = Lambda(lambda x: x[:,:,0])(x)\n\nm = Model(inp,out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4163367a508d90961c9ba19b96be82217cd7686"},"cell_type":"code","source":"pet_ids = train['PetID'].values\nn_batches = len(pet_ids) // batch_size + 1\n\nfeatures = {}\nfor b in tqdm(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/train_images/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fed0e12d69d7c43beb2de2af9c177165398b23a1"},"cell_type":"code","source":"train_feats = pd.DataFrame.from_dict(features, orient='index')\ntrain_feats.columns = [f'pic_{i}' for i in range(train_feats.shape[1])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b0400ec76abaee7258adc6c6f5ac6c38294571f"},"cell_type":"code","source":"pet_ids = test['PetID'].values\nn_batches = len(pet_ids) // batch_size + 1\n\nfeatures = {}\nfor b in tqdm(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/test_images/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"002477addeb74f3eda0ac6f0f52cea7450176417"},"cell_type":"code","source":"test_feats = pd.DataFrame.from_dict(features, orient='index')\ntest_feats.columns = [f'pic_{i}' for i in range(test_feats.shape[1])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f013e7e34e3f95519750f1c6fdb88bca9fa5058"},"cell_type":"code","source":"train_feats = train_feats.reset_index()\ntrain_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n\ntest_feats = test_feats.reset_index()\ntest_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e122ef29bc118e58bdeb9a577150f9369c0598a"},"cell_type":"code","source":"all_ids = pd.concat([train, test], axis=0, ignore_index=True, sort=False)[['PetID']]\nall_ids.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f11923a7e2fb6078d8b36b8cc8a2432d94130710"},"cell_type":"code","source":"n_components = 32\nsvd_ = TruncatedSVD(n_components=n_components, random_state=1337)\n\nfeatures_df = pd.concat([train_feats, test_feats], axis=0)\nfeatures = features_df[[f'pic_{i}' for i in range(256)]].values\n\nsvd_col = svd_.fit_transform(features)\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('IMG_SVD_')\n\nimg_features = pd.concat([all_ids, svd_col], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0826a13e23571c6685c568d4f99b3fa5512282a"},"cell_type":"markdown","source":"## About metadata and sentiment"},{"metadata":{"trusted":true,"_uuid":"4f9b7e7448cf529274068977bb309435ab605889"},"cell_type":"code","source":"labels_breed = pd.read_csv('../input/petfinder-adoption-prediction/breed_labels.csv')\nlabels_state = pd.read_csv('../input/petfinder-adoption-prediction/color_labels.csv')\nlabels_color = pd.read_csv('../input/petfinder-adoption-prediction/state_labels.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3399c9ff73a9dd37cecb657cc26e90b934f67df"},"cell_type":"code","source":"train_image_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_images/*.jpg'))\ntrain_metadata_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_metadata/*.json'))\ntrain_sentiment_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_sentiment/*.json'))\n\nprint(f'num of train images files: {len(train_image_files)}')\nprint(f'num of train metadata files: {len(train_metadata_files)}')\nprint(f'num of train sentiment files: {len(train_sentiment_files)}')\n\n\ntest_image_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_images/*.jpg'))\ntest_metadata_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_metadata/*.json'))\ntest_sentiment_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_sentiment/*.json'))\n\nprint(f'num of test images files: {len(test_image_files)}')\nprint(f'num of test metadata files: {len(test_metadata_files)}')\nprint(f'num of test sentiment files: {len(test_sentiment_files)}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a74b43503752f801202ba62495d47836f8704c9"},"cell_type":"markdown","source":"### Train"},{"metadata":{"trusted":true,"_uuid":"bc13e1b9227cc808bcba7204e7fd499c597b1796"},"cell_type":"code","source":"# Images:\ntrain_df_ids = train[['PetID']]\nprint(train_df_ids.shape)\n\n# Metadata:\ntrain_df_ids = train[['PetID']]\ntrain_df_metadata = pd.DataFrame(train_metadata_files)\ntrain_df_metadata.columns = ['metadata_filename']\ntrain_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\ntrain_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\nprint(len(train_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(train_metadata_pets.unique(), train_df_ids['PetID'].unique()))\nprint(f'fraction of pets with metadata: {pets_with_metadatas / train_df_ids.shape[0]:.3f}')\n\n# Sentiment:\ntrain_df_ids = train[['PetID']]\ntrain_df_sentiment = pd.DataFrame(train_sentiment_files)\ntrain_df_sentiment.columns = ['sentiment_filename']\ntrain_sentiment_pets = train_df_sentiment['sentiment_filename'].apply(lambda x: x.split(split_char)[-1].split('.')[0])\ntrain_df_sentiment = train_df_sentiment.assign(PetID=train_sentiment_pets)\nprint(len(train_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(train_sentiment_pets.unique(), train_df_ids['PetID'].unique()))\nprint(f'fraction of pets with sentiment: {pets_with_sentiments / train_df_ids.shape[0]:.3f}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"828ef0c92408c1b67a0f3c80efe608792a43837d"},"cell_type":"markdown","source":"### Test"},{"metadata":{"trusted":true,"_uuid":"514c3f5a3d8bf6b396425d1693f5491e874f4cc0"},"cell_type":"code","source":"# Images:\ntest_df_ids = test[['PetID']]\nprint(test_df_ids.shape)\n\n# Metadata:\ntest_df_metadata = pd.DataFrame(test_metadata_files)\ntest_df_metadata.columns = ['metadata_filename']\ntest_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\ntest_df_metadata = test_df_metadata.assign(PetID=test_metadata_pets)\nprint(len(test_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(test_metadata_pets.unique(), test_df_ids['PetID'].unique()))\nprint(f'fraction of pets with metadata: {pets_with_metadatas / test_df_ids.shape[0]:.3f}')\n\n# Sentiment:\ntest_df_sentiment = pd.DataFrame(test_sentiment_files)\ntest_df_sentiment.columns = ['sentiment_filename']\ntest_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split(split_char)[-1].split('.')[0])\ntest_df_sentiment = test_df_sentiment.assign(PetID=test_sentiment_pets)\nprint(len(test_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(test_sentiment_pets.unique(), test_df_ids['PetID'].unique()))\nprint(f'fraction of pets with sentiment: {pets_with_sentiments / test_df_ids.shape[0]:.3f}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d643202fbad8b9d04409c296148ae533eba2235e"},"cell_type":"markdown","source":"## Extract features from json"},{"metadata":{"trusted":true,"_uuid":"f2c3c16c681f5729dd737659346dc1ece81f1490"},"cell_type":"code","source":"class PetFinderParser(object):\n    \n    def __init__(self, debug=False):\n        \n        self.debug = debug\n        self.sentence_sep = ' '\n        \n        self.extract_sentiment_text = False\n    \n    def open_json_file(self, filename):\n        with open(filename, 'r', encoding='utf-8') as f:\n            json_file = json.load(f)\n        return json_file\n        \n    def parse_sentiment_file(self, file):\n        \"\"\"\n        Parse sentiment file. Output DF with sentiment features.\n        \"\"\"\n        \n        file_sentiment = file['documentSentiment']\n        file_entities = [x['name'] for x in file['entities']]\n        file_entities = self.sentence_sep.join(file_entities)\n        \n        file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]\n        \n        file_sentences_sentiment = pd.DataFrame.from_dict(\n            file_sentences_sentiment, orient='columns')\n        file_sentences_sentiment_df = pd.DataFrame(\n            {\n                'magnitude_sum': file_sentences_sentiment['magnitude'].sum(axis=0),\n                'magnitude_mean': file_sentences_sentiment['magnitude'].mean(axis=0),\n                'magnitude_var': file_sentences_sentiment['magnitude'].var(axis=0),\n                'magnitude_max': file_sentences_sentiment['magnitude'].max(axis=0),\n                'magnitude_min': file_sentences_sentiment['magnitude'].min(axis=0),\n                \n                \n                'score_sum': file_sentences_sentiment['score'].sum(axis=0),\n                'score_mean': file_sentences_sentiment['score'].mean(axis=0),\n                'score_var': file_sentences_sentiment['score'].var(axis=0),\n                'score_max': file_sentences_sentiment['score'].max(axis=0),\n                'score_min': file_sentences_sentiment['score'].min(axis=0),\n                \n            }, index=[0]\n        )\n        \n        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n        df_sentiment = pd.concat([df_sentiment, file_sentences_sentiment_df], axis=1)\n            \n        df_sentiment['entities'] = file_entities\n        df_sentiment['language'] = file['language']\n        \n        df_sentiment = df_sentiment.add_prefix('sentiment_')\n        \n        return df_sentiment\n    \n    def parse_metadata_file(self, file):\n        \"\"\"\n        Parse metadata file. Output DF with metadata features.\n        \"\"\"\n        \n        file_keys = list(file.keys())\n        \n        if 'labelAnnotations' in file_keys:\n            file_annots = file['labelAnnotations']\n            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n            file_top_score_var = np.asarray([x['score'] for x in file_annots]).var()\n            file_top_score_max = np.asarray([x['score'] for x in file_annots]).max()\n            \n            file_top_desc = [x['description'] for x in file_annots]\n        else:\n            file_top_score = np.nan\n            file_top_score_var = np.nan\n            file_top_score_max = np.nan\n            file_top_desc = ['']\n        \n        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n        file_crops = file['cropHintsAnnotation']['cropHints']\n\n        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n        file_color_score_var = np.asarray([x['score'] for x in file_colors]).var()\n        file_color_score_max = np.asarray([x['score'] for x in file_colors]).max()\n        \n        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n        file_color_pixelfrac_var = np.asarray([x['pixelFraction'] for x in file_colors]).var()\n        file_color_pixelfrac_max = np.asarray([x['pixelFraction'] for x in file_colors]).max()\n        \n\n        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n        \n        if 'importanceFraction' in file_crops[0].keys():\n            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n        else:\n            file_crop_importance = np.nan\n\n        df_metadata = {\n            'annots_score': file_top_score,\n            'annots_score_var': file_top_score_var,\n            'annots_score_max': file_top_score_max,\n            \n            'color_score': file_color_score,\n            'color_score_var': file_color_score_var,\n            'color_score_max': file_color_score_max,\n            \n            'color_pixelfrac': file_color_pixelfrac,\n            'color_pixelfrac_var': file_color_pixelfrac_var,\n            'color_pixelfrac_max': file_color_pixelfrac_max,\n            \n            'crop_conf': file_crop_conf,\n            'crop_importance': file_crop_importance,\n            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n        }\n        \n        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n        df_metadata = df_metadata.add_prefix('metadata_')\n        \n        return df_metadata\n    \n\ndef extract_additional_features(pet_id, mode='train'):\n    \n    sentiment_filename = f'../input/petfinder-adoption-prediction/{mode}_sentiment/{pet_id}.json'\n    try:\n        sentiment_file = pet_parser.open_json_file(sentiment_filename)\n        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n        df_sentiment['PetID'] = pet_id\n    except FileNotFoundError:\n        df_sentiment = []\n\n    dfs_metadata = []\n    metadata_filenames = sorted(glob.glob(f'../input/petfinder-adoption-prediction/{mode}_metadata/{pet_id}*.json'))\n    if len(metadata_filenames) > 0:\n        for f in metadata_filenames:\n            metadata_file = pet_parser.open_json_file(f)\n            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n            df_metadata['PetID'] = pet_id\n            dfs_metadata.append(df_metadata)\n        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n    dfs = [df_sentiment, dfs_metadata]\n    \n    return dfs\n\n\npet_parser = PetFinderParser()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"787925a3ae3ab2f91189729d177d57ffc938b74a","scrolled":true},"cell_type":"code","source":"debug = False\ntrain_pet_ids = train.PetID.unique()\ntest_pet_ids = test.PetID.unique()\n\nif debug:\n    train_pet_ids = train_pet_ids[:1000]\n    test_pet_ids = test_pet_ids[:500]\n\n\ndfs_train = Parallel(n_jobs=-1, verbose=1)(\n    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\n\ntrain_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\ntrain_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\n\ntrain_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\ntrain_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\n\nprint(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n\n\ndfs_test = Parallel(n_jobs=-1, verbose=1)(\n    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\n\ntest_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\ntest_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\n\ntest_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\ntest_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\n\nprint(test_dfs_sentiment.shape, test_dfs_metadata.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"a65bf6224054e2960bd2d18ea6409b2376840bf4"},"cell_type":"code","source":"train_dfs_sentiment.sentiment_language.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"031a911d515d3f5811093f224af5fe971c745f28"},"cell_type":"code","source":"train_dfs_sentiment.groupby(['PetID'])['sentiment_language'].apply(lambda x: x.mode()).reset_index().drop(columns='level_1')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60d0a0df563b4fabd29a96159492eb69d5854b94"},"cell_type":"markdown","source":"### group extracted features by PetID:"},{"metadata":{"trusted":true,"_uuid":"6fcf1858f550d128ff076ed1d3c32efb9810ef23"},"cell_type":"code","source":"aggregates = ['sum', 'mean', 'var', 'max', 'min']\nsent_agg = ['sum', 'mean', 'var', 'max', 'min']\n\n\n# Train\n# まず desc は別にする\ntrain_metadata_desc = train_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntrain_metadata_desc = train_metadata_desc.reset_index()\ntrain_metadata_desc[\n    'metadata_annots_top_desc'] = train_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\n#  desc 落として、aggregate\nprefix = 'metadata'\ntrain_metadata_gr = train_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in train_metadata_gr.columns:\n    if 'PetID' not in i:\n        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\ntrain_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\ntrain_metadata_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in train_metadata_gr.columns.tolist()])\ntrain_metadata_gr = train_metadata_gr.reset_index()\n\n\ntrain_sentiment_desc = train_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntrain_sentiment_desc = train_sentiment_desc.reset_index()\ntrain_sentiment_desc[\n    'sentiment_entities'] = train_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\ntrain_dfs_lang = train_dfs_sentiment.groupby(['PetID'])['sentiment_language'].apply(lambda x: x.mode()).reset_index().drop(columns='level_1')\n\nprefix = 'sentiment'\ntrain_sentiment_gr = train_dfs_sentiment.drop(['sentiment_entities','sentiment_language'], axis=1)\nfor i in train_sentiment_gr.columns:\n    if 'PetID' not in i:\n        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\ntrain_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(sent_agg)\ntrain_sentiment_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in train_sentiment_gr.columns.tolist()])\ntrain_sentiment_gr = train_sentiment_gr.reset_index()\n\n\n# Test\ntest_metadata_desc = test_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntest_metadata_desc = test_metadata_desc.reset_index()\ntest_metadata_desc[\n    'metadata_annots_top_desc'] = test_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\n\nprefix = 'metadata'\ntest_metadata_gr = test_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in test_metadata_gr.columns:\n    if 'PetID' not in i:\n        test_metadata_gr[i] = test_metadata_gr[i].astype(float)\ntest_metadata_gr = test_metadata_gr.groupby(['PetID']).agg(aggregates)\ntest_metadata_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in test_metadata_gr.columns.tolist()])\ntest_metadata_gr = test_metadata_gr.reset_index()\n\n\ntest_sentiment_desc = test_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntest_sentiment_desc = test_sentiment_desc.reset_index()\ntest_sentiment_desc[\n    'sentiment_entities'] = test_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\ntest_dfs_lang = test_dfs_sentiment.groupby(['PetID'])['sentiment_language'].apply(lambda x: x.mode()).reset_index().drop(columns='level_1')\n\nprefix = 'sentiment'\ntest_sentiment_gr = test_dfs_sentiment.drop(['sentiment_entities', 'sentiment_language'], axis=1)\nfor i in test_sentiment_gr.columns:\n    if 'PetID' not in i:\n        test_sentiment_gr[i] = test_sentiment_gr[i].astype(float)\ntest_sentiment_gr = test_sentiment_gr.groupby(['PetID']).agg(sent_agg)\ntest_sentiment_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in test_sentiment_gr.columns.tolist()])\ntest_sentiment_gr = test_sentiment_gr.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f7b0bd6dea30645c3a7a5e9d72d7e1b94fa32c5"},"cell_type":"code","source":"test_metadata_gr.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e4fa08ae5c47926cffb2202fc4fe5ba83a088cc"},"cell_type":"markdown","source":"### merge processed DFs with base train/test DF:"},{"metadata":{"trusted":true,"_uuid":"adba560254a6221ac0ca717581a748f984d1b9f7"},"cell_type":"code","source":"# Train merges:\ntrain_proc = train.copy()\ntrain_proc = train_proc.merge(\n    train_sentiment_gr, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_metadata_gr, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_metadata_desc, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_sentiment_desc, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_dfs_lang, how='left', on='PetID')\n\n# Test merges:\ntest_proc = test.copy()\ntest_proc = test_proc.merge(\n    test_sentiment_gr, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_metadata_gr, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_metadata_desc, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_sentiment_desc, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_dfs_lang, how='left', on='PetID')\n\nprint(train_proc.shape, test_proc.shape)\nassert train_proc.shape[0] == train.shape[0]\nassert test_proc.shape[0] == test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f379a8eafbba1bdeae37d6e7fbf8ce271fdccf65"},"cell_type":"code","source":"train_breed_main = train_proc[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\n\ntrain_breed_main = train_breed_main.iloc[:, 2:]\ntrain_breed_main = train_breed_main.add_prefix('main_breed_')\n\ntrain_breed_second = train_proc[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\n\ntrain_breed_second = train_breed_second.iloc[:, 2:]\ntrain_breed_second = train_breed_second.add_prefix('second_breed_')\n\n\ntrain_proc = pd.concat(\n    [train_proc, train_breed_main, train_breed_second], axis=1)\n\n\ntest_breed_main = test_proc[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\n\ntest_breed_main = test_breed_main.iloc[:, 2:]\ntest_breed_main = test_breed_main.add_prefix('main_breed_')\n\ntest_breed_second = test_proc[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\n\ntest_breed_second = test_breed_second.iloc[:, 2:]\ntest_breed_second = test_breed_second.add_prefix('second_breed_')\n\n\ntest_proc = pd.concat(\n    [test_proc, test_breed_main, test_breed_second], axis=1)\n\nprint(train_proc.shape, test_proc.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf129fa48290bd51a75aa8093d6e964942437f31","scrolled":true},"cell_type":"code","source":"X = pd.concat([train_proc, test_proc], ignore_index=True, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"420d28593833aa0a3d265c023c196e8201e6b9ce"},"cell_type":"code","source":"X.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7dace2bbcf7eceedfa72e1d9af39506846e7782"},"cell_type":"code","source":"X_temp = X.copy()\n\ntext_columns = ['Description', 'metadata_annots_top_desc', 'sentiment_entities']\ncategorical_columns = ['main_breed_BreedName', 'second_breed_BreedName', 'sentiment_language']\n\nto_drop_columns = ['PetID', 'Name', 'RescuerID']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8787888c4ea8bf38bf95557fe62900aef6a1c60f"},"cell_type":"code","source":"rescuer_count = X.groupby(['RescuerID'])['PetID'].count().reset_index()\nrescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n\nX_temp = X_temp.merge(rescuer_count, how='left', on='RescuerID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af969f7dcac615ba78a82474d914c5e25ce67ecb"},"cell_type":"code","source":"for i in categorical_columns:\n    X_temp.loc[:, i] = pd.factorize(X_temp.loc[:, i])[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73e3c31a0d68d0171b0c7662809eb7ebd8daebe3"},"cell_type":"code","source":"X['sentiment_language'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11b1945ece03f2294e067878d98a029857d9ced9"},"cell_type":"code","source":"X_temp['sentiment_language'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44b50fa65d691a27399f1232203b2249fc1d8c70"},"cell_type":"code","source":"X_text = X_temp[text_columns]\n\nfor i in X_text.columns:\n    X_text.loc[:, i] = X_text.loc[:, i].fillna('none')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24170b22e97131ad802f643cf3ec1a9b292e4060"},"cell_type":"code","source":"X_temp['Length_Description'] = X_text['Description'].map(len)\nX_temp['Length_metadata_annots_top_desc'] = X_text['metadata_annots_top_desc'].map(len)\nX_temp['Lengths_sentiment_entities'] = X_text['sentiment_entities'].map(len)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32a31a517e2834ffa31f0f592ad7d4240ae5c1ea"},"cell_type":"markdown","source":"### TFIDF"},{"metadata":{"trusted":true,"_uuid":"09116632baadf6842804dedc15023ffda928f7c5"},"cell_type":"code","source":"n_components = 16\ntext_features = []\n\n# Generate text features:\nfor i in X_text.columns:\n    \n    # Initialize decomposition methods:\n    print(f'generating features from: {i}')\n    tfv = TfidfVectorizer(min_df=2,  max_features=None,\n                          strip_accents='unicode', analyzer='word', token_pattern=r'(?u)\\b\\w+\\b',\n                          ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1)\n    svd_ = TruncatedSVD(\n        n_components=n_components, random_state=1337)\n    \n    tfidf_col = tfv.fit_transform(X_text.loc[:, i].values)\n    \n    svd_col = svd_.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix('TFIDF_{}_'.format(i))\n    \n    text_features.append(svd_col)\n    \ntext_features = pd.concat(text_features, axis=1)\n\nX_temp = pd.concat([X_temp, text_features], axis=1)\n\nfor i in X_text.columns:\n    X_temp = X_temp.drop(i, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dba66709bbd961656400c5c654cb3d2619710d5f"},"cell_type":"markdown","source":"### Merge image features"},{"metadata":{"trusted":true,"_uuid":"b5f17382a1089b126323da4ce91211d29971f26c"},"cell_type":"code","source":"X_temp = X_temp.merge(img_features, how='left', on='PetID')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c5e240b66fbf79c2c89a1097e39588faf97a119"},"cell_type":"markdown","source":"### Add image_size features"},{"metadata":{"trusted":true,"_uuid":"90422f43e8181ca624f2e7959542b9d1cde865e7"},"cell_type":"code","source":"from PIL import Image\ntrain_df_ids = train[['PetID']]\ntest_df_ids = test[['PetID']]\n\ndebut = False\nif debug:\n    train_pet_ids = train_pet_ids[:1000]\n    test_pet_ids = test_pet_ids[:500]\n\ntrain_df_imgs = pd.DataFrame(train_image_files)\ntrain_df_imgs.columns = ['image_filename']\ntrain_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n\ntest_df_imgs = pd.DataFrame(test_image_files)\ntest_df_imgs.columns = ['image_filename']\ntest_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n\ntrain_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\ntest_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\n\ndef getSize(filename):\n    st = os.stat(filename)\n    return st.st_size\n\ndef getDimensions(filename):\n    img_size = Image.open(filename).size\n    return img_size \n\ntrain_df_imgs['image_size'] = train_df_imgs['image_filename'].apply(getSize)\ntrain_df_imgs['temp_size'] = train_df_imgs['image_filename'].apply(getDimensions)\ntrain_df_imgs['width'] = train_df_imgs['temp_size'].apply(lambda x : x[0])\ntrain_df_imgs['height'] = train_df_imgs['temp_size'].apply(lambda x : x[1])\ntrain_df_imgs = train_df_imgs.drop(['temp_size'], axis=1)\n\ntest_df_imgs['image_size'] = test_df_imgs['image_filename'].apply(getSize)\ntest_df_imgs['temp_size'] = test_df_imgs['image_filename'].apply(getDimensions)\ntest_df_imgs['width'] = test_df_imgs['temp_size'].apply(lambda x : x[0])\ntest_df_imgs['height'] = test_df_imgs['temp_size'].apply(lambda x : x[1])\ntest_df_imgs = test_df_imgs.drop(['temp_size'], axis=1)\n\naggs = {\n    'image_size': ['sum', 'mean', 'var', 'max'],\n    'width': ['sum', 'mean', 'var', 'max'],\n    'height': ['sum', 'mean', 'var', 'max'],\n}\n\nagg_train_imgs = train_df_imgs.groupby('PetID').agg(aggs)\nnew_columns = [\n    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_train_imgs.columns = new_columns\nagg_train_imgs = agg_train_imgs.reset_index()\n\nagg_test_imgs = test_df_imgs.groupby('PetID').agg(aggs)\nnew_columns = [\n    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_test_imgs.columns = new_columns\nagg_test_imgs = agg_test_imgs.reset_index()\n\nagg_imgs = pd.concat([agg_train_imgs, agg_test_imgs], axis=0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fb61e81ee7e060cc4e2ee8f1f8c9a8671d87c40"},"cell_type":"code","source":"X_temp = X_temp.merge(agg_imgs, how='left', on='PetID')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9425dd6c4f5f045fcf3b96d42b06c9f8c5e21a7"},"cell_type":"markdown","source":"### Drop ID, name and rescuerID"},{"metadata":{"trusted":true,"_uuid":"054ca8cddec421f219099c63b710f5a21bdcedba"},"cell_type":"code","source":"X_temp = X_temp.drop(to_drop_columns, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5657331c2d46d83639d0d49bd286d664123fff98"},"cell_type":"code","source":"X_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\nX_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\n\nX_test = X_test.drop(['AdoptionSpeed'], axis=1)\n\nassert X_train.shape[0] == train.shape[0]\nassert X_test.shape[0] == test.shape[0]\n\ntrain_cols = X_train.columns.tolist()\ntrain_cols.remove('AdoptionSpeed')\n\ntest_cols = X_test.columns.tolist()\n\nassert np.all(train_cols == test_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33113d9abab481b5273bf023f139af5ad85e4f90"},"cell_type":"code","source":"X_train_non_null = X_train.fillna(-1)\nX_test_non_null = X_test.fillna(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18158d87855d8ad289edc6fff7aa442177ab0a68"},"cell_type":"code","source":"X_train_non_null.isnull().any().any(), X_test_non_null.isnull().any().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"140d7ccb13fc5b7390a08eb73fb57377c94e15f4"},"cell_type":"code","source":"X_train_non_null.shape, X_test_non_null.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0244add20ca7595d9cf385d2224309cce40e6c2"},"cell_type":"code","source":"def target_encoding(count_offset, X_te_train, X_te_valid, X_te_test, sigm_scale_divider=5):\n    categorical_column_list = [\"Breed1\", \"Breed2\", \"Gender\", \"Color1\", \"Color2\", \"Color3\", \"Health\", \"State\",\n                               'sentiment_language']\n\n    target_encoding_columns = []\n    mean_all = X_te_train.AdoptionSpeed.mean()\n    for categ_column in categorical_column_list:\n        df_categorical = X_te_train[[categ_column, \"AdoptionSpeed\"]]\n        df_categorical = df_categorical.fillna(-1).astype(int)\n\n        te_df = df_categorical.groupby(categ_column).mean()\n        df_count = df_categorical.groupby(categ_column).count()\n        sigmoid_scale = (count_offset - 1) / sigm_scale_divider\n\n        smoothing_coef = 1 / (1 + np.exp(-(df_count - count_offset) / sigmoid_scale))\n\n\n        target_enc_df = mean_all * (1 - smoothing_coef) + te_df * smoothing_coef\n        target_enc_df = target_enc_df.add_prefix(f'TargetEncoding_{categ_column}_')\n\n        X_te_train = pd.merge(X_te_train, target_enc_df.reset_index(), on=categ_column, how='left')\n        X_te_valid = pd.merge(X_te_valid, target_enc_df.reset_index(), on=categ_column, how='left')\n        X_te_test = pd.merge(X_te_test, target_enc_df.reset_index(), on=categ_column, how='left')\n    return X_te_train, X_te_valid, X_te_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3498274ff28da746aafee11a3edd2888787fda9b"},"cell_type":"code","source":"import scipy as sp\n\nfrom collections import Counter\nfrom functools import partial\nfrom math import sqrt\n\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\n\n\n# FROM: https://www.kaggle.com/myltykritik/simple-lgbm-image-features\n\n# The following 3 functions have been taken from Ben Hamner's github repository\n# https://github.com/benhamner/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              / num_scored_items)\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n\n    return (1.0 - numerator / denominator)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8a40b9aefaa6935302789be723a29403ed988bd"},"cell_type":"markdown","source":"### OptimizeRounder from [OptimizedRounder() - Improved](https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved)"},{"metadata":{"trusted":true,"_uuid":"10ddb2ef661d2c1a61e01c2a6d48908bdf858bae"},"cell_type":"code","source":"def rounder(y, thresholds=[0.5, 1.5, 2.5, 3.5]):\n    return np.digitize(y, thresholds)\n\nclass OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = [0.5, 1.5, 2.5, 3.5]\n\n    def _loss(self, coef, X, y):\n        X_p = rounder(X, np.sort(coef))\n        ll = quadratic_weighted_kappa(y, X_p)\n        return -ll\n\n    def fit(self, X, y):\n        coef = self.coef_\n        for it1 in range(10):\n            for idx in range(4):\n                golden1 = 2/(3+np.sqrt(5))\n                golden2 = 1 - golden1\n                a = 0.0 if idx-1 < 0 else coef[idx-1]\n                b = 4.0 if idx+1 > 3 else coef[idx+1]\n                coef[idx] = a + (b-a)*golden1\n                la = self._loss(coef, X, y)\n                coef[idx] = a + (b-a)*golden2\n                lb = self._loss(coef, X, y)\n                for it2 in range(20):\n                    if la > lb:\n                        a = a + (b-a)*golden1\n                        coef[idx] = a + (b-a)*golden2\n                        la = lb\n                        lb = self._loss(coef, X, y)\n                    else:\n                        b = a + (b-a)*golden2\n                        coef[idx] = a + (b-a)*golden1\n                        lb = la\n                        la = self._loss(coef, X, y)\n        self.coef_ = coef\n\n    def predict(self, X, coef):\n        return rounder(X, coef)\n\n    def coefficients(self):\n        return self.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nfrom collections import Counter, defaultdict\ndef stratified_group_k_fold(X, y, groups, k, seed=None):\n    labels_num = np.max(y) + 1\n    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n    y_distr = Counter()\n    for label, g in zip(y, groups):\n        y_counts_per_group[g][label] += 1\n        y_distr[label] += 1\n\n    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n    groups_per_fold = defaultdict(set)\n\n    def eval_y_counts_per_fold(y_counts, fold):\n        y_counts_per_fold[fold] += y_counts\n        std_per_label = []\n        for label in range(labels_num):\n            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n            std_per_label.append(label_std)\n        y_counts_per_fold[fold] -= y_counts\n        return np.mean(std_per_label)\n    \n    groups_and_y_counts = list(y_counts_per_group.items())\n    random.Random(seed).shuffle(groups_and_y_counts)\n\n    for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n        best_fold = None\n        min_eval = None\n        for i in range(k):\n            fold_eval = eval_y_counts_per_fold(y_counts, i)\n            if min_eval is None or fold_eval < min_eval:\n                min_eval = fold_eval\n                best_fold = i\n        y_counts_per_fold[best_fold] += y_counts\n        groups_per_fold[best_fold].add(g)\n\n    all_groups = set(groups)\n    for i in range(k):\n        train_groups = all_groups - groups_per_fold[i]\n        test_groups = groups_per_fold[i]\n\n        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n\n        yield train_indices, test_indices","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34c67df75d9ed9e01021426216164c7875956ac8"},"cell_type":"markdown","source":"## Train model"},{"metadata":{"trusted":true,"_uuid":"6ce6060ef879fd0d9a8a483b593b195926e6ef7f"},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\ntrain_gid = train.RescuerID.values\n\ndef run_xgb(params, X_train, X_test, cv):\n    n_splits = 10\n    verbose_eval = 10000\n    num_rounds = 5000\n    early_stop = 500\n    \n    try:\n        kf = cv(n_splits=n_splits, shuffle=True)\n    except:\n        kf = cv(n_splits=n_splits)\n\n    oof_train = np.zeros((X_train.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n\n    i = 0\n    importance_list = []\n    \n    X_test_original = X_test.copy()\n    for train_idx, valid_idx in tqdm(kf.split(X_train, X_train['AdoptionSpeed'].values, train_gid)):\n        X_tr = X_train.iloc[train_idx, :]\n        X_val = X_train.iloc[valid_idx, :]\n        \n        X_tr, X_val, X_test = target_encoding(params['count_offset'], X_tr, X_val, X_test_original, params['sigm_scale_divider'])\n        y_tr = X_tr['AdoptionSpeed'].values\n        X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n\n        y_val = X_val['AdoptionSpeed'].values\n        X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n\n        d_train = xgb.DMatrix(data=X_tr, label=y_tr, feature_names=X_tr.columns)\n        d_valid = xgb.DMatrix(data=X_val, label=y_val, feature_names=X_val.columns)\n\n        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n        model = xgb.train(dtrain=d_train, num_boost_round=num_rounds, evals=watchlist,\n                         early_stopping_rounds=early_stop, verbose_eval=verbose_eval, params=params)\n\n        valid_pred = model.predict(xgb.DMatrix(X_val, feature_names=X_val.columns), ntree_limit=model.best_ntree_limit)\n        test_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_test.columns), ntree_limit=model.best_ntree_limit)\n\n        oof_train[valid_idx] = valid_pred\n        oof_test[:, i] = test_pred\n        \n        importance_list.append(model.get_score(importance_type='gain'))\n\n        i += 1\n    return model, oof_train, oof_test.mean(axis=1)\n\ndef run_xgb_stratified_group(params, X_train, X_test):\n    n_splits = 10\n    verbose_eval = 10000\n    num_rounds = 5000\n    early_stop = 500\n\n\n    oof_train = np.zeros((X_train.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n\n    i = 0\n    importance_list = []\n    \n    X_test_original = X_test.copy()\n    for train_idx, valid_idx in tqdm(stratified_group_k_fold(X_train, X_train['AdoptionSpeed'].values.astype(np.int8), train_gid, k=n_splits)):\n        X_tr = X_train.iloc[train_idx, :]\n        X_val = X_train.iloc[valid_idx, :]\n        \n        X_tr, X_val, X_test = target_encoding(params['count_offset'], X_tr, X_val, X_test_original, params['sigm_scale_divider'])\n        y_tr = X_tr['AdoptionSpeed'].values\n        X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n\n        y_val = X_val['AdoptionSpeed'].values\n        X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n\n        d_train = xgb.DMatrix(data=X_tr, label=y_tr, feature_names=X_tr.columns)\n        d_valid = xgb.DMatrix(data=X_val, label=y_val, feature_names=X_val.columns)\n\n        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n        model = xgb.train(dtrain=d_train, num_boost_round=num_rounds, evals=watchlist,\n                         early_stopping_rounds=early_stop, verbose_eval=verbose_eval, params=params)\n\n        valid_pred = model.predict(xgb.DMatrix(X_val, feature_names=X_val.columns), ntree_limit=model.best_ntree_limit)\n        test_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_test.columns), ntree_limit=model.best_ntree_limit)\n\n        oof_train[valid_idx] = valid_pred\n        oof_test[:, i] = test_pred\n        \n        importance_list.append(model.get_score(importance_type='gain'))\n\n        i += 1\n    return model, oof_train, oof_test.mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aac8a52c4d4e8186664b22b885c6f28b5eedd5bb"},"cell_type":"code","source":"oof_train_list = []\noof_test_list = []\nfor i_seed in range(Config.num_seed_stratified):\n    print(i_seed)\n    gc.collect()\n    xgb_params = {\n        'eval_metric': 'rmse',\n        'seed': i_seed,\n        'eta': 0.02042285921574787,\n         'gamma': 0.0006332233837072564,\n         'max_depth': 4,\n         'min_child_weight': 12,\n         'subsample': 0.8928595200024736,\n         'colsample_bytree': 0.3894222269910513,\n        'colsample_bylevel': 0.7,\n        'tree_method': 'gpu_hist',\n        'device': 'gpu',\n        'silent': 1,\n        'count_offset': 40,\n        'sigm_scale_divider': 50\n    }\n    model, oof_train, oof_test = run_xgb(xgb_params, X_train_non_null, X_test_non_null, StratifiedKFold)\n    oof_train_list.append(oof_train)\n    oof_test_list.append(oof_test)\n    \nfor i_seed in range(Config.num_seed_group):\n    print(i_seed)\n    gc.collect()\n    xgb_params = {\n        'eval_metric': 'rmse',\n        'seed': i_seed,\n        'eta': 0.02042285921574787,\n         'gamma': 0.0006332233837072564,\n         'max_depth': 4,\n         'min_child_weight': 12,\n         'subsample': 0.8928595200024736,\n         'colsample_bytree': 0.3894222269910513,\n        'colsample_bylevel': 0.7,\n        'tree_method': 'gpu_hist',\n        'device': 'gpu',\n        'silent': 1,\n        'count_offset': 40,\n        'sigm_scale_divider': 50\n    }\n    model, oof_train, oof_test = run_xgb(xgb_params, X_train_non_null, X_test_non_null, GroupKFold)\n    oof_train_list.append(oof_train)\n    oof_test_list.append(oof_test)\n    \nfor i_seed in range(Config.num_seed_stratified_group):\n    print(i_seed)\n    gc.collect()\n    xgb_params = {\n        'eval_metric': 'rmse',\n        'seed': i_seed,\n        'eta': 0.02042285921574787,\n         'gamma': 0.0006332233837072564,\n         'max_depth': 4,\n         'min_child_weight': 12,\n         'subsample': 0.8928595200024736,\n         'colsample_bytree': 0.3894222269910513,\n        'colsample_bylevel': 0.7,\n        'tree_method': 'gpu_hist',\n        'device': 'gpu',\n        'silent': 1,\n        'count_offset': 40,\n        'sigm_scale_divider': 50\n    }\n    model, oof_train, oof_test = run_xgb_stratified_group(xgb_params, X_train_non_null, X_test_non_null)\n    oof_train_list.append(oof_train)\n    oof_test_list.append(oof_test)\noof_train = np.mean(oof_train_list, axis=0)\noof_test = np.mean(oof_test_list, axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10cc1850ab65895c63363e9f4aa30636883b5139"},"cell_type":"code","source":"xgb_oof_train, xgb_oof_test = oof_train, oof_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ce7525ff6f3813967221e7f5ed3f104c711d687"},"cell_type":"code","source":"def plot_pred(pred):\n    sns.distplot(pred, kde=True, hist_kws={'range': [0, 5]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0c2e531edd3a2b8995f4447436c24ca81a5ae83"},"cell_type":"code","source":"plot_pred(oof_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1b70b4eecafc658d0d5d3dec6acb9d770d34c1c"},"cell_type":"code","source":"plot_pred(oof_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46f205e208f24ae1b7207a3f8663c5dfd5ce0ebc"},"cell_type":"code","source":"optR = OptimizedRounder()\noptR.fit(oof_train, X_train['AdoptionSpeed'].values)\ncoefficients = optR.coefficients()\nvalid_pred = optR.predict(oof_train, coefficients)\nqwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, valid_pred)\nprint(\"QWK = \", qwk)\n\ncoefficients_ = optR.coefficients()\ntrain_predictions = optR.predict(oof_train, coefficients_).astype(np.int8)\nprint(f'train pred distribution: {Counter(train_predictions)}')\ntest_predictions = optR.predict(oof_test, coefficients_).astype(np.int8)\nprint(f'test pred distribution: {Counter(test_predictions)}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7ea02cec468403e6820500f5ff297dd7aeb6775"},"cell_type":"markdown","source":"# LightGBM"},{"metadata":{"trusted":true,"_uuid":"bbc0a1166dccf3a26e38f94e43d58aa7bbd7b9de"},"cell_type":"code","source":"import lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c5c4074e1cbab612bbdf9a7b1a7ac59bb8b608f"},"cell_type":"code","source":"def run_lgbm(params, X_train, X_test, cv, plot=False):\n    n_splits = 10\n    early_stop = 500\n    verbose_eval = 10000\n\n    try:\n        kf = cv(n_splits=n_splits, shuffle=True)\n    except:\n        kf = cv(n_splits=n_splits)\n\n    oof_train = np.zeros((X_train.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n\n    i = 0\n    importance_list = []\n    \n    X_test_original = X_test.copy()\n    for train_idx, valid_idx in tqdm(kf.split(X_train, X_train['AdoptionSpeed'].values, train_gid)):\n        X_tr = X_train.iloc[train_idx, :]\n        X_val = X_train.iloc[valid_idx, :]\n        \n        X_tr, X_val, X_test = target_encoding(params['count_offset'], X_tr, X_val, X_test_original, \n                                              params['sigm_scale_divider'])\n        y_tr = X_tr['AdoptionSpeed'].values\n        X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n\n        y_val = X_val['AdoptionSpeed'].values\n        X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n\n        d_train = lgb.Dataset(X_tr, label=y_tr, feature_name=X_tr.columns.tolist())\n        d_valid = lgb.Dataset(X_val, label=y_val, feature_name=X_val.columns.tolist())\n\n        model = lgb.train(params, d_train, valid_sets=[d_valid], early_stopping_rounds=early_stop,\n                         verbose_eval=verbose_eval)\n\n        valid_pred = model.predict(X_val, num_iteration=model.best_iteration)\n        test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\n        oof_train[valid_idx] = valid_pred\n        oof_test[:, i] = test_pred\n        if plot:\n            train_pred = model.predict(X_tr, num_iteration=model.best_iteration)\n            plot_pred(train_pred)\n            plot_pred(valid_pred)\n            plot_pred(test_pred)\n        \n\n        i += 1\n    return model, oof_train, oof_test.mean(axis=1)\n\n\ndef run_lgbm_stratified_group(params, X_train, X_test, plot=False):\n    n_splits = 10\n    early_stop = 500\n    verbose_eval = 10000\n\n    oof_train = np.zeros((X_train.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n\n    i = 0\n    importance_list = []\n    \n    X_test_original = X_test.copy()\n    for train_idx, valid_idx in tqdm(stratified_group_k_fold(X_train, X_train['AdoptionSpeed'].values.astype(np.int8), train_gid, k=n_splits)):\n        X_tr = X_train.iloc[train_idx, :]\n        X_val = X_train.iloc[valid_idx, :]\n        \n        X_tr, X_val, X_test = target_encoding(params['count_offset'], X_tr, X_val, X_test_original, \n                                              params['sigm_scale_divider'])\n        y_tr = X_tr['AdoptionSpeed'].values\n        X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n\n        y_val = X_val['AdoptionSpeed'].values\n        X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n\n        d_train = lgb.Dataset(X_tr, label=y_tr, feature_name=X_tr.columns.tolist())\n        d_valid = lgb.Dataset(X_val, label=y_val, feature_name=X_val.columns.tolist())\n\n        model = lgb.train(params, d_train, valid_sets=[d_valid], early_stopping_rounds=early_stop,\n                         verbose_eval=verbose_eval)\n\n        valid_pred = model.predict(X_val, num_iteration=model.best_iteration)\n        test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\n        oof_train[valid_idx] = valid_pred\n        oof_test[:, i] = test_pred\n        if plot:\n            train_pred = model.predict(X_tr, num_iteration=model.best_iteration)\n            plot_pred(train_pred)\n            plot_pred(valid_pred)\n            plot_pred(test_pred)\n        \n\n        i += 1\n    return model, oof_train, oof_test.mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f925ca2ba2aaedd4f5a6543315e3424b854a882"},"cell_type":"code","source":"oof_train_list = []\noof_test_list = []\n\nfor i_seed in tqdm(range(3)):\n    gc.collect()\n\n    lgb_params = {\n    'boosting_type': 'gbdt',\n    'objective': 'rmse',\n    'eta': 0.01235408383482946,\n    'subsample': 0.90892648949676, \n    'colsample_bytree': 0.3595323228044939,\n    'min_split_gain': 0.0017213784304419208,\n    'min_child_weight': 2.489294481302874e-05, \n    'min_child_samples': 3,\n        'n_estimators': 1838,\n        'max_depth': 12,\n        'count_offset': 21.197455354403893,\n        'sigm_scale_divider': 49.03532266115622,\n        'bagging_freq': 4,\n    'verbose': -1,\n    'silent': 1,\n    'n_jobs': -1,\n    'seed': i_seed\n    }\n\n\n    model, oof_train, oof_test = run_lgbm(lgb_params, X_train_non_null, X_test_non_null, StratifiedKFold)\n    oof_train_list.append(oof_train)\n    oof_test_list.append(oof_test)\n    \nfor i_seed in tqdm(range(3)):\n    gc.collect()\n\n    lgb_params = {\n    'boosting_type': 'gbdt',\n    'objective': 'rmse',\n    'eta': 0.01235408383482946,\n    'subsample': 0.90892648949676, \n    'colsample_bytree': 0.3595323228044939,\n    'min_split_gain': 0.0017213784304419208,\n    'min_child_weight': 2.489294481302874e-05, \n    'min_child_samples': 3,\n        'n_estimators': 1838,\n        'max_depth': 12,\n        'count_offset': 21.197455354403893,\n        'sigm_scale_divider': 49.03532266115622,\n        'bagging_freq': 4,\n    'verbose': -1,\n    'silent': 1,\n    'n_jobs': -1,\n    'seed': i_seed\n    }\n\n\n    model, oof_train, oof_test = run_lgbm(lgb_params, X_train_non_null, X_test_non_null, GroupKFold)\n    oof_train_list.append(oof_train)\n    oof_test_list.append(oof_test)\n    \nfor i_seed in tqdm(range(3)):\n    gc.collect()\n\n    lgb_params = {\n    'boosting_type': 'gbdt',\n    'objective': 'rmse',\n    'eta': 0.01235408383482946,\n    'subsample': 0.90892648949676, \n    'colsample_bytree': 0.3595323228044939,\n    'min_split_gain': 0.0017213784304419208,\n    'min_child_weight': 2.489294481302874e-05, \n    'min_child_samples': 3,\n        'n_estimators': 1838,\n        'max_depth': 12,\n        'count_offset': 21.197455354403893,\n        'sigm_scale_divider': 49.03532266115622,\n        'bagging_freq': 4,\n    'verbose': -1,\n    'silent': 1,\n    'n_jobs': -1,\n    'seed': i_seed\n    }\n\n\n    model, oof_train, oof_test = run_lgbm_stratified_group(lgb_params, X_train_non_null, X_test_non_null)\n    oof_train_list.append(oof_train)\n    oof_test_list.append(oof_test)\noof_train = np.mean(oof_train_list, axis=0)\noof_test = np.mean(oof_test_list, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45f41f139d633d27ec8c4400d0ba5df2d5edc84b"},"cell_type":"code","source":"plot_pred(oof_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"905a8d8847e96c40f0f0096e7efb0fe13f1a2a6f"},"cell_type":"code","source":"plot_pred(oof_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f7ea22a0d54513240201c7f77ad75e00218e01b"},"cell_type":"code","source":"optR = OptimizedRounder()\noptR.fit(oof_train, X_train['AdoptionSpeed'].values)\ncoefficients = optR.coefficients()\nvalid_pred = optR.predict(oof_train, coefficients)\nqwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, valid_pred)\nprint(\"QWK = \", qwk)\n\ncoefficients_ = optR.coefficients()\ntrain_predictions = optR.predict(oof_train, coefficients_).astype(np.int8)\nprint(f'train pred distribution: {Counter(train_predictions)}')\ntest_predictions = optR.predict(oof_test, coefficients_).astype(np.int8)\nprint(f'test pred distribution: {Counter(test_predictions)}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b829c424d684999159ae3b3072a6c967f344f10"},"cell_type":"markdown","source":"# LightGBM + Xgboost"},{"metadata":{"trusted":true,"_uuid":"bbed65c0f4c1a38db2f7c8e16a68a29c7c0a9f47"},"cell_type":"code","source":"lgb_oof_train = oof_train\nlgb_oof_test = oof_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e35e3c3cb921ce46b129f41ee8f4d6ca5b54c9f3"},"cell_type":"code","source":"best_qwk = 0\nfor weight in np.arange(0, 1.01, 0.1):\n    print(weight)\n    oof_train = weight * xgb_oof_train + (1 - weight) * lgb_oof_train\n    oof_test = weight * xgb_oof_test + (1 - weight) * lgb_oof_test\n\n    plot_pred(oof_train)\n\n    plot_pred(oof_test)\n\n    optR = OptimizedRounder()\n    optR.fit(oof_train, X_train['AdoptionSpeed'].values)\n    coefficients = optR.coefficients()\n    valid_pred = optR.predict(oof_train, coefficients)\n    qwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, valid_pred)\n    print(\"QWK = \", qwk)\n\n    coefficients_ = optR.coefficients()\n    train_predictions = optR.predict(oof_train, coefficients_).astype(np.int8)\n    print(f'train pred distribution: {Counter(train_predictions)}')\n    test_predictions = optR.predict(oof_test, coefficients_).astype(np.int8)\n    print(f'test pred distribution: {Counter(test_predictions)}')\n    \n    if qwk > best_qwk:\n        best_weight = weight\n        best_qwk = qwk\n        best_test_predictions = test_predictions\n\nprint(best_weight, best_qwk)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68c04615fb889da9043d7ea40dfd8bd26bd171e2"},"cell_type":"code","source":"submission = pd.DataFrame({'PetID': test['PetID'].values, 'AdoptionSpeed': best_test_predictions})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a93eb10569e4ed0fb74889920094ba8cb22f96a6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}