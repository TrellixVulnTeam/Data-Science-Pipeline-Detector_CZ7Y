{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import gc\nimport glob\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport pprint\n\nimport numpy as np\nimport pandas as pd\n\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom sklearn.metrics import cohen_kappa_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\nimport scipy as sp\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom functools import partial\nfrom collections import Counter\n\nimport random\nimport math","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"425c3038e1bf6e8ca4cb768235bc089c297c654c"},"cell_type":"markdown","source":"评价函数"},{"metadata":{"trusted":true,"_uuid":"013e09d688f8f8079f37a3936357323dd23514fc"},"cell_type":"code","source":"#%% 评价函数 Metric used for this competition \n# (Quadratic Weigthed Kappa aka Quadratic Cohen Kappa Score)\ndef metric(y1,y2):\n    return cohen_kappa_score(y1, y2, weights = 'quadratic')\n\n\n# Make scorer for scikit-learn\nscorer = make_scorer(metric)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"255eb8dab6f00e3814201ac107558a701ae97965"},"cell_type":"markdown","source":"Cross验证函数"},{"metadata":{"trusted":true,"_uuid":"e1cf0c47fc21182a0890aff67c95015c2e19786e"},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\n#\ndef split_score(model, x_train, y_train, x_test, n=10):\n    \n    y_pre = np.zeros(x_train.shape[0])\n    oof_test = np.zeros((x_test.shape[0], n))\n    \n    kfold = StratifiedKFold(n_splits=n, random_state=4)\n    aaa = 0\n    \n    for train_index, test_index in kfold.split(x_train, y_train):\n        model.fit(x_train.iloc[train_index], y_train.iloc[train_index])\n        y_pre[test_index] = model.predict(x_train.iloc[test_index])\n        test_pred = model.predict(x_test)\n        oof_test[:, aaa] = test_pred\n        \n        print(aaa)\n        aaa += 1\n    \n#    score = metric(y_pre, y)\n    print(\"{}折后的Kappa加权得分为:带补充\".format(n))\n    \n    return y_pre, oof_test\n\n#\ndef fix_y(y, coef):\n    y_fix = np.copy(y)\n    for i, pred in enumerate(y_fix):\n        if pred < coef[0]:\n            y_fix[i] = 0\n        elif pred >= coef[0] and pred < coef[1]:\n            y_fix[i] = 1\n        elif pred >= coef[1] and pred < coef[2]:\n            y_fix[i] = 2\n        elif pred >= coef[2] and pred < coef[3]:\n            y_fix[i] = 3\n        else:\n            y_fix[i] = 4    \n    return y_fix\n\n# \ndef _kappa_loss(y, y_true, coef):\n    y_fix = np.copy(y)\n    for i, pred in enumerate(y_fix):\n        if pred < coef[0]:\n            y_fix[i] = 0\n        elif pred >= coef[0] and pred < coef[1]:\n            y_fix[i] = 1\n        elif pred >= coef[1] and pred < coef[2]:\n            y_fix[i] = 2\n        elif pred >= coef[2] and pred < coef[3]:\n            y_fix[i] = 3\n        else:\n            y_fix[i] = 4\n            \n    loss = metric(y_fix, y_true)\n    return -loss\n\n# 寻找分类的最佳参数\ndef search_coef(x1, x2):\n    loss_partial = partial(_kappa_loss, x1, x2)\n    initial_coef = [1.55, 2.05, 2.5, 3]\n    coef = sp.optimize.basinhopping(loss_partial, initial_coef, niter=500, T=1,\n                                              stepsize=0.2, minimizer_kwargs={\"method\": 'nelder-mead'}, \n                                              take_step=None, accept_test=None, callback=None, \n                                              interval=100, disp=True, niter_success=10, seed=None)\n\n    return coef","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"400d4f04ad852923698bd0695e7b1deca8325f4a"},"cell_type":"markdown","source":"读取数据"},{"metadata":{"trusted":true,"_uuid":"e3c49a8c122b0bed05bbdbde953f89fe6c87b8ae"},"cell_type":"code","source":"df_train  = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\ndf_test   = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')\n\ntrain = df_train.copy()\ntest  = df_test.copy()\n\nlabels_breed = pd.read_csv('../input/petfinder-adoption-prediction/breed_labels.csv')\nlabels_state = pd.read_csv('../input/petfinder-adoption-prediction/color_labels.csv')\nlabels_color = pd.read_csv('../input/petfinder-adoption-prediction/state_labels.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"496fba0f25d24d359dcc33d501cc473923f5bcdb"},"cell_type":"markdown","source":"> **** 提取 sentiment 的特征"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"811c4614a8b0c1320e296ba5bda5caa3323e3b12","scrolled":true},"cell_type":"code","source":"def extract_sentiment_feature(i, x):    \n#    feature_sentiment = pd.DataFrame(columns=['PetID', 'token', 'sentence_magnitude', 'sentence_score','document_magnitude', 'document_score'])\n    feature_sentiment = pd.DataFrame()\n\n    if x == 'train':\n        set_file = 'train'\n    else:\n        set_file = 'test' \n        \n    file_name = '../input/petfinder-adoption-prediction/{}_sentiment/{}.json'.format(set_file,i)\n    try:\n        with open(file_name, 'r', encoding='utf-8') as f:\n            sentiment_file = json.load(f)\n\n            token = [x['name'] for x in sentiment_file['entities']]\n            token = ' '.join(token)\n\n            sentences_sentiment = [x['sentiment'] for x in sentiment_file['sentences']]\n            sentences_sentiment = pd.DataFrame.from_dict(\n                sentences_sentiment, orient='columns')\n\n            \n            docementSentiment_magnitude = sentiment_file['documentSentiment']['magnitude']\n            documentSentiment_score     = sentiment_file['documentSentiment']['score']\n            \n            new = pd.DataFrame(\n                    {'PetID'    : i, \n\n                    'magnitude_sum' : sentences_sentiment['magnitude'].sum(axis=0),\n                    'score_sum'     : sentences_sentiment['score'].sum(axis=0),\n                    'magnitude_mean': sentences_sentiment['magnitude'].mean(axis=0),\n                    'score_mean'    : sentences_sentiment['score'].mean(axis=0),\n                    'magnitude_var' : sentences_sentiment['magnitude'].var(axis=0),\n                    'score_var'     : sentences_sentiment['score'].var(axis=0),\n\n                    'document_magnitude'  : [docementSentiment_magnitude], \n                    'document_score'      : [documentSentiment_score]})  \n            feature_sentiment = feature_sentiment.append(new)\n    except:\n        print('{}没找到'.format(file_name))\n    \n    for each in feature_sentiment.columns:\n        if each not in ['PetID','token']:\n            feature_sentiment[each] = feature_sentiment[each].astype(float)\n\n    return feature_sentiment\n\n#%%\ntrain_feature_sentiment = Parallel(n_jobs=8, verbose=1)(\n        delayed(extract_sentiment_feature)(i, 'train') for i in train.PetID)\ntrain_feature_sentiment = [x for x in train_feature_sentiment]\ntrain_feature_sentiment = pd.concat(train_feature_sentiment, ignore_index=True, sort=False)\n\ntest_feature_sentiment = Parallel(n_jobs=8, verbose=1)(\n        delayed(extract_sentiment_feature)(i, 'test') for i in test.PetID)\ntest_feature_sentiment = [x for x in test_feature_sentiment]\ntest_feature_sentiment = pd.concat(test_feature_sentiment, ignore_index=True, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f7c133324169c1ad2e8bfe45c59052d1c5d8d04"},"cell_type":"markdown","source":"# 加入metadata和图像大小、神经网络提取的元素"},{"metadata":{"trusted":true,"_uuid":"8e8735ddfbf946d2806a4b72edc71524c3082874","scrolled":true},"cell_type":"code","source":"picture_metadata  = pd.read_csv(r'../input/32111111/picture.csv')\npicture_size      = pd.read_csv(r'../input/32111111/picture_size.csv')\nimg_features      = pd.read_csv(r'../input/32111111/img_features.csv')\n\n#%% 连接sentiment和metadata和原始数据\nx_train = df_train.merge(train_feature_sentiment, how='left', on='PetID')\nx_train = x_train.merge(picture_metadata, how='left', on='PetID')\nx_train = x_train.merge(picture_size, how='left', on='PetID')\nx_train = x_train.merge(img_features, how='left', on='PetID')\n\ny_train = x_train['AdoptionSpeed']\nx_train.drop(['AdoptionSpeed'], axis=1, inplace=True)\n\nx_test = df_test.merge(test_feature_sentiment, how='left', on='PetID')\nx_test = x_test.merge(picture_metadata, how='left', on='PetID')\nx_test = x_test.merge(picture_size, how='left', on='PetID')\nx_test  = x_test.merge(img_features, how='left', on='PetID')\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa74a0b28dee75f0ce643b5ba95f7c4a9c62f540"},"cell_type":"markdown","source":"# NLP"},{"metadata":{"trusted":true,"_uuid":"fd7345f91a2b2408e8945e83ca7b730d42065c67"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import SparsePCA, TruncatedSVD, LatentDirichletAllocation, NMF\n\ncol_text = ['Description']\n\nx = x_train.append(x_test).reset_index()\nx = x[['Description', 'PetID']]\n\nn_components = 50\n\nx[col_text] = x[col_text].fillna('MISSING')\ntext_features = []\n\n\nfor i in  ['Description']:\n    svd_ = TruncatedSVD(n_components=n_components)\n        \n    tfv = CountVectorizer(min_df=3,  \n                          max_df=0.9,\n                          stop_words = 'english')\n    \n    tfidf_col = tfv.fit_transform(x.loc[:, i])\n\n    \n    svd_col = svd_.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix('SVD_{}_'.format(i))\n\n\n    text_features.append(svd_col)    \n    \n    x.drop(i, axis=1, inplace=True)\n    \n# Combine all extracted features:\ntext_features = pd.concat(text_features, axis=1)\n\n# Concatenate with main DF:\nx = pd.concat([x, text_features], axis=1)\n\nx_train = x_train.merge(x, how='left', on='PetID')\nx_test  = x_test.merge(x, how='left', on='PetID')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4441300bb638db93b6a97f86d244f01408000f5f"},"cell_type":"markdown","source":"# 假如情感元素"},{"metadata":{"trusted":true,"_uuid":"3292b9df94b6a95a8bae57d22dc863363519f86e"},"cell_type":"code","source":"from textblob import TextBlob\n\nx = x_train.append(x_test)\nx = x[['PetID', 'Description']]\n\nx['Description'] = x['Description'].fillna(\"Missing\")\nx['Description'] = x['Description'].apply(lambda x:TextBlob(x))\n\nx['polarity']     = x['Description'].apply(lambda x:x.sentiment[0])\nx['subjectivity'] = x['Description'].apply(lambda x:x.sentiment[1])\n\n#对情感进行分箱\nbin=[-2,0,0.3,2]\nx['polarity'] = pd.cut(x['polarity'], bins=bin, labels=range(3))\nx['polarity'] = x['polarity'].astype(np.int32)\n\nx_train = x_train.merge(x[['PetID', 'polarity']], how='left', on='PetID')\nx_test  = x_test.merge(x[['PetID', 'polarity']], how='left', on='PetID')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ff1334e7d28c8c0673b8c72d822e69a6ad6aded"},"cell_type":"markdown","source":"# 增加新特征"},{"metadata":{"trusted":true,"_uuid":"7acee37199e6cdc3e1e03898a1b9dd32178006ae"},"cell_type":"code","source":"#是否是免费的\nx_train['IsFree'] = x_train['Fee'].apply(lambda x:True if x == 0 else False)\nx_test['IsFree']  = x_test['Fee'].apply(lambda x:True if x == 0 else False)\n\n#年龄（按年）\nx_train['Year'] = x_train['Age'].apply(lambda x:math.floor(x/12))\nx_test['Year']  = x_test['Age'].apply(lambda x:math.floor(x/12))\n\n#年龄分箱,按频划成5份\nx = x_train.append(x_test)\nx['Age_qcut'] = pd.qcut(x['Age'], 5,  duplicates='drop')\nx['Age_qcut'] = pd.factorize(x['Age_qcut'])[0]\nx_train = x_train.merge(x[['PetID','Age_qcut']], how='left', on='PetID')\nx_test  = x_test.merge(x[['PetID','Age_qcut']], how='left', on='PetID')\n\n\n#血缘的种类\nx = x_train.append(x_test).reset_index()\nBreed1_count = x.groupby('Breed1').size().to_frame('Breed1_count').reset_index()\nx_train = x_train.merge(Breed1_count, how='left', on='Breed1')\nx_test  = x_test.merge(Breed1_count, how='left', on='Breed1')\n\n#是否稀有\na = x['Breed1'].value_counts().sort_values(ascending = False).cumsum()/len(x)\nrare1_index = a[a > 0.85].index.tolist()\nx_train['IsRare1'] = x_train['Breed1'].isin(rare1_index).apply(lambda x:True if x == True else False)\nx_test['IsRare1']  = x_test['Breed1'].isin(rare1_index).apply(lambda x:True if x == True else False)\nrare2_index = a[a > 0.72].index.tolist()\nx_train['IsRare2'] = x_train['Breed1'].isin(rare2_index).apply(lambda x:True if x == True else False)\nx_test['IsRare2']  = x_test['Breed1'].isin(rare2_index).apply(lambda x:True if x == True else False)\n\n#是否常见\nx_train['Is_COMMON'] = x_train['Breed1'].apply(lambda x:True if (x == 265 or x == 307 or x == 266) else False)\nx_test['Is_COMMON']  = x_test['Breed1'].apply(lambda x:True if (x == 265 or x == 307 or x == 266) else False)\n\n#照片分箱\nbin=[-0.5,0.5,1.5,4.5,1000]\nx_train['Photo_cut'] = pd.cut(x_train['PhotoAmt'], bins=bin, labels=range(4)).astype(np.int32)\nx_test['Photo_cut']  = pd.cut(x_test['PhotoAmt'], bins=bin, labels=range(4)).astype(np.int32)\n\n# 是否是稀有颜色\nx_train['Is_rare_color1'] = x_train['Color1'].apply(lambda x:True if x==5 or x==6 or x==7 else False)\nx_test['Is_rare_color1'] = x_test['Color1'].apply(lambda x:True if x==5 or x==6 or x==7 else False)\nx_train['Is_rare_color2'] = x_train['Color2'].apply(lambda x:True if x==6 else False)\nx_test['Is_rare_color2'] = x_test['Color2'].apply(lambda x:True if x==6 else False)\n\n#年龄是否小于二月\nx_train['Is_less_than_2month']= x_train['Age'].apply(lambda x:True if x<3 else False)\nx_test['Is_less_than_2month'] = x_test['Age'].apply(lambda x:True if x<3 else False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0689592a65ec1b7163fc18f9b3cbc2b42a9a9785"},"cell_type":"markdown","source":"# RescuerID处理"},{"metadata":{"trusted":true,"_uuid":"dc4315fe441d9d1ed237682c6a4e7a3e54eebff4"},"cell_type":"code","source":"#%% RescuerID 处理\n\ndf = df_train.append(df_test)\ndata_rescuer = df.groupby(['RescuerID'])['PetID'].size().reset_index()\ndata_rescuer.columns = ['RescuerID', 'RescuerID_count']\n#data_rescuer['rank_Rescuer_count'] = data_rescuer['RescuerID_count'].rank(pct=True)\n\nx_train = x_train.merge(data_rescuer, how='left', on='RescuerID')\nx_test  = x_test.merge(data_rescuer, how='left', on='RescuerID')\n\nx = x_train.append(x_test)\nx['RescuerID_count_cut'] = pd.qcut(x['RescuerID_count'], 5, labels=range(4), duplicates='drop').astype(np.int32)\n\nx_train = x_train.merge(x[['PetID', 'RescuerID_count_cut']], how='left', on='PetID')\nx_test  = x_test.merge(x [['PetID', 'RescuerID_count_cut']], how='left', on='PetID')\n\n#x_train.drop(['RescuerID_count'], axis=1, inplace=True)\n#x_test.drop(['RescuerID_count'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f730634a3cb5c645e7eeb5ef5963593018864445"},"cell_type":"markdown","source":"# 处理Breed"},{"metadata":{"trusted":true,"_uuid":"efc5b36bc555a32cc57b72d732adc77dd14c1104"},"cell_type":"code","source":"# 增加特征 是否有第二血统\nx_train['HasSecondBreed'] = x_train['Breed2'].map(lambda x:True if x != 0 else False)\nx_test['HasSecondBreed'] = x_test['Breed2'].map(lambda x:True if x != 0 else False)\n\ntrain_breed_main = x_train[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\n\ntrain_breed_main = train_breed_main.iloc[:, 2:]\ntrain_breed_main = train_breed_main.add_prefix('main_breed_')\n\ntrain_breed_second = x_train[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\n\ntrain_breed_second = train_breed_second.iloc[:, 2:]\ntrain_breed_second = train_breed_second.add_prefix('second_breed_')\n\nx_train = pd.concat(\n    [x_train, train_breed_main, train_breed_second], axis=1)\n\n##############\ntest_breed_main = x_test[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\n\ntest_breed_main = test_breed_main.iloc[:, 2:]\ntest_breed_main = test_breed_main.add_prefix('main_breed_')\n\ntest_breed_second = x_test[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\n\ntest_breed_second = test_breed_second.iloc[:, 2:]\ntest_breed_second = test_breed_second.add_prefix('second_breed_')\n\nx_test = pd.concat(\n    [x_test, test_breed_main, test_breed_second], axis=1)\n\nprint(x_train.shape, x_test.shape)\n\ncategorical_columns = ['main_breed_BreedName', 'second_breed_BreedName']\n#for i in categorical_columns:\n#    x_train.loc[:, i] = pd.factorize(x_train.loc[:, i])[0]\n#    x_test.loc[:,i]   = pd.factorize(x_test.loc[:, i])[0]\n\n# 增加特征 是否纯种\nx_train['True_Pure'] = False\nx_train.loc[(x_train['main_breed_BreedName'] != 'Mixed Breed')&\n                    ((x_train['main_breed_BreedName'] == x_train['second_breed_BreedName'])|\n                   (x_train['second_breed_BreedName'].isnull())),'True_Pure'] = True\n\n\nx_test['True_Pure'] = False\nx_test.loc[(x_test['main_breed_BreedName'] != 'Mixed Breed')&\n                    ((x_test['main_breed_BreedName'] == x_test['second_breed_BreedName'])|\n                   (x_test['second_breed_BreedName'].isnull())),'True_Pure'] = True\n\n# 是否纯种狗\nx_train['Is_Pure_Dog'] = (x_train['True_Pure'] == True) & (x_train['Type'] == 1)\nx_test['Is_Pure_Dog']  = (x_test['True_Pure'] == True)  & (x_test['Type'] == 1)\n\n\n\n#删除没用特征\nx_train.drop(['main_breed_BreedName', 'second_breed_BreedName', 'main_breed_Type', 'second_breed_Type'], axis=1, inplace=True)\nx_test.drop(['main_breed_BreedName', 'second_breed_BreedName', 'main_breed_Type', 'second_breed_Type'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b65a5728cadded14aa00cce12fad4689e05e8f7"},"cell_type":"markdown","source":"# 数据清理"},{"metadata":{"trusted":true,"_uuid":"4965fddd1068feaa915443f8002a13b0da0c9721"},"cell_type":"code","source":"drop_columns = ['Name', 'RescuerID', 'Description', 'PetID', 'token', 'annots_top_desc']\ndrop_columns = ['Name', 'RescuerID', 'Description', 'PetID']\n\nx_train.drop(drop_columns, axis=1, inplace=True)\nx_test.drop(drop_columns, axis=1, inplace=True)\n\nx_train = x_train.fillna(0)\nx_test  = x_test.fillna(0)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a60f16598b4753d385eb11efb1f13c553189bd4e"},"cell_type":"markdown","source":"# 转换成类别属性"},{"metadata":{"trusted":true,"_uuid":"fd3201d4911dbea46c8672411090199649fbf4f8"},"cell_type":"code","source":"#属性标签\nc = ['Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3',\n    'Vaccinated', 'Dewormed', 'Sterilized', 'Health', 'Quantity',\n    'State', 'IsFree', 'Year', 'Age_qcut', 'IsRare1', 'IsRare2',\n    'Is_COMMON', 'Photo_cut', 'Is_rare_color1', 'Is_rare_color2', 'Is_less_than_2month',\n    'RescuerID_count_cut', 'HasSecondBreed', 'True_Pure', 'Is_Pure_Dog']\n\nfor each in c:\n    x_train[each] = x_train[each].astype('category')\n    x_test[each] = x_test[each].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78944d4a86c6ba779919210c3d40e62415ecbf4e"},"cell_type":"markdown","source":"LGB"},{"metadata":{"trusted":true,"_uuid":"bfd63d1f074467dc3052ebdd694862b42a4bc550"},"cell_type":"code","source":"from lightgbm.sklearn import LGBMRegressor\n\nmodel_lgb = LGBMRegressor(\n        learning_rate    = 0.01,\n        n_estimators     = 2000,\n        max_depth        = 4,\n        num_leaves       = 12 ,\n        subsample        = 0.8,      #训练时采样一定比例的数据\t\n        colsample_bytree = 0.8,\n        n_jobs           = -1,\n        random_state     = 44,\n        objective        = 'regression',\n#        reg_alpha        = 1,\n        eval_metric      = 'scorer',\n        min_child_samples = 15         #叶子节点具有的最小记录数\t\n        )\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf3bd57f1afbe87711c02fee7d71c2e0edd43f0c"},"cell_type":"code","source":"y_lgb, y_test_pre = split_score(model_lgb, x_train, y_train, x_test)\ny_test_pre = y_test_pre.mean(axis=1)\n\ncoe = search_coef(y_lgb, y_train)\nbest_lgb_coe = coe['x']\nprint('lgb的最佳系数为{}'.format(best_lgb_coe))\n\nmodel_lgb.fit(x_train, y_train)\nresult_lgb_fix = fix_y(y_test_pre, best_lgb_coe)\nprint('lgb后的分布:',Counter(result_lgb_fix))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a60127f4be3925dfce8a623fb0a567b41024f492"},"cell_type":"markdown","source":"后处理"},{"metadata":{"trusted":true,"_uuid":"27319f2c6bf6d073103ab22df2b223b942ccfc2b"},"cell_type":"code","source":"submission_lgb = pd.DataFrame({'PetID': df_test['PetID'].values, 'AdoptionSpeed': result_lgb_fix.astype(np.int32)})\nsubmission_lgb.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42aeb1c582c0f3e92586fdbd9a3f9c51fe038f2e"},"cell_type":"code","source":"#submission_xgb = pd.DataFrame({'PetID': df_test['PetID'].values, 'AdoptionSpeed': result_xgb_fix.astype(np.int32)})\n#submission_xgb.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}