{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfrom sklearn.decomposition import PCA\nimport json\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../input/keras-pretrained-models/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\ntrain_label = train.pop(\"AdoptionSpeed\")\n\ntrain[\"traintest\"] = \"train\"\nprint(train.shape)\n\ntest = pd.read_csv(\"../input/petfinder-adoption-prediction/test/test.csv\")\ntest[\"traintest\"] = \"test\"\nprint(test.shape)\n\nfulldf = pd.concat([train,test],axis = 0)\nprint(fulldf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fulldf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fulldf.loc[fulldf['PetID'] == \"269c5b546\"][\"traintest\"].values[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_feature (feature_column,original_dataset,join_column_name):\n    if (type(feature_column) != pd.DataFrame):\n        feature_column = pd.DataFrame(feature_column) \n    if (join_column_name not in feature_column.columns):\n        print(\"No ID column to merge on\")\n        return\n    \n    new_dataset = original_dataset.merge(original_dataset.merge(feature_column, how='left', on=join_column_name, sort=False))\n    return new_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ft = pd.DataFrame(fulldf.groupby(\"RescuerID\")[\"RescuerID\"].count())\nft.columns = ['RescuerExp']\nft = ft.reset_index()\nft.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fulldf = add_feature (ft,fulldf,\"RescuerID\")\nfulldf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = fulldf[fulldf.Age < 30]\nplt.hist(df[\"Age\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fulldf['RescuerExpRank'] = fulldf[\"RescuerExp\"].rank(method='min')\n\nfulldf['AgeCat'] = np.where(fulldf['Age'] < 6, 0, 1)\nfulldf['AgeCat'] = fulldf['AgeCat'].astype(\"category\")\n\nfulldf['Named'] = np.where(fulldf['Name'] == \"No Name Yet\",0, 1)\nfulldf['Named'] = fulldf['Named'].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(fulldf[\"RescuerExpRank\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_type = fulldf.loc[:,\"Vaccinated\":\"Health\"]\npca = PCA(n_components=3)\npca.fit(df_type)\ndf_pcatype = pca.transform(df_type)\ndf_pcatype.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fulldf[\"PCA_health1\"] =  df_pcatype[:,0]\nfulldf[\"PCA_health2\"] =  df_pcatype[:,1]\nfulldf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_type = fulldf.loc[:,\"Breed1\":\"FurLength\"]\npca = PCA(n_components=3)\npca.fit(df_type)\ndf_pcatype = pca.transform(df_type)\ndf_pcatype.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fulldf[\"PCA_type1\"] =  df_pcatype[:,0]\nfulldf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images = os.listdir(\"../input/petfinder-adoption-prediction/train_images\")\ntrain_folder = \"../input/petfinder-adoption-prediction/train_images/\"\n\ntest_images = os.listdir(\"../input/petfinder-adoption-prediction/test_images\")\ntest_folder = \"../input/petfinder-adoption-prediction/test_images/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#On ne garde que les premieres images\n\ncompteur = 0\n\nfor i in range(len(train_images)):\n    if (train_images[i].split(\"-\")[1].split(\".\")[0] == \"1\"):\n        compteur += 1\n        \nprint(compteur)\n\ncompteur2 = 0\n\nfor i in range(len(test_images)):\n    if (test_images[i].split(\"-\")[1].split(\".\")[0] == \"1\"):\n        compteur2 += 1\n        \nprint(compteur2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing import image\nimport os\nfrom keras.applications.vgg16  import VGG16,preprocess_input\n\nmodel = VGG16(weights='../input/keras-pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top=False)\n\nextracted_features = np.zeros((compteur, 7 * 7 * 512))\nids = np.empty((compteur,1),dtype=object)\nline = 0\n\nfor i in range(len(train_images)):\n    if (train_images[i].split(\"-\")[1].split(\".\")[0] == \"1\"):\n        img_path = train_folder + train_images[i]\n        img_id = train_images[i].split(\"-\")[0]\n        img = image.load_img(img_path, target_size=(224, 224))\n        x = image.img_to_array(img)\n        x = np.expand_dims(x, axis=0)\n        x = preprocess_input(x)\n        features = model.predict(x)\n        features = np.reshape(features, (1, 7 * 7 * 512))\n        extracted_features[line,:] = features\n        ids[line,0] = img_id\n        line += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Refaire la même chose pour les images test et merge train_test avant PCA\nextracted_features2 = np.zeros((compteur2, 7 * 7 * 512))\nids2 = np.empty((compteur2,1),dtype=object)\nline2 = 0\n\nfor i in range(len(test_images)):\n    if (test_images[i].split(\"-\")[1].split(\".\")[0] == \"1\"):\n        img_path = test_folder + test_images[i]\n        img_id = test_images[i].split(\"-\")[0]\n        img = image.load_img(img_path, target_size=(224, 224))\n        x = image.img_to_array(img)\n        x = np.expand_dims(x, axis=0)\n        x = preprocess_input(x)\n        features = model.predict(x)\n        features = np.reshape(features, (1, 7 * 7 * 512))\n        extracted_features2[line2,:] = features\n        ids2[line2,0] = img_id\n        line2 += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_extracted_features = np.vstack((extracted_features,extracted_features2))\nfull_extracted_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#release memory\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del extracted_features\ndel extracted_features2\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nstd_scaler = StandardScaler()\nfull_extracted_features = std_scaler.fit_transform(full_extracted_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=20)\ndf_pcaimgs = pca.fit_transform(full_extracted_features)\nprint(df_pcaimgs.shape)\nprint(np.sum(pca.explained_variance_ratio_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(pca.explained_variance_ratio_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del full_extracted_features\ngc.collect()\n\nfull_ids = np.vstack((ids,ids2))\ndel ids\ndel ids2\nfull_ids.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(np.hstack((full_ids,df_pcaimgs)))\n\ndel full_ids\ndel df_pcaimgs\ngc.collect()\n\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For merging\ncols = [\"pca_img\" + str(i) for i in range(df.shape[1])]\ncols[0] = \"PetID\"\ndf.columns = cols\n\ndel cols\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fulldf = add_feature (df,fulldf,\"PetID\")\n\ndel df\ngc.collect()\n\nfulldf.loc[:,\"pca_img1\":\"pca_img20\"] = fulldf.loc[:,\"pca_img1\":\"pca_img20\"].astype(\"float\")\n\nfulldf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extraction des métadonnées\n\ntrain_metadata = os.listdir(\"../input/petfinder-adoption-prediction/train_metadata/\")\ntrain_metadata_folder = \"../input/petfinder-adoption-prediction/train_metadata/\"\n\ntest_metadata = os.listdir(\"../input/petfinder-adoption-prediction/test_metadata/\")\ntest_metadata_folder = \"../input/petfinder-adoption-prediction/test_metadata/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vertex_xs = []\nvertex_ys = []\nbounding_confidences = []\nbounding_importance_fracs = []\ndominant_blues = []\ndominant_greens = []\ndominant_reds = []\ndominant_pixel_fracs = []\ndominant_scores = []\nlabel_descriptions = []\nlabel_scores = []\nface_annotations = []\nlabel_annotations = []\ntext_annotations = []\npet_ids = []\n\nnf_count = 0\nnl_count = 0\nfor pet in fulldf.PetID:\n    if (fulldf.loc[fulldf['PetID'] == pet][\"traintest\"].values[0] == \"train\") :\n        try:\n            with open(train_metadata_folder + pet + '-1.json', 'r') as f:\n                data = json.load(f)\n\n            pet_ids.append(pet)    \n            face_annotations.append(data.get('faceAnnotations', []))\n            text_annotations.append(data.get('textAnnotations', []))\n            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n            vertex_xs.append(vertex_x)\n            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n            vertex_ys.append(vertex_y)\n            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n            bounding_confidences.append(bounding_confidence)\n            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n            bounding_importance_fracs.append(bounding_importance_frac)\n            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n            dominant_blues.append(dominant_blue)\n            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n            dominant_greens.append(dominant_green)\n            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n            dominant_reds.append(dominant_red)\n            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n            dominant_pixel_fracs.append(dominant_pixel_frac)\n            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n            dominant_scores.append(dominant_score)\n            if data.get('labelAnnotations'):\n                label_annotations.append(data['labelAnnotations'])\n                label_description = data['labelAnnotations'][0]['description']\n                label_descriptions.append(label_description)\n                label_score = data['labelAnnotations'][0]['score']\n                label_scores.append(label_score)\n            else:\n                nl_count += 1\n                label_annotations.append([])\n                label_descriptions.append('nothing')\n                label_scores.append(-1)\n        except FileNotFoundError:\n            pet_ids.append(pet)\n            nf_count += 1\n            vertex_xs.append(-1)\n            vertex_ys.append(-1)\n            bounding_confidences.append(-1)\n            bounding_importance_fracs.append(-1)\n            dominant_blues.append(-1)\n            dominant_greens.append(-1)\n            dominant_reds.append(-1)\n            dominant_pixel_fracs.append(-1)\n            dominant_scores.append(-1)\n            label_annotations.append([])\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n            face_annotations.append([])\n            text_annotations.append([])\n    else:\n        try:\n            with open(test_metadata_folder + pet + '-1.json', 'r') as f:\n                data = json.load(f)\n\n            pet_ids.append(pet)    \n            face_annotations.append(data.get('faceAnnotations', []))\n            text_annotations.append(data.get('textAnnotations', []))\n            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n            vertex_xs.append(vertex_x)\n            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n            vertex_ys.append(vertex_y)\n            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n            bounding_confidences.append(bounding_confidence)\n            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n            bounding_importance_fracs.append(bounding_importance_frac)\n            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n            dominant_blues.append(dominant_blue)\n            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n            dominant_greens.append(dominant_green)\n            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n            dominant_reds.append(dominant_red)\n            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n            dominant_pixel_fracs.append(dominant_pixel_frac)\n            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n            dominant_scores.append(dominant_score)\n            if data.get('labelAnnotations'):\n                label_annotations.append(data['labelAnnotations'])\n                label_description = data['labelAnnotations'][0]['description']\n                label_descriptions.append(label_description)\n                label_score = data['labelAnnotations'][0]['score']\n                label_scores.append(label_score)\n            else:\n                nl_count += 1\n                label_annotations.append([])\n                label_descriptions.append('nothing')\n                label_scores.append(-1)\n        except FileNotFoundError:\n            nf_count += 1\n            pet_ids.append(pet)\n            vertex_xs.append(-1)\n            vertex_ys.append(-1)\n            bounding_confidences.append(-1)\n            bounding_importance_fracs.append(-1)\n            dominant_blues.append(-1)\n            dominant_greens.append(-1)\n            dominant_reds.append(-1)\n            dominant_pixel_fracs.append(-1)\n            dominant_scores.append(-1)\n            label_annotations.append([])\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n            face_annotations.append([])\n            text_annotations.append([])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_descs = []\nfor label in label_descriptions:\n    if (label == \"cat\"):\n        label_descs.append(0)\n    elif (label == \"dog\"):\n        label_descs.append(1)\n    else:\n        label_descs.append(2)\n        \n        \nmetadata_df = pd.DataFrame(np.vstack((pet_ids,vertex_xs,vertex_ys,bounding_confidences,bounding_importance_fracs,dominant_blues,\n                        dominant_greens,dominant_reds,dominant_pixel_fracs,dominant_scores,label_descs,label_scores)))\n\nmetadata_df = metadata_df.T\n\ncols = [\"PetID\",\"vertex_x\",\"vertex_y\",\"bounding_confidences\",\"bounding_importance_fracs\",\"dominant_blues\",\"dominant_greens\",\"dominant_reds\",\n        \"dominant_pixel_fracs\",\"dominant_scores\",\"label_descriptions\",\"label_scores\"]\nmetadata_df.columns = cols\n\nfor col in cols:\n    if not ((col == \"PetID\") or (col == \"label_descriptions\")):\n        metadata_df[col] = metadata_df[col].astype(\"float64\")\n\nmetadata_df[\"label_descriptions\"] = metadata_df[\"label_descriptions\"].astype('category')\nprint(metadata_df.dtypes)\n\nmetadata_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fulldf = add_feature (metadata_df,fulldf,\"PetID\")\n\ndel metadata_df\ngc.collect()\n\nfulldf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fulldf[\"Description\"] = fulldf[\"Description\"].astype(str) \nfulldf[\"Description\"] = fulldf[\"Description\"].astype(str).fillna('missing') # FILL NA\nfulldf[\"Description\"] =fulldf[\"Description\"].str.lower() \nfulldf[\"Description\"+ '_num_words'] = fulldf[\"Description\"].apply(lambda comment: len(comment.split())) # Count number of Words\nfulldf[\"Description\"+ '_num_unique_words'] = fulldf[\"Description\"].apply(lambda comment: len(set(w for w in comment.split())))  # Count Unique Words\nfulldf[\"Description\"+ '_words_vs_unique'] = fulldf[\"Description\" + '_num_unique_words'] / fulldf[\"Description\"+'_num_words'] * 100 # \n\nfulldf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer\n\nword_vect = TfidfVectorizer(\n            sublinear_tf=True,\n            strip_accents='unicode',\n            analyzer='word',\n            token_pattern=r'\\w{1,}',\n            stop_words='english',\n            ngram_range=(1, 2),\n            max_features=20000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vect.fit(fulldf['Description'])\nword_features  = word_vect.transform(fulldf['Description'])\n\nprint(type(word_features))\nword_features.get_shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vect.get_feature_names()[9000:9005]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fulldf_word_features = pd.DataFrame(word_features.toarray())\n\ndel word_features\ndel word_vect\ngc.collect()\n\nfulldf_word_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\nstd_scaler = StandardScaler()\nfulldf_word_features = std_scaler.fit_transform(fulldf_word_features)\n\n\nsvd = TruncatedSVD(n_components=20, random_state=42)\n\nfulldf_svd_word_features = pd.DataFrame(svd.fit_transform(fulldf_word_features))\n\ncols = [\"desc_svd_tfid\" + str(i+1) for i in range(fulldf_svd_word_features.shape[1])]\nfulldf_svd_word_features.columns = cols\n\n\ndel fulldf_word_features\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(svd.explained_variance_ratio_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fulldf = pd.concat([fulldf,pd.DataFrame(fulldf_svd_word_features)],axis=1)\n\ndel fulldf_svd_word_features\ngc.collect()\n\nfulldf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ajouter features sentiments\nsamplesentiment = pd.read_json('../input/petfinder-adoption-prediction/train_sentiment/{}'.format(\"4fdebca57.json\"), orient='index', typ='series')\nprint(\"Document Sentiment\")\nprint(samplesentiment[\"documentSentiment\"])\nprint(\"\\nEntities\")\nprint(samplesentiment[\"entities\"][0])\nprint(\"\\nSentences\")\nprint(samplesentiment['sentences'][0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.io.json import json_normalize\n\nsentiment_list = os.listdir('../input/petfinder-adoption-prediction/train_sentiment')\ntrain_sentiment_df = pd.DataFrame()\nfor i,x in enumerate(sentiment_list):\n    samplesentiment = pd.read_json('../input/petfinder-adoption-prediction/train_sentiment/{}'.format(x), orient='index', typ='series')\n\n    sentences = json_normalize(samplesentiment.sentences).loc[:,['sentiment.magnitude', 'sentiment.score']].agg(\n                    {\n                       'sentiment.magnitude' : ['count','mean','std'],\n                       'sentiment.score' : ['mean','std', 'sum']\n\n                    }).unstack().to_frame().sort_index(level=1).T\n    sentences.columns = sentences.columns.map('_'.join)\n\n#         words_salience_type = json_normalize(samplesentiment.entities).loc[:,['name','salience','type']].set_index('name')\\\n#             .unstack().to_frame().sort_index(level=1).T\n#         words_salience_type.columns = words_salience_type.columns.map('_'.join)\n\n    sentiment = pd.concat([json_normalize(samplesentiment[\"documentSentiment\"]),\n                           sentences,\n#                                words_salience_type\n                          ], axis =1)\n#         train_sentiment_df[x[:9]] = sentiment\n    sentiment.index = [x[:9]]\n    train_sentiment_df = pd.concat([train_sentiment_df, sentiment], axis =0)\n\ntrain_sentiment_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_list = os.listdir('../input/petfinder-adoption-prediction/test_sentiment/')\ntest_sentiment_df = pd.DataFrame()\nfor i,x in enumerate(sentiment_list):\n    samplesentiment = pd.read_json('../input/petfinder-adoption-prediction/test_sentiment/{}'.format(x), orient='index', typ='series')\n    sentences = json_normalize(samplesentiment.sentences).loc[:,['sentiment.magnitude', 'sentiment.score']].agg(\n                    {\n                       'sentiment.magnitude' : ['count','mean','std'],\n                       'sentiment.score' : ['mean','std', 'sum']\n\n                    }).unstack().to_frame().sort_index(level=1).T\n    sentences.columns = sentences.columns.map('_'.join)\n\n#         words_salience_type = json_normalize(samplesentiment.entities).loc[:,['name','salience','type']].set_index('name')\\\n#             .unstack().to_frame().sort_index(level=1).T\n#         words_salience_type.columns = words_salience_type.columns.map('_'.join)\n\n    sentiment = pd.concat([json_normalize(samplesentiment[\"documentSentiment\"]),\n                           sentences,\n#                                words_salience_type\n                          ], axis =1)\n#         test_sentiment_df[x[:9]] = sentiment\n    sentiment.index = [x[:9]]\n    test_sentiment_df = pd.concat([test_sentiment_df, sentiment], axis =0)\n\ntest_sentiment_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiments_df = pd.concat([train_sentiment_df,test_sentiment_df],axis=0)\nsentiments_df[\"PetID\"] = sentiments_df.index\nfulldf = add_feature (sentiments_df,fulldf,\"PetID\")\nfulldf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fulldf = fulldf.drop([\"Name\",\"Description\",\"RescuerID\"],axis=1)\nfulldf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add gradient boosting modelization and submit !\nfiltertrain = (fulldf[\"traintest\"] == \"train\")\ntrain = fulldf[filtertrain]\n\nfiltertest = (fulldf[\"traintest\"] == \"test\")\ntest = fulldf[filtertest]\n\ntrain.pop(\"PetID\")\n\ndel fulldf\ngc.collect()\n\nprint(train.shape)\nprint(test.shape)\ntest.pop(\"PetID\").head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.pop(\"traintest\")\ntest.pop(\"traintest\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from sklearn.model_selection import KFold\n\nlgb_data = lgb.Dataset(data = train, label = train_label)\n\nlgb_params = {'objective': 'multiclass','num_leaves': 20, 'num_class': 5 }\n\n\nlgb_cv = lgb.cv(nfold=5, params=lgb_params,train_set=lgb_data,num_boost_round=50, metrics='multi_error',early_stopping_rounds=10,stratified=True)\n\nprint(lgb_cv)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score, make_scorer\n\nkappa_scorer = make_scorer(cohen_kappa_score,weights='quadratic')\n\nparameters_lgb = {'num_leaves': np.array([20,50,200]) ,'n_estimators': np.array([50]) ,\n                  'learning_rate': np.array([0.05]) ,'min_child_samples':np.array([30,150]),'reg_alpha': np.array([0.1,0.5])}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nlgbc = lgb.LGBMRegressor()\n\ngs = GridSearchCV(estimator=lgbc, param_grid=parameters_lgb, cv=4,scoring = 'neg_mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs.fit(train, train_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs.cv_results_['mean_test_score']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.max(gs.cv_results_['mean_test_score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_preds = gs.predict(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = gs.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy as sp\n\nfrom collections import Counter\nfrom functools import partial\nfrom math import sqrt\n\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\n\n\n# FROM: https://www.kaggle.com/myltykritik/simple-lgbm-image-features\n\n# The following 3 functions have been taken from Ben Hamner's github repository\n# https://github.com/benhamner/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              / num_scored_items)\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n\n    return (1.0 - numerator / denominator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n    \n    def _kappa_loss(self, coef, X, y):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return -cohen_kappa_score(y, preds, weights='quadratic')\n    \n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X = X, y = y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n    \n    def predict(self, X, coef):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return preds\n    \n    def coefficients(self):\n        return self.coef_['x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optR = OptimizedRounder()\noptR.fit(train_preds,train_label.values)\ncoefficients = optR.coefficients()\nvalid_pred = optR.predict(train_preds, coefficients)\nqwk = quadratic_weighted_kappa(train_label.values, valid_pred)\nprint(\"QWK = \", qwk)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefficients_ = coefficients.copy()\n#coefficients_[0] = 1.66\n#coefficients_[1] = 2.13\n#coefficients_[3] = 2.85\n\ntest_predictions = optR.predict(test_preds, coefficients_).astype(np.int8)\n\nprint(test_predictions.shape)\ntest_predictions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/petfinder-adoption-prediction/test/sample_submission.csv\")\n\nprint(submission.shape)\nsubmission[\"AdoptionSpeed\"] = test_predictions\nprint(submission.shape)\n\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions.to_csv('test_predictions', index=False)\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}