{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nimport numpy as np\nimport statistics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nnp.random.seed = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"breeds_df = pd.read_csv('../input/petfinder-adoption-prediction/BreedLabels.csv')\ncolors_df = pd.read_csv('../input/petfinder-adoption-prediction/ColorLabels.csv')\nstates_df = pd.read_csv('../input/petfinder-adoption-prediction/StateLabels.csv')\ntrain_df = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\ntest_df = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_size = 256\nbatch_size = 256\npet_ids = train_df['PetID'].values\nn_batches = len(pet_ids) // batch_size + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.densenet import preprocess_input, DenseNet121\nfrom tqdm import tqdm, tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def resize_to_square(im):\n    old_size = im.shape[:2] # old_size is in (height, width) format\n    ratio = float(img_size)/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    # new_size should be in (width, height) format\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = img_size - new_size[1]\n    delta_h = img_size - new_size[0]\n    top, bottom = delta_h//2, delta_h-(delta_h//2)\n    left, right = delta_w//2, delta_w-(delta_w//2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n    return new_im","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_image(path, pet_id):\n    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n    new_image = resize_to_square(image)\n    new_image = preprocess_input(new_image)\n    return new_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\nimport keras.backend as K\ninp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor = inp,\n                       weights='../input/petfinder-densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top = False)\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\nx = AveragePooling1D(4)(x)\nout = Lambda(lambda x: x[:,:,0])(x)\nm = Model(inp,out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = []\nfor b in range(n_batches):\n    start = b * batch_size\n    end = (b + 1) * batch_size\n    batch_pets = pet_ids[start: end]\n    batch_images= np.zeros((len(batch_pets), img_size, img_size, 3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i]= load_image(\"../input/petfinder-adoption-prediction/train_images/\", pet_id)\n        except:\n            pass\n    batch_preds= m.predict(batch_images)\n    for i, pet_id in enumerate(batch_pets):\n        features.append([pet_id] + list(batch_preds[i]))\n\nX = pd.DataFrame(features, columns=[\"PetID\"] + [\"dense121_2_{}\".format(i) for i in\n                                                        range(batch_preds.shape[1])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_METADATA_PATH = '../input/petfinder-adoption-prediction/train_metadata/'\nTRAIN_SENTIMENT_PATH = '../input/petfinder-adoption-prediction/train_sentiment/'\nTEST_METADATA_PATH = '../input/petfinder-adoption-prediction/test_metadata/'\nTEST_SENTIMENT_PATH = '../input/petfinder-adoption-prediction/test_sentiment/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_dict= {}\nfor filename in os.listdir(TRAIN_SENTIMENT_PATH):\n    with open(TRAIN_SENTIMENT_PATH + filename, 'r') as f:\n        sentiment = json.load(f)\n    pet_id = filename.split('.')[0]\n    sentiment_dict[pet_id] = {}\n\n    salience = [float(x['salience']) for x in sentiment['entities'] if 'salience' in x]\n    if salience:\n        sentiment_dict[pet_id]['entities_salience_var'] = np.var(salience)\n        sentiment_dict[pet_id]['entities_salience_sum'] = np.sum(salience)\n        sentiment_dict[pet_id]['entities_salience_mean'] = np.mean(salience)\n        sentiment_dict[pet_id]['entities_salience_min'] = np.min(salience)\n        sentiment_dict[pet_id]['entities_salience_max'] = np.max(salience)\n    else:\n        sentiment_dict[pet_id]['entities_salience_var'] = 0\n        sentiment_dict[pet_id]['entities_salience_sum'] = 0\n        sentiment_dict[pet_id]['entities_salience_mean'] = 0\n        sentiment_dict[pet_id]['entities_salience_min'] = 0\n        sentiment_dict[pet_id]['entities_salience_max'] = 0\n\n    file_sentiment = ([x['sentiment'] for x in sentiment['sentences'] if 'sentiment' in x])\n    magnitude = ([float(x['magnitude']) for x in file_sentiment if 'magnitude' in x])\n    score = ([float(x['score']) for x in file_sentiment if 'score' in x])\n    \n    sentiment_dict[pet_id]['magnitude_var'] = np.var(magnitude)\n    sentiment_dict[pet_id]['magnitude_sum'] = sentiment['documentSentiment']['magnitude']\n    sentiment_dict[pet_id]['magnitude_mean'] = np.mean(magnitude)\n    sentiment_dict[pet_id]['magnitude_min'] = np.min(magnitude)\n    sentiment_dict[pet_id]['magnitude_max'] = np.max(magnitude)\n    \n    sentiment_dict[pet_id]['score_var'] = np.var(score)\n    sentiment_dict[pet_id]['score_sum'] = np.sum(score)\n    sentiment_dict[pet_id]['score_mean'] = sentiment['documentSentiment']['score']\n    sentiment_dict[pet_id]['score_min'] = np.min(score)\n    sentiment_dict[pet_id]['score_max'] = np.max(score)\n    \n    sentiment_dict[pet_id]['lang'] = sentiment['language']\n\ntrain_sentiment_df = pd.DataFrame()\ntrain_sentiment_df['PetID'] = train_df['PetID']\n\ntrain_sentiment_df['magnitude_var'] = train_df['PetID'].apply(lambda x: sentiment_dict[x]['magnitude_var'] if x in sentiment_dict else 0)\ntrain_sentiment_df['magnitude_sum'] = train_df['PetID'].apply(lambda x: sentiment_dict[x]['magnitude_sum'] if x in sentiment_dict else 0)\ntrain_sentiment_df['magnitude_mean'] = train_df['PetID'].apply(lambda x: sentiment_dict[x]['magnitude_mean'] if x in sentiment_dict else 0)\ntrain_sentiment_df['magnitude_min'] = train_df['PetID'].apply(lambda x: sentiment_dict[x]['magnitude_min'] if x in sentiment_dict else 0)\ntrain_sentiment_df['magnitude_max'] = train_df['PetID'].apply(lambda x: sentiment_dict[x]['magnitude_max'] if x in sentiment_dict else 0)\n\ntrain_sentiment_df['score_var'] = train_df['PetID'].apply(lambda x: sentiment_dict[x]['score_var'] if x in sentiment_dict else 0)\ntrain_sentiment_df['score_sum'] = train_df['PetID'].apply(lambda x: sentiment_dict[x]['score_sum'] if x in sentiment_dict else 0)\ntrain_sentiment_df['score_mean'] = train_df['PetID'].apply(lambda x: sentiment_dict[x]['score_mean'] if x in sentiment_dict else 0)\ntrain_sentiment_df['score_min'] = train_df['PetID'].apply(lambda x: sentiment_dict[x]['score_min'] if x in sentiment_dict else 0)\ntrain_sentiment_df['score_max'] = train_df['PetID'].apply(lambda x: sentiment_dict[x]['score_max'] if x in sentiment_dict else 0)\n\ntrain_sentiment_df['entities_salience_var'] = train_df['PetID'].apply(lambda x: sentiment_dict[x]['entities_salience_var'] if x in sentiment_dict else 0)\ntrain_sentiment_df['entities_salience_sum'] = train_df['PetID'].apply(lambda x: sentiment_dict[x]['entities_salience_sum'] if x in sentiment_dict else 0)\ntrain_sentiment_df['entities_salience_mean'] = train_df['PetID'].apply(lambda x: sentiment_dict[x]['entities_salience_mean'] if x in sentiment_dict else 0)\ntrain_sentiment_df['entities_salience_min'] = train_df['PetID'].apply(lambda x: sentiment_dict[x]['entities_salience_min'] if x in sentiment_dict else 0)\ntrain_sentiment_df['entities_salience_max'] = train_df['PetID'].apply(lambda x: sentiment_dict[x]['entities_salience_max'] if x in sentiment_dict else 0)\n\ntrain_sentiment_df['lang'] = train_df['PetID'].apply(lambda x: sentiment_dict[x]['lang'] if x in sentiment_dict else 0)\n\ntrain_sentiment_df.set_index('PetID', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_dict= {}\nfor filename in os.listdir(TEST_SENTIMENT_PATH):\n    with open(TEST_SENTIMENT_PATH + filename, 'r') as f:\n        sentiment = json.load(f)\n    pet_id = filename.split('.')[0]\n    sentiment_dict[pet_id] = {}\n\n    salience = ([float(x['salience']) for x in sentiment['entities'] if 'salience' in x])\n    if salience:\n        sentiment_dict[pet_id]['entities_salience_var'] = np.var(salience)\n        sentiment_dict[pet_id]['entities_salience_sum'] = np.sum(salience)\n        sentiment_dict[pet_id]['entities_salience_mean'] = np.mean(salience)\n        sentiment_dict[pet_id]['entities_salience_min'] = np.min(salience)\n        sentiment_dict[pet_id]['entities_salience_max'] = np.max(salience)\n    else:\n        sentiment_dict[pet_id]['entities_salience_var'] = 0\n        sentiment_dict[pet_id]['entities_salience_sum'] = 0\n        sentiment_dict[pet_id]['entities_salience_mean'] = 0\n        sentiment_dict[pet_id]['entities_salience_min'] = 0\n        sentiment_dict[pet_id]['entities_salience_max'] = 0\n\n    file_sentiment = [x['sentiment'] for x in sentiment['sentences'] if 'sentiment' in x]\n    magnitude = [float(x['magnitude']) for x in file_sentiment if 'magnitude' in x]\n    score = [float(x['score']) for x in file_sentiment if 'score' in x]\n    \n    sentiment_dict[pet_id]['magnitude_var'] = np.var(magnitude)\n    sentiment_dict[pet_id]['magnitude_sum'] = sentiment['documentSentiment']['magnitude']\n    sentiment_dict[pet_id]['magnitude_mean'] = np.mean(magnitude)\n    sentiment_dict[pet_id]['magnitude_min'] = np.min(magnitude)\n    sentiment_dict[pet_id]['magnitude_max'] = np.max(magnitude)\n    \n    sentiment_dict[pet_id]['score_var'] = np.var(score)\n    sentiment_dict[pet_id]['score_sum'] = np.sum(score)\n    sentiment_dict[pet_id]['score_mean'] = sentiment['documentSentiment']['score']\n    sentiment_dict[pet_id]['score_min'] = np.min(score)\n    sentiment_dict[pet_id]['score_max'] = np.max(score)\n    \n    sentiment_dict[pet_id]['lang'] = sentiment['language']\n\ntest_sentiment_df = pd.DataFrame()\ntest_sentiment_df['PetID'] = test_df['PetID']\n\ntest_sentiment_df['magnitude_var'] = test_df['PetID'].apply(lambda x: sentiment_dict[x]['magnitude_var'] if x in sentiment_dict else 0)\ntest_sentiment_df['magnitude_sum'] = test_df['PetID'].apply(lambda x: sentiment_dict[x]['magnitude_sum'] if x in sentiment_dict else 0)\ntest_sentiment_df['magnitude_mean'] = test_df['PetID'].apply(lambda x: sentiment_dict[x]['magnitude_mean'] if x in sentiment_dict else 0)\ntest_sentiment_df['magnitude_min'] = test_df['PetID'].apply(lambda x: sentiment_dict[x]['magnitude_min'] if x in sentiment_dict else 0)\ntest_sentiment_df['magnitude_max'] = test_df['PetID'].apply(lambda x: sentiment_dict[x]['magnitude_max'] if x in sentiment_dict else 0)\n\ntest_sentiment_df['score_var'] = test_df['PetID'].apply(lambda x: sentiment_dict[x]['score_var'] if x in sentiment_dict else 0)\ntest_sentiment_df['score_sum'] = test_df['PetID'].apply(lambda x: sentiment_dict[x]['score_sum'] if x in sentiment_dict else 0)\ntest_sentiment_df['score_mean'] = test_df['PetID'].apply(lambda x: sentiment_dict[x]['score_mean'] if x in sentiment_dict else 0)\ntest_sentiment_df['score_min'] = test_df['PetID'].apply(lambda x: sentiment_dict[x]['score_min'] if x in sentiment_dict else 0)\ntest_sentiment_df['score_max'] = test_df['PetID'].apply(lambda x: sentiment_dict[x]['score_max'] if x in sentiment_dict else 0)\n\ntest_sentiment_df['entities_salience_var'] = test_df['PetID'].apply(lambda x: sentiment_dict[x]['entities_salience_var'] if x in sentiment_dict else 0)\ntest_sentiment_df['entities_salience_sum'] = test_df['PetID'].apply(lambda x: sentiment_dict[x]['entities_salience_sum'] if x in sentiment_dict else 0)\ntest_sentiment_df['entities_salience_mean'] = test_df['PetID'].apply(lambda x: sentiment_dict[x]['entities_salience_mean'] if x in sentiment_dict else 0)\ntest_sentiment_df['entities_salience_min'] = test_df['PetID'].apply(lambda x: sentiment_dict[x]['entities_salience_min'] if x in sentiment_dict else 0)\ntest_sentiment_df['entities_salience_max'] = test_df['PetID'].apply(lambda x: sentiment_dict[x]['entities_salience_max'] if x in sentiment_dict else 0)\n\ntest_sentiment_df['lang'] = test_df['PetID'].apply(lambda x: sentiment_dict[x]['lang'] if x in sentiment_dict else 0)\n\ntest_sentiment_df.set_index('PetID', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata_dict = {}\n\nfor filename in os.listdir(TRAIN_METADATA_PATH):\n    with open(TRAIN_METADATA_PATH + filename, 'r') as f:\n        metadata = json.load(f)\n    pet_id = filename.split('-')[0]\n    filename = filename.split('.')[0]\n    if filename.endswith('1') == False:\n        continue\n    metadata_dict[pet_id] = {}\n    if 'labelAnnotations' in metadata:\n        annot_score = [float(x['score']) for x in metadata['labelAnnotations'] if 'score' in x]\n    else:\n        annot_score = []        \n    if 'imagePropertiesAnnotation'in metadata:\n        colors = metadata['imagePropertiesAnnotation']['dominantColors']['colors']\n        color_score = [float(x['score']) for x in colors if 'score' in x]\n        pixel_frac = [float(x['pixelFraction']) for x in colors if 'pixelFraction' in x]\n    else:\n        color_score = []\n        pixel_frac = []\n    if 'cropHintsAnnotation' in metadata:\n        crop_hints = metadata['cropHintsAnnotation']['cropHints']\n        crop_confidence = [x['confidence'] for x in crop_hints if 'confidence' in x]\n    else:\n        crop_confidence = []\n\n    if len(annot_score) > 0:\n        metadata_dict[pet_id]['annot_score_var'] = np.var(annot_score)\n        metadata_dict[pet_id]['annot_score_sum'] = sum(annot_score)\n        metadata_dict[pet_id]['annot_score_mean'] = np.mean(annot_score)\n        metadata_dict[pet_id]['annot_score_min'] = min(annot_score)\n        metadata_dict[pet_id]['annot_score_max'] = max(annot_score)\n    else:\n        metadata_dict[pet_id]['annot_score_var'] = 0\n        metadata_dict[pet_id]['annot_score_sum'] = 0\n        metadata_dict[pet_id]['annot_score_mean'] = 0\n        metadata_dict[pet_id]['annot_score_min'] = 0\n        metadata_dict[pet_id]['annot_score_max'] = 0\n\n    if len(color_score) > 0:\n        metadata_dict[pet_id]['color_score_var'] = np.var(color_score)\n        metadata_dict[pet_id]['color_score_sum'] = sum(color_score)\n        metadata_dict[pet_id]['color_score_mean'] = np.mean(color_score)\n        metadata_dict[pet_id]['color_score_min'] = min(color_score)\n        metadata_dict[pet_id]['color_score_max'] = max(color_score)\n    else:\n        metadata_dict[pet_id]['color_score_var'] = 0\n        metadata_dict[pet_id]['color_score_sum'] = 0\n        metadata_dict[pet_id]['color_score_mean'] = 0\n        metadata_dict[pet_id]['color_score_min'] = 0\n        metadata_dict[pet_id]['color_score_max'] = 0\n\n    if len(pixel_frac) > 0:\n        metadata_dict[pet_id]['pixel_frac_var'] = np.var(pixel_frac)\n        metadata_dict[pet_id]['pixel_frac_sum'] = sum(pixel_frac)\n        metadata_dict[pet_id]['pixel_frac_mean'] = np.mean(pixel_frac)\n        metadata_dict[pet_id]['pixel_frac_min'] = min(pixel_frac)\n        metadata_dict[pet_id]['pixel_frac_max'] = max(pixel_frac)\n    else:\n        metadata_dict[pet_id]['pixel_frac_var'] = 0\n        metadata_dict[pet_id]['pixel_frac_sum'] = 0\n        metadata_dict[pet_id]['pixel_frac_mean'] = 0\n        metadata_dict[pet_id]['pixel_frac_min'] = 0\n        metadata_dict[pet_id]['pixel_frac_max'] = 0\n    \n    metadata_dict[pet_id]['crop_confidence'] = crop_confidence[0]\n\ntrain_metadata_df = pd.DataFrame()\ntrain_metadata_df['PetID'] = train_df['PetID']\n\ntrain_metadata_df['annot_score_var'] = train_df['PetID'].apply(lambda x: metadata_dict[x]['annot_score_var'] if x in metadata_dict else 0)\ntrain_metadata_df['annot_score_sum'] = train_df['PetID'].apply(lambda x: metadata_dict[x]['annot_score_sum'] if x in metadata_dict else 0)\ntrain_metadata_df['annot_score_mean'] = train_df['PetID'].apply(lambda x: metadata_dict[x]['annot_score_mean'] if x in metadata_dict else 0)\ntrain_metadata_df['annot_score_min'] = train_df['PetID'].apply(lambda x: metadata_dict[x]['annot_score_min'] if x in metadata_dict else 0)\ntrain_metadata_df['annot_score_max'] = train_df['PetID'].apply(lambda x: metadata_dict[x]['annot_score_max'] if x in metadata_dict else 0)\n\ntrain_metadata_df['color_score_var'] = train_df['PetID'].apply(lambda x: metadata_dict[x]['color_score_var'] if x in metadata_dict else 0)\ntrain_metadata_df['color_score_sum'] = train_df['PetID'].apply(lambda x: metadata_dict[x]['color_score_sum'] if x in metadata_dict else 0)\ntrain_metadata_df['color_score_mean'] = train_df['PetID'].apply(lambda x: metadata_dict[x]['color_score_mean'] if x in metadata_dict else 0)\ntrain_metadata_df['color_score_min'] = train_df['PetID'].apply(lambda x: metadata_dict[x]['color_score_min'] if x in metadata_dict else 0)\ntrain_metadata_df['color_score_max'] = train_df['PetID'].apply(lambda x: metadata_dict[x]['color_score_max'] if x in metadata_dict else 0)\n\ntrain_metadata_df['pixel_frac_var'] = train_df['PetID'].apply(lambda x: metadata_dict[x]['pixel_frac_var'] if x in metadata_dict else 0)\ntrain_metadata_df['pixel_frac_sum'] = train_df['PetID'].apply(lambda x: metadata_dict[x]['pixel_frac_sum'] if x in metadata_dict else 0)\ntrain_metadata_df['pixel_frac_mean'] = train_df['PetID'].apply(lambda x: metadata_dict[x]['pixel_frac_mean'] if x in metadata_dict else 0)\ntrain_metadata_df['pixel_frac_min'] = train_df['PetID'].apply(lambda x: metadata_dict[x]['pixel_frac_min'] if x in metadata_dict else 0)\ntrain_metadata_df['pixel_frac_max'] = train_df['PetID'].apply(lambda x: metadata_dict[x]['pixel_frac_max'] if x in metadata_dict else 0)\n\ntrain_metadata_df['crop_confidence'] = train_df['PetID'].apply(lambda x: metadata_dict[x]['crop_confidence'] if x in metadata_dict else 0)\n\ntrain_metadata_df.set_index('PetID', inplace=True)\ntrain_metadata_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata_dict = {}\n\nfor filename in os.listdir(TEST_METADATA_PATH):\n    with open(TEST_METADATA_PATH + filename, 'r') as f:\n        metadata = json.load(f)\n    pet_id = filename.split('-')[0]\n    filename = filename.split('.')[0]\n    if filename.endswith('1') == False:\n        continue\n    metadata_dict[pet_id] = {}\n    if 'labelAnnotations' in metadata:\n        annot_score = [float(x['score']) for x in metadata['labelAnnotations'] if 'score' in x]\n    else:\n        annot_score = []        \n    if 'imagePropertiesAnnotation'in metadata:\n        colors = metadata['imagePropertiesAnnotation']['dominantColors']['colors']\n        color_score = [float(x['score']) for x in colors if 'score' in x]\n        pixel_frac = [float(x['pixelFraction']) for x in colors if 'pixelFraction' in x]\n    else:\n        color_score = []\n        pixel_frac = []\n    if 'cropHintsAnnotation' in metadata:\n        crop_hints = metadata['cropHintsAnnotation']['cropHints']\n        crop_confidence = [x['confidence'] for x in crop_hints if 'confidence' in x]\n    else:\n        crop_confidence = []\n\n    if len(annot_score) > 0:\n        metadata_dict[pet_id]['annot_score_var'] = np.var(annot_score)\n        metadata_dict[pet_id]['annot_score_sum'] = sum(annot_score)\n        metadata_dict[pet_id]['annot_score_mean'] = np.mean(annot_score)\n        metadata_dict[pet_id]['annot_score_min'] = min(annot_score)\n        metadata_dict[pet_id]['annot_score_max'] = max(annot_score)\n    else:\n        metadata_dict[pet_id]['annot_score_var'] = 0\n        metadata_dict[pet_id]['annot_score_sum'] = 0\n        metadata_dict[pet_id]['annot_score_mean'] = 0\n        metadata_dict[pet_id]['annot_score_min'] = 0\n        metadata_dict[pet_id]['annot_score_max'] = 0\n\n    if len(color_score) > 0:\n        metadata_dict[pet_id]['color_score_var'] = np.var(color_score)\n        metadata_dict[pet_id]['color_score_sum'] = sum(color_score)\n        metadata_dict[pet_id]['color_score_mean'] = np.mean(color_score)\n        metadata_dict[pet_id]['color_score_min'] = min(color_score)\n        metadata_dict[pet_id]['color_score_max'] = max(color_score)\n    else:\n        metadata_dict[pet_id]['color_score_var'] = 0\n        metadata_dict[pet_id]['color_score_sum'] = 0\n        metadata_dict[pet_id]['color_score_mean'] = 0\n        metadata_dict[pet_id]['color_score_min'] = 0\n        metadata_dict[pet_id]['color_score_max'] = 0\n\n    if len(pixel_frac) > 0:\n        metadata_dict[pet_id]['pixel_frac_var'] = np.var(pixel_frac)\n        metadata_dict[pet_id]['pixel_frac_sum'] = sum(pixel_frac)\n        metadata_dict[pet_id]['pixel_frac_mean'] = np.mean(pixel_frac)\n        metadata_dict[pet_id]['pixel_frac_min'] = min(pixel_frac)\n        metadata_dict[pet_id]['pixel_frac_max'] = max(pixel_frac)\n    else:\n        metadata_dict[pet_id]['pixel_frac_var'] = 0\n        metadata_dict[pet_id]['pixel_frac_sum'] = 0\n        metadata_dict[pet_id]['pixel_frac_mean'] = 0\n        metadata_dict[pet_id]['pixel_frac_min'] = 0\n        metadata_dict[pet_id]['pixel_frac_max'] = 0\n    \n    metadata_dict[pet_id]['crop_confidence'] = crop_confidence[0]\n\ntest_metadata_df = pd.DataFrame()\ntest_metadata_df['PetID'] = test_df['PetID']\ntest_metadata_df['annot_score_var'] = test_df['PetID'].apply(lambda x: metadata_dict[x]['annot_score_var'] if x in metadata_dict else 0)\ntest_metadata_df['annot_score_sum'] = test_df['PetID'].apply(lambda x: metadata_dict[x]['annot_score_sum'] if x in metadata_dict else 0)\ntest_metadata_df['annot_score_mean'] = test_df['PetID'].apply(lambda x: metadata_dict[x]['annot_score_mean'] if x in metadata_dict else 0)\ntest_metadata_df['annot_score_min'] = test_df['PetID'].apply(lambda x: metadata_dict[x]['annot_score_min'] if x in metadata_dict else 0)\ntest_metadata_df['annot_score_max'] = test_df['PetID'].apply(lambda x: metadata_dict[x]['annot_score_max'] if x in metadata_dict else 0)\n\ntest_metadata_df['color_score_var'] = test_df['PetID'].apply(lambda x: metadata_dict[x]['color_score_var'] if x in metadata_dict else 0)\ntest_metadata_df['color_score_sum'] = test_df['PetID'].apply(lambda x: metadata_dict[x]['color_score_sum'] if x in metadata_dict else 0)\ntest_metadata_df['color_score_mean'] = test_df['PetID'].apply(lambda x: metadata_dict[x]['color_score_mean'] if x in metadata_dict else 0)\ntest_metadata_df['color_score_min'] = test_df['PetID'].apply(lambda x: metadata_dict[x]['color_score_min'] if x in metadata_dict else 0)\ntest_metadata_df['color_score_max'] = test_df['PetID'].apply(lambda x: metadata_dict[x]['color_score_max'] if x in metadata_dict else 0)\n\ntest_metadata_df['pixel_frac_var'] = test_df['PetID'].apply(lambda x: metadata_dict[x]['pixel_frac_var'] if x in metadata_dict else 0)\ntest_metadata_df['pixel_frac_sum'] = test_df['PetID'].apply(lambda x: metadata_dict[x]['pixel_frac_sum'] if x in metadata_dict else 0)\ntest_metadata_df['pixel_frac_mean'] = test_df['PetID'].apply(lambda x: metadata_dict[x]['pixel_frac_mean'] if x in metadata_dict else 0)\ntest_metadata_df['pixel_frac_min'] = test_df['PetID'].apply(lambda x: metadata_dict[x]['pixel_frac_min'] if x in metadata_dict else 0)\ntest_metadata_df['pixel_frac_max'] = test_df['PetID'].apply(lambda x: metadata_dict[x]['pixel_frac_max'] if x in metadata_dict else 0)\n\ntest_metadata_df['crop_confidence'] = test_df['PetID'].apply(lambda x: metadata_dict[x]['crop_confidence'] if x in metadata_dict else 0)\n\ntest_metadata_df.set_index('PetID', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sentiment_metadata_df = pd.DataFrame()\ntrain_sentiment_metadata_df['PetID'] = train_df['PetID']\ntrain_sentiment_metadata_df.set_index('PetID', inplace=True)\n\ntrain_sentiment_metadata_df = pd.merge(train_sentiment_metadata_df, train_metadata_df, how='left', on='PetID')\ntrain_sentiment_metadata_df = pd.merge(train_sentiment_metadata_df, train_sentiment_df, how='left', on='PetID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sentiment_metadata_df = pd.DataFrame()\ntest_sentiment_metadata_df['PetID'] = test_df['PetID']\ntest_sentiment_metadata_df.set_index('PetID', inplace=True)\n\ntest_sentiment_metadata_df = pd.merge(test_sentiment_metadata_df, test_metadata_df, how='left', on='PetID')\ntest_sentiment_metadata_df = pd.merge(test_sentiment_metadata_df, test_sentiment_df, how='left', on='PetID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.merge(train_df, train_sentiment_metadata_df, how='left', on='PetID')\ntest_df = pd.merge(test_df, test_sentiment_metadata_df, how='left', on='PetID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_population = {\n    41336:3.497,\n    41325:2.072,\n    41367:2.001,\n    41401:1.808,\n    41415:0.0993,\n    41324:0.485,\n    41332:1.098,\n    41335:1.623,\n    41330:2.447,\n    41380:0.253,\n    41327:0.222,\n    41345:3.54,\n    41342:2.619,\n    41326:5.79,\n    41361:1.125\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_gdp = {\n    41336:36.394,\n    41325:21.410,\n    41367:13.668,\n    41401:121.293,\n    41415:74.337,\n    41324:47.960,\n    41332:43.047,\n    41335:35.554,\n    41330:30.303,\n    41380:24.442,\n    41327:52.937,\n    41345:25.861,\n    41342:52.301,\n    41326:51.528,\n    41361:30.216\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['state_population'] = train_df['State'].map(state_population)\ntrain_df['state_gdp'] = train_df['State'].map(state_gdp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.merge(train_df, X, how='left', on='PetID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(train_df[train_df['Fee'] >= 1000].index, inplace = True)\ntrain_df.drop(train_df[train_df['Age'] > 144].index, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Age'] = np.abs(stats.zscore(train_df['Age'], ddof=1))\ntrain_df.drop(train_df[train_df['Age'] > 3].index, inplace = True)\ntrain_df.drop(train_df[train_df['Age'] < -3].index, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Fee'] = np.abs(stats.zscore(train_df['Fee'], ddof=1))\ntrain_df.drop(train_df[train_df['Fee'] > 3].index, inplace = True)\ntrain_df.drop(train_df[train_df['Fee'] < -3].index, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in train_df.index:\n    if train_df.at[i, 'Breed1'] == 0 and train_df.at[i, 'Breed2'] != 0:\n        train_df.at[i, 'Breed1'] = train_df.at[i, 'Breed2']\n    \n    if train_df.at[i, 'Breed1'] == 307 and train_df.at[i, 'Breed2'] == 0:\n        train_df.at[i, 'Breed2'] = train_df.at[i, 'Breed1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = pd.concat(g for _, g in train_df.groupby(\"Breed1\") if len(g) <= 2)\nx = pd.concat(g for _, g in train_df.groupby(\"Breed2\") if len(g) <= 2)\ntemp_train = pd.concat([x, y])\ntemp_train = temp_train.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(['Name', 'PetID', 'RescuerID', 'Description', 'lang'], axis=1, inplace=True)\ntemp_train.drop(['Name', 'PetID', 'RescuerID', 'Description', 'lang'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nx_train, x_test, y_train, y_test = train_test_split(\n    train_df.drop('AdoptionSpeed', axis=1), train_df['AdoptionSpeed'], test_size=0.20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = pd.concat([temp_train.drop('AdoptionSpeed', axis=1), x_test])\ntemp_df = pd.concat([temp_df, temp_train.drop('AdoptionSpeed', axis=1)])\nx_test = temp_df.drop_duplicates(keep=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series = pd.Series(temp_train['AdoptionSpeed'], index=temp_train.index)\ntemp_series = temp_series.append(temp_series)\ntemp_series = temp_series.append(y_test)\n\ny_test = temp_series[~temp_series.index.duplicated(keep=False)]\nerror = [x for x in y_test.index if x not in x_test.index]\ny_test = y_test[~y_test.index.isin(error)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = pd.concat([temp_train.drop('AdoptionSpeed', axis=1), x_train])\nx_train = temp_df.drop_duplicates(keep='first')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series = temp_series.append(y_train)\n\ny_train = temp_series[~temp_series.index.duplicated(keep='first')]\nerror = [x for x in y_train.index if x not in x_train.index]\ny_train = y_train[~y_train.index.isin(error)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = [\n     'Breed1',\n     'Breed2',\n     'Color1',\n     'Color2',\n     'Color3',\n     'Dewormed',\n     'FurLength',\n     'Gender',\n     'Health',\n     'MaturitySize',\n     'State',\n     'Sterilized',\n     'Type',\n     'Vaccinated'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_features = [x for x in list(x_train.columns) if x not in categorical_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]\n    \n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\nnum_pipeline = Pipeline(\n    [\n        (\"select_numeric\", DataFrameSelector(numerical_features)),\n        (\"imputer\", SimpleImputer(strategy=\"median\"))\n    ]\n)\nnum_pipeline.fit_transform(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nclass MostFrequentImputer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],\n                                        index=X.columns)\n        return self\n    def transform(self, X, y=None):\n        return X.fillna(self.most_frequent_)\n\ncat_pipeline = Pipeline(\n    [\n        (\"select_cat\", DataFrameSelector(categorical_features)),\n        (\"imputer\", MostFrequentImputer()),\n        (\"cat_encoder\", OneHotEncoder(sparse=False)),\n    ]\n)\ncat_pipeline.fit_transform(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import FeatureUnion\npreprocess_pipeline = FeatureUnion(transformer_list=[\n        (\"num_pipeline\", num_pipeline),\n        (\"cat_pipeline\", cat_pipeline),\n    ])\nx_train = preprocess_pipeline.fit_transform(x_train)\nx_test = preprocess_pipeline.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvm_clf = SVC(C=2, gamma=\"auto\", break_ties=True, random_state=777)\nsvm_clf.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = svm_clf.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nsvm_scores = cross_val_score(svm_clf, x_train, y_train, cv=10)\nsvm_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier(n_estimators=100, random_state=777)\nforest_clf.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = forest_clf.predict(x_test)\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_scores = cross_val_score(forest_clf, x_train, y_train, cv=10)\nforest_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n\ntree_clf = DecisionTreeClassifier(max_depth=9, random_state=777)\ntree_clf.fit(x_train, y_train)\ny_pred = tree_clf.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"print(classification_report(y_test, y_pred))","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_score = cross_val_score(tree_clf, x_train, y_train, cv=10)\ntree_score.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import required libraries\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport sklearn\n\n# Import necessary modules\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n# Keras specific\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_col = ['AdoptionSpeed']\npredictors = list(set(list(train_df.columns)) - set(target_col))\ntrain_df[predictors]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.reset_index(drop=True, inplace=True)\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_df[predictors].values\ny = train_df[target_col].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\nprint(X_train.shape); print(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\ncount_classes = y_test.shape[1]\nprint(count_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Layer\nfrom keras import backend as K\n\nclass RBFLayer(Layer):\n    def __init__(self, units, gamma, **kwargs):\n        super(RBFLayer, self).__init__(**kwargs)\n        self.units = units\n        self.gamma = K.cast_to_floatx(gamma)\n\n    def build(self, input_shape):\n        self.mu = self.add_weight(name='mu',\n                                  shape=(int(input_shape[1]), self.units),\n                                  initializer='uniform',\n                                  trainable=True)\n        super(RBFLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        diff = K.expand_dims(inputs) - self.mu\n        l2 = K.sum(K.pow(diff,2), axis=1)\n        res = K.exp(-1 * self.gamma * l2)\n        return res\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.units)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(500, activation='relu', input_dim=len(predictors)))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(RBFLayer(50, 0.5))\nmodel.add(Dense(5, activation='softmax'))\n\n# Compile the model\nopt = keras.optimizers.Adam(learning_rate=0.01)\nmodel.compile(optimizer='adam', \n              loss='categorical_crossentropy', \n              metrics=[keras.metrics.CategoricalAccuracy()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train, epochs=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train= model.predict(X_train)\nscores = model.evaluate(X_train, y_train, verbose=0)\nprint('Accuracy on training data: {}% \\n Error on training data: {}'.format(scores[1], 1 - scores[1]))   \n \npred_test= model.predict(X_test)\nscores2 = model.evaluate(X_test, y_test, verbose=0)\nprint('Accuracy on test data: {}% \\n Error on test data: {}'.format(scores2[1], 1 - scores2[1])) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}