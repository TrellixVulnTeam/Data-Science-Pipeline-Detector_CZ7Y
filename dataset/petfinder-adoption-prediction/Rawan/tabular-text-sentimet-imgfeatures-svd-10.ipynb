{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n #   for filename in filenames:\n      #  print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nimport re\nimport html\nimport unicodedata\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nfrom nltk.stem import WordNetLemmatizer\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import  LogisticRegressionCV\nimport gc\nfrom catboost import CatBoostClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('max_colwidth', 500)\npd.set_option('max_columns', 500)\npd.set_option('max_rows', 100)\nimport os\nfrom sklearn.metrics import cohen_kappa_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"breeds = pd.read_csv('../input/petfinder-adoption-prediction/BreedLabels.csv')\ncolors = pd.read_csv('../input/petfinder-adoption-prediction/ColorLabels.csv')\nstates = pd.read_csv('../input/petfinder-adoption-prediction/state_labels.csv')\n\ntrain = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\ntest = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv', engine='python')\nsub = pd.read_csv('../input/petfinder-adoption-prediction/test/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv(\"../input/train-with-image-features3/train_with_image_features3.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"PetID\"].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=pd.read_csv(\"../input/test-with-image-features3/test_with_image_features3.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"PetID\"].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#train_img_features.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_img_features.set_index(\"Unnamed: 0\",inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_img_features.set_index(\"Unnamed: 0\",inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_img_features.columns = [\"img_feat{}\".format(i) for i in range(256)]\n# test_img_features.columns = [\"img_feat{}\".format(i) for i in range(256)]\n\n# train_img_features[\"PetID\"] = train_img_features.index\n# test_img_features[\"PetID\"] = test_img_features.index\n\n# #train = pd.merge(train, train_img_features, on=\"PetID\")\n# #test = pd.merge(test, test_img_features, on=\"PetID\")\n\n# print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train=train[train[\"PhotoAmt\"]!=0]\n#test=test[test[\"PhotoAmt\"]!=0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# #feature engineering","metadata":{}},{"cell_type":"code","source":"\n\ntrain['Name'] = train['Name'].fillna('No_name_found')\ntest['Name'] = test['Name'].fillna('No_name_found')\n#all_data['Name'] = all_data['Name'].fillna('No_name_found')\n\ntrain['No_name'] = 0\ntrain.loc[train['Name'] == 'No_name_found', 'No_name'] = 1\ntest['No_name'] = 0\ntest.loc[test['Name'] == 'No_name_found', 'No_name'] = 1\n#all_data['No_name'] = 0\n#all_data.loc[all_data['Name'] == 'No_name_found', 'No_name'] = 1\n\n\ntrain['Pure_breed'] = 0\ntrain.loc[train['Breed2'] == 0, 'Pure_breed'] = 1\n\ntest['Pure_breed'] = 0\ntest.loc[test['Breed2'] == 0, 'Pure_breed'] = 1\n\ntrain['Free'] = 0\ntrain.loc[train['Fee'] == 0, 'Free'] = 1\n\ntest['Free'] = 0\ntest.loc[test['Fee'] == 0, 'Free'] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['dataset_type'] = 'train'\ntest['dataset_type'] = 'test'\nall_data = pd.concat([train, test])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parsing sentiment data","metadata":{}},{"cell_type":"markdown","source":"information about Google's  NLP API:\nhttps://cloud.google.com/natural-language/docs/basics#sentiment-analysis-values\n\nThe score of a document's sentiment indicates the overall emotion of a document. The magnitude of a document's sentiment indicates how much emotional content is present within the document, and this value is often proportional to the length of the document.v\n\nA document with a neutral score (around 0.0) may indicate a low-emotion document, or may indicate mixed emotions, with both high positive and negative values which cancel each out. Generally, you can use magnitude values to disambiguate these cases, as truly neutral documents will have a low magnitude value, while mixed documents will have higher magnitude values.","metadata":{}},{"cell_type":"code","source":"#This code was  inspired from this amazing repo:\n#https://github.com/liyi61/PetFinder.my-Adoption-Speed-Prediction/tree/master\nsentiment_dict={}\nfor filename in os.listdir(\"../input/petfinder-adoption-prediction/train_sentiment/\"):\n    with open(\"../input/petfinder-adoption-prediction/train_sentiment/\"+filename, 'rb') as f:\n        sentiment = json.load(f)\n    pet_id=filename.split('.')[0]\n    sentiment_dict[pet_id]={}\n    sentiment_dict[pet_id][\"magnitude\"]=sentiment[\"documentSentiment\"][\"magnitude\"]\n    sentiment_dict[pet_id]['score'] = sentiment['documentSentiment']['score']\n    sentiment_dict[pet_id]['language'] = sentiment['language']\n#\n\nfor filename in os.listdir('../input/petfinder-adoption-prediction/test_sentiment/'):\n    with open('../input/petfinder-adoption-prediction/test_sentiment/' + filename, 'rb') as f:\n        sentiment = json.load(f)\n    pet_id = filename.split('.')[0]\n    sentiment_dict[pet_id] = {}\n    sentiment_dict[pet_id]['magnitude'] = sentiment['documentSentiment']['magnitude']\n    sentiment_dict[pet_id]['score'] = sentiment['documentSentiment']['score']\n    sentiment_dict[pet_id]['language'] = sentiment['language']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['lang'] = train['PetID'].apply(lambda x: sentiment_dict[x]['language'] if x in sentiment_dict else 'no')\ntrain['magnitude'] = train['PetID'].apply(lambda x: sentiment_dict[x]['magnitude'] if x in sentiment_dict else 0)\ntrain['score'] = train['PetID'].apply(lambda x: sentiment_dict[x]['score'] if x in sentiment_dict else 0)\n\ntest['lang'] = test['PetID'].apply(lambda x: sentiment_dict[x]['language'] if x in sentiment_dict else 'no')\ntest['magnitude'] = test['PetID'].apply(lambda x: sentiment_dict[x]['magnitude'] if x in sentiment_dict else 0)\ntest['score'] = test['PetID'].apply(lambda x: sentiment_dict[x]['score'] if x in sentiment_dict else 0)\n\nall_data['lang'] = all_data['PetID'].apply(lambda x: sentiment_dict[x]['language'] if x in sentiment_dict else 'no')\nall_data['magnitude'] = all_data['PetID'].apply(lambda x: sentiment_dict[x]['magnitude'] if x in sentiment_dict else 0)\nall_data['score'] = all_data['PetID'].apply(lambda x: sentiment_dict[x]['score'] if x in sentiment_dict else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parsing img metadata","metadata":{}},{"cell_type":"markdown","source":"Let's take an idea about what the vision API does;\n\n1.**labelAnnotations**: Labels can identify general objects, locations, activities, animal species, products, and more. \nA LABEL_DETECTION response includes the detected labels, their score, topicality, and an opaque label ID, where:\n(https://cloud.google.com/vision/docs/labels)\n       mid - if present, contains a machine-generated identifier (MID) corresponding to the entity's Google Knowledge Graph entry. Note that mid values remain unique across different languages, so you can use these values to tie entities together from different languages. To inspect MID values, refer to the Google Knowledge Graph API documentation.\n   \n    description - the label description.\n    \n    score - the confidence score, which ranges from 0 (no confidence) to 1 (very high confidence).\n    (scores are stored from the highest to the lowest. therefore, I'll only consider the **1st description** for each image. This explains why we're indexing [\"labelAnnotations\"][score] **[0]**\n    topicality - The relevancy of the ICA (Image Content Annotation) label to the image. It measures how important/central a label is to the overall context of a page.\n\n\n2.**imagePropertiesAnnotation**(https://cloud.google.com/vision/docs/detecting-properties)\n\nThe Image Properties feature detects general attributes of the image, such as dominant color.\n\na.dominant color>>colors>>scores(https://github.com/googleapis/google-cloud-node/issues/1324)\n:, higher \"scores\" means higher confidence that the color in question is prominent in the central focus of the image,\nagain,sorted from the highest to  lowest.\n\nb.dominant color>>colors>>pixelFraction \tStores the fraction of pixels the color occupies in the image. Value in range [0, 1]. \n\n3.**cropHintsAnnotation**\ncropHints>>boundingPoly: vertices for a bounding polygon for the detected image annotation. \ncropHints>>ImportanceFraction: Fraction of importance of this salient(polygon) region with respect to the original image.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"lang\"].value_counts()#we can drop this column since english dominates other languages","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_to_use = ['Type', 'Age', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2',\n       'Color3', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed',\n       'Sterilized', 'Health', 'Quantity', 'Fee', 'State', 'score',\n       'VideoAmt', \"Description\", 'PhotoAmt', 'AdoptionSpeed', 'magnitude']\n#feature engineering; 'health', 'Free',, 'No_name', 'Pure_breed', 'desc_length','desc_words',\n          #     'average_word_length'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(cols_to_use)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = train[[col for col in cols_to_use if col in train.columns]]\n# test = test[[col for col in cols_to_use if col in test.columns]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_cols = ['Type', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2',\n       'Color3', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed',\n       'Sterilized', 'Health', 'State',\"lang\",\n           \"Pure_breed\",\"Free\",\"No_name\"]\n#Fee,Age,'VideoAmt', 'PhotoAmt'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# more_cols = []\n# #feature interaction\n# for col1 in cat_cols:\n#     for col2 in cat_cols:\n#         if col1 != col2 and col1 not in ['RescuerID', 'State'] and col2 not in ['RescuerID', 'State']:\n#             train[col1 + '_' + col2] = train[col1].astype(str) + '_' + train[col2].astype(str)\n#             test[col1 + '_' + col2] = test[col1].astype(str) + '_' + test[col2].astype(str)\n#             more_cols.append(col1 + '_' + col2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cat_cols = cat_cols + more_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indexer = {}\nfor col in cat_cols:\n    # print(col)\n    _, indexer[col] = pd.factorize(train[col].astype(str))\n  #  train.loc[:,col]=pd.factorize(train.loc[:,col])[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in cat_cols:\n    # print(col)\n    train[col] = indexer[col].get_indexer(train[col].astype(str))\n    test[col] = indexer[col].get_indexer(test[col].astype(str))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ny = train['AdoptionSpeed']\ntrain = train.drop(['AdoptionSpeed'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TextCol(Descriptio)","metadata":{}},{"cell_type":"markdown","source":"## Text cleaning(Descriptio)","metadata":{}},{"cell_type":"raw","source":"Replace   nulls","metadata":{}},{"cell_type":"code","source":"\n    ##\n\ntrain.loc[:,\"Description\"] = train.loc[:,\"Description\"].fillna(\"MISSINGG\")\n    \n#train_proc.dropna(axis=0,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.loc[:,\"Description\"] = test.loc[:,\"Description\"].fillna(\"MISSINGG\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef remove_special_chars(text):\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    return re1.sub(' ', html.unescape(x1))\n\ndef remove_html(text):\n    html=re.compile(r\"<.*?>\")\n    return  html.sub(r'',  text)\n   # This should happen before all other preprocessing steps,\n   # as we will see in the full pipeline, since it will help sentence and words tokenization for example, and will reduce vocab. \n   \n\ndef remove_non_ascii(text):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n\ndef remove_punctuation(text):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\n\n#7599:#Earthquake\n#other   examples: earthquake, EARTHQUAKE\n\ndef to_lowercase(text):\n    return text.lower()\n\n#remove URLs\ndef remove_urls(text):\n  #slower than regex\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef replace_numbers(text):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    return re.sub(r'\\d+', '', text)\n\n\ndef remove_whitespaces(text):\n    return text.strip()\n\n\ndef remove_stopwords(words, stop_words):\n    \"\"\"\n    :param words:\n    :type words:\n    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n    or\n    from spacy.lang.en.stop_words import STOP_WORDS\n    :type stop_words:\n    :return:\n    :rtype:\n    \"\"\"\n    return [word for word in words if word not in stop_words]\n\n\ndef stem_words(words):\n    \"\"\"Stem words in text\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word) for word in words]\n\ndef lemmatize_words(words):\n    \"\"\"Lemmatize words in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef text2words(text):\n    return word_tokenize(text)\n\ndef clean_text( text):\n    text = remove_special_chars(text)\n    text=remove_html(text)\n    text=remove_urls(text)\n    text = remove_non_ascii(text)\n    text = remove_punctuation(text)\n    text = to_lowercase(text)\n    \n\n    text = replace_numbers(text)\n    words = text2words(text)\n    #REMOVE STOPWORDS?\n    words = remove_stopwords(words, stop_words)\n    #words = stem_words(words)# Either stem ovocar lemmatize\n    words = lemmatize_words(words)\n    words = lemmatize_verbs(words)\n\n    return ''.join(words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"descrip_clean=train[\"Description\"].apply(lambda x:clean_text(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"descrip_clean_test=test[\"Description\"].apply(lambda x:clean_text(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"Description_clean\"]=descrip_clean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"Description_clean\"]=descrip_clean_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train[\"Description_clean\"]==\"missingg\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train[\"lang\"]!=\"en\"][\"Description\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# encoding description","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bow_tf = TfidfVectorizer( ngram_range=(1,2),max_features=10000)#takeTop10000Freq","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#bow_tf.get_feature_names()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_text_tfidf = bow_tf.fit_transform(train[\"Description_clean\"])##extTrial","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_text_tfidf_test = bow_tf.fit_transform(test[\"Description_clean\"])##extTrial","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_text_tfidf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_text_tfidf.toarray()[0].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"Description_clean\"][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x_text_tfidf=pd.DataFrame(x_text_tfidf.toarray())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x_text_tfidf_test=pd.DataFrame(x_text_tfidf_test.toarray())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### validating tfidf","metadata":{}},{"cell_type":"code","source":"#X_text=trai_svd[\"Description\"]\n\ntrain.drop([\"Description\", \"Description_clean\"], axis=1,inplace=True)\n#X_text=trai_svd[\"Description\"]\n\ntest.drop([\"Description\", \"Description_clean\"], axis=1,inplace=True)\ntrain.drop(\"PetID\",axis=1,inplace=True)\ntest.drop(\"PetID\",axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop([\"Name\"], axis=1,inplace=True)\n#X_text=trai_svd[\"Description\"]\n\ntest.drop([\"Name\"], axis=1,inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop([\"dataset_type\"], axis=1,inplace=True)\n#X_text=trai_svd[\"Description\"]\n\ntest.drop([\"dataset_type\"], axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train[\"RescuerID\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"RescuerID\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train.drop([\"RescuerID\"], axis=1,inplace=True)#RescuerID in train aren't the same a test\n#X_text=trai_svd[\"Description\"]\n\n#test.drop([\"RescuerID\"], axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#concatenateTF+trai\ntrain.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train=pd.concat([train,x_text_tfidf], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#del x_text_tfidf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_tfidf=pd.concat([test,x_text_tfidf_test], axis=1)\n#Your notebook tried to allocate more memory than is available. It has restarted.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import SparsePCA, TruncatedSVD, LatentDirichletAllocation, NMF","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_text_tfidf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_features=[]\nn_components = 5\n\nsvd_ = TruncatedSVD(\n        n_components=n_components, random_state=1337)\nsvd_col = svd_.fit_transform(x_text_tfidf)\nsvd_col = pd.DataFrame(svd_col)\n  # print(svd_col)\nsvd_col = svd_col.add_prefix('SVD_Description_')\n\n#    nmf_col = nmf_.fit_transform(tfidf_col)\n#    nmf_col = pd.DataFrame(nmf_col)\n#    nmf_col = nmf_col.add_prefix('NMF_{}_'.format(i))\n\ntext_features.append(svd_col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svd_.explained_variance_ratio_.sum()*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5:4%\n\n50:15%\n\n60:17%\n\n100:22%\n\n200:31%\n\n300:37%\n\n350:40%\n\n400:42%","metadata":{}},{"cell_type":"code","source":"svd_.explained_variance_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine all extracted features:\ntext_features = pd.concat(text_features, axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_features.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(\"Unnamed: 0\", axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.drop(\"Unnamed: 0\", axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate with main DF:\ntrai_svd=pd.concat([train, text_features], axis=1)\n#test_svd=pd.concat([test, text_features], axis=1)\n#trai_svd.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trai_svd.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trai_svd.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#same4test\ntext_features=[]\nn_components = 5\n\nsvd_ = TruncatedSVD(\n        n_components=n_components, random_state=1337)\n    #Non-Negative Matrix Factorization (NMF).\n    #Find two non-negative matrices (W, H) whose product approximates the non- negative matrix X. \n#This factorization can be used for example for dimensionality reduction, source separation or topic extraction.\n#     nmf_ = NMF(\n#         n_components=n_components, random_state=1337)\nsvd_col = svd_.fit_transform(x_text_tfidf_test)\nsvd_col = pd.DataFrame(svd_col)\n  # print(svd_col)\nsvd_col = svd_col.add_prefix('SVD_Description_')\n\n#    nmf_col = nmf_.fit_transform(tfidf_col)\n#    nmf_col = pd.DataFrame(nmf_col)\n#    nmf_col = nmf_col.add_prefix('NMF_{}_'.format(i))\n\ntext_features.append(svd_col)\n# Combine all extracted features:\ntext_features = pd.concat(text_features, axis=1)\ntext_features.shape\n# Concatenate with main DF:\ntest_svd=pd.concat([test, text_features], axis=1)\ntest_svd.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How do we know that reducing the dimensions of input down to 10 is good or the best we can do?\nWe don’t; 10 was an arbitrary choice.\n\nA better approach is to evaluate the same transform and model with different numbers of input features and choose the number of features (amount of dimensionality reduction) that results in the best average performance.","metadata":{}},{"cell_type":"code","source":"def try_svd():\n    for n in range(1,2,20):\n        text_features=[]\n        n_components = n\n\n        svd_ = TruncatedSVD(\n                n_components=n_components, random_state=1337)\n        svd_col = svd_.fit_transform(x_text_tfidf)\n        svd_col = pd.DataFrame(svd_col)\n          # print(svd_col)\n        svd_col = svd_col.add_prefix('SVD_Description_')\n\n        text_features.append(svd_col)\n        \n        # Combine all extracted features:\n        text_features = pd.concat(text_features, axis=1)\n        text_features.shape\n        \n        # Concatenate with main DF:\n        trai_svd=pd.concat([train, text_features], axis=1)\n            \n        #same4test\n        text_features=[]\n        n_components = n\n\n        svd_ = TruncatedSVD(\n                n_components=n_components, random_state=1337)\n        svd_col = svd_.fit_transform(x_text_tfidf_test)\n        svd_col = pd.DataFrame(svd_col)\n          # print(svd_col)\n        svd_col = svd_col.add_prefix('SVD_Description_')\n\n        text_features.append(svd_col)\n        # Combine all extracted features:\n        text_features = pd.concat(text_features, axis=1)\n        text_features.shape\n        # Concatenate with main DF:\n        test_svd=pd.concat([test, text_features], axis=1)\n        print(\">%d\" %(n ))\n        result_dict_lgb = train_model(X=trai_svd, X_test=test_svd, y=y, params=params, model_type='lgb', plot_feature_importance=False, make_oof=True)\n        prediction_lgb = (result_dict_lgb['prediction']).argmax(1)\n        \n        print('----------train lightgbm Loopend-------------')\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svd_.explained_variance_ ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#try_svd()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# train/test split","metadata":{}},{"cell_type":"markdown","source":"since dataset size is small,the validation_set won't be representative to estimate val_score on","metadata":{}},{"cell_type":"markdown","source":"# modeling","metadata":{}},{"cell_type":"code","source":"n_fold = 3\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trai_svd.drop([\"lang\"],axis=1,inplace=True)\n# test_svd.drop([\"lang\"],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trai_svd.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_svd.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(X, X_test, y=y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False,\n                averaging='usual', make_oof=False):\n    result_dict = {}\n    if make_oof:\n        oof = np.zeros((len(X), 5))\n    prediction = np.zeros((len(X_test), 5))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        gc.collect()\n        print('Fold', fold_n + 1, 'started at', time.ctime())\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n\n        if model_type == 'lgb':\n            train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_cols)\n            valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=cat_cols)\n\n            model = lgb.train(params,\n                              train_data,\n                              num_boost_round=20000,\n                              valid_sets=[train_data, valid_data],\n                              verbose_eval=500,\n                              early_stopping_rounds=200)\n\n            del train_data, valid_data\n\n            y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)\n            del X_valid\n            gc.collect()\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200,\n                              verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test), ntree_limit=model.best_ntree_limit)\n            \n        if model_type == 'lcv':\n            model = LogisticRegressionCV(scoring='neg_log_loss', cv=3, multi_class='multinomial')\n            model.fit(X_train, y_train)\n\n            y_pred_valid = model.predict_proba(X_valid)\n            y_pred = model.predict_proba(X_test)\n\n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=500, loss_function='MultiClass')\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[],\n                      use_best_model=False, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict_proba(X_test)\n\n        if make_oof:\n            oof[valid_index] = y_pred_valid\n            \n            \n        scores.append(kappa(y_valid, y_pred_valid.argmax(1)))\n        print('Fold kappa:', kappa(y_valid, y_pred_valid.argmax(1)))\n        print('')\n\n        if averaging == 'usual':\n            prediction += y_pred\n        elif averaging == 'rank':\n            prediction += pd.Series(y_pred).rank().values\n\n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importance()\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_fold\n\n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n\n    if model_type == 'lgb':\n\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= n_fold\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12))\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n            plt.title('LGB Features (avg over folds)')\n\n            result_dict['feature_importance'] = feature_importance\n\n    result_dict['prediction'] = prediction\n    if make_oof:\n        result_dict['oof'] = oof\n\n    return result_dict\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nparams = {'num_leaves': 512,\n        #  'min_data_in_leaf': 60,\n         'objective': 'multiclass',\n         'max_depth': -1,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 3,#tells LightGBM “re-sample without replacement every 3 iterations, \n         \"bagging_fraction\": 0.9,#and draw samples of 90% of the training data”.\n         \"bagging_seed\": 11,\n         \"random_state\": 42,\n         \"verbosity\": -1,\n         \"num_class\": 5}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #########lightgbm###############\n\n# params = {'num_leaves': 512,\n#         #  'min_data_in_leaf': 60,\n#          'objective': 'multiclass',\n#          'max_depth': -1,\n#          'learning_rate': 0.01,\n#          \"boosting\": \"gbdt\",\n#          \"feature_fraction\": 0.9,\n#          \"bagging_freq\": 3,\n#          \"bagging_fraction\": 0.9,\n#          \"bagging_seed\": 11,\n#          \"random_state\": 42,\n#          \"verbosity\": -1,\n#          \"num_class\": 5}\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_svd.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trai_svd.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trai_svd.drop(\"RescuerID\", axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_svd.drop(\"RescuerID\", axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print('----------training lightgbm-------------')\nresult_dict_lgb = train_model(X=trai_svd, X_test=test_svd, y=y, params=params, model_type='lgb', plot_feature_importance=False, make_oof=True)\nprediction_lgb = (result_dict_lgb['prediction']).argmax(1)\nsubmission_lgb = pd.DataFrame({'PetID': sub.PetID, 'AdoptionSpeed': [int(i) for i in prediction_lgb]})\nsubmission_lgb.head()\nsubmission_lgb.to_csv('submission.csv', index=False)\nprint('----------train lightgbm end-------------')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training on sparse tfidf features(10000vectorWord)+tabular+sentimentMetadata>>shape;10281 columns>>1hrTrainnn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trai_svd.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## retraining on the entire train data","metadata":{}},{"cell_type":"markdown","source":"crossvalidation is for model selection only!\nOnce decided (among types, hyper-params, data pipeline)\n→ Use the whole training set to train the selected (type, set of hyperparams and data pipeline)\n","metadata":{}},{"cell_type":"code","source":"#  train_data = lgb.Dataset(trai_svd, label=y, categorical_feature=cat_cols)\n#             #valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=cat_cols)\n\n# model = lgb.train(params,\n#                   train_data,\n#                   num_boost_round=2000)\n#                #   valid_sets=[train_data, valid_data],\n#                   #verbose_eval=500)\n#                 #  early_stopping_rounds=20)\n# #del train_data, valid_da\n# #y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)\n# #del X_valid\n# #gc.collect()\n# y_pred = model.predict(test_svd)\n                  \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_svd.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#result_dict_lgb = train_model(X=train, X_test=test_tfidf, y=y, params=params, model_type='lgb', plot_feature_importance=False, make_oof=True)\nprediction_lgb = (y_pred).argmax(1)\nsubmission_lgb = pd.DataFrame({'PetID': sub.PetID, 'AdoptionSpeed': [int(i) for i in prediction_lgb]})\nsubmission_lgb.head()\nsubmission_lgb.to_csv('submission.csv', index=False)\nprint('----------train lightgbm end-------------')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ##############xgboost##################\n# print('----------training xgboost-------------')\n# xgb_params = {'eta': 0.01, 'max_depth': 9, 'subsample': 0.9, 'colsample_bytree': 0.9,\n#           'objective': 'multi:softprob', 'eval_metric': 'merror', 'silent': True, 'nthread': 4, 'num_class': 5}\n# result_dict_xgb = train_model(params=xgb_params, model_type='xgb', make_oof=True)\n# prediction_xgb= (result_dict_xgb['prediction']).argmax(1)\n# submission_xgb = pd.DataFrame({'PetID': sub.PetID, 'AdoptionSpeed': [int(i) for i in prediction_xgb]})\n# submission_xgb.head()\n# submission_xgb.to_csv('submission_xgb.csv', index=False)\n# print('----------train xgboost end-------------')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}