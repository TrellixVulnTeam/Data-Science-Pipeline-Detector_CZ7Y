{"cells":[{"metadata":{"_uuid":"d4b3ddc43ced33c50b7daff55928b6f3d4a39d4e"},"cell_type":"markdown","source":"# Petfinder.my\n![](https://3blaws.s3.amazonaws.com/images/3_27.jpg)\n\n- *Best version so far #14*\n\n## * EDA and ML in progress *\n## ** * This is a draft version * **"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.style.use('fivethirtyeight')\n\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom wordcloud import WordCloud\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87e45fa9977b80d9525ff9380c7f6d174ce622d4"},"cell_type":"markdown","source":"### Defining Datatypes"},{"metadata":{"trusted":true,"_uuid":"81d7bc6e43c3a2942012838a12e35c508823d643"},"cell_type":"code","source":"# I am grateful for the help of author of this kernel for the main idea to load the dataset and save memory space!!\n# https://www.kaggle.com/theoviel/load-the-totality-of-the-data\n\ntrain_dtypes = {\n        'PetID':                            'str',\n        'AdoptionSpeed ':                   'int8',\n        'Type':                             'category',\n        'Name':                             'str',\n        'Age':                              'int8',\n        'Breed1':                           'category',\n        'Breed2':                           'category',\n        'Gender':                           'category',\n        'Color1':                           'category',\n        'Color2':                           'category',\n        'Color3':                           'category',\n        'MaturitySize':                     'float16',\n        'FurLength':                        'int8',\n        'Vaccinated':                       'category',\n        'Dewormed':                         'category',\n        'Sterilized':                       'category',\n        'Health':                           'category',\n        'Quantity':                         'uint16',\n        'State':                            'category',\n        'Fee':                              'float',\n        'RescuerID':                        'category',\n        'VideoAmt':                         'uint16',\n        'PhotoAmt':                         'uint16',\n        'Description ':                     'str'\n        }\n\nbreeds_dtypes = {\n        'BreedID':                          'category',\n        'Type ':                            'category',\n        'BreedName':                        'str'\n        }\n\ncolors_dtypes = {\n        'ColorID':                          'category',\n        'ColorName':                        'str'\n        }\n\nstates_dtypes = {\n        'StateID':                          'category',\n        'StateName':                        'str'\n        }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f4fc2856e5765f1aa901c89e170ffadefbe027d"},"cell_type":"markdown","source":"### Loading the datasets"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"breeds = pd.read_csv('../input/breed_labels.csv', dtype=breeds_dtypes)\ncolors = pd.read_csv('../input/color_labels.csv',  dtype=colors_dtypes)\nstates = pd.read_csv('../input/state_labels.csv', dtype=states_dtypes)\n\ntrain = pd.read_csv('../input/train/train.csv', dtype=train_dtypes)\ntest = pd.read_csv('../input/test/test.csv', dtype=train_dtypes)\n\n\ntrain['dataset_type'] = 'train'\ntest['dataset_type'] = 'test'\ntrain_and_test = pd.concat([train, test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c3399ec3798ddb5984987364b14662703408866"},"cell_type":"code","source":"train_and_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c72c36ec9659d059a75155a5c70a9d0503f2b206"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f433b4607a84775fd5ab2fb7d4de59a1163d53a7"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67dc03466dee401eb264254bbc354ef409170c96"},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true,"_uuid":"771c9512034eb2a77c5fc0658bb533a4fdea403d"},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8efd0b90ada9f7775c4918386f6517b1d78128b"},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06865a076e64f2a569d1428c6a351298cd289910"},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f223f8d669842dc19a239312e92fa43e4d9ddd30"},"cell_type":"code","source":"train_and_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ef0b8490e38cd5bb24bc90950503095614e773d"},"cell_type":"code","source":"train.AdoptionSpeed.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3eb87669fe22162d398af8ba83af732114188f9"},"cell_type":"code","source":"train.AdoptionSpeed.value_counts() * 100 / train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aafb1349fd1a3f44ffcac84eca2ecf871190bcad"},"cell_type":"code","source":"train['AdoptionSpeed'].value_counts().sort_index().plot('bar');\nplt.title('Adoption speed classes counts');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82c6858864d5aafea4afc4ae4b462699ce00e797"},"cell_type":"markdown","source":"### Joining Dataframes\n- Train and Test with Breeds"},{"metadata":{"trusted":true,"_uuid":"18aa1211b98c0d61d06a14589907adc499dbc690"},"cell_type":"code","source":"breeds[\"Breed1\"] = breeds.BreedID\n#breeds.drop(\"BreedID\", axis=\"columns\", inplace=True)\n#breeds.drop(\"Type\", axis=\"columns\", inplace=True)\n\ntrain_and_test_with_breeds = pd.merge(train_and_test, breeds[[\"Breed1\", \"BreedName\"]], how= 'left',\n                                      on=\"Breed1\")\n\ntrain_and_test_with_breeds[\"BreedName_1\"] = train_and_test_with_breeds.BreedName\ntrain_and_test_with_breeds.drop(\"BreedName\", axis=\"columns\", inplace=True)\n\nbreeds[\"Breed2\"] = breeds.Breed1\ntrain_and_test_with_breeds = pd.merge(train_and_test_with_breeds, breeds[[\"Breed2\", \"BreedName\"]], how= 'left',\n                                      on=\"Breed2\")\n\ntrain_and_test_with_breeds[\"BreedName_2\"] = train_and_test_with_breeds.BreedName\ntrain_and_test_with_breeds.drop(\"BreedName\", axis=\"columns\", inplace=True)\n \ntrain_and_test_with_breeds.head(4)\n#all_data_and_breeds = pd.merge(all_data, breeds, on=\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bda174d2509146aec8f329f13e180554b3b70e3"},"cell_type":"code","source":"train_and_test_with_breeds.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07740ecdcc7cd8860915d964267ac3ed8fae6ff4"},"cell_type":"markdown","source":"### Joining Dataframes\n- Train and Test and Breeds with Color Dataframe"},{"metadata":{"trusted":true,"_uuid":"c952ca98b2f1fe419b785867c0cca41bfc758017"},"cell_type":"code","source":"colors[\"Color1\"] = colors.ColorID\n#breeds.drop(\"BreedID\", axis=\"columns\", inplace=True)\n#breeds.drop(\"Type\", axis=\"columns\", inplace=True)\n\ntrain_and_test_with_breeds_colors = pd.merge(train_and_test_with_breeds, colors[[\"Color1\", \"ColorName\"]], \n                                             how= 'left', on=\"Color1\")\n\ntrain_and_test_with_breeds_colors[\"ColorName_1\"] = train_and_test_with_breeds_colors.ColorName\ntrain_and_test_with_breeds_colors.drop(\"ColorName\", axis=\"columns\", inplace=True)\n\ncolors[\"Color2\"] = colors.ColorID\ntrain_and_test_with_breeds_colors = pd.merge(train_and_test_with_breeds_colors, colors[[\"Color2\", \"ColorName\"]], \n                                      how= 'left', on=\"Color2\")\n\ntrain_and_test_with_breeds_colors[\"ColorName_2\"] = train_and_test_with_breeds_colors.ColorName\ntrain_and_test_with_breeds_colors.drop(\"ColorName\", axis=\"columns\", inplace=True)\n\ncolors[\"Color3\"] = colors.ColorID\ntrain_and_test_with_breeds_colors = pd.merge(train_and_test_with_breeds_colors, colors[[\"Color3\", \"ColorName\"]], \n                                      how= 'left', on=\"Color3\")\n\ntrain_and_test_with_breeds_colors[\"ColorName_3\"] = train_and_test_with_breeds_colors.ColorName\ntrain_and_test_with_breeds_colors.drop(\"ColorName\", axis=\"columns\", inplace=True)\n \ntrain_and_test_with_breeds_colors.head(4)\n#all_data_and_breeds = pd.merge(all_data, breeds, on=\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebea01470b8a0a9466f3e366fffab49e9d2c175e"},"cell_type":"code","source":"train_and_test_with_breeds_colors.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3bd8fb6d2df5bed33e13f3b7b690f5c96b36057"},"cell_type":"markdown","source":" ### Joining Dataframes #3\n- Train and Test and Breeds and Colors with States Dataframe"},{"metadata":{"trusted":true,"_uuid":"c00c931c2a1e8f810814572cc7cd898ff1723212"},"cell_type":"code","source":"states[\"State\"] = states.StateID\n#breeds.drop(\"BreedID\", axis=\"columns\", inplace=True)\n#breeds.drop(\"Type\", axis=\"columns\", inplace=True)\n\ntrain_and_test_with_breeds_colors_states = pd.merge(train_and_test_with_breeds_colors, states[[\"State\", \"StateName\"]], \n                                             how= 'left', on=\"State\")\n\n'''\ntrain_and_test_with_breeds_colors_states[\"ColorName_1\"] = train_and_test_with_breeds_colors.ColorName\ntrain_and_test_with_breeds_colors.drop(\"ColorName\", axis=\"columns\", inplace=True)\n\ncolors[\"Color2\"] = colors.ColorID\ntrain_and_test_with_breeds_colors = pd.merge(train_and_test_with_breeds_colors, colors[[\"Color2\", \"ColorName\"]], \n                                      how= 'left', on=\"Color2\")\n\ntrain_and_test_with_breeds_colors[\"ColorName_2\"] = train_and_test_with_breeds_colors.ColorName\ntrain_and_test_with_breeds_colors.drop(\"ColorName\", axis=\"columns\", inplace=True)\n\ncolors[\"Color3\"] = colors.ColorID\ntrain_and_test_with_breeds_colors = pd.merge(train_and_test_with_breeds_colors, colors[[\"Color3\", \"ColorName\"]], \n                                      how= 'left', on=\"Color3\")\n\ntrain_and_test_with_breeds_colors[\"ColorName_3\"] = train_and_test_with_breeds_colors.ColorName\ntrain_and_test_with_breeds_colors.drop(\"ColorName\", axis=\"columns\", inplace=True)\n'''\n \ntrain_and_test_with_breeds_colors_states.head(4)\n#all_data_and_breeds = pd.merge(all_data, breeds, on=\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09f84eefddf28dfda2ecc1d201d322dea4273833"},"cell_type":"code","source":"train_and_test_with_breeds_colors_states.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bacf96c23962edb276ace41e1f246aad0dd8725b"},"cell_type":"code","source":"train_and_test_with_breeds_colors_states.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0825aaa8f154a27d7b0b03915257079ec93f630"},"cell_type":"markdown","source":"### Distriguish between categorical and numerical columns:"},{"metadata":{"trusted":true,"_uuid":"010201681b5ebc6a8057ceded46f447fa56cd4a9"},"cell_type":"code","source":"categorical_columns = list(train_and_test_with_breeds_colors_states.loc[:, ((train_and_test_with_breeds_colors_states.dtypes ==\"category\") | (train_and_test_with_breeds_colors_states.dtypes ==\"object\"))].columns)\nnumerical_columns = list(train_and_test_with_breeds_colors_states.loc[:, ~((train_and_test_with_breeds_colors_states.dtypes ==\"category\") | (train_and_test_with_breeds_colors_states.dtypes ==\"object\"))].columns)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"daa0af55c42d4df5d3d74c54fa41186856413124"},"cell_type":"code","source":"categorical_columns.remove(\"PetID\")\ncategorical_columns.remove(\"RescuerID\")\ncategorical_columns.remove(\"Description\")\ncategorical_columns.remove(\"dataset_type\")\ncategorical_columns.remove(\"BreedName_1\")\ncategorical_columns.remove(\"BreedName_2\")\ncategorical_columns.remove(\"ColorName_1\")\ncategorical_columns.remove(\"ColorName_2\")\ncategorical_columns.remove(\"ColorName_3\")\ncategorical_columns.remove(\"Name\")\ncategorical_columns.remove(\"StateName\")\n\nprint(categorical_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38f821d04edda1bc4796e2ebf70c30ad8a21a3df"},"cell_type":"code","source":"numerical_columns.remove(\"AdoptionSpeed\")\n\nprint(numerical_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### EDA and pets' Names"},{"metadata":{"trusted":true,"_uuid":"ec45e911c06604a9a8a991438a8029adcb827951"},"cell_type":"code","source":"def categorical_univariate_and_bivariate_stats(df, feature):\n    \n    train_sample = df\n    \n    if feature in train_sample.columns:\n    \n        print(\"Top 10 most occurred categories for the categorical feature\", feature)\n        print(train_sample[feature].value_counts().head(10))\n\n        f, axes = plt.subplots(1, 2, figsize=(21, 10))\n\n        train_sample[feature].value_counts().head(10).plot.bar(ax=axes[0], colormap=\"BrBG\")\n\n        #train_sample.groupby([\"AdoptionSpeed\", feature]).count()[\"PetID\"].unstack(0).sort_values(by=1, axis=0, ascending=False).head(10).plot.bar(ax=axes[1], colormap=\"coolwarm\")\n        train_sample.groupby([\"AdoptionSpeed\", feature]).count()[\"PetID\"].unstack(0).head(10).plot.bar(ax=axes[1], colormap=\"coolwarm\")\n\n        \n        f.suptitle(\"Categorical feature: \"+\" Univariate and Bivariate plots against the target variable\")\n        \n    else:\n        print(\"This feature has been removed from dataset due to high NaN rate or highly unbalanced values\")\n        \n        \ndef logistic_fit(df, feature):\n    \n    import warnings\n    warnings.filterwarnings(\"ignore\")\n    \n    from sklearn.metrics import accuracy_score\n    from sklearn.metrics import precision_score\n    from sklearn.metrics import recall_score\n    from sklearn.metrics import f1_score\n    from sklearn.metrics import r2_score\n    '''\n    train_sample = df\n\n    if feature in train_sample.columns:\n        \n        from sklearn.linear_model import LogisticRegression\n        \n        f, axes = plt.subplots(1, 2, figsize=(21, 10))\n\n        # test if there is a logistic relationship between the feature1 and the target.\n        print()\n        print(\"Fitting a logistic regression model for the feature\", feature,\"against the target variable\")\n        \n               \n        mask = ~train_sample[feature].isnull() & ~train_sample[\"AdoptionSpeed\"].isnull()\n\n        logmodel = LogisticRegression(C=1e5, solver='lbfgs')\n        \n        if feature in categorical_columns:        \n            logmodel.fit(train_sample[feature][mask].cat.codes.values.reshape(-1,1), train_sample[\"AdoptionSpeed\"][mask])\n            predictions = logmodel.predict(train_sample[feature][mask].cat.codes.values.reshape(-1,1))\n        else:\n            logmodel.fit(train_sample[feature][mask].values.reshape(-1,1), train_sample[\"AdoptionSpeed\"][mask])\n            predictions = logmodel.predict(train_sample[feature][mask].values.reshape(-1,1))\n\n        from sklearn.metrics import classification_report\n        print(classification_report(train_sample[\"AdoptionSpeed\"][mask], predictions))\n        print(\"\")\n        print(\"accuracy score:\", accuracy_score(train_sample[\"AdoptionSpeed\"][mask], predictions))\n        print(\"F1 score:\", accuracy_score(train_sample[\"AdoptionSpeed\"][mask], predictions))\n        #print(\"R^2 score:\", r2_score(train_sample[\"HasDetections\"][mask], predictions))\n\n        import scikitplot as skplt\n        skplt.metrics.plot_confusion_matrix(train_sample[\"AdoptionSpeed\"][mask], predictions, normalize=False,\n                                            title = \"Confusion matrix for the feature: \"+feature+\" against the target variable after fitting a logistic regression model\",\n                                           figsize=(10,8), text_fontsize='medium', cmap=\"BrBG\", ax = axes[0])\n        \n        \n        # import statsmodels.api as sm\n        # print()\n        # est = sm.Logit(train_sample[\"HasDetections\"][mask], train_sample[feature][mask].cat.codes.values.reshape(-1,1))\n        # result1=est.fit()\n        # print(result1.summary())\n        if feature in categorical_columns:\n            axes[1] = plt.scatter(train_sample[feature][mask].cat.codes.values.reshape(-1,1), predictions)\n            axes[1] = plt.scatter(train_sample[feature][mask].cat.codes.values.reshape(-1,1), logmodel.predict_proba(train_sample[feature][mask].cat.codes.values.reshape(-1,1))[:,1])\n            plt.xlabel(feature)\n            plt.ylabel(\"HasDetections Probability\")\n            plt.title(\"Probability of Detecting a Malware vs the \"+ feature)\n            plt.show()\n        else:\n            axes[1] = plt.scatter(train_sample[feature][mask].values.reshape(-1,1), predictions)\n            axes[1] = plt.scatter(train_sample[feature][mask].values.reshape(-1,1), logmodel.predict_proba(train_sample[feature][mask].values.reshape(-1,1))[:,1])\n            plt.xlabel(feature)\n            plt.ylabel(\"HasDetections Probability\")\n            plt.title(\"Probability of Detecting a Malware vs the \"+ feature)\n            plt.show()\n        \n    else:\n        print(\"This feature has been removed from dataset due to high NaN rate or highly unbalanced values\")\n    '''\n    from sklearn.tree import DecisionTreeClassifier\n\n    train_df = df\n\n    clf = DecisionTreeClassifier()\n    \n    if ((feature == \"State\") or (feature in numerical_columns)):\n        clf.fit(train_df[feature].values.reshape(-1, 1), train_df[\"AdoptionSpeed\"])\n        predictions = clf.predict(train_df[feature].values.reshape(-1, 1))\n    else:\n        clf.fit(train_df[feature].cat.codes.values.reshape(-1, 1), train_df[\"AdoptionSpeed\"])\n        predictions = clf.predict(train_df[feature].cat.codes.values.reshape(-1, 1))\n\n    from sklearn.metrics import accuracy_score\n    from sklearn.metrics import precision_score\n    from sklearn.metrics import recall_score\n    from sklearn.metrics import f1_score\n    from sklearn.metrics import r2_score\n\n    from sklearn.metrics import classification_report\n    print(classification_report(train_df[\"AdoptionSpeed\"], predictions))\n\n    import scikitplot as skplt\n    skplt.metrics.plot_confusion_matrix(train_df[\"AdoptionSpeed\"], predictions, normalize=False,\n                                        figsize=(10,8), text_fontsize='medium')\n\n#print(\"F1 score:\", accuracy_score( train_df[\"AdoptionSpeed\"], predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_and_test_with_breeds_colors_states[train_and_test_with_breeds_colors_states.dataset_type == \"train\"]\n\ncategorical_univariate_and_bivariate_stats(train_df, feature=\"Name\")\n#logistic_fit(train_df, feature=\"Type\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d0bf0c1c8e1972abc0c6a2031d4e229fc7787f6"},"cell_type":"code","source":"train_df = train_and_test_with_breeds_colors_states[train_and_test_with_breeds_colors_states.dataset_type == \"train\"]\n\ncategorical_univariate_and_bivariate_stats(train_df, feature=\"Type\")\nlogistic_fit(train_df, feature=\"Type\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64ae21f42bdd2dfabce2fa9833f80c35f2a16f9c"},"cell_type":"code","source":"categorical_univariate_and_bivariate_stats(train_df, feature=\"Vaccinated\")\nlogistic_fit(train_df, feature=\"Vaccinated\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47f4147cbbd3d5d28d2f4e396280be72482e707c"},"cell_type":"code","source":"categorical_univariate_and_bivariate_stats(train_df, feature=\"Sterilized\")\nlogistic_fit(train_df, feature=\"Sterilized\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"688071bcdab2e99edbdf2cc331c6f0989e5040bc"},"cell_type":"code","source":"categorical_univariate_and_bivariate_stats(train_df, feature=\"Dewormed\")\nlogistic_fit(train_df, feature=\"Dewormed\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61717ecdd708d8ec1680def29aec43e23cbb109e"},"cell_type":"code","source":"categorical_univariate_and_bivariate_stats(train_df, feature=\"State\")\nlogistic_fit(train_df, feature=\"State\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ababe0b608ca45c102e5ba8f4bdb22c642dbbac"},"cell_type":"code","source":"categorical_univariate_and_bivariate_stats(train_df, feature=\"Color1\")\nlogistic_fit(train_df, feature=\"Color1\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mosaic Plot and Chi-Square"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom scipy.stats import chi2_contingency\n\nclass ChiSquare:\n    def __init__(self, dataframe):\n        self.df = dataframe\n        self.p = None #P-Value\n        self.chi2 = None #Chi Test Statistic\n        self.dof = None\n        \n        self.dfObserved = None\n        self.dfExpected = None\n        \n    def _print_chisquare_result(self, colX, alpha):\n        result = \"\"\n        if self.p<alpha:\n            result=\"{0} is IMPORTANT for Prediction and has p-value {1}\".format(colX, self.p)\n        else:\n            result=\"{0} is NOT an important predictor. (Discard {0} from model)\".format(colX)\n\n        print(result)\n        \n    def TestIndependence(self,colX,colY, alpha=0.05):\n        X = self.df[colX].astype(str)\n        Y = self.df[colY].astype(str)\n        \n        self.dfObserved = pd.crosstab(Y,X) \n        chi2, p, dof, expected = stats.chi2_contingency(self.dfObserved.values)\n        self.p = p\n        self.chi2 = chi2\n        self.dof = dof \n        \n        self.dfExpected = pd.DataFrame(expected, columns=self.dfObserved.columns, index = self.dfObserved.index)\n        \n        self._print_chisquare_result(colX,alpha)\n\ncT = ChiSquare(train_df)        \nfor var in categorical_columns:\n    cT.TestIndependence(colX=var,colY=\"AdoptionSpeed\" )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.mosaicplot import mosaic\nimport matplotlib.pyplot as plt\nimport pandas\n\nsns.set(rc={'figure.figsize':(13, 8)})\n#mosaic(train_df, ['AdoptionSpeed', 'Type'])\n#plt.show()\n\ntab = pd.crosstab(train_df['AdoptionSpeed'], train_df['Type'])\nmosaic(tab.stack(), title=\"Mosaic Plot\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Numerical Features EDA"},{"metadata":{"trusted":true,"_uuid":"4c562018514c3e4b30b8f9aa6c21bb6d211ced5a"},"cell_type":"code","source":"def numerical_univariate_and_bivariate_plot(df, feature, num_of_bins = 40):\n    \n    import warnings\n    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n    \n    train_sample = df\n    \n    if feature in train_sample.columns:\n    \n        print(\"Top 10 Values counts for the numerical feature\", feature)\n        print(train_sample[feature].value_counts().head(10))\n        print(\"Min value\", train_sample[feature].min())\n        print(\"Max value\", train_sample[feature].max())\n        print(\"NaN values\", train_sample[feature].isnull().sum())\n        print(\"Number of unique values\", train_sample[feature].nunique())\n\n        if train[feature].nunique() > 2:\n            print(\"Mean value\", train_sample[feature].mean())\n            print(\"Variance value\", train_sample[feature].var())\n\n        # for binary features\n        if train[feature].nunique() <= 2:\n\n            f, axes = plt.subplots(1, 2, figsize=(21, 10))\n\n            sns.countplot(x=feature, data=train_sample, ax=axes[0])\n            sns.countplot(x=feature, hue = \"AdoptionSpeed\", data=train_sample, ax=axes[1])\n            \n            f.suptitle(\"Numerical feature: \"+feature+\" Univariate and Bivariate plots against the target variable\")\n\n        # for numeric features\n        else:\n\n            f, axes = plt.subplots(1, 3, figsize=(21, 10))\n\n            sns.distplot(train_sample[feature].dropna(), rug=False, kde=False, ax=axes[0], bins = num_of_bins)\n\n            #sns.violinplot(x=\"AdoptionSpeed\", y = feature, hue=\"AdoptionSpeed\", data=train_sample, ax=axes[1])\n            sns.boxplot(x=\"AdoptionSpeed\", y = feature, hue=\"AdoptionSpeed\", data=train_sample, ax=axes[1])\n\n            if feature == \"LocaleEnglishNameIdentifier\":\n                sns.distplot(train_sample[train_sample[\"AdoptionSpeed\"] == 0][feature].dropna().astype(\"int16\"), rug=False, kde=False, ax=axes[2], bins = num_of_bins)\n                sns.distplot(train_sample[train_sample[\"AdoptionSpeed\"] == 1][feature].dropna().astype(\"int16\"), rug=False, kde=False, ax=axes[2], bins = num_of_bins)\n            else:\n                sns.distplot(train_sample[train_sample[\"AdoptionSpeed\"] == 0][feature].dropna(), rug=False, kde=False, ax=axes[2], bins = num_of_bins)\n                sns.distplot(train_sample[train_sample[\"AdoptionSpeed\"] == 1][feature].dropna(), rug=False, kde=False, ax=axes[2], bins = num_of_bins)\n                sns.distplot(train_sample[train_sample[\"AdoptionSpeed\"] == 2][feature].dropna(), rug=False, kde=False, ax=axes[2], bins = num_of_bins)\n                sns.distplot(train_sample[train_sample[\"AdoptionSpeed\"] == 3][feature].dropna(), rug=False, kde=False, ax=axes[2], bins = num_of_bins)\n                sns.distplot(train_sample[train_sample[\"AdoptionSpeed\"] == 4][feature].dropna(), rug=False, kde=False, ax=axes[2], bins = num_of_bins)\n            \n                f.suptitle(\"Numerical feature: \"+feature+\" Univariate and Bivariate plots against the target variable\")\n    else:\n        print(\"This feature has been removed from dataset due to high NaN rate or highly unbalanced values\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f0fba4b3cb7a1d66f1d4b244389e97cc82d8d0d"},"cell_type":"code","source":"numerical_univariate_and_bivariate_plot(train_df, feature=\"Age\")\nlogistic_fit(train_df, \"Age\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75047c90f8cea7043833c099d81ba3d2b6d02e96"},"cell_type":"code","source":"numerical_univariate_and_bivariate_plot(train_df, feature=\"Fee\")\nlogistic_fit(train_df, \"Fee\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94311dbe362e6e9e4a34bc6cdb05827c34ddc383"},"cell_type":"code","source":"numerical_univariate_and_bivariate_plot(train_df, feature=\"FurLength\")\nlogistic_fit(train_df, \"FurLength\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7946fcad7a2d36bf8b24b5728e31cf528af9924e"},"cell_type":"code","source":"numerical_univariate_and_bivariate_plot(train_df, feature=\"MaturitySize\")\nlogistic_fit(train_df, \"MaturitySize\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f432eb5c17275db17c079aafabe12d9bb6a5f87b"},"cell_type":"code","source":"numerical_univariate_and_bivariate_plot(train_df, feature=\"PhotoAmt\")\nlogistic_fit(train_df, \"PhotoAmt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6301ea9c1bbc49a6a7c22323617624aa9165612b"},"cell_type":"code","source":"numerical_univariate_and_bivariate_plot(train_df, feature=\"Quantity\")\nlogistic_fit(train_df, \"Quantity\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d52866380c615b862c3e4d7457a6f8bbe3198d8"},"cell_type":"code","source":"numerical_univariate_and_bivariate_plot(train_df, feature=\"VideoAmt\")\nlogistic_fit(train_df, \"VideoAmt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8320d9d52d7ac3fca40ae31b2b8a19ed49d6ade0"},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(10, 8)})\n\n# Compute the correlation matrix\ncorr = train_df[numerical_columns].corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d93a123d2700b522dab73e4487e48aa83b51ead"},"cell_type":"code","source":"# I want to thank https://www.kaggle.com/artgor/exploration-of-data-step-by-step/notebook for the following snippet:\n\nfig, ax = plt.subplots(figsize = (16, 12))\nplt.subplot(1, 2, 1)\ntext_dog = ' '.join(train_and_test.loc[train_and_test['Type'] == '1', 'Name'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='Black',\n                      width=1200, height=1000).generate(text_dog)\nplt.imshow(wordcloud)\nplt.title('Top dog names')\nplt.axis(\"off\")\n\nplt.subplot(1, 2, 2)\ntext_cat = ' '.join(train_and_test.loc[train_and_test['Type'] == '2', 'Name'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='Black',\n                      width=1200, height=1000).generate(text_cat)\nplt.imshow(wordcloud)\nplt.title('Top cat names')\nplt.axis(\"off\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"789e32ef5cf51bd2d6c170fdcb2e0953c68ca80b"},"cell_type":"markdown","source":"## XGBoost ML Modeling"},{"metadata":{"_uuid":"b033294903109764b7d4968806427a78d4bb6004"},"cell_type":"markdown","source":"### Preparing train and test set before ML modeling"},{"metadata":{"trusted":true,"_uuid":"fcf4b82966b6c755f6aa96e55507ba682d33e146"},"cell_type":"code","source":"train_df = train_and_test_with_breeds_colors_states[train_and_test_with_breeds_colors_states.dataset_type == \"train\"]\ntest_df = train_and_test_with_breeds_colors_states[train_and_test_with_breeds_colors_states.dataset_type == \"test\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4c0e5e50f8338dbc409a7e0132c235de820fa5d"},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e52b721677ca8f064a114bf8fbbd1a7218ffaf42"},"cell_type":"code","source":"test_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b209e0365138d564c3d7708e22cfe48e44fa60df"},"cell_type":"code","source":"# A big thank you to https://www.kaggle.com/econdata/petfinder-lgbm/notebook for his intuition\n\n%time\nimport json\n\ntrain_id = train['PetID']\ntest_id = test['PetID']\ndoc_sent_mag = []\ndoc_sent_score = []\nnf_count = 0\nfor pet in train_id:\n    try:\n        with open('../input/train_sentiment/' + pet + '.json', 'r') as f:\n            sentiment = json.load(f)\n        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n        doc_sent_score.append(sentiment['documentSentiment']['score'])\n    except FileNotFoundError:\n        nf_count += 1\n        doc_sent_mag.append(-1)\n        doc_sent_score.append(-1)\n\ntrain_df['doc_sent_mag'] = doc_sent_mag\ntrain_df['doc_sent_score'] = doc_sent_score\n\ndoc_sent_mag = []\ndoc_sent_score = []\nnf_count = 0\nfor pet in test_id:\n    try:\n        with open('../input/test_sentiment/' + pet + '.json', 'r') as f:\n            sentiment = json.load(f)\n        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n        doc_sent_score.append(sentiment['documentSentiment']['score'])\n    except FileNotFoundError:\n        nf_count += 1\n        doc_sent_mag.append(-1)\n        doc_sent_score.append(-1)\n\ntest_df['doc_sent_mag'] = doc_sent_mag\ntest_df['doc_sent_score'] = doc_sent_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering with the textual Data"},{"metadata":{"trusted":true,"_uuid":"7d8b1c17190da9c4305dea196dcdbca99b0d5dd2"},"cell_type":"code","source":"%%time\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\ntrain_desc = train.Description.fillna(\"none\").values\ntest_desc = test.Description.fillna(\"none\").values\n\n\nmax_train_len = [len(x) for x in train_desc]\n\nmax_test_len = [len(x) for x in test_desc]\n\n\ntfv = TfidfVectorizer(min_df=3,  max_features=max([max(max_train_len), max(max_test_len)]),\n        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n        stop_words = 'english')\n    \n# Fit TFIDF\ntfv.fit(list(train_desc))\nX =  tfv.transform(train_desc)\nX_test = tfv.transform(test_desc)\n\ncomponents = 480\nsvd = TruncatedSVD(n_components=components)\nsvd.fit(X)\nprint(svd.explained_variance_ratio_.sum())\nprint(svd.explained_variance_ratio_)\nX = svd.transform(X)\nX = pd.DataFrame(X, columns=['svd_{}'.format(i) for i in range(components)])\ntrain_df = pd.concat((train_df, X), axis=1)\nX_test = svd.transform(X_test)\nX_test = pd.DataFrame(X_test, columns=['svd_{}'.format(i) for i in range(components)])\n\ntest_df.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)\n\ntest_df = pd.concat([test_df, X_test], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering with the matadata"},{"metadata":{"trusted":true,"_uuid":"f68726e1c6b5c286bfd383fb862a3d4c7b7b3433"},"cell_type":"code","source":"# I want to thank https://www.kaggle.com/econdata/petfinder-lgbm/notebook for his intuition\n\n%time\nvertex_xs = []\nvertex_ys = []\nbounding_confidences = []\nbounding_importance_fracs = []\ndominant_blues = []\ndominant_greens = []\ndominant_reds = []\ndominant_pixel_fracs = []\ndominant_scores = []\nlabel_descriptions = []\nlabel_scores = []\nnf_count = 0\nnl_count = 0\nfor pet in train_id:\n    try:\n        with open('../input/train_metadata/' + pet + '-1.json', 'r') as f:\n            data = json.load(f)\n        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n        vertex_xs.append(vertex_x)\n        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n        vertex_ys.append(vertex_y)\n        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n        bounding_confidences.append(bounding_confidence)\n        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n        bounding_importance_fracs.append(bounding_importance_frac)\n        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n        dominant_blues.append(dominant_blue)\n        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n        dominant_greens.append(dominant_green)\n        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n        dominant_reds.append(dominant_red)\n        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n        dominant_pixel_fracs.append(dominant_pixel_frac)\n        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n        dominant_scores.append(dominant_score)\n        if data.get('labelAnnotations'):\n            label_description = data['labelAnnotations'][0]['description']\n            label_descriptions.append(label_description)\n            label_score = data['labelAnnotations'][0]['score']\n            label_scores.append(label_score)\n        else:\n            nl_count += 1\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n    except FileNotFoundError:\n        nf_count += 1\n        vertex_xs.append(-1)\n        vertex_ys.append(-1)\n        bounding_confidences.append(-1)\n        bounding_importance_fracs.append(-1)\n        dominant_blues.append(-1)\n        dominant_greens.append(-1)\n        dominant_reds.append(-1)\n        dominant_pixel_fracs.append(-1)\n        dominant_scores.append(-1)\n        label_descriptions.append('nothing')\n        label_scores.append(-1)\n\nprint(nf_count)\nprint(nl_count)\ntrain_df.loc[:, 'vertex_x'] = vertex_xs\ntrain_df.loc[:, 'vertex_y'] = vertex_ys\ntrain_df.loc[:, 'bounding_confidence'] = bounding_confidences\ntrain_df.loc[:, 'bounding_importance'] = bounding_importance_fracs\ntrain_df.loc[:, 'dominant_blue'] = dominant_blues\ntrain_df.loc[:, 'dominant_green'] = dominant_greens\ntrain_df.loc[:, 'dominant_red'] = dominant_reds\ntrain_df.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\ntrain_df.loc[:, 'dominant_score'] = dominant_scores\ntrain_df.loc[:, 'label_description'] = label_descriptions\ntrain_df.loc[:, 'label_score'] = label_scores\n\n\nvertex_xs = []\nvertex_ys = []\nbounding_confidences = []\nbounding_importance_fracs = []\ndominant_blues = []\ndominant_greens = []\ndominant_reds = []\ndominant_pixel_fracs = []\ndominant_scores = []\nlabel_descriptions = []\nlabel_scores = []\nnf_count = 0\nnl_count = 0\nfor pet in test_id:\n    try:\n        with open('../input/test_metadata/' + pet + '-1.json', 'r') as f:\n            data = json.load(f)\n        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n        vertex_xs.append(vertex_x)\n        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n        vertex_ys.append(vertex_y)\n        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n        bounding_confidences.append(bounding_confidence)\n        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n        bounding_importance_fracs.append(bounding_importance_frac)\n        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n        dominant_blues.append(dominant_blue)\n        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n        dominant_greens.append(dominant_green)\n        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n        dominant_reds.append(dominant_red)\n        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n        dominant_pixel_fracs.append(dominant_pixel_frac)\n        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n        dominant_scores.append(dominant_score)\n        if data.get('labelAnnotations'):\n            label_description = data['labelAnnotations'][0]['description']\n            label_descriptions.append(label_description)\n            label_score = data['labelAnnotations'][0]['score']\n            label_scores.append(label_score)\n        else:\n            nl_count += 1\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n    except FileNotFoundError:\n        nf_count += 1\n        vertex_xs.append(-1)\n        vertex_ys.append(-1)\n        bounding_confidences.append(-1)\n        bounding_importance_fracs.append(-1)\n        dominant_blues.append(-1)\n        dominant_greens.append(-1)\n        dominant_reds.append(-1)\n        dominant_pixel_fracs.append(-1)\n        dominant_scores.append(-1)\n        label_descriptions.append('nothing')\n        label_scores.append(-1)\n\nprint(nf_count)\ntest_df.loc[:, 'vertex_x'] = vertex_xs\ntest_df.loc[:, 'vertex_y'] = vertex_ys\ntest_df.loc[:, 'bounding_confidence'] = bounding_confidences\ntest_df.loc[:, 'bounding_importance'] = bounding_importance_fracs\ntest_df.loc[:, 'dominant_blue'] = dominant_blues\ntest_df.loc[:, 'dominant_green'] = dominant_greens\ntest_df.loc[:, 'dominant_red'] = dominant_reds\ntest_df.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\ntest_df.loc[:, 'dominant_score'] = dominant_scores\ntest_df.loc[:, 'label_description'] = label_descriptions\ntest_df.loc[:, 'label_score'] = label_scores","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"271c2b9390fa417e6b1c204e05d538ddc5baa298"},"cell_type":"markdown","source":"### Small Feature Engineering"},{"metadata":{"trusted":true,"_uuid":"9441b9ef8a51f2a4b5a37616afdc45bf9642b030"},"cell_type":"code","source":"train_df[\"HasName\"] = np.where(train_df[\"Name\"].isnull(), 1, 0)\ntrain_df[\"HasDescription\"] = np.where(train_df[\"Description\"].isnull(), 1, 0)\n\ntest_df[\"HasName\"] = np.where(test_df[\"Name\"].isnull(), 1, 0)\ntest_df[\"HasDescription\"] = np.where(test_df[\"Description\"].isnull(), 1, 0)\n\ntrain_df.drop([\"Name\", \"PetID\", \"RescuerID\", \"dataset_type\", \"BreedName_1\", \"BreedName_2\", \"ColorName_1\", \"ColorName_2\", \"ColorName_3\",\n                    \"StateName\", \"Description\"], axis=\"columns\", inplace = True)\n\ntest_df.drop([\"Name\", \"PetID\", \"RescuerID\", \"dataset_type\", \"BreedName_1\", \"BreedName_2\", \"ColorName_1\", \"ColorName_2\", \"ColorName_3\",\n                    \"StateName\", \"Description\"], axis=\"columns\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53777b0c7b4fbd416b60f9c70858b7dae98db923"},"cell_type":"markdown","source":"### Concatenate train and test sets before label encoding"},{"metadata":{"trusted":true,"_uuid":"266c51d07716fa1d1bcd7a8db8108e57dd457194"},"cell_type":"code","source":"train_shape = train_df.shape\ntest_shape = test_df.shape\n\ntrain_and_test = pd.concat([train_df, test_df], axis=\"rows\", sort=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67a6680634c25b00e92754bacedd708668fb95c4"},"cell_type":"code","source":"train_and_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c3e6775fd5f944cd0660247be62c78808003fcf"},"cell_type":"code","source":"train_and_test.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e47faf00fde98b2efcb3ac4c8d02a493ef1913e"},"cell_type":"code","source":"#train_and_test.drop([\"Name\", \"PetID\", \"RescuerID\", \"dataset_type\", \"BreedName_1\", \"BreedName_2\", \"ColorName_1\", \"ColorName_2\", \"ColorName_3\",\n#                    \"StateName\", \"Description\"], axis=\"columns\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c227f31f1cee32dac6b4b060dbe3b7435fc99b23"},"cell_type":"markdown","source":"### Encode the Categorical features before machine learning modeling"},{"metadata":{"trusted":true,"_uuid":"16f589af3499010bc29ecb9ca298cc7d07cf41e4"},"cell_type":"code","source":"categorical_columns.append(\"label_description\")\n#categorical_columns.remove(\"label_description\")\n\ncategorical_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f2bc0197ce6a8483c0de4ab052c9233de7a2e81"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n\ndef MultiLabelEncoder(columnlist,dataframe):\n    for i in columnlist:\n        #print(i)\n        labelencoder_X=LabelEncoder()\n        dataframe[i]=labelencoder_X.fit_transform(dataframe[i])\n\n        \ntrain_and_test.loc[:, categorical_columns] = train_and_test[categorical_columns].astype('category')\nMultiLabelEncoder(categorical_columns, train_and_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e279f7804349d6e04ec1de54e4707848c82c567"},"cell_type":"code","source":"train_df = train_and_test[0:train_shape[0]]\ntest_df = train_and_test[(train_shape[0]):(train_and_test.shape[0]+1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"814aec564ba854509c0f4fca5879fece87eb9f8b"},"cell_type":"code","source":"test_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06b64394bd8da630c6bce6b56c0ca5ab1d9e84bf"},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ca1065d89c15f6e4e16bd4b2eca048f62424883"},"cell_type":"markdown","source":"### Remove the HasDetections columns from test set, it has been added during dataframe concatenation."},{"metadata":{"trusted":true,"_uuid":"242f8581f841fc2fdff0bdc1e7cbc7e916909195"},"cell_type":"code","source":"test_df = test_df.drop([\"AdoptionSpeed\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce55035b50267bb3f2f18b2b7059ef96ad1f552d"},"cell_type":"code","source":"train_df['AdoptionSpeed'] = train_df['AdoptionSpeed'].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b5d5a7491a1332dcfc4d4851df9f9bb3bbcb8c3"},"cell_type":"code","source":"y = train_df['AdoptionSpeed']\nX = train_df.drop(['AdoptionSpeed'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e634bddfc3d6a2953cade86712aef6d1df1ffd67"},"cell_type":"markdown","source":"### XGBoost Method for All purposes"},{"metadata":{"trusted":true,"_uuid":"dc9e297904191e26274ed514de280d52988aa883"},"cell_type":"code","source":"def xgbooft_all_purpose(X, y, type_of_training):\n    \n    from sklearn.model_selection import train_test_split, StratifiedKFold\n    from sklearn.metrics import accuracy_score\n    from sklearn.metrics import precision_score\n    from sklearn.metrics import recall_score\n    from sklearn.metrics import f1_score\n    from sklearn.metrics import classification_report\n    from sklearn.metrics import roc_auc_score\n    import scikitplot as skplt\n    import time\n    import random\n    \n    import xgboost as xgb\n    \n    # xgboost parameters\n    eta = 0.01\n    estimators  = 8000\n    depth = 8\n    gamma_value = 0.4\n    colsample_bytree_value = 0.6\n    max_rounds = 400\n    \n    if type_of_training == \"baseline\":\n    # create a 70/30 split of the data \n        xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n    \n        import xgboost as xgb\n\n        start_time = time.time()\n\n        clf_xgb = xgb.XGBClassifier(learning_rate=eta, \n                                    n_estimators=estimators, \n                                    max_depth=depth,\n                                    min_child_weight=1,\n                                    gamma=gamma_value,\n                                    subsample=1,\n                                    colsample_bytree=colsample_bytree_value,\n                                    objective= 'multi:softmax',\n                                    nthread=-1,\n                                    scale_pos_weight=1,\n                                    reg_alpha = 0,\n                                    reg_lambda = 1,\n                                    seed=42)\n\n        clf_xgb.fit(xtrain, ytrain, eval_set=[(xtrain, ytrain), (xvalid, yvalid)], \n                    early_stopping_rounds=max_rounds, eval_metric='mlogloss', verbose=100)\n\n        predictions = clf_xgb.predict(xvalid)\n        predictions_probas = clf_xgb.predict_proba(xvalid)\n\n        print()\n        print(classification_report(yvalid, predictions))\n\n        print()\n        print(\"f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n\n        print()\n        print(\"elapsed time in seconds: \", time.time() - start_time)\n        \n        skplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n        \n        predictions_probas = clf_xgb.predict_proba(xvalid)\n        skplt.metrics.plot_roc(yvalid, predictions_probas)\n        \n        skplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n        \n        xgb.plot_importance(clf_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n\n        print()\n        #gc.collect()\n        \n        return clf_xgb, predictions, predictions_probas\n        \n    elif type_of_training == \"stratified\":\n        \n        xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n        \n        predictions_probas_list = []\n        index_fold = 0\n        best_score = 1\n        \n        folds = StratifiedKFold(n_splits=3, shuffle=True, random_state = 42)\n        \n        clf_stra_xgb = xgb.XGBClassifier(learning_rate=eta, \n                                    n_estimators=estimators, \n                                    max_depth=depth,\n                                    min_child_weight=1,\n                                    gamma=gamma_value,\n                                    subsample=1,\n                                    colsample_bytree=colsample_bytree_value,\n                                    objective= 'multi:softmax',\n                                    nthread=-1,\n                                    scale_pos_weight=1,\n                                    reg_alpha = 0,\n                                    reg_lambda = 1,\n                                    seed=42)\n        \n        for train_index, valid_index in folds.split(xtrain, ytrain):\n            xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n            ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n            print(\"Stratified Fold:\", index_fold)\n            index_fold = index_fold + 1\n            \n            import xgboost as xgb\n\n            start_time = time.time()\n\n\n            clf_stra_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                        early_stopping_rounds=max_rounds, eval_metric='mlogloss', verbose=100)\n            \n            #if (clf_stra_xgb.best_score < best_score):\n            #    clf_best_stra_xgb = clf_stra_xgb\n            #    best_score = clf_stra_xgb.best_score\n            \n            print()\n\n            predictions_probas = clf_stra_xgb.predict_proba(xvalid)\n            predictions_probas_list.append(predictions_probas)\n            \n        \n        predictions_probas=[sum(i)/index_fold for i in zip(*predictions_probas_list)]\n        predictions = np.argmax(predictions_probas, axis=1)\n        \n        #xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n        #clf_stra_xgb = clf_best_stra_xgb\n        #del clf_best_stra_xgb\n        #print(\"Best score:\", best_score)\n        \n        predictions = clf_stra_xgb.predict(xvalid)\n        predictions_probas = clf_stra_xgb.predict_proba(xvalid)\n\n        print()\n        print(classification_report(yvalid, predictions))\n\n        print()\n        print(\"f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n\n        print()\n        print(\"elapsed time in seconds: \", time.time() - start_time)\n        \n        skplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n        \n        skplt.metrics.plot_roc(yvalid, predictions_probas)\n        \n        skplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n        \n        xgb.plot_importance(clf_stra_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n\n        print()\n        #gc.collect()\n        return clf_stra_xgb, predictions, predictions_probas\n\n    elif type_of_training == \"oversampling\":\n        \n        #### resampling techniques:\n        from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n\n        # create a 70/30 split of the data \n        xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n\n        # RandomOverSampler\n        ros = RandomOverSampler(random_state=42)\n        X_resampled, y_resampled = ros.fit_resample(xtrain, ytrain)\n        xtrain=pd.DataFrame(X_resampled, columns = X.columns)\n        ytrain = y_resampled\n        \n\n        start_time = time.time()\n\n        clf_ros_xgb = xgb.XGBClassifier(learning_rate=eta, \n                                    n_estimators=estimators, \n                                    max_depth=depth,\n                                    min_child_weight=1,\n                                    gamma=gamma_value,\n                                    subsample=1,\n                                    colsample_bytree=colsample_bytree_value,\n                                    objective= 'multi:softmax',\n                                    nthread=-1,\n                                    scale_pos_weight=1,\n                                    reg_alpha = 0,\n                                    reg_lambda = 1,\n                                    seed=42)\n\n        clf_ros_xgb.fit(xtrain, ytrain, eval_set=[(xtrain, ytrain), (xvalid, yvalid)], \n                    early_stopping_rounds=max_rounds, eval_metric='mlogloss', verbose=100)\n\n        predictions = clf_ros_xgb.predict(xvalid)\n        predictions_probas = clf_ros_xgb.predict_proba(xvalid)\n\n        print()\n        print(classification_report(yvalid, predictions))\n\n        print()\n        print(\"f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n\n        print()\n        print(\"elapsed time in seconds: \", time.time() - start_time)\n        \n        skplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n        \n        skplt.metrics.plot_roc(yvalid, predictions_probas)\n        \n        skplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n        \n        xgb.plot_importance(clf_ros_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n\n        print()\n        #gc.collect()\n        return clf_ros_xgb, predictions, predictions_probas\n    \n    elif type_of_training == \"smote\":\n        #### resampling techniques:\n        from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n\n        # create a 70/30 split of the data \n        xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n\n        # SMOTE\n        smote = SMOTE(random_state=42)\n        X_resampled, y_resampled = smote.fit_resample(xtrain, ytrain)\n        xtrain=pd.DataFrame(X_resampled, columns = X.columns)\n        ytrain = y_resampled\n\n        start_time = time.time()\n\n        clf_smote_xgb = xgb.XGBClassifier(learning_rate=eta, \n                                    n_estimators=estimators, \n                                    max_depth=depth,\n                                    min_child_weight=1,\n                                    gamma=gamma_value,\n                                    subsample=1,\n                                    colsample_bytree=colsample_bytree_value,\n                                    objective= 'multi:softmax',\n                                    nthread=-1,\n                                    scale_pos_weight=1,\n                                    reg_alpha = 0,\n                                    reg_lambda = 1,\n                                    seed=42)\n\n        clf_smote_xgb.fit(xtrain, ytrain, eval_set=[(xtrain, ytrain), (xvalid, yvalid)], \n                    early_stopping_rounds=max_rounds, eval_metric='mlogloss', verbose=100)\n\n        predictions = clf_smote_xgb.predict(xvalid)\n        predictions_probas = clf_smote_xgb.predict_proba(xvalid)\n\n        print()\n        print(classification_report(yvalid, predictions))\n\n        print()\n        print(\"f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n\n        print()\n        print(\"elapsed time in seconds: \", time.time() - start_time)\n        \n        skplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n        \n        skplt.metrics.plot_roc(yvalid, predictions_probas)\n        \n        skplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n        \n        xgb.plot_importance(clf_smote_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n\n        print()\n        #gc.collect()\n        return clf_smote_xgb, predictions, predictions_probas\n    \n    else:\n        print(\"Please specify for the argument 'type_of_training'one of the following parameters: (as-is,stratified, oversampling, smote)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5e0e6406317359101b4e11d854528ea06d4d5b4","scrolled":true},"cell_type":"code","source":"clf_xgb, predictions, predictions_probas = xgbooft_all_purpose(X,y, type_of_training =\"baseline\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41e6485cb4f7a5bb0705bfac2d22aa0249dcda8c"},"cell_type":"code","source":"clf_strat_xgb, predictions, predictions_probas = xgbooft_all_purpose(X,y, type_of_training =\"stratified\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"633445e583ec6eaabf8cbfb9e2e3a4a0eb659ee1"},"cell_type":"code","source":"clf_ros_xgb, predictions, predictions_probas = xgbooft_all_purpose(X,y, type_of_training =\"oversampling\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ae732411b1f9504b4813d8b80d99aeb9ea4caad"},"cell_type":"code","source":"clf_smote_xgb, predictions, predictions_probas = xgbooft_all_purpose(X,y, type_of_training =\"smote\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65991f730504b101666e2e3b3b072933b5a26d03"},"cell_type":"markdown","source":"## Making Predictions for the Test test\n#### so far stratified folds presented great performance in this multiclassification problem"},{"metadata":{"trusted":true,"_uuid":"d66c801d57a715aa2c4e877c8c7bf80277ca1de6"},"cell_type":"code","source":"predictions = clf_xgb.predict(test_df)\n                   \nsubmission = pd.read_csv('../input/test/sample_submission.csv')\nsubmission['AdoptionSpeed'] = [int(i) for i in predictions]\n\n\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}