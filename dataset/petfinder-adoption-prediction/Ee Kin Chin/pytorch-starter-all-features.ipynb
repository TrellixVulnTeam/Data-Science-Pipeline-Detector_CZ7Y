{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# standard imports\nimport time\nimport random\nimport os\nfrom IPython.display import display\nimport numpy as np\nimport pandas as pd\nimport gc\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nimport glob\n\n# pytorch imports\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.utils.checkpoint as checkpoint\n\n# imports for preprocessing the questions\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport re\nimport unidecode\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet, stopwords\n\n# cross validation and metrics\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import StandardScaler\n\n# progress bars\nfrom tqdm import tqdm\ntqdm.pandas()\n\nfrom contextlib import contextmanager\nimport time\n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print(f'[{name}] done in {time.time() - t0:.0f} s')\n    \n\nFILE_DIR = '../input/petfinder-adoption-prediction'\n    \nTRAIN_PATH = f'{FILE_DIR}/train/train.csv'\nTEST_PATH = f'{FILE_DIR}/test/test.csv'\nSAMPLE_SUBMISSION_PATH = f'{FILE_DIR}/test/sample_submission.csv'\n\nmaxlen = 50\nmax_namelen = 5\n\nmax_features = 95000\nbatch_size = 512\nseed = 1018\n\ndef seed_torch(seed=seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_torch()\n\ntrain_df = pd.read_csv(TRAIN_PATH)\ntest_df = pd.read_csv(TEST_PATH)\ny = train_df['AdoptionSpeed'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"267b360bc4f0961736ad99956b8980fb1696e496"},"cell_type":"code","source":"breed_labels = pd.read_csv(f'{FILE_DIR}/breed_labels.csv')\ncolor_labels = pd.read_csv(f'{FILE_DIR}/color_labels.csv')\nstate_labels = pd.read_csv(f'{FILE_DIR}/state_labels.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1a5b7e2bf045071bf55f11c2643d6e868e561b2"},"cell_type":"code","source":"\nbreed_labels['Breed1'] = breed_labels['BreedID']\nbreed_labels['Breed2'] = breed_labels['BreedID']\ncolor_labels['Color1'] = color_labels['ColorID']\ncolor_labels['Color2'] = color_labels['ColorID']\ncolor_labels['Color3'] = color_labels['ColorID']\nstate_labels['State'] = state_labels['StateID']\ndef get_data(df):\n\n    df = df.merge(breed_labels[['Breed1','Type']], on='Breed1', how='left').rename(columns={\"Type\": \"breed_one_catdog\"})\n    df = df.merge(breed_labels[['Breed2','Type']], on='Breed2',how='left').rename(columns={\"Type\": \"breed_two_catdog\"})\n    df = df.merge(color_labels[['Color1','ColorName']], on='Color1', how='left').rename(columns={\"ColorName\": \"color1_name\"})\n    df = df.merge(color_labels[['Color2','ColorName']], on='Color2', how='left').rename(columns={\"ColorName\": \"color2_name\"})\n    df = df.merge(color_labels[['Color3','ColorName']], on='Color3', how='left').rename(columns={\"ColorName\": \"color3_name\"})    \n    df = df.merge(state_labels[['State','StateName']], on='State', how='left')\n    return df\ntrain_df = get_data(train_df)\ntest_df = get_data(test_df)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def preprocess_text(text):\n    punct = [ '\"', ')', '(', '-', '|', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n        '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾']\n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    return text\ntrain_df['Description'] = train_df['Description'].astype(str)\ntest_df['Description'] = test_df['Description'].astype(str)\ntrain_df['Description'] = train_df['Description'].progress_apply(preprocess_text)\ntest_df['Description'] = test_df['Description'].progress_apply(preprocess_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ceae1c95ecce1931659f8220df6faf2d01452ed"},"cell_type":"code","source":"train_df['Name'] = train_df['Name'].astype(str)\ntest_df['Name'] = test_df['Name'].astype(str)\ntrain_df['Name'] = train_df['Name'].progress_apply(preprocess_text)\ntest_df['Name'] = test_df['Name'].progress_apply(preprocess_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ead90d68d5d7870dfac3d916a3616e34cfe426c"},"cell_type":"code","source":"tknzr = Tokenizer(num_words=max_features, lower=False)\ntknzr.fit_on_texts(pd.concat([\n    train_df['Description'], \n    train_df['Name'], \n    test_df['Description'], \n    test_df['Name'], \n]).values)\ntr_desc_seq = tknzr.texts_to_sequences(train_df['Description'].values)\nte_desc_seq = tknzr.texts_to_sequences(test_df['Description'].values)\ntr_name_seq = tknzr.texts_to_sequences(train_df['Name'].values)\nte_name_seq = tknzr.texts_to_sequences(test_df['Name'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a0080bad611cd3279d9517cd7e55296b06738f5"},"cell_type":"code","source":"tr_desc_pad = pad_sequences(tr_desc_seq, maxlen=maxlen)\nte_desc_pad = pad_sequences(te_desc_seq, maxlen=maxlen)\ntr_name_pad = pad_sequences(tr_name_seq, maxlen=max_namelen)\nte_name_pad = pad_sequences(te_name_seq, maxlen=max_namelen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1d8b00baf6419c824b1dda915d9a38fdd240f87"},"cell_type":"code","source":"categoricals = ['Breed1','Breed2','Gender','Color1','Color2','Color3','State','Vaccinated','MaturitySize','Dewormed','Health','RescuerID',\n               'Sterilized','Type_y','Type_x','FurLength']\nfor cat in tqdm(categoricals):\n    dtype = train_df[cat].dtype\n    print(train_df[cat].dtype, cat)\n    if dtype == 'int64':\n        train_df[cat] = train_df[cat].fillna(-1)\n        test_df[cat] = test_df[cat].fillna(-1)\n    elif dtype == 'float64':\n        train_df[cat] = train_df[cat].fillna(-1.0)\n        test_df[cat] = test_df[cat].fillna(-1.0)\n    else:\n        train_df[cat] = train_df[cat].fillna('unknown')\n        test_df[cat] = test_df[cat].fillna('unknown')\n    le = preprocessing.LabelEncoder()\n    le.fit(np.concatenate([train_df[cat].values,test_df[cat].values]))\n    train_df[cat] = le.transform(train_df[cat])\n    test_df[cat] = le.transform(test_df[cat])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efd0063142ae89318afcf4cc671a9699e141abdd"},"cell_type":"code","source":"def to_categorical_idx(col, df_trn, df_test):\n    merged = pd.concat([df_trn[col], df_test[col]])\n    train_size = df_trn[col].shape[0]\n    idxs, uniques = pd.factorize(merged)\n    return idxs[:train_size], idxs[train_size:], uniques\n\ntr_breed1, te_breed1, tknzr_breed1    = to_categorical_idx('Breed1', train_df, test_df)\ntr_breed2, te_breed2, tknzr_breed2    = to_categorical_idx('Breed2', train_df, test_df)\ntr_gen, te_gen, tknzr_gen   = to_categorical_idx('Gender', train_df, test_df)\ntr_col1, te_col1, tknzr_col1   = to_categorical_idx('Color1', train_df, test_df)\ntr_col2, te_col2, tknzr_col2   = to_categorical_idx('Color2', train_df, test_df)\ntr_col3, te_col3, tknzr_col3   = to_categorical_idx('Color3', train_df, test_df)\ntr_state, te_state, tknzr_state  = to_categorical_idx('State', train_df, test_df)\ntr_vac, te_vac, tknzr_vac  = to_categorical_idx('Vaccinated', train_df, test_df)\ntr_msize, te_msize, tknzr_msize  = to_categorical_idx('MaturitySize', train_df, test_df)\ntr_dworm, te_dworm, tknzr_dworm  = to_categorical_idx('Dewormed', train_df, test_df)\ntr_health, te_health, tknzr_health  = to_categorical_idx('Health', train_df, test_df)\ntr_rid, te_rid, tknzr_rid = to_categorical_idx('RescuerID', train_df, test_df)\ntr_ster, te_ster, tknzr_ster = to_categorical_idx('Sterilized', train_df, test_df)\ntr_ty, te_ty, tknzr_ty= to_categorical_idx('Type_y', train_df, test_df)\ntr_tx, te_tx, tknzr_tx= to_categorical_idx('Type_x', train_df, test_df)\ntr_fl, te_fl, tknzr_fl= to_categorical_idx('FurLength', train_df, test_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98d94ccb2b6f3c3ebc4d51c8875b8315ec42026e"},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\n            \ndef load_fasttext(word_index):\n    EMBEDDING_FILE = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in tqdm(open(EMBEDDING_FILE)))\n    return embeddings_index \n\ndef load_glove(word_index):\n    EMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if o.split(\" \")[0] in word_index)\n    return embeddings_index ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af8d37a827562353bcc02453781d512011c24ae7"},"cell_type":"code","source":"embeddings_index_1 = load_fasttext(tknzr.word_index)\nembeddings_index_2 = load_glove(tknzr.word_index)\n\nall_embs = np.stack(embeddings_index_2.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tknzr.word_index\nnb_words = min(max_features, len(word_index)+1)\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, 300))\nfor word, i in word_index.items():\n    if i >= nb_words: continue\n    embedding_vector = embeddings_index_2.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        \nmax_emb = 64\n\nemb_breed1   = min(max_emb,(len(tknzr_breed1) + 1)//2)\nemb_breed2   = min(max_emb,(len(tknzr_breed2) + 1)//2)\nemb_gen   = min(max_emb,(len(tknzr_gen) + 1)//2)\nemb_col1   = min(max_emb,(len(tknzr_col1) + 1)//2)\nemb_col2   = min(max_emb,(len(tknzr_col2) + 1)//2)\nemb_col3   = min(max_emb,(len(tknzr_col3) + 1)//2)\nemb_state   = min(max_emb,(len(tknzr_state) + 1)//2)\nemb_vac   = min(max_emb,(len(tknzr_vac) + 1)//2)\nemb_msize   = min(max_emb,(len(tknzr_msize) + 1)//2)\nemb_dworm   = min(max_emb,(len(tknzr_dworm) + 1)//2)\nemb_health   = min(max_emb,(len(tknzr_health) + 1)//2)\nemb_rid = min(max_emb,(len(tknzr_rid) + 1)//2)\nemb_ster   = min(max_emb,(len(tknzr_ster) + 1)//2)\nemb_ty   = min(max_emb,len(tknzr_ty))\nemb_tx   = min(max_emb,len(tknzr_tx))\nemb_fl   = min(max_emb,(len(tknzr_fl) + 1)//2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"206836c93524785e9226194d9bee3e12a5412dc9"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26591a7d107c16ff24e1df6364be7b09dfb44b52"},"cell_type":"code","source":"train_id = train_df['PetID']\ntest_id = test_df['PetID']\n\ndoc_sent_mag = []\ndoc_sent_score = []\nnf_count = 0\nfor pet in train_id:\n    try:\n        with open('../input/train_sentiment/' + pet + '.json', 'r') as f:\n            sentiment = json.load(f)\n        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n        doc_sent_score.append(sentiment['documentSentiment']['score'])\n    except FileNotFoundError:\n        nf_count += 1\n        doc_sent_mag.append(-1)\n        doc_sent_score.append(-1)\n\ntrain_df.loc[:, 'doc_sent_mag'] = doc_sent_mag\ntrain_df.loc[:, 'doc_sent_score'] = doc_sent_score\n\ndoc_sent_mag = []\ndoc_sent_score = []\nnf_count = 0\nfor pet in test_id:\n    try:\n        with open('../input/test_sentiment/' + pet + '.json', 'r') as f:\n            sentiment = json.load(f)\n        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n        doc_sent_score.append(sentiment['documentSentiment']['score'])\n    except FileNotFoundError:\n        nf_count += 1\n        doc_sent_mag.append(-1)\n        doc_sent_score.append(-1)\n\ntest_df.loc[:, 'doc_sent_mag'] = doc_sent_mag\ntest_df.loc[:, 'doc_sent_score'] = doc_sent_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0c5f784595ba49b55e9dac92f97a6a225493df4"},"cell_type":"code","source":"vertex_xs = []\nvertex_ys = []\nbounding_confidences = []\nbounding_importance_fracs = []\ndominant_blues = []\ndominant_greens = []\ndominant_reds = []\ndominant_pixel_fracs = []\ndominant_scores = []\nlabel_descriptions = []\nlabel_scores = []\nnf_count = 0\nnl_count = 0\nfor pet in train_id:\n    try:\n        with open('../input/train_metadata/' + pet + '-1.json', 'r') as f:\n            data = json.load(f)\n        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n        vertex_xs.append(vertex_x)\n        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n        vertex_ys.append(vertex_y)\n        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n        bounding_confidences.append(bounding_confidence)\n        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n        bounding_importance_fracs.append(bounding_importance_frac)\n        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n        dominant_blues.append(dominant_blue)\n        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n        dominant_greens.append(dominant_green)\n        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n        dominant_reds.append(dominant_red)\n        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n        dominant_pixel_fracs.append(dominant_pixel_frac)\n        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n        dominant_scores.append(dominant_score)\n        if data.get('labelAnnotations'):\n            label_description = data['labelAnnotations'][0]['description']\n            label_descriptions.append(label_description)\n            label_score = data['labelAnnotations'][0]['score']\n            label_scores.append(label_score)\n        else:\n            nl_count += 1\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n    except FileNotFoundError:\n        nf_count += 1\n        vertex_xs.append(-1)\n        vertex_ys.append(-1)\n        bounding_confidences.append(-1)\n        bounding_importance_fracs.append(-1)\n        dominant_blues.append(-1)\n        dominant_greens.append(-1)\n        dominant_reds.append(-1)\n        dominant_pixel_fracs.append(-1)\n        dominant_scores.append(-1)\n        label_descriptions.append('nothing')\n        label_scores.append(-1)\n\nprint(nf_count)\nprint(nl_count)\ntrain_df.loc[:, 'vertex_x'] = vertex_xs\ntrain_df.loc[:, 'vertex_y'] = vertex_ys\ntrain_df.loc[:, 'bounding_confidence'] = bounding_confidences\ntrain_df.loc[:, 'bounding_importance'] = bounding_importance_fracs\ntrain_df.loc[:, 'dominant_blue'] = dominant_blues\ntrain_df.loc[:, 'dominant_green'] = dominant_greens\ntrain_df.loc[:, 'dominant_red'] = dominant_reds\ntrain_df.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\ntrain_df.loc[:, 'dominant_score'] = dominant_scores\ntrain_df.loc[:, 'label_description'] = label_descriptions\ntrain_df.loc[:, 'label_score'] = label_scores\n\n\nvertex_xs = []\nvertex_ys = []\nbounding_confidences = []\nbounding_importance_fracs = []\ndominant_blues = []\ndominant_greens = []\ndominant_reds = []\ndominant_pixel_fracs = []\ndominant_scores = []\nlabel_descriptions = []\nlabel_scores = []\nnf_count = 0\nnl_count = 0\nfor pet in test_id:\n    try:\n        with open('../input/test_metadata/' + pet + '-1.json', 'r') as f:\n            data = json.load(f)\n        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n        vertex_xs.append(vertex_x)\n        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n        vertex_ys.append(vertex_y)\n        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n        bounding_confidences.append(bounding_confidence)\n        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n        bounding_importance_fracs.append(bounding_importance_frac)\n        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n        dominant_blues.append(dominant_blue)\n        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n        dominant_greens.append(dominant_green)\n        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n        dominant_reds.append(dominant_red)\n        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n        dominant_pixel_fracs.append(dominant_pixel_frac)\n        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n        dominant_scores.append(dominant_score)\n        if data.get('labelAnnotations'):\n            label_description = data['labelAnnotations'][0]['description']\n            label_descriptions.append(label_description)\n            label_score = data['labelAnnotations'][0]['score']\n            label_scores.append(label_score)\n        else:\n            nl_count += 1\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n    except FileNotFoundError:\n        nf_count += 1\n        vertex_xs.append(-1)\n        vertex_ys.append(-1)\n        bounding_confidences.append(-1)\n        bounding_importance_fracs.append(-1)\n        dominant_blues.append(-1)\n        dominant_greens.append(-1)\n        dominant_reds.append(-1)\n        dominant_pixel_fracs.append(-1)\n        dominant_scores.append(-1)\n        label_descriptions.append('nothing')\n        label_scores.append(-1)\n\nprint(nf_count)\ntest_df.loc[:, 'vertex_x'] = vertex_xs\ntest_df.loc[:, 'vertex_y'] = vertex_ys\ntest_df.loc[:, 'bounding_confidence'] = bounding_confidences\ntest_df.loc[:, 'bounding_importance'] = bounding_importance_fracs\ntest_df.loc[:, 'dominant_blue'] = dominant_blues\ntest_df.loc[:, 'dominant_green'] = dominant_greens\ntest_df.loc[:, 'dominant_red'] = dominant_reds\ntest_df.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\ntest_df.loc[:, 'dominant_score'] = dominant_scores\ntest_df.loc[:, 'label_description'] = label_descriptions\ntest_df.loc[:, 'label_score'] = label_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"412f734b31ba79694ad2eecc69026bfd4023031f"},"cell_type":"code","source":"train_image_path = '../input/petfinder-adoption-prediction/train_images'\ntest_image_path = '../input/petfinder-adoption-prediction/test_images'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2abc11f9ec889e7c6f07390586e1ba00c5ec0c2b"},"cell_type":"code","source":"from PIL import Image\nfrom torch.utils import data as D\n\nclass PureImageDataset(D.Dataset):\n    \"\"\"\n    A customized data loader.\n    \"\"\"\n    def __init__(self, path, pet_id):\n        \"\"\" Intialize the dataset\n        \"\"\"\n        self.filenames = []\n       \n        self.transform = transforms.ToTensor()\n    \n        filenames = glob.glob(os.path.join(path, '*.jpg'))\n        tmp_filenames = []\n        for image_path in filenames:\n            tmp_filenames.append(image_path.split('/')[-1].split('-')[0])\n        for pid in pet_id:\n            try:\n                self.filenames.append(filenames[tmp_filenames.index(pid)])\n            except:\n                self.filenames.append(np.nan)\n        self.len = len(self.filenames)\n        \n    # You must override __getitem__ and __len__\n    def __getitem__(self, index):\n        \"\"\" Get a sample from the dataset\n        \"\"\"\n        try:\n            image = Image.open(self.filenames[index])\n            image = image.resize((224,224))\n            images = np.transpose(image, (2, 0, 1))            \n            images = image_to_tensor_transform(images)\n        except:\n            image = Image.new('RGB', (224,224))\n            image = np.asarray(image)\n            images = np.transpose(image, (2, 0, 1))     \n            images = image_to_tensor_transform(images)\n       \n        return images\n    def __len__(self):\n        \"\"\"\n        Total number of samples in the dataset\n        \"\"\"\n        return self.len","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55fb598ff17767cee7ead201c23af8b8f7fd9e77"},"cell_type":"code","source":"numerical_feats = ['Quantity','FurLength','Age','Quantity','Fee','VideoAmt','PhotoAmt','doc_sent_mag', 'doc_sent_score', 'dominant_score', \n                   'dominant_pixel_frac', 'dominant_red', 'dominant_green', 'dominant_blue', 'bounding_importance', 'bounding_confidence',\n                   'vertex_x', 'vertex_y', 'label_score']\ntrain_features = train_df[numerical_feats].fillna(0).values\ntest_features = test_df[numerical_feats].fillna(0).values\nss = StandardScaler()\nss.fit(np.vstack((train_features, test_features)))\ntrain_features = ss.transform(train_features)\ntest_features = ss.transform(test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c9794cde37bf9c4a786d7b92a106be13b4dc770"},"cell_type":"code","source":"from torchvision import transforms, datasets, models\nfrom tqdm import tqdm_notebook as tqdm\n\nclass Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n    \ncnn_model =  models.resnet50(pretrained=False)\ncnn_model.load_state_dict(torch.load('../input/pretrained-pytorch-models/resnet50-19c8e357.pth', map_location=lambda storage, loc: storage))\nnum_ftrs = cnn_model.fc.in_features\ncnn_model.fc = torch.nn.Linear(num_ftrs, int(num_ftrs/2))\ncnn_model.cuda()\n\ncnn_model2 =  models.vgg16(pretrained=False)\ncnn_model2.load_state_dict(torch.load('../input/vgg16/vgg16.pth', map_location=lambda storage, loc: storage))\ncnn_model2.cuda()\n\n\nlayer = cnn_model._modules.get('layer3')\nlayer2 = cnn_model2._modules.get('features')\n\ndef image_to_tensor_transform(image):\n    mean=[0.5, 0.5, 0.5]\n    std =[0.5, 0.5, 0.5]\n    tensor = torch.from_numpy(image).float().div(255)\n    tensor[0] = (tensor[0] - mean[0]) / std[0]\n    tensor[1] = (tensor[1] - mean[1]) / std[1]\n    tensor[2] = (tensor[2] - mean[2]) / std[2]\n    return tensor.cuda()\nbatch_size = 50\ntrain = PureImageDataset(train_image_path,train_df['PetID'].values)\ntrain_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\ntrain_imgnet = np.zeros((len(train_df),1024+512))\n#train_imgnet = np.zeros((len(train_df), 1024, 14, 14))\navgpool = nn.AvgPool2d(14)\navgpool2 = nn.AvgPool2d(7)\nflattener = Flatten()\nlast = len(train_df)//batch_size\nfor i, (x_img) in tqdm(enumerate(train_loader),total=len(train_df)//batch_size):\n\n    my_embedding = torch.zeros((x_img.shape[0], 1024, 14, 14))\n    my_embedding2 = torch.zeros((x_img.shape[0], 512, 7, 7))\n    def copy_data(m, i, o):\n        my_embedding.copy_(o.data)\n    def copy_data2(m, i, o):\n        my_embedding2.copy_(o.data)\n    h = layer.register_forward_hook(copy_data)\n    h2 = layer2.register_forward_hook(copy_data2)\n    y_pred = cnn_model(x_img).detach().cpu().numpy()\n    y_pred2 = cnn_model2(x_img).detach().cpu().numpy()\n    h.remove()\n    h2.remove()\n    my_embedding = flattener(avgpool(my_embedding)).detach().cpu().numpy()\n    my_embedding2 = flattener(avgpool2(my_embedding2)).detach().cpu().numpy()\n    train_imgnet[i * batch_size:(i+1) * batch_size, :1024] = my_embedding\n    train_imgnet[i * batch_size:(i+1) * batch_size, 1024:] = my_embedding2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80294d8adac6491028573f8a0c0cc5efa058c004"},"cell_type":"code","source":"test = PureImageDataset(test_image_path,test_df['PetID'].values)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True)\ntest_imgnet = np.zeros((len(test_df),1024+512))\nfor i, (x_img) in tqdm(enumerate(test_loader), total=len(test_df)//batch_size):\n    my_embedding = torch.zeros((x_img.shape[0], 1024, 14, 14))\n    my_embedding2 = torch.zeros((x_img.shape[0], 512, 7, 7))\n    def copy_data(m, i, o):\n        my_embedding.copy_(o.data)\n    def copy_data2(m, i, o):\n        my_embedding2.copy_(o.data)\n    h = layer.register_forward_hook(copy_data)\n    h2 = layer2.register_forward_hook(copy_data2)\n    y_pred = cnn_model(x_img).detach().cpu().numpy()\n    y_pred2 = cnn_model2(x_img).detach().cpu().numpy()\n    h.remove()\n    h2.remove()\n    my_embedding = flattener(avgpool(my_embedding)).detach().cpu().numpy()\n    my_embedding2 = flattener(avgpool2(my_embedding2)).detach().cpu().numpy()\n    test_imgnet[i * batch_size:(i+1) * batch_size, :1024] = my_embedding\n    test_imgnet[i * batch_size:(i+1) * batch_size, 1024:] = my_embedding2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c50a450ffb84dd752f036e9b6530c114799a18f"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ee05833912c39bf482ea7410588ae1f77629a83"},"cell_type":"code","source":"from torchvision import transforms, datasets, models\n\n# The following 3 functions have been taken from Ben Hamner's github repository\n# https://github.com/benhamner/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              / num_scored_items)\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n\n    return (1.0 - numerator / denominator)\n\nclass NeuralNet(nn.Module):\n    def __init__(self):\n        super(NeuralNet, self).__init__()\n        \n        self.cnn_model =  nn.Linear(1024+512, 500)\n        self.cnn_model2 =  nn.Linear(500, 500)\n        self.batchnorm_img = nn.BatchNorm1d(500)\n        \n        self.embedding = nn.Embedding(max_features, embedding_matrix.shape[1])\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        \n        self.emb_breed1   = nn.Embedding(len(tknzr_breed1), emb_breed1)\n        self.linear_breed1 = nn.Linear(64, 10)\n        self.emb_breed2   = nn.Embedding(len(tknzr_breed2), emb_breed2)\n        self.linear_breed2 = nn.Linear(64, 10)\n        self.emb_gen   = nn.Embedding(len(tknzr_gen), emb_gen)\n        self.linear_gen = nn.Linear(2, 10)\n        self.emb_col1   = nn.Embedding(len(tknzr_col1), emb_col1)\n        self.linear_col1 = nn.Linear(4, 10)\n        self.emb_col2   = nn.Embedding(len(tknzr_col2), emb_col2)\n        self.linear_col2 = nn.Linear(4, 10)\n        self.emb_col3   = nn.Embedding(len(tknzr_col3), emb_col3)\n        self.linear_col3 = nn.Linear(3, 10)\n        self.emb_state   = nn.Embedding(len(tknzr_state), emb_state)\n        self.linear_state = nn.Linear(7, 10)\n        self.emb_flatten = Flatten()\n        self.emb_vac = nn.Embedding(len(tknzr_vac), emb_vac)\n        self.linear_vac = nn.Linear(2, 2)\n        self.emb_msize = nn.Embedding(len(tknzr_msize), emb_msize)\n        self.linear_msize = nn.Linear(2, 2)\n        self.emb_dworm = nn.Embedding(len(tknzr_dworm), emb_dworm)\n        self.linear_dworm = nn.Linear(2, 2)\n        self.emb_health = nn.Embedding(len(tknzr_health), emb_health)\n        self.linear_health = nn.Linear(2, 2)\n        self.emb_rid = nn.Embedding(len(tknzr_rid), emb_rid)\n        self.linear_rid = nn.Linear(64, 100)\n        self.emb_ster = nn.Embedding(len(tknzr_ster), emb_ster)\n        self.linear_ster = nn.Linear(2, 2)\n        self.emb_ty = nn.Embedding(len(tknzr_ty), emb_ty)\n        self.linear_ty = nn.Linear(3, 5)\n        self.emb_tx = nn.Embedding(len(tknzr_tx), emb_tx)\n        self.linear_tx = nn.Linear(2, 2)\n        self.emb_fl = nn.Embedding(len(tknzr_fl), emb_fl)\n        self.linear_fl = nn.Linear(2, 2)\n    \n        self.embedding_dropout = nn.Dropout2d(0.1)\n        \n        self.lstm = nn.LSTM(embedding_matrix.shape[1], 256, bidirectional=True, batch_first=True)\n        self.gru = nn.GRU(512, 256, bidirectional=True, batch_first=True)\n        self.lstm_name = nn.LSTM(embedding_matrix.shape[1], 20, bidirectional=True, batch_first=True)\n        self.gru_name = nn.GRU(40, 20, bidirectional=True, batch_first=True)\n        self.dense_rnn = nn.Linear(512+40, 256)\n        self.batchnorm_rnn = nn.BatchNorm1d(256)\n        self.dropout_rnn = nn.Dropout(0.2)\n        \n        self.linear_num = nn.Linear(19, 5000)\n        self.linear_cat = nn.Linear(184, 5000)\n        self.batchnorm_cat = nn.BatchNorm1d(5000)\n        self.batchnorm_num = nn.BatchNorm1d(5000)\n        \n        \n        self.linear_one = nn.Linear(10756, 500)\n        self.batchnorm = nn.BatchNorm1d(500)\n        self.linear_two = nn.Linear(500, 200)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.3)\n        self.out = nn.Linear(200, num_classes)\n    \n    def forward(self, x_img, x, x_name, breed1, breed2, gen, col1, col2, col3, state,vac,msize,dworm, health,rid,ster,ty,tx,fl, numerical):\n        \n        img_feat = self.cnn_model(x_img)\n        img_feat = self.dropout(img_feat)\n        img_feat = self.cnn_model2(img_feat)\n        img_feat = self.dropout(img_feat)\n        #img_feat = self.batchnorm_img(img_feat)\n        \n        h_embedding = self.embedding(x)\n        h_embedding = torch.squeeze(\n            self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n        h2_embedding = self.embedding(x_name)\n        h2_embedding = torch.squeeze(\n            self.embedding_dropout(torch.unsqueeze(h2_embedding, 0)))\n        \n            \n        f1 = self.emb_breed1(breed1)\n        f1 = self.linear_breed1(f1)\n        f2 = self.emb_breed2(breed2)\n        f2 = self.linear_breed2(f2)\n        f3 = self.emb_gen(gen)\n        f3 = self.linear_gen(f3)\n        f4 = self.emb_col1(col1)\n        f4 = self.linear_col1(f4)\n        f5 = self.emb_col2(col2)\n        f5 = self.linear_col2(f5)  \n        f6 = self.emb_col3(col3)\n        f6 = self.linear_col3(f6)\n        f7 = self.emb_state(state)\n        f7 = self.linear_state(f7)\n        f8 = self.emb_vac(vac)\n        f8 = self.linear_vac(f8)\n        f9 = self.emb_msize(msize)\n        f9 = self.linear_msize(f9)\n        f10 = self.emb_dworm(dworm)\n        f10 = self.linear_dworm(f10)\n        f11 = self.emb_health(health)\n        f11 = self.linear_health(f11)\n        f12 = self.emb_rid(rid)\n        f12 = self.linear_rid(f12)\n        f13 = self.emb_ster(ster)\n        f13 = self.linear_ster(f13)\n        f14 = self.emb_ty(ty)\n        f14 = self.linear_ty(f14)\n        f15 = self.emb_tx(tx)\n        f15 = self.linear_tx(f15)\n        f16 = self.emb_fl(fl)\n        f16 = self.linear_fl(f16)\n        cat_conc = torch.cat((f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f15, f16), -1)\n        #cat_conc = self.emb_flatten(cat_conc)\n        cat_conc = self.linear_cat(cat_conc)\n        #cat_conc = self.batchnorm_cat(cat_conc)\n        cat_conc = self.dropout(cat_conc)\n        \n        h_lstm, _ = self.lstm(h_embedding)\n        h_gru, (ht, ct) = self.gru(h_lstm)\n        \n        \n        h2_lstm, _ = self.lstm_name(h2_embedding)\n        h2_gru, (ht2, ct2) = self.gru_name(h2_lstm)\n        \n        # global average pooling\n        #avg_pool = torch.mean(h_gru, 1)\n        #avg_pool2 = torch.mean(h2_gru, 1)\n        # global max pooling\n        #max_pool, _ = torch.max(h_gru, 1)\n        #max_pool2, _ = torch.max(h2_gru, 1)\n        # last state\n        last_state = torch.cat([ht, ct], 1)\n        last_state2 = torch.cat([ht2, ct2], 1)\n        final_state = torch.cat([last_state, last_state2], 1)\n        final_state = self.dense_rnn(final_state)\n        #final_state = self.batchnorm_rnn(final_state)\n        final_state = self.dropout_rnn(final_state)\n        \n        \n        numerical = self.linear_num(numerical)\n        #numerical = self.batchnorm_num(numerical)\n        numerical = self.dropout(numerical)\n        \n        conc = torch.cat((numerical, cat_conc, final_state, img_feat), 1)\n        #conc = self.batchnorm(conc)\n        conc = self.relu(self.linear_one(conc))\n        #conc = self.batchnorm(conc)\n        conc = self.dropout(conc)\n        conc = self.relu(self.linear_two(conc))\n        conc = self.dropout(conc)\n        out = self.out(conc)\n        \n        return out\n    \ndef save_checkpoint(checkpoint_path, model, optimizer):\n    state = {'state_dict': model.state_dict(),\n             'optimizer' : optimizer.state_dict()}\n    torch.save(state, checkpoint_path)\n    print('model saved to %s' % checkpoint_path)\n    \ndef load_checkpoint(checkpoint_path, model, optimizer):\n    state = torch.load(checkpoint_path)\n    model.load_state_dict(state['state_dict'])\n    optimizer.load_state_dict(state['optimizer'])\n    print('model loaded from %s' % checkpoint_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ce5b06152258502857427dbf9f588f866fef7e7"},"cell_type":"code","source":"num_classes = len(np.unique(y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68cfa6958e75ab80e2f300e604e6c6034c5d9d59"},"cell_type":"code","source":"batch_size = 516\nn_folds=5\nepochs=30\ntrain_preds = np.zeros((len(train_df),num_classes))\ntest_preds = np.zeros((len(test_df),num_classes))\n\n# correct solution:\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0) # only difference\n\n\n#test_pet_img = PetImageDataset(test_image_path,test_df['PetID'].values, aug='False')\nx_img_test_cuda = torch.tensor(test_imgnet, dtype=torch.float32).cuda()\nx_test_cuda = torch.tensor(te_desc_pad, dtype=torch.long).cuda()\nx_test_name_cuda = torch.tensor(te_name_pad, dtype=torch.long).cuda()\nte_breed1_cuda = torch.tensor(te_breed1, dtype=torch.long).cuda()\nte_breed2_cuda = torch.tensor(te_breed2, dtype=torch.long).cuda()\nte_gen_cuda = torch.tensor(te_gen, dtype=torch.long).cuda()\nte_col1_cuda = torch.tensor(te_col1, dtype=torch.long).cuda()\nte_col2_cuda = torch.tensor(te_col2, dtype=torch.long).cuda()\nte_col3_cuda = torch.tensor(te_col3, dtype=torch.long).cuda()\nte_state_cuda = torch.tensor(te_state, dtype=torch.long).cuda()\nte_vac_cuda = torch.tensor(te_vac, dtype=torch.long).cuda()\nte_msize_cuda = torch.tensor(te_msize, dtype=torch.long).cuda()\nte_dworm_cuda = torch.tensor(te_dworm, dtype=torch.long).cuda()\nte_health_cuda = torch.tensor(te_health, dtype=torch.long).cuda()\nte_rid_cuda = torch.tensor(te_rid, dtype=torch.long).cuda()\nte_ster_cuda = torch.tensor(te_ster, dtype=torch.long).cuda()\nte_ty_cuda = torch.tensor(te_ty, dtype=torch.long).cuda()\nte_tx_cuda = torch.tensor(te_tx, dtype=torch.long).cuda()\nte_fl_cuda = torch.tensor(te_fl, dtype=torch.long).cuda()\n\nx_test_feats_cuda = torch.tensor(test_features, dtype=torch.float32).cuda()\n\ntest = torch.utils.data.TensorDataset(x_img_test_cuda, x_test_cuda,x_test_name_cuda, te_breed1_cuda, te_breed2_cuda,\n                                      te_gen_cuda, te_col1_cuda, te_col2_cuda, te_col3_cuda, te_state_cuda,te_vac_cuda,te_msize_cuda,\n                                      te_dworm_cuda,te_health_cuda,te_rid_cuda,te_ster_cuda,te_ty_cuda,te_tx_cuda,te_fl_cuda, x_test_feats_cuda)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n\nsplits = list(StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed).split(tr_desc_pad, y)) # shuffle or no?\n\nfor fold_id, (train_idx, valid_idx) in enumerate(splits):\n    \n    x_img_train_fold = torch.tensor(train_imgnet[train_idx], dtype=torch.float32).cuda()\n    x_train_fold = torch.tensor(tr_desc_pad[train_idx], dtype=torch.long).cuda()\n    x_name_train_fold = torch.tensor(tr_name_pad[train_idx], dtype=torch.long).cuda()\n    tr_breed1_fold = torch.tensor(tr_breed1[train_idx], dtype=torch.long).cuda()\n    tr_breed2_fold = torch.tensor(tr_breed2[train_idx], dtype=torch.long).cuda()\n    tr_gen_fold = torch.tensor(tr_gen[train_idx], dtype=torch.long).cuda()\n    tr_col1_fold = torch.tensor(tr_col1[train_idx], dtype=torch.long).cuda()\n    tr_col2_fold = torch.tensor(tr_col2[train_idx], dtype=torch.long).cuda()\n    tr_col3_fold = torch.tensor(tr_col3[train_idx], dtype=torch.long).cuda()\n    tr_state_fold = torch.tensor(tr_state[train_idx], dtype=torch.long).cuda()\n    tr_vac_fold = torch.tensor(tr_vac[train_idx], dtype=torch.long).cuda()\n    tr_msize_fold = torch.tensor(tr_msize[train_idx], dtype=torch.long).cuda()\n    tr_dworm_fold = torch.tensor(tr_dworm[train_idx], dtype=torch.long).cuda()\n    tr_health_fold = torch.tensor(tr_health[train_idx], dtype=torch.long).cuda()\n    tr_rid_fold = torch.tensor(tr_rid[train_idx], dtype=torch.long).cuda()\n    tr_ster_fold = torch.tensor(tr_ster[train_idx], dtype=torch.long).cuda()\n    tr_ty_fold = torch.tensor(tr_ty[train_idx], dtype=torch.long).cuda()\n    tr_tx_fold = torch.tensor(tr_tx[train_idx], dtype=torch.long).cuda()\n    tr_fl_fold = torch.tensor(tr_fl[train_idx], dtype=torch.long).cuda()\n    x_feats_train_fold = torch.tensor(train_features[train_idx], dtype=torch.float32).cuda()\n    y_train_fold = torch.tensor(y[train_idx], dtype=torch.long).cuda()\n    #train = PetImageDataset(train_image_path,train_df['PetID'].values[train_idx],x_train_fold,x_name_train_fold, tr_breed1_fold, tr_breed2_fold, tr_gen_fold, tr_col1_fold, tr_col2_fold,\n    #                                       tr_col3_fold, tr_state_fold, x_feats_train_fold, y_train_fold, aug='True')\n    \n    x_img_val_fold = torch.tensor(train_imgnet[valid_idx], dtype=torch.float32).cuda()\n    x_val_fold = torch.tensor(tr_desc_pad[valid_idx], dtype=torch.long).cuda()\n    x_name_val_fold = torch.tensor(tr_name_pad[valid_idx], dtype=torch.long).cuda()\n    val_breed1_fold = torch.tensor(tr_breed1[valid_idx], dtype=torch.long).cuda()\n    val_breed2_fold = torch.tensor(tr_breed2[valid_idx], dtype=torch.long).cuda()\n    val_gen_fold = torch.tensor(tr_gen[valid_idx], dtype=torch.long).cuda()\n    val_col1_fold = torch.tensor(tr_col1[valid_idx], dtype=torch.long).cuda()\n    val_col2_fold = torch.tensor(tr_col2[valid_idx], dtype=torch.long).cuda()\n    val_col3_fold = torch.tensor(tr_col3[valid_idx], dtype=torch.long).cuda()\n    val_state_fold = torch.tensor(tr_state[valid_idx], dtype=torch.long).cuda()\n    val_vac_fold = torch.tensor(tr_vac[valid_idx], dtype=torch.long).cuda()\n    val_msize_fold = torch.tensor(tr_msize[valid_idx], dtype=torch.long).cuda()\n    val_dworm_fold = torch.tensor(tr_dworm[valid_idx], dtype=torch.long).cuda()\n    val_health_fold = torch.tensor(tr_health[valid_idx], dtype=torch.long).cuda()\n    val_rid_fold = torch.tensor(tr_rid[valid_idx], dtype=torch.long).cuda()\n    val_ster_fold = torch.tensor(tr_ster[valid_idx], dtype=torch.long).cuda()\n    val_ty_fold = torch.tensor(tr_ty[valid_idx], dtype=torch.long).cuda()\n    val_tx_fold = torch.tensor(tr_tx[valid_idx], dtype=torch.long).cuda()\n    val_fl_fold = torch.tensor(tr_fl[valid_idx], dtype=torch.long).cuda()\n    x_feats_val_fold = torch.tensor(train_features[valid_idx], dtype=torch.float32).cuda()\n    y_val_fold = torch.tensor(y[valid_idx], dtype=torch.long).cuda()\n    #valid = PetImageDataset(train_image_path,train_df['PetID'].values[valid_idx], x_val_fold,x_name_val_fold, val_breed1_fold, val_breed2_fold, val_gen_fold, val_col1_fold, val_col2_fold,\n    #                                       val_col3_fold, val_state_fold, x_feats_val_fold, y_val_fold, aug='False')\n\n    model = NeuralNet()   \n    model.cuda()   \n\n    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n    train = torch.utils.data.TensorDataset(x_img_train_fold, x_train_fold,x_name_train_fold, tr_breed1_fold, tr_breed2_fold, tr_gen_fold, tr_col1_fold, tr_col2_fold,\n                                           tr_col3_fold, tr_state_fold,tr_vac_fold,tr_msize_fold, tr_dworm_fold,tr_health_fold,tr_rid_fold,\n                                           tr_ster_fold,tr_ty_fold,tr_tx_fold,tr_fl_fold,x_feats_train_fold, y_train_fold)\n    valid = torch.utils.data.TensorDataset(x_img_val_fold, x_val_fold,x_name_val_fold, val_breed1_fold, val_breed2_fold, val_gen_fold, val_col1_fold, val_col2_fold,\n                                           val_col3_fold, val_state_fold,val_vac_fold,val_msize_fold,val_dworm_fold,val_health_fold,val_rid_fold,\n                                           val_ster_fold,val_ty_fold,val_tx_fold,val_fl_fold,x_feats_val_fold, y_val_fold)\n\n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n\n    # start training on fold\n    print(f'Fold {fold_id + 1}')\n\n    best_valid_loss = 1e10\n\n    for epoch in range(epochs):\n        start_time = time.time()\n\n        # train\n        model.train()\n        avg_loss = 0.\n        for i, (x_img, x_batch,x_name_batch,tr_breed1_batch, tr_breed2_batch, tr_gen_batch, tr_col1_batch, tr_col2_batch,\n                    tr_col3_batch, tr_state_batch,tr_vac_batch, tr_msize_batch,tr_dworm_batch,tr_health_batch,tr_rid_batch,tr_ster_batch,\n                    tr_ty_batch,tr_tx_batch,tr_fl_batch,x_feats_batch, y_batch) in enumerate(train_loader):\n            optimizer.zero_grad()\n            y_pred = model(x_img, x_batch,x_name_batch,tr_breed1_batch, tr_breed2_batch, tr_gen_batch, tr_col1_batch, tr_col2_batch,\n                    tr_col3_batch, tr_state_batch,tr_vac_batch,tr_msize_batch,tr_dworm_batch,tr_health_batch,tr_rid_batch,tr_ster_batch,\n                    tr_ty_batch,tr_tx_batch,tr_fl_batch,x_feats_batch)\n            loss = loss_fn(y_pred, y_batch)\n            loss.backward()\n            optimizer.step()\n            avg_loss += loss.item() / len(train_idx)\n\n        # evaluate\n        model.eval()\n        valid_preds_fold = np.zeros((x_val_fold.size(0), num_classes))\n        test_preds_fold = np.zeros((len(te_desc_pad),num_classes))\n        avg_val_loss = 0.\n        for i, (x_img, x_batch,x_name_batch,tr_breed1_batch, tr_breed2_batch, tr_gen_batch, tr_col1_batch, tr_col2_batch,\n                    tr_col3_batch, tr_state_batch,tr_vac_batch,tr_msize_batch,tr_dworm_batch,tr_health_batch,tr_rid_batch,tr_ster_batch,\n                    tr_ty_batch,tr_tx_batch,tr_fl_batch,x_feats_batch, y_batch) in enumerate(valid_loader):\n            y_pred = model(x_img, x_batch,x_name_batch,tr_breed1_batch, tr_breed2_batch, tr_gen_batch, tr_col1_batch, tr_col2_batch,\n                        tr_col3_batch, tr_state_batch,tr_vac_batch,tr_msize_batch,tr_dworm_batch,tr_health_batch,tr_rid_batch,tr_ster_batch,\n                        tr_ty_batch,tr_tx_batch,tr_fl_batch, x_feats_batch).detach()\n\n            val_loss = loss_fn(y_pred, y_batch).item() \n            avg_val_loss += val_loss / len(valid_idx)\n            valid_preds_fold[i * batch_size:(i+1) * batch_size] = softmax(y_pred.cpu().numpy())\n        qwk = quadratic_weighted_kappa(y[valid_idx], np.argmax(valid_preds_fold, axis=1))\n        elapsed_time = time.time() - start_time \n        print('quadratic_weighted_kappa: {}'.format(qwk))\n        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n            epoch + 1, epochs, avg_loss, avg_val_loss, elapsed_time))\n\n        # checkpoint\n        if avg_val_loss < best_valid_loss:\n            best_epoch = epoch\n            best_valid_loss = avg_val_loss\n            save_checkpoint('malaysia.pth', model, optimizer)\n\n    # create predictions\n    # val_preds['fold{}'.format(fold_id+1)] = [valid_preds_fold, training_labels[valid_idx]]\n\n    load_checkpoint('malaysia.pth', model, optimizer)            \n    for i,(x_img, x_batch,x_name_batch, tr_breed1_batch, tr_breed2_batch, tr_gen_batch, tr_col1_batch, tr_col2_batch,\n                    tr_col3_batch, tr_state_batch,tr_vac_batch,tr_msize_batch,tr_dworm_batch,tr_health_batch,tr_rid_batch,tr_ster_batch,\n                    tr_ty_batch,tr_tx_batch,tr_fl_batch,x_feats_batch, ) in enumerate(test_loader):\n        y_pred = model(x_img, x_batch,x_name_batch,tr_breed1_batch, tr_breed2_batch, tr_gen_batch, tr_col1_batch, tr_col2_batch,\n                        tr_col3_batch, tr_state_batch,tr_vac_batch,tr_msize_batch,tr_dworm_batch,tr_health_batch,tr_rid_batch,tr_ster_batch,\n                        tr_ty_batch,tr_tx_batch,tr_fl_batch,x_feats_batch).detach()\n        test_preds_fold[i * batch_size:(i+1) * batch_size] = softmax(y_pred.cpu().numpy())\n\n    train_preds[valid_idx] = valid_preds_fold\n    test_preds += test_preds_fold / len(splits)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab820895160c684611ade34ea63ec922615ec63b"},"cell_type":"code","source":"final_predictions = np.argmax(test_preds, axis=1)\nsubmit_df = pd.read_csv(f'{FILE_DIR}/test/sample_submission.csv')\nsubmit_df['AdoptionSpeed'] = final_predictions\nsubmit_df.to_csv(\"submission.csv\", index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88703e71586bcae61652e5e94ec88c311950408b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b986ba5ddb97ef92587e1db6ec26d7aede61571d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d17471fd0cb9e20eb4cb25788523cd9eb6a3d6f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}